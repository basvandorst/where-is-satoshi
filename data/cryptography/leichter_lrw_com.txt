
@_date: 2008-12-06 22:51:23
@_author: Jerry Leichter 
@_subject: Attacking a secure smartcard 
I've previously mentioned Flylogic as a company that does cool attacks  on chip-level hardware protection.  In  , they talk about attacking the ST16601 Smartcard - described by the  vendor as offering "Very high security features including EEPROM flash  erase (bulk-erase)".  The chip is covered by a metal mesh that, if cut  or shorted, blocks operation.  However, Flylogic reports:
"Using our techniques we call, ?magic? (okay, it?s not magic but we?re  not telling), we opened the bus and probed it keeping the chip alive.   We didn?t use any kind of expensive SEM or FIB.  The equipment used  was available back in the 90?s to the average hacker!  We didn?t even  need a university lab.  Everything we used was commonly available for  under $100.00 USD.
This is pretty scary when you think that they are certifying these  devices under all kinds of certifications around the world."
                                                         -- Jerry

@_date: 2008-12-07 15:05:52
@_author: Jerry Leichter 
@_subject: AES HDD encryption was XOR 
Oh, but that 512-byte block is generated using Triple AES, and is  highly, highly secure!  :-)
An interesting bit of wording from the site linked to above:   "According to current cryptography research, this would be virtually  impossible, even with a short key length of only 128 bits."  Although  the sentence accurately states that AES-128 is thought to be secure  within the state of current and expected cryptographic knowledge, it  propagates the meme of the "short key length of only 128 bits".  A key  length of 128 bits is beyond any conceivable brute force attack - in  and of itself the only kind of attack for which key length, as such,  has any meaning.  But, as always, "bigger *must* be better" - which  just raises costs when it leads people to use AES-256, but all too  often opens the door for the many snake-oil "super-secure" cipher  systems using thousands of key bits.
                                                        -- Jerry

@_date: 2008-12-15 21:30:03
@_author: Jerry Leichter 
@_subject: CPRNGs are still an issue. 
Agree in general.  Just one point:
This strikes me as additional complication for little purpose.  Keep  the same AES key - in fact, it might even be useful to either store  the generated key schedules or even to generate open code for the  particular device-specific key.  Take the real time clock's value for  the upper 64 bits of a the input to AES, and use a counter starting at  0 for the lower 64 bits.  As long as the precision of the RTC is  sufficient that you can never have two boots with the same value,  you're fine.  (If you actually have a bigger RTC values you can throw  away low-order bits.)
Given that we *are* assuming an SSD, of course, you could presumably  store values across boots - though there are advantages to the RTC,  since it avoids having to have special cases for things like the  initialization of the stored value and recovery if the SSD is replaced.
                                                         -- Jerry

@_date: 2008-12-16 17:15:55
@_author: Jerry Leichter 
@_subject: CPRNGs are still an issue. 
Configuration at installation seems to be worth considering.  It's a  matter of making that as easy as possible.  Asking users for the AES  key is not easy - people aren't good at generating, or even entering,  random 128-bit strings.  However, you might be able to get them to  push a reset button - or even connect and disconnect the device - a  number of times and use the timing as a source of entropy.  For  something like a network interface, it might be reasonable to assume  that an attacker is unlikely to be present at exactly the time of  initial configuration, so simply pulling bits off the wire/out of the  air during initialization isn't unreasonable.  In general, given the  assumption that it's easier to keep the initialization environment  reasonably secure than it is the general fielded environment, and that  you can afford much more time during initial configuration than is  likely during normal operation, all kinds of things that are marginal  if used operationally may be workable for initial configuration.   (Also, of course, operational use may be unattended, but in most cases  you can assume that initial configuration is attended.)
                                                         -- Jerry

@_date: 2008-12-16 17:23:20
@_author: Jerry Leichter 
@_subject: CPRNGs are still an issue. 
They don't seem to be doing very much yet - and the problems are very  real.  All sorts of algorithms assume that an instance of a running OS  has some unique features associated with it, and at the least (a)  those will be fairly stable over time; (b) there will never be two  instances at the same time.  In different contexts and uses,  virtualization breaks both of these.  The virtual image captures  everything there is to say about the running OS and all its  processes.  Nothing stops you from running multiple copies at once.   Nothing stops you from saving an image, so replaying the same machine  state repeatedly.  Conversely, if something in the underlying hardware  is made available to provide uniqueness of some kind, the ability to  stop the VM and move it elsewhere - typically between almost any two  instructions - means that you can't rely on this uniqueness except in  very constrained ways.
People move to virtualization with the idea that a virtual machine is  just like a physical machine, only more flexible.  Well - it's either  "just like", or it's "more flexible"!  It can't be both.  In fact,  "more flexible" is what sells virtualization, and the effects can be  very subtle and far-reaching.  We don't really understand them.
                                                         -- Jerry

@_date: 2008-12-17 13:02:58
@_author: Jerry Leichter 
@_subject: CPRNGs are still an issue. 
I'm not a device guy either, but I've had reason to learn a bit more  about SSD's than is widely understood.
SSD's are complicated devices.  Deep down, the characteristics of the  underlying storage are very, very different from those of a disk.   Layers of sophisticated hardware/firmware intervene to make a solid- state memory look like a disk.  To take a very simple example:  The  smallest unit you can read from/write to solid state memory is several  times the size of a disk block.  So to allow software to continue to  read and write individual "disk blocks", you have to do a layer of  buffering and blocking/deblocking.  A much more obscure one is that  the throughput of the memory is maximum when you are doing either all  reads or all writes; anywhere in between slows it down.  So higher- performance SSD's play games with what is essentially double  buffering:  Do all reads against a segment of memory, while sending  writes to a separate copy as well as a look-aside buffer to satisfy  reads to data that was recently written.  Switch the roles of the two  segments "at some point".
Put all this together and the performance visible even at the OS  driver level will certainly show all kinds of variation.  However,  just because there's variation doesn't mean there's entropy to be  had!  You'd need to have a sufficiently detailed model of the inner  workings of the SSD to be confident that the variations aren't  predictable.  However, you're not likely to get that:  Getting good  performance out of SSD's is a black art.  The techniques are highly  proprietary right now, because they are what make an SSD competitive.   Further, of course, anything you did learn would likely apply to one  manufacturing run of one model - just about anything could change at  any time.
So ... use with extreme caution.  Estimate conservatively.  Mix any  apparent entropy you get with other sources.
                                                         -- Jerry

@_date: 2008-12-17 17:03:46
@_author: Jerry Leichter 
@_subject: CPRNGs and assurance... 
Excellent points.
For the particular case of random generators based on mixing multiple  sources, I would suggest that there are some obvious - if, apparently,  little-used - testing strategies that will eliminate the most common  failure modes:
1.  Test the combiner.  The combiner is a deterministic function.  If  you give it known inputs, the results will always be the same.  The  result is supposed to depend sensitively on all the inputs, so if you  change any input, you should get very outputs.  This kind of testing  would have avoid the Debian fiasco.
Note that knowing you have to write such a test will also discourage  throwing in all sorts of complexity you don't understand because "it  can't hurt".  It can, and has.
2.  There are many tests you can apply that will detect *non*- randomness.  Test the *inputs* to your combiner.  If an input  consistently fails, think about whether it's adding adding enough  value to be worth the complexity.  If your inputs normally succeed and  start failing ... something is wrong.
Since it's cheap to do, you might as well apply the same test to the  output of the combiner - but don't expect to learn anything:  With any  decent combiner, even fixed inputs should produce random-looking  output.  So any problem detected this way is very serious.
                                                         -- Jerry

@_date: 2008-12-24 06:42:43
@_author: Jerry Leichter 
@_subject: Security by asking the drunk whether he's drunk 
Just one minor observation:
This one is a bit more complicated.  Attackers have access to large  amounts of money *in relatively small units*.  No matter how many  credit card accounts you steal, it would be pretty much impossible to  create an actual, properly populated, physical storefront in a decent  shopping area.  You can be fairly confident that a physical store is  what it appears to be.
Granted, what you're discussing is on-line fraud.  My point is that  this is yet another difference between the on-line and brick-and- mortar worlds, and one that leads us astray when we try to apply our  real-world reasonableness filters to the on-line world.  There are  many inter-related elements here.  Perhaps the biggest factor is  *time*:  On-line frauds can be setup, draw in victims, and disappear  very quickly - only to reappear someplace else.  This allows them to  built using what is effectively the float on stolen identities - much  of which will be found and revoked by the end of a billing cycle.  The  real world has much more inertia - there are many steps involved in  building out a physical storefront, they take time, and your money has  to be "good" across that entire time.  Note that many real-world  frauds rely on the ability to short-cut what are normally time- consuming procedures and disappear before the controls can kick in.   (Think of check kiting, or of the guys from what appear to be long- established local paving companies that "pave" your driveway with  cheap oil and are gone by the next morning.)
EV certificates (unsuccessfully) attempt to bring some of this real- world checking on line:  They are expensive, and you have to pay in  one lump.  They're not going to accept a bunch of credit cards.  They  check your identity, which if done right takes time *and indirectly  checks that you actually have a history*.  Of course, the actual  practice is different and, given the incentives in the industry -  where there is no penalty for giving out an invalid EV certificate,  and a reward for getting the job done quickly - this is all illusion.
Long-running frauds, while certainly not unknown (hello, Bernie  Madoff), are relatively rare:  Every day out there is another chance  to get caught.  The preferred mode of fraud will always be "get 'em  hooked, fleece 'em, get out of town - as fast as you can".  Can we get  some of the advantages of this real-world fact in the on-line world?   The best example I know of is CMU's Perspectives effort:  If something  "looks the same" to many observers over a period of time, it's more  likely to be trustworthy.  Of course, if this kind of thing catches  on, it will be much harder for a startup to gain instant recognition.   The Internet "need for speed" isn't compatible with safety.  Some  tradeoffs are inevitable.
                                                         -- Jerry

@_date: 2008-12-27 07:19:28
@_author: Jerry Leichter 
@_subject: Security by asking the drunk whether he's drunk 
I just had an interesting experience with a different sort of  failure:  I tried to buy a DVD from The Teaching Company ( ).  When I went to check out - or even if when I connect to the top  level at  - I get a complaint that their cert  is signed  by a unknown authority.  It turns out that they recently  put an EV certificate in place.  It's issued by "VeriSign Class 3  Extended Validation SSL SGC CA" - which neither Safari 3.2.1 nor  Firefox 3.0.5 on my Mac have ever heard of!
I got in touch with the company and actually received intelligent  responses both at their 800 number - I placed my order that way - and  in a response from their customer service people.  Most remarkable -  almost all organizations ignore such communication.  It's ironic that  those who appear to be trying the hardest are being screwed over by  the system that's currently in place - and will inadvertently be  involved in training users to simply bypass yet another kind of bad  cert warning.
(I can highly recommend the courses that The Teaching Company  distributes, by the way.  I usually borrow them from the library, but  I've bought a few of the best here and there - especially when they  have sales, as they do right now.)
                                                         -- Jerry

@_date: 2008-12-27 22:22:57
@_author: Jerry Leichter 
@_subject: Security by asking the drunk whether he's drunk 
Try                                                          -- Jerry

@_date: 2008-12-28 21:48:01
@_author: Jerry Leichter 
@_subject: very high speed hardware RNG 
The thing that bothers me about this description is the too-easy jump  between "chaotic" and "random".  They're different concepts, and  chaotic doesn't imply random in a cryptographic sense:  It may be  possible to induce bias or even some degree of predictability in a  chaotic system by manipulating its environment.  I believe there are  also chaotic systems that are hard to predict in the forward  direction, but easy to run backwards, at least sometimes.
That's not to say this system isn't good - it probably is - but just  saying its chaotic shouldn't be enough.
BTW, a link from this article - at least when I looked at it - went to    : "Quantum Computing: Entanglement May Not Be Necessary".  There are  still tons of surprises in the quantum computing arena....
                                                         -- Jerry

@_date: 2008-12-30 17:11:41
@_author: Jerry Leichter 
@_subject: very high speed hardware RNG 
I don't think this quite captures the situation.  It's easy to give a  formal definition of randomness; it's just not clear that such a thing  can ever be realized.
Here's one definition:  A random bitstream generator is an "isolated"  source of an infinite stream of bits, numbered starting at zero, with  the property that a Turing machine, given as input anything about the  universe except the internal state of the "isolated" source, and bits  0 to n generated by the source, has no better than a 50% chance of  correctly guessing bit n+1.  The difficulty is entirely in that quoted  "isolated".  It's not so much that we can't define it as that given  any definition that captures the intended meaning, there are no known  systems that we can definitely say are "isolated" in that sense.   (Well, there's kind of an exception:  Quantum mechanics tells us that  the outcome of certain experiments is "random", and Bell's Theorem  gives us some kind of notion of "isolation" by saying there are no  hidden variables - but this is very technically complex and doesn't  really say anything nearly so simple.)
A while back, I wrote to this list about some work toward a stronger  notion of "computable in principle", based on results in quantum  mechanics that limit the amount of computation - in the very basic  sense of bit flips - that can be done in a given volume of space- time.  The argument is that a computation that needs more than this  many bit flips can't reasonably be defined as possible "in principle"  just because we can describe what such a computation would look like,  if the universe permitted it!  One might produce a notion of "strongly  computationally random" based on such a theory.  Curiously, as I  remarked i that message, somewhere between 128 and 256 bits of key, a  brute force search transitions from "impractical for the forseeable  future" to "not computable in principle".  So at least for brute force  attacks - we're actually at the limits already.  Perhaps it might  actually be possible to construct such a "random against any  computation that's possible in principle" source.
I don't like the notion of "opaqueness" in the context.  That just  means we can't see any order that might be in there.  There's a  classic experiment - I think Scientific American had pictures of this  maybe 10 years back - in which you take a pair of concentric  cylinders, fill the gap with a viscous fluid in which you draw a line  with dye parallel to the cylinders' common axis.  Now slowly turn the  inner cylinder, dragging the dye along.  This is a highly chaotic  process, and after a short time, you see a completely random-looking  dispersion of dye through the liquid.  Present this to someone and any  likely test will say this is quite random.  But ... if you slow turn  the inner cylinder backwards - "slowly", for both directions of turn,  depending on details of the system - the original line of dye  miraculously reappears.
That's why it's not enough to have chaos, not enough to have  opaqueness.  The last thing you want to say is "this system is so  complicated that I can't model it, so my opponent can't model it  either, so it's random".  To the contrary, you *want* a model that  tells you something about *why* this system is hard to predict!
This simply says that *known* bias and randomness are completely  separate notions.  I can always get rid of any *known* bias.  Bias  that's unknown/unmodeled to me as the *user* of the system, on the  other hand, is very dangerous if an attacker might conceivably know  more about the bias than I do.
                                                         -- Jerry

@_date: 2008-12-30 22:19:39
@_author: Jerry Leichter 
@_subject: Security by asking the drunk whether he's drunk 
============================== START ==============================
Robert Graham writes in Errata Security ( ) that the attack depends on being able to predict the serial number  field that will be assigned to a legitimate certificate by the CA.   Only a few CA's use predictable "serial numbers" - the field is  actually arbitrary text and need only be certainly unique among all  certificates issued by a given CA.
Of course, we've seen in the past that having too much freedom to  insert "known to be random" (hence uncheckable) stuff into a signed  piece of text can itself be hazardous in other ways.
So:  The current attack is only effective against a very small number  of CA's which both use MD5 *and* have predictable sequence numbers.   So the sky isn't falling - though given how hard it is to "decertify"  a CA (given that the "known good" CA's are known to literally billions  of pieces of software, and that hardly anyone checks CRL's - and are  there even CRL's for CA's?) this is certainly not a good situation.
This also doesn't mean that, now that the door has been opened, other  attacks won't follow.  In fact, it's hard to imagine that this is the  end of the story....
                                                         -- Jerry

@_date: 2009-08-01 08:01:26
@_author: Jerry Leichter 
@_subject: Manipulation and abuse of the consumer credit reporting agencies  
Found on the Telecom list (which I've subscribed to for years but  almost never read any more).  The paper is quite interesting.
                                                         -- Jerry
Manipulation and abuse of the consumer credit reporting agencies
by Christopher Soghoian
First Monday
Volume 14, Number 8
3 August 2009
This paper will present a number of loopholes and exploits against
the system of consumer credit in the United States that can enable a
careful attacker to hugely leverage her (or someone else's) credit
report for hundreds of thousands of dollars. While the techniques
outlined in this paper have been used for the personal (and legal)
profit by a small community of credit hackers, these same techniques
could equally be used by more nefarious persons - that is, criminals
willing to break the law, engage in fraud, and make off with
significant sums of money. The purpose of this paper is to shed light
on these exploits, to analyze them through the lens of the computer
security community and to propose a number of fixes which will
significantly reduce the effectiveness of the exploits, by both those
with good and ill intentions.

@_date: 2009-08-01 17:06:16
@_author: Jerry Leichter 
@_subject: The clouds are not random enough 
The problem is broader than this.  A while back, I evaluated a  technology that did it best to solve a basically insoluble problem:   How does a server, built on stock technology, keep secrets that it can  use to authenticate with other servers after an unattended reboot?   Without tamper-resistant hardware that controls access to keys,  anything the software can get at at boot, an attacker who steals a  copy of a backup, say - can also get at.  So, the trick is to use a  variety of measurements of the hardware - amount of memory, disk  sizes, disk serial numbers, whatever you can think of that varies from  machine to machine and is not stored in a backup - and combines them  to produce a key that encrypts the important secrets.  Since hardware  does need to be fixed or upgraded at times, a good implementation will  use some kind of "m unchanged out of n measurements" algorithm.   Basically, this is the kind of thing Microsoft uses to lock license  keys to particular instances of hardware.  Yes, it can be broken - but  you can make breaking it a great deal of work.
Virtualization changes all of this.  Every copy of a virtual machine  is will be identical as far as most of these measurements are  concerned.  Conversely, if you try to let the physical level show  through - e.g., use the disk serial number of the real disk on which a  virtual disk lives - you disrupt some of the things VM's are trying to  provide, lie easy transportability of instances from one hardware  "home" to another.  The last I heard about the technology I looked at,  they didn't have any good solution for VM's (though I haven't kept up  and don't know the current status).
Ultimately, the only solution is for hypervisors to take on some  security roles - passing along unforgeable ID's and random numbers  from hardware and other resources that they have access to but do not  export to the guest OS's. That doesn't *solve* the problem.  It puts  us back where we were before the virtualization craze:  Needing to  write a secure OS and various secure
services.  However, since hypervisors are much smaller and *much* more  limited in operation than full OS's, so the problems may be  correspondingly easier to solve.
                                                         -- Jerry

@_date: 2009-08-04 07:24:26
@_author: Jerry Leichter 
@_subject: Vulnerable keyboards 
A couple of weeks ago, Apple distributed a firmware update for their  keyboards - the standalone ones, not the ones built into laptops.  I  remarked at the time (perhaps on this list?) that given a way for  Apple to update the firmware ... was there a way for others with  malicious intent?  Well ... yes.     reports on reverse-engineering a previous keyboard patch and  producing a functional patch of his own.
                                                         -- Jerry

@_date: 2009-08-09 08:52:07
@_author: Jerry Leichter 
@_subject: All your notebook belong to us 
Just about all notebooks shipped in the last 5 years or more contain a  helpful bit of code in the BIOS that allows for remote tracing in case  of theft.  Unfortunately, it's got serious security holes, allowing it  to be used for much more nefarious purposes - like rootkits that  survive disk reformatting and OS installation:
*What* are the guys producing this junk thinking?  It's all there  supposedly "to help customers" - but I have yet to see an actual  service offering that uses this stuff.  So right now it's all  negatives, no positives.
                                                       -- Jerry

@_date: 2009-08-09 21:48:45
@_author: Jerry Leichter 
@_subject: cleversafe says: 3 Reasons Why Encryption is Overrated 
Since people do keep bringing up Moore's Law in an attempt to justify  larger keys our systems "stronger than cryptography," it's worth  keeping in mind that we are approaching fairly deep physical limits.   I wrote about this on this list quite a while back.  If current  physical theories are even approximately correct, there are limits to  how many "bit flips" (which would encompass all possible binary  operations) can occur in a fixed volume of space-time.  You can turn  this into a limit based solely on time through the finite speed of  light:  A computation that starts at some point and runs for n years  can't involve a volume of space more than n light years in radius.   (This is grossly optimistic - if you want the results to come back to  the point where you entered the problem, the limit is n/2 light years,  which has 1/8 the spacial volume).  I made a very approximate guess at  how many bit-flips you could get in a time-space volume of a 100 light- year sphere; the answer came out somewhere between 2^128 and 2^256,  though much closer to the former.  So physical limits prevent you from  doing a brute force scan - in fact, you can't even enumerate all  possible keys - in 100 years for key lengths somewhere not much more  than 128 bits.
It's rather remarkable that such fundamental limits on computation  exist at all, but physics over the last 100 years - and especially  over the last couple of decades - has increasingly shown us that the  world is neither continuous nor infinite; it has solid finite limits  on almost everything.  Even more remarkable is that we've pretty much  reached some of those limits.  For any recently designed cryptosystem,  brute force is simply out of the question, and will remains so forever  (unless we are very much mistaken about physics).  Moore's Law as a  justification for using "something more" makes no sense.
As you point out, the story for advances in cryptographic theory is  much more complex and impossible to predict.  That cryptographic  advances would render the "safer" AES-256 at risk while AES-128  remains secure (for now) is something no one could have predicted,  though in retrospect some of the concerns about the key scheduling may  have been right.  All the protocols and standards out there calling  for AES-256 - it's obviously "better" than AES-128 because after all  256 is *twice as large* as 128! - were just a bunch of nonsense.  And,  perhaps, dangerous nonsense.
                                                         -- Jerry

@_date: 2009-08-11 19:47:54
@_author: Jerry Leichter 
@_subject: brute force physics Was: cleversafe... 
When the first results about exponential speedup of factoring came  out, people assumed that this was true in general.  But it isn't.  In  particular, simple search, where you have only an equality test so you  can't build a hash table or some kind of ordered structure - is O(N)  on a traditional computer - and O(sqrt(N)) on a quantum computer.  I'm  not sure what the current knowledge about what a quantum machine can  do for NP computations, but there's no "probably" here.
The physical arguments to which I was referring say *nothing* about  how the computation is done.  It can be a QM computation as well.
In any case, the simple search result above applies directly to brute  force:  For that problem, you only get a polynomial speedup anyway.
That's a ... bizarre point of view.  :-)  Should freedom from related- key attacks be part of the definition of a "secure" encryption  algorithm?  We should decide that on some rational basis, not on  whether, with care, we can avoid such attacks.  Clearly, a system that  *is* secure against such attacks is more robust.  Do we know how to  build such a thing?  What's the cost of doing so?  But to say it's an  *advantage* to have a weakness seems like some kind of odd moral  argument:  If you're hurt by this it's because you *deserve* to be.
                                                         -- Jerry

@_date: 2009-08-11 21:27:09
@_author: Jerry Leichter 
@_subject: Ultimate limits to computation 
It must be the summer weather or something.  I've received a whole  bunch of messages - mainly privately - that say either "Here's another  result that has a higher upper bound on computation" or "Here's a  design for a machine that exceeds your bound".  Both ignore (a) how  bounds work:  That fact that you have a weaker bound doesn't mean I  don't have a stronger one; (b) that impossibility results can exist in  physics, not just in mathematics.  True, the nature of such results  are a bit different, since all our current physical theories might  turn out to be wrong.  But, hey, maybe our understanding of  computation or even mathematics has some fundamental flaw, too.
The estimate on the limits to brute-force search are mine, based on a  *very* rough estimate that draws on the results in the following  paper:  (I haven't actually read the paper; my analysis was based on an  article I can't find that discussed the implications of this one.)
The basic summary of the author's result is:  "[T]he total number of  bits and number of operations for the universe does not exceed  O(10^123)."  I guessed about how this value scales (as the cube of the  time - one factor for time, two for the size of the light sphere you  can reach in that time; not 3 because the information content of space  goes up as the area, *not* the volume - a very weird but by now  standard result).
Now, my scaling technique may be completely flawed, or my computations  may be wrong.  Or the paper could be wrong.  (I wouldn't bet on it.)   Or the physics may be wrong.  (I *really* wouldn't bet on that.)  But  the fact that there are other bounds around that are not as tight, or  that one can describe a machine that would do better if there were a  way to realize it, aren't evidence for any of these.  Bounds can be  improved, and a description isn't a working machine.
In fact, the whole point of the article that I *did* read is that this  result should make use re-examine the whole notion of a "possible"  computation.  It's easy to describe a computation that would take more  than 10^123 steps.  Ackerman's function exceeds that for pretty small  input values.  We've traditionally said that a computation is  "possible" if we can describe it fully.  But if it couldn't be  realized by the entire universe - is that really a *useful* notion of                                                           -- Jerry

@_date: 2009-08-27 07:16:48
@_author: Jerry Leichter 
@_subject: a crypto puzzle about digital signatures and future compatibility 
"Good enough" for what purpose?
By hypothesis, "SHA-3" is secure, so computing some additional  function can't hurt v1.7 readers.
On the other hand, v1.6 readers are still relying on the the, by  hypothesis, fallen SHA-2, so you haven't *directly* helped them at all.
*Indirectly*, any v1.7 reader who noticed that the SHA-2 hashes agreed  but the SHA-3 matches did not would have detected the attack.  The  question is, what could he do about it?  You can fall back to extra- protocol mechanisms - e.g., he's asked to send a message to the Tahoe- LAFS mailing list about it - but if we posit a world of a large number  of independent users of Tahoe-LAFS, that probably won't reach the  right audience.  Writing to the owner of the file sounds nice on  paper, but he may be anonymous, and even if he isn't, his *readers*  may be anonymous to him.
If you really want to rely on detection by v1.7 users actually  contributing to the safety of the larger community, you'd need to  build a support mechanism into the system itself.  Something like a  "Caution!" flag - which, of necessity, anyone could set, and whose  semantics would be to give any user the warning "make sure you're  running the latest version of the software, someone has noticed  something odd about this file which your version isn't sensitive to" -  would work.  If there's a reliable way to ensure that you're running  the latest version of the software, you can ignore the "Caution!"  flag, thus minimizing the effect of "caution flag spamming".   (Presumably, you store the latest version in Tahoe-LAFS somewhere ...  though you perhaps want to add some additional security on that, like  a separate signature, in case *it* gets its "Caution!" flag set.)
On a general note:  The concatenation of multiple hashes can't  possibly be less secure than the most secure of the individual hashes  (at least in protocols where the actual data being hashed is already  known to the receiver; if first pre-image resistance matters to you,  the identity function as a hash will obviously give you problems).   The attacks known have to find delicate balancing changes to the  input.  Even a very simple additional check - say a CRC - would block  them as they stand.  (That's not to say matching on an additional CRC  couldn't be incorporated into an attack if someone was interested in  doing it.)
                                                         -- Jerry

@_date: 2009-08-28 10:56:10
@_author: Jerry Leichter 
@_subject: Practical attack on WPA? 
A Practical Message Falsi?cation Attack on WPA
Toshihiro Ohigashi and Masakatu Morii
Abstract. In 2008, Beck and Tews have proposed a practical attack on
WPA. Their attack (called the Beck-Tews attack) can recover plaintext
from an encrypted short packet, and can falsify it. The execution time
of the Beck-Tews attack is about 12-15 minutes. However, the attack
has the limitation, namely, the targets are only WPA implementations
those support IEEE802.11e QoS features. In this paper, we propose a
practical message falsi?cation attack on any WPA implementation. In
order to ease targets of limitation of wireless LAN products, we apply
the Beck-Tews attack to the man-in-the-middle attack. In the man-in-
the-middle attack, the user?s communication is intercepted by an  until the attack ends. It means that the users may detect our attack  the execution time of the attack is large. Therefore, we give methods  reducing the execution time of the attack. As a result, the execution  of our attack becomes about one minute in the best case.
                                                         -- Jerry

@_date: 2009-08-28 11:03:16
@_author: Jerry Leichter 
@_subject: =?UTF-8?Q?=22Defending_Against_Sensor-Snif=EF=AC=81ng_Attacks_on?= 
Modern mobile phones possess three types of capabilities:
computing, communication, and sensing. While these capa-
bilities enable a variety of novel applications, they also raise
serious privacy concerns. We explore the vulnerability where
attackers snoop on users by sniffing on their mobile phone
sensors, such as the microphone, camera, and GPS receiver.
We show that current mobile phone platforms inadequately
protect their users from this threat. To provide better pri-
vacy for mobile phone users, we analyze desirable uses of
these sensors and discuss the properties of good privacy pro-
tection solutions. Then, we propose a general framework for
such solutions and discuss various possible approaches to
implement the framework?s components.
They have some suggestions, but feel the problems are deep and  probably not completely solvable.
                                                         -- Jerry

@_date: 2009-08-31 07:20:42
@_author: Jerry Leichter 
@_subject: Source for Skype Trojan released 
It can ?...intercept all audio data coming and going to the Skype  Proof of concept, but polished versions will surely follow.
                                                         -- Jerry

@_date: 2009-02-02 17:27:33
@_author: Jerry Leichter 
@_subject: full-disk subversion standards released 
I don't see that.  The problem being solved is exactly a DRM problem:   A gives B some data but wants to retain control the circumstances in  which B can use that data.  The algorithm proposed implements three  fundamental controls:  (a) B can only access the data through a  particular program that A trusts; (b) B can "return" the data, along  with a proof that he never actually accessed it; (c) A can then revoke  B's access to the data (although the algorithm bundles this with  (b)).  (a) and (c) are exactly the kind of thing DRM implementations  do all the time - and exactly the kind of thing that's been widely  discussed for TPM.  (b) is novel.
DRM has to do with retaining access to data that has been provided to  an untrusted party.  The entertainment industry considers its  customers untrusted, so TPM in its primary use cases is about  controlling what those customers - i.e., all consumers of computers! -  can do.  In Ryan's use case, the untrusted parties are the government  security services.  One can construct other untrusted parties as  well.  In a cloud-computing world, wouldn't it be nice to know that  your data, all though it's "out there", being operated on by all kinds  of programs "out there", is still under your control?  The problem  isn't with "DRM" in the large sense - it's that once you enable "DRM"  in the large sense, "DRM" in the small sense (as the entertainment  industry already sees it, and as many others will once the capability  is there) seems to be unavoidable.  It's a matter of tradeoffs.   (Notice that the same people who say this tradeoff isn't worth it will  also say that the tradeoffs of broadly available crypto - yes, it  protects privacy, but that includes the privacy of criminals.  I don't  think there's any broad principle that is being applied here - it's a  case by case analysis of the good and bad effects of particular  technologies.  The DRM debate in particular is inherently tainted by  the actions and attitudes of the entertainment industry.)
                                                         -- Jerry

@_date: 2009-02-04 00:01:18
@_author: Jerry Leichter 
@_subject: "Nato's cyber defence warriors " 
Interesting article from the BBC on the "state of play" in cyber  attack and defense.  Not much depth - I'm sure you weren't expecting  it, given the source - but worth looking at.
                                                         -- Jerry

@_date: 2009-02-14 15:20:54
@_author: Jerry Leichter 
@_subject: "Italy police warn of Skype threat" 
"Criminals in Italy are increasingly making phone calls over the  internet in order to avoid getting caught through mobile phone  intercepts, police say...."
Whole article at                                                          -- Jerry

@_date: 2009-02-14 15:24:40
@_author: Jerry Leichter 
@_subject: "Italy police warn of Skype threat" [USE THIS VERSION - left some stuff out] 
"Criminals in Italy are increasingly making phone calls over the  internet in order to avoid getting caught through mobile phone  intercepts, police say...."
But that's only half the story.  Much of the issue goes in the other  direction:  The Italian press apparently gets much of its information  from crime reporting from wiretaps, often of government officials!  So  the government is trying to control both directions:  Preserve its  ability to wiretap while stopping the press from doing so.
Whole article at                                                         -- Jerry

@_date: 2009-02-20 14:00:18
@_author: Jerry Leichter 
@_subject: The password-reset paradox 
I suspect some very biased analysis.  For example, people who really  need their passwords reset regularly will probably lose their tokens  just as regularly.  The cost of replacing one of those is high - not  for the token itself, but for the administrative costs, which *must*  be higher than for a password reset since they include all the work in  a password reset (properly authenticating user/identifying account  probably contribute the largest costs), plus all the costs of  physically obtaining, registering, and distributing a replacement  token - plus any implied costs due to the delays needed to physically  deliver the token versus the potential for an instantaneous reset.
I suppose the $100-$200 estimate might make sense for an organization  that actually does password resets in a secure, carefully managed  fashion.  Frankly ... I, personally, have never seen such an  organization.  Password resets these days are mainly automated, with  authentication and identification based on very weak secondary  security questions.  Even organizations you'd expect to be secure  "authenticate" password reset requests based entirely on public  information (e.g., if you know the name and badge number of an  employee and the right help desk to call, you can get the password  reset).  New passwords are typically delivered by unsecured email.   All too many organizations reset to a fixed, known value.
It's quite true that organizations have found the costs of password  resets to be too high.  What they've generally done is saved money on  the reset process itself, pushing the cost out into whatever budgets  will get hit as by the resulting security breaches.
                                                         -- Jerry

@_date: 2009-02-22 14:37:13
@_author: Jerry Leichter 
@_subject: Shamir secret sharing and information theoretic security 
It is.  Knowing some of the coefficients, or some constraints on some  of the coefficients, is just dual to knowing some of the points.   Neither affects the security of the system, because the coefficients  *aren't secrets* any more than the values of f() at particular points  are.  They are *shares* of secrets, and the security claim is that  without enough shares, you have no information about the remaining  The argument for information-theoretic security is straightforward:   An n'th degree polynomial is uniquely specified if you know its value  at n+1 points - or, dually, if you know n+1 coefficients.  On the  other hand, *any* set of n+1 points (equivalently, any set of n+1  coefficients) corresponds to a polynomial.  Taking a simple approach  where the secret is the value of the polynomial at 0, given v_1,  v_2, ..., v_n and *any* value v, there is a (unique) polynomial of  degree at most n with p(0) = v and p(i) = v_i for i from 1 to n.   Dually, the value p(0) is exactly the constant term in the  polynomial.  Given any fixed set of values c_1, c_2, ..., c_n, and any  other value c there is obviously a polynomial p(x) = Sum_{0 to n}(c_i  x^i), where c_0 = c, and indeed p(0) = c.
Or ... in terms of your problem:  Even if I give you, not just a pair  of linear equations in c1, c2, and S, but the actual values c1 and c2  - the constant term (the secret) can still be anything at all.
The description above is nominally for polynomials over the reals.  It  works equally for polynomials over any field - like the integers mod  some prime, for example.  For a finite field, there is an obvious  interpretation of probability (the uniform probability distribution),  and given that, "no information" can be interpreted in terms of the  difference between your a priori and a posterio estimates of the  probability that p(0) takes on any particular value, the values of  p(1), ..., p(n) (and that differences is exactly 0).  Because there  can be no uniform probability distribution over all the reals, you  can't state things in quite the same way, and "information theoretic  security" is a bit of a vague notion.  Then again, no one does  computations over the reals.  FP values - say, IEEE single precision -  aren't a field and there are undoubtedly big biases if you try to use  Shamir's technique there.  (It should work over infinite-precision                                                           -- Jerry

@_date: 2009-02-22 15:14:42
@_author: Jerry Leichter 
@_subject: "Sweden's air force 'can't send secret messages'" 
Summary:  Sweden developed its own secure encryption system for  communicating with fighter jets.  A new jet, which is scheduled to  replace all existing fighters by 2011, uses a NATO-standard encryption  system - only.  There is no plan in place to upgrade the ground  systems to the NATO standard.  So the new jets must communicate in the                                                           -- Jerry

@_date: 2009-02-23 14:03:01
@_author: Jerry Leichter 
@_subject: Shamir secret sharing and information theoretic security 
I've never seen any work done in this direction.  When you consider  exact values, FP arithmetic is very messy and has almost no nice  mathematical properties.  (It's nice in a model where all you care  about is relative error - which is actually a rather unnatural  model!)  As a result, I think it's unlikely you can come up with any  general theory here.  But you can probably come up with examples  showing that there's a problem.  It's usually easiest to work with a  simpler form of FP math - e.g., assume 4 decimal digits and a 1-digit  decimal exponent.  Consider just quadratics, which we can write as
p(x) = (x - r1)*(x - r2).  If r1*r2 overflows in a particular FP  system, you can't write down the value of the constant coefficient -  hence, you can't write down the value p(0).  Yet p(1) and p(2) might  have values you *can* write down.  I'm not sure how you leverage this  to produce a bias, but it certainly shows that FP arithmetic just  plain doesn't have the right properties to support the reasoning  behind Shamir secret sharing....
                                                         -- Jerry

@_date: 2009-01-10 06:13:49
@_author: Jerry Leichter 
@_subject: On the topic of "Asking the drunk"... 
I get no response.  None at  either.
On the other hand, the US-specific site,   responds just fine - but it redirects you to  .  Try that same address with https, and it's accepted - but again  redirected to the http version.
That one is at least in the Visa domain.  It gets a bit more complex  for other regions - e.g., the Asian sites are accessible via    - but that redirects to
 - even though
 actual works!
I'm guessing that Visa has country- (or perhaps region-)specific  certs, which would make some sense - but the random mix of http and  https addresses is pretty broken.
It's not clear there's anything at visa.com that's really in need of  protecting, of course.  It's not a card issuer, its member banks are.   Then again ... if you start from  and go to  "Access Account Information", you are sent to a (non-SSL) page that  claims to have links to the largest issuing banks - except that none  of the "links" actually works - which I guess is appropriate, since  you shouldn't be trusting them anyway!
A very strange set of sites....
                                                         -- Jerry

@_date: 2009-01-11 07:01:36
@_author: Jerry Leichter 
@_subject: What risk is being defended against here? 
Not cryptography, but the members of this list think in these terms,  Just recently, my 8th-grade daughter took a school placement test.   This test (the ISEE) is administered internationally.
When we arrived, we learned that she would not be allowed into the  test room without *one* of the following:
The verification letter is actually available - even now, after the  test is complete - on a web site.
So ... just what risk is being defended against here?
You could imagine that the verification letter is essentially a ticket  - the letter itself says thats what it is - but in fact the testing  locations have a complete list of who is supposed to take the test -  and of course you aren't *required* to have it with you.
Many such "high value" tests now require photo id's.  Some go further  - the LSAT's, required with law school applications, fingerprint all  test-takers.  (I think other, similar exams - like the MCAT's for  medical school and the GMAT's for MBA programs do the same.)  There's  an obvious risk here:  I can hire someone to take the test for me.  A  photo ID makes that harder and a fingerprint provides strong evidence  in case any questions arise.  But if I hired someone to take the ISEE  in my daughter's place, presumably I could easily give them a copy of  the verification letter.
I suppose the *combination* of the two does work as a ticket:  Either  you have the actual verification letter, or you name is on the list  and the photo ID proves that that's your name.  Seems a bit elaborate,  especially since taking over someone else's test spot can't gain you  anything - the results will be sent to schools in *their* name, not  yours.  Besides, there's really nothing preventing you from  *registering* in someone else's name to begin with.
Any speculations (beyond bureaucracy at its finest)?
(The actual administration of this requirement was a mess.  How many  kids this age - the exam actually has three levels, so the age range  would be from perhaps 9 to 17 - carry, or even have, photo id's?  The  verification letter itself mentions, with no emphasis, that you should  bring it with you on the test date - a fact not mentioned on the ISEE  web site, where they tell you to bring pencils and pens and not bring  calculators or cell phones.  Moreover, the verification letter can  arrive way before test day - 3.5 months before, in our case.  Luckily,  we live close to the test center, arrived early ... and were able to  rush back home for my daughter's recently-acquired passport, the only  photo ID she actually has.  Many others were caught in the same mess;  some had to leave and reschedule for another day.)
                                                         -- Jerry

@_date: 2009-01-25 07:04:59
@_author: Jerry Leichter 
@_subject: What EV certs are good for 
I just received a phishing email, allegedly from HSBC:
     Dear HSBC Member,
     Due to the high number of fraud attempts and phishing scams, it  has been decided to
     implement EV SSL Certification on this Internet Banking website.
     The use of EV SSL certification works with high security Web  browsers to clearly
     identify whether the site belongs to the company or is another  site imitating that
     company's site....
(I hope I haven't quoted enough to trigger someone's spam detectors!)   Needless to say, the message goes on to suggest clicking on a link to  update your account.
                                                         -- Jerry

@_date: 2009-01-26 16:18:39
@_author: Jerry Leichter 
@_subject: Obama's secure PDA 
I have no information, but a guess:  Phone conversation encryption, at  all levels, has been around for many years.  Email is a relative  newcomer.  Further, the problem for voice is inherently simpler:  A  conversation is transient.  It's not expected to be recorded, and I'm  sure the devices are designed to make recording a conversation  difficult even for someone with full access to the phone.  So you're  dealing with establishing a secure session, with nothing left after  the fact.  If you're talking email, on the other hand, you're  inherently dealing with information at rest.  That changes the whole  game, introducing issues of key management, maintenance of security  level of time - a conversation once completed is gone, so the question  of how to declassify it or move it to another compartment or whatever  cannot arise - how to deal with forwarding, and so on.  All of this is  inherent in a usable email system.  An email system for the White  House has the additional complication of the Presidential Records  Act:  Phone conversations don't have to be recorded, but mail messages  do (and have to remain accessible).
It makes one wonder if this is a Sect?ra limitation, a Sect?ra-for- the-President limitation, or whether there is no Top Secret email  infrastructure at all....
                                                         -- Jerry

@_date: 2009-01-26 16:56:19
@_author: Jerry Leichter 
@_subject: Obama's secure PDA 
The page you mention contains a link to a price list.  The thing is  surprisingly inexpensive:  $3150.  (Curiously, you have a choice of a  1 or 2 year warrantee.  The second year adds $200 to the price.  You  can omit the wireless module and save $500 - presumably of interest if  you already have one - they are also available separately - in Sprint,  Verizon, GSM, and WiFi versions, for $700.)  There are versions for  the UK, Canada, NATO, and some other allies.
There's a "Classified USB Cable for file transfer with Classified PC"  which is "required for installing Classified Enclave Certificates".   (Considering the obscene prices we pay for HDMI cables, this is a  steal at only $75.)  There is a similar "Unclassified USB Cable for  file transfer with Unclass PC" which is  "required for installing  Unclassified Enclave Certificates".  From the sound of it, this  probably means the USB ports are set up to authenticate connections  and, almost certainly, to encrypt everything that leaves the device.   Any conversion to Unclassified form probably occurs on the receiving  "Unclass PC".  There are also both Classified and Unclassified  keyboard/mouse USB cables.  (These are marked as "delivery 6 months  ARO" - everything else is available in 60 days.  The obvious guess is  that these don't really exist, but will be built if anyone wants them.
For $100, there's a 2GB Micro SD card for Unclassified memory  extension; the Classified memory apparently can't be extended.
There's a mail server named "Apriva" that seems to go with this.
Oh, and just to make everyone feel good about these things:  They run  Windows (mentioned in the FAQs).  The FAQ, indirectly, answers the  your previous question of why only Secret for email:  Data-at-rest is  encrypted using AES, which is only approved for Secret, not Top  Secret, data.
                                                         -- Jerry

@_date: 2009-01-27 09:04:45
@_author: Jerry Leichter 
@_subject: What EV certs are good for 
I didn't try it!  While Safari on a Mac has been reasonably secure,  it's not been *entirely* immune to attacks, and it didn't seem like a  good idea to tempt fate.
It might be useful to put together a special-purpose HTTPS client  which would initiate a connection and tell you about the cert  returned, then exit.  Absent a nasty bug in SSL itself, that should be  pretty safe.  (The client might want to go through TOR to avoid adding  your IP address to some spammer database of "IP's that follow links  found in spam", though in practice I doubt that matters much - there  are enough likely victims out there that such a database probably  wouldn't be worth the bother.)
                                                         -- Jerry

@_date: 2009-01-28 16:35:50
@_author: Jerry Leichter 
@_subject: Proof of Work -> atmospheric carbon 
[Proposals to use reversible computation, which in principle consume  no energy, elided.]
There's a contradiction here between the computer science and economic  parts of the problem
being discussed.  What gives a digital coin value is exactly that  there is some real-world expense in creating it.  We talk about "proof  of work", but in fact "work" done by a computer doesn't, in and of  itself, have any value.  It gets a value only when it's a limited  resource *which might have been used for something else* - i.e., the  value of the spare cycles that might be thrown at doing the  computations comes from the opportunity cost incurred.  If this were  not so, anyone could just create as many as they wanted at no cost to  themselves.  In fact, this is behind the cost model 'bot herders using  other people's machines.  But ultimately that only works for the 'bot  herders because there is no significant loss to the owners of those  machines either!
Now, if instead we used algorithms not based on some abstraction  notion of "work", but on the equivalent power that had to be  dissipated to do the computation, then the value of a digital token  would truly be grounded in the real world.  Spare cycles would no  longer be "free" - they would show up on your power bill.  Sure, the  'bot herders wouldn't have to pay - but if the owners of the "pwned"  machines saw a real cost, they would have an incentive to do something  about it (which they basically don't, today).
Eliminating the power cost puts you back to amortizing the fixed cost  of the CPU and memory doing the computation - a cost that's dropping  all the time.  I don't see how you get to an economically viable  mechanism that way.
So, how do you tie the cost of a token to power?  Curiously, something  of the sort has already been proposed.  It's been pointed out - I'm  afraid I don't have the reference - that CPU's keep getting faster and  more parallel and a high rate, but memories, while they are getting  enormously bigger, aren't getting much faster.  So what the paper I  read proposed is hash functions that are expensive, not in CPU  seconds, but in memory reads and writes.  Memory writes are inherently  non-reversible so inherently cost power; a high-memory-write algorithm  is also one that uses power.
(BTW, a number of years back, a VC friend ran by me a proposal to buy  the spare cycles on people's set-top boxes - which have pretty hefty  chips in them - and rent out the resulting "distributed compute  server".  The claim was that you didn't have to pay people much of  anything for use of their boxes - you'd only do it when they were  otherwise unoccupied, so they should be happy to get even very small  payments.  I pointed out the cost they had neglected:  Increased power  use.  Sure, individuals probably wouldn't notice - but at some point  some consumer organization would.  The resulting bad publicity would  kill the business.  We did a bit of calculation to add that in to what  would be paid to the box owners and the whole enterprise started  looking less interesting from a purely economic point of view - not  that it didn't have plenty of other problems.)
                                                         -- Jerry

@_date: 2009-01-28 16:42:26
@_author: Jerry Leichter 
@_subject: Obama's secure PDA 
That's probably a big part of it.
I commented earlier that $3200 seemed surprisingly cheap.  One of the  articles on this claimed this was absurdly expensive - typical DoD  gold plating.  Well ... the real price of a standard Blackberry is a  couple of hundred dollars, and put one in a room with a speaker phone  and listen to the famous "Blackberry buzz".  Shielding these things,  even to avoid obvious interference, is *not* easy.  Getting it to  Tempest specs must take some impressive engineering.  For a non-mass- market device with that kind of engineering, $3200 seems pretty cheap.
                                                         -- Jerry

@_date: 2009-01-29 16:12:08
@_author: Jerry Leichter 
@_subject: "Attack of the Wireless Worms" 
It's worth reading both the original article that describes the  simulation - cited in the blog entry as   - and the actual blog entry, which is much more reasonable.
The original article posits that, if you can get onto a wireless  network, you can load an update into the wireless router.  (They  should have said "access point", but ignore that; the confusion is now  so well established that it doesn't much matter.)  Given that  assumption, and further given the assumption that not only could you  do it, you could write a virus that would do it for you, across a wide  variety of router models from multiple vendors, they use some  simulations to determine how long it would take to infect all the  routers in several "well-wirelessed" metropolitan areas.  The numbers  come out to a matter of days to hours.  Their only recommendation is  that everyone use WPA2 with a strong password.
Of course, I could equally well write a paper on the assumption that  car computers could infect other car computers by modulating the  headlights, and then calculate how long it would take a virus to  spread through all the cars in a city.  Maybe we all need to cover the  headlights of our cars "for security".
Access to a wireless network is a long way from administrative access  to the router for that network.  Granted, some devices have weak  administrative passwords.  That's certainly a problem - but the right  approach to fixing *that* problem is, well, to fix that problem: Use a  strong password.  It's very rare that anyone needs admin access to  their wireless routers.  There's no reason not to choose a complex  password, write it on sticker, and attach it to the router:  If  someone has physical access to your router, your security is gone  anyway.  The Spectrum article makes this point, and also points out  that this would be a non-problem if vendors shipped routers with  unique passwords pre-set on them.  (In fact, DSL routers - and  probably cable routers - typically come that way.  They can also  usually be set to permit admin access only from the "home" side, not  the "network" side - as some wireless routers can be set to allow  admin access only from their wired ports.)
There are many real problems around, but there are also many pseudo- problems.  The pseudo-problems do let you publish neat papers  sometimes, but it's important not to take them *too* seriously.
                                                         -- Jerry

@_date: 2009-01-30 18:01:29
@_author: Jerry Leichter 
@_subject: UCE - a simpler approach using just digital signing? 
There is little effective difference between this an whitelists.  If I  only accept mail from people on my whitelist, spammers can only send  me mail through three modes of failure:
on my
R, and
attack -
can live
spammer takes
comes from
Really, cryptography is a non-issue here.  As long as S and R share  some information - even S's address will do - that R can use to filter  messages; and there is no cheap way to get large amounts of (S,R)-pair  information; that information can be the key to a whitelist.  (Some  mailing lists do this:  E.g., if you want to post to RISKS, you're  asked to include the string "notsp" at the beginning or end of the  subject line.  This is public information, so a spammer could easily  do this *if he chose to specifically target the RISKS mailing list*;  but there's no way he can do this automatically on a mass scale.  An  individual could easily reach a similar agreement with anyone sending  him mail.
Of course, the downside is that you can now *only* receive mail from  those on your (logical) whitelist.  That's fine in some cases,  unacceptable in others.  You can semi-
automatically grow your whitelist by sending using some kind of  challenge/response.  For example, if you could send back the message  with a note saying:  "You're not on my whitelist, if you want to reach  me resend this message with 'xyzzy' in the subject line."  Spammers  don't bother to look for such messages right now (though if you made  this automatic enough, and enough people adopted it, they would have a  reason to!) so they won't be able to sneak on your whitelist that  way.  However, many people writing to you won't want to be bothered -  and automated mailings that you *do* want to receive and don't know  the details of ahead of time (e.g., approval messages for mailing list  requests you make) won't get through either.
                                                         -- Jerry

@_date: 2009-07-09 05:52:43
@_author: Jerry Leichter 
@_subject: Very high rate true random number generation 
Randomness from quantum effects at Megabits per second (and they claim  they can get to Gb/s).  I can't say I follow all the details of what  they're doing.
                                                         -- Jerry

@_date: 2009-07-09 22:12:44
@_author: Jerry Leichter 
@_subject: Weakness in Social Security Numbers Is Found  
Different attack.  What they are saying is that given date and place  of birth - not normally considered particularly sensitive - they have  a good chance of predicting *a particular person's* SSN.
For untargetted attacks, broad statistics about the number of SSN's  out there are fine.  But much attention these days is on targetted  attacks against "high value" individuals.  It's in fact probably  *easier* to find basic biographical information about date and place  of birth of such individuals - you can often get much of it for, say,  CEO's of public companies from their own brief bio's of their senior  officers; scan newspapers for charity birthday events and you can get  quite a bit more - than for a random member of the population.
Now, whether this really buys you all that much over other ways of  getting hold of SSN's is questionable - and in fact the researchers  are quoted as saying it's more of a demonstration of principle than  anything practical.
BTW, 442 million SSN's have been issued, but how many are for people  who have since died?  For many attacks, you need one for a living  victim, which lowers the probability.
                                                         -- Jerry

@_date: 2009-07-22 08:13:05
@_author: Jerry Leichter 
@_subject: Zooko's semi-private keys 
To make a simple observation:  H matters.  If H(z)=0 for all z, then  we discard all information and clearly no inversion is possible.  If  H(z)=1 for all z, then inversion is trivial.  If H(z)=n for any fixed  non-negative integer n, the problem is exactly discrete log - given  (g^x)^n, find (g^x).
Now, these are obviously uninteresting hashes.  Let's take the  simplest 1-1 onto hash, H(z)=z.  Then we are asking if, given (g^x)^x,  we can find g^x.  This feels like the same kind of problem as discrete  log, but the additional structure is disturbing.  Other simple forms  also seem like they might leak information.  For example, H(z)=g^z  asks the question of whether, given z^z, we can find z.
If H(z) is "random" - i.e., if you use some real cryptographic hash -  it's hard to see how you would get a proof, except maybe something of  the form "for almost all H drawn from some population, the problem is  hard".  But of course one always makes such arguments before one has a                                                           -- Jerry

@_date: 2009-07-22 20:30:45
@_author: Jerry Leichter 
@_subject: New Technology to Make Digital Data Disappear, on Purpose 
The proposal makes use of an incidental property of existing DHT  implementations:  Because many nodes are running on machines with  dynamic IP addresses, rehashes - which cause the table to change and  this leads to the loss of bits.  It's not actually clear from the  paper how much of the bit loss is actually due to IP address changes  and how much to other phenomena.  In any case, if this idea catches on  and there isn't enough "noise" in the network naturally to give an  adequate bit drop rate, it would be reasonable to add an explicit bit- dropping mechanism to some new release.  You'd need one to add IPv6  support anyway!
                                                         -- Jerry

@_date: 2009-07-26 08:10:13
@_author: Jerry Leichter 
@_subject: cleversafe says: 3 Reasons Why Encryption is Overrated 
It seems to me there's a much simpler critique.  The Cleversafe  approach - which is not without its nice points - solves the "key  management problem" in exactly the same way that some version of  Windows solved the "frequent General Protection Fault crashes" problem  (by eliminating the error message).
The "key management problem" comes down to:  I have encrypted data  stored somewhere (where we assume attackers can access it, but not  make use of it without the key).  To make that data meaningful, I need  to be able to locate the key appropriate to that data.  What's a key?   It's some private information.  In Cleversafe's approach, I have data  stored in pieces all over the place.  To get at it, I need to know  where the pieces of some data are.  That information has to be secret,  since anyone who has access to it can do the same computation and  recover the data just as I can.
Alternatively, I can rely not on the secrecy of that information, but  on the discretion of those who hold the pieces.  OK, but I could have  done that with a simpler technique:  Encrypt the data conventionally,  then split the key among the trusted holders.  That's a tiny, and more  to the point, *fixed* overhead beyond the size of the data, which will  always beat the cleverest Reed-Solomon or erasure coding.  (It also  has - if I use an appropriate mode - such nice features as random  access to small parts of the data without the need to decrypt the  whole thing first.)
Granted, Cleversafe has other nice features.  But other than changing  "the key management problem" to "the secret information needed to get  at the data, which won't be used as a crypto key" problem, I don't see  how they've actually *solved* anything.
Further:  If I'm only encrypting stuff for myself, there's little  reason to use multiple keys.  The key management problem becomes  interesting when there is different encrypted data with different  access rights for different groups of users.  It's beyond me how  Cleversafe's approach makes this easier - or harder.
                                                         -- Jerry

@_date: 2009-07-26 23:09:38
@_author: Jerry Leichter 
@_subject: The latest Flash vulnerability and monoculture 
While I agree with the sentiment and the theory, I'm not sure that it  really works that way.  How many actual implementations of typical  protocols are there?  With open source, once there's a decent  implementation, there's little
incentive for anyone to start from scratch on an independent one.  Why  not just improve the one that's already there?
One way or another, a single implementation usually wins out in the  OSS community.  Even if along the way a competition - based on code  size or speed or whatever - breaks out between two implementations, in  the long one someone usually takes the best from both and produces the  ultimate "winner".
So while standard, openly defined protocols *make it possible* for  multiple OSS implementations to thrive, they certainly don't  *guarantee* it, and in many cases that's just not what we end up with.
In fact, the scenario most likely to produce multiple *usable*  implementations is probably:  An open protocol, and multiple *closed  source* competing implementations.  As an example, not of a protocol,  but of another kind of software - consider C compilers.  There  continues to be a market for proprietary C compilers, and quite a few  of them exist.  In the OSS world, gcc dominates.  (Perhaps a new LLVM- based compiler will displace it - though more likely gcc will just  absorb LLVM as an alternate back end.  That hardly leave behind all  gcc bugs.)
In the hardware world, one is typically very leery of buying from a  sole-source supplier.  It's common to require that the vendor who  developed some new chip license someone else to build the thing, too -  just in case.  (Of course, if you buy a couple of hundred chips a year  from Intel, you're not going to have much luck getting them to work  with you.  But the *big* buyers definitely force second sourcing when  they can.  It would be nice if Flash users told Adobe "find someone to  do another implementation or we stop using Flash."  But since the  space of Flash users has two components - those who *produce* Flash,  who generally won't care about this; and those who use it to get light  - it's difficult to generate such pressures.  The Flash generators  don't have any reason to care about this, and the users of Flash files  - who pay nothing - have little leverage unless they serious follow  through on a strike plan.
                                                         -- Jerry

@_date: 2009-07-27 00:18:14
@_author: Jerry Leichter 
@_subject: The latest Flash vulnerability and monoculture 
Can you name a single system that allows you to substitute different  TCP/IP stacks?  Without that, there's little practical diversity.  The  practical difference between a bug that affects 25% of the world's  systems and 100% of the world's systems - assuming unrealistically an  even division - isn't all that great.
Apache and IIS together make up the bulk of implementations.   Microsoft's long-standing drive to avoid OSS software accounts for one  of the common TCP/IP implementations, too.  On the one hand, Microsoft  isn't doing much of this any more - and no one else is trying.  On the  other, this confirms my observation that an open definition with  closed implementations is the most likely source of *multiple*  Here, a bug would hit close to half of all systems in the world.  The  minor players are irrelevant.
There's probably more diversity here than anywhere else, as the result  of first Firefox (and other Gecko-based browsers, though they are  minor players) and then Safari and other Webkit-based browsers  breaking up Microsoft's lock on the market.  Most of the others divide  off into disjoint markets which rarely share much software.
Yes, you can find examples.  But there are also examples where there  is little diversity.  How many active competitors to zlib are there?   Security bugs in zlib - which have occurred - cause grief to wide  swaths of products.  While there a independent zip implementations,  most of the less-known compression algorithms have one implementation  - and bugs in those have led to problems in multiple anti-virus  packages, which have to support all the formats and aren't about to re- implement them.
Keeping multiple implementations going is expensive - whether you're a  commercial outfit who has to find the money, or and OSS project that  has to attract developers.  There has to be a good reason to do it.   There will be cases where good reasons are present - optimization for  very different kinds of environments (low power embedded vs. larger  systems, for example).  For OSS, just simple pride and competition can  last for a long time, and sometimes get "frozen in".  Competitive  differentiation is important for commercial efforts - and is  increasingly affection OSS efforts through commercial funding.  But  all of these have to fight a natural tendency to settle on a single  solution once the problem is no longer novel, the techniques are all  well understood, and there's ultimately little to distinguish one  solution from another.  It'll happen sometimes, for some period of time.
I'm not saying more diversity isn't better.  Certainly, if the  protocol is closed, there will likely be very little if any diversity  in implementation.  So open standards are to be preferred.  All I'm  saying is that there's no magic here.  If anything, OSS *encourages* a  convergence on a single solution, because using what's already there  is so cheap that you need some really good reason *not* to.
                                                         -- Jerry

@_date: 2009-06-30 06:37:39
@_author: Jerry Leichter 
@_subject: password safes for mac 
Which brings up a question I've had about keychain:  Keychains can be  synced across Mobile Me, and the login passwords of different machines  that sync their keychains don't have to be the same.  How is the key  transformation accomplished?  Does the central server know all the  login keys?  Or ... what? It's all very convenient, but the security  implications scare me.
Note that for all other keychains, there's no problem just syncing the  encrypted keys, since you have to explicitly enter the password at  each machine to unlock the keychain.  (I put all my high-value keys in  secondary keychains for this and related reasons.)
                                                         -- Jerry

@_date: 2009-03-02 12:21:07
@_author: Jerry Leichter 
@_subject: Activation protocol for tracking devices 
Not specifically, but you can simply take the first 64 bits from a  larger cryptographically secure hash function.  If the nature of your  usage is that an attack requires finding a preimage to an externally  specified hash value, 64 bits is reasonably secure.  (If being able to  find a pair of values with the same hash value, 64 bits is way too  Both of these techniques have been used.  If you want a simple  security argument for the block cipher case, use the pre-stored random  number as the key and encrypt 0, 1, 2, and so on.  If some can use  this output to get the key; or if given encryptions up to n, they can  guess the encryption at n+1, then the cipher could not be used in  counter mode (since what you are getting is exactly the counter-mode  encryption of an all-0-bits message).  Obviously, you'll have to store  the counter across boots, since otherwise you repeat values.
With a hash, you need to be a bit fancier since, in and of itself, the  hash has no secret information.  This can be done, but it would be  trickier; I'd go with the block cipher.
Note that there are published algorithms - even part of FIPS standards  - that do exactly what you need:  Take a single random seed and safely  stretch it into a large number of random values.  The ones in the  standards - and perhaps most of the ones out there - are old and  probably not up to contemporary standards.
You're trying to produce a keyed hash function (or MAC) from a non- keyed hash function.  Just pre-pending the secret key is not  necessarily secure.  I'd suggest using HMAC (with Kt the key, of  Signed?  How?  I don't understand the 64-bit limitation.  I'm not sure  a 64-bit signing key is sufficient these days.
                                                         -- Jerry

@_date: 2009-03-02 18:11:24
@_author: Jerry Leichter 
@_subject: Activation protocol for tracking devices 
All the bits in a cryptographically secure hash function are "equally  good".  So, yes, you can construct a shorter hash by simply discarding  bits from a longer one.
I would suggest that if you're going to build an HMAC, that you shrink  the result as the very last step:  Do the internal calculations using  the full hash function.  I don't see an obvious attack from doing it  the other way, but  it just seems riskier.
Look around for "deterministic random number generators" or something  like that.  I'm sure you know this, but do *not* attempt to use a  generator designed for statistical purposes - those have good  "randomness" properties but are not secure against deliberate attack.
OK, there is a distinction between a signature and a MAC (Message  Authentication Code).  The significant difference is that it's  possible to prove to a third party that someone signed something  without actually having the ability to sign things yourself.  (Think  RSA signatures:  The public key is all you need to prove that  something was signed with the corresponding private key; but it's  insufficient to sign anything.)  A MAC is sufficient for your purpose.
It's fine to truncate the output of a MAC computation.  In fact, there  good reasons for doing so in some situations, independent of the  available space:  By discarding information, it makes certain attacks  harder.  You'll see people immediately jump in with "but the birthday  paradox says you lose half your bits", which is true for a hash, but  *not* for a MAC, where the attacker doesn't have access to the key.
You're welcome.  I hope they're helpful, but don't rely on them too  much - my quick response on a mailing list isn't a serious security  analysis of the protocol and implementation.
                                                         -- Jerry

@_date: 2009-05-05 18:44:01
@_author: Jerry Leichter 
@_subject: Has any public CA ever had their certificate revoked? 
The same question can be asked about *any* instance of criminal  behavior, or of any other kind of behavior that is considered "bad  enough" to be worthy of punishment.  To go to the extreme:  The victim  is already dead, jailing the murderer won't bring him back - all you  are doing is costing society directly (we have to pay the costs of  keeping him in jail - quite expensive, actually) and indirectly (we  won't have the fruits of his labor - like, say, new file systems).  We  punish acts to send a message that certain things are unacceptable, to  deter the actor and others, out of a sense of justice, and for other  related reasons.  The beneficiaries are *everyone else*.
The strength of Tit For Tat as a strategy shows that motives like this  tap into very basic properties of multi-party games.
As for what your punishment as a "bad CA" should be:  Realistically,  in any industry based on trust, the major component of punishment  should be loss of trust - which results in people refusing to do  business with you any more, which will usually put you out of  business.  In egregious cases, we send people to jail (where they can  spend time with Bernie Madoff).  We also have mechanisms that aren't  punishments but deal with the equities of the situation:  They try to  right the wrongs.  So if I can show that your malfeasance as a CA led  to my losing money, you have to compensate me.  There's a whole grey  area in between that centers on the principle that you should not be  allowed to profit from you ill-gotten gains - whether or not we can  figure out how to return those gains to those who rightly should have  Theirry Moreau has already pointed out that political/economic reality  here makes any meaningful punishment impossible.  That's way the CA  industry can't ever really be a trust industry - you can't rely on a  party who disclaims all responsibility, no matter what.
                                                         -- Jerry

@_date: 2009-05-09 07:33:21
@_author: Jerry Leichter 
@_subject: Solving password problems one at a time, Re: The password-reset paradox 
This is part of a broader UI issue.
I had a discussion with a guy at a company that was proposing to  create secure credit cards by embedding a chip in the card and  replacing some number of digits with an LCD display.  The card would  generate a unique card number for you when needed.  They actually had  the technology working - the card was pretty much indistinguishable  from any other.  (Of course, how rugged it would be in typical  environments is another question - but they claimed they had a  I pointed out that my wife knows one of her CC numbers by heart.  The  regularly quotes it, both on phone calls and to web forms.  The card  itself is buried in a thick wallet, which is buried in her pocketbook,  which is somewhere in the house - likely not near the phone or the  Hell, one of the nice things about on-line shopping is that I can do  it in my bathrobe - except that I *don't* know my CC by heart, so in  fact I tend to put off buying until later when I have my wallet with  me.  (This does save me money....)
When I'm in a store, I'm used to having to have my CC with me, because  I always had to have the wallet with money anyway.  At home, it's a  whole different story.  In any case, merchants are trying to make the  in-store experience as simple as possible, pushing for things like  RFID credit cards and even fingerprint recognition.
So many people would see these "safer" cards as a big step backwards  in usability.  Why would they want such a thing?  The card companies  are trying to sell "safety", but in the US, where your liability is at  most $50 if your CC number is stolen (and where in practice it's $0),  the only cost you as an individual bear is the inconvenience of  replacing a card.  Because replacements for security problems have  gotten so common, the CC companies have streamlined the process.  It's  really no big deal.  I've had CC numbers stolen a couple of times (by  means unknown); recently, two of my CC's were replaced by the  companies based on some information known only to them.  In every  case, the process was very quick and painless.  Hell, these days even  on-line continuing charges often update to the new number  automatically (though I've learned to keep track of those and check).
The person arguing for this claimed that CC companies could offer a  discount for users of the "secure" cards.  But if you look at actual  loss rates - how much could you offer?  (I'd guess it's about the same  as Discover offers:  About a 1.5% rebate on most purchases.  Not  enough to let Discover steal customers from Visa and MC.  Given all  the other charges - and the absurdly high interest rates - on cards,  anything like this gets lost in the noise.)
Security that depends on people changing their habits in a way that is  inconvenient to them ... won't happen (unless you're in an environment  where you can *force* such changes).
                                                         -- Jerry

@_date: 2009-05-10 18:55:28
@_author: Jerry Leichter 
@_subject: Warning!  New cryptographic modes! 
I recently stumbled across two attempts to solve a cryptographic  problem - which has lead to what look like rather unfortunate solutions.
The problem has to do with using rsync to maintain backups of  directories.  rsync tries to transfer a minimum of data by sending  only the differences between new and old versions of files.  Suppose,  however, that I want to keep my backup "in the cloud", and I don't  want to expose my data.  That is, what I really want to store is  encrypted versions of my files - and I don't want the server to be  able to decrypt, even as part of the transfer.  So what I do is  locally encrypt each file before trying to sync it.  However, using  CBC (apparently the only acceptable mode of operation), any change in  the file itself propagates to changes to the rest of encrypted file,  so rsync has to transfer the whole rest of the file.  The obvious  thing to do is to use a mode that doesn't propagate errors - at least  not too far.
This issue has been faced in the rsync world for compressed filesi n  the past:  If I try to minimize both byte transfers and space on the  remote system by compressing files before sending them, any adaptive  compression algorithm will tend to propagate a single-byte change to  the end of the file, forcing a big transfer.  The fix that's come be  be accepted - it's part of all recent versions of gzip (--rsync- friendly option, or something like that) is to simply reset the  compression tables every so often.  (The actual algorithm is a bit  more clever:  It resets the algorithm whenever some number of bits of  a rolling checksum match a constant.  This allow resynchronization  after insertions or deletions.)
Given this history, both murk ( and  rsyncrypto ( seem to do  the same basic thing:  Use CBC, but regularly reset the IV.  Neither  project documents their actual algorithm; a quick look at the murk  code suggests that it encrypts 8K blocks using CBC and an IV computed  as the CRC of the block.  There also seems to be some kind of  authentication checksum done, but it's not entirely clear what.
So here we have it all:  A new cryptographic mode, documented only in  C code, being proposed for broad use with no analysis.
In any case, there are obvious, well-understood solutions here:  Use  counter mode, which propagates changes by a single block of the  cryptosystem.  Or use any other stream cipher mode.  (An interesting  question is whether there's a mode that will recover from insertions  or deletions.  Perhaps something like:  Use counter mode.  If two  consecutive ciphertext bytes are 0, fill the rest of the ciphertext  block with 0's, jump the counter by 65536, and insert a special block  containing the new counter value.)
                                                        -- Jerry

@_date: 2009-05-11 16:54:24
@_author: Jerry Leichter 
@_subject: Warning! New cryptographic modes!  
Well, XOR of old a new plaintext.  But point taken.
Sounds like this might actually be an argument for a stream cipher  with a more sophisticated combiner than XOR.  (Every time I've  suggested that, the response has been "That doesn't actually add any  strength, so why bother".  And in simple data-in-motion encryption,  that's certainly true.)
Perhaps Matt Ball's suggestion of XTS works; I don't see exactly what  he's suggesting.  There is certainly a parallel with disk encryption  algorithms, but the problem is different:  Using rsync inherently  reveals what's changed in the cleartext (at least to some level of  granularity), so trying to protect against an attack that reveals this  information - something one worries about in disk encryption - is  beside the point.
                                                         -- Jerry

@_date: 2009-05-11 18:48:02
@_author: Jerry Leichter 
@_subject: Warning! New cryptographic modes! 
If you have a snapshot file system, sure, you can use it.  Caching  checksums and using a write barrier (like fsevents in MacOS) would  also work.
Any such system will eventually build up either a huge number of small  deltas, or deltas that are close to the size of the underlying files  (i.e., eventually most things get changed).  So you also need a way to  reset to a new base - that is, run an rsync as you do today.  Thus,  while this is certainly a good optimization, you still need to solve  the underlying problem.
Consider first just updates.  Then you have exactly the same problem  as for disk encryption:  You want to limit the changes needed in the  encrypted image to more or less the size of the change to the  underlying data.  Generally, we assume that the size of the encrypted  change for a given contiguous range of changed underlying bytes is  bounded roughly by rounding the size of the changed region up to a  multiple of the blocksize.  This does reveal a great deal of  information, but there isn't any good alternative.  (Of course, many  file types are never actually changed in place - they are copied with  modifications along the way - and in that case the whole thing will  get re-encrypted differently anyway.)
It's curious that the ability to add or remove blocks in the middle of  a file has never emerged as a file system primitive.  Most file system  organizations could support it easily.  (Why would you want this?   Consider all the container file organizations we have, which bundle up  segments of different kinds of data.  A .o file is a common example.   Often we reserve space in some of the embedded sections to allow for  changes later - patch areas, for example.  But when these fill,  there's no alternative but to re-create the file from scratch.  If we  could insert another block, things would get easier.)
Given that file systems don't support the operation, disk encryption  schemes haven't bothered either.
To support insertions or deletions of full blocks, you can't make the  block encryption depend on the block position in the file, since  that's subject to change.  For a disk encryptor that can't add data to  the file, that's a killer; for an rsync pre-processor, it's no big  deal - just store the necessary key-generation or tweak data with each  block.  This has no effect on security - the position data was public  To handle smaller inserts or deletes, you need to ensure that the  underlying blocks "get back into sync".  The gzip technique I  mentioned earlier works.  Keep a running cryptographically secure  checksum over the last blocksize bytes.  When some condition on the  checksum is met - equals 0 mod M - insert filler to the beginning of  the next block before encrypting; discard to the beginning of the next  block when decrypting.  Logically, this is dividing the file up into  segments whose ends occur at runs of blocksize bytes that give a  checksum obeying the condition.  A change within a segment can at most  destroy that segment and the following one; any other segments  eventually match up.  (The comparison algorithm can't have anything  that assumes either block or segment offset from beginning of file are  significant - but I think rsync already handles that.)  Yes, this  leaks *something* about the plaintext which is hard to pin down, but  it seems much less significant than what you've already given up to  allow local cleartext changes to produce local ciphertext changes.
                                                         -- Jerry

@_date: 2009-05-11 19:00:17
@_author: Jerry Leichter 
@_subject: Warning! New cryptographic modes! 
Oh, feh.  I'm typing without thinking.  In the worst case, an  insertion (deletion) of K bytes could create (delete) K/M new  (existing) segments.  In practice, this is unlikely except in an  adversarial situation, and all it can do is force extra data to be                                                           -- Jerry

@_date: 2009-05-11 20:22:40
@_author: Jerry Leichter 
@_subject: Warning! New cryptographic modes! 
To do this, the backup system needs access to both the old and new  version of the file.  rsync does, because it is inherently sync'ing  two copies, usually on two different systems, and we're doing this  exactly because we *want* that second copy.
If you want the delta computation to be done locally, you need two  local copies of the file - doubling your disk requirements.  In  principle, you could do this only at file close time, so that you'd  only need such a copy for files that are currently being written or  backed up.  What happens if the system crashes after it's updated but  before you can back it up?  Do you need full data logging?
Victor Duchovni suggested using snapshots, which also give you the  effect of a local copy - but sliced differently, as it were, into  blocks written to the file system over some defined period of time.   Very useful, but both it and any other mechanism must sometimes deal  with worst cases - an erase of the whole disk, for example; or a  single file that fills all or most of the disk.
                                                         -- Jerry

@_date: 2009-05-11 20:29:06
@_author: Jerry Leichter 
@_subject: Warning! New cryptographic modes! 
It's interesting that data-dedup-friendly modes inherently allow an  attacker to recognize duplicated plaintext based only on the  ciphertext.  That's their whole point.  But this is exactly the  primary weakness of ECB mode.  It's actually a bit funny:  ECB mode  lets you recognize repetitions of what are commonly small, probably  semantically meaningless, pieces of plaintext.  Data-dedup-friendly  modes let you recognize repetitions of what are commonly large chunks  of semantically meaningful plaintext.  Yet we reject ECB as insecure  but accept the insecurity of data-dedup-friendly modes because they  are so useful!
                                                         -- Jerry

@_date: 2009-05-11 20:39:32
@_author: Jerry Leichter 
@_subject: Warning! New cryptographic modes! 
a)  What's a "committed" file.
b)  As in my response to Victor's message, note that you can't keep a  base plus changes forever - eventually you need to resend the base.   And you'd like to do that efficiently.
Some files change often.  There are files that go back and forth  between two states.  (Consider a directory file that contains a lock  file for a program that runs frequently.)  The deltas may be huge, but  they all collapse!
A large percentage increase - and why 50%? - scales up with the amount  of storage.  There are, and will for quite some time continue to be,  applications that are limited by the amount of disk one can afford to  throw at them.  Such an approach drops the maximum size of file the  application can deal with by 50%.
I'm not sure what cost you think needs to be paid here.  Ignoring  encryption, an rsync-style algorithm uses little local memory (I think  the standard program uses more than it has to because it always works  on whole files; it could subdivide them) and transfers close to the  minimum you could possibly transfer.
If a file isn't committed when closed, then you're talking about any  commonly-used system.
                                                         -- Jerry

@_date: 2009-05-27 10:12:12
@_author: Jerry Leichter 
@_subject: consulting question.... (DRM) 
The introduction of the acronym "DRM" has drawn all the hysteria it  always does.
The description you've posted much more closely matches license (or  sometimse entitlement) management software than DRM.  There are many  companies active in this field.  Many are small, but Microsoft sells  some solution and there are moderately large companies around.  Some  of these have been around for many years.
Traditionally, license management software looked at local files or  databases rather than out on the Internet.  However, I'm sure Internet  options exist.
The better software of this sort is challenging to crack.  Certainly,  none of it is *impossible* to crack - though the best dongle-based  systems are probably extremely difficult (but also unacceptable for  most kinds of software).
For the most part, software like this aims to keep reasonably honest  people honest.  Yes, they can probably hire someone to hack around the  licensing software.  (There's generally not much motivation for J  Random User to break this stuff, since it protects business software  with a specialized audience.) But is it (a) worth the cost; (b) worth  the risk - if you get caught, there's clear evidence that you broke  things deliberately.
Probably the greatest use for such software is not in preventing  unlicensed users from running it at all but in enforcing contractual  limits - e.g., you can only use this to manage up to X machines.   Every company that has sold software with that kind of contract will  likely find that, unless the software enforces the limitation, its  customers will exceed it - often unknowingly, often by large factors.
I'd suggest that you, and the company you're consulting to, spend some  time understanding the market.  What kind of software vendors are you  selling to?  B2B is a very different marketplace from consumer.   Within B2B, "high touch" sales are very different from mass market.   If you go international, a great deal depends on where you think  you're going to sell.  If you are ultimately depending on contractual  enforcement, with the licensing software just an encouragement to good  behavior, you're fine in the US and Western Europe, but you're not  going to have a happy time in, say, Russia and China.
A Google search on "license management software" turns up many hits,  including an overview article that may be useful:      (One thing to be aware of is that this phrase is a bit ambiguous,  covering both software a vendor puts in to its code to manage  licenses, and software sold to large end users to help them keep track  of what licenses they are using.  The listing in the article covers  both, but is still incomplete - it misses one of the long-established  companies, Acresso Software - a new name - that sells the FLEXnet  license enforcement software, a business it's been in for at least 10  years or so.)
                                                         -- Jerry

@_date: 2009-05-28 17:33:51
@_author: Jerry Leichter 
@_subject: Neat idea 
Using retransmissions for steganography.
                                                         -- Jerry

@_date: 2009-05-29 10:43:33
@_author: Jerry Leichter 
@_subject: consulting question.... (DRM) 
I agree 100%.
The most important thing to keep in mind when doing license management  software is that it has *NO* value to the *customer*.  The guys who  sell this stuff will always claim that it "helps the customer keep  track of licenses" or some such rot - but it's complete nonsense.  In  fact, license management code has *negative* customer value.  That  doesn't mean it doesn't have a legitimate role - the cash registers in  the supermarket add a negative value to all the sold, but the  supermarket wouldn't be there without them.  But unless you  understand, deep down, that this is something that you're imposing on  your customer and that therefore it needs to be as close to invisible  and fail-safe as possible; and you act *effectively* on that basis -  you're just going to encourage circumvention or a search for  alternatives to your software.
                                                         -- Jerry

@_date: 2009-11-02 15:25:53
@_author: Jerry Leichter 
@_subject: Security of Mac Keychain, Filevault 
A couple of others wrote to me privately with the same general thought.
I see I'm still not managing to make my point.  Suppose the world were  as in the following diagram:
People who say they've looked				People who claim Keychain can be
Keychain and believe it's good					broken easily

@_date: 2009-11-02 20:41:04
@_author: Jerry Leichter 
@_subject: Security of Mac Keychain, Filevault 
There are two problems with this:  So many of the things you'd really  like to be able to do with your iPhone/Touch/other smart phone require  a key whose value is very difficult to calculate (e.g., just what  would you lose if someone could read all your mail?); and services  increasing bundle all kinds of things together under one password.   For example, all your Google services use the same password; and your  Apple Mobile Me mail password is also the key to such things as you  contact list (if you sync it) and Back To My Mac (which I now disable,  useful as it might be, for just this reason) and your iTunes store  account.  You can dissociate some of these, directly or indirectly,  but the services assume they are tied together and don't work nearly  as well if you do that.  The trend is for this to get worse, with  network-wide shared authentication via OpenID or whatever other  standard catches on.
Oh, absolutely.
And you know this ... how?  Have you, or anyone you know, vetted the  design?  Sure, *if* it's all implemented correctly, it maintains  *some* set of security properties.  Do you even know what those are?   I know I don't....
That would be my assumption, too.
As the value of the information in smartphones grows daily, their  vulnerabilities will be more and more of a problem.  Remote wipe -  assuming it really destroys the data - helps against loss, but does  nothing against a deliberate, targeted attack, which can probably copy  all the data within minutes.  We need some new thinking here.  One  possible approach, based on an idea IBM played with a couple of years  back but that as far as I know never made it into a product:  Build a  Bluetooth-connected ring or key fob that must be physically quite  close to the device to keep it unlocked.  IBM did this for laptops,  and just locked the screen.  For a smartphone, you'd want the phone  and the fob to mutually authenticate, and then the fob would transfer  a key that could be used to unlock critical data on the phone.  When  the fob goes out of range, the phone wipes the key and all decrypted  data.  One can certainly come up with attacks on this - even so simple  as the smart mugger scenario:  Give me your phone and your fob - but  it raises the bar, with minimal inconvenience in normal use.
                                                         -- Jerry

@_date: 2009-11-03 21:07:08
@_author: Jerry Leichter 
@_subject: Security of Mac Keychain, Filevault 
That's fine, but how much does it help?  Anything you can access,  you'll want to access using your smartphone.  In fact, there's already  a push to access some high-value things - like bank accounts - more  through smartphones than through more traditional means.  So, yes, you  can have granular access, but if you end up really wanting to put the  high-value "grains" on your smartphone, it doesn't help.
Smart*cards* aren't much help here - if you leave them it the phone,  then a stolen phone means a stolen smartcard.  Having to reach into  your wallet to get a smartcard to swipe on your phone is a non- starter.  You need a better interface - something like the Bluetooth  connection I suggested.
Multi-factor doesn't, in and of itself, help much.  "Something I know"  can't have much entropy if I need to enter it every time I unlock the  phone.  "Something I am" - well, maybe a fingerprint sensor might  help, but all such technologies have well-known issues.  "Something I  have" - that's the only one that can help all that much, *if* you get  the UI right.
                                                                -- Jerry

@_date: 2009-11-08 21:17:28
@_author: Jerry Leichter 
@_subject: Effects of OpenID or similar standards 
While I'm sure this is widely believed, I wonder if it's really true.   Is anyone aware of research on the subject?
Even if it's true to a large degree, the details may matter.  People  may routinely use the same password for all their "low value"  accounts, but come up with something better for their bank or other  "high value" accounts.  Paradoxically, the *lack* of a standard for  password quality may help here.  High-value sites often place some  requirement on the nature of passwords, but the requirements vary:   Letters and digits only; letters plus digits plus at least one  "special" character - with the set of allowed "special" characters  varying in pretty arbitrary ways; etc.  It's tough to come up with a  single password that will be broadly accepted at such sites, and  anything someone does come up with will be so inconvenient that it's  unlikely to be something they'll want to use at low-value, any- password-accepted, sites.
A widely-used single sign on system is certainly great from a  usability point of view, and does actually have some positive effects  on security:  You no longer need to hand your actual password to sites  programmed by someone whose background in security is minimal.  The  downside is that you now have a single super-high-value password, the  compromise of which would be very painful.
                                                         -- Jerry

@_date: 2009-11-08 21:41:51
@_author: Jerry Leichter 
@_subject: Crypto dongles to secure online transactions 
Wow.  *That's* scary.
Technical content is fine, with one comment:  You don't need a big  keyboard to allow for a secure "user login":  Even a single one will  do.  You'd have a list of, say, 5 key words that you memorize.  When  the device turns on, it flashes a set of 10 words across the screen,  one at a time for 1 second a piece (times/numbers subject to usability  testing).  Exactly one is from your list of 5; you need to press the  button while your word is on the screen.  Repeat this process 3 times  and the chance of guessing the right words is 1 in a thousand.  (Yes,  someone can watch you using the device, but if it continues to the end  of the set of 10 even after you press the button, it's a bit of a  challenge to know which one you picked - and of course they could  watch you type your password.)
It does need another pass for typos and such - e.g., "to defeat  attacks that steal credentials and reuse *it* to set up another  session later".
I think $50 is a very high estimate.  (Lynn Wheeler has described a  design for a more powerful version of such a device that, as I recall,  came in well under this figure a couple of years back.)  Note that if  the bank supplies the device - so that it necessarily knows any secret  contained in it, and it's designed to be resistant to attempts to  determine the secrets in it - then you don't need to use public key  crypto; symmetric algorithms are just fine.  These require very little  compute power and memory.
Once you assume that the secure endpoints are the device and the bank,  the connection between the device and the PC is something you don't  need to worry about.  For somewhat higher cost than USB, you can use  Bluetooth.  Then the device can be anything.  Look at the iPod shuffle  and imagine how Apple might build such a thing.  It could easily  become a fashion accessory - a bank could get a lot of marketing  mileage out of providing a fob with some famous designer's name on it.
                                                          -- Jerry

@_date: 2009-11-08 22:11:57
@_author: Jerry Leichter 
@_subject: hedging our bets -- in case SHA-256 turns out to be insecure  
I'd worry about using this construction if H1's input block and output  size were the same, since one might be able to leverage some kind of  extension attack.  That's not a problem for SHA256 or SHA512, but it's  something to keep in mind if this is supposed to be a general  construction, given that all likely hash functions will be constructed  by some kind of iteration over fixed-size blocks.
Rather than simply concatenating H1(x) and H2(x), you might do better  to interlace them.  Even alternating bytes - cheap enough that you'd  never notice - should break up any structure that designs of practical  hash functions might exhibit.  (As a matter of theory, a vulnerability  of alternate bytes is as likely as a vulnerability of leading bytes;  but given the way we actually build hash functions, as a practical  matter the latter seems much more likely.)
                                                         -- Jerry

@_date: 2009-11-10 09:44:58
@_author: Jerry Leichter 
@_subject: Crypto dongles to secure online transactions 
Bring two threads together:  The ZTIC is designed to work with  unmodified servers, hence implements SSL/TLS internally.  Could the  recently discovered SSL injection attack be used against it?  (I  haven't thought it through and have no idea.)  Whether or not it can,  it demonstrates the hazards of freezing implementations of crypto  protocols into ROM:  Imagine a world in which there are a couple of  hundred million ZTIC's or similar devices fielded - and a significant  vulnerability is found in the protocol they speak.  (Since we're  talking about a *protocol* vulnerability, having multiple competing  implementations doesn't help.)
Now, you could make the same argument about the encryption mechanisms  - AES, RSA, whatever else is frozen in that silicon - as well.  We're  reasonably sure of our ability to build strong block and public key  ciphers - there have been no significant (publicly known!) breaks in  any fielded system in years.  The problems with hash functions show  that our abilities there aren't as good as we thought.  But this  recent attack against SSL/TLS, studied by so many people for so many  years, should make us really humble about the state of the art in  secure protocol development.
Not that this should block the use of devices like the ZTIC!  They're  still much more secure than the alternatives.  But it's important to  keep in mind the vulnerabilities we engineer *into* systems at the  same time we engineer others *out*.
                                                         -- Jerry

@_date: 2009-11-11 21:42:21
@_author: Jerry Leichter 
@_subject: Crypto dongles to secure online transactions 
That's the flip side of the vulnerability - and it's exactly why I did  *not* suggest that the "fix" for vulnerable algorithms frozen into  silicon was to make them updatable.
Of course, there *are* situations in which that makes sense.  If one  organization distributes the dongles, they could accept only updates  signed by that organization.  We have pretty good methods for keeping  private keys secret at the enterprise level, so the risks should be                                                           -- Jerry

@_date: 2009-11-16 23:20:27
@_author: Jerry Leichter 
@_subject: Crypto dongles to secure online transactions 
I'm not sure that's the right lesson to learn.
A system has to be designed to work with available technology.  The  TI83 dates back to 1996, and used technology that was old even at the  time:  The CPU is a 6MHz Z80.  A 512-bit RSA was probably near the  outer limits of what one could expect to use in practice on such a  machine, and at the time, that was quite secure.
Nothing lasts forever, though, and an effective 13 year lifetime for  cryptography in such a low-end product is pretty good.  (The  *official* lifetime of DES was about 28 years, though it was seriously  compromised well before it was officially withdrawn in 2005.)
                                                         -- Jerry

@_date: 2009-11-21 17:56:03
@_author: Jerry Leichter 
@_subject: Crypto dongles to secure online transactions 
It goes deeper than that.  Oh, sure, marketing loves having a presence  - but their desire fits into corporate cultural biases.
When I go to work, I have to carry two key cards - one for the  building, one for my employer.  They use the same technology - if you  use the wrong one, the reader beeps in recognition but of course won't  unlock the door.  In fact, they interfere with each other - you have  to make sure to keep the "wrong" one a couple of inches away from the  reader or it will usually be confused.  It's a pain, actually.
Now, it's certainly possible that there's something proprietary on one  card or the other - though as we've discussed here before, that's only  true on badly designed systems:  It's no big deal to read these cards,  and from many times the inch or so that the standard readers require.   So all that should be on the cards is an essentially random number  which acts as a key into the lock systems database.  It's just that  the owners of each system insist on assigning that random number  themselves.  Does it give them any additional security?  Hardly.  If  you think through the scenarios, you confirm that quickly - a direct  consequence of the lack of any inherent value in the card or its  contained number in and of themselves:  The real value is in the  database entry, and both institutions retain control of their own  What's needed is some simple cooperation and agreement on how to  assign unique numbers to each card.  There already has to be  cooperation on the issuance and invalidation of building cards.  But  institutions insist on their sense of control and independence, even  when it has no real payoffs for them (and, in fact, raises their costs).
                                                         -- Jerry

@_date: 2009-11-21 18:31:40
@_author: Jerry Leichter 
@_subject: Crypto dongles to secure online transactions 
Well, my building card is plain white.  If anyone duplicated it,  there'd be nothing stopping them from going in.  But then the actual  security offered by those cards - and the building controls - is more  for show (and I suppose to keep the "riffraff" out - than anything else.
My work card has my photo and name on it, but there's nothing to  correlate name with underlying ID in normal operation.  Snap a photo  of the card while you clone it, make up a reasonable simulacrum with  your own picture and name, and walk right in.
Not really more or less secure than the old days when you flashed you  (easily copied) badge to a card who probably only noticed that it was  about the right size and had roughly the right color.  But it's higher  tech, so an improvement.  :-)
Physical security for most institutions has never been very good, and  fortunately has never *needed* to be very good.  Convenience wins out,  and technology gives a nice warm feeling.  A favorite example:  My  wife's parents live in a secured retirement community.  The main  entrance has a guard who checks if you're on a list of known visitors,  or calls the people you're visiting if not.  Residents used to have a  magnetic card, but that's a bit of pain to use.  So it was replaced by  a system probably adapted from railroad freight card ID systems:  You  stick big barcode in your passenger side window, and a laser scanner  on a post reads it and opens the door.
Of course, it's trivial to duplicate the sticker using a simple photo,  and since the system has to work from varying distances, at varying  angles, on moving cars, in all light and weather conditions, it can't  possibly be highly discriminating - almost certainly just a simple  Manchester-style decoder.
                                                         -- Jerry

@_date: 2009-10-03 23:51:16
@_author: Jerry Leichter 
@_subject: Question about Shamir secret sharing scheme 
It's nice to be able to give a size limit for the shares.  They're  going to need to be transmitted and stored.  Since there are many  primes around, working over Zp ensures that shares about about the  same size as the secret.
However, there's also a more fundamental problem:  In step (b), how do  you choose your coefficients randomly over all of Z?  There is no  uniform probability distribution over Z to work with.  Any realistic  implementation will choose from some finite subset.  But then the  scheme may not be completely secure:  If you have the value of f() at  t-1 points, the fact that the coefficients are limited to some finite  set also constrains the possible values at the remaining point - and  you don't know exactly how.  Working over Zp's group structure ensures  that if you have t-1 values, all p-1 possible remaining values are  equally likely, so you've learned nothing.
                                                         -- Jerry

@_date: 2009-10-14 22:43:48
@_author: Jerry Leichter 
@_subject: Possibly questionable security decisions in DNS root management 
As I read it, "short term" refers to the lifetime of the *key*, not  the lifetime of the *system*.
Currently, the cryptographic cost of an attack is ... 0.  How many  attacks have there been?  Perhaps the perceived value of owning part  of DNS isn't as great as you think.
If the constraints elsewhere in the system limit the number of bits of  signature you can transfer, you're stuck.  Presumably over time you'd  want to go to a more bit-efficient signature scheme, perhaps using  ECC.  But as it is, the choice appears to be between (a) continuing  the current completely unprotected system and (b) *finally* rolling  out protection sufficient to block all but very well funded attacks  for a number of years.
Should we let the best be the enemy of the good here?
                                                         -- Jerry

@_date: 2009-10-18 09:21:58
@_author: Jerry Leichter 
@_subject: Collection of code making and breaking machines 
A bit too far for a quick visit (at least for me):
                                                        -- Jerry

@_date: 2009-10-19 22:06:13
@_author: Jerry Leichter 
@_subject: Possibly questionable security decisions in DNS root management  
What factoring algorithms would be optimized for a fixed number of  bits?  I suppose one could have hardware that had 1024-bit registers,  which would limit you to no more than 1024 bits; but I can't think of  a factoring algorithm that works for 1024 bits, the top one of which  is 1, but not at least equally well when that top bit happens to be 0.
                                                         -- Jerry

@_date: 2009-10-24 17:31:27
@_author: Jerry Leichter 
@_subject: Security of Mac Keychain, File Vault 
The article at  claims  that both are easily broken.  I haven't been able to find any public  analyses of Keychain, even though the software is open-source so it's  relatively easy to check.  I ran across an analysis of File Vault not  long ago which pointed out some fairly minor nits, but basically  claimed it did what it set out to do.
The article makes a bunch of other claims which aren't obviously  Anyone one know of more recent analysis of Mac encryption stuff?  (OS  bugs/security holes are a whole other story....)
                                                         -- Jerry

@_date: 2009-10-29 23:25:17
@_author: Jerry Leichter 
@_subject: Security of Mac Keychain, Filevault 
A couple of days ago, I pointed to an article claiming that these were  easy to break, and asked if anyone knew of security analyses of these  I must say, I'm very disappointed with the responses.  Almost everyone  attacked the person quoted in the article.  The attacks they assumed  he had in mind were unproven or unimportant or insignificant.  Gee ...  sounds *exactly* like the response you get from companies when someone  finds a vulnerability in their products:  It's not proven; who is this  person anyway; even if there is an attack, it isn't of any practical  Meanwhile, I know many of us on this list use Macs, and many of us  rely on keychain and Filevault, or at least on encrypted disk images.   On what rational basis do we rely these?  The only analysis of  Filevault that I know of is Applebaum and Weinmann's  , which dates back to 2006, two releases of Mac OS ago.  (It found the  basic mechanisms sound, with some problems around the edges.)  I'm not  aware of any analyses of Keychain, although key chains can be  extremely high-value.  If no one on this list is aware of any  analyses, I'd guess they just don't exist.
Over all, Apple's designs and implementations of security code have  been good, but hardly perfect.  (Witness the recent questionable  implementation of encryption on the iPhone 3GS.)  So these are  legitimate issues.  Meanwhile, I'm sure many of us have potentially  high-value passwords - like our Mobile Me password - stored in our  iPhones and iPod Touches.  How safe is that?  I have yet to see any  analysis of that question either (though I suspect the answer is "not                                                           -- Jerry

@_date: 2009-09-01 22:55:31
@_author: Jerry Leichter 
@_subject: "Fed's RFIDiocy pwnd at DefCon" 
"NSA spooks gather for a colleague?s retirement party at a bar. What  they  don?t know is that an RFID scanner is picking them out - and a  wireless Bluetoothwebcam is taking their picture.
Could that really happen? It already did.
(The Feds got a taste of the real world risks of RFID passports and  IDs at DefCon, the annual hacker conference. According to Wired  . . . federal agents at the conference got a scare on Friday when they  were told they might have been caught in the sights of an RFID reader.
The reader, connected to a web camera, sniffed data from RFID-enabled  ID cards and other documents carried by attendees in pockets and  backpacks as they passed a table where the equipment was stationed in  full view...."
                                                         -- Jerry

@_date: 2009-09-07 08:58:10
@_author: Jerry Leichter 
@_subject: Client Certificate UI for Chrome? 
I'm not sure what version of Camino you're running.  The most recent  versions use a standard Mac OS GUI element to prompt for passwords -  it's indistinguishable from what you get from Safari.  In both cases,  a special prompt window scrolls down out of the chrome, covering some  of the main body of the window.  It has a distinctive look:  After  it's scrolled down, it appears to be "under" the chrome but over the  top of the body.  In Safari - I didn't experiment with Camino - it's  physically tied to the browser window, moving and iconifying with it,  and is fully modal at the window level - you can't switch to another  tab in the same window.  (Curiously, you *can* switch to a different  window.)  The "loading" indicator in the address bar remains active  while you're being prompted.  The *intent* is clearly to create  something hard to spoof, but I don't know enough Flash to say if one  could produce an accurate, or even plausible, fake.  (Of course,  *most* passwords on the Web are entered into some random web page.  A  distinctive secure prompt that only appears in a minority of cases  doesn't help much.)
The most common MacOS password prompts are from the keychain program,  since you typically store your passwords there.  (There are  configurations in which it just asks for permission, not for a  password; and configurations in which it just sends the password.  But  if you want to be careful, you only want keychains unlocked when you  intend to use them.)  Since *any* program - including programs with no  visible GUI - can use keychains, these prompts are necessarily stand- alone windows at least sometimes (and for uniformity, they are so all  the time).  Those could be spoofed more easily (though if you're  really cautious, you can unlock the necessary keychain by hand ahead  of time and arrange to just give permission to use the entry later, so  you're never entering your password into a window that just pops up on  its own).
                                                         -- Jerry

@_date: 2009-09-07 09:06:59
@_author: Jerry Leichter 
@_subject: Client Certificate UI for Chrome? 
I should expand on that a bit:  This GUI element is used for all kinds  of things tied to a window, not just passwords.  For example, if you  try to close a window that contains stuff you haven't saved, the same  element is used to ask you to confirm, save now, or cancel.  So it's a  pretty familiar thing to Mac users....
                                                         -- Jerry

@_date: 2009-09-08 22:08:59
@_author: Jerry Leichter 
@_subject: "Fed's RFIDiocy pwnd at DefCon" 
Remember:  Before it's actually happened, any discussion is just  reckless speculation, rumor-mongering, or worse.
After it's actually happened, it's either (a) not a real issue; (b) a  major new attack that could not have been foreseen but that will be  dealt with immediately by top people.  Top people.
                                                         -- Jerry

@_date: 2009-09-17 20:35:45
@_author: Jerry Leichter 
@_subject: Detecting attempts to decrypt with incorrect secret key in OWASP ESAPI 
Interesting.  It sounds as if the JCE developers have gone from one  extreme to another.  I no longer remember the details, but a number of  years back, in a project I was involved with, we needed to implement  some particular (sane) combination of a cipher and a mode.  JCE at the  time had a fixed list of combinations it was willing to let you use;  ours wasn't on that list.  "ECB" wasn't an accepted mode, so it wasn't  easy to build your own mode out of what the API provided.
                                                         -- Jerry

@_date: 2009-09-29 10:40:27
@_author: Jerry Leichter 
@_subject: Unexpected side-effects 
Well, here I'll expect one. :-)
As there is increasing pressure to keep
records of Internet use, there will be a counter-move to use VPN's  which promise to keep no records.  Which will lead to legal orders  that records be kept, with no notification to those being tracked.  Enter secure remote attestation - rendering it impossible for an  appropriately defined non-logging implementation to start logging  without giving this fact away.
Maybe it'll be the pirates who make the first large-scale use of those                                             -- Jerry

@_date: 2010-04-21 22:49:50
@_author: Jerry Leichter 
@_subject: What's the state of the art in factorization? 
It's perhaps worth pointing out again how little formal complexity  proves tell you about security.
Suppose we could show that RSA was as hard as factoring.  So?  Nothing  is really known about how hard factoring is.  At worst, it's NP- complete (though that's actually considered unlikely).  But suppose  *that* was in fact known to be the case.  What would it tell us about  the difficulty of factoring any particular product of two primes?   Absolutely nothing.  NP-completeness is a worst-case result.  In  principle, it could be the case that factoring is "easy" for numbers  less than a billion bits long - it just becomes much harder  "eventually".  (I put "easy" in quotes because it's usually taken to  mean "there's a poly-time algorithm", and that's a meaningless  statement for a finite set of problems.  *Every* finite set of  problems has a O(1) time solution - just make a lookup table.)
There are some concrete complexity results - the kind of stuff Rogoway  does, for example - but the ones I've seen tend to be in the block  cipher/cryptographic hash function spaces.  Does anyone one know of  similar kinds of results for systems like RSA?
                                                         -- Jerry

@_date: 2010-08-01 00:00:39
@_author: Jerry Leichter 
@_subject: init.d/urandom : saving random-seed 
On the question of what to do if we can't be sure the saved seed file  might be reused:  Stir in the date and time and anything else that  might vary - even if it's readily guessable/detectable - along with  the seed file.  This adds minimal entropy, but detecting that a seed  file has been re-used will be quite challenging.  A directed attack  can probably succeed, but if you consider the case of a large number  of nodes that reboot here and there and that, at random and not too  often, re-use a seed file, then detecting those reboots with stale  seed files seems like a rather hard problem.  (Detecting them  *quickly* will be even harder, so active attacks - as opposed to  passive attacks that can be made on recorded data - will probably be  out of the question.)
I wouldn't recommend this for high-value security, but then if you're  dealing with high-value information, there's really no excuse for not  having and using a source of true random bits.
                                                         -- Jerry

@_date: 2010-08-01 11:15:17
@_author: Jerry Leichter 
@_subject: init.d/urandom : saving random-seed 
I discussed this in the rest of my message.  Apparently I wasn't  explicit enough.
Consider the overall situation.  We have a large number of systems out  there. They reboot here and there.  Each of them saves their previous  PRNG state, and uses it on reboot.  Sometimes, the PRNG state is not  saved, and a second reboot uses the same state.
Attackers do *not* have access to the saved state.  They *can*,  however, observe the values produced by the PRNG.  (In practice, they  get a noisy, incomplete sampling through things like generated keys,  but let's make the safe assumption that they see the complete  stream.)  Thus, they can easily check that a given system is  generating the same sequence of values, so presumably didn't update  its state.  Thus, it will continue to produce the same values for a  while (until it can gather enough entropy).
Now consider the situation where a rebooting system stirs in the date  and time, the MAC and IP addresses of the first couple of messages it  sees, whatever.  Given a properly implemented PRNG, the output stream  will look nothing like the output stream it produced the last time  around.  (Flipping one bit of input to something like SHA-1 flips, on  average, about half the output bits.)
Sure, an attacker who *knew* the saved state could easily *guess* the  low-entropy inputs, match what the system emits, and break security.   But the problem here is entirely different:  Recognizing that the  stream it's seeing is the result of a small tweak applied to an  unknown previous state.  If someone an show a solution to that  problem, I'll withdraw my suggestion.  But I doubt any simple attack  of this sort exists.
If an attacker can't recognize systems that are re-using state, it has  to try attacking all of them.  Think about what that attack looks  like.  When the system works as it's supposed to, the state is saved  over the reboot, so the reboot effectively never occurred.  The extra  guessable state makes no difference:  In this situation, we're  assuming that the PRNG is secure.  When a restart does occur, the  attacker is seeing two (or more) continuations from the same *unknown*  internal state, using different guessable - assume known - small  tweaks, and he must predict the future outputs.  It's kind of a  related-key attack, and doesn't look easy.  In fact, if the number of  tweaked bits is small, it doesn't look like there's even enough  information available to solve it in principle.  (With enough reboots  without a state update, the bits accumulate.)  But there are likely  situations it's plausible.  In those - use a true RNG.
                                                         -- Jerry

@_date: 2010-08-01 21:20:18
@_author: Jerry Leichter 
@_subject: Is this the first ever practically-deployed use of a threshold scheme? 
It'll be interesting to see the responses, but ... in this particular  case, we do actually have plenty of experience from physical  applications.  Vaults that can be opened only by multiple people each  entering one part of the combination have been around for some time.   For that matter, that requires mechanisms for having multiple people  set their part of the combination.  Even requirements for dual  signatures on checks beyond a certain size are precedents.
One could certainly screw up the design of a recovery system, but one  would have to try.  There really ought not be that much of difference  between recovering from m pieces and recovering from one.
Of course, one wonders how well even the simpler mechanisms work in  practice.  The obvious guess:  At installations that actually exercise  and test their recovery mechanisms regularly, they work.  At  installations that set up a recovery mechanism and then forget about  it until a lightning strike takes out their KDC 5 years later ...  well, I wouldn't place big bets on a successful recovery.  This isn't  really any different from other business continuity operations:   Backup systems that are never exercised, redundant power systems that  are simply kept in silent reserve for years ... none of these are  likely to work when actually needed.
                                                         -- Jerry

@_date: 2010-08-02 06:22:43
@_author: Jerry Leichter 
@_subject: Is this the first ever practically-deployed use of a threshold scheme? 
Well ... we do have a history of producing horrible interfaces.
Here's how I would do it:  Key segments are stored on USB sticks.   There's a spot on the device with m USB slots, two buttons, and red  and green LED's.   You put your "USB keys" into the slots and push the  first button.  If the red LED lights - you don't have enough sticks,  or they aren't valid.  If the green LED lights, you have a valid key.   If the green LED lights, you push the second button (which is  otherwise disabled), and the device loads your key.  (The device could  also create the USB sticks initially by having a "save key" setting -  probably controlled by a key lock.  "Voting out" and replacing a  segment requires a bit more, but could be designed along similar lines.)
You can use some kind of secure USB stick if you like.  The content of  a USB stick is standard - there has to be a file with a known name and  some simple format, so it's easy to re-create a USB stick from a paper  copy of the key.
Since specialized hardware is expensive, you can approximate this  process with software (assuming you get a competent designer).  You  can get by with only one USB slot, but given the tiny cost of USB hubs  - I can buy a complete 10-port USB hub, power adapter included,  shipped free, for less than $16 at mertiline.com, for example (and  that's gross overkill) - it's probably worth it to give users a nice  physical "feel" of inserting multiple keys into multiple locks.
I just don't see the great cognitive load involved, if the problem is  presented properly.
                                                         -- Jerry

@_date: 2010-08-03 05:58:24
@_author: Jerry Leichter 
@_subject: GSM eavesdropping 
And, indeed, there are movie cons - and many episodes of Mission:  Impossible - that turn on the ability to plant false information by  convincingly impersonating such a medium.
It's not as if identification solves all problems anyway.  A  completely secure link to use when sending your money to Bernie Madoff  still leaves you with nothing.
I think it's an oversimplification.  Cryptography can, in effect,  guarantee the syntax:  You receive the right bytes from a source whose  identify matches some purely internal description, and no one else  could have seen those bytes.  But any real-world binding - of those  bytes to semantics, of that identity to some actual actor, of "no one  else" to trust that the sender didn't send it to Wikileaks because he  doesn't actually trust you ... all of this is entirely outside the  cryptographic envelope.  There's no escaping the need to understand  the semantics, not just the syntax - as valuable as the syntax is.
Are you sure?  How about maintaining the privacy of the groups to  which a user subscribes?  (Granted, whoever you get your feed from  obviously knows - but why should anyone in a position to see the  message stream?)
Indeed.  Consider the all-too-well-named SOAP, which lets arbitrary  commands and data slip right through your firewall.  If you step back  a moment and look at the whole notion of using firewalls to control  port numbers, you see just what an absurd corner we've gotten  ourselves into.  We have an OS that can run multiple independent  programs at different port numbers, and is actually very competent at  keeping them isolated from each other, relying at base on hardware  protection.  We've replaced it with a browser which nominally allows  only one kind of connection, but then runs multiple independent  programs at different HTTP addresses - and, with no hardware  protection to help, has proved quite incompetent at keeping them  isolated from each other.  This is progress?
...but it opens the door to generations of improved DPI and other  technologies that try to do it for you.  With limited success at the  original mission, but all kinds of interesting privacy-invading and  censorship-enabling new missions discovered along the way.
Hmm.  I'm not sure exactly sure how that follows.
                                                         -- Jerry

@_date: 2010-08-03 22:08:45
@_author: Jerry Leichter 
@_subject: GSM eavesdropping 
It's worth pointing out that you're here making a value judgement -  and, in effect, a political argument.  Large scale monitoring is  mainly, if not entirely, something governments do.  It's unlikely to  be cost-effective for the commercial attackers we see today.  Active,  targeted attacks, on the other hand, seem to be the purview of many  sophisticated attackers today - both governmental and non-governmental.
Cryptographic theory can help you decide which of these classes of  attackers you should be more concerned about.
BTW, economics is everywhere.  Suppose you had a cryptographic  technique that was quick and easy to apply, but also cheap to break -  say, $1 per message.  Pretty useless, right?  But now imagine that  every message is encrypted using this poor technique.  No individual  message, once known through external signals to have value greater  than $1, is safe - but  the aggregate of billions of messages being  transfered every day is safe against any plausible attacker.
                                                         -- Jerry

@_date: 2010-08-05 22:04:12
@_author: Jerry Leichter 
@_subject: The long twilight of IE6 
We discussed the question of why IE6 is still out there.  Well ...    reports that the UK government has officially decided not to replace  IE6, feeling the costs outweigh the benefits.  Quoting from the  government response:
"Complex software will always have vulnerabilities and motivated  adversaries will always work to discover and take advantage of them.  There is no evidence that upgrading away from the latest fully patched  versions of Internet Explorer to other browsers will make users more  secure. Regular software patching and updating will help defend  against the latest threats. The Government continues to work with  Microsoft and other internet browser suppliers to understand the  security of the products used by HMG, including Internet Explorer and  we welcome the work that Microsoft are continuing do on delivering  security solutions which are deployed as quickly as possible to all  Internet Explorer users....
It is not straightforward for HMG departments to upgrade IE versions  on their systems. Upgrading these systems to IE8 can be a very large  operation, taking weeks to test and roll out to all users. To test all  the web applications currently used by HMG departments can take months  at significant potential cost to the taxpayer. It is therefore more  cost effective in many cases to continue to use IE6 and rely on other  measures, such as firewalls and malware scanning software, to further  protect public sector internet users."
                                                         -- Jerry

@_date: 2010-08-10 21:53:46
@_author: Jerry Leichter 
@_subject: "Cars hacked through wireless tire sensors" 
Excerpted from                                                          -- Jerry
The tire pressure monitors built into modern cars have been shown to  be insecure by researchers from Rutgers University and the University  of South Carolina. The wireless sensors, compulsory in new automobiles  in the US since 2008, can be used to track vehicles or feed bad data  to the electronic control units (ECU), causing them to malfunction.
Earlier in the year, researchers ... showed that the ECUs could be  hacked.... The new research shows that other systems in the vehicle  are similarly insecure. The tire pressure monitors are ... wireless,  allowing attacks to be made from adjacent vehicles. The researchers  used equipment costing $1,500... to eavesdrop on, and interfere with,  two different tire pressure monitoring systems.
The pressure sensors contain unique IDs, so merely eavesdropping  enabled the researchers to identify and track vehicles remotely.  Beyond this, they could alter and forge the readings to cause warning  lights on the dashboard to turn on, or even crash the ECU completely.
Unlike the work earlier this year, these attacks are more of a  nuisance than any real danger; the tire sensors only send a message  every 60-90 seconds, giving attackers little opportunity to compromise  systems or cause any real damage. Nonetheless, both pieces of research  demonstrate that these in-car computers have been designed with  ineffective security measures.
[To be presented at Usenix.]

@_date: 2010-08-17 03:57:18
@_author: Jerry Leichter 
@_subject: Haystack 
The mainstream press is full of discussion for a new program,  Haystack, developed by a guy name Austin Heap and sponsored by the  Censorship Research Center as a new kind of secure proxy.  See    for some information.
As described, the program relies on some kind of steganography to hide  encrypted connections inside of connections to "approved" sites.  It  was specifically designed to help Iranian dissidents maintain  connections in the face of active government efforts to locate and  block proxies and Tor entry and exit nodes.
A Google search reveals absolutely no technical information about  exactly what Haystack does or now it does it.  The program is  available on multiple platforms but is closed source - the FAQ linked  to above discusses this, citing fears that making the source available  would help censors.
Anyone know anything more about what Haystack is actually doing?
                                                         -- Jerry

@_date: 2010-08-17 16:56:19
@_author: Jerry Leichter 
@_subject: A mighty fortress is our PKI, Part II 
...thus once again demonstrating how much of good cryptographic  practice is just good engineering/release management practice.
A number of years ago, in addition to being in charge of much of the  software development, I had the system management organization of the  small software maker I worked at reporting to me.  I formalized a  process that the (already well run) organization already had in  place.  Any time *any* build of the software "left the building", even  if just for a demo, we marked that build as "locked".  We would never  delete a "locked" build without a careful determination that it was,  in fact, "dead":  No longer in use at any customer.     We also,  within 24 hours, did a special backup of the build onto a tape that  went into permanent off-site storage.
The one time I know of that we didn't follow these procedures (before  they were officially put into place), a very large customer, at their  insistence and after the sales guy who dealt with them swore they  agreed to delete the copy we gave them, got a snapshot of a build from  a developer's workstation "just to see how the new version would  look".  Without telling us, the customer proceeded to roll this out at  hundreds of sites, resulting in years of support grief, since it was  impossible for us to determine exactly what went into the code they  were running.
We were later acquired by a much larger company that claimed they  would "teach us how to do big-league software engineering".  Hah.   That company was shipping software built on developer workstations as  a day-to-day practice - they were just beginning to develop mechanisms  to ensure that the stuff they shipped came through traceable,  reproducible builds.  Oh ... but their stuff was in Java, so was  signed  The signing was tightly controlled at a central location.  Cue  classic joke about using an armored car to deliver an envelope to  someone living in a cardboard box.
                                                         -- Jerry

@_date: 2010-08-18 08:21:40
@_author: Jerry Leichter 
@_subject: Collage 
Yesterday I asked about Haystack, an anti-censorship system that  appears to exist mainly as newspaper articles.  So today I ran across  another system, which appears to be real:  Collage ( ), developed by a group at Georgia Tech and to be presented at  Usenix.  On a crypto level, unlike Haystack, Collage is nothing new:   It uses steganographic techniques to hide text in photos.  What it  contributes is easy to use software for both embedding and extracting  the data, integrated with Flickr.
                                                        -- Jerry

@_date: 2010-08-21 08:54:18
@_author: Jerry Leichter 
@_subject: towards https everywhere and strict transport security (was: Has there been a change in US banking regulations recently?) 
I read through the HTTP Strict Transport Security (HSTS) Draft RFC ( ) and it's an odd mix.  It continues - and expands on - Firefox's war  against self-signed certs - while adding to the Web exactly the same  kind of SSH-style "connection continuity" security for which self- signed certs are completely appropriate!
Consider the scenarios:
1.  "First connection" to an HSTS server.  Here "first connection"  means that client is approaching this server with no pre-existing  security context - either it's never connected before, or it's stored  no information that will influence security decisions on its part,  from the previous connections, if any.  Today, that describes *all*  standard connections, though many different extensions have been  proposed, and plug-ins implemented, to change this.
The security of such a connection, in the presence of a MITM attack,  depends entirely on the trustworthiness of the certificate chain.   There is, ex hypothesi, nothing else that drives the client's decisions.
HSTS has absolutely nothing to offer here.  A MITM won't forward the  HSTS information if it finds it inconvenient to do so.  Or it will  fake it.
Because HSTS forbids self-signed certs, no legitimate server will  present a combination of a self-signed cert and an HSTS header.  No  intelligent MITM will do so either.  It will either suppress the HSTS  header - or, more likely, use a perfectly legitimate cert signed by a  perfectly legitimate CA that the client will accept.  After all, there  are hundreds of CA's out there and getting a signed cert is no big deal.
HSTS *does* provide protection against a *passive* "first connection"  attacker by blocking insecure connection attempts and requiring  HTTPS.  (Of course, you can do that in a server today with a redirect  - and in fact that's exactly how HSTS does it.)
2.  A "second connection":  One where the client has retained  information from a previous connection.  HSTS adds this state to the  standards:  For an amount of time specified by the server on the  previous connection, a new connection to the server (well, to be more  careful, to the URI or possibly subdomains - the whole question of  whether we are talking about a server or a URI or an address gets very  complex if you try to pin it down exactly) must use HTTPS, and must  get a "clean" connection, with no warnings (e.g., the CRL must be  checkable) and no self-signed certs.
So what does HSTS actually add?  I'd say three things:
But having gone this far ... the proposal then goes off into the weeds  by regulating self-signed certs for no really good reason, while  ignoring much more valuable things it *could* do.  There's nothing we  can do about the MITM vulnerabilities in the first connection scenario  without solving the PKI problem - which ain't gonna happen.  (HSTS  discusses the notion of pre-loading HSTS security information for some  sites along with CA lists, but itself notes that this doesn't really  solve any problems so much as rename them.  Besides - once you start  on this route, why not just distribute actual site keys?)
But why not let the server say "On subsequent connections, only accept  a cert with this fingerprint" (SSH connection) or "only accept a cert  signed by this CA" (sites don't change CA's often, and the time limit  means they can do so as long as they plan ahead) or "only accept certs  signed by CA's based in the following country" (like  Soghoian and  Stamm's work).  Why a "require this CA" together with a self-signed  cert shouldn't be allowed is beyond me - I can't see any situation in  which it's weaker than the current HSTS proposal.
Now, it's quite true that HSTS *allows* for extensions - but so does  HTTP, and so browsers.  What's hard is to get something very broadly  implemented.  Having the "something" be HSTS would be squandering an  opportunity.  I certainly wouldn't try to include every, or even many,  possibilities - that's a way of making sure that nothing gets done.   By being able to say:  For any connections to this server/URI/set of  domains for the next n seconds, accept only HTTPS connections with  certs signed by the following CA's (*including* self-signed certs!) -  now, that would be useful.
                                                         -- Jerry

@_date: 2010-08-26 06:25:55
@_author: Jerry Leichter 
@_subject: questions about RNGs and FIPS 140 
On Aug 25, 2010, at 4:37 PM, travis+ml-cryptography at subspacefield.org  No one has figured out a way to certify, or even really describe in a  way that could be certified, a non-deterministic generator.
IPS doesn't tell you how to *seed* your deterministic generator.  In  effect, a FIPS-compliant generator has the property that if you start  it with an unpredictable seed, it will produce unpredictable values.   Debian's problem was that it violated the "if" condition.  The  determinism of the algorithm that produced subsequent values wasn't  I'm not sure what you mean by "rolling back".  Yes, if you restart any  deterministic RNG with a previously-used internal state, it will  generate the same stream it did before.  This is true whether you are  in a VM or not.
RNG's in VM's are a big problem because the "unpredictable" values  used in the non-deterministic parts of the algorithms - whether you  use them just for seeding or during updating as well - are often much  more predictable in a VM than a "real" machine.  (For example, disk  timings on real hardware have some real entropy, but in a VM with an  emulated disk, that's open to question.)
We had a long discussion on this list a couple of weeks back which  came to the conclusion that a hidden, instance-specific state, saved  across reboots; combined with (fairly minimal) entropy at boot time;  was probably a very good way to go.
                                                         -- Jerry

@_date: 2010-08-28 07:14:20
@_author: Jerry Leichter 
@_subject: questions about RNGs and FIPS 140 
You have this precisely, and dangerously, backwards.
*Nothing* of this sort can be assumed.  You've given a plausibility  argument, based on your understanding of the behavior of a very  complex system.  Such arguments often go wrong.  You need to do some  experimentation to justify your conclusions.  You'll never *prove*  that you're getting unpredictability, since you aren't starting from a  sound theoretical base.  The best you can get is a combination of  broad analysis of the system and experimental confirmation that it  seems to behave according to your model - which, mind you, is all we  ever get for real, complex physical systems.  It's enough to build  everything from bridges to semiconductor devices.
To move beyond the pure abstract ... here are a couple of plausible  ways in which disk I/O in a VM might not deliver the same amount of  entropy as on real hardware:
1.  Multiple VM's boot the same image at the same time, reading  exactly the same disk blocks in the same order.  During the relevant  time period, the various VM's end up in a queue and run in a set  order.  They all then see (pretty much) the *same* delays - once the  heads are positioned, they satisfy all the reads in turn.
2.  Multiple VM's boot the same image at the same time, as in 1, but  the VM scheduler gives one VM enough time to run through the entire  sequence where it gathers startup entropy.  Subsequent VM's find their  disk requests satisfied from the VM-level buffer with essentially no  3.  The VM scheduler uses a large (relative to disk seek times) clock  tick.  Any disk interrupts due for a given VM are delayed until the VM  is scheduled to run.  Because the clock tick is large, there is  essentially no variation in how long it takes for a disk I/O to  complete:  It's always the period up to the next tick.
4.  SSD's have entirely different timing characteristics than spinning  disks, so require different measurement approaches.  (A collection  point based on disk seek time scales will miss any variation on SSD  time scales.)  The implementation is smart enough to look at the disk  type to determine its strategy.  However, the VMM hides the SSD  present on the real hardware from the VM, and instead presents a  standard spinning disk.  The VM then uses the wrong strategy.
Do any of these actually occur in real VM/OS combinations?  I don't  know, *and neither do you*.  Even if you know all the details of the  OS disk strategies, and have measured how your collection point works  on real hardware ... you should not believe that it will work  correctly on a VM until you test it there.  (In fact, you shouldn't  believe it will work correctly on significantly different hardware  until you test it:  Simply replacing the disk with an SSD may change  the behavior enough to require adjustment.)
                                                         -- Jerry

@_date: 2010-07-09 23:08:59
@_author: Jerry Leichter 
@_subject: Question w.r.t. AES-CBC IV 
CTR mode is dangerous unless you're also doing message authentication,  which will require an additional block or thereabouts - so if the goal  is to minimize bit overhead, it's not really appropriate here (unless  it's being included anyway).
The small number of encryptions before the IV repeats is the principle  problem   I see.  But there are a number of ways around it depending  on overall system details we don't have.  Two examples:
you don't even have to send the IV:  Agree once on a secret key, then  use CTR mode to generate an IV by encrypting the message sequence  number (which might or might not be included in the message itself).
include the end-point ID's, then you only have to worry about possible  clashes between IV's generated for the same pair of endpoints, or even  for the pair in the same direction.  (That's not *quite* true, but  it's close to true.)  If the two ends can agree on the date and time  even roughly - say to the nearest minute - then you can mix that in as  well.  Now you have to worry about a 50% chance of a clash if the same  pair starts up 2^16 connections within in a minute - probably not  likely enough to worry about.
                                                         -- Jerry

@_date: 2010-07-10 06:57:14
@_author: Jerry Leichter 
@_subject: Spy/Counterspy 
The 2G stuff wasn't designed to provide location information; that was  hacked in (by triangulating information received at multiple towers)  after the fact. I don't know that anyone has tried to do it from the  receiver side - it seems difficult, and would probably require  building specialized receiver modules (expensive).  3G provides  location information as a standard service, so it's cheap and easy.
The next attack, of course, is to use WiFi base station  triangulation.  That's widely and cheaply available already, and quite  accurate in many areas.  (It doesn't work out in the countryside if  you're far enough from buildings, but then you don't have to go more  than 60 miles or so from NYC to get to areas with no cell service,  either.)  The signals are much stronger, and you can get location data  with much less information, so jamming would be more of a challenge.   Still, I expect we'll see that in the spy vs. spy race.
I wrote message to Risks - that seems to never have appeared - citing  an article about GPS spoofing.  (I've included it below.)  In the spy  vs. spy game, of course, it's much more suspicious if the GPS suddenly  stops working than if it shows you've gone to the supermarket.  Of  course, WiFi (and presumably UMTS equipment, though that might be  harder) can also be spoofed.  I had an experience - described in  another RISKS article - in which WiFi-based location suddenly  teleported me from Manhattan to the Riviera - apparently because I was  driving past a cruise ship in dock and its on-board WiFi had been  sampled while it was in Europe.
                                                         -- Jerry
The BBC reports ( 8533157.stm) on the growing threat of jamming to satellite navigation  systems.  The fundamental vulnerability of all the systems - GPS, the  Russian Glonass, and the European Galileo - is the very low power of  the transmissions.  (Nice analogy:  A satellite puts out less power  than a car headlight, illuminating more than a third of the Earth's  surface from 20,000 kilometers.)  Jammers - which simply overwhelm the  satellite signal - are increasingly available on-line.  According to  the article, low-powered hand-held versions cost less than ?100, run  for hours on a battery, and can confuse receivers tens of kilometers  The newer threat is from spoofers, which can project a false  location.  This still costs "thousands", but the price will inevitably  come down.
A test done in 2008 showed that it was easy to badly spoof ships off  the English coast, causing them to read locations anywhere from  Ireland to Scandinavia.
Beyond simple hacking - someone is quoted saying "You can consider GPS  a little like computers before the first virus - if I had stood here  before then and cried about the risks, you would've asked 'why would  anyone bother?'." - among the possible vulnerabilities are to high- value cargo, armored cars, and rental cars tracked by GPS. As we build  more and more "location-aware" services, we are inherently building  more "false-location-vulnerable" services at the same time.

@_date: 2010-07-10 07:32:50
@_author: Jerry Leichter 
@_subject: What is required for trust? 
Given the state of the art, there appears to be no way to get any  assurance you can reasonably believe in.  See  , full paper at    - for some work in this area:  The authors took an open-source  design for a SPARC chip and made some very small modifications to it.   The resulting processor could not reasonably be distinguished from an  unmodified one by any feasible testing, but renders any software  protection you might use on the device completely ineffective against  someone who knows how to trigger the hardware hacks (which can be done  remotely).  The only way you would know this stuff is there is by  vetting the design - and detecting ~100 new lines of VHDL among  11,000, or 1000 new gates out of 1.8 million.  And, of course this is  a proof of concept, involving a very simple processor and no attempts  to absolutely minimize the visibility of the changes.
People usually fall back on "well, get chips from multiple sources,  they can't compromise them all".  But that doesn't work here:  If you  don't know which chips are "good" and which are "traitors", you don't  know there isn't a traitor in the very equipment you have to rely on.   Further, obvious ideas like running extensive comparisons of outputs  of chips from multiple sources don't work against attacks that only  open the chip on a specific command.  I suppose you could make sure  every device that operates on sensitive data has redundant chips from  multiple vendors and compare outputs - but then at the least you're  vulnerable to a denial of service attack, which in some circumstances  is almost as bad.  And even if you do find that two chips disagree -  which is the "bad" one?  And if figure that out - you now know one  "bad" source, but you have no evidence that the source of the other  chip hasn't also "spiked" it in some different way.  (The classic  trick here is to have two attacks, and let one be "found" - after  which the target *thinks* he's safe.)
The whole question of how to get trustworthy parts appears to be a  huge issue in the US military/intelligence community these days.   They're putting together consultations with academia and industry -  and undoubtedly also funding all kinds of secret work as well.  In the  old days, it was practical for sensitive operations to build their own  chips at vetted plants.  Those days are gone - there are only a  limited number of plants on the entire planet that can build state-of- the-art chips, the technology itself has been mastered by only a  limited number of players, and the costs are immense even by military/ black funding standards.
                                                         -- Jerry

@_date: 2010-07-10 07:34:29
@_author: Jerry Leichter 
@_subject: A real case of malicious steganography in the wild? 
Unfortunately (from a particular point of view), we'll probably never  find it:  It looks as if these agents will be swapped for various  Americans held by the Russians.
                                                         -- Jerry

@_date: 2010-07-11 15:40:07
@_author: Jerry Leichter 
@_subject: Spy/Counterspy 
I have no clue what "most" location-aware services will be in a year,  much less in five or ten years.  Sure, if you think that the dominant  role for such services will be targeted advertising to people passing  by storefronts, then it makes little difference if the location is  wrong, except perhaps to the stores (and hence the viability of such  services) if grossly incorrect information becomes commonplace.  But  if the service is "find me the hospital I can get to fastest, given  current road conditions", the cost of error may be rather higher.
Privacy is an entirely distinct issue.  At the least, services in  which I compute something from my location and data I've pre-loaded  for a reasonably large area - without ever revealing my location to  someone else - have no privacy implications at all.  (Note that I've  described the characteristics of most GPS units sold today.)  But it's  easy to come up with examples where such a location-aware service  becomes dangerously vulnerable - and perhaps dangerous - if it is fed  incorrect location information.
How much and how often I share my own location information, under what  conditions, and what I get in return, are all very much up in the air  - though if we don't address them, they will default to "fairly  precise location information, fairly frequently, with few usage  restrictions, for little I want".  But the inherent vulnerability to  falsified information is an inherent part of coming up with any  valuable use of true information, no matter what privacy policies we  agree on.
                                                         -- Jerry

@_date: 2010-07-28 06:59:46
@_author: Jerry Leichter 
@_subject: A mighty fortress is our PKI, Part II 
There is, of course, the problem of knowing when a signature was stolen!  You can know that it was definitely stolen *by* a particular date, but never that it wasn't stolen earlier.  Given that it was stolen, what evidence could you produce that it wasn't stolen - and simply kept around for later use - at the moment it was created?
Beyond that, you it's often hard to know when you received a file.  Perhaps the actual attack was to stick it on your system and backdate it!  A forensic examination could look at backups, but we're talking about how to decide whether to accept a signature in an operational setting.  You can't, of course, rely on any dates within the file itself, as they are protected from fakery only by the signature that you can't trust.  You could rely on a digital time-stamping service ... but that just brings into sharper focus the absurdity that actually began the moment you needed to check an on-line CRL.  The only conceivable purpose for using a signature is that you can check it *offline*.  If you assume you can connect to the network, and that you can trust what you get from the network - why bother with a signature?  Simply check a cryptographic hash of the driver against an on-line database of "known good" drivers.
This is right in line with Lynn Wheeler's frequent mention here that the use case for offline verification of certs for commerce basically doesn't exist.  It was a nice theory to develop 30 years ago, but today the rest of the framework assumes connectivity, and you buy nothing but additional problems by focusing on making just one piece work off-line.

@_date: 2010-07-29 07:19:52
@_author: Jerry Leichter 
@_subject: deliberately crashing ancient computers (was: Re: A mighty fortress is our PKI) 
I agree 100% with the statement that deliberately crashing other  people's computers is inappropriate.  Don't do that.
But the reasons you give for why there are still IE6 installations out  there (low computer literacy, slow connections, etc.) aren't quite  right.  Apparently there are many internally-developed applications at  companies that are IE6-only.  Often, these were developed by outside  consultants for customers who have no internal development staff.   These things keep the business running, and replacing them would be a  large expense that the companies involved are not in a position to  One of the biggest and most visible of such applications was the one  that the national realtor's organization used to allow its members to  get access to listings.  They resisted doing anything about that for  many years.  (I understand that within the last year or so, they  finally had to respond to complaints from their members and redo the  It will be many years before these internal applications disappear.   They are in a class similar to embedded systems, where replacement of  working stuff is almost never done, and support obligations on  long-"obsolete" software run for decades.  Microsoft would love to  forget that IE6 ever existed - what was once their way of dominating  much of the Internet has turned into a millstone around their necks;  but they can't.  (Analogies to The Ring of Sauron come to mind....)
An interesting "benefit" that some of the businesses with IE6-only  internal software are finding is that, if they keep their employee's  machines IE6-only, their employees are increasingly unable to access  most Internet sites.  Talk about perverse incentives....
                                                         -- Jerry

@_date: 2010-03-25 21:42:36
@_author: Jerry Leichter 
@_subject: New Research Suggests That Governments May Fake SSL Certificates 
While the paper provides a nice analysis and description of the  situation, what surprises me most about it is ... that anyone was  surprised.  Hardware to support man-in-the-middle splicing of HTTPS  sessions has been available in the marketplace for several years.   They are sold by companies like Bluecoat who build appliances to  monitor incoming and outgoing traffic at the interconnection points  between corporate networks and the greater Internet.  They're sold as  means to monitor and control what sites can be accessed (they block  things like gambling sites, pornography - whatever the corporation  doesn't want its employees browsing from work) and also inspect the  data for auditing/information leakage control purposes.
In the corporate environment, where desktops/laptops are managed, the  way such a device is given the ability to do MitM attacks is  straightforward:  The corporation simply pushes a new root CA - for a  CA that actually lives inside the intercept device - into the  browser's pool.  The device can then generate and sign any certs it  needs to to wedge into any HTTPS session invisibly.  Even when the  corporation allows personal machines onto the network, it will often  require users to accept a corporate CA for access to internal sites.   Of course, since browsers only have one pool of CA's, once you've  accepted that CA, you've accepted invisible MitM attacks by the  monitoring device.
Since the techniques and hardware for doing this has been around for a  while, it should come as no surprise that someone would notice that  governments are another good market - in fact, one that tends to be  fairly price-insensitive.  It's distressing how much government  intrusion technology is basically relabeled corporate security/ compliance technology.
Governments may or may not be in a position to force CA's onto a  machine, so it would be natural for them to compel existing CA's, as  the paper rightly points out.
                                                         -- Jerry

@_date: 2010-10-02 19:10:19
@_author: Jerry Leichter 
@_subject: 'Padding Oracle' Crypto Attack Affects Millions of ASP.NET Apps 
I wouldn't call that a shortcut - one has to actually define two  failure returns and choose which one to send.  More code, more  complexity.  But ... there's a reason for doing this, in virtually all  situations:  Manageability, Without some detail as to exactly what  went wrong, it's very hard to know what to correct or even where to  Cryptographic protocols are outliers here, because here you really  can't afford to make the system "manageable" if the cost is that it  can then serve as an oracle.  This is one reason it's so hard for most  developers to produce correct crypto implementations:  So much of what  is good practice almost everywhere else ends up being hazardous when  it comes to cryptography.
It's not that there are *no* other situations where better error  messages are a potential liability.  The classic example is error  messages that leak information about directory configurations or  database tables, thus enabling other attacks.  The difference is that  such information, in most cases, is only problematic if the actual  system *has some other vulnerability*.  We may not be at all good at  producing systems without such vulnerabilities, but at least we know  in general principle how to do so.  (If there are no SQL injection  attacks, knowing what tables exist probably isn't useful to an  attacker.)  One might argue that a cryptosystem that is vulnerable to  a "what failed" attack is also buggy - but we really have no general  idea how to fix them.  So we have to avoid them by accepting hard-to- manage protocols.
By the way, the "don't acknowledge whether it was the login ID or the  password that was wrong" example is one of those things "everyone  knows" - along with "change your password frequently" - that have long  passed their "use by" date.  Just what attack on a modern system does  revealing that a guessed login ID is correct actually allow?  It can  only be used in on-line attacks, and it's been years since any decent  system didn't protect against high rates of failures in on-line  authentication.  Besides, valid - or highly-probably-valid - login  ID's are typically cheaply available for most systems anyway.
I'm really tired of using up my on-line password tries, only to  discover that I accidentally erased the last character on the login  name, or added a trailing \ when I went for the RETURN key!  Systems  are *so* nice to keep re-prompting with the bogus username.
Really, most systems these days reveal valid usernames quite easily.   Both Windows boxes and Macs typically give you a pull-down list of  accounts to log into.  Web logins have ways to recover passwords for  lost accounts that usually reveal if you get the account name wrong -  and then give you a way to recover the account name.  Can you think of  an example in, say, the last 20 years of an attack that would not have  been possible if the system had made it difficult to determine valid                                                           -- Jerry

@_date: 2010-10-07 12:05:09
@_author: Jerry Leichter 
@_subject: English 19-year-old jailed for refusal to disclose decryption key 
Sure. And the technology used would have no effect on the standard used in court:  Is there sufficient convincing evidence that there's data there to decrypt (e.g., you used the system in the last day to send a message based on the kind of information sought)?  If so, decrypt or go to jail.  "Beyond a reasonable doubt" isn't the standard for everything, and even of it were, it's as understood by a judge or jury, not a logician.                                          -- Jerry

@_date: 2010-10-07 20:14:41
@_author: Jerry Leichter 
@_subject: English 19-year-old jailed for refusal to disclose decryption key 
You're thinking too much about the technology.
The court demands a company turn over its books.  The company denies it
keeps any books.  Sure - massive fines, possible jail sentences for the
Alternatively, the company turns over fake books.  There is evidence  that the
books are fake - they show the company only did 2000 transactions last  but somehow the company paid a staff of 200 to take phone calls last  Or the books don't show any payments for things that we see sitting in  warehouse.  Or maybe there are just purely statistical anomalies:  The
variation in income from week to week is way out of the range shown by
other businesses.  Or there's just someone who swears that these are not
the books he's seen in the past.  Same outcome for the company.
Maybe the high-tech cheats let you get away with stuff; maybe they  Then again, maybe the fake paper books let you get away with stuff, and
maybe they don't.  Technology lets you play some games more easily,
but it's not magic pixie dust that immunizes you from reality.

@_date: 2010-09-05 23:27:32
@_author: Jerry Leichter 
@_subject: Randomness, Quantum Mechanics - and Cryptography 
The recent discussion of random number generators reminded me of  something that I've been meaning to write a note about.  A couple of  years back, John Conway and Simon Kochen proved what they nicknamed  the Free Will Theorem.  Its informal statement is:  Given three very  simple axioms (which seem to be fundamentally part of any physical  theory even remotely consistent with relativity and quantum  mechanics), "if you have free will, then electrons do, too."  This  statement of the theorem is deliberately set up to highlight one set  of philosophical consequences.  A different, more straightforward  statement is:  The result of a QM measurement cannot be computed by  any function of the entire pre-measurement state of the universe.   Informally, the full pre-existing state of the universe does not  determine the result of a quantum measurement.
Conway gave a series of lectures on these results that are available  free from iTunes - look at iTunes U listings for Princeton.  *Well*  worth listening to.  Towards the end, he makes a very interesting and  subtle point:  We've viewed the unpredictability of QM measurements as  matters of randomness.  People always quote Einstein's complaint that  "God doesn't play dice with the universe".  Conway and Kocher's  theorem, however, show that this view is very fundamentally wrong.  If  QM results were randomly determined, then we could play the same game  in our description of the universe that we play with randomizing  Turing machines:  Rather than add randomness to the machine/universe,  simply provide a deterministic machine/universe with access to a pre- computed "set of random coin tosses" that they call on whenever they  need to make a "random" choice.  But if you try this approach with QM,  then Conway and Kocher will argue that the pre-determined tape can now  be considered part of "the complete pre-existing state of the  universe" - and their theorem shows that that cannot be sufficient to  predict the result of a QM measurement!
So QM's indeterminism is subtly different from randomness:  It's an  unpredictable choice that "isn't made until the exact moment of  measurement".  It irreducibly cannot be determined in advance.  Conway  goes on to say that he doesn't *understand* what the distinction  really means - but then he says he doesn't really understand what  randomness means anyway.  If John Conway feels this way, what are we  poor mortals to think?
If you think about the use of randomness in cryptography, what matters  isn't really randomness - it's exactly unpredictability.  This is a  very tough to pin down:  What's unpredictable to me may be predictable  to you, and unpredictability "collapses" as soon as the random value  is "known" ("measured?").  QM unpredictability as described by Conway  seems much closer to the kind of thing you really need to get crypto                                                           -- Jerry

@_date: 2010-09-07 22:22:57
@_author: Jerry Leichter 
@_subject: Randomness, Quantum Mechanics - and Cryptography 
I was talking about mathematical, even philosophical, underpinnings -  not "practical purposes".
In any case, even if you are concerned with practice, the statement  that something is "unpredictable to the attacker" sounds suspect.   After all, most junk cryptographic arguments claim that some algorithm  is "not reversible by the attacker".  One should really expect more.
But there isn't actually such a thing as classical thermodynamical  randomness!  Classical physics is fully deterministic.  Thermodynamics  uses a probabilistic model as a way to deal with situations where the  necessary information is just too difficult to gather.  Classically,  you could in principle measure the positions and momenta of all the  atoms in a cubic liter of air, and then produce completely detailed  analyses of the future behavior of the system.  There would be no  random component at all.  In practice, even classically, you can't  hope to get even a fraction of the necessary information - so you  instead look at aggregate properties and, voila, thermodynamics.   There's no randomness assumption - much less an unpredictability  assumption - for the micro-level quantities.  What you need is some  uniformity assumptions.  If I had access to the full micro details of  that liter of air, your calculations of the macro quantities would be  completely undisturbed.
As a matter of practical engineering, I agree with you.  But read what  you said over again, and distinguish it from typical snake-oil  arguments for novel crypto algorithms.  The differences that make your  claims believable while those of the snake-oil salesmen are not are  subtle and illuminating.  But, as the long argument on this subject  today has shown, that's still not the end of the story.  Just as the  snake-oil systems typically fail because their security claims require  constraints on the attacker (which real attackers will get around),  your claims assume constraints as well.  Lowering the temperature and  injecting RF.  Hmm, hadn't thought of that as an attack technique....
                                                         -- Jerry

@_date: 2010-09-28 13:56:28
@_author: Jerry Leichter 
@_subject: ciphers with keys modifying control flow? 
This reminds me of an old, apparently-abandoned idea for producing one- way hash functions:  Choose two functions f_0 and f_1 that don't  commute.  For input b_0 b_1 ... b_k, the hash is  Obviously, saying the functions "don't commute" isn't enough.  What  you really want is that if you consider the group generated by f_0 and  f_1, then there are no "short" non-trivial relations (or, same thing,  cycles).  Also, of course, you can use more functions - e.g., use four  functions and pull off successive pairs of bits.
A variant uses the input itself as the initial value, rather than 0 -  though that limits the domain.  Alternatively, you could start with  the length of the input, eliminating trivial extension attacks.
This idea goes back to the early 70's at least.  There was really no  theory of how to produce or analyze one-way hash functions in those  days, but this one clearly comes from an approach to designing  encryption functions (alternating substitutions and permutations)  suggested by Shannon in his seminal work on cryptography.  Shannon, in  turn, credits a theorem of Hopf's on mixing functions for the basic  idea - which is ultimately at the root of most encryption functions in  use today.
On its face, this approach has much to recommend it.  It's a pure  stream computation.  You can use any group for your functions.   There's a ton of theory on group presentations that might apply.  (Of  course, many interesting questions about group presentations turn out  to be non-computable; not just any group will work.  Given what we've  learned since the '70's, using two different representatives from a  universal class of hash functions might be an interesting starting  Anyone aware of why, back in the pre-history of one-way functions,  this design approach was abandoned?  Perhaps there really are  fundamental problems with it; or perhaps it's just that the MD-style  approach was so successful that it just took over.
                                                         -- Jerry

@_date: 2013-08-21 00:08:34
@_author: Jerry Leichter 
@_subject: [Cryptography] What is the state of patents on elliptic curve 
As the Wikipedia article  makes clear, the situation is ... unclear.  Certicom has a large number of patents, the most general of which seems to be  which has a priority date of July 29, 1994 and a filing date of Jan 29, 1997 - so has another 3.5 years to go.  It also holds many other patents - the NSA says more than 130 related patents.  But their validity is unclear:  The only lawsuit mentioned (Certicom went after Sony) was settled, but Sony at the time was claiming prior art.  RSA asserts (but says this isn't a legal opinion) that all of Certicom's patents are on particular implementation techniques, and that other techniques can be used to avoid the patents.  There are related patents held by everyone from Cylink to HP to Apple.
With no cases argued all the way through, it's impossible to evaluate the strength or actual breadth of the claims.  The situation is unsettled enough that commercial implementors will generally avoid treading in the area except for the particular curve that NSA licensed and allowed for general use.  RSA seems willing to challenge that position:  As part of its BSAFE library, it provides elliptic curve implementations over a variety of curves, not, it seems, just the one NSA licensed (
                                                        -- Jerry

@_date: 2013-08-25 22:53:57
@_author: Jerry Leichter 
@_subject: [Cryptography] Email and IM are ideal candidates for mix 
[Commenting on just one minor piece]
There's another problem with voice:  People have come to expect services beyond the old point-to-point conversations that the traditional phone network provided.  Group conferences are now very much an expected part of on-line voice services.  These actually require fairly sophisticated processing of the audio to balance levels, avoid or suppress echoes, and so on.  The only implementation techniques available today require a central server with access to cleartext voice streams.  Not only does the server need to be trusted to handle the cleartext voice streams, it has to be trusted to do all the authentication - what comes out of the system doesn't usually match what went in from any one endpoint.
Multi-way chat has similar, if much simpler, problems.
On the rare occasions these problems (or even multi-party video conferencing) get mentioned, someone usually suggests using homomorphic cryptography.  Besides being way too expensive to be practical at the moment, it's not even clear to me that it provides a useful kind of security.  What kind of authentication model could such a system implement?  Without it, what's to prevent a rogue server from inserting its own voice into the conversation?
There are probably a couple of nice PhD dissertations in here....
                                                        -- Jerry

@_date: 2013-08-25 23:32:32
@_author: Jerry Leichter 
@_subject: [Cryptography] Email and IM are ideal candidates for mix 
I agree, and have suggested this as the right "next step" for a couple of years.  (For services like mail, it's the right next step *even without the security considerations*.  At one time, everyone who wanted to run use mail ran his own mail server.  This was a pain to do, and didn't work well in a world of intermittent network connectivity and small disks.  Letting someone else figure out how to keep sendmail working, provide a continuous on-line presence, back up the disks, and so on, was a clear win.
Today, however, pretty much everyone (well, at least in the first world; but the problems elsewhere are of an entirely different nature anyway) has a continuous, immensely fast (relative to the demands of mail) internet connection, disk is "too cheap to meter", machines run of years with no maintenance, and you can back everything up using readily-available tools to encrypted copies in the cloud, or on friend's system.  What's been missing is the ability to configure your local mail server as easily as you set up an email address at Google or Yahoo or at any other provider.  But that's a solvable problem.
On the flip side, mail systems like gMail or Yahoo mail are complex and difficult to run *exactly because they are immense*.  But what are they getting for that size?  There are no economies of scale here - in fact, there are clear *dis*economies.
Even without the recent uproar over email privacy, at some point, someone was going to come up with a product along the following lines:  Buy a cheap, preconfigured box with an absurd amount of space (relative to the "huge" amounts of space, like 10GB, the current services give you); then sign up for a service that provides your MX record and on-line, encrypted backup space for a small monthly fee.  (Presumably free services to do the same would also appear, perhaps from some of the dynamic DNS providers.)  What's the value add of one of the giant providers?
A single-purpose appliance - a box that has exactly two open ports on the Internet, one for SMTP and one for IMAP, with management over a physically separate interface, would have a tiny attack surface and could be very secure.  The more interfaces you put on the box, the less secure it gets.
Maybe you can play games with virtualization - not the kind of virtualization that's used today, with all kinds of hooks for efficient sharing, but virtualization specifically for security, with as little sharing as possible (e.g., completely separate virtual disks; so what if you duplicate stuff, programs and such are tiny relative to disk sizes today).
*The* biggest headache is HTTP support.  Even the simplest modern HTTP server is so complex you can never be reasonably sure it's secure (though, granted, it's simpler than a browser!)  You'd want to stay simple and primitive.
Probably the biggest threat to such a device is a rogue update that installs malware.  You can try to mitigate that risk by requiring that all updates be signed by multiple independent parties who vet the patch, but there are difficult tradeoffs:  Too few checkers, and a rogue patch can get through; too many, and if a severe problem develops, you can't get a patch out quickly.
I think the goal to aim for is no patches!  Keep the device and its interfaces simple enough that you can get a decent formal proof of correctness, along with a ton of careful review and testing (per Don Knuth's comment somewhere to "Be careful of the following code, I've only proved it correct, not tested it") and then *leave it alone*.  If you don't think you can do without patches for the whole thing, maybe you can have a non-patched security kernel, with patches only to portions that cannot break your security guarantees.  (Yes, this is also a hard problem.)
An important element of a secure design is some sort of obliviousness.  A mail server doesn't, on its own, need to look at much in a message:  For the most part, it just stores and forwards bags of bytes.  Where mail servers have gotten into trouble is when they've tried to provide additional services - e.g., virus scanners, which then try to look inside of complex formats like zip files.  This is exactly the kind of thing you want to avoid - another part of the "mission creep" that we tend to see in anything that runs on a general-purpose computer. (The CPU is idle most of the time, why not give it some more work to do?)  That's 20th century thinking:  The computer is expensive, keep it busy.  Twenty first century thinking should be:  The computer is cheap - leave it alone to do its job securely.
Realistically, it will be impossible to get little appliances like this patched on a regular basis - how many people patch their WiFi routers today? - so better to design on the assumption there won't be any patches.
                                                        -- Jerry

@_date: 2013-08-26 10:54:17
@_author: Jerry Leichter 
@_subject: [Cryptography] Email and IM are ideal candidates for mix 
This is my experience as well.
My primary email address is actually served by a small ISP whose spam filter I don't trust - too many false positives.  Actually, I have yet to see a spam filter I *do* trust.  So I've configured my account at the ISP to mark what it thinks is spam in the subject line but then pass it through.  My primary spam filtering is from Mail.app - but I manually check everything in my Junk mailbox before tossing it.  I see every message it thinks is spam, everything my ISP thinks is spam, and everything they think is ham as well.  (Mail.app has no idea what the ISP's "Spam" marking means, but presumably adds it as an element in its own decisions.)
Like Perry's, my email address has been the same for a while (25 years or so, in my case - it was initially delivered via UUCP) and has been widely distributed.
My experience is that Mail.app's junk filtering is rather good, producing a small number of false positives and negatives.  My ISP's filtering is considerably worse.  Reviewing my junk mail is no big deal.
Way back when, I used to get an overwhelming amount of spam.  Looking at it, the cause became clear:  I own lrw.com, and have the only mailbox there.  I had set it up to forward mail sent to any user at lrw.com to me.  I never got anything useful that way - but I got *tons* of spam.  Simply black-holing anything not sent specifically to leichter at lrw.com cut the load *way* down.
Keep in mind that one of the starting points of this discussion was how to implement mail that was proof against PRISM-like bulk monitoring.  That rules out solutions in which a central server has access to the cleartext of your mail to do spam scanning anyway.
If people were willing to send definite spam to a central server, and accept consensus updates to their spam filter in response, there's no reason why the same algorithms that the big guys currently run couldn't be combined with local scanning.  (At least you could safely send examples of spam.  Sending ham is more problematic.  And one could speculate about the kinds of attacks that targeted spam, together with monitoring of when it gets noticed and sent back to the service, could enable.)
                                                        -- Jerry

@_date: 2013-08-26 13:39:36
@_author: Jerry Leichter 
@_subject: [Cryptography] Good private email 
Minor point in an otherwise interesting message:
This (and variants, like a direct proof-of-work requirement) has been proposed time and again in the past.  It's never worked, and it can't work, because the spammers don't use their own identities or infrastructure - they use botnets.  They don't care what it costs (in work or dollars or Bitcoins) to send their message, because they aren't going to pay it - the machine they've taken over is going to pay.
Granted, today most machines don't provide access to Bitcoins.  But assuming your idea catches on, they will.  Once a box has a legitimate capability to send some form of mail, it can be subverted to send mail of that form that the owner of that box didn't intend.  As long as endpoints can be "pwned", nothing about those endpoints can be trusted....
                                                        -- Jerry

@_date: 2013-08-26 15:23:20
@_author: Jerry Leichter 
@_subject: [Cryptography] Good private email 
You're arguing about the security of the wrong component.  The user runs some program that can send mail.  *You* have required that it have the ability to access the user's Bitcoin wallet.  At best, if everything about the wallet is implemented correctly, that just means the spammer has to slip-stream in a bunch of messages along with messages the user is already sending - while the sending is being done, there's a window during with the wallet has to be open, and you can't restrict it *too* much or the interface becomes annoying (how many times do you want to type your passphrase while sending a bunch of replies to different recipients in different domains?).
Keep in mind that individual spammer bot's don't have to send a very high volume of mail; in fact, they don't *want* to as that trips too many alarms in too many places.  They want to look like the person whose machine they have control of - and they want that machine to look the same as it always has to the user. The line between me sending n messages a day, and me sending (say) 3n messages a day, over many "me" instances, is enough to keep the spam masters going - but without a really intrusive interface it's hard to see how you're going to stop that.  If you manage such an interface, the spammers will adjust (as they have many time before) and maybe go after high-volume mailers - who will have to have a high-threshold interface from their mail agent to their Bitcoin wallets, and cannot rely on a user regularly typing a passphrase.
Somewhere or another on the net, there's a document that's intended to be sent in response to someone with a brilliant idea for finally ending spam - showing how what they thought of has not only be thought of before, but was actually tried and didn't work.  I can't seem to find it again, but the last time I read it, I found it quite convincing.  There's no one golden solution to the spam problem; there's just the ongoing, boring, back and forth of attack and defense.  (Actually, relative to a number of years back, spam doesn't seem to be all that bad - see Perry's and my messages on a parallel thread about our own experiences.)  (And if you find a contradiction between my claim that we should be able to build a provably secure system, and this claim that there's no final solution to spam:  The difference between the problems is that "spam or ham" is ultimately a *human* decision which we're trying to model.  Some spam these days is sophisticated enough that even humans aren't sure!  That's by its nature a problem that will never have a completely automated solution - well, maybe not until we can through close-to-human-level AI at it.)
                                                        -- Jerry

@_date: 2013-08-27 21:13:59
@_author: Jerry Leichter 
@_subject: [Cryptography] Implementations, attacks on DHTs, Mix Nets? 
I wonder if much of the work on secure DHT's and such is based on bad assumptions.  A DHT is just a key/value mapping.  There are two reasons to want to distribute such a thing:  To deal with high, distributed load; and because it's too large to store on any one node.  I contend that the second has become a non-problem.  The DHT uses I've seen involve at most a couple of billion small key/value pairs; most involve a few million at most.  Even at the high end, what's today a fairly small, moderately powered system can handle this much data with no problems.  The limitations are on QPS.  However, there are plenty of mundane techniques to deal with that, including replication, deterministic sharding, and caching.  They are all much simpler than DHT's and are hence less likely to have the subtle security problems that DHT's do.
Fundamentally, we're asking DHT's to solve three problems at once:  Distribute a map; be robust in the face of node failure; do it all securely.  Better to use good solutions to the individual problems and combine them than to try to find a way to do all at once.
I worked on data structures somewhat like DHT's back in the late 1970's (to implement the Linda distributed programming language on LAN's and hypercubes and similar networks).  Neat idea at the time, and it was fun to see it come back as a neat idea on a much larger scale years later; but perhaps its time is (again) passing.
                                                        -- Jerry

@_date: 2013-08-27 23:39:51
@_author: Jerry Leichter 
@_subject: [Cryptography] Implementations, attacks on DHTs, Mix Nets? 
And yet DHT's have completely failed at doing this.
Redundancy and validation of updates are issues separable from the implementation of the map and, in particular, from routing.  DHT's try to combine all four and, as we've seen, fail.
Just because it's possible to actually store the contents of a DHT in a single big database doesn't mean you'd actually want to do it that way.  I'm suggesting that you start with the idealization of a single, secure database, then make the modifications needed to actually attain the necessary properties in the face of high distributed QPS, random failures, and a variety of attacks.
Why in the world would you want to put the information for even a million users on such a server.  This would be a server that exists to provide services to at most a few 10's of people - probably fewer.  How many users will they, personally, ever contact it their collective lifetimes?  This is an ideal application for local caching of relevant information from the global database stored "somewhere else".  It might well, transparently, also contain mapping information that its own users received "out of band" and want to use - but have no reason to share globally.
Again, why would individuals want to store that much data?
The DHT model says that millions of Raspberry Pi's and thumb drives together implement this immense database.  But since a DHT, by design, scatters the data around the network at random, *my* thumb drive is full of information that I will never need - all the information *I* need is out there, somewhere - where, based on the research we've been discussing, I have no secure way to get at it.  Why would I buy into such a design?  Doesn't it make much more sense for me to store the information relevant to me?
It's not as if this isn't a design we have that we know works:  DNS.  Yes, DNS, even the "secure" versions, have security issues.  But then so do DHT's, so they are hardly an improvement.  And many of DNS's problems have to do with the assumption of a single hierarchy with, as a result, a small number of "extremely trusted" nodes up at the top.  That's a problem that can be attacked.
                                                        -- Jerry

@_date: 2013-08-27 23:52:23
@_author: Jerry Leichter 
@_subject: [Cryptography] Email and IM are ideal candidates for	mix 
<521CE337.6030706
Actually, it isn't, or shouldn't be.  Email addresses were originally things you typed into a terminal.  They had to be short, memorable, and easy to type.  "Published" meant "printed on paper", which implied typing the thing back in.
But none of that matters much any more.  "Publication" is usually on-line, so contact addresses can be arbitrary links.  When we meet in person, we can exchange large numbers of bits between our smartphones.  Hell, even a business card can easily have a QR code on the back.
Suppose, as in Bitcoin, my email address *is* my public key.  If you wanted to send me email, you'd have a routing problem - but I could even give you hints:  My address would be leichter at lrw.com:.  You can try there first, or you can look up my public key in some global dictionary.  An attacker could get your mail to me to go to them, but they can't read it - you already know my public key, so only *I* can read it.  The only attack they can mount is a denial of service.  I can have any number of public keys, and all published routes to me may go through a mix - so I can minimize metadata leakage.
The assumption that "initial contact information" has to be something human-processable creates the whole "how do I securely map contact information to a key" problem.  Flip it around and that problem vanishes.
                                                        -- Jerry

@_date: 2013-08-28 06:41:27
@_author: Jerry Leichter 
@_subject: [Cryptography] Email and IM are ideal candidates 
<521CE337.6030706
9 or 10 *characters*, not *digits*.  You need enough bits that, even given the birthday paradox, the probability of this occurring is low enough not to matter.  Since the birthday paradox will lead to a 50% probability of collision after about the square root of the number of possible values, given a 10-character signature, that's at about 5 characters.  Way too low, for digits.  If "characters" are full bytes, 2^40 generated public keys is plausible, though perhaps uncomfortably small; and if the "characters" have to be printable - then I agree, way too low.
You could use hash compression, but the retained compressed values will have to be rather larger.  Say 150 bits worth, at least.
On the underlying matter of changing my public key:  *Why* would I have to change it?  It's not, as today, because I've changed my ISP or employer or some other random bit of routing information - presumably it's because my public key has been compromised.  That's a disaster no matter how I identify myself, one that's very difficult to recover from - pretty much impossible unless (a) there's some way to revoke a key (yes, we've had problems with getting that to work even in the current PKI environment, but there's no real alternative); (b) I've prepared for the eventuality.  Given (a), I can send out a signed revocation message.  (So can the attacker, but presumably he had bigger plans for the key than just killing it.)  Given (b), I have pre-shared one or more replacement keys that I still trust, and my revocation can name the one to put into use.  (Of course, it cannot introduce a brand new key!)  Done this way, my response to key compromise is no different from normal key rollover.
                                                        -- Jerry

@_date: 2013-08-28 10:24:43
@_author: Jerry Leichter 
@_subject: [Cryptography] Why human-readable IDs (was Re: Email and IM are 
<521CE337.6030706
The apps to make the transfer easy don't exist, so we still use the old mechanisms.  Think about the absurdity:  You have a high-speed digital connection to someone, and rather than using it to transfer a couple of hundred bits reliably, you encode it ambiguously in an analogue waveform, write it down on a piece of paper, then type that data back in.  Yes, it works - but does that sound like a rational way to do things?
In exactly the same way you trust paper publications that contain today's style of addresses.
And exactly how is this different from "Perry Metzger's email is perry at piermont.com"?
A minority of people have addresses that are easy to remember.  Most - by far the majority - have some random-looking set of letters and digits with some part of their first or last name or a nickname embedded somewhere inside at gmail or yahoo or some institution.  You can say "Well, if everyone has their own server, then they can pick their own name" - but then you end up with non-memorable domain names.
Frankly, I have trouble remembering the last time I got someone's email address by having them tell it to me.  Most addresses come to me these days from LDAP or a similar institutional database; or embedded in a mail message (like one of the ones on this list); or printed somewhere.  Since I got a domain name way back when it was actually possible to get three-letter names, I have an address that's reasonably easy to tell people - so I'll often tell them, after they've rattled off something I'll certainly forget within minutes - "write to me at leichter at lrw.com so I'll have your address".  :-)
X.500 died because everything it was connected to died.  And in the end it never actually got to the point where it solved anyone's problems.
It's perfectly reasonable to have human-name-to-computer-identity maps.  It's certainly something I depend on all the time at a local level:  Mail.app knows tons of addresses I use, and if all else fails I can, and do, search my previous email's to find someone's address.  (That makes for a much more flexible, and useful, person database than any stand-alone database I've seen:  I can search based on anything I can remember about the person, such as what he wrote about, when we last corresponded, who else was involved in the conversation.)  Large institutions have their own internal databases.  But a global database seems rather pointless to me.  There are too many people with similar names.  Try using LinkedIn to find someone who you only know a bit about by name.  Sometimes it works; sometimes you find ten people who *might* be the person you're looking for.
The whole notion of talking securely to someone who you yourself have no way of specifying uniquely is incoherent.  No clever implementation can help.
                                                        -- Jerry

@_date: 2013-08-28 10:33:16
@_author: Jerry Leichter 
@_subject: [Cryptography] Why human-readable IDs (was Re: Email and IM are 
<521CE337.6030706
A different take on the problem:  Would something built around identify-based encryption help here?  It sounds very tempting:  My email address (or any other string - say a bitmap of a picture of me) *is* my public key.  The problem is that it requires a central server that implicitly has access to my private key. There are some proposals around to work around that (e.g., by constructing the key from a combination of keys from different key generators).  But we could go another route:  I can run a key generator on my own hardware.  That doesn't quite solve the problem, since you now need a secure way to find my key generator - any generator will happily tell you how to encrypt using leichter at lrw.com to generate the public key, and *it* will have the corresponding private key.
I don't quite see how to make this work, but IBE seems like a primitive that might be helpful, somehow.
                                                        -- Jerry

@_date: 2013-08-28 10:43:24
@_author: Jerry Leichter 
@_subject: [Cryptography] Why not the DNS? (was Re: Implementations, 
Read what I said:  There's a *design* that works.
I never suggested *using DNS* - either its current physical instantiation, or even necessarily the raw code.  In fact, I pointed out some of the very problems you mention.
What defines the DNS model - and is in contrast to the DHT model - is:
- Two basic classes of participants, those that track potentially large amounts of data and respond to queries and those that simply cache for local use;
- Caching of responses for authoritative-holder-limited amounts of time to avoid re-querying;
- A hierarchical namespace and a corresponding hierarchy of caches.
DNS and DNSSEC as implemented assume a single hierarchy, and they map the hierarchy to authority.  These features are undesirable and should be avoided.
                                                        -- Jerry

@_date: 2013-08-29 06:43:26
@_author: Jerry Leichter 
@_subject: [Cryptography] Email and IM are ideal candidates for mix 
<521CE337.6030706
The point of my question was that for different reasons for changing the public key, there are different issues and different potential responses.
- If I need to change because the private key was compromised, there's nothing I can do about past messages; the question is what I do to minimize the number of new messages that will arrive with a now-known-insecure key.  This was the case I assumed the previous poster was concerned with.
- If I lost the private key, all previous messages remain secure - except they are now, unfortunately, secure against me as well :-(.  New messages sent with the key will be unreadable, but if I am in a position to determine who sent them, I can tell them to re-send with a different key.  If the system is set up so that even return information is encrypted, I'll have to rely on my correspondent's realizing they need to re-send via some other mechanism.  (It could be through whatever revocation mechanism the system has; it could be through mail I send to everyone I correspond with; it could be through a phone call, or just by word of mouth.  The sender will have to check the dates and realize that some message was sent recently enough that I probably couldn't decrypt it.)
- As I outlined things, there was never a reason you couldn't have multiple public keys, and in fact it would be a good idea to make traffic analysis harder.  Adding a new key for "a new facet of your electronic life" is trivial.
                                                        -- Jerry

@_date: 2013-08-29 07:15:59
@_author: Jerry Leichter 
@_subject: [Cryptography] Separating concerns 
<521CE337.6030706
This is a broader problem, actually.  If you've ever had to take care of someone's estate, you'll know that one of the problems is contacting all the banks, other financial institutions, service providers, and other such parties they dealt with in life.  My experience dealing with my father's estate - a fairly simple one - was that having the *paper* statements was the essential starting point.  (Even so, finding his safe deposit box - I had the unlabeled keys - could have been a real pain if my sister didn't remember which bank it was at.)  Had he been getting email statements, just finding his mail accounts - and getting access to them - could have been a major undertaking.  Which is one reason I refuse to sign up for email statements ... just send me the paper, thank you.  (This is getting harder all the time.  I expect to start getting charged for paper statements any time now.)
Today at least, my executor, in principle, work with the mail provider to get access.  But for truly secure mail, my keys presumably die with me, and it's all gone.
You don't even have to consider the ultimate loss situation.  If I'm temporarily disabled and can't provide my keys - how can someone take care of my bills for me?
We can't design a system that can handle every variation and eventuality, but if we're going to design one that we intend to be broadly used, we have to include a way to handle the perfectly predictable, if unpleasant to think about, aspects of day to day life.  Absolute security *creates* new problems as it solves old ones.  There may well be aspects to my life I *don't* want revealed after I'm gone.  But there are many things I *do* want to be easily revealed; my heirs will have enough to do to clean up after me and move on as it is.
So, yes, we have to make sure we have backup mechanisms - as well as key escrow systems, much as the term "key escrow" was tainted by the Clipper experience.
                                                        -- Jerry

@_date: 2013-08-30 07:13:47
@_author: Jerry Leichter 
@_subject: [Cryptography] The Case for Formal Verification 
Many years back, I did some work with Naftaly Minsky at Rutgers on his idea of "law-governed systems".  The idea was that you have an operational system that is wrapped within a "law enforcement" system, such that all operations relevant to the "law" have to go through the enforcer.  Then you write "the law" to specify certain global properties that the implementation must always exhibit, and leave the implementation to do what it likes, knowing that the enforcer will force it to remain within the law.
You can look at this in various ways in modern terms:  As a generalized security kernel, or as the reification of the attack surface of the system.
Minsky's interests were more on the software engineering side, and he and a couple of grad students eventually put together a law-governed software development environment, which could control such things as how modules were allowed to be coupled.  (The work we did together was on an attempt to add a notion of obligations to the law, so that you could not just forbid certain actions, but also require others - e.g., if you receive message M, you must within t seconds send a response; otherwise the law enforcer will send one for you.  I'm not sure where that went after I left Rutgers.)
While we thought this kind of thing would be useful for specifying and proving security properties, we never looked at formal proofs.  (The law of the system was specified in Prolog.  We stuck to a simple subset of the language, which could probably have been handled easily by a prover.  Still, hardly transparent to most programmers!)
                                                        -- Jerry

@_date: 2013-08-30 07:17:08
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA and cryptanalysis 
So the latest Snowden data contains hints that the NSA (a) spends a great deal of money on cracking encrypted Internet traffic; (b) recently made some kind of a cryptanalytic "breakthrough".  What are we to make of this?  (Obviously, this will all be wild speculation unless Snowden leaks more specific information - which wouldn't fit his style, at least as demonstrated so far.)
                                                        -- Jerry

@_date: 2013-08-31 22:01:25
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA and cryptanalysis 
I don't understand this claim.  Shor's work opened up a really hot new area that both CS people and physicists (and others as well) have rapidly jumped into.  There's been a huge amount of publication on quantum computing and, more generally, the field of quantum information.  No one - at least publicly - claims to know how to build a non-toy quantum computer here (the D-wave machine, if it's really doing quantum computation, is a special kind of machine and couldn't run Shor's algorithm, for example).  But there are many reported advances on the physics.  Simultaneously, there's quite a bit of published work on the algorithmic/complexity side as well.
A look at  will readily confirm this.  If you want to dig deeper, there's Scott Aaronson's blog at                                                         -- Jerry

@_date: 2013-12-07 15:37:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Safeplug - TOR for the masses 
The makers of Pogoplug, little devices that let you access storage from your home via the Internet, have come up with a device that provides "TOR for the masses":  A little box that attaches to your router and sends your web traffic through TOR (probably by implementing a proxy - the details are a bit unclear from what I've read so far):
Available today for $49 + $9 shipping.
A Safeplug can be configured to be a full Tor relay node, though that setting is off by default.
Interesting points:
- This is the first attempt I know of to design and sell a way for non-techies to use TOR easily.  Pogoplug's been around for a couple of years, with limited success (though the company has obviously survived), and the makers do seem to know how to build devices that ordinary consumers can use.  It'll be interesting to see whether there actually *is* a mass market for anonymized Web access.
- If they manage to sell a reasonable number of these, and a reasonable fraction of buyers turn on relay support, the size (hence performance) of TOR could increase significantly.
- Conversely, if they succeed, they are going to have a huge bulls-eye painted on their chests.  NSA - and others - will really, really want to find a way to get hooks into these boxes, potentially letting them "own" a large number of relay nodes.
                                                        -- Jerry

@_date: 2013-12-12 16:28:10
@_author: Jerry Leichter 
@_subject: [Cryptography] Size of the PGP userbase? 
(Issues with using S/Mime - or PGP - with Outlook.)
Out of curiosity, I decide to see how this worked in Mail.app, which has built-in support for S/Mime.  Incoming support for signed messages is easy:  When a signed message arrives, the Mac automatically downloads the necessary certificate, installs it in your Keychain, and adds an indication to the mail.  I've never received an encrypted S/Mime message, so I don't know the flows in that case.
As for sending signed mail, it's easy to find an Apple article -  - telling you how to send a signed or encrypted message.  It has a link to an article telling you how to install a certificate in your keychain.
Unfortunately, it gives you no hint about how to actually *get* such a certificate.  Most users would probably get stuck at this point.
For those willing to do a bit of work, a quick Google search for "get mail signing certificate" led me to Comodo, where it was fairly straightforward to create a certificate.  After confirmation, you end up at a page that tells you it's trying to download and install your certificate.  But it just sits there - I don't know if the "and install" part can work on a Mac at all, or whether it only works because I disable "open safe files automatically".  But eventually I figured out that it had downloaded a small .p7c file.  I tried all the recommended ways to add it to Keychain.  From the GUI, nothing seemed to happen.  Using the command line "certtool" utility, I was able to get an error message claiming that the file had "Bad PEM formatting" and an abort.  Except, as I found out much later ... I had, somewhere along the way, already added the certificate.  (A discussion on the Comodo website shows that others have had the same problem for months; no solution was given.)
Since the Comodo certificate seemed not to work, I went back to my search and found CACert.  I again created and download a certificate; this one seemed to install just fine.
Unfortunately, though, Mail doesn't see the certificates.  I tried repeatedly to sign this message - including along the way marking my CACert certificate, and the CACert public CA certificate, as trusted from Email (as the Mac considers it an unknown certificate authority otherwise).  (The Mac already trusts Comodo.) The option to sign just fails to appear.  Of course, this being a Mac, when you use the GUI you get no error messages.  (There's nothing in the system logs either.)  "It just works" or "It just doesn't work" - nothing in between.
Summary:  On the surface, the Mac provides easy-to-use support.  But when you actually try to enable it, it fails in a way that is certainly beyond the ability of most users to fix - and even being quite knowledgable about this stuff, after 1/2 an hour or so of trying, I gave up.
I suspect the Mac implementation works very well when someone sets up certs for end users; just using them is easy.  But for the ordinary user trying to get going on his own, the problems are probably insurmountable.
                                                        -- Jerry

@_date: 2013-12-14 07:35:54
@_author: Jerry Leichter 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
So now you're talking about a hardware modification that detects that an XOR is being done to the output stream of the software implementation of Yarrow, and take special action in that case?  It's not like you can screw with the semantics of every invocation of XOR - that would be noticed right away.
I can believe in the plausibility of such an attack if it were hyper-targeted:  You know the exact code that the chip whose hardware you're spiking will run, down to a characteristic set of instructions that will be run exactly when a random bit stream is to be produced.  Maybe there are cases - e.g., embedded chips going into high-value targets running a specialized set of code that's almost never changed - where this is plausible.  But for a general purpose chip running an OS version that probably didn't even exist when the chip was designed?  I don't think so.
If, on the other hand, you're talking about actively spiking the microcode of a chip whose software you know all about - the RDRAND call is irrelevant:  Just latch on to the point where the output of the internal random number generator is about to be returned and return whatever you like?
                                                        -- Jerry

@_date: 2013-12-17 16:20:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Preimage Attacks on 41-Step SHA-256 and 46-Step 
True - but mind that "if"!
The question that one cannot answer from an abstract of the results - but, at best, from a careful reading of the full work, and perhaps not even then - is whether this is just some little special case or a new technique that, over time, will grow to weaken the algorithm in a significant way.  We've seen attacks of both kinds on other algorithms in the past.
If you look at the best attacks on SHA-1 to date, in and of themselves they don't amount to a significant risk.  What has people worried is that there seems to be a path forward - even if we haven't yet trodden it.
I've become leery of any statements of the form "It's just an insignificant weakness".  The fact is, we really don't understand our cryptographic primitives very well.  That's what *any* unexpected new structure or weakness is telling us.  As a matter of practical engineering, we have to somehow judge  when the risks are mounting to the point where a move - an expensive operation, and one whose cost is ever-growing with the volume of protected data an fielded equipment - is justified.  But the only way we should feel comfortable saying "Oh, it doesn't matter" is if we have some strong indications that, indeed, it doesn't matter - e.g., "yes, this attacks works on k rounds out of n, and theory convincingly shows that it cannot extend past k+1 rounds."
                                                        -- Jerry

@_date: 2013-12-17 17:21:54
@_author: Jerry Leichter 
@_subject: [Cryptography] [IP] 'We cannot trust' Intel and Via's 
A couple of invocations of RDRAND would test whether that attack had been implemented.
In fact, an attack against any particular bit of RDRAND-using code that relied on a constant could be just as easily detected.  To be undetectable, the attack has to mix in a value that (a) cannot be distinguished from a random value by an auditor with realistic resources ("distinguishable given 2^64 outputs" is effectively indistinguishable; (b) is predictable to the attacker.
I've been putting some thought into characterizing attacks against RNG's that would not be readily detectable by a reasonably-resourced opponent who suspect they were there.  I don't think this is possible if all the CPU's used are assumed to be internally identical.  But if an attacker is in a position to provide, say, a unique seed on each chip, and the chip can save state across restarts, the RDRAND instruction can simply use something like AES in counter mode.  Even if the attacker doesn't know what particular chip is being used or how often it's been power-cycled, given a sequence of RDRAND outputs, it need "only" try all known seed values and all "reasonably small" counts of generated values.  An auditor, lacking access to the database of seeds, has to consider all *possible* values for the seed. It does have the Birthday Paradox in its favor - it would look for a collision in about O(sqrt(# seeds)), but the size of the seed space can easily be large enough to defeat this.
To mount such an attack, the attacker has to be in a position to see raw RDRAND outputs (or known/predictable transformations of that output).  For one thing, this suggests that RDRAND should only be accessible to kernel code!  Feeding RDRAND into a Yarrow-like construction would block that.  Actually XOR'ing with the output of the existing RNG is dangerous to the degree that the existing RNG is predictable, but in and of itself doesn't seem to help the attacker.  If, somehow, we have some existing bits from an RNG and code implementing some function F(RNG, RDRAND) that combines them, an attacker who can recognize that RDRAND is being called inside of F can substitute a different function F'(RDRAND) that ignores RNG completely.  For any "normal" instruction, this is readily detectable by testing - just put in different RNG values and make sure that the output changes.  But there's no way to conduct such a test here, because there's no way for an auditor to force, or predict, the RDRAND output.  (A special test mode wouldn't help as the "spike" code would disable itself in that mode, so you could never run the same test in both test and regular mode.)
There is a relatively straightforward defense against all this:  Save a large number of RDRAND output values away somewhere, and only use them much later.  If RDRAND doesn't have access to the code that uses its values, it can only produce its random-looking output stream.  A separate thread continuously filling a 1000 RDRAND-output-sized values, which would then be drawn out to be XOR'ed with the output of some other RNG, would probably be so difficult to suborn that the attackers would move elsewhere (and there are plenty of other plausible attacks).
                                                        -- Jerry

@_date: 2013-12-17 20:15:38
@_author: Jerry Leichter 
@_subject: [Cryptography] DNSNMC deprecates Certificate Authorities and 
...completely missing the point of Ben's paper.  It really comes down to the following:
"So now we have either an existence proof of an efficient solution, or a proof that Bitcoin doesn?t work."
All the criticisms of the "efficient solution" are irrelevant, as the point of the paper was that *no one knows of an efficient solution (to the group consensus problem) and few believe there is one*.  The paper is offered as a proof by contrapositive that "Bitcoin doesn't work" - though you have to read that phrase in context.  What's being denied is that "Bitcoin is a decentralized currency" in the sense described in the paper.
Note the title of the paper!  Or just read the last paragraph of the abstract:
"Both Bitcoin and my alternative proposal suffer from a problem for which there is no known solution: creating consensus in a group with open, changing membership. But at least my proposal fails in an energy efficient way, unlike Bitcoin."
                                                        -- Jerry

@_date: 2013-12-18 21:24:04
@_author: Jerry Leichter 
@_subject: [Cryptography] RSA Key Extraction via Low-Bandwidth Acoustic 
I've only read a very small part of the paper, but ... this isn't true.  In fact, the paper comments that the techniques used to block traditional RF and power attacks make the acoustic attacks *easier*.  (The acoustic attacks, by their nature, operate in a very much lower frequency band than traditional attacks.  A side-effect of the traditional defenses is to tamp down the irrelevant low-frequency stuff while not stopping the low-frequency information they actually need.
They specifically attack a version of PGP which has counter-measures to the traditional attacks in place.  Based on their results, later versions of PGP are immune.
The attack is a chosen-ciphertext attack against RSA that causes the multiplications to hit some repetitive patterns.  It's likely to work, with perhaps some modifications, against any implementation that isn't hardened in specific ways to protect itself.
The paper is 50+ pages long and will take some time to absorb.  But Adi Shamir has come through again.  Where would we be without him?
                                                        -- Jerry

@_date: 2013-12-19 07:42:30
@_author: Jerry Leichter 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
I don't believe the proposed attacks are plausible.  On the other hand, making them much, much harder - to the point where they are clearly far harder than any number of other attacks - is so simple, and has such low costs, I can see no reason *not* to do them.
If you don't like the idea of mixing RDRAND into the pool rather than XOR'ing it at the end, an easy fix is to move the RDRAND instruction to any point before the value that the existing RNG will output has been computed, save the value in memory, then read it and XOR it in at the end.  This will produce exactly the output you are getting today, at the additional cost of one memory read and write, but puts a spiked RDRAND implementation in the position where it cannot determine what value its output will be combined with.  (Well, if you believe the "spike" can extend to keeping track of the memory location where the RDRAND value went and later modifying what a memory->register transfer or an XOR does based on a recent read of that location ... what can I say.)
                                                        -- Jerry

@_date: 2013-12-19 08:08:08
@_author: Jerry Leichter 
@_subject: [Cryptography] [IP] 'We cannot trust' Intel and Via's 
It's worth noting that treating RDRAND specially is not without justification.  RDRAND produces 64 (allegedly good) random bits at a shot, at a very high data rate.  Other sources that feed the mixing algorithm produce a few bits here and there, which have to be mixed and distilled over a period of time.
- If you count the 64 bits as 64 bits of entropy, RDRAND will swamp all the other sources.  If RDRAND is spiked, it could spike the output of the mixer.
- If you count the 64 bits as no entropy, or only a little entropy, and RDRAND is actually good *but the Linux mixer or its other sources are bad*, then the mixer will effectively throw away the value RDRAND could have give you.
Linux mixer XOR RDRAND is strong if *either* of the two inputs is strong (modulo the active attacks we've been discussing, and which can be neutralized).  You can't get that guarantee by making RDRAND "just another input" - it reduces you to relying on the strength of the Linux mixer.
                                                        -- Jerry

@_date: 2013-12-21 12:45:21
@_author: Jerry Leichter 
@_subject: [Cryptography] Size of the PGP userbase? 
[About problems getting signed/encrypted mail to work in Mail.app, concluding:]
Apparently, there's one step you have to take that's mentioned nowhere in the documentation:  After you put your new key in the keychain, you have to restart Mail.app.  I normally let it run indefinitely, and didn't even consider this possibility.  Then yesterday I restarted it for some reason I no longer remember.  Suddenly, the option to send signed or encrypted mail ... just appeared!
                                                        -- Jerry

@_date: 2013-12-20 18:48:49
@_author: Jerry Leichter 
@_subject: [Cryptography] [IP] 'We cannot trust' Intel and Via's 
Off topic, but:
Alpha's were not microcoded.  The standard instructions were all implemented RISC-style.  What the Alpha had was a special privileged "PALcode" mode with complete access to the hardware.  It ran the same instruction set as "normal" code, though it had special instructions to get at model-specific features (hardware control registers, a mechanism to save and restore the non-PALcode state of the machine, etc.)
PALcode managed TLB refill, so it defined how the page tables worked - the hardware itself could only access pages for which there were active TLB entries.  It also implemented process context switching and the flow during interrupt/exception handling - including for "illegal" instructions, so PALcode could implement what looked to the software like instructions.  (This idea, of course, goes way, way back - and, yes, this kind of looks like a microcoded instruction.)  In practice, there was a reserved op code (CALL_PAL - instruction code 0) that was used for PALcode instructions.  Yes, the PALcode implemented four access rings for VMS (and two for OSF/1), but that's a matter of controlling TLB's and deciding what to do with "privileged" PALcode calls:  All the hardware-defined instructions were non-privileged.
OS's didn't, as far as I know, load PALcode.  Rather, the PALcode needed to support a particular OS was loaded before the OS was loaded.  PALcode *could* provide a way for an OS to change the code later, but I don't think any standard variant did.  (Keep in mind that even the OS couldn't touch the perfectly ordinary system memory where the PALcode resided unless the PALcode consented to fill a TLB entry to map that memory - which I very much doubt the standard PALcode would ever do.)
                                                        -- Jerry

@_date: 2013-12-22 00:05:48
@_author: Jerry Leichter 
@_subject: [Cryptography] What do we know? (Was 'We cannot trust' ...) 
Nothing I've seen so far describes what the $10M actually paid for.  So nothing is inconsistent with the possibility that what RSA saw was a $10M contract to provide BSAFE to some government agency - probably *not* NSA - that happened to include the requirement for including the Dual Elliptic Curve RNG and making it the default.  These kinds of things are par for the course in government contracts, and would have raised no questions at the business level - or likely even among the technical people.  After all, ECC was the hot new thing, and this RNG *was* in the NIST standard (or would be shortly).
Of course, it's also possible that RSA knew quite a bit about what NSA was up to.  I think that's highly unlikely just on general principles - why would NSA tell them, and failing that, how would RSA come to find out?  Still, RSA is going to have a hell of a time repairing their reputation.  NSA's actions of the last couple of decades are starting to cause all kinds of collateral damage.
                                                        -- Jerry

@_date: 2013-12-22 08:25:18
@_author: Jerry Leichter 
@_subject: [Cryptography] Why don't we protect passwords properly? 
Ah, yes, *that* old fallacy.  Based on a complete misunderstanding of people and of regulation.
Regulation defines rules; governments enforce rules.  You don't expect the players in a soccer match to ref the game - or write the rulebook.  There are others who are much better at those things than the players, even though the players are those who know most about actually playing the game.  None of those people maintain the field either.
If the only goal people have is maximizing their income - how do you explain that von Mises, who claimed to understand economics and markets so well, spent his career as a government bureaucrat and a professor?  Do as I say, not as I do?
Indeed, the same issues apply to cryptography.  The best cryptographers aren't necessarily particularly good at developing software.  The best developers often do crappy UI's.  The best cryptographers, developers, and UI designers aren't generally very good at writing solid standards.  The best standards writers aren't very good at the political/marketing game of getting those standards written, accepted, implemented, and actually used.  Yes, there are rare individuals who can play more than one of these roles at a high level, but there's only so much time in a day and time spent filling one of them detracts from time spent filling another.
Hell, even at the level of cryptographers, there's specialization.  Adi Shamir is likely our best cryptanalyst/code breaker, but what new cryptosystems has he developed since RSA?
                                                        -- Jerry

@_date: 2013-12-22 11:26:30
@_author: Jerry Leichter 
@_subject: [Cryptography] Why don't we protect passwords properly? 
There's an old joke:  How could God create the entire universe in only six days?  No installed base!
It's also *so* easy to criticize yesterday's decisions based on today's knowledge and capabilities.
KDF's used specifically to strengthen weak user passwords against brute-force attack were introduced as a quick way to strengthen *existing* systems with immense numbers of already-fielded clients.  Requiring that the clients all change would be a perfect way to make sure nothing got improved.
Keep in mind that the *original* purpose of "key hashing" was rather different:  It was to keep passwords secure even when the password file (or logical equivalent) was leaked.  Password hashing was introduced in early versions of Unix with exactly this primary purpose - and the secondary purpose of allowing anyone to verify a password by reading the public value from the file, hashing an incoming alleged password, and comparing.  It took a number of years to learn that making even the hashed passwords public was a bad idea - and every Unix system in decades has had shadow password files.  Web servers and such simply followed (often badly) existing "best practice":  Unix password hashing was based on DES; they moved to more recent crypto standards.  Of course, then they (the Web server community as a whole) screwed up the entire ecosystem by implementing such poor security that they repeatedly managed to lose huge databases full of millions of hashed passwords.
The ability to do mass off-line attacks wasn't anticipated.  Its success was based on the combination of several things that would have been hard to predict:  Clever fast implementations of some of the primitives optimized for the purpose of password cracking rather than ordinary cryptography - e.g., doing encryption in bit-wise parallel; the ability to utilize large numbers of machines for password cracking purpose, whether on a service like AWS or through botnets of ever growing size; the wide deployment of graphics cards, which it turned out could be used to implement cracking algorithms very quickly - and of course the availability of all those databases of encrypted passwords, some of which didn't even bother to use salts, which was best practice by the late 1970's.
All this didn't come together until the last 2-3 years or so.  There simply were no plausible, much less *demonstrated*, large scale attacks against encrypted passwords until not long before that.  bcrypt was first published in 1999 specifically to protect against rainbow tables - a neat theoretical attack that may never have actually been used in practice anywhere.  (The newer brute force techniques are much more practical than rainbow tables in today's environment; if you hear someone talking about using rainbow tables today, it probably means they don't know what they are talking about.)  scrypt is 10 years more recent - the paper defining it was only published in 2009, and a (since-expired) draft RFC dates to 2012.  Most cryptographers would probably be leery of scrypt:  It simply hasn't been around long enough, and survived enough concerted attacks by those who are really good at such attacks, to form a good judgement as to its strength.  Just because all publicly known implementations are slow doesn't mean there aren't fast ones waiting to be found.
Using a strong key hashing function (I hesitate to say KDF because KDF's have other purposes as well) on the server as a replacement for its existing hash function is a quick fix that doesn't require any changes to clients and minimal changes to servers (of which there are quite a few as well).  There are no other measures that could be applied in anything less than years.  (And, in practice, even this substitution is moving rather slowly.)
It's also worth pointing out that strong hashing on the client is a non-starter for another reason:  Not all clients are fast workstations.  In fact, increasing they are mobile phones and tablets, which have limited computing capability and, more to the point, are heavily power constrained.  I'm not aware of any measurements, but I'd guess that bcrypt - and especially scrypt, which by design hits memory very hard - use a *lot* of power.
That makes no sense.  If you transmit the derived key, *it* becomes the "real" key.  The server has to know it - either all the time (if service providers decide that since clients are doing the hashing they don't need to bother) or, as now, every time you log in.  The only difference is that if you re-use a password across sites, currently you may be handing server A your password to server B.  To which the answer, as always, is "don't do that".  If you're assuming significant changes on the client side, you would do better "assuming in" an already-well-developed technology:  Password managers, which make it trivial to generate different passwords for every site.
Alternatively, if you're assuming the whole system changes, PAKE schemes like EKE(1992!) or SPF(RFC2945, 2000!) - are better anyway.
TrueCrypt was released in 2004, well before the style of attack you're so concerned about was known.  Hey, guess what:  TrueCrypt is open source.  If you think you can improve it by adding a better "password stretcher" - by all means do so.  You can even do it without touching the main TrueCrypt code by adding a wrapper around the code that reads the password.  There are no magic "guys in charge" to tell you not to do it.
Someone is wrong on the Internet!  Where will it end?
And a new conspiracy theory!
a)  So do it!  I'll bet writing your email took longer than that.
b)  It's been shown repeatedly that adding options - especially options that people won't even understand - doesn't help.  Almost everyone will choose the default.  You have to make the default "right".  In the world of crypto, "right" today almost always becomes "wrong" a couple of years down the road as attacks keep getting better.
                                                        -- Jerry

@_date: 2013-12-22 12:05:24
@_author: Jerry Leichter 
@_subject: [Cryptography] Decentralized, global, irreversible, 
Clever but remarkably naive about how the business world actually works.
Some personal assets of high value - real estate and cars are the common example - are only owned through titles.  A title is really just a published document, accessible through (in the US; there are analogous systems elsewhere but I know nothing about how they are implemented) a state- (cars) or locally-(real-estate) provided repository, that witnesses to everyone who wishes to check who actually owns the property in question.  To transfer ownership of such property, you need to "transfer the title":  Publish to the repository, using approved procedures, that the title has now been transferred to someone new.
For some other kinds of assets there are non-governmental systems that provide the same role.  Artwork is worth very little unless you can "establish provenance":  Show a chain of ownership going back to the original artist.  Private organizations (art dealers, auction houses) authenticate these chains of transfer.
What keeps a business from going to five different banks and pledging the same factory equipment as collateral on five different loans?  Every state in the US provides for "UCC filings" - a formal way for a lender to declare that some piece of property has been pledged as collateral.  (UCC is the "Universal Commercial Code" - a set of standardized laws for commercial transactions that in one form or another have been accepted by most of the states.  The ones that haven't accepted the UCC have very similar mechanisms in place.  Without this uniformity, inter-state business would be very cumbersome.  There are, in fact, even some international standards - e.g., everyone in the shipping business world-wide agrees on what "FOB" means.)  Any lender will "check the UCC's" before proceeding with a loan on business equipment.  (This isn't needed for real estate or cars because the loans are recorded on the title instead.)
So what Szabo proposed isn't new; it's implementing an idea that goes back almost 1000 years (see the Domesday Book) "with a computer" "on the Internet" (like all those horrible business process patents).  The old mechanisms relied on centralized, trusted third parties (usually local governments) to maintain the list of transactions - which were always public.  The proposal is to remove the trust and the third parties by using cryptography.
FWIW, it's not clear how good an idea this is.  Right now, the groups maintaining the lists are closely tied to the ones enforcing the contracts implied by those lists.  If you continue to rely on the courts to enforce the contracts, it's not clear you gain much from cryptography.  On the other hand, if you move in the direction of "self help" - as in proposals that cars be automatically disabled if you don't pay your loan ... well, we've already seen moves in that direction, and they've proved to be highly contentious:  Easily abused, providing little recourse in case of error.  A car that your bank can shut down remotely when it thinks you haven't paid is a car with DRM.  Do you *really* want that?
                                                        -- Jerry

@_date: 2013-12-22 10:57:13
@_author: Jerry Leichter 
@_subject: [Cryptography] [IP] 'We cannot trust' Intel and Via's 
The most widely published example of an attack against hardware was against the open-source SPARC chip design.  No microcode, very simple layout - and a it only took a few extra gates to suborn it completely.  The attack is somewhat different, and not have *field modifiable* microcode means the attack needs to be implanted early, but as far as vulnerability to hardware attacks in general, there's not much of a difference.
MIPS is still out there, though mainly in embedded systems.  For that matter, even SPARC still sees some use.  And as far as I know ARM implementations don't use microcode, but I could be wrong.  And PowerPC, while not a traditional RISC, is also designed to be directly implementable in hardware.
In fact, other the x86 and the old IBM 360-descendent mainframes, all the surviving architectures seem to be fairly RISC-like.
Yes, the design of the Alpha would have made a "trusted boot" option easy to fit  in.  But then it hasn't been all *that* hard to fit into other environments either.
The Alpha died before interest in a trusted boot environment really developed.  Sad - it was a nice design.
                                                        -- Jerry

@_date: 2013-12-22 17:55:25
@_author: Jerry Leichter 
@_subject: [Cryptography] RSA is dead. 
Have a look at some of the entries in the Obfuscated V contest (to write innocent-looking code that actually cheated one of the candidates).  My favorite is  - just one of many.
Come back and tell me how "capable developers" will easily find malicious code hidden in simple, clean-looking C code.
                                                        -- Jerry

@_date: 2013-12-23 07:40:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Fwd: [IP] RSA Response to Media Claims Regarding 
Oh, for God's sake, let it go.
We're talking about 2004.  Were you following the news about RSA in 2004?  In enough detail to have spotted one press blurb out of many?  Would you remember one blurb from 10 years ago?
I'm not sure about the exact timing, but EMC - RSA's parent - acquired the company I was working at in 2004 (SMARTS).  I would have had a particular interest in EMC-related stories - and, given a long-standing interest in crypto, in EMC and RSA related stories.  I have no memory of PR around any such contract.  Doesn't mean it didn't happen, but the fact that neither of us remember it means precisely nothing.
In any case, we have one story from one source asserting, in very general terms, that some kind of contract existed.  No one else has confirmed it.
I'm actually willing to believe that the NSA would have done this, but I doubt it would have been done in the way you seem to think.  All it would take is for any government agency to come to RSA and say "Hey, we have $10M in our budget to buy security stuff this year.  Our security experts tell use your stuff is the best."  And then later:  "Our security guys say we really need to have that Dual EC RNG thingie.  It's going to be in the new NIST standard, you know.  Oh, you guys already implemented it?  Great!"  [NSA seems to have tipped RSA off that Dual EC DRNG was coming; RSA would have been all too happy to get out ahead of the curve, no pun intended.  No big deal, it wasn't really a secret, and NSA may well have given the same "heads up" to the few other commercial crypto vendors as well.]  And then finally, when RSA can just *feel* that money filling a big hole in a sales target:  "Oh, our security guys tell use the new RNG needs to be the default.  Safer that way."  And the trivial change is made.
Of course, the government agency's "security guys" either are the NSA, or are being advised by the NSA.  That's one of NSA's roles:  They advise the rest of the government on cryptography.  No one would anyone question them doing their job.
Requirements for specific approved algorithms, and specific default configurations, are standard practice in government contracts; if you want to sell to the USG, you sell on their terms.
The indirect approach would have been easy for NSA to pull off, would have come out of someone else's budget (sure, it's only $10M, but any bureaucrat who can find a way to get *someone else* to spend it so that he can keep it for his own projects will be delighted) and would leave no NSA fingerprints. Even the NSA guys advising the other parts of the government probably wouldn't know *why* Dual EC DRNG was now on the "recommended" list - someone else would maintain the list.  No one outside of NSA would have to know anything about NSA interests, goals, and methods - something NSA would find much more desirable than letting their interest be known and then have to buy silence.
I'll believe NSA pulled RSA into a conspiracy when I see *much* stronger evidence than we've seen so far.
But there will be plenty of people of the "where there's smoke there's fire" persuasion, who will now avoid RSA.  NSA has managed to badly damage the reputation of RSA.  (Well, considering their fiasco with RSA access tokens not so long ago, maybe their reputation was already tarnished.)  I'm guessing we'll see more stories and rumors in the future - now that "everyone knows" RSA was infiltrated by NSA, should we trust any EMC product?  After all, RSA is EMC's "security division" - they advise the rest of the company. This is the collateral damage that flows from the kinds of games NSA has been playing.  There will be more.
(BTW, I've been out of EMC for many years now.  Only ended up there through an acquisition; never liked the place, would have no reason to defend them.)
                                                        -- Jerry
PS   The stuff about RSA advising the rest of EMC about security is true.  The SMARTS stuff had its own crypto - I and guys working for me developed it.  We initially looked at available crypto code - BSAFE was one thing we looked at - but it was either too expensive, or came with open source licensing terms we couldn't live with.  After the acquisition we kept hearing complaints from RSA security guys that we should be using the approved corporate stuff.  As long as I stayed in charge of that software, I resisted - we had better things to do than to re-architect our security code - but I hear that, long after I left, all the stuff we developed was ripped out and replaced.  I won't make any strong claims about our stuff - it could have been attacked, though you'd have to know what you were doing - but for various reasons we didn't represent a particularly high value target.  It was probably good enough for the role it played, and I remain proud of the way we managed to slip a fairly good level of security in a backwards-compatible into an existing product, selling to customers who, for the most part, didn't think security was important and didn't want it "getting in their way".

@_date: 2013-12-23 11:13:21
@_author: Jerry Leichter 
@_subject: [Cryptography] What do we know? (Was 'We cannot trust' ...) 
Well ... yes, that's the way commercial software works.  Everyone buys the same thing.  The government is certainly able to demand special configurations for itself, but those come with increased costs.  That's where you get the famous $700 toilet seats.  In this century, many of them want "COTS" - Commercial Off The Shelf.
And that, of course, is what makes the NSA game here effective.
No, here's ten million in licensing fees - just make this one tiny change to keep our security auditors happy.
Do you know the history of auto-ranging power supplies?  Prior to the late 1970's or so, power supplies were analogue beasts and typically came in 120 and 240 volt AC versions, with different attached power cords, for the US and much of Europe.  Carrying two different versions of the same thing was expensive.  As power supplies became more sophisticated, adding a switch to change the voltage became common - especially after the IEC standardize on an equipment-end plug.  So now you could produce one product for the whole world, just shipping a different local-plug-to-IEC-jack cable, and assuming that the end user would set the switch correctly.  Sounds great - but what's the default setting for the switch?  At least in the computer industry - dominated by US manufacturers at the time - it was clearly 120V.  Then local governments elsewhere - I think the Germans were the first - started to say "No, you don't get to set the default the way *you* like it; you sell in Germany, you configure your device *for Germany* out of the box."  I was at Digital at the time, and it was an open joke that devices were built and configured in the US, packed in boxes, shipped to Germany - where someone opened the box, changed the switch to 240V, resealed the box, and sent it to the customer.  While cheaper than maintaining two separate product lines, it was an expense and a pain.
What eventually happened was that digital, auto-ranging power supplies took over, and unless you're working with high-power stuff, you just plug it in and everything works.  One product for the whole world, no configuration required.
That's how governments - and, BTW, all big customers - work.  Do it my way, for the convenience of my users, or get lost.
In 2004?
The issues were raised a couple of years later, and I think it's legitimate to ask whether RSA should have changed things then.  But changing a default based on what a couple of academics are saying - which is how it would have been seen - is not an easy move.  Academics are *always* complaining about something - that's what they're *supposed* to do, raise questions.  Until Snowdonia, there was certainly a smell, but there was nothing that approached solid evidence.
In any case, as others have pointed out here:  Until Snowdonia, the general attitude of  big business - the customers for BSAFE - would have been "I don't care that the NSA can read my stuff, they're the good guys, they don't get involved in commerce, I have nothing to hide from them."
                                                        -- Jerry

@_date: 2013-12-23 11:20:58
@_author: Jerry Leichter 
@_subject: [Cryptography] [IP] 'We cannot trust' Intel and Via's 
I hadn't thought about it in those terms, but I think it's a solid bet that someone's working on that.  Chip design is already very heavily dependent on rule checking and all kinds of analysis.  And everyone is trying to reverse-engineer everyone else's designs.  All the underpinnings are there.  And various parts of the US military and security establishment are quite aware - have, in fact, talked publicly about - the problem of "spiked" chips making it into their supply chains.
Yet another arms race.
                                                        -- Jerry

@_date: 2013-12-25 09:35:34
@_author: Jerry Leichter 
@_subject: [Cryptography] Why don't we protect passwords properly? 
but far afield from what this mailing list is about.  I won't continue it here, but I'll give my opinion on the meta-issue:  Economics these days can be divided into three (sometimes overlapping) subfields:  Macro, micro, and behavioral.  From what I've seen, the latter two are predictive sciences.  Macroeconomics is not.  At best, it's a descriptive science, like, say, biological taxonomy before Darwin.  The very fact that there are Austrian and Keynesian and who-knows-who-else "schools" of macroeconomics - "schools" that disagree on the basics of goals, terminology, measurements, legitimate forms of analysis, the basic interpretation of events on which enormous amounts of data have been gathered - tell me "there's no there there".  Von Mises wrote beautiful, incisive, logical analyses.  But then so did Keynes.  Hell, so did Marx.  So did Aristotle, on topics we now consider the domain of other sciences.  The fact that Aristotle could reason well didn't make his physics correct.
There's nothing wrong with descriptive sciences.  Somehow you need to gather the data and try out various classifications before you can produce good predictions.  What's wrong is claiming that you have a predictive theory and then consistently making excuses for why your ability to predict is no better than chance - which is what I see *all* the "schools" of macroeconomics doing.  What's even worse is taking the next step and insisting that existing practice, and existing results, are wrong because your reasoning shows them to be so.  All the reasoning of all the macroeconomists in the world gathered together is worth less than a single working factory.
Thanks, but I'll take my "mixed economy", and my "mixed macroeconomic principles", and the resulting "mixed results", rather than any of the "true religions".  Look at where the "true religion" of Marxism led the world....
                                                        -- Jerry

@_date: 2013-12-25 10:05:01
@_author: Jerry Leichter 
@_subject: [Cryptography] Why don't we protect passwords properly? 
This discussion is wandering, and it's wandering for fundamental reason.
Back when RSA was first developed, and then differential cryptanalysis finally showed us an objective, mathematical approach to determining the strength of combinatorial ciphers ... *we knew what the problem was*.  Everything was about mathematics and reductions and building out of simple primitives.
Over the decades since, all the certainty has gone away.  The math is fine, but it doesn't describe the real world.  Mathematically strong primitives fail when used in inappropriate protocols.  Appropriate protocols are subject to side-channel attacks.  Even the underlying hardware is your enemy.
We're now into an even deeper quagmire.  Increasingly, our defenses have really significant tradeoffs.  Just in the last few weeks, we learned that some techniques adopted in one major implementation of RSA to block high-frequency side channels actually made the implementation more vulnerable to low-frequence acoustic analysis.  Passwords are increasingly vulnerable, but none of the proposed solutions has a good story for the "shopping on-line in my PJ's at 3:00 AM because I can't sleep" problem.  As just recently described here, scrypt may make off-line attacks slow at the expense of making side-channel attacks against on-line use easier.
Meanwhile, even the math we thought we had down pat has proved to be elusive. MD5, SHA-1, RC4 - failing primitives.  BEAST and related attacks - failing protocols.  We can no longer assume we can "get it right" forever - we have to design on the assumption that anything we build today will, within the fielded lifetimes of our systems, fail and need to be replaced.  Somehow.
It's now (and has, really, been for a while) a big-ass engineering problem.  And as I used to tell my OS classes, engineering is all about tradeoffs.  And tradeoffs are always made by someone, with a particular stake in one or another of the things being traded off.
So I expect to see many more discussions about security wandering, as we're no longer certain about what security means.  Yes, worthwhile security debates start with a definition of the attacks to be defended against; or, even better, of the risks and costs associated with different attacks and defenses.  But given the huge spectrum of entirely different classes of risks, and the very different likelihoods and costs different people will assign to them ... to accept agreement on what are, at base, the *goals* is increasingly folly.
                                                        -- Jerry

@_date: 2013-12-25 12:26:13
@_author: Jerry Leichter 
@_subject: [Cryptography] Serious paranoia... 
You know, I've been a subscriber to this mailing list since ... let's see now ... early June of 2008; at least that's the earliest message I can find.  I've stuck to it because of the high level of expertise of the participants - and the general tone and quality of the contributions.  We've had serious discussions of the issues, not personal attacks, name-calling and paranoia.
I haven't seen much in the way of technically competent analysis from you since your first posting about a hardware RNG, and you're certainly lowering the level of discourse here.  Either up your game or get out.  The Internet has plenty of places to rant.
(Just my personal opinion; I'm not the list's moderator.)
                                                        -- Jerry

@_date: 2013-12-25 13:22:16
@_author: Jerry Leichter 
@_subject: [Cryptography] RSA is dead. 
Interesting.  That's roughly in the range a professional proof reader would estimate for properly reviewing a page of technical text.
For any real software, it's also fantastically expensive, even if you work at minimum wage - and I'm sure you're not even close.  :-)  Way out of the range that open source projects could fund.
I agree with your later posting that someone who wants to use open source code in a security-essential application may be willing to fund someone with your background to do a thorough audit.  But that's a tiny fraction of the world of open source.  If you find problems - which you almost certainly will - presumably your client will fix them.  If they have the proper motivations, they'll even contribute the fixes back.  But what are the odds that anyone other than your client will ever run code with exactly your fixes?  The next version of that software will have your changes (maybe; or maybe someone on the project will change the changes for whatever reason) and a whole bunch of others besides.
I think OSS is great.  I use it all the time.  But to contrast two similar products, Linux with some recent graphics environment and MacOS with its graphics environment ... I don't see a hell of a lot of difference in terms of code quality and security.  (I see a lot of difference in MacOS's favor on consistency and usability, but that's a entirely different story.)  Neither MacOS nor Linux plus graphics is ever going to get a full code audit.  Maybe the Linux kernel will; maybe the OSS parts of the MacOS kernel will.  But I'm not putting any great faith in these, because first it's highly unlikely I'll ever run exactly the audited versions of those kernels, and second there's still going to be tons of unaudited software in any real system.
Summary:  Careful code auditing by those skilled in the practice can probably catch almost all attempts to insert back doors in software.  But the costs are so high that unless something changes to drastically lower them, only a tiny fraction of code will ever be properly audited.  This is true whether you consider OSS or closed-source code.  (In fact, one could argue that closed-source code is *more* likely to have been audited because someone who has tight control over the source is more likely to see a justification for auditing it, and making sure it *stays* audited.  I doubt that anyone has gathered the numbers needed to (dis)confirm this hypothesis.)  Maybe contributions of time by skilled auditors will help OSS - but there aren't that many skilled auditors, or hours in the day that they can possibly volunteer, to make much of a dent in the OSS code that's used widely at any moment in time.
                                                        -- Jerry

@_date: 2013-12-26 00:06:02
@_author: Jerry Leichter 
@_subject: [Cryptography] On Security Architecture, The Panopticon, 
I think "detecting that encryption is being performed" is an unlikely attack. Let's assume we're talking just AES.  There are so many implementations out there, with some many little tweaks, unless you're targeting a particular implementations, you'd pretty much have to do an input-output analysis to know it was AES you were looking at.  And that assumes you can recognize the inputs and outputs.
And suppose hardware *did* recognize an AES encryption under way.  Then what.  AES is a deterministic operation - you can't modify the output without it being easily detectable - and it's not like the hardware can tell that someone is checking its output, as you can do that easily off-chip.  (In fact, unless the attacker consistently modifies the implementations at both ends of the link, a modification of the output will just cause the data to fail to decrypt - a pretty good indication that something is wrong!)  So what you'd need to do is leak the key.  (You could leak the cleartext, but that's orders of magnitude more data.)  But how is a CPU chip supposed to do that?  Extra data coming out of the CPU is going to get detected.
Interestingly, the same arguments apply to hardware AES accelerators.  The fundamental difference here from hardware RNG's is that every operation is deterministic and has results that can be readily verified - and by their nature, are effectively *being* verified during normal operation.  This channels plausible attacks in one direction:  Leaking keys.  And it's not easy to come up with a good way to do that undetectably.  (At least *I* haven't come up with a mechanism.  The best I can think of on contemporary CPU's is for the system management subsystem to use its private access to the Ethernet to sneak out some extra packets.  But how long can you do this without someone noticing?  There are all kinds of normal operations that look closely at network traffic.  If you have an idea for a better attack, I'd like to know about it.)
Now, there are certainly special cases where attacks are possible.  For example, if you can arrange to have regular physical access to the machine, you can store the grabbed keys in some kind of non-volatile removable storage.  But if you can do that, there are plenty of much simpler, cheaper, low-tech attacks you can mount.
And that, I think, is the case for hardware attacks on encryption code in general:  Given the level of access and effort they would require, they would always be dominated by other attacks.  Attacks that do things like notice a particular bit pattern in a received Ethernet packet and jump into the code in the rest of the packet while in a privileged state have actually been built and shown to work.  Much more flexible, gains you all the access you want, and extremely difficult to detect.  If you can load code into a system - *after* determining exactly what it's running - any crypto has no chance.
                                                        -- Jerry
PS  I once proposed (never actually implemented) a version of what you describe to solve a different problem:  Embedding a key in a program to which an adversary had access in such a way as to make extracting the key as challenging as possible.  I happened to have handy code I'd written for a compiler course I taught long before:  A simulator for a CPU I'd designed (a simple if slightly odd RISC that was just a nice target for a compiler) and a somewhat-optimizing Modula-2 compiler.  (Students were asked to extend the incomplete compiler I gave them.)  So what I was going to do was expand out the entire key schedule and write out all the steps of the encryption in Modula-2, compile it, and then run that under the simulator - probably piecewise, with other operations interspersed.  Yes, a sufficiently patient attacker could figure it out eventually, but it would be a royal pain to do so.

@_date: 2013-12-26 14:23:31
@_author: Jerry Leichter 
@_subject: [Cryptography] Serious paranoia... 
Just for accuracy:
Neither of those were my statements.
                                                        -- Jerry

@_date: 2013-12-26 14:27:23
@_author: Jerry Leichter 
@_subject: [Cryptography] Can we move this list to an online forum please? 
Ah, finally something related to the topic of this list!
I actually addresses this issue a couple of weeks back as a hypothetical.  So let's think about it:  Just what *would* a "more secure" version of this discussion (ignoring the actual technology) look like?  Keep in mind that, by design, anyone can join by sending a simple request to the moderator.  They'll promptly receive copies of all messages.  Given this, what's your threat model?
                                                        -- Jerry

@_date: 2013-12-26 14:33:47
@_author: Jerry Leichter 
@_subject: [Cryptography] Fwd: [IP] RSA Response to Media Claims Regarding 
Having been in a position to hear about some of this near the time in happened: Getting RSA to actually make sustainable amounts of money was a challenge.  It's another example of the difficulty of selling security products.  People see the costs, but it's very hard to prove the benefits.
Things have changed a bit, but it's *still* a tough sell.  Best quote on the subject from a big buyer's point of view:  I don't want to buy security products.  I want to buy *secure* products.
And he was absolutely right.  (Unfortunately, the reality is that the customer wants secure products - at no additional cost, financial or otherwise.  And a pony.)
                                                        -- Jerry

@_date: 2013-12-26 14:50:52
@_author: Jerry Leichter 
@_subject: [Cryptography] how reliably do audits spot backdoors? 
The solution to this problem in large parts of the commercial world these days is the virtual appliance.  Why screw around getting the user's copy of Windows or Linux configured just as you need it?  Give him a VM image all properly configured and have him just boot it.
MacOS has done this in a weak way through its application bundles for years.  Mac applications never have dependencies you have to resolve - what you see as foo.app is actually a (hidden, unless you choose to unhide it) directory within which live the executable and all the libraries and config files and basic data - e.g., message files in multiple languages.  Sure, you end up with tons of copies of some common pre-requisite libraries, probably in multiple versions; the Sparkle automatic updating framework probably lives in half the third party application bundles on a Mac user's disk.  But who cares?  Relative to code sizes, disk space is free these days, and the developer then knows you're running against exactly the libraries he built against.
For a consumer OS, this is "good enough" isolation - and a hell of a lot better than the DLL hell of Windows or any of the Linux package managers.  It's not, of course, nearly as complete a solution as the virtual appliance.
Virtual appliances *typically* come as closed-source solutions, but there's no reason you couldn't package up the sources, and even the full build environment, inside the appliance.
                                                        -- Jerry

@_date: 2013-12-27 06:12:46
@_author: Jerry Leichter 
@_subject: [Cryptography] A modification to scrypt to reduce side channel 
OK, so now we've moved from abstraction to a concrete proposal.
And just who would use such a KDF?  Tying up 4GB for a second is a very expensive proposition on a server.  People have to manage thousands of logins a second, so you're talking about devoting Terabytes of main memory - not disk or SSD - *just to logins*.
You've suggested doing the KDF computation on the client.  How many clients have 4GB of free memory?  I've got a laptop with 8GB of memory.  WHen in active use, it never has even 2GB free.  Maybe my laptop can do the computation - but it will take a while because it'll have to swap stuff out.  (And of course then they'll have to swap it back in.)  I see this happen periodically when I've got a bit too much stuff running, and it ain't pretty.  Hardly any user would be willing to accept the performance loss.
As for portable devices - I'm not sure any of the actually *have* 4GB of RAM in total.  And the power costs of pegging the CPU for a second are non-trivial, too.  So basically you're writing them all off.
The parameters you've suggested basically limit secure communication to someone with the NSA's resources.  :-)
                                                        -- Jerry

@_date: 2013-12-27 06:46:54
@_author: Jerry Leichter 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
Well, I have a copy of every message I've received since I joined this list.  On multiple machines, backed up multiple times, in fact.  Deleting them all would be extremely difficult, even if I wanted to.
Related to this, I've been conducting an inadvertent experiment on this list for the last week or so.  As part of an experiment (which I described) to determine how hard it was to enable S/MIME in Apple's Mail.app, I got a client cert from Comodo and installed it on one of two laptops I use on a regular basis.  It turned out not to do anything ... until I had reason to restart Mail.app.  Ever since then, it's been signing my outgoing mail - including mail I send to this list.  So about half my recent mail is signed - and half isn't.  Have any of you noticed?  Have you ascribed any different significance to signed vs. unsigned messages?
A valid distinction - but an *individual* distinction, not a *list* distinction.  I clearly use a pseudonym while we can all know who "ianG" is.  :-)
This doesn't seem like a good attack mechanism, at least not against *this* list:  The moderation is very light, and usually accompanied by a personal or public message explaining why a message is being dropped.  Any significant change - as in trying arbitrarily drop all messages with a particular theme - would be noticed.
There are other lists where such an attack might work better, but on such lists it probably wouldn't be as effective.
I suppose you could say that much Chinese censorship of the Web is this attack "in the (very) large".  But of course that censorship's not at all a secret.
Nice.  Beyond that, we also want to know that *the same* messages got to everyone.  Members could periodically publish a hash representing all the contents they've seen.  But:  For a mailing list and most other mechanisms, you can't require that they were received in the same order, much less at the same time.  Because of retries if nothing else, you can't even require they were *sent* in exactly the same order, at the same time.  And yet there are situations where playing around with the order of messages might constitute a useful attack.  It would be interesting to formalize some checkable bounds on how much variation is allowed.  Note that acceptable variation in order makes it harder to define an appropriate checksum - as, for mail, does legitimate variation in Received lines, other header information, and perhaps even content:  MTA's have been known to play various games with what the consider the irrelevant formatting of mail (I'm looking at you, Exchange - though you're certainly not alone).
                                                        -- Jerry

@_date: 2013-12-27 11:19:08
@_author: Jerry Leichter 
@_subject: [Cryptography] On Security Architecture, The Panopticon, 
Hardware these days is *extremely* reliable.  (Those of us who've been around long enough remember such things as the recorded message you'd call before going down to the computer center - back when there were such things - to check on whether the machine was running; otherwise, why waste your time.)
If one in 65K packets were undecodeable, someone would notice very quickly.  Some users don't monitor the quality of their network connections, but many do. There's no way this could masquerade as bit errors on the underlying connection:  The undetected (by lower-level hardware/code) error rate is in the 1 in billions range.
Sure, you can play with the numbers of Spencer's attack.  But you can't get around a hard tradeoff the attacker faces:  If he damages more than a very tiny fraction of packets, he'll be noticed by someone, and fairly quickly.  But that fraction is so tiny that the chances of picking up anything "juicy" makes the attack pointless.
                                                        -- Jerry

@_date: 2013-12-27 14:37:16
@_author: Jerry Leichter 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
You're both right.  :-)
It's great in theory to list every possible attack and analyze risks later, but then you're going to fill the list with things like "the attacker figures out how to infiltrate the brain of the reader and plant a virus that weeks later makes him go insane and kill everyone around him".
On the other hand, you don't want to narrow things too quickly.  While I posed the question concerning this particular list, I was using it as an example.  There are other public or semi-public lists where discussion may have some higher probability of influencing important decisions or actions, and where some kinds of manipulation might actually have a payoff to the attacker.  Discussion of investments are probably a good example - they are a prime target for a variety of fraudsters.
                                                        -- Jerry

@_date: 2013-12-28 01:05:12
@_author: Jerry Leichter 
@_subject: [Cryptography] On Security Architecture, The Panopticon, 
[I assume this was on the topic of a hardware AES implementation - or a hardware hack that recognized software AES - leaking key bits.]
Maybe there are circumstances in which this could be done, but I don't see it as likely in most.  The encryption is just too far removed from the network transactions.  Suppose you had control of the AES implementation that fed data into a TCP socket.  Could you really produce the kinds of variations in TCP timing that someone could detect?  If the encryption was right at the Ethernet packet level, sure, you could easily slip data into the inter-packet timing information.  But doing it "looking through" the whole TCP stack?  I don't know.
Now, if you want to consider about *multiple* hacks into the hardware - one to grab the key from AES, the other to leak the grabbed information through manipulation at the packet level - you might have something.  In for a penny, in for a pound.
                                                        -- Jerry

@_date: 2013-12-28 07:27:19
@_author: Jerry Leichter 
@_subject: [Cryptography] On Security Architecture, The Panopticon, 
I don't understand what you're suggesting.
Let's review where we are.  The opening question here was:  Should we be concerned about an attack in which the hardware recognizes that an AES encryption is being done?  My suggestion was that that recognizing when this was happening would be a hard problem, probably an insoluble one except in specialized circumstances.  But let's grant it and go a step further:  Assume an hardware assist for AES, so that the hardware doesn't even have to solve that problem.  I then suggested that we are then at a next level of problem:  What should the hardware *do*?  I suggested the only thing it really *could* do was exfiltrate the captured key.  Which led us here.
What I'll suggest at this point is that the "exfiltrate some information undetectably" problem is difficult.  Yes, it can probably be solved by an attacker - but given a system on which the attacker has solved this problem, the game is over.  There's not much point in looking at fancy ways to pick up more information - the system's already fully compromised.  For example, it was long ago pointed out (I forget by who) that a search of memory for "high-entropy" 128-bit blocks works pretty well for finding stored keys.  Much easier than playing with the details of AES, and it finds keys even when they aren't in active use.
                                                        -- Jerry

@_date: 2013-12-28 07:33:41
@_author: Jerry Leichter 
@_subject: [Cryptography] What do we know? (Was 'We cannot trust' ...) 
It really depends.  The government can ignore price constraints whenever it wants.  And certain companies that make a living at the government trough sell nothing but government-specialty stuff.  But there's also an immense amount of "ordinary" stuff the government buys under COTS regulations.  There are huge numbers of perfectly ordinary PC's and laptops spread throughout government offices that were bought from the same distributors any large business might go to, at the same prices any large business might pay.
                                                        -- Jerry

@_date: 2013-12-28 09:44:00
@_author: Jerry Leichter 
@_subject: [Cryptography] On Security Architecture, The Panopticon, 
But this is really not news.  We've known about timing channels for decades.  We've also known that they are almost impossible to close, and sometimes even to detect.  The only practical strategy is to limit their bandwidth, which can be done.  The old work had to do with leakage channels within a single host, but we really shouldn't be surprised that it applies across hosts on a network as well.
What *is* new is along an entirely different dimension.  It used to be that you could say "OK, I've gotten the maximum timing channel to one bit/second, getting even a small document out will take forever."  Well, that same channel leaks an AES key in a bit over two minutes - even as you yourself send all your data, encrypted with that key, to an on-line backup site at speeds that are many, many orders of magnitude higher.
This wasn't considered in the old analyses, and unfortunately it really shreds them.  And it's not just on networks:  Even the old same-host attacks are more interesting again out on shared-tenant cloud hosts.
Attack and defense - the eternal battle.
                                                        -- Jerry

@_date: 2013-12-28 14:21:54
@_author: Jerry Leichter 
@_subject: [Cryptography] how reliably do audits spot backdoors? 
It really depends on what you're doing.  Java only *appears* to do it all for you; while you can't get the traditional memory leak (memory to which no accessible pointers exist), you can easily build up piles of guck that's pointed to by hash table entries you forgot to clean up, for example.  And the GC doesn't help you with non-memory resources.  This is where destructors, if used carefully, can really pay off in C++:  You can guarantee (well, almost; there are some holes, but generally if you fall into them it's because you insisted on using some low-level features that are almost never needed and should be left to those who fully understand the implications) the complete cradle-to-grave semantics of types you define.  You can't do that in Java - you're left telling users they *must* call close() before they stop using function, which they forget to do - unless they remember to use a try/finally, which has an annoying scope definition requiring yet more restructuring of the code.  The new "try-with-resources" syntax *finally* improved the story here - it's new to Java 7 - but it still has its limitations.)
I've moved back and forth between Java and C++ over the last couple of years, and I find myself cursing whichever one I'm currently using and looking back fondly at the one I'm not.  :-(  (Though in fact I find much less of the "looking back fondly" at Java.)
PHP notoriously has *incomprehensible* semantics, and sometimes it seems that every other line of PHP code is a security hole.
This is a pattern I've seen in Java as well.  With one Java project I worked on, it seemed as if the (long-departed) developers understood the notion of abstraction by introducing a class, but were completely unfamiliar with the notion of abstraction by introducing a function.  So you had pages and pages of more-or-less identical bits of code, copied rather than made into a helper function.  Fixing a bug meant changing the same damn bit of code over and over again.  And then there were the 30 identical-looking functions - except that one of them did something just slightly different.  Man, can that leave you spinning your wheels trying to figure out what's going on.
I maintained on rather large Perl program for a while.  I found Perl to be a bearable language if I simply ignored many of its "features".  I treated it as C with a nice set of string processing primitives, pretty much.  I'm sure a Perl expert would have been horrified by my code - but it worked, and over the years, as I and one other guy with a similar bent cleaned it up and reorganized it, it actually became reasonably understandable and maintainable.
Still, every time I finished doing some work on that code, I felt as if I should go wash my hands carefully.  :-)
The thing that's remarkable is that people didn't expect this.  It's been true of every macro processing language since the first ones were developed.
Lisp macros are fundamentally different in kind.  They are, in general, more powerful; but there are certain kinds of things they just can't do.  (In particular, C++ templates have access to a pretty powerful static type system, whereas Lisp has no static type system for them to have access to.  So the areas of overlap between the kinds of things Lisp macros are used for and the kinds of things C++ templates are used for is fairly small.  Since Lisp has essentially no syntax, its macros mainly focus on providing one.  C++ has enough syntax to fill anyone's needs, so templates are more about semantics.  An approximation of the reality, but not that far off as a general statement.)
                                                        -- Jerry

@_date: 2013-12-28 16:16:38
@_author: Jerry Leichter 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
Except that there is a line at synchronous vs. asynchronous communication that divides mechanisms with fundamentally different characteristics.  Synchronous communication can have perfect forward security; asynchronous communications cannot.
This division bothers me.  It seems to me there's something missing in our descriptions so that we fail to capture the nature of this distinction.  It feels as if there should be a continuum here, where you get full PFS for communications with an arbitrarily short lifetime, degenerating into the usual more limited guarantees for things that are stored long term.  And I suppose you could come up with a simple theory along that line, where you need to retain keying material only as long as some message isn't delivered.  But this seems very forced and unnatural.  I think we're missing something.
                                                        -- Jerry

@_date: 2013-12-28 18:57:34
@_author: Jerry Leichter 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
Thanks for the hint.  For those interested, have a look at (which I've only glanced at so far).
                                                        -- Jerry

@_date: 2013-12-28 23:18:32
@_author: Jerry Leichter 
@_subject: [Cryptography] how reliably do audits spot backdoors? 
Minor correction:
Untrue.  Java has only signed integral types, always 2's complement, but it has short (16 bits), int (32 bits), and long (64 bits).  (While both int's and long's are common - with int being used in some places, like hash values, that arguably should be long's today but remain int's because machines had 32 bit words when Java was defined, I don't recall ever seeing a short in a piece of Java code.)
You say that like it's a bad thing.  :-)  Seriously, it's a completely consistent definition, and if one is concerned about the numerics, it's easy to detect overflows and such.  What's good about it is that it's definitely and completely specified.  You know exactly what to expect.
The story is more complicated for floats, where whether you're operating in strictfp mode or not has a major effect on semantics (and performance).  (You would think that strictfp mode was there for numerical codes, but in fact for most numerical codes, it kills performance without actually solving any interesting problems.  It's basically there for backwards compatibility with very old versions of Java, which had *only* the equivalent of fpstrict mode.)
C (and C++) have always defined unsigned arithmetic as arithmetic mod 2^n, but signed arithmetic used to be almost completely a "quality of implementation" issue.  It's been a while since I looked, but I believe the recent standards insist on a binary representation that's either 1's- or 2's-complement.  So much for you decimal machines!  (Don't laugh:  The first computer I ever used as an IBM 1620 - which *was* a decimal machine!)
Yes, but it can't have the full semantics of the built-in integer types (in particular, conversions are not promotions and vice versa).
                                                        -- Jerry

@_date: 2013-12-30 14:26:23
@_author: Jerry Leichter 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
I think you meant Bret Hartman.  He was CTO at RSA from 2007 until he left (for Cisco) in 2010.  Long after the decisions on Dual EC RNG; long before Snowdonia.  So I'm not sure which of his recent statements you have in mind.
BTW, Bret is not a crypto-type, just as RSA is not (or hasn't been for many years) a crypto company.  He's an enterprise security guy.  (He "wrote the book" on web services security - e.g.,  - having been involved in the development of some of the XML-related security standards.  When I knew him - late 2000's - he'd come to regret the way much of that work ended up - bloated and extremely heavy-weight.)
                                                        -- Jerry

@_date: 2013-12-30 15:35:01
@_author: Jerry Leichter 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
After some more thinking about this:  Yes, the difference is fundamental; but the terms "synchronous" and "asynchronous" are almost incidental.
What's really different is whether there needs to be a round-trip communication before new data can be transmitted.  Consider basic DH, which is at the root of the PFS algorithms on the table.  (I'm using PFS as an acronym for Protocol-based (key) Forward Security.  :-) )  Alice, as the initiator, cannot send Bob any data before receiving Bob's initial response, since she only knows her exponent, not Bob's, hence doesn't know what key to encrypt with.  This is why chat with OTR can't be easily converted to "email with OTR", unless you're willing to go back to pre-SMTP, pre-queued messaging.  (Which, BTW, is not as absurd an idea as it appears.  The Internet isn't what it was back when SMTP was designed.  Most hosts are up most of the time; most connections are live most of the time.  Mobile hosts excepted - and they have other issues.)
When you look at it this way, the magic properties of PFS seem much less magic. If I'm communicating with you, we could establish a key, then after each message replace the key by its one-way hash and discard the previous key.  Compromise of an endpoint doesn't allow the decryption of anything sent earlier.  The receiver knows how to decrypt the next message that will arrive; the sender knows how to encrypt the next message to be send (but doesn't know how to decrypt any of its own previous messages).  The lockstep nature of OTR disappears:  A sender can pipeline a whole bunch of messages, each with its own key, without waiting for anything from the receiver.  It's kind of neat, actually, that in this scheme the sender can have an arbitrary number of encrypted buffers sitting in its own memory, but seizing it *does an attacker no good at all*:  The sender may have created those buffers, but it has no way to decrypt them!  Sure, having seized the sender, the attacker can send new data that the receiver will accept - but that's impossible to avoid under the assumption the "seizing the server" means grabbing all of the state that drives it.  (However, assuming the sender is using an authenticated mode, the attacker can't send any new information without first sending all the pending buffers.  If he sends nothing, the receiver will be out of sync on the key and nothing the attacker sends will authenticate.  If he sends garbage, it, too, will fail to authenticate.  Either way, the receiver will get suspicious.  He'd better hope that the sender didn't realize he was about to be captured and send an "under attack, ignore all after this" message as his last gasp!)
OTR-like protocols have an advantage if the attacker can clone all the state in one of the participants, but can't actually seize them (so they go on communicating as if nothing happened).  With OTR, the seized copy allows the decryption of data only up to the next DH exchange - which requires a round trip.
                                                        -- Jerry

@_date: 2013-12-30 22:37:14
@_author: Jerry Leichter 
@_subject: [Cryptography] On Security Architecture, The Panopticon, 
Compare BitCoins and gold.
In theory (and in practice over quite long periods - on the order of decades), gold is a remarkably steady unit of value.  Similar items purchased at times far away from each other have about the same price in units of gold.  So gold isn't an investment - it's a store of value.
At least that's what classical economics says.  In practice ... over the last three years, the price of gold in dollars has dropped over 30%.  The purchasing power of the dollar in real world terms - i.e., the dollar inflation rate - comes to about 6 percent over that interval, and it's been pretty steady.  So gold - the "hardest" currency - in practice ends up being more volatile than the dollar - the "fiat" currency!  (The price of gold fell 1.6% just today - compared to a dollar inflation rate over the *entire last year* of just 1.2%.  As far as I know, nothing particularly special occurred in the gold markets today.)
People like to compare BitCoin to gold.  Well ... BitCoin may have extremely high volatility, but that actually makes it more like gold and less like dollars!  (And it makes it clear that the price of both is strongly affected by things that have nothing to do with what they can buy.  For gold, those things tend to wash out over long periods of time.  We don't know what will happen to BitCoin over comparable periods of time as it hasn't been around long enough to observe how the market actually treats it over long periods.)
                                                        -- Jerry

@_date: 2013-12-31 07:48:54
@_author: Jerry Leichter 
@_subject: [Cryptography] how reliably do audits spot backdoors? 
These specs are "better" in the sense that they require specific behavior rather than giving implementations choices.  That's good from the point of view of program analysis and proving, but there are two other things to consider:
1.  The effect on program performance.  Java originally specified the exact behavior of FP arithmetic.  Any two implementations of the original Java specs were guaranteed to get the bit-for-bit identical results in any FP computation  The behavior exactly matched the SPARC implementation of FP, but to get the same results on the x86's of the day required immense overhead (you pretty much had to save each intermediate result to memory and then read it back into a register).  Ironically, the Java-specified results were *less* precise than the x86 could achieve.  (This isn't new:  There was a patch to the microcode of some model of VAX - I think the 750, the second VAX built - which *reduced the precision of certain FP instructions* to match the architectural definitions.)
In the case of Java, the costs were so high - and the payoff, as far as those who used FP seriously, i.e., the experienced numerical analysts, so non-existent - that the Java spec was changed to *allow* more variation.
2.  Whether the fully defined behavior is a *good* behavior from a human point of view.  Most programmers, most of the time, think of computer integers as models for mathematical integers.  Signed arithmetic with at least 32 bits is a good match for this intuition except for very large or very small values, so people rarely get in trouble with it.  Unsigned arithmetic, on the other hand, fails to match for values near 0, exactly where many common computations "live".  Such an "obvious" fact as that
is false when u is 0 - which makes it all too easy to write non-terminating loops - or at least loops that won't terminate until x wraps all the way around.  (Been there, done that, sigh.)
Also, the C idea that unsigned is a *type*, rather than unsigned operations being alternative operations on the bit patterns of a pre-existing signed type, while nice in many ways, combines badly with default arithmetic promotions.  You have to consider what happens when signed and unsigned values meet "across an operator".  This used to be ambiguous.  It no longer is - the spec is now explicit that "unsigned wins" - but it's still all too easy to write expressions whose meaning is not at all what a human being would expect from a quick glance.
C++ actually enshrines the problem in its libraries.  Lengths of strings are unsigned, but string::npos - the constant used to represent "not a legal string offset" - is -1.  Kind of - it's really the bit pattern for -1 converted into an unsigned.  People do pretty much understand that you must not use "< 0" to test whether you got a a string::npos - and compilers these days will typically warn you if they see an u < 0, telling you it's always false - did you really mean that?  But that's only part of the story.
If you ask for the first (or last) position at which a string occurs within another string, you get an offset (unsigned, since it could be as large as one less than the length of the string) into the string, or string::npos if it doesn't occur at all.  So to check that the first occurrence of s1 precedes the first occurrence of s2, if any, you can do:
But if you try:
to ask the inverted question about the *last* occurrence - does the last occurrence of s1 occur after the last occurrence of s2, if any - you get the wrong answer when s2 doesn't occur in the string.  (You can construct many similar examples, and I've caught such failures in code reviews.)
Google C++ standards actually forbid the use of unsigned values for anything other than bit masks or similar purposes - it's just too tricky for ordinary humans to get consistently right, and the original purpose - to double the range of counts and object sizes and such when int was 16 bits - is long obsolete.  No pun intended :-)
                                                        -- Jerry

@_date: 2013-07-04 15:34:35
@_author: Jerry Leichter 
@_subject: [Cryptography] crypto breakage in SALT 
Well, one does wonder about an RSA *primitive* that allows an exponent of 1.  If that's the tooling you're working atop, it's hard to imagine you're going to produce anything decent.
                                                        -- Jerry

@_date: 2013-07-08 06:16:59
@_author: Jerry Leichter 
@_subject: [Cryptography] dead man switch [was: Re: Snowden "fabricated 
A false dichotomy.
If there were an actual physical robot, it could be "interfered with" even more easily than a lawyer.
The point of the open source implementation is that it serves as a proof of context:  It shows something that could have any number of physical manifestations in unknown locations, and any one of them would be an effective dead-man switch.
However, the lawyer's instructions serve the same role:  Since *this* lawyer has instructions that would lead to release, there could be others with exactly the same instructions.
Software - and instructions to lawyers - on their own don't do anything.  They have to be physically instantiated in the appropriate medium to affect the world.  That's always the hard part to pull off in an adversarial environment.
                                                        -- Jerry

@_date: 2013-11-01 10:17:11
@_author: Jerry Leichter 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
These are good points.  I would, however, suggest an alternate axis of differentiation:  Targeted vs. bulk attacks.  In practice, the two axes are highly correlated:  Even the NSA can't afford to pay $10^4 per attack if it needs to attack 10^7 targets.  However, for a high-value target, paying $10^6 would likely be seen as cheap.
An interesting point along this line:  The easiest machine to target is one that's just been imaged.  But it's generally extremely difficult to know which brand new machines will prove to be valuable in the future!  So attacks on newly-imaged machines will have to have fairly low costs to be worth doing.
Note, however, the "generally".  If, for example, you have a subnet reserved for your "most secure, highest value" machines, booting a new machine onto that network is telegraphing its future value, thus making attacking it worth much more.
An excellent point - but be careful of the terminology here.  In careful terminology, a random value must be unpredictable to any opponent, while a nonce simply needs to never be repeated within a given cryptographic context.  Using a nonce where a random value is needed makes an otherwise-secure protocol insecure.  Using a random number where all that's needed is a nonce wastes what is, in some contexts, a precious commodity.  Also, nonces computed in a deterministic, documented way cannot be used as subliminal channels - and you can check that they follow the rules.  There's no way, even in principle, to check that a random value isn't being used as a subliminal channel.
                                                        -- Jerry

@_date: 2013-11-01 10:21:14
@_author: Jerry Leichter 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
Ahem.  This is *exactly* the kind of reasoning I started this thread to investigate.  (Though I certainly agree that a *single* DHCP packet containing a random bit string is easily attacked.)
                                                        -- Jerry

@_date: 2013-11-02 15:49:39
@_author: Jerry Leichter 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
That's not the NSA - it's Microsoft.  This pattern has been reported befor; it's the result of Microsoft searching for "evil" URL's (those that have drive-by malware, mainly, though I suppose they look for other stuff, too).
See  for one discussion.
(That's not to say NSA doesn't *also* do this, though with Microsoft already doing it, they would get more bang for the buck by just getting hold of the Microsoft databases.)
                                                        -- Jerry

@_date: 2013-11-02 23:16:25
@_author: Jerry Leichter 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
If an evil URL appears in a Skype message, the intent is almost certainly to get someone to go to that URL.  That's where the potential danger is.  Of course, for the recipient of the original URL, it's likely way too late.  This is a way to build up a database of URL's to warn people against - the *hope* is that large numbers of others will be protected.
I have no data one way or another whether this pays off, and I'm neutral on whether it's appropriate.  Scanning of email for malware of various sorts is nothing new and is generally thought to be a good idea.  I know of no argument why scanning of chats should be any different.  If it makes clear that the emails and chats are actually accessible to your provider ... well, knowing about it doesn't make it worse that having it happen without you knowing about it.  (Unless you believe in the NSA's idea that if you don't know your privacy has been invaded, it hasn't been.)
                                                        -- Jerry

@_date: 2013-11-03 18:09:08
@_author: Jerry Leichter 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
How many bits in a cupful of entropy?
                                                        -- Jerry

@_date: 2013-11-05 07:18:58
@_author: Jerry Leichter 
@_subject: [Cryptography] randomness +- entropy 
Well, yes, and in its technical definitions - either in thermodynamics where it arose or information theory where it was imported because of a similarity in formalisms - it plays virtually no role in cryptographic discussion.  In cryptography, especially when discussing random number generators, the word has a loosy-goosy meaning that's somehow tied to lack of predictability - but in an unspecified way.  When people make it a regular habit to "guess" the entropy of various processes and then go on to build systems based on guesses, you know the word has lost any formal meaning it ever had.
While the distinctions you draw are valid and important, I'm afraid "entropy" no longer has the capability to distinguish them.
I don't know what this means.
I don't know what *this* means either.  I drew a distinction in an earlier post - a distinction you can find in many papers on cryptographic primitives - between random values (unpredictable to the attackers being considered) and nonces (never repeated in a given cryptographic context, but there are no assumptions about unpredictability).  Where a random n-bit value is specified, I can think of no paper that does not assume what we call "n bits of entropy" - though a better way to say it is "a value chosen uniformly at random from the set of n-bit strings".  Sometimes the value is supposed to be chosen uniformly at random from some other set - e.g., from Z/p, i.e., between 0 and p-1.  Trying to state this in terms of entropy is a losing game - and in fact it isn't actually trivial, given a random *bit* source, to produce such a distribution.  People have gotten it wrong in the past.  (The obvious technique of choosing a k with 2^k > p, choosing a "random" k-bit value, and then reducing it mod p, if described in terms of "entropy", looks fine - I have k bits of entropy, which is more than enough to cover the choice I need to make.  But the output is biased.)
"Unicity distance" is a nice but different concept - and one with little bearing on cryptography today.  (If I draw a cipher E at random from some a collection of functions - e.g., by choosing a key - then the unicity distance is the unicity distance is just the number of pairs (x, E(x)) that specify E uniquely within the set.  I suppose you can stretch this to talk about how many samples from the "random" generator are needed to specify *it* uniquely - i.e., be able to determine all past and future states - but I've seen the term used that way.)  A broader - hence more useful - classical term (which may have been introduced by Shannon) is "equivocation".  I don't recall the formal definition, but it attempts to capture the uncertainty an attacker has about the next plaintext, given all the information he already has.  If I know a message was encrypted using a one-time pad, my uncertainty about the first character is not absolute - it's much more likely to be "t" than "z".  To provide information-theoretic security, a one-time-pad must contain "enough randomness" to keep my a priori and a postiori equivocation equal.  This *is* something that can be given in terms of the entropies of the message and one-time-pad sources, but the deracinated notion of "entropy" one sees in most discussions is way too weak to say anything useful here.)
A BBS generator is "indistinguishable" from a true random number generator.  What's missing from that statement - and from the distinction you're drawing above - is a specification of the attack model.
The BBS generator has inherent limitations that are given in its proof of correctness:  The attacker has polynomially bounded resources (in fact, "attacker" literally means a polynomially-bounded probabilistic TM), which in turn implies that it only has access to a polynomially-bounded number of outputs.  These are generally fine.  The proof doesn't bother to state (though it's implicit) the obvious:  That the attacker doesn't have access to the internal state of the generator.  This isn't an "entropy" assumption - the internal state must, to the attacker, appear to have been chosen uniformly at random from all possible internal states, and is not available to the attacker.  If you want to use "entropy-talk", all the entropy in a BBS generator is there, at the start, in the choice of the initial state.  And yet there's a profound difference between the output of a BBS generator and the output of, say, a linear congruential PRNG starting with exactly the same state.  One is predictable given a few successive samples; the other is secure given any polynomially-bounded number of them.  "Entropy" simply cannot capture this distinction.  And it's in fact exactly the distinction - in the transition from Shannon's classic notions of information-based security to modern computability-based ones - that make it possible to say anything useful at all about encryption algorithms other than one-time-pads.
While we have no proofs like those for BBS for PRNG's built on practical cryptographic primitives, we generally assume that they have similar properties.  (More correctly, we can prove some properties given assumptions about others.  But those are the same assumptions we make about the actual encryption and hashing and signature algorithms we use.  If they fail, the whole system falls down regardless of the source of "random" values.)
To summarize:  The distinction between cryptographic PRNG's and "true" RNG's has to do with the attack model.  The attacker considered in the former is polynomially bounded (OK) and doesn't receive as input some special piece that we label "internal state" (this one can be violated).  The attacker against a "true" RNG is ... what?  Formalizing that is equivalent to formalizing randomness - which no one has managed to do.  (In fact, modern developments of probability theory don't even try - random distributions are simply among the axioms.  Even more, if you believe John Conway, the "Free Will" Theorem he and he and Simon Kochen proved show that "quantum unpredictability" is *stronger* than randomness!)  In practical cryptography, this translates into "whatever I can imagine".  In my other thread on plausible attacks, I tried to focus on limiting the attacker to, well, plausible operations in a particular real-world setting.  If you *don't* do that, you can make no progress at all.  A hardware generator - even Turbid - is vulnerable if I include as plausible an attacker who's infiltrated every supplier of electronic components in the world and has slipped a small transmitter into everything built, including every diode, resistor - hell, maybe every piece of wire!
I have no problem with designing strong, practical random number generators.  I'd love to see them deployed more widely.  But clever designs of low-level primitives, as important as they are, are not a substitute for *system* designs; in fact, they can blind us to the necessary system properties.  If a cryptographic primitive needs some unpredictable input, we need to go further and ask (a) how much? (b) unpredictable under what attack model?  Only then can we begin to answer the *system* design question of "what's a suitable generator for such inputs?"  If we can, for a reasonable cost (measured in any appropriate way), get our hands on a generator whose attack model assumes way more attacker resources than attacks on multiple uses of those values - great, let's make use of it.
Too much discussion of "random number generators" is the equivalent of "I'm not sure AES is strong enough, so I'll do a ROT-13 encoding first - it can't hurt".  And it can't - until you run into Tony Hoare's comment to the effect that "You can make a system so simple that you can see at a glance that it's correct, or so complex that you can't see at a glance that it's *not* correct."
                                                        -- Jerry

@_date: 2013-11-05 10:22:22
@_author: Jerry Leichter 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
So you think it's all about *you*?  If a mechanism doesn't protect *you*, *right now*, it's not a security mechanism?  Some security mechanisms aim to protect the population at large, not any particular individual at a particular point in time.
The point of checking URL's to see if they are "bad" is to protect those who look at them *after* they are found to be bad.  Properly checking URL's takes significant time.  You wouldn't want the check inserted in real-time into the Skype message stream.
I have never seen a URL in a Skype message removed or in any way marked as invalid; but then I'm not a heavy Skype user.  But really, a bad URL in a Skype message is not in and of itself a problem - it's only a problem when someone goes to that site in a browser.  I don't use IE and don't know where it gets a list of "dangerous" URL's from - but I would be surprised if it doesn't.  (Chrome, Firefox, and Safari all use a list that Google maintains.)
Not everything is the NSA.  It's clear they want to operate in the dark, not being noticed.  It seems highly unlikely they would start hitting random web sites a short time after they got mentioned in a Skype message.  If they were doing this themselves, I'd expect them to be patient, making it very difficult to correlate on random hit on the site with any particular action.
On the other hand, this is a fine source of URL's to feed into a malware detector.
Could the NSA piggy-back on Microsoft's data capture?  Sure.  But they have so many ways to get hold of URL's, I'm not sure why they they would bother.
                                                        -- Jerry

@_date: 2013-11-05 12:38:20
@_author: Jerry Leichter 
@_subject: [Cryptography] randomness +- entropy 
However *you* may be using the word, its use in most discussions *surrounding cryptography* is not particularly technical.
An ahistorical view.  When Shannon introduced the term, it was by analogy.  In the 1930's and '40's, no one thought about physics in information terms.  Hell, no one thought about "information" as something formalizable.  That was Shannon's genius.  But even he didn't realize that his notion of information entropy would prove to have deep and profound connections to physical entropy.  That wasn't really recognized until 20 or more years later.
Agreed.  But you're the one who started on what I think is a fool's errand of trying to get the word used "correctly" in the random number generator community.
Here's a simple example - which I alluded to earlier - that demonstrates why this is much more difficult than it seems.  Suppose I have a source of true random bits.
a)  I want to draw a number uniformly at random from the set {0, 1}.  "Entropy" tells me that 1 bit from the source is enough.
b)  I want to draw a number uniformly at random from the set {0, 1, 2, 3}.  Same argument - two bits "of entropy" are exactly what we need.
c)  I want to draw a number uniformly at random from the set {0, 1, 2}.  Clearly "I need less entropy" than in case (b) - I'm *delivering* less entropy.  But ... you can't actually do this with two random bits.  About the best you can do is:  Take two bits of entropy.  If the result is 00, 01, or 10, you're done; otherwise draw another two bits.  Repeat until done.  The expectation value for the number of bits required is *at least* (3/4)*2 + (1/4)*4 = 2.5 bits; really, it's more.  (There are other correct ways - and many, many *incorrect* ways - to make this "random" choice, but none of them can use fewer bits than this.)
Naive arguments about entropy will lead you astray.  Naive entropy estimates will lead you astray.  If you want to do the math, do the math - don't guess.
I know perfectly well how to do computations with entropy.  That's exactly why I'm disturbed by the way the term gets misused.
I would put it all differently.  The two are barely comparable.
- A TRNG draws from a random distribution, a concept that is mathematically an axiom.  We believe (with very good reason) that a TRNG provides an appropriate model for the operation of some physical processes; and, flipping this around, we believe we can realize, in the physical world, something with properties akin to the mathematical abstraction by building an appropriate physical system.
- A (cryptographically strong) PRNG is a product of computational complexity theory.  It is *defined* in terms of the complexity of particular models of computation.  The "connection" between them shows up entirely in the way the PRNG and the attack models for it are defined:  The attacker is assumed to receive as input the outputs of the PRNG, but not its internal state.  If the internal state were somehow "sneaked into" the computation, the whole thing would break down.  Mathematically, we can simply say "the only information available to the machine is what is on its input tape", but in a physical realization, the internal state has to come from *somewhere*, and that "somewhere" must not leak any information to the (physical) attacker or the pretty mathematical model fails.  The only way of providing such internal state, in general, is with a TRNG.
It depends on who you count as "in the world".  If you're talking about physicists and information theorists and communications engineers, I'll agree with you.  If you're talking about designers of nifty new "random number" sources, complete with "entropy estimates" "conservatively" pulled out of a hat; or those who talk about "how many bits of entropy" the RNG must gather before we attempt to choose a value from {0, 1, 2} - well, not so much.
I don't know what "other cases" you're comparing to.
Probability theorists and statisticians generally talk about "randomness", not entropy - but surprisingly little, since that's in their axioms; rather, they talk about "drawing from a random distribution".  Sure, you can think of a random distribution as something with a source entropy, and you do that in information theory all the time, but for many other purposes - e.g., if you want to compute the moments or think about correlations - that's probably not the best representation to use.
"Unpredictability" is a nice informal catch-all for what we'd like to get, but if that's all you have, you're not ready to specify or build a system.
                                                        -- Jerry

@_date: 2013-11-06 16:45:39
@_author: Jerry Leichter 
@_subject: [Cryptography] Ah, The Circles of Life 
I'm surprised you didn't come across:  It turns out that it's possible to compute the n'th digit of pi in hex directly, without computing all the preceding ones.  The calculation is essentially O(n) time, O(log n) space!
(Hex is not unique - obviously, if you can do hex, you can do any base of the form 2^k where either k < 4 or k is a multiple of 4.  I forget what other bases are possible, but 10 is not.)
I think one of the authors of the algorithm used to have a site that would compute the n'th hex digit of pi for you on request, but it seems to have disappeared over the years.  You can readily find source code for this, however.
Which provides another way to "cheat" at choosing constants:  Use the digits of pi starting at position k, where k is chosen in some unspecified way.  Sounds really good - after all, who can muck with the digits of pi? - but of course one can search for a k such that pi starting at k has some desired property....
                                                        -- Jerry

@_date: 2013-11-06 23:16:42
@_author: Jerry Leichter 
@_subject: [Cryptography] randomness +- entropy 
I don't know how many such systems are out there, but if there are such, they are likely old or very cheap embedded systems that it'll be tough to get software updates onto, and impossible to get new hardware onto.  Declaring them "unsound" may not make the go away.
In fact, though, I can think of one simple example:  A CD Linux image used precisely to conduct operations we want to keep secure.  For example, there's a suggestion that small businesses use exactly such a thing to do their on-line banking, as their usual systems are way too vulnerable to various kinds of malware (and small businesses have been subject to attacks that bankrupted them).  The CD itself can't carry a seed, as it will be re-used repeatedly.  It has to come up quickly, and on pretty much any hardware, to be useful.  You could probably get something like Turbid in there - but there are plenty of CD's around already that have little if anything.
Engineering, like politics, is often the art of the possible - and this is exactly a situation where we need to look for solutions that make the situation as much better as we can.  A request for a random seed on the LAN - whether through a DHCP extension, or in some other way - would at least protect against attackers not in a position to watch the local LAN.  Not ideal, but compared to what may be there now - nothing - a step forward.
                                                        -- Jerry

@_date: 2013-11-07 07:01:05
@_author: Jerry Leichter 
@_subject: [Cryptography] suggestions for very very early initialization 
We really don't need this kind of dismissive advice.  There are plenty of people out there who need the best solution we can devise, given limitations on what they have available to them.
*Nothing* should persist from one boot to the next.  That's exactly the point of using a read-only image:  Even if session n is hit by malware that completely compromises it, session n+1 will come up completely clean.  Assume we're very disciplined and resist the urge to put anything on the read/write partition other than a random seed - anything more could potentially trigger some bug that carries the infection on to the next session.  If an infected session n can leave behind a known random seed, we've reduced the problem for the attacker to that of a completely read-only system.  So we *still* need some solution in that case.
BTW, what's this "read-only partition" you speak of?  Something enforced by the OS, which might get compromised?  Or are we talking some special USB flash drive that enforces the read-only state?  Got a source for those?  I've never seen one.  (SD memory cards have a write-protect notch on the side, but the protection is enforced by software on the host, and in any case is all-or-nothing.)
a)  "The best is the enemy of the good"
b)  Is this even "best"?  The proposed system is *much* more complex, and has a *much* larger attack surface.  Attacks against host systems from guests have appeared in the past, and will likely continue to appear.  "Having the advantages of a read/write file system" almost means "having the liabilities of a read/write file system."
A very simple, physically non-modifiable, one-purpose system is ideal from a security point of view.  The ideal hardware base on which to run it provides a good, non-modifiable source of randomness.  ("Non-modifiable" isn't quite the right term:  After a boot, the source must be unpredictable even to someone who had innermost-level access prior to the boot.  And of course there's a need for other controls to guarantee that the code actually booted is the code on the CD, nothing more and nothing less:  Modified BIOS's and such make any attempt at a software solution pointless, though actual fielded attacks at this level have been rare.)  If you have the necessary hardware, fantastic, use it.  If you don't, you should do the best you can.
                                                        -- Jerry

@_date: 2013-11-07 14:08:40
@_author: Jerry Leichter 
@_subject: [Cryptography] randomness +- entropy 
I answered this separately:  Adding writable storage will most likely *reduce* security.
The smartphone and the embedded system are *very* different.  Any phone has a microphone and a radio.  Unfortunately, the radios are usually sealed off so you couldn't use them as a source of radio noise; but the microphones are wide open.  And there are plenty of other environmental sensors - location, movement, orientation, ambient lighting level.  Not to mention a display you can use to ask the user to enter stuff on the keypad.  Smartphones are *easy* - though historically some of them have done a crappy job, even with the rich sources they have available.
Embedded systems are among the hardest.  People want routers and switches and similar hardware to need zero configuration, and yet some of them play essential cryptographic roles and really need good sources of randomness.
                                                        -- Jerry

@_date: 2013-11-07 20:11:57
@_author: Jerry Leichter 
@_subject: [Cryptography] randomness +- entropy 
This is a very weak and inappropriate definition of a PRNG.  The BBS generator *provably* can reveal many more bits than its seed without allowing an attacker to gain any significant advantage in computing either past or future outputs.  (The actual security statement is complicated to state.  Since BBS is way too slow to be practical, the details aren't all that interesting anyway.)  The proof, as is the case for all proofs we have in cryptography, is based on the difficulty of quadratic residue computation, which in turn is equivalent to factoring.
A practical generator that takes the one-way hash of its internal state using something like SHA doesn't have any strong proofs associated with it, but in practice is as "unpredictable" as the cryptographic algorithms it's use with even if way more bits of output than were in the initial seed.  (You still don't release the intact internal state, of course, as then anyone can compute the next value.)
This is why I don't like "entropy talk".  If you want to discuss *information theoretic security*, then, yes, you should (very carefully, see my comments earlier about the "entropy cost" of a three-way choice) count bits.  But no algorithm you're going to feed these values into comes with information theoretic security claims, just computational hardness claims.  (Well, OK, if you want to use the output to generate a one-time-pad, you want information theoretic unpredictability.  But that's not a particularly interesting case.)
A *good* PRNG releasing one bit should allow a "reasonable" (polynomially-bounded) attacker way less than that - essentially nothing.  The security claims for AES-128 state that with an unknown key K, the stream of values produced by E(K,0), E(K,1), ... - i.e., the values used in counter mode - are indistinguishable from a random sequence even as you approach 2^64 *blocks* of output.  Wouldn't you want your PRNG to have a similar security property?  And if you have such a property, what would be the point of a *stronger* requirement when generating an AES key?  (There *is* a point; see below.)
Granted, a cryptographically strong one-way or encryption function in your PRNG may be too slow.  But unless you have hardware and associated software to generate random bits you trust at a very high rate, even a couple of invocations of a good PRNG is going to generate strong values faster than you can pull them from your environment.
This is an entirely different class of attacks, and absolutely, to defend against "state exposure" attacks, you need a way to get some new, independently unpredictable, state.  Of course, the kernel debugger attack is a funny one:  You're assuming an attacker who can read all of your state, but can't modify things (like, say, the constants used to decide how many bits of entropy various collectors contribute, even if you want to assume that the code is somehow protected from modification).  I think a more interesting state exposure attack is against a VM image.  The image could be signed to prevent any modification, but still expose its internal state.
                                                        -- Jerry

@_date: 2013-11-08 07:49:59
@_author: Jerry Leichter 
@_subject: [Cryptography] suggestions for very very early initialization 
While I said the same thing myself ... there's a subtlety here.  You said "CD-ROM", which is a pressed disk, and is indeed physically unwritable.  But that requires access to a CD pressing facility - i.e., CD-ROM's are something for the commercial market.  Same for DVD-ROM.  It's not clear there's a sufficient market for anyone to sell a CD- or DVD-ROM LiveCD, and I'm sure there will be those who wouldn't trust the contents anyway.  (To get around that, you'd want to make sure the contents of the disk were created using a fully reproducible build process.  Then the suspicious could always build from source all the way up to an ISO image and compare bit by bit.)
What most people have in mind, though, is cutting their own CD or DVD.  And here you get into the whole mess of different technologies, and the question of just what enforces the "non-writability".  CD-RW is out, as its explicitly re-writeable.  Any given physical piece of a CD-R "can't be re-written", though you can add more data to previously written sections later.  What "can't be re-written" actually means physically, I'm not sure.  The bits are written to a CD-R as "permanently" altered and unaltered areas of dye.  Even if the "permanent" alterations can't be undone, one could in principle alter some of the unaltered regions.  It would require specialized hardware and software; given all the error correction needed to make these devices usable, it's not even clear what modifications you might be able to introduce.  It also seems highly unlikely that a commercial CD-R writer could be modified (by a malicious firmware alteration) to play this game.  But who knows.
With DVD, things get even more complicated, given the multiple extant technologies.
Theoretically, I suppose it might even be possible to use a laser to blast extra pits into a pressed CD or DVD - though that would certainly require specialized equipment and physical access to the disk, at which point you might as well produce a look-alike disk containing whatever you want on it.
I'm sure the three-letter agencies have had their engineers all over this stuff, just in case they mighy need the capability to modify a "read-only" disk.  I haven't seen any public discussion of the issue, though I'll admit I haven't looked hard for it.
In *practical* terms, a CD-R or DVD-R - *not* a -RW, or the logically equivalent "+" versions - can probably be treated as unmodifiable unless you're targeted by the NSA or someone with similar resources.  (Even then, they probably have many easier ways to get to you.)  It would be nice to confirm, though, that CD-R or DVD-R writers are *physically* incapable of modifying existing information, not just blocked from trying by perhaps-modifiable firmware.
                                                        -- Jerry

@_date: 2013-11-11 17:56:31
@_author: Jerry Leichter 
@_subject: [Cryptography] randomness +- entropy 
Every DSL modem I've ever seen can display noise information in multiple frequency bands.  (It's always gathering this information to adjust its use of the link.)  Firmware would have access to it, and it's likely a very good source to use for driving an RNG.  (Yes, you'd want to first invest some effort in determining exactly how this data is sampled, whether it's exported anywhere - the adjustment is done by the two ends of the link together, but I know nothing about what specific information is exchanged between them, etc.)
I know nothing about cable modems, but most likely they have access to similar kinds of information:  Sending data across long, uncontrolled spans of wire will generally require some sort of adaptation to the characteristics of that wire.
All that said ... I have yet to see a DSL or cable modem that *needs* a secure source of random numbers.  They live at L2 and below and don't encrypt or decrypt anything.  OK, they usually have http interfaces for management - which should really be https and they should come pre-configured with a certificate, as they come with a unique password.
                                                        -- Jerry

@_date: 2013-11-11 18:14:35
@_author: Jerry Leichter 
@_subject: [Cryptography] Looking for feedback on new Java crypto library 
A couple of quick responses:
1.  The security analysis says nothing about the algorithm used.  In particular, it says nothing about how the "IV" is used.  "IV" is a term usually associated with CBC mode.  It appears that you *are* using CBC mode ... you probably shouldn't be.
2.  More specifically, the user provides an IV, but there's no indication of what the constraints on the IV are.  Does it have to be random?  Or simply never re-used in a given cryptographic context (which in the case of these interfaces appears to translate to "with the same key")?  It says "Explicit, never reused, generated from strong PRNG" which doesn't answer any of the questions.  "Explicit" - well, of course.  "Never reused" - even with different keys?  Why? "generated from a strong PRNG".  Why?  And why a *PRNG*.
3.  Why provide whole separate functions to handle Base64, which has nothing to do with crypto?  Can't a caller just wrap the function in appropriate encoders and decoders?
4.  There are multiple constant salts used in the algorithm.  They are documented as having come from /dev/urandom.  But of course there's absolutely no way for anyone to know where they came from.  While I doubt these values would provide any kind of back door, the right way to pick such constants is to avoid any *possibility* that they are "cooked" somehow - e.g., use values from pi *starting at the first position*.
                                                        -- Jerry

@_date: 2013-11-11 22:03:52
@_author: Jerry Leichter 
@_subject: [Cryptography] [cryptography] NIST Randomness Beacon 
Seems awfully complicated.  The only role the destruction of that physical object plays is that it's a widely observed event whose time of occurrence everyone can agree on. But it's not hard to *manufacture* such events completely in digital.  What you need is a timed public commitment to something that is inextricably tied to whatever it is whose existence at a particular time you want to authenticate. For example, suppose you want to show that you wrote some document by midnight tonight.  If you can arrange to publish the hash of the document to a wide audience before midnight tonight, you could always show the document, and anyone can compute the hash and check the publication record.
In fact, a team at (I think) Bell Labs came up with a "digital notary" service that did exactly this, in an efficient way.  It combined the values sent to it into a public Merkle tree, and once a day, published the current root hash in an ad in the New York Times.  That service seems to have vanished (and the phrase "digital notary" seems to have been re-applied to something else).  But there are a number of "time-stamping" protocols, and a RFC (3161), an ANSI standard (X9.95), and ISO/IEC standard (18014) for different kinds of timestamps.  See  for a discussion of the general issue;  has a discussion of the style of timestamp I mentioned (there are other ways to accomplish the same ends) along with a photo of a newspaper showing a daily commitment.
(The particular system I was describing is probably described in this paper:                                                          -- Jerry
                                                        -- Jerry

@_date: 2013-11-12 06:28:04
@_author: Jerry Leichter 
@_subject: [Cryptography] NIST should publish Suite A 
The NSA would have no reason to be concerned about Suite A being attackable *by NSA*.  If you're worried that NSA chose particular curves and parameters for public elliptic-curve cryptography based on secrets they hold that make it breakable ... there's no reason they couldn't do the same for Suite A.  What we know about these back door techniques is that they don't seem to induce a weakness that someone *without* access to the secrets can leverage.
So while it would be interesting to see what's in Suite A, if you're concerned about snooping by the NSA, its own algorithms are the last things you'd want to use.
                                                        -- Jerry

@_date: 2013-11-12 16:24:40
@_author: Jerry Leichter 
@_subject: [Cryptography] randomness +- entropy 
This is why the continuous reseeding model is so insidious, and so difficult to understand.
For a CSPRNG with k bits of internal state, I would say there are two possible modes:
1.  Uninitialized.  The k bits have not yet been set to values chosen from a large enough set of possibilities.  (In general, we assume they ultimately get chosen uniformly at random from the set of all possible k-bit strings.  If you really want to consider the general case, the internal state might not be a k-bit string - it could be a value mod some large N, for example.  But this doesn't change anything fundamental.)  Once state has been set of k "unpredictable" bits, we transition to mode 2.
2.  Initialized, safe.  Those k bits have been set, and we assume an attacker has no knowledge of their values.
In mode 1, it's *impossible* to deliver useful outputs.  Any games that say things like "oh, I have k/2 bits in my internal state properly initialized so I'll only deliver k/4 bits of output and that should be safe" are just making things up as they go along.  Maybe one can design a system in which this is safe, but I haven't seen one yet.  A proper design *must not* deliver any outputs when it's in mode 1.
When a properly designed generator is in mode 2, it can deliver some reasonable fraction of 2^k output bits before its internal state is *computationally* no longer safe.  *You can't use an entropy measurement here!*  Entropy is concerned with absolute security:  Can an opponent with unbounded computation power determine the state.  It is almost certainly the case that once you've output k generated bits, the internal state is uniquely determined.  An unbounded Turing machine can simply enumerate all 2^k possible starting state values and see which produces the k outputs actually seen.  It will almost always find that there is exactly one starting state.  (If there are a couple, a few more outputs will certainly determine the starting state.  This is just a counting argument.)  From the point of view of entropy, every bit of output reduces the internal entropy by one bit.  What a CSPRNG guarantees (modulo assumptions about the crypto primitives used) is that no *polynomially bounded* (probabilistic) TM can gain any significant advantage over guessing the next bit, or working backwards to previous states.  If the primitives used are exactly the ones you're going to use elsewhere in the system, then you have exactly as much reason to trust them there as you do to trust them within the CSPRNG.
After some fraction of 2^k outputs, a CSPRNG *must* move back to mode 1 and somehow get another k bits of "unknowable" state - regardless of any possible compromise.  In the event of a possible compromise of the state, the generator must also transition back to mode 1.
But ... how can we know if a compromise was possible?  Continuously reseeding generators just start with the assumption that a compromise of the internal state *could* occur at any time.  If compromises an occur as fast as the CSPRNG can produce outputs, then it's not contributing anything:  It will have to fully re-seed between every two successive output bits, so it's just burning through the random bits used to seed it k bits at a time.  Better to simply output the true random bits as they arrive!  The obvious *safe* way to do this is to wait for k good random bits to accumulate, then replace the entire state of the CSPRNG, completely eliminating any previous knowledge an adversary might have had.  What's dangerous - and you've illustrated the problem - is to add a few bits of "good randomness" into a compromised state.  I don't see any way to avoid this problem.  (In the paper about the security of the Linux generator that Theodore Tso forwarded a link to earlier, there's a result showing that its pool combination and stirring is a "mixing function", in the sense that the result has at least much entropy as either of the inputs.  But that doesn't help in the case of a state compromise, since in that case the entropy of the internal state is 0.)
How often should we reseed?  There's no certain way to know without knowing how a state compromise might occur.  You'd like to reseed at an interval short enough that a compromise during that interval is "unlikely enough".
If you assume that there is no correlation between how fast bits are drawn from the CSPRNG and how likely a compromise is at any point in time, there are simple relationships that let you say something like:  Assuming can generate r true random bits/second, the chance of a state compromise is one every c seconds, and the total demand on the CSPRNG is d bits/second, and I've drawn n bits, the chance that m of them were compromised is no more than p.  If I can't increase r, I can make p lower by limiting d (by blocking if the demand rate is too high).
This suggests a very simple design:  There's a CSPRNG with k bits of internal state that's the source of random numbers for virtually all purposes.  There's a source of "true random" bits.  The CSPRNG blocks until it receives (either from the "true random" generator or from a previously-saved and assumed to be uncompromised) k seed bits; then it delivers those bits freely, with a possible rate cap to limit d as suggested above.  In parallel, we continue to gather bits from the true random generator; when we have k of them, we completely replace all k seed bits.
As far as I can see, this is (a) as safe as possible within constraints you can manipulate as you like (k, d, r, p); (b) simple, hence "obviously correct".  It's perfectly OK to allow access to the "true random number generator" so long as you reserve the assumed r bits/second for the CSPRNG, which almost everyone should be using.
What advantages do other approaches offer?  ("Not blocking" in favor of returning values that may not be safe is not a good option.  Except for initial startup, you can manipulate what you think is safe well enough, rather than just going by a hope and a prayer.  There's simply nothing you can to at startup if you don't have sufficient true random bits to get started, and you're not doing anyone a favor by trying.
                                                        -- Jerry

@_date: 2013-11-13 15:33:02
@_author: Jerry Leichter 
@_subject: [Cryptography] randomness +- entropy 
Why?  It's not like you're going to need to generate a new key; nor do disk encryption modes typically require an IV or any other associated random value.  (They are typically block by block, so there's no room to store anything beyond the encrypted block itself.)
Beyond that ... what data could *need* encryption?  By hypothesis, everything the system does is predictable to an opponent - otherwise we'd have our random seed.  What's the point in encrypting data an opponent can already predict?
It depends on what you wrote there - and on whether you're concerned that an attacker could have gotten his hands on it.  If you're not concerned about an attack who could see the physical disk, the raw data stored in an encrypted file system is a pretty good seed.  If you think someone go get access to the physical disk, but that they wouldn't have the key, then you can store all the state you want, securely per hypothesis, within one ore more encrypted file systems.
There really should not be.  Of course, people make dumb decisions all the time, and then find it easy to convince themselves that what they did is *right* and must be supported.    In that direction madness lies.
                                                        -- Jerry

@_date: 2013-11-18 21:21:06
@_author: Jerry Leichter 
@_subject: [Cryptography] Moving forward on improving HTTP's security 
Testing software that relies even on non-cryptographic pseudo-randomness - e.g., Monte Carlo algorithms - is a challenge exactly because we *expect* repeated runs to get different seeds and thus produce different results.  A standard trick is for the test framework to print the random seed it's using.  (This is typically generated from the time of day in production and production test use; what's important is that it varies, not that it's unpredictable).
Should a test run produce unexpected results, there's an test framework provides and alternate way to run the software, feeding back in the seed printed in a failing run - which we can now reproduce exactly.                                                         -- Jerry

@_date: 2013-11-21 20:12:22
@_author: Jerry Leichter 
@_subject: [Cryptography] Cryptolocker 
There's some malware making the rounds that applies a technique that's been talked about for years:  It (allegedly) generates a public/private key pair, sends the private key off to the mother ship, then starts encrypting all accessible files.  When it's done enough, it starts demanding money for the key to decrypt everything.  One article about it:
Nasty piece of work, apparently - it locates and encrypts accessible network-mounted disks, so it often encrypts people's backups.
Anyway ... I'll leave the virus analysis and hunting to others.  But there's also a crypto question here.  Has anyone seen an analysis of what this thing *really* does internally.  Obviously, it will *say* it's using all kinds of strong algorithms, but that doesn't mean it actually *is*.  (In particular, I'm curious about how they are doing the encryption.  Doing bulk encryption in RSA or even using elliptic curves is slow, though it might be fast enough for this purpose.  The obvious technique would be to generate a random AES key per file, encrypt *it* with the public key and store that away, then use AES for bulk encryption.  But I haven't seen any hints of a store of such keys anywhere; in fact, there are reports that the magic key is stored in one registry entry.
Anyone following this story from the crypto side?
                                                        -- Jerry

@_date: 2013-11-21 22:16:13
@_author: Jerry Leichter 
@_subject: [Cryptography] Cryptolocker 
This does mean that if you manage to catch the program while it's running, you can (in principle; the practice may be quite difficult) extract the AES key, which is all you need - the RSA keypair is purely secondary.  There are reports that the program will encrypt newly-created files on an infected machine, at least sometimes.  If this is true, the program must have some way of asking for the AES key, even if the ransom hasn't been paid.  That would be a major vulnerability.
                                                        -- Jerry

@_date: 2013-11-23 19:20:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Dark Mail Alliance specs? 
It's hard to be definitive in answering this kind of question; in essence, you learn the answer by finding a way to actually get people using.  I'll hazard the guess, though, that it's a combination of two factors:
- On the "pull" side, people haven't felt there was a need.  Before the recent NSA brouhaha, when was the last time you heard of anyone actually having their email intercepted?  Note the question carefully:  It's not whether people *have* had their unencrypted email intercepted; it's whether such interceptions have gotten "airplay".  There are tons and tons of stories about people attacked in various ways by malware in attachments and stuff.  There are many stories about email accounts being taken over by identity thieves.  There are related stories in which people's passwords are stolen, and then all their mail is stolen (e.g., Stratfor).  There are even stories about people getting into trouble when they let someone - friend, lover - have access to their email, and that person turned against them.  But I can't recall a single story of interception "on the wire", or even of interception by access to a disk where the email was stored.  It's certainly happened, but it's probably been rare, as there it's usually just as easy to mount a stronger attack view malware or password breaking.
Note, by the way, that with the limited exception of my first example, encryption wouldn't help *at all*.  They all involve cases where the attacker is able to gain access to mail as the legitimate user, so will have ability to read that mail.  In the first example, you might argue that people know not to open attachments from strangers, but are fooled by email apparently from friends.  With authenticated senders, you wouldn't be able to fool the recipient.  Unfortunately, many of these attacks are actually the result of a prior break-in to the friend's account - so the message will arrive fully authenticated.
- On the "push" side, the *system* aspects have simply not been properly implemented.  Many mail systems implement S/MIME - but how do you get keys?  There simply is no good way to get them.  Where you get email addresses out of Exchange or some LDAP service you could just as easily get the related key - but I have yet to see an organization implement that.  (I haven't checked, but I'd guess both have the *capability* to do this; it's just that no one has bothered to enable it.)
Since there's been no "pull", there's been little to encourage development of the "push" side.  So the capability to exchange encrypted emails sits there virtually unused.
There's a third recent factor, of course:  Increasingly, people read their email through Web interfaces.  There's simply no way to make that secure in current browsers.  Sure, you can download some Javascript to decrypt your mail, but there's no good reason to trust it!
My suggestion is that there are two fundamental problems that need to be attacked:
1.  The key lookup/distribution problem.  It has to be easy and straightforward to get keys in the common use cases.  In fact, it needs to be so easy and straightforward as to be invisible to most users most of the time. It's easy to get caught up in difficult edge cases that can't be handled easily.  This is a case where "the best is the enemy of the good".  Cover as much as you can cleanly today; worry about the hard cases tomorrow.  (Many people will never run into a need for a solution to a hard case, and those that do may be willing and able to do more work.)
2.  The Web mail problem.  I can see only one solution to this:  Get S/MIME implemented in browsers.  HTML/5 already contains tons of interfaces (way too many, I'd say, but that ship sailed a *long* time ago) to implement various things that simply need to be present *everywhere*, generally for performance reasons.  (After all, Javascript *is* Turing-complete.)  While S/MIME or some other secure mail protocol *could* be implemented in Javascript, it would have to be downloaded each time - and there's no way to guarantee security.  If S/MIME were built in, you would have exactly as much reason to trust it as you would to trust the S/MIME in your conventional MUA.
Getting this done will take some time:  Getting stuff in HTML5 is a big standards circus, and then you have to wait for implementations to reach critical mass in the field.  But the sooner we start, the faster it will happen.
Note that it *might* be possible to get this in through a side door:  For better or worse, HTML5 will likely have a place to stick DRM implementations. Perhaps an S/MIME "DRM plugin" would be feasible....
                                                        -- Jerry

@_date: 2013-11-25 17:25:45
@_author: Jerry Leichter 
@_subject: [Cryptography] Email is unsecurable 
It's worth pointing out that one use case for secure email has arisen, and has solutions.  Medical and financial service providers (and probably others, by now) have realized (or been pushed by regulations) that they can no longer send unencrypted email containing sensitive information to their clients.  The solution a couple I deal with are using is to send email purely as a notification, pointing to a "secure message" service.  This is a Web service that lets you read mail from (say) your broker and reply, all within a "secure" closed system.
The predecessors of such systems are common - many web sites provide a way to exchange messages directly with the site owner.  (I think people are outsourcing this exactly to try to put the liability for security issues on an outsider.)
So we're growing isolated islands of "secure" (there's really no way for anyone to judge from the outside just how "secure" these things are) email services, which have limited functionality and don't in any way interconnect.  Sad.
                                                       -- Jerry

@_date: 2013-11-26 22:49:34
@_author: Jerry Leichter 
@_subject: [Cryptography] Explaining PK to grandma 
All very true.  Several of you guys beat me to saying this.
*But*, there is one thing that may need, no so much "explanation" in the sense of conveying a deep understanding, as "training".  Somehow, a user of secure email has to know how to get a key for themselves; how to move that key to different machines; that they must *not* give that key to anyone else.  Conversely, they need to understand how to get some secure "thing" - I don't want to call it a "key" because it makes the term ambiguous and leads to people passing their private key to others - that you give to others so that they can reach you securely, and conversely that you have to get from them so that you can reach them securely.  Most of the actual work involved must be automated and invisible, but the decisions involved have to be made by the humans involved, and they need to understand the implications.
Given a carefully designed and implemented UI with appropriate stuff behind it, I think this could be done.  A beginning of an appropriate metaphor might be the keys to your house (perhaps your mailbox is better) for things that are, behind the scenes, private keys, vs. business cards with contact numbers and addresses, for things that are, behind the scenes, public keys.  The fact that mathematically they're "the same thing" is neither interesting nor relevant to most people, and certainly doesn't help them use the system.
There's no need to even mention signing keys - anything necessary can be handled silently along with encryption keys, and as far as users should be concerned, either a message is transfered "secure" - meaning encrypted *and* authenticated - or it isn't.
                                                        -- Jerry

@_date: 2013-11-26 23:01:36
@_author: Jerry Leichter 
@_subject: [Cryptography] Explaining PK to grandma 
Email isn't the only thing with this unfortunate property.  I had a long argument with a security expert about what you can trust on a Web page.  Basically, his assertion was that the contents of the browser address bar were the only thing at all trustworthy, and only under appropriate circumstances.  Everything else could be faked.  Wherever you *think* a link goes, the only way to know for use is to click on it and see what shows up in the browser bar.
He's right on the facts, but refuses to focus on the implications of this model we've built.  Users don't *look* at the contents of browser address bars.  Why should they?  It has nothing to do with what they are using the browser *for*.
While it may be true that you can't really trust any link embedded in a page to go where it seems to go - you can only click on it and see what ends up in the address bar - in world of drive-by attacks, where simply opening a page may lead to your machine being infected, this model is insane.  If people really internalized just how bad things are, it's not clear they'd be willing to use the Web.
                                                        -- Jerry

@_date: 2013-11-27 07:11:54
@_author: Jerry Leichter 
@_subject: [Cryptography] Microsoft announces new email encryption 
I read the description and couldn't figure it out.  But then you can almost never figure out the details of a Microsoft product from their marketing literature.  (They aren't alone in this, but they are certainly masters of the style.)
However... what *seems* to be involved here is a variation of the approach I described earlier, where certain businesses - particularly financial and medical - use a third party to implement a closed "secure email" facility for communication between their customers and the business.  To access your "secure email", you have to create an account with that third-party provider (who, indeed, as in MS's product can add their own branding to the service) and then log in to it.  Generally, you'll get ordinary email that contains a link to your account on the "secure email" service.
MS has used its ubiquitous email products and services to extend the model in two ways.  First, from the user's side, they've tried to hide the separate nature of the "secure" messages by making the link to the browser as transparent as possible, and, it seems, making the message you see appear to be within the mail program, rather than in a separate Web session.  (It's at this point that the marketing descriptions get the most complicated and difficult to follow.)  Second, from the corporate user's point of view, they've removed both the ability and the need to choose whether to send some mail to a customer as a traditional message or a "secure" message.  Rather, the corporate user sends the message just as any other, and then some management-configured rules decide how to route it.
One can look at this in many ways.  The security provided is certainly not air-tight, but it's a hell of a lot better than the complete insecurity of traditional email.  Yes, it's all controlled by management at the sending company, but they owned and controlled the infrastructure (or contracted it out to Microsoft) *anyway* - nothing gained but nothing lost.  Given that they don't control all the clients on the receiving side, some kind of overlay mechanism is needed, and a web link is hardly new.  (The battle to get people not to click on links or open attachments is long lost.  There was a time I could usefully surf the Web with Javascript disabled; that time is long gone, too.  The fight we have to win now is making those two activities safe - a fight I really, really wish we could have avoided, but there it is.)
Does this give Microsoft access to the "secure" mail between my doctor and me?  Maybe - it's impossible to tell from the description who actually has access to the decrypted material.  It seems likely, but then this is a product aimed at those using Microsoft's hosted mail services, which means they already have access to any cleartext mail.
Is this a land grab by Microsoft to get people to create Microsoft ID's?  I'm sure their view is "it couldn't hurt".  After all, they are in a race with Google (the most aggressive - just try using an Android tablet without creating a Google account) and Apple (which apparently has the largest collection of user credit cards, but is in some ways the easiest to avoid:  When you configure a new device, they strongly suggest you create an account, but right on the screen they offer to do it "later", and almost everything will work if "later" is in the 22nd century), not to mention Facebook and Disqus and everyone else.  So far, it's trivial to just create multiple accounts limited to very specific purposes, which has been my response.
Looking at the overall picture, the existence and spread of these services - and with Microsoft in the game, they will become much more common and visible - has a very important salutary effect:  It will lead people "in the large" to understand that "ordinary" email is *not* secure, something they've never focused on in the past.  Perhaps this will create an opportunity for other, better secure mail services, more suited to the all-to-all nature of email than the hub-and-spoke communications the Microsoft service and others like it aim at.  But someone - "one" writ large - will have to find a way to seize the opportunity when it emerges.  The window will likely be small.
                                                        -- Jerry

@_date: 2013-11-27 16:20:07
@_author: Jerry Leichter 
@_subject: [Cryptography] Email is unsecurable 
Ah, the irony.
NSA was for years resistant to software-based cryptography.  The DES initial and final permutations were trivial in hardware, a pain in software.  It's long been thought that they were in the algorithm exactly to slow software implementations.  FIPS and similar standards, whose form was clearly influence by NSA, to this day, have a bias toward hardware, to the point where parts of them have to be really stretched to even make sense for software.
To this day, NSA seems to be big on smart cards and encryption "black boxes" rather than software on general-purpose machines.
It was fashionable for years to dismiss that NSA mindset as just a hold-over from the past - we in the software world knew better.
Well ... maybe we didn't.  :-(
                                                        -- Jerry

@_date: 2013-11-28 07:30:39
@_author: Jerry Leichter 
@_subject: [Cryptography] Explaining PK to grandma 
I'm not sure what you are saying here.  "Authorizing the new machine" is just "how to move the key to a different machine" in different words.  OK, it says it even more broadly than I did, but you can't get *too* broad without losing important distinctions.  The person undertaking the actions has to understand that some actions make the encrypted text visible, so are not to be undertaken lightly.  If you really use the words "authorizing the machine", you're putting the emphasis in the wrong place:  The machine.  Who cares about the machine?  What matters is what *people* you've implicitly authorized through this action.  Handing your car keys to someone isn't about the keys - it's about who can drive away in your car.
No disagreement on the general principle:  Design that system so that it's easy to do the right thing and hard to do the wrong thing.  But, again, you can't remove all choice in the matter.  To take an extreme example, there must be a way to make the key accessible to heirs - or *not* make it accessible to heirs.  The holder of the key must have a reasonable understanding of what it would mean either way, and a straightforward mechanism for making the choice.
A useable system presents useful choices and actions in terms and with semantics that are appropriate and meaningful - where "useful", "appropriate" and "meaningful" are judged by those who use the system, not those who designed it.  Perhaps there's a role for a system with even fewer choices than I outlined, though personally I find it hard to see except in very limited circumstances.
                                                        -- Jerry

@_date: 2013-11-30 00:06:06
@_author: Jerry Leichter 
@_subject: [Cryptography] Explaining PK to grandma 
Gee, me too.  Kind of annoying, then, to be apparently quoted saying things about elderly females and clueless users when what *I* wrote deliberately mentioned neither.
Do watch the attributions, please.
                                                        -- Jerry

@_date: 2013-10-01 11:36:13
@_author: Jerry Leichter 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
It goes beyond that.  A company named Iovation sells a service that's supposed to check a fingerprint of your machine against a database so that someone like a bank can determine if your login is supposed to come from this machine.  (It also leaves behind a cookie, and may blacklist some addresses).  Anyway, the result is a connection to "iesnare.something" when you go to your bank.  I run a Little Snitch on my Mac's; it reports and ask for approval for unknown connections.  So I see alerts pop up when I go to my bank and similar sites.  Sometimes I block the connection, sometimes I let it through.  (Actually, it doesn't seem to affect the site's behavior either way.)
                                                        -- Jerry

@_date: 2013-10-01 11:59:26
@_author: Jerry Leichter 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
It's clear what "10x stronger than needed" means for a support beam:  We're pretty good at modeling the forces on a beam and we know how strong beams of given sizes are.  We have *no* models for "the strength of a crypto system" that would allow one to meaningfully make such comparisons in general.  It's like asking that houses be constructed to survive intact even when hit by the Enterprise's tractor beam.
Oh, if you're talking brute force, sure, 129 bits takes twice as long as 128 bits.  But even attacking a 128-bit cipher by brute force is way beyond anything we can even sketch today, and 256 bits is getting into "if you could use the whole known universe as a computer it would talk you more than the life of the universe" territory.
If, on the other hand, you're talking analytic attacks, there's no way to know ahead of time what matters.  The ultimate example of this occurred back when brute force attacks against DES, at 56 bits, were clearly on the horizon - so people proposed throwing away the key schedule and making the key the full expanded schedule of 448 bits, or whatever it came to.  Many times more secure - except then differential cryptography was (re-)discovered and it turned out that 448-bit DES was no stronger than 56-bit DES.
There are three places I can think of where the notion of "adding a safety factor" makes sense today; perhaps someone can add to the list, but I doubt it will grow significantly longer:
1.  Adding a bit to the key size when that key size is small enough;
2.  Using multiple encryption with different mechanisms and independent keys;
3.  Adding rounds to a round-based symmetric encryptor of the design we currently use pretty universally (multiple S and P transforms with some keying information mixed in per round, repeated for multiple rounds).  In a good cipher designed according to our best practices today, the best attacks we know of extend to some number of rounds and then just die - i.e., after some number of rounds they do no better than brute force.  Adding a few more beyond that makes sense.  But ... if you think adding many more beyond that makes sense, you're into tin-foil hat territory.  We understand what certain attacks look like and we understand how they (fail to) extend beyond some number of rounds - but the next attack down the pike, about which we have no theory, might not be sensitive to the number of rounds at all.
These arguments apply to some other primitives as well, particularly hash functions.  They *don't* apply to asymmetric cryptography, except perhaps for case 2 above - though it may not be so easy to apply.  For asymmetric crypto, the attacks are all algorithmic and mathematical in nature, and the game is different.
                                                        -- Jerry

@_date: 2013-10-01 15:18:24
@_author: Jerry Leichter 
@_subject: [Cryptography] encoding formats should not be committee'ized 
To be blunt, you have no idea what you're talking about.
I worked at Google until a short time ago; Ben Laurie still does.  Both of us have written, submitted, and reviewed substantial amounts of code in the Google code base.  Do you really want to continue to argue with us about what the Google Style Guide is actually understood within Google?
                                                        -- Jerry

@_date: 2013-10-01 17:04:52
@_author: Jerry Leichter 
@_subject: [Cryptography] Passwords 
I've (half-)jokingly suggested that any business maintaining a database of usernames and passwords must, by law, include within that database, under a set of fixed fake user names using exactly the same format and algorithms as is used for all other user accounts, such things as (a) the business's bank account data, including account numbers and full authentication information; (b) similar information about the top executives in the company and everyone on the management chain who has any responsibility for the database.  Once that information is in the database, the business can protect it or not, as they wish.  Let them sink or swim along with their users.
                                                        -- Jerry

@_date: 2013-10-01 23:48:18
@_author: Jerry Leichter 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
[Getting away from crypto, but ... ]  Having recently had significant work done on my house, I've seen this kind of thing close up.
There are three levels of construction.  If you're putting together a small garden shed, "it looks right" is generally enough - at least if it's someone with sufficient experience.  If you're talking non-load-bearing walls, or even some that bear fairly small loads, you follow standards - use 2x4's, space them 36" apart, use doubled 2x4's over openings like windows and doors, don't cut holes larger than some limit - and you'll be fine (based on what I saw, you could cut a hole large enough for a water supply, but not for a water drain pipe).  Methods of attachment are also specified.  These standards - enforced by building codes - are deliberately chosen with large safety margins so that you don't need to do any detailed calculations.  They are inherently safe over some broad range of sizes of a constructed object.
Beyond that, you get into the realm of computation.  I needed a long open span, which was accomplished with an LV beam (engineered wood - LV is Layered Veneer).  The beam was supporting a good piece of the house's roof, so the actual forces needed to be calculated.  LV beams come in multiple sizes, and the strengths are well characterized.  In this case, we would not have wanted the architect/structural engineer to just build in a larger margin of safety:  There was limited space in the attic to get this into place, and if we chose too large an LV beam "just for good measure", it wouldn't fit.  Alternatively, we could have added a vertical support beam "just to be sure" - but it would have disrupted the kitchen.  (A larger LV beam would also have cost more money, though with only one beam, the percentage it would have added to the total cost would have been small.  On a larger project - or, if we'd had to go with a steel beam if no LV beam of appropriate size and strength existed - the cost increase could have been significant.)
The larger the construction project, the tighter the limits on this stuff.  I used to work with a former structural engineer, and he repeated some of the "bad example" stories they are taught.  A famous case a number of years back involved a hotel in, I believe, Kansas City.  The hotel had a large, open atrium, with two levels of concrete "skyways" for walking above.  The "skyways" were hung from the roof.  As the structural engineer specified their attachment, a long threaded steel rod ran from the roof, through one skyway - with the skyway held on by a nut - and then down to the second skyway, also held on by a nut.  The builder, realizing that he would have to thread the nut for the upper skyway up many feet of rod, made a "minor" change:  He instead used two threaded rods, one from roof to upper skyway, one from upper skyway to lower skyway.  It's all the same, right?  Well, no:  In the original design, the upper nut holds the weight of just the upper skyway.  In the modified version, it holds the weight of *both* skyways.  The upper fastening failed, the structure collapsed, and as I recall several people on the skyways at the time were killed.  So ... not even a factor of two safety margin there.  (The take-away from the story as delivered to future structural engineers was *not* that there wasn't a large enough safety margin - the calculations were accurate and well within the margins used in building such structures.  The issue was that no one checked that the structure was actually built as designed.)
I'll leave it to others to decide whether, and how, these lessons apply to crypto design.
                                                        -- Jerry

@_date: 2013-10-01 23:54:33
@_author: Jerry Leichter 
@_subject: [Cryptography] Passwords 
Indeed.  A friend served on nuclear subs; I heard about that practice from him.  (The same practice is followed after any significant refit.)  It inspired my suggestion.
                                                        -- Jerry

@_date: 2013-10-02 00:27:11
@_author: Jerry Leichter 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
Because there's no security advantage, but a practical disadvantage.
When blocks are small enough, the birthday paradox may imply repeated blocks after too short a time to be comfortable.  Whether this matters to you actually depends on how you use the cipher.  If you're using CBC, for example, you don't want to ever see a repeated block used with a single key.  With 64-bit blocks (as in DES), you expect to see a repetition after 2^32 blocks or 2^38 bytes, which in a modern network is something that might actually come up.
A 128-bit block won't see a collision for 2^64 blocks or 2^71 bytes, which is unlikely to be an issue any time in the foreseeable future.
Note that many other modes are immune to this particular issue.  For example, CTR mode with a 64-bit block won't repeat until you've used it for 2^64 blocks (though you would probably want to rekey earlier just to be safe).
I know of no other vulnerability that are related to the block size, though they may be out there; I'd love to learn about them.
On the other hand, using different block sizes keeps you from easily substituting one cipher for another.  Interchanging AES-128 and AES-256 - or substituting in some entirely different cipher with the same block size - is straightforward.  (The changed key length can be painful, but since keys are fairly small anyway you can just reserve key space large enough for any cipher you might be interested int.)  Changing the block size affects much more code and may require changes to the protocol (e.g., you might need to reserve more bits to represent the length of a short final block).
                                                        -- Jerry

@_date: 2013-10-02 17:12:40
@_author: Jerry Leichter 
@_subject: [Cryptography] encoding formats should not be committee'ized 
Always keep in mind - when you argue for "easy readability" - that one of COBOL's design goals was for programs to be readable and understandable by non-programmers.  (There's an *immense* amount of history and sociology and assumptions about how businesses should be managed hidden under that goal.  One could write a large article, and probably a book, starting from there.)
My favorite more recent example of the pitfalls is TL1, a language and protocol used to managed high-end telecom equipment.  TL1 has a completely rigorous syntax definition, but is supposed to be readable.  This leads to such wonderful features as that SPACE is syntactically significant, and SPACE SPACE sometimes means something different from just SPACE.  I have no idea if TL1 messages have a well-defined canonical form.  I doubt it.
Correct TL1 parsers are complicated and if you need one it's generally best to bite the bullet and pay to buy one from an established vendor.   Alternatively, you can go to  and pay $728 for a document that appears to be less than 50 pages long.  Oh, and you "may wish to refer" to 6 other documents available at similarly reasonable prices.
                                                        -- Jerry

@_date: 2013-10-03 12:21:26
@_author: Jerry Leichter 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
There are *related key* attacks against full AES-192 and AES-256 with complexity  2^119.   reports on improved versions of these attacks against *reduced round variants" of AES-256; for a 10-round variant of AES-256 (the same number of rounds as AES-128), the attacks have complexity 2^45 (under a "strong related sub-key" attack).
None of these attacks gain any advantage when applied to AES-128.
As *practical attacks today*, these are of no interest - related key attacks only apply in rather unrealistic scenarios, even a 2^119 strength is way beyond any realistic attack, and no one would use a reduced-round version of AES-256.
As a *theoretical checkpoint on the strength of AES* ... the abstract says the results "raise[s] serious concern about the remaining safety margin offered by the AES family of cryptosystems".
The contact author on this paper, BTW, is Adi Shamir.
100% agreement.
                                                        -- Jerry

@_date: 2013-10-03 18:25:17
@_author: leichter@lrw.com 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
Expanding a bit on what I said:  Ideally, you'd like a cryptographic algorithm let you build a pair of black boxes.  I put my data and a key into my black box, send you the output; you put the received data and the same key (or a paired key) into your black box; and out comes the data I sent you, fully secure and authenticated.  Unfortunately, we have no clue how to build such black boxes.  Even if the black boxes implement just the secrecy transformation for a stream of blocks (i.e., they are symmetric block ciphers), if there's a related key attack, I'm in danger if I haven't chosen my keys carefully enough.
No protocol anyone is likely to use is subject to a related key attack, but it's one of those flaws that mean we haven't really gotten where we should.  Also, any flaw is a hint that there might be other, more dangerous flaws elsewhere.
If you think in these terms about asymmetric crypto, the situation is much, much worse.  It turns out that you have to be really careful about what you shove into those boxes, or you open yourself up to all kinds of attacks.  The classic paper on this subject is  the text for which appears to available only for a fee.
                                                        -- Jerry

@_date: 2013-10-04 10:38:20
@_author: Jerry Leichter 
@_subject: [Cryptography] Sha3 
If you're going to choose a single standard cryptographic algorithm, you have to consider all the places it will be used.  These range from very busy front ends - where people to this day complain (perhaps with little justification, but they believe that for them it's a problem) that doing an RSA operation per incoming connection is too expensive (and larger keys will only make it worse), to phones (where power requirements are more of an issue than raw speed) to various embedded devices (where people still use cheap devices because every penny counts) to all kinds of devices that have to run for a long time off a local battery or even off of microwatts of power transferred to an unpowered device by a reader.
We do have such machines, but we also have - and will have, for the foreseeable future - machines for which this is *not* the case.
Deciding on where to draw the line and say "I don't care if you can support this algorithm in a sensor designed to be put in a capsule and swallowed to transmit pictures of the GI tract for medical analysis" is not a scientific question; it's a policy question.
The only problem with this argument is that "the biggest problem" is hard to pin down.  There's little evidence that the symmetric algorithms we have today are significant problems.  There is some evidence that some of the asymmetric algorithms may have problems due to key size, or deliberate subversion.  Fixing the first of these does induce significant costs; fixing the second first of all requires some knowledge of the nature of the subversion.  But beyond all this the "biggest problems" we've seen have to do with other components, like random number generators, protocols, infiltration of trusted systems, and so on.  None of these is amenable to defense by removing constraints on performance.  (The standardized random number generator that "ignored performance to be really secure" turned out to be anything but!)
We're actually moving in an interesting direction.  At one time, the cost of decent crypto algorithms was high enough to be an issue for most hardware.  DES at the speed of the original 10Mb/sec Ethernet was an significant engineering accomplishment!  These days, even the lowest end "traditional computer" has plenty of spare CPU to run even fairly expensive algorithms - but at the same time we're pushing more and more into a world of tiny, low-powered machines everywhere.  The ratio of speed and supportable power consumption and memory between the average "large" machine and the average "tiny" machine is wider than it's ever been.  At the low end, the exposure is different:  An attacker typically has to be physically close to even talk to the device, there are only a small number of communications partners, and any given device has relatively little information within it.  Perhaps a lower level of security is appropriate in such situations.  (Of course, as we've seen with SCADA systems, there's a temptation to just put these things directly on the Internet - in which case all these assumptions fail.  A higher-level problem:  If you take this approach, you need to make sure that the devices are only accessible through a gateway with sufficient power to run stronger algorithms.  How do you do that?)
So perhaps the assumption that needs to be reconsidered is that we can design a single algorithm suitable across the entire spectrum.  Currently we have SHA-128 and SHA-256, but exactly why one should choose one or the other has never been clear - SHA-256 is somewhat more expensive, but I can't think of any examples where SHA-128 would be practical but SHA-256 would not.  In practice, when CPU is thought to be an issue (rightly or wrongly), people have gone with RC4 - standards be damned.
It is worth noting that NSA seems to produce suites of algorithms optimized for particular uses and targeted for different levels of security.  Maybe it's time for a similar approach in public standards.
                                                        -- Jerry

@_date: 2013-10-04 10:50:12
@_author: Jerry Leichter 
@_subject: [Cryptography] encoding formats should not be committee'ised 
More to the point, it was designed to be a *markup* format.  The markup is metadata describing various semantic attributes of the data.  If you mark up a document, typically almost all the bytes are data, not metadata!
TeX and the -roff's are markup formats, though at a low level.  LaTeX moves to a higher level.  The markup commands in TeX or roff or LaTeX documents are typically a couple of percent of the entire file.  You can typically read the content, simply ignoring the markup, with little trouble.  In fact, there are programs around at least for TeX and LaTeX that strip out all the markup so that you can read the content as "just plain text".  You can typically get the gist with little trouble.
If you look at what XML actually ended up being used for, in many cases nearly the entire damn "document" is ... "markup"!  The "data being marked up" becomes essentially vestigial.  Strip out the XML and nothing is left.  In and of itself, there may be nothing wrong with this.  But it's why I object to the use of "markup language" to describe many contemporary uses of XML.  It leads you to think you're getting something very different from what you actually do get.  (The XML world has a habit of using words in unexpected ways.  I had the damnedest time understanding much of the writing emanating from this world until I realized that when the XML world say "semantics", you should read it as "syntax".  That key unlocks many otherwise-mysterious statements.)
                                                        -- Jerry

@_date: 2013-10-05 11:59:51
@_author: Jerry Leichter 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
Yes, but think about how you would fit it into the question I raised:
Pinning down where the primitive ends and the protocol is tricky and ultimately of little value.  The takeaway is that crypto algorithms have to be used with caution.  Even a perfect block cipher, if used in "the most obvious way" (ECB mode), reveals when it has been given identical inputs.  Which is why it's been argued that any encryption primitive (at some level) has to be probabilistic, so that identical inputs don't produce identical outputs.  (Note that this implies that output must always be larger then the input!) Still, we have attainable models in which no semantic information about the input leaks (given random keys).  Related key attacks rely on a different model which has nothing much to do with practical usage but are obvious from a purely theoretical point of view:  OK, we've insulated ourselves from attacks via the plaintext input, how about the key?
More broadly there are plenty of attacks (probably including most of the related key attacks; I haven't looked closely enough to be sure) that are based on weaknesses in key scheduling.  If you're going to make a cryptographic hash function a fundamental part of your block cipher, why not use it to generate round keys?  The only reason I know of - and in practical terms it's not a trivial one - is the substantial performance hit.
                                                        -- Jerry

@_date: 2013-10-05 12:16:13
@_author: Jerry Leichter 
@_subject: [Cryptography] Sha3 
Oops - acronym confusion between brain and keyboard.  I meant to talk about AES-128 and AES-256.
                                                        -- Jerry

@_date: 2013-10-05 20:11:02
@_author: Jerry Leichter 
@_subject: [Cryptography] System level security in "low end" environments 
He had a big advantage:  He had access to power, since the system has to run the motors (which probably require an order of magnitude or more power than all his electronics).  These days, the limits on many devices are expressible in watts/"compute" for some measure of "compute".  But less often noticed (because most designers are handed a fixed device and have to run with it) dynamic RAM also draws power, even if you aren't doing any computation.  (Apple, at least, appears to have become very aware of this early on and designs their phones to make do with what most people would consider to be very small amounts of RAM - though perhaps not those of us who grew up in 16-bit-processor days. :-)  Some other makers didn't include this consideration in their designs, built with larger RAM's - sometimes even advertising that as a plus - and paid for it in reduced battery life.)
A couple of years back, I listened to a talk about where the next generation of wireless communications will be.  Historically, every so many years, we move up a decade (as in factor of 10) in the frequencies we use to build our communications devices.  We're approaching the final such move, to the edge of the TeraHertz range.  (Another factor of 10 gets you to stuff that's more like infrared than radio - useful, but in very different ways.)  What's of course great about moving to higher frequencies is that you get much more bandwidth - there's 10 times as much bandwidth from 10GHz to 100GHz as there was from DC up to 10GHz.  And the power required to transmit at a given bit rate goes down with the bandwidth, and further since near-THz radiation is highly directional you're not spewing it out over a sphere - it goes pretty much only where it's needed.  So devices operating in the near-THz range will require really tiny amounts of power.  Also, they will be very small, as the wavelengths are comparable to the size of a chip.  In fact, the talk showed pictures of classic antenna geometries - dipoles, Yagi's - etched directly onto chips.
Near-THz frequencies are highly directional, so you need active tracking - but the "computes" to do that can go on chip along with the antennas they control.  You'd guess (at least I did until I learned better) that such signals don't travel far, but in fact you have choices there:  There are bands in which air absorption is high, which is ideal for, say, a WiFi replacement (which would have some degree of inherent security as the signal would die off very rapidly).  There are other bands that have quite low air absorption.  None of these frequencies are likely to propagate far through many common building materials, however.  So we're looking at designs with tiny, extremely low powered, active repeaters all over the place.  (I visualize a little device you stick on a window that uses solar power to communicate with a box on a pole outside, and then internally to similar scattered devices to fill your house with an extremely high speed Internet connection.)
The talk I heard was from a university group doing "engineering characterization" - i.e., this stuff was out of the lab and at the point where you could construct samples easily; the job now was to come up with all the design rules and tradeoff tables and simulation techniques that you need before you can build commercial products.  They thought this might be "5G" telephone technology.  Expect to see the first glimmers in, say, 5 years.
Anyway, this is (a) a confirmation of your point that computational elements are now so cheap that components like wires are worth replacing; but (b) unlike the case with the mirror controllers, we'll want to build these things in large numbers and scatter them all over the place, so they will have to make do with very small amounts of power.  (For the first inkling of what this is like, think of RFID chips - already out there in the billions.)
So, no, I don't think you can assume that efficiency considerations will go away.  If you want pervasive security in your pervasive compute architecture, you're going to have to figure out how make it work when many of the nodes in your architecture are tiny and can't afford to drain power run complicated algorithms.
                                                        -- Jerry

@_date: 2013-10-06 18:29:02
@_author: Jerry Leichter 
@_subject: [Cryptography] Sha3 
I also found the argument here unconvincing.  After all, Keccak restricted to the set of strings of the form M|suffix reveals that it's input ends with "suffix", which the original Keccak did not.  The problem is with the vague nature of "no security problem".
To really get at this, I suspect you have to make some statement saying that your expectation about last |suffix| bits of the output is the same before and after you see the Keccak output, given your prior expectation about those bits.  But of course that's clearly the kind of statement you need *in general*:  Keccak("Hello world") is some fixed value, and if you see it, your expectation that the input was "Hello world" will get close to 1 as you receive more output bits!
If the nature of the suffix and how it's chosen could affect Keccak's output in some predictable way, it would be secure.  Keccak's security is defined in terms of indistinguishability from a sponge with the same internal construction but a random round function (chosen from some appropriate class).  A random function won't show any particular interactions with chosen suffixes, so Keccak had better not either.
Yes, it would be nice to see this argued more fully.
                                                        -- Jerry

@_date: 2013-10-06 18:35:48
@_author: Jerry Leichter 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
Q.  How did God create the world in only 6 days?
A.  No installed base.
                                                        -- Jerry

@_date: 2013-10-07 05:59:37
@_author: Jerry Leichter 
@_subject: [Cryptography] Sha3 
This style of argument makes sense for encryption functions, where it's a chosen plaintext attack, since the goal is to determine the key.  But it makes no sense for a hash function:  If the attacker can specify something about the input, he ... knows something about the input!  You need to argue that he knows *no more than that* after looking at the output than he did before.
While both Ben and I are convinced that in fact the suffix can't "affect security", the *specific wording* doesn't really give an argument for why.
                                                        -- Jerry

@_date: 2013-10-07 10:46:49
@_author: Jerry Leichter 
@_subject: [Cryptography] Universal security measures for crypto primitives 
While the paper is a nicely written joke, it does get at a fundamental point:  We are rapidly approaching *physical* limits on cryptographically-relevant computations.
I've mentioned here in the past that I did a very rough, back-of-the envelope estimate of the ultimate limits on computation imposed by quantum mechanics.  I decided to ask a friend who actually knows the physics whether a better estimate was possible.  I'm still working to understand what he described, but here's the crux:  Suppose you want an answer to your computation within 100 years.  Then your computations must fall in a sphere of space-time that has spatial radius 100 light years and time radius 100 years.  (This is a gross overestimate, but we're looking for an ultimate bound so why not keep the computation simple.)  Then:  "...fundamental limits will let you make about 3*10^94 ~ 2^315 [bit] flips and store about 2^315 bits, in your century / light-century sphere."  Note that this gives you both a limit on computation (bit flips) and a limit on memory (total bits), so time/memory tradeoffs are accounted for.
This is based on the best current understanding we have of QM.  Granted, things can always change - but any theory that works even vaguely like the way QM works will impose *some* such limit.
                                                        -- Jerry

@_date: 2013-10-07 12:55:56
@_author: Jerry Leichter 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
That's a really nice idea.  It has a non-obvious advantage:  Suppose the AES round instructions (or the round key computations instructions) have been "spiked" to leak information in some non-obvious way - e.g., they cause a power glitch that someone with the knowledge of what to look for can use to read of some of the key bits.  The round key computation instructions obviously have direct access to the actual key, while the round computation instructions have access to the round keys, and with the standard round function, given the round keys it's possible to determine the actual key.
If, on the other hand, you use a cryptographically secure transformation from key to round key, and avoid the built-in round key instructions entirely; and you use CTR mode, so that the round computation instructions never see the actual data; then AES round computation functions have nothing useful to leak (unless they are leaking all their output, which would require a huge data rate and would be easily noticed).  This also means that even if the round instructions are implemented in software which allows for side-channel attacks (i.e., it uses an optimized table instruction against which cache attacks work), there's no useful data to *be* leaked.
So this is a mode for safely using possibly rigged hardware.  (Of course there are many other ways the hardware could be rigged to work against you.  But with their intended use, hardware encryption instructions have a huge target painted on them.)
Of course, Keccak itself, in this mode, would have access to the real key.  However, it would at least for now be implemented in software, and it's designed to be implementable without exposing side-channel attacks.
There are two questions that need to be looked at:
1.  Is AES used with (essentially) random round keys secure?  At what level of security?  One would think so, but this needs to be looked at carefully.
2.  Is the performance acceptable?
BTW, some of the other SHA-3 proposals use the AES round transformation as a primitive, so could also potentially be used in generating a secure round key schedule.  That might (or might not) put security-critical information back into the hardware instructions.
If Keccak becomes the standard, we can expect to see a hardware Keccak-f implementation (the inner transformation that is the basis of each Keeccak round) at some point.  Could that be used in a way that doesn't give it the ability to leak critical information?
                                                        -- Jerry

@_date: 2013-10-07 13:59:05
@_author: Jerry Leichter 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
I don't see how there can be any solution to this.  Slow renegotiation doesn't affect users until it gets to the point where they feel the "something is broken"; at that point, the result to them is indistinguishable from just refusing connections with the old suites.  And of course what's broken is never *their* software, it's the other guy's - and given the alternative, they'll go to someone who isn't as insistent that their potential customers "do it the right way".  So you'll just set off a race to the bottom.
Revoking signatures ... well, just how effect are "bad signature" warnings today?  People learn - in fact, are often *taught* - to click through them.  If software refuses to let them do that, they'll look for other software.
Ultimately, I think you have to look at this as an economic issue.  The only reason to change your software is if the cost of changing is lower than the estimated future cost of *not* changing.  Most users (rightly) estimate that the chance of them losing much is very low.  You can change that estimate by imposing a cost on them, but in a world of competitive suppliers (and consumer protection laws) that's usually not practical.
It's actually interesting to consider the single counter-example out there;  The iOS world (and to a slightly less degree, the OSX world).  Apple doesn't force iOS users to upgrade their existing hardware (and sometimes it's "obsolete" and isn't software-upgradeable) but in fact iOS users upgrade very quickly.  (iOS 7 exceeded 50% of installations within 7 days - a faster ramp than iOS 6.  Based on past patterns, iOS 7 will be in the high 90's in a fairly short time.)  No other software comes anywhere close to that.  Moving from iOS 6 to iOS 7 is immensely more disruptive than moving to a new browser version (say) that drops support for a vulnerable encryption algorithm.  And yet huge numbers of people do it.  Clearly it's because of the new things in iOS 7 - and yet Microsoft still has a huge population of users on XP.
I think the real take-away here is that getting upgrades into the field is a technical problem only at the margins.  It has to do with people's attitudes in subtle ways that Apple has captured and others have not.  (Unanswerable question:  If the handset makers and the Telco vendors didn't make it so hard - often impossible - to upgrade, what would the market penetration numbers for different Android versions look like?)
                                                        -- Jerry

@_date: 2013-10-07 18:44:53
@_author: Jerry Leichter 
@_subject: [Cryptography] Sha3 
You're assuming what the argument is claiming to prove.
Well, sure, such a thing *might* exist, though there's no (publicly) known technique for embedding such a thing in the kind of combinatorial mixing permutation that's at the base of Keccak and pretty much every hash function and block encryption function since DES - though the basic idea goes back to Shannon in the 1940's.
I will say that the Keccak analysis shows both the strength and the weakness of the current (public) state of the art.  Before differential cryptography, pretty much everything in this area was guesswork.  In the last 30-40 years (depending on whether you want to start with IBM's unpublished knowledge of the technique going back, according to Coppersmith, to 1974, or from Biham and Shamir's rediscovery and publication in the late 1980's), the basic idea has been expanded to a variety of related attacks, with very sophisticated modeling of exactly what you can expect to get from attacks under different circumstances.  The Keccak analysis goes through a whole bunch of these.  They make a pretty convincing argument that (a) no known attack can get anything much out of Keccak; (b) it's unlikely that there's an attack along the same general lines as currently know attacks that will work against it either.
The problem - and it's an open problem for the whole field - is that none of this gets at the question of whether there is some completely different kind of attack that would slice right through Keccak or AES or any particular algorithm, or any particular class of algorithms.  If you compare the situation to that in asymmetric crypto, our asymmetric algorithms are based on clean, simple mathematical structures about which we can prove a great deal, but that have buried within them particular problems that we believe, on fairly strong if hardly completely dispositive evidence, are hard.  For symmetric algorithms, we pretty much *rely* on the lack of any simple mathematical structure - which, in a Kolmogorov-complexity-style argument, just means there appear to be no short descriptions in tractable terms of what these transformations do.  For example, if you write the transformations down as Boolean formulas in CNF or DNF, the results are extremely large, with irregular, highly inter-twined terms.  Without that, various Boolean solvers would quickly cut them to ribbons.
In some sense, DC and related techniques say "OK, the complexity of the function itself is high, but if I look at the differentials, I can find some patterns that are simple enough to work with."
If there's an attack, it's likely to be based on something other than Boolean formulas written out in any form we currently work with, or anything based on differentials.  It's likely to come out of a representation entirely different from anything anyone has thought of.  You'd need that to do key recovery; you'd also need it to embed a back door (like a sensitivity to certain input patterns).  The fact that no one has found such a thing (publicly, at least) doesn't mean it can't exist; we just don't know what we don't know.  Surprising results like this have appeared before; in a sense, all of mathematics is about finding simple, tractable representations that turn impossible problems into soluble ones.
                                                        -- Jerry

@_date: 2013-10-08 10:38:50
@_author: Jerry Leichter 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
This would make a great April Fool's RFC, to go along with the classic "evil bit".  :-(
There are embedded systems that are impractical to update and have expected lifetimes measured in decades.  RFID chips include cryptography, are completely un-updatable, and have no real limit on their lifetimes - the percentage of the population represented by any given "vintage" of chips will drop continuously, but it will never go to zero.  We are rapidly entering a world in which devices with similar characteristics will, in sheer numbers, dominate the ecosystem - see the remote-controllable Phillips Hue light bulbs ( as an early example.  (Oh, and there's been an attack against them:    The response from Phillips to that article says "In developing Hue we have used industry standard encryption and authentication techniques....  [O]ur main advice to customers is that they take steps to ensure they are secured from malicious attacks at a network level."
Even in the PC world, where updates are a part of life, makers eventually stop producing them for older products.  Windows XP, as of about 10 months ago, was running on 1/4 of all PC's - many 100's of millions of PC's.  About 9 months from now, Microsoft will ship its final security update for XP.  Many perfectly good PC's will stay on XP forever because even if there was the will and staff to upgrade, recent versions of Windows won't run on their hardware.
In the Mac world, hardware in general tends to live longer, and there's plenty of hardware still running that can't run recent OS's.  Apple pretty much only does patches for at most 3 versions of the OS (with a new version roughly every year).  The Linux world isn't really much different except that it's less likely to drop support for old hardware, and because it tends to be used by a more techie audience who are more likely to upgrade, the percentages probably look better, at least for PC's.  (But there are antique versions of Linux hidden away in all kinds of "appliances" that no one ever upgrades.)
I'm afraid the reality is that we have to design for a world in which some devices will be running very old versions of code, speaking only very old versions of protocols, pretty much forever.  In such a world, newer devices either need to shield their older brethren from the sad realities or relegate them to low-risk activities by refusing to engage in high-risk transactions with them.  It's by no means clear how one would do this, but there really aren't any other realistic alternatives.
                                                        -- Jerry

@_date: 2013-10-08 14:02:43
@_author: Jerry Leichter 
@_subject: [Cryptography] Always a weakest link 
The article is about security in the large, not cryptography specifically, but  points out that many companies think that they are increasing their security by blocking access to sites they consider risky - only to have their users migrate to less well known sites doing the same thing - and those less well known sites are often considerably riskier.
My favorite quote:  "One customer found a user who sent out a million tweets in a day, but in reality, its compromised systems were exporting data 140 characters at a time via the tweets."
                                                        -- Jerry

@_date: 2013-10-10 16:22:50
@_author: Jerry Leichter 
@_subject: [Cryptography] prism-proof email in the degenerate case 
Nice!  I like it.
A couple of comments:
1.  Obviously, this has scaling problems.  The interesting question is how to extend it while retaining the good properties.  If participants are willing to be identified to within 1/k of all the users of the system (a set which will itself remain hidden by the system), choosing one of k servers based on a hash of the recipient would work.  (A concerned recipient could, of course, check servers that he knows can't possibly have his mail.)  Can one do better?
2.  The system provides complete security for recipients (all you can tell about a recipient is that he can potentially receive messages - though the design has to be careful so that a recipient doesn't, for example, release timing information depending on whether his decryption succeeded or not).  However, the protection is more limited for senders.  A sender can hide its activity by simply sending random "messages", which of course no one will ever be able to decrypt.  Of course, that adds yet more load to the entire system.
3.  Since there's no acknowledgement when a message is picked up, the number of messages in the system grows without bound.  As you suggest, the service will have to throw out messages after some time - but that's a "blind" process which may discard a message a slow receiver hasn't had a chance to pick up while keeping one that was picked up a long time ago.  One way around this, for cooperative senders:  When creating a message, the sender selects a random R and appends tag Hash(R).  Anyone may later send a "you may delete message R" message.  A sender computes Hash(R), finds any message with that tag, and discards it.  (It will still want to delete messages that are old, but it may be able to define "old" as a larger value if enough of the senders are cooperative.)
Since an observer can already tell who created the message with tag H(R), it would normally be the original sender who deletes his messages.  Perhaps he knows they are no longer important; or perhaps he received an application-level acknowledgement message from the recipient.
                                                        -- Jerry

@_date: 2013-10-11 14:22:33
@_author: Jerry Leichter 
@_subject: [Cryptography] Key stretching 
The related key attacks out there require keys that differ in a couple of bits.  If k and k' aren't related, k+k and k'+k' won't be either.
Let's step back a moment and think about attacks:
1.  Brute force.  No public key-stretching algorithm can help, since the attacker will brute-force the k's, computing the corresponding K's as he goes.
2.  Analytic attack against AES128 that doesn't extend, in general, to AES256.  Without knowing the nature of the attack, it's impossible to estimate whether knowing that the key has some particular form would allow the attack to extend. If so ... what forms?
3.  Analytic attack against AES256.  A recognizable form for keys - e.g., k+k - might conceivably help, but it seems like a minor thing.
Realistically, k+k, or k padded with 0's, or SHA256(k), are probably equally strong except under any attacks specifically concocted to target them (e.g., suppose it turns out that there just happens to be an analytic attack against AES256 for keys with more than 3/4's of the bits equal to 0).
Since you're describing a situation in which performance is not an issue, you might as well use SHA256(k) - whitening the key can't hurt.
                                                        -- Jerry

@_date: 2013-10-12 07:02:51
@_author: Jerry Leichter 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
The research is on time delays, which you could easily enough convert to round trips.  The numbers are nowhere near 20%, but are significant if you have many users:                                                          -- Jerry

@_date: 2013-10-13 20:03:18
@_author: Jerry Leichter 
@_subject: [Cryptography] Broken RNG renders gov't-issued smartcards 
How could they?  The certification has to stop at some point; it can't trace the systems all the way to end users.  What was certified as a box that would work a certain way given certain conditions.  The box was used in a different way.  Why is it surprising that the certification was useless?  Let's consider a simple encryption box:  Key goes in top, cleartext goes in left; ciphertext comes out right.  There's an implicit assumption that you don't simply discard the ciphertext and send the plaintext on to the next subsystem in line.  No certification can possibly check that; or that, say, you don't post all your keys on your website immediately after generating them.
That depends on what they were supposed to test, and whether they did test that correctly.  A FIPS/Common Criteria Certification is handed a "box" implementing the protocol and a whole bunch of paperwork describing how it's designed, how it works internally, and how it's intended to be used.  If it passes, what passes it the exact design certified, used as described.  There are way too many possible system built out of certified modules for it to be reasonable to expect the certification to encompass them all.
I will remark that, having been involved in one certification effort, I think they offer little, especially for software - they get at some reasonable issues for hardware designs.  Still, we don't currently have much of anything better.  Hundreds of eyeballs may have been on the Linux code, but we still ended up fielding a system with a completely crippled RNG and not noticing for months.  Still, if you expect the impossible from a process, you make any improvement impossible.  Formal verification, where possible, can be very powerful - but it will also have to focus on some well-defined subsystem, and all the effort will be "wasted" if the subsystem is used in a way that doesn't meet the necessary constraints.

@_date: 2013-10-16 13:58:31
@_author: Jerry Leichter 
@_subject: [Cryptography] /dev/random is not robust 
Backwards security is a prerequisite for building PFS, among other things.  Without it, if an attacker seizes your system, he can (in principle, but we're considering *potential capabilities*, not what we know how to do in detail today) "run the random number generator backwards", which would allow any keys that were created using the RNG - like those created in "secure" DH negotiations, for example - to be generated.  (But keep in mind the adage:  Attacks only get better.)
I'm amazed and disturbed by the nature of the responses to this paper.  They are *indistinguishable* from the typical PR blather we get from every commercial operation out there when someone reports a potential attack:  It's just theory, it can't be translated into practice, we have multiple layers of security so even if one of them can be attacked the others still protect you, yadda yadda yadda.
Yes, this is a theoretical attack.  Yes, the Linux RNG is much more complex than the attackers assume in their model.  (Complexity is a good thing?)  No, no one is likely to be able to be able to use the attack actually get much out of the Linux RNG.  But attacks only get better.  The fact is, the Linux RNG, like all the "stir entropy into the pool" RNG's out there, were developed in an essentially ad hoc fashion, without even a solid idea of what the desired properties of such a primitive are supposed to be.  This paper is a step along a path begun in 2005 by Barak and Halevi (the instant paper has extensive references), and, frankly, it's about time.  For such RNG's we've been in about the same position we were in for ciphers in the 1980's and even beyond, before a great deal of theoretical effort got us to characterizations like IND-CPA and all sorts of excellent work on tight reductions and concrete security.
There's always been a strain of anti-academy bias - even anti-intellectualism - in the OSS community.  It's highly undesirable.  There's good academic academic work, and there's bad academic work.  Even with the domain of good academic work, some is of practical interest today, and some isn't.  (And some that isn't of interest today may be tomorrow.)
This kind of "shoot the messenger" approach is just plain wrong.  Look at the definition of robustness they come up with and tell me what parts of it aren't things you'd *like* to get in your RNG, if you could.  Can you come up with anything beyond hand-waving to show that the Linux RNG actually provides you with those properties?  Suppose someone was able to build on the current paper's work and design a Linux RNG that actually *did* provide those properties, with performance comparable to what we have today?  That's how, for example, we've gotten beyond the old, ineffective Modes of Operation to modern ones that have actual security techniques.  Wouldn't it be nice to be able to make the same transition for RNG's?  How will we ever do so without the initial work of the theoreticians to provide a target to aim for?
                                                        -- Jerry

@_date: 2013-10-16 17:10:00
@_author: Jerry Leichter 
@_subject: [Cryptography] /dev/random is not robust 
Can we separate the issue of actual issues in the Linux RNG from the responses to this paper?
It's been a long time since I worked on an RNG, for a product that ran on systems that provided nothing like /dev/random; our Linux version mixed in /dev/random, and our Windows system mixed in whatever Windows calls its random number provider.  I have no doubt that the Linux RNG is immensely better than the hack we were able to cobble together - not being in the kernel severely restricted our entropy sources.  From what I've seen of the Linux RNG, it does a pretty good job.  I'm not about to suggest any code changes, and frankly nothing in this paper suggests any either.  Following its guidelines would produce an entirely new RNG, which absolutely *no one should trust* until it's been kicked around in the community for a while.
My problem is *entirely* with some of the intemperate responses I've seen out there, which basically say "Oh, that's just some academic nonsense, pay no heed".  I've seen nothing of that sort from you, Theodore Ts'o, personally; while you clearly have an emotional attachment to the work you've done - don't we all? - your responses are mainly technical in nature, pointing out places where the paper misunderstood what the current implementation does.  However, you do let a bit of an attitude come through in remarks about "not caring about publishing papers".  As I said, there is good and bad academic work, and not even all the good academic work is actually useful.  As an example of good and useful academic work, I usually point to Phil Rogoway's papers.  While the papers themselves are highly technical and get into a great deal of detail, what comes out at the end are simple designs that can be, and have been, implemented.  If you implement one of Rogoway's designs, you know there's a formal proof behind it.  You don't need to actually understand the proof - others have reviewed it and it's almost certainly solid.  Merge sort will run in O(n log n) for you whether you understand the theory or not.
I see the paper as valuable for proposing strong security definitions for "PRNG's with input", showing that neither Barak and Halevi's algorithm nor the Linux RNG's algorithm attain those definitions, but suggesting an algorithm that does.  The answer "well, yes, the Linux generator fails if its entropy sources are bad in a particular way, but we have entropy sources that aren't" misses the point.  At one time, not so very long ago, no one knew how to build a cipher secure against a known-plaintext attack.  Today, that's assumed.  A defense of a modern cipher as "well, we won't let anyone see the plaintext" isn't good enough.  (Even worse is the claim that "you can only see the state of the PRNG from root, and then there are other attacks".  This isn't even true - a Linux system frozen into a VM can't prevent anyone from reading that state if they want it hard enough.)
Just as they extended Barak and Halevi's definitions, others may well come along and "tune" theirs.  That doesn't make theirs, or Barak and Halevi's, work "wrong"; it just makes them incomplete.  Others will also eventually come along and produce better algorithms that attain the various definitions that the community comes to agree on.
I'm not sure how the whole business of entropy estimation feeds into this.  There are others who've criticized it as just guesswork.  Frankly, they have a point.  John Denker's work on Turbid provides a much more principled approach to the problem.  Still, the Linux kernel has to work with what it has.
Whether new, better, strong approaches will come along in the future is impossible to tell.  If they do, I would hope those engineering Linux and other systems will have the humility to admit that the direction they've been going was, as it turns out, incorrect, and that they should start again from scratch.  That's how science and engineering are supposed to work.
Random numbers are way too important to allow the kinds of dismissive responses we've been seeing.
                                                        -- Jerry

@_date: 2013-10-17 11:55:23
@_author: Jerry Leichter 
@_subject: [Cryptography] funding Tor development 
It's also easier to find sources of smaller amounts.
I couldn't come close to funding $10K/month, but might be talked into $10/month.
                                                        -- Jerry

@_date: 2013-10-17 13:53:11
@_author: Jerry Leichter 
@_subject: [Cryptography] /dev/random is not robust 
One answer to this question appears in the FIPS standards for RNG's.  At times, they've required a continuous on-line test of the numbers being generated, with automatic shutdown if the test fail.  These requirements almost certainly came from the hardware background of the FIPS standards.  For hardware, certain failure modes - stuck at 0/stuck at 1 are the most obvious; short cycles due to some internal oscillation may be another - are extremely common, and worth checking for.  For software-based deterministic PRNG's, such tests are mainly irrelevant - code doesn't develop such failures in the field.  As the FIPS standards were adjusted for a more software-based world, the requirement for on-line testing was dropped.
Looking through some old messages on the subject here on the Cryptography list, I found one from Francois Grieu back in July of 2010:
More recently, David Johnston, who I gather was involved in the design of the Intel on-chip RNG, commented in a response to a question about malfunctions going undetected:
Of course, with generators like the Linux /dev/random, we're in some intermediate state, with hardware components that could fail feeding data into software components.
My own view on this is that there's no point in testing the output of a deterministic PRNG, but the moment you start getting information from the outside world, you should be validating it.  You can never prove that a data stream is random, but you can cheaply spot some common kinds of deviation from randomness - and if you're in a position to "pay" more (in computation/memory) you can spot many others.  You have no hope of spotting a sophisticated *attack*, and even spotting code bugs that destroy randomness can be hard, but it's hard to come up with an example of an actual real-world hardware failure that would slip through.  So you might as well do the testing.
                                                        -- Jerry

@_date: 2013-10-20 10:22:16
@_author: Jerry Leichter 
@_subject: [Cryptography] Mail Lists In the Post-Snowden Era 
Perry re-started this mailing list in response to the Snowden revelations about the NSA's attacks on Internet cryptography.  He raised the questions of whether we could make a Prism-proof Internet.
That's a big problem, and we've been debating small pieces of it ever since.  I'd like to suggest a smaller problem, just as a kind of rallying point.
This list has certainly attracted NSA interest.  Whether by subject, by keyword matching, or because so many of the participants here are clearly "adversaries" of those in the NSA responsible for gaining access to all that crunchy good stuff out there, there's no way anyone here could avoid scrutiny.
So ... imagine we don't like that.  How could this list be constituted in a "secure" way?  The quotes are on "secure" because even the definition of the word isn't clear.  Realistically, there's no way to avoid an NSA "plant" joining an open group, so perhaps there's little point in encrypting the messages.  Anonymous/pseudonymous posting?  Signed messages?  (A few members post them; hardly any of us do.)  Does that just make messages even more traceable/linkable?
We think we know what it means to have secure 1-1 email.  Adding a couple of additional participants seems as if it leaves the nature of the problem unchanged, but in fact you very quickly get into trust issues:  In a 1-1 conversation, I can decide whether to trust my correspondent.  In any multi-party conversation, it's likely that at least one of the participants doesn't know *all* the others, so must make an indirect trust decision:  I trust him because the others here seem to trust him.  For a mailing list, this problem explodes - while in addition there are all kinds of issue with how exactly to set up key exchanges.  A moderated group like this could be looked at as a star configuration:  In a way, each participant really only communicates with the moderator.  But that seems to miss the point - despite the moderation, this group *feels* like a free-flowing conversation among a group of people.
So what would a reasonable security model for the Cryptography list look like?  Is it inherently just an open discussion?  Or could we come up with something else?  If we can do more, what kind of software would be needed to make it as free-flowing and easy to participate in and manage as the current list?
                                                        -- Jerry

@_date: 2013-10-21 15:35:53
@_author: Jerry Leichter 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
...and this is the difference between real-world cryptanalysis and mathematical cryptanalysis.
Mathematical cryptanalysis, given infinite time and resources, can do nothing about OTP.
Real-world cryptanalysis, given sufficient, very finite, time and resources, can break into secure facilities and computers, bribe or blackmail people, tap into various kinds of leakage channels ... and break OTP.
                                                        -- Jerry

@_date: 2013-10-21 18:06:53
@_author: Jerry Leichter 
@_subject: [Cryptography] "Death Note" elimination for hashes 
Nice idea for how to publish a proof that a hash function should no longer be used, not so nice idea about how to tell a user:
(And other similar examples.)
Sorry, but hardly anyone will read this; most of those that do won't really understand what it means; and, in general, it will just piss users off.  You broke the Internet for them.  All this verbiage doesn't make it any better.
The *only* message you could give users that has a hope of drawing a reaction other than "they broke it" is one along the lines of:  "Your software is out of date.  Click here to update it.  You will not be able to communicate with  until you do so."
I understand that the problem you're attacking is one where there *is* no update.  Well ... guess what.  That's the way things are.  The idea that you can somehow magically get someone to upgrade them by pissing off enough naive users is just wrong.
Security is important, but the fact is that if you *ask* people whether they would choose to be locked out of their on-line banking accounts for some indefinite period, or be able to access their account at some small risk, you'll find hardly anyone who wants to be locked out.
The vast majority - hundreds of millions - of Android devices out there run versions of Android with known, sometimes severe, security bugs.  They can't, and won't, be upgraded.  Is your best response "well, brick 'em all"?
                                                        -- Jerry

@_date: 2013-10-21 21:34:22
@_author: Jerry Leichter 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
No, you're missing the point.
Mathematical cryptanalysis is about breaking highly specific idealized models using a particular set of techniques - usually formalized as something along the lines of "a probabilistic Turing machine receiving X, Y and Z on its input tape and running for no more than T seconds has a probability of less then epsilon of producing a result of a particular kind."
Real-world cryptanalysis includes the possibility that someone planted a camera in the ceiling above the desk where you do the paper-and-pencil computation of your perfect OTP cipher.
Mathematical cryptanalysis gives you an upper bound on the difficulty of the problem.  There's not much point in using a system if the upper bound you get is small relative to the value of what you're protecting.  But once it becomes large enough, measuring the mathematical difficulty becomes much less useful, as the vulnerabilities are elsewhere.
A true one-time-pad is completely secure against mathematical cryptanalysis, but that fact tells you almost nothing of interest about the security of a real-world system that uses a one-time-pad.  Remember how we got here:  From the initial claim that any system can be attacked given enough resources, to the counterclaim that a one-time-pad could *not* be attacked, to my counter-counter claim, just repeated here, that any real-world implementation *can* be attacked, given sufficient resources.  It's not a claim about the particular weaknesses of particular realizations of one-time-pads; it's a claim that *some* weakness will always be present.  The best you can do is make the cost of attacking that weakness exceed the value of whatever you're protecting.  *That* is the real-world analogue of the "perfect (mathematical) security" of a one-time-pad.
                                                        -- Jerry

@_date: 2013-10-23 06:14:53
@_author: Jerry Leichter 
@_subject: [Cryptography] programable computers inside our computers (was: 
Actually, there is a difference:  Palladium had remote attestation built in - it was a selling point.  People concentrated on that as the "bad" part, thought the rest could actually be useful.  The reference designs let you do whatever you wanted with your own device - you have full access to the trusted elements, could sign your own boot loader if you wanted.  Of course, someone providing DRM'ed material could refuse to talk to your system if it didn't attest to running "acceptable" code.
The new technologies don't build remote attestation in, so avoid the whole debate.  And the base technologies are neutral on the issue of whether you can write your own trusted code.  It's the specific implementations that block you from changing the keys, the bootloader, any of the code running in the secure element, etc.
The net effect is similar.  Nothing keeps a system builder from including remote attestation, but because of the nature of the devices, who is doing the controlling (the cell service providers), and the much higher level of integration of the components (making it harder to pull pieces out of the controlled environment) it really doesn't much matter:  If you're successfully talking to the cell network at all, they assume you have "approved" hardware. (Should people start building their own cell hardware from the ground up - certainly possible if you don't care about how practical the device is as a *cell phone*, but extremely difficult if you want something practical - they could always add remote attestation, or some simplified variant that's good enough for the cell provider's purposes, later.)
Palladium was subject to political attack because it was open about what it could do for DRM suppliers.  The new technologies are harder to attack this way because the responsibility is diffused, and the good and the bad are very thoroughly mixed together.  The availability of secure modes in the hardware can be explained as necessary to allow for safe operation in an unsafe world, and in and of themselves harmless - just a safer extension of user space/kernel space isolation.  The system builders build things to keep the systems safe from malware, a known and growing problem.  The network providers want to protect their networks.  Everyone sees the need for heavy protection - including from the device owner - of internal "wallets".
                                                        -- Jerry

@_date: 2013-10-24 12:40:31
@_author: Jerry Leichter 
@_subject: [Cryptography] "Death Note" elimination for hashes 
Who's the "you" in this process?  Some collection of parties, during the entire useful lifetime of the hash, is in possession of information that can disrupt the entire system.  Those parties will be marked for attack by anyone interested in causing trouble.  Alternatively, they will be targets for anyone interesting in *blocking* attempts to kill off the hash - perhaps because they themselves have found a break.
It's easy to imagine such a system if your time horizon is a year or two.  But for time horizons measured in decades, things are much, much harder.  Either individuals have to pass along their responsibilities to others - so you have a transitive trust issue; I may have trusted the guy I originally nominated, but do I trust the guy who replaced the guy who replace him? - or you have to trust an organization, which over a couple of years may no longer even resemble the organization I put my trust in at the beginning.
Actually, the reaction attacks highlight an entirely different issue:  Just what is it you are signing the death-note on?  The problem here isn't AES - it's CBC.    Suppose your entire suite consisted of XXX-CBC, for a 15 different XXX ciphers, all of them good.  The problem in CBC has now killed them all.  The obvious response is, "sure, but why would I use CBC mode for everything?" - but without knowing where the next attack might come from, how can you guess what dimensions you need to vary along?  Maybe the next attack is some clever timing attack that hits all the ciphers, regardless of mode, rather than one mode, regardless of cipher.
There is a fundamental problem with getting existing software upgraded.  There are two fundamental approaches you can take:
1.  Pre-load the system with a bunch of alternatives, marking one at a time as the primary.
2.  Allow for the distribution of updates to add new alternatives as required.
Method 2 is what we do today, except that the way we do it is to replace the entire piece of software within which the crypto lives.  Our success rate in doing this isn't particularly notable.  Stepping back, the products that *do* see rapid updates do so through some form of brute force:  Chrome updates itself, giving you little choice in the matter; iOS updates directly from Apple, bypassing all the intermediaries - and Apple has been very successfully in getting end users to update quickly, through a variety of mechanisms.  Microsoft has gotten more rapid updates in place through pushing them really hard, but there even today its penetration rates for new versions are nowhere near Chrome or iOS.
Method 1 is what is being proposed here.
There's little difference in principle between the two methods.  If there were a way to push out updates *just to the crypto algorithms*, the two would come even closer together - and there would be much less reluctance to update because the product itself presumably would have no visible changes, it would just use new crypto to do what it had always done.
The details of possible attacks differ.  With Method 2, a evil push of a new algorithm could remove all the security.  With Method 1, the evil algorithm would have had to be present from the start.  How different this is, in practice, is hard to say.  The corresponding attack for Method 1 - as I noted above - is a denial of service attack, which knocks out the good algorithms until the system is forced to use a bad one which slipped into the set, or just plain runs out - and must fall back on Method 2.  (Note that this would be very good for a clever attacker:  Unlike a system that used Method 2 on an everyday basis, a system based on Method 1 is likely to be pushed into quickly throwing together a way to keep the system running.  This provides the perfect environment for "helpers" to come along and slip stuff in to an update.)
Method 1 in "easy" cases - where one can publish a checkable witness to a break - seems to avoid needing to trust the party that says "OK, go change things now".  But that's a bit of an illusion.  You'd think that a pre-image would be such a witness for a hash, but anyone can generate a hash/pre-image pair - they just start from the pre-image!  The only way such a pair is a witness is if there's some way of being sure that the process started with the hash - and that's not so easy.  The only methods I can think of are (a) publish the actual cracking algorithm - which likely will require too many resources to be practical as a witness; (b) publish a pre-image that was kept secret until now, which requires trusting someone to have *held* that secret.
While clever, I just don't see what problem death notices actually solve.
                                                        -- Jerry

@_date: 2013-10-24 12:55:00
@_author: Jerry Leichter 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
As long as you're at it, ask a whole bunch of hosts, close and far, for 256 random bits from their own generators.  If even a single one of the response slips by an attacker, he's lost.
This is a process you can repeat periodically - and certainly at each boot - except that after the first time, you can use secure connections, with the best security you are able to set up with each particular host.  An attacker then would have to be able to not just see all the responses but also decrypt them.
There are no absolute assurances here, but you're inverting the usual attacker/defender relationship.  Usually, an attacker only has to find one hole, while a defender has to close all of them.  Here, the attacker has to grab *every* piece of the incoming information, without fail and repeatedly, while the defender doesn't really care whether all its sources respond with valid random data, or for that matter respond at all.  (In fact, an interesting approach in some situations might be to ask a bunch of sources to *flood* you with responses, knowing that your hardware will unpredictably drop some of them.  An attacker will have trouble knowing which ones made it through and which one's didn't; using "extra" information is just as bad as missing some information you *did* use.  I would want to know something about more about the nature of the transmission medium and the receiver before using this, though, as the way a given piece of hardware responds to overload *might* be predictable.)
                                                        -- Jerry

@_date: 2013-10-24 16:14:48
@_author: Jerry Leichter 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
Anything you can connect to, by any means, that can transfer random bits to you.
The point of my comment is to counter the usual claim that getting anything through the network isn't helpful because your attacker may be watching everything go by on the network.  In fact, in many situations, it's extremely difficult to watch everything going by, and even if you as the attacker can, the node trying to grab data of the network may only grab some of it and it may be difficult to tell *what* it grabbed.
I'll be the first to admit that all if this is Denker's "squish":  You don't know how to predict it, but you can't prove it's unpredictable either.  I'd much rather have a vetted generator based on shot noise or something of that sort.  But if I *don't* have that, I have to do *something*.  Starting with the assumption of an omniscient attacker who has unfailing access to everything I can see or do leads to no solutions at all.  But it's also unrealistic in virtually all situations.  Attackers have some variation of a wiretap channel:  They see what you see but with more random errors, or perhaps just with *randomly different* errors.  If you can estimate the magnitude of the difference between your view and theirs, you can leverage it to move your state exponentially further away from their model of it, eventually leaving them with no useful information.
                                                        -- Jerry

@_date: 2013-10-26 07:29:58
@_author: Jerry Leichter 
@_subject: [Cryptography] mailinator? 
I've used it ... but what purposes do you have in mind?  If you're thinking of using it as a quick way to quickly implement the "encrypt and broadcast to everyone" anonymous mail system ... it would quickly collapse.  The description of mailinator says that it's purely an in-memory system, with limited capacity.  For its intended usage - mail addresses where you just need something that will receive mail, you never intend to read it; or where there is one message - e.g., a confirmation of sign-up that you will grab and respond to within minutes, and then never care about again - that's fine.  When memory fills, older messages are discarded - at this point, after "a few hours".  There are already protections against too much crud being received - attachments are discarded, there's a size limit.
Did you have something else in mind?
                                                        -- Jerry

@_date: 2013-10-28 16:50:01
@_author: Jerry Leichter 
@_subject: [Cryptography] Randomness from network hardware? 
Ethernet is supposed to back off randomly after a collision.  There's no need for a strong notion of randomness, but the algorithm does have to introduce enough variation between stations that two of them will never end up following the same sequence of backoffs at the same time, or the algorithm won't work.
Is there any way to get access to whatever source of randomness drives this decision?  Old interfaces - I'm talking the original "yellow cable 10Mb/sec" stuff - used to have a test mode that would simulate a collision.  I think you could even force one.
I haven't looked at Ethernet hardware in many years/generations of the standards.  Do current interfaces, perhaps in some test mode (which a special driver could get at during boot), provide access to anything that could be used as (part of) a random seed?
                                                        -- Jerry

@_date: 2013-10-30 14:09:20
@_author: Jerry Leichter 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
Maybe.  Or maybe we just see a misapplied reasonable principle that any input that could affect sensitive data must be authenticated.
"Never attribute to malice what can be explained by incompetence."  One of the really bad things about the NSA's apparent attempts to subvert crypto is that it leads you to question this assertion.  We just have no way of knowing.
                                                        -- Jerry

@_date: 2013-10-30 16:27:06
@_author: Jerry Leichter 
@_subject: [Cryptography] Standard exponents in RSA 
Actually, that would be 1.
Exponent 0 is the equivalent of using the message itself as its own one-type-encryption pad.  Martin Minow, many years ago, wrote this up with a completely straight face for an April 1 issue of some crypto journal.  All sorts of nice analysis - such as that this was an ideal one-time-pad, as it added exactly as much randomness to each character of plaintext as there was information in that character.  It also had the unique advantage that the output was highly compressible.  (The article was published, and drew a bunch of letters from people who didn't check the date....)
                                                        -- Jerry

@_date: 2013-10-30 17:00:33
@_author: Jerry Leichter 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
Are you so sure?
Consider a Linux-style RNG.  Suppose I know that all the existing sources produce k bits/second of randomness.  If I draw k bits/second of data out of it, after a while, it has no "spare" randomness inside - it's giving me exactly what it has.  If I draw j >> k bits/second out of it, it quickly "runs out".  It may block, effectively rate-limiting me; or it may stretch what it had.
Now suppose I inject j >> k bits of my own, controlled data, declaring that it represents j bits of entropy - all the while continuing to draw j bits out.  The generator now has plenty of entropy - or thinks it does - so never blocks.  But eventually it must be the case that I'm getting way more bits out than the real entropy going in.  If I can't predict the bits I'm getting out, it can only be because of the lingering entropy from the other sources.  (If j is much larger than k, then most of the bits I get out are computed without any bits other than my own going in.)
But this is an odd state of affairs.  If the assumption is that the results remain unpredictable, no matter how much larger j is than k, then why should the generator *ever* block because it's output more bits than it got in?  After all, that situation is effectively indistinguishable from it having gotten all 0 bits at some very high rate.
So:  For extra sources to always be harmless, it must be the case that the bits are unpredictable *even if no new entropy arrives*.  All that matters, in effect, is that the internal state be unknown and unpredictable *once*.  BBS has this property, as (on different assumptions) do crypto-based PRNG's like Yarrow.  But this has a performance cost, and I'm not sure that a Linux-style generator does.  If you have it ... why would you need to allow additional (allegedly random) sources?
                                                        -- Jerry

@_date: 2013-10-31 06:49:16
@_author: Jerry Leichter 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
Ah, so like FIPS, Linux only accepts "real" entropy from "authenticated" sources.  :-)
                                                        -- Jerry

@_date: 2013-10-31 11:09:38
@_author: Jerry Leichter 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
It's become very difficult to know where sound engineering practice ends and tin-hat paranoia begins.  I'd like to propose one way to get at what reasonable boundaries are:  I'll lay out a particular, I hope realistic, proposal, and then ask the group to play attacker.  After all, the best way to understand what's needed for defense is to try to attack.  To that end:
I am the owner of a data center.  I have some hundreds of physical machines, with old ones being decommissioned and new ones being added periodically.  These run thousands of VM's that are created and destroyed much more frequently.  The machines are interconnected by a number of high-speed (say 10GigE or better) networks, further partitioned for security purposes into multiple VLAN's.  None of these networks are visible outside the boundaries of my data center.  Connection to the Internet is via bastion routers.  There's a DMZ, all the usual stuff.  I have excellent physical security, and I regularly "patrol the perimeter", as well as the internal physical plant, so I assume that any attempt to run additional network links into my data center, plant significant hardware, etc., will be noticed fairly quickly.  I'm actually paranoid enough that I've spec'ed conductive mesh throughout the outer structure, so the data center is effectively inside a Faraday cage (i.e., no magic radio connections to the outside), and of course I check this periodically as well.  As a result of all this, I choose not to consider physical threats; nor, because ultimately there's nothing I can do about them, on-going sophisticated insider attacks.
I use a very simple random number generator:  There's a "random pool" of a few thousand bits.  At first boot, it's initially populated using whatever I can get my hands on principally the MAC address, any serial numbers available, etc.  At shutdown, the state is stored locally; at restart, it's restored to the previous state.  The first boot logic will wait for a significant period of time to gather entropy as I'll describe in a moment before allowing any processes that use random numbers to proceed.  (Reboot may also wait a bit, but not nearly as long.)
There is *exactly two* sources of entropy.  The primary one is watching the network:  Each time a network packet arrives, mix into my pool the full contents of the packet, the value of the real-time clock, and the value of the CPU cycle counter.  The secondary source is other machines in the datacenter:  Each machine will periodically get from some fraction of other machines scattered through the network some number of random bits, delivered in an encrypted packet using a key agreed upon between the two (based on their own randomness), and also mix that in.  The mixing function is like that in the Linux RNG - "good enough but fast enough" - but not cryptographically secure as that's too expensive.  To make up for that, periodically (say every millisecond or so) I also scramble the pool using a cryptographically secure 1-way hash.
Whenever I've delivered more than (arbitrary number) 1/3 of the bits in the pool, I won't deliver any more until I've (a) done a cryptographically secure hash; (b) received at least N network packets; (c) received at least K updates from other machines.  If network packets aren't arriving fast enough, I'll start listening promiscuously.  If that doesn't give me enough, I can start sending "please send me a random update" requests to nodes I know about, which generates both those updates, and traffic.  (During the first boot the "wait long enough" logic actually isn't based on time but on number of packets received.)
OK, this is the classic setup in which all my apparently-random data comes from a network that *could* be observed.  The question is:  Can you mount a *plausible* attack?  Keep in mind that (a) there's no way you could send all the data, along with the timing information, from a fast intra-datacenter link out to a WAN without getting noticed.  Hell, you can't even send it *over the intra-datacenter link itself* without being noticed, as you'd be more than doubling the traffic - *and your own traffic would then also be input to the RNG"; (b) Simply *recording* it all does you no good, as you still need a way to exfiltrate it - and the data quantities would be absurdly large.  (As part of my physical security, I make sure that no disks or SSD's ever leave the facility without being physically destroyed - SOP at all good-quality data centers today.)  (c) So it seems you have to analyze the data on the spot - get software running on some machines to maintain models of the state of others.  But that doesn't seem very plausible either - the modeler will have to run continuously on a great deal of data.  How much of the aggregate memory, compute power, and network I/O can you grab without being noticed?  Further, the exchange of randomness among machines means that you can't just focus on any one machine:  In effect, there is a pool consisting of *all* the machines.  If you grab the state of one, it'll soon be mixed with the state of several others, which in turn are mixed with the state of several others, and so on.
Note that attacks which start off assuming you have root on all the machines are out of bounds:  If you have that, you can do much easier things, like take over the RNG call and return data of your own choosing.  Nothing in the RNG design can protect against this, so while it's a plausible attack, it's one that will have to be dealt with in some other way.
So - there's my "data center with secure random number generators".  Let the attacks begin.
                                                        -- Jerry

@_date: 2013-10-31 14:18:40
@_author: Jerry Leichter 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
Jeez, don't people recognize smileys any more?
                                                        -- Jerry :-)

@_date: 2013-10-31 15:15:43
@_author: Jerry Leichter 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
No.  As you say, they already come with their own random sources, and everyone argues about how good they are.  So I'm positing a system modified to use a particular RNG approach that "everyone knows" doesn't work, and then asking "OK, we all believe this isn't good enough - can we construct a plausible attack?"
Yes, but can we construct a plausible attack?
For the first ten machines I bring up - just after the company founding party - I and my partners toss coins to provide the seed entropy.  Or for a more spectacular send-off, we go to the bank, get a pile of a hundred or so bills in multiple currencies, pull a couple per system out of a hat, enter their serial numbers as the seeds, then burn all the bills and stir the ashes.  :-)
(And then go to jail for destroying US currency?)
After that, everything proceeds as I outlined.
                                                        -- Jerry

@_date: 2013-10-31 15:43:49
@_author: Jerry Leichter 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
Damn; I keep coming back to old technology from back when I worked at this level.  Yes, forget promiscuous mode.
Or maybe what's needed is a "beacon":  A system out there that grabs traffic - even from the Internet to make sure there's always stuff flowing, cryptographically hashes it, and sends it to a multi-cast address.  Use by L2 and L3 multi-cast as they'll have very different propagation properties and dependencies.
I'm deliberately assuming exactly two sources of entropy:  Contents and detailed timing of network packets; and allegedly-random bits delivered from other machines.  The problem at first boot is that (a) in a switched environment, I won't be seeing any real traffic; (b) I don't know who to ask for my random bits.
So actually the "beacon" should be done differently.  Every existing system that already has access to randomness, will periodically multicast some "random bits".  A newly booted system is able to see this stuff (it will have to know where to look, of course).  The bits themselves aren't particularly useful, but the timing information should be.  But you can bootstrap:  Once you think you've seen enough timing information to be hard to predict, start using your own generator to decide whether to include or skip packets as they arrive.
                                                        -- Jerry

@_date: 2013-10-31 22:22:17
@_author: Jerry Leichter 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
============================== START ==============================
Thank you.
And so on, with suggestions that in the real world are excellent.
But that misses the point of this exercise.
Every time there's a discussion of RNG's, someone proposes using network timing information, and someone else responds no, you can't use that, an attacker can watch the network and learn or influence enough to ruin your generator.  I'm suggesting that those who adhere to one view of the other move away from abstractions and *prove it*, in the way anything in engineering and, in particular, practical cryptography - as opposed to mathematics - is proved:  You try to make it fail every way you can, deploying theory to help you bound the space of possible attack and failure modes.  If, after enough effort, you haven't been able to make it fail, you declare it "good enough".
So ... I've proposed a *particular* fairly realistic setup with a system engineered in a *particular* way to make use of alleged entropy from the network.  It doesn't matter that the system could have been designed in a different way:  Imagine that it stands in front of you, a structure of brick and steel, silicon and glass, bits and bytes.  Can you attack it?  If, after careful analysis, you decide you can't - well, Turbid is a really nice system and I very much appreciate the design and analysis that went into it, but if I go out to build a datacenter from off-the-shelf parts today, my hardware won't include the (fairly minor) bits and pieces needed to implement Turbid on every physical machine - and my virtual machines will have no hope at all.  But I *will* have everything needed to implement the system I've described.
                                                        -- Jerry

@_date: 2013-09-01 07:11:06
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA and cryptanalysis 
Well, sure.  But ... I find it hard to be quite so confident.
In practical terms, the vast majority of encrypted data in the world, whether in motion or at rest, is protected by one of two algorithms:  RSA and AES.  In some cases, RSA is used to encrypt AES keys, so an RSA break amounts to a bypass of AES.  If you want to consider signatures and authentication, you come back to RSA again, and add SHA-1.
This is not to say there aren't other techniques out there, or that new ones aren't being developed.  But to NSA it's clearly a game of numbers - and any kind of wedge into either of just two algorithms would expose huge amounts of traffic to interception.
Meanwhile, on the authentication side, Stuxnet provided evidence that the secret community *does* have capabilities (to conduct a collision attacks) beyond those known to the public - capabilities sufficient to produce fake Windows updates.  And recent evidence elsewhere (e.g., using a bug in the version of Firefox in the Tor Browser Bundle) has shown an interest and ability to actively attack systems.  (Of course, being able to decrypt information without an active attack is always the ideal, as it leaves no traces.)
I keep seeing statements that "modern cryptographic algorithms are secure, don't worry" - but if you step back a bit, it's really hard to justify such statements.  We *know*, in a sense, that RSA is *not* secure:  Advances in factoring have come faster than expected, so recommended key sizes have also been increasing faster than expected.  Most of the world's sites will always be well behind the recommended sizes.  Yes, we have alternatives like ECC, but they don't help the large number of sites that don't use them.
Meanwhile, just what evidence do we really have that AES is secure?  It's survived all known attacks.  Good to know - but consider that until the publication of differential cryptanalysis, the public state of knowledge contained essentially *no* generic attacks newer than the WW II era attacks on Enigma.  DC, and to a lesser degree linear cryptanalysis not long after, rendered every existing block cipher (other than DES, which was designed with secret knowledge of DC) obsolete in one stroke.  There's been incremental progress since, but no breakthrough of a similar magnitude - in public.  Is there really anything we know about AES that precludes the possibility of such a breakthrough?
There's a fundamental question one should ask in designing a system:  Do you want to protect against targeted attacks, or do you want to protect against broad "fishing" attacks?
If the former, the general view is that if an organization with the resources of the NSA wants to get in, they will - generally by various kinds of bypass mechanisms.
Of the latter, the cryptographic monoculture *that the best practices insist on* - use standard protocols, algorithms and codes; don't try to invent or even implement your own crypto; design according to Kirchoff's principle that only the key is secret - are exactly the *wrong* advice:  You're allowing the attacker to amortize his attacks on you with attacks on everyone else.
If I were really concerned about my conversations with a small group of others being intercepted as part of dragnet operations, I'd design my own small variations on existing protocols.  Mix pre-shared secrets into a DH exchange to pick keys.  Use simple steganography to hide a signal in anything being signed - if something shows up signed without that signal, I'll know (a) it's not valid; (b) someone has broken in.  Modify AES in some way - e.g., insert an XOR with a separate key between two rounds.  A directed attack would eventually break all this, but generic attacks would fail.  (You could argue that the failure of generic attacks would cause my connections to stand out and thus draw attention.  This is, perhaps, true - it depends on the success rate of the generic attacks, and on how many others are playing the same games I am.  There's no free lunch.)
It's interesting that what what little evidence we have about NSA procedures - from the design of Clipper to Suite B - hints that they deploy multiple cryptosystems tuned to particular needs.  They don't seem to believe in a monoculture - at least for themselves.
                                                        -- Jerry

@_date: 2013-09-01 16:33:56
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA and cryptanalysis 
We know they *say in public* that it's acceptable.  But do we know what they *actually use*?
Same problem.
                                                        -- Jerry

@_date: 2013-09-02 00:06:21
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA and cryptanalysis 
I'll make just a couple of comments:
- Given the huge amount of material classified these days, SECRET doesn't seem to be a very high level any more, whatever its official definition.  TOP SECRET still means a great deal though.  But the really important stuff is compartmented (SCI), and Suite B is not approved for it - it has to be protected by unpublished Suite A algorithms.
- To let's look at what they want for TOP SECRET.  First off, RSA - accepted for a transition period for SECRET, and then only with 2048 bit moduli, which until the last year or so were almost unknown in commercial settings - is completely out for TOP SECRET.  So clearly they're faith in RSA is gone.  (Same for DH and DSA.)  It looks as if they are betting that factoring and discrete logs over the integers aren't as hard as people had thought.
The whole business of AES-128 vs. AES-256 has been interesting from day one.  Too many recommendations for using it are just based on some silly idea that bigger numbers are better - 128 bits is already way beyond brute force attacks. The two use the same transforms and the same key schedule.  The only clear advantage AES-256 has is 4 extra rounds - any attack against the basic algorithm would almost certainly apply to both.  On the other hand, many possible cracks might require significantly heavier computation for AES-256, even if the same fundamental attack works.  One wonders....
NSA also wants SHA-384 - which is interesting given recent concerns about attacks on SHA-1 (which so far don't seem to extend to SHA-384).
I don't want to get into deep conspiracy and disinformation campaign theories.  My read of the situation is that at the time NSA gave its approval to this particular combination of ciphers, it believed they were secure.  They seem to be having some doubts about RSA, DSA, and DH, though that could be, or could be justified as, ECC being as strong with much smaller, more practical, key lengths.
Now, imagine that NSA really did find a way in to AES.  If they were to suddenly withdraw approval for its use by the government, they would be revealing their abilities.  A classic conundrum:  How do you make use of the fruits of your cryptanalytic efforts without revealing that you've made progress?  England accepted bombing raids on major cities to keep their crack of Enigma secret.  So the continuation of such support tells us little.  What will be interesting to see is how long the support continues.  With work under way to replace SHA, a new version of the NSA recommendations will eventually have to be produced.  Will it, for example, begin a phase-out of AES-128 for SECRET communications in favor of requiring AES-256 there as well?  (Since there's no call so far to develop a cipher to replace AES, it would be difficult for NSA to recommend something else.)
It's indeed "a wilderness of mirrors", and we can only guess.  But I'm very wary of using NSA's approval of a cipher as strong evidence, as the overall situation is complex and has so many tradeoffs.
                                                        -- Jerry

@_date: 2013-09-02 07:21:25
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA and cryptanalysis 
For some version of "know".  From "Microsoft released an emergency Windows update on Sunday after revealing that one of its trusted digital signatures was being abused to certify the validity of the Flame malware that has infected computers in Iran and other Middle Eastern Countries.
The compromise exploited weaknesses in Terminal Server, a service many enterprises use to provide remote access to end-user computers. By targeting an undisclosed encryption algorithm Microsoft used to issue licenses for the service, attackers were able to create rogue intermediate certificate authorities that contained the imprimatur of Microsoft's own root authority certificate?an extremely sensitive cryptographic seal. Rogue intermediate certificate authorities that contained the stamp were then able to trick administrators and end users into trusting various Flame components by falsely certifying they were produced by Microsoft....
Based on the language in Microsoft's blog posts, it's impossible to rule out the possibility that at least one of the certificates revoked in the update was ... created using [previously reported] MD5 weaknesses [which allowed collision attacks]. Indeed, two of the underlying credentials used MD5, while the third used the more advanced SHA-1 algorithm. In a Frequently Asked Questions section of Microsoft Security Advisory (2718704), Microsoft's security team also said: "During our investigation, a third Certificate Authority has been found to have issued certificates with weak ciphers." The advisory didn't elaborate."
                                                        -- Jerry

@_date: 2013-09-02 15:09:31
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA and cryptanalysis 
a)  The very reference you give says that to be equivalent to 128 bits symmetric, you'd need a 3072 bit RSA key - but they require a 2048 bit key.  And the same reference says that to be equivalent to 256 bits symmetric, you need a 521 bit ECC key - and yet they recommend 384 bits.  So, no, even by that page, they are not recommending "equivalent" key sizes - and in fact the page says just that.
b)  Those comparisons long ago became essentially meaningless.  On the symmetric size, it's using brute force attack strengths.  But no one is going to brute force a 128-bit key with any known or suggested technology, and brute force attacks against 256-bit keys are way beyond what physics says is even remotely possible.  (I posted on this a long time back:  Any theory even vaguely consistent with what we know about quantum mechanics places a limit on the number of elementary bit flips in a finite volume of space-time.  If you want an answer in 100 years, your computer is at most a sphere in space-time 100 light-years cubed by 100 years in diameter - and that's a gross overestimate.  My quick calculation showed that the quantum limit for that sphere is not far above 128 bits.)
In any real terms, *if you're talking brute force*, 128 bits and 256 bits - and a million bits, if you want to go nuts about it - are indistinguishable.
For the other columns, they don't say where the difficulty estimate comes from. (You could get a meaningless estimate by requiring that the number of primes of the size quoted be equivalent to the number of symmetric keys, but I'm assuming they're being more intelligent about the estimate than that, as a brute force attack on primes makes no sense at all.  What makes more sense - and what they are presumably using - is the number of operations needed by the best known algorithm.  But now we're at point of comparing impossible attacks against 128- and 256-bit symmetric keys with impossible attacks against 3072- or 15360-bit RSA keys - a waste of time.  The relevant point is that attacks against RSA keys have been getting better faster than predicted, while the best publicly known attacks against AES have barely moved the needle from simple brute force.
Given *currently publicly known algorithms*, a 2048 bit RSA key is still secure.  (The same page shows that as equivalent to a 112-bit symmetric key, which is not only beyond any reasonable-term brute force attack, but longer than the keys used - according to some reports, anyway - on some Suite A algorithms.)
And here we actually agree.  Note that I didn't say there was any evidence that NSA was ahead of the public state of the art - even given the public state of the art and the rate that it's advancing, using Z/p as a field is rapidly fading as a realistic alternative.  NSA, looking forward, would be making the recommendation to move to elliptic curves whether or not they could do better than the public at large.  So we can't read much into that aspect of it.  However, note (a) that if NSA does have a theoretical breakthrough, factoring is probably more likely than AES - we know they've hired many people in related fields over many years, and even in public the state of the art has been advancing; (b) most of the Internet is way behind recommendations that are now out there for everyone.  Google recently switched to 2048 bit keys; hardly any other sites have done so, and some older software even has trouble talking to Google as a result.
Just what is it you claim we *know*?
On the asymmetric side, NSA recommends technology that Internet sites *could* have, but in practice mainly don't and for the most part won't for quite some time.  Can NSA break RSA or DH or DSA as most sites on the Internet are using it today?  Damned if I know - but either way is consistent with what we know.
On the symmetric side, I've already agreed that NSA's approval indicated that the considered AES secure 10 years ago, but if they've since learned otherwise but think they are and will remain the only ones with a viable attack for a while, they would be unlikely to admit it by changing their recommendation now.  This isn't evidence that AES is insecure or that NSA has an attack on it, simply a reason why the recommendation is not much in the way of evidence *for* its strength any more either.
                                                        -- Jerry

@_date: 2013-09-02 17:44:57
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA and cryptanalysis 
Except that that's not what happened in this case.
Someone took an old, valid Microsoft license - which should never have been issued, and which was blocked on Vista and Windows 7.  They worked around the block using a technique that required the ability to produce MD5 collisions, which allowed them to spoof Windows Update.  All the details are at A cryptographic approach for producing chosen-prefix collisions in MD5 was presented at CCC in 2008, with a cost estimate of about $20K on a 2008 Amazon EC2 cluster - the authors showed a POC using a cluster of PS3's.  Open source code to implement the attack was published in 2009.
However, the form of the collision apparently didn't match the published code, nor, more fundamentally, the theoretical work that made it possible.  Someone has a *different*, so far nowhere-published attack.  The comment that this required "world-class cryptanalysis" came from the developer of the published chosen-prefix attack, Marc Stevens.
                                                        -- Jerry

@_date: 2013-09-03 16:20:17
@_author: Jerry Leichter 
@_subject: [Cryptography] A strategy to circumvent patents? 
Sigh.  Beware hackers practicing law.
A patent gives someone the right to exclude anyone else from making, using, or selling the patented invention.  You can certainly write a program such as you describe, since it is itself presumably not covered by the patent.  And most likely, even if it stumbled upon exactly the same elliptic curve as is covered by a patent, running the program would not be seen as "making" the invention. The inventions that are out there seem to be of two classes:  Covering doing crypto over some particular curves; and various particular implementation techniques.  Now that your program has found the magic curve - which, of course, you could much more easily have "found" by reading the patent, at least in principle - you can neither use nor sell products that do crypto over that magic curve, nor apply the particular techniques, without permission from the patent holder.
Given the state of patent law and how "obviousness" is currently interpreted, in practice, it's almost impossible to invalidate a patent on those grounds.  (The patent holder would argue that, were it not for the existing patent, you would never have come up with just the right inputs to feed into your program to have it come up with the patented technique.  And, frankly, he'd be right.)  Recent Supreme Court rulings may have brought some life back into "obviousness", but it'll be a while before we know for sure.
Don't try to play games with patent law; you can get hurt.  A court, after likely invalidating your attempted hack-around, would then turn around and see it as evidence that your infringement was willful, perhaps even a deliberate attempt at patent fraud, and hit you up for additional damages.
The situation sucks, but we're stuck with it.
A few weeks ago, I would have said "go ahead and do what PGP did, ignore the patent and distribute an open source version that will give the patent holder - who up until now hasn't been at all aggressive, with only one reported law suit - try to figure out how to get a handle on all the copies".  But the situation has changed with the financial problems of BlackBerry, which now owns the patents.  Those patents will go to *someone*, and if they end up with a patent troll, whose whole business model is based on finding people to go after, often in ways deliberately designed to make defending oneself economically infeasible.  They'll know how to make money off of your attempt at improving the world.  :-( We're just going to have to wait and see who ends up with those patents....
                                                        -- Jerry

@_date: 2013-09-03 18:06:42
@_author: Jerry Leichter 
@_subject: [Cryptography] FIPS, NIST and ITAR questions 
Let H(X) = SHA-512(X) || SHA-512(X)
where '||' is concatenation.  Assuming SHA-512 is a cryptographically secure hash H trivially is as well.  (Nothing in the definition of a cryptographic hash function says anything about minimality.)  But H(X) is clearly not useful for producing a PRNG.
If you think this is "obviously" wrong, consider instead:
H1(X) = SHA-512(X) || SHA-512(SHA-512(X))
Could you determine, just from black-box access to H1, that it's equally bad as a PRNG?  (You could certainly do it with about 2^256 calls to H1 with distinct inputs - by then you have a .5 chance of a duplicated top half of the output, almost certainly with a distinct bottom half.  But that's a pretty serious bit of testing....)
I don't actually know if there exists a construction of a PRNG from a cryptographically secure hash function.  (You can build a MAC, but even that's not trivial; people tried all kinds of things that failed until the HMAC construction was proven correct.)
                                                        -- Jerry

@_date: 2013-09-04 11:18:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Hashes into Ciphers 
This first publication of differential cryptanalysis was at CRYPTO'90.  I highly doubt Karn analyzed his construction relative to DC.  (His post certainly makes no mention of it.)
At first glance - I certainly haven't worked this through - it should be straightforward to construct a hash will all kinds of desirable *hash* properties that would, in Karn's construction, produce a cipher highly vulnerable to DC.  (That is:  This is not a safe *generic* construction, and I'm not sure exactly what requirements you'd have to put on the hash in order to be sure you got a DC-resistant cipher.)
                                                        -- Jerry

@_date: 2013-09-04 11:26:24
@_author: Jerry Leichter 
@_subject: [Cryptography] FIPS, NIST and ITAR questions 
Look, if you want to play around a produce things that look secure to you and a few of your buddies - feel free to go ahead.  If your system is only used by you and a few friends, it's unlikely anyone with the appropriate skills will ever care enough to attack your system, and you'll be "secure".  As always, "security" is mainly an *economic* question, not a purely technical one.
But if you want to play in the crypto game as it's actually played today - if you want something that will survive even if you use it to protect information that has significant value to someone willing to make the investment to get it from you - well, you're going to have to up your game.  You're playing at 1980's levels.  The world has moved on - your opponents won't feel constrained to do the same.
                                                        -- Jerry

@_date: 2013-09-05 20:07:20
@_author: Jerry Leichter 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
[This drifts from the thread topic; feel free to attach a different subject line to it]
Random number generators make for a very interesting target.  Getting decent amounts of entropy on conventional machines is very difficult.  Servers have almost no random variation in their environments; desktops somewhat more; modern laptops, yet more.  Virtualization - now extremely common on the server side - makes things even harder.  But even laptops don't have much.  So we're left trying to distill "enough" randomness for security - a process that's error-prone and difficult to check.
So ... along comes Intel with a nice offer:  Built-in randomness on their latest chips.  Directly accessible to virtual machines, solving the very difficult problems they pose.  The techniques used to generate that randomness are published.  But ... how could anyone outside a few chip designers at Intel possibly check that the algorithm wasn't, in some way, spiked?  For that matter, how could anyone really even check that the outputs of the hardware Get Random Value instruction were really generated by the published algorithm?
Randomness is particularly tricky because there's really no way to test for a spiked random number generator (unless it's badly spiked, of course).  Hell, every encryption algorithm is judged by its ability to generate streams of bits that are indistinguishable from random bits (unless you know the key).
Now, absolutely, this is speculation.  I know of no reason to believe that the NSA, or anyone else, has influenced the way Intel generates randomness; or that there is anything at all wrong with Intel's implementation.  But if you're looking for places an organization like the NSA would really love to insert itself - well, it's hard to pick a better one.
Interestingly, though, there's good news here as well.  While it's hard to get at sources of entropy in things like servers, we're all carrying computers with excellent sources of entropy in our pockets.  Smartphones have access to a great deal of environmental data - accelerometers, one or two cameras, one or two microphones, GPS, WiFi, and cell signal information (metadata, data, signal strength) - more every day.  This provides a wealth of entropy, and it's hard to see how anyone could successfully bias more than a small fraction of it.  Mix these together properly and you should be able to get extremely high quality random numbers.  Normally, we assume code on the server side is "better" and should take the major role in such tasks as providing randomness.  Given what we know now about the ability of certain agencies to influence what runs on servers, *in general*, we need to move trust away from them.  The case is particularly strong in the case of randomness.
Of course, there's a whole other layer of issue introduced by the heavily managed nature of phone software.
                                                        -- Jerry

@_date: 2013-09-05 20:30:40
@_author: Jerry Leichter 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
This has bothered me from the beginning.  Even the first leaks involved material that you would expect to only be available to highly trusted people *well up in the organization* - they were slides selling capabilities to managers and unlikely to be shown to typical employees, cleared or not.  My immediate impression was that we were looking at some disgruntled higher-up.
The fact that these are coming from a sysadmin - who would never have reason to get legitimate access to pretty much *any* of the material leaked so far - is a confirmation of a complete breakdown of NSA's internal controls.  They seem to know how to do cryptography and cryptanalysis and all that stuff - but basic security and separation of privileges and internal monitoring ... that seems to be something they are just missing.
Manning got to see all kinds of material that wasn't directly related to his job because the operational stuff was *deliberately* opened up in an attempt to get better analysis.  While he obviously wasn't supposed to leak the stuff, he was authorized to look at it.  I doubt the same could be said of Snowden.  Hell, when I had a data center manager working for me, we all understood that just because root access *let* you look at everyone's files, you were not *authorized* to do so without permission.
One of the things that must be keeping the NSA guys up night after night is:  If Snowden could get away with this much without detection, who's to say what the Chinese or the Russians or who knows who else have managed to get?  Have they "spiked the spikers", grabbing the best stuff the NSA manages to find?
                                                        -- Jerry

@_date: 2013-09-05 22:31:50
@_author: Jerry Leichter 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
The actual documents - some of which the Times published with few redactions - are worthy of a close look, as they contain information beyond what the reporters decided to put into the main story.  For example, at  the following goal appears for FY 2013 appears:  "Complete enabling for [redacted] encryption chips used in Virtual Public Network and Web encryption devices".  The Times adds the following note:  "Large Internet companies use dedicated hardware to scramble traffic before it is sent. In 2013, the agency planned to be able to decode traffic that was encoded by one of these two encryption chips, either by working with the manufacturers of the chips to insert back doors or by exploiting a security flaw in the chips' design."  It's never been clear whether these kinds of notes are just guesses by the reporters, come from their own sources, or come from Snowden himself.  The Washington Post got burned on one they wrote.  But in this case, it's hard to come up with an alternative explanation.
Another interesting goal:  "Shape worldwide commercial cryptography marketplace to make it more tractable to advanced cryptanalytic capabilities being developed by NSA/CSS."  Elsewhere, "enabling access" and "exploiting systems of interest" and "inserting vulnerabilities".  These are all side-channel attacks.  I see no other reference to "cryptanalysis", so I would take this statement at face value:  NSA has techniques for doing cryptanalysis on certain algorithms/protocols out there, but not all, and they would like to steer public cryptography into whatever areas they have attacks against.  This makes any NSA recommendation *extremely* suspect.  As far as I can see, the bit push NSA is making these days is toward ECC with some particular curves.  Makes you wonder.  (I know for a fact that NSA has been interested in this area of mathematics for a *very* long time:  A mathematician I knew working in the area of algebraic curves (of which elliptic curves are an example) was recruited by - and went to - NSA in about 1975.  I heard indirectly from him after he was at NSA, where he apparently joined an active community of people with related interests.  This is a decade before the first public suggestion that elliptic curves might be useful in cryptography.  (But maybe NSA was just doing a public service, advancing the mathematics of algebraic curves.)
NSA has two separate roles:  Protect American communications, and break into the communications of adversaries.  Just this one example shows that either (a) the latter part of the mission has come to dominate the former; or (b) the current definition of an adversary has become so broad as to include pretty much everyone.
Now, the NSA will say:  Only *we* can make use of these back doors.  But given the ease with which Snowden got access to so much information ... why should we believe they can keep such secrets?
                                                        -- Jerry

@_date: 2013-09-05 23:02:14
@_author: Jerry Leichter 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Perhaps it's time to move away from public-key entirely!  We have a classic paper - Needham and Schroeder, maybe? - showing that private key can do anything public key can; it's just more complicated and less efficient.
Not only are the techniques brittle and increasingly under suspicion, but in
practice almost all of our public key crypto inherently relies on CA's - a structure that's just *full* of well-known problems and vulnerabilities.  Public key *seems to* distribute the risk - you "just get the other guy's public key" and you can then communicate with him safely.  But in practice it *centralizes* risks:  In CA's, in single magic numbers that if revealed allow complete compromise for all connections to a host (and we now suspect they *are* being revealed.)
We need to re-think everything about how we do cryptography.  Many decisions were made based on hardware limitations of 20 and more years ago.  "More efficient" claims from the 1980's often mean nothing today.  Many decisions assumed trust models (like CA's) that we know are completely unrealistic.  Mobile is very different from the server-to-server and dumb-client-to-server models that were all anyone thought about the time.  (Just look at SSL:  It has the inherent assumption that the server *must* be authenticated, but the client ... well, that's optional and rarely done.)  None of the work then anticipated the kinds of attacks that are practical today.
I pointed out in another message that today, mobile endpoints potentially have access to excellent sources of randomness, while servers have great difficulty getting good random numbers.  This is the kind of fundamental change that needs to inform new designs.
                                                        -- Jerry

@_date: 2013-09-05 23:24:54
@_author: Jerry Leichter 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
They want to buy COTS because it's much cheap, and COTS is based on standards.  So they have two contradictory constraints:  They want the stuff they buy secure, but they want to be able to break in to exactly the same stuff when anyone else buys it.  The time-honored way to do that is to embed some secret in the design of the system.  NSA, knowing the secret, can break in; no one else can.  There have been claims in this direction since NSA changed the S-boxes in DES.  For DES, we now know that was to protect against differential cryptanalysis.  No one's ever shown a really convincing case of such an embedded secret hack being done ... but now if you claim it can't happen, you have to explain how the goal in NSA's budget could be carried out in a way consistent with the two constraints.  Damned if I know....
I'm not sure exactly what you're trying to say.  Yes, Miller and Koblitz are the inventors of publicly known ECC, and a number of people (Diffie, Hellman, Merkle, Rivest, Shamir, Adelman) are the inventors of publicly known public-key cryptography.  But in fact we now know that Ellis, Cocks, and Williamson at GCHQ anticipated their public key cryptography work by several years - but in secret.
I think the odds are extremely high that NSA was looking at cryptography based on algebraic curves well before Miller and Koblitz.  Exactly what they had developed, there's no way to know.  But of course if you want to do good cryptography, you also have to do cryptanalysis.  So, yes, it's quite possible that NSA was breaking ECC a decade before its (public) invention.  :-)
                                                        -- Jerry

@_date: 2013-09-06 07:08:38
@_author: Jerry Leichter 
@_subject: [Cryptography] Can you backdoor a symmetric cipher (was Re: 
Well, if that "someone" were the NSA, they might well be very happy to be remembered as the guy who made it possible break in to much of the world's communication!
However, I find this argument unconvincing.  It has the feel of a standard reduction, but isn't, because the reduction is to a known problem that's widely thought to be difficult - it's to a vaguely defined, broad class of problems, "designing a much faster public key system".  Just because the construction gives you something like a public key system, doesn't mean if gives you anything practical for use that way.  For example, suppose the master key works - but only for 1/2^k possible cleartext blocks, or even 1/2^k possible keys.  This makes it useless as a public-key system even for very small k, but quite useful to an attacker even for larger k.
Or suppose the master key works on all messages and keys, but is inefficient, requiring large amounts of time or memory, or the pre-computation of large lookup tables.  Here's a thought experiment:  Suppose differential cryptanalysis were known only to you, and you were in a position to influence the choice of S-boxes for a cryptosystem.  Your "master key" would be the knowledge that DC would work very effectively against the system - but it would still require a lot of time and memory to do so.  You'd even have a leg up on everyone else because you'd know the differentials to use up front - though that's a trivial advantage, since once you know to look for them, finding them is easy enough.  In fact, as far as I know, no one has had any reason to pose questions like:  Could you choose S-boxes that allow some kind of pre-computation step to make application of DC to messages much faster?  DC requires gathering up a large number of plaintext/cyphertext pairs; eventually, you find examples of you can use.  But could there be some way of producing pairs so that success is more likely to come earlier?  Could it be that there's some pre-secret that allows computation both of those tables and and of the S-boxes, such that given only the S-boxes, finding the pre-secret and the tables isn't practical?  Once DC was known and it was found the NSA had actually strengthened DES's S-boxes to protect against it, no one really looked into such issues - basically, who cared?
If you really want to think in tin-hat terms, consider linear cryptanalysis.  While DES was strong against DC, it was quite weak against LC.  People interpreted this as meaning that NSA didn't know about LC.  But ... maybe out in Area 51, or wherever Langley does work with alien technologies, they knew about LC *all along*, and *deliberately* made DES weak against it!  (Why LC and not DC?  Because as we subsequently learned from Don Coppersmith, DC was already known in the non-NSA community, and the NSA knew that it was known.)
Please keep in mind that I'm not proposing that NSA actually did anything like this for any widely-used cryptosystem.  I don't even see how they could have with respect to, say, AES, since they had no hand in the design of the algorithm or the choice of constants.  The only thing I'm arguing here is that the "theorems" that say such a thing could never happen prove nothing of the sort.
                                                        -- Jerry

@_date: 2013-09-06 07:28:41
@_author: Jerry Leichter 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
I don't think we're really in disagreement here.  Much of what you say later in the message is that the way we are using symmetric-key systems (CA's and such), and the way browsers work, are fundamentally wrong, and need to be changed.  And that's really the point:  The system we have is all of a piece, and incremental changes, sadly, can only go so far.  We need to re-think things from the ground up.  And I'll stand by my contention that we need to re-examine things we think we know, based on analyses done 30 years ago.  Good theorems are forever, but design choices apply those theorems to real-world circumstances.  So much has changed, both on the technical front and on non-technical fronts, that the basis for those design choices has fundamentally changed.
Getting major changes fielded in the Internet is extremely difficult - see IPv6.  If it can be done at all, it will take years.  But the alternative of continuing on the path we're on seems less desirable every day.
                                                        -- Jerry

@_date: 2013-09-06 07:42:51
@_author: Jerry Leichter 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Argh!  And this is why I dislike using "symmetric" and "asymmetric" to describe cryptosystems:  In English, the distinction is way too brittle.  Just a one-letter difference - and in including or not the letter physically right next to the "s".
                                                        -- Jerry :-)

@_date: 2013-09-06 09:23:29
@_author: Jerry Leichter 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Following up on my own posting:
So here's a thought experiment for a particular approach:  Imagine that it's the case that half of all possible AES keys are actually "pseudo-weak", in the sense that if you use one of them, some NSA cryptanalytic technique can recover the rest of your key with "acceptable (to NSA)" effort.  Their attack fails for the other half of all possible keys.  Further, imagine that NSA has a recognizer for pseudo-weak keys.  Then their next step is simple:  Get the crypto industry to use AES with good, randomizing key generation techniques.  Make sure that there is more than one approved key generation technique, ideally even a way for new techniques to be added in later versions of the standard, so that approved implementations have to allow for a choice, leading them to separate key generation from key usage.  For the stuff *they* use, add another choice, which starts with one of the others and simply rejects pseudo-weak keys (or modifies them in some way to produce strong keys.)  Then:
- Half of all messages the world sends are open to attack by NSA until the COTS producers learn of the attack and modify their fielded systems;
- All messages NSA is responsible for are secure, even if the attack becomes known to other cryptanalytic services.
I would think NSA would be very happy with such a state of affairs.  (If they could arrange it that 255/256 keys are pseudo-weak - well, so much the better.)
Is such an attack against AES *plausible*?  I'd have to say no.  But if you were on the stand as an expert witness and were asked under cross-examination "Is this *possible*?", I contend the only answer you could give is "I suppose so" (with tone and body language trying to signal to the jury that you're being forced to give an answer that's true but you don't in your gut believe it).
Could an encryption algorithm be explicitly designed to have properties like this?  I don't know of any, but it seems possible.  I've long suspected that NSA might want this kind of property for some of its own systems:  In some cases, it completely controls key generation and distribution, so can make sure the system as fielded only uses "good" keys.  If the algorithm leaks without the key generation tricks leaking, it's not just useless to whoever grabs onto it - it's positively hazardous.  The gun that always blows up when the bad guy tries to shoot it....
                                                        -- Jerry

@_date: 2013-09-06 14:17:07
@_author: Jerry Leichter 
@_subject: [Cryptography] Aside on random numbers (was Re: Opening 
Ah, but that highlights an essential difference between OCR'ing the image and just hashing it:  I can easily check, with my own eyes, that the OCR app is really doing what it claims to be doing.  I have no hope of checking the hash-based app.  A whole class of attacks is closed off by the OCR technique.
It's not that there aren't other attacks.  The phone could, for example, leak the generated values, sending them off to Big Brother.  That kind of attack would, if done correctly, be virtually impossible to detect.  On the other hand, it's not nearly as valuable as a biased generation attack - Big Brother would receive streams of random die tosses with little context about what the resulting values would be used for or how they would be used.  Appropriately targeted attacks might work - "I know Metzger regenerates his keys on the 3rd of every month at about 8:00 AM, so let's use the values he scans at around that time as guesses for his base random values" - but we're talking quite a bit of difficulty here - and the more people use the app, and the more often they make it a habit to toss and scan dice and just discard the results, the more difficult it becomes.
                                                        -- Jerry

@_date: 2013-09-06 14:05:43
@_author: Jerry Leichter 
@_subject: [Cryptography] Sabotaged hardware (was Re: Opening Discussion: 
If you're talking about the FDE features built into disk drives - I don't know anyone who seriously trusts it.  Every "secure disk" that's been analyzed has been found to be "secured" with amateur-level crypto.  I seem to recall one that advertised itself as using AES (you know, military-grade encryption) which did something like:  Encrypt the key with AES, then XOR with the result to "encrypt" all the data.  Yes, it does indeed "use" AES....
There's very little to be gained, and a huge amount to be lost, be leaving the crypto to the drive, and whatever proprietary, hacked-up code the bit-twiddlers who do driver firmware decide to toss in to meet the marketing requirement of being able to say they are secure.  Maybe when they rely on a published standard, *and* provide a test mode so I can check to see that what they wrote to the surface is what the standard says should be there, I might change my mind.  At least them, I'd be worrying about deliberate attacks (which, if you can get into the supply chain are trivial - there's tons of space to hide away a copy of the key), rather than the nonsense we have today.
Now, wouldn't compromising HSM's be sweet.  Not that many vendors make HSM's, and they are exactly the guys who already have a close relationship with the CI (crypto-industrial) complex....
                                                        -- Jerry

@_date: 2013-09-06 16:25:12
@_author: Jerry Leichter 
@_subject: [Cryptography] Bruce Schneier has gotten seriously spooked 
A response he wrote as part of a discussion at Q: "Could the NSA be intercepting downloads of open-source encryption software and silently replacing these with their own versions?"
A: (Schneier) Yes, I believe so.
                                                        -- Jerry

@_date: 2013-09-06 23:22:02
@_author: Jerry Leichter 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
I'm sorry, but this is just nonsense.  You're starting with informal, rough definitions and claiming a mathematical theorem.
I said all this before.  A back door doesn't have to be fast.  It doesn't have to be implementable using amounts of memory that are practical for a fielded system.  It may require all kinds of expensive pre-computation to be useful at all.  It just has to allow practical attacks.  A back door that reduced the effective key size of AES to 40 bits would amount to an effective break of AES, but would be "a public key system" only in some very technical and uninteresting sense.
And none of this is relevant to whether one could have a system with many weak keys.  Some kind of structure in the round computation structure would be an obvious place to look.
In fact, now that I think of it, here's a rough example of such a system:  Take any secure round-based block cipher and decide that you're not going to use a round computation at all - you'll let the user specify the full expanded per-round key.  (People proposed doing this with DES as a way of getting beyond the 56-bit key size.  It didn't work - DES is inherently good for no more than 56 bits, more or less.  In general doing this with any decent block cipher won't make it any stronger.  But that's not the point of the example.)  There are now many weak keys - all kinds of repetitive structures allow for slide attacks, for example.  Every bad way of designing a round computation corresponds to a set of weak full keys.  On the other hand, for a certain subset of the keys - those that could have been produced by the original (good) round computation - it's *exactly* the original cipher, with *exactly* the original security guarantees.  If I carefully use only keys from that set, I've lost nothing (other than wasted space for a key longer than it needs to be).
So now I have a block cipher that has two sets of keys.  One set makes it as secure as the original cipher; one set makes it easy to break - my back door.  Have I just invented a new public key system?
                                                        -- Jerry

@_date: 2013-09-07 07:25:43
@_author: Jerry Leichter 
@_subject: [Cryptography] XORing plaintext with ciphertext 
The question is much more subtle than that, getting deep into how to define a the security of a cipher.
Consider a very simplified and limited, but standard, way you'd want to state a security result:  A Turing machine with an oracle for computing the encryption of any input with any key, when given as input the cyphertext and allowed to run for time T polynomial in the size of the key, has no more than an probability P less than (something depending on the key size) of guessing any given bit of the plaintext.  (OK, I fudged on how you want to state the probability - writing this stuff in English rather than mathematical symbols rapidly becomes unworkable.)  The fundamental piece of that statement is in "given as input..." part:  If the input contains the key itself, then obviously the machine has no problem at all producing the plaintext!  Similarly, of course, if the input contains the plaintext, the machine has an even easier time of it.
You can, and people long ago did, strengthen the requirements.  They allow for probabilistic machines as an obvious first step.  Beyond that, you want semantic security:  Not only shouldn't the attacking machine be unable to get an advantage on any particular bit of plaintext; it shouldn't be able to get an advantage on, say, the XOR of the first two bits.  Ultimately, you want so say that given any boolean function F, the machine's a postiori probability of guessing F(cleartext) should be identical (within some bounds) to its a priori probability of guessing F(cleartext).  Since it's hard to get a handle on the prior probability, another way to say pretty much the same thing is that the probability of a correct guess for F(cleartext) is the same whether the machine is given the ciphertext, or a random sequence of bits.  If you push this a bit further, you get definitions related to indistinguishability:  The machine is simply expected to say "the input is the result of applying the cipher to some plaintext" or "the input is random"; it shouldn't even be able to get an advantage on *that* simple question.
This sounds like a very strong security property (and it is) - but it says *nothing at all* about the OP's question!  It can't, because the machine *can't compute the XOR of the plaintext and the ciphertext*.  If we *give* it that information ... we've just given it the plaintext!
I can't, in fact, think of any way to model the OP's question.  The closest I can come is:  If E(K,P) defines a strong cipher (with respect to any of the variety of definitions out there), does E'(K,P) = E(K,P) XOR P *also* define a strong cipher?  One would think the answer is yes, just on general principles: To someone who doesn't know K and P, E(K,P) is "indistinguishable from random noise", so E'(K,P) should be the same.  And yet there remains the problem that it's not a value that can be computed without knowing P, so it doesn't fit into the usual definitional/proof frameworks.  Can anyone point to a proof?
The reason I'm not willing to write this off as "obvious" is an actual failure in a very different circumstance.  There was work done at DEC SRC many years ago on a system that used a fingerprint function to uniquely identify modules.  The fingerprints were long enough to avoid the birthday paradox, and were computed based on the result of a long series of coin tosses whose results were baked into the code.  There was a proof that the fingerprint "looked random".  And yet, fairly soon after the system went into production, collisions started to appear.  They were eventually tracked down to a "merge fingerprints" operation, which took the fingerprints of two modules and produces a fingerprint of the pair by some simple technique like concatenating the inputs and fingerprinting that.  Unfortunately, that operation *violated the assumptions of the theorem*.  The theorem said that the outputs of the fingerprint operation would look random *if chosen "without knowledge" of the coin tosses*.  But the inputs were outputs of the same algorithm, hence "had knowledge" of the coin tosses.  (And ... I just found the reference to this.  See ftp://ftp.dec.com/pub/dec/SRC/research-reports/SRC-113.pdf, documentation of the Fingerprint interface, page 42.)
                                                        -- Jerry

@_date: 2013-09-07 11:39:28
@_author: Jerry Leichter 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Actually, if you look at the papers cited, *they* are themselves informal.  The fundamental thing they are lacking is a definition of what would constitute a "master key".  Let's see if we can formalize this a bit:
We're given a block cipher E(K,P) and a corresponding decryption algorithm D(K,C).  The system has a master key M such that D(E(K,P),M) == P.  This is what a "master key" does in a traditional lock-and-key system, so unless we see some other definition, it's what we have to start with.  Is there such a system?  Sure, trivially.  Given any block cipher E'/D', I simply define E(K,P) = E'(M,K) || E'(K,P).  (I can optimize the extra length by leaking one randomly chosen bit of E'(M,K) per block.  It won't take long for the whole key to be transmitted.)  OK, where's the public key system?
So maybe there isn't *one* master key, but let's go to the extreme and say there is one unique master per user key, based on some secret information S.  That is:  Given K, there is a function F(S,K) which produces a *different* key K', with the property that D(K,C) == D(K',C).  Or maybe, as in public key systems, you start with S and some random bits and produce a matched pair K and K'  But how is this a "master key" system?  If I wasn't "present at the birth" of the K that produced the cyphertext I have in hand ... to get K' now, I need K (first form) or S and the random bits (second form), which also gives me K directly.  So what have I gained?
I can construct a system of the first form trivially:  Just use an n-bit key but ignore the first bit completely.  There are now two keys, one with a leading 0, one with a leading 1.  Constructing a system of the second form shouldn't be hard, though I haven't done it.  In either case, it's uninteresting - my "master key" is as hard to get at as the original key.
I'm not sure exactly where to go next.  Let's try to modify some constraints.  Eliminate directly hiding the key in the output by requiring that E(K,.) be a bijection.  There can't possibly be a single master key M, since if there were, what could D(M,E(M,0...0)) be?  It must be E(K,0...0) for any possible K, so E(K,0...0) must be constant - and in fact E must be constant.  Not very interesting.  In fact, a counting argument shows that there must be as many M's as there are K's.  It looks as we're back to the two-fold mapping on keys situation.  But as before ... how could this work?
In fact, it *could* work.  Suppose I use a modified form of E() which ignores all but the first 40 bits of K - but I don't know that E is doing this.  I can use any (say, 128-bit) key I like, and to someone not in on the secret, a brute force attack is impossible.  But someone who knows the secret simply sets all but the first 40 bits to 0 and has an easy attack.
*Modified forms (which hid what was happening to some degree) of such things were actually done in the days of export controls!*  IBM patented and sold such a thing under the name CDMF (  I worked on adding cryptography to a product back in those days, and we had to come up with a way to be able to export our stuff.  I talked to IBM about licensing CDMF, but they wanted an absurd amount of money.  (What you were actually paying for wasn't the algorithm so much as that NSA had already approved products using it for export.)  We didn't want to pay, and I designed my own algorithm to do the same thing.  It was a silly problem to have to solve, but I was rather proud of the solution - I could probably find my spec if anyone cares.  It was also never implemented, first because this was right around the time the crypto export controls got loosened; and second because we ended up deciding we didn't need crypto anyway.  We came back and did it very differently much later.  My two fun memories from the experience:  (a) Receiving a FAX from NSA - I still have it somewhere; (b) being told at one point that we might need someone with crypto clearance to work on this stuff with NSA, and one of my co-workers chiming in with "Well, I used to have it.  Unfortunately it was from the KGB."
Anyway ... yes, I can implement such a thing - but there's still no public key system here.
So ... would *you* like to take a stab at pinning down a definition relative to which the theorem you rely on makes sense?
                                                        -- Jerry

@_date: 2013-09-07 15:39:07
@_author: Jerry Leichter 
@_subject: [Cryptography] Protecting Private Keys 
I can tell you, in broad terms, what happens at Google:  The disks are physically destroyed, on site.  Every disk is tracked from cradle to grave - checked into the datacenter, where it receives a unique ID; checked in and out of the machine, carts that are used to move devices around datacenters (otherwise a great way to lose track of something), various secure storage facilities (on site), and on to eventual destruction.  No drive that was ever plugged into a live machine ever leaves its data center in a condition that the data on it is recoverable.  (This despite the fact that in many cases the data on the disk is already encrypted.)
Actual long-term key storage is done in a relatively small of locations.  And there are various other tricks to make it hard to get information out of a machine should you somehow get it out of the facility, and to make it hard to sneak a machine of your own *into* the facility.
While Google's particular approaches are unique, other large-scale providers who are concerned about security do the same general kind of thing.  I seem to recall seeing a description of how Facebook similarly tracks and manages disk drives, for example.
It would be nice if there were some published standards for such things and a third-party auditing mechanism.
                                                        -- Jerry

@_date: 2013-09-07 19:03:54
@_author: Jerry Leichter 
@_subject: [Cryptography] How to get really paranoid 
So I'm reading some of the recent threads here, and all of a sudden, Mail.app warns me of a problem with a cert.  I have an old, essentially unused, Yahoo email address that came along for free when I got DSL from AT&T years ago.  As with all my email connections, I require SSL - which may be about as effective as tossing salt over my shoulder, but hey, it's still the best we've got.  It's been working fine - well, the servers are noticeably slow - for years.  All of a sudden, I'm told by Mail.app that it can't verify the cert for connecting to smtp.att.yahoo.com because it was signed by an unknown authority.  Its signer is DigiCert - it's a "DigiCert High Assurance CA-3" signing certificate.  Expires on November 13 of this year.
Bizarre.  Time to replenish my stocks of aluminum foil.  :-)
                                                        -- Jerry

@_date: 2013-09-07 19:06:29
@_author: Jerry Leichter 
@_subject: [Cryptography] New task for the NSA 
The NY Times has done a couple of reports over the last couple of months about the incomprehensibility of hospital bills, even to those within the industry - and the refusal of hospitals to discuss their charge rates, claiming that what they will bill you for a treatment is "proprietary".
Clearly, it's time to sic the NSA on the medical care system's billing offices.  Let them do something really useful for a change!
                                                        -- Jerry :-)

@_date: 2013-09-07 21:54:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Symmetric cipher + Backdoor = Public Key System 
I've given quite a bit of argument as to why the result doesn't really say what it seems to say.  Feel free to respond to the actual counterexamples I gave, rather than simply say I "unaccountably" don't believe the paper.
                                                        -- Jerry

@_date: 2013-09-08 07:32:14
@_author: Jerry Leichter 
@_subject: [Cryptography] Why prefer symmetric crypto over public key 
Indeed, that was exactly what I had in mind when I suggested we might want to do without private key cryptography on another stream.
Not every problem needs to be solved on Internet scale.  In designing and building cryptographic systems simplicity of design, limitation to purpose, and humility are usually more important the universality.  Most of the email conversations I have are with people I've corresponded with in the past, or somehow related to people I've corresponded with in the past.  In the first case, I already have their keys - the only really meaningful notion of "the right key" is key continuity (combined with implied verification if we also have other channels of communication - if someone manages to slip me a bogus key for someone who I talk to every day, I'm going to figure that out very quickly.)  In the second case - e.g., an email address from a From field in a message on this list - the best I can possibly hope for initially is that I can be certain I'm corresponding with whoever sent that message to the list.  There's no way I can bind that to a particular person in the real world without something more.
Universal schemes, when (not if - there's no a single widely fielded system that hasn't been found to have serious bugs over its operation lifetime, and I don't expect to see one in *my* lifetime) they fail, lead to universal attacks.  I need some kind of universal scheme for setting up secure connections to buy something from a vendor I never used before, but frankly the NSA doesn't need to break into anything to get that information - the vendor, my bank, my CC company, credit agencies are call collecting and selling it anyway.
The other thing to keep in mind - and I've come back to this point repeatedly - is that the world we are now designing for is very different from the world of the mid- to late-1990's when the current schemes were designed.  Disk is so large and so cheap that any constraint in the old designs that was based on a statement like "doing this would require the user to keep n^2 keys pairs, which is too much" just doesn't make any sense any more - certainly not for individuals, not even for small organizations:  If n is determined by the number of correspondents you have, then squaring it still gives you a small number relative to current disk sizes.  Beyond that, everyone today (or in the near future) can be assumed to carry with them computing power that rivals or exceeds the fastest machines available back in the day - and to have an always-on network connection whose speed rivals that of *backbone* links back then.
Yes, there are real issues about how much you can trust that computer you carry around with you - but after the recent revelations, is the situation all that different for the servers you talk to, the routers in the network between you, the crypto accelerators many of the services use - hell, every piece of hardware and software.  For most people, that will always be the situation:  They will not be in a position to check their hardware, much less build their own stuff from the ground up.  In this situation, about all you can do is try to present attackers with as many *different* targets as possible, so that they need to split their efforts.  It's guerrilla warfare instead of a massed army.
                                                        -- Jerry

@_date: 2013-09-08 09:03:05
@_author: Jerry Leichter 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
OK, let's look at this another way.  The broader argument being made here breaks down into three propositions:
1.  If you have a way to "spike" a block cipher based on embedding a secret in it, you can a way to create something with the formal properties of a public key cryptosystem - i.e., there is a function E(P) which anyone can compute on any plaintext P, but given E(P), only you can invert to recover P.
2.  Something with the formal properties of a public key cryptosystem can be used as a *practical* public key cryptosystem.
3.  A practical public-key cryptosystem is much more valuable than a way to embed a secret in a block cipher, so if anyone came up with the latter, they would certainly use it to create the former, as it's been "the holy grail" of cryptography for many years to come up with a public key system that didn't depend on complex mathematics with uncertain properties.
If we assume these three propositions, and looks around us and observe the lack of the appropriate kinds of public key systems, we can certainly conclude that no one knows how to embed a secret in a block cipher.
Proposition 1, which is all you specifically address, is certainly true.  I claim that Propositions 2 and 3 are clearly false.
In fact, Proposition 3 isn't even vaguely mathematical - it's some kind of statement about the values that cryptographers assign to different kinds of primitives and to publication.  It's quite true that if anyone in the academic world were to come up with a way to create a practical public key cryptosystem without a dependence on factoring or DLP, they would publish to much acclaim.  (Of course, there *are* a couple of such systems known - they were published years ago - but no one uses them for various reasons.  So "acclaim" ... well, maybe.)  Then again, an academic cryptographer who discovered a way to hide a secret in a block cipher would certainly publish - it would be really significant work.  So we never needed this whole chain of propositions to begin with:  It's self-evidently true that no one in the public community knows how to embed a secret in a block cipher.
But ... since we're talking *values*, what are NSA's values?  Would *they* have any reason to publish if they found a way to embed a secret in a block cipher? Hell, no!  Why would they want to give away such valuable knowledge?  Would they produce a private-key system based on their breakthrough?  Maybe, for internal use.  How would we ever know?
But let's talk mathematics, not psychology and politics.  You've given a description of a kind of back door that *would* produce a practical public key system.  But I've elsewhere pointed out that there are all kinds of back doors.  Suppose that my back door reduces the effective key size of AES to 40 bits.  Even 20+ years ago, NSA was willing to export 40-bit crypto; presumably they were willing to do the brute-force computation to break it.  Today, it would be a piece of cake.  But would a public-key system that requires around 2^40 operations to encrypt be *practical*?  Even today, I doubt it.  And if you're willing to do 2^40 operations, are you willing to do 2^56?  With specialized hardware, that, too, has been easy for years.  NSA can certainly have that specialized hardware for code breaking - will you buy it for encryption?
In fact, this is an example I was going to give:  In a world in which differential crypto isn't known, it *is* a secret that's a back door.  Before DC was published, people seriously proposed strengthening DES by using a 448-bit (I think that's the number) key - just toss the round key computation mechanism and provide all the keying for all the rounds.  If that had been widely used, NSA would have been able to break it use DC.
Of course we know about DC.  But the only reason RSA is safe is that we don't know how to factor quickly!  (Actually, even that's not quite true - after all these years, as far as I know, we *still* haven't managed to show that RSA is as hard as factoring - only the obvious fact that it's no harder.  It would be an incredible and unexpected result to separate the problems, but it *could* happen.)  It happens that in the case of RSA, we can point to *a particular easy to state problem* that, if solved, would break the system.  Things are much more nebulous for block ciphers, but that shouldn't be surprising:  They don't have simple, clean mathematical structures.  (In fact, when Rijndael was first proposed, there was some concern that it was "too clean" - that its relatively simple structure would provide traction for mathematical attacks.  It doesn't seem to have worked out that way.)
There are, in fact, even closer analogues between potential RSA weaknesses and DC weaknesses.  What DC tells us is that certain S boxes lead to weak ciphers, even if the general structure is otherwise sound.  Well ... over the years, we've learned that certain choices of primes lead to weak RSA, so we've restricted the choices we make to the "good" primes.  This is a much more pronounced effect with discrete log-based algorithms - and yet more so with elliptic curve algorithms.  Anyone who knows about such weaknesses before they are known to the public, and is in a position to influence how primes or curves are chosen, can embed a back door - a back door that will be closed as soon as the potential weakness becomes known.  How is this different in the public-key and block cipher cases?
In the case of elliptic curve algorithms, we *know* that in some cases its possible to embed a secret, but we don't know of any way to tell if a secret *actually was embedded*.  DC happens to have the property that once you know about it, you can check if your S-boxes are bad - but where's the proof that a different attack will necessarily have this property?  If such an attack were to emerge against AES, *mathematically*, we'd have to say, well, maybe someone put a back door into AES - we really don't know one way or another, just as we don't know, one way or the other, about the NSA-suggested elliptic curves and points.  In either case, we should probably come up with something new.
                                                        -- Jerry

@_date: 2013-09-08 13:06:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Why prefer symmetric crypto over public key 
It's even better than you make out.  If Eve does manage to get hold of the Alice's current keys, and uses them to communicate with Bob, *after the communication, Bob will have updated his keys - but Alice will not have*.  The next time they communicate, they'll know they've been compromised.  That is, this is tamper-evident cryptography.
There was a proposal out there based on something very much like this to create tamper-evident signatures.  I forget the details - it was a couple of years ago - but the idea was that every time you sign something, you modify your key in some random way, resulting in signatures that are still verifiably yours, but also contain the new random modification.  Beyond that, I don't recall how it worked - it was quite clever... ah, here it is:                                                          -- Jerry

@_date: 2013-09-08 13:08:40
@_author: Jerry Leichter 
@_subject: [Cryptography] Why prefer symmetric crypto over public key 
Spoke too quickly - that paper is something else entirely.  I still can't locate the one I was thinking of.
                                                        -- Jerry

@_date: 2013-09-08 14:50:07
@_author: Jerry Leichter 
@_subject: [Cryptography] In the face of "cooperative" end-points, 
I agree, but the situation is complicated.  Consider chat.  If it's one-to-one, end-to-end encryption is pretty simple and could be made simple to use; but people also want to chat rooms, which are a much more complicated key management problem - unless you let the server do the encryption.  Do you enable it only for one-to-one conversations?  Provide different interfaces for one-to-one and chat room discussions?
Even for one-to-one discussions, these days, people want transparent movement across their hardware.  If I'm in a chat session on my laptop and leave the house, I'd like to be able to continue on my phone.  How do I hand off the conversation - and the keys?  (What this actually shows is the complexity of defining "the endpoint".  From the protocol's point of view, the endpoint is first my laptop, then my phone.  From the user's point of view, the endpoint is  the user!  How do we reconcile these points of view?  Or does the difference go away if we assume the endpoint is always the phone, since it's always with me anyway?)
The same kinds of questions arise for other communications modalities, but are often more complex.  One-to-one voice?  Sure, we could easily end-to-end encrypt that.  But these days everyone expects to do conference calls.  Handling those is quite a bit more complex.
There does appear to be some consumer interest here.  Apple found it worthwhile to advertise that iMessage - which is used in a completely transparent way, you don't even have to opt in for it to replace SMS for iOS to iOS messages - is end-to-end encrypted.  (And, it appears that it *is* end-to-end encrypted - but unfortunately key establishment protocols leave Apple with the keys - which allows them to provide useful services, like making your chat logs visible on brand new hardware, but also leaves holes of course.)  Silent Circle, among others, makes their living off of selling end-to-end encrypted chat sessions, but they've got a tiny, tiny fraction of the customer base Apple has.
I think you first need to decide *exactly* what services you're going to provide in a secure fashion, and then what customers are willing to do without (multi-party support, easy movement to new devices, backwards compatibility perhaps) before you can begin to design something new with any chance of success.
                                                        -- Jerry
                                                        -- Jerry

@_date: 2013-09-08 14:56:36
@_author: Jerry Leichter 
@_subject: [Cryptography] Why prefer symmetric crypto over public key 
Jonathan Katz found the paper I was thinking of -                                                         -- Jerry

@_date: 2013-09-08 19:20:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Der Spiegel: "NSA Can Spy on Smart Phone Data" 
A remarkably poor article.  Just what does "gain access to" mean?  There are boxes sold to law enforcement (but never, of course, to the bad guys) that claim they can get access to any phone out there.  If it's unlocked, everything is there for the taking; if it's locked, *some* of it is hard to get to, but most isn't.  Same goes for Android.
The article mentions that if they can get access to a machine the iPhone syncs with, they can get into the iPhone.  Well golly gee.  There was an attack reported just in the last couple of weeks in which someone built an attack into a fake charger!  Grab a charge at a public charger, get infected for  your trouble.  Apple's fixed that in the next release by prompting the user for permission whenever an unfamiliar device asks for connection.  But if you're in the machine the user normally connects to, that won't help.  Nothing, really, will help.
Really, for the common phones out there, the NSA could easily learn how to do this stuff with a quick Google search - and maybe paying a couple of thousand bucks to some of the companies that do it for a living.
The article then goes on to say the NSA can get SMS texts.  No kidding - so can the local cops.  It's all unencrypted, and the Telco's are only too happy to cooperate with govmint' agencies.
The only real news in the whole business is that they claim to have gotten into Blackberry's mail system.  It's implied that they bought an employee with the access needed to weaken things for them.
                                                        -- Jerry

@_date: 2013-09-08 16:47:36
@_author: Jerry Leichter 
@_subject: [Cryptography] Usage models (was Re: In the face of 
I don't see how it's possible to make any real progress within the existing cloud model, so I'm with you 100% here.  (I've said the same earlier.)
What's hard is making this so simple and transparent that anyone can do it without thinking about it.  Again, think of the iMessage model:  If Apple hadn't larded it up with extra features (that, granted, most of its users probably want), we would today have tens of millions of people exchanging end-to-end, private messages without doing anything special or even thinking about it.  (Yes, Apple could have been forced to weaken it after the fact - but it would have had to be by sending an update that broke the software.)
Apple has built some surprisingly well protected stuff (along with some really broken stuff).  There's an analysis somewhere out there of how iOS device backups work.  Apple gives you a choice of an "encrypted" or an "unencrypted" backup.  Bizarrely, the "unencrypted" one actually has some of the most sensitive data encrypted using secret information *locked into the device itself* - where it would take significant hardware hacking (as far as anyone knows) to get at it; in an encrypted backup, this information is decrypted by the device, then encrypted with the backup key.  So in some ways, it's *stronger* - an "unencrypted" backup can only be restored to the iOS device that created it, while if you know the password, an "encrypted" backup can be restored to any device - which is the point.  (Actually, you can restore an "unencrypted" backup to a new device, too, but the most sensitive items - e.g., stored passwords - are lost as the information to access them is present only in the old device.)  You'd never really know any of this from Apple's extremely sparse documentation, mind you - it took someone hacking at the implementation to figure it out.
I don't agree, at all, with the claim that users are not interested in privacy or security.  But (a) they often don't know how exposed they are - something the Snowden papers are educating many about; (b) they don't know how to judge what's secure and what isn't (gee, can any of us, post-Snowden?); (c) especially given the previous two items, but even without them, there's a limit to how much crap they'll put up with.  The bar for perceived quality and simplicity of interface these days is - thanks mainly to Apple - incredibly high.  Techies may bitch and moan that this is all just surface glitz, that what's important is underneath - but if you want to reach beyond a small coterie of hackers, you have to get that stuff right.
                                                        -- Jerry

@_date: 2013-09-08 22:39:54
@_author: Jerry Leichter 
@_subject: [Cryptography] In the face of "cooperative" end-points, 
An additional helper:  Re-keying.  Suppose you send out a new public key, signed with your old one, once a week.  Keep the chain of replacements posted publicly so that someone who hasn't connected to you in a while can confirm the entire sequence from the last public key he knew to the current one.  If someone sends you a message with an invalid key (whether it was ever actually valid or not - it makes no difference), you just send them an update.
An attacker *could* sent out a fake update with your signature, but that would be detected almost immediately.  So a one-time "donation" is now good for a week.  Sure, the leaker can keep leaking - but the cost is now considerably
greater, and ongoing.

@_date: 2013-09-08 22:53:39
@_author: Jerry Leichter 
@_subject: [Cryptography] Der Spiegel: "NSA Can Spy on Smart Phone Data" 
Apparently this was just a "teaser" article.  The following is apparently the full story:    I can't tell for sure - it's the German original, and my German is non-existent.
                                                        -- Jerry

@_date: 2013-09-08 23:56:16
@_author: Jerry Leichter 
@_subject: [Cryptography] Techniques for malevolent crypto hardware 
Which brings into the light the question:  Just *why* have so many random number generators proved to be so weak.  If we knew the past trouble spots, we could try to avoid them, or at least pay special care to them during reviews, in the future.
I'm going entirely off of memory here and a better, more data-driven approach, might be worth doing, but I can think of three broad classes of root causes of past breaks:
1.  The designers just plain didn't understand the problem and used some obvious - and, in retrospect, obviously wrong - technique.  (For example, they didn't understand the concept of entropy and simply fed a low-entropy source into a whitener of some kind - often MD5 or SHA-1.  The result can *look* impressively random, but is cryptographically worthless.)
2.  The entropy available from the sources used was much less, at least in some circumstances (e.g., at startup) than the designers assumed.
3.  The code used in good random sources can look "strange" to programmers not familiar with it, and may even look buggy.  Sometimes good generators get ruined by later programmers who "clean up the code".
                                                        -- Jerry

@_date: 2013-09-09 06:44:42
@_author: Jerry Leichter 
@_subject: [Cryptography] Points of compromise 
It's not clear to me what kinds of compromises you're considering.  You've produced a list of a number of possibilities, but not even mentioned whole classes of them - e.g., back doors in ECC.
I've expanded, however, on one element of your list.
There are two sides to a compromise in accelerator hardware:  Grabbing the information, and exfiltrating it.  The examples you give - and much discussion, because its fun to consider such stuff - look at clever ways to exfiltrate stolen information along with the data it refers to.
However, to a patient attacker with large resources, a different approach is easier:  Have the planted hardware gather up keys and exfiltrate them when it can.  The attacker builds up a large database of possible keys - many millions, even billions, of keys - but still even an exhaustive search against that database is many orders of magnitude easier than an exhaustive search on an entire keyspace, and quite plausible - consider Venona.  In addition, the database can be searched intelligently based on spatial/temporal/organizational "closeness" to the message being attacked.
An attack of this sort means you need local memory in the device - pretty cheap these days, though of course it depends on the device - and some way of exfiltrating that data later.  There are many ways one might do that, from the high tech (when asked to encrypt a message with a particular key, or bound to a particular target, instead encrypt - with some other key - and send - to some other target - the data to be exfiltrated) to low (pay someone with physical access to plug a USB stick into the device periodically).
                                                        -- Jerry

@_date: 2013-09-09 07:02:41
@_author: Jerry Leichter 
@_subject: [Cryptography] Usage models (was Re: In the face 
It's a matter of context.  For data I'm deliberately sharing with some company - sure, cloud is fine.  As I mentioned elsewhere, if the NSA wants to spend huge resources to break in to my purchasing transactions with Amazon, I may care as a citizen that they are wasting their money - but as a personal matter, it's not all that much of a big deal, as that information is already being gathered, aggregated, bought, and sold on a mass basis.  If they want to know about my buying habits and financial transactions, Axciom can sell them all they need for a couple of bucks.
On the other hand, I don't want them recording my chats or email or phone conversations.  It's *that* stuff that is "out in the cloud" these days, and as long as it remains out there in a form that someone other than I and those I'm communicating with can decrypt, it's subject to attacks - attacks so pervasive that I don't see how you could ever built a system (technical or legal) to protect against them.  The only way to win is not to play.
                                                        -- Jerry

@_date: 2013-09-09 07:16:21
@_author: Jerry Leichter 
@_subject: [Cryptography] Impossible trapdoor systems (was Re: Opening 
Before our current level of understanding of block ciphers, people actually raised - and investigated - the question of whether the DES operations formed a group.  (You can do this computationally with reasonable resources.  The answer is that it isn't.)  I don't think anyone has repeated the particular experiment with the current crop of block ciphers; but then I expect the details of their construction, and the attacks they are already explicitly built to avoid, would rule out the possibility.  But I don't know.
Stepping back, what you are considering is the possibility that there's a structure in the block cipher such that if you have some internal information, and you have some collection of plaintext/ciphertext pairs with respect to a given key, you can predict other (perhaps all) such pairs.  This is just another way of saying there's a ciphertext/known plaintext/chosen plaintext/ chosen ciphertext attack, depending on your assumptions about how that collection of pairs must be created.  That it's conveniently expressible as some kind of mathematical structure on the mappings generated by the cipher for a given key is neither here nor there.
Such a thing would contradict everything we think we know about block ciphers. Sure, it *could* happen - but I'd put it way, way down the list of possibles.
                                                        -- Jerry

@_date: 2013-09-09 07:30:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Market demands for security (was Re: Opening 
Quote from the chairman of a Fortune 50 company to a company I used to work for, made in the context of a talk to the top people at that company*:  "I don't want to buy security products.  I want to buy secure products."
This really captures the situation in a nutshell.  And it's a conundrum for all the techies with cool security technologies they want to sell.  Security isn't a product; it's a feature.  If there is a place in the world for companies selling security solutions, it's as suppliers to those producing something that fills some other need - not as suppliers to end users.
                                                        -- Jerry
*It's obvious from public facts about me that the company "receiving" this word of wisdom was EMC; but I'll leave the other company anonymous.

@_date: 2013-09-10 06:42:12
@_author: Jerry Leichter 
@_subject: [Cryptography] Techniques for malevolent crypto hardware 
Good observations, but I think you're being too pessimistic.  All the examples you give *could* be tested - but not by "ignorant black box testing" - testing that ignores not just what's inside the box, but the actual requirements on what the box is supposed to produce.  A non-seeded PRNG, and even one seeded with a very small amount of entropy, will be caught by a test that runs multiple instances of the PRNG from the system starting state and ensures that the ensemble of first outputs (and, for good measure, the first *couple* of outputs) has the right statistics.  Similarly, a test that inserts the same password into multiple instances of the same system in the same state can check that the hashed versions have the right statistics.  No, these can't catch deliberate attack code which produces random-looking values that the attacker can predict - no test can.  But it will catch a broad class of common errors.
The others aren't cryptographic issues and require different approaches.
The fact that there are bad testing practices - and that those bad practices are used all too often - doesn't mean there aren't good practices, and that they could not be applied.  Where the testing is bad because of ignorance of what is actually important and how to test for it, learning from the failures of the past is the way forward - which was exactly the point of "PRMG failures" classification.
                                                        -- Jerry

@_date: 2013-09-10 07:42:55
@_author: Jerry Leichter 
@_subject: [Cryptography] The One True Cipher Suite 
I'm not so sure I agree.  You have to consider the monoculture problem, combined with the threat you are defending against.
The large burst of discussion on this list was set off by Perry's request for ways to protect against the kinds of broad-scale, gather-everything attacks that Snowden has told us the NSA is doing.  So consider things from the side of someone attempting to mount this kind of attack:
1.  If everyone uses the same cipher, the attacker need only attack that one cipher.
2.  If there are thousands of ciphers in use, the attacker needs to attack some large fraction of them.
As a defender, if I go route 1, I'd better be really, really, really sure that my cipher won't fall to any attacks over its operational lifetime - which, if it's really universal, will extend many years *even beyond a point where a weakness is found*.
On the other hand, even if most of the ciphers in my suite are only moderately strong, the chance of any particular one of them having been compromised is lower.
This is an *ensemble* argument, not an *individual* argument.  If I'm facing an attacker who is concentrating on my messages in particular, then I want the strongest cipher I can find.
Another way of looking at this is that Many Ciphers trades higher partial failure probabilities for lower total/catastrophic failure probabilities.
Two things are definitely true, however:
1.  If you don't remove ciphers that are found to be bad, you will eventually pollute your ensemble to the point of uselessness.  If you want to go the "many different ciphers" approach, you *must* have an effective way to do this.
2.  There must be a large set of potentially good ciphers out there to choose from.  I contend that we're actually in a position to create reasonably good block ciphers fairly easily.  Look at the AES process.  Of the 15 round 1 candidates, a full third made it to the final round - which means that no significant attacks against them were known.  Some of the rejected ones failed due to minor "certificational" weaknesses - weaknesses that should certainly lead you not to want to choose them as "the One True Cipher", but which would in and of themselves not render breaking them trivial.  And, frankly, for most purposes, any of the five finalists would have been fine - much of the final choice was made for performance reasons.  (And, considering Dan Bernstein's work on timing attacks based on table lookups, the performance choices made bad assumptions about actual hardware!)
I see no reason not to double-encrypt, using different keys and any two algorithms from the ensemble.  Yes, meet-in-the-middle attacks mean this isn't nearly as strong as you might naively think, but it ups the resource demands on the attacker much more than on the defender.
Now, you can argue that AES - the only cipher really in the running for the One True Symmetric Cipher position at the moment - is so strong that worrying about attacks on it is pointless - the weaknesses are elsewhere.  I'm on the fence about that; it's hard to know.  But if you're going to argue for One True Cipher, you must be explicit about this (inherently unprovable) assumption; without it your argument fails.
The situation is much more worse for the asymmetric case:  We only have a few alternatives available and there are many correlations among their potential weaknesses, so the Many Ciphers approach isn't currently practical, so there's really nothing to debate at this point.
Finally, I'll point out that what we know publicly about NSA practices says that (a) they believe in multiple ciphers for different purposes; (b) they believe in the strength of AES, but only up to a certain point.  At this point, I'd be very leery of taking anything NSA says or reveals about it practices at face value, but there it is.
                                                        -- Jerry

@_date: 2013-09-10 17:04:04
@_author: Jerry Leichter 
@_subject: [Cryptography] Availability of plaintext/ciphertext pairs (was 
Phil Rogoway has a paper somewhere discussing the right way to implement cryptographic modes and API's.  In particular, he recommends changing the definition of CBC from:
E_0 = IV     # Not transmitted
E_{i+1} = E(E_i XOR P_{i+1})
E_0 = E(IV)  # Not transmitted
E_{i+1} = E(E_i XOR P_{i+1})
This eliminates known plaintext in the first block.  If you use this definition for chained CBC - where you use the final block of a segment as the IV for the next segment - it also eliminates the known attack (whose name escapes me - it was based on an attacker being able to insert a prefix to the next segment because he knows the IV it will use before it gets sent) that even caused problems for CBC modes in either SSH or TLS.
Beyond this, it changes the requirements on the IV as provided by the user from "random" to "never repeated for any given key" - an easier requirement that's much less likely to be accidentally violated.
The cost of this is one extra block encryption at each end of the link, per CBC segment - pretty trivial.  When I implemented a protocol that relied on CBC, I made this the exposed primitive.  When I later learned of the prefix attack, I was gratified to see that my code was already immune.
It actually amazes me that anyone uses the raw IV for the first XOR any more.
                                                        -- Jerry

@_date: 2013-09-10 18:03:58
@_author: Jerry Leichter 
@_subject: [Cryptography] how could ECC params be subverted & other 
Also keep in mind that we're not seeing the full documents exfiltrated by Snowden.  Snowden may have marked some material as "not for public release" when he gave it to the papers; the papers themselves go over it; and the papers have told us that they also talk to the government and sometimes are asked not to release certain material - though they may ignore the request.  I would also assume that the newspapers have gotten some technically competent people involved to advise them as well.
It's possible that the original documents hinted at other places that the the NSA mounted such attacks.  This is getting very close to "means and methods", and the government may have requested that none of this be released.  But the newspapers could well have pushed back and decided that the fact of such attacks is too important to suppress completely, so they compromised by only mentioning an attack with little practical import.
                                                        -- Jerry
PS  After the harassment of David Miranda in the UK, Glenn Greenwald responded that in retaliation he would now release even more material than he had previously planned.  And, in fact, there's been some recent trend for the material leaked to be more specific.  I have my doubts that personal pique should have a role in the journalistic process, but Greenwald is only human. We're in an unprecedented situation - though one much discussed in many spy thriller and science fiction books in the past - where a small group of individuals has (apparently well protected) access to information that can do really serious damage to a large organization.  One wonders if the intelligence community has quite come to grips with what a dangerous position they have found themselves in.

@_date: 2013-09-10 18:36:38
@_author: Jerry Leichter 
@_subject: [Cryptography] Availability of plaintext/ciphertext pairs (was 
As I recall the proposal, it was the same key.  (Generating a different one for this purpose is pointless - it would have to be random, in which case you might as well generate the IV randomly.)
I searched around but couldn't find it; the proposal apparently was not Rogoway's.  It apparently appears in NIST 800-38A (2001), with no citation.  In searching around, I came across a recent, unpublished paper by Rogoway:  That paper - which does detailed analyses of a large number of modes - indicates that more recent work has shown that this technique for choosing an IV is *not* secure (against a certain class of attacks) and recommends against using it.
I highly recommend that paper.  In fact, I highly recommend everything Rogoway has written.  We've been discussing authentication and session key exchange - he and Bellare wrote about the problem in 1993  giving provably secure algorithms for the 2-party case, and two years later  extending the work to the 3-party case.
                                                        -- Jerry

@_date: 2013-09-10 20:26:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Availability of plaintext/ciphertext pairs (was 
Sure.  The intent was just that the ciphertext starts with E1.  IV has to be known to both sides, but it's part of the setup, not the ciphertext.
That's certainly true.  On the other hand, there's no mode I know of that denies the attacker access to (guessed or known or even chosen, if the attacker has a way to influence the data sent) plaintext/ciphertext pairs = which is exactly why no one would even begin to consider a cipher these days unless it was convincingly secure against chosen ciphertext attack.  (Before roughly the 1950's, it was the working rule that plaintext transmitted via a cryptosystem was never released.  Embassies receiving announcements via an enciphered channel would paraphrase them before making them public.)
I responded to Perry.
No, something much simpler.  Consider a situation in which there's an ongoing exchange of messages:  Alice sends to Bob, Bob responds to Alice.  Alice uses CBC and just continues the CBC sequence from one message to the next.  In effect, this presents Eve with the initiation of a new CBC session, with a known IV.  Between the end of one of Alice's messages to Bob and the next, Eve knows the next IV.
Suppose Eve also knows a previously-transmitted plaintext block P2.  Let P1 be the (unknown) plaintext block immediately preceding P2.  P2 was transmitted as
If Eve re-inserts C2 as the first message of the response to Bob, Bob will decrypt it as IV XOR D(C2) == IV XOR E(P1) XOR P2.  Thus Eve gets Bob to accept P2 modified by XOR'ing with a known quantity - which is not supposed to be possible in a secure mode.
BTW, this reveals an interesting and little-mentioned assumption about CBC:  That Eve can't insert a ciphertext block between two of Alice's in the time between two blocks.  Probably not a good assumption on a packet network, actually.
The older literature requires that the IV be "unpredictable" (an ill-defined term), but in fact if you want any kind of security proofs for CBC, it must actually be random.
Incorrect on multiple levels.  See the paper I mentioned in my response to Perry.
                                                        -- Jerry

@_date: 2013-09-10 23:29:44
@_author: Jerry Leichter 
@_subject: [Cryptography] Availability of plaintext/ciphertext pairs (was 
You should probably look at the Rogoway paper I found after Perry pushed me to give a reference.  Yes, CBC with a true random IV is secure, though the security guarantee you can get if you don't also do authentication is rather weak.  The additional padding almost certainly doesn't help or hurt.  (I won't say that any more strongly because I haven't look at the proofs.)
                                                        -- Jerry

@_date: 2013-09-11 13:13:22
@_author: Jerry Leichter 
@_subject: [Cryptography] About those fingerprints ... 
By announcing it publicly, they put themselves on the line for lawsuits and regulatory actions all over the world if they've lied.
Realistically, what would you audit?  All the hardware?  All the software, including all subsequent versions?
This is about as strong an assurance as you could get from anything short of hardware and software you build yourself from very simple parts.
Apparently not enough to give *you* confidence.  But concerned as I am with recent revelations, it doesn't particularly concern *me* nearly as much as many other attack modalities.
There's been some very limited auditing by outsiders.  I found one paper a while back that teased apart the format of the file and figured out how the encryption worked.  It appeared to be secure (if perhaps overly complicated), but damned if I can find the paper again.  (Searching these days turns up tons of articles that center about the fact that when a keychain is unlocked, you can read its contents.  The vulnerability issues are subtle, but they only apply at all if you're on the same machine as the unlocked keychain.)
It would be a nice thing if Apple described the algorithms used to encrypt keychains.  Perhaps this is the time to push them - and others - to be much more open about their security technologies.  Apple seems to be making a point of *selling* on the basis of those technologies, so may be particularly willing/vulnerable on this front.
You don't.
Then again, you don't know if Intel has been forced to include something in its chips that allows someone with appropriate knowledge to download and run privileged code on your machine.  All modern Intel server chips include a special management mode exactly to allow remote control over servers in a large datacenter, regardless of how screwed up the software, including the OS software, on them gets.  Who's to say there isn't some other way to get into that code?
Who you choose to trust and how much is ultimately your call.  There are no answers to your questions.
                                                        -- Jerry

@_date: 2013-09-11 15:01:35
@_author: Jerry Leichter 
@_subject: [Cryptography] About those fingerprints ... 
I wouldn't take it quite that far.  Government agencies always claim broader leeway than is granted to private actors - and even for NSA and friends, exactly how the courts will react to that language parsing isn't clear.  Even their pet FISA court, we now know from declassified documents, has angrily rejected some of this game playing - a second report of this is in today's New York Times.  Not that it did much good in terms of changing behavior.  And Congress, of course, can interpret stuff as it likes.
The standard for civil lawsuits and even more so for regulatory actions is quite a bit lower.  If Apple says "no fingerprint information leaves the phone", it's going to be interpreted that way.  Another article in today's Times reports that Google has had its argument that WiFi is "radio" hence not subject to wiretap laws soundly rejected by an appeals court.
 starting at about 2:20, is one statement.  I won't try to transcribe it here, but short of a technical paper, it's about a strong and direct a statement as you're going to get.  People have a queasy feeling about fingerprint recognition.  If Apple wants to get them to use it, they have to reassure them.  It's a basic result of game theory that the only way you can get people to believe you is to put yourself in harm's way if you lie.
What counts as indirect access?  In one sense, the answer to this question is yes:  You can use your fingerprint to authorize purchases - at least from the iTunes/App store.  It's completely unclear - and Apple really should explain - how the authentication flows work.  Since they promise to keep your fingerprint information on the phone, they can't be sending it off to their store servers.  On the other hand, if it's a simple "go ahead and authorize a charge for user U" message, what's to prevent someone from faking such a message?  (In other words:  If the decision is made on the phone, how do you authenticate the phone?)
Courts are not nearly as willing to let those before them hide behind complicated and unusual word constructions as you think - especially not when dealing with consumers.  It is, in fact, a standard rule of contract law that any ambiguity is to be interpreted contrary to the interests of the drafter.
None of this is specific to Apple.  Commerce depends on trust, enforced ultimately by courts of law.  The ability to govern depends on the consent of the governed - yes, even in dictatorships; they survive as long as enough of the population grudgingly accedes, and get toppled when they lose too much of the consent.  And consent ultimately also requires trust.
The games the intelligence community have been playing are extremely corrosive to that trust, which is why they are so dangerous.  But we have to get beyond that and not see attackers behind every door.  The guys who need to be stopped run the NSA and friends, not Apple or Facebook or Google - or even the Telco's.
                                                        -- Jerry

@_date: 2013-09-11 17:56:07
@_author: Jerry Leichter 
@_subject: [Cryptography] Random number generation influenced, HW RNG 
It isn't.  See On the underlying issue of whether a software model of a hardware RNG could be accurate enough for ... some not-quite-specified purpose:  Gate-level simulation of circuits is a simple off-the-shelf technology.  If the randomness is coming from below that, you need more accurate simulations, but *no one* builds a chip these days without building a detailed physical model running in a simulator first.  The cost of getting it wrong would be way too large.  Some levels of the simulation use public information; at some depth, you probably get into process details that would be closely held.
Since it's not clear exactly how you would use this detailed model to, say, audit a real hardware generator, it's not clear just how detailed a model you would need.
                                                        -- Jerry

@_date: 2013-09-11 18:02:33
@_author: Jerry Leichter 
@_subject: [Cryptography] Why prefer symmetric crypto over public key 
This is not *necessarily* safe.  In another thread, we discussed whether choosing the IV for CBC mode by encrypting 0 with the session key was sufficient to meet the randomness requirements.  It turns out it does not.  I won't repeat the link to Rogoway's paper on the subject, where he shows that using this technique is strictly weaker than using a true random IV.
That doesn't mean the way it's done in Ed25519 is unsafe, just that you cannot generically assume that computing a random value from existing private information is safe.
                                                        -- Jerry

@_date: 2013-09-11 18:34:56
@_author: Jerry Leichter 
@_subject: [Cryptography] Availability of plaintext/ciphertext pairs (was 
The real problem is that "unpredictable" has no definition.  E(0) with the session key is "unpredictable" to an attacker, but as the paper shows, it cannot safely be used for the IV.  Rogoway specifically says that if what you mean by "unpredictable" is "random but biased" (very informally), then you lose some security in proportion to the degree of bias:  "A quantitative statement of such results would ?give up? in the ind$ advantage an amount proportional to the ?(q, t) value defined above."
I actually have no problem with your rephrased statement.  My concern was the apparently flippant dismissal of all "academic" work as "assuming a can opener".  Yes, there's some like that.  There's also some that shows how given weaker assumptions you can create a provably secure block cipher (though in practice it's not clear to me that any real block cipher is really created that way).  Beyond that, "provably secure" is slippery - there are many, many notions of security.  Rogoway's paper gives a particular definition for "secure" and does indeed show that if you have a random IV, CBC attains it.  But he also points out that that's a very weak definition of "secure" - but without authentication, you can't get any more.
Do I wish we had a way to prove something secure without assumptions beyond basic mathematics?  Absolutely; everyone would love to see that.  But we have no idea how to do it.  All we can do is follow the traditional path of mathematics and (a) make the assumptions as clear, simple, limited, and "obvious" as possible; (b) show what happens as the assumptions are relaxed or, sometimes, strengthened.  That's what you find in the good cryptographic work.  (BTW, if you think I'm defending my own work here - far from it.  I left academia and theoretical work behind a very long time ago - I've been a nuts-and-bolts systems guy for decades.)
On the matter of a secret IV:  It can't actually help much.  Any suffix of a CBC encryption (treated as a sequence of blocks, not bytes) is itself a valid CBC encryption.  Considered on its own, it has a secret IV; considered in the context of the immediately preceding block, it has a non-secret IV.  So a secret IV *at most* protects the very first block of the message.  I doubt anyone has tried to formalized just how much it might help simply because it's so small.                                                         -- Jerry

@_date: 2013-09-11 20:01:28
@_author: Jerry Leichter 
@_subject: [Cryptography] Killing two IV related birds with one stone 
Of course, now you're going to need to agree on two keys - one for the main cipher, one of the IV-generating cipher.  Seems like a great deal of trouble to go to to rescue a mode with few advantages.  (Perry and I exchanged some private mail on this subject.  He claims CBC has an advantage over CTR because CTR allows you to deterministically modify the plaintext "under" the encryption.  I used to favor CBC for that reason as well, though in fact you can modify the text anyway by replaying a previous block - it's just harder to control.  I've become convinced, though, the CBC without authentication is way too insecure to use.  Once you require authentication, CBC has no advantages I can see over CTR.)
But if you insist on CBC ... it's not clear to me whether the attack in Rogoway's paper goes through once authentication is added.  If it doesn't, E(0) does just fine (and of course doesn't have to be transmitted).
Ah, but where did the session and IV-generating keys come from?  The same random generator you now don't trust to directly give you an IV?
                                                        -- Jerry

@_date: 2013-09-12 13:42:22
@_author: Jerry Leichter 
@_subject: [Cryptography] About those fingerprints ... 
[Perry - this is likely getting too far off-topic, but I've included the list just in case you feel otherwise.  -- J]
Well ... there's an interesting point here.  If it's an open source library - what stops you from auditing it today?
On OSX, at least, if I were to worry about this, I'd just replace the libraries with my own compiled versions.  Apple has a long history of being really slow about updating the versions of open source software they use.  Things have gotten a bit better, but often through an odd path:  Apple gave up on maintaining their own release of Java and dumped the responsibility back on Oracle (who've been doing a pretty miserable job on the security front).  For the last couple of years, Apple distributed a X client which was always behind the times - and there was an open source effort, XQuartz, which provided more up to date versions.  Recently, Apple decided to have people pull down the XQuartz version.  (For both Java and X, they make the process very straightforward for users - but the important point is that they're simply giving you access to someone else's code.)  They've gone this way with a non-open source component as well, of course - Flash.  They never built it themselves; now they don't even give you help in downloading it.
But ... suppose you replaced, say, OpenSSL with your own audited copy.  There are plenty of ways for code that *uses* it to leak information, or just misuse the library.  On top of that, we're mainly talking user-level code.  Much of the privileged code is closed source.
So ... I'm not sure were auditing gets you.  If you're really concerned about security, you need to use trusted code throughout.  A perfectly reasonable choice, perhaps - though having regularly used both Linux and MacOS, I'd much rather use MacOS (and give up on some level of trustworthiness) for many kinds of things.  (There are other things - mainly having to do with development and data analysis - that I'd rather do on Linux.)
I agree with you.  I really wish they'd make (much) more information available about this.  But none of this is open source.  I've seen some analysis done by people who seem to know their stuff, and the way keys are *currently* kept on iOS devices is pretty good:  They are encrypted using device-specific data that's hard to get off the device, and the decrypted versions are destroyed when the device locks.  But there are inherent limits as what can be done here:  If you want the device to keep receiving mail when it's locked, you have to keep the keys used to connect to mail servers around even when it's locked.
Keychain syncing was part of the old .Mac stuff, and in that case it was clear: They simply synced the keychain files.  As I said, there is some information out there about how those are secured, and as best I've been able to determine, they are OK.  I wish more information was available.
It's not clear whether iCloud will do something different.  Apparently Apple removed keychain syncing from the final pre-release version of iOS 7 - it's now marked as "coming soon".  The suspicion is that in the post-Snowden era, they decided they need to do something more to get people to trust it.  (Or, I suppose, they may actually have found a bug....)  We'll see what happens when they finally turn it on.
Developers aren't *required* to use any particular API's.  Could there be some additional crypto libraries that they've kept private?  There's no way to know, but it's not clear why they would bother.  The issue is presumably that NSA might force them to include a back door in the user-visible libraries - but what would Apple gain, beyond the extra work of maintaining two sets of code, and likely future headaches on the legal front - by maintaining a separate library? What's exposed either way is *your* data, not Apple's!
Sure.  But you're highly unlikely to get detailed answers.  Then it's your choice Apple doesn't see "people (or even organizations) who want to, and are able to, do detailed security audits on their hardware and software" as a significant market.  And they are probably right - we're talking *at most* thousands of people in the entire world.  So why should they cater to them - a process with significant cost?  Many large, highly security-concious organizations are satisfied with the guarantees they get.
There are specialized vendors of phones that come with tighter security guarantees.  BlackBerry used to be one, though the recent NSA leaks have tarnished their image - perhaps without actual justification.  They operate on the "trust us" principle, too.  General Dynamics makes cell phones for the US government - see for the phone that was allegedly given to President Obama in place of his beloved BlackBerry.  One of these babies will set you back $3345, but it does come with likely the best assurance you can find anywhere that it's as secure as the NSA can make it against outsiders - but of course whether it's secure *against* the NSA is a whole other question.
Well, you can always review the EULA.  Be prepared to set aside a *lot* of time.
                                                        -- Jerry

@_date: 2013-09-13 23:32:47
@_author: Jerry Leichter 
@_subject: [Cryptography] Thoughts on hardware randomness sources 
I'm not sure what servers you're talking about here.
If by server you mean one of those things in a rack at Amazon or Google or Rackspace - power consumption, and its consequence, cooling - is *the* major issue these days.  Also, the servers used in such data centers don't have multiple free USB inputs - they may not have any.
If by server you mean some quite low-power box in someone's home ... power is again an issue.  People want these things small, fan-free, and dead reliable.  And they are increasingly aware of the electric bills always-on devices produce.
About the only "server" for which power is not an issue is one of those extra-large desktops that small businesses use.
                                                        -- Jerry

@_date: 2013-09-13 23:22:11
@_author: Jerry Leichter 
@_subject: [Cryptography] Credit for Trusting Trust 
Just to give credit where credit is due:  Ken Thompson didn't invent this attack, and cites the originators - Paul Karger and Roger Schell, way back in 1974, 10 years before Thompson.  (Thompson may have produced the first working example.)  Karger and Schell's work was done for the Air Force as part of an analysis of the security of Multics.  I never met Roger Schell, but I knew Paul at DEC back in the mid 70's.  Not realizing his connection with the underlying ideas, I showed him Thompson's paper.  Paul explained how to counter it by examining the compiler output (not practical except in specialized circumstances) but never brought up his own role.
Sadly, he died too young in 2010.  He deserves to be credited.
The full details can be found on David A. Wheeler's page at   (Wheeler's 2005 dissertation provides a complete solution to the problem; he cites Henry Spencer for suggesting the idea underlying his formal treatment back in 1998.)
                                                        -- Jerry

@_date: 2013-09-15 01:56:39
@_author: Jerry Leichter 
@_subject: [Cryptography] real random numbers 
You've completely missed what Denker was getting at with "squish".  "Squish" never applies to a fully characterized, deterministic component like a hash.  "Squish" is an "unknown unknown":  Data that you don't understand, so you think it might be random, but you really can't be sure.  Consider the example he responded to, that comparing the clocks on the CPU and on the sound card "should be usable" as a source of randomness.  If you dig in to what "should be usable" means here, it comes down to:  Both clocks show some degree of random variation, and the random variation is uncorrelated.  That might be true - or it might not:  Perhaps there's some path you haven't thought of through the power supply that tends to synchronize the two.  Lack of imagination on the analyst's part does not equate to lack of correlation (or other failure modes) on the system's part!  (In fact, the world is full of unexpected couplings between nominally independent events.  I've debugged and fought failures in systems built on the unsupported assumption that "things will smooth out on average".  They are always unexpected, and can be difficult to find after the fact.  And ... people don't seem to learn the lesson:  The next system makes the same bad assumptions.)
As Denker said:  Adding "squish" as a source of confusion in a well implemented mixer is at worst harmless - if you want to do it, go ahead.  But adding it as a source of an entropy estimate is wrong.  Either you have some way of estimating the entropy based on real physical modeling, or you're just making things up - and "just making things up" is not the way to build a secure system.
                                                       -- Jerry

@_date: 2013-09-16 15:36:48
@_author: Jerry Leichter 
@_subject: [Cryptography] The paranoid approach to crypto-plumbing 
This is trickier than it looks.
Joux's paper "Multicollisions in iterated hash functions" shows that "finding ... r-tuples of messages that all hash to the same value is not much harder than finding ... pairs of messages".  This has some surprising implications.  In particular, Joux uses it to show that, if F(X) and G(X) are cryptographic hash functions, then H(X) = F(X) || G(X) (|| is concatenation) is about as hard as the harder of F and G - but no harder.
That's not to say that it's not possible to combine multiple instances of cryptographic primitives in a way that significantly increases security.  But, as many people found when they tried to find a way to use DES as a primitive to construction an encryption function with a wider key or with a bigger block size, it's not easy - and certainly not if you want to get reasonable performance.
                                                        -- Jerry

@_date: 2013-09-16 19:02:16
@_author: Jerry Leichter 
@_subject: [Cryptography] The paranoid approach to crypto-plumbing 
Yes, this is the kind of thing that makes crypto fun.
The feeling these days among those who do such work is that unless you're going to use a specialized combined encryption and authentication mode, you might as well use counter mode (with, of course, required authentication).  For the encryption part, counter mode with multiple ciphers and independent keys has the nice property that it's trivially as strong as the strongest of the constituents.  (Proof:  If all the ciphers except one are cracked, the attacker is left with a known-plaintext attack against the remaining one.  The need for independent keys is clear since if I use two copies of the same cipher with the same key, I end up sending plaintext!  You'd need some strong independence statements about the ciphers in the set if you want to reuse keys.  Deriving them from a common key with a one-way hash function is probably safe in practice, though you'd now need some strong statements about the hash function to get any theoretical result.  Why rely on such things when you don't need to?)
It's not immediately clear to me what the right procedure for multiple authentication is.
                                                        -- Jerry

@_date: 2013-09-17 11:01:46
@_author: Jerry Leichter 
@_subject: [Cryptography] The paranoid approach to crypto-plumbing 
How about the X Property (Trust No One - with a different slant on "One")?
                                                        -- Jerry :-)

@_date: 2013-09-17 12:15:48
@_author: Jerry Leichter 
@_subject: [Cryptography] paranoid cryptoplumbing is a probably not 
Actually, I think there is a potentially interesting issue here:  RC4 is faster and requires significantly fewer resources than modern block ciphers.  As a result, people would really like to use it - and actually they *will* continue to use it even in the face of the known attacks (which, *so far*, are hardly fatal except in specialized settings).
So ... is there some simple way of combining RC4 with *something* that maintains the performance while retaining the speed?  How about two separate RC4's (with independent keys) XOR'ed together?  That would still be considerably faster than AES.
There appear to be two general classes of known attacks:
1.  The initial key setup doesn't produce enough randomness;
2.  There are long-term biases in the output bytes.
The first of these can be eliminated by using AES to generate values to scramble the internal state.  The second can be hidden by doing post-whitening, XOR'ing in a byte from AES in (say) counter mode.  If you use a given byte 64 times, then use the next byte of the output, you pay 1/64 the cost of actually using AES in counter mode, but any bias in the output would have to play out over a 64-byte segment.  (Actually, if you use ideas from other stream ciphers, changing the whitening every 64 bytes probably isn't right - you want the attacker to have to guess where the changeovers take place.  There are many ways to do that.)
Of course, don't take any of the above and go build code.  It's just speculation and likely has serious problems.  I toss it out to illustrate the idea.  Whether it's actually worthwhile ... I doubt it, but it's worth thinking about.
                                                        -- Jerry

@_date: 2013-09-17 19:05:48
@_author: Jerry Leichter 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
Given our state of knowledge both of the mathematics, and of games NSA has been playing, I don't believe anyone can give a meaningful answer to that question.  There's a second, related question:  How are attacks on the two systems correlated?  If one falls, do we need to lower our estimate of the strength of the other?  In the case of an attack using a practical quantum computer, "very strongly correlated"; in the case of improvements along the lines of current integer factoring algorithms, "not very strongly correlated".  Over all, one has to make guesses.  I'd put them as "somewhat correlated".
                                                        -- Jerry

@_date: 2013-09-17 19:18:20
@_author: Jerry Leichter 
@_subject: [Cryptography] The paranoid approach to crypto-plumbing 
On a purely practical level, to reject a damaged message, with decrypt-then-MAC (ordering things on the receiver's side...) I have to pay the cost of a decryption plus a MAC computation; with MAC-then-decrypt, I only pay the cost of the MAC.  On top of this, decryption is often more expensive than MAC computation.  So decrypt-then-MAC makes DOS attacks easier.
One could also imagine side-channel attacks triggered by chosen ciphertext.  Decrypt-then-MAC allows an attacker to trigger them; MAC-then-decrypt does not. (Attacks on MAC's seems somewhat less likely to be data dependent, but who knows for sure.  In any case, even if you had such an attack, it would get you the authentication key - and at that point you would be able to *start* your attack not the decryption key.
MAC'ing the actual data always seemed more "logical" to me, but once you look at the actual situation, it no longer seems like the right thing to do.
                                                        -- Jerry

@_date: 2013-09-18 07:05:06
@_author: Jerry Leichter 
@_subject: [Cryptography] Some (limited) info about Apple A7 security for 
A level beyond marketing talk, but nowhere near technical detail.  Still a bit more than has been previously described.  Links to some perhap
There's a link to an ARM site with a reasonable overview of the ARM TEE (Trusted Execution Environment) - which Apple's "Secure Enclave" may (or may not) be based on.  Referring back to a point Perry made a while back:  TEE mode runs its own specialized "secure" OS.  That would seem to be an ideal target for verification....
                                                        -- Jerry

@_date: 2013-09-22 09:43:21
@_author: Jerry Leichter 
@_subject: [Cryptography] RSA recommends against use of its own products. 
Wow.  You took as holy writ on a technical matter a pronouncement of the general press.  And not just of the general press, but of RT, a Russian publication with a clear pro-Russian anti-American bias.  (Not that they don't deliver useful information - I've read them in the past, along with Al Jazeera and, on the flip side, The Debka Files.  They are all valid and useful members of the press, but their points of view, AKA biases, are hardly a secret.)
The original article in Wired is still a bit sensationalistic, but at least it gets the facts right.
BSAFE is a group of related libraries of crypto primitives that RSA has sold for many years.  They generally implement everything in the relevant standards, and sometimes go beyond that and include stuff that seems to be widely accepted and used.  Naturally, they also use the same libraries in some packaged products they sell.  As far as I know, the BSAFE implementations have been reasonably well thought of over the years.  In my experience, they are conservatively written - they won't be the fastest possible implementations, but they'll hold their own, and they probably are relatively bug-free.  A big sales advantage is that they come with FIPS certifications.  For whatever you think those are actually worth, they are required for all kinds of purposes, especially if you sell products to the government.
I remember looking at BSAFE for use in a product I worked on many years ago.  We ended up deciding it was too expensive and used a combination of open source code and some stuff we wrote ourselves.  The company was eventually acquired by EMC (which also owns RSA), and I suspect our code was eventually replaced with BSAFE code.
Since Dual EC DRBG was in the NIST standards, BSAFE provided an implementation - along with five other random number generators.  But they made Dual EC DRBG the default, for reasons they haven't really explained beyond "in 2004 [when these implementations were first done] elliptic curve algorithms were becoming the rage and were considered to have advantages over other algorithms...."
I'd guess that no one remembers now, six or more years later, exactly how Dual EC DRBG came to be the default.  We now suspect that a certain government agency probably worked to tip the scales.  Whether it was directly through some contacts at RSA, or indirectly - big government client says they want to buy the product but it must be "safe by default", and "Dual EC DRBG is what our security guys say is safe" - who knows; but the effect is the same.  (If there are any other default choices in BSAFE, they would be worth a close look.  Influencing choices at this level would have huge leverage for a non-existent agency....)
Anyway, RSA has *not* recommended that people stop using BSAFE.  They've recommended that they stop using *Dual DC DRBG*, and instead select one of the other algorithms.  For their own embedded products, RSA will have to do the work.  Existing customers most likely will have to change their source code and ship a new release - few are likely to make the RNG externally controllable.  Presumably RSA will also issue new versions of the BSAFE products in which the default is different.  But it'll take years to clean all this stuff up.
                                                        -- Jerry

@_date: 2013-09-22 17:23:08
@_author: Jerry Leichter 
@_subject: [Cryptography] 
=?windows-1252?q?Technology_Animation?=
The question isn't whether it's what it claims to be.  It is that.  But is it's *more* than it claims to be.
There are a whole bunch of things in recent Intel chips to provide manageability and security.  And there are cases where this is very valuable and necessary - e.g., if you have a large cluster or processors, it's good to be able to remotely configure them no matter what state they are in.  There are many similar examples.  If it's *your* hardware, *your* ability to control it, in detail, is a good thing.  (Yes, if you've been lent the hardware by your employer, it's the *employer* who's the owner, not you, and it's the *employer* who can do what he likes.  This has always been the case to a large degree.  If it makes you uncomfortable - buy your own machine, don't use your work machine for non-work things.)
The *theory* is that the owner can enable or disable these features, and has the keys to access them if enabled.  What we don't know is whether anyone else has a back-door key.  The phrase I always use to describe such situations is "if there's a mode, there's a failure mode".  Such technology could have been present in previous generations of chips, completely invisibly - but it would have required significant effort on Intel's part with no real payback.  But once Intel is adding this stuff anyway ... well, it's only a small effort to provide a special additional back door access.
                                                        -- Jerry

@_date: 2013-09-22 21:07:13
@_author: Jerry Leichter 
@_subject: [Cryptography] 
=?windows-1252?q?Technology_Animation?=
You started off concerned about misuse of a "remote override" function that Intel deliberately puts on the chips - a valid concern - but now have wandered off into arbitrary chip modifications.  Those, too, are perhaps valid concerns - but they've been concerns for many years.  Nothing new here, except that the deeper we look, the more ways we find to hide attacks within the hardware.
That said, the doping paper, if I understood the suggestion correctly, discussed a way to modify individual chips, not whole runs of them.  (Presumably you could modify whole runs by spiking the production process, but that would be difficult to hide:  Chip manufacturing is by its nature a very tightly controlled process, and an extra step isn't something that people would miss.  It would probably even show up in the very tightly watched yield statistics:  The extra step would delay wafers on the line, which would cause the yield to drop.  The beauty of the doping attack is that it's undetectable - at least right now; for every attack, a defense; for every defense, an attack.  But exactly how one might make the *implementation* of the attack undetectable isn't at all clear.)
You'll be amazed at how slow they now seem....
Still, it raises the question:  If you can't trust your microprocessor chips, what do you do?  One possible answer:  Build yourself a processor out of MSI chips.  We used to do that, not so long ago, and got respectable performance (if not, perhaps, on anything like today's scale).  An MSI chip doesn't have enough intrinsic computation to provide much of a hook for an attack.  Oh, sure, the hardware could be spiked - but to do *what*?  Any given type of MSI chip could go into many different points of many different circuit topologies, and won't see enough of the data to do much anyway.  There may be some interface issues:  This stuff might not be fast enough to deal with modern memory chips.  (How would you attack a memory chip?  Certainly possible if you're make a targeted attack - you can slip in a small processor in the design to do all kinds of nasty things.  But commercial of the shelf memory chips are built right up to the edge of what we can make, so you can't change all that much.)
Some stuff is probably just impossible with this level of technology.  I doubt you can build a Gig-E Ethernet interface without large-scale integration.  You can certainly do the original 10 Mb/sec - after all, people did!  I have no idea if you could get to 100 Mb/sec.
Do people still make bit-slice chips?  Are they at a low-enough level to not be a plausible attack vector?
You could certainly build a respectable mail server this way - though it's probably not doing 2048-bit RSA at a usable speed.
We've been talking about crypto (math) and coding (software).  Frankly, I, personally, have no need to worry about someone attacking my hardware, and that's probably true of most people.  But it's *not* true of everyone.  So thinking about how to build "harder to attack" hardware is probably worth the effort.
                                                        -- Jerry

@_date: 2013-09-24 10:59:42
@_author: Jerry Leichter 
@_subject: [Cryptography] The hypothetical random number generator backdoor 
You shifted from "random value" to "nonce".  Given the severe effects on security that using a "nonce" - a value that is simply never repeated in a given cryptographic context; it may be predictable, even fixed - to a "random value", one needs to be careful about the language.  (There's another layer as well, partly captured by "unpredictable value" but not really:  Is it a value that we must plan on the adversary learning at some point, even though he couldn't predict it up front, or must it remain secret?  The random values in PFS are only effective in providing forward security if they remain secret forever.)
Anyway, everything you are talking about here is *supposed* to be a random value.  Using E(R,k) is a slightly complicated way of using a standard PRNG:  The output of a block cipher in counter mode.  Given (a) the security of the encryption under standard assumptions; (b) the secrecy and randomness of k; the result is a good PRNG.  (In fact, this is pretty much exactly one of the Indistinguishability assumptions.  There are subtly different forms of those around, but typically the randomness of input is irrelevant - these are semantic security assumptions so knowing something about the input can't help you.)  Putting R in there can't hurt, and if the way you got R really is random then even if k leaks or E turns out to be weak, you're still safe.  However ... where does k come from?  To be able to use any of the properties of E, k itself must be chosen at random.  If you use the same generator as way use to find R, it's not clear that this is much stronger than R itself.  If you have some assured way of getting a random k - why not use it for R itself?  (This might be worth it if you can generate a k you believe in but only at a much lower rate than you can generate an R directly.  Then you can "stretch" k over a number of R values.  But I'd really think long and hard about what you're assuming about the various components.)
BTW, one thing you *must not* do is have k and the session key relate to each other in any simple way.
For hash and XOR ... no standard property of any hash function tells you anything about the properties of R XOR H(R).  Granted, for the hash functions we generally use, it probably has about the same properties; but it won't have any more than that.  (If you look at the structure of classic iterated hashes, the last thing H did was compute S = S + R(S), where S was the internal state and R was the round function.  Since R is usually invertible, this is the only step that actually makes the whole thing non-invertible.  Your more-or-less repetition of the same operation probably neither helps more hinders.)
At least if we assume the standard properties, it's hard to get R from H(R) - but an attacker in a position to try a large but (to him) tractable number of guesses for R can readily check them all.  Using R XOR H(R) makes it no harder for him to try that brute force search.  I much prefer the encryption approach.
                                                        -- Jerry

@_date: 2013-09-24 12:01:58
@_author: Jerry Leichter 
@_subject: [Cryptography] RSA recommends against use of its own products. 
The conclusion it leads to is that *if used in the default mode*, it's (well, it *may be*) unsafe.  We know no more today about the quality of the implementation than we did yesterday.  (In fact, while I consider it a weak argument ... if NSA had managed to sneak something into the code making it insecure, they wouldn't have needed to make a *visible* change - changing the default.  So perhaps we have better reason to believe the rest of the code is OK today than we did yesterday.)
a)  How would knowing this change the actions you take today?
b)  You've posed two alternatives as if they were the only ones.  At the time this default was chosen (2005 or thereabouts), it was *not* a "mistake".  Dual EC DRBG was in a just-published NIST standard.  ECC was "hot" as the best of the new stuff - with endorsements not just from NSA but from academic researchers.  Dual EC DRBG came with a self-test suite, so could guard itself against a variety of attacks and other problems.  Really, the only mark against it *at the time* was that it was slower than the other methods - but we've learned that trading speed for security is not a good way to go, so that was not dispositive.
Since we know (or at least very strongly suspect) that the addition of Dual EC DRBG to the NIST standards was influenced by NSA, the question of whether RSA was also influenced is meaningless:  If NSA had not gotten it into the standard, RSA would probably not have implemented it.  If you're asking whether NSA directly influenced RSA to make it the default - I doubt it.  They had plenty of indirect ways to accomplish the same ends (by influencing the terms of government purchases to make that a requirement or a strong suggestion) without leaving a trail behind.
And?  It's cool for discussion, but has absolutely nothing to do with whether (a) BSAFE is, indeed, safe if you use the current default (we assume not, at least against NSA); (b) BSAFE is safe if you *change* the default (most will likely assume so); (c) users of BSAFE or BSAFE-based products should make sure the default is not used in products they build or use (if they're worried about NSA, sure) (d) implementors and users of other crypto libraries should change what they are doing (avoid Dual EC DRBG - but we already knew that).
                                                        -- Jerry

@_date: 2013-09-24 23:01:56
@_author: Jerry Leichter 
@_subject: [Cryptography] The hypothetical random number generator backdoor 
Then I'm mystified by your proposal.
If enough bits are leaked to make it possible to feed all possible values of the generated value R into whatever algorithm uses them (and, of course, recognize when you've hit the right one), then the extra cost of instead replacing each such value R with R XOR H(R) is trivial.  No fixed transformation can help here - it's no different from using an RNG with problem 1 and whitening its output:  It now looks strong, but isn't.  (In fact, in terms of "black box" behavior to someone who doesn't know about the limited randomness/internal loss/side channel, these three weaknesses are functionally equivalent - and are subject to exactly the same attacks.)
The encryption approach - replacing R by E(k,R) - helps exactly because the key it uses is unknown to the attacker.  As I said before, this approach is fine, but:  Where does this magic random key come from; and given that you have a way to generate it, why not use that way to generate R directly rather than playing games with code you don't trust?
                                                        -- Jerry

@_date: 2013-09-25 10:19:21
@_author: Jerry Leichter 
@_subject: [Cryptography] The hypothetical random number generator backdoor 
Well, that depends on how they are used and what you believe about the primitives.
If you use encryption in counter mode - E(k,counter), where k is random - then the assumption that the generated values are random is, as I remarked in another comment, pretty much equivalent to the indistinguishability assumptions that are commonly made about symmetric cryptographic algorithms.  If you don't think you have an appropriately secure symmetric cipher to work with ... it's not clear just what you're going to *do* with your random numbers anyway.
It's harder to know what to make of hashing approaches because it depends on the hash algorithm and what you believe about *it*.  For most uses, a cryptographic hash function just has to prevent first- and second-preimage attacks.  If that's all you are willing to assume your hash function provides, it's enough for the standard, intended uses of such hashes, but not enough to prove much more.  (For example, nothing in these two assumptions, on its own, says that the hash function can't always produce an output whose first bit is 0.)  People generally do assume more, but you really have to be explicit.
Indeed - though you could safely reuse the random key in counter mode up to some limit determined by the security guarantees of the cipher, and if the encryption function lives up to its guarantees, you'll still be secure.  "Stretching" a short random key into a long effectively random sequence is exactly what symmetric encryption functions *do*!
                                                        -- Jerry

@_date: 2013-09-25 14:12:25
@_author: Jerry Leichter 
@_subject: [Cryptography] RSA recommends against use of its own products. 
In favor off ... who?
We already know that GCHQ is at least as heavily into this monitoring business as NSA, so British providers are out.  The French have been playing the "oh, we're shocked, shocked that there's spying going on" game - but they have a long history of their own.  It's been reported for many years that all Air France seats are bugged by the French security services and the information recorded has been used to help French economic interests.  And even if you don't think a particular security service has been going after in-country suppliers, recall decades of US spiking of the Swiss Crypto AG machines.
It's a really, really difficult problem.  For deterministic algorithms, in principle, you can sandbox the implementation (both physically and in software) and compare inputs and outputs to a specification.  That leaves you to worry about (a) holes in the specification itself; (b) physical leakage of extra information (Tempest-like).  Both of these can be dealt with and you can gain any degree of assurance you consider necessary, at least in principle.  Sure, someone can spike your hardware - but if it only does what the spec says it's supposed to do, what does that gain them?  (Storing some of your secrets within the sandboxed system does them no good if they can't get the information out.  Of course, physical security is essential, or your attacker will just walk the system, with all its contained information, out the door!)
For probabilistic algorithms - choosing a random number is, of course, the simplest example - it's much, much harder.  You're pretty much forced to rely on some mathematics and other analysis - testing can't help you much.
There are really no absolutes; you really have to think about who you want to protect yourself from and how much you are willing to spend, because there's no limit on how much you *could* do.  Build your own foundry?  Create your own circuit synthesis code?  You very quickly get yourself into a domain where only a handful of companies or countries can even begin to go.
My take on this:  I don't much worry about attacks against general-purpose hardware.  The difficulty of modifying a processor so that you can tell when it's implementing a cipher and then do something useful about it seems insurmountable.  The exception is when the hardware actually gets into the crypto game - e.g., the Intel AES extensions and the random number generator.  If you're going to use these, you need to do so in a way that's secure even if those features are spiked - e.g., use the random number generator only as one of a couple of sources.
Still, *much* more worrisome are the badly implemented, insecure extensions to allow remote control of the hardware, which are being discussed in a separate thread here.  These are really scary - there's no protection against an attacker who can send a magic packet to your network interface and execute code with full privileges.
Code, at least for symmetric cryptography primitives and modes, is simple enough that you can find it all over the place.  Realistically, the worst attacks against implementations these days are timing attacks.  Bernstein's ciphers have the advantage of being inherently secure against these, showing that this is possible (even if you don't necessarily trust his particular constructions).
Denker's ideas about how to get random numbers whose safety is based on physical principles are great.  You do have to be careful of the hardware and software you use, but since the hardware is designed for entirely different purposes (A/D sound converters) it's unlikely anyone has, or really could, spike them all.
It's the asymmetric algorithms and implementations that seem to be the most vulnerable.  They are complex and difficult to get right, much less to get both efficient *and* right, and protocols that use them generally need to be probabilistic - so "black box testing" isn't feasible.  At the same time, they have rich mathematical structures in which we know things can be hidden.  (In the symmetric case, the algorithms are generally have little mathematical structure, and we *assume* nothing can be hidden in there - but who can really say with absolute confidence.)  I had a long debate here earlier on this subject, and my own conclusions remain:  Use symmetric crypto as little as you possibly can.  (What would be really, really nice is something like DH key exchange without all the mathematical structure.)
                                                        -- Jerry

@_date: 2013-09-29 09:01:37
@_author: Jerry Leichter 
@_subject: [Cryptography] RSA equivalent key length/strength 
The actual algorithms are classified, and about all that's leaked about them, as far as I can determine in a quick search, is the names of some of them, and general properties of a subset of those - e.g., according to Wikipedia, BATON is a block cipher with a key length of 320 bits (160 of them checksum bits - I'd guess that this is an overt way for NSA to control who can use stolen equipment, as it will presumably refuse to operate at all with an invalid key).  It looks as if much of this kind of information comes from public descriptions of equipment sold to the government that implements these algorithms, though a bit of the information (in particular, the name BATON and its key and block sizes) has made it into published standards via algorithm specifiers.  cryptome has a few leaked documents as well - again, one showing BATON mentioned in Congressional testimony about Clipper.
Cryptographic challenge:  If you have a sealed, tamper-proof box that implements, say, BATON, you can easily have it refuse to work if the key presented doesn't checksum correctly.  In fact, you'd likely have it destroy itself if presented with too many invalid keys.  NSA has always been really big about using such sealed modules for their own algorithms.  (The FIPS specs were clearly drafted by people who think in these terms.  If you're looking at them while trying to get software certified, many of the provisions look very peculiar.  OK, no one expects your software to be potted in epoxy ("opaque in the ultraviolet" - or was it infrared?); but they do expect various kinds of isolation that just affect the blocks on a picture of your software's implementation; they have no meaningful effect on security, which unlike hardware can't enforce any boundaries between the blocks.)
Anyway, this approach obviously depends on the ability of the hardware to resist attacks.  Can one design an algorithm which is inherently secure against such attacks?  For example, can one design an algorithm that's strong when used with valid keys but either outright fails (e.g., produces indexes into something like S-boxes that are out of range) or is easily invertible if used with invalid keys (e.g., has a key schedule that with invalid keys produces all 0's after a certain small number of rounds)?  You'd need something akin to asymmetric cryptography to prevent anyone from reverse-engineering the checksum algorithm from the encryption algorithm, but I know of no fundamental reason why that couldn't be done.
                                                        -- Jerry

@_date: 2013-09-29 09:13:41
@_author: Jerry Leichter 
@_subject: [Cryptography] RSA recommends against use of its own products. 
It's standard.  :-)
We've been through two rounds of standard data interchange representations:
1.  Network connections are slow, memory is limited and expensive, we can't afford any extra overhead.  Hence DER.
2.  Network connections are fast, memory is cheap, we don't have to worry about them - toss in every last feature anyone could possibly want.  Hence XML.
Starting from opposite extremes, committees of standards experts managed to produce results that are too complex and too difficult for anyone to get right - and which in cryptographic contexts manage to share the same problem of multiple representations that make signing such a joy.
BTW, the *idea* behind DER isn't inherently bad - but the way it ended up is another story.  For a comparison, look at the encodings Knuth came up with in the TeX world.  Both dvi and pk files are extremely compact binary representations - but correct encoders and decoders for them are plentiful.  (And it's not as if the Internet world hasn't come up with complex, difficult encodings when the need arose - see IDNA.)
                                                        -- Jerry

@_date: 2013-09-30 17:07:13
@_author: Jerry Leichter 
@_subject: [Cryptography] check-summed keys in secret ciphers? 
I'm *guessing* that this is what checksums are for, but I don't actually *know*.  (People used to wonder why NSA asked that DES keys be checksummed - the original IBM Lucifer algorithm used a full 64-bit key, while DES required parity bits on each byte.  On the one hand, this decreased the key size from 64 to 56 bits; on the other, it turns out that under differential crypto attack, DES only provides about 56 bits of security anyway.  NSA, based on what we saw in the Clipper chip, seems to like running crypto algorithms "tight":  Just as much effective security as the key size implies, exactly enough rounds to attain it, etc.  So *maybe* that was why they asked for 56-bit keys.  Or maybe they wanted to make brute force attacks easier for themselves.)
This reminds me of the signature line someone used for years:  A boat in a harbor is safe, but that's not what boats are for.  In some cases you need to communicate securely with someone who's "in harm's way", so any security device you give him is also "in harm's way".  This is hardly a new problem.  Back in WW I, code books on ships had lead covers and anyone who had access to them had an obligation to see they were tossed overboard if the ship was about to fall into enemy hands.  Attackers tried very hard to get to the code book before it could be tossed.
Embassies need to be able to communicate at very high levels of security.  They are normally considered quite secure, but quiet attacks against them do occur.  (There are some interesting stories of such things in Peter Wright's Spycatcher, which tells the story of his career in MI5.  If you haven't read it - get a copy right now.)  And of course people always look at the seizure of the US embassy in Iran.  I don't know if any crypto equipment was compromised, but it has been reported that the Iranians were able, by dint of a huge amount of manual labor, to piece back together shredded documents.  (This lead to an upgrade of shredders not just by the State Department but in the market at large, which came to demand cross-cut shredders, which cut the paper into longitudinal strips, but then cut across the strips to produce pieces no more than an inch or so long.  Those probably could be re-assembled using computerized techniques - originally developed to re-assemble old parchments like the Dead Sea Scrolls.)
Today, there are multiple layers of protection.  The equipment is designed to zero out any embedded keys if tampered with.  (This is common even in the commercial market for things like ATM's.)  A variety of techniques are used to make it hard to reverse-engineer the equipment.  (In fact, even FIPS certification of hardware requires some of these measures.)  At the extreme, operators of equipment are supposed to destroy it to prevent its capture.  (There was a case a number of years back of a military plane that was forced by mechanical trouble to land in China.  A big question was how much of the equipment had been destroyed.  There are similar cases even today with ships, in which people on board take axes to the equipment.)
The hardware is considered hard to break into, and one hopes it's usually destroyed.  The military, and apparently the NSA, believes in defense in depth.  If someone manages to get the checksum algorithm out, the probably have the crypto algorithm, too.
Why HMAC?  If you mean a keyed MAC ... it's not better.   But a true signature would mean that even completely breaking a captured device doesn't help you generate valid keys.  (Of course, you can modify the device - or a cloned copy - to skip the key signature check - hence my question as to whether one could create a crypto system that *inherently* had the properties that signed keys naively provide.)
In a hierarchical organization, centralized means of control are considered important.  There was an analysis of the (bad) cryptography in "secure" radios for police and fire departments, and it mainly relied on distribution of keys from a central source.
Defense in depth?  Seriously, I don't *know* what NSA does - I can only observe what they let out.
I proposed the challenge of a "secure against use of unauthorized keys" crypto system as a theoretical challenge.  I don't know how to do it; I don't know if it can be done.  Perhaps if it can be done people will find unexpected uses for it.
                                                        -- Jerry

@_date: 2013-09-30 23:50:10
@_author: Jerry Leichter 
@_subject: [Cryptography] encoding formats should not be committee'ized 
BSD 3-clause license.
I have no idea what this means.  I can tell you with certainty that all kinds of clever code is being developed and deployed within Google (though I can't give you any details, of course).  Some of it may eventually get described in the open literature; some of it may be open-sourced.
Personally, I have no deep objections to ASN.1 - though much of its complexity has been proved unnecessary by the usage of other, much simpler, approaches, like protobufs.  Still, once you have compiler for it, it doesn't really matter all that much either way.  For crypto algorithms, you're unlikely to want or need the more obscure corners.
Do keep in mind, however, that having just a way to generate C from ASN.1 no longer covers much of the necessary space, as there are now many popular languages that are no C-like.  Some are easy to bind to C libraries (e.g., Python); others are much more difficult (e.g., Java).  For ease of use, you really want generators that produce code well integrated into the target language, no some barely functional glued-together thing.
                                                        -- Jerry

@_date: 2014-04-01 06:52:07
@_author: Jerry Leichter 
@_subject: [Cryptography] Use process ID in mixing? 
It didn't.  Note the "from kernel code".
(I hope I haven't fallen for an obscure April Fool's joke.)
                                                        -- Jerry

@_date: 2014-04-01 22:31:54
@_author: Jerry Leichter 
@_subject: [Cryptography] TLS/DTLS Use Cases 
Bill Frantz asked:
"You use that word but I don't think you know what it means."
These are not use cases.  Neither are many of the others that this thread has drawn.  These are particular protocols that live on top of a reliable stream transfer method, i.e., TCP.  Not a singe one of them had even been conceived at the time TCP was designed, and yet TCP supports them all.  If TCP had to wait for requirements from a long list of "use cases" like this, we would still be lacking a spec.
Use cases are particular patterns of communication with, one thinks, distinct requirements.  IanG gave some examples - things like request-response protocols (one-shot or repeated) and character streams.
An interesting thing about SSL is how badly it was designed to match any of the use cases to which it was initially applied.  HTML was originally a one-shot request-response protocol, but SSL has expensive connection setup was - and there was no session continuation mechanism.  At a fundamental level, TCP chose to transmit reliable streams, when competing protocols were record-based.  So SSL (and TLS to this day) provide on top of TCP?  A record-based protocol.  This makes it unnecessarily difficult to take existing code that talks directly to TCP and make it talk to SSL instead.
A number of years ago, I had to deal with exactly this situation:  A proprietary TCP-based protocol that we needed to make secure.  For a long time, we had customers route their connections over SSH, but they ended up wanting us to provide the crypto built in.  (More fools them - SSH was almost certainly more secure.  But never mind.)  Anyway, I designed and implemented a secure byte stream protocol that could replace TCP more or less 1-1.  (No, it didn't use a stream cipher.  Down at the bottom, it really was exchanging 8-byte encrypted blocks; but blocks could be "short", with only some of the (decrypted) data actually meaningful.)  Worked quite nicely.
IPSec has many faults - so many as to render it unusable - but it did get one thing right:  To most code, an IPSec socket looks just like a plain TCP socket.  Anything that talks TCP can talk TCP "securely" over IPSec with essentially no changes.  ("Securely" in quotes because it's a rather specialized notion of "securely".)
I know it's not going to happen, but in an ideal world, TLS3 would be as easy to add to a TCP program as SSH tunneling when you've got all the port information and such.  Key negotiation and such should be separable from the rest of the protocol:  If you want it done in-line, fine; if you need to run it separately because you're not in a position to modify the existing connection code, fine.
                                                        -- Jerry

@_date: 2014-04-02 07:01:53
@_author: Jerry Leichter 
@_subject: [Cryptography] Clever physical 2nd-factor authentication 
It's a challenge/response style technique with a clever cheap low-tech implementation.  Basic idea:  The user gets a credit card with a transparent window on which a user-specific mask - a pattern of lines - is pre-printed.  The server sends an image that, when viewed through the mask, forms a passcode to be sent back to the server.
I didn't spend enough time exploring the site to get a feel for all the details, or how secure it might actually be.  But it's nice to see people coming up with new approaches and doing the necessary engineering work (e.g., the client side software lets you easily adjust the size and position of the image as presented so that it matches the credit card).
                                                        -- Jerry

@_date: 2014-04-03 05:21:12
@_author: Jerry Leichter 
@_subject: [Cryptography] Clever physical 2nd-factor authentication 
I have nothing to do with this system and am not in a position to defend it, but I don't like to see things dismissed without examination.
Consider the passive attack:  You assume that by matching up the data sent with the numbers the user sends back, you can figure out at least one segment on the card.  But if you read the description of the system (especially in the white paper), you'll see it's not quite that simple.  They send a bunch of random segments which produce nothing readable, then the actual digit, then more random segments, then another digit, etc.  Listening in, you have no way to know which of the data sent is deliberate noise, and which will produce a segment.  (That's not quite true:  The random noise cannot contain anything that encodes a digit, so it differs statistically from the actual digits.  This would actually provide an attack, though it would take many more samples than you count.)
They are also explicit that they only consider a card valid for some fixed number of challenges.  You count about 400 such challenges, using an attack that the random confounders prevent.  I haven't gone through the arithmetic to see what they mechanism could conceivably produce, but even if they get, say, a factor of 10, it would be realistic to replace a card after 2000 uses and have a large safety margin.  (In fact, the white paper indicates that the server itself effectively simulates such an attack, and will cause a new card to be issued when too much information has leaked.  The server, of course, is the perfect attacker in this scenario; a real MITM would likely miss some authentication events.)
An active attack would be much more powerful, but also much harder to carry out.  The user would notice if he validated and repeated failed to get connected to the appropriate site - and until you've actually figured out enough of his card pattern, you can't act as a MITM beyond the faked authentication prompts.
Anyway, it would probably be a good idea to read the white paper for some details of what the system does before announcing attacks against it.
                                                        -- Jerry

@_date: 2014-04-08 20:22:02
@_author: Jerry Leichter 
@_subject: [Cryptography] The Heartbleed Bug is a serious vulnerability in 
You're misreading what Iang wrote.  To say one should not fix a *potential problem that hasn't yet occurred* because we can't prove it's caused any losses yet is absurd.  Before the problem actually occurs, all we have to go on is our estimates of the possible cost, and we have to anticipate those costs.
However, in the case of a problem that *has actually occurred*, if you *still* can't show any loses - then you have to seriously ask whether the problem is worth fixing, or whether it isn't really a "problem" at all.
For example, there are all sorts of plausible-sounding arguments that if people are exposed to signals at the frequencies and power levels used for WiFi then various bad things will happen to them.  Well ... we've exposed many millions of people to such signals for extended periods of time, and the evidence is that the "problem" - exposure to RF - doesn't produce any real costs.  The research that shows that is valuable; and documentation of the actual costs (or lack thereof) for various classes of software bugs would *also* be valuable.  Without such research an documentation, our risk analyses are lacking a solid foundation.
                                                        -- Jerry

@_date: 2014-04-10 07:15:14
@_author: Jerry Leichter 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
We've all by now heard of the Heartbleed security disaster.  I haven't seen the actual coding error, but the descriptions indicate that an attacker could cause the server to allocate and return a large buffer, almost all of which was left uninitialized - and often ended up containing various pieces of left-over sensitive information from previously-deleted and now-reused memory blocks.
Years ago, when I developed some software in C++, I had a pair of classes called RedString and BlackString, imitating classic crypto usage of the colors.  A RedString contained "sensitive" data - data to be encrypted or that had been encrypted, keys in the clear, etc.  You could convert a RedString to a BlackString by encryption, or the other way around by decryption.  You could not misused a RedString in many obvious ways - e.g., the services that sent data over a link wouldn't access a RedString.
More to the point, the destructor of a RedString cleared its memory before releasing it to the free pool.  A Heartbleed attack that revealed 64K of former RedString's would have revealed ... 64K blocks of zeros.
It is - or should be - just a fundamental principle of secure coding that you minimize the amount and time you keep "sensitive" data around in memory - *and that you never release control of it*.  It's your responsibility - you don't hand it to the memory allocator.
People will tell you they can't afford the extra CPU cost of zeroing the memory.  Nonsense.  The vast bulk of this stuff is material you're encrypting or decrypting.  Think of the zeroization as part of the encryption or decryption process.  How many cycles does it cost to zero one byte of memory?  Not in isolation, but as part of a stream of at least tens of bytes?  It's a fraction of a cycle, added to the multiple cycles you're already paying.
I happened to develop a system in C++, which is the only widely-used language that offers *compiler enforced*, user-programmable, cradle-to-grave control over memory allocation.  By putting the code to zero the memory into the destructor, I could guarantee that no RedString could make it back to the free list without being cleared.  There's no way to do that in any other language I know of.  That means you have to enforce the discipline yourself.  Make sure your analogue of a RedString is never simply free'd, or left to the garbage collector - have an explicit routine for marking it no longer used.  There are some tricks you can apply to make it more obvious if this discipline isn't followed.  In a language like C, you can have a library routine to create a RedString that allocates extra memory and returns an address offset into the block malloc() gave you.  If someone tries to free() it directly, there's a good chance the memory allocator will crash, and someone will find the problem.  In Java, you could have every constructor register the newly-allocated RedString in a hash map somewhere.  Calling the "finish()" function to clear the memory would also remove it from the hash map.  If you just abandon the RedString, it will leak - and your server will eventually run out of memory, again inducing someone to fix the damn bug.  Programmers are trained to hate crashes.  Crashes that reveal hidden bugs in security- critical are a *good* thing!  (Actually, in Java you can use a weak reference to have the GC inform you that the object is ready to be collected, and you can clear it at that point.  Whether you want to rely on a mechanism that leaves sensitive data floating around in memory - but not available to the memory allocator - for an indeterminate time is a legitimate design question.  Also, since String's are immutable in Java, you have the problem that even if you know you've got sensitive data you no longer need stored in a String ... there's nothing you can do to get rid of it.)
I've seen comments over the years that crypto- (and all security-)related programming should not be left to "general" programmers with no domain expertise.  I'm not aware of any attempt to collect a list of "issues and programming techniques a crypto programmer must know".  Might be useful to have....
                                                        -- Jerry

@_date: 2014-04-11 07:39:45
@_author: Jerry Leichter 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
The password reader I wrote for the same system I mentioned earlier solved this problem in a way that's so obvious that it's always amazed me no one else has come up with it:  As you type your password, each character echoed as '*'.  When you hit RETURN, the system echoed enough extra '*' characters to make the total echoed a multiple of, say, 10.
You as typist got the feedback needed to know your characters were being recognized as you typed them.  Someone seeing the completed entry would know the length of your password mod 10 - which is of little practical value.  Unless you're a slow hunt-and-peck typist, someone watching the screen had little chance of detecting the difference between the character echoes and the fill echoes.  (Of course the system doesn't have to print all the fill echoes in one shot - in fact, it can echo them with a timing that matches what it's just observed in your own typing.  I didn't get that sophisticated.)
                                                        -- Jerry

@_date: 2014-04-11 07:43:17
@_author: Jerry Leichter 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
Just to clarify:  I'm not suggesting replacing the system memory allocator.  What matters is not the handling of blocks of free and allocated storage - it's the care-taking of *sensitive content* within those blocks.  It's fine to hand a block of memory back to the memory allocator; it's not so fine to hand over a block of memory full of sensitive data.  But you can erase the data without affecting the block at the level of abstraction at which the memory allocator lives.
                                                        -- Jerry

@_date: 2014-04-12 10:18:46
@_author: Jerry Leichter 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
The fix doesn't reveal where the problem was.  Both old and new code use OPENSSL_MALLOC() to allocate N bytes, and 30 or so lines later, both use dtls1_write_bytes() to write N bytes.  The same N in both cases, based on the internal data length the requester specified.
It appears that the actual fix is in validating that the internal data length is no longer than the buffer in which it is supposed to reside.
This diff doesn't show how much data is *copied*, but at this point the damage is already done when N is computed as above:
1.  If what's copied is based on the bogus internal length, then the outgoing buffer will receive whatever is in memory after the received buffer.
2.  If what's copied is based on the actual received buffer length, then when an attacker specifies a bogus large internal, most of the buffer is never over-written after being malloc'ed, and the attack receives a slice of just-allocated memory, with happened to be in it.
Either way, the attacker gets stuff he shouldn't be able to see.  And, yes, if the failure happens to be of type 1, then clearing sensitive data from memory before freeing it doesn't help.  (More properly, doesn't help *as much* - the memory beyond the buffer will often contain a mix of in-use and free memory, and if the free memory doesn't contain valuable, well, sending it isn't leaking anything.)
I must say, looking just at this brief snippet of code, that neither version would have passed my code review *even if it weren't security-sensitive code*.  Some places use 1 + 2 + length; others comment on what the constants are; in the old code, some places use 3 + length, I guess because we don't want the compiler to be too stressed out doing constant folding.  padding is set to 16, but the validity check (which is counting the size of the padding) uses an explicit 1 + 2 + 16.  We have two lengths floating around - the length of the buffer and the length of the data inside the buffer - but nothing gives you a hint about which is which.  If you're extracting data with a length from a surrounding structure with a length (like a buffer - but TLV fields often nest recursively), you should establish a convention for naming the two lengths and use stylized code to *always* validate one against the other.  (Depending on the overall structure of the code, you might have a common function to handle that.  There are many good patterns, but leaving it up to each developer of each individual function to come up with his own approach each time guarantees that someone will eventually screw it up.)
The new code is an improvement in a number of ways, but as a basis for security of a large part of the Internet ... this thing scares me.
(And yes, I've looked at plenty of open source - and non-open-source - code over many years, and I know this is *significantly better* than much of what's out there.  But "no worse than the other guy's" is hardly a good explanation - except when it comes to the *economics* of why this kind of code succeeds in the marketplace.)
                                                        -- Jerry

@_date: 2014-04-12 19:32:22
@_author: Jerry Leichter 
@_subject: [Cryptography] cryptography Digest, Vol 12, Issue 9 
That site has a revoked certificate.  I declined to continue on to the site.
Is that supposed to be the cost demonstration?
                                                        -- Jerry

@_date: 2014-04-12 20:05:53
@_author: Jerry Leichter 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
No, this really does over-write the underlying memory in the String object.  Doing this violates all kinds of assumptions the Java compiler and runtime environment are allowed to make.  For example, if you take a substring of a Java string, you share memory with the original.  Overwrite one, overwrite both.
*However*, I agree absolutely that this particular hack is insufficient:  Java is a GC'ed language, and its design allows (and all implementations I've ever seen use) compacting collectors.  So, you can wipe out the memory currently holding the sensitive data - but you have no way to know what other copies might be out there and where they might be, and no way at all to get at them.  JNI code can "pin" stuff in memory, but by the time you come up with a reliable way to pin all the appropriate memory, for the appropriate times, without giving the GC constipation, you might as well just write a kernel of code that deals with sensitive data in C or C++ and keep it out of Java entirely.
As far as I can see, the only way to write code that safely manages secure data in a language with a compacting GC is to extend the semantics of the language to include a "must be erased" flag that the GC will respect.  (You should also add a "zap the contents of this object now" call so that you don't have to wait for the GC to notice that some "must be erased" data is no longer being used.)
The general concept of "must be erased" data applies much more broadly.  The old VMS operating system had - still has, for those troglodytes still using it :-) - a "Data Security Erase" function - the equivalent, as a file system service, of such things as the Unix srm program.  (The system shipped with a default implementation of the function, but you could replace it if you wanted - something certain government agencies apparently did.)  But ... not only could you explicitly invoke this function on a file - you could also mark a file so that it was always erased when deleted.  Since this was built into the file system, the erasure happened no matter what program deleted the file.
A minor feature ... but illustrative of a basic concept:  Security features have to work *even if you aren't thinking about them*.  On every other system I've ever used (I suspect there are others I haven't used that someone will point out), it's up to you to remember, at exactly the time you delete a file, that it contains sensitive data and you must use srm, not rm (to use Unix terminology).  Slip up just once, and the only alternative is to wipe the disk.  Well, maybe you'll be satisfied with wiping all the unallocated space.  Delete a file implicitly - e.g., by accidentally redirecting over it - and you have the same problem.
A *robust* security implementation must make it possible for you to determine the security sensitivity of a file *at any time during its existence*, and then enforce the appropriate policy without further involvement on your part.  (Note that this overlaps the distinction between discretionary and non-discretionary access control, but is quite distinct.  *I* set the "must erase" at my discretion.)
                                                        -- Jerry

@_date: 2014-04-13 16:20:11
@_author: Jerry Leichter 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
Having gone back and forth between C++ and Java in different projects more than once, and having programmed in a variety of other languages over the years, I suppose I could get deeply into a debate about the flaws and merits of the two languages.  But you know what?  It's pointless and boring.
I do, however, want to correct one important error.  The inability to read stray junk left in memory is not, in general, covered by either the notion of type safety, or that of memory safety.  It's something Java adds by the way it defines the initialization process for all allocated objects:  They are guaranteed to contain type-specific constant values - in practice, to be zeroed before any user-written code (including a constructor!) could see them.  (The story is a bit more complicated for local variables, which need not be initialized - but the semantics of value assignment and propagation is carefully defined to guarantee a compilation error if code could read an uninitialized value.  I haven't looked closely to see if there are holes in this guarantee that a program could exploit.  I'd guess there are, but perhaps they aren't large enough to be significant.  I would also guess there are holes in some of the badly designed antique corners of the language like deserialization and the definition of clone(), both of which bypass all the constructors.)
There have been languages which are type- and memory-safe but do not clear memory.  A good example is Modula-3, from which Java drew many of its foundational ideas.  The guarantee in Modula-3 was simply that any uninitialized variable contained the proper representation of some arbitrary member of the value set of that variable's type.  Since any bit pattern in an integer field represents a legal integer value, no explicit initialization code was required.  (In practice, what this *mainly* ended up meaning was that variables of reference types were set to null pointers.)
One can argue the merits of an explicit initial value.  From a security point of view, it's certainly helpful - though a "security conscious" implementation might well chose to zero at deletion rather than at allocation, and perhaps zero local variables that can't be absolutely certain to be fully written before they are read.  Both are *permitted* by the definition of the language, but no required.
                                                        -- Jerry

@_date: 2014-04-15 14:42:14
@_author: Jerry Leichter 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
That's an *excellent* resource.  Thank you for sending it.  I've read parts and intend to go through the whole thing, and comment if I find something to say.  I encourage everyone here to do the same.
I should actually have pointed to another resource, one that I actually *have* sent comments on in the past (and they've been included):  CERT's C Coding Standard at There are also CERT coding standards for C++ and Java, though I'm not sure they're available on line.
                                                        -- Jerry

@_date: 2014-04-16 12:42:36
@_author: Jerry Leichter 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
AKA side channels.
All absolutely true - and this kind of thing can and does happen at multiple levels in the system stack.  A historical tale (which I may have told here in the past; if so, apologies to those who've seen it before):
The DEC VAX was an early example of an attempt to pin down exactly the architecture of a series of CPU's.  On many earlier machines, instructions were often described in terms of their expected effects - with other effects left open to implementors.  Users of particular implementations would explore these unspecified regions and often discover new and useful primitives.  Of course, those would fail on the next machine....  So the VAX architects tried to specify many details.  For example, if a register was an input to an instruction and only some of its bits were used, the others would be marked MBZ - Must Be Zero. The hardware would take a fault if an MBZ field was *not* zero.
However, the architects did leave themselves an "out" for certain *output* fields.  This might be defined as "unspecified", which means the hardware could do what it liked with these bits.
Years after there were multiple VAX implementations out in the field, someone (Hi, Joe M!) raised the following question in an internal VAX architecture discussion (Notesfile, for those who know what these were):  Could an "unspecified" field receive information to which the user program did not otherwise have access?  For example, could a user-mode program execute an instruction that would cause stuff from kernel-access-only memory to show up in an "unspecified" field?
DEC being an engineering company, the actual hardware designers jumped on this and responded within a day or two.  They went through every unspecified field and each of their designs and checked to see just what might show up.  As it happened, there was no issue:  Every existing implementation either zeroed or left unchanged all unspecified fields.
There followed an attempt to come up with language for the architecture spec that would prevent the "reveals secret information" problem without going to the extreme of mandating *exactly* how such fields were to be treated.  This turned out to be impossible to retrofit into the existing specs; it remained as one of those things hardware designers had to be aware of.  (It may have made it into annotations on the internal version of the architecture spec - the public versions was an extract and omitted various implementations hints and guidelines.)
A few years later, when the Alpha architecture was being defined, the architects were careful to avoid this problem.  Their definition of "undefined" carefully defines everything the "undefined" value might actually depend on, of course avoiding anything that the program could not have read directly for itself.
This stuff is *hard* to get right.  Side channels are insidious, and attackers keep coming up with new ones.  As a fairly straightforward and simple RISC architecture, the Alpha was probably immune to certain classes of timing attacks - all instructions took the same amount of time regardless of input.  On the other hand, timing attacks based on cache or page faults would have been easy to implement.  Why the distinction?  Because both are side-effects of completely unrelated design decisions:  No one at the time - at least in the non-clandestine community - knew or talked about timing attacks.

@_date: 2014-04-16 14:59:08
@_author: Jerry Leichter 
@_subject: [Cryptography] I  don't get it. 
Come on, iang, don't hold back - tell use what your really think!  :-)
Most of civilization is based on things that work by what appears to be blind luck.  There are some crazy estimates of how often airline pilots make mistakes - in the several times and hour range.  Almost all the mistakes have no effect because the systems are robust in immense numbers of ways.  Some of that is design; some of it is evolution:  Systems that aren't robust run into system-killer failures and disappear.
A look at the log files of any computer system out there will show you warnings and error messages and other kinds of flags of potential problems galore.  No one looks at them; no one cares.  The successful systems manage to proceed.  (Many, many years ago I wrote a note to RISKS contrasting the MS/DOS and Unix boot sequences.  Both had a crap-load of impossible-to-understand settings and tweaks, but one thing you could say about MS/DOS:  If you set things wrong, it always somehow managed to fall back to defaults that allowed it to boot.  In contrast, rendering a Unix system unbootable was trivial.)
Software in general is notoriously fragile.  Crypto software takes that fragility to an extreme, all the way from the algorithms themselves (though it now seems some of our standard algorithms are particularly fragile as the result of, err, enemy action) to the implementations to the procedures surrounding those implementations.
There are bits and pieces of work scattered around on the topic of "robust cryptography", but as a field of study or a well-understood group of practices, that doesn't really exist. We need it.  Badly.
                                                        -- Jerry

@_date: 2014-04-16 17:57:26
@_author: Jerry Leichter 
@_subject: [Cryptography] I  don't get it. 
There was never a need for attackers to go after SSL directly - there were plenty of ways to get around and get whatever you wanted done.  It's a difficult argument to make, but the kinds of attacks that SSL was designed to protect against never really emerged, even in cases where SSL was badly deployed or not deployed at all.  Even the NSA, from what we've seen leaked so far, didn't bother to go "through" SSL - they had plenty of ways to get around it instead.
The story is different for Bitcoin.  The transactions are much more limited, and you don't have all kinds of third parties who can be tricked or coerced into getting around security: The guys essential to security actually have skin in the game.  In general, the crypto is much more tightly bound to the semantics of the actual protocols and transactions than is the case with SSL.  So in this case, the weak links were in the implementations - and sure enough, they got attacked.
                                                        -- Jerry

@_date: 2014-04-16 22:02:48
@_author: Jerry Leichter 
@_subject: [Cryptography] Simpler programs? 
This was one of the original goals for VM's.  VM/370 virtual machines talked to each other through fake networks and had isolated "mini-disks".
The original view of virtualization was also the basis of an A2-secure VMS implementation at DEC many years ago.  (It never shipped - the VAX died before it gained sufficient momentum, and the port to Alpha would have been a major effort.  The effort's been written up - I don't have a handy reference.)
Unfortunately, all recent VM work has gone in an entirely different direction.  Now integration between host and hypervisor is the name of the game.  Simplicity was lost long ago in favor of performance, manageability - and all sorts of extra features.
Exactly what a VM built with security as its first goal - but usability for an interesting set of cases, given modern OS's, programming styles, interaction styles, etc. - should look like would be a nice little research project.
                                                        -- Jerry

@_date: 2014-04-17 18:51:46
@_author: Jerry Leichter 
@_subject: [Cryptography] Something that's bothering me about the 
We're all talking about a serious bug in OpenSSL code.
It's worse than that.  It could occur in any program that *uses* OpenSSL.  Among such programs, there are many that allow for plugins and other open-ended extensions.  Those are just as hazardous.
So it's not just OpenSSL.  It's every bit of code that *uses* OpenSSL, and every bit of code the *uses* the code that *uses* OpenSSL.
I can suggest a fix, but it's a hard one:  Crypto code *must never run in the same protection domain as untrusted code*.
If you assume a safe compiler for a safe language, you can assume that *it* maintains separated protection domains for different components.  I'm not sure there's any language/compiler I'd fully trust, but you can come close enough that you might reasonably take the risks involved.
If you assume existing languages like C and C++, the only way to maintain separate protection domains is via the operating system:  Put the crypto code in its own process, with a carefully minimized and controlled connection to the operational code which has no direct access to any of the crypto capabilities.
Note that both the VPN model and the "use ssh tunneling" model do this, though they provide no way for the application code to influence *anything* about the crypto - even such things as the identification to use when choosing keys.
                                                        -- Jerry

@_date: 2014-04-17 23:47:28
@_author: Jerry Leichter 
@_subject: [Cryptography] Something that's bothering me about the 
Oh, I got that.  I used OpenSSL as an example.
What it will do is lower the sensitivity level of the data available to such a bug.  Any bug that allows an attacker to read arbitrary memory from a server is a problem, but Heartbleed caused such a panic because that memory could - and often did - contain very high value information (like keys).  As part of a defense in depth, it makes sense to lower the expected - and perhaps also the maximum - value of that information.
Not quite.  What Heartbleed returns is the contents of memory returned by malloc().  If the memory allocator cleared memory on free, the actual value to an attacker of reading uninitialized memory would be zero.
There are a million possible attacks.
We've spent decades developing a model in which the unit of mutual suspicion is the process, and the hardware provides strong isolation between processes.  Then we turn around and cram mutually suspicious contexts into a single process, with no hardware support for inter-context isolation.  That leaves the compiler, or the user code itself.  The first can be effective, though history is against it.  (When was the last time you heard of a hardware bug that allowed a process to gain access to another process?  They certainly happen, but about all I can recall seeing in many years are timing attacks through shared caches and TLB's and such.  On the other hand, problems with compilers are commonplace.  It's just so much harder to generate code that will be correct, first time every time, than to have hardware that's checking a small number of conditions at every instruction.)  Still, we're getting pretty good at it.
The last - explicit user code - is what most of the Internet relies on.  It's *very* delicate - always one bug away from disaster.
Ironically, the original Unix model - of forking a new process for each incoming connection - was safe, but it failed to scale.  Could we somehow build scalable systems with the good characteristics of such isolation?  Linux actually started out with finer-grained versions of fork, allowing you to share or not share various bits of process state.  All that's still in there, but people were used to the full fork and insisted on it - and it's not clear the more tightly constrained primitives would have had sufficient performance in any case.  Other models of things we might loosely call "processes" have existed in other OS's.  There was an old DEC real-time OS in which "tasks" shared a common heap, but had  hardware-isolated stacks.  In this kind of model, if you kept all your transaction-specific stuff in stack locals, you'd be safe.  (Not appropriate for  heap-based programming models in languages like Java - but an illustration that other approaches are possible.)
                                                        -- Jerry

@_date: 2014-04-17 23:47:28
@_author: Jerry Leichter 
@_subject: [Cryptography] Something that's bothering me about the 
Oh, I got that.  I used OpenSSL as an example.
What it will do is lower the sensitivity level of the data available to such a bug.  Any bug that allows an attacker to read arbitrary memory from a server is a problem, but Heartbleed caused such a panic because that memory could - and often did - contain very high value information (like keys).  As part of a defense in depth, it makes sense to lower the expected - and perhaps also the maximum - value of that information.
Not quite.  What Heartbleed returns is the contents of memory returned by malloc().  If the memory allocator cleared memory on free, the actual value to an attacker of reading uninitialized memory would be zero.
There are a million possible attacks.
We've spent decades developing a model in which the unit of mutual suspicion is the process, and the hardware provides strong isolation between processes.  Then we turn around and cram mutually suspicious contexts into a single process, with no hardware support for inter-context isolation.  That leaves the compiler, or the user code itself.  The first can be effective, though history is against it.  (When was the last time you heard of a hardware bug that allowed a process to gain access to another process?  They certainly happen, but about all I can recall seeing in many years are timing attacks through shared caches and TLB's and such.  On the other hand, problems with compilers are commonplace.  It's just so much harder to generate code that will be correct, first time every time, than to have hardware that's checking a small number of conditions at every instruction.)  Still, we're getting pretty good at it.
The last - explicit user code - is what most of the Internet relies on.  It's *very* delicate - always one bug away from disaster.
Ironically, the original Unix model - of forking a new process for each incoming connection - was safe, but it failed to scale.  Could we somehow build scalable systems with the good characteristics of such isolation?  Linux actually started out with finer-grained versions of fork, allowing you to share or not share various bits of process state.  All that's still in there, but people were used to the full fork and insisted on it - and it's not clear the more tightly constrained primitives would have had sufficient performance in any case.  Other models of things we might loosely call "processes" have existed in other OS's.  There was an old DEC real-time OS in which "tasks" shared a common heap, but had  hardware-isolated stacks.  In this kind of model, if you kept all your transaction-specific stuff in stack locals, you'd be safe.  (Not appropriate for  heap-based programming models in languages like Java - but an illustration that other approaches are possible.)
                                                        -- Jerry

@_date: 2014-04-18 10:27:08
@_author: Jerry Leichter 
@_subject: [Cryptography] Simpler programs? 
Both iOS and Android have changed this.  The Android model is straightforward:  Keep the basic security model, but re-purpose the notion of "user" so that it identifies not a person but an installed application.  I haven't looked closely enough at iOS to understand everything it's doing, but I think that's part of it - and also there's a mechanism (perhaps the same one?) to keep each application's data separate from every other application's.  (Even MacOS has had something of this general flavor for years - I ran across it at one point but forget the details.  iCloud deliberately doesn't look like a traditional file system and maintains the same kind of separation for files written by separate applications.)
I'm not *recommending* either approach.  The iOS/iCloud approach sounds great in theory, but ends up preventing all kinds of obvious information sharing that you'd really like to do.  The Android mechanism escapes this limitation by providing explicit ways around its limitations based on a privilege mechanism - which again sounds great in theory, but turns out to be incomprehensible and impossible for users to manage in any effective way.
We've had many years of development on two broad design schemes:  The Unix-style user/group/other model, with "user" being a single human's identity and all controls being discretionary (i.e., up to the "user", who's also the owner of the file); and the mandatory access control (MAC) ideas that came out of a military context and have seen relatively little use elsewhere.  ACL's of various power have been around for years, but are rarely used.  Both MAC's and ACL's have suffered because no one has come up with a good way for people to understand what they mean as actually applied to files.
Both approaches grew up in a world of time-sharing.  They worked reasonably well for back-end servers, until servers went multi-thread multi-tenant, which required them to re-implement security for themselves.  (VMS actually provided OS support for thread-specific OS security identities.  Since this idea never caught on elsewhere, at best, it would be used in VMS-specific code - no one would have any reason to add support for this kind of thing to Apache, for example.)
These were barely appropriate to workstations and PC's, and clearly inappropriate for the "truly personal machines" (phones, tablets) of today.  While, as I mentioned, OS's aimed for these environments have been looking for a new, more appropriate, model, none of them have really found it yet.
We desperately need new ideas here.  Unfortunately, today we also have the installed base problem that wasn't present when the current schemes were developed in the 1970's.  (How did God create the world in only 6 days?  No installed base.)  Today, any operating system that doesn't provide either the Windows API's or the Unix API's is a non-starter in the back-end and desktop worlds.  (There might be more "give" for personal machines, but if you don't have the weight of an Apple or a Google behind you, you stand little chance.  Even Microsoft is completely outclassed here.)
It's not as if this is new:  Mach, started in 1985, lives on only as a kernel hidden behind a Unix compatibility layer.  Clean, powerful ideas were fine - as long as it could run all that existing Unix code (complete, of course, the the Unix security model).
                                                        -- Jerry

@_date: 2014-04-18 13:22:02
@_author: Jerry Leichter 
@_subject: [Cryptography] bounded pointers in C 
I've always thought that Modula-3 had the right idea:  It supported two "worlds" at the same time.  Most code only had access to a GC'ed, fully safe, memory allocator.  But there were a set of "unsafe" operations, including a complete equivalent of malloc/free and low-level access equivalent to C.  You could only use this in modules explicitly marked with the UNSAFE keyword.  The Modula-3 GC could be (and was) written in Modula-3, as a (partially) UNSAFE module.
A couple of years later, there was an OS project - called, I think, SPIN - that built the entire OS in Modula-3.  They used only a tiny amount of UNSAFE code.  One area that's often full of bit-twiddling and type munging is network code, where you somehow need to convert a bag-of-bytes into some typed object.  The SPIN team added an interesting primitive:  "Here's a bag-of-bytes with the same size as an object of type X.  View it as an object of type X."  This was legal only if every field in X, recursively, had a type for which any bit pattern was legal.  (I'd guess - but don't know - that you could have enum type values, with the compiler inserting a check that what was in the buffer as a enumerated value was actually legal.  It may have done this for some other types as well - e.g., there was a CARDINAL type which was a subset of INTEGER and consisted only of the non-negative values.)
Many of the ideas in Modula-3 made it into Java, though the UNSAFE operations did not.  (Native code badly fulfills the same role.)  I believe C# went back to the same well and *did* include something much like Modula-3's UNSAFE code and non-GC'ed memory hierarchy.  Beyond those influences, however, both Modula-3 and SPIN died.
                                                        -- Jerry

@_date: 2014-04-18 13:50:19
@_author: Jerry Leichter 
@_subject: [Cryptography] bounded pointers in C 
In principle, it's easy.  Nothing in C constrains the size of pointers; the compiler can make them any size it likes.  There are statements about what happens if you cast a pointer type to an integer type "large enough to contain it", but nothing says such a type must exist.  So you can simple make every pointer a "fat pointer".  The only legal ways to get initial pointer values is by taking the address of an object (and the size of an object is always known); or by calling malloc() and friends, which necessarily know the size as well.  Beyond that, it's just a matter of keeping the size updated as the pointer is modified by pointer arithmetic.
That's the principle.  The *practice* is that a huge fraction of practical, every day, C programs assume that a pointer will fit in a long.  Nothing in the language guarantees it, but "everyone knows" that this is how C works.
At the cost of performance, you can move the lengths off to a hidden data structure in which you can look up pointers.  When you're handed a pointer, you look it up and get its length.  This is messy because you can be handed a pointer into a block of legal memory.  So you need a structure to check whether a pointer occurs in any of a set of ranges.  How well this would work in practice, I don't know.  Presumably a smart compiler can elide most lookups:  In some cases it knows the size anyway (e.g., you take the address of a struct on the stack); in others, once it's looked it up, it can keep it around for a while (i.e., thin pointers in memory, fat pointers in registers).
                                                        -- Jerry

@_date: 2014-04-18 14:10:52
@_author: Jerry Leichter 
@_subject: [Cryptography] bounded pointers in C 
See my (2002) article at                                                         -- Jerry

@_date: 2014-04-18 17:57:03
@_author: Jerry Leichter 
@_subject: [Cryptography] I  don't get it. 
Heh.  This is the same gcc that said a  might cause bats to fly out of your nose, or something like that.
                                                        -- Jerry

@_date: 2014-04-18 18:54:40
@_author: Jerry Leichter 
@_subject: [Cryptography] It's all K&R's fault 
And yet the fact of the matter is that some of the most reliable code out there is written in C.
What's important isn't the language as such, and it's been proven over and over again that it's possible to write unsafe code in any language.  Ada was supposed to make coding safe, and yet the Ariane failure was due exactly to a required safety check that, in the situation at hand, actually *caused* a failure.
Java makes large classes of memory errors that have troubled C and C++ for years impossible - but Java programs regularly die with null pointer exceptions and class cast errors.
For almost 60 years, we've been promising that, really, this new language will *finally* eliminate bugs and security problems - only to be disappointed.
There's nothing in the code
Looks any different to me
And the types are replaced, by-the-bye
And the parsing on the left
Is now the parsing on the right
And the preludes have all grown longer overnight
I'll tip my hat to the new specification
Take a bow for the new revolution
Smile and grin at the change all around me
Boot my Emacs and play
Just like yesterday
Then I'll get on my knees and pray
We don't get fooled again
Don't get fooled again
No, no!
Meet the new language
Same as the old language.
Safe, secure code is produced by careful, deliberate processes.  The processes include discipline on the use of features, the avoidance of reliance on known-dangerous libraries, careful documentation, careful testing, standards for validation of parameters, enforcement of pre- and post-conditions, code scanning by static analyzers, detailed code reviews, and many other techniques.  They are both social and technical.  Doing it this way is very difficult, and very expensive; but it's sometimes necessary and justifiable because of the costs of getting it wrong:  Planes falling out of the sky, reactors going critical, medical equipment killing patients.  Such code isn't perfect, but then nothing in the real world is perfect.  Planes still crash due to mechanical failures or pilot error, tsunamis render nuclear plants highly dangerous, patients die due to all kinds of errors.  But code written for such applications is good enough to not be a driving factor in the failures.
Crypto code, collectively, is now becoming as critical as much of that other code.  And yet no one is willing to put up the funds necessary to build it to the necessary standards.  Never mind open source.  The kind of closely supervised and controlled programming needed - much of it profoundly boring - would have no hope of making it as an OSS project.
Compared to all of these factors, C vs. L, for any reasonable value of L, is lost in the noise.  (And keep in mind that if L is some new toy ... why should you believe that its compiler and library faithfully implement the language spec?  Without that, you have no reason to believe much of anything about programs written in L.  At least C compilers have been around for a really long time and we can have reasonable faith that they do what they claim to do.)
                                                        -- Jerry

@_date: 2014-04-18 20:35:24
@_author: Jerry Leichter 
@_subject: [Cryptography] Apple and OpenSSL 
Be aware that this is a strongly pro-Apple site, and that comes through plainly in the article.  Still, it's an interesting history of how one company has been dealing with the issue of crypto software.
                                                        -- Jerry

@_date: 2014-04-19 00:09:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Apple and OpenSSL 
What would you have wanted them to say instead?
*Apple* dodged the bullet.  There's no way they could prevent other developers from putting themselves (and their users) in harms way (as it turned out - no one could really know for sure at the time) by using OpenSSL.  (Well, in iOS Apps - available only through the App store, or with MacOS applications in the Mac App store, they could if they really wanted, as they control what goes into the App stores.  But you can imagine the complaints if they did that.)
                                                        -- Jerry

@_date: 2014-04-19 08:08:20
@_author: Jerry Leichter 
@_subject: [Cryptography] Apple and OpenSSL 
I suspect the "incredible" comment is from the website author.  Apple, like other successful software developers, knows that small teams are often better than large ones.  There are types of software that really need large teams - if you need to produce 500 different screens and interface with 300 different back ends, a team of 5 won't get you very far - but any tightly integrated single piece of software shouldn't require large numbers of people.  If you're developing something like SSL and you need a large team, you're doing something wrong - and I'm sure Apple knows that.
Whether *4* is an appropriate number of OpenSSL, I don't know.  Not having looked at the code base, I'd guess that 4 is somewhat low but 40 is too big.  Somewhere around 10 might be right during periods of intense activity (like now, where there's apparently a big push to clean up the code, get rid of old junk, eliminate every bug even suspected of being in there), but for maintenance, 4 might even make sense.  But, again, that's guesswork based on familiarity with the general functionality provided, not the actual codebase.
None of this has anything to do with Apple specifically.  The Apple-specific stuff in that quote rings true as a matter of corporate culture:  Apple, channeling Steve Jobs's experiences and fears (some say paranoia), has long had a culture that actively avoids being dependent on outsiders for anything deemed critical to Apple's success.  That's why they design their own CPU's (and there are rumors that they are planning to design their own baseband chips) as a prominent big-ticket example.  What's interesting in the quote - if it's true, and the evidence supports it - is that *Apple has decided that security is critical to its success*.  This is quite a change from their previous attitude, in which they relied on third parties to patch holes in critical security software, and were quite lackadaisical about getting the patches into the software they then repackaged and distributed to their customers.
The recent publication of their iOS security white paper is another example of this apparent change in direction.
Where it actually gets them, we'll have to wait and see.  After all, they only a month or two back had a major embarrassment with the "gotofail" bug, which they introduced themselves.  (Of course, there's nothing like such an embarrassment to really light the fire under efforts to improve the process.)
                                                        -- Jerry

@_date: 2014-04-19 08:38:01
@_author: Jerry Leichter 
@_subject: [Cryptography] bounded pointers in C 
It's a synthesis of a whole bunch of ideas that have been out there for years.  The single address space and the way they use it is the basis of the old IBM AS/400 architecture, and also appears in some form in Power.  The Intel 432 took this to its logical extreme, though parts of it still show up in the x86 memory segmentation architecture.  The portals appear in both x86 and in the old DECSystem 10.
Of course, all of this traces back to Multics in some form.
None of this is a criticism of Mill!  Learning from what others have done, digesting it, and putting it all together in new ways, is the essence of successful, useful design.  It's a myth that good design comes out of nowhere.
The common factor in the Power and x86 memory extensions is that they aren't used.  (Well, the Power memory stuff is likely used in the Power-based replacements for the AS/400.)  C and related languages, and the OS's built using them, just turn everything into one linear address space per process.  The Mill design goes out of its way to avoid that fate.  But whether anyone today is interested in a CPU architecture that is neither x64 nor ARM is questionable.
Of course, a couple of years ago, you could have said the same thing about a language that wasn't C++ or Visual Basic, or an OS that wasn't Windows or Unix.
                                                        -- Jerry

@_date: 2014-04-19 08:51:43
@_author: Jerry Leichter 
@_subject: [Cryptography] bounded pointers in C 
Almost - and the "almost" is what stops people.  Yes, you can have a SafeInt, and it will "work" in most cases.  But you can't, for example, have SafeInt constants.  So you can't have switch statements that switch on SafeInt.  The built-in types have all kinds of special properties (the standard numeric conversions, for example) that a user-defined class can't imitate, but which are a fundamental part of the way C++ code is written.
If you write your code using a SafeInt class to begin with, you can structure it to look nice and work correctly.  (Some of the matrix math classes you can find do an impressive job of making matrix algebra look as if it was built in to C++.)  You'll have to write some explicit conversions and constructor calls where the built-in types didn't require them, but it's probably not too bad.
However, substituting SafeInt for int in an existing code base written using accepted C++ styles would be a huge effort that few would undertake.
Defining effective array-like classes is easier because arrays have much more limited inherent semantics.  The traditional issue has been the lack of a way to write down a constant of an array-like class, as you can an array (though only in limited contexts, i.e., initializers).  I haven't gotten into the details of C++11, but I gather this has gotten a bit easier.
(I've toyed over the years with two notions to make this easier.  The first is the ability to "subclass" the built-in types.  You would be strongly limited on what you could add, but you'd inherit the special conversion semantics and such unless you explicitly overrode it.  The second is to have "const constructors" that produced a compile-time constant.  Obviously, they could only have compile-time-constant parameters, and could only use proper compile-time-constant expressions inside of them.  Never quite got either figured out to the point where I could submit it as a proposal....)
                                                        -- Jerry

@_date: 2014-04-19 22:05:28
@_author: Jerry Leichter 
@_subject: [Cryptography] Simpler programs? 
Reflection does indeed allow you to access private variables.  And it even allows you to modify final variables - a feature that gives you a great way to elicit very unpredictable behavior, since the compiler is allowed to assume that the value of final variables doesn't change, so it can in-line the values rather than refer to them through the variable itself.  In both cases, you find the appropriate Field object and then make a call on it that overrides access protections.  In effect, you create a "root" access path to the underlying field that ignores all the access protections that Java implements.  (The field itself is unchanged - unmodified Field objects pointing to it continue to be restricted in their access.)  I haven't had reason to look at the mechanics involved, but I'm pretty sure similar ways to disable the access protections exist for methods and even whole classes.
Also, when you create what passes for a closure in Java - pre-8; I haven't looked at how they've changed it - the only variable values you are allowed to capture are those of final variables, since, again, the "can't change".  What your code will actually see if you use reflection to change the final is an interesting question in head-up-your-ass philosophy.
All of this and more can, of course, also be done through JNI.  I suspect it could also be done by simply gen'ing up your own byte codes to do the access.  (The implementation of Field that I looked at essentially does just that, in fact.)
If you run under a security manager, it can forbid access to private variables and modifications of final ones - but very little code runs under a non-trivial security manager.  (I'm not sure a security manager has fine-grained control over a native (JNI) function, but it can forbid you from loading such functions.)
                                                        -- Jerry

@_date: 2014-04-19 22:27:11
@_author: Jerry Leichter 
@_subject: [Cryptography] bounded pointers in C 
where s is a SafeInt, I have to first define a variable to hold 12?  Sorry, but no.
Yes, and you can provide automatic conversions back and forth between int's and SafeInt's in all kinds of contexts.  The 12 * s issue above could be dealt with by having an operator *(int, SafeInt).  But once you provided too many conversions - in particular *any* implicit conversions from a SafeInt back to an int - you potentially lose the safety of SafeInt!  There's the potential that at some point what you thought was SafeInt arithmetic is actually turned into int arithmetic.
I have never seen a successful effort to incrementally update a code base like this.  Maybe someone's managed to do it.
The easiest example:  Look at the rules for type conversions in determining matching functions.  The compiler can (in principle) apply any number of built-in conversions (in practice, because of the set of built-in conversions available, the number is small, but definitely greater than 1 - e.g., widen char to int, then convert to float).  However, it can *never* implicitly apply more than one user-defined conversion.  You can try and get around some of this by defining more overloads - but if you have too many overloads, overload resolution becomes ambiguous (there are pairs of overloads neither of which strictly dominates the other in the partial ordering of potential overload resolutions that the Standard defines) and then your program fails to compile.
Been there, done that, been bitten.  (In fact, I've actually used this mechanism to force deliberate ambiguities to keep people from accidentally accessing the "wrong" function.)
                                                        -- Jerry

@_date: 2014-04-19 23:36:14
@_author: Jerry Leichter 
@_subject: [Cryptography] Are dynamic libs compatible with security? was: 
Actually, it depends on how you set up your dynamic linking.  The usual Unix convention - which Mac OS sticks with - is that libfoo.dylib is a softlink to the libfoo2.3.1.dylib (for example), where 2.3.1 is the most recent version of the foo library.  But the older libfoo.2.1 library is often still on the system.    Programs that want the "official latest supported version" link against libfoo; but instead they link directly to libfoo.2.1.dylib ... that's what you get.
To take a specific example from my Mavericks laptop:
1864 -rwxr-xr-x  1 root  wheel  2072560 Oct 31 02:43 /usr/lib/libcrypto.0.9.7.dylib
2272 -rwxr-xr-x  1 root  wheel  2590960 Oct 31 02:43 /usr/lib/libcrypto.0.9.8.dylib
   8 lrwxr-xr-x  1 root  wheel       21 Oct 31 02:44 /usr/lib/libcrypto.dylib -> libcrypto.0.9.8.dylib
The situation for MacOS is particularly interesting because MacOS provides an easy mechanism for an application to carry along inside of itself all the dynamically loaded material it needs.  What you see as an "application" Foo will correspond to a Foo.app file.  (The .app is hidden by default.)  You appear to run Foo[.app] by double-clicking on it - but it's not executable code.  In fact, it's a directory, a fact that is also hidden by default, though you can navigate into it if you like.  Hidden inside of that directory is space for all the dynamically linked libraries and other kinds of material - like translatable string constants - the actual program (which is in there under a specific path) will need.  When starting the program, MacOS will arrange for the embedded collection of libraries and other dynamic material to be searched *first*, before any system-provided libraries.
So for MacOS-style applications, the static vs. dynamic issue is as much or as little of an issue as you choose to make it.  Any libraries placed in the appropriate directories under the Foo.app directory are "morally statically linked":  The program will always choose them first, ahead of anything else on the system.  System libraries aren't normally carried along with applications, but nothing (as far as I know) says they can't be.
So all this concern is related to programs built and run the old-fashioned way, as simple executable files that dynamically load from the system library path.  Applications built as .app files can easily carry along as little or as much of their environment as appropriate.  For code that relies on OpenSSL, carrying along a private copy that you code was built against seems like a fine idea.  But you do have to package it all up in MacOS style - something your typical OSS projects, supporting multiple platforms, are unlikely to want to bother with.
                                                        -- Jerry

@_date: 2014-04-19 23:48:22
@_author: Jerry Leichter 
@_subject: [Cryptography] bounded pointers in C 
Yes, thanks - I should have used the right term of art.  I hadn't slipped into "standardese" mode.
Nothing, other than that (a) it's rather verbose and makes for ugly expressions; (b) as I've said repeatedly, it all works fine *if you've written the code to work with the library*.  What you can't do is just add the library, change some type declarations, and suddenly have existing code "just work" but with a new datatype.
It's always been a goal of C++ to allow user-defined types to look as if they were completely built into the language.  (You can disagree with the goal, in which case none of this matters to you, but that was and is a goal.)  C++ has always been remarkably good about that - much better than any other language I know of.  Yes, C++11 has - with things like literal suffixes (I forgot about those - I haven't actually had the chance to *program* in C++11, so haven't internalized much of the new stuff) and the new initializer formats - gotten even better.  But it's still not possible to provide *exactly* the detailed semantics of the built-in types - semantics inherited from C, mind you; that C++ been defined from the ground up with semantics appropriate for the "user can write classes that look completely built in" goal, the story would likely have been different.  Mind you, it's not necessarily a failing of C++ that you can't do this - few ever have the need, and providing it for them would mean wading into the huge morass of details that is the automatic type coercions and conversions and adjusting everything to fit together.
Still, every one in a while, it would be really nice to be able to do such a thing....
                                                        -- Jerry

@_date: 2014-04-20 16:59:42
@_author: Jerry Leichter 
@_subject: [Cryptography] It's all K&R's fault 
If you look at my old RISKS article, linked to from another note in this thread, you'll see that I comment that the Vector class I implemented had a checked operator[] and an unchecked UnsafeAt() function, and that I used the latter in the innards of a hash table where a bounds violation is provably impossible.
Well ... some time after I wrote that note, something led us to look at the hash table more closely.  (It may have been a Purify - the dynamic memory access tester - test run.)  It looked like the "proof" wasn't really right.  So I went through the code, took out the UnsafeAt() calls, and put in operator[].  A test run identified an oddball corner case that violated the bounds.
An easy fix, but I never took the checked operator[] calls out after that (though in many years of use in the field, it never triggered again).  And you know what:  Despite very heavy usage of the hash table code (and of the Vector code in general), the bounds checking never showed up as using enough time to even be visible in profiling.
The cost of bounds (and other sanity) checking, even without compiler assistance, is often completely irrelevant to program performance.  Too many programmers "know in their guts" that *their* algorithms are so good, and *their* code so tight, and what *they* are writing is so central to their project, that they just "can't afford" the time wasted in such checks.  (Not to mention that they are such good coders that there aren't any errors to check for anyway.)  The only answer to such claims is:  Prove it.  They will almost never succeed - and even if they do, often there are other ways to gain significantly more performance *safely* by re-considering algorithms and data structures.
                                                        -- Jerry

@_date: 2014-04-20 18:19:54
@_author: Jerry Leichter 
@_subject: [Cryptography] Something that's bothering me about the 
Well, of as stated that gives you an infinite regress:  Somehow, you have to send information from one protection domain to another, and the receiver then has to parse it, and that parser has to go into another protection domain. :-)
Heartbleed is a parser bug.  Since the data received from the outsider world *is* the attack surface, it should be no surprise that many attacks arise there.  For them to arise deeper in the code, an attacker first has to slip past the parser, which almost always requires that they find a bug in the parser - or even in the specification that the parser implements.
Given this ... isn't it astounding that *we are still writing parsers for network protocols by hand*?  And from "semi-formal" specifications that often turn out to be ambiguous?
We faced this problem for programming languages in the 1960's, and we solved it, first with formal, verifiably unambiguous, syntactic specifications, then with parser generators.  And yet you look at pretty much any piece of software that reads data from a socket and it's got hand-written code to pull out the length field as a couple of bytes, convert it to a length, verify that the length makes sense (if the programmer remembers), copy the data off to a buffer, etc.
There are many reasons why the Heartbleed bug should never have slipped into the code, but beyond that *there's no reason why any human being should have been writing that code to begin with*.  While far from an *ideal*, or even a *good*, solution, one could probably have done this in lex (with some action code to deal with things like converting length fields) and had a much better chance of getting it *consistently* right.
                                                        -- Jerry

@_date: 2014-04-20 22:39:13
@_author: Jerry Leichter 
@_subject: [Cryptography] Apple and OpenSSL 
Of course, the same thing happens in C if you duplicate the underlying cause:
struct A {
struct B {
   struct A a;
   ...
If you do anything that changes struct A's length, everything referring to B is likely to break.
I implemented a workaround for this many years ago:  Add a couple of spare fields of a couple of basic types.  Should you need to do a patch while retaining ABI compatibility, just start using one of the spares.  (In C++, you also want a couple of spare virtual function entry points.)
(The goal in my case wasn't ABI compatibility as such.  The system involved was structured as a tiny main program, with all the functionality implemented in a large number of dynamically loaded libraries.  Servers for different purposes were configured by telling the main program what to load.  Changes to low-level libraries required rebuilding *everything*, which made for large patch files that *changed* everything, which made for unhappy customers.  The spare fields and entry points trick let us push that off to the next major release, when people expected everything to change anyway.  Using the macro pre-processor made this easy to manage:  If you needed a new int field name xCounter, you could  xCounter int_field_1 for the current release, then add the new xCounter field and get rid of the  for the next release.)
                                                        -- Jerry

@_date: 2014-04-21 06:07:18
@_author: Jerry Leichter 
@_subject: [Cryptography] It's all K&R's fault 
It's a matter of defense in depth.  Values that have been erased can't be leaked due to misconfigurations, snarfed by attack code that gets slipped into the process, recovered from memory by someone who grabs the device while it's running.  I agree with you that encrypted swap plus language-level protections are pretty good, especially in something like a data center environment.  But if we're talking, say, a phone - maybe you want to go further.
                                                       -- Jerry

@_date: 2014-04-22 22:34:08
@_author: Jerry Leichter 
@_subject: [Cryptography] Open Source developer employment agreements, 
And how much success do you have getting programmers to produce decent Javadoc documentation?
I've been pushing hard to get people on my current project to produce decent Javadoc (and comments in general), but it's an uphill battle.  In a previous job, we used a Javadoc-like tool for C++ whose name escapes me.  A small team of hand-picked people who worked directly for me were pretty vigilant about writing appropriate comments - though some weren't very good at it, and I ended up re-writing most of their documentation-level comments.  But beyond that, in the rest of the organization ... nada.
Unfortunately, once you've filtered out those who will do anything to avoid writing documentation/comments; and those who are willing but do such a mediocre job that it's barely worth the trouble; you don't have many people left.  It's a real challenge.
                                                        -- Jerry

@_date: 2014-04-23 19:41:19
@_author: Jerry Leichter 
@_subject: [Cryptography] GCC bug 30475 (was Re:  bounded pointers  in C) 
This is where C changes.  Way back when, before the first ANSI C, K&R (probably; I can't remember for certain now) had comments to the effect that certain things were "implementation defined" and the choices made by implementers were assumed to be such that programmers who understood the machine architecture would not be surprised.  Yes, K&R C had all kinds of ambiguities, and we went through a long an painful porting effort as people learned that "not everything is a VAX".  But for the most part, this approach worked reasonably well for the relatively small, highly experienced, audience that C addressed in those days.
As the C community grew, and more and more compilers came into existence on different machine architectures (back when there were more than three machine architectures) and different OS's (back when there were more than two OS's), stuff had to get formalized.  The first edition of the C spec did a pretty good job of pinning things down - but it was nice challenge at the time to write any useful code in purely conformant C.
The underlying issue here is:  While the C standards have advanced to the point where many things *are* possible in conformant C, the way it defines (more properly, chooses not to define) the semantics of signed overflow, it's impossible to write conformant, reasonably efficient C code that checks for signed overflow.  This is unfortunate, and really should be fixed:  Just as the standard these days lets you get at various special operations - like checking for NaN's in floating point - it should provide a standard way to check for signed overflow.  I don't know if there will ever be another version of the C standard, but if there is, the only way this kind of thing would stand a hope of getting included is for someone to define, and for large numbers of users and significant implementations to accept, a header file and a set of predicates or safe arithmetic functions or what have you that enable programmers to write safe code in the presence of signed arithmetic that might overflow.  Implementations can figure out how to support these.  They might simply provide an implementation compiled with all optimization turned off (verifying that for *their* implementation, that was safe).  They might provide an assembler implementation.  They might add built-ins.
Any of these is would be acceptable.  The current situation is ... absurd.  The compiler writers are sticking to the exact language of Standard, rather than worrying about what we used to call "quality of implementation" issues.
                                                        -- Jerry

@_date: 2014-04-24 14:42:47
@_author: Jerry Leichter 
@_subject: [Cryptography] Swap space (Re:  It's all K&R's fault) 
This is a messy area.  On first glance, just after a fork(), the only additional resources needed are those needed for another entry in the process table, and maybe some similar entries (e.g., for file descriptors) - trivial amounts of space.
The reality is more complicated.  Somewhere, there are maps describing the virtual memory of all processes.  Those can be of non-trivial size.  Depending on the architecture and the implementation, there are limits to how much of the *maps* can be shared.  What can't be shared has to be allocated an initialized. (Back in VAX days, the maps for P0/P1 lived in S0 - and machines eventually got to the point where there was no room in S0 to map the desired number of processes!)
Also, COW requires that there be some place to copy *too*.  As previously noted, you can allocate all necessary swap space up front, or overcommit, wait until it's needed - and then somehow deal with the discovery that there isn't enough swap space available.  After a fork(), if you don't overcommit, you need to allocate enough space to cover the possibility that every writable page in the new process actually *is* written.
An interesting effect of growing address spaces and faster CPU's is that the fraction of data pages (relative to code and other pure pages) has been going up for many years.  While programs are much bigger now than they used to be, all programs spend most of their time in loops, and those loops haven't gotten all that more complex - they just now traverse larger and larger datasets.  So your fork'ed process today has many more writable pages for which swap needs to be reserved.
There are all kinds of unexpected side-effects.  Here's an interesting one:  A very large program does a fork to start another, also-large, program, then exits.  (This particular situation is the same on a system with a spawn() function.)  There should be plenty of free space - the large program just released a whole bunch - but there's a timing issue:  The large program may have freed the memory, but it takes time for the OS to "digest" that memory and return it to the free list.  The dirty pages need to be written; anything added to the free list has to be zeroed.  Implementations often deliberately cap the rate at which they will free pages to avoid starving other processes on the system of CPU.  Actually making large numbers of pages available could take seconds a number of years ago when I discovered this effect in tracking down a problem a customer was running into.  I have no idea if it's gotten better, worse, or stayed the same over the intervening years.
                                                        -- Jerry

@_date: 2014-04-24 17:29:26
@_author: Jerry Leichter 
@_subject: [Cryptography] Swap space (Re:  It's all K&R's fault) 
Actually, AIX implemented overcommitted memory *way* before Linux did.  In fact, it originally *only* implemented overcommitted memory:  When you did a large malloc(), you probably got memory pages that would be committed on first access (or your process would die at that point).
This cause so many problems with existing code that it was, as I recall, eventually made an option.
Elsewhere you mention the Linux OOM killer process.  This, in fact, was also an AIX innovation, many years ago.  It (and a number of other system-management-related features) comes from the mainframe world, where you expect the OS to do its best to keep as much running as possible, even under severe stress.
                                                        -- Jerry

@_date: 2014-04-24 17:47:23
@_author: Jerry Leichter 
@_subject: [Cryptography] Swap space (Re: It's all K&R's fault) 
This is false:  Any files open at the time of the exec() remain open.  That's now redirection is implemented, for example.  (It's possible on modern Unix implementations to mark a file "close on exec"; it's often important to do that.)  If by "args" you mean command line arguments, you can either let them be inherited (really, copied) or set them yourself, depending on the form of execXXX you use.
A fork()'ed process gets everything, other than file locks and pending signals (and the PID and PPID, to be technical).  It definitely gets the same UID, GID, terminal group ... all that stuff.
I'm not sure what a "library activity" is.
If you're talking about threads, the interaction of threads with fork() is complex to begin with.  fork() in a thread produces a new process in which only the thread that called fork() is running.  (All the data corresponding to the other threads is there, but they are never run.)  If the next thing the process does is exec(), the left-over threads don't matter; but if the program wants to continue to run, things are messy (and if you want to remain in the standard-specified zone, *extremely* limited - you can only call async-signal-safe interfaces).  Pthreads provides pthread_atfork() as a callback to help manage this stuff.
                                                        -- Jerry

@_date: 2014-04-24 23:18:00
@_author: Jerry Leichter 
@_subject: [Cryptography] GCC bug 30475 (was Re:  bounded pointers  in C) 
I believe C99 guarantees that int's representation is either 1's or 2's complement.  That makes it easy to produce the maximum value for a signed type. Assume we know T is some signed integral type.  Then:
T max = ~(T)0;
if (max == 0)
{	// 1's complement
else	// 2's complement
{	max = -(max + 1);
If we don't know whether T is signed or unsigned, it's easy enough to check:
T t = ~(T)0;
if (t < 0)
else	
Unfortunately, this doesn't produce a compile-time constant.  I don't know how to do that; it's probably not possible.
                                                        -- Jerry

@_date: 2014-04-25 07:05:15
@_author: Jerry Leichter 
@_subject: [Cryptography] GCC bug 30475 (was Re:  bounded pointers  in C) 
You're half right.  Looking at the final draft of C11 at  (one of course has the problem that there are different version so "the standard" around), the actual wording is along the lines of:  If x has a signed integer type, the x << n is define if (as a mathematically correct value) x*2**n is representable in x's type, in which case that's the value; otherwise, the result is undefined.
The actual representation of types has been pinned down somewhat more tightly than in the past.  A signed integer consists of three kinds of bits:  One or more value bits; zero or one sign bits; zero or more padding bits.  The value bits must represent successive powers of two for an overall binary representation of the magnitude.  The sign bit, if present, affects the magnitude to produce one of three encodings:  Sign and magnitude; 1's complement; 2's complement.  The padding bits have no effect on the value.  For each (standard) signed integer type, there's a corresponding unsigned type which has the same size, and for positive values that are representable in the signed type, the representation of that value in the unsigned type must be the same.
C11 does distinguish "bounded undefined behavior" (things like integer overflow) from "critical undefined behaviour" (wild writes to memory), though it's up to an implementation to say whether it maintains the distinction (there's a new macro, __STDC_ANALYZABLE__).  I haven't been able to find the actual defintion of "bounded undefined behavior", though it can certainly trap, perhaps terminating the program.  (But it can't "cause bats to fly out of your nose".)
It's easy to determine the maximum value of an *unsigned* type.  It's not clear to me now that there is any portable way to determine the maximum value of a *signed* type.
One wonders just what possible machine types the C standard was trying to accommodate with that elaborate language.
                                                        -- Jerry

@_date: 2014-04-26 08:10:34
@_author: Jerry Leichter 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
ASN.1 was designed at a time when networks were slow and every bit counted.  It's an *extremely* tight encoding.
Ironically, today networks are immensely faster - but CPU's have gotten faster "even faster".  Because CPU is so cheap relative to networks (not to mention disks and even SSD's), it's worth it to throw CPU at data to render it into a highly compressed form.  Take a look at Google's protobuf format for an example.
It's an absolutely standard TLV (Type or Tag/Length/Value) encoding.  The advantage of a format like this is that a reader can easily skip over fields it doesn't understand - or even forward them on to another reader who might be able to understand them.  Google protobuf's use something similar, for exactly this reason:  You can add new fields to an existing protobuf, and old code will have no trouble with it - and can even modify the fields it does understand while passing the new ones off to someone else unchanged.
You couldn't do that *anyway*!   covers all of .  Once you know it, you can encode it - variable length encoding or not.  Until then, you can do nothing.
I don't understand this comment.  Any TLV coding has to use the style:
1.  Decide on T
2.  Compute (the encoding of) a T instance as V
3.  Return encoding as T Encode(Length(V)) V
This is the same no matter what Encode() is.
Again, this is true of any TLV-like encoding.  I've implemented many such things, and very long ago realized that the parser can be recursive descent, but it must have the following form:
Where Entity is the Container is the V field of the surrounding entity.  Then every Decode_T() starts off as:
Yes, writing that out by hand every time is boring and leads to shortcuts and errors.  You want a generator to do this for you or you, or some successor, will eventually get it wrong.
The problem isn't with TLV encodings - ASN.1 or otherwise.  The problem is that after all these years, we're still writing this stuff by hand - and on top of it, when it's written in C, it's pretty much guaranteed that things what's being passed around is a raw buffer pointer - and often no one bothers to pass the length along, at least when the "know" it doesn't matter (e.g., the decoder for the L field will probably get just a raw pointer, because how long can a length field be?)

@_date: 2014-04-27 12:14:23
@_author: Jerry Leichter 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
I'm actually perfectly happy to use the C/C++ pre-processor for this kind of stuff.  I know there are people who recoil in horror at the notion of macros, much less macros that assume that they are in an environment where certain names mean certain things.  Tough.  If I have 100 entry points that all have the same first three parameters, I have no problem insisting that they be named the same in all 100.  I'm willing to go so far as to have a FOO_ENTRY macro that generates the entry point, inserting the fixed parameters.  (Well, you end up needing a DECLARE_FOO_ENTRY and a DEFINE_FOO_ENTRY.)
Yes, this requires very careful programming and testing, but you get exactly the code you want - you can actually look at it easily if you care - and you only have to get it right *once*.  Lisp programmers have no problem with this kind of thing (though they have an immensely better macro expander available to them).
Frankly, some of the C++ metaprogramming hackery does much the same thing - but in a way that's often even more opaque.
                                                        -- Jerry

@_date: 2014-04-27 12:34:48
@_author: Jerry Leichter 
@_subject: [Cryptography] Heartbleed and fundamental crypto 
There are a number of things that lead to these kinds of designs (which are often, but not always, standards).  It's not "design by committee" as such.  There are committees that produce excellent designs; there are individuals who produce horrible ones.  It's a question of who's involved and what their motivations are.  Among the things that produce bad results:
1.  The designers are big names who no longer actually implement anything, but feel the need to justify theories about design that have worked for them in the past.  (Often, the theories are post hoc explanations for past successes and have little causal connection to those successes.)
2.  The designers fairly recently implemented something very much like what is being designed, know all the problems they ran into, and want to get it *right* this time.  They fall for a variant of Second System Syndrome.
3.  There are multiple people who can say "yes", but no one who has the authority to say "no".  This leads to compromises of the form "I'll OK your proposal if you OK mine."
4.  There are multiple designers in each subdivision of the spec, they have different ideas about the best approach (counted length vs. terminating sequence, for example).  Since each is "as right" as the others, and each is "as easy to implement" as the others, they all make it in as alternatives.
5.  If you're talking standards committees, politics (the same people tend to be involved with each other for multiple standards) and money (companies have their own technologies and want them included in the standard so they don't lose their investment) come in as well.
6.  Once a certain threshold of complexity is reached, it becomes extremely difficult to argue against one more "small" addition.  If the spec is two pages, everyone can see that a new alternative that takes a paragraph to describe is a big deal.  If a spec is two hundred pages, it starts to become difficult.  By 1000 pages - hey, why not add it.
I'm sure there are others - and in the case of crypto, we strongly suspect that in addition to all the usual factors, there's been active subversion as well.
For a design to succeed, the designers have to be committed to *it's* success, not to their own.  One-person designs have an edge because the two overlap.  Groups with strong tyrants as leads can succeed for the same reason.  These can also fail badly if led in the wrong direction.
There are no easy solutions.
                                                        -- Jerry

@_date: 2014-04-28 22:37:27
@_author: Jerry Leichter 
@_subject: [Cryptography] Because one TLS bug per month is just not enough 
One of the most interesting things here is the huge change in Apple's approach to security problems.  Apple has a long history of taking their time about producing and distributing security patches.  But in this case, they are out in front of the pack:  As of a couple of minutes ago, a search on "ssl triple handshake patch" found no references to a fix from anyone but Apple.
Whatever the reason for Apple deciding to push things out so quickly this time, it's good for the industry (not to mention Apple customers) that they did.  We'll see if they continue on this new path.  If they've decide to make security, actually implemented well, part of their sales pitch, we might see real pressure on others to do the same.  (Yes, I know, Microsoft turned really serious about security a number of years back - but they have yet to escape their horrible reputation from Win2K/WinXP days.)
                                                        -- Jerry

@_date: 2014-04-29 11:09:59
@_author: Jerry Leichter 
@_subject: [Cryptography] New slogan for the NSA 
It just hit me today, and I had to share it:
"We're the NSA.  We hear you."
Now I'm trying to come up with the poster on which to put that slogan.  One idea, which hasn't quite jelled:  Backdrop is a picture of an office building at night.  There's a single lighted office, in which we see one person, laboring away, along an lonely, in front of this computer.  Meanwhile, somewhere off to the side, we see a picture of NSA headquarters.  In front are a couple of your stock friend, smiley employees.
                                                        -- Jerry

@_date: 2014-08-01 16:34:26
@_author: Jerry Leichter 
@_subject: [Cryptography] You can't trust any of your hardware 
How many USB devices have ever been patched after sale?  I know of one example, which I mentioned in my original posting:  The Apple aluminum keyboard, introduced in August of 2007, has received exactly one update during its lifetime.
I, personally, know of no other examples.  If anyone else does, I'd like to hear about them.
USB devices are generally pretty cheap.  There's little motivation to publish patches - there's no general way to reach users, you have to worry about whether the users even have a system they could use to patch the device, and most will never bother anyway.  So for your expense you gain nothing in the market.  It's hard enough (effectively impossible) to get more complex, expensive devices like WiFi routers patched in the field - typical USB devices are just not worth thinking about.  (Apple could even *consider* doing a patch because it had an easy pathway to push patches out to all Apple systems, thus capturing most Apple keyboards.)
Doing patching on devices still at the factory, or even in the supply chain, might be more realistic - but most organizations these days run such lean supply chains that the fraction of devices you could actually get to this way is tiny.  Hardly worth the effort except maybe at initial release.
So, yes, for pretty much all USB devices, the code that was initially installed will be the code that's in the device until it's discarded.  It has to be "right enough".
I suspect most "patchable" devices are that way because that was the interface used to upload their firmware to begin with, and it's not considered worth the cost to lock out later uploads, even if there's no realistic change there will ever be a need for them.
                                                        -- Jerry

@_date: 2014-08-02 07:25:42
@_author: Jerry Leichter 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
I haven't dug into all the technical issues here, but I can't buy these assertions.  A change in work factor of 2^16 when you change the number of bits in the key by 16 makes sense only for searches equivalent to brute force.
All of these numbers are way beyond any possible brute force search.  (No, not just based on existing technologies, but based on any technology consistent with what we know about physics.)  Answering that, sure, brute force isn't relevant, but that for *any* attack, more bits is always harder, ignores the important question of *how much* harder.  Your assertions assume exponential scaling in the number of bits.  That means attacks equivalent to brute force - so just as impractical as brute force at these sizes, regardless of the details.
                                                        -- Jerry

@_date: 2014-08-03 22:28:47
@_author: Jerry Leichter 
@_subject: [Cryptography] You can't trust any of your hardware 
In iPhone is not a "USB device".  It's a device that has a USB port.  The same goes for an PC or laptop with a USB port - which is pretty much any PC or laptop built in the last 5, maybe 10, years.
I think it's pretty clear that when people think of a USB device, it's a device that exists solely be be used through its USB port.  Memory sticks are by far the most common such devices, but you can find others as well.  I'd say there's a pretty clear line at the point where the device implements - as far as the user is concerned - only and exactly one of the standard USB profiles.  While these devices *also* tend to implement firmware update mechanisms - in fact, there's even a standard profile for that - this is never documented for end users and hardly anyone has been aware that that additional interface is present.  That's what makes this attack surprising.
An iPhone uses its USB port for ancillary functions - charging, syncing, software updates.  The updates primarily - perhaps exclusively, in the life of the iPhone; it's hard to tell - apply to firmware that has little to do with running the USB port.
The Bose headphone you mention is primarily used through Bluetooth.  A quick look through the documentation indicates that the overwhelming use for the USB port is to charge the device.  Yes, it can also be used to update the device - but again presumably the intended primary updates are to the parts of the firmware that, well, run the headset - not the USB port.
Your printer is an interesting case, and I don't have an answer.
There are few sharp lines here, but there is a very broad, very heavily populated, set of "USB devices" that we commonly look at as having fixed functions based on code that will never be changed.  USB memory sticks are extremely cheap and produced in the hundreds of millions.  No one thinks of them as active devices.  And yet ... they are.  They contain significant processing power running non-trivial code - and that code can be replaced.  That's the big message here.  Yes, obvious in retrospect - but how much have *you* thought about defenses against legitimate memory sticks from major manufactures that have had their standard firmware replaced with attack code?
                                                        -- Jerry

@_date: 2014-08-04 14:49:33
@_author: Jerry Leichter 
@_subject: [Cryptography] "The Visual Microphone: Passive Recovery of Sound 
Everything leaks.  Film a bag of chips through a soundproof window, decode its vibrations to listen to what's being said in the room.
I'll bet you could go a pretty good job of decoding keyclicks from a "secure" room this way.
(From Lauren Weinstein's Privacy list).
                                                        -- Jerry

@_date: 2014-08-04 14:41:02
@_author: Jerry Leichter 
@_subject: [Cryptography] ADMIN: Periodic reminder about top posting 
Your friend is right.  I've been criticized by non-geek friends for this style of response - they aren't expecting it and often don't even realize there *is* any reply information embedded within the message.
Most people who've started of with today's mail programs - which place you at the top of the message you're responding to, ready to top-post; you have to do something deliberate to format your mail any other way - just assume "that's the way it's supposed to be" and top post.
gMail was one of the first to do this.  (I'm sure there were others.)  And it's quite aggressive out it - I've seen it hide quoted material *that didn't come from the message being replied to* - it just happened to use the same quote format.  Sometimes this makes message unreadable unless you know to "reveal" the hidden information.
There's a broadly-held view out there that email is a dying technology, destined to be replaced by the immediacy of text messaging.  gMail is deliberately trying to imitate the text message "feel", with mechanisms that allow very quick replies (with limited context), deliberate hiding of previous messages (because, as with text messages, it's assumed your reader has been following along the whole time and *knows* the context), and so on.  Within Google, there's an actively encouraged convention of putting your entire message into the subject line where possible.  (You put EOM at the end of the line to signal that there's no point looking at the (empty) message body.  Think how much time can be saved by not having to open emails!)
Email has never been the *best* medium for careful discussion of issues (in which nuances and context are incredibly important).  I long ago realized that one thing email was remarkably bad at was "inducing conviction":  Email debates tend to go on forever without any participant succeeding in convincing any other participant.  Still, it was *usable* for such purposes, and nothing out there stands ready to supplant it in this role.  (Ah, DEC Notes....)
Text messaging is great for quick conversations that have relatively little nuance or context and don't need to "retain state" for long periods of time.  Perfect for the ADHD side that modern life seems to bring out in all of us.
                                                        -- Jerry

@_date: 2014-08-05 00:13:05
@_author: Jerry Leichter 
@_subject: [Cryptography] "The Visual Microphone: Passive Recovery of 
Did you watch through to the end?  Starting at around 3 minutes in, they show how to use artifacts due to shutter motion to recover sound reasonably well using a standard camera - frequencies 5 times the frame rate of the camera.  The Nyquist Theorem is a valid result about Fourier transforms, but its real-world application can be subtle.
                                                        -- Jerry

@_date: 2014-08-05 07:30:35
@_author: Jerry Leichter 
@_subject: [Cryptography] "The Visual Microphone: Passive Recovery of 
A Pentax K-01 is available from Amazon for $399 new.  The appropriate lens will depend on circumstances, but suppose we want the same one they used.  They only specify "31mm" and there are multiple 31mm lenses available for it, but if we go specifically for a Pentax version, a new one at Amazon - a very nice F/1.8 bit of glass, 5-star rating - will set you back $1168.84 new.  On the optics side, we're talking $1500 or so for what's pretty standard (nowhere near high-end) "prosumer" gear, and we could find equivalent stuff for much less with a bit of shopping.
Their high-speed camera was a Phantom V10.  This is a rather different beast.   will rent you one - for $1800/day.  I wasn't able to find anyone specifically selling it on-line; the closest was  which says:  "Price Range:  Phantom camera line from $50,000 to $150,000 depending on specific model and features."   Not many people could put that on their credit card, so perhaps it's not surprising that it's hard to find on-line sellers....  :-)
Of course the Phantom may be overkill for this application; there are probably cheaper high-speed cameras out there.  But you're not currently likely to find one for what most people would consider a reasonable price.
The spooks will, of course, have no problem with these prices - and will likely build their own equipment anyway.  But the rolling shutter trick - which one might be able to emulate using much cheaper equipment, perhaps a couple of web cams on a spinning board for a start - is well within the range of amateurs.  (The back-end processing is still prohibitive, but we all know what happens to the cost of processing.)
The point is that the Nyquist theorem doesn't protect you here.  It just makes the attacker's job a bit harder and more expensive.
                                                        -- Jerry

@_date: 2014-08-05 19:59:17
@_author: Jerry Leichter 
@_subject: [Cryptography] "The Visual Microphone: Passive Recovery 
Stories about listening to conversations in a room by bouncing a laser of the window have made the rounds for years.  Someone - maybe MythBusters - set out to test whether this actually worked.  And they found it didn't - all they got was noise.
But as we know from this bit of work, they were missing something fundamental:  Just because the unaided human ear - or even the human eye, looking at an oscilloscope - can't pick out the signal doesn't mean it isn't there.  Had they done the theoretical work and the signal processing it pointed to, they would undoubtedly have reached the conclusion that the "laser and window" microphone is a great way to listen in on a room.  After all, the signal was undoubtedly *much* better than even the high-speed camera image - much less the low-speed, edge-of-the-scan image.
                                                        -- Jerry

@_date: 2014-08-06 10:11:55
@_author: Jerry Leichter 
@_subject: [Cryptography] "The Visual Microphone: Passive Recovery of 
The approach the spooks seem to have taken for years is "a room within a room":  Essentially a big box you set up inside some room somewhere.  It would be heavily soundproofed and mounted in such a way as to isolate any vibrations from the containing room.  There would be no reason for the box to have any windows.
While in theory no matter how well you insulate the walls of the box *some* internal vibrations are transmitted - but you don't need perfection.  Noise is inherently present, and if you can reduce the transmitted signal to be far enough below the noise, you can lower the available data rate below a point where you feel safe - basic information theory about channel capacity used in the reverse of the usual direction.  (Of course, you can also deliberately add noise to the out walls.)
It's an endless battle against new technologies.  Remember the famous "metal diaphragm embedded in the eagle seal and read by microwaves" from the annals of the cold war?  Given today's signal processing and RF technology, you can probably find *something* metallic in almost any room whose vibrations you can read off at some RF frequency and then beat into a source of audio with enough computation.  So now you need to screen or drown with noise a huge swath of frequencies.  Using a Faraday cage as part of the "room within a room" is probably standard, but practical Faraday cages (a) are typically built as meshes, which these days have to have very small openings as an opening comparable to the wavelength of incident RF makes the screen ineffective; (b) the materials of the Faraday cage itself are potentially great sources of reflected information modulated by sound from the room.
At the spook-against-spook level, this is probably a more or less even game, at least for well-funded spooks working for organizations that have access to leading-edge technology.  For everyone else - even including most of the world's security services - defense is probably effectively impossible against the high-end spooks.  And the trends seem to favor other attackers as well:   The attack technologies tend to be adaptations of common technologies (e.g., $1500 digital cameras) so are much more broadly available and affordable than the defensive technologies.
                                                        -- Jerry

@_date: 2014-08-11 10:10:58
@_author: Jerry Leichter 
@_subject: [Cryptography] Many curves versus one curve 
I put this another way here not long ago:  It's basic game theory.  You have a bunch of alternative moves to choose from (each curve, or each cryptographic algorithm, is a move) and (initially) no way of knowing which move your opponent will make "in response" (i.e., which curve/algorithm he chooses to attack, or gets lucky in attacking - *how* he operates is irrelevant, all that matters is that he makes a move).  Assuming all moves by your opponent are equally likely, your best approach is a mixed strategy, choosing among all (a priori equivalent) moves at random.
If you have some kind of probability distribution on your opponents responses, you can adjust your probability distribution to maximum your expected results.  This way, broken systems naturally get chosen less frequently - and ultimately not at all if the break is bad enough.
                                                        -- Jerry

@_date: 2014-08-12 13:59:26
@_author: Jerry Leichter 
@_subject: [Cryptography] Dumb question -> 3AES? 
Both Enigma and Purple were broken over 70 years ago, before we really had real theory of cryptography. (Even the basic approaches were old when they were designed.). So it's unreasonable to include them. Modern cryptography started some time after World War II; modern public crypto started with work at IBM on the late 1960's, leading eventually to DES. Last I heard, DES is only slightly weaker than would be implied by its key length. I'd say the history of widely studied and fielded block ciphers says that we can build such things to last at their design strength.  There's always room for a surprise, but in this particular corner of the crypto universe we haven't seen one in quite some time. The MDx's, and recent attacks against SHA, make it clear that we are not on this position for hash functions. The same goes for RC4 and stream ciphers. It's difficult to assess the state for asymmetric key crypto. RSA has held on, but the key size requirements initially grew much faster than expected - and of late, even at expected growth rates, are becoming impractically large. We haven't seen a dramatic improvement on factoring in two decades, but there's so much math there that it's dangerous to discount the possibility. I have the same fear about ECC.  Structure may give you proofs - but it may equally give you attacks.
Cryptography is far from a finished field of study....
                                          -- Jerry

@_date: 2014-08-12 16:27:12
@_author: Jerry Leichter 
@_subject: [Cryptography] Dumb question -> 3AES? 
If all you want is protection against exhaustive search attacks, Rogoway showed years ago ( that DES-X (where encryption is K1 XOR DES(K, P XOR K2)) is essentially as strong as 3-DES *against brute force attacks*.
It turns out that you can safely set K1 = K2 - see Section 4 of the paper - so you end up with a 256-bit key.  It's not quite 256 bits of security against brute force - see the paper for the detailed calculation - but it's still plenty strong.
The proof doesn't rely on anything special about DES and would work just as well for AES.
Again, this is a defense *only against exhaustive search attacks*.  But if exhaustive search against a 128-bit key keeps you up at night, but you don't want to use AES-256, by all means use AES-128-X, which has a trivial (two XOR's) additional cost.
                                                        -- Jerry

@_date: 2014-08-14 06:22:29
@_author: Jerry Leichter 
@_subject: [Cryptography] cryptography Digest, Vol 16, Issue 11 
It's not clear what point you're trying to make, but if it's that algorithms get broken, Skipjack is a poor example:  In the 15+ years since it was first published, no significant attack has been published against it.  The best published attacks are against reduced-round variants - including one against 31 rounds out of 32 using impossible differentials, an attack that gains no significant advantage over brute force that no one has been able to improve since it was published in 1999.  So, no, Skipjack is not *publicly* "broken" except in the sense that its 80-bit key is too short to survive modern brute force.
BTW, the precision of the defense in Skipjack is remarkable:  32 rounds are safe, 31 rounds are not (at least "not safe" in the certification sense).  There's no publicly known methodology for skating so close to the edge - publicly designed ciphers seem to always tack on an extra couple of rounds "just to be sure".  Between Skipjack (fully NSA-designed) and DES (NSA-modified), we have two ciphers that have survived the best public cryptanalysis for many years, delivering *exactly* the level of security NSA promised, with the minimum resources needed.  (OK, DES isn't quite there as linear cryptanalysis gets a bit of a toe-hold.)  This suggests that NSA has some design tricks for block ciphers up its sleeve that the public world has yet to find.  (There are vaguer hints that they have some similar design secrets for stream ciphers:  No public stream cipher has survived public attack, but while we don't know how they work internally, NSA has continue to field stream ciphers for its own use, so it apparently thinks it can produce secure ones.)
                                                        -- Jerry

@_date: 2014-08-14 17:37:31
@_author: Jerry Leichter 
@_subject: [Cryptography] cryptography Digest, Vol 16, Issue 11 
Schneier's argument is very weak.  Without some idea of the kinds of attacks you are defending against, the notion that you can get a safety margin by just "doing more of the same" is nonsense.  It's like saying you can make ROT-13 safer by just repeating it a couple of times.
The fact is, Skipjack at 31 rounds has a weakness; Skipjack at 32 rounds has survived unscratched for 15+ years.  We have no idea if adding another round would increase the "safety margins," whatever exactly that might mean.  For all we know, it might *decrease* them:  The round function is among the harder parts of designing an iterated block cipher, and for all we know extending Skipjack's round function and using the same key bits over one more time might leak more information than it protects.
Before differential cryptography was published, there were proposals to use the DES innards but discard the round function:  Just supply a full (448 bit?  I no longer recall and don't care to go compute it) key, i.e., supply all the sub keys that will be needed in all the rounds.  Enough "safety margin"?  Well ... in fact, the security (against differential cryptography) is still about 56 bits worth.
                                                        -- Jerry

@_date: 2014-08-14 22:35:33
@_author: Jerry Leichter 
@_subject: [Cryptography] cryptography Digest, Vol 16, Issue 11 
Yes, you can turn any block cipher into a stream cipher.  But ciphers built as stream ciphers tend to have a simpler structure and give you much faster operation with less hardware.  The NSA seems to believe there's a meaningful distinction as they continue to produce both block and stream ciphers intended for different application domains.
                                                        -- Jerry

@_date: 2014-08-15 08:57:30
@_author: Jerry Leichter 
@_subject: [Cryptography] cryptography Digest, Vol 16, Issue 11 
ChaCha may prove to be the first secure public stream cipher.  It probably needs more time to settle in, though - it's about 6 years old.  (Salsa, of which it's a variant, is a year older.)
Ultimately what matters is performance in "stream-cipher-like" situations (encrypting small and large streams of data without delays).  The Salsa/ChaCha family seems closer on these measures to traditional stream ciphers like RC4 than to block ciphers like AES.  Two quotes from "ARM speeds: At the SASC 2007 workshop, Cedric Lauradoux reported a Salsa20 implementation for a 200MHz ARM920T taking 69 cycles/byte and using just 868 bytes of code. For comparison, Lauradoux reported an AES implementation taking 101 cycles/byte with 15920 bytes of code.
FPGA speeds: At the SASC 2007 workshop, Marcin Rogawski reported an unrolled-double-round Salsa20 implementation using 3510 logic elements on a Altera Cyclone EP1C20F324C6 (130nm process). The implementation is estimated to drain 450.14 mW at 30MHz and produce 1280 Mbps. For comparison, Rogawski reported an AES implementation using 5053 logic elements; the implementation is estimated to drain 1191.01 mW at 105MHz and produce 611 Mbps."
(It's difficult to give exact comparisons since both Salsa and ChaCha are families of ciphers and the particular family member you pick affects both speed and security.)
                                                        -- Jerry

@_date: 2014-08-16 12:35:39
@_author: Jerry Leichter 
@_subject: [Cryptography] Cost of remembering a password 
Actually, recent versions of Safari do that.  When they recognize a password field on a page that they don't have a password stored for, they generate one and offer to save it for you.  If you share your keychains through iCloud, the generated passwords become accessible on all your Apple devices.  Doesn't help with non-Apple devices, though.
There are pluses and minuses to this.  For most people, letting Safari (well, the Keychain application with which it's integrated) generate and save passwords would probably lead to a huge leap in security.  But I don't like that anyone who has momentary access to my unlocked laptop *also* has access to all my Web logins.  It may be possible to move the automatically generated passwords to a secondary keychain which would not be unlocked on login.  (That's actually how I save passwords now - but I keep them under manual control.)
Right now, the offers to save passwords in a way I don't use are ... annoying.  Apple is providing a solution that probably helps most people, but I have to find a way to integrate it with my own workflows.
                                                        -- Jerry

@_date: 2014-08-16 15:34:24
@_author: Jerry Leichter 
@_subject: [Cryptography] Cost of remembering a password 
It's quite possible that, indeed, their users would *not* like it.
It's also quite possible, in this litigious world, that some novel approach would be seen as "not up to best industry standards".  Going along with the crowd may not give you the best possible result, but it does tend to bring iwht it a certain degree of safety.
                                                       -- Jerry

@_date: 2014-08-16 21:42:51
@_author: Jerry Leichter 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
The descriptions of PFS tend to focus on asymmetric crypto, but its actual properties are easy to get without asymmetric crypto - something I've discussed here in the past.
Sending side:
(We have cleartext block C1 ... Cn to send)
1.  Negotiate initial key K0.
2.  For i = 1 to n do
      Ki = OneWayHash(K{i-1})
      Destroy K{i-1}
      send Enc(Ki, Ci)
The receiving side is obvious.
This pseudo-code generates a new key for every block, but of course you don't have to do that - you can generate them for every new user-defined message, or every K blocks, or whenever the phase of the moon becomes waxing gibbous.
The only real difficulty here is step 1.  If you don't want to use asymmetric crypto but are willing to accept simple DH, then this is simple.  Otherwise, I'm not sure how to accomplish it:  Typical work on key exchange protocols doesn't worry about retrospective attacks if the keys are later revealed.
                                                        -- Jerry

@_date: 2014-08-17 05:51:46
@_author: Jerry Leichter 
@_subject: [Cryptography] "password manager" --> _authorization manager_ 
My credit card vendor used to supply such a service.  They dropped it because - according to them - uptake was very low.
There were problems with their implementation which made it harder to use than it should have been, so one might argue that that was the cause of the failure. And yet most implementations of *anything* aren't as good as they might be - waiting for a perfect implementation is equivalent to doing nothing at all.
Ultimately, in the US, I'm not liable for more than $50 if my credit card is stolen - and in practice for nothing.  Since I've had CC numbers compromised more than once in the past, in circumstances that no one has ever explained, I've arranged things so that I can recover quickly when it happens again.  I take reasonable care not to reveal it - but I basically see it as the bank's problem.  If they find that virtual card numbers aren't worth the bother ... that's on them.
                                                        -- Jerry

@_date: 2014-08-19 06:06:32
@_author: Jerry Leichter 
@_subject: [Cryptography] [cryptography] STARTTLS for HTTP 
Ah, that famous "false sense of security".  Justifying not doing anything - because we can't do the absolute best - since, what, 1985 or so?
As always, specifying (a) what attacks you need to defend against; (b) how much you're willing to pay; is essential.  For most people, (b) is "not very much" (where the payment will be in inconvenience).  For most people, the most likely attack is "none at all"; the second most likely attack is "passive listening".  Active MITM is way down there.  Opportunistic encryption is much better than what they would otherwise have, which is nothing at all.
Besides, there are positive effects on the larger eco-system:  The more traffic that's encrypted, the harder mass monitoring becomes.
                                                        -- Jerry

@_date: 2014-08-19 21:38:38
@_author: Jerry Leichter 
@_subject: [Cryptography] Cost of creating huge theft targets [Was: Cost 
Interestingly, Apple has addressed this issue in a white paper (  They claim they don't store anything that they could decrypt.
In the case of transfers between already-registered devices, it's not hard to see how to do this.  Each registering device generates a public/private key pair and sends the public key to Apple, which in turn sends it to each other registered device.  A device that uploads passwords encrypts them with a key-encryption-key, then delivers the encrypted data, plus the key encryption key encrypted with each of the public keys, to Apple, which in turn pushes it out to all the other devices.  Each device can decrypt because it has its own private key, but Apple can't as all it has is a bunch of public keys, and no private keys.
The hard part occurs when you get a new device and want to register it for the first time.  This uses your password - there's nothing else that can bind a new device to you.  I don't recall how it does things from here.  Perhaps is as simple as deriving the key encryption key from the password.
I'm not saying Apple does everything right.  I doubt it.  What I'm saying is that it's *possible* to have such a system without creating a significant vulnerability in the central distribution point.  *Some* vulnerability, sure.  But it's by no means clear that the vulnerability is in any significant way different from the vulnerability of storing your passwords anywhere else.  Keeping the stuff on your own machine - perhaps one in your pocket - is adding one kind of physical security to the security the crypto gives you.  Against most attackers, it's much easier to steal your phone or a server in your basement than to get into some major corporation's secure data center.  The relative difficulty may go the other way for a government agency, and if that's your biggest concern, keeping the stuff entirely in your head is your only realistic approach.  For most people, the end result of that is heavy reuse of a couple of passwords, since very few of us can remember more than that.
                                                        -- Jerry

@_date: 2014-08-19 22:54:47
@_author: Jerry Leichter 
@_subject: [Cryptography] CSPRNG for password salt 
Where did you see this?
The only purpose a salt serves is to make it impossible to reuse brute-force attacks made against a user whose password was hashed with one salt against a user whose password was hashed with a different one.  You could as well assign sequential salts to accounts as they are created.
I can think of exactly one situation where the predictability of salts - which is the same whether you use sequential values or an insecure PRNG.  Suppose you knew that a large number of users would change their passwords at the same time.  (E.g., consider a university when students return from summer break).  If you knew the order in which the changes were applied, and how the salt was assigned, you would have the chance to predict the salt and pre-compute a rainbow table to allow you to rapidly attack a given password.
Sounds like you'd need to have an unrealistic amount of information about exactly how the password changes were done, by an unrealistically deterministic password change process.  And even then, rainbow tables are a clever but essentially dead technique today, since one can use hardware to simply brute-force passwords at immense rates.  Knowing the salt ahead of time doesn't help for such attacks - they need the hashed password.
Maybe someone has come up with a plausible attack based on the ability to predict salts - the only thing you're giving the attacker by not using a CSPRNG.  But this sounds to me more like a bogus expansion of something that started out, way back when, as sound advice.  (In fact, what it *may* have come from is advice not to *generate passwords for users* from a non-CS PRNG.  *That* makes sense, for pretty obvious reasons - but I'll bet it's been done by many an organization.)
                                                        -- Jerry

@_date: 2014-08-19 23:09:02
@_author: Jerry Leichter 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
*File* encryption is the easy case, since one can move the code into a "legacy support" state in which it will *decrypt* files created with any cipher at all, but will only *encrypt* with new, safe ciphers.
Unfortunately that doesn't work for communications - although it does suggest an interesting approach.  Suppose you allowed each side to pick its own cipher.  You connect to me and say "Hey, I still like CAST5".  I turn around and say, "Fine, I guess you're willing to leak what you send - which you can do in a million ways, certainly with my own data, and even with data you send me.  But even so, I choose to use AES."  Certainly nowhere near ideal, but at least half the communications channel is protected - and I can use my safe connection back to you to send a message saying "Hey, idiot - how about upgrading that antique piece of leaky software?"
                                                        -- Jerry

@_date: 2014-08-20 06:02:41
@_author: Jerry Leichter 
@_subject: [Cryptography] CSPRNG for password salt 
The second of these just makes a bald assertion, so one can't really respond to it.  It does have the text:  "The salt needs to be unique per-user per-password. Every time a user creates an account or changes their password, the password should be hashed using a new random salt. Never reuse a salt."  Since some of the standard algorithms use a 32-bit salt, this is an impossible requirement:  The birthday paradox implies that you'll likely repeat a salt within 65000 or so "new password" events.  That's not long at many organizations.  (Maybe they meant "never reuse a salt for a given user".  Even then, with 32-bit random salts, you're taking *some* noticeable risk that this might happen by chance - but unless you're worried about targeted attacks against particular users, rather than population attacks against all of them, the distinction makes no sense at all.)
The OWASP page has a tad of justification:  "Salts serve two purposes: 1) prevent the protected form from revealing two identical credentials and 2) augment entropy fed to protecting function without relying on credential complexity.   The second aims to make pre-computed lookup attacks [*2] on an individual credential and time-based attacks on a population intractable."  "*2" is a reference to rainbow tables.  So, yes, if you have a situation in which the salts that will be used in the future are predictable, even in a probabilistic sense, an attacker could pre-compute rainbow tables that will be useful against some user in the population, or maybe against a particular user perhaps by forcing his hand (e.g., if I know what salts will be used tomorrow - even a small number of possibilities - I can do something that will make the user change his password, like making it *look* as if it's been attacked or sending a fake administration message or triggering an alert by repeatedly trying and failing to log into his account), then I can pre-compute rainbow tables that will likely work against some account, or a particular account.
This assumes rainbow tables (the only pre-computation mechanism of which I'm aware) are a reasonable attack mechanism.  Yes, at one time they were.  But today there's little reason to bother with them, as attacks based on multiple GPU's are cheap and fast enough to beat them easily.
Is it *prudent* to use unpredictable salts?  Sure, on a defense-in-depth principle:  It costs almost nothing for the defender, can't possibly *help* the attacker, and does help protect against an attack that once made sense, was rendered obsolete by changes in hardware - but, who knows, might become relevant again at some point in the future.
If random number generation were expensive, and I had to generate salts in batches, it would be fine to generate one random starting point for the batch, then the rest from the starting point in some simple way.  Why?  Because this breaks an attackers ability to predict *ahead of time*.  Sure, in principle, once he sees one new salt, he may know a bunch of others - but that doesn't help him get out ahead of the process by more than the interval between the beginning of the batch and the end.  (I suppose you could worry about more sophisticate attacks, easier to visualize if you imagine you just add one to generate the next salt.  This produces bands of sequential salts as long as a batch, and a clever attacker could choose to start rainbow tables with samples spaced apart by the expected length of a batch.  But that's just the starting point - after one hash round, he's just probing randomly.  So the advantage is tiny.)
My personal conclusion:  If, in your environment, you can generate salts in an unpredictable way without imposing too much cost on the system - you might as well go for it.  If not ... nothing terrible is likely to happen from using predictable salts.
BTW, there's little justification for using short (32-bit) salts.  You can easily eliminate all possible birthday attacks by using 80 or more bits of salt.  If your hashing function doesn't accept a longer salt, XOR the extra bits in before hashing.  (Just to avoid any possible interaction, *don't* XOR in those bits that will actually go into your hashing function.)
                                                        -- Jerry

@_date: 2014-08-20 06:22:57
@_author: Jerry Leichter 
@_subject: [Cryptography] CSPRNG for password salt 
This is a rather weak argument.  You're contrasting a CSPRNG - something that's quite difficult to build correctly, as we've seen from many discussions on this list - with something as simple as "remember the last one and add one to get the next".  We've had predictable "CSPRNG's" in the past.
If all you need for a salt is a value that won't be re-used, getting software that guarantees that won't happen is a very minor problem.  Arguing for CSPRNG's *on the basis that solving that problem is a likely source of weakness*, when there are *so* many much more complex things to get right, makes no sense to me.
                                                        -- Jerry

@_date: 2014-08-20 07:23:22
@_author: Jerry Leichter 
@_subject: [Cryptography] CSPRNG for password salt 
Sorry, but no.  I consider it similar to the following argument:  An electrical circuit needs a fuse.  But, you know, people might stick the wrong size fuse in there.  Or they might even stick in a penny (OK, this style of fuse hasn't been used in 50 years).  I can do better.  I have a digital voltage sensor, which I can turn into an amperage sensor with a resistor.  I can feed that into my Arduino and monitor the current drawn.  I'll include a digital model of the kind of fuse that's supposed to go here, so that it lets peaks of the right sort through, etc.  Then I'll write a driver to control a relay.  Now I've got solid, modern protection for my circuit, and people can't screw it up!
Replacing a simple design with a more complex one "just because you have the parts around" adds to risk, it doesn't ameliorate it.
I've written separately - in messages that haven't yet been forwarded to the list; our moderators are likely still asleep - about the actual threats, and that one could argue for using a CSPRNG on "defense in depth" principles, even if the argument is a bit of a stretch.  But I don't buy the "it's better engineering because you might screw up code to add 1 to the last number" argument.
                                                        -- Jerry

@_date: 2014-08-20 21:49:19
@_author: Jerry Leichter 
@_subject: [Cryptography] On 40-bit encryption 
Oh, but you could check.  Someone form Iran or North Korea trying to download the strong build would be required to set the Evil bit on all their packets.
                                                        -- Jerry :-)

@_date: 2014-08-24 07:36:02
@_author: Jerry Leichter 
@_subject: [Cryptography] Open Source Sandboxes to Enforce Security on 
A fascinating bit of work on the inverse problem:  Running an application securely under a hostile OS:  Basic idea:  Use virtualization techniques on a process-by-process level.  The hypervisor considers every page of memory to be in of two states:  Accessible to a particular process; accessible to the OS.  On the transition from process to OS, encrypt and checksum it; on the transition back, verify and decrypt.  So the OS can screw up a process - but can't see any of its actual data.
There's obviously much more to it than that, but it's really clever stuff.  In a way, you can think of this as taking the very old idea of the security kernel into the modern era by completely divorcing it from the OS.
                                                        -- Jerry

@_date: 2014-08-25 15:16:34
@_author: Jerry Leichter 
@_subject: [Cryptography] Open Source Sandboxes to Enforce Security on 
That point of view makes sense as well.  But neither is the whole story.
In traditional sandboxing, I trust the OS and its sandbox framework to run possibly-hostile code.
Here, I trust the framework, I may or may not trust the application, but I don't trust the OS "between" them.
The really novel idea in Virtual Ghost (though they credit earlier efforts of which I was not previously aware as paving the way) is that of creating "trusted compartments" within a generally untrusted framework.  In a way, it's much like using an appropriately devised encrypted tunnel to connect trusted endpoints through an untrusted network.
I get the feeling there's a general abstraction underlying these various implementations that we haven't quite grasped yet.
                                                        -- Jerry

@_date: 2014-08-27 08:18:59
@_author: Jerry Leichter 
@_subject: [Cryptography] Encryption opinion 
*No* solution to these problems has *ever* emerged.  *Proposals* for solutions have been all the rage, and various experimental implementations have been done, but none has succeeded - if by "success" you mean not "led to publishable papers" but "led to fieldable (and fielded) useful (and used) systems".
The Rainbow series of papers, of which the Orange Book was simply the best known, was an attempt to turn the nominal structure of *military* systems into the basis for secure systems for the entire world.  In practice, it didn't even satisfy the requirements of the military.  I still have a copy of the entire Rainbow series somewhere, but let's face it:  It's a historical curiosity.
PKI's "solved the problem" of key distribution and MITM attacks.  RSA "solved the problem" of signatures.  And it's not just security:  Nelson's dissertation on RPC (or CORBA, if you want to talk about the most complete widely-available realization of those ideas) "solved the problem" of distributed computation.  A variety of papers, include Hoare's CSP, "solved the problem" of parallel programming.  Codd's definition of the relational algebra and a whole bunch of work on transactions "solved the problem" of databases.  *And these are examples of ideas that actually have seen wide and continuing success!*  But the problems they've "solved" have proved to be broader than originally thought.  If you want to list other "solutions" that have have simply vanished over they years, they are easy to find.  Thinking Machines hardware and CM* "solved the problem" of massive parallel computation.  C++ "solved the problem" of object-oriented programming for the masses.  And, hell, next year will absolutely, definitely be the year of Linux on the desktop.
When you see ideas that have been sold for years as a solution to a significant problem, but somehow have never quite caught on - two that come to mind immediately are functional languages and capability-based systems - you have to ask yourself:  Might there be a reason?  Might there be something missing - perhaps just a small thing?  Sometimes that's enough!
For capability-based systems, I think *the* hard problem is configurability:  How do you turn an access policy defined in human terms into a set of capabilities that accurately and completely implements the policy?  Given a set of capabilities, how do you turn it into something human beings can actually understand?  The *implications* of security policies - what is *actually* granted or forbidden, not as a result of the explicit policies but as a result of what they imply - is something that's difficult or impossible to understand, whether the policies are stated in English or in some formal capability language.  In human-based systems, we get around our lack of understanding by allowing humans to override the policies (which also opens the system up to social engineering).  When we freeze the enforcement of such policies into code, we often produce unusable systems.
Note that I'm not claiming capability-based systems *can't* work.  I'm saying that *even on the theoretical side*, we need to do more work on *known issues*; and on the practical, fieldable, fielded side - we have almost no experience.
Claims that capability-based systems can, *in practice*, solve the difficult issues of end-point security at Internet scale simply cannot be supported today.
                                                        -- Jerry

@_date: 2014-08-27 12:27:54
@_author: Jerry Leichter 
@_subject: [Cryptography] toll bills, was Encryption opinion 
From what I've seen concerning the E-ZPass transponders, there's no crypto; they simply respond with a unique ID.  They deal with cloning attacks by ensuring that every transponder reader takes a photo of the license plate and the driver to record along with the transponder ID.  So, sure, you could clone a transponder - or even steal a whole bunch of ID's and roll through them - but all it takes is one person to notice and complain and you're toast.  Since the legitimate license plate is registered with E-ZPass, there's no ambiguity about which charges are legitimate and which ones are from cloned transponders.
Also, the system could easily flag uses of the same transponder at two locations too far apart for a car to have traveled from one location to another in the time between them; for two closely-timed uses in the same location.  This would quickly catch most cloned usage (either capture now and use later, which will eventually trip the "too far apart" test; or capture now and use immediately, which will trip the "closely-time" test) without the legitimate user having to do anything.  With the increasing deployment of license plate OCR, they can detect a cloner immediately.  (The Henry Hudson bridge between Manhattan and the Bronx has a no-stop toll plaza:  If you have E-ZPass, they charge your account; if you don't, they send you a bill based on your license plate.  I doubt they have people reading the plates....)
An interesting and little-known facet of E-ZPass is that they will fine you for using your tag on the "wrong" car.  I was told this by a car rental agent: If you want to use your own tag with a rented car, make sure you register the rented car's plate with your account.  Enforcement of this rule is on a state-by-state basis:  As of a year or two ago, New Jersey was very aggressive about it, New York and Connecticut didn't seem to bother.
(The designers of E-ZPass do think some of these things through:  An account can have multiple registered plates and multiple registered tags, but they don't require you to associate plates with tags, as (a) it would be a pain for people to get right; (b) it would be a real pain for people to *keep* right.  In fact, you don't even have to have the same number of tags and plates.  Over all, the system works pretty well - surprisingly so, perhaps.)
                                                        -- Jerry

@_date: 2014-08-27 22:45:28
@_author: Jerry Leichter 
@_subject: [Cryptography] toll bills, was Encryption opinion 
Interesting way to look at it.  In some ways it's *three*-factor:  The photograph the driver's face, too.  But in actual use, the normal pathways only use one factor (almost always transponder ID; sometimes plate) - both "something you have", as it happens.
                                                        -- Jerry

@_date: 2014-08-29 12:46:09
@_author: Jerry Leichter 
@_subject: [Cryptography] Google proposes a Web of Trust replacement to 
SMTP had a flat, democratic model:  Anyone who got hold of an email address could send mail to it.
"Enterprise-ready" email systems - as Microsoft and Exchange customers define them - allow access control on who can send to a particular address.  CEO's don't want to be bothered by email from "the little people" way down the hierarchy.
Before you object to the basic unfairness, keep in mind that we've had moderated lists - like this one - for many years, implementing the same thing.  The only real difference is that we provide the filtering at the level of "the list".
In both cases, the address itself often shows up on return addresses, so as an address, it's "public".
So ... "public" may not mean quite ... "public".  Never has.
Complaints about leakage of email addresses go back years.  Personally I've found most of the complaints over-wrought.  It's not as if spammers have had the slightest problem getting hold of addresses to send to.  The end result of all the angst is something that failed to solve any real problem while getting in the way of possibly useful functions - like the ability to find an email address without being part of the organization that controls his LDAP server.  (Not that LDAP is much to get excited about:  How it manages to be so damn slow will never cease to amaze me.)
                                                        -- Jerry

@_date: 2014-08-31 09:51:06
@_author: Jerry Leichter 
@_subject: [Cryptography] Made a modification to RC4 
The biases were initially found by running some simple counting programs.  Rather than saying "should" ... have you run the same tests?
Of course, if all you are concerned about is biases in the first 256 values, you can simply discard them, keeping the security properties (such as they are) of RC4 unchanged.  This has been discussed for years and has even been named:  RC4drop-n, which drops the first n outputs (n=768 seems popular).  Other attacks - correlation between successive outputs - are a more significant problem as they persist no matter how much of the output you drop.
How does this compare - in space and time required, and in security against the known attacks - to simply running two copies of RC4 in parallel (use some kind of good key splitting to initialize them with different keys) and XOR the results?  Or do something that's proven effective for LFSR-like stream ciphers: Use the output of each to force an occasional extra "stutter" on the other (e.g., generate two value V1 and V2 from the two instances; if V1 == 255, run the second generator a second time (keeping V2); if V2 == 255, run the first generator a second time (replacing V1); return V1 XOR V2.  (The reason for the "replacing" the second time is that otherwise when the output is not 0, the attacker knows for certain that no extra step was taken.  On the other hand, if you also replace V2 times, you bias the output slightly away from 0.)
It's easy to propose new algorithms.  Showing they are better than what they've replace - much less that they are secure *at all* - is harder.  Three proposed improvements have been published widely enough to have made it into the Wikipedia article on RC4:  RC4A (proposed by Paul and Preneel, who published a general distinguishing attack against RC4), VMPC, and RC4+.  RC4A and VMPC have been attacked - ironically, there's a distinguishing attack against RC4A! - while RC4+, which is considerably slower than RC4, apparently hasn't seen much analysis.
                                                        -- Jerry

@_date: 2014-08-31 09:56:38
@_author: Jerry Leichter 
@_subject: [Cryptography] heartbleed first blood? 
Ah yes, crunchy outside, soft and juicy inside.
This is why putting *all* the defense a the perimeter stopped being recommended practice a while ago.
Granted, if your attacker can read your VPN sessions, you're in a bad way.  But that need not - and must not - be the end of the story.
                                                        -- Jerry

@_date: 2014-12-01 00:00:28
@_author: Jerry Leichter 
@_subject: [Cryptography] Toxic Combination 
The weaknesses of current PKI system are well documented.  There are examples governments getting false credentials by having effective control over CA's.  But I'm not aware of any attack in which criminal organizations got a fake EV certificate (it would have to be an EV cert to turn the address bar green).  The *potential* vulnerabilities are real and there are a number of ideas out there (cert pinning, certificate transparency, others) that would make them much hard to pull off.  Nevertheless, at the moment, this does not appear to be a vulnerability that's widely exploited.
No clue what this is supposed to mean.  Checking certificate fingerprints would only be useful if you had some independent way of know what those fingerprints were supposed to be.  What would that independent way be?  This sounds like a poor man's approach to certificate pinning - which is automated, not based on human matching.
I'm unaware of any broad attacks based on stealing passwords this way.  Why bother?  If you can get the user to trust a bogus site, you just act as a MITM of the conversation.  No problem with slight (or major) differences in how the site looks - it *is* the real site.  Send whatever commands you want.
Again, this is a *potential* vulnerability, but it's not one that scammers are exploiting in any big way.  It's so much easier to steal the password file from the site itself.  It may not even be hashed - and if hashed, is often hashed using bad techniques, make brute force attacks easy.  And then people re-use passwords on other sites.
Security is not an absolute; it's about managing risk.  The *actual* risk of an attack by "scammers" based on getting a fake certificate and successfully impersonating a site in order to get passwords appears to be very small; other risks are *much* greater.  (Again, if you're talking about attacks by intelligence agencies and similar institutions, the story is different - but in many ways, making it a much harder problem.)  There are a number of mechanisms out there that ameliorate entire classes of related risks, including these.
                                                        -- Jerry

@_date: 2014-12-01 20:00:36
@_author: Jerry Leichter 
@_subject: [Cryptography] "completely unexpected" drop in Cisco's foreign 
Before you take this at face value - think back a couple of years, when every CEO was out there citing "the disruptive effects of El Ni?o on markets" as an excuse for the poor performance of their firms.
Cisco has many other challenges.  Their basic technological approach - specialized hardware for routing, using proprietary software and protocols, which the market took to justify high margins - is under serious attack by SDN. Their investments outside of their core strengths haven't panned out - just what made them think buying Flip cameras was a good idea?  Their high end teleconferencing stuff seems to have a limited market, as there are a number of "good enough" competitors out there already.
How much the NSA revelations affected them is very difficult to say.  Frankly, I doubt they really know themselves.  But they sure make for a good excuse....
                                                        -- Jerry

@_date: 2014-12-03 12:26:49
@_author: Jerry Leichter 
@_subject: [Cryptography] MITM watch - Tor exit nodes patching binaries 
It's not clear to me what you're trying to cover with "Bayesian impossibility syndrome", but it sounds very much like the Base Rate Fallacy.
                                                        -- Jerry

@_date: 2014-12-04 16:21:03
@_author: Jerry Leichter 
@_subject: [Cryptography] Mapping numbers to permutations 
[Just to you]
I'm not sure exactly what you're trying prove, but if it's the statement "Given a generator for values uniformly distributed in {0, N-1}, where N is not a power of 2, generating a bit stream either discards some values or may wait indefinitely before outputting anything" is false.  Take N=6.  If the generated value n is in {0, 3}, use it to generate two bits.  Otherwise, use n-4 as a single bit.  This works for any even number - what you're doing is generating bits for every 1 in the binary representation of N.  In the special case where there is only a single 1 bit - i.e., a power of two - you can always generate exactly that many bits.
This doesn't work when N is odd and it's interesting to see what happens:  You pull off values by powers of two and are finally left with only a single possible value, which doesn't give you even one bit of output.  I agree with you that in this case, you're stuck.  Using the trick above, you only have to throw out one possible value - and by thinking of k outputs of the N-value generator as a k-bit base N number, you can make the chance of having to throw away a value decrease exponentially with k.  So the probability of being forced to wait indefinitely can cheaply be made exponentially small - but I agree that it can't be eliminated.
The reverse argument is, I think, stronger:  Given an unbiased bit stream, producing an unbiased choice of from {0, N-1} without the possibility of infinite waiting seems to only be possible when N is a power of 2.  (Here, the simple counting argument works:  You can't possibly use all states of the input, and you *might* get an infinite stream of "skipped" states.)
                                                        -- Jerry

@_date: 2014-12-05 22:49:55
@_author: Jerry Leichter 
@_subject: [Cryptography] cost-watch - the cost of the Target breach 
There's plenty of blame to go around.  Target didn't do its part here - and the banks were able to show it.  I agree with the principle that the costs should be imposed on those in a position to fix the problem (but fail to do so), but the facts as decided in this case were that Target *was* (at least partially) in that position, and indeed didn't do what they should.
It's also not at all clear that the banks were the ones who resisted on chip and pin.  *They* wouldn't be the ones bearing the costs of replacing all the card readers out there - and they stand to gain from the liability shift that leaves merchants who don't get new terminals stuck with any loses.  Over all, win/win for the banks.
                                                        -- Jerry

@_date: 2014-12-06 18:42:11
@_author: Jerry Leichter 
@_subject: [Cryptography] cost-watch - the cost of the Target breach 
I think that's a gross overstatement of Anderson's finding.  Problems, sure.  But attacking chip and pin requires a much higher level of sophistication than attacking magstripe cards.  As George Orwell said, the best is the enemy of the good.  It's not as if there's been a realistic, significantly better, alternative out there.  (Perhaps Apple Pay and similar token-based systems will prove to be the next step.  But new systems *always* look much stronger - until people start attacking them.  We'll see.)
There's a difference between the British and US legal systems - and always has been.  For years, British banks were successfully able to push liability on to customers (they had their own customers prosecuted for fraud for claiming that withdrawals made using the bank's "completely secure" ATM cards weren't theirs) and merchants.  In the US, they've never been able to push anything on to customers, and in the case of merchants, it's a messy system (though the merchants do usually get stuck).  With chip and pin (but more on that in a moment) they seem to have deployed both carrot and stick:  Once it's in place, any merchants who aren't using it will get stuck with the entire loss, while they have some degree of protection if they are using it.  However ... just to be clear, most banks in the US aren't even rolling out true chip and pin.  They are going with a bizarre blend called chip and signature.  This is better in one way:  Cloning a chip card is, the last I heard, impossible (or at least very expensive), while cloning a magstripe card is trivial.  But you don't get the true second factor of a pin since no one actually checks signatures.
I don't know where you get this from:  It's produced significant improvements where it's been deployed.  Crime based on cloned cards - the major kind of CC crime in the US - disappeared and hasn't returned.  New, more sophisticated attacks (e.g., based on modifying terminals - sometimes way up the supply chain) have emerged and overall levels of attack are rising.  Of course, the US, coming late to the party, won't have the honeymoon period as the attacks are already out there.
Reference?  (And again, keep in mind that the US is mainly rolling out chip and signature.  The goal is to get to chip and pin eventually, but the industry claims that going there in one step is too much change for consumers to absorb in one shot.)
                                                        -- Jerry

@_date: 2014-12-06 21:37:24
@_author: Jerry Leichter 
@_subject: [Cryptography] cost-watch - the cost of the Target breach 
To be fair, Anderson was heavily involved in the defense of people unfairly blamed for loses by banks who claimed their systems were "completely secure", when in fact they were horrendously insecure.  This caused real harm to real people.  So on the matter of assignment of responsibility, his views are understandable. The British banks have always been much better at fobbing responsibility off on consumers than the American banks - hardly something to be proud of.  (I don't know what the "state of play" is in the rest of the world.)
On the general matter of the way he reports on vulnerabilities, at least his published papers - my only exposure to his work - he's *at least* as responsible in describing the nature of the attacks he and his students find as other researchers I'm aware of.  Given the general response of corporations to any demonstrations of problems in their systems - denial, obfuscation, attacks on the reporters - it's understandable that not all reports remain sober and careful analyses of the true risks.
                                                        -- Jerry

@_date: 2014-12-07 10:20:08
@_author: Jerry Leichter 
@_subject: [Cryptography] cost-watch - the cost of the Target breach 
And how many Web-based systems continue to be fielded today with security enforcement in the browser or other client?
                                                        -- Jerry

@_date: 2014-12-08 18:02:07
@_author: Jerry Leichter 
@_subject: [Cryptography] Toxic Combination 
Older observations. In the mid-to-late 90s, the CA industry was floating
Just curious, but for that $100/year, were they offering any kind of assurance/insurance - to anyone?
Or am I just being silly to ask?
                                                        -- Jerry

@_date: 2014-12-09 15:29:20
@_author: Jerry Leichter 
@_subject: [Cryptography] A TRNG review per day (week?): ATSHA204A has low 
It's easy enough to make a ring oscillator whose frequency drifts and which is sensitive to things like temperature.  In fact, you have to design it properly to *avoid* such effects.  A "badly implemented" ring oscillator would itself be a source of entropy.
*How much* real, usable entropy is the important question, however.  And how easy is it for an attacker to either (a) build a model of a particular ring oscillator on a particular part based on observations of its behavior and then predict what it will do; (b) *influence* what it will do using techniques like RF fields.
For low- to medium-security use in suitably controlled environments, this might be a "good enough" solution - but you'd need a careful analysis to really say.
                                                        -- Jerry

@_date: 2014-12-10 14:12:44
@_author: Jerry Leichter 
@_subject: [Cryptography] North Korea and Sony 
[Same thing as plain text]
Mainly the former.
I'm most of the way through Timothy Geithner's Stress Test, his personal history of the 2008 financial crisis.  One of the things that becomes crystal clear is the bizarre way in which value in the financial system is created as a result of participant's faith in the value of other participants.  So Citibank (to pick an arbitrary example) is willing to lend some manufacturer money to build a plant because it believes the plant will produce goods that will make the manufacturer enough money to buy it back; Goldman is willing to buy Citibank's bonds because it believes in the value of the loans on Citibank's books; the stock market is willing to buy shares in Goldman because of the value of its loans to Citibank; and so on.  This is a simple linear chain, but in fact the interconnections these days are extremely complex - so complex that no one can really know what they are, and have to go on faith.
You'd think that the values are ultimately grounded in the loans backed by land or machines in a factory, but they get decoupled because of different time constants:  Loan periods can be as long as a hundred years and as short as overnight, and banks live off of lending four long periods while borrowing (repeatedly) for short ones.  The short loans have to be repeated, so their values within the system are re-established all the time, but the long ones have a value that can only be guessed at if they aren't re-sold.  The net result is that there are many fixed points to the system - some in which the value is high, some in which it's low.  The only thing that keeps the markets stable at a high point is faith in everyone's ability ... to keep playing as if the values are high.
Any interruption can cause the whole thing to collapse quickly.  Traditionally, anything that reveals that some of the long-term estimates are wrong - e.g., that the holders of many mortgages won't actually be able to repay them, or that companies that borrowed money to buy equipment won't be able to sell what they make because there's no demand - can rapidly ripple through the system and cause the whole thing to fall apart.
The concern here is that a new source of instability has been introduced into the system.  It's always been a (justifiable) article of faith that as long as a bank, say, actually has enough assets to pay off its creditors on an ongoing basis, it will be able to do so.  But now people look at Sony and realize:  What would happen if a similar attack were mounted against Citibank?  It would have plenty of assets - but with its computer systems crippled, it would be unable to pay debts it owes (or, really, roll them over by incurring new debts while retiring old ones) or collect on debts owed to it.
The time constants here are very short: A bank could probably survive - with significant damage - if unable to effectively manipulate its (all virtual - it's not like Citibank has a vault full of gold somewhere) assets for a day.  But it would be severely damaged in a week, and dead within two weeks - probably less.
We've never seen a collapse of this sort.  Back when ATM's were first being introduced, I recall seeing an article speculating about the effects of a collapse in the ATM networks.  But they were looking at the "Main Street" effects:  What happens to individuals and local businesses when no one can get cash.  The time constants here are much longer.
We've had bigger disasters (e.g., Sandy) where not only were all the ATM's knocked out (because power and telecom were down) but all kinds of other infrastructure was damaged, and people adjusted in various ways until the systems could be brought back up.
But the evidence of past financial crises is that there is little adaptability to large shocks in the financial system ("Wall Street").  It's Perrow's risky system:  Very highly interconnected, with very short time constants.  While "Main Street"'s coupling to the system isn't as close, it ultimately *is* there:  When the big banks can't borrow, they can't lend, and ultimately that leads to a general decline in the economy (as we saw in 2008).
So, yes, there is something very real and very scary here.  The danger has been there for a while, but no one thought about it - and since no one thought about it, it had no effect in how financial players viewed each others' riskiness.  Now we have an example, in Sony, of what could happen. No one believes the security of the financial institutions is significantly better than Sony's - everyone buys from the same vendors and follows the same "industry standards" which have now been shown to be hollow.  Not only does no one know how to do better - no one has a clue about how to respond if a Sony-like attack hit someone "systemically important" like Citibank.  The traditional backstops - sufficient capitalization, for example - don't help if the capital is there but can't be accessed because all the computers are frozen.
My guess is we're going to see the equivalent of capital requirements in backup systems:  The development of isolated mechanisms that allow access to enough functionality to see a bank through any reasonable foreseeable damage to its main systems.  Manual systems?  Isolated computer systems with no network connections at all?  How can these help to interact with counter-parties who rely on all their day-to-day systems?  It's really not clear.
Do the banks *also* have some ulterior motives?  Always - though those motives are hardly secret:  They want maximum profit for themselves.  Banks are the ultimate capitalists:  They work with nothing but money, and interact with their customers solely through exchanges of money.
Banks will want to emphasize the "cyberdefense" side rather than the "system capital" side because (a) it's cheaper - banks hate to have their capital requirements increased because that costs them money every day; (b) it's easier to fake, and since in their heart of hearts, they all know it's about faith, not about reality - why not go with a fake everyone believes in?  (Which would actually work - up to the moment someone mounts a real attack.  But, hey, the bank guys already invested their bonuses in things like real estate.)
                                                        -- Jerry

@_date: 2014-12-10 17:01:04
@_author: Jerry Leichter 
@_subject: [Cryptography] Sony-certified malware 
Apropos a thread about concern in the financial sector:  Signing games containing malware *after the attack has been made public* is amateur stuff.  Imagine getting access to a bank's signing authority and issuing some big money transfers.  (Even if the bank has an HSM and you can't actually extract the private keys, a Sony-level attack could well give you a way to create the necessary orders and have the HSM sign them for you.)
                                                        -- Jerry

@_date: 2014-12-10 22:30:25
@_author: Jerry Leichter 
@_subject: [Cryptography] North Korea and Sony 
Well, sure, but the point is that the naive view is that ultimately everything is backed by something with some kind of inherent value.  Taylor Swift's song catalog is valuable because enough people like to listen to her music; Google *has* a cash flow because it delivers ads - which have value exactly when they get people to pay for things.
This view is "naive" because no one's really been able to come up with a way to define "inherent value" except in relation to other things of "inherent value" - it's the market that makes the value.  But we are all ready to believe that edible food, say, pretty much always has some kind of inherent value, even if we can't quantify it.
None of this matters to the point I was making though:  The financial system abstracts money so far from the underlying "things of value" that we might think underpin it that any underpinning becomes irrelevant.
This was one of the big lessons of the 2008 disaster.
It's not just management bonuses - it's also return to investors.  All the incentives in the system point in the direction of higher leverage and higher risks.  That's why the grand vision of "deregulate and let the market do its magic" is a bunch of crap when it comes to financial firms.  The market will indeed do its magic - but where it ends up taking the economy is not necessarily a place the vast bulk of people dependent on the economy would want it to go.
But to return to the point:  Increased capital is the classic solution to the classic risks that financial firms face.  What Sony reveals is an entirely new class of risks, which the classic techniques cannot mitigate.  The point of capital is that the firm can use it to pay debts without any limitations:  It's highly liquid, it never has to be paid back.  But a nominally liquid asset may be inaccessible if all your computers are trashed.
I suspect the next step will be something akin to an escrow account for capital.  Having a firm hold its own capital, in a world where the mechanisms it uses are subject to severe attack, isn't sufficient protection.  Somehow, the capital will have to be placed outside the day to day control of the institution it protects.  Figuring out how to accomplish this - while ensuring it can be released quickly when needed - will be a big, complex jobs.
                                                        -- Jerry

@_date: 2014-12-11 16:44:37
@_author: Jerry Leichter 
@_subject: [Cryptography] North Korea and Sony 
There's also the quote from one of the Unix designers - I think - that programmers build the most complex systems they can understand.  Since debugging is widely understood to be much harder than programming, this means we regularly build systems we aren't smart enough to debug.
Good engineers look at systems that are too complex and think about how to make them better.  Financial guys look at systems that are too complex and think "I see ways to make money off that."
Many years ago, a friend was starting out as a lawyer dealing with the financial industries.  He described multiple ways that people found to make money by exploiting loopholes in the laws and regulations.  I came to a realization:  "Financial engineering and lawyering" and "software engineering" both have a large component that involves finding bugs in existing specifications and systems.  The difference is that over on the finance side, finding loopholes is seen as a great way to make gobs of money; while over on the software side, it's seen as an opportunity to make the specs and systems better.  Of course, on the software side we have the "bad hackers".  But few of us celebrate them.
The rollout of Obamacare is certainly not reason for optimism.
Obamacare also revealed an old, but recently growing, problem with the perception of what software developers can actually do.  A huge system that will deal with millions of people to implement a complex, sometimes badly written law - to roll out in one big bang in 18 months?  Sure, no problem.  A system that will block "inappropriate material" on line, without blocking what shouldn't be blocked?  "If they can figure out a way to do X, surely they can do *this*.  Let's mandate it...."
Indeed, we're on the same page.  But ... the largest Internet players do this kind of thing already.  Netflix has its Chaos Monkey, which goes out and randomly breaks things on the live infrastructure, just to test that the overall system will survive.  Google does exercises every year in which "bad things" happen - e.g., an earthquake knocks out communications with Mountain View.  Having seen the Google exercises close up ... they are taken very seriously, and help make the overall system much more resilient.
One wonders, however, whether the willingness and cultural ability to embrace such exercises is tied to a certain kind of "by the seat of your pants" Internet way of doing things.  How would deliberately induced chaos for testing and improvement fit into a top-down, control-everything culture like Apple's?  I would think not well.  Are there alternate models that *would* work?  Does Apple have an approach?  (It's not the kind of thing they would *ever* talk about.)                                                         -- Jerry

@_date: 2014-12-12 10:06:42
@_author: Jerry Leichter 
@_subject: [Cryptography] North Korea and Sony 
Both Android and, I believe, iOS do this already.  Probably ChromeOS, too.
It's an interesting re-use of the basic system capabilities.  All three of these systems grew out of Unix, which was a time-sharing system.  Such a system needs to protect users against each other, so developed a way to identify users and then do authentication based on user id.  Modern devices are pretty much intended for use by one user - but that user no longer completely trusts the code he runs.  So we can flip things around and identify not users, but individual programs.  All the isolation that the traditional systems enforced between users now isolate programs from one another; in fact, now you have to figure out what should be shared between programs and how to share it.
The old division between discretionary access control - controlled by the users - and mandatory access control - controlled by system policy, not directly influenced by the user - has been flipped around, and the mechanisms that used to be discretionary are now controlled by policy and are mandatory.
BTW, it's this repurposing of the user ID mechanism that makes it complicated to support multiple users on devices running these OS's.
                                                        -- Jerry

@_date: 2014-12-12 13:31:58
@_author: Jerry Leichter 
@_subject: [Cryptography] Sony finding SHA1 collisions? 
It doesn't actually say that - in fact, in what might be a later edit, it says the opposite:   "[The SHA1 signature is in the metadata provided with the seed, not a result of a file that causes a SHA1 "collision" by matching the file's exact hash.]"
I can *claim* any fingerprint function I like in the torrent description.  You have no way to know if the file actually matches the fingerprint until after you've downloaded it - but then you'd know it was bogus as soon as you tried to watch it anyway.
It would be possible to do better by publishing a list of hashes of relatively small segments of the file.  As soon as any segment fails to match its hash, you know you have at least one bad source.  But that makes the torrent description significantly larger.
                                                        -- Jerry

@_date: 2014-12-13 08:19:19
@_author: Jerry Leichter 
@_subject: [Cryptography] North Korea and Sony 
You end up with a user id for every user/program pair, as you certainly don't want to move inter-user controls into each program.  With only a single user, the classic suid mechanism grants the right access to a program while it's running; with multiple users, you'll need something more elaborate.
Certainly not impossible, just much more complicated.
                                                        -- Jerry

@_date: 2014-12-13 08:23:42
@_author: Jerry Leichter 
@_subject: [Cryptography] Keith Alexander's IronNet Cybersecurity Comsec 
Someone should point out that the same Mr. Alexander's security experience included allowing a contractor by the name of, err, Snowden was it, to make off with some off the allegedly most highly protected information in the world.  Is this who you want running *your* security operation?
                                                        -- Jerry

@_date: 2014-12-15 12:28:25
@_author: Jerry Leichter 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
Insufficient evidence.  It's hard to imagine that anyone could build this kind of thing and keep it secret with or without Microsoft's help - but they obviously did.  "Hard to imagine" isn't useful evidence.
The idea of hiding stuff in NTFS extended attributes - not really metadata - is old.  I remember doing it years ago.  There are standard command line programs that come with Windows that can do it.  People used to Unix file system semantics tend not to think of this stuff, but both Windows and MacOS have long had file systems that supported stuff beyond the "a file is just a stream of bytes" concept for years.  It's never been widely used on Windows, and it's been getting less and less use in MacOS for a while now - but there's nothing new here.
Again, not much really new here.  In fact, SOP for many complex files in Windows:  The classic .doc/.xls file formats are more or less FAT file systems stored within a single containing file.  (All these programs had to face the issue of how to store independent streams of data - e.g., the base contents of a document and recent changes - within a single file.  Since FAT already solved that - why not re-use it?  This became less of an issue when hard disks became faster - and much faster than the original floppies, so simpler formats that required completely re-writing the contents on disk became practical.  More recently, this flipped around again as highly graphic content made writing the whole thing expensive again.  When Apple initially came up with Pages/Keynote/Numbers as its own "office suite", it used multiple separate files, but hid them within a "package" - a directory marked so that Finder would usually treat it as a single file.  But this didn't work well for iCloud, so the newest versions use a single linear file re-written each time - which is noticeably slower in common cases.)
Anyway ... no surprise, whoever designed Regin had deep familiarity with the innards of Windows.  But the information and the basic design patterns here didn't require insider knowledge.  I'm not particularly versed in Windows internals (and the last time I had to use Windows, it was Win2K), but none of this is new to me.  That doesn't minimize the sophistication of Regin as a whole - but nothing here leads me to say "Microsoft had to be involved".
                                                        -- Jerry

@_date: 2014-12-15 15:05:34
@_author: Jerry Leichter 
@_subject: [Cryptography] Sony "root" certificates exposed 
Given that, the attackers could likely have stolen millions before they were noticed.  The whole movie business revolves around corporations set up for particular movies, or particular parts of particular movies - and those one-off corporations have sudden large expenses.  Someone with access to the corporate accounts should have been able to shift large amounts of money without it looking suspicious - especially as they also had the access needed to manipulate or create schedules so that it would really look like a shoot was starting up.  It would take a while for anyone to notice.
The guys who did this were after (a) mayhem; (b) lulz; in some combination.  Much more of (a) than of (b).  They weren't after money, nor were they trying to do damage to the broader industry, much less the American economy.
                                                        -- Jerry

@_date: 2014-12-18 14:53:44
@_author: Jerry Leichter 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
What it would actually guarantee is the complete collapse of high tech.  No single country in the world today has the capability to design and build even the components you list.  Nor could they easily gain that capability. And if they did gain the capability, it would be at a much higher cost.  This is most obvious at the bottom:  IC manufacturing facilities cost many billions of dollars, and can only make stuff cheaply if they sell huge volumes.  *Maybe* China's market, on its own, is large enough.  *Maybe* the US market, on its own, is large enough.  There's no chance that any other individual market is close to large enough.
And why are you looking at "countries"?  Should the EU do EU-wide development - or should the French be concerned about using German-developed routers, and vice versa?  Should the Catalonians use their own infrastructure to make sure the Spanish government hasn't compromised it?
This sort of stuff is jingoist nonsense.  You think we can't find ways to compromise Chinese manufacturing?   You think the Chinese can't find ways to compromise ours?
                                                        -- Jerry

@_date: 2014-12-19 06:38:23
@_author: Jerry Leichter 
@_subject: [Cryptography] Certificates and PKI 
So Google has announced that in the future they Chrome mark HTTP connections as "risky", perhaps moving the world toward "all encryption all the time".  However, all the browser makers - Firefox in particular - continue their war against self-signed certificates.
If your goal is security against passive eavesdroppers - and, in particular, against "record everything" government agencies - then a self-signed certificate is as good as anything.
If you want to defend against active MITM attacks, then you need a trustworthy certificate.  But as we all know, the current model of hundreds of equally-trusted CA's cannot possibly produce legitimate trust.
Recent efforts like certificate pinning and certificate transparency can go a long way toward proving trust in certificates - *but they can work equally well no matter who signs the certificate!*  Granted, they were *designed* on the assumption that the pinned/recorded CA was one of the "blessed" CA's that every browser comes with - but there's nothing that requires that.  A "pinned" self-signed certificate - pinned to itself - is as trustworthy as any other pinned certificate.  (In fact, it's basically just a wasteful representation of trusted public key store, something I've discussed here previously.  But it fits into the existing infrastructure with no changes.)  The security of certificate transparency doesn't come from CA's - it comes from the owners of sites watching for attempts to create certificates in their name.  That works no matter where the legitimate certificates come from.
How can we get the browser makers to stop buying in to the PKI fiction that does little except keep the CA business model alive?
                                                        -- Jerry

@_date: 2014-12-21 14:22:29
@_author: Jerry Leichter 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
The pattern up to now has been for that attackers to move to lower and lower levels of abstraction.  Hack user code; hack OS code; hack boot-time code; hack firmware; hack the logic-level description of the hardware; hack the individual transistors (changing dopant levels so that the circuitry doesn't do *quite* what the logic assumes).  Every step down this hierarchy is harder for the attackers, but *much* harder for the defenders - and also much harder for the subset of defenders who analyze the details of attacks.  The lower you go, the fewer access points there are for analysis and the more delicate and hard to detect are the modifications.
Detecting chip-level attacks on chips with the complexity that's common today is an extremely difficult problem.  There are just so many places to hide things, and so many changes an attacker can potentially make.  And the technology is moving forward at a rapid clip, so techniques you develop against today's chips will likely be useless in a year or two.
As for direct comparison:  Every chip maker has its own "secret sauce", it's own detailed process technology.  I doubt a point-by-point comparison would be useful - too many false positives.
On the other hand, attacks against the on-chip implementations are probably as low as you can go in the abstraction hierarchy - we don't have the technology to change the inherent properties of silicon or electrons. :-)  So in some sense, perhaps the chase has finally reached an endpoint.
BTW, for a fascinating fictional look at the attack hierarchy (and a great mind-stretching read), see David Brin's "Existence".  (You'll have to get well into the story to see what I mean.)
                                                        -- Jerry

@_date: 2014-12-22 13:48:43
@_author: Jerry Leichter 
@_subject: [Cryptography] 78716A 
That's a silly conclusion.  I would not have expected it ever to be part of the NSA's mission to protect your home network.  It's about like expecting the Army to provide you with guidance and tools to protect your home from foreign invaders.
Let's get real here.  There are levels of attack and protection.  The Army is supposed to protect *American society as a whole* against foreign attacks.  The NSA is supposed to primarily protect the communications of the US government, and secondarily the communications of the Americans in the aggregate.  They haven't been doing much of a job at that - too busy listening in on everyone - but that at least is arguably their mission.  Protecting you, personally ... no.  (And frankly, would you real *want* them to have that responsibility?  With responsibility necessarily comes power.)
Hell, it's been confirmed by the courts that municipalities and their police forces don't even have any specific duty to protect individuals:  If you call the cops to say someone is breaking in to your house but they don't get there in time to protect you - well, it's sad, but they have no *particularized* responsibility to protect *you*, and you have no recourse.
                                                        -- Jerry

@_date: 2014-12-23 00:15:07
@_author: Jerry Leichter 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
In terms of attacks "in the wild", I'm not aware of any that get below the boot code level.  (That doesn't include physical attacks based on modifying the hardware.)  Oh, there are always rumors - but I don't know of anything confirmed.  The attacks at lower levels are described in academic publications.
How viable they actually are, we don't know.
Mounting attacks lower in the stack is difficult and expensive.  It should come as no surprise that those who are in it for the money or the power will go with the cheaper software attacks as long they remain successful - which, today, they are.  Should those become too difficult, someone will go find the academic papers and start building real exploits.
Of course, we can't know if lower-abstraction-level exploits are being mounted today - but are so well hidden that we never detect them.  But the existence of state-of-the-art attack mechanisms like Stuxnet and Darkhotel - none of which go deeper than the OS - argues that if lower-level attacks are being mounted, they are being mounted by the most sophisticated parties in extremely unusual and specialized circumstances.
                                                        -- Jerry

@_date: 2014-12-23 19:12:21
@_author: Jerry Leichter 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
Ah, yes, the IBM 1620 - sometimes affectionately known as CADET, for Can't Add Doesn't Even Try.
Memory consisted of decimal digits.  (To be accurate, memory consisted of 5-bit (wired core) units.  One bit was the "flag" bit, which indicated the most significant end of a field - typically a decimal number.  A field always had at least two digits, addressed by its least-significant end (which had the highest address); the flag on the least significant digit indicated a negative value.  The remain four bits encoded the decimal digits, a special record mark (used to define groups of fields) and a group mark (used to define groups of records - mainly for I/O).  The remain four combinations were unused; they would not be produced by any normal operation, and if read from memory caused the machine to stop.  (The 1620 was a single user desk machine.  Not desk top - it was an entire rather large desk, complete with an embedded pre-Selectric IBM typewriter that formed the "console".)
Addition was done using addition tables that had to be loaded into memory:  Given two digits n and m, the hardware looked at location 0001nm in memory.  It had to contain n+m mod 10 as the digit; its flag indicated if there was a carry.   Some of the most complex wired logic in the machine handled adding in the carry's, and complementing mod 10 to do subtraction (or addition of negative numbers, of course).  Addition was done digit pair by digit pair, just as you would do it on paper.  The addends could be of arbitrary length; the result, as I recall, replaced the first.  You had to make sure there was enough room - the input from a terminated field was just forced to be zero, but the output went right to memory beyond the end of the field.
Multiplication was repeated addition, again as you would do it on paper.  I can't remember the exact details, but the multiply tables went from 000200 to 000399, with, of course, two-digit results.  Division was repeated subtraction.  The region of memory going down from 000099 (and wrapping back around to the top of memory for really long results) was the target of any multiplication, and the  dividend for division.
As I said above, the special locations from 000100 to 000399 were reserved for use as tables, but it was up to you to populate them.  If you didn't, arithmetic ... didn't work.  I actually loaded tables to do binary addition for a truth-table evaluator I wrote.
Interesting early (1960's) machines.  The only machines ever built, as far as I know, that did arbitrary-length arithmetic in hardware.  There was a complete floating point library that came with FORTRAN - decimal mantissa with power-of-ten exponent.  Like the hardware, it allowed arbitrary lengths for the mantissa.  (No one had a use for more than two-digit exponents, though supporting that would be easy enough.)  The FORTRAN (that would be FORTRAN II) compiler had a "FANDK" control card on which you specified the sizes to be used for floating and integer variables.  You could also specify a "noise digit", which would be shifted in on left shifts of floating point values.  Try your computation with noise digits of 0 and 9 and you can quickly see how many trailing digits of your output was just noise.  In general, decimal FP is a much better match to people's intuition than binary FP, so some of the common traps for the unwary in FP arithmetic aren't there.  And the ability to just add some more digits to the mantissa and see how things change is great.
Memory was 20,000, 40,000, or 60,000 decimal digits.  You could get a disk that was, I think, 1.5 million decimal digits.  (We're talking one of the washing-machine size units here.)  Available peripherals were a card reader/punch, a paper tape reader/punch, and a line printer.
Slow?  I once calculated that a 1620 was slower, on an equivalent addition, than one of the original vacuum tube computers.  Still ... it was a workable "personal" computer, and people did some significant engineering work on them.  (There was a "Model II" version which was at least twice as fast in general, and also came with an actual decimal adder, rather than using the add tables.  Multiplication and division were unchanged, however.)
People used to race the FORTRAN compiler.  Good coders won.  (It helped that the instruction set was very simple and logical.)
I don't recall ever seeing this done.  Given how slow the machine was to begin with, it doesn't seem to be the kind of thing that would be used on any regular basis.
                                                        -- Jerry

@_date: 2014-12-23 23:15:20
@_author: Jerry Leichter 
@_subject: [Cryptography] floating point 
I think he was discussing comparing floating 0.0 with integer 0 - that last "." is the period at the end of the sentence,  Nevertheless, any reasonable hardware and software implementation that allows such a comparison (by converting both to a common representation) will decide they are equal.
In any areas, there's a general, easy to state rule that will keep you safe. "Don't compare floating point numbers for equality" is such a rule.  Aa you come to understand the field better, you learn that the general rule can be broken in many cases - e.g., "small enough" integers, exactly representable values.  The cases are complicated enough that we don't try to summarize them in a simple rule:  You need to be wary not just of the mathematical values but of the way they came to be computed.  Just because the mathematical value of an expression is an exactly representable number doesn't mean that the evaluation of that expression using FP arithmetic actually produces that value!
For most people who don't dig deeper, following the two rules "Don't compare floating values for equality" and "don't subtract floating values that are very close to each other" will avoid many of the common traps of floating point.  (Actually, there's another one that has no really simple statement, but "never print more digits of precision than your floating point numbers can actually represent" is one way of getting at it.  Following this rule avoids all kinds of traps people fall into when they think they can get more precision out of a calculation than the numbers themselves can actually carry.  Seen it happen....)
                                                        -- Jerry

@_date: 2014-12-24 12:08:12
@_author: Jerry Leichter 
@_subject: [Cryptography] floating point 
I was talking about printing numbers for humans to read - i.e., final results.
Attempting to save and then read back FP numbers in decimal format is either straightforward - if your programming language provides a direct way to do it - or an "advanced topic" if it doesn't.  (Modula-3 may have been the first language to provide an explicit way to do this; its default method for printing an FP method produced (one of the set of) decimal representations for the FP value given that was (a) as short as possible; (b) if read by the corresponding default FP reader, would re-create the same bit pattern as the original value.  This isn't true of at least C or FORTRAN:  FORTRAN D or F or G format, and the C %f/%g formats they inspired, when given no precision, simply arbitrarily give you 6 digits.  If you want to do a round-trip, you have to figure out the appropriate representation for yourself.
At about the same time, a pair of papers by different sets of authors, by coincidence, were submitted to and appeared in some ACM conference (I forget which), one on how to print FP numbers efficiently and correctly, one on how to read them.  Most implementations at the time were either inefficient, incorrect, or often both.
Those papers, and the Modula-3 approach, have influenced implementations since.
... or less.  If the goal is to minimize what you print, it's a harder problem.
And perhaps those who do know about a subject shouldn't look down their noses at those who don't, but try to provide helpful guidance.  Knowing the details of FP arithmetic doesn't make you a better person - it just makes you a person who knows the details of FP arithmetic.
If I can get across to those without detailed knowledge that:  If you're doing fairly simple stuff and don't have to worry about ultimate performance or high accuracy, following a few simple rules will keep you out of trouble; and if you need more, go ask an expert - I feel I've done my job.
                                                        -- Jerry

@_date: 2014-12-27 08:54:18
@_author: Jerry Leichter 
@_subject: [Cryptography] floating point 
Executive summary:
...long and worthwhile article about the subtleties, oddities, and "non-mathematical nature" of (mainly IEEE) floating point omitted.
I spent my early years as a mathematician, and "bad" properties like "equality" that isn't an equality relation still bother me intuitively.  But I've spent many more years writing software, and I've come to a somewhat more nuanced view.  Ultimately, what matters isn't mathematical purity - it's usefulness, which is much harder to pin down and much more multi-faceted.  (BTW, this applies to mathematics as well.  The criteria for "usefulness" are different - mainly, "leads to elegant theorems" - but the underlying drive is the same.)
*There is no such thing as a mathematically reasonable fix-precision floating point model*.  The floats are not the rationals, and they are certainly not the reals.  Fixed-length integers aren't the integers either, but at least if properly designed they match the integers mod some value - though those aren't ordered, which leads to all kinds of traps for the unwary when "overflows" occur.  But floats don't really correspond to any well-understood, pre-existing mathematical model.
There are a variety of properties of the rationals (or the reals) you'd like an FP system to have, but you can't have them all at once.  So you have to pick and choose what you keep and what you toss.  Many years ago, APL\360 defined a consistent notion of arithmetic in which two FP values were considered equal if they were within a (configurable) epsilon of each other.  This has all kinds of interesting implications - such as that a (small enough) FP value was considered to be an integer and treated as such - if it's within epsilon of an integer.  So A[1.000001] - or A[1/3 + 1/3 + 1/3] - might be silently treated as A[1].  Mathematically, this is a horrible system - equality isn't even transitive, for a start.  For most day-to-day computations, however, it turns out to be very useful, giving "intuitive" results.  Tons of calculations - engineering, financial, and other - were done using this system over the years, and people were reasonably happy with the results.  (One could argue that the computations it replaced were previously done on slide rules or on calculators with a fixed number of decimal positions - both of which would, informally, implement a related arithmetic.)
IEEE FP was designed partially in reaction to a variety of earlier FP systems that had really bad properties.  Most of the earlier systems were designed by computer engineers who didn't understand FP but did understand how to get high performance.  (The FP on the old CDC supercomputers was a case in point.)  IEEE FP was designed by numerical analysts.  Except ... it was a subset of numerical analysts, mainly those interested in linear algebra.  They designed a system with properties that were good for one class of problems, perhaps not so good for others.  (Numerical analysts working in other fields have criticized the IEEE design.)
IEEE representations have one enormous advantage:  They come in hardware, so have much higher performance than any software alternative.  (I haven't looked at the FP that comes with graphics cards; I suspect it's mainly a return to the "make it fast" philosophy of the early days, especially since the uses for which they are primarily aimed - computing stuff to put on the screen - isn't sensitive to fine details.  It'll be interesting to see how it evolves as graphics cards are increasingly used as highly-parallel compute engines for other purposes entirely.)  Those who are unhappy with the properties of IEEE arithmetic spend their efforts in building better - more appropriate for their purpose - systems with IEEE as the primitives.
Henry Baker described a standard for geometrical description of objects that will be 3D-printed based on triples of IEEE floats.  A moment's thought reveals how inappropriate that representation is.  In fact, the use of floats *at all* is inappropriate.  Why?  Because floats are entirely about relative error, while the construction of objects in the physical world is about absolute error.  This representation guarantees that you'll be fighting the basic design every step of the way - and, indeed, Baker comments that to determine geometrical properties, you need to ensure that certain values are bitwise identical.  That breaks the entire abstraction that the FP representation gave you.  (Java made that mistake early on, and had to adjust - viz. "fpstrict".)  This (and the related desire for completely reproducible results) is why TeX doesn't use FP any place that "matters".  (It's only used when printing to log files and in similar situations; all computation that affects the typeset output file is done using rational arithmetic.)
Does that mean the standard Baker refers to is "wrong"?  From an elegance/correctness point of view, sure.  From a "usefulness" point of view, probably not.  IEEE FP support is cheap and fast and everywhere.  Any other representation would be more difficult to support widely - though one could well argue that Knuth dealt with exactly this issue in two dimensions in TeX, and one should just use his design and code, which is freely available.  (TeX positioning errors are given in absolute terms and are well under the wavelength of light - certainly good enough for typesetting, good enough for 3D fabrication unless you're building metamaterials.)
                                                        -- Jerry

@_date: 2014-12-27 13:15:29
@_author: Jerry Leichter 
@_subject: [Cryptography] floating point 
Is the issue here *IEEE* floating point in particular, or *floating point* in general?  I think the latter.  And it's not even FP.  Many of the algebraic properties you'd like to be able to rely on aren't even true for bounded integer arithmetic.  There was a long thread on exactly this topic on this list not long ago.  I forget the exact details, but the question was whether a C compiler was allowed to "optimize away" an overflow "present in the source" (in quotes since the standards specifically said that the overflow causes "undefined behavior", so "present in the source" is a statement that's tricky to make in the model presented by the standard).
I've never seen a floating point representation that has clean algebraic properties.  I doubt such a thing could exist:  In any finite FP representation, there will always be a non-zero value e such that 1+e produces the same (rounded/truncated) representation as 1.  So 1+e == 1 and yet e != 0.  You could trap such values or produce something like NaN, but neither of those gives you nice algebraic properties.  You could try to remove such small values from the set of model numbers, but since FP deals with *relative* values ... where do you stop?  The E such that 10^20+E produces 10^20 is rather large.
It's not even clear that this is worth doing.  Some compilers have modes that allow "unsafe" optimizations, but they don't seem to buy much.  The optimizations you need to get real gains *with accurate results* in FP computations are subtle and way beyond reasonable compiler technology.
The set of FP computations that gain much from this kind of thing are rather specialized.  Those who do these kinds of computations complain about features missing from machines out there, but they are an insignificant minority of the audience, so have never gained any traction.  (There was a guy who used to be a regular on the comp.compilers newsgroup years ago.  He was a statistician at some university - apparently quite competent in that field - but he kept complaining about the "obvious" things missing from existing programming languages and machine instruction sets.  The ability to do certain bit operations on FP values was one thing he insisted was "absolutely essential".  People eventually gave up arguing with him - he just could not see that the specialized computations that were important to *him* were simply not that interesting to anyone else - and that no one was going to build him either the compiler or the hardware he wanted just because he wanted it.)
The old CDC 6000's used the same registers for integers (well, some integers) and floats.  Efficient algorithms for certain common operations - ABS(), MAX(), MIN() - made use of this fact:  They could be computed using the integer and Boolean operations.  (In fact, the FORTRAN compiler generated in-line code using these algorithms.)  But even these machines had separate functional units for Boolean/integer/FP operations (though integer multiply was actually done by using non-standard FP values in the FP multiply units).  As you move on to the Cray vectorized hardware, the vector registers split off as a completely separate set which mostly gets accessed from the FP units.  That's been a broad trend:  The more potential units a register has to feed, the more complex the logic.  Keeping FP computation separate from "control" - which is what the boolean and integer operations are principally about in heavily FP code - gets you more bang for the buck.
Yes, the unusual algorithms that mix the two domains are expensive.  It's all tradeoffs.
Many years ago, when I was working at DEC, a friend was asked to port a FORTRAN program from PDP10's to VAXes.  The code implemented a model that predicted the cost of maintaining hardware; it was used to set support prices.  Deep in the FORTRAN, there was a central algorithm that rounded intermediate values for no apparent reason.  The comment described the mathematics involved - all of which made perfect sense; but it made no mention of intermediate rounding.  My friend asked me what to do with this code.  I examined it, examined the comments, and told her that the code clearly didn't implement the model correctly and that she should just toss all the extra rounding. The new program was delivered to the customer.   They soon came back with a problem:  As a test, they had run it for some older hardware.  The answers differed from what the old code produced.
We looked closely at the results and, indeed, it was due to the rounding we'd removed.  DEC being an engineering company at that time, we could easily explain this to our customers.  They agreed - our results were a more accurate representation of their model than the old results.  But ... many millions of dollars were tied up in support contracts based on the old results. It would produce a huge mess if those had to be changed.  Could we put things back the way they had been?
Now, you can take this as a sign of irrationality.  But then again, the model itself was a simplification of reality.  Who could say whether it, or the messy output with the internal rounding, was a better predictor of actual costs?  The fact is, the old version's output had been found to "work" empirically.  Living with the churn that would result from a change, with no actual assurance that the new results were in any real sense "better", wasn't justifiable.
                                                        -- Jerry

@_date: 2014-12-28 16:57:39
@_author: Jerry Leichter 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
Imagine that, in fact, every router did come with such a "kill packet".  Could you use them to build a secure network?  In principle, I believe the answer may be yes.
What you need to do is prevent anyone on the outside from controlling the actual bits in the packets as seen by the "important" routers.  This requires encrypting the entire packet at the boundary of the "core" with a key that an attacker has no access to.  It's tempting to think you can leave bits that have the property that normal traffic sets all possible combinations, so a vulnerable combination would quickly become obvious - but that falls to an attack that looks at, say, three specific bit combinations received within some short period of time.
In effect, you need the network to be oblivious to the traffic it's transmitting.  For user data, this is no big deal (unless you want to do DPI, of course).  The hard part is the routing and other control information.  In one sense, this is trivial:  The binding of meaning to particular addresses is arbitrary, and if you uniformly applied a permutation to all the addresses (and similarly things like network protocol values) to every component of the protected core, nothing would change.  (Well, all kinds of assumptions about network *ranges* would change - but between the exhaustion of the IPv4 space and the much larger IPv6 space this is being lost to a large degree anyway.)  Of course, any fixed permutation would have to be assumed to "leak" eventually, so you'd need a way to change permutations.  The easiest way is to let all packets that were already in the network just die a natural death when the permutation changes; but you can do better if you need to (with the equivalent of double buffering).
Of course, this moves the locus of attacks to the edge routers which have to be relied on to apply the permutation.  I suppose you can have layers of them, sourced from different vendors in different countries, to make it much harder to get any attack combinations through.
But ... it's a deep and difficult question, and practical as opposed to "in principle" answers are much more difficult to find.
Expected cost is probability times cost per incident.  An incoming asteroid may be unlikely, but it can have huge cost.
Of course, knowing of the possibility doesn't do you much good if you have no defense....
                                                        -- Jerry

@_date: 2014-12-30 10:32:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Actually useful "quantum crypto" advance 
(full text is available from there) describes "Quantum-secure authentication of a physical unclonable key".  This isn't quantum cryptography - it's a way of creating a physical key that can be easily verified but cannot be copied or simulated.
The key consists of zinc oxide nano particles spray-painted onto a surface.  This produces a huge number of small, randomly-oriented reflective particles.  While it's trivial to produce these, there is no known way to copy one (and good reason to believe that it can't be done).
A challenge is produced by sending light through an array of about 2000 pixels, each of which can be set independently to change the phase of the light by either 0 or pi.  The response is the reflection from the key is the response, and depends in a delicate way on the input settings and the exact orientation and position of the nano particles in the key.
Classically, you could record a whole bunch of challenge/response pairs, and then when the key is presented, pick a known challenge, find the expected response, and compare.  The problem is that anyone who steals the database can now play a man-in-the-middle game and return the right response for any challenge.
So now you use quantum mechanics.  By using very low intensity light for the challenges, you make the response "very quantum".  One effect of this is that it's impossible to actually read the challenge.  So how do you check it?  You arrange for response from the key to interfere with the expected response.  Physically, the two can be compared and you can tell if they match - even though you can't actually physically determine either, just the settings for arrays to create the challenge and the response.
If you choose a set of challenges that form a basis set, then you can construct new challenges as linear combinations of the members of the set, and the linearity of QM guarantees that the response is similarly a linear combination of the base responses.  So you can check any of a broad spectrum of challenges, but QM guarantees that even someone in possession of the full database cannot compute the right response, so cannot construct a fake key.  And no given challenge is likely ever to be repeated, so knowing legitimate responses from the key doesn't help.
They've done some work to show that blinding attacks that have worked against implementations of quantum crypto don't work here.  Interestingly, it appears that even a quantum computer - at least one with plausible properties - may not be sufficiently powerful to emulate a key.  Their discussion of this stuff is in the Supplement to the paper (linked right at the end).
Hardly the last word on the subject - more like one of the first - but clever stuff, actually implementing "thought experiments" by Bennett and Brassard and others going back to the '80's.  Unlike most quantum crypto - which arguably doesn't actually solve any new problems - this does give you something that doesn't exist classically:  Information that is accessible but cannot be copied.
                                                        -- Jerry

@_date: 2014-02-01 17:57:03
@_author: Jerry Leichter 
@_subject: [Cryptography] Unified resource on Random Number Generation 
Either way is fine concerning my comments.
                                                        -- Jerry

@_date: 2014-02-01 23:27:08
@_author: Jerry Leichter 
@_subject: [Cryptography] cheap sources of entropy 
No, Bill Stewart is right.  There are multiple layers of software with all kinds of buffering, queuing, operations that are kicked off by clocks at fairly long intervals (way longer than the timing variations seen in disk responses), in between.  It's highly unlikely that any low-level variation in disk response times will be visible by the time you reach the guest OS.
There *will* be variations, but exactly what produces them, what they are correlated with, how predictable they are, would be extremely difficult to answer.  If you go back to the original paper on disk drive timing variations, you'll see careful work to figure out exactly what kinds of variations disk drive timings will produce, and then actual measurements to show that the results really match the physical models.  No one, as far as I know, has done any work like that in a virtual environment - and frankly I doubt anyone could.  The pieces are just too complicated.
Now, you could if you wanted just say, well, if it's too complicated to analyze, it's too complicated to attack.  *You* could.  Personally, having read Knuth's introductory section on PRNG's in TAOCP, I would not.  (And also having had to debug and fix distributed systems that failed exactly because of completely unanticipated correlations in behavior, I would not.)
Go back to the paper that proposed using turbulence and repeat some of their tests in a virtual environment.  Let us know what you *actually observe*.
(BTW, it's not even clear that those measurements are relevant to today's disk drives and adapters.  Technology changes.  The intelligence in every component is much higher today than in was even a few years ago.  That intelligence gets used to adjust behavior in ways that potentially wash out low-level variations.  We don't need to repeat the idiocy surrounding Peter Gutmann's "magic 35 disk erase patterns" - carefully chosen to cover all the relevant technologies of 1996, not one of which has been used in a decade or more.  If you want to take the old analyses and apply them to modern technologies, you might produce something worthwhile.  Hard work, though.  Just applying the old *results* without considering the context, though, is nonsense.)
                                                        -- Jerry

@_date: 2014-02-01 23:47:34
@_author: Jerry Leichter 
@_subject: [Cryptography] cheap sources of entropy 
Your model of how modern disks work is way out of date.  To take one very simple example, the notion of missing a block because the sector has already passed under the head hasn't happened in years.  Disk firmware buffers entire tracks.  If you need sectors n through n+k on a track, the disk firmware will start filling the track buffer as soon as any sector in the range is available, and continue as long as the head is over sectors in that range.  Then, depending on where it started, it will do that again starting and sector n.  (Actually, for simplicity it may well just buffer the entire track on the theory that there is likely to be another read from the same track "soon", and the data is moving under the head anyway.  There's plenty of RAM in a modern disk drive to buffer several tracks.)
Virtualization would be impractical if the hypervisor had to worry about scheduling VM's fast enough to pick off individual disk sectors.
Beyond this ... you're not even going to *see* the real disk.  That data will be delivered over fiber channel emulating a LUN, or (probably the majority of the time these days) over an Ethernet, either emulating FC protocols or running iSCSI or even NFS.  Oh, if you're using an FC-like or iSCSI-based disk, you might send a "read sectors n to n+k" command, but that's going to be interpreted by some fairly high-level code in the disk server, and it's likely to look quite different by the time it hits the actual disk - and by the time the data wends its way back to the requesting machine.  To take an extreme example:  NetApp, the second-largest supplier of disk arrays, started out building NFS servers, then added support for block I/O to virtual disks later.  What you see as a "disk" is actually a file in a file system (WAFL) that was designed to support NFS efficiently.  (The biggest maker, EMC, took the opposite approach, initially selling block servers, then later building an NFS server that layered over the block server.)
                                                        -- Jerry

@_date: 2014-02-02 06:40:01
@_author: Jerry Leichter 
@_subject: [Cryptography] cheap sources of entropy 
You keep saying that as if you know it, while others say "not so" and cite evidence.
There's no point in discussing this any further:  Those who say "not so" would (a) have a great deal of difficulty proving a negative; (b) have little interest in (in our opinion) wasting our time looking for something that isn't there.  So it's on you:  If you feel such variations will survive - prove it!  The techniques were published years ago, and are not particularly difficult or complex.  A VM to which you have root access, and virtual disks of different apparent sizes and speeds, are readily available for minimal cost from Amazon, Google, Microsoft, and many others.  Have at it!
                                                        -- Jerry

@_date: 2014-02-02 18:56:46
@_author: Jerry Leichter 
@_subject: [Cryptography] Now it's personal -- Belgian cryptographer 
Commenting on only one thing in a long message:
And the alternative is ... what?  The witch hunt?  The mob?  Kill all, God will know his own?
It's so very easy to slip over the line, for (apparently) all the very best reasons.  That's what the intelligence community did.  The answer is not to slip over the same line.
The rule of law, the presumption of innocence - they are fundamentally based on the notion that we, as civilized members of society, will play fair, even when others don't.  We accept the costs because not to do so produce results that are even worse.
Granted, the rest of your message doesn't rise to the strident and dangerous call that this paragraph does.  But the urge is there.  It's there in all of us.
"He who fights with monsters should look to it that he himself does not become a monster.  And when you gaze long into an abyss the abyss also gazes into you." - Nietzsche
                                                        -- Jerry

@_date: 2014-02-03 16:00:07
@_author: Jerry Leichter 
@_subject: [Cryptography] Mac OS 10.7.5 Random Numbers 
"RSA2048 or longer" has a clear meaning:  The RSA algorithm is well defined, the key lengths it uses are well defined.  Sure, there are implicit assumptions (like that the modulus is actually a product of exactly two primes of close to equal size), but we can deal with that if we have to.
I have *no clue* what "SHA-1 or longer" means.  The SHA-1 algorithm is defined over 512-byte input blocks, with 160 bits of intermediate state and 160 bits of output.  You can readily decrease the  output block size (choose some subset of the bits).  It's not clear that there's a secure way to decrease the input block size - some kind of padding, but padding has proven to introduce vulnerabilities in the past.  You can do nothing meaningful about the intermediate state without changing the compression algorithm, which is specific to the state size.  How what everyone would agree is a "longer version of SHA-1" might be produced is anyone's guess.
Now, if you say "a one-way hash with all the properties of SHA-1 but with a longer intermediate state and output block size", well, that's kind of saying what you want - but not in a way that's actually useful unless you can pin down just what "all the properties of SHA-1" might mean.
In effect, you're making the validation dependent on the validation of something else (the hash function) - but you aren't saying what *kind* of validation is appropriate for that "something else".  Can I substitute some particular variant of Salsa, because being accepted by DJB counts as "validation"?  How about a hash algorithm developed and published for general use by the Chinese government?  Or, for that matter, the NSA?
The sponge construction, for the first time, gives us a generic description of a family of hash functions whose strength is given by security parameters we can set pretty much as we want.  It'll take a bit of time before we have adequate confidence to say "just go ahead and change the security parameters; we understand the effects".  With luck, we might even end up with an appropriately parameterized family of stream ciphers out of the same construction as well.  There have been attempts to construct similar families of block ciphers - I vaguely recall MARS, an AES candidate, was one - but none so far has gotten broad acceptance.
*If* we have such accepted families of components, and *if* we come up with ways to approve entire families (note that some combinations of security parameters might, in principle, open an other-wise secure primitive much more vulnerable to side-channel attacks), and *if* we can come up with ways to specify appropriate combinations of security parameters for different primitives making up some reviewed component ... then we might be in a position to define "family approval" for a class of instantiations of some software module.
But we have enough trouble doing meaningful approvals for single implementations today that I don't see this happening soon.
                                                        -- Jerry

@_date: 2014-02-03 18:12:40
@_author: Jerry Leichter 
@_subject: [Cryptography] cheap sources of entropy 
No, you've got it right.  A hypervisor has complete control over everything the guest OS can see.  It controls the horizontal; it controls the vertical.  :-)  While a hypervisor could not realistic monitor every instruction a guest executes - the slowdown would be recognized rapidly - it does have control over every interaction the guest has with the outside world.  And that includes, of course, program loading, paging, and so on.
In general*, every layer of the abstraction stack can completely subvert every layer above it.  A user-mode process is completely at the mercy of the OS; the OS is completely at the mercy of the hypervisor; the hypervisor is completely at the mercy of the microcode; the microcode is completely at the mercy of the hardware.  (Everything above the microcode is also completely at the mercy of System Management Mode, which doesn't fit exactly into this hierarchy - but in some sense is really "below" the hypervisor.  Similarly, you have to consider attacks on pieces of hardware other than the CPU - see the paper recently referred to here showing how to "spike" disk drive code.)
*The "in general" isn't quite the case.  There's some very interesting recent work on how to use the virtualization hardware to protect processes from malicious OS's.  Very clever stuff.  Basically, you arrange that any given page is set in the hardware so that the process XOR the OS has access to it.  When the OS needs access, you encrypt and MAC the page before resetting the page tables; when the process needs access back, you check the MAC and decrypt.  You similarly protect anything that gets written to disk.  So the OS can manage the processes - create them, give them memory, map code into them - but it can neither read nor modify the code that gets run or anything that code actually does in memory.  (There are tons more details.  I can't immediately lay my hands on the paper I read, but  an overview from some members of the group working on this.)
The reason I mention this is to raise the question:  Can similar ideas be applied at other levels of the abstraction hierarchy?  Until I saw these papers, I would have dismissed the idea that one could protect a process from a malicious OS.  Granted, you now have to trust the virtualization hardware and the protection code that uses it.  But (a) you always end up trusting *something*; (b) the pieces you have to trust are much smaller (hence much mor amenable to verification) than a full OS.
                                                        -- Jerry

@_date: 2014-02-04 12:10:18
@_author: Jerry Leichter 
@_subject: [Cryptography] cheap sources of entropy 
Missing space :-).  The link above should work.
                                                        -- Jerry

@_date: 2014-02-04 20:07:00
@_author: Jerry Leichter 
@_subject: [Cryptography] Random numbers only once 
Well, it's nice to know I'm in such illustrious company.  I made the same argument here a couple of weeks back.
Well, keep in mind that it's not just a fixed 32 random bytes.  You have to updated either the seed bytes, or some kind of counter or other varying input to the PRF, or you'll generate the same sequence of "random" values every time you boot.
There are a couple of problems with extending "forever" to encompass saving the values to disk:
1.  A saved disk can be silently copied.  No matter what kinds of protection your OS provides, while the disk is just sitting there with the power down, it can't control access, or even know about accesses.
2.  The same issue as 1 except concerning backup copies.
3.  If an attacker manages to keep you from writing a new seed or counter value, you'll generate the same "random" numbers the next time around.
Note that encrypting the disk image *seems* to block 1 and 2, but leaves you with the problem of getting the disk key to the machine securely at boot.  This is pretty much the problem you started with:  How to have a value that the machine can get to at boot, but which no one else can get to even while the machine is shut down.  Hardware solutions to this problem do exist; then again, hardware random number generators exist.  In fact, modern chipsets supply one or both.  The question comes down to what you want to assume is available - and what you trust.  Given the extremely high value an attacker could gain by compromising a central element like this, many people have become (understandably) wary of relying on them.
Note that attack 3 is more "active" than 1 or 2, so easier to detect and more difficult to pull off.
There are many details that depend on how you think these machines are actually physically realized and used.  Can you control physical access?  Or perhaps just *detect* that the machine may have been accessed?  That moves problems 1-3 to other domains:  How much you trust your physical access controls, how you recover if you detect that they've been bypassed.  At the other extreme, are you talking about a virtual machine, which may spend much of its time existing as just a disk image on someone else's disk - and is *intended* to be easy to copy?  How do you get each instance it's own seed value?  Again, solutions are possible, but there are many details to get - and keep - right.
Then again, one can say the same of a "true random number generator".
                                                       -- Jerry

@_date: 2014-02-05 15:11:27
@_author: Jerry Leichter 
@_subject: [Cryptography] who cares about actual randomness? 
I'll do you one better.  I'll bring my favorite "true random number generator".  I built it using your Turbid designs, and it also mixes in the RDRAND output of four separate Intel chips.  Plus a couple of other sources for good measure.
See, I have it all beautifully assembled in this clear plastic box.  You can even see all the parts.
Ready to play?
                                                        -- Jerry

@_date: 2014-02-06 07:09:05
@_author: Jerry Leichter 
@_subject: [Cryptography] who cares about actual randomness? 
Indeed.  The question is whether *you* are willing to place your trust in *my* clear plastic box - just as you asked ianG whether *he* would place his trust in *your* PRNG.  What I was highlighting was that in both cases, there are two things you need to trust:  The theoretical underpinnings of the system, and the actual physical realization.  As you posed the problem - your opponent presents the physical realization as a fait accompli - the underlying theory *doesn't matter*:  The possibilities available for attacking the physical realization are so broad as to completely dominate.
The people who place their trust in such things for real games are actually placing their trust in designers, implementors, and regulators who have no stake in the game other than in ensuring a fair outcome, and even in the nominal opponent ("the house") to which being perceived as fair is worth more than the advantage that could be gained in any single game.  (Even so, it's not as if there isn't a history of corrupted "games of chance" - which is why we have regulators.)
Sure.  It's not as if the vast majority of them understand how the system works anyway.  Their trust is based on reputation and on experience, both theirs and their fellow players.  It's hard to pin down where true physical entropy enters into card shuffling, and card sharks have been able to influence the outcome for centuries.  People still play card games for high stakes based on the outcome of card shuffling.
From the point of view of the end user, it doesn't matter.  Either one, if properly constructed, maintained, and operated, in an appropriate environment, works just fine for any use of "random" values I can think of.  Remove any of those assumptions and neither one works.  So in effect you're asking if I can make an argument for using a gcc-based compiler rather than an LLVM-based compiler for the C code in the system.
Suppose a number of us are getting ready to play poker.  None of us trusts any of the others.  We all do trust the provider and operator of the following machine:  It scans US currency, recording the serial number, and destroys the bill.  (OK, that's illegal; maybe it just locks it securely inside, to be returned to a bank later.)  When a button is pushed, it takes all the serial numbers scanned, in the order they were seen, hashes with SHA-256, and uses the result to initialize a "good" PRNG that will control the cards in the game.  The machine is configured to generate no more than 100,000 hands before it shuts itself down and must be re-initialized with fresh bills.
To start a game, each of us in turn will pull a bill from our wallet and scan it through the machine, in any order we please.  Then we push the button and start to play.
Will you join us?  (Again, we're *assuming* that all of us trust the provider and operator of the machine:  We trust that it really does what's claimed, and it leaks no information to anyone.)
I would argue that there is essentially no identifiable physical entropy present in this scenario but, nonetheless, there's no good reason for you *not* to play.
                                                        -- Jerry

@_date: 2014-02-13 18:51:29
@_author: Jerry Leichter 
@_subject: [Cryptography] RAM memories as one source of entropy 
I've become very suspicious of all approaches like this.  They rely on details of current-generation technologies - often *side effects* of details of current technologies.  The problem is that technologies change very rapidly.  They actually sometimes change on time scales comparable to research completion/ publication delays!  And those changes can quickly render older work obsolete.
Peter Gutmann, by being out at the forefront, manages have his papers become prime examples of this effect.  We all know about his classic "35 patterns to wipe any disk" paper, which is still cited today to justify strategies used in disk-clearing programs - though it was based on detailed, careful research on technologies that have been obsolete for two decades.  (While I haven't come across such a thing yet, I fully expect to one day see an SSD-clearing program claim to use the "Gutmann's 35-pass algorithm".)
Similarly, you cite Gutmann's 2001 paper - which, again by careful and meticulous research - proved that what "everyone knew" - that DRAM would lose all its state within milliseconds of losing power - was wrong.  But in 2001 we would be talking 128M bit DDR memory chips at 2.4V, 100-200MHz with a feature size of around 130nm; today, we're talking 4Gbit DDR3 memory chips at 1V .8-1.6GHz, with a feature size of around 22nm.  The number of electrons used to store a single bit has come down from about several thousand total to under 500 total.  The construction technologies have changed radically.  The materials used have changed.
Yes, they are both "memory" in an abstract sense, but just about every significant detail of the physical realization is *completely* different.
Does this mean that current generations of RAM *don't* have remanence?  Of course not!  What it means is *we don't really know* - unless we go out there and *look*.  Gutmann's 2001 paper should inspire us to look at the chips we have today with some suspicion - but as a guide to the actual physical properties of modern DRAM chips, it's been obsolete for many years.
And what about tomorrow's chips and disks and SSD's?  Can you rely on measurements of incidental properties of today's versions to lead you into a secure future?  Absolutely not.
If you want *physical* randomness, you need to rely on basic physical principles.  Denker's work is one example; generators based on radioactive decay (*carefully* analyzed - there are traps for the unwary here) are another.  A bit of quick hacking with some chips you happen to have sitting on your desk just ain't gonna do it....
                                                        -- Jerry

@_date: 2014-02-13 19:11:51
@_author: Jerry Leichter 
@_subject: [Cryptography] The ultimate random source 
And in the latest Snowden leak, it's revealed that the NSA, with help from GCHQ, BND, has for years been infiltrating the makers of colorful candies, and of chemistry flasks.  (The latter actually started after the anthrax scares of 2001.  Western intelligence organizations have had trouble infiltrating the Web cam makers as the Chinese were already in place there.)
                                                        -- Jerry :-)

@_date: 2014-02-14 19:58:04
@_author: Jerry Leichter 
@_subject: [Cryptography] RNG exploits are stealthy 
It's not just theory.  Mac OS actually plays this game, starting with Mavericks.    There's a quick overview at  - see the "timer coalescing" section.  Much more complete descriptions of the technology have appeared, but I don't have a handy link.
Mavericks uses a number of other strategies, at least some of which might similarly have unexpected effects on programs that think they know how scheduling and such are implemented.  The overall effect is significant, according to reports from people who see noticeably long battery life when running Mavericks - so expect to see these ideas implemented elsewhere.  As in my message about assuming that details of hardware implementation (and particularly side-effects of hardware implementation) will stay constant over time ... the same applies to OS implementation (and, more generally, to any software).                                                         -- Jerry

@_date: 2014-02-15 14:28:22
@_author: Jerry Leichter 
@_subject: [Cryptography] RAM memories as one source of entropy 
One might argue that the Burroughs "Algol" machines of the 1960's were way ahead on this.  (The lowest level documented interface was a simplified/extended for systems programming purposes form of Algol.)
                                                        -- Jerry

@_date: 2014-02-16 07:55:55
@_author: Jerry Leichter 
@_subject: [Cryptography] The ultimate random source 
See                                                         -- Jerry

@_date: 2014-02-16 08:00:13
@_author: Jerry Leichter 
@_subject: [Cryptography] The ultimate random source 
...however, as with the paper on the not-quite-randomness of coin tosses that John Denker forwarded yesterday, dice aren't quite random either:  Interestingly, the same factors come into play:  Initial position is important; air resistance is not; bouncing helps but not as much as you'd expect.
                                                        -- Jerry

@_date: 2014-02-17 14:56:54
@_author: Jerry Leichter 
@_subject: [Cryptography] The ultimate random source 
If you really want to go the route of physical "randomness", get a bunch of glitter, put it in a fluid, and use something to stir it.  Put a strong light on it from above, point a camera, and record only the timing and positions of the flashes of light when the light source is reflected into the lens.
You can certainly get chaotic behavior by correct choices of the parameters of the moving fluid.  One would have to do the math to be sure, but I suspect that there is an element of true randomness from Brownian motion.  (If there isn't with standard glitter, you could use smaller reflective particles and a microscope.)
An interesting (and probably very difficult) question is how many camera angles you can use simultaneously without there being too much correlation between them.
Not that I feel the need to go and do the computations, because I also don't feel the need to actually build such a thing.  But if you're driven to it - by all means!  Beyond providing random numbers, this could be really pretty to watch.
                                                        -- Jerry

@_date: 2014-02-18 06:56:39
@_author: Jerry Leichter 
@_subject: [Cryptography] The ultimate random source 
What do you think inspired my design?  :-)
Actually, it was a combination of two ideas:  The suggestion of using glitter nail polish as a tamper detector, as the exact pattern in which it dries is "random" - really just "highly complex and (apparently) impossible to replicate" - with a snow globe to make the pattern dynamic.
Using Denker's terminology, I would classify that as "squish":  You have no idea how to predict the results (because of the complexity of the system), but also no way to prove that *no one else* could predict the results.
My proposal, while not meant to be taken entirely seriously, *could* be realized in a way amenable to actual analysis.  It has a source of true physical randomness (Brownian motion), and is chaotic (thus magnifying any small true randomness into larger degrees of unpredictability).  That's just my unchecked assertion, but the physics is well understood and could be worked out if anyone cared.
Mind you, as a matter of practical cryptographic engineering, all kinds of physical generators are likely plenty unpredictable enough.  But if you're going to go to the trouble of building a physical generator, why not build one you actually understand?  Then, for one thing, you'll know what parameters to check to make sure someone hasn't snuck in and loaded the dice:  Modified the system to make it easy to predict.
                                                        -- Jerry

@_date: 2014-02-18 10:53:04
@_author: Jerry Leichter 
@_subject: [Cryptography] Are Tor hidden services really hidden? 
Going off on a side track, but are you seeing any effect from SafePlug (  This is a little box made by the guys who build PogoPlug, which lets you make your disks available over the Internet.  SafePlug acts as a proxy that sends your browser request through Tor.  If can also be configured to offer itself as a full Tor node.  Hard to beat at $49.
                                                        -- Jerry

@_date: 2014-02-21 06:45:57
@_author: Jerry Leichter 
@_subject: [Cryptography] RNG exploits are stealthy 
Different argument.  Your code is getting better *because you're improving it* by incorporating new sources.  Had you simply left the code alone to forever depend on the sources you started with, it might get better or worse, but silently, and without you ever knowing.
"The price of randomness is eternal vigilance".  :-)
                                                        -- Jerry

@_date: 2014-02-21 14:38:53
@_author: Jerry Leichter 
@_subject: [Cryptography] RNG exploits are stealthy 
So there *is* new code .... but you're not making me feel any better about it.  Assume I'm using your system.  Now, rather than relying on new code written by Peter Gutmann - who I have some reason to trust on such matters - I'm relying on new code written by someone I've never heard of, whose expertise is in drivers for various kinds of environmental sensors.
In fact, it's some collection of people, some of whom I probably would *not* trust.  Writing drivers for some environmental sensor, whose output will be mixed with output of other environmental sensors - if my job were to sabotage the RNG on a system, I can't think of a better place to be.  I can think of *all kinds* of "improvements" to, say, some common code that helps out in developing such drivers.  And the improvements *will* actually help in using the sensors as, well, sensors.  That they also happen, say, to induce much more correlation among apparently-independent sensors - well, that's just a minor side-effect.
                                                        -- Jerry

@_date: 2014-02-22 23:32:05
@_author: Jerry Leichter 
@_subject: [Cryptography] Entropy?  Who needs stinkin' entropy? 
Hey, this is state of the art encryption for your critical hardware.  It gets by without any entropy.  So the rest of us should just stop complaining about how hard it is and get with the program.
                                                        -- Jerry

@_date: 2014-02-28 15:13:09
@_author: Jerry Leichter 
@_subject: [Cryptography] GOTO Considered Harmful 
Apple's or OpenSSL's?  Apple has modified this stuff, but most of it is from OpenSSL.  (I don't know about this code specifically; nor do I know about any analogous code else in OpenSSL.)
One of many, many ways to do this.
The original code was more efficient as it didn't repeatedly test 'failed'.  Whether that makes any noticeable difference is something one can debate.  (Really, one should measure it.  I'll bet if you actually try, not with this particular code - which deals with a relatively small, fixed-length header, but with a loop over a long run of memory - that you would be able to measure the effect, but that it would be a fraction of a percent.  On the other hand, as AES calculations get faster - e.g., using the new AES acceleration instructions on recent chips - the effect of extra tests becomes more noticeable.  But this is just numbers; whether they matter in a real application is an entirely different question.)
In any case, this is exactly why the goto-style is used by kernel hackers and others who feel they must be concerned with every instruction.
This was a very unusual error, and the fact there was a 'goto' in the code was incidental.  A simple typo breaks yours, too.  Just accidentally change any '||=' to '='.  Or change the "if (!failed)" to "if (failed)".  Or change "err =" to "err ==".
Somehow the fact there happened to be goto's in this code has become a lightning rod for ressurrecting a 45-year-old (justified) rant about programming languages and programming styles that vanished decades ago.  (If you don't remember what the extended range of a DO loop was in FORTRAN; or if you think the proof that finding the shortest program representation on a VAX with different-sized branch instructions is NP-complete relies on code no one could ever possibly write; then you don't understand why Dijkstra wrote what he wrote.)
In fact, if you go back to what bothered Dijkstra in that article (the CACM version is behind a paywall, but a version from Dijkstra's files is at  you'll see that the particular style of "forward goto" used here would likely not have been a concern.  On the other hand, virtually everything about a functional programming style would be covered by that Dijkstra was getting at.
As far as I'm concerned, the best recommendation here, which avoids changing code that's worked correctly for years prior to this bug, is to add a sanity check:
  ...
  goto cleanup;
  assert(err != 0);  // Whatever you want here to die *in production code*!
  SSLFreeBuffer(...)
  ...
  return err;
(You can also flow through into the cleanup code, then jump back to it from the fail code.  Choose your poison.  Both code organizations were common back in assembler days.)
This turns the meaning of "fail" into something specific:  We go here when there's an error.  If we try to go here when there's no error, the code will die.  Then the code does what it says, and says what it does.  (In he original code, executing the "fail" statement doesn't mean there was a failure!)
                                                        -- Jerry

@_date: 2014-02-28 20:19:25
@_author: Jerry Leichter 
@_subject: [Cryptography] GOTO Considered Harmful 
Two things:
Actually, the *original code was indented correctly*!  The *buggy* code was not.
I fault the code review process:  The funny indentation should have caught the eye of a reviewer.  Once you're drawn to the break in the proper indentation, you really can't miss that there are two goto's in a row.  This bothers me; it suggests that Apple's code review process is just not up to snuff.
(It's been suggested that this smells like a git merge problem.  Most of us have probably become pretty cavalier about merges - if the SCCS doesn't complain, and the code compiles, we figure it's fine.  If this really was a merge error, it suggests that merges need to be carefully code reviewed, too.  I don't know of anyone who does that.)
It would be messy to fully test this code without constructing a rather unusual test harness.  The update function for an AES computation takes a current state and a block.  In raw form, there's no way for it to fail (short of accessing unmapped memory):  Any bit pattern is legal for both.  Many crypto API's add a bit of error checking by keeping the state in a structure that includes some kind of identification of the algorithm it's meant for.  So you could get an error if you passed in an improperly initialized, or deliberately damaged, context block.  But the context is allocated within the body of the function; it's not a parameter.  So there's no way, from the outside, to cause any of the calls to return a non-zero value.
There probably *should* be at least a pair of tests, one of which verifies successfully, one of which doesn't.  The failure (because it would succeed when it shouldn't) of the second wouldn't give any clear indication of where the problem was, but it should have lead someone to look closely.  The lack of such tests is not good.
BTW, I got curious and had a look at the latest (1.0.0l) sources.  The code base, excluding tests, has 6313 goto's in it; at a quick glance, most of them are used in the same pattern as the code in question.  On the other hand, I wasn't able to locate the code corresponding to Apple's code - it seems to have been branched so long ago that even most of the names are different.
                                                        -- Jerry

@_date: 2014-01-02 17:47:08
@_author: Jerry Leichter 
@_subject: [Cryptography] TAO, NSA crypto backdoor program 
In almost all cases, they give prices and discuss availability.  There's enough detail in the descriptions that one can make a reasonably educated guess whether the approach would work - and I saw no examples that set of *my* bullshit detector.  It all seemed reasonable enough.  (As an example, people are making a great deal out of the assertion that they claim a 100% success rate against iOS devices.  What's getting forgotten is the date:  2008.  That would be iOS 2.  iOS 3 wasn't released until mid-2009.  But in fact there were jailbreaks available, providing full access to the phone, for all versions of iOS 3 - and even much later.  Apple didn't start to get serious about keeping jailbreakers out until iOS 6.  (It's actually my guess that Apple did this deliberately, as a form of market research:  Had really innovative apps that worked only on jailbroken phones emerged, and had such phones become popular, Apple could have simply announced that they would provide greater access for legitimate developers.  But in fact jailbreaking remind the province of a very small group of people who were really clever at breaking into iOS, but showed little creativity in developing apps people cared about.  Eventually Apple decided to work at locking things down better.  People still break in, but it's gotten harder and Apple pushes out fixes much faster.)
Anyway, the grand goal - achieved repeatedly all the way through iOS 6.1.2 - is an untethered exploit.  The NSA claim for 2008 required physical access to the phone - at best a tethered exploit.  Easy stuff, in 2008.
(This says absolutely nothing about what they can do to iOS today.  I expect they can break in with little trouble - certainly with physical access, probably through a Web page or some other network-based hack.  But none of that is related to the plausibility of the reported 2008 attack.)
You have to separate the technical means from the political ends.  I have little doubt that TAO and associated organizations, which are concerned with technical means and measures, have been extremely successful at gaining access to pretty much everything, everywhere, all the time.  *They* aren't tasked with achieving the political ends of finishing of terrorists or whatever, so they shouldn't be measured on that basis.
BTW, this isn't an "I was just following orders" defense.  We can argue about whether creating these devices is illegal - I'd say certainly not - or immoral - to me, a gray area; espionage has been both condemned and attacked forever, often by the same people at almost the same time.  Producing tools of espionage is at least as gray.  The stuff in this catalogue at least is aimed at particular individuals, not bulk surveillance of entire populations.  (I would have much more serious qualms about those who built the mass surveillance frameworks.)
                                                        -- Jerry

@_date: 2014-01-03 08:50:36
@_author: Jerry Leichter 
@_subject: [Cryptography] nuclear arming codes 
Use of public key cryptography for this kind of thing was in common public discussion when RSA first gained broad use.  Was it just an obvious application, given the confluence of the invention of public key and the wide discussions surrounding the negotiations on test bans at the time, or did something of the secret work actually slip out?  A topic for some future historian's dissertation, perhaps.
Meanwhile, a forward-looking question:  Given what we know today - about cryptography in general, and NSA's infiltration of pretty much anything having to *do* with cryptographic implementations in particular - would it be possible to have an agreement such as the old test-ban treaty whose verification relies on cryptography?  The 1980's ideas about public key looked naive even prior to Snodownia, but at least one could argue that it was *possible* to get the required level of assurance for both parties.  Is that even conceivable today?
                                                        -- Jerry

@_date: 2014-01-04 11:06:54
@_author: Jerry Leichter 
@_subject: [Cryptography] defaults, black boxes, APIs, 
I would contend the dynamic, monthly update model is a sign of failure, not success.  For it to be a success, it would have to be putting itself out of business - i.e., the quantity and severity of problems would be going down over time, aiming for complete cessation in some visible future.  In fact, there is no evidence I've seen that this is happening.  Most likely, the *opposite* is happening:  One of the reasons we've gone to monthly updates is that the volume of individual updates was so large that people couldn't keep up.  And then we went to automatically, silently installed updates because people couldn't even keep up with the monthly updates.
And as a side effect, we've expanded the attack surface.  You could, in the past, design systems with no privileged network-facing components.  In a world of automatic monthly updates, the update mechanism is itself a prime means of attack:  By its design and nature, it's *supposed* to be able to make arbitrary modifications to the system, while being accessible to the network. Just step back and think about what we're saying:  We have a system we *know* is vulnerable to some kind of attack, else why would we be patching it.  And yet we completely trust the update mechanism to make things better, not worse.  Does that make a lot of sense?
I contend the only way to a secure future is to consider Tony Hoare's wonderful comment:  You can either make your system so simple it obviously has no bugs, or so complex it has no obvious bugs.  Most of our systems - certainly any system we have that has anything to do with the Web - have gone in the opposite direction.  And we've done this both at the individual element level, and every level above.  Everything depends on everything else; everything has to be trusted, in ways that we don't even understand.  We call the results "sophisticated"; a better word would be "incomprehensible".  Every successful attack simply shines the light on another piece of the mess that the attackers understood better than the designers.
As an example of what could be an alternative approach ... our moderator (hi, Perry, haven't seen you saying anything in quite a while) started off this discussion looking at secure email.  I would contend that we could solve one small piece of this puzzle:  We could build a secure IMAP server.  The IMAP API is complicated, but fairly well understood.  I believe we could write a security specification for that API - and we could build a sealed box that implements that API.  We'd have to figure out how to add and remove users, but that, too, could be a very simple API.  And that would be it:  The box would have no other exposed interfaces.  You could tailor its TCP implementation to inherently not support any other incoming ports.  It wouldn't need UDP - well, maybe for DNS - so don't include it.  Leave out everything but exactly what you need to support the external service offered.  To a large degree, this can even be automated - it's constant folding writ large.  Not in particular that providing a way to patch this thing should be the *last* thing you consider adding.
Could this be done?  I suspect so, though someone would actually have to set out to do it; they we'd know.  Is there a market for such a device?  Not the point - this would be a proof of technical capabilities, nothing else.  Could the underlying hardware be spiked?  Sure.  The best you would be able to say about the box is that it's as secure today as the day it was installed.
Would this solve any significant piece of the Internet security puzzle?  In and of itself, no.  It's a tiny piece of the puzzle.  But what we're doing now not only isn't working - if we look ahead in the direction we're moving, "working" isn't visible out there.
                                                        -- Jerry

@_date: 2014-01-05 17:59:54
@_author: Jerry Leichter 
@_subject: [Cryptography] defaults, black boxes, APIs, 
I'm with you.  For what it's worth, I think Chrome is probably, across time, the most secure, because Google puts a huge amount of effort involving a really experienced team into making it so.  I place some amount of trust in Safari, but that's a matter of statistics, not anything special about the code:  People aren't attacking it as much.  (Apple seems to have been getting ever more serious, but how far they've come is hard to judge.)
But a modern browser is the ultimate example of immense complexity in commonly used software today.  What people like in browsers is their universality:  They get judged on their ability to accept any trash that comes from a website, in any bizarre file format anyone's invented over they years, and "work".  I just don't see how they can possibly be made secure.
I do find fascinating the reaction to the never-ending series of security issues in Flash and Java.  What people have learned from this is:  Plugins are bad; Flash itself is bad.  They looked to the promised land of HTML5 as a way to do Flash-like stuff without a plug-in and without Flash.  But just what reason does anyone have to think that HTML5 implementations will be any better than Flash?  If anything, HTML5 is even more complex than Flash ever was.  And the demands for high performance, even while dealing with an area that's immensely complicated from top to bottom - from the inputs to the graphics devices at the other end, plus the complex input models, are there for HTML5, just as they were for Flash, and will tend to make secure coding an immense challenge.
docx - maybe.  There's no hope for the old .doc files.
As far as I know, no one other than Microsoft has ever managed to write a Word document processor that works 100% of the time.  There are some that work most of the time, but Word has so many odd corners - none of them documented in any reasonable way; as I recall, the European courts made Microsoft publish documentation, but Microsoft managed to make them so obfuscated that they didn't help - that it's proven impossible.  If you can't even get the *intended* functionality right, getting the security right is rather challenging.
I think we have the makings of an excellent context here:  Pick one of these - PDF is probably the best choice - and ask for a secure implementation.  The implementor may omit parts of the spec if he doesn't believe they can be implemented securely.  Points are charged for such an omission only if someone else in the contest manages to produce a secure implementation of the same features.
Code to be judged in a way that's similar to the contest that chose AES:  Public review by experts in the field, by other contestants, and by anyone else who chooses to comment.
The AES contest drew people in because there was a chance for an academic or other expert to have his work become a national standard.  I don't think that would work here - you'd need someone to kick in serious money to offer, both to the chosen developer, and to anyone who came up with worthwhile attacks.
The real purpose of the contest would be to examine, and push, the state of the art in secure code development.  If done correctly, this could teach us a great deal about how to go about this.
What I find most disturbing is that many of these bugs are trivial to avoid using techniques that have been known forever.
Many, many years ago, I took over an internal hack project that someone at DEC had written.  (For those who remember, it as a LAT server that ran on VMS:  It played the role of a LAT terminal server, connected to other systems using LAT rather than CTERM.  On a LAN, LAT did much better than CTERM.)  The guy who wrote the original piece of code wrote it in a style that is all too familiar to anyone who's looked at most commercial code today:  Just assume everything you receive is valid because checking takes too long in programmer time and is "too inefficient".  The result was that the program, when it received Ethernet frames that didn't quite match what it was expecting, would crash in various horrible ways.  For reasons no one could ever really explain, such frames were surprisingly common.
I set out to eliminate the crashes.  It turned out that almost all of them came down to one root cause:  The LAT protocol was defined in recursive components and subcomponents, each of them encode in TLV (Type Length Value) format.  They were parsed and handled in what you might, in a language parser, call recursive-descent style:  You used the T field of a subcomponent to select the right function; it pulled off the leading L field and dealt with the subcomponent and returned a pointer to where the next T field should start.  Unfortunately, damaged components often had garbage length fields.  (Well, the other fields might be garbage, too, but they didn't cause as much *immediate* havoc.)  Given that this was C, with no array bounds or other memory object checking, the result was that a subcomponent parser would happily walk of the end of the buffer it was handed based on the bogus length field.  The simple fix:  Walk the path of the data, from where it was pulled off the wire, through ever subcomponent parser, making sure that each function received a pointer to the end of the *containing* component, and have it check that the subcomponent didn't go beyond the containing component.  A boring couple of days of code cleanup - but, miraculously, the crashes ... stopped happening.  Who would have thought....  :-)
This would have been, oh, 1986, give or take.  But somehow C programmers at larger never learned - or were even taught - the lesson.  The only thing that's gotten us away from the never-ending stream of bad C code that scribbles memory is the fading of C from most commercial products:  C++ can be somewhat resistant (if you use the built-in types, *carefully*), and the newer languages all check array and string bounds.
                                                        -- Jerry

@_date: 2014-01-06 07:46:37
@_author: Jerry Leichter 
@_subject: [Cryptography] defaults, black boxes, APIs, 
A tale for the newbies:  Algol 68 had an extensible type system which implied and extensible syntax.  The Algol 68 report describe the language using W-grammers (W for their developer, Adriaan van Wijngaarden).  W-grammars were intended to provide just enough of a step beyond context-free grammars to represent the context sensitivity implied by Algol 68's type system.
It was a great embarrassment to all involved when it turned out that W-grammars were, in fact, universal.  As was pointed out at the time, this implied that you could not solve the termination problem for an Algol 68 compiler, so if it ran for a very long time, you couldn't tell if it had a bug or just really needed the time.
(I learned about this many years ago, probably from Alan Perlis.  One thing I hadn't heard - I found it just now in the Wikipedia article about W-grammars - is that Don Knuth was working on attribute grammars at about the same time, for similar purposes, and that there are some underlying similarities in the approaches.  Knuth apparently spoke to the Algol 68 committee about this.  Attribute grammars survived, while W-grammars faded away.  But most compilers to this day use ad hoc semantic techniques - i.e., reference to a symbol table - to handle the non-context free aspects of their languages during parsing - for example, the type names that typedef produces in C.)
Years later, we've learned to embrace the inner universality in our languages.  For example, the C++ template language is universal.  (This kind of thing is most often shown these days by implementing the lambda calculus, which tends to be straightforward in anything that has recursion plus macro-like substitution.  TeX is easily seen to be universal through that mechanism - in fact, there are implementations of the lambda calculus that are used to build useful TeX macros.)  We seem to have forgotten about the downsides.
But ... bringing this back to security:  There's some recent work that makes the point that our network protocols are too complex for us to prove much about them - but they formalize this by looking at it from the view of language complexity theory.  Nice work that I need to find some more time to read more about - see   (I *may* have found this a while back through a message on this list.  If you posted it previously - thanks.)
                                                        -- Jerry

@_date: 2014-01-07 06:52:16
@_author: Jerry Leichter 
@_subject: [Cryptography] defaults, black boxes, APIs, 
The idea (and even commercial implementations) goes back way further than that.  Alan Feuer did this, but someone sold a product called Safe C back in the early 1980's.  (I was offered a part-time gig helping with the development, but it didn't make sense with my graduate studies at the time.)  As best I can recall, Safe C was a pre-processor that modified all arrays and pointers and C operations in them to enforce bounds checks.  As I recall, it was considered too inefficient to be practical for most operational code, but you could use it during testing.  With more aggressive work to minimize and optimize the actual inserted, runtime overhead should be minimal.  (The cost of using "fat pointers" in place of simple memory pointers is harder to deal with invisibly.)
I think the name "Safe C" was, many years later, repurposed for an entirely separate effort to define a subset of C that could be used in secure real time applications, or something like that.  If you run across this - it's unrelated.
A "secure C" wouldn't make for a good contest because, as John says, we already know it can be done, and in fact it *has* been done, repeatedly.  The issue now is how to get someone to actually use the tools.  Safe C died; the tools John mentioned are barely alive, if at all.  We regularly sacrifice security on the alter of efficiency - see the C++ STL and its approach to iterators.  (I wrote about this years ago:  A contest, to be worth holding, has to:
a)  Challenge the state of the art by doing something that while clearly useful is commonly believed to be impractical;
b)  Actually produce a practical, useful result.
"C without buffer overflows" meets neither criterion - the first because it's been done, the second because the market has clearly shown that it won't take up the result.
                                                        -- Jerry

@_date: 2014-01-07 07:09:18
@_author: Jerry Leichter 
@_subject: [Cryptography] defaults, black boxes, APIs, 
gets() is a total loss.
sprintf() can actually be used safely - but it's much, much harder than people realize.  I used sprintf() to implement a collection of C++ functions to format the built-in types.  All the power of sprintf() was there, but in a controlled, type-safe fashion - e.g., length and precision were passed as integer arguments, and were range-checked.
An interesting question to ask in this context:  In the call sprintf(buf, "%f", x) - how large does buf have to be to guarantee there is no buffer overflow.  (I no longer remember the exact number, but its something like 350!  Almost all the digits printed are noise, of course, but printf doesn't care.  Obvious once you think about it through - but people rarely do).
Nevertheless, since snprintf() is universally available these days (it wasn't when I wrote that C++ code) there's no excuse for sprintf().
                                                        -- Jerry

@_date: 2014-01-07 19:58:23
@_author: Jerry Leichter 
@_subject: [Cryptography] defaults, black boxes, APIs, 
Urk.  A miss-edited splice.  What I was trying to say what the Alan Feuer actually developed and sold Safe C in the early 1980's.
                                                        -- Jerry

@_date: 2014-01-08 07:15:14
@_author: Jerry Leichter 
@_subject: [Cryptography] defaults, black boxes, APIs, 
On the general issue of Java "escapes from the sandbox":  When Java fielded the notion that you could let someone run arbitrary programs in a general purpose language safely on your machine because the compiler and run time system would ensure that they did nothing untoward, they were replaying a theme that's surfaced in the computer science world repeatedly over the years.  Burroughs actually built a series of machines, starting, I believe in the late 1950's, which "had no assembly language":  It was Algol all the way down.  The hardware was designed to enforce Algol rules.  Only approved compilers could mark files as executable, and they "correct by design" and could never emit code that violated the rules.
It was a great idea, completely secure - until it wasn't, when someone realized that you could take a backup tape written on one of these machines to a different kind of machine, muck with the contents, then restore it with unsafe code marked as "certified safe".
Knowing this history - and the history of other, similar failures - I was skeptical when I saw these claims.  Sun and the Java designers, to their credit, put a good security team together and designed a sane, reasonably small security model that one just might be able to get right and *keep* right.  But it was not to be.  Java has joined the ranks of many such "absolutely secure" environments that are "absolutely secure" - except when they aren't, and need yet another patch.
This is a dream that won't die.  In the Web world, we tell people to turn off Java - but Javascript is pervasive.  (I ran my browsers with Javascript disabled for years, but it eventually became untenable.)  Javascript has a reasonable track record for safety, though how much of that is limitations on what it can do, how much is at the cost of performance, and how much is a laxer view of what security properties Javascript is supposed to enforce anyway, is hard to disentangle.  Google's NACL is an attempt to do what Java did with actual raw code.  We'll see how that works out.
We seem to be learning.  We've been trying for 60-odd years, after all!  There's some good theory now (e.g., proof-carrying code) though how much actual application that theory has had, I don't know - NACL seems to have picked up from themes from PCC, but I think the actual implementation is entirely different.  Still, the long history of failures - often, as in the case of Java, after apparent initial success - should make one extremely cautious.
BTW, I find the distinction between the continuing failure of software-based security and the apparent success of hardware-enforced security quite fascinating.  There were some very early failures in hardware-enforced privileged separation - MIT's old Project Mac had the classic examples - but I haven't heard of problems of this sort in many years.  The Project Mac failures were clearly the result of too much complexity - hardware support of complex segment-based memory with complicated security semantics - and for many years we kept things very simple - a privileged and a non-privileged mode, with a simple division of resources into accessible everywhere or only from privileged mode.  Virtual memory was more complex, but still fairly straightforward, with a central "reference monitor" in the page tables.  In general, we kept things fairly simple, and they worked.
Looked at from this point of view, the explosion in complexity in modern chips - virtual machine support, System Management Mode, "secure enclaves", etc. - should be worrying.  System Management Mode is well known to have some very problematic aspects - though so far as I know, they are all in the way it gets used, rather than in the hardware implementations of the primitives.  But should we have any confidence that it will stay that way?  Yes, the hardware guys have gotten really good at making sure their hardware actually does what's specified - the Intel divide bug probably could not be repeated today. But all those techniques are aimed at errors "in natural conditions", not under active attack from an intelligent opponent.
                                                        -- Jerry

@_date: 2014-01-09 15:38:13
@_author: Jerry Leichter 
@_subject: [Cryptography] On threat models and progress 
If you look at any of the classic texts on computer security from 20+ years ago - sometimes even more recently - you'll find some interesting assumptions about threat models.  They're "interesting" because they seems so obvious at the time - but none are valid any more.
1.  Once an opponent gets physical access to the machine, all bets are off.  But machines with valuable information in them live in secure data centers, and we have thousands of years of experience in protecting physical assets.  So there's no point worrying about attacks based on physical access.
Of course, today valuable data is on machines people carry with them - and as the recent information about NSA techniques shows, even the machines that go into data centers may not be secure because they were sabotaged before they ever got there.
2.  Denial of service attacks are impractical to defend against, but they don't matter because no attacker would have much reason to carry them out (with an exception perhaps made even back then in military settings, though the military wasn't nearly as vulnerable then), and besides they are expensive and difficult to organize.  Along the way, we discovered such motives - whether simple playing around, or to make a political point, or as a means of extortion.  We also learned that DoS is actually quite easy to carry out in a networked world full of botnets, at levels far beyond what anyone could have imagined back then.  On the other hand, when the need arose, it turned out we actually *could* defend against them.
3.  There's no way to close timing channels, but you can arbitrarily reduce their rate to the point where they don't matter.  Everything about this statement remains true, except for those crucial words "to the point where they don't matter".  We still have no way to completely close such channels, but we can get them down to very small data rates.  Unfortunately, "to the point where they don't matter" was based on a model where the timing channel was being used to exfiltrate data from a secure data center.  If you can only exfiltrate a bit per second, it's difficult to get very much useful information from a terabyte database out.  (Oh, there were always special cases - getting the name of a spy out might only take a minute once you've found it in the database.)  Today, however, the database is likely backed up somewhere else, in encrypted form - and thus is "safe".  Except that now the key can be exfiltrated in a couple of minutes through that very slow timing channel.
                                                        -- Jerry

@_date: 2014-01-14 14:38:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
None of the above.
The RSA Conference has always been two things:  A technical conference on crypto, and a place to sell crypto wares.  At most, a boycott will kill the first.  The guys buying and selling won't care much.  They aren't the ones going to the technical talks.
It amazes me as I read this how little people even know about what RSA does.  It hasn't been a *cryptography* supplier in many years.  It's "the security division of EMC".  RSA sells security consulting - internally and externally. (  It sells fraud protetion.  It sells compliance mechanisms.  It's big on data leak prevention, risk management.  (
BSAFE?  A big deal when there were still patent issues - what you were buying was protection from lawsuits (and a reasonable implementation).  Today, what you're buying is FIPS and such approval so that you can sell the governments, or clients that sell to governments, and need the stamp.  But it's a now so minor that it's hard to find on the web pages.  (Many of these are bullshit products that consultants sell to C-level execs.  But that only makes them all the less vulnerable to a pissing contest among "the little people" who do all that silly tech stuff.)
SecureID is still a nice business, but if you want to hurt RSA there, you have to provide a market alternative.  There already *are* a few market alternatives, but none has hurt SecureID.  RSA's issues with SecureID (there have been a couple of significant ones over the years) haven't hurt them much either.
If you want to make a point by boycotting the conference, go ahead and do so.  Personally I think you're aiming your complaint in the wrong direction, but I understand the feelings involved and certainly won't try to convince you otherwise.
But don't kid yourself into thinking that a boycott of the technical sessions at the RSA Conference will do much of anything to RSA.  They're already in a position of having to re-establish they position, especially outside the US - and a boycott will be a tiny little disturbance in a large amount of pre-existing noise.
                                                        -- Jerry

@_date: 2014-01-18 14:54:31
@_author: Jerry Leichter 
@_subject: [Cryptography] "On Cryptographic Assumptions" 
Very nice essay by Oded Goldreich:
                                                        -- Jerry

@_date: 2014-01-18 15:07:12
@_author: Jerry Leichter 
@_subject: [Cryptography] HSM's 
I'd look at this differently:  Is there a construction that preserves the good properties of HSM's (potential for a very small attack surface) without the bad ones (you either have to trust a sealed box that someone else built, or be willing to create it yourself from scratch)?  If you look at this as analogous to network routing - where there is great utility in using the large number of "black box" routers out there to communicate, even if you don't trust any of them - then something akin to the construction of a mix suggests itself.  That is, could you define a standard interface to an HSM with the property that it's "securely composable":  You can combine a bunch of HSM's and get something with the same interface, but such that as long as at least one of the HSM's lives up to its security properties, the whole ensemble does?  Can you, under some assumption like "each box may be separately cheating, but they don't cooperate" get something even stronger?
It seems to me that such a thing should be possible, but it would take some work to actually formalize (a) the relevant security properties; (b) the HSM interface; and only then (c) the proof that what you have is, indeed, securely composable.
                                                        -- Jerry

@_date: 2014-01-20 11:45:49
@_author: Jerry Leichter 
@_subject: [Cryptography] cheap sources of entropy 
Getting quality random bits when you have (a) almost any kind of high-rate real-world sensor and (b) a human being willing to help is an easy problem.  Any modern cellphone can provide tons of randomness if a person moves it around, talks to it, waves his hands around in front of it.  (The bigger challenge is making sure the software in the cellphone doesn't either strip away or "leak" all that fine randomness.)
                                                        -- Jerry

@_date: 2014-01-20 14:39:03
@_author: Jerry Leichter 
@_subject: [Cryptography] RSA is dead. 
Absolutely.  And it's not just a matter of living inside the government bubble.
NSA has had a surprisingly good reputation pretty much until Snodownia.  Before their involvement with DES, no one really knew anything about them - but every interaction I've ever heard of with NSA people left the impression that they were extremely bright and extremely competent.  (A friend who, many years ago interviewed with both CIA and NSA, thought the interviewers for the former were a bunch of bumbling idiots, while he was very impressed with the latter.  He never took a government job, however.)
NSA managed to appear not to be much involved in the old crypto wars.  Sure, everyone knew that they were the ones who wanted to be able to keep decrypting stuff, but they managed to come across as mere implementers of policies set elsewhere.  Their involvement with DES looked bad for a while - why *those* S boxes?  Why 56 bits? - but then differential cryptanalysis was re-discovered in public and it turned out that NSA had actually specified S-boxes as strong against it as possible - and that the real strength really was around 56 bits.  NSA came out as being ahead of the rest of the world, and using their lead to strengthen publicly available crypto.
This is one reason I find all the whining about the NSA/RSA business a bit of revisionist history.  You can't look at what RSA did in the light of what we know today.  You have to look at it based on what was known or reasonably strongly suspected at the time.  Certainly at the time DUAL EC DRBG was added to the NIST standards, and RSA added it to BSAFE, NSA was accepted in the role of "helper".  The demonstration that it *could* have a trap door didn't show it *did* have a trap door - and after all NSA was fulfilling its role of helping to improve the security of American communications, no?  (Well, that *was and is*  one of its legally-defined roles, and that was the one we all saw, repeatedly, in public.)
Absolutely.  Whoever thought this was a good idea should have been shown the door a *long* time ago.  It took incredible arrogance to think this kind of thing could be kept secret - and in fact the suspicions were raised a long time ago.  It was only an aggressive "good cop" campaign - and a great deal of luck, e.g., the long history of suspicion that NSA had planted back doors in the DES S-boxes that we now know was nonsense, thus making claims that they planted back doors elsewhere seem like tinfoil-hat stuff - that let it last as long as it did.
In the end, one wonders just how much they actually gained anyway.  What significant NSA targets ever used BSAFE and DUAL EC DRBG?  I'd guess relatively few.  Terrorist organizations use home-brew or open source stuff - they don't spend money on crypto libraries.  (If NSA had managed to subvert the Linux RNG, they'd have had something.)  The larger governments have their own crypto organizations.  Maybe this helped them with some smaller governments and some (likely mainly American) large corporations.  Hardly seems worth in light of what they've now lost.
                                                        -- Jerry

@_date: 2014-01-20 15:55:43
@_author: Jerry Leichter 
@_subject: [Cryptography] cheap sources of entropy 
I have no problem with Turbid and with the general notion of getting randomness from good physical sources.  But you missed the point of my message entirely.
We keep coming back to discussions of randomness generators in general, and being side-tracked by focusing on generators in "easy" situations.  Situations where you can add hardware and software to do something like Turbid are easy.  (It's not that about whether *Turbid* is easy, it's about what you can do given that you already have it.)  Situations like the one I mentioned above are also easy.
Perhaps it would be better to get away from "randomness" and talk about "unpredictability".  Yes, I consider the inputs from a bunch of sensors in a cell phone being swung around by a human being to be unpredictable.  Even to someone who has no access to the cellphone but does have several high-quality sight and sound recordings of the event.  All the processes in play are noisy and have large chaotic components.  A sound field in a complex natural environment, if looked at to high precision, is extremely variable from place to place and from moment to moment.  Sure, you can get the general field pretty precisely - but knowing whether the bottom bit of output from a sensitive A/D converter is set at any particular sampling time (which you don't know exactly either) - no.  Human motion is governed by the firing of multiple nerve cells triggering multiple muscle cells, subject to multiple layers of neural control, all interacting in complicated ways with feedback from sensors of internal and external state.  Even the most practiced of movements vary in unpredictable ways when repeated and measured to the kind of accuracy that you can easily get from cell phone sensors.  And you know ... some of the variation in neural behavior is due to exactly the kind of quantum noise down at the synaptic level that you're basing Turbid on.  A real neuron isn't the same as the neurons we use for nice CS examples - it's much more complicated.
If I had a *choice* between a carefully implemented physical circuit based on shot noise or some similar well-understood source of "core randomness" or something fairly ad hoc based on sensors and human interaction, *of course* I would choose the former.  But that may not be available.  Still, the latter isn't bad, even if the actual randomness available can't be as easily quantified.
Very few things in the real world are subject to proof in any mathematical sense.  No one can compute the strength of steel beams from first principles; we measure the strengths of materials in particular quantities, configurations, at particular temperatures and other real word conditions, make tables, and go from there.  Even then, exact computation of the dynamic forces - often even the static forces - present on the beams in a building are way beyond our capabilities.  We make approximations, and we add a safety factor just in case.  Still - buildings and bridges and such stay up and behave as predicted, for the most part.
I contend that I can build a system of the sort I described and have sufficient confidence in the unpredictability of the values, and of their range ("entropy", if you like - I find that word so often misused in cryptographic discussions that I try to avoid it), that I'd have no qualms using it as a source of "randomness" for pretty much any purpose.
Would I *prefer* to have a Turbid-style generator to mix the values up with?Absolutely, but I can live without it if I have to.  (Why "mix it up"?  Because you can prove all you like about the *design*, but you can prove absolutely nothing about the *physical artifact* I have sitting on the desk in front of me. That thing over there is *supposed* to be just a 47K resistor; and that's *supposed* to be a plain USB connector.  But can you know for sure?)
                                                        -- Jerry

@_date: 2014-01-21 17:47:57
@_author: Jerry Leichter 
@_subject: [Cryptography] Does PGP use sign-then-encrypt or 
Verifying a signature is a fairly expensive operation, and one wonders if it, too, is subject to some kind of attack.
Perhaps the right solution is to do a MAC last - whether you do the signature or the encryption first.  A MAC is cheap to compute, cheap to check, and simple enough that you have some hope of being sure it won't information.
Or you can use a combined encryption and authentication mode.  I would think that you then want to do Sign-Then-Encrypt&Authenticate, as the outer authentication protects the inner signature from attack - but such reasoning has proved tricky and wrong in the past.
                                                        -- Jerry

@_date: 2014-01-21 18:17:03
@_author: Jerry Leichter 
@_subject: [Cryptography] Does PGP use sign-then-encrypt or 
MAC's and digital signature systems are different in a more fundamental way:  With a signature system, Bob can prove to anyone that a message was signed by Alice without himself being able to produce messages with Alice's signature on them.  With a MAC, Bob has everything needed to produce messages "MAC'ed" by Alice.  But that's fine, because the entire purpose of a MAC is for Bob to be able to prove *to himself* that Alice produced a message.  There's not much point in him forging a message and then proving to himself that he forged it!
While this certainly has a flavor similar to the symmetric/asymmetric system distinction, it's not quite the same thing.  DSA does signatures, but doesn't in and of itself provide an asymmetric encryption system.  And while it's much less convenient and requires a trusted third party, you can construct a signature-like system using only symmetric primitives:  The trusted third party holds the actual MAC key and will apply it for message creation only for Alice, but for anyone for message verification.  (Alice's messages to the trusted third party are MAC'ed using a key known only to the two of them; the TTP can forge messages from Alice, but we assume that away because it's *trusted*.  Similarly the TTP shares a unique key with anyone who might want a signature verification done.  Bob still can't prove to anyone else that the message was from Alice - but he can point anyone at the TTP to do it for him.)
                                                        -- Jerry

@_date: 2014-01-21 22:18:21
@_author: Jerry Leichter 
@_subject: [Cryptography] RSA is dead. 
You're talking reality, I'm talking reputation.  I don't disagree with you that NSA, behind the scenes, was engaging in all sorts of shenanigans (to put it very mildly).  While that may have been known in some communities most directly affected, I can tell you - having *not* been part of those communities, but being somewhat involved in both the academic and crypto-*using* commercial communities at the time - that NSA's public relations efforts were quite successful.  For example, you mention the sad history of Photuris.  For most of us on the outside, this all just looked inexplicable - or perhaps a commercial power play.
NSA played the game very well.  Key escrow?  40-bit limitations?  The names that were visible were not NSA - they were politicians.  I actually had to work with NSA to get export approval on a product in the 40-bit days.  They were somewhat slow to respond - no surprise, they were after all a government bureaucracy - but what they had to say was reasonable, within the context of a policy they enforced.  (Approval for export moved from NSA to State while we were in the middle of the process, and we ended up losing interest anyway for our own internal reasons.  We didn't implement crypto in our product until many years later, by which time the process had turned into little more than filing some paperwork to which you never got a response - which meant automatic approval after 30 days.)
Look, I'm not defending NSA.  I'm telling you that I, and many other people I knew, and from what I could see most members of the broader community, were fooled by their charm offensive.  If you found little support in those years - this is why.  You were out-maneuvered by some of the best professionals in the business.
                                                        -- Jerry

@_date: 2014-01-23 09:31:27
@_author: Jerry Leichter 
@_subject: [Cryptography] Does PGP use sign-then-encrypt or 
Let's see if I understand the attack you're describing:  No one uses asymmetric crypto for bulk encryption for many very good reasons, so Alice and Bob share a symmetric key K that Alice uses to encrypt messages to Bob.  They also share a MAC key K' that Alice uses to compute MAC's on messages sent to Bob.  If Charles gets is hands on both K and K', yes, he can synthesize messages to Bob that Bob will accept as genuine.  (He can also intercept messages from Alice to Bob and read them, but he doesn't need to do that to send his own messages.)
If Alice instead signs here messages using an asymmetric system with private key P_A, Charles will need to get hold of K and P_A.  Granted, Charles can get both K and K' from Bob, but he can only get P_A from Alice.  But is this really enough of a difference to matter?  At least K' is per-session; if the per-session keys are chosen correctly, it will become worthless when the next session starts.  P_A, on the other hand, is a long-term secret.  If Alice loses it, she'd better hope that she (a) finds out quickly; (b) has access to an actual working key revocation system.
BTW, one might argue that using a combined encryption/authentication mode is less secure because then effectively K == K'.  But again, is it really all that different?  If you can't keep your session keys secure, you're dead.
                                                        -- Jerry

@_date: 2014-07-08 07:07:17
@_author: Jerry Leichter 
@_subject: [Cryptography] Unlearned lessons 
It appears that every new generation of products must relearn, from scratch, all the hard lessons of cryptography and security.  While we have a way to go, we're beginning to have some handle on the software and services that date back 10 years and more:  Data at rest, email, Web, and so on.  We're just beginning to see serious effort put into such things as process control software - not new in and of itself, but newly exposed to the Internet.
Meanwhile, the latest rage is The Internet Of Things - which is rapidly showing itself to be The Internet Of Insecure Things, all ready to compromise everything it connects to.  Case in point:   in which smart WiFi connected light bulbs used a vulnerable pre-shared (fixed?) AES key with the result that an attacker within 30 meters of one of the bulbs could grab the WiFi password.
Just one of many recent attacks.  (And of course we're already at the level of "connected" door locks, so this crosses back into physical security - previously a long-solved problem - as well.)
Back in the early days of Java applets, some wag came up with the phrase:  Give me full access to run code in your browser, and I'll give you pictures of jumping beans on the screen!  We need some similar smart phrase for these "smart" devices....
                                                        -- Jerry

@_date: 2014-07-11 05:44:23
@_author: Jerry Leichter 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
Beautiful analysis; thank you.
It reminds me of an experience I had many years ago.  I interviewed a woman applying for a job as a developer.  She had a number of years of experience working for a large military contractor, working on codes for missile control or something of that sort.  I was curious - and asked - what they did to make sure that such code was correct; after all, dropping a nuclear-tipped missile on the wrong city because of a bug could really ruin your day.  Her answer left me chilled:  "Oh, we don't have to worry about bugs.  All our developers have security clearances.  We can trust them completely."  (Or words to more or less that effect.)
                                                        -- Jerry

@_date: 2014-07-12 06:59:46
@_author: Jerry Leichter 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
It was, indeed, in the US.
I have no way of knowing how true this person's statements actually were, and if true, how broadly such attitudes were held and at what levels of the organization.  The fact is, one person who had worked in this field came away with this understanding of how to produce reliable software.  It was *that* that I found chilling.
But ... this wasn't a completely isolated incident.  A co-worker had previously spent a number of years working on nuclear bomb codes - the large FORTRAN (in those days, and perhaps even today) programs that simulated nuclear bomb blasts (and which would eventually allow the US to enter into the Nuclear Test Ban treaty).  Some of his experiences were ... peculiar.  I remember one story about a manager who had gone off to a seminar on best practices in programming.  There, he learned about the concept of "information hiding" - the term then for what we now usually call encapsulation.  He wanted to apply these ideas to his own organization.  Unfortunately, the FORTRAN of the day had no support whatsoever for such notions.  So he "implemented" the ideas by administrative fiat, making lists of people who were not allowed to talk to each other - nominally not about how their code worked, but as implemented not at all.
Again, these are individual anecdotes.  I also knew people - Paul Karger was one - who approached these issues with great care and diligence, and whose work I highly respected.
It's always a mixed bag.
(I probably wasn't using them at the time, but for many years the two initial questions in interviews I did were:  "How do you produce reliable software?" and "How do you produce maintainable software?" - with the candidate told to interpret the words in the questions in whatever way seemed most appropriate.  Most of the answers I got were ... disappointing.  But some, from candidates who usually proved to be very good in other ways, were excellent.)
                                                        -- Jerry

@_date: 2014-07-14 20:10:16
@_author: Jerry Leichter 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
Well, it's been over 30 years.  But I doubt it - it was not just a quick one-off question, we talked about it a bit.  And then I spoke to coworkers about it.
I suppose if you think the only vulnerabilities in your software are the ones deliberately introduced by attackers then this attitude may be reasonable to some degree.  But it's a peculiar point of view for any software professional to hold.
                                                        -- Jerry

@_date: 2014-07-15 20:07:43
@_author: Jerry Leichter 
@_subject: [Cryptography] multi-key encryption of "meta" data 
I agree with what you say, but want to inject one note of caution:  We've been down this road before, in defining XML security.  The financial industries needed to support all kinds of complex trust models and layers of authentication and encryption.  The end result some something its designers have pretty much disavowed.
The main message has to be:  Keep it as simple as possible.  There's tons of stuff that would be "nice to have" but that ultimately it's better to live without.  Does email really need its own security model?  Forget the data/metadata distinction entirely.  (The main reason it's maintained in the mail-related protocols is to support various server-side sorting and searching operations - most of which don't work well anyway.)  We have some data to deliver; we need a way to specify and resolve who to send it to.  We may want to protect against traffic analysis.  These are issues common to many transport protocols.  There's little reason - other than history - for mail-specific encryption.
                                                        -- Jerry

@_date: 2014-07-18 06:17:07
@_author: Jerry Leichter 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
What you are describing is another case of "attacking today's problem with yesterday's tools".  Yesterday's problem was the single bad actor who was out to steal data or modify systems to steal money.  The combination "bad actor competent enough to get and stay hired and not be readily detectable" was sufficiently rare that you didn't have to worry that you had two of them - and even if you did, the nature of what they were doing made it unlikely that they would cooperate.
Today's problem is concerted action by state intelligence agencies and, reportedly, organized crime.  The goals of the former include long-term data extrication and various forms of sabotage.  The goal of the later may include data extraction, but also large scale theft.  Certainly the former, and perhaps the latter, have access to large numbers of competent, motivated people who at least in the former case don't even see themselves as bad actors - they seem themselves as patriots.  There is no reason to assume you only have one of these guys on board.  Further, unlike the lone wolves of the past would might discover each other only by accident, and then would have no reason to trust each other, these attackers are coordinated and can work together.  Hell, given the competition for talent these days, every company makes a big deal of having its employees recommend friends.  Get one infiltrator on board, and you can be sure he'll recommend associates.
In the face of this kind of attack, code reviews lose their effectiveness.  Attackers will simply review each others' code.  Oh, sure, *most* of their development and review efforts will be directed toward company goals - but that little fix that opens a hole wide - but on its surface looks completely boring and unlikely to inspire others to have a look - is the one that will be reviewed by a co-conspiritor.
Theoretically, you can get around this by requiring more reviewers.  But in practice just getting a single competent review is usually a hard problem.  Spotting the planting of a security bug by someone who knows what they are doing is very difficult, often impossibly so without detailed knowledge of the code being reviewed.  In any large project, the number of people deeply familiar with any one piece of code will be small - often, there is just one person.  If that one person is the attacker, and he arranges for another attacker to review his attack code, the chance of anyone noticing is tiny.
I can't think of any reasonable defense against the "multiple attackers" problem - which makes me despair of ever getting the intelligence services out of our code bases.  The advantages are all theirs.
                                                        -- Jerry

@_date: 2014-07-19 17:38:35
@_author: Jerry Leichter 
@_subject: [Cryptography] hard to trust all those root CAs 
This is an "it depends" situation.
"Legitimate" (I'll come back to the quotes later) packet inspection is done by companies or other large organizations that provide both the computers (or other devices) and the network connectivity to people they employ.  They act as their own CA, setting up the computers they own to trust a cert that they deploy to the packet inspection device.  This is "legitimate" in the sense that the computers, the network, the CA and the packet inspection device are all owned by the same party, which trusts itself and its own certificate.  SSL is, from the owner's point of view, doing exactly what it's supposed to do.
Of course, from the point of view of those *using* the computers and other equipment, this may not look quite so legitimate.  In theory, those computers and networks are only supposed to be used for business-related activity - so for the owner to look at the messages is perfectly fine.  But we all know that this is a fiction:  Everyone uses employer-provided computers for personal stuff.  And sometimes the lines get very hard to draw anyway - consider someone dealing with their company-provided health insurance provider.  The issue of "legitimacy" here is then not about cryptography, but about legalities and appropriate social policy and expectations.  (Which for me come down on the "not legitimate" side almost all the time, though I see this as an issue that smart phones and 4G Internet connections have the potential to eliminate.)
There *have* been instances where these packet inspection devices have been given certs from "trusted" CA's.  Where these have become public knowledge, they've been universally condemned as "illegitimate" and withdrawn.  How many that *aren't* public knowledge is anyone's guess, of course.  I very strongly doubt the number is 0.
                                                        -- Jerry

@_date: 2014-07-19 17:41:44
@_author: Jerry Leichter 
@_subject: [Cryptography] Steganography and bringing encryption to a piece 
Heh.  Post that as a response to any of a variety of blogs and it would blend right in.  Granted, close readers might thing it was an example of either blog spam or ranting or trolling, but those are so common anyway that you'd be well hidden.  :-)
                                                        -- Jerry

@_date: 2014-07-19 18:07:50
@_author: Jerry Leichter 
@_subject: [Cryptography] hard to trust all those root CAs 
Better approaches are known and even implemented.
The biggest cause of insecurity in SSL is the insistence of various elements of the system that they must be allowed broad freedom of action and damn the consequences to the security of everyone else.  The reason there are so many trusted CA's is that we can't have some random browser maker deciding that a Chinese CA isn't trustworthy - that violates Chinese sovereignty.  (That a Chinese dissident might have very strong feelings on this matter is just too bad.)
In theory, we could have different "trusted CA list" distributions; but in practice, this is first hard to implement because the browser makers have made it complicated to change the lists.  Beyond that, it runs into the freedom of action reserved to those who run the big sites:  They can pick any CA they like.  Not only can they pick one, but they insist on the right to pick multiple CA's, or to switch CA's whenever they feel like it.
One "fix" for this is certificate pinning:  Providing a way for a site to announce via a secondary channel that it will always use a cert signed by a particular CA (or even a particular cert).  This gives away the site's freedom to switch things when it feels like it, but short-circuits the entire CA trust scheme.  Chrome does this - it has a built-in set of "pinned" certificates for Google sites, and I think a few others.  I don't know if any other browser has implemented this.  If the top 500 sites had their certificates pinned in all the widely used browsers, most potential MITM attacks would be blunted.  (Of course, all the users of MITM'ing packet inspectors would scream bloody murder.)
A broader technique is to cross-check certificates seen by different users at different times.  Certificates thus have an additional trust measure:  The longer they've been around, and the more people scattered around the world who've seen them, the more trustworthy they are.  MITM attacks have to be running around the world for long periods of time to pass this kind of validation.  (Or, of course, someone could attack the database.)  This approach is even stronger when sites monitor the public database and make sure no certs they don't own show for their own URL's.  The Perspectives project is one effort in this direction.
Of course, to *really* work, these techniques must be implemented in commonly used browsers in a way that's pretty much invisible to users.  As with pretty much everything in crypto, the human interface/usability aspects trump all the technical issues.  And there will be pushback - as there was, for example, from schools when Google went all-SSL for searches.
                                                        -- Jerry

@_date: 2014-07-21 06:32:49
@_author: Jerry Leichter 
@_subject: [Cryptography] The role of the IETF in security of the 
The words really are the problem.  While "trustworthy" is pretty unambiguous, "trusted" is widely used to meant two different things:  We've placed trust in it in the past (and continue to do so), for whatever reasons; or as a synonym for trustworthy.  The ambiguity is present even in English, and grows from the inherent difficulty of knowing whether trust is properly placed:  "He's a trusted friend" (i.e., he's trustworthy); "I was devastated when my trusted friend cheated me" (I guess he was never trustworthy to begin with).
In security lingo, we use "trusted system" as a noun phrase - one that was unlikely to arise in earlier discourse - with the *meaning* that the system is trustworthy.
Bruce Schneier has quoted a definition from some contact in the spook world:  A trusted system (or, presumably, person) is one that can break your security.  What's interesting about this definition is that it's like an operational definition in physics:  It completely removes elements about belief and certification and motivation and focuses solely on capability.  This is an essential aspect that we don't usually capture.
When normal English words fail to capture technical distinctions adequately, the typical response is to develop a technical vocabulary that *does* capture the distinctions.  Sometimes the technical vocabulary simply re-purposes common existing English words; sometimes it either makes up its own words, or uses obscure real words - or perhaps words from a different language.  The former leads to no end of problems for those who are not in the field - consider "work" or "energy" in physics.  The latter causes those not in the field to believe those in it are being deliberately obscurantist.  But for those actually in the field, once consensus is reached, either approach works fine.
The security field is one where precise definitions are *essential*.  Often, the hardest part in developing some particular secure property is pinning down precisely what the property *is*!  We haven't done that for the notions surrounding "trust", where, to summarize, we have at least three:
1.  A property of a sub-system a containing system assumes as part of its design process ("trusted");
2.  A property the sub-system *actually provides* ("trustworthy").
3.  A property of a sub-system which, if not attained, causes actual security problems in the containing system (spook definition of "trusted").
As far as I can see, none of these imply any of the others.  The distinction between 1 and 3 roughly parallels a distinction in software engineering between problems in the way code is written, and problems that can actually cause externally visible failures.  BTW, the software engineering community hasn't quite settled on distinct technical words for these either - bugs versus faults versus errors versus latent faults versus whatever.  To this day, careful papers will define these terms up front, since everyone uses them differently.
                                                        -- Jerry

@_date: 2014-07-21 06:39:29
@_author: Jerry Leichter 
@_subject: [Cryptography] [cryptography] Steganography and bringing 
A case of the real world being foreseen by science fiction:  See Battlestar Galactica (newer series, of course).
                                                        -- Jerry

@_date: 2014-07-22 18:07:20
@_author: Jerry Leichter 
@_subject: [Cryptography] hard to trust all those root CAs 
So who gets to sign certs in .com?  Or, more amusingly, in some of the new vanity domains like .expert?
If you take the point of view that if you want to be a CA for .foo, you should go out and get yourself an address in .foo - in short order, you'll have not 150 CA's but (almost 150) * (number of TLD's).  And the Domain Industrial Complex - all the registrars - will pocket a nice bit of change.
(There have been *practical* efforts to flag the same kind of thing.  I forget the name, but there was a plugin that would warn you of unexpected changes in location of the CA.  So if some US domain moved from one US CA to another - no big deal.  But if suddenly it moves to a Chinese domain - alert the user.)
                                                        -- Jerry

@_date: 2014-07-23 06:36:24
@_author: Jerry Leichter 
@_subject: [Cryptography] Quantum crypto in the popular media 
I believe you meant this one:  It's a cute article, but I see nothing really new here.  (What's new in the underlying work is the ability to use entangled electrons rather than photons, which is nice engineering with possible practical applications but hardly new physics or cryptography.)
And like too many articles of this ilk, it misunderstands - or at least fails to explain - just how complex the whole business is.  (For example, Einstein's complaint about "spooky action at a distance" hasn't so much been "put to bed" as moved aside, since what you really get is action without information transfer.  And the article as written implies that quantum teleportation is "at at least 10,000 times faster than the speed of light" - except that it isn't; you need a classical speed-of-light bound initial transfer first.)
But, hey, it *is* cool stuff.
                                                        -- Jerry

@_date: 2014-07-23 10:11:07
@_author: Jerry Leichter 
@_subject: [Cryptography] hard to trust all those root CAs 
No, it was one of those experiments that seemed to fade away.  Clever idea, but we need more than clever ideas - we need widely adopted, working implementations.
As I mentioned earlier, Chrome actually implements pinning.  See There's a bit more at which indicates that all Google properties use pinned certificates.  It's not hard to add your own pins (though of course that's at best a hobby for a tiny minority of users), and Google said they would consider adding others to its official list on request.  On the other hand, it also says that "User installed root CAs are given the authority to override pins" - so that corporate MITM proxies will continue to "work" (for the appropriate sense of "work") if the corporation installs its own root CA's.  Sigh.
Apparently ( "[T]he official Android Twitter client includes certificate pinning.".
However I find little evidence that pinning, or any related technology, is receiving widespread adoption beyond those special-case experiments.  (Apps are a great use case, since they know who they need to "call home" to.  But for all too many apps even using https rather than http seems to be too much trouble.)
                                                        -- Jerry

@_date: 2014-07-26 18:48:44
@_author: Jerry Leichter 
@_subject: [Cryptography] hard to trust all those root CAs 
The comment I heard on this from a very experienced, very tech-savvy lawyer was:  I would have loved to take this case back when I was a prosecutor (making it quite clear that he was sure he would win).
For an idea of the direction the law has gone on this general class of things:  Destruction of evidence has long been illegal and severely punished. At one time, it was assumed (more or less) that you were at risk once you were told you (well, a criminal act) were under investigation and reasonably knew that what you were destroying was evidence. Then the standard became broader - you needed to know little in the way of details. Today, we've got prosecutors claiming that if you knew you were destroying evidence of a criminal act, they can go after you. The courts are more or less agreeing.
The prosecutor's view will be:  *You* deliberately put yourself in a situation where to obey the law requires you to keep publishing. It's not the law's problem. You get to live with the consequences of your decisions.                                           -- Jerry

@_date: 2014-07-27 09:58:17
@_author: Jerry Leichter 
@_subject: [Cryptography] hard to trust all those root CAs 
My interpretation - the lawyer's comment was brief and without detail.
You'll often see complaints that "this law is inconsistent with the contracts I've made with my customers" or "this law makes it impossible for me to make money" or "it's impossible for my business to comply both with law A and law B".  When you get comments like this, it's usually with the implication "...therefor the law is invalid".  But in fact the *correct* implication is "...therefor I need to find another business to run."
That's not to say the law might not be invalid!  But there would have to be *other reasons* beyond "it puts me out of business".  After all, street corner heroin dealers - and clear fraudsters - could make the same arguments.
                                                        -- Jerry

@_date: 2014-07-31 11:46:00
@_author: Jerry Leichter 
@_subject: [Cryptography] You can't trust any of your hardware 
The full talk/paper don't seem to be available yet, but they (a) figured out how to write malware that attacks a system via something plugged into its USB port (no, it doesn't depend on AUTORUN); (b) flipped that around and figured out how to replace the firmware on a USB device from the host.  I wouldn't have thought (b) was possible - after all, how many firmware updates for USB devices have you ever seen? - but I guess it's handy at the end of manufacturing, and gets left open because ... who would ever think of attacking it?
On further reflection, though, I realized that the only thing new here is that they actually went and built a full-cycle virus.  All the rest was done a couple of years ago:  Apple published an update for its (USB) keyboards -  - and someone reverse-engineered it and figured out how to upload any code they liked - The fun never ends....
                                                        -- Jerry

@_date: 2014-06-01 05:50:44
@_author: Jerry Leichter 
@_subject: [Cryptography] DOJ Wants to Expand Authority to Break Into 
Nothing, of course, could go wrong:
"Software used by law enforcement organizations to intercept the communications of suspected criminals contains a litany of critical weaknesses, including an undocumented backdoor secured with a hardcoded password, security researchers said today...."
(I guess this is a way for the Feds to investigate state and local cops.)
                                                        -- Jerry

@_date: 2014-06-01 06:14:06
@_author: Jerry Leichter 
@_subject: [Cryptography] DOJ Wants to Expand Authority to Break Into 
It is worth pointing out that *legally* there's a world of difference between making use of a bug that's already there and forcing a 3rd party to insert a bug into code that goes to anyone other than the target.  What DOJ is asking for here *on the legal front* is more of a procedural matter than anything substantive:  They already have the ability to get a search warrant that lets them break into a particular computer; they are just currently limited by physical location (a judge can only grant a search warrant valid in his physical area of responsibility).
Whether they can force a 3rd party to cooperate is more complicated.  One case I can remember from a couple of years back:  The FBI got (I don't know if they *compelled*) GM to turn on the OnStar mike in a GM car, turning it into a listening device.  The evidence, after multiple appeals, was suppressed - but on very specialized grounds.  (Modifying the system this way disabled its normal functionality, so that if the car had been involved in an accident, it would have been unable to call for help.  That was considered unacceptable.)  The current law on all this can be found at Of course, the TLA's don't seem to much care about the legal niceties - or they claim that they have other authority under which they don't need no steenk'n search warrants, or pretty much anything at all beyond an internal decision that the case involves "terrorism".  In terms of specific effect on such activities, this change is irrelevant.  (It's more a matter of setting a tone that justifies ever broader and more intrusive measures.)
A more interesting and disturbing example of legal changes - or rather non-changes, so far - is the recent failure to change the ECPA after objections from the SEC and IRS that getting search warrants for email would be "too hard" for them and would interfere with their investigations.  Well ... yes, that kind of *is* the point.
                                                        -- Jerry

@_date: 2014-06-01 06:22:09
@_author: Jerry Leichter 
@_subject: [Cryptography] FW: RFC 7253 on The OCB Authenticated-Encryption 
Text from the Draft RFC:  "It is crucial that, as one encrypts, one does not repeat a nonce.  The inadvertent reuse of the same nonce by two invocations of the OCB encryption operation, with the same key, but with distinct plaintext values, undermines the confidentiality of the plaintexts protected in those two invocations and undermines all of the authenticity and integrity protection provided by that key."
                                                        -- Jerry

@_date: 2014-06-02 15:53:25
@_author: Jerry Leichter 
@_subject: [Cryptography] What is going on with TrueCrypt? 
I had a similar thought.
What it really comes down to is the goal of such an effort.  If what you want is a hacker's toy, there's probably nothing anyone could do to stop it.  The record and movie companies have been trying to stop unauthorized copying for years to little effect - and they have huge amounts of capital, both monetary and political, to throw at the problem.
On the other hand, no corporation large enough to hire a lawyer would touch the resulting code.  Too many unknowns; too much uncertainty.  Bad for business.  If you succeed, you're just creating a pot of money worth suing over.
The situation is analogous to RSA back in the days when the patents on it were still live.  Eventually, the patents ran out and RSA became a tool everyone uses.  But copyright, unlike patents, for all practical purposes never runs out.
                                                        -- Jerry

@_date: 2014-06-03 07:27:48
@_author: Jerry Leichter 
@_subject: [Cryptography] Interesting new article by Ross Anderson... 
on surveillance, privacy, network effects, and whole bunch of related stuff:
I think the paper needs some tightening - some of the examples may be a bit far-fetched, or maybe valid but in need of more justification - but overall it's of a piece with Anderson's classic work on how economics makes it very hard to justify "obvious" security measures.
                                                        -- Jerry

@_date: 2014-06-03 07:29:54
@_author: Jerry Leichter 
@_subject: [Cryptography] To what is Anderson referring here? 
The following text appears in the Ross Anderson paper I just forwarded the link to:  "A security-economics example is the thicket of conflicting patent claims on authentication protocols, one of the two main reasons we?ve been unable to improve browser security and deal with phishing...."  What patents is he referring to?
                                                        -- Jerry

@_date: 2014-06-03 13:57:17
@_author: Jerry Leichter 
@_subject: [Cryptography] It's GnuTLS's turn: "Critical new bug in crypto 
"A recently discovered bug in the GnuTLS cryptographic code library puts users of Linux and hundreds of other open source packages at risk of surreptitious malware attacks until they incorporate a fix developers quietly pushed out late last week."
It's a buffer overflow induced by sending an overly long session ID.  Allegedly code execution has already been demonstrated.
So now we've had serious attacks on Apple's private SSL implementation, OpenSSL,  and now GnuTLS.  Is anything left standing?  What does Windows use for its SSL implementation?
If this isn't the death of "it's open source, any bugs will be found quickly", I don't know what will be.  But it really is way past time to get beyond that, and  every other technique we're currently using.  They ain't working.  We have new tools - better languages, better analyzers, better ways of managing sensitive code bases.  Maybe they're better, maybe they aren't - but we'll never find out because we aren't using any of them.
But that's all just whistling into a hurricane.  The economics say "use the free code, ship first and worry about security later" - the long, all-too-familiar list of reasons not to do the right thing.
                                                        -- Jerry

@_date: 2014-06-03 17:49:26
@_author: Jerry Leichter 
@_subject: [Cryptography] It's GnuTLS's turn: "Critical new bug in crypto 
I could shorten that to "Show me *one* company that does the right thing".  :-(
But seriously, I don't really disagree.  It's all trade-offs, but some of the cost estimates used are likely wrong - and others are changing.  Facebook went along for years, successfully assuming that no one cared about privacy.  "Security" hasn't emerged in quite the same way - partly because it's less obvious where problems might emerge - but the Snowden leaks appear to have created a broad sense of concern.
The problem with this approach is path effects.  Protocols and widely-used implementations live for a *very* long time.  If you accept crap protocol and implementations while waiting to solve the (very hard) problems of social engineering, if you ever do make progress on those, you then find yourself in the impossible position of replacing decades of installed base.  You need only look at the disaster that is network-based industrial control systems with no access controls being exposed to the Internet - and the immense costs (which no one is in a position to pay) to fix the resulting holes.
It's tough to know how to make the tradeoffs.
                                                        -- Jerry

@_date: 2014-06-03 17:55:51
@_author: Jerry Leichter 
@_subject: [Cryptography] It's GnuTLS's turn: "Critical new bug in crypto 
This is the "lemon market" problem that Ross Anderson refers to in the paper I sent a link to earlier today.
                                                        -- Jerry

@_date: 2014-06-04 09:52:23
@_author: Jerry Leichter 
@_subject: [Cryptography] Even when they get it right they get it wrong 
Certificates for email servers are basically pointless, but still ... Yahoo's mail cert expired this morning at 8.  (Apple's Mail.app complains about it and asks me to confirm before allowing a connection.)
                                                        -- Jerry

@_date: 2014-06-04 13:10:54
@_author: Jerry Leichter 
@_subject: [Cryptography] What has Bitcoin achieved? 
A nice pair of papers - especially the second.  I found the following particularly significant:  "[N]ot only does the usual crypto threat model greatly overestimate users? trust in software running on their own devices, it equally underestimates trust in (some) third parties. Many crypto protocols treat service providers as adversaries, a model that?s nonsensical in the modern computing environment. Consumers don?t seek technological privacy protection against governments and service providers but against their peers, nosy neighbors, stalkers, employers, insurance companies, advertisers, and the like. Even when the adversary in a crypto protocol isn?t the service provider, it isn?t necessarily practical to use crypto. Often, it?s simpler for the trusted service provider to act as a privacy intermediary. Facebook?s ad-targeting platform is a good example?it never directly hands over user data to advertisers."
                                                        -- Jerry

@_date: 2014-06-04 17:47:35
@_author: Jerry Leichter 
@_subject: [Cryptography] What has Bitcoin achieved? 
You're taking a remarkably ahistorical view.  In the US, the power to create coinage (out of materials that carried their own value - silver and gold) was restricted to the Federal government, but states and many banks issued their own paper currency until after the Civil war.  At least in the US, we've all grown up in a world where for several generations the only practical difference between paper money and coinage is that the latter had small face value.  This was not previously the case, and may or may not be the case in different places and times.
Credit cards had a long and complex history, starting out as something the wealthy (who were used to dealing with banks and credit) and only become a mass market, nationwide item in 1966.  People didn't understand or trust credit cards at first and it took significant marketing efforts to get people to see them as "just like cash".  (The limits on liability when cards are lost or stolen were actually made into law at the bank's request, as a way of gaining trust.)
"Money" is a concept we've not only internalized, but given an internal deep semantics.  There are some neat behavioral economics experiments that show how our reasoning about "money" can be quite different from our reasoning about goods of equivalent value.  But we can, and do, extend our notion of "money" as times change.  We can't encompass *anything* within our internal "money" concept, but the concept is pretty broad.
It's only we engineers who think that an idea isn't acceptable until its internal features are understood.  Most people have no clue how credit cards actually work.  If you push them, they have no idea how paper money actually works.  They know how to *use* them - i.e., they know the API's - but the implementation is something left to someone else.  I see no reason to believe that Bitcoin, if provided with a suitable interface, wouldn't easily fit into most people's conception of "money".  (And if you think that the instability of the value will stop people ... consider all the long, sad history of hyperinflation.  Sure, when they can get it, people prefer a stable currency - but they manage to deal with unstable ones quite well if necessary.)
                                                        -- Jerry

@_date: 2014-06-04 17:53:21
@_author: Jerry Leichter 
@_subject: [Cryptography] Fork of TrueCrypt 
On the other hand, regularly checking a public location on which the latest versions of a wide variety of products are listed reveals pretty much nothing.;
If you act on what you find and go looking for the new version, of course, you reveal your interest.  But that's true *no matter how you check for or download new versions*:  The metadata about where you connect reveals your interests.  Shock, horror.  Tor.
                                                        -- Jerry

@_date: 2014-06-04 07:36:34
@_author: Jerry Leichter 
@_subject: [Cryptography] Crippling Javascript for safer browsing 
As software engineers, we're always drawn to make our systems as general and powerful as possible; but it's long been clear that power and security are antithetical.  Javascript in browsers is a great example.  A browser is fundamentally a display, and yet we've been drawn to include within it the a Turing-complete programming language - which inevitably opens holes for abuse, which then get patched or limited through some kind of wrapping sandbox.
Meanwhile, the only full defense is browser extensions that simply block Javascript completely - something that's impractical for most major websites today.
The fundamental problem here is the accepted belief that a browser's quality is related to the faithfulness of its implementation of the Javascript specs - even when those specs lead to browsers that facilitate attacks against their users.
Let me give a couple of examples of the kind of thing I'm talking about:
1.  Javascript lets you create new windows - and because some companies and programmers have a goal of allowing browser apps to look just like native apps, it lets you create any kind of window at all.  As a result, it's impossible for a browser to define a "secure password prompt" window (though some - Safari, for example - have tried):  Anything the browser can do, a Javascript hack can do.  Other attacks - e.g., clickjacking - are based on the same basic power.
But why grant that power?  A Javascript implementation that always created windows with a specific chrome on them, forced them to be visible, and enforced a minimum size, might inconvenience some developers, but would render a whole class of attacks impossible.  You might actually be able to trust that a password prompt really *was* a password prompt.
2.  Javascript implements a ton of environmental queries, from screen resolution and size to fonts on the machine to cache information.  This allows for easy fingerprinting of browsers.  A Javascript implementation that simply lied about this stuff - rounding screen information to one of a few standardized values, sending back a fixed list of standard fonts in a standard order, and so on - could stop such attacks.
3.  If a Javascript implementation ignored actions that triggered on attempts to close a window, all kinds of extortionate programs would have a much harder time harassing and terrifying ordinary users.
Of course, what I've said about Javascript applies equally well to other browser facilities.  If there are things in HTML5 that make attacks easier while not adding anything significant - don't implement them, or implement them in a way that mitigates the risks, even if doing so violates the standards.
It's a fundamental (and paradoxical-appearing) result in game theory that sometimes deliberately limiting your own choices improves your position.  We've been discussing keeping the level of language in protocols low in the Chomsky hierarchy to limit vulnerabilities in parsers.  The same idea applies elsewhere.  It'll be a tough lesson to learn, given decades of design philosophy that looks for ever more power - even the tools that disable Javascript are perceived not as limitations but as a way to give the user more choices.  But it's a lesson we need to learn if we're to have any hope of building more secure systems.
                                                       -- Jerry

@_date: 2014-06-04 07:37:53
@_author: Jerry Leichter 
@_subject: [Cryptography] To what is Anderson referring here? 
Thanks to fukami and to everyone else who pointed this out.
I think we have the meat here for an Anderson-like study:  Has any actually made money from a crypto-related patent?  The EKE/SPEKE patents have blocked progress on authentication for years; but has anyone actually licensed them?
The results are of both theoretical and practical interest.  From a theoretical point of view, it would inform, with actual data, the debate about the public interest issues in patents:  If the only real effect of crypto patents is to make the subject of the patent unavailable to the public for the patent's lifetime, then patenting of such material does not fulfill the public interest goals of making inventions broadly available.  From a practical point of view, if it can be shown that patents in this area are essentially worthless - you pay to get a piece of paper, but no one will buy what you're selling - it might be easier to convince people to freely license their patents (assuming they get them at all).
Sounds like a nice master's thesis for someone in an economics department or some related field.
                                                       -- Jerry

@_date: 2014-06-04 22:05:36
@_author: Jerry Leichter 
@_subject: [Cryptography] To what is Anderson referring here? 
There are generally design-arounds available for most patents in most areas - albeit at additional cost, complexity, whatever.  What's interesting in the crypto area in particular is that in many cases there are no practical work-arounds.  What you might then expect is that there would be an active interest in licensing.  But, for whatever reason, that doesn't appear to happen.  Instead, the functionality that the patent could deliver is simply never marketed.
I suspect a big part of this is that the *economic* value of the functionality - i.e., the value that customers are actually willing to put down their money for - is actually quite small.  Selling crypto is hard in general; selling "better" crypto at greater cost is even harder.  As I said, an interesting subject for a master's thesis.  (Expand it to a general study of the economics of cryptography and you might have a PhD, though there's already work in this field.)
A personal story:  Many years ago, I was asked to design and implement a cryptographic protocol to use with a product the small company I was working for was selling.  This was back in the day when export of crypto went through DoD, and you couldn't export anything with symmetric keys longer than 40 bits.  (There were other unwritten rules, such as if you used anything other than DES, you would probably be denied.)
It turned out that IBM had developed a scheme, called something like the Data Masking Facility, for expanding a 40-bit key into a 56-bit key for use with DES.  (Yes, I know, real rocket science.)  The big selling point:  NSA/DoD had effectively (again, nothing in the official writing) pre-approved systems using DMF for export.  The gotcha':  DMF was patented.  I approached IBM about a license, which they were glad to provide.  For some absurd amount - I think there was a base plus a couple of percent of the total value of the product in which DMF was embedded.  We were a startup; there was no way we would give away that big a piece of our product.
I wonder if anyone ever licensed DMF.
This was a case for design-around.  I designed my own bit of "rocket science" in a different protocol.  I was actually rather proud of it - rather than using a 40-bit key, it used a full 56 bit key but included the encryption of 16 bits - using a key we would share with NSA - in the negotiation.  Except that it wasn't quite like that:  It was set up so that NSA could do its 2^40 brute force search, recognize when they had the right 40 bits, then use them plus a shared secret to recover the remaining 16 bits.  I submitted all this to NSA, but right around that time, enforcement of the crypto regs moved from DoD to State.  All movement stalled while they figured out what they would do.  Eventually the policy became "you can ship unless we say no in 90 days" or something like that, and they almost never said no.  I never heard back from NSA, DoD, or State; and we ended up losing all interest in crypto in our product for several years.  Later, I designed and implemented an entirely different protocol based on full AES.
                                                        -- Jerry

@_date: 2014-06-04 22:40:58
@_author: Jerry Leichter 
@_subject: [Cryptography] Crippling Javascript for safer browsing 
I expect nothing from the usual gaggle of browser vendors.  But that doesn't mean it wouldn't be possible to create a new, safer browser - say by adding options to (if the upstream maintainers will accept them) or forking (if they won't) WebKit.  These would be fairly isolated and shallow changes, so even if you have to fork, bringing over WebKit updates should be simple.
If you can get an uptake comparable to NoScript, people will notice - and that should be possible.
The days of competition on "percentage of features supported" are thankfully long gone.  No one is going to win any battles against a reduced Javascript browser by saying "we implement 100% of the Javascript standard".
Google still believes that the browser should replace the operating system, so I don't see Chrome going along.  Apple and Microsoft, on the other hand, aren't nearly as reliant on Javascript, and one could imagine them adding support for "reduced Javascript" just to cause Google grief.  I have no clue what Firefox would do - probably add separately settable options for each feature in Javascript.  :-)  Is there anyone else even worth mentioning?
A private WebKit won't directly get you into iOS because Apple insists that you use their own Webkit, but hey, that's OK; you can't please everyone.  (Besides, if you really want, you can get an iOS developer's license for $99/year and then build whatever apps you want for your own phone.  As long as you don't submit them to the App Store, the App Store rules are irrelevant.)
A personal experience on this front:  Many years ago - the young'uns may not recognize some of the terms in the rest of this sentence - I used Netscape on a Sun X server box with a black and white CRT as the display.  HTML was evolving rapidly at that point, and all of a sudden you could set a background image on pages.  Everyone did - usually with no taste and no thought about how (un)readable the result would be, especially on a black and white screen.  Many sites became unusable.  I fixed this by finding the string constant for the HTML element to set the background to something else.  I didn't have the sources and didn't feel like building them, so I just patched the executable - EMACS makes a fine binary patcher when all else fails.  Sites using the actual spec'ed element ... were readable again.  I think I may have done the same thing again later when  became all the rage.  If some site had known to send me  in their HTML, they might have been able to annoy me - but not with .
                                                        -- Jerry

@_date: 2014-06-06 07:52:35
@_author: Jerry Leichter 
@_subject: [Cryptography] Is it mathematically provably impossible to 
I've used the same problem (in the form "here's an alleged sorting routine; how do you write a test for it that convinces you it's correct") as a interview question for QA people.  Very few get it right without significant prompting.  (In the testing scenario, I can also follow up with questions about performance testing.)
BTW, "monotonically increasing" and "permutation" get tricky if you allow duplicate values in the input - another thing people often forget about.
Back in the 1970's, someone wrote an article in SIGPLAN Notices proposing an abstract datatype called a "traversable stack" - details long forgotten, but it was a stack which you could also "walk down".  The datatype wasn't interesting in and of itself; the point was to develop a formal spec from the English description.  The original article had bugs, which were pointed out in a response article - which also had bugs.  There were a whole series of correction articles, one of which I recall had the amusingly honest title "Traversable Stack With Fewer Errors".
But when the formal, mathematical world meets the messiness of the real physical world, you have to decide whose standards to use in defining "works".  If you use the standards of the formal, mathematical world - there's no hope.  But if you use the standards of the real world - where we live by "good enough/cheap enough" approximations because nothing else is available - your conclusion might be very different.
                                                        -- Jerry

@_date: 2014-06-06 10:46:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Crippling Javascript for safer browsing 
What I proposed was removing or modifying Javascript functions that are dangerous but just annoying in such a way that the server wouldn't, in general, know.  For example, any window created by Javascript might have some distinguishable chrome around it.  The Javascript code would not be able to tell.  The Javascript code might try to position an invisible window over existing controls and clickjack; the window would simply not be created.  Actions on window close would ever go off.  Pop-unders would never get created. And so on.
With judicious selection of what you disable or modify and how, the vast majority of sites will just work - perhaps not "unmodified" from the point of view of the server-side developer who's trying to get some particular nasty effect through, but still "working" from the point of view of the browser user. And you always have the option of allowing things on a site-by-site basis where appropriate.  (That's a bad solution for anyone other than hackers, but if you can make most people very unlikely to encounter the need, it's acceptable.)
                                                        -- Jerry

@_date: 2014-06-08 05:01:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Help investigate cell phone snooping by police 
The problem is that a refrigerator is not a Faraday cage.  Every refrigerator I've ever used has rubber gasket between the door and the body of the refrigerator.  It's typically 2-3 cm thick.  Cell phone wavelengths vary with band, but range from somewhere around 30 cm down to around 8 cm.  A refrigerator should attenuate the signal, probably quite a bit, but it's not going to block completely.
Of course, this assumes that the point of the exercise is to block the RF.  A refrigerator will also, to some degree, block sounds.  Since nothing inside the food box of a refrigerator typically generates any noise (and of course it's not *sensitive* to noise either), there's no reason to design a refrigerator to be soundproof.  So how much it will muffle outside sounds is unclear.
These things are easy to check experimentally, at least to a good enough approximation to say whether they are likely to be effective for the task at hand.  If it's important to you ... let us know how the experiments come out.
                                                        -- Jerry

@_date: 2014-06-09 11:35:31
@_author: Jerry Leichter 
@_subject: [Cryptography] Yet more formal methods news: seL4 to go open 
Workable formal verification is going to have significant effects on the whole programming ecosystem. Consider the interaction between verification and open source. If you make *any* changes to a verified piece of code, you lose the verification. So just what does having access to the source buy you?  Keep in mind that such things as checking for unexpected functionality would be much better done as part of the spec that's verified than by looking for carefully hidden code traps. Perhaps we can get to a world in which anyone who changes the code is expected to update the verification. With the state of the tools we have now, that seems unworkable - it's just too hard and specialized. If a product says it uses, say, OpenSSL, will we ask for a proof that the version being used is a verified one?  The question of how to safely interface verified with unverified is a difficult one - a special case of the general composability problem. At a fundamental level, moving to verified code requires changing a "whatever" work culture into a true engineering culture. It will be wrenching for many. Please don't take any of this as a critique of or a complaint about formal verification!  If will be a great tool to have. (I say "will be" because while we are now at the point where we can see practical application, the tools aren't there yet for broad adoption.)  But nothing comes without costs and side-effects.                                           -- Jerry

@_date: 2014-06-10 07:55:00
@_author: Jerry Leichter 
@_subject: [Cryptography] Swift and cryptography 
Apple had a very specific need:  To replace a language that had long passed its sell-by date (Objective C, which was an important improvement of C in 1983 when it was first introduced) with something that would be compatible with 30 years of libraries, interfaces, and programming/development approaches, none of which fit will with any other language out there.
Special problem, special solution.
On the more general question of whether we need yet another programming language to add to the thousands we already have - well, it's not like people are going to stop developing new languages.  And it's not a bad thing, since historically expressive new ideas go hand in hand with new languages, and often end up enriching old ones.  Both C++ and Java now have "for each" style loops, but neither of those languages invented the idea; it came from a long series of (almost entirely) now-dead languages, probably going back to Clu from the mid-1970's.  As a languages guy, I'm on in favor.
On the more specific issue of fastening on the latest shiny new language as, *finally*, the right one in which to implement crypto algorithms ... there I call bullshit.  OK, maybe RUST is nice.  (I haven't actually looked at it.)  Is its semantics well understood - not just in some formal sense, but by a significant community?  Are there trustworthy compilers - yes, plural, no matter how careful the designers are, a language with only one compiler is defined by that compiler, not by its specs?  Will it still be around and being actively used, maintained, and extended 5 years from now?  Ten?  Only a tiny fraction of neat new language survive - but good crypto algorithms and protocols can live for decades.
And just what is it an implementation language for crypto needs?  Automatic range checking?  Sure ... but you do realize that FORTRAN compilers starting doing that before many of the people on this list were born.  If C has a single major fault, it's that it fed *generations* of programmers the line that arrays were just shorthand for pointers and that range checking was for wimps - and idea that even got transplanted in the C++ standard library's iterators.  Fortunately, this particular virus has infected few other language families.  GC?  Very nice to have, but hardly at the top of the list for the kinds of code that's common in crypto implementations.  (A bit more in protocol implementations.)  And it brings with it a bunch of related issues about tight management of the lifetime of sensitive data in memory.  User-defined operations?  Hardly.  Better ways to express parallelism?  Maybe for protocols, but you're buying into multiple layers of complexity and system-specific approaches if you go that route, both very wrong for general-purpose crypto or protocol implementations.  What else?
The ideal crypto implementation language has three features:  The ability to generate highly efficient code; the ability to control that code very closely (to prevent or at least control things like timing attacks); and a semantics that allows for formal verification (whether you actually do it right now or not; that's an indirect way of saying that the language's semantics is completely and tightly specified).  I don't see anything out there that gives you all three; I don't really see anything on the horizon either.  We have languages that give you the first two, and languages that give you the third.  As the paper Perry recently forwarded shows, after years of research we're getting to the point where we can add the third on to a language that has the first two.  (Languages that have the third property have been promising to get the first two for decades.  I saw claims that network protocols - not crypto protocols - implemented in S/ML could be compiled to produce code at least as good as C code 15-20 years ago.  But somehow the work there didn't generalize, or just plain wasn't picked up - or maybe things were never quite as good as was claimed at the time.)  Frankly, I think we're more likely to get the "ideal" programming language for crypto this way than by jumping to the latest craze.
                                                        -- Jerry

@_date: 2014-06-11 22:37:48
@_author: Jerry Leichter 
@_subject: [Cryptography] End-to-End, One-to-Many, Encryption Question 
The problem as describe to this point has an easy solution:  Alice encrypts the data with a key K.  She then appends to it a bunch of pairs of the form (Charley-1, Enc(C1, K)) (Charley-2, Enc(C2, K)), and so on, where Cn is a key unique to Charley-n and known to him and Alice (but not Bob).  Charley-n can look for a pair with his name on it, decrypt it using Cn, then use the resulting key K to decrypt all the data.
Bob learns nothing about the data.
To remove Charley-n's access, simply remove the name/encrypted key pair with his name on it.
If the data has a reasonably small number of subpieces not all of which are necessarily accessible to all the Charley's, they can be encrypted with unique Ki's and then there will be multiple pairs for a given Charley giving him access to exactly those subpieces he is allowed to see.  (Of course, you can add a level of indirection and have keys for groups, with membership being granted to the group by giving someone a key for the group pair.)
There are more sophisticated protocols for related but distinct problems, e.g., granting the ability to decrypt only certain fields of a database.  But for the problem you seem to be describing, this seems to be the best general solution.
                                                        -- Jerry

@_date: 2014-06-12 15:32:38
@_author: Jerry Leichter 
@_subject: [Cryptography] Swift and cryptography 
Just to clear up a minor point before panicky rumors spread:
No *OS* in many years (maybe since Windows 95 or so) has handed uninitialized memory to programs.  That's way too big a security hole.  What you get from an OS is zeroed pages.  As to *when* they are zeroed ... that depends.  Most OS's have a concept of a "demand 0 page", which is logically added to your process but marked inaccessible to you.  When you first access it, the hardware page faults.  The OS finds and zeroes a page of real memory, maps that real memory to your process, and lets you proceed.  Not super-high-security - sensitive data can sit in physical memory for long periods of time - but simple and efficient.  (It's surprisingly common for a process to want to re-acquire a page that got bounced from its working set.  If it hasn't been handed to anyone else, it can simply to returned to the original process.)
Some OS's have a background scrubber process to find pages of free memory and zero them, but in my experience this is unusual.
*Libraries* that allocate memory *within* a process, on the other hand, usually do *not* zero the memory, either on allocation or on deletion.  In C, you have a choice on the allocation side:  malloc() returns memory with whatever was there, calloc() zeroes the memory first.  Some libraries fill deleted memory with a distinctive pattern - e.g., 0xDEADBEEF - to help catch attempts to use freed memory.  (Often this is done only in debugging mode.)
Security-aware systems use encrypted swap.  (Zeroing "disks" is expensive and increasingly ineffective:  The "disks" may be SSD's, where writing to a given block is redirected to some block you can't control.)
                                                        -- Jerry

@_date: 2014-06-13 14:54:35
@_author: Jerry Leichter 
@_subject: [Cryptography] End-to-End, One-to-Many, Encryption Question 
No one who understands the theory ever proposed NP-completeness as a certificate of good-quality cryptography.  (That doesn't mean many who don't understand the problems aren't all too happy to spout off about how a solution to some *different* problem makes for a great cryptosystem.)
The problem, of course, is that NP-hardness is a measure of the *most difficult* problems in some set.  It's not even a measure of the *average* difficulty - and even if you knew that the *average* difficulty was high, when you do cryptography, you're relying on the the difficulty of *some particular instance*, and if it turns out not to be one of the hard ones, you're dead.
Suppose it were found that one in a million AES keys is "weak" in the sense that encryption under those keys requires only 2^40 effort to break - but there was no way to tell, without expending that effort, whether a key is one of the weak ones.  In just about every area of technology, odds like that would be considered fantastically good.  But for a cryptosystem ... they would render AES unacceptable in any serious use.
The distinctions here apply much more broadly than just to cryptography, BTW.  Suppose you're concerned about the perceived service time of some Web service.  The first (and in all too many cases the only) idea many people have for improving service time is "cache stuff".  Which is great as far as it goes, but a cache by its very nature can only improve *average* service time.  Except in very specialized circumstances, it does nothing about maximum service time, and it does very little about, say, 95th percentile service time.  But if you're talking about *perceived* service time, having a good average but a big delay 1 in 20 times is very bad.
                                                        -- Jerry

@_date: 2014-06-14 08:00:05
@_author: Jerry Leichter 
@_subject: [Cryptography] ghash.io hits 50% of the Bitcoin compute power 
There's a social/economic process that I'm surprised hasn't kicked in yet - and perhaps it tells us something.
If the press - and the Winklevoss brothers - are to be believed, Bitcoin has become a vehicle of speculation (I won't say investment) on a large scale.  A substantial pool of large investors in Bitcoin would, of course, be a group with an interest in keeping Bitcoin not just safe, but trusted.  Having 50% of Bitcoin compute power in the hands of a single other group, whatever its effect on actual safety, certainly has the power to destroy trust in Bitcoin - and hence the value of investment.  So it would behoove those with large positions to begin mining as a way to break up the control, even if on a per-Bitcoin basis that mining loses money.  In a way, it's like a national bank trading in its own currency in an attempt to break the influence of others on its value.
The economics are not trivial, since for one thing it's hard to know where the red line of lost trust is - and once you've crossed it, how much further back you have to push things to restore trust once lost.  And the analogy to national banks indicates that maybe you can't succeed in this game anyway - the international markets in currencies have for years been so large that most national banks that have tried to influence them have failed.  (The Chinese, who use other techniques as well, may be the only ones who've succeeded.)
Still, the lack of any hint of an effort in this direction may well be evidence that there's much less high-end speculation in Bitcoin than we've been led to believe.
                                                        -- Jerry

@_date: 2014-06-15 06:02:23
@_author: Jerry Leichter 
@_subject: [Cryptography] End-to-End, One-to-Many, Encryption Question 
Information once transferred can't be recalled.  The strongest thing you could ever get is a record of who accessed the information (for some notion of "who" - i.e., some kind of identification scheme on which you rely).  As the NSA's apparent inability to determine exactly what documents Snowden accessed testifies, that's often not attainable in the real world even for the "first order" access.  Once you ask who might have received the information from whoever accessed it directly ... the answer quickly becomes "just assume everyone has seen it".
I don't see how the cloud changes anything other than scale (which, granted, may change things significantly from a real-world point).
                                                        -- Jerry

@_date: 2014-06-16 10:45:45
@_author: Jerry Leichter 
@_subject: [Cryptography] [cryptography] Dual EC backdoor was patented by 
There may not be any, as such.  In 2005, patent trolls were still a minor part of the patent landscape.  Most lawsuits were between big companies, and the most important thing to have in your war chest in such a battle is a large collection of patents you can trade to fend of an attacker.
In addition, if you're trying to prove that you're a significant company in a technical field, being able to say you're "one of the top three patent holders world-wide in cryptographic algorithms" (or whatever low number you can hope to put in there; I'll bet IBM is number 1 even today) is good for your marketing position.
                                                        -- Jerry

@_date: 2014-06-17 23:04:35
@_author: Jerry Leichter 
@_subject: [Cryptography] Implementing constant-time string comparison 
If you're going to go this far, you'd better understand everything about the code generator.  Peephole optimizers have long been capable of determining the actual semantics of the short sequences of machine code that expressions like this generate and then finding "optimal" (according a different set of measures than motivated this code) if sometimes quite surprising equivalent sequences.  For example, C and C++ lack explicit notations for bit rotation, but their compilers have long turned expressions like (x << 4) | ((x >> 28) & 0xF) into a "rotate left 4 four bits" instruction.  (This one is probably a built-in sequence - and compilers can be curiously sensitive to minor nuances in the way the expression is written - but more general optimizers have been written and fielded in the past.)
The technology to recognize that the loop computing the XOR can be done "better" using comparisons and a quick exit when a difference is found certainly exists, though it probably isn't used in fielded compilers (because the cost/benefit ratio would be too small).
Even C is really too high-level to allow you to be sure of the instructions compilers will emit.  If you want instruction-level control, you still need to use an assembler - and, yes, it's non-portable.  How can "instruction-level control" be anything but non-portable?  (And of course you'd better understand how the instructions themselves are implemented in the hardware.  Successful attacks have been mounted against various machine-level caches.)
                                                        -- Jerry

@_date: 2014-06-19 13:36:08
@_author: Jerry Leichter 
@_subject: [Cryptography] Help please, 
There is an interesting problem hiding in here.
- If you are concerned about demands on you personally to deliver the key; or,
alternatively, if you are concerned you'll forget the key; then something like a piece of paper in a safety deposit box is a good solution.
- If you are concerned that any physical copy can be compromised, but believe that your Fourth Amendment rights (or equivalent outside the US) against self-incrimination extend to revealing keying information and will be protected, then you want to use a memorized key.
If you're concerned about *both* kinds of attacks, it would seem you want a defense in depth, somehow combining the two mechanisms in such a way that an attacker (including "forgetting" as an attacker against stuff in human memory that isn't regularly exercised) would have to go through both.
That suggests splitting the key into two pieces.  One is on that piece of paper.  One is in your head.  To protect against forgetting, the piece of paper has *something* on it that you are reasonably sure will remind you of your half, even years later.
A simple approach would be a bunch of those personal questions - name of your kindergarden teacher, the first TV show you loved - with the answers hashed together and the result XOR'ed with a value on also on the page to generate your half of the secret.  Yes, this can be attacked by someone who investigates your life carefully enough, but it's better than nothing.  More of a problem is that as stated it's exquisitely sensitive to minor changes in spacing, punctuation, capitalization, and other perhaps-hard-to-remember meta-properties of the answers.  You'd want to run the answers through some kind of normalizer first.
It is possible to do better?
                                                        -- Jerry

@_date: 2014-06-19 15:06:35
@_author: Jerry Leichter 
@_subject: [Cryptography] Shredding a file on a flash-based file system? 
Absolutely concur.  In fact, for the specific case of flash, load-leveling algorithms make it virtually certain that the block you zeroed is *not* the one that contained the data you wanted to get rid of.  That block has been added to the end of some internal queue over which you have no control and which you can't even see using any standardized or even documented mechanism, where it will *eventually* be erased - but no one can tell you when.
And of course your opponent, who will have access to all kinds of hardware-hacking equipment and either has gotten - by fair means or foul - or has determined by deep hardware analysis the hidden mechanisms for reading everything hidden away on the underlying chips - will have little trouble pulling that material out.
Modern disk drives increasingly have NVRAM in them to improve their performance.  It's getting so cheap that it will eventually be universal.  (Disks have had tons of RAM for things like track buffers for years now.  Maybe some of them have an internal battery to ride out short power failures....)
Every iPhone since the 4 or maybe 4S has done exactly this.  (I think iOS uses a "file-by-file" approach and only protects stuff that's been declared "sensitive".)  At least some Android phones do the same thing.
As always, the killer is the stand-alone system that has to reboot without someone there to type in the disk decryption password.  TPM's are a great solution to this problem - or would be if they hadn't gotten twisted to support DRM.
(I know of a company that has a bunch of standalone servers that need access to encryption keys and are scattered throughout the company campus.  They decided that these things are almost never shut down, so it's not worth deploying a secure way to store the keying information.  Instead, a rebooting machine has enough smarts to ping the support center for help; a person comes out and supplies the keying information.  Makes for a big fire-drill after a large power-failure - which has happened - but that's considered a worthwhile tradeoff.)
                                                        -- Jerry

@_date: 2014-06-19 17:04:01
@_author: Jerry Leichter 
@_subject: [Cryptography] from CNBC: "Cybersecurity firm says large hedge 
The main thing that makes no sense to me is that, according to the story, the attackers "installed a malicious computer program on the servers of a large hedge fund, crippling its high-speed trading strategy and sending information about its trades to unknown offsite computers".
Either of these two attacks makes financial sense:  You can badly screw up their strategy and make money as a counter-party; or you can watch what they do and make money running some combination of parallel or adverse strategy.  The second is more subtle, difficult, and takes longer, but the chance of it being detected is extremely small if you're careful.  The former may get you big money quickly, but it's certain to be noticed.
Depending on you goals and capabilities, either approach would make sense, but it's hard to come up with a reason to do both simultaneously:  The second is unlikely to increase the gains much over what the first gives you before the first gives the whole game away.
                                                        -- Jerry

@_date: 2014-06-20 09:00:04
@_author: Jerry Leichter 
@_subject: [Cryptography] "Is FIPS 140-2 Actively harmful to software?" 
He never quite says "yes" but he clearly thinks it.
On a related note, pointed to from another blog entry:  NIAP has recommended against further development of or evaluation against the Common Criteria profiles for general-purpose OS's and DBMS's:
(Just for good measure, they say the same about "Enterprise Security Management Products".)
OK, it's time for a set of acronyms and a bunch of new paperwork to keep the security/industrial complex - all those consultants ringing DC - fully employed (er, "guaranteeing the security of our critical infrastructure".)
                                                        -- Jerry

@_date: 2014-06-20 11:42:36
@_author: Jerry Leichter 
@_subject: [Cryptography] Shredding a file on a flash-based file system? 
You can these days buy USB sticks with capacities anywhere from 4GB (maybe there are still some smaller ones) to 128GB or even more.  You pay by capacity - generally 50 cents to a dollar a GB these days.  It's not that the chip makers are making a whole range of different sized NVRAM's.  Rather, they make "the largest they can" accepting a high rate of bad cells, and simply put them in different bins depending on how much of that memory shows up as working.  That the prices per bit are more or less constant (except at the very high end) tells you something out the yield curves....
The same thing as the CPU makers do with speed binning, but on a much larger scale over bins that span a much larger range.
                                                        -- Jerry

@_date: 2014-06-21 15:04:11
@_author: Jerry Leichter 
@_subject: [Cryptography] Spaces in web passwords 
Somewhat crypto-related, I think...
I'd guess this has nothing to do with cryptography and everything to do with human information processing.  Spaces are not generally considered significant in human communication.  Theyareaconveniencewhichcanbedispensedwith.  Yes, reading that sentence required some effort, but you could figure it out.  Sure, syntactically I might have meant "They area convenience" but you never seriously considered that possibility, did you?
Even where spaces are genuinely needed for disambiguation, humans are unlikely to notice or response to leading or trailing spaces, or doubled spaces within the text.  This is especially so when using a variable-width font (i.e., most fonts) where it's impossible to distinguish one from two spaces with any degree of certainty.  In typography, "space" isn't really a character....
I'd guess these organizations just got tired of support calls caused by people who accidentally entered a leading or trailing space (it's very natural to hit the space key after typing a "word"), or were too clever by a half and created a secure password with embedded spaces and then couldn't remember - because this generally is not significant information - just where they put that space.
Whether to simply ignore spaces, or forbid them, is an interesting question.  On a purely theoretical basis, you could argue that ignoring any characters in a password is bad practice.  But on a human interaction basis, it may well be an excellent idea:  People can remember phrases with about as many words as they can remember "single word" passwords with that many letters.  But they are likely to have trouble typing them with consistent spacing, or with consistent *lack* of spacing.  So suggesting they use a phrase and then ignoring the spaces may well result in better passwords over all.
It would make for an interesting study.
                                                        -- Jerry

@_date: 2014-06-27 10:56:16
@_author: Jerry Leichter 
@_subject: [Cryptography] "Perfect Forward Secrecy - The Next Step in Data 
What are the chances that the authors of this white paper understand anything they are talking about?
                                                        -- Jerry
Begin forwarded message:

@_date: 2014-06-30 09:28:30
@_author: Jerry Leichter 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
But this leads us to an interesting position.  We have two schemes (algorithms, protocols what have you), A and B, that both ends definitely implement and which in other ways (performance, cost) are essentially interchangeable (else, again, we would not be in a position to stop supporting either).  We think both are secure, but one of them *may* have been broken - we just don't know which.
This is then simple game theory:  We have two strategies, with equivalent payoffs that may be low (if the opponent "chose" to attack the same strategy we chose), or high otherwise.  And game theory tells us that the optimal strategy is a mixed one:  Choose A or B at random with equal probability.  (You can weight the probability to match knowledge of a weighted probability on the opponent's side.)
So the logical outcome of asking for algorithm agility is ... a single more complex algorithm.
                                                        -- Jerry

@_date: 2014-06-30 15:16:01
@_author: Jerry Leichter 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
No, the situation I'm assuming subsumes yours, as you're ignoring the break that doesn't make it into the open literature.  The mixed strategy covers both.  (Actually, as was pointed out earlier, encrypting with first one algorithm and then the other also does - but the order may matter.  Choosing the order at random per message is then again the best strategy.)
Once the academic papers start to come out, your estimate of the probability that the attacked algorithm is actually broken goes up, and your best mixed strategy naturally shifts to the other - until one of the algorithms is considered to be definitely unsafe, at which point the optimal strategy is clearly to ignore it.
Game theory is about optimal decision making in the presence of uncertainty.  That's the situation you face any time you field a cryptographic system:  You have to assume others will try to attack it, but you have no way to know if they'll succeed.  Game theory is cast in terms of the potential strategies available to you and your opponent.  Here, your strategies involve using A or B.  You opponent *has a strategy that counters A (respectively B) if and only if he has broken A(B)."
                                                        -- Jerry

@_date: 2014-06-30 15:27:13
@_author: Jerry Leichter 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
============================== START ==============================
I'm responding to John Kelsey's point that a fallback is only useful if you're willing and able to throw the primary away.  You're basically saying that you trust the primary, but will kind of accept the fallback if you're forced to.  Not at all the same thing.  And if that's really how you feel ... many will argue that the threat to the primary isn't really so bad, we aren't really so confident in the fallback, no one's seen an actual break yet ... and you're left in the position that many won't move to the fallback, so you're forced to use the primary you no longer trust in order to communicate with them.
BTW, it's not *a* new, more complex algorithm.  The encode side is given
I was being a bit facetious about the "more complex algorithm".
                                                        -- Jerry

@_date: 2014-03-03 11:28:01
@_author: Jerry Leichter 
@_subject: [Cryptography] Testing crypto protocol implementations 
Indeed.  See  for some potentially significant work in this direction.  (It'll have to prove itself.)
                                                        -- Jerry

@_date: 2014-03-03 14:28:39
@_author: Jerry Leichter 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
You'd be showing your ignorance.  Both gcc and llvm have this warning turned off by default, at least for C.  In fact, neither of them have it as part of -Wall - you have to explicitly request it (-Wunreachable-code).  From the documentation:  "This option is not made part of -Wall because in a debugging version of a program there is often substantial code which checks correct functioning of the program and is, hopefully, unreachable because the program does work.  Another common use of unreachable code is to provide behavior which is selectable at compile-time."
You may not like it - I don't - but that's the way the compilers are configured - and have been for a long time.
I have no idea what this means.
                                                        -- Jerry

@_date: 2014-03-05 14:43:58
@_author: Jerry Leichter 
@_subject: [Cryptography] GnuTLS -- time to look at the diff. 
If you're talking about *formal* validation (i.e., program proving), then no.  Forty years ago, you could argue that program proof techniques could work with structured code, but could not with goto's.  Today, they work just fine with goto's - and especially with the very restricted kinds of goto's that are at issue here.
If you're talking about code reviewing and such ... it depends.
People have been promising that their approach to programming will eliminate bugs and make verification easy for as long as there have been programming languages.  We've learned a few things *not* to do, and they help; but nothing we have so far makes an overwhelming difference.
It's possible to write bad, unvalidateable code in every language.  It's not even particularly challenging.  It's also possible to write good code in almost every language (well, perhaps not Intercal  if you're disciplined enough.  (It's widely believed today that it's impossible to write good, understandable, validateable in assembler.  Those who believe this have probably never seen a non-trivial program written in assembler by a good programmer.  There were conventions for writing in assembler that made it quite readable, understandable, and verifiable.  I certainly don't recommend going back to coding in assembler, but understanding *why* that would be a bad idea, rather than just taking it as received wisdom, is important.  Back when I taught programming languages, I made it a point to show students some of the *good* features of PL/I, not have them just repeat the old saw that "if you want PL/I, you know where to get it".  People who did well in my classes would likely choose C over PL/I for many problems - but they could give you good reasons for doing so.
                                                        -- Jerry

@_date: 2014-03-06 20:14:31
@_author: Jerry Leichter 
@_subject: [Cryptography] GnuTLS -- time to look at the diff. 
So what else is new?
                                                        -- Jerry

@_date: 2014-03-07 16:24:43
@_author: Jerry Leichter 
@_subject: [Cryptography] GnuTLS -- time to look at the diff. 
Interestingly, this is where C++'s static typing and "cradle-to-grave" constructor/destructor style is a perfect fit.  At a previous job, we designed a pair of classes, RedMem and BlackMem.  This picked up on classic crypto language:  RedMem contained unencrypted, sensitive data; BlackMem contained encrypted versions of stuff from RedMem.  Encryption functions took RedMem input and produced BlackMem output; decryption functions went the other way.  I/O functions worked on BlackMem, but would refuse RedMem.  The RedMem destructor cleared memory before releasing it.  And so on.
Ordinary code could pass BlackMem around safely and treat it as just some values it didn't know how to interpret.  In fact, BlackMem was essentially a subclass of an "ordinary memory" class.  But RedMem was special, and could only be handled by trusted code.
You can't cover every possible scenario this way - e.g., when you want to create a message, the inputs have to come from *somewhere* that isn't RedMem; and presumably you want to *use* the decrypted stuff in RedMem for something.  And it's always possible to deliberately circumvent the protection offered.  But all kinds of mistaken code just outright won't compile.
It's a mistake to concentrate too much on the features the language itself gives you.  You need to be able to construct datatypes and functions appropriate to the problem domain.  With a decent language and well-designed libraries, you can write reliable code.  Without them; without careful design and implementation and checking ... all bets are off.
                                                        -- Jerry

@_date: 2014-03-09 07:20:26
@_author: Jerry Leichter 
@_subject: [Cryptography] RC4 again (actual security, 
The reasoning here disturbs me.  Use chacha20 because "everyone out there thinks [it] provides very high security"?  Was a poll taken of "everyone out there" for any reasonable definition of "everyone"?  And the *all* agreed?  Really?
RC4 has been around for much longer the chacha20, and has been subject to a hell of a lot more cryptanalytic attack.  So far, it's stood up remarkably well - especially when you consider how simple its basic ideas are, and how far cryptanalysis has advanced in the interim.  (RC4 was designed in 1987 - making it roughly contemporaneous with the publication of differential cryptanalysis, arguably the beginning of a serious public cryptanalytic capability.)
Perhaps chacha20 is the way to go.  I think the design behind it is a very nice bit of work, but whether it will stand the test of time is impossible to answer.
What *is* important, though, is to avoid the temptation to rush off after "the new shiny", just because it's new and shiny.  Even most cryptosystems proposed by the best in the business - and djb is certainly in that category - don't survive the community's attacks.
                                                        -- Jerry

@_date: 2014-03-09 08:09:59
@_author: Jerry Leichter 
@_subject: [Cryptography] Fwd:  GnuTLS -- time to look at the diff. 
Interesting.  Some of the ideas are good in general; others are highly specific to C.  I'm a bit disturbed, given that they are specific to C, to find that:  "The use of the preprocessor must be limited to the inclusion of header files and simple macro definitions. Token pasting, variable argument lists (ellipses), and recursive macro calls are not allowed."
Ahem:  No version of the C preprocessor has ever supported recursive macros!  (There are other It's also disturbing to see the section on checking error returns give as example functions printf() and strcat() - two functions that are inherently extremely difficult to use in a safe way, and which by 2006 when the paper was written had alternatives that provided explicit lengths that were much safer.
They recommend assertions that turn into no-ops at run-time, resulting in the rather bizzare-looking recommended form:
which in essence punts the problem to the caller.  The Ariane 5 failure  shows what happens when errors are checked but doing something about them is considered Someone Else's Problem.
I pointed this out earlier - but it's a bit more complicated than that.  The functions pass around a state object, and it's a common design pattern in careful C code - I don't know whether it's followed here - to mark such objects with an indication of what kind of object they are.  For one thing, if the state objects for, say, different hashes are of different sizes, this provides some assurance that the state object passed in is actually large enough that the function can safely write into it!  In that case, a checked error may prevent a memory overwrite.  By choosing a different reasonably large tag value for each type you use, you can even have some chance of detecting heap memory overwrites.  (You can think of this as manually adding dynamic type checking to C.  The technique is almost never used in C++ because *well-written* C++ can be statically type checked.)
                                                        -- Jerry

@_date: 2014-03-10 06:48:42
@_author: Jerry Leichter 
@_subject: [Cryptography] RC4 again (actual security, 
Thanks for the pointer.
The paper references two different kinds of biases in RC4.  The first is single-byte biases after key setup.  The second is double-byte biases in the "steady state", long after key setup.
The first of these doesn't really surprise me.  From the first time I saw RC4 described, the key setup bothered me.  The basic algorithm assumed implicitly that the permutation table was random.  But as the algorithm is described, it requires both that the result of running the stream generation process randomize the table, *and* that the initial key setup do so as well.  Running the key generation process for a while during key setup eliminates the second assumption for very little cost.  I don't see this as "using RC4 in a secure way"; I see it as a change in the key setup procedure, produce a new RC4' algorithm that avoids the single-byte biases.
The double-byte biases, on the other hand, are killers.  They're present in the long-term state of the algorithm, and while small, are demonstrably sufficient to actually break encrypted text with a reasonable amount of input.  This isn't a matter of how RC4 is used; it's a fundamental weakness.  It's interesting that the biases were found in 1978, but apparently no one until now tried to apply the - fairly straightforward - statistical attacks needed.  It's long been assumed that a modern cipher has to be secure against a chosen-plaintext attack.  The difficulty here was in setting up the chosen plaintext inside of a browser session; but the whole reason we assume we need chosen-plaintext security is that it's been proven repeatedly that a determined adversary can always, somehow, manage to pull it off.
That's convinced me personally.  RIP RC4.
                                                       -- Jerry

@_date: 2014-03-11 19:57:19
@_author: Jerry Leichter 
@_subject: [Cryptography] GnuTLS -- time to look at the diff. 
Many, many years ago, when DEC had produced a couple of VT1xx terminals which were supposed to be compatible but had some subtle differences, it was decided that it was time to write and publish (internally) a "VT100 standard".  The guy who put it together (Ram Sudama) decided on an interesting approach:  There was a textual definition of each possible command you could send to a VT1xx terminal and what would happen; and a bit Pascal that implemented that command on a simple simulation of a terminal.  You could actually use a tools to extract all the Pascal code, compile it, and get a running (if rather slow) emulator of a VT102 terminal.  You could also extract just the textual descriptions - which I believe formed the basis of the published VT102 programming manuals.  (To this day, the VT102 as described in that manual is the best definition of a "VT100" terminal - the VT102 added a couple of commands to the VT100.)
The Pascal code was, in standardese terms, "normative" (though there was a really significant effort to ensure that the Pascal and the English "said the same thing".)
We used to define processor instruction sets in fairly informal terms, which led to programmers finding odd corners of instruction behavior with sometimes model-specific features.  The IBM 360 was the first architecture that tried to pin down fairly exactly what every instruction did and didn't do.  The VAX took that a step further; the Alpha even further.  By now, we expect instruction sets to be fully and rigorously defined.
We really should expect no less of our protocols.  But we still tend to specify just the responses to *correct* messages, leaving it up to the implementor to decide for himself what to do about *incorrect* ones.  That's what we did - but haven't done in decades - in specifying instruction sets.
It takes some concentrated effort, but it's really not all *that* hard.  And the excuse that providing some freedom for implementors so that they can get higher performance is just not acceptable any more.
There have been specification languages out there for years, but they aren't really used much for Internet and related protocols.  The bias has always been for "code", to the extent that to this day, if you had only the RFC's to work from, it's unlikely you could produce an interoperable TCP implementation.  Understandable when you know the history - but still pretty disturbing.
                                                        -- Jerry

@_date: 2014-03-13 06:52:30
@_author: Jerry Leichter 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
As software developers, we've lived through a couple of decades of engineering heaven, during which we could always excuse our excesses by saying the hardware would catch up "soon" because of the magic of Moore.  No matter what resource we squandered.
Well, that game was, as you point out, never valid in the embedded world to begin with, and isn't one to rely on *anywhere* any more.  CPU speeds are still increasing, but that curve flattened considerably several years back when clock rates more or less stopped increasing because we could no longer cool the damned chips.  The easy single-threaded speedups vanished; now if you want greater speed, you either eke out a couple of percent by dint of hard work on architecture, go parallel, or make use of specialized hardware.  The big increases in speed for AES in the last couple of years are due to moving the implementations into hardware.  Great where you have that hardware; not so great if you don't.  And all that talk about crypto algorithm agility?  Forget it.  If switching algorithms makes all your hardware suddenly 10 times slower, no one will do it.
Meanwhile, in many domains the hardware replacement cycles have gotten much longer.  Desktops used to be replaced every couple of years, at least at the technologically hip places - the latest generation of software needed the new hardware.  But desktops long ago passed the "good enough for everything you want to do" point and get replaced if they break - which they basically don't any more.  Laptops are almost at the same point.  Data center machines?  When you're talking about replacement at the level of thousands of machines minimum, no one's in a rush to write off capital investment that fast unless the gains are really significant.  I doubt you could justify replacing a data center CPU for less than a factor of 3 performance gain.  (You might buy some newer machines to add to your fleet, but you're going to keep the older ones in production, too.)  Now tell me where I can buy a CPU that's 3 times "better" on useful measures than last year's.  Or even 2011's.
The only place you can rely on consistent year-over-year performance increases these days is in mobile, though there power constraints are probably going to dominate until mobile devices, too, pass the "good enough for everything I need" point.
That's not to say there won't be individual gains.  Maybe ARM in the data center will greatly improve the watts/cycle ratio - though that won't give you more single-stream cycles/second, just more total cycles (and we're already having trouble making good use of parallel cycles for crypto).  Same for general-purpose computing on GPU's.  Crypto in the hardware - yes, that gives you big gains, but at the loss of flexibility.
Yes, performance matters.  It always has:  A system that's secure only as long as it operates below the data rates necessary to get the job it's paid to do done isn't secure in any meaningful sense.  We've been lulled by years of being able to make raw single-stream performance (and ever-increasing memory usage, and ever-growing disk usage) Someone Else's Problem, which miraculously Someone Else has solved for us.  Those days are over.  It's *our* problem now.
                                                        -- Jerry

@_date: 2014-03-14 06:53:41
@_author: Jerry Leichter 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
My favorite recent (last Sunday) example:  My built-in GPS reported that a trip would take half an hour, and that I would arrive at 10:30.  Meanwhile, my clock reported that it was 9:00.  The GPS had automatically adjusted for DST, but the clock - that had to be set manually.  (In general, the GPS has a knowledge of the time way beyond human resolution, but when the clock drifts, it's my job to resync it.  Typical lack of any reasonable level of systems thinking in embedded systems.)
Many years ago, I taught an operating systems course that was required of all "computer engineering" students.  (The school had both CS and CE programs because way back when, both the school of arts and sciences and the engineering school decided they needed to teach computing.)  CE students learned their C in a digital signal processing course.  Ugh.  I made it a secondary goal of my OS course to teach some good C coding practices - focusing on something other than "make it absolutely as fast as possible".
Liability is hardly the panacea it's been held out to be - as witnessed by GM's current problems with allegedly defective ignition locks that for a decade have, now and then, completely shut down all systems, sometimes at highway speeds.  These are simple mechanical systems that haven't changed much in a century, and assuming there is a design issue, liability is completely clear.
                                                        -- Jerry

@_date: 2014-03-15 06:33:58
@_author: Jerry Leichter 
@_subject: [Cryptography] =?windows-1252?q?=27=93Ooh_Aah=2E=2E=2E_Just_a_Lit?= 
=?windows-1252?q?ong_way_=27?=
Naomi Benger, Joop van de Pol, Nigel P. Smart, and Yuval Yarom
Abstract. We apply the FLUSH+RELOAD side channel attack based on cache hits/misses to extract a small amount of data from OpenSSL ECDSA signature requests. We then apply a ?standard? lattice technique to extract the private key, but unlike previous attacks we are able to make use of the side-channel information from almost all of the observed executions. This means we obtain private key recovery by observing a relatively small number of executions, and by expending a relatively small amount of post-processing via lattice reduction. We demonstrate our analysis via experiments using the curve secp256k1 used in the Bitcoin protocol. In particular we show that with as little as 200 signatures we are able to achieve a reasonable level of success in recovering the secret key for a 256-bit curve. This is significantly better than prior methods of applying lattice reduction techqniques to similar side channel information.
                                                        -- Jerry

@_date: 2014-03-16 09:56:33
@_author: Jerry Leichter 
@_subject: [Cryptography] Client certificates as a defense against MITM attacks 
SSL was designed primarily - and is used almost exclusively - under a model in which servers are considered trustworthy while clients were not. Thus, we have server-side certificates that clients rely on, but everything about server trust of clients is ad hoc.
But we're in a different world.  DNS can't be trusted; CA's can't be trusted; browser lists of who *can* be trusted among CA's are a joke.  MITM attacks - the thing we built the entire structure to block - are commonplace.  There's are many "legitimate" corporations selling wares to make this easy, and we know there have been legal attempts to get hold of server keys (so there have almost certainly been "black bag" jobs to do the same thing).
So let's expand the model a bit.  Imagine clients have certs, too, and that servers know their client's certs.  If, once an SSL connection was established, the client sent a message, signed with its own cert, saying "This is the SSL hash of the CA that signed the cert I used for you and the session key I'm using to talk to you", the server could immediately detect any MITM attack.  We can think about what to do from there, but fundamentally if it's concerned about its user's security, it must shut the connection down.
An attacker could still completely impersonate a site; and a site might be compelled to consider the connection OK anyway; or the client's cert might be tampered with.  But any of these is much more intrusive and visible than simply tapping in, and doing stuff on the client side requires a hugely larger scale of operation.
Where does the client's cert come from, and how does the server authenticate it?  I'd suggest that you get most of the way there by having the server generate the cert for the user and save it on his browser when his session is created and the first time he connects from a given browser - that is, pretty much the same way as the username and some kind of "this computer has been used by this user before" cookies are now commonly set. This way, the server actually knows the user's public key; it doesn't need to rely on a CA, even an internal one.  (The server should destroy the client's private key once it's been sent.)  Yes, that data could be grabbed, legally or otherwise, but there's more of it and it's not one fixed piece of data for all time - it keeps growing.
This doesn't solve the "first connection between a pair of parties neither of whom has any way of identifying the other".  Sure, you might as well allow the user to send a cert registered with some CA.  Someone in the middle who can "open up" the SSL session being initiated can just replace that with their own cert, and now there's no basis for trusting the the client cert is actually bound to the user.  It seems we always end up back at "key continuity" as the best we can do - which in the "parties initially unknown to each other" setting seems unavoidable.
Am I missing something obvious here?
                                                        -- Jerry

@_date: 2014-03-16 15:23:29
@_author: Jerry Leichter 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
I don't buy that contention.  It certainly doesn't describe the relationship of EC to RSA - I haven't heard any claims that EC is inherently more secure than RSA (in fact one might argue that the underlying hard problem in EC is less well-understood than the one in RSA); rather, the claim is that RSA is becoming too slow to be practical to retain adequate security in the face of advances in attacks, while EC gets by with much smaller keys (hence gets much better performance) at the required level of security.
SHA-3 seems comparable to SHA-2 in performance, even though we've had much less time to come up with clever optimizations.  (Of course, if you have hardware help, the story is different.)
A fallback for AES with significantly worse performance simply wouldn't be used.  People put as much hardware as needed out there to solve a problem; they don't add a whole bunch extra "just in case".  There might be specialized areas where higher security would trump performance, but as a *standard*, intended to be widely or even universally deployed, an algorithm that required a significant performance hit would have a really tough time getting accepted.  Note that, again, the comparison will shortly be, in some sense, rigged:  An ever-growing percentage of fielded machines will have hardware support for AES, making reaching even approximate parity using a pure software implementation of some other algorithm extremely difficult to achieve.
This latter issue raises an interesting question:  Give that you have fast hardware primitives for the SHA-2 and AES round functions, can you use them to construct different algorithms that you can reasonably hope will be secure against attacks that seriously weaken SHA-2 and AES?  Here's one example (which I make *no* claims about):  All the attacks we know of against iterative block ciphers are limited in the number of rounds that they can attack.  Given a decent round function and a decent key scheduling algorithm, with enough rounds the construction appears secure.  So suppose we added a couple of extra rounds to AES - relatively cheap given that the round function is in hardware.  To extend the key schedule, stretch the original key by applying the fast hardware SHA-2 to it.
                                                        -- Jerry

@_date: 2014-03-16 15:58:42
@_author: Jerry Leichter 
@_subject: [Cryptography] Client certificates as a defense against MITM 
I'm actually not suggesting using (a) TLS as it stands; (b) client certificates as a way of authenticating the client in a traditional sense.  Rather, the client cert exist solely to provide a mechanism for the client to send an unforgeable (without attacks on parts of the system that don't currently need to be attacked) message to the server.  This would be automatic - effectively an additional round in a TLSX session setup.  It can be piggy-backed on the first client-to-server message, though that does increase the risk as that message would be visible to the MITM.
Servers already *do* place information on clients to let them detect "known" machines.  This becomes one other authentication factor; when it isn't available, sites may ask security questions or whatever.  That, however, is static data which can simply be forwarded by the MITM.  By using the same kind of approach but tying it to a particular connection as seen by the two end nodes, we can make it impossible for the MITM to forge.
Explicitly not covered by this proposal.  A true "first connection" with no previously shared information and no mutually trusted third party (effectively, an introducer) is an incoherent concept anyway.
Then they aren't protected.  Though providing a way to export and import the information on a USB stick or whatever seems easy enough.  You can even imagine techniques for transferring it over a network, though this gets tricky.
There's no password for the user to forget in my proposal.
Given a client cert, you *could* decide to use it in place of the password.  In general, that's a *bad idea*, as it means a stolen laptop gives immediate access to tons of websites.  Of course, if you store the client certs in something akin to a password safe - which the user has to unlock - then both the user experience and the safety are pretty much as they are today.  (Note, BTW, that many password safes sync over the Internet.  That might be OK here *with* the proviso that the crypto used doesn't just prevent reading a stolen copy of the safe - it prevents a MITM modification that modifies or even just adds a new client cert.  I doubt any existing implementations have worried about that problem - though with autofill for account names and passwords, perhaps they should.)
None.  The only thing this is designed to do is make MITM attacks harder.
I don't follow this.
And also:
Which technology?  Clients certs have certainly been around for years, but I'm suggesting a non-standard use for them.
Frankly, if we spend our time worrying about how to secure things that are inherently unsecurable, we'll make no progress at all.  I don't really care about the security of someone who reads his mail on some machine he has no reason to trust.  *He* clearly doesn't - it's not like the hazards aren't obvious once explained (and they *have* been explained many times) even to the technically unsophisticated.  If he wants to take the risks, fine - but his willingness to live with high risk shouldn't block the rest of us from building things better.
                                                        -- Jerry

@_date: 2014-03-16 16:50:10
@_author: Jerry Leichter 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
An ever-growing percentage of fielded machines will have
That's a battle we lost long before we ever knew we were fighting it.  The very notion that there should be a standard, universal encryption algorithm - a notion that made it into public awareness with DES, and was so well established by the time of the AES design contest that it wasn't even questioned - guaranteed that, eventually, something like AES-NI would exist.  (In fact, early on, DES accelerator chips were a big deal, because software implementations couldn't keep up with emerging network technologies - like the original 10Mb/sec Ethernet.  The long period in which we did crypto entirely in software may soon appear to be an anomaly.)
It's interesting to note that (a) NSA always seemed to believe in hardware-based crypto - the DES initial and final permutations appear to play no role other than making software implementations harder; (b) NSA also has never believed in just one cipher.  We know publicly of several families of ciphers specialized for different purposes, though we know little or nothing about the details.  Of course, NSA has the funds and control needed to mix multiple algorithms with hardware implementation:  They can afford to recall and replace hardware as needed.
Don't hold your breath.  No one would see a clear market need for such a thing - after all, everyone uses AES, don't they?
On the other hand, we *do* have GPU's.  If we took some reasonable common subset of GPU functionality as a given ... how would we design a cipher that used it to attain very high performance?  (Note that this is the opposite of the efforts we've already seen to compute *existing* ciphers as quickly as possible in a GPU - and often in unusual configurations that work for attacks but don't help normal usage.)
                                                        -- Jerry

@_date: 2014-03-16 16:58:53
@_author: Jerry Leichter 
@_subject: [Cryptography] Client certificates as a defense against MITM 
In this algorithm, it's not just a mistake to avoid - it's absolutely fundamental.  The client and server can be sure that the client's message, if it arrives at the server, will only be accepted by the server it if was unmodified in transit.  And both can be sure that if the included information is found to be valid by the server, then no MITM attack is taking place and further secure communication is possible.  But if the server either doesn't receive, or can't validate, this client message, no further conclusions will be possible.  In particular, if the server tries to inform the client of the problem, it has no reason to believe the client will receive that message unmolested.  The only message it can, in some sense, send securely is a link shutdown.  That leaves the MITM in a position where to avoid that message getting through to the client, it must successfully imitate the server.  If it could do that, it wouldn't have had to use a MITM attack.
                                                        -- Jerry

@_date: 2014-03-17 06:51:26
@_author: Jerry Leichter 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
That contention is new to me; reference?  I've seen nothing to indicate RSA doesn't continue to scale in strength exactly as it always has as far out as you'd like to go.  It's just that even at 2048 it's becoming rather resource-heavy.
As Martin Minow, a long-departed friend of mine, put it years ago:  Virtual memory is fine if you want to do virtual work.
Server farms are built of real hardware that someone has to buy with real money.  They may be a very larger resource, but they are not an infinite resource.
Besides ... (a) they don't do anything to improve single-stream performance; (b) they are of no help on the client side.
                                                        -- Jerry

@_date: 2014-03-17 10:56:11
@_author: Jerry Leichter 
@_subject: [Cryptography] Client certificates as a defense against MITM 
An HTML cookie isn't bound to the end-to-end connection context.  A MITM simply passes it through.  The signed information I'm suggesting the client send *is* bound to that context, and isn't subject to this trivial vulnerability.
One relatively non-intrusive way to introduce this, in fact, might be to allow for "dynamic cookies":  Cookies whose content is computed inside the browser.  (This parallels the transition from the original view of servers sending the contents of static files to servers sending content that's created on the fly.)  If the HTTP client cookie contained a value computed from the end-point connection information, a MITM who merely passed it along would reveal its presence; but it couldn't compute a credible replacement with the client's private key.  (In fact, since the cookie will be passed along with every request, you might as well compute it based on the full end-point connection history.  Then, if anyone somehow slips a message into the stream and gets the client to accept it, the server will know as soon as it receives the next request.  The real problem here is that the party that really needs to know this is the *client*, and there's no way for the server to send back a message that the client can rely on seeing.  Perhaps using an out-of-band channel is what's needed - if the server sees a problem, it sends you a text message.)
Absolutely.  We knew CA's were a weak link - one that was getting weaker - even before Snowdownia.
CA's (and PKI) remain the only solution proposed for the "how do I form a trustworthy connection to amazon.com if I've never been there before."  We need to do everything we can to strengthen them in that role.  (Certificate pinning, various means for checking the history and visibility of their certs, securing DNS, etc.)  But they inherently remain centers of trust that are not and cannot be trustworthy in general public usage.  Wherever we don't absolutely need them, we should avoid relying on them.
                                                        -- Jerry

@_date: 2014-03-17 18:16:28
@_author: Jerry Leichter 
@_subject: [Cryptography] Client certificates as a defense against MITM 
Yup.  The same idea.  The details are a bit different (and their approach may be more general) but fundamentally I'm proposing the same thing they did.
I guess the time comes when some ideas are just ready to be found.
Thanks for the reference.
                                                        -- Jerry

@_date: 2014-03-18 06:56:24
@_author: Jerry Leichter 
@_subject: [Cryptography] Better than passwords and cookies 
Unfortunately this is just a small subset of the original OBC proposal.  All the MITM protection has been stripped out.  The certs here are used to replace traditional login mechanisms (something I was skeptical of for the certs I was suggesting), and the authentication is bound to the identities of the two ends, not to the particular endpoint-to-endpoint connection rendering it useless as protection against MITM attacks.
Of course, if the HOBA mechanism *were* to be standardized, its certs might be useful for building a MITM protection mechanism.  But that would require an additional layer of standardization.
Why did the OBC stuff die out?  Inertia, lack of interest, or objections from somewhere?
                                                        -- Jerry

@_date: 2014-03-18 07:19:02
@_author: Jerry Leichter 
@_subject: [Cryptography] How to build trust in crypto (was:recommending 
The challenge is this:
...for some suitably weak notion of all the words involved.  If I do a simple unauthenticated D-H key exchange with someone out on the net, we (the two parties to that exchange) who have never met before (will) have successfully created a secure channel between us.  Of course, neither of us has any idea *who* we set up that secure channel with, which was the problem to begin with.  (No MITM is possible here:  The secure channel is from the initiator to whoever happens to receive the packets unmolested.  You can't say that's the "wrong" party because, well, they are strangers to each other.  And you can't complain that the MITM forms some other connection and passes information along - even a fully authenticated party at the other end is perfectly free to do that.)
There's classic movie the details of which I don't remember at all, but with the following setup:  An American woman traveling somewhere in Europe is approached and asked to "help her country".  She's told to go the American embassy at such and such a time and ask for Mr. Such and So.  She does, is lead to his office.  They talk; he's a charming fellow, and asks her to do some kind of espionage work for the US.  He'll be her contact.  She agrees.
As the movie proceeds, she does what's asked.  But eventually things go awry and she finds herself in deeper and deeper trouble.
Somewhere along the way, we find ourselves back the the US Embassy, at the office of her contact.  Except that the person sitting at the secretary's desk is different; some details of the room are different - and eventually we see that the person in the office isn't the one we met initially.  The sign on the door that said this is the office of someone who's involved with espionage (or might plausibly be) is shown now to say this guy has some position like representative to the king's goats.  It's revealed that he and his secretary go out for lunch every day at the same time; they are always out at the time of the initial meeting.  The people our heroine met with simply set themselves up in the office.
So the problem with your proposal is hardly new.  :-)
                                                        -- Jerry

@_date: 2014-03-18 22:23:46
@_author: Jerry Leichter 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
TCP - and more particularly bad TCP flow/congestion control mechanisms - are well on their way to rendering the Internet sclerotic.  It's an unwritten (though there are written hints) rule that any proposal for a new Internet protocol will not be accepted by the IETF unless it's "TCP friendly" - code for "doesn't screw up even old and stupid TCP flow control mechanisms".  While the rule is more or less political, there is significant reality behind it, as the experience of those who've violated "TCP-friendliness" has shown:  The problem is often not so much that the new protocol interferes with TCP as that TCP interferes with the new protocol.  Since so many components are optimized to make TCP as it currently exists work well, it should not be surprising that the new guys always get beaten up.
It's a long and messy story, with techno-political biases way, way back (when it was decided that packet loses were good enough to do flow control and that doing anything more was unnecessary complexity).  There are old, old papers showing that this would eventually cause all kinds of instabilities and other problems, but during the long golden era when link speeds and installed capacity kept way ahead of demand they were held at bay.  We'd fallen off the room, but as we passed window after window, we could keep saying "we're fine so far".  That golden era ended a couple of years back, and the old predictions of an impending crash into the sidewalk are starting to seem a bit more real.
Anyway:  Crafting a new protocol to run alongside TCP (TCP will be around indefinitely, and no matter what will be the dominant form of traffic for years to come) is an extremely difficult problem.  No solutions are apparent right now.  CurveCP was an elegant attempt, and the designers even tried to account for interactions with TCP; but apparently they failed.  CurveCP on a "pure" network, without a dominant presence of TCP - probably very nice.  But no one's going to run it that way.
                                                        -- Jerry

@_date: 2014-03-19 14:06:58
@_author: Jerry Leichter 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
The classic paper on this is:
Goldwasser, S., Micali, S., and Tong, P. Why and How to Establish a Private Code on a Public Network. Proceedings of the 23rd Annual Symposium on Foundations of Computer Science (FOCS'82), Chicago, Illinois, pages 134-144, October 1982
Unfortunately, it doesn't appear to be available on-line anywhere.  Summary (of just part if the paper - it's been many years since I read it):  The ability for anyone to send a message using the same public key is a significant weakness, which can be leveraged.  An attack outlined in the paper:  Suppose I agree to a protocol in which answers to yes/no questions are determined by the bottom bit of a random encrypted message.  You've captured a message encrypted with my public key and wish to decrypt it.  You notice that I often send a message to a friend; he replies shortly after, and then I either leave my office and join him for lunch, or stay in my office and don't dine with him that day.  So you guess that I send out a "how about lunch" message, and you respond with a yes/no response.
So the next time you see me send out an apparent "how about lunch?" message, you send me the message you captured, and watch what I do.  If I promptly leave for lunch, you know you "said" Yes, so the bottom bit of the encrypted message is 1; otherwise it's 0.  You've turned my lunch habits into an oracle for the bottom bit of any message encrypted with my public key.
What's the good of a one-bit oracle?  In the case of RSA, it completely destroys security:  Using the fact that RSA is multiplicative, if you know the bottom bit of the encrypted message, it's possible to "shift the message right" by one bit, discarding the bit you already know.  Now repeat until you've read off the message, one bit at a time.
No, this isn't a practical attack against a realistic protocol.  The paper is a theory paper, and what it's showing is that even under the assumption the RSA is "secure" in the sense that there's no practical way, as an abstract problem, to invert the encryption, unless you design your protocols carefully, you can be attacked.  Over the intervening years we've found many more attacks confirming this general fact.
In general, asymmetric encryption is very brittle.  It's not just performance that argues against its use for encrypting semantically meaningful information!
I haven't looked at the cryptographic details of curveCP.  DJB certainly knows about these results, and I assume any protocol he designs would avoid the pitfalls.  But I would advise anyone who thinks "Oh, let's use ECC throughout and stop all this nonsense with session keys" to proceed with extreme caution.
                                                        -- Jerry

@_date: 2014-03-19 22:11:48
@_author: Jerry Leichter 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Hmm.  A day or so back, I sent one message explaining why MITM was a meaningless concept in one usage of DH, and got the response "but DH is vulnerable to MITM".  Here, I described an attack explaining why you want to use asymmetric crypto such as RSA to establish a key for a symmetric crypto system, rather than directly encrypting semantically meaningful information ... and the response is that the attack isn't relevant because we use RSA to encrypt keys for symmetric cryptosystems.
Either my writing has become very unclear, or some of my correspondents aren't bothering to read what I wrote before responding with canned repetitions of well-known cryptographic principles, whether they are relevant to the situation at hand or not.
So for this one, let me repeat myself:  The Goldwasser/Micali/Tong paper shows why you should not encrypt semantically meaningful messages using an asymmetric key system.  The fact that anyone can send a message using the same public key means it's possible to turn the recipient into an oracle for information about the message, which may leak enough information to allow the message to be decrypted.  That's why, *independent of performance considerations*, it's best to use an asymmetric key system to establish keying information for a symmetric key system.  When used this way, the data sent is random and not directly meaningful to the recipient, so it's harder (probably not impossible, but much harder) to turn the recipient into an oracle.
                                                        -- Jerry

@_date: 2014-03-19 22:22:54
@_author: Jerry Leichter 
@_subject: [Cryptography] Italians invent SHA-7 in 2011 
I'll bet it supports million-bit keys - so much more secure than the wimpy 256-bit keys that AES tops out at.
                                                        -- Jerry :-)

@_date: 2014-03-21 16:10:33
@_author: Jerry Leichter 
@_subject: [Cryptography] Tamper-evident cryptographic systems 
So Snowden revealed the extent to which the Internet had been infiltrated and compromised, and now many are rushing off to form a "more secure" Internet.  Imagine it's five years from now.  TLS with PFS is essentially universal for all Web services.  The big guys have strongly encrypted all their internal links.  Data at rest is encrypted.  Etc.
Are we really secure?  Or are we just waiting for the next Snowden to reveal another layer of infiltration that we never suspected - just as we never suspected things like, say, the Quantum XXX series of attacks?  Is there some way we can reasonably raise our level of confidence that certain things are *not* going on?
Let's bound those "things" a bit better.  It's unrealistic to think that targeted attacks against even hundreds of thousands of individuals can be stopped.  There are too many ways to compromise hardware, software, people, environments.  (Sure, your computer is completely secure, but video cameras are pointed at you, your screen, your keyboard....)  So let's limit it to *mass* surveillance.
Perry resurrected this list after a long hiatus with the challenge to develop new systems secure again such attacks.  I think we have some idea how to accomplish this kind of thing.  But ... one thing we really must learn from the Snowden experience is that even apparently secure systems can be attacked by a well-funded, motivated attacker.  You can't just introduce a new system and walk away saying "it's done".  You also need an active defense.
So ... how might one build "tamper evident cryptographic systems"?  Are there collections of sensitive signals of possible attacks that can be tracked to provide an early warning - even if no individual signal has a sufficiently low false positive/false negative rate?  Are there ways to construct "honey pots" that will attract attackers to systems specially configured to notice they are there?
Clearly, thinking about this kind of thing is part of classic espionage tradecraft - see the classic stories about England allowing German bombers to do major damage to avoid leaking the fact that Enigma had been broken.  I'm sure NSA does something to watch for success in attacks against its systems.  (Of course, Snowden is a particularly spectacular example of a failure of such measures.)
I can think of some very simple examples of this kind of thing, but nothing really broad or effective.  This seems to be an under-explored area in general. I recall seeing some work on forgery-revealing signature systems a couple of years back.  (The basic idea was that each time you sign, you modify your signature in one a number of possible ways chosen at random.  All the choices produce a valid "next" signature.  If someone steals you signature, his random choices won't match yours, and the two "signature streams" will soon diverge.  The existence of two distinct signature streams for a given signature is detectable.)
                                                        -- Jerry

@_date: 2014-03-23 17:36:25
@_author: Jerry Leichter 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
I know it's going to sound like a sell-out, but ... a combination of the things everyone one of us repeats argues that you go with NIST's choice.
1.  There is no such thing as absolute security.  We know how to break many algorithms, and we have some algorithms that cannot be broken using any of the techniques we know.  But that's all we can say.  We simply do not have any kind of theory that can tell us that an algorithm is secure against attacks we haven't considered yet.  See below.
2.  Security is about mitigating risk.  It's not an absolute.
Given 1, there is not *absolute* way of choosing a cryptographic algorithm; it's always about make the best-supported guess available.  Whatever you think about NIST and any possible connection it has with NSA, the NIST *contests* have drawn some of the best minds in public cryptography, and have been held out in the open with visible participation from many other of the best minds.  DJB is very good, but he's not uniquely so.
Given 2, you have to ask the question:  What is my exposure if I use algorithm X, and it turns out to fail?  (This is for any particular value of "my".)  The fact is, it's much easier to obtain insurance (actual monetary insurance coverage, or any moral equivalent thereof - someone to cover your ass and make you hole if what you chose goes wrong) for something endorsed by NIST than for something with no official endorsement.
There's theoretical science and there's practical engineering.  Practical engineering, unlike theoretical science, is about money and practical usefulness (among other things).  These argue for going with NIST.
Thinking about one element of this more quantitatively:  What's the longest time any algorithm has remained secure against all significant published attacks?
There are no really strong analytic attacks against DES to this day, though its key is way too small by today's standards (which counts as an attack).  It's hard to exactly pin down how long DES could reasonably be considered secure, but if we go from initial publication in 1977 through the EFF DES cracker in 1998, we get about a 20 year lifetime.
AES is still going strong at 13 years from initial official publication in 2001.  (If you start the clock at entry in the competition, you get a little extra time, but it makes it hard to compare algorithms uniformly.) Triple-DES was published in 1998 and still appears strong at 16 years.  (Curiously, the only specific weakness Triple-DES is known to cure is brute force attack - but Rogoway showed years ago the DES-X - DES with simple XOR pre- and post-whitening - is about as strong against brute force attack as Triple-DES.  But no one seems to use it, while complaining that Triple-DES is too slow!)
There are plenty of other potential contenders (Blowfish, RC5), though the great grand-daddy appears to be IDEA:  Initial patent proposal in 1990, full patent proposal in 1991, no known attacks to date.  That puts it at 24 years or so.
It's impossible to compare the level of cryptanalytic attack brought to bear against widely used standards and against algorithms no one is using; we'll just assume the academic glory of breaking IDEA or DES would match that of breaking AES (though the monetary rewards would be much lower).
We so far have *absolutely no evidence* of a block cipher surviving more than 25 years of public cryptanalytic advances.  A prudent guess would place a lifetime in that range.
Most widely use stream ciphers have been ad hoc and have proved to be quite weak.  RC4 is the outlier.  It was designed in 1987 but the design was secret until 1994; so it's not entirely clear when to start the clock running.  One can also debate whether to stop the clock at the recent (last year or so) demonstration of practical attacks, or the 2000 work by Fluhrer and McGrew that found the weaknesses that were eventually turned into attacks.  Make your own estimate from this about a prudent guess for a lifetime.
MD5 was published in 1992 and was considered broken by 2004 - 12 years.  SHA-1 was published in 1995 but by 2005 - 10 years later - was considered to be weak.  An almost-practical attack was published in 2011.  SHA-2 was published in 2001 but was under suspicion by 2012 or so - 13 years.  Based on this history, it would be prudent to assume a maximum practical lifetime for a cryptographic hash function to be around 15 years.
In contrast, RSA was first published in 1977 and remains free of known general attacks 37 years later - though we've had to increase key sizes faster than was predicted a couple of times.  ECC was first proposed in 1985.  It took a while for the details to be worked out - widespread use didn't begin until maybe 2004.  It's difficult to compare ECC directly to RSA since unlike RSA, it's a family of systems, one per curve - and some of the initial curves were weak.
These numbers are actually quite shockingly low when you think about them.  There is certainly a degree of "infant mortality" in new cryptographic algorithms - you certainly want them to "bake in" for, say, 3 years of public visibility before you rely on them.  That leaves a practical lifetime for block ciphers of about 22 years, for hash functions of about 12 years.  That's long relative to laptop or server CPU lifetimes, but quite short relative to protocol lifetimes - and very uncomfortable relative to embedded CPU lifetimes.
                                                        -- Jerry

@_date: 2014-03-24 10:36:08
@_author: Jerry Leichter 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
It's easy to make the "batting average" go down - there are tons of failed proposals; some of them fail embarrassingly late.  What I was trying to do is look at the lifetimes of "the best of the best".
Note that the cloud over SHA-2 has lifted, so it has the opportunity to extend its run.  That leaves it, at 15 years, the champion of cryptographic hash functions.
BTW, another embarrassing thing to note is that having some tweaking done by the NSA is correlated with longevity - at least for block ciphers and cryptographic hash functions.  You can interpret this in many ways, but it certainly hints that at least as of 15 or so years ago, the NSA appeared to still have some "secret sauce" that the open community did not.  (In the case of asymmetric crypto, and especially signatures, they seem to have used their abilities to damage the standardization process.  As far as we can tell, they did the opposite for symmetric crypto and hash functions.  Different time?  Different organizations with the NSA?  Something we still don't even have a clue about concerning symmetric crypto and hash functions?  Impossible to say at this point.)
                                                        -- Jerry

@_date: 2014-03-24 14:09:41
@_author: Jerry Leichter 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
There's a paper we mentioned here quite a way back that showed that if you're looking for collision resistance, using multiple hashes in parallel - i.e., compute and all k hashes and concatenate to produce a "super hash" - is only minimally stronger than the strongest of the hashes you started with.  (The paper proves this counter-intuitive result based on the - counterintuitive - ease of finding multi-collisions once you can find collisions.)
If you're looking to combine block ciphers, the simplest approach is probably to use counter mode for all of them and just XOR with all the ciphers.  This is certainly as strong as the strongest individual cipher, assuming they aren't correlated in some way (e.g., if I can toss a cipher into your pool, I'll choose the strongest one you've put in, "canceling" it.  One can imagine defenses, but they're all going to require some way of enforcing some notion of independence.)  I don't recall seeing any proposed cipher combination method that's provably *strictly stronger* than all its constituents.
How one constructs an authentication mode to go with the combined counter mode is a question to which I don't know the answer.
Maybe.  And maybe we should have a protocol for those super-secure million-bit-key ciphers we keep hearing about. :-(
Just because people demand it doesn't mean it's a good idea.  First you need to find a piggy-backing method that has meaningful security benefits.  Then we can talk about a protocol.
                                                        -- Jerry

@_date: 2014-03-24 19:20:35
@_author: Jerry Leichter 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
I pulled the data from Wikipedia, the reference of choice of all lazy writers.  :-)  It refers to the draft of FIPS 180-2 as being published in 2001, but gives no further details.
SHA-2 is actually patented (  The application for the patent was filed on March 5, 2001.
One of the things we in computer science in general have been very bad at is understanding, or even keeping track of, our own history.
While it's hardly ideal, the best way we have to estimate the longevity of our artifacts it to look at the history of related artifacts.  CS tends to view every new advance as ab initio - all that old crud is now obsolete and not worth thinking about.  I was quite shocked at myself for never having thought about how long cryptosystems past had actually survived in the real world.  I probably could not have given a reasonable quick estimate for any of these.
Of course, the push for "standards" - and particular the push for *one* standard algorithm of each class - tends to push those that didn't make it to the top out of the picture.  That makes it even harder to learn from the mistakes - and successes - of the past.
I'd love to see your history published.  It will be a valuable reference.
                                                        -- Jerry

@_date: 2014-03-24 19:29:57
@_author: Jerry Leichter 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
The same attack (and resulting complexity) is reported for AES - you get about two bits for all of AES-128, 192, and 256.  I found a reference to a paper -  - applying the biclique attack to a Korean standard algorithm known as ARIA-256.  (It gets about 1 bit of advantage with 2^80 chosen plaintexts - hardly a realistic attack right now.)
Who knows, the biclique technique may get broadened into a whole new class of attacks on block ciphers, to add to our toolkit along with differential and linear cryptanalysis and a few lesser-known ones.  Or it may prove unable to go beyond the tiny advantages it can get today.  It should stand as a warning, though, that new analytic techniques are always "just around the corner".
                                                        -- Jerry

@_date: 2014-03-26 05:41:34
@_author: Jerry Leichter 
@_subject: [Cryptography] The role of the IETF in security of the 
Beyond that, much of the complaint has a "shoot the messenger" quality to it.  Official MITM in the style described by the RFC is commonplace today in many business networks and in some national networks.  Right now, it operates entirely in the shadows, with the proxy's CA inserted into browsers as part of standard corporate configuration procedures or by more nefarious means.
Approval of the RFC wouldn't make this practice any more common than it already is.  Actual *conformance* with the RFC would at least make the presence of the proxy visible and allow users to say no to it.  (Of course, the result of saying no may be denial of network access.)
In practice, looking at how such proxies have been used so far, I think we're unlikely to see much conformance with the notification and end-user control mechanisms the RFC requires.  Those who put proxies in place generally don't want them noticed - they don't want to answer questions about them.  To me, the whole exercise seems pointless.
                                                        -- Jerry

@_date: 2014-03-26 18:30:35
@_author: Jerry Leichter 
@_subject: [Cryptography] Dark Mail Alliance specs? 
...which is the way things are in essentially all large-scale settings.  It's easy to say that "GE" - to pick the name of a large company at random - owns all the machines in all its data centers, but in fact the "ownership" is complicated, because machines may be assigned for use by particular groups which have their own security policies, which somehow have to mesh with the policies of their super-organizations all the way up to GE the corporation.  And even that ignores cross-cutting concerns like corporate mandates that an auditing organization has access.
And what if the machines are on GE's property but are actually leased from someone else?
You definition of "secure", while it sounds nice, just doesn't cover much beyond personally owned machines.
There is absolutely no reason why the owner's and user's security interests will be the same.  They almost never will be.  *But this is exactly why we have contract law.*
If I rent a car, the car owner retains ownership, and from his point of view, the best thing would be for the car to sit in a well-guarded, climate-controlled parking lot 24x7.  But I'm not about to pay to rent a car under terms that satisfy the car owner's best interests; I pay in order to satisfy *my* best interests.  So I and the car owner agree on what I'm allowed to do ("drive the car in this and the two adjacent states; carry insurance that will make the owner whole should I wreck the car), and what I'm not allowed to do (drive in the desert; do damage to the car and not pay to get it fixed).  To compensate for the car owner not getting his best interests served, he gets some money.
It's no different with cloud servers.  Either you can agree on a contract that satisfies both parties along all the relevant dimensions, including security; or you can't.  Yes, you have to place additional trust that the provider will actually live up to that contract - but that's what legal systems are all about.                                                            -- Jerry

@_date: 2014-03-27 09:40:37
@_author: Jerry Leichter 
@_subject: [Cryptography] Dark Mail Alliance specs? 
And the alternative is ... what, exactly?
All security ultimately depends on physical security, and governments are defined by their monopoly on force.
                                                        -- Jerry

@_date: 2014-03-29 10:23:45
@_author: Jerry Leichter 
@_subject: [Cryptography] The ultimate physical limits of privacy 
I suspect much of this is based on  Esther H?nggi's ScD dissertation at ETH on "Device-Independent Quantum Key Distribution".  I've only glanced at it - tons of QM math that at this point is way beyond me.  Where the "randomness amplification" stuff comes in is interesting:  Every time quantum cryptography has been discussed on this list in the past, many have rejected it as uninteresting because it provides no way to assure authentication.  Well, recent work in the field deals with this quite explicitly.  The view taken in this dissertation is that authentication requires pre-sharing of some random bits between the two parties - the analogue of the old spy novel trick of cutting a dollar bill in half in some ragged line and giving each half to one of two parties that will then be able to authenticate to each other by matching the halves.  That initial randomness is then amplified securely and with authentication by the quantum protocol.
Another element here is that you assume a design for the system that delivers correlated "pre-bits" to the two parties, which then select some out as their random bits.  It turns out there are results in QM that guarantee the boxes that hand you the "pre-bits" cannot leak their information due to QM limits, like the No Cloning theorem, which says that you can't make an exact copy of a quantum state without destroying the original; and limits on correlation, which show that while you can have two particles whose states are correlated - the basis of quantum crypto - it's impossible to have three or more particles correlated.  So you don't need to trust whoever gave you the boxes that produces the "pre-bits" - you can *test* that it provides the necessary correlated particles, and then be sure it can't leak or record their states.  (The same arguments, working backward, indicate that the creator of the boxes couldn't have pre-recorded the outputs they would later deliver.)  These arguments are all about the quantum states themselves, not the details of how the boxes achieve them - going way beyond earlier work which focused on realizing particular systems.
The latest work - as noted, behind an expensive paywall - is likely more a matter of thinking through the broader implications of this work.  (If you're wondering what free will has to do with this, download Conway's talks on the Free Will Theorem  - no advanced math or physics needed, great listening - as we said way back when, it'll blow your mind.
                                                        -- Jerry

@_date: 2014-03-30 10:26:13
@_author: Jerry Leichter 
@_subject: [Cryptography] OpenPGP and trust 
You still haven't done what Peter Fairbrother quite rightly suggested:  Write down exactly what you want you system to allow and disallow, and other details like how large (roughly - factor of 10) how many players (services needing authentication, people) are involved.
Without that, you're just tossing around random crypto-related words.
                                                        -- Jerry

@_date: 2014-03-31 21:27:05
@_author: Jerry Leichter 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve numbers 
Exactly backwards.
Instead, compose a message of the following form:  "The constants are computed by taking the closing prices of the following 100 stocks, as published in  on , feed in the order given into the following procedure: ..."  Publicly commit to this message - or, better, to the one-way hash of this message - before the magic date arrives.
There are enough values published that there's always the possibility that you rooted around until you found an appropriate one.  By committing to a value that you could not have known in advance - there's tons of data you can use for this purpose - you eliminate that possibility, which will always be present if you use pre-existing values.
                                                        -- Jerry

@_date: 2014-05-01 06:33:47
@_author: Jerry Leichter 
@_subject: [Cryptography] GCC bug 30475 (was Re: bounded pointers in C) 
"Some" implementations?  This is the original x86 FPU semantics, the ones present on every 32-bit x86 CPU ever built.  I haven't had any reason to keep up, and I don't know if the most recent FP instruction sets still do the same thing; I suspect they might very well.
This behavior caused incredible tumult in the early days of Java.  Java defined FP arithmetic "to the bit level", insisting that every implementation produce exactly the same bit-level results as the reference implementation on SPARC hardware - independently definable in terms of the IEEE standard arithmetic for float and double.  x86's used in a reasonable way produced "wrong" results - they're "too accurate" in some cases, and can avoid overflow and produce an exact result in other cases where an implementation that sticks to float and double cannot.  The best way anyone ever found to avoid that was to store results into memory after every FP operation, then load them back.  Needless to say, this completely killed performance.  Eventually - under pressure from those who actually understood the numerics - the Java spec was modified to allow "more accurate" results.  (You can still get the original semantics if you use the strictfp modifier.)
Another example of the same phenomenon is "fused multiply and add" - a single three-operand instruction that computes A*B+C.  (This is an extremely common operation in many FP programs - it's the basic computation step of an inner product AKA a projection of a vector on another, and hence also of a matrix multiplication).  One can implement this with both better accuracy and increased performance than can be achieved by doing a multiply and then an add.  IBM's Power processors have this instruction, and their compilers recognize when it can be used.
If you get into questions of parallelism and memory models, all modern machines have the potential for unexpected behavior.  One classic example:
Initially both x and y are 0
Thread 1	Thread 2
x = 1;          print x;
y = 1;          print y;
What can thread 2 print?  There are three obvious possibilities:
0 0     - Thread 2 completes before thread 1
1 1     - Thread 1 completes before thread 2
1 0     - Thread 1 executes its first statement, then thread 2 completes
But 0 1 is impossible.  Except ... due to everything from compiler optimizations (OK, not in *this* trivial example, but in more complex examples that come down to the same pattern) to memory buffering effects, it *is* possible!
We long, long ago got to the point where performance and "obvious" semantics parted ways.  C deliberately makes the results of certain operations undefined since otherwise, as with the bit-by-bit equivalence of FP operations in early Java, its compilers would be forced to produce unacceptably bad performance in certain real-life situations.  That's just the way things are, and they are not likely to change in the foreseeable future.
As I've said before, what's wrong with C is *not* that certain operations have undefined results, and *not* that compilers do things you don't expect if you run into those corner cases (though I will agree that some actions, like GCC's, violate the old C notion of "acting as someone with knowledge of the machine architecture would expect").  What's really missing is a standard way to detect some important conditions, like overflow.  Every architecture *can* do this, even if on some it might be more expensive than on others.  What's needed is a standardized API like:
sum_overflows(a,b)  - true if computing a+b overflows
diff_overflows(a,b) ...
There might even be an optional (presence checkable by a macro)
These would come from a standard header, but could be implemented in all kinds of ways, from simple range tests when used with compilers that know they won't optimize them away to something involving pragmas to disable inappropriate optimizations for the computation to built-ins.  It's actually a bit of a mystery to me why this has never made it into C - it's and obvious thing to have, matches the C model of providing direct access to machine capabilities, and is probably fairly easy on implementers:  "Dumb" compiler implementations will leave the standard test expressions alone so won't have to do much work; "smart" optimizers should have enough mechanism to add this stuff cheaply.
                                                        -- Jerry

@_date: 2014-05-01 12:38:42
@_author: Jerry Leichter 
@_subject: [Cryptography] GCC bug 30475 (was Re: bounded pointers in  C) 
The C Standards, from the very first, have had two categories here:  "Implementation Defined" and "Undefined".  "Implementation Defined" means that each implementation makes its own choice, but nominally it has to document it.  The size of an int is an example of an "Implementation Defined" property.  "Undefined" properties are ... undefined.  The reason overflow handling is Undefined, rather than Implementation Defined, is that it *might* cause a trap - and there is no such concept in the C Standard.  What happens next depends on the OS, the library, the surrounding environment ... there's really no way to pin it down.
Not that *nothing says that a particular implementation can't choose to define what happens in the case of an integer overflow*.  If you're working with such an implementation, you can write your code based on that definition.  But your code is still non-portable.
No.  It allows three mappings from a set of bits to a mathematical value.  Those arithmetic operations that "don't overflow" produce sets of bits whose value, interpreted as given by the implementation, is the correct mathematical result. Otherwise, the result is undefined.
The compiler always knows the target architecture.  But that result of an expression like x > x + 1 is undefined if x + 1 overflows.  Again, it might trap.
But the C Standard deliberately does *not* assume well-defined forms of arithmetic - because some hardware can't guarantee it.
A classic - now very, very dated - example of the complexities:  The CDC 6000 series had 60-bit registers.  Integer addition and subtraction operated on the the full 60 bits.  Integer multiplication only operated on the bottom 48 bits.  So now what does "overflow" mean?  You could decide that in is really 48 bits, artificially cap each addition or subtraction at 48 bits - some extra execution cost, some loss in useful functionality; or you could do a software implementation of a 60-bit multiply - *very* big execution cost.
The hardware had no integer divide instruction.  The usual approach was to convert the integer to float, divide, then convert back.  This also limited the effective size of an integer participant in a divide to the mantissa size of a double - which was also 48 bits.  (In fact, the 48 bit limit for integer multiplication came from the hardware re-using the FP multiply unit to do integer multiplies.)
(Oh, and to break two other assumptions that people often make about the C execution model:  Pointers were 18 bits, but pointed to a word, not a byte.  The hardware itself had no representation for a pointer to anything smaller than a word.)
                                                        -- Jerry

@_date: 2014-05-01 12:50:26
@_author: Jerry Leichter 
@_subject: [Cryptography] GCC bug 30475 (was Re: bounded pointers in 	C) 
I can't speak to .NET, but the Java Standard specifically says:  "The integral types are byte, short, int, and long, whose values are 8-bit, 16-bit, 32-bit and 64-bit signed two's-complement integers, respectively....".  Good luck producing an efficient - or even reasonable - implementation of Java on a machine which uses 1's complement or sign-and-value as its native representation.  These do still exist, and can be important, especially in embedded applications and some "supercomputer" applications.
As I mentioned before, Java originally specified FP arithmetic this tightly - and as a result compliant, reasonably efficient implementations were *impossible* on tons of hardware.  The Java community eventually backed off because this was an issue for so much of what they decided was the Java audience; but when it comes to other forms of integer arithmetic, they've effectively written off multiple classes of machines.
                                                        -- Jerry

@_date: 2014-05-01 15:45:53
@_author: Jerry Leichter 
@_subject: [Cryptography] GCC bug 30475 (was Re: bounded pointers in 	C) 
That just means you can't write "portable, Standard-compliant" code on that hardware.  An implementation can specify any semantics it likes for things the Standard chooses to leave undefined.  A good implementation might include things like:
 SIGNED_OVERFLOW_IS_2S_COMPLEMENT
in some include file, and you could then test for it.
An implementation clearly can't constrain whether *other* implementations  this, in an include file with exactly the same name, to mean something else - but in practice this isn't likely.  (Many, many years ago, I knew someone who wrote FORTRAN programs that in case of a severe error did CALL ABORT.  There was no standard ABORT subroutine - but his argument was:  If there's no ABORT in the library, the program will die at run time - FORTRAN and linkers of the day didn't check earlier - with a call to an undefined entry point.  If there *is* a subroutine named ABORT in the library - what else could it possibly do?)
With luck and some community involvement, a include file of this sort, with a common, agreed-upon set of macros, could become widely available.  And then it might eventually even get into the next version of the C Standard.
On the other hand, no reasonable Java-compliant implementation is possible on a machine that only provides (say) 1's complement integers, or non-IEEE floats.
                                                        -- Jerry

@_date: 2014-05-02 13:46:57
@_author: Jerry Leichter 
@_subject: [Cryptography] GCC bug 30475 (was Re: bounded pointers in  C) 
As I pointed out earlier, "implementation defined" requires that the result fit into the abstract machine model used to define C's semantics.  That model has no notion of anything like a trap.  So "trap on overflow" cannot be the "defined" behavior of any implementation.
Of course, a particular implementation might well explain what it means to "trap" and then go ahead and then go on to say than an overflow causes a trap.  But then an implementation could define a type that does ternary arithmetic if it so chose.  The Standard does and and cannot define, or limit, behavior outside of the domain the Standard covers.
                                                        -- Jerry

@_date: 2014-05-02 14:34:48
@_author: Jerry Leichter 
@_subject: [Cryptography] GCC bug 30475 (was Re: bounded pointers in 	C) 
Oh, you're being way too modest.  x - 1 might underflow.  It's pretty much impossible to write an arithmetic operation that can be *guaranteed* not to cause undefined behavior for some inputs.  Combining values with 0 is safe - if x is an int, 0*x, 0+x, and x-0 have defined behavior; so does 1*x or x/1.  But you're never far from a cliff.  After:
int y = x + 0;
x == y is true, but memcmp(&x, &y, sizeof(int)) is undefined!  If the underlying arithmetic is 1's complement or sign and magnitude, there may - but need not necessarily - be two representations for 0, often written as +0 and -0; and the results of any particular operation involving -0 that produces a mathematically zero result may or may not be "normalized" to +0.  (Actually, to be accurate the result of memcmp() is undefined for two distinct reasons:  Not only might there be a -0 with a different representation, but an int could contain padding bits that don't contribute to its numeric value.)
You can rant about this all you like, but if you want to program (a) close to the machine - i.e., have efficient access to all machine facilities; and (b) write portable code, these kinds of tradeoffs are inevitable.
Before C was invented, you pretty much had to choose portability or performance.  C has done a remarkable job of providing high performance along with an extraordinary amount of portability.  Take a look at the machines on which NetBSD, written almost entirely in C, manages to run.
All this whining about undefined aspects of the language is just that - pointless whining.  The C Standard pins down as much as it can while still managing to efficiently support even some very obscure and oddball (but important to some constituents) architectures.
As I've said over and over on this thread:  Every implementation is free to define the "undefined" aspects of the Standard for itself.  In fact, simply supporting Posix interfaces pins down a number of them (the result of 1/0, for example).  It's perfectly legitimate to decide to restrict your code to environments where certain "undefined" aspects are pinned down in particular ways.  The fact is there are probably no practical, interesting pieces of code that are really fully portable.  It doesn't much bother anyone.
Yes, it would be a good thing if the C Standard provided some (probably optional) standardized ways to check whether operations have overflowed.  Apparently this isn't that big a deal for most coders or informal standards would have arisen years ago.  (stdint.h wasn't in the original C Standard, but there was a clear perceived need and multiple implementations of things kind of like it emerged, and eventually converged.  The converged version became the basis of stdint.h.)
Rather than whining about what the C Standard writers and compiler developers haven't given you ... develop some appropriate portable interfaces, implement them (in non-portable ways), and use them!  Maybe they'll even make it into the Standard some day.
                                                        -- Jerry

@_date: 2014-05-02 17:18:08
@_author: Jerry Leichter 
@_subject: [Cryptography] Need Debunking help 
One wonders if it's actually malware....
                                                        -- Jerry

@_date: 2014-05-02 17:58:58
@_author: Jerry Leichter 
@_subject: [Cryptography] "Covert Redirect" vulnerability in OAuth, OpenID 
Anyone looked at the details on this one?
                                                        -- Jerry

@_date: 2014-05-02 22:12:04
@_author: Jerry Leichter 
@_subject: [Cryptography] GCC bug 30475 
The C standards - except possibly C11 - were written at a time when FP on an x86 meant the old instruction sets.  "Very old" isn't quite as old as you make it out to be.  To get double precision FP, you need SSE2 - SSE was single precision only, and C FP arithmetic is usually double precision anyway.  SSE2 was introduced in Pentium 4 at the end of 2000.  (Even SSE only takes you back about a year from that.)  The older chips were the majority for quite some time after that, and you'd certainly want to support them for at least 10 years.  Not so long ago....
I've never heard anyone complaining that the old FP instructions did not have "proper" IEEE-754 behavior.  IEEE-754 allows for a variety of approaches and the 80-bit long double type has been there as long as the spec; Intel didn't invent it.  I've also never heard an numerical analyst complain about the extra precision delivered by the old instruction set.  There was *plenty* of complaining by those familiar with these matters about Java's original attempts to force particular bit-level results on FP computations - it was those who wanted to use Java for numerical computations who wanted it to allow the use of the x86 FP hardware.
C is rarely used for numerical work because it has no advantages over FORTRAN, which is what numerical analysts grew up with.  (Alan Perlis:  "I don't know what programming language numerical analysts will be using in YYYY, but it will be called FORTRAN."  I think YYYY was 2000 and well in the future when he first said this.)
C++, on the other hand, is widely used in certain numerical areas because one can build packages with types like arrays or polynomials that closely match the traditional syntax and semantics.  The one thing that held numerical computation back for a while was that compilers were given too much ability to do mathematically valid but numerically invalid transformations on FP code.  That was fixed by various modifications to the C Standard many years ago - mainly preventing the compiler from re-arranging the order implied by parentheses in FP computations.  The claim of "under-specification" is simply wrong.
Ah, but that's because you think the column headers are x y; they are actually
y x!  :-)
You are, of course, correct - x=1 y=0 is the result of an interleaving in which Thread 2 runs between the two assignments in Thread 1; it's x=0 y=1 that appears to be impossible.  To expand a bit on why it might happen, even on a machine with a very simple and straightforward memory model:  If after these two assignments, y isn't used again for a while but x is used heavily, a compiler might decide to keep flush y to memory but keep x in a register.  For sequential code, this is perfectly correct - you could never tell the difference.  But a parallel thread *can* see the ordering of the writes to memory.  If a language doesn't explicitly support parallelism, a compiler for it will be *expected* to do these kinds of optimizations.  (In fact, if you're going to insist that *all* stores to memory occur in program order, you're going to seriously cripple any decent register allocation algorithm.)
                                                        -- Jerry

@_date: 2014-05-09 08:08:27
@_author: Jerry Leichter 
@_subject: [Cryptography] Steganography by hiding in channel noise 
Full article at                                                         -- Jerry

@_date: 2014-05-09 09:55:03
@_author: Jerry Leichter 
@_subject: [Cryptography] How to lock registers with GCC? 
This used to be possible. I recall some experimental languages that generated C but somehow reserved a register. It may have been as simple as creating a fake target description for the back end that didn't know about one of the registers. Of course then you have to use assembler somehow to get at that register.  Or it could have been so long ago that the compiler on question actually implemented the C register storage class in a simple and predictable fashion. I can't imagine any way to do this in an even vaguely portable fashion unless you specifically designed you compiler to allow it - unlikely as the demand is so limited. It's also not clear this will help you much. Any time the OS deschedules your process, it will spill the registers into memory - in fact, into memory you know nothing about and can't do anything with. People have played around with encryption kernel algorithms that keep all state in registers, but this only seems possible with complete control of the hardware - assembler code within the OS running on non-virtualized hardware. Might make sense for embedded code, maybe a hardware crypto module.                                             -- Jerry

@_date: 2014-05-14 06:47:10
@_author: Jerry Leichter 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
For years now, "pre-shared keys" has been mainly a phrase of derision.  WPA with pre-shared keys is what unsophisticated end-users deploy - professionals use "enterprise-level" security.  Per-shared keys are fine for toys, but they "don't scale".  Pre-shared keys are 1940's cryptography.
I've argued here before that the solution to many asymmetric cryptosystem/PKI problems is *not to use asymmetric cryptosystems/PKI's*.  Yes, there are use cases where you need them.  But there are plenty where you don't.  VPN's are a great example:  Just how often do you need to connect to a VPN without having a trust relationship with whatever is behind that VPN and the opportunity to safely pre-share keys?
If door locks were designed along the same principles, you won't need to carry keys in your pocket - after all, there are so many doors you might need to unlock, so many keys you have to carry.  Just use a PKI system to establish who you are and who the house belongs to and let The System determine if the door should open for you.
It's certainly be simpler to have one, global, uniform, secure solution to all communications problems.  If only such a thing existed!  But it doesn't, and won't - not unless you're willing to ignore all the cracks and breaks and holes and agree that it's "best practice" even though "best practice" just doesn't work.
                                                        -- Jerry

@_date: 2014-05-15 07:11:46
@_author: Jerry Leichter 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
It's *impossible* to share them over public channels.  That's exactly the point!  Sharing a key over an *existing trusted encrypted connection that runs over a public channel* works fine, of course.
Sharing a public key over a public channel is meaningful only if you have a way to authenticate that it came from who you think it came from.  That's a problem just as hard as keeping the shared key secret as it's being transmitted.
That's false.  PFS has a mystique around it, but in fact it's trivial to produce in a symmetric system, as I showed on this list not long ago.  Suppose you and I share a secret initial key K.  The operations are then:
where H is a one-way hash function, and the operation of assigning to K irretrievably destroys the old value.
Practical implementations can adjust this.  The size of M, for example, is a security choice; it could be as small as a single block for a block cipher, or as large as you are comfortable with.  It's just necessary that the receive know where M ends.  (Ideally that information is inside the plaintext so an attacker can't even tell when the key changes.)
In practice, K is a session key, negotiated using a DH combined with a "negotiation shared secret" never used for anything else.
If you are concerned about key recovery attacks that work against some encrypted messages (and which would allow an attacker to "walk forward" from a recovered key), you can replace the key updates by:
Sending i along in the clear is safe and allows the receiver to decrypt messages out of order and recover from lost ones.
Here H is your favorite keyed one-way hash.  (MS remains unique per connection to avoid having to save i across connections.)
                                                        -- Jerry

@_date: 2014-05-17 07:05:41
@_author: Jerry Leichter 
@_subject: [Cryptography] [cryptography] Is it time for a revolution to 
This is an interesting assertion, much broader than cryptography but with a fundamental effect on it.
Prior to Unix and C, OS's and programming languages used a record model for I/O.  Historically, this undoubtedly arose in the US because the fundamental medium for I/O was initially punched cards.  Tape and disk formats emulated cards, and everything followed from there.  (You might think that fixed-length blocks on disks were another primary determinant, but in fact very early disks supported variable-length blocks, so probably were not as big a determinant.  Of course, tape drives also supported variable-length blocks.  In both cases, the general approach was to use a fixed maximum block length, and then "optimize" short blocks by using the variable-length mechanism to only write as much data as needed.  In Europe, cards were less common than paper tape, so some early machines and languages didn't have a strong block I/O bias; but the US completely dominated the industry in those days.)  The big advantage to record orientation was that you knew up front how large a buffer you needed to allocate.  Typical systems of the day did I/O directly into and out of user-provided buffers, for performance as well as total memory usage reasons.  Records with a fixed size, or a fairly small maximum size, were a good match.
Unix and C introduced the pure-stream-of-bytes model for I/O.  Unix got around the buffer problem by using an in-OS buffer pool, hiding the underlying fixed block length structure of disks.  They could get away with this because memory was getting cheaper and CPU's were getting faster - and the early Unix users weren't writing the kind of I/O intensive code that business users cared about (updated customer records:  maximum data access, minimum computation).
Early Unix fans were quite rabid about this being the only good model; forget all that old record-oriented stuff.  Besides, the argument went, if you want records you can always build them on top of streams.  I used to point out that this was false in an important sense.  FORTRAN was quite explicit in requiring that after the following sequence (ignore the syntax):
A = 1
B = 2
C = 3
WRITE A, B
WRITE C
READ X
READ Y
X will contain 1 and Y will contain 3 - the extra value of 2 in "the first record" is skipped.  It's impossible to design a file layout that simultaneously (a) allows a FORTRAN reader to get its desired semantics; (b) allows a "stream" reader to see 1 2 3 with no extra information.
Conversely, emulating a pure stream access method on top of a record-oriented one wasn't really possible either - as VMS demonstrated when VAX C was introduced.  Its library did the best it could, but significant rough edges (particularly around how fseek/ftell were supposed to seek into the "middle" of a record; but there were other issues) remained until VMS eventually added a native stream access method - in the process inevitably losing a certain level of universality of its files.  I'm not going to argue that the record-oriented system was better - making *terminals* look record-oriented (as VMS did) lead to some real absurdities.
Pre-TCP network protocols were designed using the familiar block-I/O principles.  Early systems didn't actually integrate their network I/O into their file systems - the concept of device-independence pretty much didn't exist at the time.  But people stick to tried-and-true principles and reuse designs.  TCP, of course, did the same on the flip side:  They designers took the Unix stream access method and applied it to networks.  (That Unix wanted to fit network connections into its general file I/O system only encouraged this design.)
Some early uses of TCP - FTP transfer, especially of stream-oriented files - was a perfect match.  (FTP and record-oriented file systems were always a problematic pairing.)  Others - email - fit with minor effort, adding a notion of "lines" - records in another guise - defined by terminators for certain purposes.  Telnet wasn't a great match - terminals as used by interactive programs aren't a great match to either record I/O or stream I/O and the delays of a remote connection made the mismatch very clear.  Many other programs found that they really wanted to exchange messages, which were essentially records.  They each got to build that for themselves, introducing generations of buffer overflow issues that plague us literally to this day.
SSL came at this and built a message (record) interface on top of TCP (because that was convenient for defining a crypto layer), and then a (mainly) stream interface on top of its message interface - because programmers were by now familiar with streams, not records.
And so ... here we are.  Living in a city built on top of generations of older cities.  Dig down and see the accreted layers.
What *is* the "right" (easiest to use correctly, hardest to use incorrectly, with good performance, across a large number of distinct application API's) underlying interface for a secure network link?  The fact that the first thing pretty much all API's do is create a message structure on top of TCP makes it clear that "pure stream" isn't it.  Record-oriented designs derived from 80-column punch cards are unlikely to be the answer either.  What a "clean slate" interface would look like is an interesting question, and perhaps it's finally time to explore it.
                                                        -- Jerry

@_date: 2014-05-19 17:28:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
Beyond this, site owners quickly found all kinds of reasons to mix content from different domains - e.g., put the static assets in one domain and the dynamic stuff in another.  So even if you really had an SSL connection all the way through ... it wasn't clear what on the page was being validated.
SSL really has a remarkable history of giving the world enough confidence to actually get e-commerce going while at the same time *never* really providing any reasonable security properties.  The best one can say is that it raised the minimum level of competence required to pull of certain kinds of attacks - though whether anyone would have found those attacks to be the most efficacious ones available even lacking SSL, we'll never know.
To this day, I've never seen a good description of a full protocol - from the end-user-visible components down to the bits on the wire - that, were it implemented, would solve the problem:  How can I be sure that when the browser says I'm talking to eBay, I'm *really* talking to eBay?  (I not even concerned with the "my conversation is visible only to me and eBay" (encryption) part, was that's trivial once you've solved the "is it the right eBay" (authentication) part.)  Things like certificate pinning and such are an attempt to solve this problem without ripping out the entire existing SSL/PKI infrastructure - and are likely the only *practical* solution we are likely to get; but I'm not sure we even know what a "clean whiteboard" solution would look like.
                                                        -- Jerry

@_date: 2014-05-19 18:01:24
@_author: Jerry Leichter 
@_subject: [Cryptography] updating a counter 
As others have pointed out, this is probably not worth doing:  If you cipher is vulnerable to this kind of attack, it's just vulnerable.
However ... if it makes feel better, an approach I've use for very different purposes will work here:  Choose a random odd increment and instead of incrementing by 1, increment by that random value.  Send the increment as the first (encrypted, why not) block.  This is about as fast and cheap as you can get, and if you're worried you're still revealing too much and may be leaving your cipher open to attack ... you really need to choose a better cipher.
                                                        -- Jerry

@_date: 2014-05-20 10:50:23
@_author: Jerry Leichter 
@_subject: [Cryptography] The Trust Problem 
So I ran across the Mustbin iOS app (  Cool, simple idea:  Take pictures of important documents, the contents of your wallet, etc.; organize them in "bins"; upload and sync to all your devices.  The data is encrypted with "military grade security" (they actually specify RSA - no key length mentioned - and AES-256); they don't have access to your decryption keys.    "Our technology has been reviewed and verified by one of the best firms in the security analysis business."  (This is mainly from a blog entry:  So ... should I believe their stuff is secure?  Let's suppose they really are good guys doing their best to provide a secure service:  What could they do to help me trust them with such sensitive information?
With security, we're now at a level well beyond technical questions about algorithms and key lengths.  What should you demand to be convinced that you can use some software safely?  What should someone offering secure software put out there that would help you reach a decision?
The facile answer is "only use OSS" - like OpenSSL, home of Heartbleed.  :-( (Actually, Mustbin uses OpenSSL - they have another blog entry about what affect Heartbleed had on them.)
Openness is certainly *part of* the answer.  I'd find Mustbin's comments much more convincing it they named that "best firm" *and published their report* so I could judge what was actually examined.  But it's not the whole story.  Apple's recent white paper on iOS security  may not be perfect - what is? - but it's certainly way beyond what you get with most products, which basically say "We're experts, trust us."
                                                        -- Jerry

@_date: 2014-05-20 18:30:08
@_author: Jerry Leichter 
@_subject: [Cryptography] The Trust Problem 
You and Tom Ritter misunderstood my posting entirely.
I don't care about Mustbin specifically; it was just an example I happened to run across.  I'm asking *generally*:  How do you produce/gain trust in security software?  If you to say to people:  "Don't do your own crypto" - you're saying either "don't use crypto" or "use crypto someone else developed".  Presumably, you want the latter.  So *even for those who might be able to throw together some crypto software for themselves* ... how should they judge software that's out there?  (And of course the overwhelming majority of people couldn't possibly write their own crypto anyway....)
I'm specifically making this a two-party problem:  What should the software maker provide to help the software purchaser make a good decision?  It's also a problem that *good* software makers have to solve - the *bad* software makers don't care.  But of course it must be as difficult as possible for a *bad* software maker to make himself look like a *good* software maker.
BTW, "sharing with people you trust" could mean many things.  It might mean "people to who you've given an access key", in which case the problem is simple.                                                         -- Jerry

@_date: 2014-05-20 20:30:56
@_author: Jerry Leichter 
@_subject: [Cryptography] Facebook on the state of STARTTLS 
This is less of a problem than it appears to be.  One of the wonderful things about email was that it allowed anyone to send email to anyone.  But as we learned, one of the terrible things about email was that it allowed anyone (including spammers) to send email to anyone.
Let's divide received mail into two buckets:  Email from someone I already know, and email from someone I don't know.  Messages in the first bucket should use a private key agreed upon between me and that other party.  A spammer can't create mail like this.  (Well, he *can*, but only by being present "inside" my correspondent, with access to his private key store.  If my correspondent is vulnerable to that kind of thing, our communication isn't secure anyway - and in fact the spam provides me with a service, as it tells me my correspondent has been compromised.)
The second bucket - the stuff from people I don't already know, has traditionally been an example of a need for asymmetric cryptography and something like a PKI:  My new correspondent gets my public key from some key store and sends me the message.  Of course then a spammer can do the same.  But there are ways to limit that if I'm willing to limit "mail from an unknown sender".  For example, that interface might only support very restricted messages - the sender has to fill in one of a small number of forms (you don't know me but I got your address from X, I also know Y who you work with, I'd like to talk to you about Z).  The receiver can then decide if he wants to talk to this person and if so negotiates a private key; subsequent messages are in the first bucket.
No, it's not the free-wheeling open email of the early Internet, but the spammers killed that a *long* time ago.
To deal with this, you have to assume the employer can control (to at least some degree) the endpoint device and software.  Then the problem becomes fairly straightforward:  A second copy of any encrypted message is sent to the employer's archive facility, encrypted with a key the employer knows.
Nothing that's not along these general lines can possibly work.  (I personally think the regulators are fighting a losing battle here.  There was a time not so very long ago that a stockbroker, say, worked in an office, using telephone equipment provided by his firm, sending letters and later faxes and email through firm-controlled equipment.  So archiving this stuff was easy, but for a broker to "get around" the equipment and leak significant amounts of data quickly enough to matter was a significant physical challenge.
Today, alternative communications mechanisms and tons of compute power are everywhere.  I've heard of financial firms that require people to surrender their personal cell phones as they come in to work.  But that's only workable in specialized circumstances, even in the financial world.  Many of the people who've been caught of late aren't sitting in the office all day - they are out and about meeting others, and there's no way to prevent them from communicating.  The regulatory authorities are adjusting - as in recent cases where they used wiretaps - and they're going to have to continue to adjust.
You bet.  *The* big unsolved problem.
                                                        -- Jerry

@_date: 2014-05-21 21:00:38
@_author: Jerry Leichter 
@_subject: [Cryptography] New attacks on discrete logs? 
I can't figure out from these two articles exactly what's been attacked.  (It's not even clear to me if they are describing two *different* attacks.)  Anyone been following the details?
                                                        -- Jerry

@_date: 2014-05-21 22:09:07
@_author: Jerry Leichter 
@_subject: [Cryptography] How secure are hashed passwords? 
Forget rainbow tables; they're irrelevant for modern attacks.  Pure brute force rules the day, using specialized hardware based on multiple GPU's running highly optimized algorithms - and tons of statistical data on the passwords people actually use.   will give you an idea of the state of the art ... almost two years ago.  (At the time, a single AMD Radeon HD7970 GPU could do 8.2 GigaHashes/second - I think MD5, but there's not much difference here among the different commonly-used hashes.  (Hashes like bcrypt and scrypt have been designed to make the problem harder, but they are not widely deployed.)
                                                        -- Jerry

@_date: 2014-05-22 11:45:06
@_author: Jerry Leichter 
@_subject: [Cryptography] The Trust Problem 
Just on the "claims" thread:
Startups are like patent trolls:  They have a huge upside and a small downside. We all celebrate rapid innovation and rollout of not-quite-there products, and a whole "it's easier to ask forgiveness than permission" culture.  It gets us all kinds of new stuff.  But it also guarantees that the stuff will be full of security and privacy issues, because avoiding them is very time-consuming and gets in the way of functionality.
It's not like this is really new, either.  Windows got where it was at its peak for many reasons - but among them was the view that users want magic functionality, quickly.  People want their applications to integrate with each other, even if they weren't built to work that way, so it should be easy for applications to do all kinds of things to each other.  That Internet thing - people want it, so just open up your "personal" machine, built on the assumption that you didn't need to protect yourself from yourself, to the bigger world.  All of these things lead to huge success.  So of course others followed along.
If a startup is given a choice between path A, which will produce 10 million new users in weeks but might lead to a huge security or privacy problem (but of course we can convince ourselves that "it's very unlikely); or path B, which is safe but which might mean only a few hundreds of thousands of new users and as a result no next-round funding and a likely death of the company ... well, path A is hard to resist.  And if path A requires a little "creativity" with the facts; well, so what, everyone does that.  We can always apologize and fix things later.
These are fundamental tradeoffs, and you can't make them go away.  Producing "new stuff" involves risks - and with a startup, many of those risks get dumped on others because the startup itself doesn't much to lose.  Larger companies face the potential (not always realized, of course) of government and individual lawsuits, and they have real money at stake.  So they tend to be somewhat more risk-averse.
Personally, I'm conservative about taking on such risks, which means there are many new, shiny toys I choose to pass on.  The whole point of starting this thread was to ask "How can I make better judgements about actual risk, so that I'm not forced to pass on stuff I'd like to use and which really is reasonably safe?"
                                                        -- Jerry

@_date: 2014-05-22 16:47:14
@_author: Jerry Leichter 
@_subject: [Cryptography] The proper way to hash password files 
I'll repeat my (only partially facetious) suggestion:  Require that any company that maintains a password database have entries for pseudo-accounts with fixed, known names like "CEO Bank Account Password" and "CSO Retirement Account Password" contained the hashes - using exactly the same algorithm as used for the rest of the database - of that banking information.  If having the database stolen is going to be bad for all the customers, make sure it's *really, really, really bad* for those in a position to approve or reject efforts to make sure it's kept secure.
                                                        -- Jerry

@_date: 2014-05-22 21:34:14
@_author: Jerry Leichter 
@_subject: [Cryptography] New attacks on discrete logs? 
I know nothing about the details of this particular patent - not even that it exists - but it's certainly possible to patent the use of a particular mathematical object for a particular real-world purpose.  This is not significantly different from patenting the use of a particular molecule as a drug.  (Drugs tend to be protected in other ways as well; in particular, the steps used to synthesize them are usually patented.  But such a patent wouldn't stop someone else from finding a different way of making the same molecule.  The patent on the use of that molecule would, however, keep them from selling it as a drug.)
                                                        -- Jerry

@_date: 2014-05-23 18:00:12
@_author: Jerry Leichter 
@_subject: [Cryptography] The proper way to hash password files 
Ari Juels and Ron Rivest have a paper - "Honeywords: Making Password-Cracking Detectable" -  - proposing a more sophisticated variant of this proposal.  (They deal with the problem that the attacker may be able to determine which users are legitimate users - or, in a more restricted fashion, be able to generate a partial list of legitimate users on the system - and by limiting himself to just those, avoid hitting any of the fake entries.)
                                                        -- Jerry

@_date: 2014-05-23 19:45:57
@_author: Jerry Leichter 
@_subject: [Cryptography] The proper way to hash password files 
I don't know when the idea were first published (perhaps in a technical report), but this paper was presented at CCS'13 in November of last year.  That's not even one year....
                                                        -- Jerry

@_date: 2014-05-24 08:07:32
@_author: Jerry Leichter 
@_subject: [Cryptography] New attacks on discrete logs? 
Okay, to clear this up:
Ah, time for some fun math I haven't thought about in years.
A (commutative) group G is a bunch of elements with a binary operation . with the following properties:
If g, h, and j are in G, 1. g.h is defined and is in G ("closure").
2. g.h = h.g ("commutativity")
3. (g.h).j = g.(h.j) ("associativity")
4. There is some element e in G such that e.g = g ("identity element")
5. There is an element g' such that g.g' = e ("inverse elements").
It's easy to show that e is unique (if there were another identity element f, then e.f = f but also e.f = f.e = e, so f = e).  Similarly, g' is also the uniquely defined "inverse of g".
Groups occur all over the place.  Addition of integers is a group with identity element 0 and the inverse of n is -n.  The same is true for the rational numbers, the real numbers, and the complex numbers; all of these are infinite.  The integers mod N are similarly a group, but a finite one.  The non-negative numbers are not a group under addition because they lack inverses.  The non-zero rationals (or reals or complex numbers) are groups under multiplication, with 1 as the identity element and g' given by 1/g; that doesn't work for the integers, because there are no multiplicative inverses.  The integers mod N, less 0, are groups under multiplication if and only if N is a prime.  (If N = nm, then n and m are members of the non-zero integers mod N, but their product is 0 mod N, which is not a non-zero integer mod N - so we don't have closure.)
The rationals, reals, and complex numbers are actually what are known as fields.  A field F is a set of elements with two operations, usually write as + and *.  It has the following properties:
1.  F is a group with the operation +.  The identity element for + is usually written as 0.
2.  F-{0} - i.e., the elements of F other than the identity element for addition - is a group with the operation *.  The identity for this group is usually written as 1.
3.  For any f, g, h in F, f*(g+h) = f*g + f*h.  ("distributivity")
Since 1 is in F-{0}, it's certainly true that 1 != 0.
In order to make multiplication apply to all of F, we define f*0 = 0*f = 0.  It's easy to see that this definition is consistent with distributivity.  Note that 0 can't have an inverse:  0*f = 0 but 0 != 1.
The rationals, reals, complex numbers, and the integers mod a prime are all fields.
Suppose f and g are in F and f*g = 0.  Then either f or g is itself 0, since the non-zero elements were a group under multiplication.
Now, take some element f in F and start adding it to itself:  f, f+f, f+f+f, etc.  Does this sequence ever contain 0?  Suppose it does.  In fact, suppose n is the smallest number of additions you need so that {f+f...+f} n times is 0.  Now suppose n = ab, where a, b > 1.  You can subdivide the additions into groups of size a, repeated b times:  {f+f...+f} n times is {{f+f...+f} a times + ... +} b times.  {f+f...+f} a times is some other element g, and this is just saying that {g+g+...+g} b times is 0; and b < n.  So we can find elements with smaller and smaller "number of repetitions to get to 0" - until that number of repetitions is a prime number.
So:  If there is any element f which if added up n times produces 0, there is some element g which if added up p (a prime) times is also zero.
But if g+g...+g = 0 (p times on the left, g non-zero of course) then also
(1/g) * (g+g...+g) = (1/g) * 0 = 0
Distribute on the left and you get:
(((1/g) * g)+...) = 0
1+1...+1 p times = 0.
But now you can multiply this equation by *any* element f in the field, showing that if you add f up p times, you always get 0.
Finally, can there be two distinct primes p and q such that adding up 1 either p *or* q times gives you 0?  The answer is no.  Suppose there was more than one such prime.  Let p be the *smallest* one, and let q be a different one.  We have:
1+1+...1 q times = 0
But then
((1+1...) p times) + ((1+1...) q-p times) = 0
The first of these terms is 0 already, so:
((1+1...) q-p times) = 0
Repeat this, pulling off groups of p 1's.  Since p and q are primes, you can't possibly end up with no 1's - but you do eventually end up with q mod p < p 1's.  But that can't be - p was by hypothesis the *smallest* prime that gave you 0.  (q mod p might not be prime, but as we saw early that some prime divisor of it would also give 0, and that would have to be a prime < p - and there isn't one, by hypothesis.)
So:  If a field has any element f that if added to itself n times produces 0, then there is a *unique* prime p such that if you add *any* element of F to itself p times, you get 0.  This prime p is called the characteristic of the field.
It may well be that you *never* get 0 if you add an element to itself over and over.  In that case, the field is said to have characteristic 0.  (It would perhaps make more sense to say they have infinite characteristic, but the convention is to use 0.)
The rationals, reals, and complex number fields all have characteristic 0.  The integers mod a prime p have characteristic p.
It's not hard to see that any finite field must have a non-zero characteristic, and only a bit harder to see that a finite field with characteristic p must have p^n elements.  There are infinite fields with non-zero characteristic but they aren't familiar kinds of objects (and of course in finite computing we always have to restrict ourselves to finite objects).
Fields with characteristic 0 "feel like the rationals and reals and complexes". All such fields share certain "natural" properties that fields with non-0 characteristic don't.  For many properties, have a large characteristic means "being closer to having that property".  So results showing that certain problems that appear to be hard in characteristic 0 are likely also hard in large characteristic fields, but not in small characteristic fields, are expected.  (That doesn't mean they are easy to prove, or that this is a universal "meta-property" - it's just a heuristic.)
                                                        -- Jerry

@_date: 2014-05-27 06:38:22
@_author: Jerry Leichter 
@_subject: [Cryptography] client certificates ... as opposed to password 
Passwords - or any authentication mechanism used by human beings - live or die on their user experience.  Note that your list only has two items, a and b, which address the user experience.  If you step back and think about how people want to use computers, you need to restate the ones you have and add others (keeping in mind that you wanted to describe an *ideal* system):
a')  Is consistent with human memory characteristics (difficulty in remembering large amounts of meaningless data, prone to forgetting over long periods without rehearsal, etc.)
b')  Does not require long or complex physical operations.
1)  Identifies the *person*, not the computer they log in from.
2)  Can be used by a person carrying no specific hardware.
If you look at historical real-world identification systems, they are mainly built to match these requirements.  There are compromises made in specialized, situations.  For example, opening safe doors may deliberately not match b'.  Entry gates for residential complexes often identify cars, not individuals (which can be a pain when you have guests).  But by far the most common one to compromise on is 2.  We carry physical keys, identity cards, credit cards.  But notice how often we "compromise on the compromise".  When the technology to build them became widely available, people started using combination locks on doors that would historically have had key locks.  If not specifically required not to, guards who are supposed to check ID cards historically will let people they recognize through without checking.  A credit card system that nominally relies on possession of physical token comes to be used over the telephone system and later on-line.  Phones use fingerprint sensors.  (Actually, fingerprint sensors have been used as an adjunct to password systems where people have to repeatedly and quickly log on to large numbers of distinct endpoints - think nurses and doctors logging on to access medical records in each patient room, something that's becoming increasingly common.)  So the pressure to implement 2, or as good an approximation to it as you can manage, is hard to resist.
Much traditional security is based on ad hoc implementation of biometrics (people recognizing other people's faces or voices) or knowledge of complex matters (speaking a particular language/dialect, appearing to know the right other people/names of departments/ways of referring to things).  Social engineering is often, perhaps almost always, about ways to hack such identification systems.  And note that such systems do a good job of implementing my list of UX requirements.
One interesting thing to note is that passwords as personal identification are an invention of the computer age.  Earlier uses of passwords were for group identification:  Soldiers in a particular (subgroup of) an army is the standard example.  Police departments often use a "color of the day" signal to identify themselves (very approximately and noisily) to other police.  Passwords in these roles are easy for attackers to observe and would be easy to guess, except that the nature of the usage makes guessing slow and risky.  So deliberately changing passwords frequently provides sufficient safety in the relatively limited security tasks they fulfill.  This is quite different from the way we've historically tried to use passwords in computer-based systems.
Yes, once you formalize and computerize and pack large amounts of information into very small, easily exfiltrated form, you find that it's impossible to actually build a system with all these properties.  It's not as if we haven't known this for years:  When we guard large amount of money or valuable information, we force people to put up with inconvenience (you must have and show your id card to enter, even though you and the guard have known each other for 5 years).  But that doesn't make the drive to meet these requirements any less.  It's why the *real* alternative to as-good-as-possible password systems isn't client certificates or magic one-time-signon, it's badly implemented password systems.  It's why, whatever the facts about their limitations, we're going to see biometric systems keep getting designed.
And there's another factor here, from the other side of the table as it were:  Businesses are leery of outsourcing their identity/security systems to others.  Something like the RSA (or other maker's) key fobs that generate one-time-passwords would be a great alternative (we're willing to carry physical keys most of the time if they are small and light enough, and you can imagine even better form factors for some people, like rings) - except that the alternative only works if I can carry *one* such fob, and that requires that everyone I identify to has to agree on using it - which no one has managed to make happen.
*Maybe* smart phones will end up providing the solution.  Maybe smart wearables will improve on it.  All speculative.  The hardest part of any proposal to replace passwords is the UX.  The technical parts are easy and have been understood for years.
                                                        -- Jerry

@_date: 2014-05-27 14:45:01
@_author: Jerry Leichter 
@_subject: [Cryptography] Langsec & authentication 
And yet it's *way* too common.  Someone realizes that there are cases where it would be handy to write:
which of course is trivial in shell syntax:
...and we're off to the races.
Self-control - *not* using the cool hack that gives you all that power - is one of the most under-appreciated virtues of programming.
                                                        -- Jerry

@_date: 2014-05-28 17:20:18
@_author: Jerry Leichter 
@_subject: [Cryptography] client certificates ... as opposed to password 
I agree in principle, and in fact I even agree about client certificates, though I disagree that this particular slide show proves it.  The slide show covers way too much material.  A comparable slide show about pin-tumbler locks would include the disclaimer that "we won't go deeply into metallurgy, but here's the basic idea of why the pins are steel while the plug is bronze."
*Using* client certificates, once they've been set up problem, is a reasonably well solved problem in a number of existing implementations.  What's not solved in any I know of - and others here have made the same comment about all the implementations *they* know of, which probably pretty much covers the space - is getting and configuring a client certificate, and beyond that knowing *what* you should get, for what purposes, and what you actually gain by having such a thing (in terms that make sense in the end-user's world).
                                                        -- Jerry

@_date: 2014-05-30 15:00:12
@_author: Jerry Leichter 
@_subject: [Cryptography] FW: RFC 7253 on The OCB Authenticated-Encryption 
What's the policy concerning patents and RFC's?  OCB is patented, though with a bunch of exceptions   There's not (so far?) any exception related to implementing an RFC as such (though there is a FRAND promise on that page).
                                                        -- Jerry

@_date: 2014-11-01 07:08:52
@_author: Jerry Leichter 
@_subject: [Cryptography] [messaging] Gossip doesn't save Certificate 
There was some debate about whether regularly distributing a 30MB file of keys was feasible.  I recently came across an interesting bit of data:   has statistics on the average sizes of web pages.
For the Top 100 sites, they report an average page size of 1.288MB.  The size grows as you move to larger samples:  The top 1000 sites have an average page size of 1.728MB, while their entire set measures out to 1.944MB.
The trend is upward - a year ago, the measurement for the entire set was 1.617MB.  (The Top 100 sites also grew, but much more slowly - a year ago they were at 1.27MB.)
I didn't look into details of how they define their groupings of sites and how they do their measurements, all of which matter; and I don't know if they have information about mobile sites, which are presumably smaller.  (The site has plenty of these details.)  But these numbers give a least a rough idea of what kind of data sizes you can expect on the Internet today.  Even if you read just the Top 100 sites (in which case why would you need the keys for the Top 100,000?), my simple-minded 30MB download is the equivalent of a bit over 23 page loads.  Hardly seems excessive.
                                                        -- Jerry

@_date: 2014-11-01 18:44:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Best internet crypto clock: hmmmmm... 
An interesting side note on this:  Historically, when a government changes the form of currencies, it declares a fairly short period during which old bills can be converted to new ones, after which the old one stop being accepted.
The US replaced its $100 bills - which had been fundamentally unchanged from 1957 to a major reworking (in response to counterfeits) - in 1996.  By then, US $100 notes had become the standard holder of value in much of the world, widely used in the black and gray markets of countries whose currency was not seen to be trustworthy.  I remember reading articles about panics among Russia citizens who kept their savings in such bills (illegally) and feared they would be left holding worthless paper.  A panicky conversion to new bills took place on the black market - where old bills were accepted at a substantial discount from their face value.  Reassurance from the US Treasury that the old bills would continue to be accepted didn't help.  (They are accepted to this day, as far as I know.)  Of course, the black market traders - who made a killing by buying old bills at a discount and selling them overseas at face value - were only too happy to help spread the story of the upcoming worthlessness of the old bills.
                                                        -- Jerry

@_date: 2014-11-01 22:21:45
@_author: Jerry Leichter 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
The chips used ceramic packages - made ultimately from clay.  The clay used by IBM and other American makers had a much higher concentration of alpha emitters than that used by the Japanese chip makers.  ("Much higher" is relative - the clay was far from radioactive in any normal sense and no one had previously considered the significance of naturally occurring radioactive materials in clay.  They are actually used now to date pottery:  The emitted particles cause detectable damage to fired clay which builds up at a known rate.)
The net effect was that Japanese memory chips were much more reliable than American ones.  The effect depends on memory cell size, hence capacity.  At some point (I no longer remember where), chips became dense enough for this to be a big issue.  There was all kinds of teeth-gnashing and complaints about American workers and interest in Japanese approaches to reliability before the real cause was worked out.  But the effect on the industry was dramatic:  American makers, who had invented memory chips and completely dominated the market for chips at size S, pretty much completely lost it at size 4S (and, of course, never got it back).
                                                        -- Jerry

@_date: 2014-11-02 14:56:13
@_author: Jerry Leichter 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
Cue the famous Arpanet collapse of 1980.  For those whose memories have lost the bits, or never had them:  Multiple things contributed, but one router - IMP, in those days - developed a stuck-at-zero block of memory.  Unfortunately what was in that block of memory was the table of distances to other nodes.  The Arpanet used a distance-vector routing algorithm, in which each not maintains such a table and floods it to its nearest neighbors.  The net effect was that that IMP informed all other IMP's that it had a path of length 0 to every node on the Arpanet - promptly re-direction *all* traffic on the Arpanet to itself.
The table was actually checksummed in memory, but the checksum used computed a zero result for an all-zero input - so it passed.  There were other issues that made this worse - see  for a discussion - but two lessons learned were (a) the checksum must follow the data from birth to death - memory to network transmission to memory to use, in this case; (b) given that stuck-at-zero (and stuck-at-one) are common failure modes, checksums should be designed so that the checksum of all zeroes is not zero, and the checksum of all ones is not one.
(Related long-ago story:  The original VAX, the 11/780, had a UBA - Unibus adapter - to allow it to use existing peripherals designed for PDP-11.  Among those peripherals was the first DEC Ethernet adapter, the DEUNA.  The Unibus had no parity, much less ECC, checking - at the time of its design in 1969 it was considered too expensive and complex.  (In fact, the whole Unibus design was based on keeping the necessary logic to an absolute minimum - extra logic was a big deal in MSI days.)  The Unibus was also known to corrupt bits if the devices on it drew too much power - and the DEUNA was a *very* power-hungry device; it could share a Unibus only with low-powered devices.  (You could have more than one UBA on a VAX 11/780, so this wasn't a killer constraint.)
If you transferred stuff through DECnet - this would have been Phase II DECnet, which only did one-hop routing; you had to specify a path if you wanted to go multi-hop), the only part of the low-level path that was *not* checksummed was the transit of the data from the DEUNA over the UBA to memory, and then back.  The analogue of FTP used with DECnet *did* compute an end-to-end checksum.  File transfers would appear to work - but on completion would report a "DAP CRC checksum failure" when the end-to-end checksum didn't match.  You could be pretty sure that the cause was someone's overloaded DEUNA.  Those who understood this stuff would look at the path the data had followed and then use divide-and-conquer to find the machine that was corrupting the data.  Many a VAX system manager came in to work to find mail from someone he'd never heard of telling him that his DEUNA was on an overloaded UBA - the management software for DECnet allowed you to determine pretty exactly how the device forwarding the packets was configured on the machine.)

@_date: 2014-11-04 22:58:28
@_author: Jerry Leichter 
@_subject: [Cryptography] Wind River Security Features and Cryptography 
Highly unlikely.  Much the same information *is* published, if not perhaps in one place.
There's a plausible explanation of the whole business.  After the battles of the Clinton era, the export control regime moved into a very permissive mode:  If you want to export, you file a form and the government has a fixed, relatively short, period to say "no".  If you hear nothing, you have a license and can go ahead and export.
In practice, the government almost always has let the period expire without comment, and pretty much anything is thus exportable.
Even though the system lets almost everything through, the penalties for not going through the process are severe, including I'm pretty sure jail time.
What it *looks* like has happened is that Wind River (and they are probably not alone) simply ignored the "pointless" step of applying for an export license.  I'll bet many companies today do, too - they treat these regs as relics of a bygone era.
Apparently someone decided it was time to send a wakeup call to industry saying "Hey, we're actually still here, the law remains as it was - file the forms".
Whether there's something more behind it, I don't know.  It could be that Wind River was not just failing to file the forms, but also exporting to countries that we're currently unhappy with.  But, again, why now?  Why Wind River?  There may be no reason at all - just someone deciding to make a point.  Or there may be something we know nothing about, and likely will never learn anything about.
                                                        -- Jerry

@_date: 2014-11-07 07:31:27
@_author: Jerry Leichter 
@_subject: [Cryptography] Who's solving difficult security problems 
From an article a couple of weeks ago in "Krebs on Security" discussing Google's support Universal 2nd Factor USB devices protect accounts, which then goes on to mention Apple Pay, and makes the point:
"I find it remarkable that Google, Apple and other major tech companies continue to offer more secure and robust authentication options than are currently available to consumers by their financial institutions. I, for one, will be glad to see Apple, Google or any other legitimate player give the entire mag-stripe based payment infrastructure a run for its money. They could hardly do worse."
                                                       -- Jerry

@_date: 2014-11-07 16:01:18
@_author: Jerry Leichter 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
...However, I think the above sequence
So if the original checksum function was C(x), you've replaced it by C(ASCII(x)), where ASCII(x) converts x into a series of decimal digits.  Why is that a better checksum than C() was?  If the answer is that C(0) = 0 so it's vulnerable to "stuck at 0" faults, then you're simply saying that C was a bad checksum (for this purpose) to begin with.  Why not choose a better one in some disciplined way, rather using the arbitrary technique?
                                                        -- Jerry

@_date: 2014-11-08 06:10:30
@_author: Jerry Leichter 
@_subject: [Cryptography] $750k Fine for exporting crypto 
I'm troubled comparison.  Nixon et al targeted political foes - an extension of a long history of nasty political tricks techniques engaged in by hard-ball politicians since ... forever.  Illegal, pushing beyond the boundaries - but a fight limited to inner circles.
What we are seeing today is unprecedented in American history:  Wholesale monitoring of entire populations, "just in case" the information might be "needed" later.  Saying "beware, someone evil like Nixon could use this stuff" *misses the point*:  It's bad *even if never abused*.  Its mere *existence* is abuse, no matter who controls it.  If the system were under the control of a saintly administration consisting of nothing but good actors, and there were a magic button that would be pressed just before they handed over the reigns to someone not so saintly that magically erased all the stored information and destroyed the information-gathering systems ... it would *still* be wrong.
                                                        -- Jerry

@_date: 2014-11-08 23:24:54
@_author: Jerry Leichter 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
"All possible problems?"  I highly doubt it!
If we were talking ordinary code, "closing the loop" like this would certainly seem like an excellent idea whose only possible downside was cost.  But for crypto code ... one worries.  For example, are you opening up some possible side-channel attack by running two closely correlated exponentiations one after the other?  If we were talking about a *mathematical* attack, there would be no problem, as the attacker could have done the computations himself.  But he can't run this code *on you hardware*, with, for example, the caches warmed by the inverse calculation (that he can't perform).  It's not that I have any idea of a potential attack ... but the history of side-channel attacks should keep us cautious.
Another potential area of trouble is error recovery.  Does the fact that the verification failed reveal something?  Does recomputing using a very slightly different key reveal something?
                                                        -- Jerry

@_date: 2014-11-10 15:15:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
Actually, I suspect all three of us are in violent agreement here.  Peter specifically asked about catching "all possible problems" - something only a theory we aren't even close to having could answer.  I answered along the lines of what such a theory would have to account for.
As a practical matter, I'd consider the closed loop check myself - though in fact such checks are pretty rare, in any kind of code.  If we're talking about RSA, you could, of course, use its multiplicative property to blind the data, so that you would be checking the signature not on the original data but on some random multiple of it - which would presumably make attacks that much more difficult, for little cost.  Though that raises the interesting question of whether you could come up with a check that's much more efficient - something along the lines of "casting out nines" to check additions.  Probably.  Or you could say that machines today are fast enough that it's not worth the trouble.
                                                        -- Jerry

@_date: 2014-11-12 07:37:35
@_author: Jerry Leichter 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
You mean you *don't* think we should deal with Ebola, global warming, or world hunger?  What kind of a monster are you? :-)
I'm not sure what a "memory-based problem" would be.  If you allow the memory to be swapped - especially if it goes to encrypted swap, which you would presumably want to use - a disk error would result in a completely arbitrary change to any relevant value.  Measurements show that while very rare, undetected disk errors do occur.  (Note that a "memory error" could also produce arbitrary changes in the *code*.  I don't know whether any work has been done on that - obviously, you have to bound "arbitrary" or you can say nothing at all.)
The attacks I mentioned were variations of well-known side-channel attacks that tried to leverage the unusual code paths and behavior that a memory error could trigger - and which the check code could, in principle, amplify.
Complexity, as always, is the enemy of security.  Checking this way seems as if it would help - but then again, anything that uses private information has the *potential* to leak it.  If we were talking about *encryption*, doing a checksum before encryption and checking it after - to *detect faults*, *not* to do authentication - seems reasonable, though if *it* does leak information by letting an attacker know he has a valid decryption.  There's no free lunch.  (It's not clear to me how to accomplish the same thing for signatures.)
Let me be clear here:  I believe that sound engineering practices and intuition must play a big role alongside formal proofs and such.  Only formalization and proofs can get you to "all possible attacks", and I don't think you can hope for that here - nor do you need it.
We're talking exceedingly rare events.  The question becomes:  Is the cost justified?  Does the cure *possibly* introduce as much vulnerability as the (extremely tiny) vulnerability it prevents?  Are there simpler approaches that improve the situation without leaving as many open questions?
BTW, there *is* a theory of "error detecting computation" out there.  It's been years since I've seen it, and it was more oriented to traditional scientific computation.  (It probably became of interest after the Intel division issue surfaced way back when.)  Perhaps there's something to learn there.
                                                        -- Jerry

@_date: 2014-11-12 14:21:09
@_author: Jerry Leichter 
@_subject: [Cryptography] A TRNG Review Per Day: TrueRNG 
It's not so simple.  Correlation and entropy in an active attack environment don't play well.
Suppose you had source R1.  I use R1 delayed by one bit as a second source R2.  Any correlation between R1 and R2 is an autocorrelation at length 1 of R1 with itself - one of the things we would presumably check for to begin with, and it's something even fairly simple RNG's would be expected to attain.  And yet if someone learns either R1 or R2, there is no randomness left.  Also, the combined entropy had better be exactly that of R1 (or R2) or we could bootstrap arbitrarily high entropy out of any source by simply combining itself with itself at different lags.  (Note that what I'm suggesting outputs bits at the same rate as R1.  If you actually used two successive bits of R1 to generate one bit of output, you would have gained entropy per output bit.)
This is one of those situations (common in tests for RNG's) where if we *find* something (correlation), we know the RNG is bad; but if we fail to find it, we haven't really learned anything.
                                                        -- Jerry

@_date: 2014-11-13 07:07:07
@_author: Jerry Leichter 
@_subject: [Cryptography] How iOS 8.0.1 Got Screwed Up 
Apple has said a problem not in the software, but in the "wrapping", led to an iOS 8.0.1 patch that did such nasty things as preventing phones from making calls.  I've found this puzzling - how packaging disable just some features?  In a Re/Code interview, Greg Joswiak (Apple VP of iOS marketing among other things) reveals in passing that the problem was that they "messed up the certs".
                                                       -- Jerry

@_date: 2014-11-14 18:01:47
@_author: Jerry Leichter 
@_subject: [Cryptography] ISPs caught in STARTTLS downgrade attacks 
A minor point, but telling:  I have yet to see a server-side search that performs as well, not to mention as flexibly, as search in Mail.app on my Mac, which is done entirely locally on my laptop - and was extremely fast, even against very large mailboxes, on Mac's several generations back.
(Before I started using Mail.app, I used pine for many years.  It had great mechanisms for selecting messages based on various attributes - mechanisms that remain unmatched in any other mailer I've used - but it didn't do a particularly good job on search.  I used to keep copies of all my mail locally and build a GLIMPSE index over it every night so that I could search it quickly.)
SMTP works reasonably well for what it is, but many of its promises - like decent server-side search - remain unfulfilled.  Quality of implementation reigns, and few implementations care - the "minimal acceptable level" for a mail server was established years ago, and everyone has stuck there.  (gMail strikes out in some new directions, but has its own costs to go with its benefits.)
Returning to the other points you raised:  Actually, many of them concern *encrypted mail at rest*, not STARTTLS.  Server search - if it's worth having - would be completely unaffected by STARTTLS.  About the only service that's (sometimes) implement "within the network" - that is, the only service that encrypted connections would break - is spam and malware detection.  And there are alternative architectures even there, and strong arguments for why such services belong at the endpoints, not in the network itself.  (End-to-end arguments and all.)  And, yes, some find these added services valuable.  If they want to opt in by *not* encrypting their connections, I have no problem.
Look, as discussed on this list previously, STARTTLS even if universally deployed is not much of a security solution, depending as it does on insecure MX (not to mention other issues).  So in that sense, this is a tempest in a teapot.  But on the broader scale, the general question of network service providers forcing you to take services you don't want - and which in fact you might object to - is an important one.  STARTTLS today, SSL and VPN's tomorrow.  After all, many of the arguments for in-network services for mail - like malware screening - apply equally (in fact more strongly) to Web sites, downloads, etc.
                                                        -- Jerry

@_date: 2014-11-15 08:55:53
@_author: Jerry Leichter 
@_subject: [Cryptography] ISPs caught in STARTTLS downgrade attacks 
"Not in the network" doesn't mean "in the phone".  Unless you're running an IMAP server or something like that in your phone, your mail is received by some system out in the cloud somewhere.  It can do - and probably already does - spam detection and such.  What it sees is "mail at rest" (often, "mail on its way to being at rest", but that's an implementation detail), which is *not* what what STARTTLS is about.
I will grant you that if you encrypt your mail at rest in such a way that the server cannot access the plaintext, you're giving up the ability to do spam filtering and such on the server.  Different patterns of usage have different tradeoffs.  The more generic computational power you assume at the user endpoint, the more you can potentially keep encrypted.  If all that's at the user endpoint is an arbitrary Web browser, there's not much choice:  The server has to have access to the unencrypted mail.  (It doesn't have to *store* it unencrypted, but it has to have access.)  A modern phone with an appropriate app could easily and safely decrypt mail that the server would have no access to, but you would indeed have to do the de-spaming on the phone - probably marginally possible because of power costs today, but you'd pay for the data transfer costs.  Once you get to a laptop with a WiFi connection, you can pretty easily do a decent job of spam detection at the endpoint.  (I rely on Mail.app's built-in spam detector, but some of my mail providers also do spam detection.  Where possible, I have them pass the messages through but add a marker.  To what degree Mail.app's detector is being triggered by the markers, rather than by intrinsic qualities of the message, I have no way to tell.  The combination works pretty well but has enough false positives that I manually check everything before flushing it.)
                                                        -- Jerry

@_date: 2014-11-17 14:57:31
@_author: Jerry Leichter 
@_subject: [Cryptography] IAB Statement on Internet Confidentiality 
Given our recent experience with STARTTLS rollback by at least one ISP ... do we still feel so good about opportunistic encryption, at least defined in this way?
                                                        -- Jerry

@_date: 2014-11-17 23:18:13
@_author: Jerry Leichter 
@_subject: [Cryptography] IAB Statement on Internet Confidentiality 
There seems to be unanimity in the response here, but I'm not sure I agree.  Note my "at least defined in this way".
The STARTTLS business involves two levels:  First, it allows anyone along the path to easily force a fall-back; second, that fall-back is essentially invisible unless someone happens to dig into details of logs.  The fall-back issue is pretty much inseparable from the whole notion of "opportunistic encryption", so one just has to live with it.  But the invisibility is something that can be dealt with - *if* you're willing to expand the range of stuff you include in a protocol definition.  Today, communications from the guts of the protocol to the user is binary:  Either you deliver a result, or you fail.  In fact, protocols have been broken when they attempted to deliver more detailed information, so the tendency has been to keep the communication channel extremely limited.
However, if you limit it this way, opportunistic encryption has no way to tell you that it's been blocked.  If no one notices attacks, the step forward looks much less dramatic, no?
Given the huge variety of protocols and protocol usage frameworks out there, it would be impossible to prescribe what kind of communications is appropriate.  But we could think about general frameworks and guidelines.  It's tricky, because any attempt to deliver the information in-line can be forged.  (E.g., if you try to add a "Delivered using STARTTLS" header, an attacker can disable STARTTLS, then add the header himself.)  But if you're going to say "this helps because it turns passive attacks into active attacks, and active attacks will be noticed" - you have to make sure they can be noticed.  By large numbers of people, in ordinary operation, not just by experts who happen to be looking for exactly such an attack.
                                                        -- Jerry

@_date: 2014-11-18 10:43:49
@_author: Jerry Leichter 
@_subject: [Cryptography] IAB Statement on Internet Confidentiality 
Facts?  You want actual facts?  Just what's wrong with you anyway?  Aren't rumors, innuendo, and truthiness good enough for you?  :-)
You make a good point.  It would be nice to know more.  But detailed information has been missing.
                                                        -- Jerry

@_date: 2014-11-18 17:46:43
@_author: Jerry Leichter 
@_subject: [Cryptography] IAB Statement on Internet Confidentiality 
A possible partial answer is at  - some Cisco equipment does this by default.  So the question may have no meaning as applied to the ISP - they may not even know how it works.  There is the question of why Cisco would make such an option the default.  Are customers asking for it?  Or was there no real conscious decision made - it's on by default because that's the way it happened to come out in the code and no one complained.
                                                        -- Jerry

@_date: 2014-11-19 15:00:57
@_author: Jerry Leichter 
@_subject: [Cryptography] SUBMIT is not SMTP, 
To be a bit less doctrinaire about this:  "SMTP" is the name used by pretty much every piece of user-facing software in existence today.  The host you send mail to is pretty much universally "smtp.example.com".  Every server response string I've ever seen says "SMTP" or "ESMTP".
MacOS lets you configure the port to use for your "SMTP" server, but in general you accept its default ports - 25, 465, 587.  (I think during verification it tries them in order, and accepts the first one that lets it log in to the designated server.)  iOS doesn't have any explicit text of this sort, but it appears to do the same thing.
I don't have a recent Windows box around to check what how it describes things, and my Android phone is at home.
So, sure, within the community of email experts, the protocol may have a new name, and port 25 may be effectively deprecated.  It's perfectly legitimate that experts in the domain speak in those terms.  But to everyone else in the world, it's still "SMTP", and 25 remains a possible submission port (even if it's blocked on most networks), and if an expert wants to communicate with everyone else ... he needs to be aware of that.
                                                        -- Jerry

@_date: 2014-11-19 20:28:03
@_author: Jerry Leichter 
@_subject: [Cryptography] SUBMIT is not SMTP, 
Everything you say is fine, but it ignores the implications.  Silently disabling  a user's explicit attempt a security is *in and of itself* nefarious behavior.  All the explanations of *why* this is being done miss this point.  Once the principle that such manipulations are acceptable is granted - even in special situations - there will be no end to the special situations.  The same anti-spam explanation applies to port 587.  You'll say, oh, but 587 is always used for authenticated connections.  But 25 is *sometimes* used for authenticated connections, too.  Why should *they* not be protected (to the admittedly small degree that STARTTLS in typical configurations supplies much protection).
If port 25 these days is mainly useful to spammers, then block it.  While annoying, this has been the expected - and generally accepted - behavior for years.  Based on the lack of any follow-on reports, it appears to be what everyone but Cricket does.  Having a single small mobile provider support enable port 25 provides no *advantages* - no widely used software will rely on it, because it's pretty much certain *not* to be there.  So the *positives* of "enable port 25 but disable encryption for it" compared to "block port 25" are pretty much nil.  The *negatives* are probably fairly minor right now - but we've all seen how "mission creep" works in the world of Telco's and ISP's.  Give them a *fraction* of an inch, and in a couple of years they've taken a mile.
"Silently disabling encryption" should simply not be acceptable behavior.
                                                        -- Jerry

@_date: 2014-11-20 16:55:59
@_author: Jerry Leichter 
@_subject: [Cryptography] FW: IAB Statement on Internet  Confidentiality 
Not only does this list have a public archive, but it has open membership. Anyone wishing to "snoop" on it need merely join.  Hello out there, NSA representative.
Neither of these facts about the list has changed since it was created.  Nevertheless, proposals to encrypt it surface periodically.
We have here a wonderful "teachable moment" concerning the need to define the relevant security properties of a system before trying to attain them.  If you don't know where you're going, any road will get you there....
(Perhaps we *should* enable encryption as a way of testing just how well - or, more likely, badly - existing solutions work.  It might inspire someone on the list to do better.)
                                                        -- Jerry

@_date: 2014-11-20 22:11:47
@_author: Jerry Leichter 
@_subject: [Cryptography] Walmart fooled by non-authenticated web pages 
[Walmart scammed into matching the "price" of either fake or erroneous on-line pages.]
It's not as if this is a new possibility.  Stores have long matched prices in newspaper ads - which are also sometimes erroneous, and it's not all that hard to fake a newspaper ad either.  In fact, I recall getting prices matched *entirely on my say-so* of a price someone else gave me.  Stores have considered this an acceptable risk because most people wouldn't bother.
But ... they've also had a simple way of preventing significant fraud.  The cost to the store of the item, and its general price in the market, are both things that are pretty well known to the sellers.  They know what a reasonable markup is, and what a reasonable price is.  If you come in with a newspaper ad offering an item at an "unreasonable" price ... at that point, they'll want proof.  In the case of an erroneous listing, they can generally avoid paying because they can call the other store and ask what it's price is - and they'll get a confirmation that the ad was wrong, and won't match it.
The only unusual thing here is that Walmart apparently believed that someone else was selling a popular, in-demand device for a quarter the going price.  That's highly unlikely.  It's particularly unlikely that anyone would go to a quarter of *Walmart's* price, since they are pretty aggressive on being the low-price supplier.  I would have expected them to check.  (Then again, reports are that Walmart has seriously cut back on store personnel in an effort to keep margins up.  Perhaps the real vulnerability here has nothing to do with the Web and everything to do with not enough people with not enough time to check things properly.)
                                                        -- Jerry

@_date: 2014-11-21 11:15:37
@_author: Jerry Leichter 
@_subject: [Cryptography] IAB Statement on Internet  Confidentiality 
Actually, why is all that necessary?  Consider the following algorithm:  Each list member, and the moderator, has a public/private key pair.  All list members share their public key with the moderator, and the moderator shares his public key with them.  Flow:
- Sender chooses a random message key; encrypts his message with a symmetric algorithm; signs it with his private key; appends the message key, encrypted with the moderator's public key.
- Moderator checks the signature; decrypts the message key; adds a second level of signature to the (signed by the sender, encrypted) message; and, for each list member, forward the doubly signed, encrypted message, with the message key encrypted with that member's private key, to the list member.
- List member checks both levels of signature, decrypts the message key, decrypts the message.
Yes, this sends different messages to each recipient.  But if that's a problem, any number of encrypted message keys can be attached to a message, so you can send the same message text to groups of recipients.
                                                        -- Jerry

@_date: 2014-11-22 11:48:54
@_author: Jerry Leichter 
@_subject: [Cryptography] encrypted list mail, 
Probably nothing.  I never claimed originality - the design seemed obvious to me, and the only reason I wrote it out was to answer the claim that an encrypted list would have to re-encrypt and re-sign every message for every recipient.
(An interesting thing about the design, BTW, is that if all group members also share a secret key that is *not* revealed to the moderator, then by super-encrypting with that key in addition to using the session key, we can have a system in which the moderator - more properly the forwarder, in this case - can properly forward the mail, but isn't able to read it.)
                                                        -- Jerry

@_date: 2014-11-28 08:32:49
@_author: Jerry Leichter 
@_subject: [Cryptography] [cryptography] Underhanded Crypto 
That's a wonderful (and scary) point.  It's obvious when you point it out, but I must say I've never focused on it as a general property - just as a property of *particular* protocols and implementations, SSL in particular.
Many years back, I was pushed into doing a protocol and crypto implementation for a system I worked on.  (It had been designed so it could work easily through an SSH tunnel, but that was seen as too complicated.  For various reasons, some better than others, we needed to do pretty much all the work from the ground up.)  The design I came up with was actually pretty simple and not a hell of a lot of code - but then I knew about holes in it when I designed it.  We didn't have the time or manpower to do even what we would have considered a reasonably secure design and implementation.  (In fact, while it had a space for authentication using a novel technique that I still think was clever - though it's mainly obsolete due to the later development of combined encryption / authentication modes), we didn't have a chance to fill it.)  Some of the basic techniques were taken from the leading-edge crypto papers at the time - but have since been shown to be flawed.
Converting what we built into something actually up to today's standards would have required several times as much complex, finicky code as was already there.
(The most ... interesting ... discussion about the system was with a military contractor who wanted to use the thing in a tank communication platform.  They started asking about how we did our crypto.  After explaining a bit, I asked them if they *really* wanted to rely on a crypto system a small company had designed and built - didn't they just get crypto boxes from NSA?  We left it at that.)
Anyhow ... are we really in the situation where the complexity of the protocols is great enough that we simply *can't* get them right?  The attacks of the last year, from Heartbleed to Poodle, certainly point in that direction.  Yes, Poodle was an attack against a protocol that should have died years ago - but security is a *system* property, and if we can't get old protocols to die when they must, out *system* isn't secure.
It's not easy to come up with solutions.  Some of our "best practices" make things worse:
1.  The insistence that you don't "roll your own" crypto.  If crypto from the experts really was solid, this might be good advice.  But what *that* is fragile, it creates a monoculture in which any failures have very broad effects.
2.  The insistence that systems be universally useful, rather than one-off's, ties into 1.  It also guarantees complexity.
3.  The insistence on covering every attack that anyone has thought of forces complexity.  Yes, attacks always get better.  Yes, you don't know the capabilities of your attackers.  But just how much security do you *really* need on your credit card number?  On your tax information?  Pretty much everything most people want their crypto to protect is available in other ways.
Would we be better off with a large variety of "good enough" simple designs and implementations which could reasonably be verified, than with a few "crown jewel" designs and implementations that no one can realistically have any confidence in?  At the least, this may be the best approach to stopping the mass surveillance we now know is being practiced - which has been combined with subtle "supply chain" attacks against the very "crown jewel" protocols and designs we've relied on.
                                                        -- Jerry

@_date: 2014-10-02 11:35:46
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA versus DES etc.... 
One can read way too much into this rule.  There's a countervailing principle:  Defense in depth.  Your data is protected (a) by the secrecy of your algorithm; (b) by the secrecy of your code.  The enemy needs *both* to read your data.  Why give him one for free?
Granted, the algorithm lives much longer and is much more widely distributed than any given key.  So in your analyses you're going assume that the probability of the algorithm leaking is much higher than that of any given key being lost.  But that doesn't change the basic assumption needed for defense in depth:  That failure of any given level is *independent* of failure of any other level.
NSA has traditionally favored crypto embedded in hardware.  The hardware itself is subject to defense in depth.  It's kept in secure locations, and there are mechanisms for quickly destroying it if it's about to fall into enemy hands.  The hardware itself resists attack.
"The enemy knows my algorithm" is akin to "the enemy will figure out my attack plan".  Yes, you try to keep the attack plan secret.  But it will eventually become clear to the enemy, and you'd better be prepared for what happens when it does.  That doesn't mean you don't do your damnedest to keep the plans secret until the last possible moment.
                                                        -- Jerry

@_date: 2014-10-02 12:04:38
@_author: Jerry Leichter 
@_subject: [Cryptography] Cryptography for consensual sex in California ? 
Phone companies have done this (in a limited way) for years:  If you are faced with a harassing phone call, you hang up and enter some key sequence. Information about the caller is saved at the phone company, which will make it available only to internal investigators or the police, and only at your request.  In the past, they'd sometimes require you to sign an agreement to prosecute before they'd give you the information.
This pre-dates CallerID, and the information saved is typically not blockable by the caller.  (These days, with the wide use of private PBX's and VoIP, it's probably no longer all that useful against anyone but dumb callers.)
Since CallerID has made the identification of the caller almost universally available, the old "privacy" arguments that drove the design are mainly irrelevant today.  In fact, the whole mechanism is probably more or less obsolete.  I'm not sure if it's even offered any more.
                                                        -- Jerry

@_date: 2014-10-02 16:17:52
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA versus DES etc.... 
A friend (Martin Minow, in case anyone here remembers him) years ago told me a story he heard in Sweden.  The rivers and bays along the Swedish coast are extremely tricky to navigate, being full of underwater canyons and ridges.  For many years, the maps of some of the major ports were considered to be essential state secrets, a means of defense against (mainly Soviet) naval attack.  If you wanted to sail into one of these harbors, you had best get a Swedish pilot, who had access to the maps but would ban you from watching while he used them.
One day, a number of Soviet vessels arrived for some kind of political port visit.  The Soviets had always asked for Swedish pilots in the past - but for some reason this time they went ahead and sailed up river at speed, easily navigating around the not-quite-so-secret underwater obstacles.
This was certainly a decision made at a high level in the Soviet Navy, if not at political levels, though as far as I know, exactly *why* they chose to do it has never been explained.
The Swedes, being reasonable people, stopped classifying the maps.  (If they had followed current American practice, anyone with a government position would have been required to continue to treat the information as secret.)
                                                        -- Jerry

@_date: 2014-10-03 11:15:43
@_author: Jerry Leichter 
@_subject: [Cryptography] Creating a Parallelizeable Cryptographic Hash 
Keep in mind that "parallelizable" is often taken to mean "linear in the number of available processors".  No tree algorithm is "parallelizable" in this sense - it has a logarithmic delay to roll up the results.
Some encryption modes - e.g., CTR mode - *are* parallelizable in this strong sense.  But it shouldn't be hard to prove that hashing can't possibly be.  (In fact, I suspect that just the requirement that flipping any bit of the input has a 50% chance of flipping any given bit of the output should be enough to show that.)
                                                        -- Jerry

@_date: 2014-10-03 15:56:46
@_author: Jerry Leichter 
@_subject: [Cryptography] Creating a Parallelizeable Cryptographic Hash 
This construction is insecure.  Call your function J.  Let A and B be any L-bit values.  Then:
J(A || B) = H(A || 0) ^ H(B || 1) ^ H(1)
          = (H(A || 0) ^ H(0)) ^ H(0) ^ H(B || 1) ^ H(1)
          = J(A) ^ (H(0) ^ H(1) ^ H(B || 1))
(Your use of N is a bit odd - it "feels" like it's the number of blocks, but in fact it's one less than that.  This becomes obvious here when you have to add the constant length-dependent terms - with one block, I expected to add H(1)!)
So if I'm given J(A) for an unknown A, I can compute J(A || B) for any B.  This is a form of length extension attack.  In this case, I don't think the usual tricks for preventing length extension attacks, like replacing H by an HMAC based on H or just wrapping an additional H around the whole thing, will help you - they are meant for cases where there is a secret key that the attacker doesn't have access to, but here everything but the original message hashed is assumed to be available.
                                                        -- Jerry

@_date: 2014-10-04 15:50:01
@_author: Jerry Leichter 
@_subject: [Cryptography] Best internet crypto clock 
There are two issues here:  The clock, and the original problem of establishing that some event occurred no later than a given time.
The first isn't hard to solve, in the traditional way of producing trustworthy random number generators:  Simply have NIST, the NSA, the EFF, the Russian and Chinese governments - whoever is willing - implement beacons.  To produce a beacon you trust, choose any subset, combine the "random" numbers, and sign the result in the usual way.  The subset and the method of combination are all public and committed to; all the inputs are public.  Since the individual beacons can only be corrupted by entirely stopping them, or by producing predictable (to the attacker) values, unless someone corrupts *all* the sources, the combination is unpredictable.
The question of replicating the "picture of the kidnapped person" scenario, however, seems impossible.  Consider what it claims to deliver:  Anyone looking at the photo, at any time after it was made, can be sure that the person in the photo was actually alive when the photo was taken, and the photo could not have been taken earlier than the date on the newspaper.  Well, maybe that was more or less true back in the days of black-and-white photography; but there would not be the slightest difficulty in faking such a photograph today using Photoshop or similar software.  You then are reduced to the battle of the photo experts - the ones who produce better and better fakes vs. the ones doing better and better detection of fakes.
The fundamental thing you're trying to prove is that some *event* - the taking of the photograph - took place after some time T.  This isn't the kind of thing we deal with in cryptography, where the usual starting point is "some string of bits" B.  Proving that "some string of bits" could not have been produced before T seems difficult.  In fact, if you pose the problem as "combine B with some other string of bits S(T), such that the result proves that B was not known before T", the problem is clearly insoluble.
(Before you go, oh, but you can commit a hash of B to the blockchain at time T - that solves the *inverse* problem:  It proves that you knew B *no later than* T.)
If you instead go back to trying to solve the original problem, you can pose it a different way:  I want to "apply" my victim to S(T) to produce an output that (a) only the victim could have produced; (b) could only be produced with the knowledge of S(T).  For example, suppose that voice-printing were an infallible way of identifying a speaker.  Then we could use a recording of the victim reading S(T) aloud.  (Of course, "infallible" has to include the ability to detect splices and other ways of modifying or combining recordings made earlier to produce the "proof of life".)  Having him write it out with pen and paper would work about as well.
If there were a way to produce a (digital) signature based on "something you are" - assuming that this becomes unavailable after death - then the victim's signature of S(T) would serve this purpose.  Some of the work on biometrics might eventually get us there, though it seems doubtful.
I'm not even sure how to pose a general version of this problem.  There are some special cases that work and might be useful.  Extending the signature example, suppose we have a tamper-proof signing box.  Using it to sign S(T) is proof of possession of the box at some time after T.  Perhaps this could provide some kind of proof of receipt.
                                                        -- Jerry

@_date: 2014-10-05 08:06:38
@_author: Jerry Leichter 
@_subject: [Cryptography] Best internet crypto clock 
I'm not sure what it is you would want to check.  The protocol for each beacon would be, at each time T, to send the triple , where T is the current time, Tp < T is the value that T had in the last emitted triple, and R is the random value.  The triple is signed using the beacon's signature.  Any collection of such value can be merged to create a new triple.  The Tp/T values are re-computed relative to the new triple.  A careful "combining beacon" will forward the triples that went into its computation.  (There are some obvious requirements for how the various input Tp/T value can be combined to produce the output Tp/T values.)
A signed triple from a beacon is self-identifying - there is no need for anyone who doesn't intend to use it as part of an assertion to store it, and there's no need for a history.  Chaining the values together makes it harder for a beacon to go back and "revise history", though how much that adds isn't clear - anyone who *uses* a beacon value will present a signed triple, and if the beacon ever lies about a past value that someone has used, it will get caught.  (If it wishes to lie about past values that no one used ... why should we care?)
(The reason for including Tp is that it makes the question "what was the triple emitted by a given beacon that had the smallest value T >= t?" unambiguously answerable.)
Someone here a couple of months back discussed an actual, real-world attempt to compute a value this way.  It failed.  (I searched around for it but was unable to find it....)  The numbers get corrected and changed after the fact.  The changes may be trivial, and they may be infrequent, but they are frequent enough to make the process fail.
While this wasn't part of the previous posting, I think the lesson to be learned is that public sources like this *make no claim that what they've published will never change*.  There's no reason why they should - such a claim isn't relevant to the reason the sources exist.  Errors get corrected, stuff gets reformatted to match some new standard.  The nominal semantics of what's in the database is supposed to never change, but that's based on human understanding, not something you could readily create a hash from.  And, in fact ... even *that's* probably not true.  Documents get screwed up in production - someone leaves out a paragraph, or includes some material in both old a new forms by mistake, or does something else that doesn't get noticed until later.  Then the document gets fixed and the database updated.  *Maybe* the new one gets a "Revised" marker.  Almost certainly, however, the old document gets deleted:  Saving it doesn't add to - and likely detracts from - the value of the database.
Ask yourself:  To whom would this be valuable?  Would the value exceed the cost of maintaining such a thing?  You cite the SEC as an example of a potential user, but there is, as far as I can tell, nothing in any SEC regulation that would require such a thing.  It would supposedly be for protection against someone producing a faked version of a document from the past.  While such issues aren't common, they do occur - consider Paul Ceglia's claim that he owns half of Facebook.  We already have plenty of ways of investigating the validity of such a claim.  Unless you *require* that all documents be added to this database, anyone creating a fake will simply say "Oh, we didn't think it was important at the time so we didn't send it in."
                                                        -- Jerry

@_date: 2014-10-07 07:14:47
@_author: Jerry Leichter 
@_subject: [Cryptography] 1023 nails in the coffin of 1024 RSA... 
tweeted:
Well, sure, but N/ln N is just an approximation.  For all you know, the true number of primes could be only a millionth of that!
                                                        -- Jerry :-)

@_date: 2014-10-07 14:58:49
@_author: Jerry Leichter 
@_subject: [Cryptography] Creating a Parallelizeable Cryptographic Hash 
This falls immediately to a prefix attack:  If I know Digest(M) and length(M) (assume for simplicity that length(M) is a multiple of the block size) then
Digest(M || Bn+1) = Digest(M) * H(n + 1 || Bn+1) mod p
- taking the remainder mod p twice produces the same result as doing it only once.
You've tossed around a powerful result without tying it to the security of what you wanted to secure!
                                                        -- Jerry

@_date: 2014-10-08 10:33:17
@_author: Jerry Leichter 
@_subject: [Cryptography] Creating a Parallelizeable Cryptographic Hash 
Urk, I kind of applied the wrong tool.  In fact, the iterative structure of the commonly used hash functions all have this prefix property - which is what makes the obvious techniques of pre- or post-fixing a key to make a keyed MAC from a hash fail.
However, this has always been seen as a weakness.  You'd like a hash function to "look like a random function".  Anything that makes it look less like a random function leaves potential traps for the unwary - many, many people have made the mistake of using H(key || X) as a keyed MAC.  It's good to have robust primitives that are hard to break.  Every algebraic property of a hash is also a potential trap for the unwary.  In fact, the newer generation of hash functions, I believe, do *not* have this prefix property - nor do they have any other algebraic properties anyone is aware of.
In the case you propose, there's another issue:  Digest(X) == 0 if and only if one of the constituent hashes is 0.  That breaks one of the basic requirements for a secure hash function:  Given H(X), it's difficult to find a Y != X such that H(Y) == H(X).  When H(X) == 0, it's trivial to find as many examples as you like.
I'm not sure what the goal is.  This is not a property of a random function, so is not a property expected of a secure hash function.  You're proposing a new primitive, with its own set of security properties (which you haven't fully written down), and which may or may not be useful.
See above; this is false when D == 0.
I have no idea.  You would need to propose a use, define the security properties needed for that use, then show (under appropriate assumptions) that you've attained them.
Note that the cost of computing your digest is at least double that of simply doing a hash over the original data (as you run the hash on twice as much data - not to mention the cost of all those modular multiplications). You'd need to justify that cost.
I have no clue.
I also see no obvious advantage to your scheme over simply adding the constituent hashes together (effectively mod 2^n) - a much, much cheaper operation which doesn't have problems with 0.  (You can make it even cheaper by considering each H() value as a vector of 32- or 64-bit values and adding them as vectors.)  Can you describe an attack on the cheaper approach that fails for yours?
                                                        -- Jerry

@_date: 2014-10-11 15:57:54
@_author: Jerry Leichter 
@_subject: [Cryptography] HP accidentally signs malware, 
Yes ... but.  Part of the problem here is that revocation was never meant, and is clearly inappropriate for, this application.  If I sign a software update using an "unleaked" key, that signature, on that update, is logically valid forever.  The fact that the key is later revoked changes nothing.  And, in fact, *allowing* to change "past facts" produces high costs.  Patches - or, really, distributions of software - can have an extremely long lifetime.  Going back and re-creating them with a new key is a big undertaking.  Worse, *the big advantage of a signature is that it eliminates dependence on the distribution channel*.  If things work as they are supposed to, I can safely download a properly signed copy of an updated from Malware-R-US and be quite sure that the bits I got really came from the signer.
But lets consider the failure modes.  Suppose a signing key is leaked.  You obviously can no longer trust anything created after the leak - but stuff signed *before* the leak is still fine.  But you need to determine *when* the signature was done - which brings us back to the recent thread on secure timestamps.  If (a) a software distribution is signed; (b) that same software distribution is time-stamped as "created before time T" in a way that can be checked independently of the signature (i.e., the fact that it says "Made in 2005" inside the signed envelope means nothing); (c) a revocation indicates the last known "secure" date; then you get the most usable possible system:  Artifacts signed before the key was leaked are still verifiably safe; artifacts signed after the key was leaked will be rejected.  (You can argue about how someone can definitely give a "good up to" date, but then any use of an unrevoked key is implicitly an assertion that it hasn't yet leaked....)
In HP's case, we have a completely different issue:  The key, according to HP, was never leaked.  (Their assertion that they were never hacked *is* meaningful, because this is what it's asserting.)  So in fact *all signatures with that key are valid, and will remain invalid indefinitely*.  The problem isn't with the signing; it's with *one particular piece of software that was signed*.  Revocation is *not* the "right" solution here, as there's nothing at all wrong with the key or any signatures.  The *right* solution is something like what Microsoft does to blacklist particular pieces of software.  If HP had something like this, they could blacklist the malware without changing any keys or re-issuing any patches.
The problem of the validity of signed material has been discussed for years, and my comment about the need for timestamping is not new.  (It probably appeared in the papers discussing uses for digital timestamps!)  The only attack against a signing system I've ever seen mentioned is signing key leakage, and as a result, the only solution on offer is revocation.  What we have here is an entirely different attack, which directly contradicts the usual assumptions about signing:  Yes, my signing provides a perfectly correct proof of provenance; but "what I said wasn't want I meant".  In the typical toy examples of digital signatures that get discussed, what's signed is always an assertion or a commitment, and the whole point is to bind it to the signer forever.  Here we're signing something that, if interpreted as an assertion or commitment, loses its whole point:  It's something that actually affects the real world, and it's potentially harmful.  A different attack requiring a different defense.
                                                        -- Jerry

@_date: 2014-10-11 20:28:07
@_author: Jerry Leichter 
@_subject: [Cryptography] HP accidentally signs malware, 
Microsoft has had such a mechanism - known as a killbit  - for many years.  It applies only to Active-X controls - it's not clear why they never extended the idea to arbitrary code.  However, they could probably get essentially the same effect with their malware scanner.
OS X has a similar mechanism with its simple-minded malware blacklisting mechanism, which has a special-purpose extension to do such things as blacklisting outdated versions of Java and Flash.
iOS apparently includes a "kill application" mechanism which would allow Apple to quickly prevent a malicious app from running.  (Apple has never used this, saying it's there for emergencies.)  I don't think Android has an equivalent mechanism, and it certainly wouldn't work for stuff installed from alternative stores.
Generally, these are integrated with patch mechanisms (though I think the iOS one is an "instant push"):  You don't poll the "CRL", you update it on a schedule like anything else.  While instant revocation might be useful in extreme situations, in practice even something polled every week would be a huge improvement over the (almost nothing) we have today.
I'm not aware of any similar mechanism in the OSS world (which doesn't mean it isn't out there).
The extreme version of all this is whitelisting of software - pioneered in the Windows world by bit9, now also available (though I don't know any details) in Windows itself.
Actually, they could have done something else:  Send out a patch that specifically looks for this malware and kills it, and also updates the patch mechanism to filter out any subsequent attempts to install the malware.  Microsoft has done this in the past enough times that they have the mechanism fully developed.  HP would have had to start from scratch.  Frankly, it's not clear that what they did makes a whole load of sense.  Since signatures are only checked during installation, they haven't done anything at all to protect customers who already installed the malware - and it's been out there for quite some time.
When all you have is a hammer and you want to look like you're *doing* something ... go bang on whatever it is.
I was deliberately distinguishing between two problems:  The bad software with a proper signature, and the leaked signature.  The timestamp is useful only for the latter case, where it's really an optimization of a CRL.
Again, two distinct problems:  The signer declaring "don't trust this signature (if made after time T)" vs. "don't trust this piece of code, I no longer believe it's safe".
That would be a fine idea.  As I pointed out above, the closed-source world does this kind of thing.  I suspect it hasn't made much headway in the OSS world because many people - especially the developers - use OSS exactly because they want the freedom to run whatever they want.  The notion that *someone else* - even the author of the software - could shut down their ability to do what they want on their own box would be anathema to many in the OSS community.
That doesn't mean such a mechanism couldn't be built for those who want to use it, of course.
                                                        -- Jerry

@_date: 2014-10-13 15:20:13
@_author: Jerry Leichter 
@_subject: [Cryptography] [messaging] Gossip doesn't save Certificate 
How many years did it take to get here?
Still, I'm glad we're here.
Browsers are already moving to automatic updates.  Updating the list of keys could be done on a faster schedule.
Today's internet isn't yesterday's.
Why?  I did the calculation in my original posting.  You can cover the top 100,000 sites in 30MB.  That's the size of a couple of image files used to make the browser demos look nice.
Plus ... the *changes* to the list are very simple:  Just insertions and deletions, nothing fancy.  So distributing deltas is simple and very cheap.
As opposed to ... what?  That they rely on the CA's to be the *official* managers, while the browser makers - who after all are the ones with their hands on the code that actually *uses* all those cert's - just stay in the background, with no one needing to examine where they fit in the trust framework?  Besides, if others want to distribute key lists - why not?
Multiple-centralized is good if you can trust the aggregate as long as you can trust any single member (ideal case) or the majority (a common case) or really any fraction of the members of the trust set.  But what we have now requires trust *everyone*.  That's *significantly worse* than the single-centralized approach.  (Besides, again, anyone can offer to maintain and distribute a key collection.)
You really need to look at the details of what they provide to decide if they help at all.  (See the recent long-running thread here complaining that CT doesn't actually help anything.  I've stayed out of that entirely because I'm not sure who's right, so I want to see both sides present their best arguments.)
But the point of my posting was not to attack attempts to improve what we have, but that we should *also* consider "clean sheet" solutions (even if some of them are retreads of ideas form 20+ years ago).
                                                        -- Jerry

@_date: 2014-10-13 17:08:09
@_author: Jerry Leichter 
@_subject: [Cryptography] Secure parallel hash, with constant time update 
You can take it as a sign that people aren't very interested.
I lost interest after an exchange we had that went:  "It's secure because of the discrete log problem.  But it violates its own basic security requirements when it produces a 0 result!  Oh, that's so unlikely - why worry about it?"  At that point, we left the realm of mathematics and proofs for someplace else where I, for one prefer not to go.
Best of luck.
This is my last message on this subject.
                                                        -- Jerry

@_date: 2014-10-15 18:32:21
@_author: Jerry Leichter 
@_subject: [Cryptography] [messaging] Gossip doesn't save Certificate 
a)  Deltas will be tiny.  How often does a site need to change its keys?
b)  You never connect to WiFi?  Just how up-to-the-minute do you need your list of keys to be?
What modern web sites are they looking at over dialup?
At some point, one has to move on and stop supporting IE6 :-).  Should we also worry about people still using 2400 baud modems?
                                                        -- Jerry

@_date: 2014-10-16 07:43:14
@_author: Jerry Leichter 
@_subject: [Cryptography] Secure transfer of subsequences 
Suppose Alice has a sequence of bits S and wishes to securely transfer them to Bob, in the sense that Bob can prove that the bits S' he receives are actually the same as S.  Depending on what we mean by "prove", either a keyed MAC or a signature will do the trick.
But suppose S is very large, and Bob is only interested some subsequence T of S.  (That is, there is a sequence i0 < i1 < ... < ik such that
T == S[i0] || S[i1] || ... || S[ik].)   Again, Bob wishes to be able to prove that T is indeed of this form.  Obviously, Bob can accomplish this by transferring all of S and then constructing T, but if |T| << |S|, this is very wasteful.  Bob would like to do this by transferring a number of bits "not much larger" than |T|.  (If the subsequence really is arbitrary, the cost of sending the i's might dominate.  That's not the interesting part of the problem, and can be ignored.)
Does anyone know of solutions to problems of this sort?  For the case that the subsequences consist of a small number of long contiguous runs of bits, using a tree hash on substrings of contiguous bits of S might work.  I'm guessing that *some* constraints on S are necessary, but who knows.
In general form, the closest thing I've seen to this is "proofs of storage".  In terms of this problem, Bob sent S to Alice, who promises to keep it stored; Bob will later ask for something small - T might be an example, but is not enough - that Alice will be unable to produce unless she has, indeed, retained all of S. There are some clever ideas here that might be adapted, but I haven't kept up with the literature.
                                                        -- Jerry

@_date: 2014-10-17 07:32:43
@_author: Jerry Leichter 
@_subject: [Cryptography] [messaging] Gossip doesn't save Certificate 
Since we're talking specifically about size, keep in mind that my 30 meg estimate was deliberately on the high side, assuming 100,000 RSA keys and uncompressed site names.  If you use ECC, compressed site names, and let those for whom size is a major issue use a shorter list, the file could be dramatically smaller,
Keep in mind that for this to work there needs to be a proper key rollover procedure.  Presumably the site will send you its new key signed with its old one, which you'll normally accept if your copy of the old key is recent enough.  That doesn't handle revocation in the case of key compromise - but that's impossible without real-time validation of keys, which (a) is an independent issue that's the same whether we are talking certs or keys; (b) comes with its own bag of problems.
A decent implementation would need to allow you to control this.  It's not hard.  (In fact, in iOS the ability to use cellular data is directly controllable on an app by app basis through the OS, exactly to deal with this kind of thing.  I don't know if recent versions of Android provide the same capability.)
I stopped thinking of dialup as a significant constraint when by wife's parents finally, after complaining for years about how "they take away the stuff you use and make you pay more", finally dropped their AOL dialup account and got their Internet through the cable provider they were paying for anyway.  One bases one's views on personal experience, I guess.  The most recent statistic for the US that I was able to find in a quick search was a Pew survey done in May 2013, at which time 3% of users used dialup for Internet access from home - a percentage that hadn't changed since an August 2011 survey, even as broadband usage grew from 62% to 70%.  Having seen - even a couple of years back - just how limited a view of the Internet dialup provides today, I wouldn't worry very much about how to provide crypto updates to that remaining population.  What they need much more is better access.
I don't see any particular responsibility on anyone's part to adjust things to an obsolete least-common-denominator.
Back in the 1970's, I designed protocols that had to run in Europe over X.25.  The European PTT's at the time had a complete monopoly on any communications infrastructure that cross public space.  If you had two buildings on opposite sides of a road, you couldn't run a wire between them - you had to use the PTT and X.25.  X.25 was charged by the packet - I think 128 bytes.  Send a one-byte ACK - pay for a packet.  And that packet did not come cheap.  Designing for this environment was crippling.  Imagine if the designers of IP had been told that it had to work - at "reasonable cost" - over X.25.
Times change.  Technology changes.  Yes, people get left behind unless they are in a position to upgrade.  The US continues to have massive fraud problems with credit cards because we refused for so long to move off of magnetic stripe technology:  Just think of the cost of replacing all those POS systems!  Even now, we're doing a half-way move to chip-and-signature, which is only a small improvement over chip-and-pin - but does a better job of keeping all those "legacy" players going.

@_date: 2014-10-19 09:01:28
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA versus DES etc.... 
It turns out this isn't just "the dead past".  See "Sweden searches for suspected Russian submarine off Stockholm.  Helicopters, minesweepers and 200 service personnel mobilised in search after tipoff about ?foreign underwater activity?".
                                                        -- Jerry

@_date: 2014-10-21 11:47:25
@_author: Jerry Leichter 
@_subject: [Cryptography] Better Version of Triple-DES 
Please define "better".  (You should probably also define what "the second encrypting being the data" is supposed to mean.)
                                                        -- Jerry

@_date: 2014-10-21 14:40:33
@_author: Jerry Leichter 
@_subject: [Cryptography] Best internet crypto clock 
That's a cool technique.  Do you have any references?
                                                        -- Jerry

@_date: 2014-10-22 12:46:17
@_author: Jerry Leichter 
@_subject: [Cryptography] A review per day of TRNGs: OneRNG 
Sounds like a great application for "sparkly nail polish" security.  Paint over the access points - the outside screws, the chips and on to the board, over a piece of tap sealing the USB - with one of those nail polishes with sparkly bits in it.  Take photos of each spot and deliver separately from the device itself, preferably through multiple channels (e.g., send in a separate envelope, and put signed copies on line).  The exact speckle pattern is random and as far as I know impossible to duplicate.  It's also easy to check "by eye".
                                                        -- Jerry

@_date: 2014-10-23 07:27:51
@_author: Jerry Leichter 
@_subject: [Cryptography] Best internet crypto clock 
Somehow we went from using the AC hum as a forensic mechanism to using it as a clock.
The use of the 60 (or 50) Hz baseline power frequency to produce accurate electric clocks goes way back.  In fact, this was a usage specifically supported by the power companies:  While all the generators in a system need to be synchronized, there's no need for them to maintain long-range stability or remain centered at 60Hz.  But cheap, accurate, synchronized electric clocks were an early selling point, so the systems were built to actually stay close to the nominal center frequency, and were deliberately manipulated to long-term average stability.  To do this, the systems themselves need an accurate time scale to refer to - and most likely they rely on NIST.  So you'd be getting the NIST timebase, with noise.  On a human scale, over reasonable human periods of time, the errors are nil.  On a scale appropriate to today's computers, the story would be very different.  Deliberately manipulating the frequency across a whole grid for long enough to matter to humans would be extremely difficult and would get noticed:  We now have other, accurate time providers to compare our old electric clocks to.  Of course, if you're manipulating the environment, you can plug the device, not into the wall, but into your own frequency generator and make it see whatever you like.
The article Jonathan Thornburg linked to ( describes the *forensic* mechanism.  It's based on looking at the short-time-scale variations in frequency around the nominal center point.  These are caused by variations in load, variations in supply (generators coming on and off line), lightning strikes, surges due to solar weather, and the interaction of these effects with the synchronizers that are put into the system exactly to keep those variations under control.  They are *not* local, but are constant within a single electric grid - synchronization across a grid is exactly the point!  Grids are very large.  England is covered by just one grid.  The continental US is covered by something like three, if I remember correctly.  (It may be a bit more, but we're still talking a handful.)  These variations are unpredictable in detail, but easily recorded anywhere on the grid.  Recording them *deliberately* as an absolute measure of time is an interesting idea - essentially aid the use the forensic technique.  In principle, one could probably replay past variations in a highly controlled setting to make it look as if a recording was made at some point in the past, but sounds rather hard to accomplish.  How successful one might be in replacing a recorded signal with a different one without leaving detectable artifacts is impossible to say without actual testing.
BTW, there's an interesting contrast here between a "tick generator" - which gives you an accurate repeatable way to step a clock from some fixed point - and an "absolute time reference", which lets you map from something (like a record of hum frequency variation) to absolute date and time.  We generally think of "clocks" as tick generators that we start off at some external date and time; absolute time references are relatively infrequent in day to day use.  (They are omnipresent in analyses of records of the past - e.g., looking at the stratum in which a fossil is found as a way of dating it.)  There's a Youtube video out there of a "clock" that measures elapsed time directly by looking at a couple of simple measurements that change as a potato rots....
                                                        -- Jerry

@_date: 2014-10-23 21:06:50
@_author: Jerry Leichter 
@_subject: [Cryptography] Samsung Knox 
Proving again that (a) most companies have no clue how to do security; (b) most government agencies have no clue how to audit it; we have two related bits of news:
1.  Two days ago, Samsung proudly announced that "Samsung Galaxy Devices based on KNOX platform are the First Consumer Mobile Devices NIAP-Validated and Approved for U.S. Government Classified Use" -
2.  And this was followed by:
which completely demolishes Knox security.  (The user's password is encrypted using a key derived from a fixed constant and a device serial number available to any app on the device.)
I would laugh if I weren't crying....
                                                        -- Jerry

@_date: 2014-10-24 12:22:41
@_author: Jerry Leichter 
@_subject: [Cryptography] Samsung Knox 
A big weakness with these certification programs is that you get to define the "box" that gets certified.  A reasonable bet is that "secure storage of the password" was simply not within the certification boundaries - it was just assumed to be secure.  Or, more likely, nothing in the certification boundaries had anything to do with storage of the password - it was written on the assumption that the password simply appears (presumably the user enters it) and then we go from there.
BTW, if you read the linked article, you realize how bad things really were.  The hole was found because the guy doing the analysis first figured out how to get at the "password hint" - which is automatically computed for you as the first and last characters, and the actual length, of your password!  (This alone is *already* a big security issue.)  He then wondered whether that was computed on the fly from the actual password - implying that it was somehow available in cleartext.  And, indeed - in some deeply buried and obfuscated, but ultimately reversible, code, it was.
If, indeed, my guess about how this slipped by is correct, then the validations are, according to the rules of the process, perfectly valid - which would put the validating agencies in an embarrassing position.  It's not as if those of us who've ever dealt with these processes don't already know that they have very, very limited worth - it's just that *most* users have no clue.
Defining the boundaries of the validation is important, of course - you don't want to fail a device because someone watching as its user enters the keystrokes can read the key.  But when I last had anything to do with these validations, there were basically no rules about where you put the boundaries.  Technically, you might only have certification for a low-level crypto module; but it was easy to describe things to imply to virtually everyone that it's the entire device that's been validated, all without violating the letter of any standards or regulations.
                                                        -- Jerry

@_date: 2014-10-24 23:24:47
@_author: Jerry Leichter 
@_subject: [Cryptography] Uncorrelated sequence length, 
A CPRNG that is at least as "hard" as the algorithms with which it's used cannot provide a point of attack.  For example, if you rely on AES-256 for your cryptography, and your protocols are secure under the assumption (as is common these days) that AES-256 is indistinguishable from a random sequence, then generating your random numbers using AES-256 in counter mode with a true random key exposes you to no attack that wasn't already present.
Now, I'll agree this is not a very "clean" assumption.  You'd really like a random number generator that you can use with any cryptosystem of interest.  If you want to use ChaCha, then using AES to generate your "random" numbers leaves two points for analytic attacks:  AES and ChaCha.  What this argument comes down to is that there is no such thing as a generic CPRNG.  What's generic is the true random number generator - though it may only have to supply a fixed, fairly small, number of bits.  They can be used to initialize a CPRNG that should be part of the crypto suite, chosen so that it's no weaker, under the attacks considered, than any other part of the suite that depends on it.
                                                        -- Jerry

@_date: 2014-10-24 23:37:50
@_author: Jerry Leichter 
@_subject: [Cryptography] Simon, Speck and ISO 
If these are designed with the same approach as Skipjack, they will have *exactly* enough rounds to block differential cryptanalysis and perhaps some other attacks.  NSA seems to believe in designing to the edges of the envelope.  (They also appear to have more sensitive techniques than any available to the public for determining exactly where those edges lie.)
                                                        -- Jerry

@_date: 2014-10-25 23:32:13
@_author: Jerry Leichter 
@_subject: [Cryptography] Uncorrelated sequence length, 
It makes no difference whether AES-256 is "hard".  What matters is that "it's as hard as itself".  If there's an attack against it, it applies equally to the CPRNG and to the encryption.
This argument only works when you can show that any attack against the generator also gives you one against the encryptor.  For combinatoric algorithms like AES-256, this is unlikely to be something you can argue convincingly unless the same algorithm is used in both places.  (For algorithms with more mathematical structure, the story *might* be different.  In particular, something *like* Dual EC DRGB - but done in a way that ensured no one had a back door - *might* be provably as hard as some EC cryptographic algorithm.)
I don't know what this means.  Any *specific* property - like a long uncorrelated sequence length - is just a special instance of a way of distinguishing the output of some algorithm from a true random sequence.
                                                        -- Jerry

@_date: 2014-10-26 18:54:54
@_author: Jerry Leichter 
@_subject: [Cryptography] Best internet crypto clock: hmmmmm... 
Quite a few years ago, I argued that it should be possible to identify laser printers by small variations in toner placement.  The argument the other way was that manufacturing tolerances would make this impossible.  Nothing new here.
Manufacturing tolerances are reduced down to the point where they produce artifacts relevant for the use at hand.  For a laser printer, that means visual effects noticeable to the human eye at the closest distance a paper page is likely to be held in normal usage.  For a camera, it means visual effects noticed at the largest print sizes viewed at their appropriate ranges.
All of these things have fundamental limits set by human sensory capabilities.  Most of our digital technologies are near those limits - most obviously in high-resolution LCD displays (what Apple calls "Retina" displays).  Sure, under some circumstances, some well-trained observers can easily spot the remaining variations.  But it's getting harder every day, and soon only the "golden ears" (and their analogues in different spheres) will even claim to be able to tell, and they'll consistently fail careful tests.
Once you get to that point, there's no reason to go further in controlling manufacturing processes and such.  (Oh, some will for advertising points, but it's a very expensive business to wring out minor variations, so few will try.) And yet it's easy to *measure* details to orders of magnitude finer than you can *control* them.  We may yet be in the period where images are getting more controlled fast enough to annoy the authenticators - but that period will end soon.

@_date: 2014-10-26 20:54:29
@_author: Jerry Leichter 
@_subject: [Cryptography] A TRNG review per day: RDRAND and the right TRNG 
As has been mentioned here recently - and discussed in various papers - this last is false if the generator is forced to produce output while it's trying to recover.  If outputs are produced at a rate at least equal to the rate in which new entropy is fed into the pool, and that feed rate is low enough, the generator may never recover.  The work-around with the design as given is to block long enough to build up the necessary entropy.  This may be difficult if you have only a very conservative estimate of entropy to work from - you may have to block for a while.

@_date: 2014-10-27 06:17:59
@_author: Jerry Leichter 
@_subject: [Cryptography] Auditable logs? 
That's what expert witnesses are for.  Juries accept DNA evidence, which is technically much more complex than a hash chain.
(Of course, juries accept evidence from "trained experts" in what latter turns out to be pseudo-science and wishful thinking - like much bite analysis, for example.  But that's a different problem.)
                                                        -- Jerry

@_date: 2014-10-27 07:35:05
@_author: Jerry Leichter 
@_subject: [Cryptography] Paranoia for a Monday Morning 
We've seen increasing evidence that the NSA influenced the choice of cryptographic standards towards designs that were extremely difficult to get right - e.g., Dan Bernstein's claims that the standard elliptic curves have arithmetic whose implementations need special-case paths that make side-channel attacks much easier than they need to be.
As I look at the world around me, however, I see few proven attacks against fielded cryptographic implementations - but an ever-flowing stream of attacks against another class of standardized software.  I'm talking, of course, about browsers.  The complexity of browser standards - and of ancillary software like Flash - has proved way beyond our capability to program without error.  It's easy to blame Adobe or the Microsoft of old for incompetent programming; but even the latest IE, produced under what may be the best "secure software development chain" in the world; and Chrome, a clean-sheet, open-source implementation by a team containing some of the best security guys out there; continue to be found to have gaping holes.  At some point, you have to step back and admit that the problem doesn't lie with the developers:  They are being set up to fail, handed a set of specifications that we simply too hard to get right.
And that, of course, raises the question:  Accident, or enemy action?
                                                        -- Jerry

@_date: 2014-10-27 19:37:57
@_author: Jerry Leichter 
@_subject: [Cryptography] Paranoia for a Monday Morning 
Widely described that way by people who I trust.  Feel free to accept or reject the characterization.
Yes, but they put a huge effort into isolating the rendering engine to contain any problems.  And you need only look at the bugs reported and fixed *by the Chrome team* to see how seriously they looked at the code they did take.
I'm developing a new, completely secure-by-design language in which I'll implement a *really* secure browser.  It's not available yet, but trust me, it will *finally* kill off all those pesky browser security bugs.
OK, Mozilla and Rust are a real effort with real publications.  But let's keep in mind that Mozilla itself started out to be, among other things, a secure browser (not just "more secure than IE6", a rather low bar, but really secure).  So did Chrome.  Both were major steps forward; both have been attacked successfully.  No claim that some new implementation *will be secure* is worth very much.  We'll see how their new effort holds up "in the heat of battle".
You know, two years ago I would have similarly classified claims that the NSA was recording every phone call in whole countries, keeping records on every US citizen - and deliberately working to weaken cryptographic standards.  I agree with you that the NSA probably had little or nothing to do with the state of browsers - but mainly because they really didn't need to do anything, masses of attackable software were given to them for free.
                                                        -- Jerry

@_date: 2014-10-29 17:26:45
@_author: Jerry Leichter 
@_subject: [Cryptography] Best internet crypto clock 
I guessed this was a response to a posting of mine - and it is, to one back on October 4th!  I had to re-read it to guess that the "ends of the problem" might be.
As I pointed out, in and of itself, this doesn't do anything very interesting.  Using my notation - where S(T) is a new, unpredictable value made available to all no earlier than T - yes, if I utter S(T), that proves my utterance occurred no earlier than T.  But why would anyone care?  To be useful, I somehow need to bind S(T) to something else in such a way that I end up with the proof that the something else "occurred" no earlier than T.  For example, that the picture of my kidnap victim, clearly alive, was taken no earlier than T.  But given the picture and S(T) as bit strings, it appears to be impossible to do that.  There are ways of binding *events* to S(T) to produce proofs that the events occurred no earlier than T - but once you've frozen an event into a bit string, you loose the ability to establish how late it occurred.
Yes, this one is easy.
Yes, the running of the experiment is an "event", and if you use S(T) as an *input* in an appropriate way, you can prove that the event could not have occurred before T.
There's probably some kind of covariance/contravariance thing hiding here if you can set up the model correctly:  You can prove something occurs *after* T if it takes a published S(T) as an input parameter; you can prove something occurs *before* T if you take its output combine it with some public information (e.g., a public hash chain).
                                                        -- Jerry

@_date: 2014-10-31 15:08:11
@_author: Jerry Leichter 
@_subject: [Cryptography] Uncorrelated sequence length, 
The *test* "Has an uncorrelated sequence length of N or greater" is a special case of distinguisher from a random sequence.  Yes, if you are asking the question "Is this sequence distinguishable from a known random sequence?" you have to invert the output of the "USL > N" test, but that's a triviality.
BTW, I've responded based on the assumption that "uncorrelated sequence length" is actually a well-defined concept with a meaning based on the plain English words.  I just did what I should have done earlier:  A Google search in an attempt to find the technical definition.  The search finds exactly four instances of this exact phrase - all of them in the present discussion!  So I guess on the statement "Uncorrelated sequence length is a thing", the *correct* response is "citation needed".
                                                        -- Jerry

@_date: 2014-09-03 06:35:06
@_author: Jerry Leichter 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
There are, as far as I know, no ciphers unconditionally proven secure, based only on accepted mathematical axioms and theorems, other than the one-time-pad.
There are a couple of ciphers proven secure given specific non-cryptographic assumptions.  The earliest was probably Rabin's variant of RSA, which is secure if factoring is hard.  (RSA in general is *not* known to be as hard as factoring.)  The difficulty of factoring is arguably an old mathematical hypothesis (though even that is subject to some debate).  Note that Rabin's variant was constructed specifically to allow it to be proven equivalent in difficulty to factoring, but it's not really a practical cipher and as far as I know no one uses it.
Beyond that, all "absolute" proofs of security are reductions to problems that are further and further from pure mathematics and more cryptographic in nature, starting with assumptions about discrete logs (which while mathematical in nature were never really attacked as computational problems until they became of interest to cryptography) and ending with "AES is a pseudo-random permutation".
All asymmetric cryptosystems are in NP, so they can't possibly be secure unless P!=NP, which of course we haven't proven.  I believe there are (specially constructed) systems whose security is NP-complete, but of course again that's not a proof, it's a reduction to a widely-believed but unproven hypothesis.  (And being NP-complete is not in and of itself a proof a security anyway.)
The remainder are proofs of security against particular kinds of attacks - e.g., AES is provably secure against a very broad array of "differential cryptography" style attacks.
                                                        -- Jerry

@_date: 2014-09-04 14:29:13
@_author: Jerry Leichter 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
Consider the simple (technically incorrect, but one can formalize this) description of a problem being in NP:  If you can somehow guess a solution, you can verify that it is, indeed, a solution in polynomial time.
*Asymmetric* cryptography is in NP because given cipher text C and a guessed private key Kprivate, I can test whether Kprivate is the "right" key by computing E_Kpublic(D_Kprivate(C)) == C.  (If the encryption includes some random element, I need to do the test the other way around - D(E(msg)) == msg.)
For symmetric cryptography, this trick doesn't work:  Without the public key, I have no way, in general, of testing whether a guessed key is the correct one.  Yes, *in practice*, material with some kind of internal structure - say, English text - is easy to recognize.  If I guess an AES key and decryption of 10 blocks all produce recognizable English, it's overwhelmingly probable that I've got the right key.  But that's not what the *theory* is about.  To be in NP, it would have to be able to possible to confirm my guess even if the material being encrypted was completely random.
Indeed not (though if P=NP, hierarchy of harder problems does collapsed).  But there are plenty of harder problems out there.
Symmetric crypto avoids being in NP by a kind of a slight of hand (the non-checkability of the results).  But it's exactly this that allows one-time-pads to be provably secure:  For a given cipher text, any possible input of the same length corresponds to a key of that length.  An attacker has no way to check which of all the possible outputs is actually "right".
BTW, this has an interesting connection with the "encrypt then MAC" vs. "MAC then encrypt" "debate" (in quotes because the mathematics was actually settled long ago).  If you MAC then encrypt, an attacker has a way to check if an alleged decryption is valid or not.  This doesn't *quite* say that encryption after a MAC is in NP, since there may be many guessed key/MAC-key pairs that happen to produce decryptions that have a valid MAC, and there's no way to choose among them.  But it comes much closer than encrypt then MAC, which leaks no information about whether the decryption is valid.
                                                        -- Jerry

@_date: 2014-09-04 14:42:23
@_author: Jerry Leichter 
@_subject: [Cryptography] What is the difference between a code and a 
Traditionally, codes operated at the word level, while ciphers operated at the character level.
In the days of human encryption and decryption, codes relied on "dictionaries" that took a word to some specific sequence of symbols (in better codes, to one of a set of symbols to avoid leaking which sequences corresponded to common words), and symbols back to the word they came from.  The boundaries between words were as used in traditional grammar, and the boundaries between symbols were (necessarily) maintained.  Generally, punctuation was spelled out, so the input was really just words separated by spaces.  The encoding would usually just be groups of letters separated by spaces, too.  (Since the most common use of codes was in telegrams, the constraints of telegraphy would apply to the encoded form - and in practice would normally apply to the clear text as well.) Ciphers, conversely, transformed one character at a time.
Encoding "sand" and "sandy" would produce two completely unrelated "code words".
Enciphering "sand" and "sandy" would produce some four-character sequence for "sand", and that sequence followed by some fifth character for "sandy".
The boundaries here were always somewhat arbitrary.  There were ciphers that operated on pairs of successive letters, for example.  (In principle you could have made the "block size" as long as you liked, but for paper-and-pencil operation more than two letters - which would likely be used to specify a row and column in an enciphering table - was probably the practical limit.)
Codes in the traditional sense are no longer used.  (Actually, if I had to bet, they probably *are* still used in some specialized applications.)  The distinction is likely of historical interest only at this point.
                                                        -- Jerry

@_date: 2014-09-05 13:52:13
@_author: Jerry Leichter 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
This is a meaningless statement.  NP has an exact, technical definition.  There's nothing to "allow" or "disallow".
Whether "is in NP" or even "is NP complete" is relevant to cryptography is something one can debate - though not very much, as the answer is clear:  No.  (NP is about the hardest problems in some set.  Knowing that there is *some* hard problem is not particularly useful when what you want to know is that *a particular problem* - reading a particular cipher text, for example - is hard.)
But if you're going to talk about a technical matter, you have to use the language as it's understood in the community.
                                                        -- Jerry

@_date: 2014-09-05 16:18:02
@_author: Jerry Leichter 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
That's not a good way to say it; and, again, if we want to discuss a technical subject on a technical level, details matter.
P: There are problems we know have a poly-time algorithm.
not-P: There are problems we know do *not* have a poly-time algorithm.
Unknown: And, of course, there are problems where we don't know whether there's a poly-time algorithm.
Among problems in Unknown, there is a subset that we know *can* be solved in "non-deterministic polynomial time", something that has a number of equivalent formulations, but is perhaps easiest to visualize as "given a proposed solution, you can check whether it really is one in polynomial time (in the size of the original problem)."  If P=NP, then in fact all of those problems actually are in P.  If P!=NP, at least one of them is not. Since NP is a subclass of problems about which we lack some knowledge, it's inherently a "no harder than" descriptions.  There may well be problems that we currently put in NP (because we haven't found a poly-time algorithm for them) which we later determine to be in P.  Linear programming was long in NP - the best algorithm was actually exponential time in the worst case.  Then Karmarker showed that it was actually in P.
Among all the problems in NP, there is a further subclass of the NP-complete problems, which are the "hardest" problems in NP, in the following sense:  If you can find a poly-time algorithm for any NP-complete problem, it will implicitly give you a poly-time algorithm for all problems in NP.
BTW, factoring is in NP, but is not known to be NP-complete.  In fact, there is good reason to believe it is *not* NP-complete.
Actually that's not a good example.
The "hardness" in P and in NP are (a) asymptotic - an NP, or even an NP-complete, problem could be very easy for n up to 10^1000, but *eventually* turn into something for which we know of no poly-time algorithm; (b) about *worst case* time:  If, for every n, there is a one problem of size n such that the difficulty of solving those single, special problems grows faster than any polynomial, then the problem as a whole is not in P, even if "almost all" cases are easy.
Since we do cryptography with a fixed size parameter, and we want "almost all" - probably "all" - of our particular uses of it to be hard, the P and NP classes are simply irrelevant.
On the other hand, the question of asymptotic complexity *does* tell you something about the scaling behavior of your algorithm.  If your algorithm asymptotically takes time 2^n, adding 1 to n doubles the difficulty.  If you algorithm grows as n^50, adding 1 to n grows the difficulty by (n+1/n)^50, which grows pretty slowly even for reasonably small n.  So if you find you chose a security parameter that's too small, recovery for the exponential case may be easy, while recovery for the polynomial case - even with a polynomial of degree 50 - may be difficult.
But just what security parameter *do* you need?  It really depends on the constant terms.  If the "concrete complexity" (something that's usually very difficult to compute:  Real numbers, not just growth rates) of your exponential algorithm is 10^-100*2^n, it's not a reasonable choice unless n is absurdly large; with if you polynomial algorithm has complexity 10^100*n^50, even very small n give you a high degree of security.
Different measures of complexity are good for different things.  Use the wrong measure and your results are meaningless.
                                                        -- Jerry

@_date: 2014-09-07 12:45:52
@_author: Jerry Leichter 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
Suppose S(x,k) is AES(x || R, k), where R is a random bit string of the same length as x.  (This is a simplified version of the randomness you need to add to get semantic security anyway.)  You can try all the keys you like, but you're unlikely to every get back a value that equals y.
You can fix this by guessing k and checking Sinverse(y,k) = x.  That's in NP (though you should be careful about specifying the "n" of concern, as the size of all three of x, y, and k enter into the picture).  But realize that this is a rather artificial problem, and tells you nothing about attacks without exactly chosen plaintext.
I'd actually state this in a different way, which makes it clearer what you've actually done.  Suppose we are given an oracle Sinverse(.,k).  Then relative to this oracle, the problem of inverting S(.,k) is in NP.  Of course, it's in NP *relative to that oracle*, not the generic NP - and that's a huge difference, since we've known for years that there are oracles relative to which P=NP, and others for which P!=NP.
                                                        -- Jerry

@_date: 2014-09-08 13:21:54
@_author: Jerry Leichter 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
R can be arbitrarily large.  In fact, I deliberately made it so in my example:  It's the same size as the message.  Why bother guessing R?  Why not just guess the message?
Again, if you want to talk about formal matters, you have to do it formally and exactly.  I mentioned in an earlier post that you need to watch out what "n" is ... which is exactly what's gone wrong here:  The size of the search space is arbitrarily larger than the security parameter.  That won't do.
                                                        -- Jerry

@_date: 2014-09-09 11:50:38
@_author: Jerry Leichter 
@_subject: [Cryptography] phishing, was Encryption opinion 
I've always found this one complicated.  If done without the end user's permission, it's an issue.  But there are people who believe it's bad even *with* the end-user's permission - for reasons I find hard to follow.
Of course, the Web site providers all do this themselves - with mobile versions of their sites and various edge distribution networks.  So this all descends into layers of fuzz, and in this case in particular you get into the whole question of "who benefits"?  "Who pays extra costs" (whether in dollars, damaged data - the site isn't as usable as it should be, "stolen attention" paid to ads, whatever).  For a simple point to point connection between Alice and Bob, who the "man in the middle" is is clear. When a single page on the Web has contributions from tens to hundreds of servers for ads, tracking, and who knows what - not to mention multiple network and edge service providers - it becomes very vague.
                                                        -- Jerry

@_date: 2014-09-10 07:45:13
@_author: Jerry Leichter 
@_subject: [Cryptography] phishing, was Encryption opinion 
If you're talking about SSL/HTTPS - this is a re-writing of history.  Some of the re-writing was almost contemporaneous, but it's still a re-writing.
SSL/HTTPS were introduced to "make the Web safe for e-commerce".  One part of this is obvious:  To this day, e-commerce sites tell you that your transactions are "protected by encryption", with the meaning being that they can't be observed and (perhaps, but few get that sophisticated) can't be altered.  But the threat most people understand is observation, and they want to be assured of protection against that.
The more subtle point, though, is what I call the secure introduction problem.  If I'm in a city I've never visited before and I see a sign for a Sheraton hotel, I know they'll have nice, clean rooms and be safe.  If I need to buy a new piece of luggage because the airline destroyed my bag, I can go into Macy's and buy one with the assurance that it won't be a piece of junk, that they'll stand behind it, etc.  If I'm hungry and I see a McDonald's ... well, let's not go there.
In all these cases, there are various external indicia of trustworthiness.  I listed examples that rely on trademarks, but there are plenty of other indicia in the real world that would lead me to trust (or not trust) a hotel that's not part of a chain, or local leather goods store, or a restaurant.  For those of us in the Western world, these indicia, while certainly not perfect, are in practice highly reliable.  (That's not true, for example, in China - where there have been whole fake divisions of well-known companies set up.)  A combination of economics - fakes that will fool people are expensive to produce - and enforced legal mechanisms, particularly trademarks - made, and keeps, the system effective.
When e-commerce was abornin', it wasn't at all clear that people would buy into it.  "On the Internet, no one knows you're a dog" was already a widely repeated trope.  With the screen resolutions and network speeds then available, no Web site could "look" very trustworthy (not that quality of visual representation ever *really* meant much - but it's something people are used to looking at).  The big problem was:  How do you get people to treat these new-fangled sites as "the real thing", worthy of trust and my hard-earned cash?  With so few existing businesses on line, it wasn't even clear that people would trust that "macys.com" really was Macy's.
Enter the certificate.  Here was an attestation by a "Higher Authority" that the site was what it claimed.  Psychologically, it also put a kind of stamp of approval on the site:  Having a certificate felt like having an expensive-looking storefront:  No scammer would bother to make the investment.
That certificates provided no guarantee even vaguely related to this apparent assurance was something only a small coterie of crypto experts understood.  And when scammers did indeed start buying certificates, the response was to double down with EV certificates - deliberately made expensive "to keep the scammers out" (right) and providing all kinds of words to re-assure you that these really were checked and backed by some "Higher Authority" (though they really weren't).  That wonderful green bar - in the US, at least, the color of money:  What a nice way to keep the illusion going.
MITM attacks?  Try and find people outside of the small group of experts who have any clue what those are.  If the term means anything at all to most people, it means that someone can *observe* transactions.
Note that there's a second problem, the Ongoing Connection problem:  I found the macys.com Web site last month and used it successfully (stuff arrived in a real Macy's box).  Just as in the real world I feel safe going back to a store that treated me well - implicitly assuming that "it's the same place" - I'm willing to go back to "the same web site".  While all the certificate stuff is nice, I doubt many people think it has a role in making sure that when I go to macys.com today, it's really the same macys.com I went to last month.  (And, in fact, *it doesn't even do that* - so in this case, that's a good thing!)
Once you see the distinctions between the problems, you can also see why encrypted email has never caught on.  Most email is exchanged by people who already know each other - or who are introduced by mutual friends.  Most people never need their email systems to solve the Introduction Problem.  And we believe we can recognize our correspondents, just as we recognize our friends by their faces, or their voice and manner of speaking on the phone, or their styles of writing, the subjects they talk about, the real-world events they refer to, in letters.  As a result, we believe we really don't need a solution to the Ongoing Connection problem either.  By the time email from commercial sites became commonplace, "everyone already knew" that you could validate the sender by these kinds of indicia.  All that was left was concern about eavesdropping - and it's not as if we've really worried much about eavesdropping on our phone conversations for many decades.  (I suspect the fact that so many people initially used the Internet over dialup connections helped establish the feeling that "listening in on Internet connections" is like "listening in on phone calls" - something that most people just don't worry about.)
There are multiple disconnects here:  Between what most people think systems are capable of providing; what they think they *actually do* provide; what makers of those systems *sell* as their capabilities; and what they systems actually can and do provide.  Pointing at one particular problem - MITM, which has, at least to me, a fairly narrow and clear *technical* definition - and somehow trying to stretch it to cover all the complex issues (either by broadening its definition to the point where it essentially means "any attack", or by saying it's the one problem that all the systems out there are trying to solve) doesn't help.  In fact, it makes things harder.
                                                        -- Jerry

@_date: 2014-09-10 12:08:54
@_author: Jerry Leichter 
@_subject: [Cryptography] phishing, was Encryption opinion 
...thus demonstrating that email in fact *does not* solve the Introduction Problem.  In the first case, mutual memories will solve it.  In the second, the email will usually lead to something else so that both sides can satisfy themselves as to the bona fides of their counter parties.  Though I must say every unsolicited email I've ever gotten offering a highly lucrative contract has been spam.  :-(
I doubt it.  SMTP mail was cross-platform; in this case, all the various walled gardens were eventually seen to be too limiting.
People forget just how non-cross-platform early solutions were.  DEC has MAIL-11 (free, limited functionality, DEC only) and DECmail (a paid product oriented toward business use - but also DEC only).  IBM had something I no longer remember (based on BITNET?  They must have also had SNA-based mail.)
But you didn't have to be the size of a DEC or IBM to come up with a proprietary mail solution:  Apollo Computer, an early workstation producer, had its own non-SMTP mail protocol.  Neither DEC nor Apollo provided an SMTP gateway; third parties did that initially (and they both eventually were forced onto the bandwagon).
And don't forget UUCP-mail, which predated all of these, supported unintroduced mail, was widely implemented and used - but faded (though you could argue that it more or less evolved into SMTP).
BTW, both MAIL-11 and Apollo's mail supported unintroduced mail; I'm not sure about DECmail, but it probably did as well.  And MAIL-11 really *was* "Simple" - unlike SMTP; it *could* have been adapted to TCP with little trouble.  But no one cared, because "unintroduced mail" was barely a concept at the time, and wasn't a decision criterion.
No disagreement.  On the other hand, we've never come up with a good solution for the problem of finding someone's email address.  LDAP works (in my experience not very well) within single organizations.  In the broader Internet, if I knew of John Levine only through mentions of your name, the best I could do is to search around the Internet for mentions of your name, look at the pages, and hope to eventually come upon a page that seemed to refer to the right John Levine and also happened to have an email address for you.
This works because you're reasonably well known.  Within the broader techie community, email academics can usually (but not always) be found if you can figure out what school they are at.  (Sometimes you need to guess the department, too.)  For other techies, LinkIn serves as a de facto introducer.  Beyond that ... you're on your own.  And if you want something that has some degree of associated assurance - there's basically nothing.  (In fact, not only isn't there anything, we haven't even figured out what and appropriate "something" would be.)
                                                        -- Jerry

@_date: 2014-09-12 20:03:58
@_author: Jerry Leichter 
@_subject: [Cryptography] distributing fingerprints etc. via QR codes etc. 
QR codes don't work particularly well for much of anything.  Their big selling point was - and is - that they can link directly to a web site.  This allows all kinds of things in the real world to be connected to the on-line world - e.g., see an ad with an embedded QR code, go directly to the web site for the seller.
In a world of drive-by web attacks and continuous warnings (well founded or not) not to click on "unfamiliar" links ... the concern they raise is reasonable.  Advertisers want a quick, no-effort path from the real-world QR code to a site that sells you something.  Such a path is incompatible with security in today's world.
If QR codes were truly "just a glob of data" which could not trigger any automatic action, I might be willing to scan one.  But unfortunately they trod the same path as e-mail, but before they were even released:  From just a blob of data that couldn't harm you to something "convenient" - but laden with all kinds of hidden semantics that can not just deliver, but even execute, attack code on your system.
Yes, it's *possible* to create "safe" QR codes.  And it's possible to send "safe" mail.  It's also possible to run an email program that will ignore all the dangerous stuff - Alpine is still out there - and it's possible to run a QR reader that won't do anything dangerous.  But the software most people have on their phones for this purpose is *not* safe - and what's important is not that it's possible to produce "safe" messages/QR codes, but that it's possible to produce "unsafe" ones.                                                          -- Jerry

@_date: 2014-09-13 19:09:28
@_author: Jerry Leichter 
@_subject: [Cryptography] distributing fingerprints etc. via QR codes etc. 
A QR image can contain a URL.  Common software scanning such a QR image will pass the URL to the default browser, which will typically open it.  I don't know - never had any reason to experiment - whether non-HTTP URL's also get passed to their registered handlers, though I suspect at least some QR-reading software will do that.
I used PINE for years - even had some custom patches to it that the developers refused to accept, so I kept them going for years.  (For example:  Open-ended ranges.)  All my co-workers kept laughing at me for being a troglodyte and not using the modern mail reader they used - Outlook.  I, in turn, laughed as their systems kept having to be cleaned of the virus du jour, probably snuck in via some attachment.  (This is back in Win2K days.)
Then I bought a Mac and after a while started using Mail.app.  (Officially you weren't allowed to use those on the corporate network; unofficially ... well, let's just say the corporate CTO was an early convert.)  The same people wondered why I now switched.  I answered that I was willing to use a new technology if it was actually *better*.  :-)
                                                        -- Jerry

@_date: 2014-09-15 07:37:24
@_author: Jerry Leichter 
@_subject: [Cryptography] RFC possible changes for Linux random device 
"Disk"?  What is this "disk" you speak of?  :-)
ARM's are widely used in embedded applications which may have no need for persistent memory.  Your solution amounts to a configuration option, though one that the device can configure for itself.  Which is a great approach, for devices that actually have modifiable configurations!
                                                        -- Jerry

@_date: 2014-09-15 13:18:10
@_author: Jerry Leichter 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
No.  RSA is no harder than factoring (we still have no proof that it's *as hard as factoring*).  Factoring is in the intersection of NP and co-NP, the class of decision problems whose complement is in NP.  (The "decision problem for factoring is":  Does N have a factor other than itself or 1?  This is obviously in NP because if I give you a proposed factor m, you can quickly compute the GCD and check.  Membership in co-NP is checked by a deterministic version of primality testing (AKS algorithm).)
If there were an NP-complete problem in that intersection, then NP and co-NP would be the same - which is "thought to be false".  However I haven't been able to find any argument for *why* this is thought to be false - every reference I could find simply states this.  ( refers this statement to the 2nd edition of Hopcraft's "Introduction to Automata Theory..." but I don't have a copy handy to check right now.)
                                                        -- Jerry

@_date: 2014-09-15 17:31:53
@_author: Jerry Leichter 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
Yes, but that's breaking RSA *for the private key*.  Suppose I have a method that decrypts one in 10 messages encrypted with a given private key, but *without* directly revealing the public key.  (No one can even suggest a way one might do this, but imagine such an algorithm exists.)  Can you turn that into a factoring algorithm?
                                                        -- Jerry

@_date: 2014-09-15 19:47:22
@_author: Jerry Leichter 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
Urk.  Exchange "public" and "private".
                                                        -- Jerry

@_date: 2014-09-16 10:37:34
@_author: Jerry Leichter 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Agreed.  But let's also be more careful about attacks.
Suppose we implemented an AES-CTR PRNG, initialized using a TRNG.
- If I'm implementing a crypto system whose fundamental security depends on AES "acting like a random function" (yes, this should be formalized carefully), then relying on this PRNG as my source of random values does not add any new weaknesses.
- If, on the other hand, my crypto system can survive a break of AES (probably because it doesn't use AES at all - many systems rely on the security of AES *plus* the security of, say, RSA, and they are just as vulnerable to a break of AES), then I should *not* rely on this PRNG as my source of randomness.
So we need a finer-grained distinction than TRNG vs. PRNG:  If we want to do a careful threat analysis, we need to understand how the PRNG works.  The definition of a deterministic PRNG *must* indicate, in detail, what security assumptions it depends on.
In practice today, many widely-used algorithms are inherently dependent on the security of AES, so relying on an AES-dependent PRNG has minor, if any, affects on the security of the system as a whole.  So why not use this is a fast alternative - which as a deterministic algorithm also has other good features like testability.  (Of course, *security* is still dependent on a proper TRNG for initialization.  You can't get away from that.)
One thing to note is that if you ever decide that AES is vulnerable, you can upgrade the generator to something else *entirely on your own*:  Unlike changing the ciphers used in communication, the cipher or other mechanism used to build a PRNG is a purely local decision.  So you aren't making the cipher upgrade problem any harder.
I don't see why this is wasteful.  As Ts'o proposes it, only those processes that happen to use it get it.  The only thing I would add to his proposal is a system call to securely destroy the current process PRNG data structure, resetting the process to the "no PRNG" state.  That way (a) a process that doesn't think it will need the PRNG any more can return the necessary resources to the system; (b) since the next call to get a PRNG output will create a whole new PRNG, seeded with true entropy, a process can at any point destroy any connection between "random values" it has already used and "random values" it will use from this point on.  I'd also suggest that the PRNG state be destroyed on exec - a new program should not be able to get any information about the "random number" some *other* program generated in the past, even if they happen to live in the same process.
                                                        -- Jerry

@_date: 2014-09-16 11:17:01
@_author: Jerry Leichter 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Yes, I'm guessing that this wasn't intended to be the whole proposal.  My assumption is that the system call will return a success/failure status, and that this describes the failure case:  Beyond returning a status, it *also* clears the output buffer.
Properly written code will check the status and proceed in some appropriate way on failure; so what's in the buffer doesn't matter.  Code that doesn't check the status - and *no one* writes code like that any more, right?  :-( - will at least end up with the most obvious possible "bad randomness":  Always all zeros.  Perhaps whoever it talks to will notice....
                                                        -- Jerry

@_date: 2014-09-17 08:22:10
@_author: Jerry Leichter 
@_subject: [Cryptography] RFC possible changes for Linux random device 
In fact, it has nothing really to do with random values.  If such an interface were available, I'd want to put all "red" (sensitive, unencrypted) data in such pages.
We're talking much bigger changes here, but if we're going to think big:  Encrypted swap space, with *per process* encryption keys, would be almost as effective, without the potential for such a denial of service attack.  The per-process swap key would go into this kind of "crypto-secure" memory, but that would be a strictly limited bit of memory.
                                                        -- Jerry

@_date: 2014-09-18 10:24:42
@_author: Jerry Leichter 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Just on general isolation principles.  There is never a need for one process to be able to read another process's swap space, so why should it even have the capability?
In fact, if you think of it in "capability" terms, using encryption to enforce the capabilities, it's a natural.  "Per-process" is not a good characterization, however; rather, it should be per memory region.  That way it's easy to have some shared memory with no key, or shared memory with a key you need to make any sense of it - or even have encrypted disk-mapped I/O.
Key rotation solves an entirely different problem.  And, in the context of encrypting swap space, I'm not even sure *what* problem it solves!
Existing encrypted swap devices attempt to prevent one attack:  Seizure of a swap device leaking information about what was in memory in the running processes.  They're fine for that.  Key rotation is essentially irrelevant for that attack.  It becomes relevant if you think about attacks that leak the encrypted swap contents, and maybe the keys, over a period of time.  But then making it harder for a process to determine some other process's swap encryption key might be of interest as well.
                                                        -- Jerry

@_date: 2014-09-18 10:26:05
@_author: Jerry Leichter 
@_subject: [Cryptography] RFC possible changes for Linux random device 
See my response to Viktor Dukhovni.
                                                        -- Jerry

@_date: 2014-09-19 10:34:19
@_author: Jerry Leichter 
@_subject: [Cryptography] new wiretap resistance in iOS 8 
Actually, I think for iMessage and maybe FaceTime, they've said this before.  They go beyond that to say that they are unable get data off a locked iPhone 6 - a really big change, and as far as I know the first such statement by any device maker.  (They do *not* make a similar statement about stuff in iCloud.  There are all kinds of reasons why that's much more challenging; it'll be interesting to see if they move in that direction, however.)
I doubt it.  The attacks will be subtle, indirect, and well hidden.  An attempt at doing something in the open right now would produce a firestorm and would almost certainly fail, which from the point of view of the Peeping Tom Agencies (PTA's) would be worse than nothing.  They lost the Clipper war in a time when there was little public understanding or even awareness of the issues.
Always remember that these are professional players in the political and bureaucratic game.  They were here yesterday; they are here today; they'll be here tomorrow, next week, next year.  The long game is a natural for them - their opponents lose interest, move on to other things, miss the series of small moves behind the curtains.  Unfortunately, the only hope of keeping them in check is to have professionals oppose them.  Right now, the big tech powers are beginning to play that role.  As long as Apple and Google and Microsoft see it as important to their businesses to fight the PTA's, the PTA's will find themselves constrained.  If the big guys decide it's no long important to them, the PTA's will be back in control within a fairly short time.
                                                        -- Jerry

@_date: 2014-09-19 10:51:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Simple non-invertible function? 
Please define "non-invertible".  F(x) = 0 is non-invertible - but likely not what you had in mind.  Presumably you mean something like a 1-1 function such that computing F(x) is easy but computing its inverse is hard - for some appropriate definition of "easy" and "hard".  The example you give is 1-1 and easy to compute, but the inverse isn't likely to be that hard, or we'd be using it in place of much more complex functions like the SHA's.
                                                        -- Jerry

@_date: 2014-09-19 11:40:58
@_author: Jerry Leichter 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Nice paper.  Tt's interesting to see how the technological concerns have changed over the intervening decade and a half.  In particular, no one today would need find it of interest to show that the overhead of encrypting pages before writing them to disk isn't a big deal.
However, they attacked a different problem:  Ensuring that data no longer referenced by any process will also be (logically) wiped from swap within a known time T.  They divide the backing store into sections (nominally of 512KB each) and assign a key to each section.  Once a key has been in use for T seconds, a new key is chosen and any pages in that section that are still in use are re-encrypted.  Any that are no longer in use still contain their old data, but can no longer be decrypted as the old key is gone.  (The paper, however, notes that they hadn't yet implemented re-encryption. So the implementation they based the paper on didn't actually solve the problem they set out to solve!  Do you know whether they ever finished the implementation and added it to the standard distributions?)
The keys, BTW, are stored in volatile kernel memory and are never written to the swap device.
                                                        -- Jerry

@_date: 2014-09-19 11:52:58
@_author: Jerry Leichter 
@_subject: [Cryptography] Email encryption for the wider public 
It is worth pointing out that, *in principle*, Identity Based Encryption solves this problem:  You could chose your public key yourself;
"HenryAugustusChamberlain at gmail.com" would be a perfectly valid public key.
Unfortunately, IBE has other issues, particularly its very strong centralized private key generator, which ends up able to read the messages of anyone who joins the system.  I believe the currently-known techniques are also patented.
                                                        -- Jerry

@_date: 2014-09-19 12:03:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Shaming sites that send sensitive information over 
My favorite:  The NSA's web site *redirects HTTPS to HTTP*.  Some kind of back-handed acknowledgement of what they do?
                                                       -- Jerry

@_date: 2014-09-19 15:52:05
@_author: Jerry Leichter 
@_subject: [Cryptography] Email encryption for the wider public 
A as in aisle, c as in ctenophore, d as in Djibouti, p as in pneumonia....

@_date: 2014-09-20 07:06:06
@_author: Jerry Leichter 
@_subject: [Cryptography] new wiretap resistance in iOS 8? 
Because (a) if they make this kind of public pledge and it's shown to be false it would be a PR nightmare and would certainly result in lawsuits, FTC action, Congressional hearings, and similar events that would have costs way exceeding what they could possibly gain; (b) in the specific case of responding to a proper court request by lying about their inability to respond, the result would be massive fines and quite possibly jail time.  It's also been shown repeatedly that lies, or even mistakes, in widely publicized categorical statements of this sort are discovered sooner or later - and given the size Apple is as a target, it would certainly be sooner.
There are all kinds of assurances in the world, but outside of mathematics, there are no proofs.  Assurance based on the assurer's enlightened best interest is of a very different nature than assurance based on reading the code - but that doesn't make it less of an assurance.  Ultimately, you have to decide what you trust for any particular purpose - there's no golden road.
                                                        -- Jerry

@_date: 2014-09-21 09:13:27
@_author: Jerry Leichter 
@_subject: [Cryptography] new wiretap resistance in iOS 8? 
I'm not sure this is an accurate rendering of the history.  NSA comes up with ciphers that are approved for some period of time.  They are then replaced with new, stronger ones.  When it was approved in 1976, it's not clear even NSA could muster the hardware for a brute force attack; in fact, I'd guess not.  The first *public* attack wouldn't come until 1999 - 23 years later.  The difference between hardware resources available even to the NSA across that period of time is almost impossible to fathom.  In 1976 "supercomputer" was a Cray-1 - with a single CPU (albeit with vector processing) and an 80 MHz clock.  I don't know how much memory it could support, but it would certainly have been way less than is in smartphone today.  The fastest long-distance digital links were probably 56Kb/sec, so distributed processing would have been a non-starter.  In 1976, an estimate of 10 years until *anyone* could brute-force DES would have been reasonable.  I don't know what time limit might have been set in 1976, but the standard was re-affirmed in 1988.  I will agree that the re-affirmation was getting somewhat dicy - and the second reaffirmation, in 1993, was clearly not correct (though the second reaffirmation did add 3-DES).
The problem, however, is that by 1988 and 1993, there were other political moves in play.  Business isn't much interested in security - it's interested in standards it can say it followed, thus shifting any responsibility on someone else.  By 1988, there was a large and growing infrastructure relying on DES - much of it, given the CPU's of that era, in hardware.  Even without NSA involvement, there would have been resistance to a new standard as "too expensive" - see our current use of magnetic stripe credit cards in the US.  NSA, in its internal systems, doesn't face any such constraints:  If they decide a system needs replacing, money isn't an issue.  Give the human propensity to assume everyone else thinks and works like you do until you get overwhelming evidence to the contrary, I'll bet that NSA in the mid-70's would have assumed that if they told everyone to drop DES because there was a serious attack against it - they would have done so.
There's also a technical issue.  NSA always claimed that they didn't design DES - IBM did.  All NSA did was change the S boxes and drop the key from 64 to 56 bits.  In fact, as we know since the discovery of differential cryptography in the outside world, the S boxes NSA specified were the strongest possible against DC - and the resulting system was effectively good for 56 bits anyway.  To have done better, NSA would have had to toss IBM's algorithm entirely.
Being paranoid about NSA, given what we now know about them, is rational and even prudent.  (Now how often is "paranoia" prudent....)  On the other hand, assuming they are omniscient means you might as well give up.
                                                        -- Jerry

@_date: 2014-09-22 09:16:41
@_author: Jerry Leichter 
@_subject: [Cryptography] Of writing down passwords 
It's worth noting that "only allows administration from the Ethernet side" isn't nearly as good a protection as you'd think.  There have been numerous examples over the years of attack code slipped into the network, where it can attack the Ethernet side.  For example, Javascript in a browser is perfectly capable of manipulating one of these devices.
So ... even if you believe your internal network is secure, you should *still* use a strong password on configurable devices.  And then it's quite reasonable to write it down, as long as you don't leave it in a place that is easily seen by people you might not trust.  (If you're going to tape it to the device, tape it to the back or bottom so the delivery guy can't spot it as he walks by.)
I've also been a fan of obfuscating written passwords.  If you're the only one who will have to read them, there are all kinds of simple tricks that, while certainly not secure against a determined attacker, make their job harder.  (I used to write an obfuscated version of the combination on the back of my old Master combination locks.  There's nothing more annoying than needing a lock, finding one in the bottom of a drawer - and not remembering the combination.  In practice, it would take more work for someone other than me to reverse the obfuscation than to open the lock by other well-known means.)
                                                        -- Jerry

@_date: 2014-09-22 09:48:51
@_author: Jerry Leichter 
@_subject: [Cryptography] new wiretap resistance in iOS 8? 
Since no one actually did more than a back-of-the-envelope design, much less an implementation of even a small part of the machine, it's hard to know exactly how to approach the estimates.  What made the EFF DES cracker valuable was that it was a real, working machine - there was no longer any place of argument about what was *possible*.
Then again, the EFF DES cracker came in at $250,000 with a design goal of a mean crack time of 4.5 days (though hardware problems make it roughly 3 times slower), roughly 20 years after the Diffie/Hellman paper was published.  That paper estimates that *10* years after publication, their machine would cost $200,000 for a 12-hour crack time.  Since we're assuming exponential drops in cost, there's a *huge* difference between 10 years and 20 years.  Even the paper's "within an order of magnitude" estimates won't cover that.
So, yes, it's fascinating reading after all these years; and it's "directionally correct", to use that horrible bit of business-speak; but the fact is, on the important details, they got it wrong.  Estimation of costs for unbuilt hardware should be taken with large grains of salt.  (BTW, the EFF reports that they *budgeted* $210,000, which prove too low by $40,000 - about 20%.  It's not easy getting accurate cost estimates even fairly close in to actual design/build time.)
(None of these numbers are adjusted for inflation, but it wasn't wildly high in that period.)
The paper argues that going to 128 or even 256 bits would not increase the cost of encryption hardware by much.  What I remember hearing from hardware guys around that time was that 64-bit internals of a DES chip were about at the limit of practical (cost/yield/etc.) hardware technology at the time.
There's also the question of *what algorithm to use*.  People keep repeating the story that the NSA "weakened" DES by reducing the key to 56 bits; but in fact we now know, and have known for years, that given the basic DES algorithm (a) the S-boxes NSA specified are the strongest possible against differential cryptography; (b) the inherent strength of the DES algorithm against DC is only about 56 bits.
If the NSA, at that moment in time, had wanted to reserve the ability to break DES to itself, it could have simply left the 64-bit keys in place.  Everyone else would be looking at strength against brute force attack, and would conclude that with a 64 bit key (well, 63 because of the complement property) things were safe for a while; but NSA could use DC and get a roughly 55-bit attack.  (A few years later, when NSA had begun to see the degree of penetration public use of encryption was starting to have, I have no doubt they would have done just that - at least in a hypothetical world where DC was not yet publicly known.  But I think they just missed what was coming down the road - they though that crypto would move from the realm of spies to big banks and some of the largest corporations, which they could penetrate easily enough in other ways.  So at that moment in time, my guess is they really wanted to get a strong system fielded for those giants.)
                                                        -- Jerry

@_date: 2014-09-23 08:33:01
@_author: Jerry Leichter 
@_subject: [Cryptography] Of writing down passwords 
There's one closely related one that I would put even higher than "don't write it down":  Frequent, forced password changes.  The troika of:
1.  You must choose a password that no human being can reasonably remember;
2.  You must not write that password down;
3.  Just about the time you can reliably remember you password, you must change it to something entirely new;
could only be made more user-hostile than by adding such measures as "you will receive increasing electric shocks for each incorrect password entry".
One financial institution (used for retirement accounts by an employer, so I had no choice but to use them) required quarterly password changes.  Since in practice I only logged in about one a quarter, I had to change the damn password on every login.  Manual OTP?
You have enough adversaries without making your users into yet more adversaries. They'll choose related passwords (at one large company I used to add a suffix like Q314 to deal with the forced quarterly changes) or they'll write the thing down, not on a piece of paper in their wallet, but on a sticky under the keyboard - and then you'll be left adding yet more user-hostile layers (like code that looks for patterns in passwords, and inevitably rejects perfectly good ones).
A prominent notification of where and when someone last logged in to your account - suppressed if it was at the usual time and place - will do more to stop on-going use of a stolen password than forced changes.  But no one seems to bother with that any more - or they implement it minimally (not prominent, no attempt to filter out obviously-good logins).
One side-effect of these policies is that users get locked out of their accounts and then need to contact the company call center.  (I suppose if your goal as a manager is "justify need for all the call center guys" then these policies make sense.)  Or maybe, these days, just follow the "reset password" script on line. Both of these use some of the worst security procedures ever.
I once was sick and working from home and found I couldn't log in:  My password had expired, and *all* the password reset mechanisms were disabled when you were on the VPN ("for security").  I ended up having to call the support center - which reset my password based on my name and badge number, both easily - and by policy - visible to one at all when I was at work.  "We have a super-secure safe with a 10-digit combination that's written on a piece of paper inside one of those diary books with a lock on it conveniently placed on a table next to the safe."
                                                        -- Jerry

@_date: 2014-09-23 12:23:53
@_author: Jerry Leichter 
@_subject: [Cryptography] new wiretap resistance in iOS 8? 
Hard to know.  History repeatedly shows great resistance to attempts to change fielded cryptosystems - even in the face of evidence that they've been broken.
The NSA had a great stalking horse in, err, what was the name of that Swiss company, which appeared to be an independent source of secure hardware, based in a neutral country with a reputation for building fine machinery.  It would be easy for them to send out re-assuring messages - oh, yes, the Allies broke the German codes - but ours are different enough that the old techniques don't work.  (Keep in mind that many of the *US* of WWII vintage were themselves based on the same ideas.)
Compare that to a system *apparently designed by NSA*, and if you're some government bureaucrat with no real knowledge of crypto, tasked to choose a system to keep stuff secure from the US ... and what would *you* choose?
Could well be.
                                                        -- Jerry

@_date: 2014-09-24 08:22:33
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA versus DES etc.... was: iOS 8 
A fascinating bit of history; I read through the whole thing.  Thanks for forwarding the link.
The referenced page (and all the material about DES in both papers) is ambiguous because of redaction.  For those who don't want to spend the time, the relevant portion from page 232 reads:  "(FOUO) Once that decision had been made, the debate turned to the issue of minimizing the damage. Narrowing the encryption problem to a single, influential algorithm might drive out competitors, and that would reduce the field that NSA had to be concerned about [redaction of about lines of material] they compromised on a 56-bit key.122"  Footnote 122 is just "ibid" whose exact reference is hard to determine because of a redaction in an earlier footnote.  However, it doesn't add any information to explain the text.
The reason I say this is "ambiguous" is we can't tell what the disagreement was, who compromised, and over what.
This may have been a legitimate argument to make in the 1970's, but once Shamir published his work on differential cryptography, it became rather weak.  The S-boxes that IBM chose were weak against DC.  The S-boxes NSA choose are among the strongest possible against DC.  (They show a bit of weakness - never turned into a significant attack - against linear cryptanalysis.  I don't know if any set of S-boxes is simultaneously strong against both DC and LC.)  So changing the S-boxes *absolutely, unambiguously* strengthened DES.
There's a weird side to this story, indicating that we still don't know everything.  We've always been told that "NSA changed the S boxes" and no one knew why.  On the other hand, after DC became public, Don Coopersmith of IBM was
able to reveal that the IBM designers knew about DC and had themselves chosen S-boxes secure against it.  (  Coopersmith makes no mention of a change to the S-boxes by anyone.
DC against DES requires about 2^47 chosen plaintext encryptions.  It's impossible to directly compare this against 2^54 expected decryptions (one factor of 2 from the complementation property) from a brute-force attack.  On a pure "work" basis, DC requires much less then 2^56 work, and *way* less than 2^64 work (based on the nominal key length).  But the information needed to mount an attack is very different.
That's an assumption everyone has made, and continues to make - but the redactions in the references - probably deliberately - make it impossible to unambiguously confirm or deny the assumption.
Reference 2 also makes it clear that "Lucifer" was a name that described a series of different encryption algorithms, where the later ones were presumably intended to be improvements over the earlier ones.  They all apparently had 128 bit keys.  At least some versions of Lucifer were published - does anyone know of an analysis of their strength using modern tools?
Reference 1 also contains an interesting paragraph (page 239):  "As for DES, the controversy quieted for a period of years.  DES chips were being manufactured by several firms and had become a profitable business.  In 1987, NSA proposed a more sophisticated algorithm, but the banking community, the prime user of DES, had a good deal of money invested in it and asked that no modifications be made for the time.  By the early 1990s it had become the most widely used encryption algorithm in the world.  Though its export was restricted, it was known to be widely used outside the United States.  According to a March 1994 study, there were some 1,952 products developed and distributed in thirty-three countries.l43"  Footnote 143 is a reference to two papers and a phone interview with David Kahn.  This is a couple of years *before* the whole Clipper business, so it's unclear what this "more sophisticated algorithm" was.
Just to be clear, there are actually not that many "good" S-boxes.  You have essentially no chance of finding them unless you already know about DC.
You know this ... how?  You just quoted Adi Shamir saying that 128-bit Lucifer was actually remarkably *weak*.  DC - discovered independently at least three times, by the NSA, but Coopersmith and a team and IBM, and by Shamir, who finally made it public - is an extraordinarily powerful attack.  Its knowledge marked the beginning of any realistic design criteria for cryptosystems.
Repeating the references, because they are valuable:
                                                        -- Jerry

@_date: 2014-09-25 19:47:56
@_author: Jerry Leichter 
@_subject: [Cryptography] Of writing down passwords 
There's little reason for pixel density to get much higher.  Oh, there are certainly specialty uses, but the 41Mpixels on a Lumia have little practical use - they are there for bragging rights.
On the other hand, pixel density is only the relevant factor if you assume a particular image size and all magnification done after digitization.  If your iris is 10 pixels across, a 20x magnifying lens - nothing particularly exotic on an DSLR - makes it 200 pixels across.  The tiny lenses and imaging matrices in phones produce astounding results, but they are subject to fundamental physical limits (particularly diffraction limits) that stuff in more pixels can't solve.
If the question is "when will you be able to read text of the image of a human eye as taken by something reasonably related to today's cellphone camera", I think the answer is likely to be "never".  Even if it were physically possible, there's simply no reason for cell phone cameras to evolve in that direction.
On the other hand, if the question is "when could you create a photographic setup that would take a picture of a human eye from which you could extract text", the answer is that it could be done easily today.
                                                        -- Jerry

@_date: 2014-09-28 08:30:28
@_author: Jerry Leichter 
@_subject: [Cryptography] [messaging] Gossip doesn't save Certificate 
Think about the broader implications of this assertion:  Certificate pinning is, in effect, doing away with PKI.  The entire structure of layers of trust in CA's becomes meaningless:  You're trusting whoever gives you the "pins" to tell you how to reach Google or Paypal.  Oh, sure, they gave you a cert signed by some CA - but to verify that cert, you go look up the CA - and it had better be pinned as well, or what have you gained?  The whole point of pinning was to have *trustable* ways to reach important sites....
The logical outcome of pinning is to get rid of the certs entirely.  Your browser vendor provides you with a bucket of public keys for well-known sites, and you just use them.  The only thing the intermediate layer of CA certs provides in this scenario is the ability for the site to issue new keys - but it does so at the cost of requiring you to trust an extra party.  Proper rekeying mechanisms shouldn't need that.
Pinning is a hack to buttress a PKI system that we know is failing.  I appreciate the importance of having something that improves existing systems as transparently as possible - it's so difficult to deploy anything entirely new.  As a transition - that's fine.  But it shouldn't block us from thinking about a better replacement.
PKI was introduced to the Web as a solution to a problem:  There were too many sites you might want to go to - many of which you'd never been to before - for it to be practical for you to have all the necessary keys up front.  Besides, the links themselves were too slow (dialup!) and storage was too expensive, to make distribution of a list of keys possible.  None of those apply today.
Consider the sites that correspond to 1. on the list.  Those are supposedly the top sites on the Internet - the ones "everyone goes to".  How many are there?  What percentage of connections does it cover?  The Alexa "top sites" lists seem to provide a good starting point, but I had trouble getting that kind of data - it's probably what Alexa provides for a fee.  The closest I could find was  which claims that the top 10000 sites got 75% of "Google organic search traffic" 2013.  One can debate the details - even if Alexa is measuring *exactly* what it claims to be measuring, that's not quite what we are looking for - but it certainly looks as if a list of 100,000 pre-distributed keys would easily cover what anyone would reasonably call the members of that class.  (For any given person, it would be gross overkill - hardly anyone regularly contacts the top sites in English, Chinese *and* Arabic, say.)  How much data is that?  Let's assume 2048-bit RSA keys - 256 bytes.  The name of the site has to be in there - not typically very long, but let's simplify the math a round the total up to 300 bytes.  So we're talking 30MB of data.  Today, that's no big deal.  Why not just distribute it to everyone?
For sites in this list - which would probably cover the vast majority of connections made by the vast majority of people - you don't need certs, CA's, or any of that junk.  The browser makers - who you have to trust *anyway*; crypto is useless if the code is spiked - can distribute and update this list on a regular basis.  If the list is signed, there's no hazard in getting copies from random places on the net - the browser will reject a fake or modified version.
Revocation can work as well - or as badly - as today, using similar mechanisms.
Of course, that leaves the item 2 sites - the First Bank of Podunk, et al.  These are probably of low enough value, and visited infrequently enough, that serious attacks against them are unlikely.  Key continuity (the site uses the same key it used last time, or it's used a proper key rollover procedure to tell me what its new key is) covers everything but the initial contact problem. Again, in today's world, storage of keys of sites I've spoken to in the past is no big deal - and distribution of my stored keys to all my devices is a problem that can be solved.  And the initial contact for any but the best-known sites was always problematic anyway:  Just what *is* the proper web address for the First Bank of Podunk?  Just because firstbankofpodunk.com has a cert for that address saying First Bank of Podunk doesn't mean it's really the site for that bank down the street - *they* are using 1stbankofpodunk.com, and don't have the time or money to go after all the fake addresses similar to their name.  If I want to be sure of the site, I need to walk up the street and find out where their site is.  They can just as easily give me their key - e.g., with a QR code - at the same time.
The PKI structure we're trying to patch up attempted to solve problems that are barely relevant to the world we actually live in today.  It's time to move on.
                                                        -- Jerry

@_date: 2014-09-29 14:30:03
@_author: Jerry Leichter 
@_subject: [Cryptography] new wiretap resistance in iOS 8 
These are the probes to see what the reaction will be.  It hasn't gone so well for them.  Orin Kerr started out with support, then backed off.  Ronald Hosko, a former assistant director at the FBI, wrote an op-ed piece at the Washington Post - and the Post had to twice go back and fix errors of fact, destroying the rationale for Hosko's complaints.  There are op-ed articles answering the outraged ones from the PTA side.  And many of the news articles, amazingly enough, talk to others besides their usual PTA contacts.
The open and public side isn't going so well for the PTA's.  The plotting and planning for the subtle side proceeds as we speak....
                                                        -- Jerry

@_date: 2014-09-29 23:51:55
@_author: Jerry Leichter 
@_subject: [Cryptography] "Spy Agencies Urge Caution on Phone Deal" 
Not directly crypto-related, but an example of the tangle of relationships that drive surveillance:   reports on a backstage argument going on in Washington about a special network/database - oddly never named in the article - that "rout[es] millions of phone calls and text messages in the United States".  Apparently this was a system created back in the late 1990's to implement number portability.  It's not clear from the article whether it's a database of number-to-carrier mappings, or actually routes call based on such a database.
A small Virginia company named Neustar created the system and has managed it ever since.  Recently, the major carriers recommended to the FCC that Neustar be replaced by Telcordia, an American subsidiary of Ericsson, which allegedly can do the job more cheaply.  The "intelligence community" has been pushed to leave the job in Neustar's hands, claiming that letting a European company run the system would leak important information about how US surveillance of the phone networks is implemented.
Neustar, obviously no stranger to the Washington inside game, has hired good ol' Michael Chertoff to represent them.
The bullshit and inside baseball and lobbying here runs so deep you can't see bottom.  And underneath it all, another piece of the vast tapping network we've built in the US in the last 12 or so years is revealed, just a little bit.
                                                        -- Jerry

@_date: 2015-04-04 07:26:53
@_author: Jerry Leichter 
@_subject: [Cryptography] Fwd:  OPENSSL FREAK 
So in the last 50 years or so, can you give an example in which a Death Note would have actually been published?
I can think of many cases of *implementations* being revealed to be broken by the publication of attack code.  And of cases where *security parameters* were shown to be two small (size of DES and RSA keys) by the publication of descriptions of such breaks using just-now-possible amounts of hardware.  You might be able to get something like a death notice out of the latter (by pre-publishing challenge problems), though you'd have to trust whoever generates the challenges to keep them private.  (As I understand it, a "real" Death Notice is an actual proof which is impossible to fake, even given special knowledge.)
If you can't show the existence of such a mechanism would actually help in plausible real cases, this discussion seems rather pointless.
                                                        -- Jerry

@_date: 2015-04-04 15:35:35
@_author: Jerry Leichter 
@_subject: [Cryptography] Fwd:  OPENSSL FREAK 
This completely misses my understanding of what a Death Notice is supposed to be.  A Death Notice is a machine-checkable proof that some cryptographic primitive has been broken.  To be useful, the check has to be reasonably easily checkable.  For example, it's trivial to prove that a cryptographically secure hash function is not second-preimage resistant:  Simply provide at least one pair of distinct values that hash to the same thing.  The cost of checking this is essentially the cost of computing the hash function twice.  Proving that there's a way to compute pre-images of the hash function can be done by publishing an algorithm to do just that.  Checking this may be much harder, however.  Suppose that the published algorithm requires a computation equivalent to 10^8 hash computations.  This would clearly be a bad break of the hash function, but it's not a useful Death Notice, as hardly any implementation of the hash function will be in a position to check the assertion in that Notice.
The same goes for the export mode encryptions.  Yes, all of them fall to brute force today.  But an independent machine-checkable proof of that fact would require checks that are, even today, outside the bounds of practicality *for most if not all implementations that actually use those primitives*.
Since it's impractical for all instances of implementations to verify these alleged proofs for themselves - and were they to try, there would be subjecting themselves to trivial denial-of-service attacks - you're left with some central authority certifying that, yes, they've received a Death Notice and checked it and found it valid.  How exactly that's different from what we do (very badly) today isn't clear.
They could also have just published the magic numbers.  And yet no one has.  There's little reason to believe Snowden or Manning would have ever been close to having access to that level of detail - the actual "magic numbers", from the point of view of an attacker his keys - would be held very tightly, as there's simply no reason to make them widely known.  Unlike the stuff that Snowden and Manning have released - which had to be circulated to be useful - the "magic numbers" only need to be known to those who build the attack code that uses them.  Assuming those numbers exist, they might be known to a handful of people in the world; and, frankly, many of them might have known them at one time - as they were working on code - but would have had not reason to remember them after.  I'd rate the chance that these numbers ever leak as effectively 0.
The non-existence of Death Notices is irrelevant here.  We have to rely on a proof that, given the algorithm used, such "magic numbers" *could* exist; and the *human* judgement that the way the constants were chosen *could have allowed* for someone in the right place at the right time to siphon off the relevant values.  (Short of the numbers leaking, there's no way to *prove* that anyone actually did.)
This may be your best examples, as some of these are so weak that, indeed, implementations might, in principle, run published attack algorithms to check them with reasonable resources.  Of course, these algorithms were implemented in secret by organizations with no reason to want to have them tested for strength after the fact - and every reason to want them *not* to be tested.  Further, they were in implementations that offered no alternative algorithms.  The right solution here wasn't Death Notices (which no one involved would have dreamed of including support for); it's not letting critical systems rely on untested, secret algorithms developed by people with little expertise in the field.  Fortunately, we seem to have moved away from this kind of thing.  All recent breaks of this sort are of algorithms developed and fielded years ago, living on only because of backwards compatibility requirements.  And, indeed, such systems need to be retired.
It's not that Death Notices aren't a cool theoretical concept; it's that they appear to have almost no *practical* application.
                                                        -- Jerry

@_date: 2015-04-07 21:04:31
@_author: Jerry Leichter 
@_subject: [Cryptography] Fwd: OPENSSL FREAK 
The reaction to such devices is completely predictable:  It's all about profit, they're forcing me to by a new X when my old X was working perfectly well.  What a scam.
And, of course, there will be plenty of X devices out there for which this will be exactly the case.  Don't you know that the new Version 2 K-cups for coffee makers include DRM to "protect you, the consumer"?
Look at all the complaints that Apple (and now Google) prevents you from reverting you phone to an earlier version of the software.  It's all just a conspiracy to take control of your devices away from you!
There's some device out there - I can't remember what - that is limited by software to 150 uses.  Then you have to go buy another one.  (Or look around on-line for ways to reset the counter.)
Right now, it's only a small minority of techies who complain about this stuff.  But when significant parts of the population find themselves required to replace "perfectly good" devices because the maker decided it was time for them to die ... expect consumer protection laws requiring that makers provide "unlock keys".
I could imagine a requirement that the device shut down, but that it could be revived by establishing a connection to an update service.  But this is much harder than it looks as a general approach.  The obvious thing to do is bring it to the store where you got it, or some similar location, where a direct connection can be made.  But "embedded" devices may well be *physically* embedded, and there may be no practical way to "bring" them anywhere.  Do you then somehow have to reconfigure your network to allow connection?  How many people will know how to do that?  For that matter, how will an embedded device even tell you that it needs an update?  It may not talk directly to you at all - only provide some simple data stream to layers of software.
I don't think anything of this sort can work except in constrained environments.  *Maybe* you can get the military to do this - though I have my doubts; a device that may suddenly shut itself down because it *might* be insecure, in the middle of a battle, is not going to get any military support.  Similar kinds of limitations apply in industrial settings.  In fact, I'm finding it hard to come up with a realistic scenario in which such a device would be acceptable:  Basically, from the outside it's a device with an additional failure mode.  Why would I buy such a thing?  There are enough failure modes already!
Besides ... breaks occur at unpredictable times.  A device with a 5-year lifetime against which an attack is found 6 months after release is a hell of a lot more dangerous than a device with no defined lifetime against which no attack has yet been found.
Every time I think about this issue, I come back to the same place:  We need very simple enforcers of very simple security properties - secure kernels - which we can use as a base.  These things have to be so simple that we can realistically convince ourselves that they obviously have no bugs (cue Tony Hoare).  They then never need to be replaced, never need to be upgraded, need no management interface.  Instead, you build your more complex functionality - including a management interface - on top of these things.  The base has enough security properties that you can build a secure update facility for the rest on top of it.
Can we really build such a thing?  I don't know, though we seem to be at the point where we can prove implementations of entire simple microprocessors and entire OS's secure.  We have enough confidence in *some* of our crypto protocols -particularly symmetric primitives - to think we can keep data safe for tens of years.  So perhaps we're getting there.  Exactly what security properties such a secure kernel should implement, I don't think we know.  It's an interesting question.
The only way this kind of capability makes sense is if there's a layer within the remote management interface that you trust to enforce that self-protection - and then you can't update that layer for any *current* purpose, only for possible *future* purposes.  *Safely* fixing bugs in the self-protection mechanism is very challenging.  Maybe this isn't quite my "never to be changed" secure kernel, but it's pretty close.
I don't buy the analogy, sorry.  How does the inevitable death of individual human beings contribute to the *security* of any human-based system?  (Purposive resistance can cut both ways:  If correctly applied to resist an attack, it helps; if incorrectly applied, it's a way to produce denial of service, and it inappropriately applied - to work around or just plain refuse to implement real security requirements - it's a common cause of broken systems.)
                                                        -- Jerry

@_date: 2015-04-10 07:18:51
@_author: Jerry Leichter 
@_subject: [Cryptography] Untrusted Turtles all the way down 
Where exactly do you see this?  At the hardware level, we went to microprogrammed machines a *long* time ago; then went back to RISC; then went to something in between (x86 is kind of JIT-compiling-to-RISC).  At the software level, interpreters go way back, and again we keep hoping back and forth.  Java started as a pure interpreter, but all real implementations these days are compiled - it's just that the compilation is "late".  Javascript was interpreted for a long time, but it's being (late-)compiled these days, too.
I don't really see any necessary relationship between style of execution and attack surface either.  There might be more code, but less of it might be attackable - e.g., attacking a the byte code of a program with a typed, assertion-checking interpreter is much more difficult than attacking the native executable code of some traditional RISC, where type punning and free modification of addresses is a given.
Because the only way anyone has proposed to construct a secure system is to channel all the attacks onto smaller and smaller components?  Then you can concentrate all your protective efforts there.
Would you build a safe entirely out of plywood and then try to figure out how to keep attackers with Sawzalls away?  Or you use thick steel, forcing attackers to go after the lock?
SMM isn't a level of interpretation - it's a response to people wanting their chips to do "more", like use less power, or shut down gracefully when overheating, or (one early use) emulate PS/2 peripherals when only USB mice and keyboards are connected.  Sure, you could hard-wire this stuff - at much greater cost.  Trustzone can be abused, but without something like it, how do you store secrets in hardware that can resist attacks on the hardware itself - or even on the OS, the level you seem to think should be the only one in there?
UEFI's purpose was neither to be an interpreter for the running system, nor in and of itself to provide security.  Do you have reason to believe UEFI systems are inherently less secure than the BIOS-based systems it they designed to replace?
Look, I agree entirely that complexity is the enemy of security.  But complexity is also the necessary concomitant of solving complex problems - and the problems we are asking the our computers to solve are much more complex than those we attacked years ago.  An IBM 360 had no microphone, so could not possibly listen in to conversations in the machine room - not that it would likely have heard much of interest over the fan noise, the disk motors, and all the other stuff!  On the other hand, it couldn't run Skype or respond to voice commands either.  Its interactions with the world were extremely limited.
Oh, and IBM 360's were hacked, too.
The *only* way anyone has ever proposed to provide trust in such systems is exactly to channel the trust down to smaller and smaller pieces, hoping that you can eventually get down to something small and simple enough that it *deserves* trust.  Yes, that gives an attacker one spot - the lock on the safe door - to attack.  But that's much better than all that plywood.
Bruce Schneier has reported that the NSA's definition of a "trusted party" is "someone who can break your security".  Sounds odd, but it's exactly right.
                                                        -- Jerry

@_date: 2015-04-11 23:14:49
@_author: Jerry Leichter 
@_subject: [Cryptography] German investigation says the NSA probably 
So if a newspaper runs a story saying that an anonymous informant says that you, Andreas Junius, personally wiretapped Angela Merkel ... we must believe it thereafter?  What evidence can you produce that it *didn't* happen?
This is how conspiracy theories begin and grow.  No amount of "proof" is ever enough.
Did the NSA tap Angela Merkel's phone?  Did it tap Fran?ois Hollande's?  Damned if I know - but in Merkel's case we have a newspaper report citing an anonymous source (people have assumed it was Snowden, but the newspaper has never actually said it was) versus the apparent results of an investigation, and in Hollande's case we have ... nothing.  If I were forced to bet one way or the other, I'd bet on Hollande.  (Actually, if I had a choice among Merkel, Hollande, and Netanyahu, I'd bet on Netanyahu hands down.  Many more surprises to be gained from listening in to his phone calls than the other two combined.)
                                                        -- Jerry

@_date: 2015-04-18 21:56:20
@_author: Jerry Leichter 
@_subject: [Cryptography] upgrade mechanisms and policies 
This answer actually contradicts the rest of your point, which comes down to:   You can't make sensible security choices without considering the entire system.
This is an open mailing list.  Anyone can ask to join, and will receive all the messages.  Given that ... encryption is pointless.  The list itself acts as an oracle for any encryption done on the list!
As for authentication, for one thing, there are (at least) two possible kinds of authentication for messages here:  Of the original author, and that the message actually came through the list forwarder.  One can easily construct scenarios in which the presence or absence of either of these is desirable, and the two are independent of each other.  And even if authentication of the original author is desirable - that covers a huge piece of ground.  Would you want to bind the identity of this message to my passport - one extreme - or merely to previous messages - on this list?  In this response stream?  Anywhere on the Internet? - sent with the same apparent identity?
Simple point-to-point communication between parties who have some external way of identifying each other is the easy case.  Everything else is harder - sometimes *much* harder.
                                                        -- Jerry

@_date: 2015-04-19 07:27:14
@_author: Jerry Leichter 
@_subject: [Cryptography] Entropy is forever ... 
And the problem is that we almost never are in a position to measure the expectation value when designing a cryptographic system.  So we end up making assumptions.  In fact, we almost always end up making the same assumption:  The plaintext is drawn from a uniform distribution.  There are other areas of engineering where we make different assumptions about input distributions.  (There's no point even trying to compress a data stream under the assumption that it's drawn from a uniform distribution, for example.)  But crypto discussions usually start either there or at the opposite extreme, that the distribution is actually just a single point.
Interestingly, the uniform distribution case is the easiest one for cryptography - which is exactly why making the opposite assumption is good.  This is where work on semantic security comes from, dealing with questions like:  If, in fact, I never send anything but the encryption of the word "yes", how can I prevent an opponent from learning that from the encrypted stream?  Or even that I'm always sending the encryption of the *same* word?
...and then we come full circle to the recent discussion of whether the result of an encryption must be random, or "indistinguishable from random", relative to whatever model of "indistinguishability" you want to use, and what you want to apply it to.  For example, a stream cipher reveals the byte length of the cleartext in the byte length of the cipher text.  If the set of possible inputs is "yes"  or "no", a stream cipher based on a true random source - the canonical (actually single) example of a "provably random encryption) - reveals everything about the input in the output, even though every bit in the output is completely random.
                                                        -- Jerry

@_date: 2015-04-19 18:54:46
@_author: Jerry Leichter 
@_subject: [Cryptography] Entropy is forever ... 
Kolmogorov complexity (and the essentially equivalent measures others came up with - I now recall Chaitin, but there's another name there, too) is "universal" only in some infinitistic sense.  Any finite set of strings can be special-cased and dumped into the machine used to define complexity.  If you take all the passwords revealed in all the password leaks we've seen and make them special cases for the machine ... you have a reasonable start at a model of the *effective* complexity of any password chosen from that set today:  Pretty much none, since the attackers try all of those first.  Fold in dictionaries for a large group of languages, a few billion variations constructed using the kinds of algorithms hackers try (pairs of words, 133t speak, etc.) and you just have a larger, but still finite set to dump into your base machine.
Kolmogorov complexity is a beautiful theory, and the fact that it actually allows you to make sense of the feeling that some strings are "more random" than others is itself remarkable and unexpected.  But beyond some hints about heuristic methods for judging password quality, by its nature it tells you nothing about the real-world complexity of any specific thing, as you can't pin down the relevant information about the real world specifically enough to get it off the ground.
                                                        -- Jerry

@_date: 2015-04-29 21:25:50
@_author: Jerry Leichter 
@_subject: [Cryptography] "Trust in digital certificate ecosystem eroding" 
Summary:  The greater business world is starting to figure out just how untrustworthy today's CA system really is.
                                                        -- Jerry

@_date: 2015-08-02 06:35:02
@_author: Jerry Leichter 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
This is an issue that's broader than crypto protocols and broader than "rough consensus".  It's a fundamental issue with group decision-making when group members believe that logical argument - which is infinitely sub-divisible - is the only basis for resolving arguments.  In fact, it's a fundamental problem of rational decision-making - see "Buridan's ass" (
I saw similar processes occurring internally at DEC decades ago.  You can doubtless find them throughout academia.  And they can form without any external deliberate agency - though as you point out, they can be *encouraged* to form.
My own solution:  If two different approaches have each been successfully argued by two roughly equally teams for "a while", *neither is "better" than the other*.  They are simply *different*.  At that point, logical argument is beside the point - pick one at random.  Making a choice has become more important than which choice you make.
Often, there are multiple objective functions to satisfy, and you find that each side is arguing that, over all, they satisfy "more" of them.  But then you can end up with a cyclic majority, in which there *is* no overall "better" choice - each can be dominated by another.  Just choose at random.
But ... it could be that it's not so much that there's a cyclic majority as that different factions simply weight the different objectives differently.  It may not be obvious that this is happening; it may even be deliberately hidden, especially when the objectives favor one external group over another.    Surfacing these differences may eliminate the (false) equivalence of the competing approaches; or it may simply move the argument to a new plane.  But at least the argument on that plane is about real differences.
If the situation truly is a Buridan's ass one, you may find that techies *still* aren't willing to cede a choice to a random choice.  An argument I've made about elections may swing them.  Imagine we held an election, and the results were extremely close.  We do a recount - and the results are still close, but go the other way.  We do *another* recount, and get yet another set of results.  It's impossible to re-run the election, but we can't get convergence on the result.  This leads to all kinds of fights, but the underlying basic claim is that the election *had* a true result - we just need to determine what it was.  I claim that in such a situation we're "in the quantum domain":  The election *didn't have a result*.  It's in a mixed state between two equally probably results, and which one we see is indeterminate.  Just flip a coin.
                                                        -- Jerry

@_date: 2015-08-03 15:19:09
@_author: Jerry Leichter 
@_subject: [Cryptography] SRP for mutual authentication - as an 
There's a history of issues involving patents with SRP and similar protocols.  (The underlying EKE patents were owned by Lucent, which didn't seem to want to make them broadly available.  SRP was allegedly designed to avoid the EKE patents, but there were enough doubts about whether it did to keep people away.)
The EKE patents have recently expired, so perhaps its time to go look at this again.
                                                        -- Jerry

@_date: 2015-08-03 15:45:29
@_author: Jerry Leichter 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
You're changing the nature of the attack.  I took your attack to be "find two essentially equal protocols and keep the decision procedure stuck on deciding between them".  If one of the protocols is actually *better* along the agreed-upon dimensions - for example, if one has a security flaw - the whole assumption of the "rough consensus" approach is that this will be found eventually and the better protocol will win on the technical merits.
If you can't determine that one of the proposed protocols is actually unacceptable according to the agreed criteria, you have a very different problem, which has nothing to do with rough consensus, working code, committee procedures, or what have you.
                                                        -- Jerry

@_date: 2015-08-03 17:33:24
@_author: Jerry Leichter 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
You never need to outright compare the protocols.  In fact, the nature of arguments of this sort - whether a deliberate attack or just arising by themselves - is that each side simply trumpets the virtues of its own approach, with only minor mention of the other approach.
My assumption is that if there are any significant problems with an approach, they will be found and called out, and that approach will be removed from the running.  If an approach has survived all attempts to knock it out for some appropriate length of time, we assume that it's "good enough".  We usually operate on the assumption that the ranking of approaches induces a total order. I claim that's not true:  When you get to the point where each side is simply listing the ways it's better than the others - and the lists are comparable - then you have two approaches that are simply not ordered with respect to each other.  At that point, you toss a coin.
In terms of actual operation, I'd have the procedure work like this:
1.  Anyone can enter a proposal, or make an argument about proposals that have been entered.
2.  There's a (pre-determined) cutoff time after which no new proposals can be entered.
3.  Some arguments about proposals are classed as "significant attacks".  It's usually obvious which these are; but should there be any debate, an argument shall *not* be deemed a "significant attack on a proposal" unless a super-majority agrees that it is.
4.  Any proposal that's the subject of a "significant attack" is taken out of the running.
5.  If more than one proposal remains in the running, and no "significant attacks" have been mooted in some (pre-determined) amount of time, a random choice among the remaining proposals is made.
This procedure is guaranteed to terminate in a pre-determined amount of time, having chosen 0 or 1 proposals.  It is vulnerable to a "heckler's veto" in clause 3:  A sufficiently large (but under a super-majority) group can block all attempts to kill off a bad proposal.  But it's hard to hide that you're doing this - this is a technical discussion, and if you say "no, that's not an attack" without being able to advance a good reason, people will quickly figure out what you're up to.  I said "a super-majority agrees", not "a super-majority votes" - I'm maintaining the "rough consensus" nature of the interaction.
(Note that it *may* be the case that there are genuinely two distinct audiences with different needs, and no one proposal can really satisfy both.  In that case, you may really not *want* there to be a single winner.  In some cases, providing two alternative approaches, each covering part of the space of application - with perhaps substantial overlap - may simply be the best you can do.)
                                                        -- Jerry

@_date: 2015-08-03 20:38:18
@_author: Jerry Leichter 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
If you have a new proposal that's sufficiently better than all the existing ones, showing that it is so amounts to an attack against the existing proposals, knocking them out of the competition.  (There's no formal definition of "an attack".  It doesn't *have* to show a weakness - showing that you can do much better is sufficient.)
Presumably you then restart the competition.
Just allowing new proposals - even proposals that will rapidly be knocked out of the running - to be mooted forever allows anyone to delay the process indefinitely.  In effect, what I'm suggesting is that after the cutoff, the only way to get a proposal in is by getting it recognized as significantly better than what's already there - a deliberately high barrier.  Treating this as an attack just lets you do it without invoking some new ad hoc mechanism for judging what's "better by enough":  It's decided by the same people, in the same way, that they judge when an attack that looks like and attack is "significant".
These deadlocks arise often enough even without a deliberate attack that having a way to deal with them is important.  At some point, *any* decision is better than *no* decision.  (Or it isn't.  Sometimes what you learn from the process is that this is decision you don't need to make about a protocol - or whatever - that you don't need.  The way this tends to play out is that eventually most of the participants who were so eager at the start fade away, realizing that the effort is just not worth their while.)
                                                        -- Jerry

@_date: 2015-08-05 13:09:55
@_author: Jerry Leichter 
@_subject: [Cryptography] SRP for mutual authentication - as an 
Safari actually implements such a mechanism:  If the remote site asks for authentication in "the right way" - and, frankly, I have no idea what it is; some sites do manage to trigger the mechanism; most don't - a special box "unrolls" from the top chrome over the page.  I don't know if the effect can be duplicated in Javascript; it would take some effort, I would think.  But since people don't expect this anyway ... there's little point.
                                                        -- Jerry

@_date: 2015-08-10 11:10:36
@_author: Jerry Leichter 
@_subject: [Cryptography] Threatwatch:  CIN - Corruptor-Injector Network 
Seen  for some knowledgable responses.
                                                        -- Jerry

@_date: 2015-08-10 12:26:10
@_author: Jerry Leichter 
@_subject: [Cryptography] Threatwatch:  CIN - Corruptor-Injector Network 
Definitely - but why limit this to "security"?  Our ability to *correctly* build *working*, large computerized systems hasn't kept up with our desire for them.  Security is an area where this happens to stand out, for a number of reasons - but it's an endemic problem, it's been around for years, and it's not clear how to do better.
What makes you think even the large players can defend themselves?
Complexity - it's not alone - has led to a situation where the attack/defend tradeoff is is all on the attacker's side.  This probably won't last - it never has - though one has to be careful about the lessons of history:  Network and system architectures may prove more pervasive and thus much harder to change than things like military strategy.
                                                        -- Jerry

@_date: 2015-08-10 18:35:57
@_author: Jerry Leichter 
@_subject: [Cryptography] Threatwatch:  CIN - Corruptor-Injector Network 
Maybe.  Both Android and, to an even greater extent, iOS (and to a degree MacOS before that, and on a continuing basis), have done violence to many of the traditional assumptions people have about OS's, processes, file systems, and so on.
Note that I'm *not* saying any of these are implementing a capability model - though MacOS with its xpc's is moving in that direction - just that they've implemented *different* models, and programmers have adjusted.
So perhaps there's some hope.
                                                        -- Jerry

@_date: 2015-08-14 17:26:38
@_author: Jerry Leichter 
@_subject: [Cryptography] Threatwatch: CIN - Corruptor-Injector Network 
Perhaps this is true in some generic sense, but it's bizarre to say this in this case.
Someone *from Google* tells you stuff about stuff *done by Google* that's readily checked.  Is google.fr a SAN in the certificate in question?  Simply convert the damn cert into readable form and check.  Is this what Google *intended*?  Who should you ask other than someone *at Google*?  An OSCP at Google 404's if connected to by a browser.  Did Google intend that to happen?  Again ... who would you ask other than someone at Google.
You then have the second level question:  Is this a reasonable configuration?  And that's not hard to check.  For the google.fr case ... this is exactly what SAN's are for.  For OSCP's ... there's documentation out there, but what possible security vulnerability is returning a 404 supposed to represent, even if other OSCP providers choose to do something different.
Then there's all the weird stuff about Comodo and CT.  The question is whether this is a legitimate Google cert.  Someone from Google says it is.  Who else could make a stronger claim for that fact?  CT can provide some evidence that others have seen that cert from Google, which you can accept or not.
Really, this is an absurd claim at this point.  Think about it:  Someone claims that if you try to get stuff from Google, you'll be MITM'ed and will actually get something else.  Someone at Google says, no, what you're seeing is exactly what you should expect to see.  Let's look at the cases here:
1. Google and the person at Google know what they are telling the truth as they understand it, and they are correct:  There's no MITM attack.
2. Google and the person at Google know what they are telling the truth as they understand it, and they are *in*correct:  There really is a MITM attack.
3. Google and the person at Google are, honestly or dishonestly, telling you all is OK; but in fact there is no MITM attack.
4. Google and the person at Google are, honestly or dishonestly, telling you all is OK; but in fact there *is* a MITM attack.
In case 1, all is golden.
In case 2, it makes no difference who makes the statements - there's no point looking for conspiracies.  The attackers are just too good; go back to notes on paper left at dead drops.
In case 3, Google's incompetent, but in fact you're safe anyway.  (But it's hard to square this situation with the actual observations.)
That leaves us case 4 ... but it makes no sense.  If Google is complicit or has just had the wool pulled over its eyes - why would anyone bother with a MITM attack?  Just have Google distribute the "bad" versions of Chrome directly.
There's taking care, and there's tin-hattery.  If you fall into the latter pit ... The Terrorists/Eavesdroppers Have Won.  :-(
                                                        -- Jerry

@_date: 2015-08-23 07:28:43
@_author: Jerry Leichter 
@_subject: [Cryptography] Augmented Reality Encrypted Displays 
Cute idea, but let's do a simple threat analysis:  The device doing the displaying necessarily has access to both images.  Such a device necessarily has access to reasonably amounts of computing power.  Just what is the extra security threat in letting the device do the combining for you?
You might try to argue that the images to be combined would be hidden in other data, so the device wouldn't know what to combine.  But in fact the images are highly distinctive and easy to recognize - they have to be for humans to readily be able to combine them.
I just don't see a situation where having the user combine the images adds anything to the security.

@_date: 2015-08-31 12:36:47
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA looking for quantum-computing resistant 
There are generic attacks, and there are attacks on specific techniques.
The best general attack known is Grover's algorithm, which allows you to search through N elements to find the one for which some predicate is true (e.g., the one encryption key that produces a known decrypted result) in O(sqrt(N)) time (as opposed to O(N) classical time).  There are some restrictions on the predicate, but it gets really complicated to determine with certainty what can and can't be computed this way (assuming, of course, you have a quantum computer!).  So it's probably safer to just assume that "brute force search on a quantum computer is O(sqrt(N))".  The net effect is that to retain the same level of security against brute force search that you had on a classical machine, you have to double the number of bits in your key.  This isn't all that big a deal:  No (classical) technology we can conceive of today could brute force an AES-128 key, so simply going to AES-256 gives you the same safety in a quantum world.
The specific attacks depend on the details of the cryptographic algorithm.  What got this whole field started is Shor's algorithm, which allows factoring on a quantum computer in polynomial time.  (The best known classical technique has exponential time complexity, though it's not actually known whether factoring can be done in classical polynomial time.)  The detailed numbers, again if one could build an appropriate quantum machine, would require that secure RSA keys be large enough to be impractical.  Related algorithms are effective against the Discrete Logarithm problem for integers mod N (used in Diffie-Hellman key exchange) and the related algorithms that use elliptic curves rather then integers mod N.  So most currently-used public key algorithms are attackable if quantum computers are practical.  (Just for comparison:  The current record for factoring an integer using a quantum computer is 56153, which you could factor by hand.  Then again, 20-odd years ago, we didn't know that quantum factoring algorithms existed, much less that they could be implemented at all, so we can't be smug about this - the field is evolving *very* rapidly.)
We have no idea whether there are specific quantum attacks against, say, AES.  It seems highly unlikely.  (The same is, in some sense, true of classical specific attacks against AES.  But the theory and experience in support of the claim that such a classical attack is highly unlikely is much better developed, and has been around a lot longer.)  If you're being paranoid, all you can really say is "we don't know"; but that doesn't give you much in the way of alternatives.
                                                        -- Jerry

@_date: 2015-12-01 16:23:34
@_author: Jerry Leichter 
@_subject: [Cryptography] [cryptography] Paris Attacks Blamed on Strong 
"May have".  Translation:  "We can't prove but you can't disprove it.  So we'll continue to blame encryption for our failures so you won't notice them."  The rest of the NYT articles is a description of those failures.
Fuller context:  "He had given Mr. Hame an email address to reach him on and a USB stick with an encryption key he was to download on his computer. Mr. Abaaoud had promised further instructions by email on where to obtain weapons for the attack and which specific concert hall to strike."  This needed encryption?  Casual messages with directions for places to meet?
When encryption is outlawed, outlaws will use cheap burner phones.
                                                        -- Jerry

@_date: 2015-12-01 16:40:57
@_author: Jerry Leichter 
@_subject: [Cryptography] Large companies sued for using Elliptic 
You're reaching conclusions based on quotes of quotes of text from a patent - and patents are hardly the best places to learn how any complex technology actually works.  Nevertheless, this patent seems to have a detailed explanation of the underlying math.  (I haven't gone through it in detail.)
Both authors have done a great deal of work in cryptography.  Both in particular have worked on "kleptography" - techniques for "spiking" various cryptographic algorithms in undetectable ways.  This work is actually a variation on a theme:  The generated public keys leak information to a suitably-informed third party, while being secure against anyone else - as is generally the case in kleptography.  The difference is that in this case the leakage is an open and desired (by all participants) property of the system.
It also seems clear - to me, anyway - that the details of what the patent claims do not cover any normal use of ECC.  But we may actually never know, because as pointed out in "allenpmd"'s message earlier today, Netflix is attacking the patent on procedural/technical deficiencies in the way it was drafted, and if they prevail - which on the surface seems likely - the patent will be tossed without anyone ever examining what it actually means or covers.)
                                                        -- Jerry

@_date: 2015-12-04 15:09:17
@_author: Jerry Leichter 
@_subject: [Cryptography] Anyone else seen some odd shipping delays? 
Two examples hardly prove anything.  The Snowden leaks concerning this particular supply chain attack implied it was an on-going, well-organized effort.  I very much doubt you'd see anything in the UPS data.  Meanwhile, I regularly get stuff of all sorts - typically *not* computer equipment - which has bizarre tracking data.
I'm not saying your stuff *wasn't* "spiked" - just that what you've observed, to me, neither increases nor decreases my estimate of the probability.
Without access to detailed information almost certainly available only to QNAP - and perhaps not even then - I doubt you have much chance of detecting a competent implant.
                                                        -- Jerry

@_date: 2015-12-08 06:28:40
@_author: Jerry Leichter 
@_subject: [Cryptography] Opinions on signatures algorithms for 
Ahem.  Babai has recently claimed a proof that graph isomorphism is "almost polynomial" and in particular cannot be NP-complete.  This is a big enough result to have hit the mainstream press.  One link:  NTRU is "not known to be vulnerable to quantum attacks" - a much weaker statement than "NTRU is known not to be vulnerable".  There is surprisingly little literature on NTRU, given how long it's been around.  Most of the papers that show up on a quick search are about fast implementations - which is nice, but irrelevant from the point of view of security.
Jaulmes and Joux  claimed a polynomial chosen-ciphertext attack.  That's quite some time ago and presumably the algorithm was modified to counter the attack.
 is a good overview.  It includes the fact that by 2006 an attack (the transcript attack) was known that destroyed the then-extent version of the system.  It was modified again, and the new version is provably secure.
The general claims about NTRU follow an RSA-like pattern:
1.  The underlying problem is hard (much harder, in fact, than factoring)
2.  There's no known reduction from the cryptosystem to the hard problem, but we think there ought to be one.  (Unlike RSA, proposed variants *have* been shown not to have such reductions, i.e., they were attackable.)
It's really hard to know how NTRU will hold up.  The pattern of successful attack followed by modification to avoid the attack has in the past often been an indicator that the underlying idea is unsound:  You close off one attack, and another appears a few years later.  Then again, it's not clear how else you might develop a secure system.  DH has survived multiple rounds of this kind of back and forth; ECC less so. (RSA has proven remarkably robust, in some senses.)
The fact that so much of what's out there spends so much time saying "NP-complete!" and then selling the speed and simplicity and small sizes is ... worrying.
Given who NSA has proven to be, one needs to be very concerned about the way they are pushing cryptography.  Just as we're starting to get a handle on some of the subtle issues in ECC - e.g., that the choice of curve can have fundamental effects on the difficulty of writing correct, secure implementations - the NSA comes along and says "OK, time to move on to all new algorithms that no one (maybe) or NOBUS (No One But Us) really understands the implications of", so move on back to complexity and uncertainty.  Are they pulling us out of the quicksand or pushing us into another trap?
                                                        -- Jerry

@_date: 2015-12-08 15:42:14
@_author: Jerry Leichter 
@_subject: [Cryptography] Who needs NSA implants? 
Dell, Toshiba, and Lenovo PC's come with full remote access vulnerabilities out of the box.  Why bother diverting them?  They're already spiked.
                                                        -- Jerry

@_date: 2015-12-09 07:05:22
@_author: Jerry Leichter 
@_subject: [Cryptography] Who needs NSA implants? 
Sure, I'll take credit for making that point.  :-)
Actually, I should have put a smiley on my subject line.  The only real point I was making was that the stuff we buy is so full of holes, as manufactured and delivered, that the highly sophisticated attacks we like to talk about don't much matter.  It's as if safes were regularly delivered with holes in the walls made of balsa wood painted to look like solid steel, and we were worrying about whether the locks could be picked by doing synthesis of 3D volumetric models based on backscatter X-rays.
BTW, this particular round of vulnerabilities *would* disappear if you wiped the disk and reinstalled the OS, but the previous two sets of vulnerabilities in shipping PC's would survive a clean installation of Windows.  Who knows what else is in there?  The current crop are *probably* due to incompetence rather than enemy action - but could you really tell for sure?
                                                        -- Jerry

@_date: 2015-12-14 15:24:43
@_author: Jerry Leichter 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
You do realize that the reason traditional electric clocks are accurate are that the electric power companies maintain a steady, predictable 60Hz - except that they don't, really, they too only "guarantee" (a) slow drift; (b) long-term accuracy.
Just thought you needed to know that to help you in your new world-view.
"Know yourself.  If you have trouble, call the FBI."
                                                        -- Jerry :-)

@_date: 2015-12-17 07:05:07
@_author: Jerry Leichter 
@_subject: [Cryptography] Satoshi's PGP key. 
Ah.  So you think owning Bitcoins is a bad idea, then?
                                                        -- Jerry ;-)

@_date: 2015-12-17 07:10:48
@_author: Jerry Leichter 
@_subject: [Cryptography] Photon beam splitters for "true" random 
Those trying to draw a distinction between quantum - "real" - randomness and other physical randomness sources might find the following paper of interest:
Origin of probabilities and their application to the multiverse
Andreas Albrecht, Daniel Phillips
(Submitted on 5 Dec 2012 (v1), last revised 19 Nov 2014 (this version, v3))
We argue using simple models that all successful practical uses of probabilities originate in quantum fluctuations in the microscopic physical world around us, often propagated to macroscopic scales. Thus we claim there is no physically verified fully classical theory of probability. We comment on the general implications of this view, and specifically question the application of classical probability theory to cosmology in cases where key questions are known to have no quantum answer. We argue that the ideas developed here may offer a way out of the notorious measure problems of eternal inflation.
                                                        -- Jerry

@_date: 2015-12-17 12:23:37
@_author: Jerry Leichter 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
I suggest you read the paper.
Chaos != randomness.  Nor is it quite the same as unpredictability.  This a common mistake from newbies who define a chaotic system, use it to key a one-time pad, and say "Here's my completely secure system".  Well ... no.
                                                        -- Jerry

@_date: 2015-12-20 19:29:14
@_author: Jerry Leichter 
@_subject: [Cryptography] Questions about crypto that lay people want to 
I'm not aware of any such insurance.  Given that the CA's consistently refuse to accept any liability, it's unlikely any insurance company would want to step up.
Consumers, at least in the US, are generally protected against fake withdrawals and similar problems.  To some degree this is by law, but much of it is just standard industry practice - not because the banks have big hearts, but because they realize that if they didn't provide such protection, they would quickly lose customers.
Interestingly, business accounts are generally provided no such protection, and businesses regularly lose large sums of money to criminals who fake money transfers from their accounts, and similar attacks.  The banks generally disclaim any responsibility, and generally get away with it.  These are not attacks on HTTPS - they usually "go around" the encryption by planting virus on the end-user machines using spear-fished emails.  The same attacks *could* be mounted, just as effectively, against consumers - but since even the accounts of relatively small businesses contain much more money than your typical consumer accounts, and businesses much more commonly deal in much larger monetary transfers (to pay salaries or buy supplies) than you typical consumer.
HTTPS is good enough to protect the privacy of your transactions with your bank, and to assure you that you're probably talking to your real bank:  While attacks could be mounted that would compromise either of these, they would not be anywhere near the easiest attacks to mount for that purpose.  No one will try to break down the steel front door when there's a wood back door with windows not far away.
When you access your bank account, you want reasonable assurance that you've actually reached your bank, and no one can see your transactions.  Beyond that, you trust the bank - you have no real choice, as they are the ones holding your money and all the records.
When you vote, you should not trust whatever agency collects the votes.  In traditional voting systems, you votes are secret from *everyone*, even the people running the polling station.  Think about paper ballots:  The system has to insure that no one but you can see what you wrote on your ballot, but also that your vote is actually counted correctly.  To count votes, someone has to see the ballots!  So what we do is make sure that no one can tie a particular person to a particular ballot - and that no one can remove ballots from, or adds ballots to, the ballot collection box.
In addition, representatives of all the candidates (generally through their parties), and other civic organizations, monitor the entire system to detect any attempts at fraud.  Among they things they watch for is that only those registered to vote actually do so, and that they do so only once.
These are not at all the kinds of things HTTPS provides.  In effect, HTTPS guarantees that the polling place you go to is the real, legitimate polling place; and that no one is looking over your shoulder as you fill in your paper ballot and deposit it in the box.  You need to build all the other guarantees, along with the means for public monitoring, into the system.  This turns out to be a very hard problem.  All the systems out there that have tried to solve it have proved to be flawed.  It's probably possible to have effective, safe on-line voting, but we're nowhere near there yet - and the risks of someone cheating and "stealing an election" are very high.
                                                        -- Jerry

@_date: 2015-12-24 06:09:30
@_author: Jerry Leichter 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
In the interest of paranoid completeness:  If your computer has an SSD rather than a traditional magnetic disk, shred won't actually destroy the data - its writes will simply go to new blocks on the SSD, and the old blocks will go onto an internal, inaccessible list for later cleaning and reuse.  Or perhaps it was detected as beginning to fail and got moved onto the internal bad block list, where no ordinary use of the device will ever touch it again.
In fact, short of physical destruction, there's no effective way to ensure that data written to an SSD is really gone.  (The latest version of MacOS recognizes this fact, and that almost all Macs sold today have only SSD's for storage, and its Disk Utility program no longer offers a "secure erase" function, as it has no way to implement it that actually *is* secure.)
Perhaps you should have run gpg directly on the file from the SD card your camera wrote it to (assuming that's how you transferred it) and them destroyed the SD card; they're pretty cheap after all.
                                                        -- Jerry

@_date: 2015-12-24 12:01:14
@_author: Jerry Leichter 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
And the key you used with dm-crypt was generated randomly ... how?  :-(
It's turtles all the way down.
SSD's are configured with plenty of spare blocks to be swapped in for those that appear to be getting "weak".  If the one where you wrote your secret information looks as if it's failing, it will be moved assigned and you can write zeros all you like, it'll be there forever.
Mind you, you'd have to be unlucky, and effort required to pull it out would probably be beyond the abilities of anyone but the NSA and a few other similarly well-funded organizations.  If you're trying to protect your random bits from them ... you've got real problems.
                                                        -- Jerry

@_date: 2015-12-28 18:25:59
@_author: Jerry Leichter 
@_subject: [Cryptography] Senator Burr: Stopping Terrorists From 'Going 
Nothing new here, actually.  The financial industry has always been heavily regulated - and heavily monitored.  Phone calls on internal lines are recorded and must be made available to the appropriate regulators.  (I helped a lawyer friend in an attempt to improve the quality of such a recording years ago, when we were still talking analogue phones and analogue magnetic tape.  The big problem was the loud beep that was put on the line regularly to remind both ends of the recording - it obscured what was being said.  When did the requirement for such a warning tone disappear?  These days, you get a verbal warning at the beginning of the conversation and that's it.)
Financial crimes have historically been almost unique in that they were "crimes of information":  Trading on illicitly obtained tips.  Improper recommendations to clients.  Exactly how something is said may be as important as what was said.  The records of the actual transactions are rarely sufficient to prove much of anything.
Much of what makes the system work is its transparent, public nature.  Financial crimes depend on keeping stuff hidden.  Trading in stocks was (at one time) only done on public exchanges, where full information about the trades (at least in terms of quantity and price) was available to all equally.  (We've let that erode, with "dark pools" and a wealth of specialty exchanges.  We'll eventually pay the price for that.)
The only other place where "crimes of information" have traditionally been dominant is in politics - selling votes and such.  Here again, we have, in theory, open meetings and records laws and such to keep the system transparent.  Private email servers for public business are forbidden, just as private conversations among brokers in public stock.  Of course, there's law, and there's reality....
                                                        -- Jerry

@_date: 2015-12-29 01:30:44
@_author: Jerry Leichter 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
I wonder about the physics here.  Can a block go bad in such a way that it can be read but can't be erased?  (The practical significance of such a failure mode - assuming it's unlikely - is likely of little importance; I'm just "wondering out loud".)
Of course, *if* such a failure is possible, the next question is whether it can be "encouraged" by an attacker.
                                                        -- Jerry

@_date: 2015-12-29 10:17:37
@_author: Jerry Leichter 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
If someone is in a position to change the hardware, there are tons of things they can do that are certainly undetectable by software, and even extremely difficult to detect by hardware inspected.  Inducing rare faults is hardly high on the list of attacks it would be worth mounting; there are many easier, more effective things to do.
I was instead raising the question of environmentally induced faults.  Could playing around with the power, or heating or cooling, or microwave irradiation, encourage the particular "doesn't erase but can still be read" behavior?
Yes, all of this is almost certainly tin hat territory, but it's by exploring the apparently completely "out there" possibilities that one learns where the edges of the envelope within which safe operation is possible lie.
                                                        -- Jerry

@_date: 2015-12-29 10:42:45
@_author: Jerry Leichter 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
VMS, many many years ago (and still, for the few still using it) had a feature that it amazes me no one has copied since:  You could mark a file "erase on delete".  This had no effect until the file was deleted - at which point it was subject to a secure erasure step before the blocks were freed.  (The erasure mechanism was pluggable; the default one used multiple overwrites, and could even be set for the famous "35 overwrites" that everyone imitated with no understanding of what it was.)
Since this was implemented by the file system, it didn't matter how the file got deleted; it would be erased.  I'm pretty sure the "pending erasure" even survived a reboot - the blocks would not go onto the free list until they had been erased.
Once you build such a feature into the file system, it can be implemented correctly for the features of that file system.  Even if the system uses data journaling, it can use this setting to ensure that replaced blocks are scrubbed immediately.  (There's not much it can do with current SSD's - another of the list of examples where the duplication of functionality between the file system and the disk emulation layer has unexpected and nasty side-effects - but given support in the chips - an extension of TRIM support would do - a file system could make sure that relevant blocks were wiped.)
Of course, you can take this feature "to the limit" and consider every file to be "erase on delete".  There are situations in which this makes sense; there are many where it's expensive overkill.
Sometimes layering considerations can lead you to make sub-optimal choices.  For example, we currently put encryption into one of two layers:  The user layer, or the driver layer, underneath the file system.  But one could imagine encryption integrated into the file system.  The difficult UI issues of requiring the user to make the right choice every time go away.  But some of the annoying constraints of the driver layer - in particular, the fixed size of blocks - go away, too.  A per-file key, with extra space in the metadata for nonces and such, has all kinds of advantages.  And integrating encryption with "erase on delete" makes it possible to erase just the keys, not all the blocks in huge files.
The hardware technologies available for persistent storage went through one revolution with CCD's, and are about to go through further revolutions that are even more significant.  SSD's were (are) the kind of hack that fits a new technology into old slots - the huge electric motor running all the drive belts to individual tools in a factory, simply replacing the previous water wheel or steam engine.  We're only beginning to see use of CCD's in "native" ways, rather than through a disk emulation layer.  As we move forward with newer technologies, it would be good to build in appropriate security support - as John Denker has been proposing for SSD's.  This will require rethinking and often discarding some of the assumptions and architectural verities of the past.  This won't be easy:  Note the Linux team's rejection of ZFS because it didn't fit the existing layering model.
                                                        -- Jerry

@_date: 2015-12-30 00:08:59
@_author: Jerry Leichter 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
It could be much more than half.  Looking just at USB sticks, you can buy them with capacities from 16MB (pretty rare these days) up to at least 256GB, in multiples of two.  No one runs lines at all those points.  They run a couple of lines at some maximum size, then bin the chips based on how much memory is actually good, rounding down to the nearest power of two since that's the way the industry has chosen to market them.  (There's absolutely nothing other than marketing preventing someone from selling 765MB USB sticks.  In fact, some SSD's are sold at non-power-of-two sizes - e.g., 480 and 960GB.)
The "half" you refer to may well be the almost-half that's left over in rounding down to a power of two - at least some part of which will be dedicated to spares.  (Actually, a chip might have to be rounded down by a further factor of two to provide some minimum number of spares.)
However, just because the rest of the stick tested bad, doesn't mean it's actually fully unusable.  It may work but not completely reliably, or some cells may work while others don't.  This stuff will be useless for the purposes for which the stick is sold  - but given suitably written software, it could be used for all kinds of special, hidden purposes.
Story from years back:  The CDC 6600 - the supercomputer of the early 1970's - could be purchased with either (I think) 128KW (Kilo-Words - a word was 60 bits) or 256KW of main memory.  NYU purchased one.  They asked for the 128KW version.  CDC tried to convince them that they should really get the full 256KW.  But the institution had a grant and couldn't swing the extra money - probably a couple of hundred thousand in those days.  So CDC finally delivered a 128KW version.  But ... the developers who played around with the machine found that they could actually get at memory beyond the 128KW limit.  Writes and reads to it ... worked just fine.  In fact, what they eventually determined was that the machine they had came equipped with the full 256KW - CDC had just made a special patch to their copy of the OS to have it limit itself to the lower half.  A source within CDC eventually explained to them that CDC listed a 128KW option, but that they didn't really expect anyone to buy one: If you were spending the tens of millions one of these things cost, you were not going to go short on main memory.  NYU was, in fact, the first customer to buy one of the "small memory" configurations - and CDC hadn't actually worked out the necessary manufacturing changes to build one.  So they just shipped the configuration they had and patched the OS.
(During the anti-Vietnam-war protests of the late 1960's, a group took over the machine room in which the CDC 6600 was stored.  They held it hostage - and when they left, the left behind some incendiary devices on long fuses.  The machine was barely saved from destruction by some faculty members - the story was recently retold.  What I heard from people there was that they would not have minded so much if the machine had been torched:  Insurance would have replaced it, but the replacement would not have been one of the very early runs - I think serial number 4 - which was too early to support Extended Core Storage, which used slower but cheaper memory - we're talking magnetic core in those days - as a kind of I/O device.  The fact the machine was such an early model makes the story about main memory more believable.)
                                                        -- Jerry

@_date: 2015-12-30 20:46:35
@_author: Jerry Leichter 
@_subject: [Cryptography] Write-protect switches, etc. 
At one time, you could get special "logging magnetic tape drives":  They physically could not backspace the tape, and when they reached end-of-tape they automatically rewound and unmounted the reel.  (The old magnetic tapes also had a "write ring" that had to be inserted in the back to be writeable at all.  Given the technology of the era, these almost certainly controlled physical switches controlling the write current.)
A number of mechanisms to implement append-only logs have have been proposed over the years - I think Bruce Schneier is a co-author on one.  They all have the property that an attacker can only destroy the previous logs completely, not modify entries within them - the strongest property such a system can have.  (Add periodic distribution to a number of independent storage sites if you want to ensure availability.)  While the blockchain would do it, it's likely overkill for this particular problem.  (Of course, if you already have it and are using it for other purposes, there's no reason not to use it here, too.)
As I noted above, it's always possible to destroy the log.  If all else fails, physical destruction of any recording medium is available.  All you can expect is that tampering (or destruction) is always detectable.
                                                        -- Jerry

@_date: 2015-12-30 20:54:27
@_author: Jerry Leichter 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
It could be much more than half.  Looking just at USB sticks, you can buy them with capacities from 16MB (pretty rare these days) up to at least 256GB, in multiples of two.  No one runs lines at all those points.  They run a couple of lines at some maximum size, then bin the chips based on how much memory is actually good, rounding down to the nearest power of two since that's the way the industry has chosen to market them.  (There's absolutely nothing other than marketing preventing someone from selling 765MB USB sticks.  In fact, some SSD's are sold at non-power-of-two sizes - e.g., 480 and 960GB.)
The "half" you refer to may well be the almost-half that's left over in rounding down to a power of two - at least some part of which will be dedicated to spares.  (Actually, a chip might have to be rounded down by a further factor of two to provide some minimum number of spares.)
However, just because the rest of the stick tested bad, doesn't mean it's actually fully unusable.  It may work but not completely reliably, or some cells may work while others don't.  This stuff will be useless for the purposes for which the stick is sold  - but given suitably written software, it could be used for all kinds of special, hidden purposes.
Story from years back:  The CDC 6600 - the supercomputer of the early 1970's - could be purchased with either (I think) 128KW (Kilo-Words - a word was 60 bits) or 256KW of main memory.  NYU purchased one.  They asked for the 128KW version.  CDC tried to convince them that they should really get the full 256KW.  But the institution had a grant and couldn't swing the extra money - probably a couple of hundred thousand in those days.  So CDC finally delivered a 128KW version.  But ... the developers who played around with the machine found that they could actually get at memory beyond the 128KW limit.  Writes and reads to it ... worked just fine.  In fact, what they eventually determined was that the machine they had came equipped with the full 256KW - CDC had just made a special patch to their copy of the OS to have it limit itself to the lower half.  A source within CDC eventually explained to them that CDC listed a 128KW option, but that they didn't really expect anyone to buy one: If you were spending the tens of millions one of these things cost, you were not going to go short on main memory.  NYU was, in fact, the first customer to buy one of the "small memory" configurations - and CDC hadn't actually worked out the necessary manufacturing changes to build one.  So they just shipped the configuration they had and patched the OS.
(During the anti-Vietnam-war protests of the late 1960's, a group took over the machine room in which the CDC 6600 was stored.  They held it hostage - and when they left, the left behind some incendiary devices on long fuses.  The machine was barely saved from destruction by some faculty members - the story was recently retold.  What I heard from people there was that they would not have minded so much if the machine had been torched:  Insurance would have replaced it, but the replacement would not have been one of the very early runs - I think serial number 4 - which was too early to support Extended Core Storage, which used slower but cheaper memory - we're talking magnetic core in those days - as a kind of I/O device.  The fact the machine was such an early model makes the story about main memory more believable.)
                                                       -- Jerry

@_date: 2015-02-01 10:29:36
@_author: Jerry Leichter 
@_subject: [Cryptography] best practices considered bad term 
The Secure  Coding CERT - where  can be instantiated with several widely-used programming languages - generally do this very well.  Not only do they provide a rationale, they usually provide examples, both positive and negative. People will remember a good example long after the wording of the principle has faded from memory.
                                                        -- Jerry

@_date: 2015-02-01 23:22:53
@_author: Jerry Leichter 
@_subject: [Cryptography] best practices considered bad term 
Is there some truth to this assertion?  Sure.  But consider the same discussion about the National Electrical Code.  It's a bunch of rules - no justifications or arguments, mind you, just rules.  If you follow the rules, you won't have trouble getting your town's electrical inspector to approve your work.  Or ... you can do it your own way and get into infinite arguments.
If you're an electrician, and you follow the rules, you also are much less likely to be sued, or to lose a suit, it something goes wrong and the house burns down.
Is following the rules in the Code a way of avoiding fights with the town and lawsuits?  Sure.  But is it *just* that?  Hardly.  The Code is, to a large degree, the distillation of many decades of experience with electrical wiring and how it fails.  Is it overkill?  Does it sometimes retain old requirements that no longer make much sense?  Sure.  But if I'm buying a house, I like knowing that it's "up to code".  It may be boring and over-engineered, but it probably won't start a fire while I'm asleep.
                                                        -- Jerry

@_date: 2015-02-02 21:55:22
@_author: Jerry Leichter 
@_subject: [Cryptography] best practices considered bad term 
Given the degree to which we are all dependent on these technologies already, that just won't fly.
You can actually say that with a straight face after the last 12 months?  Heartbleed - and all the other bugs that the first serious look at OpenSSL then revealed?  Multiple serious bugs in bash?  Now GHOST?
Prediction for 2015:  As HTML5 begins to take over from Flash, the first serious vulnerability in an HTML5 implementation will appear this year.  It won't be the last.
The dream that OSS would magically give us bug-free, secure code died in 2014.  There is no magic, only hard work - and we don't even know exactly *what* we should be working on.
                                                        -- Jerry

@_date: 2015-02-03 06:46:42
@_author: Jerry Leichter 
@_subject: [Cryptography] best practices considered bad term 
I've seen similar contrasts between *commercial* projects.
Unfortunately, the economics of the marketplace - whether it's a matter of the actual sales needed to keep a commercial project going, or the adoption and ongoing interest that provides the people and ongoing interest to keep an OSS project going - pushes hard against measures of internal quality and care.  Such projects are very few and far between.
Apple has figured out a way to sell *external* care and quality.  (Whether you, personally, like their designs isn't the point - enough people do, and are willing to pay to show that they do, to make it as successful as it is.)  Back in the heyday of Windows, no one (outside of a then-tiny and insignificant group of Apple groupies) believed that this could possibly matter.  "What people want is the cheapest possible PC."  Or phone, or whatever.  Time to market dominates all - first-mover advantage, yada yada yada,  You still hear those arguments made all the time when it comes to security.
So far no one has managed to find the way to market the qualities that emerge from solid internal design and care.
                                                        -- Jerry

@_date: 2015-02-03 19:35:39
@_author: Jerry Leichter 
@_subject: [Cryptography] traffic analysis -> let's write an RFC?, 
I've always read this is a matter of defense in depth.  Why give your opponents a leg up?  If you can keep them in the dark about how the system you're using works, they have a harder time breaking it.
Also ... if the system *really* is secure against attack by anyone, including its maker, when the key is unknown ... why would you hand it to your opponent?  He can then use it and keep *you* out.
                                                        -- Jerry

@_date: 2015-02-10 05:21:05
@_author: Jerry Leichter 
@_subject: [Cryptography] Security vulnerabilities in BMW's ConnectedDrive 
Auto makers have a long history of having to replace hardware in order to resolve problems with cars they've sold - including cars they sold many years ago.  (Look at the rolling disaster of the airbags in millions of cars for an example.)  So, of all the players out there, they are probably the best able to deal with that need, should it come to it.
There are infrequent physical fixes done by computer hardware makers.  Apple has had a couple - e.g., to replace disk drives that failed too frequently in one generation of iMac's.  (In other cases, they haven't been as willing to replace allegedly faulty hardware, which has led to some lawsuits.)  Other hardware makes have probably done the same, though I don't off-hand recall any examples.
(One thing Apple and auto makers share is that they maintain a relationship with their customers - so they have some way of reaching them and getting them to come in.  It's a great deal harder for companies that sell indirectly through retailers; often they have no idea who has their products.)
                                                        -- Jerry

@_date: 2015-02-10 17:49:12
@_author: Jerry Leichter 
@_subject: [Cryptography] Do capabilities work?  Do ACLs work? 
There's a more fundamental issue here:  As engineers, we try to formalize everything.  But many human processes are not amenable to formalization.  They involve tons of assumptions about the way we interact.  If Bob doesn't drive directly to Dave's but takes a 10-mile detour to get lunch, most people would say that was well within the access Alice granted.  If he takes it on a 500-mile drive, because he always wanted to tour in a Porsche, clearly not.  If Bob brings along his friend Sam, that's fine.  If
Bob happens to be an Uber driver and takes along a paying passenger ... not so much.
The whole issue of authorization and who owes what to whom gets to questions that legal systems have been trying to deal with for thousands of years.  They have limited formalizations, but still needed judges and juries to deal with the edge cases.
I'd claim that a system that can really cover all these kinds of complications would be a fully-functioning human-level AI - and we would no more be able to formalize what it was doing than we could formalize what we are doing.
Computers have lead to a formalization of policies.  Anyone who's ever had to interact with an organization where policies are enforced "by the system" knows that what's lost immediately is the flexibility that humans bring.  Anyone who's worked in such an organization knows that human beings end up producing "hack arounds" because "the system" doesn't let them get their work done.  Does this introduce vulnerabilities?  Of course; but as in all things in the real world, it's a tradeoff.
Imagine a world in which your car would not let you violate any traffic law.  Do you think that would be workable?
Yes, we need more powerful ways to describe policies (and, more generally, business practices).  Yes, even more, we need ways to make the policies and practices we formalize comprehensible to and manipulable by human beings in a useful way.  (I can guarantee you that any organization that uses more than a couple of trivial ACL's cannot answer pretty simple questions about who has access to what resources.  In some ways, capabilities are *worse* - without significant help from the system, the kinds of questions we regularly ask - who could have read the file? - cannot be answered.  We as human beings are really bad at following chains of relationships.  This shows up for ACL's when you start nesting them - or worrying about what someone with Control access might be able to do.  For capabilities, "portability" is exactly the point, so you hit this much sooner and much more pervasively.)
But don't make the mistake of thinking that the goal should be to represent every trust and responsibility and authorization relationship.  That way lies madness.  In the old days, a bank didn't try to track access to each dollar bill or each account.  Stuff was controlled at a broad level, with dual controls for particularly valuable assets and - returning to the message to which I'm responding - auditing to catch violations.  That's a much better model than what we typically try to build these days.
                                                        -- Jerry

@_date: 2015-02-11 06:37:19
@_author: Jerry Leichter 
@_subject: [Cryptography] What do we mean by Secure? 
Prove to me the *presence* of a desk in the room in which you are working (or of any other specific fact).
The problem here isn't with the kind of fact, it's with the whole notion of "proof".  Mathematics has "proofs" that are alleged to be eternal and independent of the prover and of any specific background.  No such things *ever* exist about facts in the real world, where facts are inherently contingent, reliant on particular observations, dependent on interpretation.
And you know what:  Even the mathematical proofs aren't quite what they seem.  While they may appear to exist in some Platonic elsewhere, independent of human beings, it's ultimately created and checked by humans (or perhaps these days by programs created and checked by humans).  Proofs have stood for years, only to be found erroneous later.  Their meaning is embedded in a huge matrix of definitions and complexities; later reinterpretations have rendered them, not false, but not as all-powerful as they seemed.  It's trivial to prove that there are no infinitesimal values in the real numbers.  But ... it's also possible to define "non-standard" models of the reals that share all the properties you thought defined the reals ... but which have infinitesimals.  Logic itself isn't as black and white as we like to think.  The the ancient Greeks, the parallel postulate was either true or false - we just hadn't found a proof one way or the other.  Well ... no.
                                                        -- Jerry

@_date: 2015-02-12 13:38:43
@_author: Jerry Leichter 
@_subject: [Cryptography] Do capabilities work? Do ACLs work? 
I doubt this explanation.  The reason Unix (and Windows, and VMS) went with ACL's is that they were a relatively simple extension to what they already had:  Information on the files that defined access for well-defined groups of users.  Files were already persistent and already had all kinds of metadata associated with them.  The mechanisms for checking access at the time of access were already there and organized as "does the currently running user have access to this file".  File systems fit in a hierarchy and there was a model (more complex than most people thought, and somewhat different on the various systems) for how access to directories influenced access to files and directories under them.  Commands were organized around access to groups of directories and files - only specialized commands saw "user" or "group" as an object at all, and they usually didn't have easy ways to do things to multiple users or groups at once.
So adding ACL's was relatively simple and fit nicely with what was there.  Adding capabilities would have required some top-down redesign, and users would have to learn entirely new concepts.
BTW, not all ACL systems are the same.  The Unix ACL system is in a way just a glorified extension of the existing group system.  The VMS ACL system was much more elaborate and interesting.  I haven't looked at in years, but among the things you could do were:
- Associate an ACL with an executable that would be granted to the process while that executable was running.  This is setuid/setgid taken to the ultimate level.
- Associate an ACL that would send an alert to the security logs if various actions touched the ACL - from attempting to remove it to even just looking at it.
- Make ACL's non-browsable.  If you knew the exact ACL (and had the appropriate privileges) you could change it, but wildcards wouldn't show it.  On the other hand, even if you couldn't see an alarm ACL, it could still raise an alert.  That is:  You could plant invisible trip wires anywhere you wanted in the file system.
                                                        -- Jerry

@_date: 2015-02-12 19:04:49
@_author: Jerry Leichter 
@_subject: [Cryptography] Do capabilities work? Do ACLs work? 
Well ... yes.  But then Unix came along and simplified things - and its model of hierarchical directories and user/group/world access bits pretty much won out.  The old models and mechanisms were mainly forgotten, to be rediscovered later.  (It's an old joke that most OS work in the last couple of decades is a rediscovery of stuff done in Multics.)
Yes.  And it's not as if we haven't gone through this before.  When I was teaching OS classes in the early 1980's, I would first talk about the protection mechanisms needed on time-shared machines, then ask the question:  On single-user PC's, do we need security?  It made for interesting discussion.  At the time, of course, the Internet was a novelty used mainly by academics and a few companies and a bit of the military, viruses were a very new phenomenon, and the attacks we consider boring and commonplace today hadn't been invented.
There *were* people thinking about these issues even then - and even earlier.  Most systems only provided DAC (Discretionary Access Control - the user has complete ability to change everything).  But there were those calling for MAC (Mandatory ...) exactly because you wanted to be able to keep things safe even when people - or, more likely, programs they ran - did things they shouldn't.  At the time, most of the impetus for this came from the military, where MAC had to do with classification level - which never fit well with anything else.  This hid the broader ideas underneath it all....
The only mechanism I've seen implemented for this is the very old idea of a CONTROL access right on an object that you need to change the ACL's on the object.  The interaction of this with ownership of the object is ... interesting.  People generally assume that ownership implies all access rights - but that's not necessarily what you always want.
BTW, an interesting side issue is what objects can be controlled by the various mechanisms.  Unix systems limit their richest access control to file system objects.  Other objects (e.g., System V IPC) have ad hoc mechanisms.  Plan 9 fixed this by logically moving everything into the file system space, an idea picked up in a much more limited fashion by Linux and the /proc file system.  VMS did this by making the access control mechanism a separate component and applying it fairly uniformly to all kinds of objects, not just file objects.  Windows went in the same direction but had so much backwards-compatibility crud to deal with that it could never do it uniformly.
                                                        -- Jerry

@_date: 2015-02-15 08:03:58
@_author: Jerry Leichter 
@_subject: [Cryptography] Do capabilities work? Do ACLs work? 
Consider applying the security policy:  File F can only be accessed by those working in Area 51.  This is an ambiguous definition of a set (perhaps) defined by comprehension rather than extension.  (Note that I'm assuming "working in Area 51" is not a pre-existing set in the system - it's an example of some arbitrary testable predicate that we wish to be able to support.)  We can look at three plausible interpretations:
1.  Policies are defined by extension at the time they are created:  Produce the list of all people (or other actors in the system, if appropriate), construct the set of those who are currently working in Area 51, and grant each of them access.
2.  Policies are defined by extension at the time of attempted access:  At the moment an actor tries to gain access to an object, update the policies to reflect current reality before applying them.  That is, what matters is whether the actor is working in Area 51 at the time they try to gain access to the object.
3.  Policies are defined by extension at the time of each operation on an object.  (The difference between this and 2, in more familiar terms, is that 2 controls who can open a file, but once it's open they can do what they wish with it; while 3 would validate every read, write, etc.)
Interpretation 1 could be implemented by either ACL's or capabilities.  For suitably defined ACL's, it involves attaching the definition of a predicate on actors to F, and evaluating it against any actor who attempts access.  (Most ACL systems today simply provide a list of exact matches or memberships in pre-defined groups and OR's the result.  One can be much more general; at the simplest, have a pair of lists, "allow" and "deny", and take the AND of the OR of membership in the "allow" list and non-membership in the "deny" list.  VMS had at least this.)  For capabilities, there's no issue in principle, but implementation seems expensive:  You have to scan every actor in the system, apply the predicate to them, and then modify their capabilities accordingly.
Interpretation 2 adds little or nothing new for ACL's.  It's unlikely that any ACL system would implement Interpretation 1 by materializing a list of all the actors who are working in Area 51, which is what a capability system would have to do - so I cheated a bit.  If you really wanted to implement Interpretation 1 in an ACL system, you'd want to store a historical record of changes to the Area 51 roster, and have ACL's that looked, not at current membership, but at membership at some point in the past.  I've never seen a system that does this, though databases that allow such queries to be posed are widely used - and it's worth noting that if you want to be able to do complete auditing and forensics, you need to be able to answer questions like:  "Did P have access to F at time T"?
Interpretation 2 is by far the most common one.  It's also not at all clear how one might reasonably implement it in a capabilities system.  It requires keeping the predicates used to define capabilities around, and re-applying them to add and remove capabilities whenever their values change.  In effect, this would be a system in which you kept hidden ACL's (the predicates), and the capabilities were simply cached, pre-computed values of those ACL's attached to each actor.  (Of course, you could make the caching lazy and do the computation when needed. The result would be pretty much equivalent to using ACL's.)
Interpretation 3 just ups the ante on how often you need to evaluate the set of workers in Area 51.  If that's expensive ... it becomes much more so.  The only system I know of that implements this, and only to a limited degree, is Windows:  If you remove a user, his existing processes are quickly killed.  (If you go to lower levels of abstraction, this is common:  CPU's check access rights on every memory access.)
Someone earlier mentioned the paper Capability Myths Demolished
( a paper a read a while back and re-read now.  It's a nice piece of work, and makes some excellent points, but I've always felt that as what amounts to a polemic, it cheats a bit, in two different ways:
1.  It's response to complaints that "ACL's can do X but capabilities can't" is "oh, but that's because we have this neat extension to the capabilities model and a new implementation that lets them do X"; but it never considers the possibility that those favoring ACL's might also be able to play the same game. (For example, VMS, when it computed the list of predicates to be checked, took not just the current accessed object's own ACL, but merged in ACL's for a hierarchical list for - just going from vague memory now - the process, the group, and even a system-wide list.  That's how an ACL associated with the currently running program became effective.  The system-wide list could be used to produce effects similar to Unix run levels.)
2.  It cherry-picks examples for which capabilities do well and ACL's don't.  There's nothing there like my Area 51 workers example.
In fact, you can sometimes take duals of their examples to produce examples that cut the other way.  The Confused Deputy problem is based on a timing ambiguity about which object an ACL is supposed to apply to; but there's a dual issue in the Area 51 problem, which has a similar timing ambiguity about which actors are in the set.
                                                        -- Jerry

@_date: 2015-02-16 07:56:47
@_author: Jerry Leichter 
@_subject: [Cryptography] Do capabilities work? Do ACLs work? 
My intent was to be ambiguous; but I intended to cover two possibilities, which ultimately only differ in how dynamic they are:
1.  Some people are assigned to work in Area 51; some are not.  The ones who are are allowed access.
2.  Some people are *physically present* in Area 51; some are not.  The ones who are are allowed access.
Obviously, we assume that any actor within the system "gains its access rights" based on a person for whom it's acting, and that the system can determine the person and can evaluate the appropriate predicate.
Yes, this is the intent - though be careful of the time aspect, which was explicitly part of the problem; isInA51() can vary with time.
Terms from philosophy/logic.  A set defined by extension is given as a particular list of elements; a set defined by comprehension is picked out of a larger universe by a predicate.  The set H of all pairs  where TM is a Turing machine and T is an input tape such that TM halts when started on it is defined by comprehension, but no Turing machine could write down the members.  (This is an extreme example since no Turing machine could even *test membership* in the set for all possible inputs.)  Since everything we do on computers is finite, *in principle* there's no difference between the two notions - but in practice there often is - and when you consider the time element, and the extension of the set varies over time and we can at best record past values, not predict future ones, the difference can be stark.
Yes, that's related.
No, not at all.  I'm explicitly saying that you need to record the predicate as some kind of executable code; you can't just record the set of actors who match it at some fixed point in time.  All I'm forbidding is that you can't just assume away the problem by positing that a test for the predicate is already there.
That's fine, but you're really no longer talking about anything I would recognize as a capability.  In fact, you've pretty much lost your organizing model!
- For ACL's, the form of open()'s code is:  For each ACL on the file, check (using fixed code) whether this actor "matches" the ACL and grant/deny access on that basis.
- For capabilities, for each capability on this actor, check (using fixed code) that whether this capability "refers to" this file and grant/deny access on that basis.
Here, you're delegating the test to an accessPermitted() function.  But *we're inside of the systems's accessPermitted() function*, and the whole point of choosing either ACL's or capabilities was to provide a simple structure for such a function that we could reason about.  If you defer to an external function, you have to start your design all over again.  I'll say more about this below.
All three are important, for different reasons.  The two you mention are obvious, but if I'm attempting to audit my *policy*, as opposed to a particular stream of events that happened to occur, this is important information.  (There's a similar and fundamental idea in databases, where there are predicates that are constraints and must always be true in all possible states of the database, and predicates that just happen to be true right now.  One of the things the relational theorists clarified was the importance of this distinction.)
I don't see that you've "implemented it in a capabilities system".  You've essentially punted it to code that's up to someone else to write.
BTW, I'll grant you that it's a bit of a stretch for the ACL system as well - but much less of one.  An ACL system always has to ask:  Does this actor "match" this access control entry?  For the Area 51 problem, the user has to provide a way to  answer the question "Is this actor working in Area 51 right now"?  The function is independent of the file, and of any other ACE's on that file; all decisions about actual access remain within the ACL implementation.  That's very different from being asked to implement a "accessPermitted()" function.
You're now talking specific operations in KeyKOS, and nothing at all having to do with the capability model as such.
Stepping back for a moment:  I've implemented access policy mechanism (within particular programs) in the past, and I've always found that if you've already got an object-oriented model, capabilities are the most natural way to go *for the implementation*.  How you want to *represent policies to the user of the system* is a whole other story.  In practice, something like ACL's is usually a better fit.  Policies are phrased in terms of sets of users (often defined by predicates that vary over time - i.e., by comprehension).  But the user-interface "ACL-like" things usually are not directly representable *as* ACL's; one way or another, you have to translate them into whatever your system actually uses internally.  And, *much* harder (I have yet to see a plausible solution) you have to translate the *net effect* of all the rules in the system into something comprehensible to users.
None of the operating systems in wide use hew to object-oriented principles.  There are bits and pieces that look object-like, but that's as far as it goes.  Since people have been arguing for - and building examples of - object-oriented OS's for several decades now, with little effect on what's actually being used, at some point you have to concede that maybe there really *is* something missing from the object-oriented framework when it comes to OS's.  (I've come to that conclusion more generally, but that would take us too far astray.  OO is something that can be appropriate, but it can also be inappropriate.  The trick is knowing how to judge that, and having alternatives design styles to use when it's inappropriate.)
It's also the case that we've spent so many years on the debate of ACL's OR capabilities that we've closed our eyes to the limitations of *both* - and, indeed, of the underlying access matrix model.  Relational database systems, and the security policies appropriate to them, don't fit either; and, indeed, the usual way to define access policies in RDBM's (beyond simple ones that are usually done in a style that's mainly ACL-like) is to define a query that gets combined with a user's query, filtering out the results that are not supposed to be accessible.  (I guess, looking back, that that style of doing things led me to the Area 51 problem.)
                                                        -- Jerry

@_date: 2015-02-16 19:58:29
@_author: Jerry Leichter 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
Kaspersky Q and A for Equation Group multiple malware program, in use early
Two articles that are well worth reading.
Back in the 1980's, I knew a bunch of the security guys at DEC.  While this was a much less threatening time, even the DEC internal network of that period saw attacks here and there.  What the security guys said was that they had all kinds of attacks that they would find, analyze, and lock out. But there was this residual collection of "ghosts":  They'd see hints that something kind of attack had taken place, but they couldn't find any detailed trace of how, where, or by whom.  The guys doing it could get in and out and at most leave a bit of an odd, unexplainable event behind.  They assumed it was government attackers, but could never prove anything.
It should be no surprise that this kind of thing has been going on for years.  The first papers on attacks on and defenses of computer systems from a military point of view go back to the 1970's.  (The Air Force took the early lead - or perhaps they just let more out.)  For a while, some of this work was in the open; the famous Rainbow Series of reports was one result.  But then it all went dark - a fact that's now obvious in retrospect, though I don't recall anyone commenting on it at the time.  (One wonders if this was the result of the NSA taking over fully.)
With unlimited funding and years of practice, these guys are way ahead of the rest of us.
Here's an interesting comparison.  Most academic cryptographers believe that the NSA has lost its lead:  While for years they were the only ones doing cryptography, and were decades ahead of anyone on the outside, but now we have so many good people on the outside that we've caught up to, and perhaps even surpassed, the NSA.  I've always found this reasoning a bit too pat.  But getting actual evidence has been impossible.
So now we have some evidence from a closely related domain.  It's not as if the world isn't full of people attacking software and hardware, for academic fame, for money, just for the hell of it.  And yet here we have evidence that the secret community is *way* out ahead.  Sure, there are papers speculating about how to take over disk drive firmware.  But these guys *actually do it*, at scale.
Should we be so confident that our claims about cryptography are on any firmer ground?
                                                        -- Jerry

@_date: 2015-02-16 19:35:44
@_author: Jerry Leichter 
@_subject: [Cryptography] Encryption used to secure transports is used to 
Where's this from?  Is this really supposed to be news?
Is this connected to the previous link?  (Perhaps it was before it got updated, but when I looked at it, I could see no connection.)
                                                        -- Jerry

@_date: 2015-02-16 20:27:04
@_author: Jerry Leichter 
@_subject: [Cryptography] phishing attack again - $300m in losses? 
The closest I've seen is a feature in  Apple's Mail.app which allows you to whitelist a bunch of, not really domains, but really just suffixes on email addresses - usually something like "  Any address that doesn't match the list gets highlighted.
Unfortunately, this isn't useful for the kind of thing you have in mind because it only applies to *outgoing* mail - i.e., it lets you quickly check if you're sending mail someplace unexpected.
You could, however, used Mail.app's rule facility to do something closely related:  You can define a rule that triggers if the sender is/is not in your address book; is/is not a member of a specific group defined in your address book; and is/is not in your "previous recipients" (i.e., the addresses you've gotten mail from before).  Based on the results, you can set the color of the message, move it to a separate folder ... all that sort of stuff.
I don't feel like digging through the Outlook interface to see whether its rules can do the same, but I bet they can.  gMail rules may well be able do it as well.
Of course, you have to decide to do this.  It's funny, but I never thought about doing it before.
This would eliminate *forged* emails - perhaps slowing the initial penetration.  But the typical modus operandi for expanding from the initial beachhead is to use the account(s) one has to send phishing mail to the accounts of people they already correspond with.
Maybe.  The mechanisms are already there; it would be a worthwhile exercise to start trying them out to see how they work in practice.
                                                        -- Jerry

@_date: 2015-02-17 06:24:10
@_author: Jerry Leichter 
@_subject: [Cryptography] Do capabilities work? Do ACLs work? 
A whole other world.  Interestingly, most of the work tends to center on reading - a database by its nature is usually filled by some background process that has very broad rights, but is then available to a large number of individuals how have varying access rights.
A very simple policy might be:  An individual can only read his own salary and that of people who work for him.  Keep in mind that "work for him" is stored in the database itself.  In general, *everything* is stored in the database itself.  So you could have a policy like:  A person can only see the salaries of people who make no more than he does.  (I actually used a system with a policy that you could only see "target salary" ranges for "bands" of salaries below your own.  People found a hack - I can't recall how it worked - that actually let you see your own band's "target" range. BTW, this was, of course, restricted to "managers" - there were nominally parallel tracks for managers and technical people, but no matter how high on the technical track you were, the "manager" stuff was closed to you.  And what defined a manager?  That was in the database, too:  A manager was anyone who had at least one person reporting to him.)
It's because everything is in the database system itself that the natural way to define policies is by adding to each user query a further filter using the DB language itself.
There's been a whole ton of work on how to allow access to statistical information without allowing access to individual information - e.g., average income in a census tract but not the income of any one person.  But you want that combined with other selections, like average income for men vs. women in a census tract.  Unfortunately, it turns out this is unachievable - with any kind of reasonable query mechanism, it's possible to produce sets of queries that zoom in on an individual.  (The phrase to look for is "trackers in database systems".)  But of course this has led to alternative formulations of the problem and some practical solutions.
Returning to cryptography, there's all sorts of interesting work on enforcing some constraints cryptographically - how to have a tuple of data that's encrypted as a whole, but such that you and I have keys that can only decrypt some fields but not others.
A very different world than OS's.
                                                        -- Jerry

@_date: 2015-02-17 10:56:56
@_author: Jerry Leichter 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
What evidence is there for this?
Again, do you have any evidence?
It's not that I have evidence the other way.  We just don't know.  What concerns me is that most of the arguments are "faith-based" - the kind of arguments that support "open always wins":  No matter how big/smart you are, there are more smart people who *don't* work for you than who *do*, and in the long run the larger number of people, openly communicating and sharing, will win.  And yet Apple sold more phones in the US last quarter than all Android makers combined - the first time they've been in the lead.  It's not even clear how to compare the number of smart cryptographers inside and outside of NSA - and NSA has more funding and years of experience they keep to themselves.  This is exactly how organizations win over smart individuals:  They build a database of expertise over many years, and they are patient and can keep at it indefinitely.
Why would they push for new stuff out in the open world?  They *should* be pushing for it, because they *should* be putting more emphasis on defense of non-NSA systems. But what we've seen confirmed repeatedly over the last couple of years is that they have concentrated on offense - and against everything that *isn't* an NSA system.  (To the point where they've apparently even neglected defense of their own internal systems:  What Snowden did was certainly something they *thought* they had a defense against.)
Actually, in that case, I think there's a simpler explanation:  Their models were really the only ones out there, because they'd been dealing with the problem for many years.  Industry hadn't - its needs for security models were, until the pervasive computerization of information, much simpler and in little need of formalization.
There's precedent for this.  When large-scale industrial organizations came into being - a fairly recent development; Engels, Marx's friend, owned what was then one of largest factories in England, employing a few hundred people - they had to figure out how manage themselves.  They copied the only form of organizational structure for large numbers of people that then existed:  Militaries, which followed a style going back to Roman times.  Think about the traditional factory:  Large numbers of "workers" out on the floor; a much smaller number of ex-workers promoted to line management; and then a hierarchy of "professional managers" - with specialized training; almost never promoted from among the line workers - above them.  It's not coincidence that this looks exactly like the traditional army, with its privates, non-coms, and a professional officer corps.  New models for large corporations only started to arise in the late 1960's, with the development of so-called "knowledge organizations".  (The military has had to back-port some of these innovations as it, too, has become more knowledge/expertise based.)
They apparently haven't even tried, on the defense side - and I agree that we're probably out ahead because of this.  But they're certainly working hard on the offense side....
Maybe.  It's really impossible to say.  Two days ago, I would probably have agreed with you.  Now ... I'm not so sure.
*But attacking these security systems is exactly what they appear to be experts at!*
                                                        -- Jerry

@_date: 2015-02-17 15:52:45
@_author: Jerry Leichter 
@_subject: [Cryptography] phishing attack again - $300m in losses? 
MacOS does this.  You can run your Mac in one of three modes:  Allow only stuff signed by Apple (this is stuff from the Apple App store); allow only stuff signed by Apple or a developer approved by Apple (i.e., by a developer who has a key that was signed by Apple); allow anything.  Signature checks are done every time the application is run; an admin can override the checks once or forever.  (There's a separate, older check that goes off the first time you run an code that was downloaded - it tells you the Web site it came from and the date.)
Of course, iOS had only the first mode - a source of endless whining by some.
MacOS has a sandbox (based on the BSD sandbox facility), but they haven't figured out a way to expose this to end users - programming API or command line only.  Some Apple-provided stuff runs in a sandbox, and stuff from the Apple App store is also sandbox.  The former works well, because of course Apple can tune its own code, its own sandbox implementation, and the sandbox configuration it uses to make *sure* it all works well.  The latter has been problematic, as the sandbox isn't tunable and as configured prevents sandboxed apps from doing many useful things.
It's a tough tradeoff.  No one I know has produced a reasonable way for users to answer questions about how to configure a sandbox.  Android has a pretty fine-grained privilege system, hardly anyone really looks at the privileges that apps ask for - it's too hard to understand what they *should* reasonably be asking for, and what the implications are.  At best, cautious people will refuse to install apps that seem to ask for inappropriate privileges.  (Even cautious people can get trapped:  A new version of an app may ask for additional privileges.  Android will ask you to approve the change, but it's easy to miss the implications.)  Unfortunately, this is a binary decision:  You can't say "install this app but don't give it access to the location information it asked for".
On iOS, certain API's are controllable - e.g., whether an app can gain access to location information.  iOS handles this in an interesting way:  It asks on the applications first attempt to use the privilege, and then for some privileges asks for confirmation - once - after you've been using the app for a while.  For example, it'll pop up a query saying "App such-and-so has been using your location while in the background for the last week, would you like it to continue?"  The basic idea is that people have trouble answer such questions in the abstract, but when tied to when they are actually using the application, and it's trying to do something for them, they may be able to give a reasonable answer.
The limitation is that this kind of model can't scale.  It works fine for a handful of easy-to-understand privileges, but if there were a dozen, you'd be annoyed by questions all the time - and probably many of those privileges would no longer be easy to understand, even in context.
Why?  If the system works, you just block the escalation.  I'm not sure why anyone would care about this kind of reputation.  If the app works fine but is consistently doing something that the sandbox consistently blocks - so what?
                                                        -- Jerry

@_date: 2015-02-17 17:21:54
@_author: Jerry Leichter 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
Snowden may be wrong.  He may have been deliberately lead astray.  He may be a plant.
It's not that I think any of these is true, but presumably Snowden believes *the encryption he was given to use while at NSA* works.  But that's not the encryption any of us use, so it doesn't help.  His statement that encryption, *in general*, works is odd because some of the stuff he released implies that there may be breaks.  It's all ambiguous - you could interpret the same material to say that the encryption itself is fine but the eavesdroppers have all kinds of way to get hold of keys.  But in the end ... does it really matter?  If we knew that AES absolutely, positively could not be broken by NSA - but we had *no idea* what key distribution or generation methods had been broken - we would be absolutely nowhere.
But they also use simple XOR (see the slide set).  This is a situation where unbreakability isn't necessary.  They're more interested in hiding what they are doing - once someone *notices* an exfiltration or command stream, it doesn't matter all that much whether they can read exactly what it says; the game is up.  Even fairly weak encryption would be adequate to prevent simple recognition of such streams based on content.
The FBI complains all the time.  The NSA isn't about to share their advanced techniques with the FBI; they don't want them exposed, which, "parallel construction" or not, will inevitably leak out if used in support of criminal cases.
And I still don't have an strong belief that the NSA can break AES or any other particular magic thing.  But my belief that they *can't* is much weaker today than it was the day before yesterday.
Unfair to whom?  Besides, they've as much as admitted (by the actions they've taken since) that they've come to see their pre-Snowden precautions as insufficient.
Depends on what you count, I suppose.  The military services have large numbers of people doing "cyber warfare", most of whom are doing defense.  Their jobs are not all that different from those of security managers (not developers) at commercial organizations - though since this is the military, the procedures are much more standardized, as are the systems being managed, and there are many more people there on a day by day basis.
If you mean people doing security development - defense or attack; or even those training to do attack with tools the bigwigs develop for them ... it's anyone's guess.  My gut says closer to 10,000 than 100,000, and that would be including a large number of people with basic training on how to attack low-level, ill-defended targets.
Remember how the Navy Seals who took out OBL grabbed all the computers and USB sticks and such?  They clearly had training on what to look for and how to seize it in a way that didn't destroy the data.  And there are undoubtedly rooms full of people who work on extracting data from such seized devices.  They are part of the "cyber warfare" community, by any reasonable definition of the term.
                                                        -- Jerry

@_date: 2015-02-17 17:38:05
@_author: Jerry Leichter 
@_subject: [Cryptography] What do we mean by ... ??? 
A tangent, and just a matter of satisfying my curiosity:  Can Achmed forge a session from me *to himself*?  It sounds odd, but if he can, he can create a fake order apparently from me and insist I pay for it.  Sure, I can add a separate signature to every order - but if it could someone come out of this protocol, so much the better.
                                                        -- Jerry

@_date: 2015-02-17 18:17:03
@_author: Jerry Leichter 
@_subject: [Cryptography] Passwords: Perfect, except for being Flawed 
I basically agree.  The attacks we see on passwords are all based on a combination of bad and weakly protected implementations (which store password-equivalents on the server - hashing was intended to keep what the server stores from being equivalent to a password, but bad security that allows those stores to leak, combined with ever-faster offline password testing systems, have killed that idea), and on bad user practices (reuse of passwords).  There are technical fixes to the former (SRP et al).  The latter become somewhat less of an issue if the former is resolved, but beyond stopping the dumb practice of telling people to *never* write down their passwords, requires some more work to help on the human side.
I'm not sure that all the alternatives are worse.  We're starting to see alternatives that are actually workable in at least some situations.  The first is two-factor systems - enabled by the near-universal presence of phones that can receive texts.  (Keyfob solutions are workable in specialized situations, but are unlikely to be really broadly acceptable.)
The second has its beginnings in Apple Pay.  For years, we rejected biometrics with all kinds of snide remarks about not being able to change your fingerprints if they were stolen, the ease of faking fingerprints, the fact that no matter where you got the biometric data, once it was on the net, it was just bits that could be copied.  But we missed the fundamental insight:  If you combine a fingerprint - something you *are* - with a phone that has local intelligence and secure hardware that can store some secrets for you - something you *have* - you can construct an inherently two-factor system that can be quite secure.  Using "tokenization" (a horrible term) means that nothing password- or biometric-equivalent ever leaves the phone.  The intelligence in the phone, combined with the fact that it's *yours* - unlike a fob that belongs to one bank - means that supporting multiple ID's for multiple mutually-suspicious organizations is straightforward.
The password is great because you don't have to bring anything along with you to use it.  Most proposals to replace it foundered on the problem of getting people to carry some additional piece of hardware - other than biometrics, which if not done locally, on your own device, founders on other issues.  If everyone can be assumed to have a smartphone ... the whole world changes.  (And, yes, for the solution to be Apple Pay, it has to be not just a smartphone but an iPhone.  Unless Apple decides to allow other phone or gadget makers to use Apple Pay, something else will have to be developed.  But the *basic idea* can work.)
                                                        -- Jerry

@_date: 2015-02-20 06:19:55
@_author: Jerry Leichter 
@_subject: [Cryptography] trojans in the firmware 
Good luck with that.  SSD performance and even proper operation is still somewhat of a black art; much of the value of the device comes from the proprietary algorithms that control it, which are build knowing details of the design.  Samsung, like other SSD makers, has every reason to keep that stuff secret.  The market advantage of increments in speed and other features is significant; the market to people who want to program it themselves is essentially non-existent.
It depends on the implementation and what kind of attacker you're considering.  There have been implementations in the past which use simply match a password stored in the device - encrypted with AES so that the advertising claims aren't outright lies - against a password entered at boot; the data itself was left unencrypted.  But there's plenty of power in a device like this to essentially build FDE right into the SSD.  That's probably proof against any attack against a stolen/seized SSD.  (Of course, Samsung may have deliberately, or through incompetence, provided a back door - we'd never know.  But most attackers wouldn't know either.  I'm sure North Korea would *assume* that the South Korean intelligence services have access, whether it's true or not.)
Low-enough level attacks against the boot sequence could intercept and leak the password.  The OS typically would come in way too late to see the password - but of course if you take it over, you have full access to the device.
In summary:  Assuming a decent implementation and no back doors available to the attackers of interest to you, this has exactly the strengths and weaknesses of FDE, with no overhead in the host.  Not really security theatre, but given modern hardware, perhaps not much of an advantage either.  You could go for defense in depth by using FDE on top of what the device provides.
                                                        -- Jerry

@_date: 2015-02-22 10:02:33
@_author: Jerry Leichter 
@_subject: [Cryptography] Lenovo laptops with preloaded adware and an evil 
I agree with the earlier comments in this message about past mistakes, but:
And just what is your proposed alternative for a way to set up a shared, mutually authenticated connection between a phone and a network?  At some point, there have to be secrets.  You can use symmetric cryptography and have a single secret, or asymmetric and have a pair of them, but in either case, someone has to make those secrets available to the appropriate parties - and that someone is the SIM card programmer.  If the SIM card programmer is fully penetrated, as is the case here ... just what is it that you propose to do?  The only alternative I know of is pure DH - secure but inherently unauthenticated.  Even so, probably beyond the capability of most phones in the world even today - we in the rich white west have a rather special idea of what makes for a minimally acceptable phone.
None of the above.  Forward secrecy was first proposed in 1992, but it didn't really get much interest until 2000 or so - and it would likely have been too compute-intensive for even desk-top class machines (much less phones) until years later.  SIM cards, on the other hand, go back much further than you might expect:  The first one dates back to 1991!  Even the "modern" min-SIM dates to 1996.  These are dates of introduction; given that these are international standards, design must go back at least a year earlier, probably more.
So we're not really talking about dumb design decisions here.  We're talking about the inevitable delay and inertia in changing literally billions of end-points, all over the world, under the administrative responsibility of hundreds of telco's.  The lesson *I* would take from this is given what we now know about the nature and scale of attacks on this massive infrastructure, the mistake is to think it can possibly be secure.  View it like the raw Internet:  Any security will have to be built on top.
The other lesson, which needs to be repeated over and over to those not versed in the whole crypto thought process is:  This is why the notion of a "golden key" accessible only to law enforcement is just nonsense.  If there's a central database of keying information, someone will get at it.  The techniques used to steal this information, from what we know, were nowhere near the level of sophistication required to, say, implant root kits in disk firmware.  Phishing, breaking into individual systems, monitoring what they're doing, learning procedures - the guys who stole hundreds of millions from many of the world's banks could have done the same thing.  So could much small criminal organizations.  And if it makes you happy that it's *our guys* who did it - this is also well within the reach of many other intelligence organizations.  Since passive listening once you've stolen all the keys is undetectable, any number of parties could be doing it at the same time.  For all you know, your conversations are being tapped by GCHQ/NSA, the Russians, the Chinese, the Germans, the Israelis, the North and South Koreans, and who knows who else, all at the same time.
                                                        -- Jerry

@_date: 2015-02-23 06:51:08
@_author: Jerry Leichter 
@_subject: [Cryptography] A better random number generator... 
You can also read what the author actually say on this exact subject (
"I know that if I were trying to predict a random number generator, I'd want something easier than the PCG family. But if I wanted actual crypographic security for secure communication, I'd probably want to use something that has been around longer and seen more scrutiny.
Hopefully as time passes, the PCG generation scheme will receive scrutiny from people with far more expertise in crypographic security than me, and we will have a clearer picture about how easily it can be predicted. With that in mind, I hope to offer some crypographic secuity challenges in the future to encourage people to try to break it."
This is preceded by a discussion of how various other generators are broken, and some argument about why the known techniques may not extend to PCG's.
                                                        -- Jerry

@_date: 2015-02-23 07:01:02
@_author: Jerry Leichter 
@_subject: [Cryptography] [cryptography] PGP word list 
A as in aisle, b as in bee, c as in ctenophora, d as in Djibouti....
The original joke started with "I spell my name with p, as in pneumonia".  I spent some time a while back trying to construct an entire alphabet.  Some letters - like "b" - are surprisingly difficult.
A puzzle - which I can't give in email for obvious reasons - involves pronouncing a particular 8 letter word that has fallen out of use but was the common name of a disease in the 19th century and asking someone to spell it.  Unless they know it, it will take them multiple guesses to find *any* of the letters.  (If there's interest I can try to find a spoken version of the word and put it up somewhere for people to try.)
There's also a shuffling of the alphabet for the classic children's song of the alphabet which, in place of "now I know my a b c's" ends with "now I know by b d v's".
                                                        -- Jerry

@_date: 2015-02-24 20:53:21
@_author: Jerry Leichter 
@_subject: [Cryptography] trojans in the firmware 
But in fact you can't design or manufacture *everything*.  Do you need control of your chips all the way back to mining the sand?
I'm pushing this beyond the logical limits, but there is a deep, underlying question:  Just what *do* you have to control?  Is there a theory we can apply here?  There seem to be some "obvious" lines.  If I build a computer out of (logical) MSI-level chips - simple logic gates JK-flipflops - it seems that even someone who could arbitrarily modify their behavior would have a tough time breaking my security.  Disk drives today, with tons of microcode embedded in the on-board controller, are clearly vulnerable.  How about a 1970's era drive, which left the CPU to handle all the complicated stuff?
But ... is that even true?  The attacker is not constrained to operate at the same level of abstraction as I am.  What I think is a JK-flipflop might be transmitting every set/reset to an attacker.  Can he do something with that?
Somehow, we're looking for something analogous to the theory of oblivious computation.  That's usually set up in a fairly high-powered framework - you assume a CPU similar to what we are used to today - say, an AWS instance - then require that the CPU instructions executed are not dependent on the secret data.  Is that enough?  Is there a meaningful way for memory to be oblivious?  There's a sense in which the answer is easy:  Storage of encrypted data, without the key.  But is that enough, as a component, to build a useful system?
The original problem in practical computing was how to build a *reliable* system out of *unreliable* components.  The modern, emerging problem is:  Can we build a secure system out of actively insecure components?
More questions than answers here.
Some interesting speculations about hiding stuff at ever lower levels of the abstraction hierarchy - and a great read - in David Brin's book Existence.
                                                        -- Jerry

@_date: 2015-02-25 23:57:52
@_author: Jerry Leichter 
@_subject: [Cryptography] information, Shannon, and quantum mechanics 
Of course, "in the limit" tends to be "in the limit of arbitrary slowness" (adiabatic systems, where you let things get to equilibrium).
There are all kinds of interesting physical limits on computation found in the last couple of decades, and their interplay with computational theory is still not properly appreciated.  For example, consider the whole notion of "computable in principle".  There are problems - like the Halting Problem - that are not computable "in principle" by any (Turing) machine (which, by a hypothesis that we now accept as given without much thought - though it was the center of much debate 80 years or so ago - is equivalent to "by any meaningfully defined computation:).
At the other extreme are poly-time algorithms, which we for most purposes consider "easily" computable, though of course the constants and the exponent may be very large.
But there are problems that are computable "in (mathematical) principle" that *could not have been computed* if the entire universe were working on just that problem since the Big Bang.  Or that the entire universe could not compute in 100 years.  Or any similar bound.  These are computable "in principle" but not "in this universe".  Do we really still want to accept the "in principle" label for such things?
A while back on this list, I pointed out that if you wanted to break a K-bit key by brute force in 100 years, at the least, you needed to perform 2^K bit flips just to generate the possibilities.  "In 100 years" means your computation has to take place in a hypersphere in space-time of radius 100 light years, by 100 years time.  (This is a gross over-estimate:  If you want the answer to come back to *you*, the spatial size is limited to 50 light years.  But let's be generous.)  I had done a back-of-the-envelope estimate and came up with a QM limit for K of somewhere between 128 and 256.  I later asked a physicist I knew, and he did a more accurate estimate.  It turns out that this hypersphere could do about 2^315 bit flips - and store about 2^315 bits of information.  So AES-256 "can be broken in principle" in 100 years by brute force - but you don't need to make the keys all that much larger to eliminate that possibility.  :-)
While our ability to realize anything approaching the ultimate computer is nowhere in sight, we are already talking about real problems - like brute-force attacks on keys - that come close to those ultimate limits.  "In principle" computation should already be treated with respect....
                                                        -- Jerry

@_date: 2015-02-26 11:40:04
@_author: Jerry Leichter 
@_subject: [Cryptography] My ignorance and drive firmware hacking 
I'd heard - I don't recall the source, and I don't know if it's true - that modern disks and controllers are "bonded at the hip":  There is factory equipment that determines a variety of parameters relevant to a particular physical disk and records these on the controller board.  The firmware uses these parameters to correctly access the disk.
A controller without the right set of attributes will have trouble operating a disk properly.  Unless you have access to the equivalent of the factory equipment, there is no practical way to replace a controller board (and even then, existing data on the disk may be difficult to read).
Again, I don't know if this is true - but it's consistent with a general trend in manufacturing:  Rather than keep pushing the limits and bring variances in manufactured parts ever downward, accept that individual units vary and compensate in software.  (Note that biological systems have evolved to do exactly this:  Your eyes, for example, present highly distorted and idiosyncratic information about the visual field in front of them to your brain, which corrects for the distortions to produce a useful image.  Look at the world through a set of prisms that shift everything to the right, and when you reach for something in front of you, you'll actually aim to its left.  But after a couple of minutes, your difficulty will disappear - until later, when you take the prisms off and find yourself aiming too far to the right!)
                                                        -- Jerry

@_date: 2015-02-27 10:59:47
@_author: Jerry Leichter 
@_subject: [Cryptography] information, Shannon, and quantum mechanics 
I've asked myself the same question.
The guy I spoke to about this works in this field, and I told him why I was asking the question.  So if Grover's Algorithm would affect the answer, I suspect he would have told me.  Then again, he just may not have thought about it in a general context while answering a particular question.
The next time I have the chance, I'll ask for clarification.  My guess at the moment is that Grover's Algorithm doesn't actually help you here, but it's just that, a guess.   contains a nice discussion of Grover's Algorithm, including some discussion of cryptographic implementations.  I've skimmed it, but it's not the kind of thing that can be understood without a close reading.  I don't know whether it will answer the question at hand.  One thing it makes clear is that Grover's Algorithm is only useful if you can convert your classical query into a quantum oracle "efficiently enough".  If you can't evaluating that query will end up dominating the cost of the overall computation, and you'll lose your apparent speedup.  It's not clear - it may not be known - if you can construct a classical encryption function that cannot be translated into an efficient quantum oracle.
Note that the underlying point - that there's an "in principle" limitation that is based on limits on computation in the physical universe, and that we are actually approaching those limits in some cases - doesn't change, even if the numbers get larger.
                                                        -- Jerry

@_date: 2015-02-27 18:07:47
@_author: Jerry Leichter 
@_subject: [Cryptography] The Crypto Bone's Threat Model 
FYI, full-size SD cards have a write-protect capability.  Most have a little slider on the upper left side (when you are looking at the non-contact side of the card with the cut off corner at upper right).  If the slider is down, leaving a gap, or (for cards intended never to be written again) if there is a cut-out gap in the same position, the card is (supposed to be) write-locked.  (This is essentially identical to the way 5 1/4" floppies worked.)
Note the "supposed to be".  Unfortunately, support for write locking is an optional part of the SD standard.  Devices are free to ignore the lock.  If you want to rely on this feature, you need to choose your SD card reader carefully.  (I have no idea how widespread support for the feature actually is.)
The smaller SD card sizes (mini and micro) completely dropped support for the "write protect notch".  I guess the feeling was that no one cared enough to justify trying to fit this into the much smaller physical form factors.
All sizes support host commands to enable permanent write-locking, and to enable and disable temporary write-locking.  I don't think I've ever come across user-level software that could access these commands, and I don't even know how widely (or correctly) they are implemented.
Nevertheless, a signature checking is a useful thing to have.  It means that even if someone has temporary physical access to the device, they can't update it without also knowing the key.
I'd suggest the following:
1.  The modifiable portion of the device contains a signature key based on a published algorithm.
2.  To change the key, not only do you have to enable the mechanical interlock, but you must provide both the old key and the new key.  (Or perhaps sign the update with the old key.  It would seem these are equivalent, though I'd want to think about it some more.)
3.  All devices are shipped with the same published, initial key.
4.  If you lose the key, there's no way to ever update the device again - though it will continue to operate in its current configuration indefinitely.  (Allowing a "reset to factory state" would allow an attacker to load arbitrary code, which we obviously want to prevent.  Note that you can deliberately turn the device into a ROM by changing the key to a new randomly-chosen one and promptly discarding the new key.)
                                                        -- Jerry

@_date: 2015-02-28 17:47:26
@_author: Jerry Leichter 
@_subject: [Cryptography] DIME // Pending Questions // Seeking Your Input 
There are notions of normalization to remove non-semantically meaningful information.  These are highly language-dependent, and only someone who really knows the language should attempt to define such things.  Even Western languages have traps for the unwary who try to generalize from their own native language - e.g., in German upper case SS (two characters) corresponds to lower case ... hmm, can't figure out how to generate the single character, it looks similar to a beta.
I have no idea what would be appropriate for Chinese or Arabic.  If I remember right, Arabic has no case distinction, but some letters come in three variants, one used at the beginning of words, one in the middle, one at the end.  But I don't know if there would be any reason to "normalize" these to one of the three forms in name matching.  Perhaps there *is* no universal normalization for some languages; in that case, you have to leave the name alone.  (There are additional levels of complexity when a character set is used by more than one language, and the languages have different rules.  This occurs for sorting of names in the related but distinct Nordic languages.)
I'm sure all of this has been beaten to death in the internationalization community.  A mail system standard shouldn't attempt to repeat the work - it should simply defer to existing standards on how to handle names in different languages and cultures.
                                                        -- Jerry

@_date: 2015-01-01 16:02:34
@_author: Jerry Leichter 
@_subject: [Cryptography] on brute forcing 3DES to attack SIMs 
The article goes on to state:  "Deploying standard processing power, like the Intel CPU (Core i7-2600k), would take roughly five years to break DES and more than 20 years to break 3DES."  Which is where my bullshit detector went off full blast.  Four times as long to break 3DES as DES?  We're talking simply brute force here; that's nonsense.  3DES has a key space 2^56 bits larger than DES, not 2^2.
Let's go back to the other claim.  They are claim about 3*10^8 encryptions/sec, or 3*24*60*60*3*10^8 ~= 4*10^6*3*10^5 ~= 10^12 encryptions in 3 days.  Using the standard approximation that 2^10 ~= 10^3, we get about 2^40 encryptions in three days.  That's about enough to break the old export 40-bit version of DES.  It's nowhere near a practical attack against full DES, much less 3DES.
Now, there's another reading of this:  Since they are European, the "." above may have been intended as a digits separator - i.e., it's 245760 Mcrypt/sec.  That gains a factor of about 10^3, or about 2^10, bring then to 50 bits of key. Using the complement property of DES, it actually has an effective key length of 55 bits, and we're talking expect time which means looking at half the keyspace, which they could do in 48 days or so.  If the speeds are for a single one of those 8 boards, they're back down to 6 days.  But that's still DES; 3DES remains way out of reach.
                                                        -- Jerry

@_date: 2015-01-03 19:02:20
@_author: Jerry Leichter 
@_subject: [Cryptography] 
=?windows-1252?q?rything=3F?=
There are three widely-known protocols to provided authenticated/encrypted connections:
1.  SSL/TLS.
2.  SSH.
3.  IPSEC.
It's interesting to compare the three.  In terms of programming interface, IPSEC is what you would design if you had a blank piece of paper and the goal of allowing any network-enabled program to work completely transparently.  This is how we generally add new functionality while retaining old code.
SSL/TLS is at the far extreme:  It presents a network programming model entirely unlike the TCP stream-oriented model.  That doesn't make it bad, it just means you can't take, say, FTP and layer it over SSL/TLS without significant rework.  (Again, the issue isn't whether that's a good idea.  If you don't like FTP, pick something else.)  No surprise here - SSL/TLS didn't start off with the goal of supporting "secure connections" in the abstract, it started off with the goal of providing secure HTTP connections.  On the other hand, there are libraries for SSL/TLS that you can build against - though they've traditionally been very complex and difficult to use.
SSH also started off with a specific goal:  Providing remote terminal connections.  But the semantics of remote terminal connections is pretty close to TCP streams, so allowing forwarding of arbitrary connections was much easier than with SSL/TLS.  But for whatever reason, SSH has continued to exist as a standalone program rather than a library, which makes configuration more complex for the unsophisticated user.
IPSEC, of course, suffered from various design-by-standards-committee problems, leading to huge complexity underneath what could have been a very simple interface.  Early VPN's relied on IPSEC, but ran into all kinds of issues (like early WiFi routers that didn't recognize IPSEC connections as TCP.)  Today pretty much all VPN's you use are SSL/TLS based.  In this indirect way, SSL/TLS is finally supporting programs using the TCP socket bindings - by mapping the IP layer, which is packet oriented, to the SSL/TLS record orientation.
Of course, going to VPN software means that you're using SSL/TLS in a separate program - just like SSH, but much less configurable.  It's interesting that no one, as far as I know, has taken the reasonably standardized SSL/TLS VPN bindings and turned them into a library that forwards connections only for the program it's linked to.  Meanwhile, in the grand convergence, SSH is being used to implement VPN's.  But, it, too, could benefit from being turned into a library.
Or ... why not take either SSL/TLS or SSH and build them into system libraries, so that you need to nothing other than, say, change the format of a host name to get SSL/TLS or SSH protection for the socket you create?  (The socket API was, unfortunately, not designed with anything like this in mind - to much low-level crud is exposed to users.  But some of the changes made to support IPv6 raised the level a bit.)
From the point of view of the actual connection, whether it's SSL/TLS or SSH doesn't much matter.  The two do have very different ways of managing the credentials needed - each with advantages and disadvantages.  SSL/TLS is slowly picking up support for SSH-like "connection continuity" ideas, though as essentially additional steps on top of its existing base; SSH hasn't, as far as I know, moved toward SSL-like ideas.  I'd say neither approach is complete - we need a synthesis of the good ideas from both, and from elsewhere (OpenAuth and such).
Encryption and authentication have to "just be there" using standard techniques, not something you need to put in using a special, complicated procedure.  This was definitely the idea in IPSEC - and, BTW, would have also been consistent with the design approach of the ISO network protocols.  Two good germs of ideas completely spoiled by the standards process.
(Me, I miss DECnet in VMS:  Network support was built into the file syntax and the file system.  The file syntax included a place for a username and password.  Authentication was done by the OS when receiving the connection attempt, not re-invented by every program.  This was way too early to support encryption, so would have needed years of development for the modern era - but a much better base than what we ended up with.  Plan 9 ran with similar ideas in a Unix-like context; there were probably others.)
                                                        -- Jerry

@_date: 2015-01-09 17:06:21
@_author: Jerry Leichter 
@_subject: [Cryptography] Compression before encryption? 
Many others have made this comment.  But it's by no means the whole story.  Encrypted voice channels have been successfully attacked by looking at the lengths of encrypted packets.  Certain patterns of lengths are associated with particular sequences of phonemes.  This lets you read off the likely sound sequences even if you can't recover the details of sounds actually being transmitted.
The real problem here is that the underlying voice encoders are, in a sense, too good.  They are designed to make use of a little channel capacity as possible, so have short encodings for common combinations of sounds, which they can then transmit much faster than the time required to actually utter those sounds.  So there are gaps in the transmitted bitstream, which turn into gaps in the encrypted bitstream, which allows the lengths to be readily determined - and those are enough.
This is not a known plaintext attack, but it's part of the broad class of probable plaintext attacks - a very successful member of that class.  But it's also essentially a side-channel attack:  If you use the traditional setting for describing an encryption setting, where a vector of plaintext bits is mapped to a vector of cyphertext bits, there's no place to represent the timing information - so the system can look completely secure.
Note that there are attacks of a similar sort against interactive login sessions and such.  They tend not to have quite as much information available, and are not quite as successful as the voice attacks.
One way to look at the lesson here is that the mathematics makes sense if the only information available to the attacker is the encrypted stream of bits, and any departure from that - when the departure is correlated to the plaintext - provides a potential attack.  For voice, what you probably want to do is send data at a fixed rate, regardless of the sound being encrypted.  A simple A/D converter, with no fancy speech processing, gives you that.  The fancier your vocoder, the more you'll end up leaking - unless you backfill with random bits until you can get an essentially constant rate.
Applying these arguments in other situations is complicated.  A single, simple bulk transfer of data might not be affected by compression - but if you imagine a situation where I transfer a file of the same underlying length each day, but compress before encryption, I'm leaking some semantic information, because the length of my transmission says something about the compressibility of the plaintext.
                                                        -- Jerry

@_date: 2015-01-12 06:40:00
@_author: Jerry Leichter 
@_subject: [Cryptography] Compression before encryption? 
Sigh.  I addressed this earlier.
An ideal cryptosystem leaks no semantic information about the contents of the ciphertext:  An attacker's a postiori estimates of the probability that any particular message was sent is identical to his a priori estimates.  This is the basic goal we aim for in our encryption primitives.
Now, this goal is rarely attained for the entire system.  Unless you continuously send cover traffic, the length of your cryptotext reveals the length of your plaintext.  In a real-time protocol, the timing with which you send encrypted packets reveals the timing with which plaintext packets were sent.  There's classic spy-story attacks in which an attacker watches the actions of multiple individuals and correlates them with the timing of messages sent, thus learning who is sending the packets.
And that correlation between real-world semantic information and encrypted data sent is exactly the point.  Without compression, the length of the ciphertext correlates with the length of the plaintext.  *With* compression, the correlation shifts to something much more subtle:  To the contents of the plaintext, as it's related to the degree it can be compressed.  Whether this matters depends on the overall usage of the combined encryption/compression system.  Sure, in many cases you'd say "who cares".  *But that misses the point*.  The goal of modern cryptography for years has been to provide a system in which the end user *doesn't need to do very subtle analyses of exactly how they use the system.*  He just takes his data and sends it.  But it's never that simple.
Imagine you're a company that wants to keep your actual sales volumes highly confidential.  You invest in the highest level security connections between your offices and between your offices and you suppliers and customers.  Knowing that you "chatter" - the number and length of messages sent and received - reveals how much business you're doing, you send cover traffic at all times, keeping the actual transmitted data rates essentially constant.
And then someone introduces a remote backup system.  Everything remains encrypted, but it goes over lower-grade commercial lines - no cover traffic.  As long as you use a dumb backup system that backs up your entire disk every day, very little information leaks.  But then someone gets clever and transmits just the (encrypted) deltas.  Suddenly you're leaking something highly correlated with the business done in the previous day.
                                                        -- Jerry

@_date: 2015-01-14 12:35:59
@_author: Jerry Leichter 
@_subject: [Cryptography] Summary: compression before encryption 
That's because you aren't actually compressing the certs - you're compressing deltas on pre-communicated sets of certs - and you can make assumptions about the nature of the deltas.  Very different problem.  The technique is *very* nice, but it amounts to recognizing and working with the exact kind of non-randomness inherent in the data you're compressing.
                                                        -- Jerry

@_date: 2015-01-14 17:38:15
@_author: Jerry Leichter 
@_subject: [Cryptography] Summary: compression before encryption 
Many years ago, Martin Minow got a an article published in a cryptography journal proposing auto-key cryptography.  It was all carefully written in mathematical language to cover up what it was up to, but it amounted to using the message itself as the key for a stream cipher.  This had all kinds of nice properties - e.g., it introduced *exactly* as much randomness as was needed to balance the information content of the message.  Martin inserted an off-hand comment that, with most cryptosystems, you got better results by compressing before encryption, but that his experiments had shown that with this system, compression after encryption did extremely well.
A number of people, of course, wrote in to point out that "didn't he realize that he was just XOR'ing the message with itself so that the result was always all 0's."
Others responded, pointing to the (April 1) date of the issue....
                                                        -- Jerry :-)

@_date: 2015-01-15 20:01:00
@_author: Jerry Leichter 
@_subject: [Cryptography] $10 USB charger steals MS keyboard strokes 
David Cameron doesn't need one of these.  Per his recent proposals, all cryptography that governments can't break is to be outlawed anyway.
                                                        -- Jerry

@_date: 2015-01-16 13:56:33
@_author: Jerry Leichter 
@_subject: [Cryptography] Summary: compression before encryption 
Cribs are an old, obsolete concept.  All modern cryptosystems are required to be secure against known, even chosen, plaintext attacks.  We don't accept them otherwise.  Sending a one megabyte message which is entirely known to the attacker except for 1 bit at a particular position in the message is required to be safe, in the sense that the attacker's chance of guessing the bit is unchanged after he's received the message.
Attacks against encrypted compressed data are all side-channel attacks.  They rely on differences in message sizes that correlate with other semantically interesting information in the plaintext - e.g., that some data has been repeated; or that a voice signal isn't an arbitrary audio stream but consists of a small number of semantically meaningful phonemes which are compressed into messages of different lengths, so that reading off the lengths gives you information about the phonemes.
These are side-channel attacks because the underlying design principles either don't consider message lengths at all (they only look at cipher texts in isolation, or as a continuous, unbroken stream) or they say "yes, you can determine the length of the plaintext from the length of the cipher text, we accept that, it's not particularly useful - but that's all you can determine".  Which then fails when those lengths correlate, or can be caused to correlate, with something interesting.
In the usual models, compression before encryption neither adds nor subtracts anything.  We get the same semantic security guarantees either way.
Once you consider broader models in which messages lengths aren't a side channel but are explicitly part of what's presented to the attacker, compression pretty much always helps the attacker, exactly because it converts some kind of semantic information into message sizes.
                                                        -- Jerry

@_date: 2015-01-21 23:17:39
@_author: Jerry Leichter 
@_subject: [Cryptography] information gain; 
[represented as binary written out in ASCII as 0 or 1 digits]
While valid, this is besides the point.  A common strong definition of semantic security says:  A polynomially-bounded machine will receive as input either (a) k random bits; or (b) the encryption of k' (not necessarily exactly k) known (or chosen) bits with a randomly chosen, unknown key.  The machine must be able to gain no more than an epsilon advantage over a random guess of an answer to the question "What's the source of the following bits?".
There's an implicit assumption that all data is passed in binary.  Sure, if you define your encryption mechanism to output only the ASCII characters 0 and 1, it may be quite secure (and quite compressible), but it will *not* be secure under this definition:  There's a trivial test to determine, with very high probability, whether a string came from this odd encryptor or from a random bit source.  (An even simpler counter-example is to take the encryption and tack on 1000 0 bits - trivially compressible.  Or you could take on 1000 *random* bits, which would not be compressible, and would create a system that would pass the "compare to random" test exactly when the original did - but would add nothing to actual security.)
Do people play "fast and loose" with the term "random"?  Absolutely.  They also play "fast and loose" with other important terms, especially entropy.  If you work with informal definitions, formal results will likely turn out to be false - the first thing you need to get anywhere in formalization is precise definitions.  In fact, it's exactly the existence of counterexamples that "miss the point" that drives the tightening of the definitions.  The counterexamples you give are examples of the costs of fuzzy thinking on mathematical matters, not problems with the underlying exact definitions.
BTW, the one case where there is a legitimate, if small, issue is with a number-theory based cryptosystem like RSA.  Since an encrypted message is a value mod some product for two primes (for RSA; there are similar constraints on other systems), if you represent it in binary with enough bits to cover the possible values, you will necessarily have some bit patterns that don't represent anything.  You can come up with better representations that differ from pure random binary by less, but you can't make the problem go away completely.  A correct definition of semantic security for such a system can readily be set up, and if you want to investigate the semantic security of such a system, you'll need to go through the exercise.
                                                        -- Jerry

@_date: 2015-01-22 07:09:47
@_author: Jerry Leichter 
@_subject: [Cryptography] actual NSA protocol docs to mine... 
This is an odd document:
- RSA-128 (what we usually write as a 1024 bit key) for public key crypto;
- RC6-16 as the symmetric encryption function;
- SHA1 for hashing.
So no NSA-specific crypto at all.  An attempt at deniability, since the code would be "published" in attacked systems?
The dates shown on the page are:
- "Dated 24 Feb 98"
- "Last Saved 4/7/2013"
- "Last Printed 6/15/2009"
A 1024-bit RSA key would have been reasonable if somewhat aggressive in 1998.  (In 2003, published numbers claimed it roughly equivalent to an 80-bit symmetric key.)
RC6 was first published (as an AES candidate) in 1998 - I haven't been able to track down the actual date, though NIST's announcement of the set of candidates was in late August.  If you want to use "public" stuff, RC6 would be an odd choice 6 months before broad publication!  At the time, there would also have been little analysis of RC6.  What if it turned out to be a dud?  Even the best cryptographers slip up sometimes.  Why not use RC5, which had been around and well thought of for a couple of years?  (One speculation is that NSA thought that RC6 would win the AES competition....)
SHA1 had been around for a couple of years and so was reasonable in 1998.
I don't know what to make of a document that was last saved four years after it was last printed.
                                                        -- Jerry

@_date: 2015-01-22 07:40:42
@_author: Jerry Leichter 
@_subject: [Cryptography] Summary: compression before encryption 
Back in the old days, we generally encrypted letters (and maybe digits) to letters (and maybe digits).  Even then, "recognizing the plaintext as plaintext" was possible without a crib if you assumed the plaintext was a human language.
Today, we encrypt bytes to bytes and if, again, you're assuming a human language - often the case - even a couple of decrypted bytes usually tell you that you've got it wrong.  Compression might help here, but only if the compression isn't known.  Otherwise, instead of "try decryption and see if result is reasonable" is simply replaced by "try decryption, decompress, see if result is reasonable".
In any case, this is not what was meant by a "crib".
That's not what the claim about one bit is about.  If you want semantic security, you can't use a mode like CBC (which doesn't come closet to providing it).  Semantic security requires that you be able to securely transmit a single bit, even in the face of a chosen-plaintext attack.  That immediately implies that encryption with a particular key *cannot be deterministic*.  Otherwise, I as attacker ask you to give me the encryptions of 0 and 1, and later when I want to know if a give ciphertext encrypts 0 or 1 - I just compare the ciphertext sent to my two samples.  To protect against this, the encryption mode must map a given plaintext to a large number of possible ciphertexts - so many that (within the security parameters) an attacker will never see the same particular encryption twice.
I'll certainly grant you that, while semantically secure modes do exist, they are not typically used in practice (because the ciphertext is necessarily larger than the plaintext, which can be inconvenient, for one thing).  Note that random padding provides some degree of semantic security - though if not done carefully, it can create other vulnerabilities.
                                                        -- Jerry

@_date: 2015-01-26 12:29:44
@_author: Jerry Leichter 
@_subject: [Cryptography] Android's Secure ADB as a security hole 
I'm missing something here.  There's some code that will take a private key and sign things with it.  This is hazardous - how?  If you have access to the private key, there are many ways to use it to sign things.  Why is this one special?
                                                        -- Jerry

@_date: 2015-01-26 13:05:35
@_author: Jerry Leichter 
@_subject: [Cryptography] The Crypto Pi 
That's actually not true.  If I can capture the state at time N, and then withdraw bits slowly - and you feed them back in slowly - then the system may never recover.  I wait for you to feed in 1 bit, then withdraw 1 bit. Half the time, on average, the single bit I got back tells me what bit was added.  If it doesn't, I draw another bit and play the same guessing game for what must have been added.  I quickly get to the point where there's only one possible set of inputs that would produce the output I saw - thus advancing my knowledge of the state.
That's why you should add "entropy" in large chunks, not dribble it in slowly.  (Or decide that this attack is uninteresting for any of a variety of perhaps-plausible reasons.)
While there is a real notion of entropy (several, in fact) and entropy is measured in bits, trying to use naive arguments to *count* bits of entropy is very dangerous.
                                                        -- Jerry

@_date: 2015-01-26 13:57:28
@_author: Jerry Leichter 
@_subject: [Cryptography] random numbers on virtual machines? 
The only thing that actually works is to "pierce the virtualization" and give the VM access to true random number generation on the host (assuming *it* has a decent source of random numbers).  This is pretty universally available in VMM's these days, and modern guest OS's make use of the facilities provided.
No, getting this right isn't trivial.  And it introduces new potential attacks.
                                                        -- Jerry

@_date: 2015-01-27 07:56:47
@_author: Jerry Leichter 
@_subject: [Cryptography] random numbers on virtual machines? 
I think that first "Virtual machines" was supposed to be "Physical machines", or something of that sort.
This makes little sense.  You could make the exact same statement about distributing the random bits to different processes.
Looked at "from the bottom up", the generator on the host system - whether dedicated hardware or software that depends on whatever physical processes we've decided produce sufficient randomness - sees a certain demand for random bits coming from the entire software stack above it.  It has to supply suitably random, suitably uncorrelated bits to meet that demand.  Whether that demand, at some higher level of abstraction, comes from a single thread of execution in a single program; a bunch of threads in a single program; a bunch of processes each running a separate program; a bunch of containers each running a bunch of processes; or a bunch of virtual machines each running its own OS - makes absolutely no difference in the correctness conditions for the low-level generator.
Yes, as soon as you move from multiple threads in one program to multiple processes, you have users of that stream of random bits who don't trust each other, so you need to provide suitable levels of protection between them.  But that's hardly a new problem (if one that's subtle and difficult to solve).
                                                        -- Jerry

@_date: 2015-01-27 13:35:40
@_author: Jerry Leichter 
@_subject: [Cryptography] traffic analysis 
The problem is that every network out there is built on the basis of statistical assumptions about demand:  Not everyone will want to send stuff at the same time, and even peak demand is well below theoretical peak demand.  To go to the extreme, sure, Google Fiber offers 1Gb/second - but how many customers running all out will overload any possible backbone behind the single link from the house to the concentrator?
If everyone starts sending constant cover traffic, links will be quickly overloaded all over the place.  At which point the providers will start charging per bit (which is what caps really amount to - they're just quantized units of per bit traffic) and have a real case for doing so - and nobody will be happy.
There's room to do much better.  For one thing, you don't need to saturate your link with cover traffic - you need to send enough cover traffic so that a listener can't tell the difference between cover and real traffic.  If your cover traffic rate equals your average rate over some period of time, you're not adding more traffic - you're simply replacing some of your cover messages with real messages.  But ... what happens when you have a peak demand way above your average?  As Stealthmonger has commented concerning anonymity, if you want security against traffic analysis, you have to accept delays:  Set your cover traffic rate somewhat higher than your average rate, and you'll *eventually* catch up with peaks (though as with any queueing system, the delays can grow without bound - requiring unbounded memory *somewhere*).
Different network designs can also help.  If you own the link and both of its ends, it costs you exactly the same to send continuous random bits as to leave the line idle.  Any encrypted traffic (assuming an encryptor whose output is indistinguishable from random noise() is then safe from observation.  The Hot Line between the US and Moscow was, I believe, designed to work this way.  How this extends to a packet-switched network, especially one where you can't trust the switches, is unclear.
I'm not aware of any open research on these kinds of questions - though it may well be out there.  What's the optimum cover traffic rate under various assumptions about the real traffic rate and distribution?  When is it safe to use the traffic other users present as cover for your own?  Clearly if there's only one other user sending traffic, you can't use him for cover as *he* can tell which of the packets are yours.  But is there a way to mix traffic from multiple users in a way that requires large numbers of them to conspire to reveal anything?  The mixmaster stuff looks at this specifically from the point of view of a store-and-forward node - is there some suitable useful analogue on a single link?  Can we somehow get the same guarantees without storage inside the network?
Just as researchers, for many years, ignored denial of service attacks on the theory that "We can't do much about them, but who would bother to make such attacks anyway?", most modern work ignores traffic analysis as "impractical".  Well ... we had to change our attitudes about DoS, and we're now going to have to change our attitudes toward traffic analysis.
                                                        -- Jerry

@_date: 2015-01-27 16:33:18
@_author: Jerry Leichter 
@_subject: [Cryptography] traffic analysis 
Just my point.  We don't have a theory that would allow us to make the tradeoff in a rational way; we can only guess.
                                                        -- Jerry

@_date: 2015-01-27 19:19:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Facebook -> SteganographyBook 
There's tons of existing work on embedding invisible watermarks in images in ways that survive some fairly nasty conversions.  It would be really nice to apply work intended to support DRM to providing security....
                                                        -- Jerry

@_date: 2015-01-28 00:13:06
@_author: Jerry Leichter 
@_subject: [Cryptography] traffic analysis 
Hmm.  View spam traffic as a carrier signal.  Use a cryptographic spread spectrum modulation on top of it as your channel.  To get maximum bandwidth, you really want to get rid of all those spam filters.
Eventually of course we'll have to deal with spam-over-spam so the filters will come back.  Then perhaps we go meta and modulate the spam-over-spam.
                                                        -- Jerry :-)

@_date: 2015-01-29 07:01:16
@_author: Jerry Leichter 
@_subject: [Cryptography] traffic analysis -> let's write an RFC? 
No.  The idea was to examine possible approaches.  Someone, after all, *does* own the link; if that someone wishes to prevent traffic analysis, they have an easy approach available to them.
Using packet-switching on shared links has had enormous cost benefits - it's what made the Internet possible.  But as we're discovering, it's also had very unfortunate side-effects on security and privacy.  The issue now is how to mitigate the effects without losing the benefits.  Encryption at multiple layers in the stack is part of the solution, but is insufficient on its on to protect against traffic analysis.  We need new ideas here.
                                                        -- Jerry

@_date: 2015-01-29 15:29:43
@_author: Jerry Leichter 
@_subject: [Cryptography] traffic analysis -> let's write an RFC? 
Very nice analysis.
I wonder if one can re-purpose network coding techniques for this purpose? (Network coding is a technique designed - initially - to allow multi-casts to more efficiently use a network than appears naively possible.  The trick is that nodes in the network have more choices about what to do when they have multiple input packets that need to go over a single output link:  Rather than simply choose one packet at a time to send, they can send any linear (over Z/2) combination of the input packets.   has an introduction - Figure 1.1 gives you the basic idea.)
The interesting thing about network coding which makes it potentially applicable here is that it can be built by forwarding *random* linear combinations of the inputs; each node sends along a description of which inputs you chose to combine.  If instead the nodes shared a key and, in synchrony, generated those random combinations deterministically, they wouldn't have to forward them - but an attacker without the key wouldn't be able to decode what was being sent where.  (Maybe.  Much analysis needed - this is just a wild-assed suggestion.  Assuming this hasn't already been done - I don't follow the literature on this stuff - and it can be made to work, it's at least a CS Masters thesis, perhaps a PhD.  Anyone looking for a topic?)
                                                        -- Jerry

@_date: 2015-01-29 17:10:11
@_author: Jerry Leichter 
@_subject: [Cryptography] How the CIA Made Google 
Indeed.  The form of the argument is "all these people involved with DoD and NSA were involved with giving money to the project Sergey Brin was working on - obviously they were controlling Google".  Let's get real:  ARPA and other defense organizations were throwing money at pretty much everything that had to do with computers back in those days.  No surprise - the military and spy agencies saw how important this would be to them.  You can't find any American researcher - or any researcher who ever trained in the US or worked with an American co-worker - who you couldn't tar with similar accusations.  You'd have similar trouble finding any American (or They approached Jeffrey Ullman - whose name was on the grants, and whose student Brin was - and, I'm sure, spun all kinds of accusations.  He basically told them to fuck off - which of course is just further proof that he's in on the conspiracy.
And I love the pictures that show one person standing next to another somewhere at some time.  See - they obviously work together.
Well ... if you Want To Believe (cue X-Files music), Google works to keep the information available to itself *and its military/spy overlords* - it keeps *everyone else* out.                                                          -- Jerry

@_date: 2015-01-30 13:44:15
@_author: Jerry Leichter 
@_subject: [Cryptography] De-Anonymizing 
Speaking of which:  "'Anonymized' Credit Card Data Not So Anonymous, Study Shows"
Basically - who needs RFID.  Patterns of what you buy and how you buy it will generally make you easily identifiable in the stream of credit card charges.
Work done at MIT - I haven't dug up the paper.
                                                        -- Jerry

@_date: 2015-01-31 19:04:52
@_author: Jerry Leichter 
@_subject: [Cryptography] best practices considered bad term 
Yes and no.
The specific term "best practices", as far as I can tell, was used by SAP sales guys - back in SAP's heyday - as a way to get people to turn their business operations inside out to work with the way SAP designed its software.  SAP took the point of view that they wouldn't customize their software - customers had to adapt to their *right* way of doing things.  They're answer to the complaint from a customer who organized things differently was "Oh ... you mean you don't follow industry best practices?"
Whether they invented the term or picked up on something that was already around, I don't know.  It became, and remains, a way for consultants and sales guys of all stripes to try to force people to abandon their "legacy" approaches and move on to whatever the consultant or sales guy is trying to move them to (for an appropriate fee, of course).
So the *term* is poisoned.  But let's ignore the *term* and think about the underlying *concept*.  The fact of the matter is, the vast majority of people and companies who need to secure their systems and data are *not* security developers, are not going to *hire* security developers, and are going to only get themselves in trouble if they try to set off in some new direction.  They need something they can actually deploy that will give them some measure of security.  Much of the stuff - and advice - out there is useless or actually makes things *less* secure.  Some of it is good.  It's extremely difficult for most potential buyers to tell the difference.
A *good* description of "best practices" would actually help things.  It would certainly include such advice as "Keep systems patched", "Don't continue to use Windows XP", "Don't reuse passwords at multiple sites" (yes, you can make exceptions for very-low-value sites; I'm talking about general advice), "Don't leave default passwords on any devices", "Have backups at an offsite location to which your systems have effectively append-only access", "Have a procedure in place to quickly revoke all access to your systems by people who leave their jobs, for whatever reason", and many more.  The fact is, there are many basic security issues for most people or companies that *do* have straightforward, well-understood solutions.  No, these don't solve *all of* *everyones* problems - but if you don't have them in place, there's not much point in doing more sophisticated stuff.
From this point of view, there absolutely *are* best practices for security - and we should do a better job of defining them.  Yes, they are a "lower bound", and many will go no further.  But what's the alternative?  A "lower bound" of nothing  Or one defined by whatever sales guy last walked through the door?
                                                        -- Jerry

@_date: 2015-01-31 19:36:36
@_author: Jerry Leichter 
@_subject: [Cryptography] How the CIA Made Google 
Now *that's* setting a pretty low bar, based on everything we know about the security of Federal agencies.  As for "most" ... one would have to believe that of all Federal agencies, the one doing the best job at protecting its digital assets would be the NSA.  And yet ... Snowden and, most likely, at least one other leaker as well.
Having worked at Google at one time ... yes, they are *extremely* careful about maintaining the security of their data.  I can't talk about some of the things I know they were using at the time - and (a) I certainly didn't know about all of them; (b) given the revelations of the last couple of years, I'm sure they've added much more.
It is important to keep in mind that security implements a particular policy.  It's strong to the degree that it prevents violations of the policy.  You may not like the way Google targets ads based on what you say in gMail, say, but providing some access to the contents of gMail by the advertising systems is part of the policy.  Similarly, providing access to government agencies under certain conditions is also part of the policy - perhaps not by Google's choice, but because it's a constraint that they have to work under as a US (and EU, and various other jurisdictions) corporation.  Just because you would prefer that Google have some different policies doesn't make it a security issue when they implement theirs and violate the policies you would prefer.
What Google would consider a violation of its security policies concerning gMail, for example, is that those not specifically permitted access gain it.  Google was extremely upset by some Chinese hacking into its systems to get at gMail, and by NSA's listening in on inter-data-center links for the same purpose.  In both cases, they responded by strengthening the appropriate security layers - e.g., encrypting all those links.  Every security system has its failures.  What you need to look at is how frequent are they, and what's the response.
Google's certainly not the only company with this kind of attitude.  Apple seems to have "gotten religion" under Tim Cook.  (Security doesn't seem to be something that much interested Steve Jobs.  It's not that he was against it - he just focused on other things.)  Microsoft is harder to read.  On the one hand, they've put a huge effort into moving away from the insecure coding practices that plagued them - and every user of their software - for many years.  On the other, they arguably took Skype and "de-secured" it.  Facebook ... let's not go there.  Twitter seems to take things seriously.  Most smaller, rapidly-growing consumer Web companies - not so good, anything that would slow down mega-growth gets ignored.
Once you move beyond the consumer-facing companies, operations are typically very opaque to outsiders, so it's hard to comment.
                                                        -- Jerry

@_date: 2015-07-05 00:01:36
@_author: Jerry Leichter 
@_subject: [Cryptography] Best AES candidate broken 
Clearly many of the best cryptographers out there disagreed with you, as it didn't make it to the final round - and there's been general agreement that the AES selection process was of extremely high quality.
Given that the criteria were a mix of security and performance, if it didn't make it to the final 5 and was the fastest, it must have been dinged on security.
While there's plenty discussion of the 5 algorithms that made it to the final round, I haven't been able to find anything on why the remaining 10, including Crypton, the subject of the paper at hand, were rejected.  Prior to this paper, there are no known obvious breaks.  The best published attacks are against Crypton with a significantly reduced number of rounds.  In fact, by the (rather weak) measure of "security margin" you get by comparing number of rounds attacked vs. number of rounds proposed, Crypton probably did better than most of the Final Five.  As best I can tell, it mainly got dinged on security because of lack of history:  The Final Five, in general, were descendants of pre-existing algorithms which had themselves survived significant public attack.
This is an interesting bit of work.  We've had arguments on this list that no widely attacked, widely accepted symmetric encryption algorithm has been attacked since before DES - so worrying about what to do if a new attack against AES arises is pointless.  Well ... there's no clear reason why Crypton could not have been chosen as the standard rather than Rijndael.  It was provably secure against both linear and differential cryptography, and the variations of those in the last 15 years also left it unscathed.  As an AES candidate, it received some significant attention during the contest (not as much as the finalists, of course), and additional attacks (against severely reduced-round versions; there's also a related-key attack) have appeared over the years, so it hasn't been forgotten.
Now, the attacks in the paper - which I've only skimmed - are (a) against a slightly different algorithm than was proposed in AES, though the changes were intended to strengthen it, and the paper claims the techniques are applicable to the original algorithm (to be studied in a followup paper); and (b) are far from a complete break (they seem to gain about 2 bits over brute force).  Still ... attacks only get better.
I don't think this is by any means a signal that AES is about to fall.  But it's a warning about hubris:  It remains the case that the most we can say about the security of our encryption algorithms remains "They are secure against all known attacks - some of which are by now extremely general and powerful; and the brightest guys out there haven't found any holes".  New attacks are always possible.  You can't anticipate when and from what direction they will appear.  While I would agree that the probability of a major break in AES is small, it's not zero - and its cost would be incalculably immense.  Since any rational cost analysis has to consider the product of those two factors (neither of which we can estimate in any reasonable way!), it can't be something we just ignore - even if we decide that the expected risk is much smaller than that of other failures, so we can rationally decide to accept it.
                                                        -- Jerry

@_date: 2015-07-06 19:34:45
@_author: Jerry Leichter 
@_subject: [Cryptography] Only the best management from Yahoo 
I'm getting warnings for my (rarely used, it came with my DSL service years ago and has hung around ever since) Yahoo Mail account because they are offering an expired certificate.  It expired on June 23.
Way to stay on top of things, Yahoo.
                                                       -- Jerry

@_date: 2015-07-06 22:24:03
@_author: Jerry Leichter 
@_subject: [Cryptography] Best AES candidate broken 
Thanks for the reference.  It's interesting reading.
For all the claims that the AES competition ignored various leakage channels, it this reference mentions two different kinds of power analysis attacks.  Ironically, in both cases, Rijndael is estimated to be more vulnerable than Crypton; in one case, Crypton ends up in the "most vulnerable" category.
The overall analysis of Crypton reads:
CRYPTON: This candidate has the same general profile as candidates like Rijndael and Twofish, but CRYPTON has a lower security margin, based on evidence produced during Round 1. Additionally, the original version of CRYPTON has a key schedule weakness (a modified version was submitted but rejected (cf. Sec. 2.8.1)). Rijndael and Twofish have no known general security gaps. Regarding performance, CRYPTON is among the faster of the candidates, but it is slower than either Rijndael or Twofish on most platforms. Taking into account all of these factors, it was considered unlikely that CRYPTON would surpass either Rijndael or Twofish when the AES algorithm(s) is selected. Thus, CRYPTON is not advanced to Round 2.
...2.8.1 CRYPTON
Version 1.0 of CRYPTON was submitted as a modification [28] to the original submission. The submitter claims that the modification will correct the key schedule weaknesses of the original submission. However, the modification also changes the S-boxes. It was felt by NIST that the combination of these two changes violates the criterion that a modification should not invalidate the majority of Round 1 analysis. Since changes to both the key schedule and S-boxes would presumably necessitate significant re-analysis, the modification was not accepted.
Beyond this, Crypton was reported in two separate papers to have 2^32 "weak keys".  NIST didn't consider this a killer problem, but it certainly didn't help.
The claim that started much of this discussion - that Crypton was "the best AES candidate" - is simply false, based on the evidence available at the time; and there's nothing I know of that's emerged since that would change the assessment.
                                                        -- Jerry

@_date: 2015-07-07 04:55:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Best AES candidate broken 
Oops, typo there:  Rijndael was estimated to be *less* vulnerable than Crypton.
                                                        -- Jerry

@_date: 2015-07-08 14:50:47
@_author: Jerry Leichter 
@_subject: [Cryptography] Senate Judiciary "Going Dark" https site is 
Just to save others the trouble:  It's untrusted because the cert is for "senate.gov" so the names don't match.  Typical mis-configuration issue (but, agreed, delicious irony).
                                                        -- Jerry

@_date: 2015-07-08 15:27:04
@_author: Jerry Leichter 
@_subject: [Cryptography] Anti-clipper team re-assembles 
Oh, but we already did that with "TSA-accepted" locks for checked bags.  Look how well *that's* working!
                                                        -- Jerry :-(

@_date: 2015-07-09 17:06:17
@_author: Jerry Leichter 
@_subject: [Cryptography] "Office of Personnel Management Says Hackers Got 
In case you thought the previous hack that revealed data about 4.2 million Federal employees was too small to be bothered by ... OPM now says there was a *separate* hack that revealed data on 21.5 million individuals.
                                                       -- Jerry

@_date: 2015-07-10 06:46:37
@_author: Jerry Leichter 
@_subject: [Cryptography] Apple requires 2048 bits for email DH, 
The latest releases of MacOS and iOS won't accept small DH groups when setting up SSL connections.
I thought that Google had updated a long time ago, but some gMail connections apparently are failing because of this.
This will cause some pain, but it's interesting to see Apple increasingly saying to pretty much everyone:  Fix your problems or we won't play with you.
                                                        -- Jerry

@_date: 2015-07-14 17:00:38
@_author: Jerry Leichter 
@_subject: [Cryptography] Ad hoc "exceptional access" discussion at 
[Moderator:  This may be wandering far afield from crypto.  Obviously, feel free to choose not to forward to the list.]
This is way off.  A search warrant lets them ... search.  They can physically search your property, even your body.  They can't require you to tell them where you store your records, or unlock your safe.  Of course, if you don't, they'll search everywhere and get someone to open your safe - and if that requires destroying your safe, tough - you had the choice to open it yourself.
It's an undecided question whether they can compel you to unlock your computer or give then your crypto keys.  Cases have gone both ways.
Other kinds of cooperation (perhaps under physical compulsion) - e.g., getting a DNA sample - require something beyond a general search warrant.
Oh, and "arrest" is not a punishment or penalty - it's a way of gaining physical control over your person.  Many innocent people are "arrested"; that's an expected side-effect of needing to take control over them *before* they can be tried and adjudged guilty or innocent.  They are *assumed* innocent until then, but they're still arrested.
This is true.  There are court orders other than search warrants.  "Surveillance order" isn't, as far as I know, a legal term - but many things these words would appear to cover may well be available under lower standards than search warrants.
Incorrect.  Wiretaps have been allowed (with suitable court orders) for many decades.  The courts readily allow the planting of bugs in, say, restaurants where Mafia dons are alleged to gather.  Courts have decided that the FBI can't *on its own* decide to plant a GPS tracker on someone's car - but they didn't say the FBI can't do it, they said the FBI has to get permission from a court first.
*You* may think this is "unconstitutional", but you're simply wrong.  There's nothing to debate here:  "Unconstitutional" is a term from *within* the current American legal system, which has its own standards and mechanisms for deciding when it applies.  Those standards and mechanisms *define* what "unconstitutional" means.  To say otherwise is like deciding that, sure, your team scored fewer runs in the game yesterday - but *your* definition of "winning" a game of baseball is "left fewer men on base", so your team *really* won.
Not only can the government do so, it can *compel* a third party (like your landlord) to provide them.
*In general*, there is no issue with the government compelling the production of cryptographic keys.  The only open issue whether *you, personally* can be compelled to produce *your own* keys - because we have a special rule in the Constitution protecting you against self incrimination.  There are some other special protections - spousal privilege, attorney-client privilege - that might, in some circumstances, prevent certain people from being compelled to produce keys - but those are *exceptions*.  This is *exactly why* you don't want to share your keys with, say, your cloud storage provider:  The provider can be compelled to produce your keys and you have no say in the matter.  The government doesn't have to inform you; your provider doesn't have to inform you and, even if they want to, they may be ordered not to.
Incorrect.  Just as an exercise:  Where in the words of the Constitution do you find any such protection?  The closest you will get is the Sixth Amendment:  "The right of the people to be secure in their persons, houses, papers, and effects, against unreasonable searches and seizures, shall not be violated, and no Warrants shall issue, but upon probable cause, supported by Oath or affirmation, and particularly describing the place to be searched, and the persons or things to be seized."  Nothing here about *how* the search or seizure is done - just that the US government (later extended to the States through the Fourteenth Amendment) need probable cause before proceeding.
But that record can be kept secret pretty much indefinitely under the right circumstances.
If you want to say these laws are wrong, immoral, against public policy, dangerous, a threat to democracy, an affront to human privacy and decency - I'm with you every step of the way.
Stating they are "unconstitutional" is bringing a knife to a gunfight.  Let the lawyers fight that one out.  I'm not a lawyer myself, so there are probably some mistakes in my statements above - but based on reading about this stuff, I think I'm closer to the gist of things than you are.  (You might want to spend some time at  and  and  - and a number of other quality sources they will lead you to - to get some idea of how the law really works.)
                                                        -- Jerry

@_date: 2015-07-28 08:17:26
@_author: Jerry Leichter 
@_subject: [Cryptography] 
=?utf-8?q?Blockchain?=
There may be some backstory here.  It turns out that "owning shares of a stock" isn't quite so simple as you might think.  You can fall into (at least, there may be more) kinds of ownership:  You can be a beneficial owner, or a registered owner.
A registered owner has his ownership registered on the corporation's books.  That's how stock ownership used to work.  When a few million shares a day total changed hands, keeping registrations up to date was reasonable.  But by the '70's - I didn't check the exact date - the number of shares trading hands just got to be two large for the corporations.  Enter Cede and Company, which is the actual registered owner of most American shares.  The people (natural and otherwise) who actually buy and sell shares are *beneficial* owners:  They get all the rights of ownership (dividends, voting rights), but Cede keeps track of them; the corporations themselves are out of the registration business.
It's actually more complicated for individual, small-scale investors who deal with retail brokers.  Normally, the shares are held "in street name" - i.e., the broker keeps track of who the beneficial owners are among their customers.  I don't know if the brokers themselves are registered owners, or if they leave that up to Cede and then just transfer the beneficial ownerships around.  But large investment funds, at least, deal directly with Cede.  It's always been taken as a given in the industry that beneficial ownership is equivalent to registered ownership.
Except ... it turns out that it isn't.  A group of large funds that held shares in Dell when it was public didn't like the terms under which Michael Dell and some partners took Dell private in 2013.  They sued - and they just lost (  Under Delaware law, for the purpose of certain kinds of lawsuits, only *registered* owners have standing.
Needless to say, every large fund and partnership out there is now looking for ways to ensure that they are registered, not just beneficial, owners.  The corporations involved have little reason to want to go back to the old registration maintenance game.  So I'll bet Nasdaq is floating this idea now in an attempt to get more business.
                                                        -- Jerry

@_date: 2015-07-29 18:49:30
@_author: Jerry Leichter 
@_subject: [Cryptography] Graphs for asymmetric crypto? 
Patents are not technical documents in the normal sense; it's usually very difficult to determine what's really going on from a patent.  Fortunately, the authors cite an actual technical paper, which you can find at  I haven't looked at it, but if you want to understand what they are doing, I'd suggest starting there.
                                                        -- Jerry

@_date: 2015-06-06 06:00:31
@_author: Jerry Leichter 
@_subject: [Cryptography] let's kill md5sum! 
While more bits for a hash function is certainly better, 2^64 is a *big* number.  You really need to run the economics here:  Assuming technology keeps advancing at current rates, in (say) 25 years, how much will it cost to do 2^64 BLAKE2 computations?  How does that compare to the value of one collision?
Many applications that store a checksum also store a data length.  To be useful, would a collision have to be for data of (close to) the same length?  If so, an attack gets harder, as you can't simultaneously attack all protected items - only those with (close to) the same length; and each computation has to be over data of that length, which is more expensive.  Since we're only talking about brute force here, defining a standard salting mechanism and choosing a per-site salt would force an attacker to pick a particular site to attack.  Some applications could be finer-grained - e.g., a per database salt.
So ... I wouldn't dismiss their decision completely.  Schema changes at large scale can be very disruptive, and people do try to avoid them.  (I'm actually not even sure how someone would transition from a large collection of existing MD5 checksums to shrunken BLAKE2 checksums.  Would they recompute all the checksums at once?  This could be an impractically long operation.  Do they have a spare bit somewhere they can use as an algorithm flag?  I suppose if records always have a creation date, you could define a cutover time:  Any record created before T uses MD5, any record at T or later uses BLAKE2.  You are forever subject to attacks against old records, but maybe they get less interesting over time - and if you're really worried you can do a long-term project to recompute checksums for old records and move T backwards.)
                                                        -- Jerry

@_date: 2015-06-06 07:42:59
@_author: Jerry Leichter 
@_subject: [Cryptography] How many CA's are active out there? 
"I think one thing in the  world that really popped out was we have this very large certificate authority ecosystem, and a lot of the attention is focused on a small number of authorities, but actually there is this very long tail  there are hundreds of certificate authorities that we dont really think about on a daily basis, but that still have permission to sign for any Web site. Thats something we didnt necessary expect. We knew there were a lot, but we didnt really know what would come up until we looked at those."
(Zakir Durumeric of the University of Illinois at Urbana-Champaign talking about a research project to look at large-scale properties of Internet-connected devices; quoted in:  )
                                                        -- Jerry

@_date: 2015-06-08 05:51:49
@_author: Jerry Leichter 
@_subject: [Cryptography] let's kill md5sum! 
Recall Zooko's comment in the base post:  "I did a quick and dirty benchmark ... and was delighted that b2sum (in BLAKE2sp mode) was almost twice as fast as md5sum on my Intel Core-i5 laptop!"
It's *possible* that one could find other cases where md5sum is actually faster, but it seems unlikely:  Even when it was first introduced, MD5 was the "slower but more conservative hash, compared to MD4".  (Not long after, MD4 was broken badly enough that its use security-related applications never caught on - though in non-security use cases it was perhaps a better alternative!)
Given this, the only possible reason in *any* use case to choose MD5 is to compare to existing, historical records of checksums.  It's hard to judge how many of these are out there - and, more to the point, how many are out there *and use md5sum as a command line utility*, rather than using the MD5 algorithm internally.  Probably not many, would be my guess.
BTW, Zooko's goal, broad as it is, barely scratches the surface.  I personally never think of using md5sum - "openssl dgst" is probably more broadly available and by default it provides a MD5 checksum!  (Of course, openssl tries to cover the historical bases - it also supports MD2, among other long-dead algorithms.)  If we're talking about killing md5sum, we should simultaneously be planning to change the default for openssl dgst.
Then there's the flip side:  There are plenty of sites on the Internet that publish the MD5 checksums of things like software distributions to allow people to check that they have an untampered-with download.  This is *exactly* the use case most subject to significant attack!  I can't recall the last time I saw a site that provided *only* MD5 - most provide both MD5 and SHA1 - but as long as the checksums are there and the md5sum and openssl dgst command lines provide for quick checks ... people will rely on them.
                                                        -- Jerry

@_date: 2015-06-09 16:48:16
@_author: Jerry Leichter 
@_subject: [Cryptography] reality +- mathematical guarantees 
I think you're missing his point.  CRC32 - or any CRC-like (remainder mod a suitable polynomial in a suitable ring) provides certain specific hard guarantees.  For example, any single-bit change in the input will definitely change the checksum; any change of two bits where the two bits are not more than n apart (n the degree of the polynomial) will definitely change the output; and I forget what else.
These are deterministic guarantees.  There are also probabilistic guarantees:  Any random change (where the random choice is from a distribution independent of the polynomial) has only a 1 in 2^n chance of leaving the checksum unchanged.  (This is usually looked at the other way around:  If you change any number of bits of an input string, a randomly chosen (uniformly from a suitable set) polynomial will have a 1 in 2^n chance of producing a different output for the changed input.  See  .)
CRC's are, of course, not cryptographically secure.  On the other hand, the cryptographically secure checksums provide *only* probabilistic guarantees - it's *possible* that two strings differing in only a single bit will have the same checksum; it's just immensely unlikely.  (Actually, to the degree that a cryptographically secure checksum looks like a random function, it *must* be the case that there are pairs of strings differing in only one bit that produce the same checksum, since that would be true for an actual random function!  *Finding* such a pair is a whole other story, of course.)
CRC's are not the only kind of non-cryptographic checksums out there.  But all of them - and their more powerful cousins, error-correcting codes, *all* provide hard, deterministic guarantees against failure due to some well-defined class of errors, and probabilistic guarantees against errors outside the class.
There is almost no overlap in reasonable use cases for CRC-like and cryptographically-secure checksums.  CRC-like checksums are good against  changes randomly chosen from distributions of changes that are strongly biased toward those that CRC will definitely catch.  For example, CRC's are a good choice for protection against burst noise, where bit errors are highly likely to be close together.  Except in the rather specialized approach Rabin's paper suggests, they are not useful against non-random changes.  Cryptographically secure checksums, on the other hand, are appropriate against non-random deliberate attacks.
The birthday paradox is irrelevant for most uses of CRC-like checksums, but highly relevant against most uses of cryptographically secure checksums.  As a result, effective CRC-like checksums can be much smaller than cryptographically secure checksums - as well as being much cheaper to compute.  They are simply different tools appropriate for different jobs.
                                                        -- Jerry

@_date: 2015-06-16 22:34:03
@_author: Jerry Leichter 
@_subject: [Cryptography] Lastpass hacked. 
Apparently lastpass was hacked, What else should a password service do
That's too strong.  Don't store critical secrets *along with access credentials* on others' systems.
I have no problem storing encrypted data even on publicly accessible systems if the key never leaves systems I control.  For example, I use a cloud backup system which can be set so that data is encrypted before being sent and the key is kept on my own system.  (Yes, there is the problem of whether to believe the provider when he says it works this way.  That he's in fact lying a risk I choose to take.)  In doing this, I give up the possibility of accessing my backed-up files using a Web browser - a service offered if I choose a lower level of security in which the provider has a copy of my key.
I've never used a password manager, but I'm willing to store encrypted passwords out in the cloud.  I have to decrypt them by hand and copy and paste them into place - I've deliberately given up the convenience of having them auto-magically populate the relevant fields.  I've always thought that was a good security tradeoff, and I'm even more sure now.
                                                        -- Jerry

@_date: 2015-03-01 10:28:46
@_author: Jerry Leichter 
@_subject: [Cryptography] DIME // Pending Questions // Seeking Your Input 
Not to disagree that this is a good feature, but ... having written (actually, fixed) a parser for an encoding (not ASN.1, which has its own special complexities) that used a nested TLV (Type/Length/Value encoding), I'd say it's not particularly hard to get the bounds checking right.  But you have to design for it from the beginning and follow the design consistently.  I used something very much like recursive descent parse, but the rule was that every call took a begin pointer and end pointer (this was C).  Internally, you maintain a "current position" that starts at the begin pointer and may never reach the end pointer; when you've read the length of a subelement, you compute its end pointer, which you'll pass to the subelement parser and which will become your new current pointer on return.
Note that C programmers will all too often ignore the "pass the end pointer" part (as an "optimization" since, e.g., if the field has a known length, the caller would have checked, right?).  Programmers of languages with native strings will all too often just pass the starting point - which guarantees that a sub-element parser can't read past whoever constructed the top-most string, but doesn't prevent reading past the sub-element's boundaries.
Perhaps the easiest language to get this right in is Java, since the substring operation is essentially free:  It points into the parent string but has a different length.  Of course, you do have to get your substring arithmetic right consistently - having a helper function that pulls it out is the way to go.  In C++, a substring operation will copy the data, so you generally need to compute and pass the end pointer yourself.
Still, I'll agree that people get this on-its-face trivial bit of coding wrong all the time.  A parser generator is really the way to go:  Get it right once and for all.
                                                        -- Jerry

@_date: 2015-03-03 16:21:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Air Traffic Control computers are maintained about 
From  quoting from a GAO audit of the ATC system:
"While the Federal Aviation Administration (FAA) has taken steps to protect its air traffic control systems from cyber-based and other threats, significant security control weaknesses remain, threatening the agency's ability to ensure the safe and uninterrupted operation of the national airspace system (NAS). These include weaknesses in controls intended to prevent, limit, and detect unauthorized access to computer resources, such as controls for protecting system boundaries, identifying and authenticating users, authorizing users to access systems, encrypting sensitive data, and auditing and monitoring activity on FAA's systems. Additionally, shortcomings in boundary protection controls between less-secure systems and the operational NAS environment increase the risk from these weaknesses....
Additionally, the agency did not always ensure that security patches were applied in a timely manner to servers and network devices supporting air traffic control systems, or that servers were using software that was up-to-date. For example, certain systems were missing patches dating back more than 3 years. Additionally, certain key servers had reached end-of-life and were no longer supported by the vendor. As a result, FAA is at an increased risk that unpatched vulnerabilities could allow its information and information systems to be compromised."
[T]he FAA "did not always ensure that sensitive data were encrypted when transmitted or stored."  That information included stored passwords and "authentication data."
                                                        -- Jerry

@_date: 2015-03-04 12:18:32
@_author: Jerry Leichter 
@_subject: [Cryptography] FREAK attack 
Latest attack on SSL, affecting some huge percentage of both servers and clients:   Remember all those export modes for SSL that we had to live with two decades ago?  Well, it turns out they are still present in at least two code bases (OpenSSL and Apple's SSL implementation), though they aren't offered to the peer.  However, if you MITM the connection and simply tell both ends to use export RSA (512 bit=) - due to bad checking, they will.
Lessons to learn:
1.  Modes and choices are bad in crypto protocols.
2.  Leaving holes to let "good governments" in will inevitably leave holes for others as well.
3.  In code, assume nothing ever really goes away.
Not, I'm sure, that anyone on this list needs persuading.  But this needs to be repeated, over and over again, so that even non-crypies - and even non-techies - come to internalize it.
                                                        -- Jerry

@_date: 2015-03-04 18:19:32
@_author: Jerry Leichter 
@_subject: [Cryptography] FREAK attack 
For some reason people seem to be concentrating on the MITM aspect.  Just to make clear what happens:  This is a MITM attack *against the initial protocol negotiation*.  This is necessarily done in the clear, as by its nature it occurs before the peers have decided how they'll encrypt.  The attacker takes one of the messages and replaces it to say "I want to use RSA_Export".  The recipient - and this is the bug - says "Oh, OK, I can do that" - even though it never offered to use RSA_Export.  (In effect, when you think you're configuring *allowed* ciphers, what you're really doing is configuring *offered* ciphers.  But a bad peer - or someone in the middle playing with the negotiation - can cause your SSL implementation to use one of your "not allowed" ciphers.)
In the specific attack, the two ends "agree" on RSA_Export - RSA with 512-bit keys.  These are readily attackable today - a couple of hours and $100 on AWS.  That's bad enough, but it turns out that some common server implementations "save CPU" by re-using the RSA base for long periods of time.  Factor and many sessions are open to you.
                                                        -- Jerry

@_date: 2015-03-05 06:22:12
@_author: Jerry Leichter 
@_subject: [Cryptography] Proof of preservation... 
This is an interesting question, but it's well-posed.  Who are the actors involved here?  Who has to prove what to whom?
Some of the components - hashing, digital notaries, timestamping, proofs of retrievability - seem to be there, but it's not at all clear how they might produce the kind of proof you have in mind because they often provide a proof to the wrong party, or produce a proof of the wrong thing.  For example, releasing hashes of all my emails only allows someone to check later, when I've released the emails, that the hashes correspond to those purported emails - not that those were the emails I actually sent, nor that there could not have been further emails.  Notarizing those hashes doesn't change things.  Chaining them just means, if I want to fake things, I have to be consistent about it - no big deal.  Proofs of retrievability come closest, but the prover is whoever holds the mail for me, and it's only useful to prove *to me* that he still holds what I sent him.  In and of itself, it doesn't prove to anyone else that I sent him the right stuff.
Also ... we want the *incoming* mail to be archived as well.  Seeing only one side of a conversation is not particularly useful.  In the situation described, some of the conversations will be with private parties who are under no obligation to archive the messages *they* send.  Even if *everyone's* messages are archived, it's unreasonable to assume a search of every outgoing mailbox in the world in order to find all messages sent to the person offering the proof.  (Of course, I'm sure a certain three-letter agency would be glad to offer such a service - but for their internal use only....)
We've seen over the last couple of decades that all kinds of apparently-impossible system properties an actually be attained using cryptography.  Whether something of like a "proof of preservation" is possible ... I don't know.  If it is, the proof would have to be deeply entangled with the actual operation of the system.  For example, it can't be that there's one "channel" by which I receive an email, and another by which my receipt of it is recorded for later proof - that would be like a signature separate from the document signed. The two have to be linked.  It must be the case that by the time I'm in the position to read a message (e.g., I'm in possession of a cleartext copy), a record that I decrypted it has already been made.  The very nature of the problem requires a third party to hold *something* - so the third party will inevitably be involved.
                                                        -- Jerry

@_date: 2015-03-05 09:35:16
@_author: Jerry Leichter 
@_subject: [Cryptography] practical verifiable systems -- forensic and 
I've spent way too many years working on system and network management software that had to interface to the remote interfaces of all kinds of vendor equipment.  It's uniformly dreadful from the point of view of security.
Recent example:  NetApp sells high-end disk arrays.  The management system for them is based on SOAP over HTTPS.  (I think you can configure it to use HTTP, but let's not go there.)  Older versions of the software - way too recent for there to be any excuse for this - supported only SSLv3.  More recent versions support both SSLv3 and TLS1.0 - but TLS support is off by default.  I don't know if any versions of the software support the latest TLS versions.
There appear to be many NetApp devices out there running either old software, or software in the default configuration - so managing them requires SSLv3.  We've had to explicitly re-enable support for (outgoing) SSLv3 in our software (Java - SSLv3 support is off by default starting with Java 7) just to talk to these things.  I hate putting stuff out there that supports a protocol that's been considered vulnerable and deprecated for, what, a decade - but I have no choice.
The reason vendors get away with this is that most customers - the groups that manage the data centers - seem to (implicitly) take that attitude that "This stuff is all on internal networks, we don't need to worry about security.  Besides, we manage the systems, we don't look at the actual data - that's Someone Else's Problem".  The basic attitudes haven't changed in 20 years.  (The managed systems and the protocols have gotten immensely more complex over that time.  Also, the great trend has been to make everything configurable in software.  At one time, you could monitor remotely, but significant changes required either using the command line - often over a hard-wired port - or actual physical access.  All that's gone now; it's "nothing but net".  And of course *everything* is accessed through a window in some browser - so now the damn browser is part of your core management function.  None of these trends, needless to say, have helped matters.)
Side note:  You might think, hey, maybe there's a market here for a browser designed for industrial, not consumer, applications.  Support just the bare minimum needed; throw out all the cruft needed for consumer movie sites and such.  Unfortunately, there's no hope in that direction:  While the Intel examples in that presentation show pretty basic Web pages, the management pages presented by many other devices are full of the latest fancy gimmicks.  Hey, it's the Web, it's supposed to look flashy!  Not to mention that convincing the guy who wants to use these interfaces to run two browsers - you *know* he's watching cats on Youtube during slower moments - is a non-starter.
                                                        -- Jerry

@_date: 2015-03-05 14:14:48
@_author: Jerry Leichter 
@_subject: [Cryptography] FREAK attack 
There's an elephant in the room that we don't like to talk about:
*Security is fundamentally incompatible with backwards-compatibility.*
Once RSA-512 becomes readily factorable, an implementation built when it was not can never be be secure again.  If a new implementation supports a backwards-compatibility mode, it's an *insecure* backwards-compatibility mode.
As software and hardware guys, we generally hold backwards compatibility as one of our highest goals.  We know all the tricks for making the new stuff work with the old.  We insist that our old stuff work with our new stuff.  We bitch and moan that Apple - and increasingly other vendors - makes it impossible to revert from release N back to release N-1.  "They broke feature X in release N!  I insist on my right to move back!"
But the fact is that security is *different*.  Security will never move forward if devices can be forced back to old modes of operation, or if software can be reverted - perhaps by an attacker - to a previous, insecure state.
So I'd pose things differently:
In a world where backwards-compatibility is sacrosanct, algorithm-agility has a *potential* to work - but as we've seen, in practice, it's a continuing source of effective downgrade attacks of different sorts.  "If the software were better implemented..." is just wishful thinking.  There's no evidence anyone can get this right.
In a world liberated from backwards-compatibility, your question can be seen to be based on a bad assumption:  That 15 years ago, you had to pick one good suite once and for all.  No, 15 years ago, you would have picked one suite that provided adequate security at acceptable cost, given the knowledge and technology of the time.  And then 5 or 10 years later, if the tradeoffs had shifted, you would have chosen a *different* single good suite.
The new suite *supplants* the *old*; it doesn't *supplement* it.  When the engine goes on your old car and it's not longer repairable, you supplant it with a new one; if you "supplement" it, you're soon in possession of a yard full of junkers on blocks.
BTW, some of the most successful companies in the tech world have, to a large degree, rejected the cult of "backwards compatibility".  Apple's an obvious one, but Google and every other web app-based company downloads the latest version of the software into your browser.  No, you can't go back to last week's version.
                                                        -- Jerry

@_date: 2015-03-07 07:27:15
@_author: Jerry Leichter 
@_subject: [Cryptography] FREAK attack 
I'm not sure how you're getting your estimate for 1K RSA.  The RSA-1024 factoring challenge - with a $100,000 prize - remains unsolved.  Indications are that someone will break it "any time now", but it hasn't happened.
Of course, the initial break will likely be with a very large network of machines and a great deal of time.  To get to routine factoring using reasonable resources will take a number of years more.  NIST "deprecated" RSA-1024 as of 2011, but only "disallowed" it starting in 2014, which seems conservative and prudent.
Accepted asymmetric encryption algorithms have not be attacked successfully; they've only "timed out" in the sense that their security parameters have become too small to provide an adequate defense against ever-faster attackers.  The "time out" periods have been predicted well in advance over quite a few years, so can reasonably be planned for.  At least with RSA, trying to stay "way ahead" - say, using 16K keys - is impractical for performance reasons.  What's needed for asymmetric algorithms appears to be:  Pick an algorithm and provide a pre-planned means to increase key sizes over time.
At some point we might well want to "transition" from RSA to ECC.  But there's little reason to have them as *alternatives*.
                                                        -- Jerry

@_date: 2015-03-14 18:21:16
@_author: Jerry Leichter 
@_subject: [Cryptography] Fun and games with international transaction 
[Fascinating history/discussion of how international settlements work elided.]
One thing that gets left out of all these discussions:  Calling something a "fiat currency" is a great political ploy, but misses a fundamental point.  Unlike banks, which simply hold financial assets that others have lent them (they own some of them, to some degree or another - that's their capitalization - but that's a tiny part of their total assets even if they are very well capitalized) nation-states actually govern (as in control) large numbers of people, who in turn have large capital assets.  Combining those two gives them productive capacity - the ability to create things with actual, not just notional, value - food, energy, homes, car; all that boring stuff that keeps us alive.  The people and the capital have a limited ability to leave the nation-state (unlike the runnable assets that can so quickly disappear from a bank's books), and the acceptance by the population of its legitimacy - even mor thab nation-state's monopoly on force - means that it has the ability to take portions of that productive capacity (i.e., it can raise taxes).
A bank's ability to meet its obligations is based entirely on its ability to attract deposits (and to a certain degree, capital).  A nation-state - and hence a central bank's - ability to meet its obligations is a much more complicated thing.  Most nation-states, most of the time, can pay back what they owe (though it may be painful).  The nature of the exponential growth in interest payments means that even a nation-state can get to a point where its productive capacity simply can never pay back what it owes.  This has happened many times in history, and is arguably happening today in, say, Greece.  But ... it's not the normal situation.
The reason the world treats the dollar as having real value is that the US has an almost-250-year-long history of paying back all its debts, going all the way back to Hamilton and his insistence that the newly-created nation pay back all its war debts, difficult as that was at the time.  So not only is the underlying productive capacity there, but so is the commitment to apply it.  (That's why the Republican game-playing with the debt ceiling was so incredibly dangerous, and incredibly stupid.  Throwing away a 250-year history of good behavior that produced the world's highest credit rating for some nice quotes and newspaper headlines?  Really?)
A bank's "currency" - its ability to keep attracting deposits for lending - is much more fragile, and ultimately banks that don't have strong governments to back them up are very vulnerable.  They trust each other as long as they are confident that their counter-parties are either stable in and of themselves, or have a stable nation-state to back them up.  In the 2008 financial crisis, that confidence was lost, and the very largest banks refused to lend to each other as no one could be sure who could repay.  The world economic system came close to total collapse - rescued only by the willingness of the US and a few of the strong European governments to back up the biggest banks.
It's not clear how Bitcoin would necessarily change any of this.  If Bitcoin somehow becomes completely independent of the existing banking system - and its hierarchy of backing mechanisms that go up to the largest nation-states and their productive capacity - it's not clear how it can ever be stable.  (Granted, the existing monetary systems have problems with stability, too - but we've more or less managed to keep them under control.  In fact, we kept them very stable - no major panics; they were once an every-decade-or-two-occurence - from the 1940's until 2008.)  It's a nice theory that, with a strictly limited quantity in circulation, and a strictly controlled growth rate, Bitcoins should act kind of like gold, and not be susceptible to inflation (or deflation).  The wild gyrations in the value of Bitcoins - and in the price of gold, for that matter - indicates just how little that theory tells us about the real world.
                                                        -- Jerry

@_date: 2015-03-15 07:38:57
@_author: Jerry Leichter 
@_subject: [Cryptography] Fun and games with international transaction 
The US GDP for 2014 was about $17.7 trillion (  Its public debt was about $18 trillion (same page).  Using your figures, double that to fold in all debts public and private - though that's a gross over-estimate, since the vast majority of private debt is internal and cancels itself out.  (That's true of much of the public debt as well.  Over 28% of the US debt is money one arm of the government owes to another So by even a very generous calculation, total debt is twice annual income.  What does this mean in practical terms?
Back in the "good old days", before we got wild and crazy with debt here in the US, the rule of thumb for a safe cost for a house was 2.5 times your annual income. (I don't know where to find a reference for this - I recall it from bank ads for mortgages during my childhood.)  Today that would be considered way too conservative - no one but the very wealthy could buy housing in many areas of the country if we stuck to that rule.
During the "2.5 times value" era, US mortgage rates were in the 5-6% range (   reports that the average interest rate the US government is paying is half that - 2.84%.  (Yes, the comparison isn't an ideal one, as the mortgage rates were almost universally for 30-year fixed rate loans, while much of the US debt is in much shorter-term instruments.)
Mortgage delinquency rates would provide an indicator of how good this rule actually was in determining whether people could pay their mortgages, but I can't find any data sets going back before the early 1990's, when Fannie Mae was created - before that, the market was highly fragmented among a large number of savings and loans.   shows that even through 2007 - which includes an era with much higher loan-to-value ratios and much higher rates - delinquency rates were around 0.5%.  Not a bad bet.  (There's always going to be some delinquency - people get sick or injured and can't work, die young, become addicted to alcohol or gambling, or run into any number of other of life's pitfalls.  That's why mortgages are not at the risk-free interest rate.)
So by the old, very conservative standards, the US would be considered extremely likely to be able to pay back its debt based on its income stream.  And, indeed, the US is currently paying close to the estimates for the risk-free rate.
Those who have an actual stake in the system don't act as if they believe it's a house of cards.
What the 2008 crisis - after, again, two generations of boring stability in the financial system - showed us is that if we're stupid enough to let the financial markets go off and set the rules for themselves, they have the capability and the motivation to bring the whole thing down on everyone's head.
The doctrinaire free market types refuse to believe it, but the financial markets are an inherently unstable system.  Just like every high-performance airplane built today.  Both work anyway.
Then again, the reason the Wright brothers succeeded was because they knew bicycles worked even though they are inherently unstable.  What's needed is dynamic stabilization.  The more I hear the big banks complaining that they can't make those huge profits any more because of the drag of their new capital requirements, the better I feel.  Those profits are a sign of the system's ability to take wide swings.  Financial firms profit from those swings.  If the swings are damped, if the system once again becomes boring (it's hard to remember that banking was, not so long ago, the place to work if you didn't want to be challenged and wanted to get home early - "banker's hours"), the rest of us profit as money (and the best minds) move out of financial game-playing and into the real economy.
You need a certain amount of "irrational" belief for people to be willing to take risk - which is how the economy as a whole grows, even if many of those risks don't pan out.  Most startups - most new businesses - fail.  But enough succeed that the optimists will keep trying, and some will succeed and grow the economic pie.
But "beliefs" can turn around very quickly.  Too much reliance on unsupported belief and, indeed, you have a house of cards.
                                                        -- Jerry

@_date: 2015-03-18 14:38:20
@_author: Jerry Leichter 
@_subject: [Cryptography] Kali Linux security is a joke! 
HTTP is clear text, they can simply record it.  No need for MITM as the term is generally used.
And this is different from screwing up the HTTPS negotiation as a way to deny service ... how?
I would also be very surprised to learn that anyone has made it a practice to block updates via denial of service.  The Western intelligence agencies want to be invisible.  Blocking access to updates is anything but.  The, err, what's the opposite of "Western" - not really a geographical designation - in this context, other powers don't really care if you know you've been blocked.  They are already implementing tons of blocks anyway - if they don't want you going to that update site, adding its name and IP to their block list is trivial.
Do you have *any* evidence of NSA/GCHQ actually doing this?
                                                        -- Jerry

@_date: 2015-03-18 15:31:41
@_author: Jerry Leichter 
@_subject: [Cryptography] Kali Linux security is a joke! 
Ahem.  It's one thing to edit a message for length before replying; it's quite another to completely change the meaning.
"Do you have *any* evidence..." was in reply to "...NSA/GCHQ can deny
their victims the opportunity to upgrade their security."
                                                        -- Jerry

@_date: 2015-03-23 06:53:56
@_author: Jerry Leichter 
@_subject: [Cryptography] "Most Americans Don't Mind Being on Candid Camera" 
Maybe the most important job for those of us concerned about security is convincing our fellow citizens that there's something to be concerned about.
Depressing article:
                                                        -- Jerry

@_date: 2015-05-03 16:02:05
@_author: Jerry Leichter 
@_subject: [Cryptography] "Trust in digital certificate ecosystem eroding" 
I agree in general, but even though I don't think you meant it that way all, the general engineer's mindset of blaming the user slipped in there.  "If only users would take the training to heart!"  No.  If users need training to make reasonable choices, the first thing to look at is whether the choices really *are* reasonable.
I'll give you an example.  One of my mail services comes from a small ISP. Periodically, my Mac reports that their certificate is invalid because it comes from an unknown authority.  The first 10 times or so this happened, I'd look at the certificate.  It would come from some GoDaddy variant, and would otherwise appear to be correct.  How would I check further?  With hundreds of trusted CA's on the list - including a bunch of GoDaddy variants - the effort to determine just why this one is being flagged is too difficult to be practical.  So in the end, after staring at the thing a bit, I end up clicking Trust (temporarily).  Eventually, I found myself skipping over the "staring" stage - which always has the same outcome anyway - and just hitting Trust immediately.  I know it's bad practice - but do I have any realistic alternatives?  The entire system is set up so that I don't, if I want my mail to continue to flow.  *And I know a hell of a lot more about this stuff than most users!*
Forget the blame game.  Forget training.  We long ago decide that telling people to drive carefully didn't save lives - adding seat belts and air bags to cars saved lives.
Yes ... but.  "Automatically" is another of those words beloved by engineers.  Translated, it means "without humans in the loop to screw things up."  But we're talking about *trust* here, which is a *human* concept, not a machine concept.
What we've come down to, in reality, is this:  Humans place their trust in their browser makers, who are effectively super-CA's.  (Businesses, in theory, could build their own internal lists of trusted CA's, but I've never seen any that do.  Rather, they just add their own internal CA to the list they get from the browser makers.)
Browser makers never really agreed to take on this job, they aren't compensated for it, and they aren't even really punished when they get it wrong.  It should be no surprise that they are not deserving of the trust we are forced to give them.
I've previously argued here that we could do pretty well entirely without CA's - and with certification pinning we're heading there (in an inefficient, roundabout fashion) anyway.  But as things are today, that wouldn't change the "super-CA" role of the browser makers anyway.
You can argue that you have to trust your browser maker anyway, since he has control of all the code.  But while I trust, say, Google to write pretty secure browser code, that's very different from say that I trust Google to make check that the CA's they list are actually "good guys", for pretty much any sense of "good guy."  It's not a job they ever really offered to do, and they aren't well placed - because they have to do business all over the world - to say "no" except in really egregious cases.
What we need is that CA's were *supposed* to be to begin with:  Organizations that built up trust, whose value came from the trust they built up, and whose usage was based on that trust.  There's no hope of that now.  There *might* be hope in separating the roles of "browser maker" and "trusted CA database supplier".  If browser makers made it really easy to replace the databases of CA's - so simple that anyone could do it - you might see a market for such databases emerge.  Some of it might be for profit; some of it might be governmental; some might be charitable.  (Perhaps there'd be an EFF CA database, and an ACLU CA database.)  Some of these databases might accrue enough value and users that CA's would be forced to make a case to them that they deserved to be listed.
Humans can make trust decisions about other humans, and about organizations.  They may be wrong - this world offers few absolutes - but such decisions are likely a hell of a lot more reliable than trust decisions based on some random prompt from a program.
                                                        -- Jerry

@_date: 2015-05-06 19:08:31
@_author: Jerry Leichter 
@_subject: [Cryptography] Is there a good algorithm providing both 
I doubt there's much work on something like this because I don't see how it could be very good.  A compressor works by removing redundancy from the input.  The compressed text will reveal where the redundancy was.  For example, suppose you used an LZ-style compressor.  If the input starts with two repeated characters, the encoding will start with two repeated symbols.  Granted, without the dictionary, you can't tell *what* character was repeated - but the fact of the repetition will be right there, and often gives much information away.
In fact, for most common compression algorithms, you don't need the table to know how to divide the compressed data into symbols.  So what you end up with is effectively an old-style code book.  Techniques for breaking those were well developed.  It's true that in the old days, a single codebook was used for all messages (though super-encipherment was common, and was broken); while here there's a new code book for each message.  But individual "messages" today contain as much data - if not more - than might be gathered in years of effort in the old days; so that doesn't help you much.
Perhaps you could tailor an compression algorithm to work in the context, but I doubt it.  Even the obvious and straightforward "compress then encrypt the entire result" leaks information about the lengths of messages - and in some contexts (e.g., compressed speech fragments) that's been found to be enough to completely break the system.  The basic issue is:  Compression turns semantic information into message lengths; and most encryption algorithms leak information about the clear-text message lengths.  So the combination is inherently dangerous.
                                                        -- Jerry

@_date: 2015-05-08 06:50:23
@_author: Jerry Leichter 
@_subject: [Cryptography] Is there a good algorithm providing both 
There's a fundamental problem with this argument:  In the case of compression, there's no secret information.  Anyone can run the decompression algorithm.  The output may "look like noise", but it's readily distinguishable from noise by anyone.  (This is different from compression, where it's distinguishable from noise by an opponent who has the key, subject to whatever security constraints apply, e.g., polynomial bound on opponent time.)  The original proposal being discussed kept some of the information from the opponent - i.e., it assumed a compression that used a dictionary which was itself encrypted, so for simplicity we assume it's just not present.  But as the example I gave showed, that failed to hide some of the information - e.g., in an LZ-style compression, the exact pattern of repetitions of previous strings is revealed, even if the opponent doesn't know what strings are being repeated.  Still, given some known (or guessed) ciphertext, this is quite damaging.  (And it's certainly a good *distinguisher* from random noise.  I don't know what the output of an LZ encoder on random input looks like, but it's certainly going to have characteristics far from, say, English texts.)
Err... did you read the message you're responding to, and the message *it* was responding to?  Why would you think I was claiming it was?
This is provably incorrect, in some cases.  I cited a published example (  If you take a constant-rate stream of audio samples of speech and run it through any decent encryption, it's secure.  If you first compress the speech using one of the state-of-the-art algorithms designed for this purpose, you can get a factor of 8 or even 16 compression.  The result is a series of varying-length packets whose inter-packet timing is also important.  It's been experimentally verified that just the timing and the lengths of the packets is enough to extract some of the original speech.  So even encrypting such a stream with a one-time-pad is completely ineffective.  Sure, you can insert random filler to hide the packet lengths and inter-packet gap lengths - but then you're throwing away what you gained in compression.
There are cases where compress-before-encrypt is fine.  But there's no general result of this form, nor can there be unless you first reduce your model to that of simple mathematical transformations, ignoring the physical realities of transmission and the side-channel information they inherently leak.
It's good to know that leaked message lengths are fine for you.  I guess I won't be buying your "encrypted conversation" app....  :-)
                                                        -- Jerry

@_date: 2015-05-10 07:19:06
@_author: Jerry Leichter 
@_subject: [Cryptography] A Fun Trick: The Little MAC Attack 
Export may have entered into it in some cases, but performance was probably a bigger issue.  We're talking about a time when "hash" was MD5, "encrypt" was DES (or maybe 3DES), and a "fast chip" was an i386.  MD5 was *much* faster.  You'd probably have trouble streaming encrypted audio - but you could manage streaming authenticated audio.  "Authenticated/integrity protected but not encrypted"  modes were common in proposals in those days.  They still remain in IPSEC (whose origins go back that far) - though there hasn't been any *technical* need for them in many years.
Most proposals of the day simply used MD5(key || message), a few might go for the "more secure" MD5(key || message || key).  You could find tons of hand-waving arguments for why this was safe.  (I started on a project that used the first form - justified with those hand-wavy arguments - in about 1995.  It took me about 10 minutes - well, maybe a bit more - to (re-)discover extension attacks against it.  The "safer" double-key mode takes more sophistication to attack, but isn't actually any good either.  BTW, this particular project was gathering data from a large number of small devices.  It was argued that (a) the actual data wasn't secret - it was observations about public information, so didn't need to be encrypted; (b) falsified information could lead you to incorrect decisions, so authentication/integrity guarantees were useful; (c) the devices were too small and weak to run DES anyway.)
HMAC at least gave you a mode that "worked" in some reasonable sense.  As a technological compromise given the realities of the era, it made sense.
                                                        -- Jerry

@_date: 2015-05-10 08:33:48
@_author: Jerry Leichter 
@_subject: [Cryptography] Is there a good algorithm providing both 
One can readily construct an example where this is false.  All it takes is for there to be a semantically meaningful difference in compressibility between messages.  Trivially:  If I want to prevent traffic analysis by sending random cover messages when I have no real message to send, and I feed those messages into a compress-then-encrypt engine, the real messages will probably compress much more than the cover messages.  Alternatively, if my cover messages just say "cover message" over and over again, it will be much more compressible than real messages.
Sure, you can avoid that by being careful about how you construct your cover messages - but a "safe" cryptosystem isn't supposed to require that much diligence from its users.
Also, sure, if you *don't* use compression first, most cryptosystems will leak information about message lengths.  But that's something that's much easier to understand and protect against than leaking "whatever redundancy the compression algorithm is able to find".
Also worth mentioning:  There's a bunch of work - Dan Bernstein is big on this - at making encryption algorithms side-channel free by ensuring that they don't vary anything - timing, power use, cache hits - based on their input data.  You throw all that away if you compress.  I find it hard to even imagine a side-channel-free compression algorithm.
It's not that "compress then encrypt" is *necessarily* bad.  It's just that because it introduces hidden, subtle data dependencies, it requires careful analysis *of the entire end-to-end system* before use.
                                                        -- Jerry

@_date: 2015-05-10 16:55:40
@_author: Jerry Leichter 
@_subject: [Cryptography] Is there a good algorithm providing both 
Well, fine, but the responsibility is on use security geeks to understand what's safe and what's not and make sure the system is appropriate for its intended use - and the reasonably foreseeable additional uses to which it will be put.
And even then there will be surprises.  You'd have thought that compress-then-encrypt would be perfectly safe for VoIP, but as the article I cited earlier shows, you'd have been very wrong.
                                                        -- Jerry

@_date: 2015-05-10 17:13:13
@_author: Jerry Leichter 
@_subject: [Cryptography] A Fun Trick: The Little MAC Attack 
No.  "||" is "concatenate".
Unfortunately, we don't have universal agreement on some basic notation.  The use of "||" goes back at least to PL/I, probably further.  C re-purposed it for "short-cut or" years later; at least that's an unlikely interpretation in this context.  I don't think I've ever seen it used for XOR.
Similarly, "^" for exponentiation goes back to BASIC, if I remember right, but again C re-purposed it for XOR, so it's also ambiguous.
Many languages use "+" for concatenation.  As a programmer, I use this all the time; as an ex-mathematician, it bothers me.  (Mathematicians generally assume that any operator written in the form of an addition is commutative.  String concatenation in a mathematical context would normally be written as a multiplication; multiplicative operators may be commutative or not - usually not.)
Anyway, if I'd used "+" you might have taken it for addition; and if it's used "*" I doubt anyone would have read it as concatenation.
My personal convention in ASCII text is || for concatenation and ~, OR, XOR, and AND for the binary bitwise operations.
                                                        -- Jerry

@_date: 2015-05-29 07:44:32
@_author: Jerry Leichter 
@_subject: [Cryptography] From The Daily WTF 
Always amusing, but today's collection happens to contain a couple relevant to this list.
                                                        -- Jerry

@_date: 2015-05-30 22:21:43
@_author: Jerry Leichter 
@_subject: [Cryptography] open questions in secure protocol design? 
This is an interesting approach, but it's missing a careful threat analysis.  You've baked in consideration of the "death by 1000 cuts" failure, where there's plenty of time to anticipate that the current algorithm is heading for trouble.  But what if the current algorithm fails suddenly?  You have no mechanism to move quickly to something else.  Alternatively, what if it turns out, half way through your distribution of a new algorithm, that the *new* one fails, while the old one seems to still be strong?  Can you somehow move back to it, without enabling rollback attacks?
Negotiation of the protocol to use makes sense in situations where endpoints can actually make a rational choice among alternatives.  You might be able to construct such a situation for communication within a group of experts - e.g., well-trained spies.  I find it very hard to come up with more common situations in which it makes sense.
Picking a single cipher is like engineering with a known single point of failure.  That's not necessarily a bad thing!  Your car's steering system has multiple single points of failure, because there's really not much you can do to make it redundant without increasing other risks due to complexity.  On the other hand, your car's brakes are dual-redundant because that's a better assignment of risks for them.  Whether a single-cipher system makes more sense than one with N ciphers depends on your failure scenarios.  Do you include rollback attacks?  How about "directed failure" attacks, where an opponent is able to break one of your ciphers and then force connections to use that one?  Against such attacks, adding more ciphers *decreases* your security!
Consider a situation where you have two ciphers, and you think your opponent might be able to break one, *but you don't know which*.  Then you're into game theory, and the optimal strategy is a mixed one:  Choose one cipher or the other by tossing a coin each time.  (Of course, you could also encrypt with both.  That reduces you to a single cipher approach - but the cipher is a more complicated one resulting from combining the other two, a combination that may be less well studied that either of your original choices.)
We go back and forth on 1TCS versus algorithm agility versus algorithm evolution without stepping back and admitting that *this choice is itself a protocol design problem*, and needs to be approached as such.  A threat analysis is the first step....
                                                        -- Jerry

@_date: 2015-05-31 10:50:25
@_author: Jerry Leichter 
@_subject: [Cryptography] open questions in secure protocol design? 
Well-studied modern *encryption algorithms* have never failed.  But they are tiny pieces of cryptographic *systems*, which have failed fairly suddenly.
With respect to the inner core encryption algorithms, any mechanism to provide on-the-fly substitution will almost certainly *increase* the overall vulnerability of the system in which the encryption algorithm is embedded.
(BTW, I'm not sure MD5 really belongs comfortably on your list.  Yes, people were worrying about it - but Wang and Yu's paper was still considered a significant breakthrough.  And their techniques broke a number of other systems that weren't in use but that "the day before" might have looked like possible alternatives to MD5.)
Indeed.  This is another aspect of the problem - and, again, something that needs to be considered in an analysis of the *overall system*, including its fail-over modes.  (One advantage of the "flip a coin to determine which cipher to use" is that it guarantees that both paths are regularly exercised.  Not a reason to choose that approach, but something to consider in analyzing the entire design.)
                                                        -- Jerry

@_date: 2015-11-01 07:37:51
@_author: Jerry Leichter 
@_subject: [Cryptography] Are zero knowledge authentication systems safe? 
Koblitz and Menezes have been writing about issues with "proofs of difficulty" for close to a decade.  One overview is   Over all, they make several points:
1.  The proofs that are out there, even if true, often don't prove what they claim;
2.  The trend in these proofs is in an unfortunate direction, based on reductions to problems more or less made up for the purpose and which there is actually almost no reason to accept at face value any claim of difficulty;
3.  One can find examples where algorithms were almost certainly made *weaker* in order to enable a "proof of security".
They haven't specifically addressed the issue you raise - brittleness - though it's implicit in much of what they write.  My own take on this is that traditional mathematical techniques and results are *inherently* brittle.  Why?  Because the mathematician's goal is the most general possible theorem - the one that requires the bare minimum of constraints to be true.  But that means the instant you remove any of the remaining constraints - the theorem becomes false!  (Compare the reported NSA definition of a trusted party:  Someone who can break your security.)
It's not that there hasn't been work that attempts to develop some kind of theory of robustness.  The whole field of concrete complexity - some of the pioneering work here was done by Bellare and Rogoway - at least started of as a way to develop such a theory.  (I haven't kept up with the literature and don't know where it is these days.)  But it's the direction relatively few have followed:  After a great deal of careful, complex analysis, you get fairly boring-sounding results.  Meanwhile, the guys making the assumptions come up with all kinds of sexy new algorithms and protocols....
                                                        -- Jerry

@_date: 2015-11-01 19:09:44
@_author: Jerry Leichter 
@_subject: [Cryptography] Are zero knowledge authentication systems safe? 
You'll have to refer to the papers for the example.  If your *definition* of security is "provable security", then of course the kind of thing I describe makes no sense.  But if you agree with Koblitz and Menezes that given the current state of the art, proofs of security aren't worth much and we're stuck with heuristic/engineering arguments - as is the case for all symmetric algorithms, for example; at best, we have proofs that certain classes of attacks can't work - then the story is different.  The example they give - I read this a couple of years back so this is only an approximation - was of a protocol that had been subject to all kinds of attacks and looked solid.  Someone modified it to make it "provably secure".  The modifications added all kinds of complexity - and arguably made the whole thing much more brittle, in that minor mistakes not only lost you the proof of security, but you were now left with a complicated protocol about which you could no longer make any believable informal arguments.
                                                        -- Jerry

@_date: 2015-11-02 06:40:51
@_author: Jerry Leichter 
@_subject: [Cryptography] How programming language design can help us 
Actually, that's still the case.  IEEE 754 pins down a great deal, but not everything.  In particular, intermediate results may be carried to a higher precision, which can have a significant effect on the final results.  Early Java ran into this by being even more explicit than IEEE 754, requiring "bit by bit" equality of results.  This destroyed performance on x86 hardware, which carried values in registers to extra precision, and only rounded when storing into memory.  IEEE 754 permits this.
IBM's Power series of chips have run into a simpler version of the same issue by providing a fused multiply and add instruction, which gives you more precision than doing the operations separately can attain.  (It's also faster.)
In each case, there was strong sentiment that if you're writing FP code, you don't expect bit-by-bit equality, and you'll take the extra performance.  Compilers for Power will generate multiply-and-add when they can (you can disable this if you want), and Java was changed to eliminate the bit-for-bit-equality requirement (except in strict_fp code).
In these two cases, you got *more* precision - but if you go back to the old CDC machines - the champions of big numeric computations for many years - their FP arithmetic wasn't as precise as it could have been - but that was considered a worthwhile tradeoff for increased performance.
I've read some of Kahan's stuff and heard him speak once.  He's made important contributions, but realize that not all numerical analysts agree 100% with his approach.
Numerical analysis deal with several classes of problems, and Kahan has been accused of mainly worrying about algorithms for doing linear algebra.  While these are very important, they aren't the whole story.  This isn't my field, but I gather that as one example, the guys doing interval analysis (where you try to compute a tight bound around the true result of your computation - so in general your inputs and outputs are not individual values but intervals guaranteed to contain the actual value) have never been particularly happy with IEEE 754, even though the rounding modes were supposed to help them.  (I suspect one complaint is exactly that they are *modes*, not separate instructions, so if, as in interval arithmetic, you want results with different kinds of rounding, you end up switching modes all the time - which kills performance.)
The whole non-standard arithmetic business - Infinity and NaN - helps with some kinds of code, adds complexity to other kinds of code.  (I mentioned earlier that a bug I've seen repeatedly is combining some vector of measures M with a weight vector W and assuming that when you set some Wi to 0, you've "knocked out" the corresponding Mi.  This is true *only as long as Mi is finite*.  Once NaN and Infinity enter the domain of values, you have to add a special check each time you do the inner product - which in many algorithms is right at the center of all your loops.)
It's easy to say that "getting the wrong answer faster" is pointless, but in numerical work, you pretty much *always* get "the wrong answer" because the "right" answer requires infinite precision.  Even as simple an equation as 3x=1 has a root that cannot be represented exactly in any binary FP system.
So the issue isn't rightness or wrongness, it's being able to understand and control what the bounds of the wrongness are and get them small enough for the result to be useful.  Once you've done that, performance is everything.
                                                        -- Jerry

@_date: 2015-11-08 14:35:28
@_author: Jerry Leichter 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
This is a bizarre characterization.
There are three different concepts here:
1.  A MAC - Message Authentication Code - attests that the bytes delivered were the bytes sent.  Since everything about a MAC is public, anyone can take a MAC-protected message and replace it with a different message that's also MAC protected.  So in and of itself, it's a rather limited form of protection.  But if you combine it with encryption, it can prevent alteration of messages.
2.  A Keyed MAC is a MAC that incorporates a common key shared between sender and receiver.  To either compute or check the authentication code, you need to know the key.  Thus, a check shows not just that the message wasn't modified since the authentication code was computed, but that the code was computed by someone who knows the key.
3.  A signature algorithm has a separate signing key - which can only be used to generated a signature - and verification key, which can only be used to verify a signature produced by the corresponding signing key.  The signing key is private; the verification key can be public.
The most salient difference between a Keyed MAC and a signature is that in order to convince a third party that something was produced by a party who had inside information, in the Keyed MAC case, you have to reveal the secret key, rendering it useless.  In the signature case, the verification key is already public (or can be made public at no cost).  Also, of course, for the same reasons, you need a single signature key - but one keyed MAC key per peer you need to talk to.
In practical terms, signatures are much more expensive to compute than Keyed MAC's, and unkeyed MAC's are even cheaper, so a good design will chose the cheapest that's strong enough to provide the required properties.  A signature isn't "more permanent" than a Keyed MAC.
                                                        -- Jerry

@_date: 2015-11-08 16:37:20
@_author: Jerry Leichter 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
Well, we really are off into the woods here.
The paper to refer to is Extracting the summary of the results:
Yes, the encryption functions used in the proof of *generic* insecurity are rather artificial and would never be used in practice.  But the power of the result should make you at least concerned.  Conversely, since we have a proof of security for encrypt-then-authenticate, unless you have some specific reason to use authenticate-then-encrypt - why not use it?
Note that the state of the art has moved on, and we now have *specific* "E&A" modes that are provably secure, and do less work in general than the separated modes.  (There are issues with complexity of implementation resulting in brittleness with some modes, and with patents for one of the best.)
Too often in this field, we come down to absolute statements:  Do this.  Don't do that.  The story is often more subtle than can be captured in such statements.  That doesn't make them wrong - just incomplete (and *sometimes* wrong).
                                                        -- Jerry

@_date: 2015-11-09 09:15:38
@_author: Jerry Leichter 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
You know, I've seen good arguments for use of theory and proofs in cryptography.  And I've seen good arguments that the whole proof enterprise as it currently exists today is of little value.
But this is the first time I've seen an argument saying "let's pick the parts of the theory we like and ignore the parts we don't like".
"Practically pseudorandom functions/permutations".  Ah, now there's a deep concept.
                                                        -- Jerry

@_date: 2015-11-10 00:15:47
@_author: Jerry Leichter 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
That link is the fifth hit for a Google search for "Encrypt Then Mac".  If you can't get to that site, I'm sure the search will show you some other place to find the relevant information.
                                                        -- Jerry

@_date: 2015-11-16 12:54:37
@_author: Jerry Leichter 
@_subject: [Cryptography] ratcheting DH strengths over time 
I think you're comparing unlike things, because our knowledge of cryptography is in a funny state.
Our asymmetric cryptography is based on well-understood mathematical problems, and there are well-understood, way-better-than-brute-force, attacks on them.  Security for these algorithms is based on (a) assuming no significantly better mathematical attacks will arise; (b) setting the key sizes and other parameters so large that even the best known algorithms are too slow to be practical in some specific time frame.  But the growth rate for these attacks is not very fast - certainly not relative to the growth rate of encryption and decryption - so over time any fixed size key will become vulnerable.
In this case, increasing the key size makes sense.
Our symmetric cryptography, on the other hand, is based on algorithms that we believe cannot be attacked using anything significantly better than a brute-force attack.  In this case, even 128-bit keys are pretty much beyond, not just current technical capabilities, but any technical capabilities we can reasonably imagine.  256-bit keys are immune to any brute-force-attack that's even vaguely consistent with the physics we know.  Meanwhile, encryption and decryption with 256-bit keys is very fast on existing hardware.
If there's an attack on our symmetric crypto, it will be through some kind of surprising breakthrough.  This hasn't happened with any fielded encryption algorithm in modern memory, though it *has* happened, repeatedly, for one-way functions.
Given this reality, there's really nothing to be gained by increasing key sizes, or any other security parameter, for symmetric crypto and similar algorithms.  It's either a waste of effort since the current key sizes are already forever out of reach; or useless against an attack against the algorithm itself.
We could certainly put DH or RSA or any of the ECC algorithms into a similar state by choosing very large security parameters - e.g., 1-Mbit keys.  Unfortunately, we can't do that because it would put actual *use* of the algorithms out of practical reach.
                                                        -- Jerry

@_date: 2015-11-16 14:58:10
@_author: Jerry Leichter 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
Of course you can.  Just forget about public key cryptography!
Public key solves the "any-any" communications problem, where each pair of speakers needs to have its own shared key.  It also solves the introduction problem, in which a node may need to talk to a new node it knows nothing about.
Neither of these is relevant to a SCADA network.  SCADA elements talk to controllers.  A controller can easily keep track of a unique key per element.  A element only needs the key to talk to its controller.  Build the things with 256-bit AES keys and your vulnerability is to someone coming up with a break in AES.  How to protect against *that* ... I have no proposals.
Burning a unique key into each element is no big deal; nor is registering that key when the element is itself registered.  Or you can create and install a key during registration.
SCADA elements don't spontaneously get bored and start talking to friends out on the Internet.  :-)  Even if someone does get physical control over an element - the worst they can do is manipulate that element.  The key stored in it can be rendered difficult to extract using well-understood techniques, at which point getting physical control gives you ... the ability to manipulate things you could have controlled anyway, given physical control.
If you need to do anything more sophisticated than simple point-to-point controller/controlled element communication, a Kerberos-like system would be very appropriate.
I know of an installation that does something like this with secure video conferencing stations.  They are designed to wipe their keys after any but a minimal power outage.  Yes, after a big power outage, you need to send technicians around to set up new keys, but that's considered a good security tradeoff - a stolen device has no special access to the teleconference setup.  Sometimes clever technology alone isn't the answer....
                                                        -- Jerry

@_date: 2015-11-16 15:34:58
@_author: Jerry Leichter 
@_subject: [Cryptography] ratcheting DH strengths over time 
[Funny, I've often argued the opposite side on this question.]
I don't believe anyone ever saw RC4 as in a class with modern block encryption algorithms.  Little oddities were found pretty much as soon as the algorithm became public.  It took a while to turn them into practical attacks.
DES has been subject to public attacks for forty years now.  (It was subject to more attacks internally at IBM, and by NSA (take that for what you will) for several years before that.)  To this day, there are no attacks significantly better than brute force.  That's quite a track record.
IDEA has been around for 25 years.  The best known attacks reduce the complexity from 2^128 to about 2^126.
Skipjack, the encryption algorithm proposed as part of the infamous Clipper chip, has been under public attack for about 17 years; the NSA claimed they had done 10 years of vetting before declassifying it.  No significant attacks have been published.
AES has been around for about 15 years, and is probably the most attacked cryptographic algorithm in existence.  (Well, maybe after DES.)  The best attack   is from 2^128 to 2^126.  (This is the same biclique attack that gets about the same advantage over IDEA.)
None of this *guarantees* anything.  Someone could come up with an attack that demolishes some broad class of cryptographic algorithms tomorrow.  But as engineering/risk bets, modern block algorithms are pretty good.
Hash functions have indeed been problematic.  Note, however, where attacks on them are the most hazardous:  When they are used as part of signature algorithms.  Eliminate the signatures and you don't need the hash functions.  It's not clear why a signature is needed in a SCADA system using shared keys.  A MAC is quite enough to guarantee authenticity, and we know how to build those from a symmetric encryption algorithm.
The fancier cryptographic modes that try to provide too much have gotten hit.  The basic modes - CBC, CTR - continue to provide the guarantees they always did.  You do have to be careful about understanding *exactly what* they guarantee.
If you can determine that your current cryptographic mechanisms are become weak long enough ahead of time that you can trust them to deliver an update ... a solution is, in principle, possible.
Against a "0-day" attack, only rip out and replace can possibly work.
                                                        -- Jerry

@_date: 2015-11-18 15:39:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
Indeed.  Consider expiration dates on drugs.  At one time, these were based on actual estimates of when the drugs would go bad.  Some drugs came with no expiration dates.
Then the drug makers discovered that (a) retailers and other distributers of drugs could not sell drugs past their expiration dates (often this is legally enforced, but in any case customers won't buy them; (b) even people who have the drugs at home will discard them when the expiration dates pass.  There are no binding definitions of what makes a drug "expired".  So the result is just what you would expect:  Expiration dates are placed closer and closer to the present, forcing distributers and customers to replace what may be perfectly good drugs with newer ones.
The US DoD, which buys drugs in huge quantities, did a study a number of years ago and determined that the could safely keep most drugs for twice the lifetime the manufacturers claimed, saving huge amounts of money.  The multiplier is probably significantly higher - it's rare to see drugs with an estimated lifetime of more then two years these days, even over-the-counter drugs that are simple, stable compounds.
We've already seen printer manufacturers include chips in their toner cartridges that declare the cartridge "used up" when there's still plenty of toner in there - which the printers will refuse to use.  Do we really want to encourage more of this kind of thing?
(Not to mention the blowback when a thermostat declares itself "obsolete" and shuts down in the middle of a blizzard and someone freezes to death.)
The IoT is introducing a huge variety of new risks which we don't now how to manage - but forced obsolescence is not the solution.
                                                        -- Jerry

@_date: 2015-11-19 08:23:48
@_author: Jerry Leichter 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
The problem is we also have plenty of examples "done wrong".  Printers being a great example.  Lest you think this is only something unsophisticated home users get hit by, there's an article out there by an owner of a printing service who suspected - and proved - that the multi-hundred-dollar toner cartridges in his multi-ten-thousand-dollar digital press were claiming to be empty (shutting down the equipment) when there was still plenty of ink in them.  In fact, they were about a third full.
Cars and their service indicators are actually interesting for a different reason:  The fancy automatic "service required" indicators have only been around for 10 years or so.  Before that, service stations or oil change stops would give you a little sticker showing you when your next service was due.  They typically used a service interval half what the manufacturers recommended.  But who reads the manual that comes with their car?  By building the recommendation into the car's interface, manufacturers pretty much eliminated this little game.  (Not that they did it out of altruism:  When you get your service interval from a sticker, you'll probably go back to whoever stuck that on your window.  If you get it from the car itself, you're probably more likely to bring it in to the dealer.)
Would anyone deliberately build a thermostat that shut down the heating system when it expired?  Certainly not.  But would the designer put much thought into what exactly happens when something they sold 10 years ago reaches its expiration date?  Nah.  Edge case, the guy has to get a new one anyway - who cares what it does.  Who can justify the extra cost to make sure there isn't some odd transition - say, if the owner tries to reset it by turning it back and forth between A/C and heat mode a couple of times - which leaves it stuck off?
Expiration because the cryptography in the device "may no longer be good" is really a dumb idea.  Sorry.  It fails in way too many ways.
Failure of cryptography is only weakly a function of time.  We have cryptographic primitives (e.g., AES) for which there is absolutely no known correlation with time.  If it fails, it will fail because of some new attack.  It's only the various public key systems where the work factor is low enough that we use key lengths today that will be too short, even with current attacks, in the reasonably foreseeable future.  And even then ... the future may arrive much faster than we expected (quantum computers).
Thus:  There is no principled way to determine the expiration date.  It will almost certainly prove to be way too early, or way too late.
With no principled approach way to compute an expiration date, it'll be entirely up to the device makers to pick one.  Given that long experience with consumer devices shows that *most* sales are based on initial price, "longer expiration date" is not likely to be a telling factor.  Apple sells at premium prices, but not because its buyers want devices that last longer - it's because they are (perceived to be, if you want to be a cynic) better made.  Kodak tried to sell printers whose explicit selling point was that the cartridges lasted longer and were cheaper to replace; they went nowhere.
*Maybe* you can do this for *certain kinds* of commercial equipment.  But I doubt it. The time horizons for most commercial equipment goes out many years.  (The expected life of a home furnace is 10-15 years.  The boilers in even a small commercial building will last at least 50, probably much more.  The tradeoffs are just different.)
                                                        -- Jerry

@_date: 2015-11-20 12:49:51
@_author: Jerry Leichter 
@_subject: [Cryptography] NY DA Vance's 'Smartphone Encryption and Public 
If you want to argue this to the already convinced - which is who you're talking to in this group - that's one thing.  If you want to argue it to the general public ... there are a couple of fundamental misunderstandings in what you say that will leave holes in your argument that the opposition will drive a truck through:
And they probably require that an unlock key be available to the state.  That's best practice for any commercial entity that provides equipment to its employees.  The employer owns the machine; the user has no privacy rights on that machine.  In fact, I would go so far as to say that it would be gross incompetence for a NY State government agency to issue a laptop and allow encryption on it that it cannot reverse.  (Yes, an employee can install their own encryption utilities.  That would be against policy and in some situations would constitute a firing offense.)
It's a not completely settled question as to whether a CALEA-like law applying to cell phone makers would pass Constitutional muster.  CALEA has it easier because the companies it aims at are already highly regulated utilities.  Apple and Google are not.  But if you got into a debate on this, I guarantee you that your opposition will say "We have a long, long established understanding that regulations to promote safety - and public safety - are legitimate.  Why do you think there are brake lights on your car?  This is just another public safety regulation."
Riley says that the police can't access the information *without a warrant*.  Vance is careful to say that he wants access *with a warrant*.  The opposition here is not stupid - they've been arguing that they don't need a warrant, but now that the Supreme Court has spoken, they are falling back from a lost position.
You started off this item by talking about the courts.  Keep in mind that except in very unusual circumstances, courts do not decide matters like "allowing access to encrypted phones is/is not riskier than allowing fully secure encryption".  This is exactly the kind of "finding" on which they defer to the legislature.  Note that most "findings" by legislatures are based on ... nothing beyond what will sell well politically.  Nevertheless, these kinds of tradeoffs are generally left to the legislatures and the political process.  (Which *in principle* is exactly as it should be.  The problem is not with who is called upon to make such decisions - it's that they are doing such a piss-poor job of it.)
There is absolutely nothing in this proposal requiring that makers bring back old versions of the software.
What he seems to have in mind is that, just as in the case of FDE for commercial laptops, there be a secondary key that only the manufacturer has access to.  In a perfect world, where only the good guys ever demand access, and only for good reasons; and the keys are kept forever secure; this would be a great solution.  Unfortunately that's not the world we live in.
Sad to say, countering fear with understanding is usually ineffective.  Countering fear with fear works better.  It's important to keep pounding on all the data that's being leaked, every day.  Let people know that when someone like Vance says "we know Apple can keep the master key safe" he's speaking nonsense.
Again, in an ideal world, there's no reason why that should be true.  The special access could require opening up the phone and wiring in to something inside it.
Again, you need to convince the general public that the real world is not like this.
These are completely separate issues.
Data at Google *is* encrypted; that I can tell you from personal experience.  I have no direct knowledge of Apple, but data there is almost certainly encrypted as well.
The problem is that people want remote access to their data, and they want some of their data to be processed remotely as well.  In many scenarios, this requires that the cloud provider be able to access the data.  In some cases - where the data is accessed remotely but has to be decrypted locally - the provider might only have access *while the user is connected and has provided a password*, but that's enough.
Cell phones can protect the data in them because they only need to process it internally - and even then, they need specialized hardware to keep stuff secure in the case of a physical attack.
Actually, it's very relevant.  If we provide access for the US government, every other government in the world will insist on getting access as well.
The courts disagree with you.  After Enron's massive destruction of documents in anticipation of the Feds coming after them, the law was changed so that destruction of data when (I'm going from memory here) you know that it might be evidence of a crime, and where there is any kind of investigation of a crime to which the data might be relevant, even if you don't know about it, is itself a crime.  The law was *intended* to cover cases of corporate malfeasance, but of course prosecutors have now tried to extend it as far as they can.
Exactly where the lines will end up being drawn isn't yet settled, but the excuse "the computer made me do it" won't hold up in the future.  There will be situations in which you *may not* destroy data - and your computer had better be configured to do it.  (This has long been the case *once you've been informed of the existence of legal processes that will require you to hold some data.  What's different now is that you don't even have to be informed.)
So you've just proved that in some undefined cases, you *may not use* forward security - unless you log the data.
This is the dumb thing about all these proposals:  The bad guys will easily get around them.  It's the average citizen who's most exposed.  This needs to be explained again and again, so that people understand it.  Distasteful as it is to say it, the NRA understands this kind of thing extremely well.  No matter how tailored a law regulating guns useful to bad guys might be, they will attack it as "the Feds are trying to take away John Q. Public's guns".  These kinds of arguments, done right, are effective.
BTW, if you find yourself in a debate with one of the proponents of this stuff and he uses the recent argument that "Apple and Google are just doing this because they make a profit from it" - you can hit them hard.  Just go right-wing on them:  "We're a capitalist country.  They make a profit because that's what a free people want to buy.  What are you, some kind of socialist?  You want the government deciding what people are around to buy, for their own good?"  Trying to demonize some of the country's favorite companies by arguing against the profit motive - really, really dumb move by the anti=crypies.  Give them hell for it.
                                                        -- Jerry

@_date: 2015-11-20 15:19:37
@_author: Jerry Leichter 
@_subject: [Cryptography] WSJ: WH wants to meet Techies in DC re encryption 
Let's call government dictating product specifications to companies what it is: Socialism.
Look, the Obama administration actually wanted to do it!  We know what *they* are.  Fortunately a right-thinking Congress wouldn't let them.  I'm sure Bernie Sander's would love the idea.
Let them come for our encrypted phones and they'll be coming for our guns next! Stop them now!
                                                        -- Jerry
[Only half a :-) - get that meme spreading and just watch support for legislation on this matter splinter.]

@_date: 2015-11-21 12:52:31
@_author: Jerry Leichter 
@_subject: [Cryptography] WSJ: WH wants to meet Techies in DC re encryption 
I guess the sarcasm (kind of) alert after my message wasn't enough.
The point of my post had nothing to do with its actual accuracy - just as the point of the attacks on cryptography have nothing to do with *their* accuracy.  Truth and effectiveness in political debate have all too little to do with each other.
Remember when Microsoft used to attack Linux for being anti-capitalist?  Someone - I forget who - proposed using Microsoft marketing techniques against them:  Every time anyone who wanted to help the campaign mentioned "the Windows operating system", they should *always* say "the deeply flawed Windows operating system".  Repeat a phrase often enough and it becomes part of common knowledge.  This is how marketing works; this is how politics works (not that it's easy to draw a line between them any more).
The fact is, LE *is* trying to get the government to dictate how to build the products sold to you and me and all the liberals and all the civil libertarians and all the NRA members and all the Trump followers and everyone else out there.  And they are trying to dictate that they be built in ways that *hurt us as individuals*, in the name of what *they claim* is some greater good.  Stopping that isn't going to happen by cryptographers talking to other cryptographers about what a dumb idea "golden keys" are.  The most effective way of stopping would be convincing those who are the natural audience for the "help LE stop the bad guys" pitch to instead see the issue the same way they see gun control (also pushed as a way to "help the LE stop the bad guys").
This is politics now, hence not really appropriate for this list - though these political decisions will have profound impacts on how we will be allowed to deploy crypto, so *some* discussion is unavoidable.
                                                        -- Jerry

@_date: 2015-11-22 15:52:16
@_author: Jerry Leichter 
@_subject: [Cryptography] Dan Bernstein has a new blog entry on key 
This comes back to Killian and Rogoway's result on DESX (DES with a fixed random whitener XOR's in before an after encryption):  DESX, when the only attack under consideration is a brute force attack, is about as strong as any pre- and post-whitened version of DES (where the whitener changes from block to block).  How to Protect DES Against Exhaustive Key Search (an analysis of DESX) The result has nothing to do with DES as such; it applies to any block cipher.  You can, I suppose, view the inner XOR as making it impossible to identify a successful decryption, though there is more to it than that, since given just the inner XOR with a fixed value, one could guess small pieces of the mask across different blocks.
                                                        -- Jerry

@_date: 2015-11-24 14:03:40
@_author: Jerry Leichter 
@_subject: [Cryptography] Dan Bernstein has a new blog entry on key 
Parenthesis fail.  That should be:
K_1 xor AES(K_2, data xor K_1)
                                                        -- Jerry

@_date: 2015-10-01 22:29:13
@_author: Jerry Leichter 
@_subject: [Cryptography] Hyper-V claims to protect tenant secrets ?? 
You might want to watch the talk a bit before being snarky.  The basics are pretty simple and follow a familiar pattern:  The VM image is encrypted, and the key service will only hand a key to an attested-good hypervisor image running on attested-good hardware.  Of course, the attested-good hypervisor has to enforce an appropriate policy, e.g., not allow anyone to attach a debugger to the shielded VM.
This is just virtualizing the notion of attested boots of trusted OS's.  At a very high level, nothing new to see here.  Actually making it happen takes tons of detailed work.
                                                        -- Jerry

@_date: 2015-10-01 23:00:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Paper check security 
Just what novel issues are involved here?  What's written on a check has never been secret:  You obviously hand it over to anyone you give a check to.  So the security of the system could never rely on the secrecy of any of that stuff.
The physical validators of valid checks - which have grown much more sophisticated recently, just as paper checks are in the process of dying - are funny things.  Consider:  If I hand try to deposit a check made out to me, in general, my bank *has no way of knowing what physical validators should actually be present*.  Only the party that creates the checks knows what it's put there, and they vary greatly.  Oh, it says on the check what validators are supposed to be there ... but if I produce a fake check, I'll obviously produce put in whatever validators I want and write on the check that those are exactly the ones that should be there.
A check is an order from Alice to her bank to deliver money to Bob.  The security of the system has always been based on the bank not delivering the money until it has some assurance from Alice that, indeed, this is a valid order.  In theory, in the old days, the bank could check Alice's signature on the check against the signature card on file.  In practice ... that was almost never done.  Today, the vast majority of checks are computer-generated and have no useful signature anyway.  They are "truncated" - photographed and destroyed - without anyone being in a position to check all the fancy watermarks, microprinting, and other neat paper security features.
What the system *really* works on is trust - and closed loops.  The bank won't deliver money to Bob unless it feels it knows who Bob is, and how to find him and get the money back should Alice later complain that the check the bank accepted was not one she wrote.  In the old days, you or I only got informed about checks the bank acted on at the end of the month - but now I signed up for, and get alerts for any check over some small limit.  Large organizations undoubtedly get the same services, presumably delivered in some standard machine-readable form so that they can match notifications to checks they know they sent out and stop bogus checks immediately.  (Note that putting the money into Bob's account - but not letting him take it out of the system as cash - doesn't put anyone at much risk:  The bank can always take it back.)
The system is hardly without risks, but it's not clear they are worse today than they've been historically.  But the protection isn't on the paper checks, which are barely needed any more - and never has been.
                                                        -- Jerry

@_date: 2015-10-02 06:32:13
@_author: Jerry Leichter 
@_subject: [Cryptography] Paper check security 
You're of course correct.  There are many "old days".
The standardization of checks that came with computerization and nationwide clearing has been around for long enough that people have forgotten just what a check, historically, *is*.  It's just an order to a bank.  In fact, if I remember correctly, legally the only required features were an amount, a signature, and words along the lines of "pay to .  Checks written on pieces of wood were held to be legal and binding.
By now we undoubtedly have other regulations to keep the system running smoothly at the immensely larger and more physically distributed scales that have been so common for decades that no one remembers when "cashing a check" meant getting it physically presented to the check writer's bank.
                                                        -- Jerry

@_date: 2015-10-02 14:29:07
@_author: Jerry Leichter 
@_subject: [Cryptography] Insecure Chip 'n' PIN starts tomorrow 
The model is valid *as long as the relative costs involved remain as they are today*.  There's no inherent reason what that can't be the case for a very long time.  Yes, new attacks emerge, and new defenses are fielded.  Few people remember that for many years, credit cards were accepted with no checks for fraud at all:  There was no effective way to do it, and fraud was low enough that the cost - borne by the banks -  was acceptable.  By the mid-1970's, the cost of fraud became high enough that it became worthwhile for banks to distribute paper booklets of "known fraudulent" card numbers; cashiers were trained to check them.  Initially they came out every week (I think); eventually there were new ones every day.  They eventually became unwieldy; rescue came from the availability of an on-line infrastructure and mag stripes so that checks could be made by machine.
This changed the nature of frauds from "make a fraudulent card and use it for a couple of weeks" to "clone a card and use it quickly before it gets canceled".
The important thing here is that the costs be assigned to the right party or parties.  *In the US*, the consumer is almost never on the hook for any fraudulent charges.  (They are, theoretically, in cases of negligence, but that's so hard to prove that US banks don't really try.)  Again in the US, the banks have tried to shift costs to merchants, but the political culture resists that, so what has actually happened is that the banks have rolled out new security measures and then pushed liability on merchants who don't take advantage of them.  As long as the situation remains pretty much like this, I'm perfectly happy to let the banks and, to some degree, the merchants (remember that some of them are are large and powerful as the banks) make their own security tradeoffs.
The story elsewhere in the world is different.  In England, the banks have been much more aggressive in pushing liability onto their customers - and well-documented abuses have followed.
(BTW, the history of "banks are liable" in the US is interesting.  It was *the banks themselves* that got this principle written into law, way back when credit cards were first introduced.  It was a way for them to sell the idea - to both merchants and customers - that these new-fangled credit cards were "better than cash or a check".)
                                                        -- Jerry

@_date: 2015-10-02 21:46:33
@_author: Jerry Leichter 
@_subject: [Cryptography] Hyper-V claims to protect tenant secrets ?? 
I don't understand this statement.  *Microsoft* supplies cloud services.
I guess you're assuming that you have to use a Microsoft-managed KDC.  Yes, if Microsoft manages your KDC, *and* that management gives it the ability to extract keys from the KDC, then Microsoft Microsoft can give those keys to anyone who is in a position to demand them.  But (a) I see nothing in the design that say Microsoft even has to manage the KDC; (b) "managing" in the sense of making sure the thing keeps running does not give the manager of a properly designed KDC access to the keys it stores.
Yes, the whole theory of "trusted boot" relies on trusting a chain of software.  No matter how you run your MS software, you have to trust some of the stuff they wrote.  Or you have to verify it.  Same as you have to trust the hardware and firmware Intel wrote into the chips, and tons of other software in all kinds of peripherals.  Welcome to reality.
I just don't understand the antipathy and paranoia here.  This is an effort to greatly reduce the available attack surface of machines in the cloud, bringing them quite close to the best you could reasonably hope to achieve with careful design and management of physical hardware you own and control.  It's a much higher level of assurance than all but the best managed systems can achieve.
Frankly, if you want to protect yourself from a serious attack by the three-letter agencies ... you'd better start of with a budget comparable to theirs.  Good luck with that.
                                                        -- Jerry

@_date: 2015-10-08 11:18:16
@_author: Jerry Leichter 
@_subject: [Cryptography] Reproducible results, scientific method, 
Paul Karger used to have a great name for this approach:  Bull in the china shop security.  As in:  You can always buy more china, but the bull is dead meat.
(What's particularly good about this is the way it turns your assumptions around.  Most people think of the bull in the china shop as a sign of ultimate disaster, to be prevented at all costs.  But ... it's not.  Pretty much nothing is really "at all costs".)
                                                        -- Jerry

@_date: 2015-10-11 11:46:58
@_author: Jerry Leichter 
@_subject: [Cryptography] Collisions w/SHA-1 ~$100,000 TODAY 
Iang's boss is a Vulcan.
                                                        -- Jerry

@_date: 2015-10-20 05:40:59
@_author: Jerry Leichter 
@_subject: [Cryptography] Other obvious issues being ignored? 
If you step back a moment, all of these "verify the parameters the other guy gave you" issues require a rather funny threat analysis.  After all, even if all the parameters and everything in the protocol and the algorithms is perfect ... the whole point of the mechanism is to give the other guy access to the cleartext.  He can then promptly publish it to the world - because his machine has been infected with a virus that forwards all decrypted material, for example.  Nothing at all can protect you from that.
The only purpose I can see in checking the parameters handed to you is to help catch errors.  Of course, it would be much, much better if the *sending* code mode those (self-)checks, as it can avoid exposing information:  By the time you've received the message, it's too late!
                                                        -- Jerry

@_date: 2015-10-20 23:40:21
@_author: Jerry Leichter 
@_subject: [Cryptography] Other obvious issues being ignored? 
You make this sound like a criticism of C.
The vast majority of C code is *not* security code, and is helped by such optimizations.  That's why compilers implement them.  C may have had a simple, direct mapping to hardware back in the days of the PDP-11, but today's machines are very different, and people use C not because they want to do low-level optimization of every aspect of the machine's operation, but because it's light weight and can get really good performance for appropriate code.  (Appropriate:  If you want your array operations to be optimized to the limit, you're probably still best off with FORTRAN in most situations.)
So ... I'd turn this around.  The issue isn't with C, or Java, or any other general purpose language.  It's that we really don't have a suitable way to express security-sensitive code.  General-purpose languages will always lean toward satisfying the vast majority of programming needs, which will likely provide a semantics that is not consistent with the needs of security-related code.
I doubt having a whole separate language for writing security-related code is the right approach either.  A better approach would probably be to define an appropriate set of semantic constraints and provide a way to constrain segments of code in an existing language to obey them.  It's easy to give a quick, informal list of requirements - assignments always write through to memory even if the variable appears dead; to duplication of sensitive values; etc.  But that's a *long* way from a good formalization of the necessary semantics that compiler writers can work with across very different kinds of architectures.
I wonder how the NSA writes its security-related code?
                                                        -- Jerry

@_date: 2015-10-21 07:18:33
@_author: Jerry Leichter 
@_subject: [Cryptography] Other obvious issues being ignored? 
Let's distinguish "reason" from "good reason".  The *reason* PGP left the Subject line in plaintext is to support server-side search.  If you want to access your mail on an unmodified server, and are not in a position to store a local copy, search has to be done by the server; bringing messages over one at a time to be searched was, not so long ago, too slow to be useful.  While desktops and laptops today have no problem keeping all your mail locally, we've moved on to phones which also have limited resources.
In the case of the Subject line, the sender can of course put anything he wants.  Knowing it won't be encrypted, he can simply leave it empty.  Similarly, the From line could also be set arbitrarily; as we know, it's completely insecure, a fact which the sender could use to his advantage.
Other header lines are much more problematic.  The Date line you see was added *by the receiving server* when the message arrived.  Nothing to encrypt here.  The To line is part of the envelope information and had to be in the clear as the message was transmitted or it could not have been delivered.  Various routing messages - ReceivedBy and such - were added in transit and so are outside the PGP envelope (and are rarely searched on anyway, though a full-text search will typically see them).
As we've discussed here previously, SMTP mail is inherently not securable.  From the way the recipient is found (through an MX record which can't be bound in any secure way to the sender's intent) to all the metadata tacked on as the message travels, usually by intermediaries outside the control of either the sender or the receiver, the design of the basic transport protocols is inherently insecure.  And if you need to support standard mail servers as well - pretty much required if you want to fit into the existing ecosystem rather than trying to build a whole new one from scratch - well, you're going to have to live with some significant limits.
Are these "reasons" or "good reasons"?  If your goal is secure mail, and damn the existing infrastructure, then these are "reasons" or even "excuses".  If your goal is to do the best you can while leveraging what's already out there, these are "good reasons".
                                                        -- Jerry

@_date: 2015-10-23 07:31:17
@_author: Jerry Leichter 
@_subject: [Cryptography] Other obvious issues being ignored? 
Step back a moment and think about where you're going.
1.  You're saying out loud what's obvious but unacknowledged:  The base of your trust is not in any CA, it's in your browser's code.  Whether open source or closed, browsers are way to complex and change way too often to be effectively audited by any outside team.  All the cryptography in the world can't protect you from attack code within your browser itself.
2.  The theory of PKCS was that everything besides the authority at the top could be changed any time.  The practice is that there are multiple untrustworthy "authorities at the top" and multiple ways to game the system, so we've come up with notions like pinning which give up on the ideas of distributed, hierarchical trust exactly in order to get back security.
3.  There's really no difference between a pinned cert and just being given the public key for the site in question.  People somehow feel that trusting the browser to give you the key is less secure than letting a CA authenticate the site - after all, the browser could lie to you - but see point 1.
4.  The whole point of certificates based on URL's is that it allows you to trust a site you've never visited.  If you've visited before, key continuity a la SSH allows gives the much more interesting assertion that the site you are talking to today is the same one you spoke to yesterday, last week, and three years ago.  You can't get that same assertion in a world of ever-shifting certificates and CA's without trusting the whole PKCS hierarchy each time you connect.  The other efforts out there to improve trust in the system, based on checking whether the cert you got matches what others have been seeing for a while, is kind of a distributed version of the same thing (though in effect it lets you assert that the site you are connecting to is continuous with the one that "we" - all the observers - have talked to in the past.)
5.  Certificates let you make *off-line* assertions.  But, even ignoring the whole issue of revocation, where's the need to make off-line assertions *when you are forming an on-line connection*?
The entire PKCS infrastructure is out there trying to solve problems hardly anyone has - and not solving problems that we all have.  At great cost to all but the CA's, who are making a nice bundle off of the current setup and have every reason to try to block any change.
I've proposed previously on this list that browser makers could simply distribute a list of public keys for the top 100,000 sites.  Forget about all the intermediaries.  Put *all* your trust where it inherently has to be, in the browser; don't *add* more trusted parties as you do today.  You can, if you like, provide *parallel* checking of the list of keys - any number of *additional* parties can sign that list.
Then solve the "trusted introduction to a new domain" problem for the tiny remaining fraction of connections made on the Web each day in a way appropriate to just that.
                                                        -- Jerry

@_date: 2015-10-23 11:21:57
@_author: Jerry Leichter 
@_subject: [Cryptography] Other obvious issues being ignored? 
This grossly understates the problem.  Modern machines *don't match C's model".  There *is* no one-to-one mapping.  Actually, there wasn't really one even when C was designed:  Registers were a fundamental part of the real machines even then, and don't appear in the C abstract model at a all.  (Yes, there has always been a "register" declaration, and way, way back, people actually wrote code that knew how pcc would map things declared "register" to actual machine registers and use that to interface to assembly code.  But that wasn't portable even in those days.)
C is not glorified assembler.  It could kind of be treated that way back in the days of PDP-11's and roughly similar machines, but those days are long gone.  Even then, there were machines that were a very poor match the C - e.g., the CDC6600, where getting good performance required planning out memory accesses and functional unit usage in ways that could not be expressed in C.  (Today's machines have closely related issues but in a much more significant way.  On a 6600, a memory access cost you approximately as much time as three Boolean operations; a branch at least four times.  The analogous numbers today can be in the tens to hundreds depending on what level of cache you hit.)
Yes, the first requirement of crypto code is that it be correct.  But way up there on the other requirements is that it be fast enough for its intended use. In many applications, that's a significant constraint.  Sure, you can treat pretty much any machine as having a simple memory-to-memory architecture.  If nothing else, you can write an emulator for such a machine, and write all your crypto code to the emulator's interface, rather than the underlying machine's.  (You could argue that using Java byte code with a pure interpreter - disable all the JIT stuff - is one way to get there, though even the base Java model has unexpected complexities, particularly in memory synchronization.)
Will the result run fast enough to be useful?  Try it and find out.
                                                        -- Jerry

@_date: 2015-10-24 09:39:31
@_author: Jerry Leichter 
@_subject: [Cryptography] letter versus spirit of the law ... UB delenda 
And the *spirit* of the language was that C gave you more-or-less direct access to the fastest operation available on the machine; you could then proceed to build other semantics on top of it if you wished.
"The right answer" is what the Standard says you're supposed to deliver.  There is no other definition.
You can argue about whether that's a *useful* answer, but that's a different question.
With the exception of shift, unsigned's have completely defined behavior:  They provide arithmetic mod 2^n.  So according to you arguments, they are a safer choice.
However, in my 40+ years of writing code, I've been burned more than once by unsigned overflow (e.g., in loop bounds), but I can't recall a single case of being burned by signed overflow.
If you stay far away from the maximum and minimum representable values, and avoid division, either signed or unsigned arithmetic is an accurate model of the ring of integers with ordering.  It fits your natural intuitions; things work as you expect.  The problem with the unsigned integers is that it's kind of hard to stay away from the minimum representable value of 0!
When integers were commonly 16 (or even 8!) bits long, you really had to watch what you were doing even with signed values.  But as we went to 32- and now 64-bit integers, in the overwhelming majority of uses made for integers - typically, indexes into arrays, loop repetition counts, and counts of "things" (real-world objects, instances of objects in memory) - signed integer values will never stray anywhere near the boundaries and programs will have the expected behavior.
Yes, there are plenty of attacks based on pushing integers out to their representational boundaries.  These are best dealt with at the edge, before they get deep into the code.  If you don't bounds-check your inputs, overflows are just one problem you're prone to.
And, yes, if you're writing code that has to deal with possibly-hazardous inputs - which is a large fraction of code these days, perhaps even most code - you should have access to some basic tools, like safe range checkers.  And of course you should *use* them.
Integers, unsigned or signed, with any semantics you can define for overflow, are not Z.  Floating point values are not R.  Whether exactly what happens when you push these approximations beyond the limits is fully specified or left undefined is less important, in actual practice, than the likelihood that your code will wander into those areas to begin with.  (And, yes, in security "likelihood" changes drastically because an attacker can manipulate it.)
Over the years, there have been different approaches to dealing with this reality:
-  Google programming standards for C++ forbid the use of unsigned integers, except in very limited special cases:  The view is that the original need for unsigned (to be able to represent larger integer values in a 16-bit word) has long disappeared and what remains is the hazards near 0.
- Ada specified that integer overflow would throw an exception - and the Ariadne rocket crashed because exactly that happened, and there was nothing to handle the exception.  If you don't think about the edge cases, your programming language can't help you when you hit them.
- APL many years ago decided that the difference between integer and floating arithmetic (and, for that matter, booleans) was a pointless complication, so internally a variable may be represented as a bit (mainly relevant for arrays - APL programs often manipulate very long bit arrays as part of selection), an integer, or a float, and the system will convert as necessary.  (Division is always done on floats; if you want the result of an integer division, you truncate the result.)  APL also defines equality as "within epsilon" (for a suitable small epsilon) and carries this through consistently, so in fact (1/3)*3 prints 1.  You can easily construct oddball edge cases but for the most part this works well for computational purposes.  I imitated this semantics in a special-purpose language I designed years ago, and it worked well there, too.
- IEEE floating point adds Infinity and NaN to the represented values and has a well-defined semantics for them.  It works well in many cases, but not in all.  A classic example I've seen as the cause of real-world bugs is computing the inner product of a weight vector with a sequence of values, expecting that a 0 in the weight vector will cause the corresponding value to be ignored.  That fails as soon as a value can be infinite or NaN.
- Java chose to simply not support unsigned arithmetic.  It signed arithmetic is defined to be 2's complement, and overflow/underflow behavior is defined simply as "keep the lowest 32 (64) bits of the result".  While completely specified, this has no reasonable mathematical semantics.  If you do integer arithmetic in Java, stay away from the representational boundaries.  Meanwhile, Java originally choose a super-strict, fully-defined interpretation of IEEE arithmetic, guaranteeing bit-by-bit equality on all implementations.  It turns out that no one much cares for bit-by-bit equality, but they do care about performance - and the original Java requirements killed performance on anything other than SPARC.  So that got dropped. - Modula-3 had two built-in integer types, INTEGER and CARDINAL.  CARDINAL was *not* unsigned:  It was a subtype of INTEGER consisting of the non-negative values.  You could make your own range subtypes.  Exceeding the bounds of the subclass representation trapped.  "Unsigned" was available as a library implementing a class, which had its own operations (like ADD) - no possibility of confusion about what "+" meant, the class implemented arithmetic mod n.  I don't recall if it provided division or even ordering.
- C originally allowed arbitrary algebraic re-ordering as long as it was "mathematically" correct, i.e., of the reals.  This caused grief for numeric programs written to stick carefully to FP, not real, semantics.  Eventually the standard was changed to limit the reordering of FP expressions.  This was explicitly not done for integer expressions because it was felt that too little code made use of such careful handling and significant optimization possibilities in real, everyday code would be lost.  And the fact is that's probably true.  Some of the optimizations being done today arguably have no benefits in any real code, so they get the bad side-effects without the good primary effects - but that's, as always, a "quality of implementation" issue.
- When Knuth developed TeX, he wanted the results to be the same on all machines, which would not be the case even though he was writing in Pascal, which was much more tightly specified than C.  So he wrote an infinite-precision rational arithmetic package that's used for all computations that affect the final result.  FP is used only in computations that produce log messages and nothing else.
- LISP's have had infinite-precision integer and rational arithmetic for many years.  How much they are used in typical LISP programs is another question - but they are there.  (Even Java has an infinite precision integer library - it's the right thing to use when computing monetary values.)  Of course, in each case, you pay significantly in performance.
Many problem spaces, many solutions.  No absolute answers.
                                                        -- Jerry

@_date: 2015-10-24 15:06:53
@_author: Jerry Leichter 
@_subject: [Cryptography] "We need crypto code training" and other 
The view that side-channel attacks are *crypto* problem is like the FIPS approach to certification:  Draw your boundaries, say "all the important stuff is inside this box I made up", show that the stuff inside is secure, and bam - you've got certification. The fact that stuff outside the box is sending all your cleartext to some unknown location on the Internet is just declared out of bounds.
The side-channel attacks we've seen concentrate on the crypto because that's code shared by many applications that deal in sensitive data, so attacking it gets you a great deal of bang for the buck.  It also gets you bragging and publication rights.
I don't recall seeing any publications on power-analysis or timing-analysis or other such attacks against the functional code of any application.  You know, the stuff that actually works on the "red" data directly.
If anyone is exploiting side-channel attacks in the real world - given all the simpler attacks available, it's not clear anyone needs to bother - I'll bet they're going against the soft underbelly - the vast bulk of code that does the actual work, with no one looking closely at it to see if it's vulnerable.
                                                        -- Jerry

@_date: 2015-10-24 15:16:49
@_author: Jerry Leichter 
@_subject: [Cryptography] letter versus spirit of the law ... UB delenda 
Further on the matter of "undefined".  Early computers often left the result of some operations "undefined" - implementations did whatever they did.  The PDP-8 was famous for the "undefined" operations that people tested out and found uses for.  Of course, this made it hard on designers of new implementations of old architectures, as they were often called on to make new designs duplicate incidentals of old ones.
By the time DEC designed the VAX - intending to a whole series of machines conforming to a single architecture - they explicitly specified as much as possible.  For example, unused input bits were specified as MBZ (Must Be Zero) and implementations were required to take a fault if any MBZ bit was passed as 1.
Still, there were some situation in which it was undesirable to pin down the exact results.  The VAX architecture defined two terms:  Undefined and unspecified.  Undefined was essentially as you think of it:  You got results written where you would  get them in the normal case, but the could be anything.  Unspecified meant the machine could do anything at all.  Unspecified results were allowed only in kernel mode.
Sounds like they pinned everything down fully, right?  Well ... a friend of mine who liked to stir up trouble brought up the following question (which brings us back to security and crypto):  Could an "undefined" result include information that the current process was not entitled to - e.g., the value that had been in a register while the previous process was running?  He brought this up in the internal VAX Architecture discussion group (Notesfile, for those who remember such things.)  All the VAX designers hung out there.
DEC was an engineer's company, and people didn't ignore challenges like this.  They quickly determined that the *architecture* didn't forbid this.  But the hardware guys went of and checked every implementation for each of the "undefined" results.  As it happened, in all cases, the actual result was either whatever had been there before, or 0.  Big sigh of relief by all - followed by discussions of how to modify the architecture spec to make sure no such leakage was possible.  We all eventually decided that there was no effective way to add the appropriate language at that point, so we left it alone - with internal guidance to future architects to warn them of the potential exposure.
The Alpha architecture, designed a few years later, inherited the notion of Undefined and Unspecified behavior.  However, the definition of Undefined was pinned down:  There's a defined "user state" of the machine at any point in time, all of it accessible to user-mode code; and the value produced by an Undefined operation must be a function of the user state and nothing else.
The point of all this?  The problem with the C standard is that it's use of "Undefined" is really like DEC's use of "Unspecified".  The reasoning is roughly similar:  For a VAX or Alpha, a kernel-mode programmer is assumed to know what he's doing, and the software he's writing is tightly integrated with the hardware.  The traditional C "contract" is "trust the programmer".  Same idea:  The hardware, the C compiler, and the programmer are assumed to be tightly coupled partners in getting the job done.
Unfortunately, that description covers only a tiny fraction of users of C today.  DEC's notion of "Undefined" things would be much safer and more predictable.  The compiler jocks will claim the resulting code won't be as small or fast ... but that's a claim that needs to be defended on real code, not on artificial examples concocted to show off some new kind of optimization.
                                                        -- Jerry

@_date: 2015-10-24 15:53:27
@_author: Jerry Leichter 
@_subject: [Cryptography] Other obvious issues being ignored? 
The C community has always relied on "quality of implementation" as a backup to the standard itself.  In effect, it's adding social pressure to legal restrictions.  This is generally a good thing, as some things that are pretty obvious to all practitioners are hard to get down in the formal language of a spec.
I haven't seen anyone mention that on this thread.  In fact, it seems to be a concept that's disappeared from public discourse.
What the paper you're citing proves is the gcc's "quality of implementation" these days is poor, and that people should choose another compiler.  In fact, many people long ago abandoned gcc for llvm-based compilers.  (I don't know how they are doing these days on the handling of these corner, but sometimes important, cases.)
It's also worth pointing out that just because some behavior is left as undefined by the standard doesn't mean it has to be left undefined by an implementation.  An implementation is free to document exactly what it will do in each of these cases.  I would say an implementation that did so, *and* chose "reasonable" behavior, would get some high marks on "quality of implementation".  (Not that I can think of any compiler that chose this route.)
                                                        -- Jerry

@_date: 2015-10-24 16:35:08
@_author: Jerry Leichter 
@_subject: [Cryptography] Other obvious issues being ignored? 
We've always been in that domain.  Who knows what's actually in your hardware?
It's all a question of risks:  Trying to pin them down, trying to determine who you are trusting (recall the NSA definition of "trusted party":  Some who can break your security).  At no time, in no place, has anyone had absolute security.  Monarchs who relied on trusted praetorian guards have been assassinated by those very same guards.
Audits are important; program proving techniques are important; cryptography is important.  But the search for absolutes is pointless.  Build your own systems from NAND chips, write your own code from the ground up - and you're still vulnerable to someone who slips a camera into your home to watch your keyboard as you type.  If the NSA can't protect itself against an Edward Snowden, what chance do any of the rest of us have?
Provable program obfuscation may be on its way, but *practical* program obfuscation has been here for many years.  Suppliers of software will have to build trust the way suppliers of physical objects and services have built trust.  And sometimes that trust will be violated.
Are we safer or less safe if people can hack into and analyze car computer systems?  Right now, the debate is over the *permission* meaning of "can".  If we use the *possibility* meaning instead - well, in theory, if no one, black hat or white, can hack in - the world's a better place.
                                                        -- Jerry

@_date: 2015-10-25 08:41:43
@_author: Jerry Leichter 
@_subject: [Cryptography] letter versus spirit of the law ... UB delenda 
Way back when, when pcc was the only C compiler, "portable C" meant "compiles with pcc".  Since the "portable" in pcc extended to very few machines and exactly one operating system, it also meant "runs on PDP-11's" and maybe a few other machines under Unix.  This was the era when it was "portable" to assume that *(char*)0 == 0 because "all machines" mapped the bottom of memory and had a 0 word as the first word of memory.  (Yes, programs were really written on that assumption.  It allowed you to do things like pass NULL when you wanted to pass an empty string:  Code would look for the '\0' and find it right there at location 0.  This habit has cursed C and even C++ ever since.)
Later we went through the "all the world's a VAX" era, when "portable" code was anything that compiled and ran on a VAX.  This was followed by the "all the world's a Sun" (initially 68K, later SPARC).  When (quite a bit later) SunOS had been renamed Solaris and ported to x86 (I'm talking about the original port, done some time in the 90's, years before Sun started building x86-based hardware), code sometimes required work to move from Solaris SPARC to Solaris x86, even though they were nominally "the same".
The first C Standard really served to write down minimal assumptions about C compilers.  It was common knowledge among C programmers that you couldn't write any non-toy code entirely in the language defined by the Standard.  You referred to the Standard as a base; you referred to the documentation of your compiler and OS to get real work done.
I've been pretty much out of the C programming world for, hmm, close to two decades now.  I'll still write a small C tool now and then, and I have a bunch of older tools that I modify as new needs arise, but my heavy coding has alternated between C++ and Java.  I haven't even looked at the more recent versions of the C standards.  It seems there's a feeling out there now that C is supposed to give you a way to write truly portable code, and that the way to get there is to write to the C Standard.
It never was this way, and it was never intended to be this way.  C was a low-level language, intended to get you close to the hardware.  The original definition of "int" - I don't know if the current Standard retains it - was something along the lines of "a signed integer that has the best performance on the targeted hardware".
*If* you can manage to stay away from the undefined edges of data representations (which is where most "undefined" behavior lives), you can actually write useful code that's portable to all reasonable implementations of the language.  Back in the day when you ran code in your own name on your own data, this was a reasonable assumption.  In an era where every piece of code seems to get exposed to the Internet eventually, and where the data you work on is handed to you by attackers, things are not nearly as simple.  Note that developers of packages like SafeInt are *deliberately venturing out into the dangerous areas*:  You don't really need such libraries if you're writing code for 1990's usage model.  You need it when not only is someone out to make your program fail, they are out to make it fail in a way that lets them attack you.
Back in the early 1990's, I taught a compiler writing class.  I implemented a simple Modula-2 compiler with parts left out; students had to find the (deliberately introduced) bugs (extra points for finding bugs I didn't know about!) and fill in parts I hadn't done.  The compiler was written in C.  I would work on it on my VMS VAX at home, bring it to campus and compile and run it on my SunOS (Solaris?) Sparc workstation, then give it to students who would often compile and run it on their MSDOS x86 desktops.  It took discipline, but, yes, I could and did write code that was portable and correct across these very different environments.
Would it have been safe against code deliberately trying to attack it?  Almost certainly not.  That was simply not an issue, and I certainly didn't even consider the possibility.  (I still have the code around somewhere.  Peter, if you want I can dig it out and see how it holds up against modern analysis tools.)
Anyway ... as an old-world C programmer, I find the discovery that you can't, based on the C Standard, write completely portable code *even in a non-adversarial environment* kind of amusing.  My main response is - so what else is new?
C is what it is.  It promises certain things, which it delivers pretty well.  It doesn't promise others.  Don't be upset that it doesn't deliver them.
Now, as to what an alternative might be ... that's a very, very interesting question. The most portable language I know of also dates to Bell Labs in the mid-1960's:  SNOBOL4.  SNOBOL4 was implemented in two levels:  There's SIL (SNOBOL4 Implementation Language), a pseudo-assembler for a memory-to-memory fake machine, originally implemented as a set of assembler macros on a particular target; and the compiler/interpreter, written entirely in SIL.  Really, really the same whether back in the day you ran the original IBM360 implementation or the CDC6000 port; or today, when there's a version based on implementing the SIL operations in C.  But ... after all these generations of progress, *still* slow.  (That doesn't mean it isn't useful. There have also been other, more efficient, implementations - all but one highly specific to particular machines and OS's.)
Languages can have multiple desirable attributes:  Portability, safety, capable of extreme performance, capable of getting "down to the hardware" for OS-like functions, ease of analysis (formal or otherwise), tight control of memory allocation, expressiveness (whatever that means to you - it's almost certainly not exactly what it means to me) ... you can add to the list.  There is not, and almost certainly never will be, a single language that can cover all the bases.
In the case of a language like C, the definition in the Standard is *still* not enough to write secure code.  (Whether, today, it's enough to write *useful* code, I don't know.)  The combination of C *with a good compiler and run time environment*, on the other hand, *can* be used to write secure code - though it remains a very hard job, because (by design!) C leaves most of the necessary work (array/pointer bounds checks, overflow checks, and memory management being the largest portion) to the programmer.
What this discussion makes clear is that C plus gcc is not a suitable environment for this kind of code.
If you think the issue is C - suggest a good alternative.  I still have a sweet spot in my heart for Modula-3, but it's long dead.  (Since I never actually did any significant programming in Modula-3, my affection may be misplaced anyway.  All kinds of things look great when you read the documentation; day-to-day usage gives you a different viewpoint.)
As an area of research and even commercial development, "safe languages suitable for systems programming" died about the same time Modula-3 did.  After all, we knew what the OS's of the present and future were/would be (Windows and Unix), and we knew what they were/would be written in (C, some C++).
Today, exposure to the Internet has brought the issues of "systems programming" to almost all code anyone writes; but the area still appears moribund.
                                                        -- Jerry

@_date: 2015-10-25 17:46:49
@_author: Jerry Leichter 
@_subject: [Cryptography] composing EC & RSA encryption? 
Not sure what you're getting at.  The only thing you want to encrypt with a public key is a key for an asymmetric cipher.  Generate two random values, encrypt one with EC, one with RSA; use the XOR of the two as your actual asymmetric key.  As strong as the stronger of the two.
Same goes for key agreement.
                                                        -- Jerry

@_date: 2015-10-28 21:38:56
@_author: Jerry Leichter 
@_subject: [Cryptography] composing EC & RSA encryption? 
It *is* information-theoretically secure.  In sketch:  The secret is the value at 0 of  an k'th degree polynomial.  The secrets are p(1), p(2), ..., p(n).  If you have any k of the p(i)'s, you can determine p and hence p(0).  If you have k-1 values, you can take any value you like for the k'th value and get a different value for p(0).
If you do this over the integers or the rationals or the reals (or even over the complex plane, I suppose), some information may leak because the sizes of the numbers are likely bounded.  So you do it over Z/p for some large enough p, and all values are equally likely.
Adding quantum computation changes nothing at all since the problem isn't in computing the missing value - it's that any possible value is equally likely.
Shamir's scheme implements threshold secrets:  Any subset of k shares gives you the secret, any small subset gives you nothing at all.  Josh Benaloh and I showed many years ago that you could implement any secret sharing scheme that made sense (e.g., any share from collection A and any share from collection B).  The secrets are bigger, but the computation is pretty much as simple and the security remains information-theoretic.  There's a scanned copy at                                                         -- Jerry

@_date: 2015-10-30 06:51:13
@_author: Jerry Leichter 
@_subject: [Cryptography] letter versus spirit of the law ... Eventus 
It depends.  It's possible to structure you entire architecture around failure and recovery by restart.
In fact, Google does this.  At Google scales, the assumption that your hardware won't just randomly flake out as you're running is no longer tenable.  Given that, Google in turn saves money by deliberately trading number and performance of processors and other elements against lower reliability.  So programmers have to develop code on the assumption that the hardware could die between any two instructions.  Having decided that ... you might as well apply the same approach to software faults.  It's not that you ignore them; in fact, you try to detect them - e.g., check that parameters that are not supposed to be null are, in fact, not null.  But the response - provided directly by the checking primitives in the standard libraries - is to simply crash the program when the check fails.  The libraries are full of primitives with names like "openFileForReadOrFail()" - the "OrFail" means "crash the program if you can't do it".
Writing repair/recovery code within functional elements is discouraged.  Recovery is left to higher-level mechanisms.  As a simple example, if one of the mappers in a map/reduce dies, the map/reduce framework will eventually notice it hasn't delivered its results and pass its work off to someone else.
In some ways, you can compare this to the difference between the early designs of database systems and file systems - which put enormous effort into pre-validating requests, planning locking strategies to prevent deadlocks, running elaborate repair and recovery strategies (fsck, in the file system case) - and the strategy that most such systems use today:  Do work in transactions that, when things go wrong, just bail and try again from the top.
I'm not recommending this approach as the right one *in general*; it has its own set of advantages and disadvantages, appropriate and inappropriate areas of application.  If you come to it from a more traditional programming environment, it takes time to internalize the design approach necessary to make it work.  But you should be aware that the alternative is viable in many cases.
                                                        -- Jerry

@_date: 2015-10-30 07:04:19
@_author: Jerry Leichter 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
I have no idea exactly how gcc actually uses this annotation, but you might want to read   This paper introduces the notion of "non-void" (Eiffel terminology for "non-null") reference types.  A non-void reference type is a reference type which does not include "void" in its range.  The compiler guarantees that a variable of this type cannot ever be observed to contain void.  If you think about it, this is really no different from guaranteeing, in a safe language, that a reference cannot point to random memory, or to an object of the wrong type.  This is a *compile-time* check:  The compiler doesn't add run-time checks for void; there would be no point, as it can prove they will never actually find void.
Surprisingly, once you decide this is the right way to do things ... it turns out not to be that hard.  The Eiffel collections libraries were modified to consistently take and return non-void references pretty much everywhere.
Java these days has a standardized  annotation which is intended to mean the same thing, though enforcement is generally left to lint-like code checkers.  I've used C++ compilers with similar annotation and checking.  Typically, their response to checking whether a non-null-typed reference is null is to complain about dead code.
                                                        -- Jerry

@_date: 2015-10-30 19:52:35
@_author: Jerry Leichter 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
This is a nice theory, but in practice, probably the most common cause of failures of Java programs "in the wild" is an NPE (NullPointerException).
The fact is, in most cases you don't need or want to represent the fact that some reference "isn't determined".  Interfaces that return a reference to an object, or a null if the object isn't there, can be written not to do that, eliminating all the pieces of code that *know for certain* that the object they looked for is there, so they don't even bother to check for null - and promptly die.
Yes, there are uses for null; and there are also alternative ways to represent the fact that some value may not be there, which force the programmer to be aware of, and somehow deal with, that fact.  (You can compare this to returning an error code or special value on failure - which every caller must explicitly check for - to throwing an exception, which a caller must explicitly catch or his program will die at that point.)
Strong type systems ensure that the data you have supports some set of operations.  Allowing null in says "well, the type system says the operation can be invoked, but there's the run-time special case you have to watch out for".
If you haven't tried programming with "not null" as a checked part of the type system ... you should.  You'll be surprised how much it clarifies the code and makes it safer.  (It's akin to maintaining const-correctness in C++.)
                                                        -- Jerry

@_date: 2015-10-31 23:57:29
@_author: Jerry Leichter 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
============================== START ==============================
This is very much in keeping with the original "spirit of C", one part of which was explicitly "Trust the programmer".
Now, all kinds of things made sense - or at least sounded like they made sense - back in the 1970's when C was originally designed for use by highly skilled, highly experienced programmers.  Even they got in trouble - but rarely.  This was an era where the designers of C could seriously argue that C was like a fighter jet.  If you wanted a Piper Cub - safe, easy to control, appropriate for a neophyte - there were plenty of other languages available to you.  C "got out of the way" of the serious programmer.
C programmers operate under the illusion that because they are writing in the same language as some of the legendary programmers of lore, they, too can write perfect code, and they don't need some compiler complaining a them.  (In fact, if you look at the code in early Unix or in some of the early tools, you'll find that in general it was deliberately written very conservatively, by people who understood their own limits and used various conventions and other techniques to do what compilers today, running on vastly faster machines using much improved algorithms, can do easily.)  One of the early Unix guys - I forget which - has a line something like:  Everyone knows that debugging is much harder then coding. So if you write code to the very limits of your ability, by definition you can't debug it.
It appears the the gcc developers continue to operate in the world of the 1970's.  Nice little utopia - unfortunately, hardly anyone actually lives there.
                                                        -- Jerry

@_date: 2015-09-10 09:56:53
@_author: Jerry Leichter 
@_subject: [Cryptography] 
=?utf-8?q?ying_Has_One_Flaw?=
Apple is producing something for mass consumption.  Checking against some out of band information, transmitted in some unspecified way?  Not the kind of thing that's compatible with mass usage.
Besides, the obvious approaches *don't work*.  The Apple model has one public/private key pair *per device*, not *per user*.  The private keys never leave the device that created them; there's no way to transfer your old private key to your new iPhone.  The Apple server sends you a set of public keys for all the devices the user you want to talk to has registered with Apple.  The attack would be for the Apple server to send an *additional* key that the FBI (say) owned.  A check that the key your correspondent is using is in the set of keys you received will succeed!  What you would need to check is that *all* the keys you received are for devices your correspondent knows about and approves.  This is way beyond what almost anyone would be in a position to check.
If you've previously talked to Bob, your system *could* tell you that the set of keys the server returned for Bob includes a new one.  But this would be a very frequent occurrence - replacing a phone, getting and registering an additional device, even some Mac OS version upgrades add new keys.  Virtually all the time, this alert would be a false alarm.  Not only would it annoy users - but in practice, with no easy, safe way to verify the new keys - you can't do it over the iMessage channel you're trying to establish! - who would it actually help?
It's easy to come up with solutions if you don't actually consider all the details of the problem.  An example:  Apple's Facetime chat service provides end-to-end encryption, while Google's Hangout's don't.  People jump on Google for this, but there's a really solid technical reason behind it:  FaceTime is 1-1; Hangouts are n-n broadcasts.  This requires level-adjusting, mixing, echo-canceling, and other processing of the audio so that n-n conversations actually work well.  (Sometimes playing games with the video may also be needed).  Hangouts do this in the server - which requires the server to have access to the unencrypted streams.  To do it purely end-to-end, each endpoint would have to maintain a connection to all the other endpoints and do its own mixing and other processing locally.  This impractical for network bandwidth reasons, if nothing else, even for a fairly small number of people on a single call.
In fact, this is very roughly analogous to what happens in iMessage.  Its connections are "1-1" in terms of people, but "n-m" in terms of devices.  Text messages don't need mixing and level balancing and such, so group chats are no big deal - but just the key distribution for those n and m endpoints requires something beyond pure endpoint-to-endpoint encryption.  That "something else"  adds usage complexity, opens holes, or both.
What Apple has done, and is defending in court, is by no means trivial.  In general, *existing* business records can be obtained through relatively simple court proceedings.  If Apple had access to unencrypted streams, they could easily be required to turn them over, no muss, no fuss.  The same would occur if Apple had access to the encrypted messages and the corresponding keys - the LE agency would take the information and do its own decryption.
It's considerably more challenging to force a third party to actively create something they don't already have.  Requiring Apple to change what it actually sends to its customers - which would probably first require them to add flows to their software to even enable such a thing - while perhaps possible, is harder.  For traditional telephone conversations, CALEA required a new law - LE didn't have the authority to simply order the telco's to build such a feature into their hardware.
Law enforcement agencies don't want "harder".  They want the courts involved as little as possible; they want as little paper trail as possible.  They certainly don't want to have to ask Congress to give them the authority to make such demands, should the courts tell them they don't currently have such authority.  They'd much rather find a way to force Apple to simply hand stuff over quietly.
                                                        -- Jerry

@_date: 2015-09-10 15:49:11
@_author: Jerry Leichter 
@_subject: [Cryptography] Apple?s iMessage Defense Against Spying Has One 
I'm not sure in what universe this is "easy".
And you're skipping lightly over the important steps.  In the iMessage world, Alice doesn't send her key to Bob.  She isn't even aware that a key exchange took place.  And, of course, she doesn't need some external, secure, verifiable channel to complete this transfer.  When Alices's phone dies and he gets a new one (hence a new key), are you expecting her to somehow send an update of her list of trusted devices - signed by another trusted device that her receivers already know about, if she has one; or through the magic external channel if she doesn't - to everyone she communicates with?
Encrypted mail and other kinds of encryption have been around for decades.  Hardly anyone uses them.  Exchanging and setting up keys, checking fingerprints ... too complicated, too easy to get wrong.  There are plenty of studies of this stuff, and they've all come to the same conclusion:  Cryptography is easy; usable (and hence used) cryptography is difficult.  Cryptography at large scale (tens of millions of people - and up) is *very* difficult.
I'm willing to bet that the total number of bytes encrypted and transmitted securely by iMessages in the last year - which, given forward security, are now forever secure - exceeds by a large multiple the total number of bytes of email, chat, etc., (not counting government messaging) that were encrypted and transmitted with a comparable level of security in the previous 20 years.  (OK, maybe Skype conversations back in the early days - who knows what the security is now.  Not that anyone on the outside really knew anything for sure about the security even then.)
You can focus on super high security against ever-unlikelier threats for a tiny fraction of the population; or you can build something that provides quite high levels of security for large fractions of the population.  Apple has gone for the latter.
                                                        -- Jerry

@_date: 2015-09-13 17:28:31
@_author: Jerry Leichter 
@_subject: [Cryptography] "Ulysses pacts": better than "warrant canaries" ? 
All a spy agency has to do to get around this is require that the same update be sent to everyone.  The implanted code can determine when it should actually do something.
Stuxnet shows exactly how this has already been done:  It spread all over the place, but (fairly successfully) only activated for the target machines.
If the goal is to attack one or more identified users of the program in question ... the targeting is that much easier.
It's not that this kind of thing isn't worth doing ... it's that it isn't a magic bullet.  (It's particularly effective in cases where the attacker can't get the software provider to actually modify software, but is able to MitM connections back to the provider.  Of course, then it could also MitM connections to the third-party checker ... there's no free lunch.)
                                                        -- Jerry

@_date: 2015-09-13 17:38:49
@_author: Jerry Leichter 
@_subject: [Cryptography] Comey: targeted ads => plaintext access 
The problem with his argument is that everyone who understands email as currently implemented knows that it's fundamentally insecure, at many levels.  The basic design assumes the server has some degree of access to the plaintext of messages.  Targeting ads is one use; so is server-side searching of messages, which most servers today support.
So, yes, no one worries about the effect of security of the ability of the server to read messages that, by design, are accessible to the server as plaintext.
The plaintext itself *can* be protected by using an end-to-end encryption protocol.  That will prevent targeting of ads - and things like server-side searches.  (It doesn't provide the kind of security properties you'd probably really like to have, but that's another story.)
                                                        -- Jerry

@_date: 2015-09-17 07:09:38
@_author: Jerry Leichter 
@_subject: [Cryptography] Comey: targeted ads => plaintext access 
The "anonymous ad" protocol is an attempt to do what we often do in crypto:  Replace a straightforward protocol based on a trusted third party with some complicated protocol that does pretty much the same thing without trusting anyone.  It's always interesting  from an intellectual point of view to do something of this sort; sometimes we learn something from the effort.  Whether it's of any practical interest is a whole other question.
Google today implements exactly such an "anonymous ad" protocol (though it's repeatedly misunderstood).  Advertisers buy the right to send their ads to people who match certain classes of descriptions; they provide the ads, Google forwards them to people who match the descriptions.  Google never provides information about the people to whom the ads have been sent.
Of course, nothing *in the technology* prevents an advertiser from using the tricks described to figure out who has received the ads.  But to do so would be to breach the contract Google has its advertisers sign, and if found out they would quickly be banned from the Google networks.  Google's pretty aggressive about enforcing this.  An advertiser could get away with it on a small scale, but it's unlikely they could do so for long at a scale that would make it worth their while.
You can argue about whether Google does this to maintain user privacy, or to keep their most valuable asset (the database of users and their characteristics) from walking out the door:  If an advertiser could get the contact information for those who match its ads, why bother placing more ads - just contact the people involved directly?  And of course even the privacy aspect can be seen "just as a way to keep people from complaining about what Google does" (though they do a remarkably poor job of explaining to people how this works).  Still, in the end, would you rather trust Google to do the right thing "just because that's what they do" or because their economic best interests align with what you want them to do?
Of course, Google's competitors do the same things, for the same reasons.  They all want to remain in the middle of interactions between their users and their advertisers, even though the advertisers would love to push them out.  You can even see analogous things - for similarly ambiguous and complex reasons - in Apple's App Store:  App sellers are given no information about their customers, and indeed are forbidden (on pain of quick banishment from the store) from initiating contacts with their customers or even providing any kind of direct pathway from customers back to them.  For example, apps can't provide a way to report problems directly back to their makers.
So ... if you replace one piece of the protocol (anonymity in sending the ads out) with an equivalent one with no TTP, it should come as no surprise that the existing "deanonymization" techniques continue to be available and will need a solution.  The existing solutions are all based on a TTP; whether there's some way to turn them with something with distributed/minimal trust is another fine (intellectual) crypto challenge....
                                                        -- Jerry

@_date: 2015-09-20 23:22:21
@_author: Jerry Leichter 
@_subject: [Cryptography] Feedback welcome on autentication/password 
It's a fine idea, but you've been anticipated by many years:  Leslie Lamport proposed this as a way to have one-time passwords back in the 1980's, and implemented as a system called S/KEY - see   The idea was exactly as you suggest:  A user chose a password P, computed Hash^K(P), and gave that to the computer.  When he logged in, he presented Hash^{K-1}(P).  The computer hashed it, noted that it equaled the value it had stored, and let him in.  It also replaced its stored value with the one presented.  The next time, the password would be Hash^{K-2}(P).
To make the whole thing usable, you have to add techniques for resynchronizing if, for example, the connection drops after the user has sent his key but before the computer responds.  What should he present as his password the next time around?
                                                        -- Jerry

@_date: 2015-09-25 07:20:32
@_author: Jerry Leichter 
@_subject: [Cryptography] VW/EPA tests as crypto protocols ? 
This idea has been around for many years in many forms.  One example I first saw some time in the early '70s was a response to one of those clever memes about management that made the rounds:  "You can't manage what you don't measure".  Response:  "You'll get exactly what you measure".
On the more general issue of detection:  This is an issue not so much of measurement as of testing.  There are multiple kinds of systems and system failures (deliberate or otherwise), and testing strategies have to be chosen to match the kind of system and failure that's possible.  Thus:
1.  You cannot distinguish correct from incorrect outputs.  These are rare, but there is one cryptographic element that falls into this category:  Random number generators.  These simply cannot be tested by looking at the results.  (It's interesting that the only example I know of in this class takes no inputs.)
2.  You can construct an input set with the property that if the results are correct on the input set, they are correct everywhere.  There are systems that we assume have this property in the face of *random* failures, and we often implicitly assume it - e.g., with sets of test vectors for cryptographic algorithms.  Of course, in the face of deliberate wrongware, it's easy for the system to give the right answers exactly for the test vectors.
Almost all traditional testing really falls into this category, even if we don't explicitly write down the assumed "indicator set":  It's implicit in any fixed collection of deterministic tests.
3.  The set of inputs that induce failure makes up a substantial fraction of the input space, even though you don't know how to characterize the members of that fraction.  A sufficient number of random inputs (chosen independently each time you run the tests) can give you any desired probability that there are no failures.
Tests like this are done, but in my experience are relatively rare.  They shouldn't be.  In fact, the best tests consist of two parts:  Tests on a fixed set of typically sensitive areas (e.g., for any kind of numeric code, 0, -0 if your system has one, small, medium, large, and "at the limits" negative and positive); and as many random tests as you can manage.
4.  The set of inputs that induce failure is very small and unpredictable.  Unless you can afford to sample pretty much the entire input space, these cannot be detected by testing.  The famous Intel division bug of years back was in this class (though once the causes were understood, related bugs could probably be detected by carefully chosen tests).  Much wrongware falls into this class:  The Ethernet adapter that detects a particular random 16-byte header on a packet and drops the rest of the packet over sensitive code in the driver, for example.  In general, carefully targeted  attacks against hardware fall into this class.
The VW wrongware is actually in class 3:  It does the wrong things *almost always*.  It was able to stay hidden only as long as the wrong kind of tests - tests appropriate to class 2 - were the only ones being applied.  The real lesson is:  Class 2 testing *is only appropriate for detecting random failures* (and not always even then, of course).
There is a real conflict with traditional views of law here.  When we prohibit or require something in law, it's a general principle that it has to be clear *exactly what* is being prohibited or required.  VW could (very weakly, because of other aspects of the law that forbade special cases) argue:  The regulations require certain outputs when run on a defined set of inputs; we provide that, what's your problem?  If you think no one could hope to get away with this - just look at tons of agreements between Telco's and governments, where the agreement "says" the Telco will make (say) broadband available to 90% of its customers in return for a tax break; but the actual *test* is that broadband "passes" 90% of the customers - as in the fiber goes down the street, but you can't actually connect to it.
For an example of a law that goes the other way, the SEC has always refused to actually define "insider trading".  It has some broad and deliberately vague language about improper manipulation of markets and improper use of proprietary information, but that's it.  The argument - for which there is tons of evidence - is that the instant you explicitly write down what's prohibited, the market manipulators will find a way to do something "just the other side of the line".  So you need the flexibility to go after them.  This is one of the few areas of the law that's openly and deliberately vague, and the SEC's track record in going after people is mixed:  Sometimes the courts accept the argument, sometimes they don't.
                                                        -- Jerry

@_date: 2015-09-27 18:32:47
@_author: Jerry Leichter 
@_subject: [Cryptography] VW/EPA tests as crypto protocols ? 
It occurred to me after writing that there is another kind of software to be found "in the wild" that falls into this class:  Any implementation of proprietary cryptography.  And how is such crud sold?  Why by challenging anyone to break it (i.e., find a test it fails).
Deterministic cryptography allows you to separate the algorithm from the implementation.  Testing of the implementation can prove it implements the algorithm correctly "with high probability".  If you're paranoid, you wrap the algorithm (both plaintext/ciphertext and key) in a whitening function, protecting yourself even from attacks that depend on certain rare inputs known only to the attacker.
However, deterministic cryptography cannot attain certain desirable security guarantees.  Those require a randomizing element within the crypto.  Add that, and the system becomes untestable.
No disagreement.  I was specifically talking about *testing*.  The fact is, very little software is formally verified.  Way too little software is even subject to proper static analysis, which is considerably weaker than formal verification - but still useful, and increasingly useful as the tools and techniques get better.
The industry go-to answer to problems with software quality is ... more testing.  While that's fine as far as it goes ... it doesn't go nearly far enough.  And too much of what we do to improve testing remains uninformed about the realities.  My favorite:  Coverage statistics.  At one time, I started a little project, long abandoned due to lack of time, to collect and categorize classes of bugs that would be missed by tests with 100% coverage.  I once tried to use that as a question in interviews of QA people, but pretty much no one could answer it.  I *do* have a question that involves handing an alleged implementation of a simple, fully-defined input-output transformation to a candidate and asking them to construct a thorough test.  Hardly anyone gets that right either - though at least people can make a start on it.
So ... even in areas where testing is appropriate, those who do it don't manage to get it right.  And we know that there are things that even thorough testing cannot find.
BTW, formal verification without a great deal of support is *hard*.  I once wrote a classic buffering algorithm with get's, unget's, and buffer re-fills.  This was at the center of a communications systems, so needed to be very efficient.  It also had to be very safe, as it faced the outside world.  So I devised a set of correctness conditions for the variables defining the algorithm, and proved that each call preserved them.  Several people checked this and agreed that the assertions and the proofs were fine.
And they were.  The code was never seen to exceed the buffer bounds, the various pointers were always in the right relative positions, etc.  But ... there was a bug *anyway*.  It was possible to make a sequence of calls which resulted in get() returning a byte that had not actually received new data (either from the input or from a unget()) since the last time its value was returned.  (That is, some byte value could be returned twice.)  Fixing the bug was easy - but even formalizing what needed to be shown here proved much harder than all the rest of it combined.
                                                        -- Jerry

@_date: 2016-04-05 06:07:17
@_author: Jerry Leichter 
@_subject: [Cryptography] [cryptography] Secure universal message 
In other words, a private key (which can be used for symmetric cryptography) solves a number of problems - attribution, recovery from loss, non-transitivity of trust - which don't even get mentioned in most analyses of desirable characteristics of mail (or other communications) systems, so naturally do not influence the choices made it system design.
Probably because we're still living the dream of the original Arpanet days.  One of the wonderful features email systems provided was the flattening of communications hierarchies.  Anyone could send anyone email.  It sometimes even reached them - Steve Jobs was known for personally responding to email from random customers here and there.
Of course, the dream mainly faded decades ago.  Spammers showed us that open accessibility could have a very dark side.  At those at the top simply interposed the traditional access controllers (admins and such) between them and their "public" email addresses.
The whole idea that you need a way to securely communicate with someone who you've never had any contact with before is mainly incoherent.  If I want to contact "Ray Dillinger" having no previous communications with him whatsoever ... how does it even make sense to ask whether I've reached "the right one"?  I might meaningfully reach "the person using the name Ray Dillinger who listed a contact address on this message" - or "the Ray Dillinger who posts to the Cryptography mailing list" (which in turn I communicate with because years ago I had direct contact with Perry), but neither of these use cases (and very few others) need a PKI.
                                                        -- Jerry

@_date: 2016-04-05 12:45:04
@_author: Jerry Leichter 
@_subject: [Cryptography] Hayden on encryption v. metadata 
Do you want to talk about legality or about reality?
Yes, we need to provide much better protections for privacy in the legal domain, especially in the US - though frankly I don't know any part of the world that's immune.  (I know many European nations want to feel holier than though, but look at what they are doing to themselves post-Brussels.  Fear is a great motivator; unfortunately, what it motivates is rarely based on much thought about consequences.)
But the *reality* is exactly as Dan has stated it:  By using something that keeps you connected continuously, you're inherently making yourself continuously trackable.  Bug?  Feature?  Both!  When it serves you - feature.  When it serves someone who means you harm - bug.  When it serves someone who wants to give you a good deal based on your location outside their store - well, it depends.
I'll agree that Dan should probably not have used the legal phrase "reasonable expectation of privacy", with all the freight it brings with it.  Rephrase it as:  It's unreasonable to expect privacy if you're carrying a continuously connected device around with you and it's a truism.
                                                        -- Jerry

@_date: 2016-04-10 13:44:45
@_author: Jerry Leichter 
@_subject: [Cryptography] More magical encryption thinking from James Comey 
You're missing what he's getting at here (and, granted, he doesn't do a good job of explaining it).
*As a general matter*, slippery-slope arguments in the legal context are consider very weak.  Legal principles and decisions have to do with human interactions and human realities, and in the real human world, clear bright lines rarely present themselves.
When is someone legally responsible?  It's obvious that a 4-year-old should not be; it's obvious that a 30-year-old should be.  But where's the bright line?  We arbitrarily choose 18 (for most purposes).  Is there any real difference between 18 minus one day and 18?  Clearly not - but we have a rule, because there are circumstances in which we have to be able to decide.  And the whole point of "rule of law rather than rule of men" is *predictability*.
And that's the way legal thinkers generally approach "slippery slope" arguments:  We know *this* should be regulated/made illegal/whatever; we know *that* should not.  We don't right now know exactly where the line should be drawn ... but that's the purpose of legal cases and precedent.  The Common Law has been crafting lines to divide this from that for hundreds of years.  That's the role of judges - especially those at the appeals level; it's also the job of legislatures (though they usually don't do nearly as good a job at it).
BTW, reductio ad absurdum arguments get rejected even more strongly, and for similar reasons.  The view is "sure, you can take things to an extreme and make them look silly - but that's not the reality, the legal system doesn't do that."
Now, I'm not trying to defend what Comey said, or even less to defend the his position on cryptography.  Nor do I claim that the legal system always gets this stuff right - clearly not.  I'm just trying to point out the kinds of arguments that get accepted in different spheres of thought vary, for very good reasons - and that making a list of "silly exceptional cases" that laws might reach will get you nowhere in the domains that really matter for *application* of laws.
                                                        -- Jerry

@_date: 2016-04-12 17:35:17
@_author: Jerry Leichter 
@_subject: [Cryptography] At what point should people not use TLS? 
I haven't looked at the whole protocol so may well be missing something essential, but the forward secrecy part seems easy to fix:  Rather than caching the original session's master secret, cache its one-way hash.  Assuming both ends do this, any further communication continues exactly as before, for better or worse - as long as both ends do the same thing, they agree on the cached value and it's just as good a master secret as the original.  But compromise of the cached value now provides no information about previous messages.
                                                        -- Jerry

@_date: 2016-04-12 17:43:27
@_author: Jerry Leichter 
@_subject: [Cryptography] Is storing a hash of a private key a security 
It's not clear to me what the assumptions are here.
Is the private key known outside the secure enclave?  If not, detecting the bit rot is all well and good but how do you recover?
How much cryptography does the secure enclave do?  If it implements a deterministic cryptographic algorithm - input is (key id, plaintext), output is cipher text (or the other way around) ... why do you need a separate hash function?  Compute and remember the encryption of a couple of set values; to test for bit rot, compute the same thing again and make sure you get the same values out.  This uses the secure enclave exactly as it was meant to be used, so can't possibly open any new vulnerabilities.
If the enclave is non-deterministic, it's a bit more complex - you would have to do a full round trip through a paired enclave containing the same key.
                                                        -- Jerry

@_date: 2016-04-13 19:16:23
@_author: Jerry Leichter 
@_subject: [Cryptography] [cryptography]  Show Crypto: prototype USB HSM 
I wonder if one could get rid of the display per se and add some kind of MEMS steerable laser to it.  The output would be projected onto some nearby surface.  This could be physically much smaller.
People have built "virtual keyboards" using this idea- here's a random one:  In another message, you suggested using a passphrase to unlock the thing, so even decapping wouldn't reveal the secrets.  That requires a secure input device.  Going all the way to a virtual keyboard might do the trick.  The keyboard doesn't have to be very good, just functional for this one purpose.
Of course, this would add significantly to cost, though the one I listed above only costs $40.   What size you could end up with isn't clear.
                                                        -- Jerry

@_date: 2016-04-14 06:06:44
@_author: Jerry Leichter 
@_subject: [Cryptography] USB 3.0 authentication 
There have been recent press reports about a new spec release by the USB 3.0 standards group for a mechanism to certify USB 3.0 devices and cables have them cryptographically authenticate.  The use case that drives much of the coverage is the story from a couple of month back in which a cheap mis-wired USB 3.0 cable fried someone's Chromebook.  Sounds reasonable.  But then you get to the use of "128-bit security for all cryptography", which is already sounding like a bit of overkill - and a press release (quoted 3rd hand so I have no real idea where it comes from) that:  "For a traveler concerned about charging their phone at a public terminal, their phone can implement a policy only allowing charge from certified USB chargers.  A company, tasked with protecting corporate assets, can set a policy in its PCs granting access only to verified USB storage devices."
Anyone know exactly what crypto is going into these things, and what its capabilities are?  The ability to limit connections to "verified" devices - depending on who gets to do the verifying - could be used to attempt to close down leaks by preventing people from transferring data onto devices they then take away with them.  Or it might be used for anti-competitive purposes:  XYZ Corp PC's only support external keyboards manufactured by XYZ Corp or its licensees.
As with TPM, likely both pluses and minuses,  But I've seen no discussion beyond repetition of press releases of what we are gaining - or potentially losing.
                                                        -- Jerry

@_date: 2016-04-14 16:14:33
@_author: Jerry Leichter 
@_subject: [Cryptography] Simple IoT sensor encryption ? 
I don't see a threat model here.  Yes, these are bits and pieces of attack techniques ... but what exactly are you intending to secure?  All I see here is the values sent by the sensor.  Do you need to protect them from visibility, or are you concerned with integrity?  How about availability?
Without saying what properties you want the system to provide, you can't possibly analyze the threats - threats to do *what*?
                                                        -- Jerry

@_date: 2016-04-15 07:03:18
@_author: Jerry Leichter 
@_subject: [Cryptography] Simple IoT sensor encryption ? 
Even dumb temperature sensors can fail, reporting either high or low temperatures.  The systems they connect to have to have some kind of protection from such failures.  In the case of a boiler, inherent limits in the amount of heat you can generate vs. the rate of heat loss from the house make the scenario of "cooking" anyone unrealistic.  (There are likely local over-temp sensors are parts of the system that are prone to such problems - if any.  A boiler might have a shutoff tripped by running out of water.)
The original scenario envisioned physical access to the sensor.  If I have physical access to your sensor, I can easily modify it so that it senses whatever I want it to sense.  I can even add my own remote control for my override.  If I can control the inputs, all the crypto you put on the outputs buys you ... nothing.
Granted, if a single symmetric key is shared by all your IoT devices, breaking one breaks them all - but I think we kind of understand that these days.
You can always construct wild scenarios.  How much would you be willing to pay to make sure your temperature sensor can't be hacked (for some meanings of "can't be" and "hacked")?  Costs are a part of a threat model, too.
                                                        -- Jerry

@_date: 2016-04-15 15:13:35
@_author: Jerry Leichter 
@_subject: [Cryptography] Simple IoT sensor encryption ? 
That, in combination with the assumption that the adversary has physical access to the device, makes little sense.  Given physical access, I would simply place my own sensor next to yours.  A hell of a lot easier than disassembling your device, getting the key out, and then putting it back in place in a way that you won't detect.
I don't see the use case for IoT authenticity secure against deep device hacking.  Can you give me an example of what you have in mind?
                                                        -- Jerry

@_date: 2016-04-15 22:31:23
@_author: Jerry Leichter 
@_subject: [Cryptography] USB 3.0 authentication 
All I've seen are press releases, which are of course lacking in any real detail.  Here's one:  In passing, it says:
Yes ... and no.  One of the great things about USB was that "U" - Universal.  To some degree, we're losing that.  Yes, a USB 3.1 port might support DisplayPort - or it might not.  In the future, there might be any number of such protocols that it might support - or might not.  There are already proposals for a complex set of symbols near the port to tell you what protocols it supports.  A far cry from the last couple of years when a USB connector pretty much universally supported USB 2.0 - no more and no less (except for the backwards compatible 1.1 and 1.0 modes, of course).
Having the flexibility is great; having the variation and confusion, not so much.
Note that the USB guys started the confusion right out of the gate, with USB 3.0 (good to 5Mb/sec) quickly followed by USB 3.1 (10Mb/sec) and a whole bunch of misunderstandings about what level of support the C connector implied.
                                                        -- Jerry

@_date: 2016-04-17 22:38:29
@_author: Jerry Leichter 
@_subject: [Cryptography] Simple IoT sensor encryption ? 
OK, so if the manufacturer follows dumb (if common) practice and assigns the same keys to every device, and users leave the keys as specified, either voluntarily or otherwise ... then you're reliant entirely on physical protections to keep attackers away from the information necessary to spoof one end or the other of the connection.
Devices that are intended to be able to survive in such an environment exist.  Equipment that has to communicate securely from hostile territory or on a battlefield is an example.  So are ATM's.  But they're expensive and the do get broken into when it's  important enough.  This doesn't seem like a reasonable approach to building IoT devices.
On the flip side, assigning a unique id from a very large space to each of a large number of devices is a long-solved problem.  SIM cards are an obvious example.
Dumb designs are everywhere....
                                                        -- Jerry

@_date: 2016-04-17 22:46:27
@_author: Jerry Leichter 
@_subject: [Cryptography] USB 3.0 authentication 
Given the purported use case here - a way to identify certified power supplies and cables - "overkill" makes sense.  It's not even clear what security properties make sense for a USB 3 cable - yes, they contain chips, they are not purely passive devices - or how they might be attained, but the value of cheating *given that that's the problem you're trying to solve* make it hard to come up with a reasonable use for AES.
But keep in mind the point of my comment:  It wasn't so much that AES was overkill or that it was hard to credit that it wasn't there for additional purposes as well.
                                                        -- Jerry

@_date: 2016-04-22 06:03:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Security on TRIM for full-disk encrypted SSDs 
...and here we see another basic issue.  "Contiguous" has an obvious meaning for physical disks, but for SSD's it's something else again.  Sure, the file may be contiguous in the space of block number presented to the OS ... but in the space of pages on the SSD, it's pretty much always going to be randomly scattered.  *Maybe* if you started with a new, or completely erased, SSD, the internal algorithms would end up giving you contiguous SSD blocks ... but quite likely not even then.  There's really no need for them to, as physical location on an SSD makes no sense.
So stepping back a moment on the question:  The real concern here is that there is a level of metadata - the block to SSD page map - which is maintained inside the device, is outside of your control, is not encrypted, and which contains some information that correlates with the actual data stored, even if only at the level of *how much* data is actually being stored.  In effect, the TRIM operation takes the free block list, which in FDE would have been stored, encrypted, in the data of the device, and exports it out to the unencrypted SSD metadata.  The information is, at most, at the granularity of the TRIM operation (which is one block); but it is there.
Compared to other issues with SSD's - in particular, the impossibility (in current designs) of ever being sure you've securely erased anything short of destroying the device - SSD's are probably a bad technology for the absolute highest levels of security.
How relevant that is in what situations is a separate question.
                                                        -- Jerry

@_date: 2016-04-26 15:33:49
@_author: Jerry Leichter 
@_subject: [Cryptography] Darpa wants a secure messaging app based on 
There's nothing in the SBIR proposal that calls for back doors.  That was Henry's cynical comment.
Keep in mind that the FBI is at this point pretty much out there alone in demanding back doors (though they won't admit that that's what they really want).
                                                        -- Jerry

@_date: 2016-04-27 15:24:36
@_author: Jerry Leichter 
@_subject: [Cryptography] Darpa wants a secure messaging app based on 
And what, exactly, in this litany of "maybe"'s couldn't happen *if the SBIR didn't exist at all*?  Just what is it you're concerned about?
The SBIR doesn't even pretend that it's funding the building of a product for normal users - it's funding the creation of a product for the USG to use, and in particular for the military and intelligence communities.  If they want a back door ... well, that's their call - though I very much doubt the NSA would want a back door in something built for USG use.  They know the liabilities just as well as anyone outside - and they can get what they need because they control key distribution.
As for classification ... this one looks to me like an engineering project more than a research product.  The basic techniques are all out there - it's a matter of putting them to use toward a particular end.  *Maybe* there's a need for something new for scalability; maybe not.  If they classify it, someone else can do the same work.
Actually, I think it's great that the *proposal* is out there!  Its a nifty-sounding idea and it may inspire *someone else* to go build an open source version along similar lines.
                                                        -- Jerry

@_date: 2016-04-27 16:48:41
@_author: Jerry Leichter 
@_subject: [Cryptography] Pragmatic, 
[Questions about encryption of databases.]
There have been three major streams of work on this general field.
A number of years back, a couple of papers were published on ways to encrypt an entire record in such a way that you could give away keys for subfields, and someone with a key could only decrypt those subfields.  There was some pretty math involved here (extensions of RSA) but as far as I know no one ever managed to turn this into a practical solution, and the papers remain academic curiosities.
More recently, a group at MIT took a more pragmatic approach, in which they choose different kinds of algorithms for different fields depending on the access and security requirements for each of those fields.  The project page is a   This work has been criticized for leaking information - e.g., some encryptions need to be order-preserving, which inherently leaks information and, in some situations, can leak a great deal of information.  The CryptDB people have basically responded that, yes, this is a tradeoff you have to make.  The jury is out on this.  For some kinds of data and CryptDB-style encryptions, the resulting database would arguably not be, say, HIPAA compliant.  (E.g., imagine a row of SSN's encrypted with an order-preserving encryption.  If I can cause records with arbitrary SSN's to be added to the database, I can look to see where they fall and triangulate on the record for a particular SSN - and at the least determine if it's present in the database, probably already a privacy violation.)
The third stream is more general and centers on homomorphic encryption, which allows you to do arbitrary computations on encrypted data producing encrypted results.  (Note that the party doing the computations *doesn't learn anything about the actual data*; all they see is the encrypted data and results.)  Cool idea, cool and complex mathematics - but no generally usable results yet as far as I know.
Given the constraints you're under, CryptDB *with very careful specification of the required security properties, followed by proper mapping to the CryptDB primitives available*, might give you something useful and practical.  But you'd have to do the up-front work to be sure - and look at the papers on attacks against CryptDB, not just the papers by the designers, before going this route.
An orthogonal alternative is to encrypt the entire database and use hardware to ensure the even if the whole thing is stolen the keys are lost.  Put the computer and storage inside a safe with various security features that detect an attempt to open it and zero the internal record of the keys.  Get your keys back from a properly designed KDC.
What attacks were you trying to secure against again?  :-)
                                                        -- Jerry

@_date: 2016-04-27 19:58:36
@_author: Jerry Leichter 
@_subject: [Cryptography] Darpa wants a secure messaging app based on 
They make a big deal about separation of concerns:  Separating message creation, message transport, message stream commitment (the block chain - but nothing says that the block chain is the *only* way messages are transported), message reception, message decryption.  (The language does become a bit ambiguous at the end of the list.)
I'm guessing what they have in mind is a secure keying distribution mechanism separate from everything else.  You expire messages by refusing to deliver their keys after some point.  (The auditors presumably can always get the keys.)  In the domain of interest (military systems), central key servers are an accepted design - but you can do better.  One plausible approach is that encrypted messages have a unique ID derived from their position in the block chain, and the key server, if policy allows, computes and delivers a key based on the unique ID (and, of course, a secret known to the key server).  You can have as many key servers as you like:  You can use the block chain to distribute signed policy updates, which apply go into effect at the next message after them in the block chain.
They explicitly want to support Web browsers as endpoints, so, no, they cannot be counting on the endpoints to enforce policy.
                                                        -- Jerry

@_date: 2016-04-28 20:18:24
@_author: Jerry Leichter 
@_subject: [Cryptography] US Case: Infinite Jail Contempt for Disk Crypto, 
The reality is likely to be a bit different.  After some amount of time - a year, two years - he can go before the court and argue that he no longer remembers the password.  He should have no problem finding expert witnesses to testify that this claim is believable - it's one thing to remember a good password (and if it weren't a good password it would have been broken by a brute force attack by now) you use every day; quite another thing to remember one you've had no opportunity to use in years.
Once the court is convinced that compliance is impossible, the basis for the contempt citation vanishes.  Jail for contempt in a situation like this exists to compel an action; it's not a punishment.
So the net effect of using the compulsion of jail for contempt is to place a ceiling on the kinds of crimes for which you effectively can compel unlocking.  If the guy knows there's child porn on those drives, he knows he faces much more than a year or two in jail if convicted.
Granted, criminals do all kinds of dumb things.  Some will be ordered to decrypt and will go ahead and do so, imagining that somehow they'll dodge the bullet in the subsequent trial.  But the really bad criminals, accused of the worst crimes, who are shielding the most damning evidence behind a password only they know ... they'll keep their mouths shut and accept the relatively low penalty of a contempt jailing.
                                                        -- Jerry

@_date: 2016-04-29 21:02:24
@_author: Jerry Leichter 
@_subject: [Cryptography] USB 3.0 authentication 
How do you protect the embedded cert against physical attacks?  These are pretty low-end devices - I don't see it being possible to have really high-grade protection.  And the guys who want to build fake devices will have access to chip-level debugging stuff.
It only takes one weak implementation to expose a certificate and the whole system collapses.  I'm guessing the fallback is legal protection.  But if you really think that's a useable fallback, you can just use the law from the get-go:  Valid devices deliver some particular piece of copyrightable text, along with a trademarked image just for good measure.
                                                        -- Jerry

@_date: 2016-04-29 21:13:39
@_author: Jerry Leichter 
@_subject: [Cryptography] US Case: Infinite Jail Contempt for Disk Crypto, 
Why?  Because you believe the guy?
Criminals deny their guilt.  They deny knowing where the loot is stashed.  Basically ... they lie.  Courts have been making judgements of the plausibility of their statements as long as there have been courts.  Yes, he denies he knows the password.  The judge has decided, based on whatever evidence it has, to not believe him.  He may be right; he may be wrong; but it's the way the system works.  It's the way the system *has* to work.
For the most closely analogous cases, you can probably look to child custody cases, where the non-custodial parent (allegedly) has spirited the child away.  Sometimes that parent refuses to reveal where the child is; sometimes that parent claims not to know where the child is.  When the court doesn't believe them, that parent will get sent to jail until they reveal the location - which of course they may not actually know.  Judges are human; the entire legal system is ultimately based on human judgement, such as it is.  We do the best we can.
                                                        -- Jerry

@_date: 2016-04-29 22:45:18
@_author: Jerry Leichter 
@_subject: [Cryptography] USB 3.0 authentication 
The guys who would need to break into the chips are not the ultimate purchasers - they are the manufacturers who want to sell "USB 3.0" parts without getting certified.  You get into that business if you expect to parts in the hundreds of thousands - probably more.  The savings on that can cover a lot of attacks.
You can compare the situation to Apple Lightning cables.  Apple has some kind of authentication chip in there, and different devices with different versions of software are (pretty much randomly) more or less accepting of counterfeit cables.  Nevertheless, a market in knock-off cables continues to exist.
That's really a non-starter.  Revoke a certificate and you've suddenly bricked some number of devices - many of them legitimate - that a moment ago worked just fine.  Do that *once* and watch your market fall apart.
The Blu Ray standards have fairly a sophisticated revocation mechanism which was supposed to have all kinds of great properties, but in the end it hasn't delivered.
                                                        -- Jerry

@_date: 2016-08-05 15:39:04
@_author: Jerry Leichter 
@_subject: [Cryptography] Generating random values in a particular range 
Suppose you want to generate a fair random integer in the range [0, N); but your random number generator only produces fair random bit strings.  What do you do?
The obvious thing is to generate a k-bit string, k = ceil(log2 N) or larger, and then reduce the result modulo N.  But that's biased unless N is itself a power of 2.  So there's always the other classic technique:  Generate the random value; if it's less than N, keep it; otherwise try again.
Guess what:  The use of that second technique *for generating a random element of a group of order q for use in cryptograpnhy* is the subject of a patent, filed in 2000,   Blackberry is asserting it (among others that I haven't looked at) against Avaya.
                                                        -- Jerry

@_date: 2016-08-08 18:29:51
@_author: Jerry Leichter 
@_subject: [Cryptography] BBC to deploy detection vans to snoop on 
More recent articles - e.g., "The real scandal is that you still believe TV licence detector vans are real"  - argue that this is all a fake, aimed at frightening people into paying up.
The claim that detectors could work on packet length and timing does bring up a point I've mentioned before:  We define security for ciphers algorithms, and often for modes and even protocols, in a way that completely ignores leakage of message length and timing - assuming that it doesn't really matter very much (if you use a block cipher the length is only know mod 16 or maybe somewhat more, who cares) or, in the case of timing, that this is simply outside the domain of analysis.
And yet we keep seeing attacks (or even proposals for attacks) that look at exactly this "metadata" that we leave unencrypted.  We've seen such things as recovery of encrypted compressed speech based entirely on the sequence of packet lengths; discovery of web pages being read over and HTTPS connection based, again, on the sequence of lengths of messages; a bunch of attacks that make the attacked element an oracle for the "have you seen this string in the data you encrypted recently?" by checking the length of responses.
It's time to take this stuff seriously.
Widely used encryption algorithms and modes guarantee "semantic security" where the semantics is defined by the bits being transmitted.  Blocking, message lengths, and delays between blocks or messages are not considered.  In this situation, it's important that senders avoid coupling sensitive semantics to stuff "outside the semantic envelope".  In particular, compression of data before encryption is a disaster, as it inherently leaks information about the bit-level semantics in the lengths of the messages.  Any non-uniformity in message sizes or sending rates that's tied to the underlying bits similarly moves information from the protected domain to the (deliberately) unprotected one.
If we broaden the definition of "semantic security" to include "the attacker gains no (or a defined, limited amount of) information about message lengths and timings" - can we define cryptosystems that inherently provide such security?  Or do we need to fall back to the old definitions of security that required the sender to follow some rules about message formation?  (For example, not so long ago, ciphers were not secure against known-plaintext attacks, so the rule was that information sent through such a system was *never* released in its original form.  So announcements would be sent to overseas embassies - and then paraphrased before being delivered.)
                                                        -- Jerry

@_date: 2016-08-10 14:59:19
@_author: Jerry Leichter 
@_subject: [Cryptography] BBC to deploy detection vans to snoop 
It's reasoning like this which, while valid in its own way, undermines the real value in cryptographic research.
When modern era of cryptographic research started, not so long ago - as a graduate student, I gave a talk about Shannon's work on security and brought up RSA in a CS department theory seminar to a group that had no idea there was anything in the field in 1979 or so; a few years later, several people in the department were doing research on cryptography and a fellow grad student was doing his thesis on cryptographically secure voting algorithms - we really had no solid idea what cryptography was supposed to actually accomplish.  People would toss out such ideas as "well, if you use RSA, determining the plaintext is as hard as factoring" which we were to learn to our chagrin that this missed the point - e.g., given an oracle for the bottom bit of a RSA-encrypted message, you could decrypt the whole thing, and "obvious" protocols for using RSA made it easy to turn the legitimate recipient into such an oracle.  (Of course, even the statement itself is still not known to be true!)
Known plaintext attacks, chosen plaintext attacks, adaptive plaintext attacks - it wasn't at all clear how to fit these into a theory.  Suppose you had one encryption function E_k(P) that you could prove could not be broken (against some ill-defined set of attacks, but OK) better than by brute force.  Consider a new function, E1_k(P) = E_k(P) || P0, where || is concatenation and P0 is the bottom bit of P.  It's "within a factor of 2 as good" as E.  And yet you really don't want to use it!
Examples of this (and more subtle) sorts finally lead us to notions of semantic security.  Along with that emerged a goal:  The security of the system *could not depend on the data being exchanged!*.  After all, E1 is a perfectly good system if you just tell users to always set the bottom bit of every message to 0.
All that was fine for single blocks, but for multiple blocks, we need to look further - even a perfectly secure block cipher leaks information in ECB mode.  So we developed a theory - still not as solid as we'd really like - for encryption modes.
Symmetric systems proved to be a much tougher nut to crack - which is why we only use them to exchange random session keys for asymmetric systems.  And even so, there are issues to this day.
The goal of "just get the data securely from here to there, whatever the data is" *has to be* the goal of a science of cryptography.  Note that it's a science and and an associated engineering practice; it is *not* mathematics.  It *relies heavily on mathematical methods*, but it's something different.  As often happens in technical fields that use a great deal of mathematics, the tail can end up wagging the dog:  Much of the research ends up being pretty math that doesn't directly address the driving problems.  That's fine as far as it goes, but if it goes too far, the field risks disconnecting itself from those who rely on it.
What I'm pointing out is that cryptography as a science has chosen to rule out of bounds various "semantic leaks" that, early on, were considered (a) of minor importance; (b) too hard to address.  This is like the early work on system and network security, which declared DoS attacks out of bounds for the same reasons.  As we've learned, however, these are very much *in bounds* for real systems!  So we need to think about them and come up with some good solutions.
                                                        -- Jerry

@_date: 2016-08-11 09:56:07
@_author: Jerry Leichter 
@_subject: [Cryptography] Public-key auth as envisaged by first-year 
Beware of gross over-generalizations.  Not all communications is long range or over a general-purpose network.  There are plenty of potential applications where the delays for legitimate communications are predictably short.  Within a data center, say.
Or consider a bunch of sensor nodes scattered over an area that need to set up secure communications among themselves, without allowing an attacker into the network.  There's plenty of work on this problem that rely on "continuity of identity":  Each node initially does a DH exchange of some sort with nearby neighbors (on the assumption that no attacker is likely to be nearby when it first arrives), then uses the agreed-upon keys to make sure that it's continuing to talk to the same neighbors.
Typically, we expect that each node should only communicate with nodes no further from it than, say, 100 meters.  One implicit part of the protocol is the use of very low power radio, on the assumption that if there are any attackers, they are probably far enough away during initial configuration that they won't even hear the setup communication.  (Later, they can detect it - or detect a higher-power "call home" uplink - but by then it's too late to join the network.)  But maybe the attacker has already seeded some nodes nearby.  Adding a check of expected round-trip times might be a good additional security measure:  We know what the devices involved are and how long they should take to complete their part of the exchange.
The devil is, of course, in the details.  You need to work the numbers to see how predictable the timing on legitimate exchanges is, and how quickly an attacker might be able to complete the MITM exchange.
                                                        -- Jerry

@_date: 2016-08-12 11:44:49
@_author: Jerry Leichter 
@_subject: [Cryptography] Apple iOS and Keychain Sharing Security 
Ivan Krstic gave a take at Blackhat about how iOS security works:
Of interest is that it answers a question discussed on this list a while back about keychain synchronization:  Apple has always stated that it does not have access to passwords in keychains shared through iCloud, which seemed to be contradicted by its ability to send those secrets to a new iPhone (say) even if no existing user devices with those secrets were connected.
The answer is really quite clever, if obvious once it's described:  Apple datacenters have HSM's built on the same principles as the "secure enclaves" in current iPhone's.  The user's secrets are encrypted with a key known only to the HSM.  The HSM in turn encrypts that user-specific key with a key generated by "tangling" the user's login password with an HSM-specific random secret which the HSM will never divulge.  Assuming Apple doesn't store the user's password, it cannot retrieve the embedded key.  (Actually, the HSM never returns the embedded key - it just allows you to encrypt or decrypt with it.)  The HSM itself has anti-brute-forcing protection built into it, so seizing it doesn't let you guess the password.
Yes, a government could force Apple to divulge the user's password the next time he enters it.
Many interesting details about practical security engineering (like how to keep the security properties while allowing redundant storage of the information)  in that slide show.  Worth a look.
                                                        -- Jerry

@_date: 2016-08-17 02:36:58
@_author: Jerry Leichter 
@_subject: [Cryptography] Shadow Brokers :: powerful NSA hacking tools 
Of course, in the wilderness of mirrors that is the NSA and spycraft, you can always imagine a deeper level:  These are three (and more) year old tools that the NSA has since moved beyond, using exploits that others have since found and are beginning to use.  So it's time to leak and burn the old techniques, letting the old exploits be patched, and leaving the NSA way out front.
Sure, they could have just told the vendors about the exploits directly, but you can surely imagine a whole bunch of movie-script reasons why they might go with a more spectacular approach....
                                                        -- Jerry

@_date: 2016-08-17 18:31:14
@_author: Jerry Leichter 
@_subject: [Cryptography] Electronic currency revived after 20-year hiatus 
Wow.  How many false statements can we pack in here in an attempt to make Bitcoin into some bit of magic?
1.  The Byzantine Generals problem was never considered unsolvable.  In fact, the very paper that introduced it also provided a solution.
2.  Ignoring the problem statement doesn't solve a problem.  It may or may not solve a different problem, but let's stick to reasonable use of language here.
3.  The original paper shows that a deterministic solution in a completely general model had no solutions if 1/3 of the participants were traitors.  That's just true, and no amount of "probabilistic approximation" (whatever exactly that is) changes it.
4.  The original paper also shows that given the assumption of unforgeable message signatures, a deterministic solution exists with any number of traitors.  It's not clear what cryptographic assumptions whatever parts of Bitcoin you want to use actually need, but if they include enough to have unforgeable message signatures, a complete solution without any kind of approximation is at hand - and has been for years.  (BTW, this result is generally stated in terms of signatures, but I suspect that secure keyed MAC's are sufficient, though the message get much larger:  I need to prove to myself that any message alleged to come from any sender S actually *did* come from S.  I don't need to prove that to anyone else - I need merely forward such messages along.)
                                                        -- Jerry

@_date: 2016-08-18 01:11:35
@_author: Jerry Leichter 
@_subject: [Cryptography] Electronic currency revived after 20-year hiatus 
There are many unique idea out there.
I'm not sure what problem you're describing, but it's not the Byzantine Generals problem.
                                                        -- Jerry

@_date: 2016-08-18 01:29:11
@_author: Jerry Leichter 
@_subject: [Cryptography] Electronic currency revived after 20-year hiatus 
I should expand on this a bit.  There are results of great theoretical interest, in that they increase our understanding of how things work and lead to other results; and there are results of great practical interest in that they solve a real-world problem that people want solved.  There are even some results that are both.
The blockchain appears to be of great practical interest.  It does not appear to be of great theoretical interest.
The Byzantine Generals problem was of great theoretical interest.  There's a ton of research following up on variations of the original, and the Fischer/Lynch/
Patterson result, for example, has had a profound impact on our understanding of the limitations of asynchronous systems.  However, in and of itself, the Byzantine Generals problem had little practical interest (even though it was a formalization of a practical problem), mainly because it was deliberately set in the most general possible setting, which required expensive solutions.
In fact, the generality of the setting is often what distinguishes solutions of practical interest from those of theoretical interest.  The whole point of theoretical work is to get to some kind of fundamental understanding, and to do that you need to make your assumptions as inclusive as possible.  Typical solutions of practical interest then narrow the problem domain down by dismissing cases that aren't going to arise and finding efficient approaches.
There are plenty of useless (both to theory and to practice) theoretical results; in fact, the vast majority of published theory papers probably include just such results!  And there are plenty of useful but not very deep practical results.  But trying to argue that Bitcoin "solved this problem the theory people couldn't solve" is just silly.
                                                        -- Jerry

@_date: 2016-08-19 00:54:59
@_author: Jerry Leichter 
@_subject: [Cryptography] Phishing Attacks - Alice, HAL and Bob 
There is a great deal of published work in this direction.  One extreme example (first page only; the paper should be around somewhere)   (A "PUF" is a Physically Uncloneable Function" - this search term will lead you to some of the published work.  The particular paper - which I haven't read - uses a "PPUF" - a Public PUF).
Ideas of this sort go *way* back.  Bennett et al used quantum mechanics in a (thought experiment) design for "unforgeable subway tokens" back in 1983 (
An actual fielded system - to which I don't have a reference; this is from memory - was used to make tamperproof seals:  It's possible to pull a length of many fiber-optic strands.  The individual strands assort and mix themselves quite randomly, unpredictably, and uncontrollably.  Pick some number of them at one end and light them; record which ones are lit at the other.  Thread the glass through a hasp controlling access to something.  If it's cut, there's no known way to repair or replace it such that the resulting strand will reproduce the pattern.
There's even a science fiction book I remember reading, probably dating from the 80's, of a society in which everyone got an ID card that had a random, unpredictable value manufactured into it by some physical process that could not be reproduced afterwards.  The minor but significant plot point centered around a way of hacking this "unhackable" system.  As best I can recall, the attacker couldn't clone a card - but was able to break in at the point where a new card was bound to a person's identity, roll the process back, and create a second identity bound to the same brand new card.
                                                        -- Jerry

@_date: 2016-08-23 04:37:01
@_author: Jerry Leichter 
@_subject: [Cryptography] Real-world crypto/PRNG problem:  Bridge 
[A couple of posts factoring the multiplier]
The quality of a LCG (for appropriate use, which this is not) does *not* require a prime multiplier.  (That was the advice many, many years ago before anyone really did a proper analysis.  There are bad primes and good non-primes.)
The "48 bit state" looked familiar, and indeed a table at  confirms that (with the correction of an additive constant of 11 rather than 13) this is exactly the generator used in Posix and glibc rand48 (some variants of each, actually) and Java's java.util.Random.  However, these generators return bits 47-16 of the full product before reducing the next state mod 2^48 - important since the bottom bits of any LCG are the "least random".  The English description of what the bridge dealing program does isn't clear enough to me to understand if it's taking this essential step.
Note that these generators (as specified in Java and Posix) have been analyzed are among the best for LCG's for the given modulus - *which isn't saying much*, since *no* LCG is considered good for serious statistical/simulation work any more.
As many have commented, if you want any degree of security and fairness in such a generator, you need much more state.  For the purpose at hand, the standardized MT19937 Mersenne Twister would be a reasonable choice:  Period 2^19937-1, 2.5KB of internal state, good theoretical properties, passes a wide variety of statistical tests - and implementations are widely available.  Considered "slow" by modern standards, but for this usage, it hardly matters.
Generating an initial state this large is tricky.  Given the use case, shuffling a deck of cards and typing in the sequence of cards should be sufficient, and then adding some checkable but unpredictable data - e.g., the first paragraph of the first article on some pre-chosen Web page.  (There are some issues if this initializes only a fraction of the bits in the state, as states with many initial 0 bits are problematic for some number of cycles; the literature discusses the need to run the generator a bit to mix the state).  Using a Web page assumes Web access, but you could instead use the local newspaper, saving it for a later cross-check.
Or go fully automated and use /dev/rand to determine the initial state.
However the initial state is determined, it could be recorded, with its SHA256 checksum published at the beginning of the tournament, and the full initial state published at the end.  This would allow anyone to check that the deals were fairly generated.  (These are overkill, but given that it's a program doing the work - why not?)
                                                        -- Jerry

@_date: 2016-08-23 16:45:05
@_author: Jerry Leichter 
@_subject: [Cryptography] "NSA-linked Cisco exploit poses bigger threat than 
We haven't had any discussion of the recent leak of NSA hacking tools.  The PIX firewall attacker can actually attack even current versions of ASA, PIX's replacement.  In fact, there is apparently still no actual patch for the vulnerability:
Buffer overflows.  How are we *still* fighting buffer overflows?  Why haven't we  developed and standardized, not just improved string libraries, but generic internally-length-checked libraries for the basic problems that programmers keep solving incorrectly?
                                                       -- Jerry

@_date: 2016-08-23 21:19:50
@_author: Jerry Leichter 
@_subject: [Cryptography] Voynich Manuscript to be published 
Indeed.  The "video screens" that appear to cover the outside walls are *not* screens at all:  They are panels of fairly thinly sliced natural stone.  The glow is sunlight coming through the stone.  (From the outside, it just looks well, like pieces of stone.)  Quite an impressive site.
                                                        -- Jerry

@_date: 2016-08-23 21:31:20
@_author: Jerry Leichter 
@_subject: [Cryptography] "NSA-linked Cisco exploit poses bigger threat 
We've had safe programming languages for quite some time, but this kind of code continues to be written in C.  C itself is what it is - inherently unsafe - but in fact hardly anyone writes in C alone:  They write in a language defined by C and a bunch of standard libraries.  Those libraries contain many functions that cannot be used safely (gets(), banned from recent libraries; sprintf()) or are extremely easy to use in hazardous ways.  The improved versions - which, for example, require you to pass in the length of an output buffer you provide, rather than just a raw pointer to a "large enough" buffer - are a huge improvement, but they still require that the programmer pass in the right length.
And when it comes to buffer manipulation - e.g., using read() and write() safely - you always end up rolling your own.
I'm thinking of something like a "safe buffer" struct that would have an internally set size.  The provide a wrapper around read() that takes one of these safe buffers and makes sure it doesn't overflow it.  If the buffer operations keep track of what bytes in the buffer have been written to (presumably only as an upper limit, with everything below that having been written to) then a write() wrapper would refuse to send data that hadn't been marked as written to - avoiding the particular bug that appears to have been exercised in the Cisco attack.
                                                        -- Jerry

@_date: 2016-08-25 06:21:37
@_author: Jerry Leichter 
@_subject: [Cryptography] "NSA-linked Cisco exploit poses bigger threat 
[Description of cool features in Rust.  I'm really going to have to go look at it now]
Many of the ideas in Rust that you describe seem to have evolved (or were independently reinvented - I have no idea) from Modula-3.  Small, fast, extensively type-checked, ability to explicitly write unsafe code.  All there.
The Modula-3 guys had an interesting approach to keeping the language small:  The designers agreed up front to a page limit on the language reference manual. Once they reached the limit (I think it was 75 pages), anyone proposing to add something had to first remove something else to make room.  (Wouldn't it be nice if, say, the IRS Code were subject to a similar requirement?)
One thing Modula-3 had which Rust didn't:  A GC.  But it approached things in a unique way.  All reference objects ultimately descended from one of two roots - say Object and Address.  Descendants of Object were GC'ed; descendants of Address were not.  Only Unsafe code could perform hazardous operations on Address's.  The GC could be, and was, written in the language itself, using Unsafe operations.
There was an experiment done to develop an OS entirely in Modula-3 - I think it was called Spin.  No assembly help *at all*.  It turned out that it needed one extension to the language:  When you're writing something like network code, you  have to take an array of raw bytes and turn it into a TCP packet with a known structure.  In C, of course, you simply cast the pointer - completely unsafe.  The extension to Modula-3 added something that was like a cast - but it only allowed casting to a type with the property that all its members, recursively, had types in which any bit pattern was legal.  So the result of such a cast was definitely a legal object of the cast-to type.  (Of course, the compiler couldn't check dynamic properties - e.g., that fields that were supposed to be lengths made sense. e.g., didn't imply that an embedded field was longer than the packet it was in.  But the writer of the code could do that; array-bound checking ensured that at worst you got a checked failure.)
Modula-3 died many years ago, though in fact many of the ideas in Java clearly came from it.  If Rust or something similar catches on, Modula-3 could begin to rival Algol as a dead predecessor to a whole family of very live languages.
                                                        -- Jerry

@_date: 2016-08-26 05:48:44
@_author: Jerry Leichter 
@_subject: [Cryptography] "NSA-linked Cisco exploit poses bigger threat 
C will never be safe.  But it could be safer.  When the hazard is in the way the language is used, well, the language is what it is.  The only thing that can help is avoidance of dangerous styles in favor of safer ones.
When the danger comes from the libraries - either in the interfaces they present, or in the things they *could* cover safely but instead leave to programmers to consistently re-implement, often unsafely - the libraries *could* be improved.
It's easy to forget, but one of the innovations in C was to move many facilities from the core of the language out to libraries.  Earlier languages had build-in I/O operations, build-in string manipulation, even special mechanisms for mathematical functions.  C tossed all that.  The only build in support for any of this stuff is string constants.  Everything else is in the libraries - and early on, they actually changed considerably.
In principle, they could change again.
Not that I consider this at all likely.  There's way too much stuff built on the current base, way too many programmers who think strcat() is built into C.
Still, one must hope.
                                                        -- Jerry

@_date: 2016-08-26 22:42:17
@_author: Jerry Leichter 
@_subject: [Cryptography] Say 'unguessable' not random 
I don't see that as workable for most applications for human-interface reasons, but there is one place it *is* used:  Apple lets you generate "recovery keys" for various things (encrypted disks, iCloud accounts).  The keys are long and random.  When you first generate one, it appears on your screen.  You'd better record it, because it will never be shown to you again.  Typically, you print it and store the result someplace safe.
                                                        -- Jerry

@_date: 2016-08-27 07:11:22
@_author: Jerry Leichter 
@_subject: [Cryptography] Secure VNC implementation 
It's interesting to consider this statement in the light of our recent discussion of leakage via things like message lengths and timings.  VNC doesn't send entire frame buffers each time; it sends frame buffer updates.  Can the size of a sequence of these updates be used - as in published attacks against HTTPS - if a particular set of images is being transmitted?  There are multiple encodings of updates - including a very small update to copy a client-side value from one place to another.  This suggests attacks similar to those based on LZ-style compression, where the attacker can effectively query whether a particular sub-image appears somewhere on the screen.
This is not a criticism of VNC or SSH!  It's just an indication that once you start thinking about the possible attacks enabled by the metadata leakage of current cryptographic algorithms and protocols, you start seeing the possibilities in unexpected places.
I'm not aware of any published work attacking VNC this way.  Since VNC isn't particularly common - and certainly not over the greater Internet - relative to the biggies like HTTPS and encrypted voice, the payoff from such an attack wouldn't be great (though one could imagine targeted situations in which it could be).  Still, if someone (probably some small group, this feels like a bigger project than one person would want to take on) is interested in a nice paper....
                                                        -- Jerry

@_date: 2016-08-29 05:59:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Key meshing (Re: [Crypto-practicum] Retire all 
What I find most interesting is that the procedure as specified is run so often:  Every 1024 octets.  One wonders what class of attacks the designers were concerned about.  The text says it's to deal with "timing and EMI analysis"; the connection between that and frequent rekeying is unclear.
Looking more closely at the specified meshing algorithm:  If someone mounts a full key recovery attack against block n, they can readily compute all past and future keys, as K[i+1] is the decryption of a fixed, known constant with K[i].  If they also then recover any IV, they can similarly recover all IV's.  This makes the attack model even more obscure.
The cost is substantial:  An extra two cipher operations and a rekeying every 16 blocks.  And you lose the ability to parallelize encryption and decryption and the ability to resynchronize if blocks are lost (not that the latter is available in most good modes anyway; it's very hazardous, since the loss might be the result of a deliberate attack).
Back when DES was the only algorithm out there, I (and many others no doubt) thought about using something of this form to effectively double the key size:  Use two DES keys, with one used to periodically create a new block key from the other.  (This doesn't add nearly as much keying material as you'd naively expect; hardly worth the cost.  You can think of DESX as a much cheaper mechanism that actually does help.)
A more modern analogue might be to use a tweakable cipher and change the tweak frequently.
                                                        -- Jerry

@_date: 2016-08-29 14:06:23
@_author: Jerry Leichter 
@_subject: [Cryptography] ORWL - The First Open Source, 
It's not the FPGA or anything else in particular that's the issue.  There are open-sourced versions of several chips around.  "Open" fabs are another issue.  But assuming that could be solved ... the problem is that the gap between the performance of proprietary silicon and what you could conceivably build using an open-source technology is immense.
On the one hand, you could probably build something today that would beat anything available 15 years ago.  On the other ... what we demand today has move substantially past that point.
Over the years, we've repeatedly come back to the idea of a "reference monitor" or "security kernel" through which all security-relevant decisions would flow.  Of course, as we've learned along the way, there are often all kinds of way to route around such central decision/control points.  But it seems as if the way to get at the issues here might be to have a (limited performance) open-source verified "security kernel" managing access to a high-performance unverified, possibly malicious, proprietary part.  Of course, the security properties the combined system is supposed to provide would have to be carefully specified and themselves verified.
It's not as if we haven't built this at the software level many times.  OS's run malicious user processes; hypervisors run malicious OS's.  Sure, there are bugs, but in principle all this stuff could be verified.
What would a hardware architecture analogous to this look like?  I'm sure such things have been done in the past.
                                                        -- Jerry

@_date: 2016-08-31 16:34:26
@_author: Jerry Leichter 
@_subject: [Cryptography] Key meshing (Re: [Crypto-practicum] Retire all 
============================== START ==============================
If you want to go this route, don't change the raw key; change the key schedule directly.  That is:  The raw key goes through a transformation into a series of round keys which get saved and then repeatedly drive the algorithm.  You can modify the round keys directly.  The exact effect of some particular change on the raw key on each of the round keys could be computed; it would be a bit complex, but potentially much less complex than recomputing a deliberately- complex-to-compute schedule.
Of course, *most* modifications of the key schedule do not correspond to *any* possible raw key (simple counting argument).  What changes can be made *safely* is an interesting question to which I doubt a full answer is known.  Since we do know that key scheduling is essential to resistance to various attacks, you'd want to analyze this carefully before going this route.
                                                        -- Jerry

@_date: 2016-12-01 14:55:30
@_author: Jerry Leichter 
@_subject: [Cryptography] Gaslighting ~= power droop == side channel attack 
The physical layer is encrypted.  I couldn't find much in the way of detail other than two facts:  The encryption uses 128-bit AES, and there's a "push the button to set it up" mode for adding new devices.  I'm guessing they are simply re-using WiFi technologies, including WPS (which, yes, has recently been shown to have vulnerabilities).
                                                        -- Jerry

@_date: 2016-12-03 07:47:37
@_author: Jerry Leichter 
@_subject: [Cryptography] TV set power correlates to TV channel? 
It's worth pointing out - since we're getting into the details of the technology - that OLED screens are very different:  Rather than filtering a backlight, each pixel is a small OLED, so the power drawn depends on the brightness of the pixels.  A black screen draws almost no power; a bright white screen, maximum power.  LCD's should only draw significant power while switching (I'm not sure what exactly the quoted specs are for), while OLED's draw power continuously.
I would guess - given tons of published work on similar attacks - that it would be possible to correlate the power drawn by an OLED screen with a database of "power signatures" and pretty easily determine the show that was being watched.  Doing this from the DC side of the power supply is likely just as easy, since not much else draws varying amounts of power - and what does (the audio output comes to mind) is just as correlated with the show being watched.
If you move to the AC side of the power supply, your signal is going to get distorted.  You'd be looking at the input of a digital converter that likely has a significant amount of capacitive filtering which will level out the power draw.  At the least, this will serve as a low-pass filter on the signal.  I'd guess you could still do it, but it would take a longer sample.
Now move out to the point where you're seeing the power drawn by the whole house.  Yes, the signal will still be there - but whether you can pull it out from other junk on the line, and the distortions introduced by the house circuitry, is a difficult question to answer without trying it.  Note that at this point we can deliver gigabit-speed streams of data over house wiring; but doing so requires substantial intelligence and cooperation between both sender and receiver.
Of course, you do get the question:  Why would anyone bother?  The information is already available to whoever is supplying you with the signal.  Various slightly more intrusive mechanisms - how many TV's are in rooms without outside windows? - would be simpler, cheaper, and more reliable.
                                                        -- Jerry

@_date: 2016-12-04 07:37:29
@_author: Jerry Leichter 
@_subject: [Cryptography] Final words on RNG design 
This goes wrong right from the get-go.  A completely random source whose outputs are leaked to the opponent - even long after they are produced - is completely useless.
It's easy to set up a *formal* definition of the requirements by writing definitions in terms of inputs to functions:  The legitimate participants get the "random values" as inputs; the opponent does not; (ideally) the opponent gets no advantage in computing whatever it is that would constitute a break in the system over just guessing the missing input.
But turning this formalization into actual real-world specifications - pinning down the actual information leakages and biases and such - is very tricky.  Yes, if the opponent can predict the output of the random number generator, it's game over - but that's the simplest case.
Personally, I try to avoid using the word in this context.  It's a powerful theoretical/formal notion - well, *multiple* related but different theoretical/formal notions.  The vast majority of people who use the word in this context have a gut feel for what they *think* it is, but that gut feel doesn't align well with the formal definitions.  Talking about the entropy of your source or your generator makes it *sound* as if you have a theoretical basis for what you're doing - but most of the time, there's actually no such basis there.
So now not only are we talking about unpredictability - the wrong concept - but you're making assumptions about the kinds of attacks that can exist (recovering the state of the pool) *as part of your definition*.  That's like defining the security of a protocol that uses a block cipher in terms of attacks that recover the key schedule.
Ah, you've finally crossed over from "prediction" to "remaining unknown".  That's at least a good thing - but a bit late in the game.
There's a great deal more.  Some of it is good; some of it is "motherhood and apple pie".  Some of it gets at the important points that knowledge of any collection of outputs of the RNG gives an opponent no advantage in computing any other outputs - but it does this in passing, late in the game, when it's actually pretty fundamental (but, again, had to pin down).
I'm not going to try to go through it line by line.  As I said in a previous message:  We have some good theoretical work now on randomness, on mixers - and on protocols that rely on randomness.  It's time to start with those and see if we can build on them, not keep playing around with gut feelings.
                                                        -- Jerry

@_date: 2016-12-05 07:12:39
@_author: Jerry Leichter 
@_subject: [Cryptography] OpenSSL and random 
Note the similarity to what Apple does in iPhones (and, as it turns out, in their HSM's):  Leverage a fixed *but secret* value to greatly increase the effective security of a guessable user secret (the passcode).
At first glance, it would seem that a fixed value can add no security.  But if you use physical security, together with details of the particular system and attack models to be defended against; and you use that fixed value carefully (i.e., you never export information that makes the secret value computationally accessible); you can end up with a rather powerful primitive.
I'm sure there will be those who claims this is just "security through obscurity", but they're missing the point:  In the end, *all* security comes through obscurity - no matter how good your algorithms, you're helpless if I have access to all your keying material.
                                                        -- Jerry

@_date: 2016-12-07 06:11:09
@_author: Jerry Leichter 
@_subject: [Cryptography] Anyone else seeing an uptick in infected IoT 
This is a global phenomenon.  Since Mirai was released, there's been a burst of activity in the IoT botnet world - first with multiple teams fighting to build botnets using the same code then with adaptations to other devices.  Brian Krebs tends to keep up with this stuff.  See "New Mirai Worm Knocks 900K Germans Offline" ( and "Researchers Find Fresh Fodder for IoT Attack Cannons" ( for two recent examples.
                                                        -- Jerry

@_date: 2016-12-08 08:49:21
@_author: Jerry Leichter 
@_subject: [Cryptography] Anyone else seeing an uptick in infected IoT 
...which protects the stuff on your internal network - though it wasn't clear that there was a particular threat to it anyway.
The issue here is that the ISP-required router gets recruited as part of a botnet.  Nothing you can do on your side of the router can prevent that.
                                                        -- Jerry

@_date: 2016-12-13 13:21:11
@_author: Jerry Leichter 
@_subject: [Cryptography] =?utf-8?q?=22Op-ed=3A_I=E2=80=99m_throwing_in_the_?= 
If you need to securely contact me... DM me asking for my Signal number.
"After years of wrestling with GnuPG with varying levels of enthusiasm, I came to the conclusion that it's just not worth it, and I'm giving upat least on the concept of long-term PGP keys. This editorial is not about the gpg tool itself, or about tools at all. Many others have already written about that. It's about the long-term PGP key modelbe it secured by Web of Trust, fingerprints or Trust on First Useand how it failed me....
which got it from:
                                                        -- Jerry

@_date: 2016-12-14 07:20:27
@_author: Jerry Leichter 
@_subject: [Cryptography] "The myth of video anonymity" 
AI techniques to "de-pixelate" faces that have been deliberately pixelated for anonymity.
It's always seemed to me that this should be possible:  A single pixelated image only gives you averages of pixel values over a grid (though even that might correlate surprisingly well with a database of possible values); but in a video of even a couple of seconds, there are hundreds of different somewhat overlapping averages taken, which will highly constrain the original image - especially if there's a priori information about the image (such as the fact that it's a human face).
Our brains already do something related when you walk by a picket fence getting repeated partial snapshots of what's behind it at different angles - and forming a pretty clear overall picture.
There's a link to the paper at arxiv.org - which has a cert that expired a month ago!
                                                       -- Jerry

@_date: 2016-12-16 16:50:07
@_author: Jerry Leichter 
@_subject: [Cryptography] Photojournalists & filmmakers want cameras to 
The basic security features for SD cards were a write-lock notch, reversible and irreversible "go into read only mode" commands, and password protection.  In fact many implementations ignore the write lock (sometimes the hardware does; sometimes the hardware relies on the driver for enforcement and the driver then ignores the setting) and I've never seen an implementation of an on-card password:  I'm sure it's been done, but given the general unavailability of devices that could read the resulting cards, there's little reason to create devices to write them.
Encryption was not part of the original specs - it would likely have been too expensive at the time (1999 for the original specs).  I believe the "Secure" in "Secure Digital" was mainly aimed at DRM:  There were (and are) specs for how to limit access to DRM'ed data on SD cards.  (This use of the word "secure" is common in the industry - a way for the manufacturers to keep the RIAA and similar organizations off their backs.  "Yes, we support DRM in our standard so you can keep your precious music bytes safe.  No one implements it?  Don't look at us - we just produce specs.")
Not that that has seen much take-up either.
There would certainly be a negative pushback on an attempt to use DRM to control what you can write to an SD card or how you can use the data you put on it.  If this mechanism was ever used in a fielded product, it didn't last long.
I don't see how you can say that buyers have rejected the notion of secure protection of SD cards because manufacturers pretty much haven't even bothered to provide them, so buyers have never been in a position to decide.  About all you can say is that buyers have certainly not called for such a thing.
Note that the story is similar for disk drives:  For a while there, drive makers provided "secure drives", sometimes with just password protection in the firmware, somewhere with "military-grade" encryption.  All of the widely marketed ones were so weak there was no point in using them.  In theory, encryption in the drive itself (or in the card or USB stick) itself would be a great way to protect data. In practice, the implementations are *generally* very poor (there are a few exceptions, e.g., Ironkey for secure USB drives), and even if they were of good quality, someone would have to provide appropriate support in a variety of OS's, which hasn't been done.
                                                        -- Jerry

@_date: 2016-12-17 08:11:18
@_author: Jerry Leichter 
@_subject: [Cryptography] Photojournalists & filmmakers want cameras to be 
I disagree.  This is all typical crypto talk, ignoring the realities.
It's trivial these days to produce a photograph - and possible, though right now much harder, to produce a video - that will appear to the untrained human eye to be genuine.  Even trained human eyes - of which there are few - can be fooled.
It's also trivial to argue successfully that the photo or video doesn't show what it appears to show.  How many videos of cops shooting a suspect lying face down on the ground being shot in the head draw the response "You can't just look at the picture, you need to understand the totality of the circumstances?"  Those responses are made exactly because, most of the time, they actually work.  (I'm not here commenting on the *validity* of the argument - just its effectiveness.)
On the other hand, when it's *really* necessary to check whether a photo or video has been tampered with, there are a variety of very sophisticated techniques to do so.  The editing technology we have is designed to fool the human eye and brain.  It's not designed to fool an analysis that examines stuff at the pixel level.  Sure, your eye may be unable to notice that part of the background has been duplicated to cover the hole left by removing a picture element; but it'll pop right out of an autocorrelation.  A trained eye *might* notice that the apparent light sources for two picture elements aren't at *quite* the same point; but light source analysis algorithms will spot this easily and prove that, no, those two guys were not actually standing next to each other when the picture was taken.
The real threat is *not* the fake pictures - which in important cases can be validated.  It's, ironically, exactly what the PBA spokesman brings up:  The selection of what to photograph and which photographs to present.  The individual photographs themselves are completely unmanipulated, but the narrative they present is false.  Defending against this kind of thing is not a technological problem - though in fact in many situations, technology *does* defend against it as a side-effect:  The broad distribution of cameras means that any selective presentation is likely to be answered by other pictures taken by many other witnesses.
One can come up with situations where a cryptographically verified "seal" on a picture - which to be useful must include the time and location - would be useful.  Consider photos taken by red light cameras.  Their location is fixed and known, and the photos contain a timestamp.  If there were reason to believe the photos were being manipulated, an cryptographic seal by a third party would be valuable.  (Right now, it seems unlikely anyone would bother.  The cheating around these cameras is done in much cheaper and easier ways.)  A more realistic scenario is the cameras put in place to monitor nuclear installations as part of treaty agreements to prevent diversion of bomb material.
But I'll contend that such situation are few and far between.  Sure, the necessary technology is pretty cheap these days.  If you're talking either pro-sumer of professional level camera equipment, or high-end smart phones, the additional cost would be minimal.  But I suspect interest would wane (if it were ever there) when people realized that they'd taken tens of thousands of shots - and no one had ever needed to check the signatures.
                                                        -- Jerry

@_date: 2016-12-18 17:51:34
@_author: Jerry Leichter 
@_subject: [Cryptography] DNSChanger in ad malware attacks home routers 
That article is way below Ars's usual standards - approaching clickbait.  The stuff about steganography - while true as far as it goes - is a way of making the whole thing look more scary:  There can be an attack *hidden invisibly inside a picture!*  Well, no ... there can be some data used by an attack that's carried along inside a picture thus making the job of scanners harder - but scanners are pretty much a lost cause today anyway.
And, oh my goodness, it will use a STUN attack!  Kind of like a stun gun.  Very scary.  Well, no, STUN is simply a Secure TUNnel protocol.
The Proofpoint article has a bit of this as well, but at least it gives actual details.  Yes, it's a pretty sophisticated package of attacks against a variety of home routers.  It tries to avoid attacking systems used by those likely to notice and analyze the attack - though the fact that this article exists shows that it got noticed, grabbed, and analyzed anyway.  It also follows a pattern we first saw in "military grade" attacks in which an initial download does reconnaissance, sends the information back home to an attack server, and receives a downloaded attack that knows how to deal with its specific router.
What exactly they do to infected routers gets a bit vague.  Home routers could get involved in DNS in one of three ways:
1.  Providing a DNS server address as part of DHCP.
2.  Providing a local DNS cache.
3.  Re-routing specific known DNS resolver addresses someplace else.
Method 1 is pretty much universal, and the vast majority of home devices are configured to get their DNS server information this way.  Most ISP's configure routers to direct DNS queries to *their* routers.  Obviously, if you take over a router you can send DNS queries where you like.
Most "larger" devices allow you to override the DHCP settings and send the queries wherever you like.  Realistically, few people want to do this, much less now how.  And I'm not sure IoT devices even allow you to play with this.  So ... if this is the whole attack, sophisticated users can protect their PC-class devices and some phones (the iPhone seems to provide a setting, though you have to dig down pretty deep).  But in practice, even this is asking too much of most users.
At least some of these routers can be configured as the local DNS server, sending out their own local address with DHCP.  I actually do that on my home network - but I don't use my ISP's (Frontier) router to do it, but rather my own WiFi access point, which in turn points not to the ISP DNS offered to it by the router's DHCP to to resolvers of my own choosing.  Obviously, if the router is such a configuration is compromised, the individual devices on the network will all look just fine - they'll all continue to use the local resolver - but that will be compromised to go to the attackers resolver.
And I suppose, since the number of commonly used top resolvers isn't that long (and in any case the initial reconnaissance can determine what the victim is using), the router could invisibly re-direct traffic intended for a "known good" resolver to the attacker's resolver.  If I were crafting an attack of this sort, this is what I'd do:  It would leave nothing at all visible within the network itself without some pretty careful analysis.
The article is a bit vague about what's actually being done.  It almost seems to imply that the router interposes itself into DNS queries and alters them.  While this would be possible in theory, it would be an unnecessarily roundabout and and complex way to do things.
                                                        -- Jerry

@_date: 2016-12-23 16:29:49
@_author: Jerry Leichter 
@_subject: [Cryptography] Photojournalists & filmmakers want cameras, 
I never thought of it this way before, but curiously ... iOS has almost exactly such a mode:  You can take photos from the lock screen, without first unlocking the phone.  This begins a "photo session" that continues until you return to the lock screen (quick press on the top button does it).  You can scroll back and examine or even delete any photos taken within a session; but older ones are inaccessible.
Once a session ends, none of its photos are accessible unless you unlock the phone.
Oh, and the phone can also be configured to upload automatically to the Apple cloud if it has a connection.  Whether that's a plus or a minus depends on the situation. Uploading is something Android phones do, too, of course; I don't know if they have similar "take pictures from lock screen" functionality.
                                                        -- Jerry

@_date: 2016-12-25 21:13:04
@_author: Jerry Leichter 
@_subject: [Cryptography] Are there random sources? 
Ron Garret just posted a link pointing to an overview of some recent results on producing a strong random source from two weaker random sources.  And, of course, we have many long discussions here about how to build random sources from physical hardware.
It's worth raising the question:  Is all this theory actually based on anything?  Does the physical world actually *have* any random sources?  The answer is more subtle than you might think.
If classical physics described our universe, the answer would be straight-forward:  No!  While thermodynamics talks about randomness, that randomness is not intrinsic to the physics - it's a reflection of our ignorance of detailed physical micro-states.  Yes, we describe a gas as a bunch of randomly moving particles - but classically, in principle, we could know the positions of momenta of all the individual particles, after which the deterministic classical laws would allow us (again in principle) to predict the future state of all the particles at any point in the future.  (Curiously, if you reverse time, you can construct classical non-deterministic scenarios - but let's not go there, they don't help you generate random values.)  The apparent randomness of classical physics is more a chaotic behavior than actual randomness.
But of course we know our universe is described by quantum mechanics, which is random "to the core", right?  Well ... not according to John Conway.  In a wonderful series of lectures on the fancifully named "Free Will Theorem" he and Simon Kochen proved ( argues that whatever the core of QM is, it's *not* randomness.  The Free Will Theorem (listen to the lectures to see why it's been given that nickname) proves that, if anything even vaguely like quantum mechanics is a true description of the universe, then there exists an experiment whose outcome can be 0 or 1, and a complete description of the entire state of the universe before the experiment is run is insufficient to predict (other than as a 50/50 probability) what the next output will be.  It's interactions like this that are the ultimate basis of all "random" physical processes.
But ... random?  Sort of.  Conway points out that when we talk about computation using probabilistic machines, we have an alternative, completely equivalent, description:  The machine runs a deterministic machine which is given access to a special tape containing "random", initially unknown 0's and 1's, and when it needs to toss a coin, the machine simply reads the next square on the tape.  But ... the tape can be made part - a hidden part, but that doesn't matter - of the universe.  The Free Will Theorem then says that *even if you have access to that tape* - now part of the previous state of the universe - you can't determine the outcome of the experiment.
Now, I don't claim to understand just what that means about the "choices" in QM.  Then again, Conway - who's a hell of a lot brighter than I am - says he doesn't understand it either.  So I don't feel bad about it.  :-)
Whatever is going on here, it tells us is that at least our conception of probabilistic machines; and perhaps our conception of "randomness" as a whole; apparently *doesn't correspond to anything physical*.  Both are useful abstractions, like the "infinite lines with zero width" of Euclidean geometry; but they live only in their theories, not out here in the world.
So should we take away from this the message that using randomization in cryptography is a pointless exercise?  Hardly.  Even if the world *were* classical, as a *practical* matter the chaotic nature of various kinds of thermodynamic properties are quite good enough to build secure "true random number generators".  No, they aren't "really" random - in principle, someone with a God's view of the universe could predict their values - but God is not an Opponent one can hope to defend against in any case!
It does suggest, however, that attempts to *define* randomness are misdirected. The more you try to pin it down, the more you drive yourself into a realm of mathematical theorizing that doesn't correspond to anything in physical reality.
Treat random number generation as an *engineering* problem - to which a great deal of physical and engineering theory, as well as recent computer science theory - can be applied to good effect, and useful solutions exist.
                                                        -- Jerry

@_date: 2016-12-27 06:40:00
@_author: Jerry Leichter 
@_subject: [Cryptography] where shall we put the random-seed? 
It occurs to me as I read this that any device that can boot from a LiveCD or a USB stick *inherently has a potential user-controllable input device* - namely, the CD or USB reader.  If you can get the user to pull out and replace the CD or USB drive a couple of times, the timing could give you a reasonable amount of unpredictable, uncontrollable variability to leverage in initializing your RNG.
This is obviously not applicable in all situations.  The assumptions are:
1.  The user is actually present during boot;
2.  The user is in a position to, and willing to, help;
3.  The medium is removable and re-insertable without causing issues for the hardware (e.g., forcing a hardware reset);
4.  Software can detect removal and re-insertion;
5.  There is some means for the system to communicate to the user when it wants him to pull and replace the medium, and when it's done booting.  This may be a very relaxed requirement, with the "communication" being virtual - through user-level elapsed time.  For example, something like:  "Boot the device; wait for a count of 20; remove the medium; count to 5; re-insert; count to 5; repeat the remove/re-insert sequence about 5 times" might be good enough in some cases.
Note that the availability of a high-precision clock is *not* required - the system isn't doing anything else useful while waiting for the removal and re-insertion so it can use timing loops with counters to measure elapsed time.  (R
The time between when it requests removal and when that actually happens is just as variable as the re-insertion time and should be included.)
A specialized set of requirements - many systems have much more capability to communicate with the user; some on the other hand have none.  But it's always useful to have mechanisms available for as broad an array of potential situations as possible.
                                                        -- Jerry

@_date: 2016-12-28 17:55:50
@_author: Jerry Leichter 
@_subject: [Cryptography] where shall we put the random-seed? 
So ... I'm a bit puzzled. Early in boot there may not be enough seeding material to get good kernel address randomization. So why not boot without it into a special restricted mode which only allows pre-vetted code to run, gather up some good seed bits - and reboot, passing along the seed bits?  Takes a bit longer but if your concerns are at this level you should be willing to accept that.                                           -- Jerry

@_date: 2016-02-15 12:06:38
@_author: Jerry Leichter 
@_subject: [Cryptography] XOR linked list & crypto 
I remember some paper in which links were encoded as keys, provide a way to represent arbitrary graphs in a secure fashion.  Can't remember any of the details, though - sorry.
Speaking of the XOR trick:  I don't know who invented it or when, but I first heard about it in 1970 or thereabouts.  *Why* it worked bothered me for many years, until I finally came up with a way to think about it.  Imagine a linked list as a physical chain.  You hold onto one link and the chain falls off your hand in either direction.  You can easily move left or right along the chain.
The analogue for a linked list is that you can construct a two-way linked list from a one-way linked list by reversing the links as you traverse the list.  Unlike the XOR trick, this works just fine in a strongly-typed language where the only things you can do with pointers is assign them, follow them, or compare them for equality.  Of course, it means you need write access to the list even to read it, and the list can't be read by two separate threads at once.
Link reversal has actually been used in this fashion not for list traversal as such, but for garbage collection:  Walk the "points to" relation from the roots, reversing links as you go.  Then whenever you relocate a node,  you have a pointer to a list of everyone who points to that node, so you can flip the pointers back, but to the new location.
                                                        -- Jerry

@_date: 2016-02-15 23:17:41
@_author: Jerry Leichter 
@_subject: [Cryptography] 
=?utf-8?q?of_Suite_B?=
That's one possible reading.  There weren't any real choices before this update:  There were two levels (Secret and below; Top Secret) which differed in apparent security and in the cost they imposed on implementations.  As the document itself says, these days, the performance of hardware has gotten good enough that they really don't need the old Secret level - so they just moved everything up to what had been the old Top Secret level.
There was a nominal choice before - Secret (or below) stuff *could* be sent using the TS-level values - but in practice this was unlikely to be done because of interoperability constraints:  Implementations that limited themselves to S or below would typically not implement the TS algorithms at all.
The choice of RSA vs. ECC hasn't really gone away; in fact, they've left it more open than earlier versions, which pushed for evolution toward ECC.
From what we've seen NSA *never* seems to have been big on offering choices.  They build systems for particular distinct use cases - one per use case.  Of course, given the nature of the nation-state spying game, they have to be prepared for the possibility of large-scale compromises, which have happened many times over the years.  They appear to do this by being in a position to completely replace a system when necessary.  Since, large as they are, the total  number of units they have fielded is small compared with, say, cell phones; and they resources they have to push a change is immensely larger on a per-unit basis; they can live with this much better than the open world can.  (It is interesting that they don't seem concerned about forward security.  You'd think that would be very valuable when doing damage assessment and control.)
                                                        -- Jerry

@_date: 2016-02-17 13:52:46
@_author: Jerry Leichter 
@_subject: [Cryptography] Hope Apple Fights This! 
The phone in question is apparently a iPhone 5C, which implemented all the security in the OS.
Starting with the next generation (the 5S), the chips implement a "secure enclave" which controls all access to the keying material and the OS cannot breach.  Reportedly its (presumably very rarely modified) code cannot be updated without clearing all the stored secrets.
                                                        -- Jerry

@_date: 2016-02-17 16:56:33
@_author: Jerry Leichter 
@_subject: [Cryptography] Thoughts on the Apple iPhone fiasco 
If the FBI wins on this, they will use it the next time around.  I can see at least two ways they can do so:
1.  When the time comes that they need access to a more recent iPhone (and they'll choose a case where everyone will agree that giving them access isn't such a bad idea) they'll get a court order and Apple will respond "What you're asking for is impossible."  The FBI will then play that up - in the courts (where they will almost certainly lose) and in public/political opinion (where they stand a chance of winning):  See, Apple has taken *deliberate steps* to frustrate a legal court order.
2.  Once the precedent is set to allow the courts to force Apple to create a signed OS image and hand it over, the next order will be for an image to be loaded *before* the phone is captured - perhaps even using Apple's own updating mechanisms.  So the target will end up with a phone that secures its encryption keys - but uses the identity function for actual encryption.
I'm sure clever lawyers will find other uses for a ruling in the FBI's favor.
There's a saying in the law:  Hard cases make bad law.  You better believe that the FBI chose this case carefully, knowing that everything about it made their position seem as reasonable as possible, while making Apple look as bad as possible.  Note also that they demanded much more of Apple than is necessary.  They *could* have demanded that Apple provide a copy of the data in the phone.  Since the attackers are dead, this couldn't compromise prosecution, and would not have left Apple with the ability to say "they want us to make something we consider so dangerous that we refuse to try."  (Some have painted this as "oh, they aren't asking Apple to get its hands dirty by actually doing the deed - they just hand over the code.  But that gets things exactly backwards.  In fact, in the past, when it was relatively easy to do, Apple *has* agreed to get and turn over the contents of phones.)
The FBI would *like* to look at this phone - though realistically I'd bet they don't expect to find anything of interest on it:  The attackers were apparently pretty good about operational security, and were careful to destroy their own phones and computers.  Why would they use a work phone - which they might be called upon to turn in and unlock, even for routine purposes - for stuff they needed hidden?
Just on the basis of what they are likely to find on the phone, I doubt the FBI would take things to this point.  But as a way to set a precedent in their favor ... it's a godsend.
                                                        -- Jerry

@_date: 2016-02-17 18:29:33
@_author: Jerry Leichter 
@_subject: [Cryptography] Hope Apple Fights This! 
TBD.  Courts will require you to turn over any material that already exists - no issues there.  *In general*, they will not compel you to go out and create stuff that doesn't already exist.  The line is gray.  Note that the order itself tells Apple that it can push back if it believes the effort is too onerous.
This case will likely be precedent-setting on exactly the issue of how far the government can go in ordering a third party to do work for it.
Apple has never claimed that ability, and there's no evidence they have it.  Normal updates are user-initiated, at at least user-approved.  Over-the-air updates require the phone to be unlocked.  Updates through iTunes require that the computer running be marked in the phone as "trusted".  Only an unlocked iPhone can mark a new computer as "trusted".  In most cases like this, the FBI would have access to the subject's laptop or other computer, which has probably already been marked trusted.  (Mac's don't have the kind of hardware protection that iPhones do; breaking into them is relatively much easier.)  In this case, the subjects apparently removed the hard disk from their computer, and it hasn't been found - so it's not clear if the FBI has access to a computer the phone considers "trusted".  (I *think*, even in the case of a trusted computer, the normal update process requires an unlocked phone; but I'm not sure of that.)
So it's not clear exactly what Apple would have to do to get into this phone.  We have the court ordering them to do that, and Apple pushing back on principle; we don't know if they could also push back with "we have no technical means to do what you want".  My guess is Apple can find a way to do it - but it would be difficult and would effectively reveal the existence of security hole.
a) They don't do such updates today.  There *may* be exceptions for phones set up for remote administration.  I don't think so, though - the main things remote administration can do forcibly is wipe a stolen phone and push policies on things like minimum password lengths - though for the latter, I think you still have to accept the modification.  The main enforcement mechanism is that if you don't accept the profiles, you're denied access to corporate assets like mail accounts.
Since this is a county-owned phone, it *may* have been set up for remote administration - but in that case it's extremely unlikely the attackers would have stored anything even remotely incriminating on it.
b) Apple *has* built such a hardware security module (the "secure enclave" that's part of the CPU chip), and all iPhones since the iPhone 5S include it.
                                                        -- Jerry

@_date: 2016-02-18 17:21:09
@_author: Jerry Leichter 
@_subject: [Cryptography] XOR linked list & crypto 
I'm sure you know this, but this isn't some wild theory:  These instructions - with exactly these names - were available on the VAX many years back.  They were very widely used with VMS (DEC's operating system for the VAX), so when DEC produced the Alpha as a successor to the VAX, the VMS version of the PAL code (essentially loadable microcode that could be tailored for a particular OS) included these instructions.
Well ... yes.  But even if you just replace subtraction with XOR, your queue is no longer location-blind - you can't map it to any location in memory with no change, as you could with difference.  (BTW, if you want to think about this in a broader way:  Using the difference essentially replaces a coordinate with a vector, which makes the choice of origin irrelevant.  We don't normally think of "one-dimensional vectors" this way, though it's a central (indeed motivating) reason to use them in higher dimensions.)
If you change things to just the XOR, you further lose the ability to use a single address to define an entry point to the queue.  You always need a pair.
Side note:  We're all so used to singly- and doubly-linked lists (they are among the first data structures covered in data structures courses) that we tend to use them even in situation where a closer analysis shows they are suboptimal.  A number of years back, I built a set of C++ data structure classes, including things like hash tables and queues.  (This was more or least contemporaneous with the development of the STL.)  There were *no* linked lists in the collection!  In their place, I had a data structure I called a sequence.  A sequence is a resizable array with a pair of pointers to the beginning and end.  You can add or remove at either end by incrementing or decrementing one or the other pointer.  Note that either pointer can wrap around the array - the actual elements start at the "beginning" pointer and continue to "increasing" values (mod the array size) until you get to the "end" pointer.  If the array fills, you resize and copy it - typical multiplicative growth.
What's not immediately obvious is that *if order doesn't matter*, you can delete from anywhere in a sequence in constant time:  Swap the element to be deleted with the first (or last) and then remove.  This is enough for many of the data structures you often use lists for - e.g., stacks, queues, buckets in a hash table.
But ... doesn't it waste memory?  If you do a detailed analysis, unless your elements are much larger than a pointer, you typically end up using *less* memory than you would with a linked list.  And traversing it on modern architectures - where the next or previous element in the array is usually already in the the cache, while whatever a pointer goes to probably isn't - tends to be much faster.
And the code is simpler to boot!
                                                        -- Jerry

@_date: 2016-02-19 15:33:20
@_author: Jerry Leichter 
@_subject: [Cryptography] 
=?utf-8?q?le=E2=80=99s_refusal_to_aid_decryption?=
Already withdrawn - Of course, this is one minor skirmish - the battle is far from over.
                                                        -- Jerry

@_date: 2016-02-19 16:42:12
@_author: Jerry Leichter 
@_subject: [Cryptography] Is Apple correct? 
This is the first mention I've seen of the IMEI.  I don't believe the IMEI is in any way involved in the iPhone security as being discussed here.  In fact, it pretty certainly isn't:  The same mechanism is present in iPads (and probably recent iPods) which don't (necessarily, in the case of iPads) have cellular hardware and hence IMEI's.
                                                        -- Jerry

@_date: 2016-02-19 17:00:33
@_author: Jerry Leichter 
@_subject: [Cryptography] Is Apple correct? 
No.  The IMEI identifies the physical phone, not the subscriber.
                                                        -- Jerry

@_date: 2016-02-19 17:07:17
@_author: Jerry Leichter 
@_subject: [Cryptography] Is Apple correct? 
Oh, and BTW, the serial number portion of an IMEI is only 6 decimal digits long. The rest of the IMEI identifies maker and model, possibly a software version number, and a checksum.  Those parts are easily determined in typical attack scenarios.  Not to mention that IMEI's aren't treated as particularly confidential anyway.
Basing anything cryptographic on the IMEI would be a really bad idea.
                                                        -- Jerry

@_date: 2016-02-20 23:32:50
@_author: Jerry Leichter 
@_subject: [Cryptography] the consequences of changing the password on 
Well ... yes.  That's the whole point of automatic backup.
Reports are that backups from the device were "sporadic" and that the last one was made some time in October.  There was some speculation - given the long gap - that backups had actually been turned off.  My own guess - give the "sporadic" nature of the backups - is that *automatic* backups were never turned on.  Rather, they were done manually every once in a while (in which case this approach wouldn't work anyway.
Manual backups can obviously be triggered externally, though only from a "trusted" computer (one you've told the phone to trust) and, I'm pretty sure, only when the phone is unlocked - though where that's enforced is another question.
I've always thought that automated iCloud backups were triggered by the phone - for one thing, they only occur over WiFi, and only the phone knows how it's connected to the Internet.  Perhaps there's a way for Apple to trigger an iCloud backup - though it's not clear why they would want or need such a feature.
The San Bernadino guys say they did this on the advice of (or at least in consultation with) the FBI.  Someone didn't think through the implications - a classic example of "it seemed like a good idea at the time".
That's an interesting question.  It may be that the phone itself notices that the password it has was rejected as obsolete and will wipe it or at least refuse to try it again.  This would be protection against a kind of replay attack, though it seems unlikely:  The reason that Apple is in a position to be ordered to hack around the protection on the phone is that they apparently didn't consider an attack *by Apple* as something that needed to be protected against.  In the case of iCloud backups - which Apple ends up holding anyway - that would be doubly true.
There's more to this particular part of the story than meets the eye.
Another interesting question:  Backups to local disk can be encrypted with a user-chosen password; but backups to iCloud cannot.  If they could be, Apple would not be in a position to deliver backed-up information.  It's not at all clear why Apple hasn't gone this route.  Perhaps they don't want to start yet another battle.
                                                        -- Jerry

@_date: 2016-02-21 07:27:57
@_author: Jerry Leichter 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
The problem is that file systems, from the disk layout to the paths the data flows to and from programs, don't leave you that 0.05%.  They read a write contiguous blocks of a size that's assumed, at deep levels of the code, to either be fixed, or to be an exact small multiple of the system page size.
So, yes, if you have the option to do a clean-slate design, this is no big deal (In fact, at a lower level and a number of years ago, the designers of disk subsystems did that:  All the code around knew that disk blocks were 512, but if the lowest layers actually set aside, say, 536 bytes - using the extra space for ECC - who was to know?  This required that the drive makers also wrote the match drivers - but that was expected anyway.  These days, that level of code is all inside the drives and no one outside the maker's lab knows much about it.)
Technology does advance, though.  When you were talking about spinning rust, it was essential that any metadata associated with a block be physically part of the block - putting something like an IV off elsewhere in a metadata area would destroyed performance.  With SSD's, the cost of an extra read is much less, and you might be able to get away with separated metadata.  On the other hand, you have to write in large pages, so updating that information is expensive.  So a mode that could rely on a *fixed* set of IV's might be a big win.
New storage technologies are emerging, and what we know about good and bad designs for the disk stack is increasingly out of date.  As we develop new data structures appropriate to these new technologies, it would be a really good idea to consider cryptographic support from the ground up.
                                                        -- Jerry

@_date: 2016-02-21 18:01:32
@_author: Jerry Leichter 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
You're re-inventing tweakable block ciphers.  See                                                         -- Jerry

@_date: 2016-02-22 06:47:08
@_author: Jerry Leichter 
@_subject: [Cryptography] Yes, Apple is correct 
The IMEI is *public information*.  Historically it was printed on the outside of the boxes phones came in and behind the battery, in phones that had batteries.  It's broadcast, in the clear, by the phone.  As I noted earlier, it's also constructed in a way that draws possible values for a particular phone from a fairly small set.  Also, may iOS devices are *not* phones, have no cellular hardware, and likely don't have an IMEI assigned at all.
I don't know how we got on this IMEI sideline to begin with.  The IMEI has nothing to do with cryptography unless Apple is much less competent than it appears to be.
*If* Apple were to be forced to build this software, and *if* they wanted to ensure it was tied to a specific phone, they could do so.  The phone has a number of other unique characteristics that software could use.  There's a serial number, a perhaps-related, perhaps not, UUID (advertisers used to use this for tracking; Apple forbade that, adding an "advertiser ID" that's regularly reset to a new value, preventing long-term tracking); the iCloud id the phone has been registered to (we know it was backed up in the past and it's unlikely this was changed).  And those are just elements *specifically designed for identification*, none of which are easy, if even possible, to change on a locked, or perhaps any, phone.
Beyond that, there are any number of secondary characteristics that would likely identify the phone, which again would be impractical to change on a locked phone.
The issue is not, and has never been, that the software might leak and then get applied to other phones *by modifying those phones to look like the one that it was targeted at*.  It's how many angels can dance on the head of a pin.  No, wait, that's one of the few *less* relevant issues out there....
                                                        -- Jerry

@_date: 2016-02-22 18:35:00
@_author: Jerry Leichter 
@_subject: [Cryptography] eliminating manufacturer's ability to backdoor 
And this is helpful to your typical - or even highly atypical - buyer of an iPhone - how?
                                                        -- Jerry

@_date: 2016-02-22 18:51:21
@_author: Jerry Leichter 
@_subject: [Cryptography] eliminating manufacturer's ability to backdoor 
Sorry, but this is nonsense - as is much of the ranting here about how Apple did "this" (for some unspecified value of "this") for reasons of market control, "because they want to own the phone", or whatever.
Apple provides a mechanism to force-boot a new OS on a phone, overriding anything that's there, because people are unhappy to be told that their expensive piece of equipment has been turned into brick.
Two weeks ago, the big tumult on the Internet as about "Error 53", in which indeed iPhones *were* "bricked" by a too-strict enforcement of a security feature that binds the CPU to the fingerprint detector.  Were you out there arguing that Apple was doing the right thing?
Now, the ability to restore a dead iPhone by force-loading a new OS can be implemented in one of two ways:
1.  The force-load erases the previous content of the phone before loading the new OS image;
2.  The force-load preserves the previous content of the phone.
Apple implemented (2).  Had they implemented (1), the entire debate we are now having would never have occurred.  (Perhaps it would have occurred about some other aspect of iOS.  All of you who've implemented perfect code, which even after being distributed to hundreds of millions of people showed no bugs ... please raise your hands.)
It's clear that from a best-possible-security point of view, (1) is better.  Now, I'd like to have someone tell me how choosing to do (2) was in any way advantageous to Apple itself.
No, (2) is advantageous *to the vast majority of users, most of the time* when their phones die, due to hardware or software issues.  It means they can get their data back.  Most people will tell you they want the tradeoff done that way.  Hell, a *really* secure phone would erase its contents as soon as the back was removed.  You want a repair done on your phone?  Better be sure you have a backup.  Technically, Apple already more or less tells you this - especially with Mac's - but in practice most repairs preserve the data.  This makes customers happy.
                                                        -- Jerry

@_date: 2016-02-22 18:58:04
@_author: Jerry Leichter 
@_subject: [Cryptography] eliminating manufacturer's ability to backdoor 
If you think *any* technological model is resistant to legislation, you're kidding yourself.  Sure, for small groups of individuals communicating among themselves with privately developed hardware and code - sure.  (Though they'll probably screw up the OpSec anyway.  It's really, really hard to get the right, consistently, every time, forever.)
But for stuff actually being sold?  Legislation may not prevent you from building this it, but it sure will prevent you from gaining much of a market, or making any money.  (Well, I suppose you can go the same route as drug dealers.  But secure phones aren't as addictive as the stuff they sell, so what you can make that way seems rather limited.)
                                                        -- Jerry

@_date: 2016-02-22 21:22:21
@_author: Jerry Leichter 
@_subject: [Cryptography] eliminating manufacturer's ability to backdoor 
What in the iPhone is illegal *today*?
                                                        -- Jerry

@_date: 2016-02-23 06:15:01
@_author: Jerry Leichter 
@_subject: [Cryptography] eliminating manufacturer's ability to backdoor 
Which simply means that the LE agencies are not interpreting Wassenaar the way you are, since international trade and commerce in these things isn't being interfered with.
Wassenaar is bad law - really bad law.  But it's hardly unique, and all kinds of bad laws are rendered less noxious by being "interpreted down".
                                                        -- Jerry

@_date: 2016-02-23 15:48:19
@_author: Jerry Leichter 
@_subject: [Cryptography] "On-chip random key generation done using carbon 
Simplified idea:  Prepare an area on a silicon die with a number of connection points.  Apply an aqueous solution of carbon nanotubes.  If you set the situation up right, 50% of the connections end up populated - but at random, the locations are determined by complex random motions in the solution.  The resulting array is delicate - standard de-capping techniques will destroy it, so there's (currently, attacks only get better) no known way to read it.  Compatible with existing chip processes.
                                                        -- Jerry

@_date: 2016-02-23 16:17:58
@_author: Jerry Leichter 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
This is an oversimplification.
1.  There's always *some* metadata that describes the current state of the file/object/whatever.  Sure, the actual data is in the block - but you have to have a pointer to it somewhere so you can find it.  Writing the data without the pointer makes it impossible to find; writing the pointer without the data means that anyone following the pointer will get garbage.
In practice, you may use transactional mechanisms to tie the two together; or you may do "careful writes" where you ensure the data is there before you write the pointer, so that the failure mode is that the write is lost, to be recovered later by an fsck-like file system cleaner.  In reality, at some level, the transactional mechanisms rely on "careful writes":  E.g., you have to write the commit log to stable storage before you can write the data.
If you write the authentication/integrity metadata along with the pointer, your existing mechanisms handle the issue.  Then again, if you're using transactional writes, you can generally handle adding another piece of data to each transaction with little problem.  So ... the problem is easier to solve than it appears (though of course it's *far* from free if you want to add it to an existing system).
2.  We generally assume that a write to a disk block is atomic:  It either completes successfully (replacing all the old data with the new) or fails completely (leaving the old data unchanged).  Unfortunately, this isn't true.  I can't give a reference right now, but detailed studies of disk failure modes show that all kinds of bizarre failures can and do occur.  For example, partial overwrites can occur; usually the remainder of the block is zeroed.  Or data may be written correctly - to the wrong block.  These are very rare, but should come as no surprise:  There's more code running inside a typical disk these days than in some operating systems.  The disk drive makers haven't solved the problem of writing 100% reliable code any more than the rest of us have.  And that doesn't even consider hardware failures.
Given the size of modern disks and the amount of data they are called upon to read and write, these failures can't be ignored.  (The paper had to do with whether existing file systems could recover from the actual bizarre errors that can be found in the field.  They did this by constructing models of the failures, of the file system algorithms, and of correctness conditions, and then running a theorem prover against them to try to find counter-examples.  It turned out that not one of the file systems they studied could.  As I recall, ZFS was one of the best - but it could be "led astray", too.)  So ... the problem is actually much harder than you expect *even if you keep all the relevant data together*.
                                                        -- Jerry

@_date: 2016-02-23 17:30:41
@_author: Jerry Leichter 
@_subject: [Cryptography] RIP Claude Shannon 
Isn't it great to be able to sit here comfortably in the future, looking back at how silly our predecessors were?  Why bother with voice encryption for secure communications during WW II when all you have to do is wait, oh, 15-20 years for the concept to be developed, and 40 or so years for it to be really practical?
BTW, until Shannon, it would have been difficult for anyone to conceive of digital voice transmission.  The idea of "information" as a generic thing that could be transmitted in many forms wasn't there yet.  There were completely independent technologies to deal with voice and with Morse code.  (I'm not sure what other modes of electronic communication existed at the time.  Certainly there were remote sensors and actuators - selsyns's and such - but they would probably not have been viewed as being involved in "communication".  It was Shannon who introduced the common notion of "communication" and showed that the fundamental issues were independent of domain.  His breakthrough was similar to the emergence of a unifying notion of "energy" from separate ideas of energy of motion and potential energy and heat and electromagnetic energy and so on.
The unbreakability of a 1-time pad is a pretty obvious notion and easy to understand.  Shannon's proof is important more for his way of formalizing the concepts to the point where a mathematical proof could be written down than in what it actually proved - I doubt any real crypies needed convincing.
Book codes had been known for many years, and methods for attacking them had also been known - which meant it was understood that to have an attack, you needed a handle on the statistics of the "book".  No statistics, no attack; and even then, you needed repeated messages encrypted against the same book to make practical progress.  What was needed to make Venona - where the "book" had flat statistics - attackable, and the general form of an attack, would have been well understood.  The details of making it practical ... that's another story, and one that Shannon may well have been involved in.  (I don't know if anything about this has been declassified and published.)
At the time Shannon was working, we were fighting for our lives.  That tends to bring out actual leaders and actual understanding of the realities.  While politics is never absent, it's usually submerged somewhat when the survival of the country is at stake.  (At least it is in countries that survive....)
                                                        -- Jerry

@_date: 2016-02-23 17:48:07
@_author: Jerry Leichter 
@_subject: [Cryptography] RIP Claude Shannon 
In the period shortly after WW II, there was no doubt in anyone's mind that we were at war with the Soviets.  It just wasn't a shooting war - except perhaps in Greece and later in Korea.  Yes, there's tons of revisionist history on the issue today, but none of it matters to how things looked then.
Oh, and the Soviets may have been allies in WW II, but they were allies of convenience - competition with them (and deep spying) was well under way before the war.  (Of course, at the time the US was a relatively minor player on the world stage.)  Most of the Venona decrypts where of messages intercepted between 1942 and 1945 - i.e., *during the time the Soviets were allies*.
                                                        -- Jerry

@_date: 2016-02-23 18:14:04
@_author: Jerry Leichter 
@_subject: [Cryptography] "On-chip random key generation done using carbon 
Not sure what you are talking about here.  This is not a one-time pad; it's a unique, random, unpredictable key, or ID, or whatever you want to make of it.  But it certainly wouldn't be used as a one-time pad.  The test chip they built had 64 bits on it - not really even enough for a decent key today.  While I can easily see them scaling up to, say, 256 bits or even a few Kb, they don't seem to be talking about millions or billions of bits.  And even if they were - we have plenty of ways of *generating* secure one-time pads today.  This would be a bizarre and complex technology to solve the easy part of the problem - the hard part, as always, being key distribution.
You might want to read the article on the sense in which the data is "fragile".
                                                        -- Jerry

@_date: 2016-02-24 05:50:19
@_author: Jerry Leichter 
@_subject: [Cryptography] RIP Claude Shannon 
Actually, this illustrates when one-time pads might be appropriate:  If your communications are *asymmetrical*, so that you have a low-latency, high-bandwidth, *secure* (you left that part out) channel in one direction but not the other.
Historically, the asymmetry was often in time:  It existed when the system was put in place, but not later.  When you prepare your spy, you can give him a large quantity of keying material which he can carry with him.  When he needs to report his findings, he has no secure channel to report on, but can create one using the keying material.
In fact, any system with pre-shared keys involves the same asymmetry.  Cryptographic functions don't eliminate the need for the *secure* "outgoing" channel - they simply "stretch" the initial shared secret immensely so that the "high-bandwidth" part goes away.  (Whether "low-latency" is an issue depends on other details - in the classic spy case, it might take days for the one-time pads to reach their destination - but if they were traveling along with the spy, the effective additional latency is zero.)
You might think that public-key systems eliminate the whole problem - and for pure secrecy, they do.  But if you want to to know *who* you're sending your information to, you need to get the initial trust base out there *somehow*.
                                                        -- Jerry

@_date: 2016-02-25 09:58:07
@_author: Jerry Leichter 
@_subject: [Cryptography] Hope Apple Fights This! 
Let's step back and look at the general picture here.
Defenses - cryptographic an otherwise - are built in response to a threat model. (Well, the competent ones anyway.)  Up until a couple of years ago, "the USA government is listening in" was not part of the threat model of American corporations or individuals.  It really wasn't really in the threat model of most corporations and individuals in the world.  (Sure, those involved in illegal transactions, or the governments of those on the outs with the US - and to a limited degree even US allies - worried about this; but the former do not represent a large commercial market, and the latter don't rely on commercial software.)
Then came the Snowden revelations - though in hindsight there were plenty of hints earlier.  The Google security guys, to give one example in which some of their comments were reported, were furious to discover that their own government was tapping their lines.  The threat model expanded - and increasing all sorts of internal connections moved to encryption.
We distinguish between passive and active attacks.  Most of the NSA attacks on network infrastructure were "passive" in that they just listened - though of course they were very "active" in putting themselves in a position to "just listen".  But some of the attacks were "active" in sending packets to redirect connections to themselves - and the threat model had to be expanded again.  The response has been growing efforts to encrypt *everything*.
Apple (and presumably Android makers, though it's not entirely clear who the Feds go to) soon faced another "attack" from their own government:  Growing demands to gain access to the contents of locked iPhones.  This is an even more "active" attack, requiring Apple to actually do some work.  Another thread model expansion - and another response:  Encrypt the phones with a key inaccessible even to Apple.
So the government has expanded the threat model yet again - demanding that Apple write code to disable various protections.  The legal side of this will get played out, but there are already reports out of Apples response:  Make it impossible to update the OS without unlocking the phone.  You could argue that they should have done this to begin with - and frankly I'm a bit surprised they didn't.  It does add complexity to the process of recovering a phone that for software or hardware reasons has become non-responsive, but that's life.
So far, Apple (and others) have decided *not* to respond technically to demands for copies of information uploaded to their servers.  Apple could certainly encrypt the uploads.  It would have to do so using a key unavailable to them, which makes sharing across devices more complicated:  There would have to be a common key across all the devices sharing the software *distinct from* the key used to log in to the Apple account, since that's revealed to Apple every time you log in.  But it could certainly be done.  (Most on-line backup services already provide the option to use a private - never revealed to the service - key.)
It'll be interesting to see whether Apple and others offering cloud services move in this direction.  It would well and truly piss off the LE people who've gotten used to getting this information easily.
                                                        -- Jerry

@_date: 2016-02-25 16:24:44
@_author: Jerry Leichter 
@_subject: [Cryptography] Hope Apple Fights This! 
The US is currently suing Microsoft to gain access to data stored only on a server in England.  If they win, where you put the data becomes irrelevant.
Many countries are beginning to require that anyone doing business in that country store its data there.  The arguments are a bit different depending on where you are - the Europeans claim it's to protect their citizen's privacy by preventing the movement of their data to places where the privacy protections aren't as strong.  (It's been argued that the hidden forces behind these proposals are the intelligence services of those countries, which want to be able to get at their own citizen's data more easily.)
Your proposal is akin to all sorts of notions of data havens that have been around for years.  None has produced anything much, and given the direction laws have been going, they are unlikely to do much now.
                                                        -- Jerry

@_date: 2016-02-25 17:50:42
@_author: Jerry Leichter 
@_subject: [Cryptography] Hope Apple Fights This! 
Congratulations.  You've rediscovered the argument every kiddie comes up with to protect themselves from copyright lawsuits:  I don't actually have your protect music on my server.  I have a bunch of random numbers.  So does my friend across the street.  It happens that if you XOR the two together you get the music, but neither of us actually has your music....
It's nonsense.  You're acting as if judges were idiots.  They're not.
If you encrypt your stuff locally before putting it in the cloud, and hold the key yourself, you're protected against anything the cloud provider can do.  They can only deliver what they have (encrypted text that neither they nor the government can read), not what they don't have (the corresponding plaintext.)  This is much safer than any hacks for spreading the stuff around.
Add integrity checks if you're concerned about modification attacks.  Use replicas and error correction to deal with failures of individual replicas.
The rest is just noise.
                                                        -- Jerry

@_date: 2016-02-26 16:47:53
@_author: Jerry Leichter 
@_subject: [Cryptography] USG v. Apple, 
First off, just because something is covered by the same regulations as weapons for trade purposes doesn't make it a weapon.
But more importantly, the 2nd Amendment refers to "the right to bear arms".  What are "arms"?  Given the ambiguous context with its references to militias, most scholars today believe (and the courts have more or less held - this has never been fully pinned down) that "arms" are "the kinds of things that would have been considered appropriate arms for a militia member in the mid- to late-18th century", when the Amendment was written.  So tanks, to use an obvious example, are simply not covered.  Actually, machine guns are (probably - there are arguments and the Supreme Court has never directly addressed the question) aren't  either.  While cannon were around at the time, they are probably not covered because they would not have been the arms of an individual militia man.
Under this definition, cryptography is simply not in the same universe.
                                                        -- Jerry

@_date: 2016-02-27 00:03:26
@_author: Jerry Leichter 
@_subject: [Cryptography] USG v. Apple, 
Why?  If cars and bicycles are regulated on the road by the same laws, does that make a car a bicycle?  A bicycle a car?
People had chamber pots in the 18th century, too.  Members of the militia used them.  Does the Second Amendment thus cover chamber pots?  Does its protection extend to modern toilets?  If so, how are the Feds able to regulate the volume of water per flush?  Inquiring minds want to know!
The issue isn't whether this stuff existed in the 18th century, it's whether it would have been considered "arms" traditionally used by militia members.  Jefferson developed his machines for diplomats, not field soldiers.
If it had existed and been used by militias in the 18th century, maybe - though, again ... "used by militia members" is not the same as "arms".
                                                        -- Jerry

@_date: 2016-02-29 16:23:24
@_author: Jerry Leichter 
@_subject: [Cryptography] 9999 keys for this one iPhone 
Where's the unique ID?  I very much doubt it's in the flash.  I'd bet it's on the CPU chip - after all, the RAM is already there - and difficult to extract.  If so, the ability to clone the flash gains you nothing - without  a CPU chip with the right unique ID, you can't decrypt the flash contents - even if you know the passcode.
Yes, if you can run some of your own code on the CPU, you might be able to get it to give you the unique ID.  But that leaves you with the original problem:  How to get your own code to run on the phone without unlocking it first.
                                                        -- Jerry

@_date: 2016-02-29 20:11:06
@_author: Jerry Leichter 
@_subject: [Cryptography] 9999 keys for this one iPhone 
It turns out that Apple has actually done better than that.  This same discussion showed up on RISKS Digest and someone dug up an Apple description of the security mechanisms that covers the phone in question.  Here it is:
"Every iOS device has a dedicated AES 256 crypto engine built into the DMA
path between the flash storage and main system memory, making file
encryption highly efficient."
"The device's unique ID (UID) and a device group ID (GID) are AES 256-bit
keys fused into the application processor during manufacturing.  No software
or firmware can read them directly; they can see only the results of
encryption or decryption operations performed using them. The UID is unique
to each device and is not recorded by Apple or any of its
suppliers. ... Burning these keys into the silicon prevents them from being
tampered with or bypassed, and guarantees that they can be accessed only by
the AES engine."
The way in which the passcode is combined with the UID/GID isn't described but that's pretty simple stuff.
So you can clone the SSD and even the RAM of a locked device all you like:  It's useless to you without the information in the CPU chip, which you can't clone (without specialized, expensive attacks against the hardware).
                                                        -- Jerry

@_date: 2016-01-02 15:32:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Formal definition of lightweight crypto 
That's an odd definition.  After all, in some sense you can apply it to any security system:  There's no such thing as absolute security, but if the cost of breaking the security exceeds the value of what's being protected (suitably computed - all secondary effects included) then any additional expenditure on security is wasted.
It's also highly technology-dependent.  Even the weakest reasonable processor available today is pretty strong compared to the strongest processor available 30 years ago.  In the example you gave that I elided, you talked about "50-60 bits of security" - i.e., about as strong as DES, which for quite some time after it was introduced was at the limits of practical implementability with hardware of reasonable cost.
"Lightweight" implies a binary criterion.  What it doesn't *necessarily* imply, to me, is anything about strength.
Personally, I'd focus just on the hardware requirements:  Lightweight cryptography allows for some kind of useful cryptographic operation using very cheap, widely available hardware.  The definition is inherently technology-based and will shift over time, though more slowly than you might expect because the primary use case is in various kinds of embedded systems, which themselves change relatively slowly.
To achieve the goal of light weight, various tradeoffs might need to be made.  Strength may well be one.  But you could imagine, for example, a system which had a very cheap encryption operation but a much more expensive decryption operation - suitable for remote systems that have to "report home" securely but never, or almost never, need to receive secure commands.  (I don't know of any such systems; the closest idea is using small exponents to make RSA signature *generation* cheap at the expense of making signature *checking* more expensive.  The mechanisms we currently use for constructing symmetric cryptosystems do pretty much if not exactly the same operations in both directions, so none will have this property; but that doesn't mean there aren't ways to construct such primitives.  We've just never had reason to look for them.  Note that if you can make either direction cheap while the other direction is expensive, you can do two-way communication between a cheap remote and a strong center.  And, yes, you could make both directions cheap by using counter mode based on the "cheap" side - but perhaps a tradeoff in the system is that isn't very secure against encrypting large numbers of blocks with small Hamming distance between them.)
                                                        -- Jerry

@_date: 2016-01-03 20:57:25
@_author: Jerry Leichter 
@_subject: [Cryptography] How can you enter a 256-bit key in 12 decimal 
And it's unlikely anyone ever will.  The encryption has a 256-bit key, exactly as claimed.  The fact that there's no way to enter more than a tiny fraction of all possible keys is almost certain to be seen as a separate issue.
V-rated tires are good for a maximum speed of 149 mph and the rating doesn't change if the car they are mounted can make it to 100 mph, on a good day, going downhill with a strong wind at its back.
More to the point, a common Master combination lock has 36 numbers on the face and the combination is 3 positions long.  But in fact the is so much slop in the mechanism that "off by 1" usually works.  So instead of 36^3 combinations, there are really 12^3.  Fraud?
The legal system is good for many things, but this kind of fine-grained analysis of a technical issue ... isn't one of them.  Better to rely on education and informed consumers....
                                                        -- Jerry

@_date: 2016-01-05 03:18:31
@_author: Jerry Leichter 
@_subject: [Cryptography] How can you enter a 256-bit key in 12 decimal 
Since this configuration forces you to go through the hardware, it can easily impose, say, a limit on the total number of failed tries before it wipes the secret.  Or a limit on the rate of tries.  Either way, this makes an attack dependent on either a way to get the secret out of the hardware, or on some hack to prevent the hardware from tracking tries.  (This is, BTW, the iPhone model - and that latter bug was indeed present in earlier iPhones - you could cut the power between the time the phone rejected an unlock code and the time it updated its counter of failed attempts.  I believe that's since been fixed.)
                                                        -- Jerry

@_date: 2016-01-07 11:31:17
@_author: Jerry Leichter 
@_subject: [Cryptography] Chaum Has a Plan to End the Crypto War 
Actually, the problem is a bit broader than that.  The politicians will now be able to say "Look, you techies said this was impossible, but you were either just wrong or you lied.  It *is* possible.  Chaum showed you.  So now go off and build us the golden key."
Anyone who's worked as a tech person in industry knows the pattern.  A manager (or often a sales guy) comes along and says "We need to build X".  The tech person says "X can't be done".  But something that's impossible is an itch to a techie, and eventually he comes up with some really hacky, ugly method that'll cause his successors to curse him forever as they try to support it - and he hints "well, maybe if...".  And sure enough, X now becomes a requirement.
Even after being burned repeatedly, I still have to discipline myself to stick to "no, it's impossible" in such situations.
                                                        -- Jerry

@_date: 2016-01-07 16:20:17
@_author: Jerry Leichter 
@_subject: [Cryptography] Chaum Has a Plan to End the Crypto War 
Actually, Chaum may end up having done us a favor.  I've only skimmed the paper, but the essence of it is to separate the expensive operations out.  So you get a pre-computation phase which does public key operations, and a real-time phase that uses the pre-computed values repeatedly to do fast, secure routing.
There's nothing magic about the use of 9 forwarding nodes; the algorithm works fine for any n.  There's inherently no requirement that you use *the same set of nodes each time*.
Tor is fully transparent to someone who controls all the nodes - exactly as is the case for cMix.  The difference is that Tor uses a whole bunch of nodes, and you pick some small number to do your forwarding.  *Your* particular forwarding is fully transparent to someone who controls *all* the nodes you picked.  We just assume that there are enough "good" nodes in Tor that an opponent it unlikely to control all the ones you chose.  As far as I can see, the exact same thing could be said of cMix.
Looked at another way:  If you want a Tor that has a "golden key", simply build one with all nodes under the control of people who will respond to "appropriate" demands from LE/spying agencies.  So ... nothing really new in the "golden key" proposal, just Chaum looking for publicity (and doing the damage others have noted along the way).
On the other hand, the algorithms themselves appear to be quite worthwhile.  I expect others will pick up the ideas - if they are good - and will build systems that have the good features of cMix without layering over it the "transparency" stuff.
BTW, this is another of those "provably secure if problem P is hard", which of course is no more useful than the strength of the evidence that P is, indeed, hard.   Here, P appears to be the decision-Diffie-Hellman assumption for prime order cyclic groups.  But based on a quick search, DDH is known *not* be be hard in those groups!  Perhaps I'm not understanding his assumptions, or not applying them correctly to the known results.
                                                        -- Jerry

@_date: 2016-01-08 06:44:11
@_author: Jerry Leichter 
@_subject: [Cryptography] FTC sues for crappy crypto 
The FTC can't make up rules for what goes into software.  No vendor is under any obligation to include cryptography, strong or otherwise, and I'd say that's as it should be:  Not every application *needs* cryptography; not every buyer *wants* it.  But no seller is, or should be, free to outright lie about what it's selling.
Now, you may say that software that maintains medical records "clearly needs" cryptography.  That may be, in some broad sense, but there's no law mandating that - and if there were, it would make more sense for the mandate to apply *to the medical provider* and not to the seller of the software.  The same software might be used, say, by a veterinarian - where mandating encryption seems rather an over-reach.
I'm not anti-regulation myself - I think there are many things that *need* to be regulated.  But regulation should be targeted where it's appropriate - and it should be the minimum regulation needed to accomplish some important policy goal.  Demanding that the FTC define "strong cryptography" and then mandate, on its own, which applications require it, strikes me as a good way to generate tons of new regulations that will do more harm than good.
Keep in mind, too, that any regulation has to be quite explicit - to cite the kind of thing that drives people nuts, you can't write a regulation that says "fire extinguishers must be at a height that most people can get at" because no one will be able to objectively test if someone is in violation; so instead you end up with a mandate that says "fire extinguishers must be hung 60 inches above the ground" and you end up fining people who hang them at 61 inches.  Also, once a regulation is in place, it becomes almost impossible to change it because of the costs.  That's a disaster in a field moving as rapidly as cryptography has been.
                                                        -- Jerry

@_date: 2016-01-08 15:05:33
@_author: Jerry Leichter 
@_subject: [Cryptography] FTC sues for crappy crypto 
There's no such thing as an indirect mandate.  You're either covered by HIPAA, or you aren't.  And HIPAA is surprisingly narrow.  (For example, college students have been unpleasantly surprised to learn that their records at campus health centers are *not* covered.)
Even if HIPAA were broader, the maker of an app is not itself receiving any health data, and it would be a stretch to cover them - any more than the manufacturer of the PC used to maintain the data is under any obligation to ensure that the PC has FDE turned on, is physically secured, what have you.
*Claiming that your app provides protection in compliance with HIPAA rules* doesn't bring you under HIPAA - but if it's a lie, the FTC can come after you for your false claim.  As, indeed, happened here.  The same would apply to a finance application claiming to provide "industry-standard encryption support" - assuming, as is surely by now the case, that everyone in the relevant industry understands what "industry-standard encryption support" is, and the application only implements ROT-13.
If the supplier removed all claims of HIPAA compliance, but marketed to dentists treating human beings within the US, the FDA would likely still go after them, claiming that their marketing, whether using the words or not, was clearly predicated on the assumption that those dentists would believe that they could use the system and stay within the legal limits.  (It might be a harder argument if the company fought back, but my guess is the FTC would win.)
If the supplier only marketed the software outside of the US, and some dentist bought it overseas and brought it home - as far as I can see, the dentist would be in trouble, but the company should be in the clear.
                                                        -- Jerry

@_date: 2016-01-09 08:30:45
@_author: Jerry Leichter 
@_subject: [Cryptography] Plan to End the Crypto War 
Am I the only person who's even *looked* at the paper?  If we're talking about Chaum's proposed system, it works *nothing like this*.  The nodes in the system reveal pieces of information that identify a *single user* of the system, and the messages he's sent. There's no single "golden key" which, once revealed, opens up the entire system.
You might ask why users can't simply establish new identities.  That's actually another part of the assumptions:  That the nodes only allow the participation of users who provide "good identities":  The paper suggests mobile phone numbers, photos, and so on.  These are combined (by all the nodes - the user presents partial information to each node) to create the identity that the system uses; you can't just make up your own.  The *assumption*, of course, is that users can't effectively lie, and that the information available to the nodes will always effectively identify them.  Government "Internet access license", anyone?
The algorithms and techniques look nice, but the whole business with accountability really only works under the assumption that someone puts together a service that "follows the rules" - *and everyone, particularly all the bad guys, choose to use that service, not simply implement their own*.  Granted, that does provide *some* degree of protection:  If *most* people use the government-approved system, anyone trying to reach mass audiences (spammers?) will be unable to do so anonymously.  But it seems like quite a lot of government involvement to relatively little effect.  The bad guys want to talk anonymously *to each other*, and will no more choose to use the government-approved system than they would be to simply talk openly.
Note the difference between *anonymity* and *privacy*.  You could also have said, back in Clipper days, that the bad guys wouldn't use Clipper, they'd use their own stuff.  But if the vast majority of people had used the encryption delivered by Clipper chips in their consumer goods, the bad guys would be forced to use it at all their touchpoints with the outside world.  Yes, they would have secure communication among themselves, but when they went to rent a truck or buy a plane ticket, they would have to use a back-doored mechanism.
It's clear that this has been NSA's driving strategy since then:  Grab as much as you can from everywhere you can, because even if the bad guys do manage to exchange secure messages among themselves, they will have to also rely on (insecure-to-NSA) systems to actually *do* anything in the real world.
There's some validity to this approach, because everyone needs privacy in many of their messages.  But *anonymity* isn't really something most people want or need most of the time - and, in particular, most people almost never need or even *want* to receive anonymous messages.  The real-world interactions that most communications mediate inherently require that the parties identify themselves to each other.  There's not much point in buying something on Amazon if you don't provide them with sufficient information for them to deliver it to you!  (Mix networks for packages, perhaps?)  So its hard to see how a government-approved anonymity system would find many customers.
The "accountable" part of Chaum's "accountable anonymity" strikes me *mainly* as a solution in search of a problem.
                                                        -- Jerry

@_date: 2016-01-16 18:42:58
@_author: Jerry Leichter 
@_subject: [Cryptography] TRNG review: Arduino based TRNGs 
If you really want to get stuff through the shielding, you use Terahertz radiation.  At these frequencies, it's the pins and lines inside the chips that act as antennas.
In principle, not hard to shield against - but small holes in the shield, openings around any through-holes, etc., can ruin all the careful work.  Not the kind of noise you run into in normal operation, and I doubt most people will have the equipment needed to do appropriate tests.
(I don't know if anyone has explicitly tried to bias a hardware RNG this way, but this is the way modern "bug detectors" work:  They're actually non-linear-junction detectors that emit Terahertz radiation and look for the harmonics generated when the radiation leaks into chips and is transmitted through any PN junction.  Note that this works just as well on power-off devices as on active ones.  A full test also includes a metal detector:  A shield good enough to block the Terahertz scanner will have enough conductive material to set off the metal detector.  I'm sure there's some counter to this - the battle between those hiding the devices and those trying to detect them is never ending.  I suppose vacuum tubes would work....)
                                                       -- Jerry

@_date: 2016-01-19 16:11:34
@_author: Jerry Leichter 
@_subject: [Cryptography] WSJ notices that home routers and such often have 
and that no one is trying to fix them.
I can't give you a working link unless you have a WSJ online account, but if you search for the following, you should get there:
"Rarely Patched Software Bugs in Home Routers Cripple Security
Wi-Fi devices, vulnerable to hackers, show difficulty of updating software after release"
It's actually an very well written and accurate article - especially compared to the typical mainstream press article on tech-related issues - and it's on the front page of today's paper issue.
                                                       -- Jerry

@_date: 2016-07-03 07:21:48
@_author: Jerry Leichter 
@_subject: [Cryptography] Android Full Disk Encryption Broken - Extracting 
The underlying blog posts on github are well worth reading for an understanding of what hacking is these days.  (FYI, it's not *all* that different from hacking back in the early 1970's - the basic approach, the style of thinking, the general things you look for and abuse, even some of the basic problems you need to solve,b were the same back then - but the scale and intensity necessary to break through these days is so much higher.  Then again, the tools are better, too.  If you still think not releasing source code is protecting you, well, I can get you a good deal on a bridge in NYC.)
While the headlines are about Android FDE, what the attacker here has done is show how to take over the entire TrustZone used in standard ARM chips.  This is a mechanism somewhat like the TPM in x86 chips:  An isolated environment that can hold secrets and perform various operations in a way that even the operating system can neither interfere with nor even observe.  Unlike Apple's "Secure Enclave" - which has limited capabilities implemented entirely in hardware - TrustZone is a protected mode of operation in which arbitrary software can be run - and what's running in it is the equivalent in a small OS.  That OS is then used to load some fairly substantial bits of software - including some stuff to implement DRM, which in fact happened to provide the entry point used in these attacks - though there's nothing specific to DRM that opens the way to the attack.
Lessons?  Generality and power lead (to complexity, which is the enemy of security.  Even fairly simple and apparently limited systems have been hacked by turning their own mechanisms against them (see the classic attacks against IBM HSM's).  But the more you add, the easier this becomes.  In particular, making a mechanism to implement secure key management general enough to implement DRM is a mistake.  KISS - Keep It Simple, Stupid - needs to be the guiding principle.
                                                        -- Jerry

@_date: 2016-07-07 17:53:19
@_author: Jerry Leichter 
@_subject: [Cryptography] "Android Keystore Encryption Scheme Broken, 
It's a complex key-size downgrade attack whose actual real-world significance is unclear.  But I found the following quote from the authors of interest, given our recent discussion of simplicity:
Intuition often goes wrong when security is concerned, the two write, Unfortunately, system designers still tend to choose cryptographic schemes not for their proved security but for their apparent simplicity. We show, once again, that this is not a good choice, since it usually results in severe consequences for the whole underlying system.
                                                        -- Jerry

@_date: 2016-07-09 06:12:32
@_author: Jerry Leichter 
@_subject: [Cryptography] What to put in a new cryptography course 
It's *not quite* like that.
The *generic* discrete log problem (over the integers mod some N, or over an elliptic group) is considered hard because no one, after years of work, has found any attacks on it.
There's no *general* result that says these problems are hard.  What we have instead is some specific, very powerful attacks against particular instances of the problem.  We eliminate those particular instances from consideration and say "the rest look good".
*As a matter of theory and mathematics*, we're living in sin:  We assume the problems we pick are hard because our best minds have brought our best methods to bear on them and have not only made no progress, they've managed to prove that *using these techniques* no progress is possible.
Note that the same is true of RSA - and even of factoring, which *may* in fact be harder; we still, after all these years, don't know for sure.
Then again, the same is true for all of our other cryptographic primitives.  The best we can say about, say, AES is that it's secure against all known attack methods.
That's the state of our knowledge in cryptography today.  We use what we use as a matter of engineering, not science.
It's worth noting that one of the critiques of "provable security" is that when you dig deep into much of the work, you find that it starts off with assumed-hard problems that are rather specialized - a paper will seem to be assuming the difficulty of discrete logs, but it's *actually* assuming the difficulty of some decision problem related to discrete logs.  We may have no idea exactly how the difficulty of that specialized problem relates to the general discrete log problem, and it's unlikely all that much effort has been directed at the specialized problem.  See, for example, "The Brave New World of Bodacious Assumptions in Cryptography"                                                         -- Jerry

@_date: 2016-07-13 23:11:58
@_author: Jerry Leichter 
@_subject: [Cryptography] The Laws (was the principles) of secure 
Virtual:  Something that appears to be there although it isn't.
Transparent:  Something that appears not to be there although it is.
When someone tries to sell you something that's "virtually transparent", you know what you're getting.
                                                        -- Jerry

@_date: 2016-07-15 19:29:25
@_author: Jerry Leichter 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
There are plenty of proofs around of complexity of algorithms, starting with the familiar "sorting is O(n log n)".  (Which is good but also illustrates a significant point:  Sorting *where the only operation available is order comparison of pairs* requires O(n log n) comparisons.  Radix sort of O(n) - ignoring the dependency on key size.)
It's probably not hard to construct an artificial problem specifically so that you can prove it requires O(n^20) operations.  (He says glibly....)  Natural problems (*not* in the Razborov/Rudich sense of a natural proof, just problems that have arisen naturally rather than being constructed for a particular purpose) with proven lower bounds tend to be bounded by fairly low-degree polynomials.  O(n^2) is reasonably common; there are probably some O(n^3) algorithms around.  I don't off-hand know of any natural problems with a known higher bound, but they may be out there.
One could probably construct cryptosystems from some of these problems.  There are three issues, though:
1.  The results are worst case.  There could easily be many "easy" instances, and it may be as difficult as solving the particular instance to determine if it's "easy" or not.  Some of the results on the difficulty of approximations might lead to a way around this.
2.  Even if we can avoid the easy instances, you really need to know more than that the problem is O(n^2) - you need the constant in order to know what actual n's meet particular security specs.  This is known in a few instances - actually, for sorting using comparison, we can give the exact value by counting arguments - and not known in others.
3.  Even if you have the exact complexity, for low-degree polynomials you'd need such large values of n that the whole thing may fall apart.
Still, it's an interesting question as to whether one can construct a practical algorithm this way.  Here's one that *almost* works:  You can construct a permutation and its inverse in linear time.  But given a permutation, computing its inverse is equivalent to sorting, so is O(n log n).  So you can get an asymmetric key system by publishing the permutation, encrypting by applying it, and decrypting using the inverse (the private key).  This only gives you a log n advantage over an attacker so n would have to be immense - and of course that doesn't really work because of radix sort.
                                                        -- Jerry

@_date: 2016-07-16 19:18:08
@_author: Jerry Leichter 
@_subject: [Cryptography] hard natural problems 
I'm not sure how to treat the complexity of these things.  The chaotic systems that are usually studied have very simple, low-complexity forward computations. But ... they are generally defined over the *reals*.  The extreme sensitivity to initial values is present in the real computations.
What exactly happens when you use finite precision arithmetic to compute these functions isn't at all clear (though of course we do it all the time).  You're layering another level of fuzziness on a problem that was fuzzy (no, not in the technical sense) on top.
FP arithmetic has "one-way" functions even without chaos.  Here's a classic example:  Given the interest rate, the number of periods, and the initial loan of a fixed-rate mortgage, computing the monthly payment is trivial.  But suppose you want to determine the interest rate that will result in a given monthly payment.  This turns out to be the solution to a high-degree polynomial (the degree is the number of periods) that's extremely ill-conditioned.  It's impractical to solve it.  (You can, of course, simply run the forward problem repeatedly to search for approximate solutions.  Newton-Raphson may help.)
Can one turn computations over some computable approximation to the reals (you don't have to use floating point - fixed precision might work; a more interesting approach is rationals with bignum numerators and and denominators, or maybe continued fractions) into a usable cryptosystem?  I vaguely recall proposals for such a thing many years back, but I don't know of any practical designs.
I also don't know what the proper measures of complexity/difficulty are in this domain.  Counting operations - when an operation might be an arithmetic operation on bignums whose lengths grow as the problem proceeds - won't cut it.
                                                       -- Jerry

@_date: 2016-07-21 07:08:08
@_author: Jerry Leichter 
@_subject: [Cryptography] The Laws (was the principles) of secure 
I'm not sure of the continuing validity of Morris's analysis.  Big data, sophisticated analysis of metadata, all kinds of correlation analyses, have made it very difficult to know exactly what data I can safely "not care if its known".  I would have thought that no one would care if Netflix made anonymized records of movie preferences known, because without identifying information, hey, it isn't even "my data".  And yet that data was de-anonymized.
Granted, in that case, for most people, their viewing preferences were *also* something "they didn't care about".  But that's just a very early and telling example.
Note recent discussions about whether Wikipedia should use HTTPS.  All the information there is public.  What's the point of encrypting it?  And yet....
Personal story:  When drugs, poisons, explosive chemicals, whatever get mentioned in TV shows or books, I have a habit of looking them up on Wikipedia. Often there's little connection between what the substance actually does and what it's claimed to do in fiction, but I happen to like to read about the chemistry and pharmacology and such of these substances.  Just last night, I got around to reading The Martian - which lead to an interesting tour through the chemistry of hydrazine, and then of sodium azide - formerly used in air bag inflators so perhaps readily available in millions of junked cars, and a poison with a potency similar to cyanide.  Who knew?
But I do worry sometimes about what a prosecutor might make of my Wikipedia reading habits....
                                                        -- Jerry

@_date: 2016-06-05 16:32:22
@_author: Jerry Leichter 
@_subject: [Cryptography] "Physical Key Extraction Attacks on PCs" 
The underlying work seems mainly to be about two years old.  The authors demonstrate a technique for determining keys (RSA, El Gamel, EC-DH - by looking at various low-frequency physical emanations (acoustic, electrical coupled through the ground plan, electromagnetic) that they can force to be correlated with key bits by using (sometimes adaptive) chosen plaintexts that will present "poisoned" values to the multiplication and exponentiation steps.
Beautiful work, but it raises two questions about actual applicability:
1.  The cause of the correlation in the multiplication routines is that there's a shortcut in the particular routines they look at that skips over "digits" that are all 0 in the bignum representation.  This seems like a completely pointless optimization in a cryptographic context, where the inputs are (except under attack!) random looking, and thus almost never actually 0 (1 in 2^32 in typical 32-bits-at-a-time representations).  Why bother with this optimization?  Is it just a side-effect of using general-purpose bignum routines - in which case that teaches us a valuable lesson right there....
2.  Much more generally:  The attacks rely on being able to send chosen ciphertexts into the public key functions.  The authors mention the use of encrypted emails, web pages, or encrypted files.  And yet ... who applies public key algorithms to presented data directly?  We've known that's hazardous for many years - but in any case, even today, it's very expensive.  The typical pattern is to apply public key functions to cryptographically secure hashes of data.  If we assume the hash is, secure, generating plaintext whose hash has particular desired properties isn't practical.
The case of DH, where the value being computed is the product of the value received and a random value computed by the recipient (which can't be known or controlled by the attacker), is a bit different, as the attack only seems to depend on being able to inject a single "poison" multiplicand.  The *particular* attacks here should be easily detectable, as the very unusual properties of some of the intermediate values stand out.  But that of course says nothing about other attacks with different characteristics.  Blinding seems like the best defense, though as the authors point out, it can be expensive.  (Also, to blind the value, you have to multiply the value you receive by the blinding value - and by hypothesis *that* leaks the information right away!)
                                                        -- Jerry

@_date: 2016-06-09 17:20:24
@_author: Jerry Leichter 
@_subject: [Cryptography] Rumor has it that AES-256 is broken (again!) 
That could conceivably allow an attack on RSA, but AES?  A connection between factoring and AES would in and of itself be an huge (and totally unexpected, and vanishingly unlikely) deal.
What's in the linked report makes no sense.
It's not out of the question that someone has come up with a new approach to factoring, but if so we're talking major award material.  Not impossible, but highly unlikely.  Certainly not anything to worry about without more information - and absolutely unrelated to AES.
                                                        -- Jerry

@_date: 2016-06-11 12:48:36
@_author: Jerry Leichter 
@_subject: [Cryptography] GNU's "anonymous-but-taxable electronic payments 
You have to give up *something* to prevent double spending, but not necessarily network connectivity.  You could, for example, insist on attested hardware/software to carry out the transactions.  If I can trust that the payment fob you present is running unmodified official software on unmodified official hardware, and you can trust the same about my fob; and, of course, we trust that that software "does the right thing"; then trusted movement of money between my fob and yours is no big deal.
The last couple of decades of work on cryptography has mainly concentrated on a model in which (a) endpoints don't trust each other; (b) there are no trusted third parties; (c) communications mechanisms can't be guaranteed to deliver messages unmodified or un-intercepted, but they *can* be trusted to be available and actually deliver messages between the parties as specified.  (Early on, the "positive" sides of (c) weren't assumed because they couldn't be provided in the real world.  That led an an even weaker and more limited model.)
As we've learned, you can accomplish much more within this model than anyone would have thought possible 50 years ago  - but there are costs and tradeoffs.
If you modify or eliminate any of the assumptions, you can get different solutions.  The blockchain is a quasi-trusted-third-party, for example.
A big part of (a) is the assumption that endpoints are just software running on general-purpose machines.  This turns out to have all kinds of costs, both in terms of the difficulty of others trusting you - and even of *you* trusting your own endpoint.  In fact, that last bit - your inability to really gain trust in your own endpoint - is a topic of a huge amount of discussion on this list!  This is an area that's been explored, but proposed solutions have been rejected by many because of the potential for abuse.  How to keep the good features without *allowing* the bad is ... unknown.  It may simply not be possible.
It's curious, BTW, that those who reject solutions with many good properties "because they can be used to enforce DRM" are perfectly happy to accept solutions that allow for, say, truly anonymous extortion. I'm not recommending one set of solutions over the other - looking back, I'd have include myself among those who had both of these reactions.  An honest appraisal of the pluses and minuses is the only way forward.
                                                        -- Jerry

@_date: 2016-06-12 06:34:12
@_author: Jerry Leichter 
@_subject: [Cryptography] Determining TLS session keys from the hypervisor 
Very elegant and powerful attack in which code running in a hypervisor can extract the keying material for any TLS session a guest establishes.  The basic ideas:
1.  Use packet filters to monitor traffic and recognize the beginning of TLS connections - in particular look for Server Hello and Client Finished messages. Observe that (a) at the time of Server Hello, the keying material *is not* present in the VM's memory; (b) at the time of Client Finished, the keying material *is* present in the VM's memory; (c) Client Finished is encrypted using the agreed-upon key; (d) Client Finished contains some known plaintext.
2.  In order to support efficient snapshots of the memory state of running VM's, hypervisors support the ability for the hypervisor to receive a stream of pages as they become "dirty" (i.e., are modified).  So at the time of Server Hello, subscribe to receive such a stream and save it; at the time of Client Finished, disconnect the stream.  Since the keying material was written to memory somewhere between those two messages, it must appear somewhere in the pages saved.  (Experimentally, this is typically a few megabytes of data.)
3.  The TLS key block - containing all the keying material - has a fixed length and layout, and appears somewhere in the saved data.  Try all possible positions for it, using the known plaintext of the Client Finished message to test the proposed key.  (This is the only place we assume anything about the code running in the attacked machine:  We need to know what a TLS key block looks like.  There aren't that many TLS implementations around, and which one is being used should be easy enough to determine by once examining the VM's memory - or, for that matter, the file that was exec'ed to start the server - so this is no big deal.
Depending on the ciphersuite chosen, some straightforward attacks are linear in the number of possible keys while others are quadratic (because, e.g., one needs to guess at the IV as well).  But there are clever optimizations that reduce some of the quadratic attacks down to linear time.  Ironically, AES-GCM - the most widely used ciphersuite - takes linear time and doesn't even require known plaintext:  The authentication tag can be leveraged to validate the block directly.
The attack can be generalized to cover the more realistic case in which a the server being attacked is initiating multiple simultaneous connections; things get slightly more expensive but all the keys are extracted.  Typical "brute force" times are a couple of seconds, and of course a brute force attack is trivially parallelizable.
The attack slows the server a bit, but the additional time per connection is a few milliseconds - very hard to detect among network overheads.
Defenses are not clear.  A library could randomize the layout of the key block it uses per connection.  It would have to compute and store the parameters *before* Server Hello or the attacker would simply target that material.  But this strikes me as an expensive defense that would only slow the attacker down a bit.
                                                        -- Jerry

@_date: 2016-06-13 06:34:06
@_author: Jerry Leichter 
@_subject: [Cryptography] Newbie cryptoanalysis question 
Unless the function is very simple-minded, determining it strictly from a black-box implementation is a very challenging proposition.  There are just so many possible quite simple transformations.
If all you want to prove is that no secret is involved, there's a fairly straightforward procedure:  Take two different instantiations of the same code (which should presumably have different secrets) and show that they produce the same output for different inputs.
If you're not in a position to do that, the notion of a "secret" in the algorithm makes little sense:  The "secret" *is simply part of the algorithm".
If you want to go further:  A good signature should have the property that, on average, flipping an single bit of the input flips about half the bits in the output.  That isn't hard for fairly simple functions to achieve, but it takes a designer with at least a bit of understanding of the situation to decide to try it.  Similarly, runs of increasing values in the input should produce a random collection of increasing/decreasing values in the output.  Other simple transformations of the input - e.g., exchanging pairs of bytes - should similarly result in no obvious patterns of changes in the output.
All of these are heuristic tests.  Again, it isn't particularly difficult to write a function that's completely insecure against anyone who doesn't know the algorithm, but that will pass all these tests.  For example:  Suppose the code is in Java.  Appending all the inputs into a string, adding some constant value to the end, and taking the hash, will *look* quite strong.  (OK, the result is only 32 bits, which is too short.  So generate a couple of different strings with slightly different constants, hash, and combine the results.)
The constant value(s) can vary per installation - perhaps the date and time of installation is already being recorded somewhere, or maybe a license number is available - in which case the result will even appear, from the outside, to be based on a "secret".  Against someone who has no access to the implementation, this would be very difficult to reverse engineer.  Of course, to someone who has access to the compiled code and a Java disassembler, it's at most a few hour's work....
                                                        -- Jerry

@_date: 2016-06-13 14:53:05
@_author: Jerry Leichter 
@_subject: [Cryptography] Determining TLS session keys from the hypervisor 
This leads to an interesting challenge:  How could you design a system in which it would be possible for a hypervisor to prove to a guest OS that something like Secure Memory Encryption is enabled, and remains enabled?
One way would be to provide a non-virtualizable instruction that would check the SME status of a page.  Yes, a hypervisor could rewrite the code to eliminate the instruction - but if that's within your attack model, there's clearly nothing you can do, as you have no control at all over what runs in the guest.  (There are various mechanisms out there - Microsoft has some papers on the problem - that make this considerably harder, though it'll always be possible.)
Of course, a hypervisor could turn SME off after the fact, so you'd also need some kind of callback - directly to the guest, not through the hypervisor's typical mediated dispatch - if the setting ever changes.
Granted, ultimately you have to trust the chip to get these instructions right.  There's not really a way around trusting *some* base level.
There's a neat proposal out there - I read the paper but don't recall exactly where - that uses the inverse of this idea:  If you trust your hypervisor, it allows you to run trusted processes *inside an untrusted guest OS*.  The basic idea is that the hypervisor leaves pages that are declared by a process to be "sensitive" inaccessible even to the guest OS.  Attempts to access such a page trap to the hypervisor as always.  If they come from user mode, the page is decrypted and rendered accessible; if they come from the guest OS, the page is encrypted (using an authenticated mode) and then rendered accessible to the OS level.  On a transition from user to OS mode, the pages are rendered inaccessible again.  So both user mode and OS mode can see the pages - but OS mode only sees an encrypted version, and if it tries to modify the values, the hypervisor will know the next time it decrypts.  (Many details to deal with, but they actually have it all working.  The guest OS is unmodified.  You can even have individual processes within the guest OS that run in encrypted mode while others don't.)
                                                        -- Jerry

@_date: 2016-06-13 14:58:20
@_author: Jerry Leichter 
@_subject: [Cryptography] Determining TLS session keys from the hypervisor 
Should have read more of the paper.  This is dealt with.
                                                        -- Jerry

@_date: 2016-06-13 19:23:29
@_author: Jerry Leichter 
@_subject: [Cryptography] Determining TLS session keys from the hypervisor 
Well, sure ... but the fact is that a large and probably growing percentage of Web servers *are* running "in the Cloud" - i.e., on hardware controlled by someone else.  The economics are likely to push ever more stuff "out there".
Now, you can argue that Amazon or Microsoft or Google - or the smaller providers of virtual hosting - are not going to attack you; and *in general*, that's probably true.  But they may well be forced to by government order - without telling you.  And when it comes to the smaller providers - just how much should you trust them?
It's also worth keeping in mind that bugs allowing attackers to escape from their guest OS instances and gain access to the hypervisor have occurred.  So even if you trust your provider, you do have *some* level of exposure to your "running mates" on the host you share.
As will all (properly analyzed) security issues, it's about costs and risks.  By moving out to a Cloud instance, you're generally saving money and you're mitigating many practical risks - the big data centers are much less vulnerable to power outages, fires, and all kinds of similar events than you would likely be able to afford.  This paper shows that you're not quite as secure within your VM as you might think.  You now have to go make the tradeoffs for yourself.
                                                        -- Jerry

@_date: 2016-06-27 12:12:26
@_author: Jerry Leichter 
@_subject: [Cryptography] RFC: block cipher randomization 
It doubles the size of the data to be encrypted while protecting against - what, exactly?
Analytic attacks the depend on some property of the cipher are likely unaffected.
Known plaintext attacks may or may not be affected.  Probably not - many of them only need to know a small amount of the data.
Chosen plaintext attacks are probably affected, assuming they need to choose the entire contents of a block.
Brute-force attacks are essentially unaffected:  A brute force attack needs some way to judge whether the decryption is likely successful, which means that the plaintext has some recognizable format - more generally, that it's entropy is low.  For example, that it's English text.  That will stand out pretty trivially even if half the bytes are random noise.
As steganography, it's pointless:  Either the attacker has a way to decrypt the data, or he doesn't.  If he doesn't, the cipher text is "just random noise" either way.  If he does, it's not clear what this has accomplished.  Sure, if the "random bytes" are actually the encryption of the real information, *and the attacker doesn't know this game is being played*, he'll miss it - but there are plenty of much easier ways to do steganography *when the attacker doesn't know it's there*.
I just don't see this adding enough value to make up for the cost of (a) cutting the encryption throughput in half just to start with; (b) adding the cost of generating random bytes at the same rate as the data rate; (c) adding the cost of computing the gamma function.
Meanwhile, it's not really "harmless".  For one thing, it's providing a potential covert channel *to an attacker*!  Someone who can influence the "random" bytes can slip information out under your nose - at the same rate as you *you* can send data!
Also, there's a long history of attacks against protocols that insert data that's unpredictable to the receiver - hence *uncheckable by* the receiver.  Other than a covert channel, no attacks come to mind immediately - but that doesn't mean they aren't there.
                                                        -- Jerry

@_date: 2016-06-27 21:37:26
@_author: Jerry Leichter 
@_subject: [Cryptography] RFC: block cipher randomization 
If you're going to go this route, there's little reason to believe that you can beat the classic DESX construction - W1 XOR DES(W2 XOR P) - where P is the plaintext and W1 and W2 are "whitening" keys.  This construction was analyzed by Rogoway years ago - quick intro at  - and is surprisingly strong.  (Granted, its strength is analyzed against brute force attacks - an issue for DES but not for modern ciphers.)  Still, it will hide any fixed pieces of plaintext quite nicely.  (And, yes, you need to apply the XOR both before and after encryption or the construction adds little.)
                                                        -- Jerry

@_date: 2016-06-28 16:59:41
@_author: Jerry Leichter 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
You're actually displaying the race condition that's lurking at the heart of the traditional view.
Alice as offeror controls the offer, and can revoke at any time - until there's a "meeting of minds".  Except that we don't want to say there's a "meeting of minds" until Alice "actually knows" that Bob has accepted.  Is that when the letter (or email) arrives at Alice's mailbox?  When she opens it?  When she reads it?  If it's in a foreign language she doesn't understand (or if its encrypted), does it wait until it's been translated/decrypted, or does it occur immediately?
You could probably make an argument for any of these in the right circumstances I suppose, but in practice the evidentiary issues would be so difficult that what actually happened in the real world becomes irrelevant.  (Shock, horror:  The reality doesn't matter!)  Here the legal discussions stop and questions about who the judge or jury, as the case may be, finds more believable take over - the ultimate fallback in the legal system, and something that's always missing from these attempts at technological solutions that have no edge cases.
The Postal Rule is one (almost) extreme, making the postal service a third party and defining the "meeting of minds" to have occurred upon delivery to that third party - even though Bob himself is completely unaware of it, and the third party has no idea what is actually being communicated.  Seems very odd - but it does have exactly one significant advantage:  It's easy for Bob to prove what he did!  (Oh, sure, you can gen up oddball examples - Bob claims the registered letter he sent was an acceptance, Alice claims it simply contained blank sheets of paper.  Back to believability....)                                                          -- Jerry

@_date: 2016-03-01 05:48:27
@_author: Jerry Leichter 
@_subject: [Cryptography] The FBI can (almost certainly) crack the San 
Sigh.  This has become a meme so quickly, and it's just wrong.
You can clone the memory chip of an iPhone.  But even in the iPhone 5C in question, that doesn't give you the chip UUID, which is embedded in the processor - which provides no way to read it.  Without the UUID, knowing the lock code doesn't tell you the encryption key.
We've spent so many years working with universal Turing machines and software and virtual machines that we sometimes forget that underneath it all there's actual hardware, and that universal replicators of hardware don't exist....
                                                        -- Jerry

@_date: 2016-03-02 14:56:38
@_author: Jerry Leichter 
@_subject: [Cryptography] The FBI can (almost certainly) crack the San 
You are, of course, correct.  I should have read your page rather than just doing a quick skim.  An attack based on cloning the phone has become a quick meme - but in most cases, those repeating it don't understand what's involved and promptly suggest that once you have a clone, you can run a large number of copies - someone even suggested doing it in AWS, ignoring the difference between ARM and x86 code among many other issues - and get the passcode quickly.
I will point out that this attack assumes that the external flash is the only form of non-volatile storage in the phone.  Counting to 10 retries only requires 4 bits of such storage on the CPU chip.  If Apple actually implemented it that way (obviously there would be many details - it's not just a matter of storing the count there), the cloning attack would fail.  However, I don't know one way or another.
                                                        -- Jerry

@_date: 2016-03-02 15:38:02
@_author: Jerry Leichter 
@_subject: [Cryptography] iPhone hardware attacks 
The NSA can almost certainly do this, if they feel the need (or are just making sure they have the capability ready to go should a need arise).  It's possible to make hardware that is extremely resistant to any such probing, but it's expensive, difficult and of course new attacks always emerge - see your own posting a little earlier about side-channel attacks - so what's secure today has to be updated tomorrow.  Security has to start with a threat model, and building an iPhone to consider the threat model of physical attacks by the NSA would make no sense.
Then again, up until the iPhone 5, the threat model didn't consider someone fairly sophisticated in long-term physical possession of the phone.  And until this case the threat model didn't include the possibility of Apple being ordered to break the security mechanisms.  It's clear that thread models are growing....
I suspect this is one of those "maximum possible truth" things where if you parse it really carefully, you discover they haven't said quite what you think they said.  For example, it might be the case that NSA has the capability to do this, but the lawyers have rendered an opinion saying that they are legally not allowed to do it.  So that falls within "can".
Or NSA may simply consider their capabilities so secret that no one within the FBI is cleared even to be told what they can do, so when FBI asks NSA the answer  that comes back is always "we can't comment".
Since it's clear that the FBI considers this a wonderful test case, they have little incentive to get as full an answer as possible.
                                                        -- Jerry

@_date: 2016-03-02 20:23:22
@_author: Jerry Leichter 
@_subject: [Cryptography] The FBI can (almost certainly) crack the San 
Frankly, as far as I can tell, pretty much *everyone* is more secure in the walled garden.
The day when any one person understood everything about what was happening on any useful computer system are long passeed.  Hell, even the days when any one person was familiar with all the security-relevant research have long passed.  I  could honestly say 25 years ago that I knew at least a bit of something about cryptography, access control, secure implementation techniques - pretty much everything being done in the field.  Today it's completely impossible.  You have sophisticated mathematical techniques, chip analysis techniques, bizarro attacks like ROP and JSFUCK - the range of stuff out there is astounding.
So the idea that you can somehow build a secure environment for yourself, confidently picking and choosing exactly what to trust and what can safely be combined with what is about as realistic as the idea that you could go out into the wilderness alone and in a couple of years build a car.
Attacks these days are team efforts.  So are defenses.  And if you think you can build a team of like-minded individuals to form your own team with any real degree of assurance that not one of them isn't quite what he seems ... well, forget it.  There's a reason organized militaries have been beating disorganized  but larger and fiercer groups since Roman times.
It's hard to let go of the dream, but it's necessary.
I've said this before, but perhaps not here:  "Open" - and particularly the lack of openness - as we use it is to a large degree an illusion.  It's missing what's really going on, which is a movement to higher levels of abstraction.
I've been around long enough to have seen the tail end of the period when hardware was open:  A decently equipped university lab could build a reasonably competitive computer from small parts.  And many did, to experiment with all kinds of architectural ideas.  But eventually designing new computers became uninteresting as an academic exercise as the basic problems all got pretty well understood good solutions.  The remaining problems moved onto silicon, and the challenges were at a scale that only a few large corporations could compete.
Operating systems went through a similar evolution.  So did UI's, compilers, etc.
In the 1980's, anyone could come up with a nifty new text-based Unix tool.  I wrote a bunch myself, some of which I still use.  But when was the last time you saw something new like this?  Again, the old problems have good solutions, and the new problems are in different areas:  Much larger scale, much higher levels of abstraction.
If you look at the most closed system in wide use today - iOS - indeed, everything from the innards of the chips to the OS to the system UI is fully controlled.  But ... on top of the based, there's an incredibly rich collection of apps.  App development is pretty much open.  (Yes, Apple controls it - but the interesting thing is that back in the days when jailbreaks were easily available, the apps you could find for jailbroken phones really didn't do all that much more than the Apple-approved apps.)  Even beyond that ... anyone can, for $100/year, become a developer.  Writing and using *your own apps, yourself* is essentially open.  Yes, the layer of abstraction provided by iOS is there and you can't change it - but above that, do your damnedest.  Any Apple policies only come into play if you want to *sell* your app.  (I find it fascinating - and hard to explain - why a culture of software sharing of app sources among hacker developers has never emerged.)
What we're seeing is directly analogous to the tradeoff between standards and non-standard development.  TCP/IP pinned down a standard.  If you want to play in the Internet world, you need to use it - even if it limits you.  But ... the very closedness of the TCP/IP world ensures that anyone can write code to use it, and it will work everywhere and talk to everything.
                                                        -- Jerry

@_date: 2016-03-06 11:28:51
@_author: Jerry Leichter 
@_subject: [Cryptography] EFF amicus brief in support of Apple 
Beyond that ... the most damaging recent bug is the DNS resolver bug in glibc, code which has been open and available for inspection from the day it was written - and the (rather obvious, once you see it) bug managed to survive for 8 years.
It's time we stopped believing that there's something magic about open source software.  There's good software and there's bad software, and even the best software has bugs.  And beyond bugs, all software can be deliberately subverted.  There are no silver bullets, just tons of continuing hard work.
                                                        -- Jerry

@_date: 2016-03-06 20:53:33
@_author: Jerry Leichter 
@_subject: [Cryptography] EFF amicus brief in support of Apple 
How long has the Gnu project been around?  How long has Linux been around?  The various open BSD's?
This sounds like the radical libertarian's answer to every objection:  Libertarianism has never *really* been tried, so you have have no reason to believe it won't work (and my beautiful theoretical arguments show it *will* work).
The parallel is actually deeper, because in both cases when examples of *partial* steps in the theoretically-correctly direction are shown to produce *worse* results, the answer is always "oh, you have to go further before it all starts working right".
The fact of the matter is, we have zero evidence that open source produces more "inherently secure" systems than closed source.  We also have zero evidence of the opposite.  Over the last month or so, we've found that Apple's closed-source iOS has a vulnerability to a certain kind of government coercion that it was supposed to be secure against; and that every Linux box on the planet has a remote execution vulnerability in its name resolver, and has had for 8 years.  (Comparing these two isn't the issue - they are simply bugs that have been revealed at a particular point in time.)
Now, that's "inherent security" - which is actually of little more than theoretical interest.  What we're talking about is the security seen by *real* users of *real* systems.  And here the evidence is absolutely clear:  Closed source, with automatic updates, wins hands down.  Let's watch it at work.  Apple will reportedly fix the current vulnerability (probably by requiring the unlock code to be entered even for a forced update - or perhaps giving the option of allowing the update without the unlock code if the device is zeroed) in a forthcoming release of iOS.  Let's compare the percentage of fixed iOS devices to the percentage of fixed Linux boxes over time.
Let's also be clear about something:  Most of the people who are in a position to inspect all that code and build their own hyper-secure system ... *have no rational need* for a hyper-secure system to begin with.  They like to imagine that they are protecting themselves from nasty government agents ... but the fact is few of them will ever be of the slightest interest to such agents.  And most attackers aren't that sophisticated anyway.  If you like to build stuff from the ground up - by all means, go ahead.  But don't for a moment imagine that what you're doing has anything to do with the *needs* of all but a tiny fraction of your fellow human beings.  They need stuff that stays out of their way, doesn't require that they devote significant effort to it - and "just works" in protecting them against the threats they reasonably face.
                                                        -- Jerry

@_date: 2016-03-09 13:55:22
@_author: Jerry Leichter 
@_subject: [Cryptography] All applications need top security (was Re: 
This is a "yes, but" situation.  There may be some browser sessions you need to run through TOR, but running through TOR is inappropriate for anything that requires high throughput and/or low, predictable jitter.  There are large publicly available scientific data sets available through HTTP; moving them to HTTPS would require significant money to upgrade hardware.
In a world where you can always get the necessary money hence compute and other resources, there's little reason not to use the strongest crypto you can, uniformly.  But in a real, current, resource-constrained world, you sometimes need to make decisions.
Note that even the NSA, in previous iterations of Suite B, described different levels of security based on the material being protected (e.g., AES-128 at SECRET and below, AES-256 at TOP SECRET).  In the latest iteration, as hardware has gotten faster and cheaper, they pretty much dropped the distinction.  But the Suite B recommendations generally assume something with the power of, say, a modern laptop or better.  Dropping *all* such distinctions is another story.
*If* we have to maintain such distinctions, however, and important question is whether we can somehow give users a reasonable, understandable way to specify the level appropriate for a given connection/piece of data.  This is a *hard* job, and like most interesting UI questions related to security, has seen way too little useful work.
                                                        -- Jerry

@_date: 2016-03-09 15:30:09
@_author: Jerry Leichter 
@_subject: [Cryptography] EFF amicus brief in support of Apple 
This argument might have been persuasive in the first third of the 20th century.  Since then, the Supreme Court has held that First Amendment protections apply to obscenities on tee shirts, to simple physical presence as a means of protest, to *not* having to carry a 4-word, state-defined phrase on your state-issued license plate - even to nude dancing.
Does the Amendment *definitely* apply to a digital signature, or to code simply as an executable?  No, because as far as I know there's no case on this point.  There are cases that are close, but this one raises new issues.  But given that Apple explicitly adds this signature for the purpose of expressing "this is code we approve of", and explicitly configures its software to outright reject and report as untrustworthy, any updates that are not so signed - the argument that the presence of such a signature is an expression of Apple's opinions about the signed software seems very strong.  If you want, you could say that it's *the rejection or acceptance of the patch*, that's Apple's protected speech:  It communicates information from Apple to a human individual about Apple's opinion concerning the software.  The machinery in between is irrelevant - it's just a means for transmitting that message.
Apple advances many distinct theories as to why they should not be required to build the unlock software.  The basic ones - that the All Writs Act does not apply for any of a variety of reasons - might be enough to get the current order tossed; but Congress could turn around pass a new law to allow such orders.  (Given the current Congress, that seems unlikely right now, but who can say for sure.)  The advantage of a holding that such an order is disallowed under the First Amendment (or others - there are a couple of theories floating around) is, of course, that it can't be overridden by Congress.
Depending on what you want to accomplish, you can choose different strategies.  The hazard of advancing both the Constitutional claims, and the All Writs Act-specific claims, is that a lower court could hand you a victory on the latter, it could be upheld by higher courts - and then the Supreme Court could decide to stay out as no Constitutional question is involved.  So you might win, but have to fight again if Congress changes the law.  Of course, if you advance only the Constitutional arguments ... who knows, perhaps the courts will decide that a digital signature, or code, or some combination thereof, are not "speech" within the meaning of the First Amendment, which would be a huge loss.  Clearly Apple and its lawyers think they have a strong Constitutional case - but they're taking the cautious approach of also including the lower-level defenses.
                                                        -- Jerry

@_date: 2016-03-09 16:22:48
@_author: Jerry Leichter 
@_subject: [Cryptography] EFF amicus brief in support of Apple 
Meh.  Apple called for a high-level commission to look into the issue.  We all know what high-level commissions do:  They put off taking any action at all.  Meanwhile, Apple is reportedly raising the bar further by modifying the update procedure so as to make it impossible, in future versions, to build the kind of unlocking software they are now being called on to produce.  That wouldn't leave many alternatives for Congress.  In fact, the only once I can think of (that would cover this particular type of issue) would be to extend CALEA to companies like Apple and require them to leave an opening for LE in their software.
Anyway, we're now speculating on (a) what courts might hold; (b) what Apple's lawyer's believe.  Perhaps should move on to something easier, like who might win the 2016 World Series.  :-)
                                                        -- Jerry

@_date: 2016-03-10 06:28:48
@_author: Jerry Leichter 
@_subject: [Cryptography] Would open source solve current security issues? 
I agree overall, but want to point out one additional bias in the data:
It depends on the kind of patch and the length of supply lines.
Open source projects can very quickly ship a source patch for the particular module involved.  This is fine for those who build from source - a tiny minority.
The next step is getting the patch into a binary build.  Depending on the project to which the patch is made, that can take a couple of days.  The delays can also mount for the more branched distributions.  SuSE Linux, for example, always seems to take longer than many of the more mainstream distributions because they have their own variants of all the patches.  (Exactly what's different and why, I don't know, but their version id's are consistently different - which causes all kinds of grief for audits based on "version x.y.z and earlier have the bug".)  We're not talking big numbers - a couple of days at most - but the effect is there.
Beyond that, of course, are products built on top of the binary distributions - and products built on top of *them*.  All of these take time to incorporate changes and push them out to clients.  Some - most famously Android phones, all the way at the end of supply lines running through handset makers and Telco's - never receive the changes at all.
That's all open source.  On the closed source side, all the supply chains and testing and related processes are internalized.  There is, of course, never a source code patch.  But the closed source suppliers - pushed by their customers - are these days loath to push large numbers of small patches.  Hence the "patch Tuesday" phenomenon.  This also puts more pressure on them to test - all the way down the supply chain - since "fixing the fix" if it goes wrong is also disruptive.
So, again, there's theory and there's reality.  In theory, open source can get a patch out very quickly - and users who are willing to track the latest patches, in source, continuously, can apply them very quickly.  In practice, this mainly applies to a small coterie of developers of the software.  Others have to wait for the patch to make it through the layers of distributions - and on top of that, while open source projects may not have a "patch Tuesday" distribution policy, most sites have some kind of "patch day" *intake* policy since no one wants to spend all their time updating their systems.
The net effect is, I suspect, close to a wash in terms of *actual spread of security patches throughout the community of users*.  Open source probably wins if you measure actual time, but not by much.
Of course, there are always the high-priority emergency patches - e.g., Heartbleed.  Recently, these have been reported to the software vendors privately and coordinated announcements and releases of patches have been standard.  In this case, the delays were all "up front", before the announcement - and the differences between open and closed source have *largely* been nil.  (Apple delayed on shipping a Heartbleed patch - but then it was pretty much irrelevant to the vast majority of their devices, which don't run HTTP servers.)
I think the important take-away from the whole discussion is that these are *systems* issues and have to be analyzed at that level.  How long it takes for someone to get out there and produce a patch is dwarfed by the time it takes for working secure code to actually get deployed on a significant proportion (ideally, all!) vulnerable systems.  Both open and closed source communities have been improving their approaches for a number of years now, and both have (in general - let's not get started on, say, industrial controller makers) improved things, often dramatically.
                                                        -- Jerry

@_date: 2016-03-12 16:21:40
@_author: Jerry Leichter 
@_subject: [Cryptography] Govt Can't Let Smartphones Be 'Black Boxes, 
This is completely wrong.  The moment you let a piece of information outside of your brain - by saying it, by writing it, even just by some little wink to someone - it's outside the protection of the Fifth Amendment.  It's always been this way.
The purpose of the Fifth Amendment is not to protect your private information - it's to protect *you* against the kind of compulsion that would be applied to extract that information, if the government were allowed to use it.
There have been a few carefully circumscribed exceptions to the general rule that "once it's out of your head, it's fair game":  Discussions with your lawyers, your priest, your spouse (but not other family members!), your doctor (sometimes).  But again, if you look at why these exceptions exist, they again have nothing to do with your privacy as such; they are there explicitly to preserve a perceived greater social good (allowing defendants to get good legal advice; the sanctity of the religious experience/the marital relationship; the need for honesty in getting medical care).
None of these apply to the use of phones.  You could argue that perhaps they *should*, that the relation between a person and his phone is now so close that it needs protection - but it's not an argument the courts, or the political system, are likely to take seriously.  In fact, the general trend has been in the opposite direction - from the ever-broader application of the third party doctrine, to laws written in response to Enron's shredders that have broadened the crime of "destruction of evidence" to just about anything that might, at some point, be of interest to any prosecutor.
There are plenty of arguments for why the FBI's attempt to order Apple to do its bidding should be blocked, but I'm afraid this isn't one of them.
                                                        -- Jerry

@_date: 2016-03-13 16:06:43
@_author: Jerry Leichter 
@_subject: [Cryptography] "WhatsApp Encryption Said to Stymie Wiretap Order" 
From WASHINGTON  While the Justice Department wages a public fight with Apple over access to a locked iPhone, government officials are privately debating how to resolve a prolonged standoff with another technology company, WhatsApp, over access to its popular instant messaging application, officials and others involved in the case said.
No decision has been made, but a court fight with WhatsApp, the worlds largest mobile messaging service, would open a new front in the Obama administrations dispute with Silicon Valley over encryption, security and privacy.
WhatsApp, which is owned by Facebook, allows customers to send messages and make phone calls over the Internet. In the last year, the company has been adding encryption to those conversations, making it impossible for the Justice Department to read or eavesdrop, even with a judges wiretap order.
As recently as this past week, officials said, the Justice Department was discussing how to proceed in a continuing criminal investigation in which a federal judge had approved a wiretap, but investigators were stymied by WhatsApps encryption.
The Justice Department and WhatsApp declined to comment. The government officials and others who discussed the dispute did so on condition of anonymity because the wiretap order and all the information associated with it were under seal. The nature of the case was not clear, except that officials said it was not a terrorism investigation. The location of the investigation was also unclear.
To understand the battle lines, consider this imperfect analogy from the predigital world: If the Apple dispute is akin to whether the F.B.I. can unlock your front door and search your house, the issue with WhatsApp is whether it can listen to your phone calls. In the era of encryption, neither question has a clear answer.
Some investigators view the WhatsApp issue as even more significant than the one over locked phones because it goes to the heart of the future of wiretapping. They say the Justice Department should ask a judge to force WhatsApp to help the government get information that has been encrypted. Others are reluctant to escalate the dispute, particularly with senators saying they will soon introduce legislation to help the government get data in a format it can read.
Whether the WhatsApp dispute ends in a court fight that sets precedents, many law enforcement officials and security experts say that such a case may be inevitable because the nations wiretapping laws were last updated a generation ago, when people communicated by landline telephones that were easy to tap.
The F.B.I. and the Justice Department are just choosing the exact circumstance to pick the fight that looks the best for them, said Peter Eckersley, the chief computer scientist at the Electronic Frontier Foundation, a nonprofit group that focuses on digital rights. Theyre waiting for the case that makes the demand look reasonable.
A senior law enforcement official disputed the notion that the government was angling for the perfect case, and said that litigation was not inevitable.
This is not the first time that the governments wiretaps have been thwarted by encryption. And WhatsApp is not the only company to clash with the government over the issue. But with a billion users and a particularly strong international customer base, it is by far the largest.
Last year, a dispute with Apple over encrypted iMessages in an investigation of guns and drugs, for instance, nearly led to a court showdown in Maryland. In that case, as in others, the company helped the government where it was able to, and the Justice Department backed down.
The messaging service WhatsApp has been adding encryption, challenging wiretapping laws last updated a generation ago. Jan Koum, WhatsApps founder, who was born in Ukraine, has talked about his family members fears that the government was eavesdropping on their phone calls. In the companys early years, WhatsApp had the ability to read messages as they passed through its servers. That meant it could comply with government wiretap orders.
But in late 2014, the company said that it would begin adding sophisticated encoding, known as end-to-end encryption, to its systems. Only the intended recipients would be able to read the messages.
WhatsApp cannot provide information we do not have, the company said this month when Brazilian police arrested a Facebook executive after the company failed to turn over information about a customer who was the subject of a drug trafficking investigation.
The iPhone case, which revolves around whether Apple can be forced to help the F.B.I. unlock a phone used by one of the killers in last years San Bernardino, Calif., massacre, has received worldwide attention for the precedent it might set. But to many in law enforcement, disputes like the one with WhatsApp are of far greater concern.
For more than a half-century, the Justice Department has relied on wiretaps as a fundamental crime-fighting tool. To some in law enforcement, if companies like WhatsApp, Signal and Telegram can design unbreakable encryption, then the future of wiretapping is in doubt.
Youre getting useless data, said Joseph DeMarco, a former federal prosecutor who now represents law enforcement agencies that filed briefs supporting the Justice Department in its fight with Apple. The only way to make this not gibberish is if the company helps.
As we know from intercepted prisoner wiretaps, he added, criminals think that advanced encryption is great.
Businesses, customers and the United States government also rely on strong encryption to help protect information from hackers, identity thieves and foreign cyberattacks. That is why, in 2013, a White House report said the government should not in any way subvert, undermine, weaken, or make vulnerable generally available commercial encryption.
In a twist, the government helped develop the technology behind WhatsApps encryption. To promote civil rights in countries with repressive governments, the Open Technology Fund, which promotes open societies by supporting technology that allows people to communicate without the fear of surveillance, provided $2.2 million to help develop Open Whisper Systems, the encryption backbone behind WhatsApp.
Because of such support for encryption, Obama administration officials disagree over how far they should push companies to accommodate the requests of law enforcement. Senior leaders at the Justice Department and the F.B.I. have held out hope that Congress will settle the matter by updating the wiretap laws to address new technology. But the White House has declined to push for such legislation. Josh Earnest, the White House spokesman, said on Friday that he was skeptical of Congresss ability to handle such a complicated policy area.
James B. Comey, the F.B.I. director, told Congress this month that strong encryption was vital and acknowledged that there are undoubtedly international implications for the United States to try to break encryption, especially for wiretaps, as in the WhatsApp case. But he has called for technology companies and the government to find a middle ground that allows for strong encryption but accommodates law enforcement efforts. President Obama echoed those remarks on Friday, saying technology executives who were absolutist on the issue were wrong.
Those who support digital privacy fear that if the Justice Department succeeds in forcing Apple to help break into the iPhone in the San Bernardino case, the governments next move will be to force companies like WhatsApp to rewrite their software to remove encryption from the accounts of certain customers. That would be like going to nuclear war with Silicon Valley, said Chris Soghoian, a technology analyst with the American Civil Liberties Union.
That view is one reason government officials have been hesitant to rush to court in the WhatsApp case and others like it. The legal and policy implications are great. While no immediate resolution is in sight, more and more companies offer encryption. And technology analysts say that WhatsApps yearlong effort to add encryption to all one billion of its customer accounts is nearly complete.
                                                        -- Jerry

@_date: 2016-03-15 20:56:57
@_author: Jerry Leichter 
@_subject: [Cryptography] MSFT doesn't retain keys for its own German cloud 
This is a response to the ongoing lawsuit by the US gov to get at data that's stored at a Microsoft server in, I think, Ireland.  Microsoft's position is that a US warrant can't reach data stored overseas.  The US position is that since Microsoft is a US corporation, it doesn't matter where the data lives - it falls under US law.  That suit is likely on its way to the Supreme Court.  Note that this is another case with potential serious repercussions:  If the US claims that they can get at data stored anywhere in the world, why can't other countries make the same about data stored here?
In any case, the whole business with Deutsche Telekom was deliberately set up so that Microsoft cannot order DT to give it access to the data.  DT is governed by its own decisions, and German law.  So if the US demands access from Microsoft, they can legitimately say "We have no ability to provide it, you'll have to go negotiate with the Germans".
Of course, this is only relevant when the US (or, for that matter, the Germans) go through standard legal channels.  The spies get their access behind the scenes, and when operating outside of their own borders (let's close our eyes to the stuff within their own borders, shall we?) can do pretty much whatever they can get away with - which is pretty much anything.
                                                        -- Jerry

@_date: 2016-03-15 21:41:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Apple Reply to USG Opposition to Vacate Decrypt 
This is an extraordinary document, and well worth reading in its entirety.  (It's not nearly as long as it appears - the actual text is 25 pages, followed by perhaps five times as many pages of attached documents, letters, and other exhibits.)
It starts of (in a footnote) saying "The governments brief assails Apples intentions and motivations. We do not intend to respond in kind."  That is, it claims the high ground.  But what follows is a detailed exposition of how the government's filing took citations out of context, picked words from within sentences so as to almost reverse the meaning of the original, and even in one case took a quotation and stuck three words after it in such a way as to give a completely impression of what the cited case was about.
This is nasty stuff.  In a system based on precedent, accurate citation is key. Opposing sides may present different citations that they believe are on point; and they may disagree on the interpretation of citations; but to deliberately distort the meaning of citations is a complete violation of the way the system is supposed to work.
I'm not a lawyer, but it strikes me that this kind of thing is coming close to a fraud upon the court.  Certainly, no judge is going to be happy if a party tries to mislead him about what the cited cases actually say.
And it's not as if the government could have any hope of getting away with this.  The citation system is completely open - it should have come as no surprise to them that lawyers for Apple - certainly lawyers of the calibre that Apple has hired - would check every single word closely.  Not only is this kind of thing unacceptable - but there's not really any chance of getting away with it.
                                                        -- Jerry

@_date: 2016-03-17 16:38:44
@_author: Jerry Leichter 
@_subject: [Cryptography] DoJ/FBI's "nuclear"/Lavabit option 
None of this would work today to do anything but get the company fined or its people put into jail.  The courts have established rules about "E-discovery" which require, for example, that if information that you have to supply is available in a machine-readable format, you have to deliver it that way.
The rules, in fact, can impose some big costs.  Some of the rules around e-mail are particularly strict and intrusive.  And after Enron, the rules about data retention are pretty sharp-edged.
"E-discovery software" is a thing these days....
                                                        -- Jerry
BTW, there's a nice story about the IBM data.  CDC (Control Data) had a private monopoly lawsuit against IBM.  One day, it was suddenly settled.  One of the most important features of the settlement from IBM's point of view (for which they probably tossed quite a bit of money on the table) was that CDC was required to immediately destroy a huge (for the era) database they had built of the thousands of boxes of material they'd gotten from IBM as part of discovery.  This was in the 1960's - the idea of building a searchable database of documentation was novel, and hardly anyone would have had the resources to do it even if they understood its value.  CDC had, and had used it effectively.  IBM knew that were the DOJ to learn of an impending settlement, they would immediately try to get an order that the CDC make it available to them.  So IBM insisted on that it be irretrievably gone by the time the settlement went public.

@_date: 2016-03-18 22:18:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Would open source solve current security issues? 
The iOS security model is very different.  They don't declare what permissions they want up front, so there's no analogous list of privileges you look at.  Rather, there are a few resources - location, the camera, the microphone - which require explicit permission.  iOS prompts you the first time you use the app and lets you decide then.  (You can also manage this stuff from Settings.)  In at least one case (access to location when in the background), iOS will actually prompt you again (once) after you've had the setting on for two weeks or so and let you confirm that you really do want it.
The contrasts:
- iOS has a number of special resources, where Android has many fine-grained privileges.  In practice, the actual meaning of many of Android's privileges are  obscure and hardly anyone tries to manage them.
- Android prompts at installation time, when it may be hard to tell whether the app actually needs the privileges.  iOS prompts at first use, when it'll typically be very clear what the app is trying to do.
- Android privilege settings are "take it or leave it":  You can't pick and choose which to grant an app.  (There was discussion about changing this, but I haven't kept up and don't know if it ever was changed.)  iOS settings are all optional - its up to the app to decide to continue if it can't get the resource it asked for.  In practice, I've never seen an app that refused to run without some resource - though it may not be useful.  For example, WhatsApp will use your contacts if you give it access - but will run perfectly well, maintaining its own list of recent connections, if you deny it access.  Actually, the same goes the access the the microphone:  If you don't give it access, you can't make calls, but you can still text.  (I don't recall when WhatsApp added support for audio.  Some update silently added the setting, but it defaulted to Off.)
The contrast is really interesting.  The typical programmer loves the Android approach because it gives the appearance of fine-grained control, but in practice it's not clear it solves any real problem.  The Apple approach started off *really* simple (most resources available today were initially kept completely private) and grew slowly over time, but the association with particular human-understandable resources, rather than abstract privileges, seems to work much better.
                                                        -- Jerry

@_date: 2016-03-19 18:54:18
@_author: Jerry Leichter 
@_subject: [Cryptography] Apple GovtOS/FBiOS & Proof of Work 
This discussion has gotten into all kinds of technologies, but it's not clear to me that the underlying properties achieved make much sense.  So let's go back and look at what we're trying to accomplish and why.
Problem:  Apple (the working example) could be forced into producing a signing a "bad" update.  We want to make sure that phones refuse to apply "bad" updates.  (It's worth noting that the assumption that you can get Apple to issue any update you want and remain quiet about it is equivalent to being able to get a copy of Apple's signing key, as far as anyone on the outside could tell.)
Proposed Solution:  Apple binds itself into a situation in which it can, at least on average, issue no more than one legitimate update per interval T.  Since it will, in fact, *always* issue an update once per interval T, if someone manages to force it to issue a "rogue" update, when the time comes to issue the next legitimate one, it won't be able to and the "rogue" update will be "outed".
Note that as given, this solution requires no cryptography at all!  It's ultimately the phone that has to enforce the property that only one update per interval will be treated as valid, and it can do that by simply remembering the time of the last update.
So ... does this solution help?  Not really.  Given the assumption that the attacker can force Apple to sign anything, and the assumption that they are willing to send a rogue update to every phone in the world, at best this delays the updates by T/2 on average.
If you remove the second part of the assumption and require that the rogue update be delivered only to a single phone ... you're no better off.  Within T/2 at most, every phone in the world except the target receives a signed proper update, and the single target receives the "rogue" update.  The only way it could tell that its update is rogue is by comparing it to (a sample of) everyone else's.  But if it has that ability ... where's the need for the timing mechanism?
And, of course, this ignores the need for emergency updates.  But even regular updates tend to be reasonably frequent - T really can't be more than 3 months or so, which makes the average delay to a rogue push only 6 weeks.  That's not much protection.
So ... it's not at all clear to me what you would gain by this mechanism - regardless of the fancy technologies used (unnecessarily) to implement it.
                                                        -- Jerry

@_date: 2016-03-19 20:52:30
@_author: Jerry Leichter 
@_subject: [Cryptography] Apple GovtOS/FBiOS & Proof of Work 
Wow.  Talk about killing about killing a fly with a nuke.  An iPhone has access to a number of time sources - the telephone system, the Internet, GPS, as well as an internal clock.
But ... who cares?  The point of my message was that your proposed system doesn't seem to actually solve any interesting problem.
                                                        -- Jerry

@_date: 2016-03-22 06:26:20
@_author: Jerry Leichter 
@_subject: [Cryptography] This is why we have Stuxnet 
Many years back (1980's), I was on the faculty at the Rutgers CS department.  When CS departments were first created, there was a big battle at many universities:  Was CS going to be part of the school of arts and sciences; or was it going to be part of the engineering school?  At Rutgers, the answer was "yes":  There was both a CS department and a CE (Computer Engineering) department.  At the time (and, I gather, until very recently), the CE department was smaller and didn't have the faculty needed to teach a few of its core courses; so CE majors were required to take at least two core course (Programming Languages/Compilers; Operating Systems) in the CS department.
I taught both those courses, so saw all the CE students.  In general, I found the CE guys (as best I can recall, no women - there were a few in the CS department) were bright and hard working.  (In fact, they were on average better than the CS students.)  But ... most of them had learned programming in the same course:  A DSP course they took a semester or so before my course.  And the habits they learned ... ugh.  OK, given the era, making DSP's work in software even for audio (video was just impossible) required serious optimization and low-level hacking.  You could do it in C, but you had to treat C as a glorified assembler.  Abstraction, even to the limited degree that C supports it, was just not an option.
I ended up making it a goal of my courses that beyond the actual subject material, I'd make sure the CE (and CS!) student actually learned something about writing good code - something college courses rarely want to bother with anyway.
Based on the interviews I do - and the code I see written - I'm not sure the programming education of *CS* students today is really much better.  But I'm sure the hardware guys still see programming as the trivial final bit of the job, to be thrown together as quickly as possible, except when in has to be as efficient as possible in some of the inner loops.
BTW, I've seen good *security* guys whose code is ... not good.  Hey, I got the protocol all formally verified, the rest is just a bit of quick hacking.
As an industry, we have a *long* way to go.
                                                        -- Jerry

@_date: 2016-03-22 11:34:00
@_author: Jerry Leichter 
@_subject: [Cryptography] And they're off... 
The bombers relied on *clothing* to conceal their bombs.  Why do we allow clothing that conceals bombs to be sold?  In the world we live in today, this is no longer acceptable.  I know people will say that it's not possible.  But you know, we all have to give up a bit of privacy for security.  I call on the smart men and women of the clothing industry to come up with a solution to this problem.
                                                        -- Jerry

@_date: 2016-03-23 16:18:50
@_author: Jerry Leichter 
@_subject: [Cryptography] Paris attackers used OTP's: One Time Phones 
Actually *Apple* sometimes considers bulk purchases of iPhones to be suspicious.  (Sometimes they consider purchases of iPhones with cash suspicious and decline them.)
This has nothing to do with terrorism - it has (or had) to do with a big demand for iPhones in China.  There was a whole "gray market" supply chain set up to buy them here (eventually by hiring mules) and ship them to China for sale at premium prices.  Ironic when you think about where they were built!
This is pretty much obsolete now as Apple sells directly to China in a big way.
                                                        -- Jerry

@_date: 2016-03-23 20:30:41
@_author: Jerry Leichter 
@_subject: [Cryptography] "Apple moves to bring iCloud infrastructure in-house 
From It's a big pro-Apple site, but the "insider news" is often true.  (They were among the first to find and describe Apple's secret car efforts.)  How real *this* is, I can't say.
                                                        -- Jerry
By AppleInsider Staff	
Wednesday, March 23, 2016, 09:16 am PT (12:16 pm ET)
Apple's multi-year effort to develop its own servers and networking hardware has reportedly been driven in large part by security concerns, as the company worries that supply chain tampering may lead to deeply embedded vulnerabilities which are difficult to find and remediate.
[Removed picture with the following text underneath:  National Security Agency personnel are shown delicately opening a Cisco box to add malware to the device within after intercepting it during shipping.]
Apple's fears center around the possibility that infrastructure equipment could be intercepted by third parties between the time it leaves the manufacturer and the time it arrives at Apple's datacenters, according to The Information. The company believes that malicious actors could be adding new or modified components that would enable unauthorized access.
This fear is said to have been a primary driver of the company's strategy to move as much infrastructure design as possible in-house. The gargantuan size of such a task  Apple's cloud services serve tens of billions of requests each day  has led to delays in reducing its reliance on outside service providers like Google and Amazon.
Unfortunately, Apple's worries are not unfounded.
While it may never be known who the targets were, information revealed by NSA leaker Edward Snowden revealed the existence of government programs designed to do exactly the thing Apple fears. The National Security Agency's Tailored Operations Access unit was, and may still be, tasked withredirecting shipments of servers and routers headed for targeted organizations to government facilities. The packages would be opened, compromised firmware installed, and then the packages re-sealed and delivered. One NSA manager described the program as "some of the most productive operations in TAO because they pre-position access points into hard target networks around the world."
Photos which accompanied the leaks showed intelligence agency workers modifying Cisco gear, infuriating the networking giant. Cisco later announced that it would address shipments to empty houses to avoid government tracking.
"We ship [boxes] to an address that's has nothing to do with the customer, and then you have no idea who ultimately it is going to," Cisco security chief John Stewart said at last year's CiscoLive 2015 conference.
"When customers are truly worried... it causes other issues to make [interception] more difficult in that [agencies] don't quite know where that router is going, so it's very hard to target - you'd have to target all of them. There is always going to be inherent risk."
Apple is said to have gone to extreme lengths to verify the integrity of products it receives, even comparing photographs of motherboards with explanations of each component and its function. "You can't go take an X-Ray of every computer that hits the floor. You want to make sure there's no extracurricular activity" by building servers in-house, one source told the publication.

@_date: 2016-03-23 20:35:29
@_author: Jerry Leichter 
@_subject: [Cryptography] "Apple moves to bring iCloud infrastructure 
Reported elsewhere as well:  That's a broader article and the piece they let you see without logging in doesn't touch on this subject, but I saw a post of a paragraph deeper in that discusses the same fears.
                                                        -- Jerry

@_date: 2016-03-24 11:20:03
@_author: Jerry Leichter 
@_subject: [Cryptography] Unicity distance of Playfair 
Playfair uses a 5x5 table - 25 cells.  Each input character has to correspond to a unique cell, but there are 26 letters.  So traditionally we replace every I in the input with a J (or every X with a Y).  Now the input has only 25 different characters.
The replacement means "we can't distinguish I [from] J (or X [from] Y)" after decoding - which has no significant effect on the understandability of English-language text.
                                                        -- Jerry

@_date: 2016-03-24 22:19:05
@_author: Jerry Leichter 
@_subject: [Cryptography] Lavabit's and Snowden's Solos 
Those are easier, because it's argued that changing switch settings is just a part of normal operations, so is not as big imposition and is consistent with what the compellee does anyway.  This kind of reasoning was central to the court case that compelled a telco to connect a pen register for the FBI.
Note that the FBI tried to stretch the same reasoning to apply against Apple:  Apple is "in the business of writing software", so it's no big deal to compel them to write GovOS.  Expect them to try to use this wedge wherever they can.  If you *ever* enable a debug or test mode that lets you see the keys or the cleartext, bam, you've just acknowledged that (a) you can do it; (b) it's not repugnant to you to do so; (c) you'll do it for the government, too.
                                                        -- Jerry

@_date: 2016-03-27 06:23:11
@_author: Jerry Leichter 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
I'm not sure you're assessing the *nature* of the risks correctly.
Let's contract a couple of possible situations.
If there's an AES monoculture, and AES is never broken, everything is great.  Highest probability world, lowest costs.
If there's an AES monoculture and a fallback, and AES is never broken, everything is great.  Highest probability world, slightly higher costs because of the need to establish and maintain the fallback.  The fallback code remains untested in real world service.
If we choose k equally standard algorithms, and use different algorithms in different situations, and none of them is broken, again all is golden - but the costs are higher.
If there's an AES monoculture, and AES is broken, every message ever encrypted is broken, and until an alternative can be established and fielded, every new message is also broken.  Extremely low probability event, immensely high cost.
If there's an AES monoculture and a fallback, and AES is broken, every existing message is broken, but future messages are safe once you can get everything switched to the alternative (which we'll assume is fairly quickly).  Same extremely low probability event, but a "half infinity" cost.  If, alternatively, it's the fallback that's broken (while AES survives), there's a much lower (but still significant) cost of find and fielding a new alternative.  (Or you could just continue in the "AES monoculture" world.)
If we choose k equally standard algorithms, and use different algorithms in different situations, and any one of them is broken, 1/k of previous messages ever encrypted is broken.  Once you can effectively blacklist the broken algorithm, future messages are secure.  Since attacker now have k algorithms to attack, perhaps their chances of breaking one are better - but it's still an extremely low probability event, thought costs are now considerably lower.
These are simplified scenarios, and without being able to plug in some values, it's hard to say anything meaningful.  And the problem is exactly that we have no way to guess at the values.  What's the probability of AES being broken in the next 10 years?  20 years?  50 years?  We have insufficient information to make any reasonable guess.  I think most people would say "vanishingly small" for 10 and even 20 years, but trying to predict *anything* out 50 years is very problematic.
And on the cost side, things are only a bit better.  Sure, "half infinity" is "less than" "infinity" (since "infinity" is just a codeword here for "so I high I can't even imagine") but is that really a meaningful comparison?
What about the additional costs of maintaining a fallback, or choosing amoung k alternatives?  That's probably not *that* high, but how do you compare it to the "infinitesimal * infinite" expected costs of a break if you don't do it?
BTW, the "k alternatives" can be seen in different ways.  At one extreme, each protocol might choose a single one of the alternatives.  At the other, you could randomly choose an alternative for each connection, in effect using a single "meta-algorithm" in which part of the key determines the algorithm.  (If you're doing PFS, the algorithm might switch per message.)  Should you assume that the probability of a break of the "meta-algorithm" is different from the cost of a break of one of the constituent algorithms?  Given that it's all just guesswork anyway....
The net result:  The choice to be made here is simply not informed by enough data to be more than philosophy and gut feel.  I'm sympathetic to the arguments against complexity - but sometimes you need complexity in your system to deal with real complexity in the world.  Codebook mode is the simplest mode, but it's not quite good enough.
                                                        -- Jerry

@_date: 2016-03-28 15:59:40
@_author: Jerry Leichter 
@_subject: [Cryptography] Unicity distance of Playfair 
None of this matters.
Unicity distance is a very simple measure:  It is simply the number of bits of ciphertext encrypted with a given key which uniquely determines the key.
Since this is a *guarantee*, not an average, it's necessarily an integer number of bits.  If the keys must be represented by larger units than bits - say, bytes - then the unicity distance is perhaps more naturally represented in bytes.
Similarly, it's independent of any probability distributions over the keys or the plaintexts:  It's a *worst case* measure, so the fact that 50 bits is enough for 2^128-5 cases but you need 55 bits for the remaining 5 cases means your unicity distance is still 55.
The unicity distance can be infinite for completely broken ciphers.  If the key is two bits long, and the encryption is "XOR all input bits with the bottom bit of the key", then no amount of ciphertext ever distinguishes the keys 01 and 11.  You can try to modify the definition of unicity distance so as to ask for the amount of input until you can uniquely find a key that produces the same outputs as the original key - but that actually doesn't help much:  Encrypt by XOR'ing with the bottom bit of the two bit key for 10^10 bits, and from there XOR with the top bit.  The unicity distance is now 10^10+1, but that tells you absolutely nothing about the quality (or lack thereof) of the encryption.
While the distribution over the inputs or keys don't matter, the actual input or key sets *do* matter.  The "XOR with bottom bit" encryption goes from a infinite unicity distance to a one-bit unicity distance if keys must be chosen from the set {00, 01} - or in the limit to 0 if the key must be chosen from the set {00}.  The silly 10^10 example goes from 10^10+1 to infinite if the input is chosen from bit streams of length no more than 10^10 bits.
Padding the output with irrelevant bits obviously increases the unicity distance, since those bits "increase the count" without actually reducing the equivocation about the key.
Unicity distance is an simplified way to present some information-theoretical ideas about the safety of encrypted text, but it's not really useful for much else.
                                                        -- Jerry

@_date: 2016-03-28 16:22:52
@_author: Jerry Leichter 
@_subject: [Cryptography] Gates are cheap. Should cipher design change? 
Suppose we ignore those.  The linked paper indicates that the smallest known AES implementations require about 2500 gates.  Suppose you had a budget of a million gates and wanted to design a cipher that made full use of them.  What would you do?
One interesting issue this highlights is the difference between hardware and software implementation:  Given that many gates, you can build a very large S-box in hardware.  Software trying to implement such an algorithm would have significant problems avoiding various kinds of side-channel leaks via memory accesses.
                                                        -- Jerry

@_date: 2016-03-29 14:55:07
@_author: Jerry Leichter 
@_subject: [Cryptography] Gates are cheap. Should cipher design change? 
Funny you should say that.  I had remembered a cipher - described, I think, in Scheier's book - which allowed arbitrary (or at least highly variable) block sizes.  I couldn't remember the name nor find a reference (and I couldn't quickly get my hands on the book) so didn't mention in my message.
One can look at the evolution of machine-implemented ciphers as:  First we had stream ciphers, because they can be implemented with very limited hardware resources.  But stream ciphers have their own sets of issues, so about the time chips became available, we moved on to block ciphers.  DES was right at the edge of implementability when first proposed - the first DES implementations that could run at full Ethernet (10Mbps!) rates were significant breakthroughs.  Eventually, as we could use more resources, we moved from 64- to 128-bit blocks to deal with the very specific issue of the birthday paradox limiting the number of bytes that could be safely transferred.  And there we've stopped.
Now, one can argue about whether a larger block size, or simply implementing something like counter mode in hardware parallel, is the better way forward.  Worth thinking about, though.
                                                        -- Jerry

@_date: 2016-03-29 15:01:52
@_author: Jerry Leichter 
@_subject: [Cryptography] Gates are cheap. Should cipher design change? 
I don't have a reference, but as I recall, S-box design is hard *for small S-boxes*.  Once S-boxes get large enough, getting the necessary properties becomes much easier.
Of course, large S-boxes are a really bad idea for software implementations.  Whether you want to design something that is aimed, as exclusively as possible, at hardware implementations, is a whole other story.  The initial and final permutations in DES were allegedly added (though they have no security impact) because they were trivial in hardware but expensive in software - and the NSA at the time didn't believe in software cryptography.  Over the last 30 or so years, almost all crypto implementations have been in software - and we've discovered repeatedly that it's hard to avoid various kinds of side-channel attacks against software.  Since Intel moved AES support into hardware, we've seen some shift back from software to hardware (well, it's a mix, but the complicated performance-critical stuff is in hardware).  Perhaps that's where we're heading.
                                                        -- Jerry

@_date: 2016-03-29 17:04:52
@_author: Jerry Leichter 
@_subject: [Cryptography] USG Moves to Vacate Apple Decrypt Order 
Not a lawyer, but to the best of my understanding:  Apple could object if they wished, but it's highly unlikely the judge would agree to keep the case alive.  A fundamental principle in the Common Law system is that the courts restrict themselves to cases where there's an actual dispute.  Where there's (no longer) a real dispute, the case is dismissed as moot.
Some other legal traditions allow you to ask a court to rule on something that isn't currently in dispute in order to establish what the actual law is - but the Common Law frowns on that kind of thing very strongly.  Part of the reasoning:  The courts are supposed to resolve disputes by having the best arguments from both size presented and argued out.  If there's no active dispute, there's likely no one available to represent one of the sides, and any ruling that came out would be viewed with suspicion.
The restriction to "actual disputes" is also seen as a way to limit the power of courts:  They can't go off on their own and look at stuff; someone really involved has to bring things to them.
There are exceptions to this, usually from ways to keep a dispute alive through agreed tricks from both sides, but they are very unusual.
BTW, there are many examples of this principle to be found in the NSA surveillance cases.   A lawsuit was dismissed just last week - it was rendered moot because Congress changed the laws.  But many other cases have been tossed because the plaintiffs could not establish that they themselves had been surveilled, so they were considered not to have an actual dispute to be resolved.
                                                        -- Jerry

@_date: 2016-03-30 12:54:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Gates are cheap. Should cipher design change? 
Jon will have to answer that.
An extra parameter, kind of like a second key, which changes the encryption.  What distinguishes a tweak from the key is that it's very cheap to change.  You can think of a tweak as kind of like the IV in CBC mode.  (Not a great analogy, but it gives the idea.)
The original paper is                                                         -- Jerry

@_date: 2016-05-02 07:27:21
@_author: Jerry Leichter 
@_subject: [Cryptography] USB 3.0 authentication: market power and DRM? 
It would make no sense to *distribute* 20v from a central point.  But putting a converter into the wall socket ... that makes more sense.
In fact, we don't have to speculate about such things much:  USB 2 versions of such things are already readily available.
Stepping back, there's an interesting bit of evolution going on here.  The proliferation of power distribution standards - different voltages, different frequencies - let to the development of different, incompatible plugs and sockets, which protected you against accidentally plugging a 110V device into a 220V socket (but also sometimes prevented you from making a connection that *did* make sense - but that's another story).  As the international market in more sophisticated devices developed in the 1970's, the it became expensive to produce different devices for different markets.  So the IEC 60302 standards - for all those power cords you've seen on computers that connect a local plug to a standard 3-wire jack, not to mention smaller versions for things like electric shavers - emerged.  I used to joke that these were devices to help you destroy your equipment:  Typically, on earlier devices, you had to throw a switch on the device to configure the correct voltage.  Forget to do that could be a very expensive mistake.
Eventually - partly as a result of pressure from Europe (Germany, in particular) which required that the device come with the switch *already set in the appropriate position for the country of delivery* - auto-ranging power supplies were developed and became pretty much universal.  Wall jacks continue to come in a large variety of configurations around the world, but pretty much all electronics "just works" if you have a simple pass-through adapter.
The USB standard demonstrated the need for a lighter lower-power standard.  Portable devices don't want to pay the size and weight cost of a power-line-to-low-voltage-DC convertor, so rely on an external "bug", and those "bug's" have over time pretty settled on the USB voltages/amperages (though less on these)/physical configurations just for power.
USB 3 may be killing the goose here.  The great thing about USB is that first letter: Universal.  We completed the effective evolution from USB 1.0 to 1.1 to 2.0 quite some time ago.  There was a period of disagreement about how phones should tell chargers they could supply "more than USB 2.0" power for a couple of years, but that's hidden by smarter silicon these days.  If you see a USB socket, you know what you're getting (modulo malware, of course), both on the power side and on the data side.
USB 3, unfortunately, has introduced variety.  The initial nonsense of 3.0 vs. 3.1 and just what speed you can get (5Gb/s or 10Gb/s) probably didn't poison the market because the market so far is fairly small, but it was a bad sign.  The whole notion of extensible uses for the USB wires is great technically, but it's incompatible with the notion that "I just connect it and it works".
My concern with the whole USB authentication process is that it will fragment the market even more.  You'll have situations where device A can connect to B using cable C, A and D can also talk over C - but B won't talk to D over C.  Users will have no way to figure out why:  Everything will, by design, be labeled the same to maintain the illusion that it *is* the same.  But under the covers, it won't be.
I'm afraid engineer's love for "generality" - combined with varying business drivers, from IP protection to the ability to avoid the current commodity market in USB parts and carve out protected areas for rent-seekers - will lead us back to an era of confusion - and various increased costs.
The easiest and best way to block dangerous USB cables that fry equipment is through reputation - reputation maintained through enforceable trademarks.  And, of course, more robust equipment that blocks voltages and currents going to "the wrong place" to being with, protecting itself from a much wider array of faults than any certification process possibly could.
This whole effort strikes me as wrong-headed.
                                                        -- Jerry

@_date: 2016-05-04 22:35:03
@_author: Jerry Leichter 
@_subject: [Cryptography] USB 3.0 authentication: market power and 	DRM? 
You're confusing two things.
*Authentication* chips in computer cables are supposed to be a way of protecting users from cheap and potentially dangerous cables.  In reality, they are more likely to be a kind of DRM, maintaining the profits of the cable makers.  We didn't need them to have safe power cables - we looked for the UL label.  We don't need them for computer cables.
*Chips* in cables are a done deal.  The kinds of speeds we are pushing through copper today are impossible without active components to do pulse shaping and various other kinds of adaptation.  Why it's worse to have those chips in the cables than in the jacks (that may be possible, but you really need to be right near the cable for this kind of things to work) is beyond me.
Of course, if you go fiber rather than copper, you need a chip to do the translation between the electronic and photonic domains.  At least some part of it has to physically couple to the fiber.
Every chip can potentially be compromised.  Chips in cables seem neither more nor less vulnerable than others.  I see little basis for singling chips in cables as particularly hazardous.
                                                        -- Jerry

@_date: 2016-05-06 20:09:20
@_author: Jerry Leichter 
@_subject: [Cryptography] Why two keys? [was: Re:  WhatsApp, 
Note that "additional entropy" is not generally a requirement here.  Mainly what's necessary is that knowing one key gives you no help in learning the other.
A common technique is to derive keys for different purposes from a common master key.  For example, given master key M, you might use E = H(0 || M) as your encryption key and A = H(1 || M) as your authentication key.  H, obviously, has to be a cryptographically secure one-way hash.  M must have at least as many bits as E and A, but could well have more.  But there's little reason to go nuts about this.  0 and 1 are arbitrary - people will often do things like E = H("Encrypt" || M).
Is this "as secure" as using different random E and A?  Clearly not.  Perhaps there's a weakness in H which allows one to get A from E or E from A.  Or perhaps there's some really odd connection between your encryption and authentication algorithms and H, used this way, that allows one to break the combination.  In practice ... this is rather unlikely.
Of course, you now have to trust that H's properties are secure.  You could, as an alternative, build H from a primitive (like your encryption function) in such a way that the security properties of H that you relay on can be derived from the security properties of the encryption algorithm you're already relying on.  That avoids adding any new things you have to trust - but perhaps increases the likelihood of an interaction between your hash and encryption functions.
The main advantage of this approach is that you can easily derive new keys for new purposes without having to generate and securely store new random bits.  The more you can isolate different cryptographic contexts from each other - so that even a leak of a key in one context provides no help in breaking messages in another - the better.
                                                        -- Jerry

@_date: 2016-05-12 15:59:13
@_author: Jerry Leichter 
@_subject: [Cryptography] 2nd Amendment Case for the Right to Bear Crypto 
I can't speak for bulletproof *cars* - didn't bother to do the simple Google search - but bulletproof *vests* are, in fact, regulated. From  :
"Under federal law, a bulletproof vest is considered body armor, which is regulated by statute, 18 U.S.C.A. Section 931. That law forbids anyone convicted of a violent felony to own or possess a vest, unless the person wearing the vest is an employee who is doing so in order to perform a lawful business activity and who has obtained prior written certification from the employer. A violation incurs a maximum of three years in prison. And using a vest during the commission of a federal crime of violence or a federal drug-trafficking crime will result in an enhanced sentence. (42 U.S.C. Section 3796ll-3(d)(1).)
The federal law has been challenged on several grounds, all of them unsuccessfully."
(The last of the challenges discussed was on Second Amendment grounds.)
The states also have regulations:
"A few states prohibit the use or possession in specified situations or circumstances, without regard to the criminal background of the wearer. One state prohibits wearing armor on school property or school-sponsored functions (Louisiana), while in Connecticut, sale of body armor must be done in personInternet and phone purchases are illegal."
We now return you to our regular fact-based discussions....
                                                        -- Jerry

@_date: 2016-05-12 16:57:55
@_author: Jerry Leichter 
@_subject: [Cryptography] =?utf-8?q?=22Chinese_ARM_vendor_left_developer_bac?= 
From Ars Technica (
Allwinner, a Chinese system-on-a-chip company that makes the processor used in many low-cost Android tablets, set-top boxes, ARM-based PCs, and other devices, apparently shipped a version of its Linux kernel with a ridiculously easy-to-use backdoor built in.  All any code needs to do to gain root access is send the text "rootmydevice" to an undocumented debugging process.
The backdoor code may have inadvertently been left in the kernel after developers completed debugging.  But the company has been less than transparent about it: information about the backdoor was released and then apparently deleted through Allwinner's own Github account. The kernel, linux-3.4-sunxi, which was originally developed to support Android on Allwinner's ARM processors for tablets, has also been used to develop a community version. The kernel was also the basis for porting over various versions of Linux to Allwinner's processors, which are used in the Orange Pi and Banana Pi micro-PCs (developer boards compatible with Raspberry Pi) along with a number of other devices.
The way Allwinner has distributed its Linux kernel has been frustrating to many developers.  The company has not encouraged or participated in community development and has been accused of numerous violations of the GPL license for the Linux kernel.  The kernel "drops" by Allwinner include a number of binaries that are essentially closed source, as well as code released under other licenseslargely to support the graphics engines of its processors.
                                                        -- Jerry

@_date: 2016-05-21 07:00:51
@_author: Jerry Leichter 
@_subject: [Cryptography] "60 Minutes" hacks Congressman's phone 
Hardly.  While I don't don't the intelligence community provides a word in the ear here and there, the fundamental problem here is much more deep-seated.  The telcos and their systems were built in an age of mutual trust - well-place or otherwise.  Before SS7, all switching was based on in-band signaling:  Various tones transmitted over the same lines as voice.  That's why the Black and Red Boxes of the late 1960's/early 1970's could be built:  There were tones you could send of a phone line that gave you direct control over the switching equipment.  "No one would do that" ... until they did.
Before that, everything controlled by human beings (operators) - mainly over the same lines.  If you knew the right lingo, you could fool operators into treating you as telco employees, letting you manipulate all kinds of stuff.  "No one would do that".
SS7 solved the in-band signaling problem by moving the signaling out of band.  Nothing you sent on the line went to the switching equipment.  Hook into the network used by SS7, though, and you were completely trusted.  After all - who could hook into those lines?  Just the telco's - initially AT&T and a few small companies in the US, and government-run PTT's in the rest of the world.  "NOBUS" in a different sense.  We're all friends here; we trust each other.
Retrofitting SS7 with a system that's not based on trust would be a huge undertaking - but even that pales compared to the organizational changes needed.  The telcos world-wide work as a fairly closed community.  They would have to move to a system of mutual distrust and verification.
Another place you can see this issue is in some of the billing abuses that the system has historically been rife with.  In the US, you can switch LD carriers. As initially set up, your new carrier told your old one that you had switched - and by law, they had to allow the switch to take place.  After all, all LD carriers are trustworthy and wouldn't take over an account without permission.  Mutual trust, NOBUS.  Similar things happened with third-party LD charges.
The telcos are hardly alone here.  Once you're accepted as a bank - anywhere in the world, vetted by any local government - you've historically had insider access to the entire banking system.  After all, one bank wouldn't abuse another's trust, right?
Our world was built on these kinds of trust relationships.  The diamond trade is an example where this is very explicit.  Before you can be accepted into the community, your picture is circulated to and posted very visibly at all the major trading floors for some period of time.  If anyone recognizes your picture as that of someone they don't trust, you won't be accepted.  Once you're in, you're in - deals for millions in diamonds are made with the shake of a hand.  Abuse that trust and you're tossed out of the community.  The word is spread very quickly; the community isn't that large.  Here you clearly see the extension into an institution of the way individuals maintain their trust relationships.
Of course, the entire Internet was built on similar ideas.  Enter the IS-IS network and grab nearby packets for yourself.  Get accepted as a BGP speaker and grab packets on a world-wide scale.
Today's institutions work at scales and at speeds way beyond human abilities to judge trust.  Global interconnectivity has removed the need for physical presence to carry out many attacks.  And attackers have become much more technologically sophisticated.
Changing what is often a century or more of design and practice is difficult and will take a long time, even given the best of intentions and the strongest motivations.  In fact, many of these institutions will never change.  Instead, solutions will get built "over the top".  Do end-to-end encryption - realistic for phone conversations on a mass scale only in the last decade - and leakage of phone conversations by the SS7-based network becomes irrelevant.  (Notice, BTW, that the cellphone network encrypts - for better or worse - *between cell and base station*.  Once it's on a landline ... it's NOBUS, "our lines are secure". And people believe this stuff:  There's a quote at the end of the WaPo article in which someone says he won't trust his cellphone any more, for confidential stuff he'll use a landline.  Right.)  Metadata is much harder because that's the  stuff SS7 is saying *to itself*.  Tor is an over-the-top solution for metadata on the Internet, but it's probably not the right solution for phone conversations.  And the location information is entirely between your phone and the SS7 infrastructure - it's not clear that any over-the-top solution is possible.  And ... if you eliminate the notion that all telco's trust each other to exchange location information, how do you do roaming?  (You can get that effect for non-real-time communication using "dead drops", but real-time is much harder.)
Very difficult problems.  A golden age for the intelligence guys, and they only have to tap into it, not get it designed for them.
                                                        -- Jerry

@_date: 2016-05-23 06:02:07
@_author: Jerry Leichter 
@_subject: [Cryptography] immortal quote about randomness 
And of course parallel statements apply to entropy.  Entropy is a property of a source of bits (well, values drawn from some set, but in most cases bits is good enough).  If you have a bunch of bits ... you have a bunch of bits.  Any entropy is in the source you drew it from, not in the bits you have.
(A bit oversimplified, both for entropy and randomness, since the connection between a particular set of bits, a particular source, and what exactly you measure may be subtle - see Kolmogorov complexity - but the point remains.  English is not a precise way to express mathematical concepts.)
                                                        -- Jerry

@_date: 2016-05-23 22:44:18
@_author: Jerry Leichter 
@_subject: [Cryptography] Entropy Needed for SSH Keys? 
Let's think this through a bit.  Kernel ASLR, stack canaries, and so on, are there to protect against external code that finds holes.  Early during boot, *there's no external code running*.  We're before network initialization, so there's nothing coming in from the network links.  Basically, if an attacker has managed to get code running at this point during boot, you don't have much hope anyway.
So it seems to me you want to address a different issue:  Not how do I get enough randomness to set up kernel ASLR and related mechanisms early in boot, but how to I *put off* setting up kernel ASLR and related mechanisms until I have a usable source of randomness?
                                                        -- Jerry

@_date: 2016-05-27 06:58:37
@_author: Jerry Leichter 
@_subject: [Cryptography] Hacking spread spectrum clocking of HW ? 
Depends on what you think the purpose is.  If it's to communicate through noisy channels, or just to experiment to improve or even just personally understand the technology, or to provide a public service - then it's within the ambit of the amateur services, at least as they are defined in the US.  If it's defined to include secret communications, it's definitely outside the US definitions.
I'm not sure that's ever permitted in the US under any circumstances.
                                                       -- Jerry

@_date: 2016-11-09 13:54:29
@_author: Jerry Leichter 
@_subject: [Cryptography] Happy birthday, NSA! 
This is actually pretty common for any agency that's concerned about security.  I heard about it many years ago from someone who worked in a prosecutor's office.
(On the other hand, I heard a story from someone who went to a job fair at his college - again many years ago.  He talked to some CIA recruiters who in his estimation were ... not very good.  One of them really tried to recruit him, and gave him a business card.  The guy looked at the business card, back at the recruiter, and said "so I guess this isn't your real name, then."  The recruiter turned red, reached out, and took the card back.  The same guy talked to NSA recruiters as well.  While he had no interest in taking a government position, he did think the NSA guys, unlike the CIA guys, were really sharp and knew their stuff.)
                                                        -- Jerry

@_date: 2016-11-13 01:12:52
@_author: Jerry Leichter 
@_subject: [Cryptography] October 28th is now National Cryptography Day 
If Hilary had encrypted her email, she would have been faced with a court ordering her to decrypt it.  Yes, there could have been a whole battle over whether the 5th Amendment prevents the government from forcing you to disclose your password.  The law on this is currently completely unsettled.
But ... it would have made little *political* difference.  First of all, she actually turned over the relevant email, leaving out only what she claimed was "irrelevant" personal stuff.  I see no reason to believe she would not have decrypted this material if the demand was made.  And if she had refused, the political liabilities would have been enormous - way beyond any outcry about Trump not releasing his taxes.  The actual contents of the emails were, in fact, pretty mild as these things go.  Had she refused to decrypt them, people - certainly the Republicans, but even many who would otherwise sympathize with her - would have assumed there must be something in there to hide.
Cryptography isn't a magic bullet solving every social or political problem.  Hilary was at least partially defeated by a history of keeping things hidden and quiet - which ended up leading to much more trouble when they finally came out than they likely would have had she been more up-front to begin with.  This made her look untrustworthy:  No matter how many investigations might clear her of any wrong-doing, each likely revealed *something* new - leading to the suspicion that there was yet more to be found.  This is perception vs. reality, and a *perception* of dishonesty can only get worse if material is hidden - by cryptography or otherwise.
Cryptography is a tool, and like any tool, it has to be used wisely and appropriately.
                                                        -- Jerry

@_date: 2016-11-15 05:08:11
@_author: Jerry Leichter 
@_subject: [Cryptography] On the deployment of client-side certs 
A system based on a client-side certificate consists of the following on the client side:
1.  A private/public key pair;
2.  Secure storage of the private key;
3.  Secure computation of a signature using the private key;
4.  Delivery of the public key along with appropriate signed material.
Both steps 2 and 3 represent the key implementation requirement:  A secure mechanism to hold and apply a private key.
Given the hardware necessary to do that, wouldn't it be easier, more efficient, and less likely to leak identity information to use it to implement a password-authenticated key agreement protocol like SRP?  Note that the "password", being stored in the secure hardware rather than the user's head, can be an arbitrary bitstring.
                                                        -- Jerry

@_date: 2016-11-15 14:55:04
@_author: Jerry Leichter 
@_subject: [Cryptography] On the deployment of client-side certs 
It's not clear to me how *any* mechanism in which a server has multiple clients can possibly work without the client first identifying itself so that the server can figure out who's logging in to it.  (Of course, I suppose any mechanism could be used with an expensive protocol in which the client *doesn't* identify itself but rather the server simply tries the credentials against every account it knows about.)
That said ... there's no reason why the client's unencrypted identity should be the same for different servers.  In fact, if your concern is that actions across multiple servers can be correlated back to the same user, it *shouldn't* be - at the least, even if everything else is protected, multiple conspiring servers could correlate their own users.
So, sure, you could use client certs - but you'll want to use a different one for each server, which is moving far away from the intended use case, where the cert identifies the individual user.
Note that if you assume everything is handled by software, both username and password should be "just bit strings", separate for each correspondent site.  (They could easily be generated in a deterministic way from the site identification and a single local secret.)
SRP brings two things to the table:  It allows authentication without leaking any of the private information (the "password"); and it blocks brute-force attacks against weak passwords.  I would actually agree that the latter is overkill when the "password" is handled by hardware and can be assumed to be safe against such attacks.  So SRP is overkill.  But it's perhaps a bit less overkill than the whole cert structure, which has layers of stuff in it that are completely unrelated to the problem at hand.
If the goal is to leverage the existing infrastructure, perhaps "abusing" clients certs is the way to go.  But in fact client cert support seems to be in bad shape anyway, so there may not be enough existing infrastructure to make it worthwhile.
                                                        -- Jerry

@_date: 2016-11-15 18:10:16
@_author: Jerry Leichter 
@_subject: [Cryptography] On the deployment of client-side certs 
The issue is entirely one of UI and interaction design.  No one has come up with a compelling presentation for a hardware authentication device - one that most people will find to have a usefulness/inconvenience ratio that's high enough to justify using it all the time.
Using a smartphone piggy-backs on the value of the smartphone, which is high enough to get hundreds of millions of people to carry it along.  But that's just the hardware.  What's the interaction model to use the phone as an authentication/authorization device?  Contrast the steps needed to use Apple Pay with the steps needed to use some of the attempted competition.  That's why Apple Pay is leading.  This is exactly one of Apple's greatest strengths:  Designing user interaction models that people find so simple to learn and use that they don't even think about them - they just fit them into their daily lives.  No one does that better than Apple - but even so Apple Pay has a long way to go to reach universality.
Are watches, in some form, a better alternative?  Rings?  Something entirely different that no one has even built yet?  If future interfaces are heavy on Augmented Reality ... where/how should authentication devices fit in?
Ten years from now, we'll look back at all of today's solutions and marvel and how primitive and clunky they all appear....
                                                        -- Jerry

@_date: 2016-11-16 13:30:00
@_author: Jerry Leichter 
@_subject: [Cryptography] On the deployment of client-side certs 
Note that Apple is trying for the best of both worlds:  Hardware security inside the phone's chip even while the surrounding device is general-purpose and has all kinds of downloadable software.  If designed and implemented properly, this is clearly the best way to gain both security and usability.  No comment on how successful Apple is at such proper design and implementation - though I don't see anyone else trying.  (The advantage of controlling both hardware and OS....)
                                                        -- Jerry

@_date: 2016-11-17 06:34:05
@_author: Jerry Leichter 
@_subject: [Cryptography] On the deployment of client-side certs 
That's, by design, a low-level hardware cryptographic engine.  "It will focus on the classic low level cryptographic functions and primitives, and not get drawn into re-implementation of application protocol layers."  It's an HSM.  Nice to have in open source form, but by intent hardly an innovation.
No, I don't see it even trying to solve the problem addressed by "Hardware security inside the phone's chip even while the surrounding device is general-purpose and has all kinds of downloadable software.  If designed and implemented properly, this is clearly the best way to gain both security and usability."
                                                        -- Jerry

@_date: 2016-11-17 17:21:32
@_author: Jerry Leichter 
@_subject: [Cryptography] On the deployment of client-side certs 
In the case of iOS devices, it's a part of the same chip; the separation between the Secure Enclave and the OS is enforced by the hardware, but granted such separations have been known to be breakable in the past.
However, in the case of the latest MacBook Pros, it's a completely separate chip, the T1 - a small ARM chip separate from the Intel CPU.
It's unclear exactly what roles this chip plays.  It's known that it controls the fingerprint reader - the fingerprint data never leaves the chip (which leads to a limitation on the number of distinct fingerprints you can register at once:  5).  It apparently also controls the Touch Bar, but that's not currently a security issue.
There are *rumors* that it controls the light that tells you the camera is on, but I've seen no evidence (and I doubt it's true, even if just for the extra wiring needed - the T1 is under the keyboard somewhere, the camera and light are all the way at the top of the display).  In at least some designs (I haven't seen anyone actually keeping track of this on recent Apple devices), the light was kind-of hardware controlled - there was no direct software access to it, but rather it lit as a side-effect of other actions the software undertook to turn on the camera.  Someone did find an attack that managed to break the apparent hard link between the two actions in older MacBooks.  That attack only worked on some now very old hardware, but where current hardware stands ... I don't know.
It is interesting to note that no one, not even Apple, bothers to give you any indication, even software-controlled, that your microphone is live....
The fingerprint sensor and the ARM chip it connects to are bound to each other - as people who've replaced the fingerprint sensor on iPhone's have discovered to their regret.  (The ARM chip won't talk to the new sensor.)  One guess is that they share a key so that fingerprint data is encrypted and authenticated between sensor and chip.  Allegedly the sensor is only accessible from the Secure Enclave mode (or maybe its data is encrypted and only the Enclave has access to the key).
Note that if the chip in the MacBook Pro really does control the Touch Bar, it would have complete control over a small but very-high-resolution display with integrated touch input.  You could easily output secure messages for confirmation through a virtual button, all completely outside the OS's (and even the main CPU hardware's) control.  Right now, Apple doesn't do anything of that sort - and indeed they discourage any use of the Bar as an output device, or to do something that can't be done on models that lack it.  Makes sense until devices with a Touch Bar attain critical mass.
But ... "first iteration" indeed.  There's room for all kinds of interesting development of the T1 and Touch Bar as a true security kernel/secure interface once enough of the things are out there.
It'll be interesting to see where Apple goes with this over time.
                                                        -- Jerry

@_date: 2016-11-18 17:42:37
@_author: Jerry Leichter 
@_subject: [Cryptography] On the deployment of client-side certs 
There are multiple microphones - if I remember right, the new MacBook Pro's have three!  And it's not particularly easy to block even one....
Are you sure that both directions are broken?  I doubt the paths have all that much in common - you could easily have a broken audio output with a perfectly functional audio input.
                                                        -- Jerry

@_date: 2016-11-19 05:26:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Trump sparks downloads of encrypted chat app 
Of course the sad thing is that that sentence would have been valid whichever major party candidate won.
                                                        -- Jerry

@_date: 2016-11-19 05:48:50
@_author: Jerry Leichter 
@_subject: [Cryptography] Crypto and rustling 
If the set of possible brands is open-ended, it's clear that no solution is possible:  An altered brand is just a new symbol that's as valid as the original.
If there's agreement up front about what the set of legitimate brands is, you just need to ensure that no brand in the set, when viewed as a collection of curves, is a subset of any other.  A set of brands all consisting of the same number of constituent curves is an obvious example.
A real-world example from way back when:  DEC wanted to ship a "limited" VAX that would be cheaper but would only run some software, not other software.  (I don't recall for sure, but I think they wanted it to run a real-time OS called ELN, but not VMS.)  The limitation would be implemented through an ID delivered from the boot ROM.  It was assumed that copyright law could prevent anyone from copying the existing ROM from an unlimited machine and burning a new one; but that couldn't stop someone from modifying a ROM they had received with their machine.  Given the realities of the ROM used, it was possible to change bits in only one direction, not the other - say, you could change a 0 to a 1 but not a 1 to a 0.  So the problem was stated as:  Devise an ID code (or set of ID codes) with the property that changing 0's to 1's in a valid ID code for a limited VAX never gave you a valid ID code for an unlimited VAX.
Not a particularly difficult puzzle, but amusing.
(DEC - and others - were casting about for ways to maintain margins without ceding the low end to cheap PC's.  Some here may remember the MicroVAX II RC.  RC stood for "Restricted Configuration" - restricted to keep you from adding more cards to the backplane.  The "restriction" was implemented by pouring epoxy into the otherwise-open slots in the Q-bus.  Some sophisticated users simply bought Q-bus backplane modules - DEC sold them separately - and replaced the epoxied versions....  In general, all these attempts went nowhere.)
                                                        -- Jerry

@_date: 2016-11-22 06:13:02
@_author: Jerry Leichter 
@_subject: [Cryptography] combining lots of lousy RNGs ... or not 
I would put this in slightly different terms.  Combining two (or more) random number generators is just like building a reliable system by combining multiple, redundant, unreliable components.  This is a great approach - indeed, the only approach we have for building highly reliable systems - but it only works *if the failure modes of the components are uncorrelated*.  Major systems of all sorts - from buildings to economic constructs on Wall Street - have failed exactly because "I don't see a correlation" is very far from "there *is* no correlation".
If you can't characterize why some alleged source of randomness is unpredictable to an opponent, it's unlikely you can characterize whether two such "squishy" (to use John's term) sources are correlated.  If, as it happens, both are actually pretty good, but both can be made predictable by imposing an AC signal on their power rails - you've gained precisely nothing by combining them.
Knuth has the classic example of a (non-cryptographic) "super pseudo-random number generator" that was clearly really strong - and which in fact had a short cycle. Designing random number sources based on ... nothing much, and then combining them and saying "Oh, now it's certainly secure", is no better than designing block ciphers  based on ... nothing much, and then simply chaining them together.  Triple rot13, indeed.
                                                        -- Jerry

@_date: 2016-11-23 17:37:22
@_author: Jerry Leichter 
@_subject: [Cryptography] Is Ron right on randomness 
It's tough to say, because the description is a bit odd.  I actually agree with the last point (though it's one that is often contentious):  A properly seeded DRNG is every bit as secure, and much easier to *show* secure, than some fancy mixed pool of uncertain "random" sources.  For example, if you're going to encrypt with AES anyway, you're already assuming it has properties that would make AES in counter mode with a random key secure.
So ... you only need enough bits to feed your DRNG.  But ... where do they come from?  If you have a way to get (say) 256 bits with 256 bits of entropy ... you're good to go.  But if you instead have 10 sources each of which gives you 256 bits which you are only willing to assume have 26 bits of entropy each ... what next?  Whitening is kind of beside the point.  If there are correlations among your sources, it won't help - though it will *look like* it does.  (In fact, *nothing* will help:  Adding up the 10 sources of 26 bits each is only correct when they aren't correlated; if they might be, and you just don't know, the only safe thing to say is that you have ... the same 26 bits you start with.)
*If* your sources are "known to be uncorrelated" (in whatever way you "know" that they actually give you 26 bits of unpredictability ... but let's not go there), then any function taking the 2560 bits into 256 bits that takes all the bits into account is a reasonable start.  But to avoid getting trapped by what "taking the bits into account" means, you're probably best off using some cryptographic primitive.  A cryptographic hash function will certainly do the trick.  Of course, if you want to keep your security assumptions to a minimum - and, as before, you're going to use AES-256 anyway - a secure MAC with a 256-bit output based on AES-256 is the way to go.
BTW, notice the direction of my argument, which pushes toward making everything rely on *one* primitive.  This sounds counterintuitive, but think about it:  If you design a system *no stronger than its weakest link*, relying on multiple primitives for those links cannot possibly strengthen your system - it can only weaken it.
I suppose you can describe this process as whitening, but it's a bit of a stretch.
                                                        -- Jerry

@_date: 2016-11-28 08:39:51
@_author: Jerry Leichter 
@_subject: [Cryptography] OpenSSL and random 
Feed them into a pool, stir - and, voila, opinions at least as valid as the most valid of the individual ones.
                                                        -- Jerry :-)

@_date: 2016-11-28 16:37:46
@_author: Jerry Leichter 
@_subject: [Cryptography] randomness for libraries, e.g. OpenSSL 
Ahem.  Intel went and provided the hardware.  And if you read the messages here ... you shouldn't trust it.  So what's the next step?  (Audibility is a fine idea ... but how many people in the world have the knowledge, and access to the equipment, needed to check *the actual implementation on a state of the art chip*?)
BTW, if we agree on a standard protocol for an external "random source" ... who's to say the next Intel chips won't recognize that standard protocol and send the bits being generated off to some some server in Utah or Russia or China?
                                                        -- Jerry

@_date: 2016-11-28 16:46:58
@_author: Jerry Leichter 
@_subject: [Cryptography] RNG design principles 
Hmm.  Sounds kind of like a pipe, or a socket.  The semantics is mainly already there, just not specifically as a file system.
This could be done through something pipe- or socket-like, too - whatever is listening will only accept one write, then close its side.
                                                        -- Jerry

@_date: 2016-11-29 05:33:07
@_author: Jerry Leichter 
@_subject: [Cryptography] RNG design principles 
Yes, of course.  But the point I was making is that the user side of the semantics is pretty much already there.  You don't need a file with special properties.  You need something like a pipe that gets filled with information obtained from some form of persistent storage not otherwise accessible at all.
Put another way:  Imagine there's a named pipe at a known location accessible during the early boot sequence.  The named pipe *acts as if* it was created by a process that wrote a single initial value to it, then waited for a single return value to be written back, closed the pipe, and exited.  That process would get the initial value from some private-and-accessible-only-to-itself persistent memory, and write the new value back to the same place.
That pipe would have the right semantics.  The conjectural process couldn't be implemented as a normal Unix process because (a) it would have to run too early; (b) the only persistent storage available to a Unix process is the file system; (c) there's no way to have a file accessible to exactly one process.  So you'd have to find a different way to hack that into boot.  But that seems easier than adding a whole new kind of file object to file systems, just to implement this one special file.
BTW, the Mac OS System Integrity Protection - which makes certain parts of the file system accessible only for reading, even to root, except to specifically entitled programs - would, with a minor extension (the ability to make some files inaccessible even for read) get you close to being able to implement what you want using an actual boot-time-started process.  (If I remember right, various access mechanisms in VMS would have made similar restrictions trivial to implement 20+ years ago.)
                                                        -- Jerry

@_date: 2016-11-30 07:18:12
@_author: Jerry Leichter 
@_subject: [Cryptography] randomness for libraries, e.g. OpenSSL 
Why five bits?  What kind of attack leaves you with 5 bits of randomness, but no more?
If Intel was "allowed and motivated to act in good faith and didn't get sabotaged" then *all* the bits, produced at a very high rate, are good.
If Intel *was* somehow forced to cooperate, for all you know what's really in there is a DRNG based on, say, time and a wired-in unique 256-bit starting point.  (The starting point might not be visible to anyone - but some TLA receives a list of all possible starting points, of which there will only be a couple of hundred billion at most, readily searchable.)  Then you get *no* bits from the hardware.
You're pulling a number - 5 bits - out of thin air, without an attack model or any kind of theory to justify it.  Looking more broadly ... that's what leads to these endless discussions.  A number of years back, arguing from "engineering gut" that various sources had some "entropy", and that if you just mixed enough of them, the result had "enough entropy", might have been acceptable.  There wasn't really any theoretical base to do better.  But that's no longer true.  John Denker, among others, has proposed mechanisms that produce "hard randomness" based on pretty solid physical assumptions.  And we now even have a theory of mixers (combiners), some of it amazing recent.
I would suggest that we should be treating proposals to build random number sources based on unsupported assumptions pulled out of the air, and complex algorithms that no one can justify, in *exactly* the same way as we would treat a proposal to "strengthen" AES by making some small "obvious" improvements to its key scheduling algorithm.
                                                        -- Jerry

@_date: 2016-11-30 15:58:19
@_author: Jerry Leichter 
@_subject: [Cryptography] "Great. Now Even Your Headphones Can Spy on You" 
You thought you disabled all the microphones on your computer.  Think again.
(The fact that pretty much all the technologies used to produce sound from electrical signals can inherently also be used in the reverse direction is hardly something new.  That one of the widely used D/A converters will happily work in the reverse direction is ... interesting.
                                                        -- Jerry

@_date: 2016-10-01 15:39:15
@_author: Jerry Leichter 
@_subject: [Cryptography] distrusted root CA: WoSign 
It goes even further than that.  Suppose you want to provide a secure connection between a couple of endpoints, all of which you own.  Unless you want to roll your own software, you have two widely-accepted choices for secure connections:  SSH and SSL.  SSH *as a library* is a fairly unusual thing.  There are a couple of implementations to be had, but you'd find yourself justifying the choice, and its not clear how the configuration works.
SSL libraries, on the other hand, are almost universal - and come with Java and other common frameworks because they are needed for HTTPS.  So SSL has become pretty much the standard as a secure channel.  Which in and of itself isn't a bad thing - it's a very closely studied protocol, and likely becoming as secure as any library you're likely to find.
But ... buying into SSL requires buying into certificates.  While I believe it's *possible* to use pre-shared keys with SSL, it's not a common or well-understood configuration.  And besides, every security audit your code then has to go through will assume you're using certificates - because SSL needs then, right?  And who will you get to sign your certificates?  Suppose you're selling a system that will be installed as a bunch of communicating processes, which need to authenticate to each other.  Are you going to ask your customer to go to his CA to get signed certificates for each process?  Not a good customer experience.
Here's the reality for managing the infrastructure of large data centers - something I have to deal with every day.  They generally have HTTP/HTTPS protocols.  When you use HTTPS, they pretty universally have self-signed certificates.  Changing this is extremely difficult:  These things are almost always on an isolated, non-routed network (almost always net 10), and often have no DNS.  Good luck getting a cert for 10.0.5.10.  (And would you trust anyone who would give you one?)  Most of the vendors don't even try to give you the option.  (One large vendor, after multiple complaints from customers about the self-signed certs, provided a mechanism to have a CA sign the certs - but the CA had to be the managed system itself!)
For all its problems in the domain for which it was originally intended, the CA design is disaster in other settings.
What I would *really* want to do - if I have to use SSL, and anything else raises so many issues with security auditors it's rarely worth the effort - is to use a self-signed cert and then use certificate pinning to ensure that exactly that cert is the only one that will ever be accepted.  In other works, turn the certificate into a pre-shared key.  But I don't even know of any SSL implementation that supports certificate pinning - it's an extension implemented by some browsers.
In terms of raw numbers of people affected, browsers and their users would certainly be the largest clients of SSL.  But it's become a standard way beyond the Web/browser domain - and the needs of other users are pretty much ignored.
                                                        -- Jerry

@_date: 2016-10-01 17:48:55
@_author: Jerry Leichter 
@_subject: [Cryptography] another security vulnerability / travesty 
Hard enough that attackers will choose other routes.
For the vast majority of people communicating health information with their doctors - the example with which we were dealing - fax communication is much more secure than the computer sitting in the practice's back room.  Or even the paper records sitting in the office.
But really the whole question is a mess.  Your medical information goes from you to the doctor by some route.  It's stored by the doctor - somewhere.  The doctor sends it to the insurance company - somehow.  He sends you prescriptions (often almost as telling as your full medical history) to your pharmacy - somehow.  Spending time looking at one of the many places your medical stuff goes, while ignoring the rest, is rather pointless.  Personally, I would rather not send sensitive stuff through email, because it's so easily tapped and we *know* it's being tapped.  FAX may be a small step up, but at least it keeps things somewhat more limited:  The FAX message isn't stored indefinitely in what could be an easily network-accessible server.  But that's about as far as it makes sense to me to go.
                                                        -- Jerry

@_date: 2016-10-02 08:45:56
@_author: Jerry Leichter 
@_subject: [Cryptography] distrusted root CA: WoSign 
As I've pointed out before:
1.  Trusted introduction is a psuedo-problem.  The original idea was that I know Macy's because I've seen the stores around, they've been around for decades, I've shopped there - so I trust them.  When I type macys.com into my browser, I know (how, exactly?) that I'm going to a site owned by the same store.  When I get a signed cert from macys.com, I know I really reached macys.com, so I can trust it really is Macy's - and it's safe to shop.
Let's now look at the reality.  I see an ad for cheap socks from SocksRUs.com.  I connect to SocksRUs.com and get a signed cert that it really is SocksRUs.com, so I can trust ... what, exactly?  Do I even have any reason to believe that the SocksRUs.com I reached had anything to do with the ad I saw?  And even if I can trust they are the same - just what did that trust gain me?
Or I get email claiming to be from Bank Of America, which has an embedded link that goes to bank0famerica.com, which has a signed cert that it really is bank0famerica.com - so I can trust that ... I've safely and securely reached a fake site.
The fact of the matter is that the trusted introduction problem - safely binding a first connection to a site to some real-world knowledge about that site - comes up so infrequently as to be of vanishing importance.  Sure, you can construct examples that actually work - I give you a business card with my printed web address (or the bank gives me a brochure with the same), I type it in and can get reasonable confidence that the site I reached is the one belonging to the person/organization handing out the pieces of paper.
But ... who types web addresses any more?  People find web addresses by doing searches.  So I *search* for Bank of America, go to the indicated link (which sure enough has a cert saying it is, indeed, the link it says it is), and proceed.  So *I can trust that I reached the site the search engine told me about*, *not* that I reached Bank of America.  To gain *that* level of trust, I need to trust the results of my search engine.  If my search engine handed me the public key that corresponded to the URL directly - or if one of the standard techniques for embedding trust *in the link itself* were used (not practical in a world where people type URL's, but that's not the world we live in), the elements I have to trust are *reduced*.  The CA never added anything here, because it simply was not in a position to do so by the nature of the problem.
2.  The vast majority of my connections to a site occur *after* the first.  Why would I need a CA for those?  What I really want is ssh-style key continuity.  The CA simply gets in the way here - and in fact the introduce a whole level of extra vulnerability, because it *may* lead to to trust a fake site *even when I have reliable information about the real one*.  Oh, and of course the site doesn't need the CA to change keys - it simply needs to sign the new ones using its old ones, maintaining continuity.
3.  Ultimately, any trust I might have based on cryptography has to rely on trusting the browser vendor.  His code can do whatever it likes.  In the CA world, beyond trusting his code, I also have to trust his list of CA's.  (Yes, I can create my own list of CA's, but who does that?)  So before I even get to an individual CA, I'm trusting the browser vendor at two different levels - and on the second level, choice of CA's, the browser vendors have limited credibility, because they are embedded in a political system that would make it very hard for them to say no to a CA except in really exceptional situations (like WoSign).
So if you want alternatives to the *real* problem of secure connections on the Internet, not the pseudo-problem of secure introductions, there are at least two:
a.  I've previously on this list worked the numbers and shown that it would be perfectly possible for browser vendors - or others who want to provide an equivalent service - to regularly distribute the public keys of, say, the top 100,000 sites on the Internet.  Once you get beyond those, there's frankly not much to be gained in knowing that SocksRUs.com really is SocksRUs.com.
b.  As I outlined above - people don't really use URL's, they use searches.  Fortunately, the top search engines are pretty good - actually, *really* good - at returning URL's that match the *intent* of the search.  Why can't they extend that so that if you click on the link they offer, you can trust that you got to the *intended* site?  A CA doesn't help here *because it establishes trust in a URL, and the URL is not what I intended to reach!*
                                                        -- Jerry

@_date: 2016-10-02 14:56:27
@_author: Jerry Leichter 
@_subject: [Cryptography] distrusted root CA: WoSign 
I can't avoid trusting the browser vendor, short of writing my own browser.  But why would I want to add a *second* trusted party - in the reported NSA sense of "someone who can break my security" - when I don't have to?
What exactly did the CA add to the process?  The search engine knows that macys.com is the right address for answers to certain queries not because of the certificate that macys.com presented, but because of a large number of signals of what the site is and why it's trustworthy.  (Yes, for all I know, the cert is one of the signals used.  I highly doubt it's the only one.)
Look, I'm not criticizing cert pinning or CT.  They are good partial solutions.  What I'm saying is that the better they become, *the less you need CA's in the first place.*
Lynn Wheeler used to make the same point here in a more general framework (for payment systems and such).  We haven't heard from him in over a year (I hope he and Anne are OK) - ironically, the last message to this group I see from him (May 7, 2015) was on exactly this topic:  "The CA industry had sold the wonders of digital signatures (along with requiring digital certificates) to the financial industry for "safe" financial transactions.... I would also make the point that the relying-party-only certificates were redundant and superfluous ... since the financial institution (relying party) already had the public key on file in the account record. I also made a point that a typical credit card transaction payload size was 60-80 bytes. Appending digital certificates to every transaction would add 6kbyte-12kbytes to every transaction ... a factor of 100 times size bloat (for something that was redundant and superfluous)."
My point is that CA's and the entire CA infrastructure collect a considerable tax from every site on the Web - while adding pretty much nothing significant.  We shouldn't be looking for work-arounds to allow certs to do the job they theoretically are there to do - we should get rid of them entirely and rely on the much better mechanisms we could put into place.  The transition strategy isn't even all that hard:  Simply deploy the new mechanisms in parallel with the old, and once they are in place and have gained sufficient trust, stop relying on CA certificates and let the CA businesses fade to nothing.
                                                        -- Jerry

@_date: 2016-10-03 16:48:22
@_author: Jerry Leichter 
@_subject: [Cryptography] French credit card has time-varying PIN 
Hmm.  RSA developed this kind of thing at least 8 years ago.  They had trouble making the cards reliable enough in real-world use - e.g., being flexed in a wallet in someone's back pocket - and they also couldn't gen up enough interest, so the project died.
What surprise me, though:  In an era of chip-and-pin - which is pretty much universal in Europe by now, except for US tourists - why would you want this?  What's the number printed on the card being used for?  I suppose you could use it for Internet shopping and similar card-not-present transactions - is that what this is really aimed at?
                                                        -- Jerry

@_date: 2016-10-03 19:58:34
@_author: Jerry Leichter 
@_subject: [Cryptography] French credit card has time-varying PIN 
All of mine have them, though it took until some time this summer.  I suspect all the majors have rolled out chip cards - it's to their advantage since they can dump liability on vendors once they have a chip card out there.
Around here, probably 2/3 of the places I regularly use cards at read the chip.  I have yet to see a gas station that takes it, though.  Most of the chip readers are very annoyingly slow - a few have gotten dramatically better, but not most.
On another note ... when I was in Europe a few weeks back, I used my chip card - which I carefully get a pin for.  Pretty much no one asked for the pin - the card readers all spit back a "get signature" to the vendor.
                                                        -- Jerry

@_date: 2016-10-04 23:26:12
@_author: Jerry Leichter 
@_subject: [Cryptography] distrusted root CA: WoSign 
Since Ben has repeated asked "what's the alternative to the existing CA system" - let  me toss out a quick back-of-the-envelope sketch of a possible one.
For backwards compatibility, we'll assume that sites will continue to support the current CA/cert system for some time.  I'll ignore that and discuss only the alternative.
Let's take the "subsequent connection" case first:  I'm connecting to a site I've talked to in the past.  My browser stores a triplet of information for that site:  The URL, a signature verification key, and a signing key generation.  To initiate a connection, I go to the URL and start a DH exchange.  However, I require the site sign its DH message.  It includes the signing generation of the key it's used.  If the signing generation matches and the signature is valid, I proceed with DH.  Otherwise, the connection fails.  There's one built-in recovery:  If the signing generation I received is larger than the one I have, I ask the site for a "roll-forward":  The site sends me a new signature verification key and signing generation, signed with the one I already have.  If I'm satisfied with it (I could do other checks if I wished), I replace my record of the key and generation and try again.
How about the "first connection" case?  Let's take today's overwhelmingly common case:  I've done a web search, and a search engine and I have high confidence that it's found the URL I was intending to locate.  In addition to sending me the URL, it sends me its record of the signature verification key and signing generation.  Note that *it already has these* - it got them when it last connected to the site to scrape it.  It follows the same protocols I do.  How does it get the information the first time it scrapes a site?  Actually, it can pretty much just believe what it gets.  About all it has to do is make sure it's not the subject of a MITM attack - something it had better have ways to check anyway.  It's binding the signature to what it finds at the site - which assuming it's actually talking to the site (no MITM) can't be wrong by its very nature.
Note that the search engine is taking on a new *role*, but fundamentally, it's not taking on any new *responsibilities*.  I trust, today, that the search engine does a really good job of mapping the intent of my search to the appropriate URL.  I have no reasonable way to check a URL I got from a search; and in fact I probably never even see it.  So a cert that tells me that the URL (which I never saw) took me to the site named by that URL is meaningless.
In the new approach, my trust in the search engine is unchanged, except that I now trust that it will return to me the signature verification information that it got from that site.  The search engine isn't promising that that information is valid in any broad sense - it's just passing it along.
Obviously, I can have other sources for signature verification information.  I certainly should have it built into my browser for the search engines I intend to use!  All kinds of techniques applied to certs go over easily - e.g., noticing that different sites are being handed different certs/signing information for what's allegedly the same site.
If you want to trade space for communications, you can store fingerprints of the signature verification keys and ask a site to send you its key as needed.
An obvious extension is to provide client authentication by having the client sign its half of the DH exchange with a signature that the server can check.
                                                        -- Jerry

@_date: 2016-10-09 17:17:09
@_author: Jerry Leichter 
@_subject: [Cryptography] Security Fatigue 
The only thing remarkable here is that anyone thought it remarkable enough to publish.
You could find all the examples you need years ago at any organization that enforces a quarterly password change policy.
Personally, I long ago decided that when faced with a poorly designed security system  it's important to understand first who is really being protected.  If it really is *me*, and I have no alternative but to use the system ... I'll work hard to keep things safe regardless.  If the real victim of a security issue will be the organization that is enforcing the stupid policies ... I'm perfectly happy to game the system to make using it tolerable.
                                                        -- Jerry

@_date: 2016-10-11 11:56:47
@_author: Jerry Leichter 
@_subject: [Cryptography] =?utf-8?q?=22NSA_could_put_undetectable_=E2=80=9Ct?= 
Ars Technicha at "Researchers have devised a way to place undetectable backdoors in the cryptographic keys that protect websites, virtual private networks, and Internet servers. The feat allows hackers to passively decrypt hundreds of millions of encrypted communications as well as cryptographically impersonate key owners."
Basically the researchers describe a way to generate primes for which number sieve is much easier if you know the secret - and there's no way to detect this by looking at the prime.  In the case of 1024 bit D-H primes, the result would be to move cracking into a fairly easy range.  And in the case of most of the widely-used 1024-bit D-H primes, nothing is known about how they were generated.
Original paper at   The paper points out that all the basic work was done by Gordon back in 1992, but his technique wasn't able to hide the "spike" successfully, partly because doing so at the time seemed to require an impractical amount of computation.  The authors were able to expand the attack and use more modern hardware to make the attack go through.
                                                        -- Jerry

@_date: 2016-10-11 21:26:08
@_author: Jerry Leichter 
@_subject: [Cryptography] 
=?utf-8?q?rapdoors=E2=80=9D_in_millions_of_crypto_keys=22?=
No, that's not it at all.  The primes are really prime.  The weakness is much more subtle.  Number field algorithms are in a sense statistical:  You generate a large number of "probes" each of which gives you a tiny bit of information, and when you have enough information you can combine it to factor (or do a DLOG, as the case may be).  The hidden information essentially lets you generate the "probes" much more efficiently - you can get by with about a factor of 800 fewer "probes" against a 1024-bit prime than someone who doesn't know the hidden information.
So with the same equipment, it would have taken someone without access to the hidden information about 70 years.  (If you add in further scaling based on having more CPU's, you can see the origin of the push to get off of 1024-bit primes entirely:  We're at the point where certain organizations can probably mount attacks without any help at all, and soon such attacks will be within the reach of many much smaller organizations.  It is worth pointing out, though, that all number field algorithms have a large extremely parallizable component - the "probes" - followed by a stage for which parallelism doesn't seem to help and extremely large memories are needed.  This has for years been the real bottleneck - but, again, machines are now large enough that 1024-bit primes are vulnerable.)
The issue is that non-randomly-generated primes *may* contain trap doors, and it's impractical to check.  This would be just as true for 2048 bit primes - or *any* primes - though with near-future hardware, even a prime with a trap door is safe (and larger primes are safe for that much longer).
Still ... the real take-away here is:  Don't trust primes generated in non-transparent ways.  You don't know what might be hidden in them.  Almost all the widely used D-H primes were *allegedly* generated using random techniques - but with no details published.  The primes ... just sort of dropped from the sky into the standards.
                                                        -- Jerry

@_date: 2016-10-13 14:18:04
@_author: Jerry Leichter 
@_subject: [Cryptography] Defending against weak/trapdoored keys 
"Might", huh.
The history of attempts to make cryptographic primitives stronger by simply combining them and claiming that the attacker will have to break all is littered with failures.  For example, 2-DES (or similar) falls to a meet-in-the-middle attack; using two cryptographically secure MAC's and checking both isn't much stronger than just using one; and so on.  I would be *very* leery of going this route without a proof.
This particular class of attacks can be eliminated by using a verifiably randomly generated prime.  You can generate one using some kind of ceremony where a number of parties jointly construct the random starting point, publish the result together with their attestation that they were present and the random value was derived using a pre-agreed public procedure based on values published by them and the other participants; and then feeding that random value into another pre-agreed public procedure for generating a prime.  Anyone can then check the results by starting from the attested, published random values and running the algorithms.
Alternatively, you could get a similar effect by pre-publishing an algorithm that will take as input a large number of values that will be published at a pre-agreed time in the future.  For example, you might take the highest temperature recorded on a particular day in the future for a long list of cities, as published by a pre-determined collection of newspaper editions.
I'm a bit leery of the idea of using "some number of digits of pi" because then you get the problem of what the starting point should be.  If I can choose any starting point, I can probably mount an attack.  And there are plenty of mathematical constants in place of pi.  If I allow combinations, the potential for "cooking" the result gets that much higher.  This idea worked fine the first couple of times - when the random values were very early in pi, for example - but you quickly run out of variations that way.  Give the chooser too much freedom "to avoid values that have been used in other algorithms" and you start to give him the freedom to force a very safe-looking but spiked value.
                                                        -- Jerry

@_date: 2016-10-26 07:37:44
@_author: Jerry Leichter 
@_subject: [Cryptography] A PKI without CRLs or OCSP 
How does using a blockchain differ from having a PKI broadcast its entire set of signed public certificates?  Or, equivalently for reasonable efficiency, every delta to its set?  (With a serial number, of course - and the entire delta signed by the CA - so that receivers could detect modifications or missed deltas.)
A blockchain supports agreed-to modifications by anyone (to simplify the semantics). But a PKI has just one sender, broadcasting to many receivers.  You don't need a blockchain for that, just signatures.
A CRL blockchain - on which anyone could mark their own certificate as canceled - might make more sense, but even here it's the wrong semantics.  If I believe my certificate should be invalidated ... that's *my* call and my call alone.  The last thing I want to have to do is get a whole bunch of others on the blockchain to agree with me that it should be invalidated - it's *my* call, not theirs.  My signature alone on the invalidation is sufficient proof that I sent the invalidation and it should be honored.  I want some form of reliable broadcast to ensure that my invalidation has reached all the relevant parties, but that's a much weaker (and cheaper to produce) primitive than a blockchain.
                                                        -- Jerry

@_date: 2016-10-28 18:02:47
@_author: Jerry Leichter 
@_subject: [Cryptography] =?utf-8?b?Ikdvb2dsZSB0ZWFjaGVzIOKAnEFJc+KAnSB0byBp?= 
Summary:  Alice, Bob, and Eve are neural nets; Alice and Bob share a key.  Alice tries to send messages to Bob.  Bob is rewarded for correctly reading the messages; Eve is likewise rewarded for correctly intercepting them; Alice is rewarded when Eve fails (and Bob succeeds, I guess).
In the experiments reported, Alice and Bob turn out to be pretty good at fooling Eve, who proves to be a mediocre cryptanalyst.
And next we'll get bots posting their proposed algorithms to any crypto list that will have them.... :-)
Writeup at  paper at   Reference to an XKCD I hadn't seen before in a comment:                                                          -- Jerry

@_date: 2016-09-01 11:33:16
@_author: Jerry Leichter 
@_subject: [Cryptography] "Flip Feng Shui: Hammering a Needle in the Software 
"We introduce Flip Feng Shui (FFS), a new exploitation vector which allows an attacker to induce bit flips over arbitrary physical memory in a fully controlled way. FFS relies on hardware bugs to induce bit flips over memory and on the ability to surgically control the physical memory layout to corrupt attacker-targeted data anywhere in the software stack.... Memory deduplication allows an attacker to reverse-map any physical page into a virtual page she owns as long as the pages contents are known. Rowhammer, in turn, allows an attacker to flip bits in controlled (initially unknown) locations in the target page.
We show FFS is extremely powerful: a malicious VM in a practical cloud setting can gain unauthorized access to a co-hosted victim VM running OpenSSH. Using FFS, we exemplify end-to-end attacks breaking OpenSSH public-key authentication, and forging GPG signatures from trusted keys, thereby compromising the Ubuntu/Debian update mechanism."
                                                        -- Jerry

@_date: 2016-09-02 10:56:10
@_author: Jerry Leichter 
@_subject: [Cryptography] "Flip Feng Shui: Hammering a Needle in the 
The technique cannot be aimed exactly:  You can flip some unpredictable, uncontrollable subset of the bits in a word.  (The vulnerability of particular bits is dependent on physical variations in the memory cells.)
If you try this against executable code, you stand a good chance of breaking something with a visible effect - a much better chance than of producing a useful code change.  The neat observation in the paper is that RSA moduli are particularly vulnerable:  *Any* bit flip is highly likely to make the modulus easily factorable.  Yes, the change in the modulus will be detectable since messages either won't decrypt or signatures won't verify - but that happens on other systems, not the one under attack, and it may take a while for anyone to notice.  (E.g., if you break an ssh key, no one will notice until the legitimate user tries, and fails, to log in - and even than, many users will just assume the system has been reconfigured and generate a new key.)
Attacks against the executable code are certainly the worst case, and you might be able to find security-sensitive but very rarely executed code to attack.  But this is likely much harder to pull off than the attack outlined here.
BTW, this is yet another in a long, long line of attacks against RSA that are based on its exquisite sensitivity.  Leak the bottom bit of messages? Leak the top log n (or something like that) bits of messages?  Make very rare errors in the arithmetic operations?  Flip a bit in the moduli?  All of these lead to full breaks.
Contrast this with, say, AES.  Flip an unknown bit in the sender's key?  The resulting message is just as secure - in fact, more so:  Now, *no one* can decrypt it!  (Well, if you know the key you can try all keys with close Hamming distances.)  At best, an attacker might be able to generate related key attacks this way.
RSA is like a beautiful piece of blown glass:  Admire it, but treat it with great care.
(One thing I don't recall seeing, BTW, is analyses of the sensitivity of ECC to things like leaked bits.  My guess would be that it's also sensitive, though perhaps less so than RSA.)
Anyway, coming back around:  Yes, this attack does show that hardware that's vulnerable to this attack simply cannot be trusted to run the software you think it's supposed to be running.  An attack would be difficult, but as with some other things, cryptography has the property that it concentrates vulnerabilities.  (Leaking a 1MB document through a slow channel will take a long time; leaking the AES-256 key that protects the "black", encrypted version that anyone can get hold of is pretty quick.)
                                                        -- Jerry

@_date: 2016-09-03 06:18:44
@_author: Jerry Leichter 
@_subject: [Cryptography] "Flip Feng Shui: Hammering a Needle in the 
The paper's description of the attack gets a bit vague at points, but as far as I can tell, it relies on the technique outlined in the earlier Rowhammer paper, which randomly perturbs a word (where the length of a word has to do with the physical organization of memory and *might* be independent of any similar concept visible in the machine ABI, though in common hardware it isn't).  To the degree that the effect is actually truly random, of course, you can check the results and keep trying until you get the desired results - but if your attacking code, that makes the problem of revealing yourself that much worse.
Then again, perhaps I missed something about the way the attack is carried out.  If anyone here understands how one can use the Rowhammer technique to reliably flip a chosen bit, could they explain it?
                                                        -- Jerry

@_date: 2016-09-03 06:51:40
@_author: Jerry Leichter 
@_subject: [Cryptography] "Flip Feng Shui: Hammering a Needle in the 
Thanks, but this is only half of it.
Hardware has been unreliable since we started building it.  Nothing in the physical world can ever be fully reliable.  The challenge of building reliable computation on top of unreliable parts has been with us since the Industrial Revolution.  A big part of the reason that digital computation won out of analogue computation is that you can regenerate bits as they decay.  As long as you do so while random perturbations below a threshold - the point at which 0 and 1 have decayed toward each other enough that you can't be sure which you started with - you can get back the exact signal you started with. The same principle applies in error-correcting codes:  As long as no more than a certain threshold number of bits has gone bad, you can unambiguously decode the original value.
In fact, looked at in the most general possible setting, a digital encoding is *ideally* the choice of two distinct points in some metric space.  But in *practice* the points get smeared into some probability distributions.  As long as the distributions don't overlap at all, you can with certainty recover the original points; and if they do overlap, the size of the overlap gives the probability of getting the decoding wrong.  In practice, we can make that probability low enough that it's unimportant with respect to other factors in the entire system.
Abstractly, the original Rowhammer attacks showed a technique by which the error rate could be artificially raised.  Other such attacks have existed over the years - exposing parts to radiation, modifying supply voltages, raising or lower part temperatures, and so on.  What was really new in Rowhammer was that it was an error-inducing attack via physical means which could be carried out entirely through software, without physical access to the hardware.  (You could argue that fuzzing is a predecessor, though it operates on a different level of abstraction.)  Rowhammer is also much more targeted than most earlier techniques, which can only induce an error somewhere in, at best, a single chip; and sometimes in an entire assembly.
Like all failures, induced or otherwise, Rowhammer is the result of a perturbation outside the range covered by the error protections designed into the hardware.  That it's *induced* doesn't change that fact - though it does confirm that when designing parts, you can't just consider statistical models.  Statistically, a Rowhammer-style attack will never happen.  When your opponent is not statistics but an intelligent entity's manipulation, you need a different analysis - worst case, not average or expected case.  As it happens, not all hardware is actually vulnerable anyway, and now that the attack is known, hardware will be designed to avoid it.  Of course, changing out hardware is a much, much slower process than patching software....
Beyond showing that they could induce faults, the Rowhammer authors also showed actual exploitation by flipping "high value" bits, particularly things like privilege bits.  FFS showed a different exploit vector, one in which the targeting need not be as precise.  Indeed, the idea of breaking RSA by inducing faults in its implementation has been explored in the past.
Cryptographic implementations inherently have a much harder job than most code.  Most code doesn't face an intelligent opponent.  And most algorithms don't exhibit the exquisite sensitivity to small failures that cryptographic code - especially RSA - does.  We've seen plenty of examples at higher levels of abstraction.  "Trivial" errors in protocol design, protocol implementation, encryption design and implementation, have lead to broken systems.  There are reasonable but unprovable arguments that intelligent opponents have deliberately steered the design of protocols and algorithms (particularly ECC) into areas where implementation errors are so hard to avoid as to be inevitable.
The broad question of how to design *the entire stack* to be resistant to failures, from the hardware layer all the way up to the operational layer, is a difficult one. "Eternal vigilance is the price of security".  :-)
                                                        -- Jerry

@_date: 2016-09-09 15:37:26
@_author: Jerry Leichter 
@_subject: [Cryptography] Secure erasure in C. 
FYI, JVM JIT implementations (a) can make optimizations based on whole program analysis; (b) allow late loading of additional classes.  For example, if A has no subclasses, a JVM JIT compiler can generate code based on that assumption - e.g., it can statically resolve references even to non-final methods in A, knowing that no existing class overrides them.
So how can this work?  JIT compilers have a process called "de-optimization", in which later events - like a subclass of A being loaded - can cause generated code to be deleted, falling back to the JVM interpreter.  It may be re-compiled later, using an updated global state.
There are all sorts of interesting issues here - such as de-optimizing functions which are currently active further up the stack.  (Finding all the places in which a method of A was used so that the calling code can be de-optimized may, in general, be too expensive to be practical.  I think the direct call - and thus the subsequent de-optimization - may only be done for calls to members *within the class itself*.)
                                                        -- Jerry

@_date: 2016-09-10 04:46:08
@_author: Jerry Leichter 
@_subject: [Cryptography] Secure erasure 
Let's step back a moment here.  The entire stack, hardware to OS to libraries and compilers and languages, are designed to be (a) general purpose; (b) efficient; (c) usable.  Cryptographic code represents a vanishingly small percentage of either bytes or cycles used on any system.  The system is not and *will not* be designed around its needs.
Back in the early days of DES, the NSA was reportedly against the idea of doing crypto in software *at all*.  Their approach was always to provide a sealed hardware "black box".  They weren't all wrong....
There's an inherent tension between general-purpose hardware and cryptography.  It's not going away.  Diddling with compilers isn't going to help.  Changing OS's isn't going to help:  As fast as the OS declares "I won't cache that page", the hardware introduces some new kind of optimization, or another layer of software gets shoved in under the OS, and wham, the stuff gets copied into some cache no one ever heard of.
In fact, what we're seeing is the migration of crypto into hardware.  AES built into processors; secure enclaves to hold keys and do crypto that even the most privileged software can't even observe, much less modify.  No, this doesn't make the hill of dirt under the rug go away - it just moves it elsewhere.  Now you worry that there's no way to know just what the hardware is doing.
It is worth pointing out - and this and many other discussions here make it clear - that "trusted, audited software" running on non-trustworthy hardware (which is pretty much all the hardware available today) is really no more secure than the same algorithms in dedicated hardware.  No matter what you do ... you have to trust the hardware.  Sure, we as software hackers lose the ability to play around with new algorithms and read all that fun code ... but realistically using the built-in AES instructions on an x64 chip *may well reduce* the attack surface relative to writing AES in software and running it on that same chip.
So:  If we admit that the cause of really secure crypto on general-purpose hardware is lost ... what do we get to?  I suspect it's something in the direction of verified crypto processors embedded in the hardware, with verified isolation from the rest of the chip.  What exactly verification means in the context, and how it might be accomplished, are open research questions.  (It is clear that "simple is better":  The Apple approach of a fairly fixed-function "secure enclave" is more secure than the - already attacked - more general "secure enclaves" that allow code (to implement DRM, for example) to be loaded into them.
                                                        -- Jerry

@_date: 2016-09-10 19:20:52
@_author: Jerry Leichter 
@_subject: [Cryptography] Secure erasure 
Frankly ... I don't see it happening.  The demand is simply not there.  The sophisticated attacks we talk about here are *not* how hacking is done today.  We haven't even seen evidence of the government actors going that far.  There are way too many easier attacks.  If you some how manage to build really secure traditional OS's and eliminate all the easy attacks, people may start doing the obscure stuff.  Until the attackers move there, no one will pay for the defenses.
But at some point the unencrypted inputs and outputs have to enter and leave the system - so in the end we're saying the same things in different words:  Assume the bulk of the system is and will forever remain untrustworthy, and only some secure core element *is* trustworthy.  Design your system so that the trustworthy part has enough "leverage" that it doesn't matter what the untrustworthy part does.  Using various forms of "oblivious computation" - if we can get it efficient enough - is certainly one approach to doing this.
This is hardly a new concept.  We called that core the "reference monitor" (and other similar terms) since the 1960's at least.  Back then, most descriptions assumed that the hardware itself could be trusted to keep the reference monitor isolated - and fully in control of the rest of the software.  Given the hardware of the time, that was probably reasonable.  What I'm arguing is that, given today's extremely complex hardware, you have to partition off the hardware support for the reference monitor, too.
                                                        -- Jerry

@_date: 2016-09-11 07:23:54
@_author: Jerry Leichter 
@_subject: [Cryptography] Secure erasure 
The argument is:  No one will pay for special compiler modes, special code in OS's, special hardware support, all to carefully route around optimizations that may, as a side effect, in special circumstances, cause data to leak - when that's not a leakage path that anyone is likely to actually attack, because much easier ones exist.
A true story on the other side of the argument - make of it what you will.  (I may have told this here before.)
The VAX was the second mainstream machine to be built to a carefully specified, published standard.  (The IBM 360 was the first, by a good long margin, though actually I'm not sure when IBM first published its standard.  In both cases, the reason for a careful standard was the same:  Both companies wanted to create a line of fully compatible computers - the IBM 360 series were the first such line - and without a standard to write to, there was to way to ensure that the different machines really were "the same".)  DEC, at least, really published its standard as a printed book.  (There was an internal-only version, but both versions were produced from the same source with different option settings - and the only thing missing from the public version where (a) historical information about proposals that were abandoned during development; (b) hints to hardware developers about the best way to do some things.)
The VAX architecture had two carefully-defined terms used to describe things that were *not* fully nailed down by the architecture:  "Unspecified" and "undefined".  "Unspecified" referred to the explicit results of an operation, and said that the bit pattern that ended up in them could be anything.  "Undefined" meant the machine could *do* anything - the proverbial "cause bats to fly up your nose".  Only privileged instructions could have "undefined" outcomes.
One day, someone pointed out that "unspecified" did *not* constrain how the output value was derived.  Could it depend on values that were not otherwise accessible to the user-mode caller?  For example, could an unspecified write to a register leave in that register the value some other process had written there?  (In modern terms:  Imagine a design with register renaming, and a unspecified operation in which the output register gets assigned a new renamed register from the pool - but then nothing is written to the register and it retains its previous value, which we'll posit has been left unchanged since the last process changeover.)
Well, this lead to a huge discussion in the VAX architecture and designer community. One of the designers actually spent the following weekend pin-pointing all uses of "unspecified" in the architecture, then checking all extant VAX implementations to see what each one did.  As it turned out, they all actually fell into two groups:  Either the output location was unchanged from what it had been; or the result was always zero.  So all existing and in-design hardware was safe.
The discussion then turned to how to modify the architecture specifications to ensure that all future hardware would be designed safely - without losing the freedom that the "unspecified" marking gave to implementers to do things efficiently.  The eventual conclusion was that it was impossible; we just added an internal note to implementers warning them of this issue.
However ... when the Alpha architecture was written, the writers already knew of this issue.  So they *specified "unspecified"* :-)  They enumerated the machine state that must functionally determine the value written when the result of an operations was "unspecified".
I'm not sure anyone has followed up on this in specifications for still-living architectures.  Perhaps it was done, at least internally, for the x64 architecture:  Many of the guys involved in its design were ex-Alpha designers.  But they had to build on a many-year-old legacy design going back to the 8080, and retrofitting that would probably have been impossible.
                                                        -- Jerry

@_date: 2016-09-11 08:44:34
@_author: Jerry Leichter 
@_subject: [Cryptography] Secure erasure 
...which is exactly the point I'm arguing as well:  The general-purpose machine will be insecure because it's impractically expensive to make it secure, so the right approach is to live with that and create a design where security issues in that portion of the machine are irrelevant to the security of the system as a whole.
Actually, I prefaced it by saying it was an argument in the opposite direction. :-)
However ... both the VAX and the Alpha are long-dead architectures (though I still have a bunch of micro-VAXes in my garage; haven't powered them up in many years).  So ... at one point, architects and implementers actually worried about this stuff.  Do they still?  I have my doubts....
                                                        -- Jerry

@_date: 2016-09-12 20:46:18
@_author: Jerry Leichter 
@_subject: [Cryptography] Secure erasure 
I know of no examples of such a thing, but a widely-deployed DRM system where the protection relied on DES might well be attacked through DES.
                                                        -- Jerry :-)

@_date: 2016-09-13 06:32:20
@_author: Jerry Leichter 
@_subject: [Cryptography] Secure erasure in C. 
I'm not sure what the significance of two different core sizes is.  Processes migrate across cores in all multi-core implementations I'm aware of.  OS's *try* to keep a process on a single core because it improves performance - any per-core caches or other bits of state are likely to retain some information for the process, even if some OS code or even another process has used the code since - but that's certainly no guarantee.  Depending on the details of the chip/cache/other state design, some stuff may need to be flushed while other stuff will be protected by the hardware.  For example, in a simple design with virtually addressed cache, cache lines may have to be flushed to prevent the next process reading data the previous one loaded into the cache and the same virtual address (which of course may correspond to some entirely different physical address).  More sophisticated caches tag lines with a process id, so even if the virtual address matches, the new process will not be given access to the old one's line.  Process tags are probably universal in modern CPU's.
Having two different kinds of cores makes the design of the OS scheduler somewhat more complex, but the resulting reduction in power usage can be worth it.  (For example, Apple - which has complete control over both the hardware and software and measures things in fine detail - hasn't previously used a big/little design, but has done so in the A10 chip that's in the new iPhone 7.)  Perhaps this makes core-to-core migration more common, though it's not clear that it would - choice of big or little core is based on power demand, which is unlikely to change very often relative to the time scales of other events in the OS.  But the cache control issues remain unchanged.
                                                        -- Jerry

@_date: 2016-09-13 15:21:26
@_author: Jerry Leichter 
@_subject: [Cryptography] Secure erasure 
Gee.  We did this.  Not so long ago.  Remember the RISC revolution?  Pretty much everything got re-thought, from the ground up.  Tons of complexity was removed - or, more accurately, removed from the hardware and added to the compilers and operating systems.  Case in point:  Some RISC's got by with no TLB refresh in hardware.  Let the OS worry about flushing old TLB entries and inserting new ones from the page tables it maintained any way it liked.  All that documentation in the hardware manual about the layout and care and feeding of page tables - gone.  That was a pretty radical clean-sheet design.
RISC lost - and kind of won.  Initially, RISC killed off what were considered the classic examples of CISC, the VAX and some imitators (e.g., 68K).  But then for general purpose computation, the x86 and the x64 pretty much killed RISC.  Though if you looked deeper, the say x86/x64 survived was by translating their complex instructions into micro-operations, on the fly, for a somewhat RISC-like underlying but invisible architecture.  Meanwhile ... ARM is kind of RISC'ie - as, for that matter, is PowerPC, which is making a comeback, though PowerPC is also radically different in its own ways.  Did RISC win?  Lose?  Damned if I know.
We used to believe that worrying about micro-optimizations and a few cycles here and there didn't matter, because a year from now, there would be a new chip iteration which would be significantly faster.  Good optimizing compilers gained you, after a big software investment, about what you gained by using two-year's-ago compiler with this year's hardware.  If the advantages were multiplicative - so you got your 50% speedup from the compiler for code running on the new 2x faster hardware - that was great, but it didn't always work out that way, as the compilers often needed to be tuned to the performance quirks of the new hardware.
And then speedups fell off a cliff.  We could shrink the gates allowing for faster clock speeds - but you couldn't cool the damn thing, so you had to put those gates to some other use.  (Now we're reaching other walls:  The wavelength of an electron under typical working conditions is around 5nm, so if you manage to get features that small - not that far off - the question of whether the electron is or is not at the gate becomes rather fuzzy.)
So ... the argument that we'll have cycles to spare just doesn't work any more.  So far, we've managed to find more computational work for our machines to do as fast as we've managed to generate more cycles to do the work.
Which brings me back to my argument all along:  Machines that do general computation will continue to be full of tons of speedups - hacks, if you want to define them that way.  They will not, in the foreseeable future, themselves be secure, because performance demands, not security, will drive the market for them.
So the alternative is to look elsewhere:  Security is a *system* property, just like reliability; so as we build reliable systems from unreliable components, we need to build secure systems out of insecure components.  Though as far as we can tell, there needs to be more of a secure core to bootstrap with than a reliable core.
                                                        -- Jerry

@_date: 2016-09-15 18:48:14
@_author: Jerry Leichter 
@_subject: [Cryptography] Bug in Signal - and what it says about programming 
(which has links to the actual release) describes a few recently-reported bugs in the Android version of Signal.  They actually include the vulnerable line of code:
int remainingData = (int) file.length() - mac.getMacLength();
remainingData will be used to bound the data to be covered by the Mac.  The problem here is that file.length() is a long.  If you have an attachment whose length is greater than will fit in an int, only part of it will be MAC'ed, so an attacker can append stuff to it.
This is Java code.  Had the programmer written:
int remainingData = file.length() - mac.getMacLength();
Java would have issued a truncation warning because 64-bit value was being assigned to a 32-bit value.  But that little cast to int silently discards the high 32 bits of the length; there's now nothing to worry about.
The guys who found the problem actually compliment Signal:  These are the first reported security issues.  Over all, it's held up remarkably well.  But even so, and even in a "safe" language like Java ... there are sharp edges, and it's all too easy to get cut.
Then again:  One wonders how this cast came to be there.  Did the programmer write it without the cast, see the warning, somehow decide it was unjustified, and added the cast to shut the compiler up?  (That should not have been allowed without a careful examination of the affected code.  Code changes to "shut the compiler up" are almost always a bad idea.)  Did the programmer just have some incorrect understanding of constraints on the files involved and thought that, even though the value was stored in a long, it would always fit in an int.  Or - and one can never dismiss such questions - was this a really subtle, hard to find, hole slipped into the software?  Even if a review caught it ... it would be easy enough to explain away, so the risk to some nominal attacker would be small.
                                                        -- Jerry

@_date: 2016-09-17 06:51:52
@_author: Jerry Leichter 
@_subject: [Cryptography] Ada vs Rust vs safer C 
The VAX C compiler for VMS made this distinction.  It wasn't documented or perhaps intended, but apparently just "kind of happened" when the developers fed v[offset] into the array reference code of a universal back end.  It didn't do bounds checking, but it *did* forbid 2[x] - which in C is identical to x[2], a fact that surprises even many experienced C programmers (but brings a smile to the face of old assembler hackers).  I forget exactly where else the assumption showed up; likely, while C allows exp1[exp2] for arbitrary expressions, VAX C would only allow a limited set of exp1's (variable name, another subscripted expression - x[1][2] - or a function call - f(x)[3] - but not (x+5)[3],would be my guess).
In practice VAX C's treatment of array references broke very little code.  As far as I know, it was purely syntactic.  The real problem isn't distinguishing an array reference from an arbitrary pointer dereference; it's defining a semantics for arrays (or all pointers) that would carry along bounds to check.
VAX C did break *other* code, but then this was in the period when the C language was more or less defined by pcc's quirks and there was code around that made all kinds of assumptions about the details of the implementation.  A classic, long forgotten and unlamented, was that the first word in the address space had address 0 and always contained 0.  So you'd see code like:
and then
which would copy a single \0 character into x.  (And now you know why there remain vestiges of the assumption that a char* containing NULL acts like the empty string even in a few places in C++.)
C is deliberately designed these days to *allow* "fat pointers" (carrying bounds information), but other than some compilers specifically designed for debugging/testing purposes - one of the first ones, which I did some very minor work on, was named SaferC or maybe SafeC - this has never seen much uptake in the C community.  (Of course, "everyone knows" that a pointer fits in an int - and if not, certainly in a long.)
Of course, language with real bounds checking don't typically allow arrays to be treated like pointers to begin with, so arrays don't *usually* need to be "fat" in order to let the compiler generate bounds checks.  Now that C since C99 has had variable-length arrays - which always have the size information available - it's possible to have a compiler actually generate proper bounds checking for them.  Forbid any other kind of array and you have your wish.  Unfortunately, (a) variable-length arrays can only be allocated on the stack (an easy thing to fix in the language); (b) few of the standard library calls that deal with arrays - or likely other common packages of libraries - deal with VLA's; (c) many common C practices that rely on letting an array reference decay to a pointer can't work properly with VLA's (though decent compilation analysis might allow many to be accepted).
                                                        -- Jerry

@_date: 2016-09-17 06:57:01
@_author: Jerry Leichter 
@_subject: [Cryptography] Ada vs Rust vs safer C 
There have been many attempts to define such a language.  Sometimes just as a matter of (checked, if possible) conventions about what constructs to allow and what to forbid; sometimes by actually defining a new subset language and developing a compiler for it.  Googling "safe c" returns a plethora of hits.
                                                        -- Jerry

@_date: 2016-09-17 09:16:41
@_author: Jerry Leichter 
@_subject: [Cryptography] True RNG: elementary particle noise sensed with 
Hardware that could do this exists.  In fact, many hundreds of millions of instances of it are out there - perhaps right in your pocket.  Every reasonably recent iPhone or iPad has a "secure enclave" in its CPU which has a secret random value which gets "tangled" with user-entered passwords to produce keys for encryption.  The security of the whole design is predicated on the unpredictability of the secret random value, and the impracticality of extracting it or the generated keys.
There's apparently also a larger-scale version of this in Apple datacenters, used to render inaccessible to Apple secure information synced among customer machines.
The exact external capabilities of the secure enclave have never been documented that I know of.  I don't think we know how many bits the secret random value contains, nor whether you can produce multiple distinct keys, though that seems likely.  A quickie approach to a random number generator would be to have the enclave produce a key from a known string (say, "random"), then use that key to do counter-mode to generate a stream of "random" values.  If you feed the counter in from the outside, this approach has a bad weakness:  The counter values can be replayed to regenerate the "random" sequence.  To do this right, the counter would have to be stored within the secure enclave, inaccessible to the outside world.  We know that at least some iPhones could be attacked because the enclave didn't itself keep track of failed attempts to unlock the phone - the try counter was external and could be reset.  So older versions could probably also not generate a secure random stream.  Most likely the reset problem has been fixed by moving the try counter into the enclave - so the *capability* to do a real random number generator is likely there now.
***Please note that I'm not arguing that Apple generates the secret value in a safe way; that it doesn't save the secret values "just in case"; that they don't have back doors in their chips, maybe even ones they don't know are there.***  I'm not trying to sell anyone on using iPhones this way (assuming it's even possible).  I'm simply pointing to an existence proof:  Such hardware is well within our capabilities to build - Apple's built it - and it has other important uses - important enough to actually build it hundreds of millions of times.
                                                        -- Jerry

@_date: 2016-09-17 20:02:06
@_author: Jerry Leichter 
@_subject: [Cryptography] Ada vs Rust vs safer C 
So ... quick question:  Which one does bounds checking, v[i] or v.at(i)?  Which has the shorter, more natural syntax?  Right there, you get a quick view of the biases in the C/C++ community.
A number of years back, before the C++ library became universally available, I wrote a library with a vector class.  It had v[i] and v.unsafeAt(i).  Now which one do you think elides the bounds checks?
In practice, unsafeAt() was never used.  *I* used it in a hash table implementation, where you could prove that the index into a vector representing a bucket had to be in range.  Except that the proof - wasn't, and the result was a nasty bug - quickly revealed by changing from unsafeAt() back to operator[].  After fixing the bug, I left the operator[] in.  No one ever complained, and the code never even showed up in profiles.
                                                        -- Jerry

@_date: 2016-09-17 20:14:35
@_author: Jerry Leichter 
@_subject: [Cryptography] True RNG: elementary particle noise sensed with 
I have no problem with either approach, implemented correctly.
The area/power tradeoffs are relevant if you are making the choice of *adding* either a TRNG or a CSPRNG.  What I was pointing out is that if you're building a secure encryption facility of the sort Apple has tried to build, you get the CSPRNG for free.  Note that if you want a secure encryption facility of this sort, it will necessarily need to have secure NVM in it.  You're paying for that either way....
                                                        -- Jerry

@_date: 2016-09-18 19:13:34
@_author: Jerry Leichter 
@_subject: [Cryptography] Ada vs Rust vs safer C 
There was a project called ESC (Enhanced Static Checking or something like that) at the DEC labs way back when that did analysis like this - based on annotations - of Modula-3 code.  It later got picked up by a group at MIT would produced a portable version which targeted C, among other languages.  The name, however, is something I can no longer recall.
This was based on a very expressive annotation language and a fairly powerful proof engine, so with proper annotations you could verify some quite meaningful properties of C (or other language) programs.  Unfortunately, the annotation language was highly mathematical in form, well beyond the comfort zone (or even competence) of the vast majority of programmers.  So the effort never had any significant real-world impact.
This is the side of annotations and program verification systems that's proved a stumbling block pretty much forever.  No one has really solved the problem of working either with un-annotated code in languages that are in actual use; or designing an annotation system that real-world programmers can use effectively.
And that doesn't even get to the problem of existing code.  If you design this stuff in from the beginning, it can work fairly well.  In fact, the MIT system could probably be workable in a project that started off with someone with the appropriate background defining a bunch of assertions appropriate to the project at hand, providing the developers with what would effectively be an annotation DSL that they could use without needing to understand all the details.
But trying to retrofit any reasonable level of annotation and you're likely to end up throwing your hands up in despair.  A classic example is const-correctness in C++.  If done consistently, up front, it's a *big* help in pinning down the semantics of your code - in a compiler-checkable way - and avoiding all kinds of bugs.  But anyone who's tried to add const declarations to an existing code base of any size knows how that goes:  It's an all-or-nothing effort.  You start at the bottom, add some const's, find that the methods that call your methods now fail to compile, fix those ... and repeat until you've walked through a substantial portion of the code, or have given it up as a lost cause.
I've recently been repeating this experience in Java.  Java now has a way to annotate a variable or parameter or return value - and even, since Java 8, things like the values in a map - as non-null; and even the standard compilers will tell you about violations.  A *fantastic* way to avoid the dreaded NPE's of too much Java code.  But like const-correctness, non-null annotations spread.  You annotate the parameters of some method, then have to fix up all the callers.  The job can quickly become overwhelmingly large.  Since the compiler messages for violations are typically warnings, what you tend to find is that someone changed a subtree of the call graph and gave up, leaving the warnings from callers into that subtree alone.  So now you have the worst of both worlds:  Looking at subtree, it appears that it's "NPE safe" - the  annotations guarantee it, no?  But in fact the warnings at the points of call mean that the annotations can't actually be trusted to mean anything.  (The compiler doesn't insert checks at transitions from unannotated to annotated code.  That might be an interesting addition.)
The fact that no one has annotated the Java library - nor is anyone likely to - doesn't help the cause.
There are actually some fairly general kinds of program assertion annotations available now for Java, but other then  none are yet checked by any compiler.  There are some generic checking frameworks available, but they don't seem to have gotten much traction.  The Java community, in general, seems to have little interest in compile-time checking - "Just write a complete set of unit tests".  (Hmm, I wonder if there would be more support if someone developed an analyzer that generated unit test to cover the annotations in a program.  TDD via annotations....)
                                                        -- Jerry

@_date: 2016-09-20 06:51:43
@_author: Jerry Leichter 
@_subject: [Cryptography] Ada vs Rust vs safer C 
Yup.  That was it exactly.
This is a criticism I've made myself.  While valid in some cases, I think one can overstate it.  An annotation system - or even a specification system like Z - inherently provides some redundant description of the code.  The questions to ask are:  How much of the code has to be redundantly described?  Is the redundant description simpler, significantly shorter, easier to understand and reason about than the original code?
Consider that type systems are themselves to a large degree redundant descriptions of the code - even in a language with so simple a type system as C.  Yes, some of the type information goes into memory allocation and disambiguation of overloaded operations - whether "+" is integer or floating addition.  But some of it goes into making the program easier to understand.  Consider that very early versions of C allowed you to write I.x where I was an integer variable and there happened to be a struct {float x; float y;} in scope.  The meaning was:  Implicitly cast I to a pointer to such a struct and pull out the x reference.  This would only be an error if there were two struct's in scope with x fields whose types or offsets were different.  If this seems odd ... keep in mind that C struct was inspired by an assembler concept of a (dummy) section, known as a DSECT at least in IBM360 assembler parlance.  All you got from the assembler construct was an offset, represented a bit more cleanly than a bunch of constant declarations.
In any case, the preference for static over dynamic typing *in program correctness contexts* is precisely that the programmer has to specify the assumed properties of certain names.  Yes, dynamically typed languages check this at run-time - but understanding what they are doing is harder.  Yes, there are now pretty powerful type derivation systems out there that let you elide the types in many cases.  But even in languages with such derivation systems, people often add "unnecessary" types anyway - because a human being wants to know what the code is supposed to mean without having to first derive all the types.
In a language like C, the expressivity of types is rather limited.  Much of the type system is based on primitive machine types, some of whose semantics aren't even fully specified (2's complement arithmetic in the presence of overflows).  But even there, well-written C code will use a variety of struct types and pointers to them to make the intended semantics of objects clearer.  In C++, which tightens down the free and easy conversion of pointer types into each other, this can be used to even better effect.  Same for enum's versus a set of  of integer constants.
One can take this further - and languages have.  Consider subrange types in Ada, for example.  How far is too far is difficult to pin down.  You know when you've gotten there.  You also know when the language you've chosen is inappropriate to its intended purpose:  Making the code easier to understand.  The original Larch work was probably reasonable, at least within the rather sophisticated community in which it was developed.  But then the curse of generalization set in, and what were a few clear pre-defined concepts became a whole bunch of complex mathematics.
I'll have to have a look at it.
                                                        -- Jerry

@_date: 2016-09-20 12:29:01
@_author: Jerry Leichter 
@_subject: [Cryptography] Ada vs Rust vs safer C 
Actually Standard C long ago settled on "binary and either one's or two's complement" for signed integral types; it may by now say "two's complement".  But that isn't the issue!  The Standard works just fine *as long as you don't exceed the bounds of the integral type*, whether two's complement or otherwise.  As soon as you do, the results are undefined.  Unfortunately, even testing at run time whether the result of an arithmetic operation will be undefined inherently gets you into undefined behavior.
I've never understood the reluctance of the C Standards guys to pin this down further.  They agreed long ago that *unsigned* arithmetic was purely mod 2^n.  And way, way back, it may have made sense to be inclusive and allow C to be used on machines with non-binary arithmetic (IBM 1620 decimal arithmetic), or with odd-ball binary arithmetic (sign and magnitude, anyone?)  But the fact is no one has built machines like that in many years, and no one is likely to - and if they do, it'll be because they have an architecture that's so different C won't work on it anyway.
At this point, it's all two's complement, and overflows either produce an easily predictable result, or trap.  C could move this out of "undefined" to "one of the following small sets of possibilities; you can check which using the following macros".  And many of the issues would go away.
BTW, the relationship between "undefined semantics" and "dangerous semantics" isn't simple!  Unsigned integral values have fully specified semantics in C - but unsigned integral values *as loop indices* are dangerously easy to get wrong!  (Been there, done that.)  In fact, Google's standards for C and C++ forbid the use of unsigned integers except in very limited circumstances (e.g., bit operations).  Back when int was 16 bits, the ability to represent larger numbers in unsigned int was important.  But that need faded years ago.
                                                        -- Jerry

@_date: 2016-09-21 06:19:03
@_author: Jerry Leichter 
@_subject: [Cryptography] Secure erasure 
With the VAX, there were two different books:  There was an assembler programming manual that told you how to write assembler code, with the entire instruction set called out and published as a looseleaf binder along with all the other manuals; and there was a VAX Standard, which described the detailed operation of the whole machine and was published as a hardcover book.
Now that I think about it, the 360 Principles of Operation were more like the Standard - and there was also a separate Assembler book.  Then again, Assembler books of that era needed to describe pseudo-operations - e.g., the 360's USING - and rather complex macro processors.
Your comment about improving any section someone found hard to understand is, to this day, how I do code reviews:  I repeatedly tell people that if, in a code review, I comment that I don't understand something, the response I want is *not* for them to come tell me how it works, or even add an explanation to the review; it's to add to or improve the comments *in the code* so that the next reader won't have a problem.
Going full old-fogie here:  The manuals produced by the larger, better companies in those days were extraordinary.  The Principles of Operation really did describe, in clear, well-written English, all the details of a 360.  IBM language programming manuals were excellent, as were DEC's.  There was a DEC RT-11 manual that I used to recommend to people as one the best operating system theory introductions out there.  People would joke about taking up a whole orange (later gray) wall with VMS documentation - but everything was there, fairly easy to find, easy to read and understand.
All this is gone,  When the hardware costs millions, users are willing to pay hundreds for a set of manuals, and you can afford to pay technical writers to create and maintain that set to the highest standards.  When the hardware costs hundreds, no one can afford to put in the effort.  This was a phenomenon that first became noticeable (to me at least) in early MS/DOS days.  I wanted to learn MS/DOS the way I'd learned many other OS's.  But ... Microsoft didn't publish programming guides.  Hell, you needed to know the BIOS calls, too - and no manufacturer published those
either.  Instead, outside writers developed books of varying quality, always somewhat out of date.
Microsoft, over the years, has actually gotten much better - though the ability to put everything on-line, readily searchable, has suppressed the demand for a well thought through, organized, broad presentation.  Those are hard to find for any system these days.
DEC never did for CPU's - the Alpha standard was public - but went down that path with the BI bus.
                                                        -- Jerry

@_date: 2016-09-21 15:50:58
@_author: Jerry Leichter 
@_subject: [Cryptography] Ada vs Rust vs safer C 
This is widely repeated, but generally false.
The fraction of people using most OSS today who could, even in principle, fix a bug and rebuild the code is tiny.  And even then ... if the upstream team won't accept the fix, they've now put themselves on a branch, which they will own indefinitely.  A bad place to be, as they will be stuck with merging their change with newer version of the source.  Sometimes, this starts out easy; pretty much always, it eventually becomes impractical.
Branching an active open source project is something that should be taken up only with good reason and great caution.
                                                        -- Jerry

@_date: 2016-09-23 12:39:45
@_author: Jerry Leichter 
@_subject: [Cryptography] Spooky quantum radar at a distance 
There is an important concept in quantum mechanics called "weak measurements"   The original presentation of these was the following problem:  Inside a box there may - or may not - be a nuclear bomb.  The bomb is attached to a trigger that will go off if the bomb absorbs even a single photon.  You want to determine if there's a bomb in the box, but obviously you would rather not set it off if it's there.  Can you do so?
Bizarrely enough, the answer is yes - mainly.  You can trade off your chance of accidentally triggering the bomb against the chance that the answer you get to your question is wrong (beyond the classical extremes of:  Don't test, never cause an explosion, give an arbitrary answer and be right half the time; and always test, give an always-correct answer the half of the times the bomb isn't there, blow up the other half.)
The article seems to hint that they are doing something of this general sort, but it's really impossible to tell.  It's full of all kinds of buzzwords and hints, but what exactly it's doing you can't determine.
Stealth aircraft have extremely small radar scattering cross-section:  If you aim a radar at them, almost none of the energy gets reflect back to the sender, so it can't "see" the plane.  There's some odd language in there that kind of hints that you create pairs of correlated particles, send one toward the plane, and then learn about what happens to it based on what happens to its stay-at-home pair.  This is based first off on a misunderstanding (in the press article) about how "spooky action at a distance" actually works.  It never actually transfers information - QM doesn't violate relativity theory, which says information can't be transferred faster than the speed of light. The "spooky action" is really spooky - what gets transferred is correlations between the two ends.  Yes, a photon that makes it to the stealth airplane interacts with it *somehow*.  Traditional radar learns something if the interaction reflects the photon back towards the source.  If it's reflected off in some other direction (what the shape of a stealth aircraft is all about) or absorbed (what the coatings do), radar can't distinguish that from "the photon flies by the plane and flies into space".  The article would seem to hint that somehow measuring the stay-at-home photon gives you information about the difference among "bounces in another direction", "is absorbed", and "flies into space".  With exactly that description, it sounds impossible; correlation between the two photons is *not* communications.  Add something else - perhaps a bunch of sensors with a broad view of the entire volume of space in which the plane might be, and combine that with the "stay at home" photon (similar to what happens when you use the reference beam to reconstruct a hologram) ... that at least sounds physically plausible.  How it might work ... who knows.
Based just on this article, I'd say bullshit.  If there's really something there, we'll hear more about it.  Stealth technology (which, ironically, Americans developed based on a paper by a Russian mathematician, whose work wasn't recognized in his own country, possibly because the computation required were well beyond their capabilities at the time) probably stayed secret longer than pretty much any advance based on physical principles; but even it came out in the end.  If there's something here ... it'll come out, too.
                                                        -- Jerry

@_date: 2016-09-25 10:04:53
@_author: Jerry Leichter 
@_subject: [Cryptography] Spooky quantum radar at a distance 
And even then, the days of visual dogfights and machine guns are way behind us.  You're probably shooting at the other guy from over the horizon using radar-guided munitions.
There are and always will be plenty of ways to *detect* the presence of aircraft.  During the Vietnam war, the North Vietnamese dug pits designed to resonate around the sound frequencies produced by incoming aircraft.  "Spotters" were stationed in the pits and sounded the alarm.  With enough pits they could get a pretty good idea of the general track being flown.  Good enough to alert potential targets; nowhere near good enough to get a shot at a plane.
As you say, perfection isn't the goal - it's getting in, doing the job (to be really euphemistic about it), and getting out without getting shot down.  Anything that delays or blurs your opponent's reaction is good enough.
Perfect undetectability is needed only if your goals are political (hiding what was done and by whom).  And even then, plausible deniability is likely good enough.
                                                        -- Jerry

@_date: 2016-09-27 11:48:42
@_author: Jerry Leichter 
@_subject: [Cryptography] Use Linux for its security 
"Critical and high-severity security bugs in the upstream kernel have lifespans from 3.3 to 6.4 years between commit and discovery."
                                                      -- Jerry

@_date: 2016-09-28 12:39:15
@_author: Jerry Leichter 
@_subject: [Cryptography] Use Linux for its security 
It's worth reading the talks and articles linked to from the article I referred to
(  The fundamental criticism is that Linux is way behind the times:  It's still trying to squish one security bug at a time, rather than using more modern techniques that close off entire classes of attacks, even if no specific ones have been identified; or like ASLR that make exploits much more difficult even if attacks are found.  None of these is perfect, but they raise the bar.  And ... Linus has explicitly rejected them, because they cost you raw performance.
There are people I trust who say that Microsoft and Windows today - not the Microsoft and Windows of many years back - is at the leading edge of software and OS security.  While it's not a choice for anything other than Apple products, I'd trust an iOS-based "iOT" device over one based on Linux.
                                                        -- Jerry

@_date: 2017-04-02 00:47:15
@_author: Jerry Leichter 
@_subject: [Cryptography] Removal of spaces in NIST Draft SP-800-63B 
For a product I worked on years back, we set the initial password for the admin account to something along the lines of cHaNGeMe.
Some customers ... didn't.
                                                        -- Jerry

@_date: 2017-04-04 18:54:57
@_author: Jerry Leichter 
@_subject: [Cryptography] Regulations of Tempest protections of buildings 
The obvious solution would be a pair of transceivers, one inside the Faraday cage, one outside, connected to each other using a fiber optic link.  Perhaps challenging if you want shielding all the way up to the Terahertz range, but I'm sure it could be done.
There are tons of working spaces that are effectively Faraday cages, even if that's not what they were built for.  Things like simple warehouses build of sheet metal come to mind.  Repeaters are required to get any decent signal into and out of such spaces, and at the least need to connect to an external antenna.  Adding an amp on the antenna is common.  Using fiber instead requires more conversions but is hardly a big deal.
                                                        -- Jerry

@_date: 2017-04-04 19:00:32
@_author: Jerry Leichter 
@_subject: [Cryptography] Regulations of Tempest protections of buildings 
You're thinking of Carl Malamud ... who as it happens just *lost* a case in US District Court against Georgia, whose full official laws, in hardcopy, were only available to him from LexisNexis for $1,207.02.  See There are many state laws a regulations that "include by reference" standards published by various organizations who sell them for large amounts of money.  Yes, you are legally responsible for working within the standards.
                                                        -- Jerry

@_date: 2017-04-04 19:11:45
@_author: Jerry Leichter 
@_subject: [Cryptography] Tempest and limits on receiving 
There is not, to the best of my knowledge, any law preventing you from listening to any frequency you like.  (You aren't allowed to repeat anything you heard, or even acknowledge that you heard it, except if you heard it on bands specifically open to public/shared listening - broadcast bands, ham and CB radio bands, probably WiFi though there have been some arguments about what is permitted here and what amounts to "computer hacking".)
The limitations on receivers are enforced by a hack in the law:  Modern receivers are all likely to use a superheterodyne design, so are inherently transmitters as well.  To ensure they don't interfere with anyone else, they need to be approved by the FCC.  The FCC has written into its rules that receivers sold to consumers (or however they define the class) will not be approved unless they are hard-wired to block certain frequency ranges - particularly cell-phone and police bands.
(Digital radios - based on an A/D converter and signal processing in the digital domain - probably escape these regulations.  But they are not widely available in the mass markets the FCC cares about.)
                                                        -- Jerry

@_date: 2017-04-05 05:33:20
@_author: Jerry Leichter 
@_subject: [Cryptography] Regulations of Tempest protections of buildings 
Hey, I'm not apologizing for or attempting to justify anything.  I'm reporting on today's reality.  (I know, "reality-based" seems to be a concept under challenge right now.)
Feel free to decide for yourself just what the law actually *is* as the State provides you with free meal and board for violating what *it* has decided it is.
                                                        -- Jerry

@_date: 2017-04-05 15:36:00
@_author: Jerry Leichter 
@_subject: [Cryptography] Regulations of Tempest protections of buildings 
Those in the relevant field (electrical engineering) would not interpret the word "interfere" to include Faraday cages.  (They also wouldn't include something like turning off power to an active transmitter.)
Could some enterprising lawyer try to make the case that "interfere" encompasses this kind of activity?  Sure - we've seen enterprising lawyers do much worse.  Generally they don't succeed - courts are not that stupid.
I would accept your claims if you could point to any case where the FCC went after someone for building a Faraday cage.
BTW, the closest cases on point might involve buildings that are deliberately configured to block cellphone signals.  You can, in fact, buy conductive paint and wallpaper for this purpose.  On the other hand, devices that actively transmit in the cell phone bands in order to block cell phone usage are definitely illegal, and the FCC has gone after those who sell and use such devices.
Law is not about theory.  Law is all practical application.  Looking at the exact wording of regulations won't teach you much.
                                                        -- Jerry

@_date: 2017-04-26 19:49:45
@_author: Jerry Leichter 
@_subject: [Cryptography] 
=?utf-8?q?_AES_Counter_Mode=E2=80=A6?=
Counter mode is a stream cipher.  It relies on the assumption that the value of AES(K,n) is unpredictable given the values of AES(K,n-1), AES(K,n-2), and so on, down through any number m of previous values AES(K,n-m).  (In practice, of course, there has to be *some* bound on m - if you let it be 2^128, you've given out all the values of AES(K,x) for all x!  In practice, m=2^32 is certainly safe; probably m=2^64 is, too, but I don't know what the current recommendations are.)
Note that nothing here depends on the "first" value - the IV - which would be n-m.  It really makes no difference what you use.  Always starting at 0 is as good as anything else.
So why this?  Remember, counter mode is a stream cipher - so also depends on another essential property:  Key uniqueness.  If you ever encrypt two messages with the same key (and the same IV), you leak information about both messages.  That *should* never happen ... keys in this mode *must* be unique.  But it's probably a worthwhile protection against bugs/operational problems to protect against it.  If you choose your IV in the recommended way, even if a key gets reused, you're protected, as the sequences generated will be independent of each other.  In fact, if you always rekey after no more than 2^64 bytes, even using just a single key, you won't see a duplicate of both K and n.  (Well, of course, you could always generate the same 64 bit top bits - something that approaches a 50% chance after 2^32 tries.)
*Something* has to go in the bottom 64 bits.  It might as well be constant (to ensure you get a full "cycle" of 2^64 values before you can increment from a value generated with one initial top 64 bits to another one with a different top 64 bits), but it makes no difference at all what you use.  1 is as good as anything else.
Assuming that K is properly chosen (so that it never repeats), this adds nothing to the security.  What it does for the repeated K case is complicated to compute (at least for me, right now).  But it's certainly no better than the recommended technique.
Why do you think that matters?  All values are equivalent.  The encryption doesn't do arithmetic on this value.  In fact, one of the advantages of the recommended technique is that you can generate your 2^64 values using 64-bit arithmetic, as there will be no carries out into the top 64 bits.  When the value for the bottom 64 bits wraps back to 0, you've emitted 2^64 blocks and should re-key.
The entropy of the IV is irrelevant if K is properly chosen, and not the right measure if K is *not* properly chosen.
                                                        -- Jerry

@_date: 2017-04-30 18:07:18
@_author: Jerry Leichter 
@_subject: [Cryptography] 
=?utf-8?q?_AES_Counter_Mode=E2=80=A6?=
Two reasons come to mind.  First, key setup is fairly expensive.  Changing the key every block is generally not a good way to go - though if you get get enough security, it might be worth it.  But ... second, you're deliberately creating a situation in which successive blocks are encrypted by closely related keys.  This can be hazardous, as related key attacks are known (though how significant they are against typical uses of AES is another question).
                                                        -- Jerry

@_date: 2017-04-30 18:38:41
@_author: Jerry Leichter 
@_subject: [Cryptography] 
=?utf-8?q?_AES_Counter_Mode=E2=80=A6?=
On the other hand, it's widely understood today that encryption without authentication is hazardous *anyway*, no matter how you do it.  Once you assume you have to use authentication, XOR attacks become uninteresting.
The unavoidable hazard with stream ciphers is that if you repeat a key, you immediately leak information about the XOR of the two messages.
I find the general comments here about counter mode an interesting change in attitude.  In the past (a couple of years back), I had several participants on this list who I respect strongly *favor* counter mode as very simple, having very simple proof of security, easy to implement, etc., etc.  At one point, I suggested using a stronger combiner than XOR in order to protect against XOR attacks.  The general response was as I responded above:  It's pointless, you have to authenticate anyway.
What I think has changed over the years is a greater appreciation of the need for mechanisms that are secure against the inevitable operational and implementation errors. That's the main concern with stream ciphers; they do have the advantage of very simple implementations, which are easy to get right.  Of course, once you add authentication, you lose some of that.
There are some interesting other advantages to counter mode.  Suppose you don't entirely trust your (hardware) AES implementation - you think it might leak information about the plaintext being encrypted.  In counter mode, that's harmless - what's being encrypted is known to the opponent anyway.  Looking at this a bit more broadly, the "red" data never has to go near the AES implementation.  (Of course, the key does, so this reasoning only goes so far.  But you can make things much harder on the attacker by generating key streams all the time, and picking later which to use for what.  So the attacker may get a key, but can't tell what it applies to.)
I also wonder if the terms "stream" and "block" encryption are too limiting.  If you define a stream mode to encrypt bits to bits independently of each other, then the only possible combiner is XOR (or XOR with complement, which changes nothing).  We then jump from individual bits to (currently) 128-bit blocks mapped to 128-bit blocks, again independently of each other.  We then invent modes to get around the data leaked by ECB for block mode - but I'm not aware of much work to define modes that do something similar for stream encryption.  Naturally, you'd have to first lose the "bit independence" - but block modes lose the block independence anyway.
For example, you can map pairs of key and plaintext bits to pairs of ciphertext bits.  You now have a couple of choices of invertible mappings - I'm too lazy to enumerate them - and you can avoid the XOR attack.  Of course, there are other two-bit transformations that produce predictable outputs, so this doesn't gain much.  You need to go to more bits - and have the map depend on the key.  The result might or might not be secure - careful analysis needed - but it doesn't fit into either the stream or the block characterization.
                                                        -- Jerry

@_date: 2017-08-14 06:04:52
@_author: Jerry Leichter 
@_subject: [Cryptography] NIST SP 800-63-3 
Two comments on this:
What happens after the 100 failed attempts?  As has been pointed out many times in the past, locking out the account gives an attacker a simple denial of service attack.
I've always liked the VMS approach - dating back to the 1980's:  After some threshold of incorrect attempts within a defined period (say, 5 within 2 minutes) is reached, the account is put into "evasion mode" for a random period of time between (say, 2 to 10 minutes).  In evasion mode, *all* passwords are rejected.  As long as attempts to log in continue, evasion mode is extended.  Once they stop, evasion mode times out and the correct password will work again.  Yes, denial of service is possible - but the attack has to continue indefinitely.
This has a downside:  It may make it more difficult to recover from a lost password.
A system I work on stores passwords suitably one-way hashed.  The way the system is structured, there's relatively little reason for the administrator to have to log in - though it does happen now and then.  Our support guys receive a significant number of calls from admins who's forgotten their administrator password.  Assuming they haven't also forgotten the root password on the box :-), we can get them back in operation by inserting a known hashed password/salt value into the password file.  Were we to also encrypt using a secret key, this would be much harder.
Yes, we already have a secret key for other purposes; and, yes, we could presumably write some code to generate a new admin password from it.  FYI, we use the secret key for a related, but different, purpose:  As part of encrypting customer information that we don't want to see in diagnostic uploads.  Our diagnostic procedures never look at the area where the secret is kept, so even though we get encrypted copies of some of their sensitive information, we can't decrypt it.
                                                        -- Jerry

@_date: 2017-08-14 06:08:09
@_author: Jerry Leichter 
@_subject: [Cryptography] National Navajo Code Talkers Day 
One wonders why Navaho, at the time an isolated language spoken by a group of people living in a desert hundreds of miles from the nearest ocean, would have a word for "shark".  :-)
                                                        -- Jerry

@_date: 2017-08-14 20:07:17
@_author: Jerry Leichter 
@_subject: [Cryptography] NIST SP 800-63-3 
This discussion has wandered off into HSM's and their pluses and minuses which is all beside the point. It's a sad fact that user databases, encrypted/salted passwords and all, are regularly stolen. Typically these are actually relational databases directly accesses by web software that provides direct customer services. This leads to large attack surfaces and leaves room for tons of vulnerabilities. A secondary secret password, properly implemented, can be stored in a small file somewhere or even embedded in code. It should be completely separate from anything user facing - ideally on another machine. It has a simple interface: encrypt or decrypt this value. An attacker would have to use very different techniques to steal this - but without it offline attacks on the stolen user file are impossible.  That second machine could be an HSM though it would be difficult to justify getting one just for this purpose. Every password hashing mechanism we devised since Unix introduced the idea some 40 years ago until very recently has fallen to better and better attacks. We believe we're ok now ... but then we believed that last year and 5 and 10 years ago, too. Adding a local secret to the mix is cheap and easy insurance. Why not use it?
                                          -- Jerry

@_date: 2017-08-15 06:03:05
@_author: Jerry Leichter 
@_subject: [Cryptography] NIST SP 800-63-3 
I'm not saying not to do it ... just pointing out that it has a downside.  A great deal depends on the details of the system and its usage.
Another alternative to help keep such databases safe (tongue in cheek here ... but it would be very effective):  Require that the company CIO, CEO, and all members of the board of directors post encrypted copies of all their financial account information, including passwords, to a public site - encrypted with a password stored in the same database as all the user passwords, using the same technology.  'Twood focus the minds on what's important.... :-)
                                                        -- Jerry

@_date: 2017-08-17 15:43:01
@_author: Jerry Leichter 
@_subject: [Cryptography] [FORGED]  NIST SP 800-63-3 
I find this one of those principles that contradicts normal user experience.
In most contexts, spaces have little semantic significance.  Certainly, one space versus multiple spaces has virtually none.  And what about TAB?  In traditional contexts, there's no difference in appearance - or semantics - between TAB and some number of spaces.  Users would be completely unaware of the difference.  In others (e.g., browsers) TAB has an entirely different semantics ("move to next field" and can't even be entered into the current field).  So if you're going to go with the "evaluate as stream of actual bytes" principle, what if I create a password in a context where TAB is accepted, then have to enter it in a context where it isn't?  (For that matter, you could apply the same argument to RETURN and other "format effectors", to use the official ASCII name for these characters.)
If you allow Unicode characters, there are many different whitespace characters - half spaces, for example.  What do you want to do with those?
Passwords are not just strings manipulated by computers.  They are user interface elements.  The fact that upper and lower case are distinct is somewhat unnatural to people, but they've gotten used to it (though exactly how this is handled in languages with more complex systems - multiple cases, multiple logically representations of the same character, etc., I don't know - I've always wondered, though).
The theoretical loss of key space produced by trimming leading and trailing whitespace and converting all internal whitespace to single space characters - is, in the end, completely trivial compared to the increase in user acceptability.
Never forget that the system is there for its users, not the other way around!
                                                        -- Jerry

@_date: 2017-08-27 08:14:00
@_author: Jerry Leichter 
@_subject: [Cryptography] How to find hidden/undocumented instructions 
It's amusing to see a very, very old technique recycled for a new purpose.
The "split page" technique figures out the length of a variable-length instruction by aligning it so that, initially, the first byte is on page P, and the next is on page P+1.  Page P is mapped; paged P+1 is unmapped (or mapped with no execute access).  If you get a page fault, you know the process read past the first byte in decoding the instruction; in that case, you slide the instruction over so that the first two bytes are on P and try again.  Otherwise, something other than a page fault occurs - in particular, the "debug fault after every instruction" mode is enabled so the processor will fault after a single full instruction.  (There are additional details - some of which are only noted as bugs found i.e., the process specs say the page fault must occur if any bytes of the instruction can't be read, but in some cases on some processors, you get some other fault first.)
Now for some prehistory:  Either TOPS-10 or TOPS-20 (or its original version, Tenex) - all operating system for the DEC PDP-10 back in the late 60's/early 70's - was broken into by this technique.  There was an OS call that would validate a username and password.  The system pre-dated the famous Unix paper that introduced the notion of one-way hashed passwords, so it compared the offered password with the stored password directly.  It did the comparison of the values byte by byte, using the offered password in its original memory location, and returning immediately a failure.  So you could align the test password with the first byte on page P and the next on P+1.  You would get a page fault exactly when the first byte was correct.  This turned an exponential search into a linear one.
Those who remember the PDP-10 will realize that this is a description in modern terms.  The PDP-10 was a word-oriented machine - 36 bit words, usually packing 6 6-bit per word.  There were built-in character manipulation functions that would do things like compare strings character by character.  These were very general - strings could have arbitrary lengths and start part way into a word.  (You could also specify the character size - the hardware knew, for example, how to skip the unused bit if you packed 5 7-bit ASCII characters in a word.)  If a page fault occurred part way through, information about the position that had been reached was saved so the instruction could continue from where it left off after the page fault was resolved.
The final piece of the puzzle:  Whichever OS it was allowed the user to provide a page-replacement algorithm.  When a page fault occurred, the OS would call back to user mode with the full context of the fault, and the user code could decide what to do next.  (Actual assignment of physical pages and manipulation of the virtual-to-physical mapping was still up to the OS, of course.)  So a page fault handler could see exactly how far into a string the comparison had gotten.
There were other hacks in that era based on the vulnerability of accessing parameters to system calls directly from user memory (e.g., modifying the value from some kind of parallel thread after the OS had validated it but before it used it).  Eventually we learned to immediately copy parameters into system space, and validate and use them from there.
                                                        -- Jerry

@_date: 2017-12-01 15:18:09
@_author: Jerry Leichter 
@_subject: [Cryptography] Intel Management Engine pwnd 
Where did you get an iPad with no power switch?  Every one I've ever seen has one on the top - you hold it until the Slide To Power down display appears, then slide.
(Granted this isn't something people do regularly.)
                                                        -- Jerry :-)

@_date: 2017-12-02 15:09:36
@_author: Jerry Leichter 
@_subject: [Cryptography] Cryptocurrency: CME Approved, Coin Paychecks, FED, 
Why should the two be comparable?
Even if you think cryptocurrencies are equivalent to gold, why do you choose just Bitcoin to match gold's value?  Why not the aggregate value of all the various coins out there?
I'm trying to think of a way to short Bitcoin....
                                                        -- Jerry

@_date: 2017-12-15 15:54:53
@_author: Jerry Leichter 
@_subject: [Cryptography] High volume thermal entropy from an iPhone 
There's another thing to consider:  Even assuming NSA-level resources, it would be impossible to rig every potential source of randomness out there.  And suppose you actually put this out there, and thousands of random hackers adopted it.  Would that be important enough for NSA to go to the trouble of forcing Apple to rig everyone's phones just to get to those?  And - unless they could accomplish that with just a software update - it wouldn't work against any pre-existing phones *anyway*.
Now, if it were known that someone like the North Korean military liked your design and started to use it ... everything would change.
Things like that have to go into a reasonable risk analysis.
We often remark here that you should rely on proven code from experts, not do your own crypto code.  That's true with respect to some kinds of attack scenarios, not so simple with respect to others.  If you're a small target, making sure that attacks against you are significantly distinct from attacks against the big targets is probably not a bad idea.
For example:  Suppose you are suspicious that the NSA has an attack on AES - and you're a little guy who doesn't need to have high-speed hardware, just software; and you don't need to interoperate with anyone.  Take your AES software implementation, run the half the rounds, insert any keyed, reversible operation on the state (*using a key completely independent from the AES key*), then run the other half of the rounds.  In practical terms, no one will ever break that, even if they have some fancy attack on AES itself - unless you use it for something worth many billions of dollars.
                                                        -- Jerry

@_date: 2017-12-20 17:27:59
@_author: Jerry Leichter 
@_subject: [Cryptography] Rubber-hose resistance? 
This of course may not do what you think on an SSD - which it probably is these days.
What attack will filling the disk with random bits stop that simply zeroing the disk won't stop?  If you're worried about national-lab level attacks ... who really knows what they are?  Some of the attacks against physical disks - like reading off the edge of the track - may well work just as well against random erasure as against zeroing.  And at that level, SSD attacks are significant - in fact, many much lower-level labs could carry them out - and nothing you can do from the host can really protect you.
And using gpg checksumming as a replacement for 'sync' ... doesn't really work:  If the data is in memory buffers waiting to be written, gpg will (under the covers, inside the implementation for read()) get it from there.
                                                        -- Jerry

@_date: 2017-12-20 22:12:17
@_author: Jerry Leichter 
@_subject: [Cryptography] Rubber-hose resistance? 
And you know this ... how?  The actual magnetization pattern on a modern disk has no simple relationship with the the bit pattern as seen all the way outside the disk.
Maybe all zeroes are easier to remove.  Maybe a random bit pattern is easier to remove.  Take a simple analogy:  If you were talking about audio, a random bit pattern would be like high-frequency hiss, easily removed by a low-pass filter.
The classic "35 patterns" paper carefully examined a number of long, long obsolete disk encodings and proposed specific patterns suitable for maximally erasing bits encoded with each particular technology.  There were 35 patterns because *different* erasure patterns, and different *combinations* of erasure patterns were most suitable.  If random bits were good enough, there would have been no paper to write.
I don't know what pattern are best for modern disks - or, for that matter, how this varies from disk type to disk type.  And neither do you.  Without careful study, simply asserting that random bits "make it more difficult" just won't do.
Of course, if you're talking SSD's, even ignoring remapping ... the pattern written is likely irrelevant, since each page has to be erased before it's written.
                                                        -- Jerry

@_date: 2017-12-20 22:29:14
@_author: Jerry Leichter 
@_subject: [Cryptography] Rubber-hose resistance? 
Then you don't understand how SSD's work.
The number of pages actually available inside the SSD may be - likely is - quite a bit larger than the size visible outside the device.  When you write a block, it goes on some page.  You don't know - there's no interface to find out - what page that block lies on.  If you write the same block again, it almost certainly ends up on some other page.  The old page goes into a "to be erased and reused later" list.
Just because you filled up every block does not mean the list of free pages is empty.  Nor does it mean those pages have been erased.
There is simply no way to know you've erased all the pages in an SSD using only the interface the device presents to you that makes it look like a disk.
If you don't know enough about how the device you are trying to erase is organized internally to rule out or rule in such possibilities, you have no business claiming you have an effective erasure tool.  FYI, for an ordinary disk, this is very unlikely, as it would make "the amount of free space available on the disk" a meaningless number.
Someone else brought up more general compression and deduplication.  These are common for shared/remote file systems, rather less likely for local storage, where they make the interface much more complex and probably don't save all that much anyway.  Erasing something that is on a remote file system is generally pretty much impossible.  What if that system does remote mirroring?  Or some kind of more efficient redundant encoding?  Can you even get an accurate read of the physical size of the "disk" - think thin provisioning.
Ultimately it comes down to:  If you don't understand how the underlying system works, you shouldn't be asserting that a particular technique is "secure".  You just don't - can't - know.
                                                        -- Jerry

@_date: 2017-12-21 17:21:00
@_author: Jerry Leichter 
@_subject: [Cryptography] Rubber-hose resistance? 
Correct.  As an interesting datapoint:  Apple's MacOS has a Disk Utility that does all kinds of low-level stuff on a disk.  It used to provide a Secure Erase option, which erased everything on a hard drive using well-known techniques.  It no longer does:  Apple no longer sells any devices with "spinning rust" disks, just SSD's; and there is no secure way, even if you are the OS/driver author, to do a secure erasure.
Note that this is problem arises *because SSD's implement a backwards-compatible interface to a disk*.  The underlying technology is actually not a great match to the way disks work; there's a lot of code inside and SSD to make the device "look like" a disk.  The underlying layer *could* securely erase all the contents; and an interface to request erasure *could* be provided.  Such interfaces have been proposed and perhaps even implemented, but as far as I know none has actually been implemented in a mass-market product.  (It would not surprise me to learn that parts with this capability exist in specialized markets, e.g., for the military.  The prices would likely be extremely high.)
For the rest of us, probably the best thing to do is to encrypt everything before it goes to the device.  Destroy the key, and the device is logically erased instantly.  (Both iPhones and some Android devices actually do this.)
Of course you run into the "turtles all the way down" problem:  If you store the key on the device itself ... how do you erase it when you can't control what gets written where?
While the information may be *present* on the drive, getting it out requires specialized hardware and techniques.  How valuable is this information?  How serious an attack are you concerned about having to survive?
                                                        -- Jerry

@_date: 2017-12-22 17:51:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Rubber-hose resistance? 
It was an analogy, nothing more.  If you're talking about typical audio - as in sounds of interest to human beings - then a low-pass filter (with a high enough cut-off) usually leaves enough information around that a human can easily access it.
The real weakness of the analogy, in fact, is on the other side:  All zeros would amount to a DC offset, which is trivial to remove.
You have no basis for saying that without knowing something about the technology "inside the box".
Suppose we're dealing with a disk, and the attack is based on reading "off the centerline", where previous values sometimes stick around.  (There have been real attacks like this.)  Imagine that the particular technology happens to  have the property that writing 0's "spills over" more of the track than writing 1's.   If you overwrite with all 0's, all the previous information is lost; if you write random bits, half of it is still there.  (I don't know of any technology with this property, but if we don't know "what's inside the box", hey, maybe it works this way.)
Yes, finding which (zero!) bits are "real" and which ones aren't is hard - but one can, for example, imagine cases where it might be worthwhile to figure out that some particular bit pattern was likely on the disk or not, which you could reasonably check for.
The most you can say is that, lacking any information about "what's inside the box", any two possible erasure patterns are equally likely to be good.  But you need to look inside to actually know that one is better than another.
It's not clear, from the documentation I've seen (I haven't dug into this *too* deeply), what effaceable memory actually is.  I believe what the documentation describes is just memory that the OS guarantees to wipe when the device is reset.  There's no specific indication (correct me if I'm wrong) that this is actually implemented at the hardware level:  It may be that it's simply normal space on the SSD.
Keep in mind that Apple isn't trying to protect you from some three-letter agency.  Even if effaceable memory were just on the SSD, recovering the key would require specialized hardware and techniques beyond the capabilities of most attackers.  The "zero on reset" capability is there mainly to ensure that re-sold phones don't trivially pass along their previous owners' data.
It's certainly *possible* that effaceable memory is actually sealed inside the "secure enclave", part of the CPU chip which does have the capability to store keys.  But I don't think Apple has said.
Note also that at this point it's a secondary technique.  Based on complaints from the LE community, the encryption off all the data stored in the phone is already beyond the reach of "normal" techniques.  (That is, if NSA knows how to do it, they won't admit it even to the FBI.)
                                                        -- Jerry

@_date: 2017-12-23 18:24:01
@_author: Jerry Leichter 
@_subject: [Cryptography] Rubber-hose resistance? 
Only available on some high-end Intel SSD's, and the software to talk to the drive to trigger it is available only on Windows.  It does appear to be based on a standard ATA (but not SCSI) command, so others could implement the command (in the hardware) or access to the command (in the software).
Nice to see this is out there, but it's going to have to be much more widely available before it makes much of a dent.
                                                        -- Jerry

@_date: 2017-02-01 07:42:29
@_author: Jerry Leichter 
@_subject: [Cryptography] Firewall penetration 
I'm not sure what you're referring to.  We are not yet at the point where "ordinary consumer connections" can't listen for incoming traffic, nor where outgoing traffic to such endpoints is blocked.  Yes, there are some special cases (mainly for mail, to block spam) but it's not a general phenomenon.
There are two common issues.  First, "ordinary consumer connections" don't have static IP's, so finding your target requires something special.  Two solutions are common:  Dynamic DNS, which follows the varying IP address around as it changes; and third-party "rendezvous" sites which come down to the same thing, just effectively using a private namespace separate from DNS.  Some of these "rendezvous" sites may act as proxies, allowing both ends to have outbound connections and simply forwarding the traffic onward; others pass along the needed information and then let the endpoints connect.
The second issue is consumer-level firewalls.  But there are commonly-implemented protocols allowing hosts behind the firewall to create openings through it.
                                                        -- Jerry

@_date: 2017-02-01 17:20:06
@_author: Jerry Leichter 
@_subject: [Cryptography] Why is a short HMAC key zero-padded instead of 
If you look at RFC2104, it doesn't say quite this.  In fact, it seems to say two different things.  The informal description in Section 2 mentions padding the key with zeroes - but says nothing at all about keys that are too long.  Then Section 3 says "The key for HMAC can be of any length (keys longer than B bytes are first hashed using H).  However, less than L bytes is strongly discouraged as it would decrease the security strength of the function.  Keys longer than L bytes are acceptable but the extra length would not significantly increase the function strength. (A longer key may be advisable if the randomness of the key is considered weak.)"  L is the hash function's output length.
But then in the Appendix we have a sample implementation, which hashes keys that are too long - but has no explicit code to zero-pad short keys.  Rather, this falls out of copying the key into an appropriately-sized buffer which has been pre-zeroed.
There is a reported erratum against that RFC, which changes the text about long keys to say "Applications MUST not use keys longer than B bytes."  The explanation given is that "Using this approach creates an exploitable vulnerability where there are two known K instances, one the hashed key, and the other the key itself.... To cite a real world vulnerability; for all keys longer than B, using password storage configurations which store the hash of the key for integrity checks, and store the key itself in a tamper proof device, there will exist plain text keys stored on both storage systems. Compromising a hash database should not reveal plain text secrets, which will only be true if an implementation first hashes the key and uses the resultant L byte string as the actual key to HMAC."
The disposition of the Erratum is not clear.
                                                        -- Jerry

@_date: 2017-02-02 19:22:57
@_author: Jerry Leichter 
@_subject: [Cryptography] Why is a short HMAC key zero-padded instead of 
I went back to the original paper by Bellare, Canetti, and Krawczyk.  There, the function is defined as HMAC_k(x) = F(kbar XOR opad, F(kbar XOR ipad, x)) "where kbar is the completion by adding 0s of k to a full b-bit block-size of the iterated hash function...."  (Yes, that's the exact original language.)
So the original paper didn't even consider the case of a key longer than the input block size.  I suspect the way they made their security statement doesn't work for longer keys - which doesn't mean that the construction isn't secure, but the actual security statement is going to be a bit different.  Then again ... maybe doing this is *not* secure.  (The proof of security of HMAC is based on treating it as a special case of a more general NMAC construction, so it would take some careful study to understand exactly what's being claimed and how it might be affected by a long key.)
It's actually rather disturbing that the standards allowed for a case that the reference document they were relying on for a proof of security didn't cover it.
                                                        -- Jerry

@_date: 2017-02-03 16:06:08
@_author: Jerry Leichter 
@_subject: [Cryptography] Why is a short HMAC key zero-padded instead of 
The proof of security in the original Bellare et al paper *explictly assumes a random key*.  You're wandering off into the wilderness of "it's secure because I don't see any obvious way to attack it" when you start worrying about how the HMAC primitive should deal with non-random keys.
Consider AES.  It takes a key of a given, fixed length.  Full stop.  How you get it is up to you.
The security promise for HMAC includes a dependency on the number of bits of *random* keying material you provide.  Appending zeroes to the key to get it up to the input block size doesn't change this value.  Truncating or hashing gives you a different security guarantee for these longer keys.  And dealing with non-random keys is a completely different issue.
Why should HMAC even deal with varying-length keys?  We already have ways to construct a key of a given length from a longer or shorter input.  Why build that into every primitive that takes a key?
My suggestion is that HMAC should be like AES:  Defined for a random key whose length equal to the input block size of the hash function on which it's based.  Creating such a key is a problem for the user of the primitive.  (It may be worth remarking that appending zeroes to a short random key is "good enough" - though you won't find any mention of that in the AES standard either.)
                                                        -- Jerry

@_date: 2017-02-05 06:31:52
@_author: Jerry Leichter 
@_subject: [Cryptography] Why is a short HMAC key zero-padded instead of 
That may well be, but we're talking not about the actual usage of the algorithm but about two *standards*, one from the IETF, one from NIST, recommending a procedure ... for no known reason.  Does anyone know where the "hash it if too long" mechanism came from, as it's not in the base research paper?  *Someone* must have proposed it.  Or was it in some pre-existing implementation that got standardized without any thinking about it?  Note that unlike the zero extension - which *could* have appeared as a side-effect of careful code; the reference implementation in the RFC does it because it pre-clears a fixed-size buffer - pre-hasing requires a deliberate effort.  (Not that that's where the zero extension came from; it was in the base research paper.)
This is *probably* not a big deal:  No one does it, and it *probably* doesn't introduce a vulnerability even if they do.  But what does it say about our standards processes that unnecessary complexity, solving no real problem, and *perhaps* introducing one, somehow gets slipped in to them?  Haven't we seen this story before?  It didn't work out so well for us - the "big" us - in the case of Dual_EC_DRBG....
                                                        -- Jerry

@_date: 2017-02-05 08:06:38
@_author: Jerry Leichter 
@_subject: [Cryptography] Why is a short HMAC key zero-padded instead of 
Well, that's interesting isn't it?
My questions were somewhat rhetorical; I understand how standards come to be.  In the case of cryptographic standards, however, we've learned how dangerous this process is - how easy it is to subvert it to produce results that look good but have subtle problems that lead those in the know to breaks, whether of a cryptographic or simply a procedural nature.
It's not that others haven't exploited the standards processes in other ways before. Microsoft was historically a master of getting standards to include tons of options that everyone would have to implement "for compatibility" while MS had pre-chosen options that *they* would actually use well ahead of time.  And there are plenty of even older examples:  As I heard it told, the original Ethernet standards, as developed by DEC/Intel/Xerox, deliberately left out some details (on timing or shaping of pulses, something like that) that made it extremely difficult for anyone but those three to build reliable Ethernet repeaters.
Still ... cryptographic standards *are* different, and we now know that if they're to be useful, they need to be *treated* differently.  At the least, everything in a cryptographic standard should be traceable to an independent, published analysis.  That's not the case here.  The only reference in either the NIST standard or the RFC is to the original paper - which doesn't mention long keys.
Ironically, the listed authors of the RFC are actually the same as the authors of the original paper!  Were they actually involved at the point the final algorithm was decided?  Or was it some late editorial decision by some committee somewhere?
                                                        -- Jerry

@_date: 2017-02-05 08:33:44
@_author: Jerry Leichter 
@_subject: [Cryptography] Why is a short HMAC key zero-padded instead of 
Well ... I'm glad that in this case it was possible to track things back.
My concern was never with the particular bit of the standards.  It was, and remains, with the process.  Until the last couple of days, how many people in the world even knew that HMAC as officially standardized, was not the same as HMAC, proposed and proved secure?  Surprised the hell out of me!  What other little surprises are sitting there, in the open but effectively hidden, because who has the time to cross-check everything?
BTW, if you really want to be paranoid ... a question does remain:  Why did the IKE guys feel the need to push their particular construction into HMAC this way?  Had HMAC been standardized to accept only keys no longer than the block length, they could have simply defined IKE with trivially different language to construct an HMAC key from the passphrase, then apply the standard HMAC.  Why diddle with one standard to make the wording of another one slightly simpler?
                                                        -- Jerry

@_date: 2017-02-06 14:20:05
@_author: Jerry Leichter 
@_subject: [Cryptography] FYI: quantum cloning 
Not an expert in the field, but ... no.  In QM, everything is probabilistic.  So all you can say is that a quantum Eve will be detected with no less than some probability.  They approach in this paper is two-fold:  First, the construct an "optimal cloner":  If Eve can clone the state of a photon that Alice sent Bob, Bob will see the original whole Eve will at her leisure read the clone.  Except that the No Cloning Theorem proves that you can't clone an unknown quantum state.  You can *copy* it, but you lose the original.
Still, QM never says "never"; it just puts bounds on what you can accomplish.  You can *approximately* clone a quantum state.  And they show how to do it optimally, even for complex (multi-dimensional) states.
So then they ask the question:  What effect does that have on the security of the original quantum key exchange protocol?  Except that they "improve" on that protocol by using high-dimensional states rather than the very simple states of the original thought experiments.  This turns out to make the system much more robust - you're encoding the information in multiple separate measurements at once so noise has less of an effect.  Of course, this potentially means a cloning attack might copy enough state to read the bit, even though the copy is "lossy".  So ... they analyze what happens if Eve uses the optimal cloner to try to read Alice's high-dimensionally-encoded bit.  It turns out this has a specific, observable effect on Alice and Bob's communication, and they can in turn optimally detect *that*.  So they end up showing how to construct a variant on the original key exchange protocol that's more error-resistant and actually does a better job of detecting Eve than the original.  And they actually show that they can do these things physically and the results seem to match the theory, which is nice.
Except ... you end up with a bound *for a particular attack*.  The attack happens to be "optimal" in some sense, but it's not the right sense.  You would want to look at the probabilities across *all possible attacks*, i.e., "optimal way to attack the protocol", not "optimal way to clone a high-D state", which is what you get.  So ... who knows for sure what this gives you.  The treatment of this stuff has gotten reasonably sophisticated, so I suspect we'll see some treatment of the general question in the future.
Exactly how this affects the original low-D protocol ... I don't know.
                                                        -- Jerry

@_date: 2017-02-20 19:10:09
@_author: Jerry Leichter 
@_subject: [Cryptography] German govt tells parents to destroy 
Odd that they didn't require the maker to compensate the buyers.
                                                        -- Jerry

@_date: 2017-02-25 07:03:16
@_author: Jerry Leichter 
@_subject: [Cryptography] Security proofs prove non-failproof 
I think you're missing the point here.  We started with the assertion the C++'s semantics was, at the moment, too complex/messy for formal analysis.  The response was "yes, but certain C++ features would make verification easier".  And that latter is true:  If you have a verified smart pointer implementation, then proving assertions about the lifetime of resources managed through smart pointers becomes very easy.  All kinds of intractable issues about pointer equivalence go away with smart pointers.  So C++ code written using that style should be more easily verifiable than traditional C code.
This is low-level stuff on which the operating system and everything else has to be built.  And ... since C++, despite the new features, still has all the old complicated stuff that makes verifiers choke, the fact that *some* C++ programs are now easier to verify doesn't help you write a verifier for *all* C++ programs.
Also, off on the side, even if you don't get to full verification, partial verification - human, machine, combined, formal or informal - of C++ code which uses the smart pointer style is also much easier.  It's necessarily "partial" - the fact that correct C++ semantics for such a pointer ensures that no code outside of this scope can access the value can't help if there's other code that casts pointers to long's and back and walks through all of memory.  (Or simply exceeds an array bound.) But we should take what we can get.
                                                        -- Jerry

@_date: 2017-02-27 13:22:44
@_author: Jerry Leichter 
@_subject: [Cryptography] SHA-1 collision could also allow ePassport 
The current attack does not permit creating forgeries of pre-existing documents.  It allows one to create pairs that have the same hash - but you create the *pair*, not a new document to match an existing one.
That's *right now* this is not a direct threat.  That's not to say a more powerful attack might not come along before 2020....
                                                        -- Jerry

@_date: 2017-02-27 13:44:54
@_author: Jerry Leichter 
@_subject: [Cryptography] Just in case it isn't obvious... 
I wonder how many more instances of this kind of design there are out there.  Many object store implementations (AKA "content-addressable storage") - some of which manage immense volumes of data - use SHA1 hashes to identify objects.  At least one of these - I forget the marketing name, EMC sells it - has been around long enough that it had to deal with this problem back when MD5 failed!
De-duplication engines may well have the same issues.  These run in backup programs, in cloud services (e.g., Dropbox) - even (very slightly different use case) in rsync.  In some cases, these are very high performance hardware boxes which likely do their hashing in dedicated hardware.
Compared to these, fixing git is child's play.
                                                        -- Jerry

@_date: 2017-02-27 14:16:39
@_author: Jerry Leichter 
@_subject: [Cryptography] Updating firmware/hardware 
We've talked about the need to securely update hardware or firmware - and the difficulties doing it.  "Router assimilated into the Borg, sends 3TB in 24 hours"  describes the author's experience with a Netgear R6400 WiFi router which apparently got recruited into some on-line botnet - and which he was unable to rescue.
                                                        -- Jerry

@_date: 2017-02-27 22:37:08
@_author: Jerry Leichter 
@_subject: [Cryptography] Schneier's Internet Security Agency - bad idea 
Actually, Apple *is* trying to do this.  They have a standard for IoT devices - HomeKit - that is designed to be secure.  In fact, various device makers grumble about the costs of doing things Apple's way - too hard, too expensive.  (E.g., they allegedly require 3072-bit keys and use Curve25519.  How these two fit together, I don't know.)
The net result is that HomeKit-certified devices remain pretty rare.
Obviously, don't take this as an endorsement of the details of Apple's approach.  There's probably a published version of the requirements, but I've never looked for it - and I'm unaware of any third-party analyses.  It's certainly *possible* that the HomeKit requirements are all PR hot air.  Then again, given what we've seen about the security of iPhone's, it appears Apple really is taking this stuff seriously, and really does have some very competent people working on it.
But going back to the question of whether this "solves" the market failure:  iPhones appear to be more secure than Android phones - but even our dear leader, er, president, using an Android phone, and more than that, one that is apparently so old it cannot have received any security updates in quite some time.  Apple's (by hypothesis) secure stuff would cost more than some bleeding edge, dirt-cheap internet-connected flea comb - but that flea comb and all its compatriots will be serving in some bot army no matter how secure Apple makes HomeKit thermostats and cameras.
                                                        -- Jerry

@_date: 2017-02-28 13:41:00
@_author: Jerry Leichter 
@_subject: [Cryptography] jammers, nor not 
Disney Research Labs - who knew there was such a thing? - has recently talked about some work they've been doing in wireless power:    They use a room (could be as large as a whole warehouse) with metal walls and a pipe running through the middle that's the exciter; the whole room is some kind of resonant cavity within which they can efficiently transfer power.
So ... we may be about to solve several classic problems at once:
1.  Wireless power;
2.  Privacy via home/office Faraday cages;
3.  A place to practice pole dancing.
Oh ... and it comes from an LA company to boot!
                                                        -- Jerry :-)

@_date: 2017-01-01 07:12:29
@_author: Jerry Leichter 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
Oh, come on.
A smart electric meter provides a way to read electricity used remotely.  It might, I suppose, have a way to shut off current remotely - but I rather doubt it - how often do electric companies need to cut off the power to a subscriber?  There may be separate - though perhaps integrated - controls that can turn power off to certain appliances to shed load.  OK, so your A/C and such might get shut down.
"An attacker could also see whether a home had any expensive electronics."  From hacking the smart meter?  How, exactly?
"He will have power over all of your smart devices connected to the electricity."  How?  Yes, the protocols used for these things are badly insecure - but they communicate wireless, not through the power lines.  Just what does hacking the power meter have to do with breaking into IoT communications?
"This will have more severe consequences: imagine you woke up to find youd been robbed by a burglar who didnt have to break in."  Just what exactly could you have been robbed of?
I can believe billing fraud - though we're talking mischief here, not theft, which tends to limit the scale.  There's also potential annoyance from having your A/C shut down.  But explosions and house fires?  How, exactly?
"[I]n 2015 a house fire in Ontario was traced back to a faulty smart meter, although hacking was not implicated in that."  Well, gee, a malfunctioning device attached to main power - in front of even the home main breaker in typical installations, mind you - had a "malfunction" and caused a fire.  Malfunction - as in short, perhaps?  These things are rare, but they do happen.  I once saw a short develop in a wall socket.  In what in retrospect was probably a fraction of a second before the breaker tripped, an impressive amount of energy was dumped into the surrounding wood panelling.  Fortunately, it didn't catch fire.
Yes, the security of IoT sucks.  That's widely reported.  Personally, I've avoided IoT devices, as the cost/benefit tradeoffs - from the overpriced devices to the potential costs due to hacking - just don't make sense to me.  But let's keep things in some perspective.  I know "based on reality" is out of style these days, but ... call me old-fashioned, but I still believe in it.
For many years, I avoided doing on-line banking because I didn't trust the security systems involved.  Then I read Don Knuth's stories about how someone used a paper check he'd written to get his account number and produce and use bogus checks in his name.  It took significant effort on his part to get the money back.  In fact, even closing the account wasn't enough - when some "late" checks arrived, the bank re-opened the account, paid them, and tried to bill him.  At that point, it became clear that the security issues are elsewhere, so I might as well go for the convenience.  I've never looked back.
                                                        -- Jerry

@_date: 2017-01-01 08:44:09
@_author: Jerry Leichter 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
You're missing the point.  Yes, customers get cut off for failure to pay.  Does it happen often enough to make it worthwhile to build "turn off power remotely" capabilities into meters?  The traditional method - sending someone out there to pull the meter - continues to work just fine.  It's very visible, very sure, and I'll bet is legally required in some places.  Plus - what can be turned off remotely, can be turned back on remotely, providing easier ways to steal power.  The legal liabilities if the power company installed something that allowed improper remote shutdowns would be interesting.  So the economics is not at all clear.
Do smart meters have a remote shutdown capability?  I don't know.  Even if they did, yes, shutting down power would be disruptive, perhaps even dangerous - but "explosions and fires"?  No.
BTW, the grid itself is managed through remote-controlled switches, which themselves have long been known to be badly insecure.  There's a remote-controlled switch on the line not far from my house.  Trigger that, and I lose power - along with 100+ neighbors.
                                                        -- Jerry

@_date: 2017-01-02 18:43:47
@_author: Jerry Leichter 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
You guys are lucky, then.  Here in the US we had a crash of the grid back in the 1960's.  It took over a day to get things going again.
Of course, a great deal depends on what you mean by a "crash".  The 1960's crash involved about the eastern third of the USA and Canada.  The rest of the country stayed up - but (well, because) in those days there country was divided into a couple of independent grids.  Now they are all interconnected, but we've also put a huge amount of work into keeping problems from propagating too widely.  (The great crash started with a lighting hit on one high power line, as I recall.)  There were also other lessons learned - e.g., large generators draw power for their exciters.  They were typically designed to assume that the rest of the grid would always be there to supply excitation power.  Not a good assumption in a big crash.  (Now there are local diesel generators that don't need external power for exciters in place to supply the big guys.)
There are many examples of huge infrastructure vulnerabilities, though they are usually discussed in terms of physical destruction.  A classic example:  All natural gas to Manhattan flows through one of three distribution points - big manifolds along the Hudson River, as I recall.  These are large and impossible to hide.  A determined (physical) attacker could pretty easily take all three out.  Repair time would be many months:  Each of these is a custom design that would have to be built from scratch.  And then you have the unsolved problem of hundreds of thousands of pilot lights that went out when the gas failed, so that if you start supplying new gas, it'll be leaking from a huge number of points - each an explosion waiting to happen.
Electrical substations are also mainly custom (though they are more custom combinations of standard units).  Reconstructing those would be a lengthy operation (though they've been regularly knocked out in various air wars around the world, and people have figured out how to get them back on line).
Yes, some of these physical vulnerabilities may now be vulnerable to attacks via their computerized management infrastructure.
                                                        -- Jerry

@_date: 2017-01-02 19:17:04
@_author: Jerry Leichter 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
Why?  Transformers reflect demand back to the primary side pretty much instantaneously.  Ultimately the components that get stressed by sudden load changes are the generators (and some related compensation components), which may be unable to react quickly because of their huge mechanical inertia.
Lighting strikes dump huge amounts of energy into power systems on a regular basis. Even the largest power lines can get cut and short to ground - rather spectacular events when they occur.  There are also huge surges due to solar bursts hitting the atmosphere.  Power systems are designed to survive this sort of stuff ... up to a certain point.  (If the Earth were to be hit by a large ejection of charge particles - something that happens regularly - the surges would knock out most of the world's power systems for long periods of time.)  But below the point of total failure, the systems are designed exactly to deal with massive fluctuations in supply and demand.  Absent some specific, carefully computed vulnerability (which may well exist, but you'd have to find it and target it), I'm skeptical that just dropping demand drastically would do much more than maybe trip some breakers.
                                                        -- Jerry

@_date: 2017-01-03 22:44:03
@_author: Jerry Leichter 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
I'm not sure that cost is the driving factor here.  What seems to be more relevant is that power companies are simply unfamiliar with modern computing technology.  They have a long-standing, very sophisticated understanding of high-power systems, which are basically electromechanical.  When you unit of power is the megawatt, very few semiconductor - much less IC - parts are relevant.  It took years to build workable rectifier stacks.
An idea of the kind of technology base that power companies build on:  Primary power distribution lines - which carry power, typically at 3-15KV, from substations to local step-down transformers - typically have fuses or breakers of some sort at each transformer, and elsewhere along the line.  However, most power faults on overhead lines are transient - caused by things like falling branches.  So a fuse or breaker that must be manually replace or reset can unnecessarily length outages.  So ... there are more modern devices known as reclosers.  A recloser responds to a current surge by disconnecting the line, but then it re-connects it.  Typically, it has one or more "fast" cycles in which it reconnects within a 30 seconds or less.  Then it may have a longer cycle.  Finally, if the line is still faulted, the recloser will stay open, requiring manual reset.  If no faults occur for a while, the recloser will eventually go back to its initial "a couple of fast cycles" state.
Now, as you read this description, the implementation that probably came into your mind was of a SoC controlling the timing and some kind of software-controlled switch.  Reclosers like that exist today, but the most common ones use much older technologies.  The mechanical engineering is complicated, but the way some models work is basically as follows:  On a fault, the switch opens, and a vane in a vat of insulating oil moves to the "open" position.  A spring tries to drive it back to the "closed" position, but this takes the requisite time because the oil has to be forced through an opening in the vane.  After the vane makes it back, it also moves over a bit.  By the time it needs to do a "slow" cycle, it presents a much smaller hole for the oil to flow through - so it takes longer to close again.  If it trips in this state, the next jog of the vane leaves no hole at all, so the vane can't move back to the "closed" position.  It locks here.  If the vane is left in any other position, some secondary mechanism - probably oil flowing slowly through other channels - eventually "unjogs" the vane back to its original position.  Fluidic computing at its finest!
You might think these are antiques - but reclosers are relatively new devices.  I don't know when they were first introduced, but they seem to have become common (at least on primary distribution circuits0 only in the last decade or so.
Sounds absurdly complex - but the engineering of such systems has a long, successful history.  It integrates easily with the switches needed to actually cut and restore multi-KV, multi-megawatt power lines.  (These switch are typically bathed in oil for insulation anyway.  A system like this is very rugged, unfazed by electrical surges.  Building and certifying a microprocessor-controlled equivalent to serve out on top of a power pole, unattended, for many years, is not an easy task.
So ... I think the power guys are just out of their depths here.  It'll take a while for them to develop the necessary expertise to integrate modern processors properly.
                                                        -- Jerry

@_date: 2017-01-04 19:51:10
@_author: Jerry Leichter 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
This is a really, really important question.
The software engineering community has generally been very resistant to the kinds of certification programs - and standards and codes - that traditional engineering fields have long relied on to ensure that the errors of yesterday are much less likely to be repeated today.  I don't want to get into that - there are good arguments to be made that software engineering is too young and still growing and changing too rapidly for it to make sense to try to pin things down.
But in the field of security, what we need is a serious "security engineering" program, with certifications and codes and best practices and all that.  I'm *not* talking about pretty much any of the certifications that are out there today.  I've interviewed people with a bunch of security-related acronyms after their names - and their knowledge proved to be quite shallow - they knew the acronyms but not much of what was behind them.  I'm talking about a conjectural program that might include, as one of its core courses, a year-long course based on Ross Anderson's Security Engineering book; and another based on Schnier and Ferguson's Practical Cryptography; and others based on books of similar calibre and depth that probably haven't even been written yet.
Right now, it's not only hard to *find* people with the necessary broad background knowledge on these matters - it's hard to *recognize* them, because there are no degrees or other kinds of certifications that show someone has at least worked through this material.  If you don't already have a good security guy to interview candidates, you have no hope of distinguishing the experts from the expert bullshitters.
Security engineering is still a craft, not a profession.  It's small scale, and there are not enough people in the world to fill the demand.  I see little sign yet that this is changing - even though the demand is high, and people who can actually do this stuff well (justifiably) command high salaries.
                                                        -- Jerry

@_date: 2017-01-06 06:35:06
@_author: Jerry Leichter 
@_subject: [Cryptography] Internet of things - can we secure it by going 
You'd be running counter to a couple of decades of evolution and "best practice".  Don't reinvent - use what's out there.
Not so long ago, devices typically ran no OS at all - all the code running in the thing was a single program written for the specific hardware.  The code ranged from dreadful to excellent - but each device and each code base was pretty much unique.  No one would bother to take the time to put more in one of these things than was needed.  Since devices didn't talk to each other much, and certainly could not generally be reached remotely, security was not a big issue:  Undoubtedly there were many security bugs in there, but hardly anyone was in a position to exploit them.
Then small real-time kernels emerged and gained some traction - until Linux caught on (and also the BSD's, even if they don't get talked about much).  Hey, look, for free you can get a whole OS, written by skilled developers you don't have to pay, with toolchains and development environments and such you don't have to buy.  They run on all the hardware you care about.  Someone, somewhere, has probably already written a driver for that weird I/O chip the hardware guys spec'ed because it cost 3 cents less than the common competitor.  What's not to like?
There's an old joke from the era when workstations were first coming into wide use:  "The mainframe is shared with many people - I don't have to work off hours, my workstation is just as fast during the day!"  "Yes, but it's just as slow at night!"  Using a full OS makes your device "just as easy to program" as your workstation - and just as vulnerable.  Add lack of patching, and it's just as vulnerable as the workstation you had four (or many more) years ago.
Yes, implementing just a minimal subset of capabilities in OS's and protocols appropriate for use in embedded devices would go a long way to make them more secure.  And there have been repeated attempts to do this over the years - at many layers of the software stack.  One example have been C subsets that leave out or at least minimize the more hazardous parts of the language.  Busybox at least lets you fix any security issues once, rather than in every one of 20 or more basic shell commands.
But none of these has caught on.  You need to gather a sufficiently large group to buy inton using and building and maintaining the thing, or it quickly dies.  Alternatively, you can try for a commercial effort - but making money in this area is just about impossible, since the free alternatives are considered "good enough":  Insecurity doesn't show up as a cost factor in most embedded systems.
In theory, you could "modularize" Linux so that someone wanting a cut-down implementation could pick and choose.  In practice ... this does get done to some degree, but the smallest practical Linux implementation is still full of features you'd rather not have.
Over in the datacenter world, containers are actually providing a way to build smaller minimal "installations" of a different sort - though they aim at solving an entirely different set of problems in a very different space.  I'm working on a system like this, and we run our containers with no root or other privileged account, only a handful of open ports, the minimum set of services we can get away with ... etc.
This is the downside of free software:  Everyone uses the same thing, so they get the advantages of sharing.  But they also get the disadvantages of over-sharing and common failure modes - one failure mode being that features keep getting added, and once added, are almost impossible to remove.
The BSD's are probably closer to providing what you describe than Linux.  Since the OS inside of embedded devices isn't easily determined, it's difficult to compare the popularity of BSD- vs. Linux-based devices.  I have the feeling, based on no evidence, that BSD penetration is slowly declining.
And, of course, Linux vs. BSD vs. anything else at the OS level has no influence on the network protocols, which just keep getting bigger and hairier every year.   Since these are inherently there so that devices can talk to each other, no individual implementation can decide not to play the game.
We're not getting to the promised land.  We can't even see it from here.
                                                        -- Jerry

@_date: 2017-01-15 19:58:55
@_author: Jerry Leichter 
@_subject: [Cryptography] ZK meeting scheduling protocol? 
Ray Dillinger already pointed out some of the ambiguities in the problem definition.  But let's consider a simpler one:  A useful scheduling algorithm can be iterated, and applied to arbitrary sets of users.  But then to determine whether you are free at time X, I need merely fill my schedule from now to X, then propose a meeting with you and see what the system comes back with!
Yes, this might expose what I'm doing.  But there are "inverse" methods like trying to set up an appointment with you and n-1 others, then with just the n-1; if the resulting appointment changes, you were the one who had a problem with it.
This feels like the problem of trackers in databases - which turned out not to have any simple solutions, only statistical ones (and even those are tricky).
Pinning down the exact definition of a "secure anonymous meeting protocol" in such a way that it actually *has* any interesting solutions seems quite difficult....
                                                        -- Jerry

@_date: 2017-01-17 06:57:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Cryptocurrency Exchange without a trusted third 
While I agree with the general principle, do note the second paragraph:  "Back in the 1960s...."  I don't know about you, but I doubt I would remember the name of a long-dead Hong Kong company I bought something from 40+ years ago.  And researching the story would be rather difficult in any case.  It doesn't sound like the kind of thing that the mainstream press would cover.  Likely there were stories in contemporary photo magazines - most of which are likely also long gone, and if any are still around, are unlikely to see much reason to have copies of such old issues available for research.  (Magazines like this live for their reviews and their ads - both of which are obsolete within a couple of years after publication.)
While I'm willing to believe that the story Bill recounts actually happened, proving it today would be difficult.  And ... keep in mind just what a different place the world was in the 1960's.  China was Maoist (as in Mao was still very much alive an in control) and a closed off mystery; Hong Kong was a British colony, very far away and very far out of the economic mainstream.  If you dealt with a Hong Kong company in those days - known only through an ad in the back of a magazine plus some word of mouth, entirely through letters that would take a week or more each way; when the whole point of the transaction was to skirt the law - would obviously have been a risky business.  I'm sure events such as the one Bill recounts happened multiple times.
                                                        -- Jerry

@_date: 2017-01-20 17:17:07
@_author: Jerry Leichter 
@_subject: [Cryptography] ProtonMail accessible via Tor onion site 
...and one wonders, after the recent discussion of attacks that leverage large amounts of work on particular fields to attack connections across many servers, if they are using one of the "standard" fields.
                                                        -- Jerry

@_date: 2017-01-22 08:05:41
@_author: Jerry Leichter 
@_subject: [Cryptography] Oracle discovers the 1990s in crypto 
To be slightly fairer to Oracle:  You would not believe the environments in which software that tries to be secure is forced to fit.  (Well ... you of all people would.)
A couple of releases back, Oracle dropped support for older versions of SSL from the JDK, leaving only TLS.  (There's a documented option to turn the support back on, but it's never worked.)  TLS is by now almost 10 years old (depending on exactly what you want to count), so this won't break anything ... right?  Hah.  The software I work on has to communicate with various elements of enterprise hardware - minor things like disk arrays.  With the new JDK, we could no longer connect to one vendor's hardware.  Not legacy stuff, mind you - stuff currently being sold.  It turns out that secure connections to that particular hardware support only older versions of SSL.
To be more accurate:  They support only older versions of SSL *by default*.  There's a command-line option to turn on TLS support; it's off as the system and its most recent updates are delivered.  Mind you, this isn't an option to *replace* SSL with TLS - it simply enables TLS *in addition to* SSL.  But for some reason ... it's off by default.
So customers will just turn it on, right?  Oh, you make good joke.  Managing the settings on the disk arrays is owned by a completely separate group.  You have to get it on their list of to-do's.  They may resist any "non-standard" settings.  They may insist on multiple layers of sign-offs.  They may insist on getting approval from the vendor.
Removing support for protocols and cryptographic primitives is very, very difficult.  The systems we build are simply not designed to adjust appropriately.
Anyone want to bet on how many pre-build jar files, signed years ago with MD5 or short RSA keys, are out there in Maven repositories, waiting to cause build and run-time failures all over the planet?  How many of them will turn out to have long-lost source trees, or will have source trees that can no longer be built because the tooling around them has deteriorated?
Actually, I suspect that things won't be as bad as they might have simply because so many of these widely-shared artifacts aren't signed anyway....
                                                        -- Jerry

@_date: 2017-07-11 05:34:42
@_author: Jerry Leichter 
@_subject: [Cryptography] A software for combining text files to obtain 
I didn't see where the original proposal suggested using the entire Internet's text corpus as input.
However:  This reminds me of an old proposal - I don't recall where I saw it - for something of this sort as a much more secure version of a book code.  Take a high-resolution image as a source of bits; I think the original suggested some NASA image of the moon.  A key is several starting points and step sizes in a fixed linearization of the image.  Each starting point and step size gives you a stream of bits.  XOR together and then use the result as input to a stream cipher.
Yes, insecure in theory - you're reusing a "one-time pad"; worse, the contents of the "one-time pad" are public!  But the only plausible attack appears to be brute force, and there are a *lot* of keys....
                                                        -- Jerry

@_date: 2017-07-12 06:32:33
@_author: Jerry Leichter 
@_subject: [Cryptography] A software for combining text files to obtain 
Thanks.  That was what I was thinking of (though I don't recall ever reading the original paper, just saw a reference to the idea).
For completeness, from the abstract:  "We should point out that our analysis does not apply to the original cipher proposed by Maurer, where the pool is chosen to be large enough that it is infeasible to read the entire contents of the pool. Unfortunately, Maurers cipher is not very practicalin his paper, he proposed digitizing the surface of the moon as one means of getting enough public randomness to make the cipher workand hence not suited to present-technology implementation. We attack a simplified version, one which is more practical."
Almost 40 years later, there probably do exist databases large enough to make the original idea plausible.  Just consider the concatenation of all Google Street Map images, for example.  Whether there are any practical applications is another question.  (Whether anyone other than Google itself would be able to reliably have access to a fixed set of those images is part of the "practicality" issue.)
                                                        -- Jerry

@_date: 2017-07-12 12:15:03
@_author: Jerry Leichter 
@_subject: [Cryptography] [FORGED] Attackers will always win, 
Many, perhaps most, published side-channel attacks are based on:
1.  Algorithmic issues.  The work the algorithm itself specifies must be done is dependent upon sensitive data; it really doesn't much matter what optimizations you apply or don't apply during code generation.
2.  Hardware issues.  The hardware responds differentially to sensitive data - e.g., there are sensitive-data-dependent rotations, and rotations in the hardware have a signature that depends on how far you're rotating.  (This is so common, in fact, that it's now standard not to use key-dependent rotations.)  This is a mismatch between the basic operations as seen by the algorithm designer and as seen all the way down in the hardware; again, optimization or lack thereof doesn't matter.
I'm sure there exist examples of optimizations introducing side-channel attacks, but they appear to be few and far between.  We've mainly figured out how to deal with the algorithmic issues, but the hardware issues appear to be intractable.  CPU's today are stupendously complex, and vendors long ago stopped giving any details about how operations are actually carried out.  And they feel free to change them all the time in the name of performance.  If you think there's little hope in getting compiler writers to have a mode which gives you fine-grained control ... it's way harder to get the chip makers to give you a set of instructions with a simple, controllable semantic model.  Their answer has been to build the crypto directly into the hardware - hopefully making it resistant to side channel attacks.
So ... a compiler specialized for crypto software, which let you specify every last detail of the code generated, would buy you little.  That's not where the problems are.
It's interesting that the NSA has historically concentrated on hardware implementations of complete cryptosystems.  There are undoubtedly many reasons for this - and unlike pretty much any organization other than a couple of counterparts of NSA around the world, they are in a position to build exactly what they want and require people to use their hardware - but I suspect that part of this is their experience in successfully attacking software-based implementations.
                                                        -- Jerry

@_date: 2017-07-12 15:37:06
@_author: Jerry Leichter 
@_subject: [Cryptography] [FORGED] Attackers will always win, 
Who's going to write the first paper on attacking such a device through power analysis of the sounds it makes as it runs?
                                                        -- Jerry

@_date: 2017-07-12 17:34:03
@_author: Jerry Leichter 
@_subject: [Cryptography] [FORGED] Attackers will always win, 
You're missing my point.  If the *hardware* won't guarantee anything about instruction execution time ... what the compiler possibly do?  Do you assume a compiler that targets a particular exact mask and microcode level of a particular chip?
                                                        -- Jerry

@_date: 2017-07-12 17:58:10
@_author: Jerry Leichter 
@_subject: [Cryptography] ***SPAM*** Re: [FORGED] Attackers will always 
My answer was tongue in cheek, but in fact if I remember right, Spycatcher -   - discusses listening to the sound of typing on Soviet crypto machines to determine the cleartext that was being typed into them.
                                                        -- Jerry

@_date: 2017-07-17 10:31:06
@_author: Jerry Leichter 
@_subject: [Cryptography] SW requirements to block timing side-channel 
And right here the idea breaks down, as you're assuming that whatever you use to waste time up to the limit if you finish early will be externally indistinguishable from actual cryptographic computations.
I don't think people appreciate just how complex modern hardware is.  Here's something I just recently learned:  The last 2 (3?) generations of Intel chips have non-uniform memory access *on chip*.  When you look at the chip, you see some number of cores, and some amount of memory.  They all look the same.  But on the chip, the cores are grouped into blocks, each with some fraction of the memory; and the blocks are on an on-chip ring.  (Early versions used a uni-directional ring; later versions a bi-directional ring.)  So memory latency depends on whether you are accessing memory in the same block as the current core, or memory in some other core; and if on some other core, how far away it is along the ring.  In fact, the difference (viewed at a high level) comes to over 30% (after caches are considered).  If you look just at raw access times, the difference is much greater.  Also, there's a distributed cache consistency algorithm *running inside the chip itself*.  So you could probe what other cores are using from the cache by looking at cache hits and misses on yours.
Oh, and BTW, Skylake replaces the ring with a crossbar - whose detailed performance characteristics no one outside of Intel yet knows.
On top of all this, OS's have complex algorithms that try to optimize usage in the face of the complexity of the hardware - e.g., trying to allocate memory in the same block as the core running the thread that allocated it (which can actually fail badly if the design of the application allocates memory in one thread - e.g., a UI responder - and promptly passes it off to another).
Given this level of complexity, I think the notion of "really secure" cryptography running on general-purpose hardware is nonsense - if you define "really secure" as safe against all kinds of side-channel attacks by NSA and its friends - not to mention attacks through holes deliberately or accidentally introduced in the huge amounts of microcode that runs all this stuff.
Either you define your security needs at a lower level, or you build your own hardware from the ground up.  Design your system so that "red" data flows only through your blessed hardware; only allow general purpose hardware to see "black" data.
                                                        -- Jerry

@_date: 2017-07-23 05:09:31
@_author: Jerry Leichter 
@_subject: [Cryptography] Blinding quantum computations 
I haven't looked at the underlying paper - I suspect I'd find it hard to follow; my knowledge of QM is decades old and limited - but the summary looks interesting:  A purely classical user of a remote quantum computer can hide what is being computed from a QM attacker.  (With caveats at the end of the summary, as always seems to be the case with quantum computation - very complex stuff.)
                                                        -- Jerry

@_date: 2017-06-07 06:21:23
@_author: Jerry Leichter 
@_subject: [Cryptography] stego mechanism used in real life (presumably), 
There's an interesting and significant sidelight to the previous discussion of watermarking, and the message a couple of days ago from "M373" concerning the Seaglass project at U of Washington, which is developing means for detecting IMSI catchers at city-wide scale.  In both cases - and there are others - we have legitimate research devoted entirely to discovering, publicly explaining, and perhaps effectively neutralizing, mechanisms that LE has put in place.  As far as I can tell, this has little historical precedent.  Criminals/revolutionaries/freedom fighters - it all depends on you viewpoint in particular situations - have long conducted exactly this kind of research.  But it's been clandestine, done in support of their own activities, and passed around as secret tradecraft.  (Of course, state actors have also long targeted each other this way.)
We've crossed a threshold when entitled members of society feel the need to work to subvert their own society's enforcement mechanisms.  (No, university faculty members and EFF researchers and such - while hardly among the big movers in shakers - cannot reasonably be considered the downtrodden in any Western society.)
                                                        -- Jerry

@_date: 2017-06-16 09:38:50
@_author: Jerry Leichter 
@_subject: [Cryptography] 1984! US Senate Launches Forfeiture and Crypto / 
"Average" is in almost all contexts "mean".  For example, dictionary.com says:
1.  a quantity, rating, or the like that represents or approximates an arithmetic mean
If you really want to be deliberately ambiguous, you have to go with a complex technical term like "measure of central tendency".
But since you asked, let me Google all the data for you:
Sen. Grassley, Chuck [R-IA]       born September 17, 1933
Sen. Feinstein, Dianne [D-CA]     born June 22, 1933
Sen. Cornyn, John [R-TX]          born February 2, 1952
Sen. Whitehouse, Sheldon [D-RI]   born October 20, 1955
There.  Feel free to any statistical measures you like.  Report any interesting findings here.  :-)
                                                        -- Jerry

@_date: 2017-06-20 18:47:59
@_author: Jerry Leichter 
@_subject: [Cryptography] Can we get some damn security defaults?! 
It's been locked up now.
Will the Russians do a better job protecting their copy? :-)
                                                        -- Jerry

@_date: 2017-03-02 12:14:58
@_author: Jerry Leichter 
@_subject: [Cryptography] Bizarre "latent entropy" kernel patch 
Of, course we now have the AnC attack ( which gets around ASLR on most (all?) existing hardware.  In Javascript, no less.
Given that vulnerability, the effective randomization for kernel ASLR seems to be beside the point.  If the hardware is going to have to change to blunt the AnC attack, we might as well require it to provide random values at the same time.
(If you're going to say you don't trust the random numbers the hardware will give you ... why would you trust the AnC workaround?)
                                                        -- Jerry

@_date: 2017-03-10 15:42:16
@_author: Jerry Leichter 
@_subject: [Cryptography] Crypto best practices 
The only thing this brings immediately to mind is related key attacks.  Though if your re-keying mechanism allows related-key attacks you have other problems.
Anyone have any insight into just what they're referring to?  It could be extremely significant, given that forward security relies on very frequent re-keying.
                                                        -- Jerry

@_date: 2017-03-12 16:11:15
@_author: Jerry Leichter 
@_subject: [Cryptography] Secret Handshake problem. 
Let's split this into two subproblems.
If C is sure that D is a member, then C wants to send a proof of exactly the fact "C is actually the member known as A" to D.  Zero-knowledge proof theory provides a solution to this problem.
But C would not wish to send this proof to a D who is *not* a member.  So suppose all members of the group share a (symmetric) key K.  C encrypts the proof with K and sends it to D.  If D is a member, he decrypts it and learns about C.  If not, D can't read the proof and learns nothing.
Except of course that D may wonder about the message.  There are all kinds of "social engineering" excuses that could be made for the "misdirected" message.  Or in some cases D might be expecting a message that he can't read anyway.  For example, C and D might be running Tor forwarders.  D decrypts a message and finds within it something it *expects* to look like next-step forwarding instructions, and then random bits.  Except that if D knows K, those "random bits" are actually the proof.  (Of course, in this situation, the proof is overkill - the fact that C knew the key to encrypt "I'm A" - with suitable redundancy - is enough.)
Alternatively, if C and D are using a protocol that allows for subliminal channels, C might be able to hide the message there.
                                                        -- Jerry

@_date: 2017-03-12 16:00:31
@_author: Jerry Leichter 
@_subject: [Cryptography] Secret Handshake problem. 
If you flip the problem around and loosen the "identified by phone numbers and/or care plates" just a bit, there's a straightforward solution - which is a variation of what all "secret but public" organizations have done since time immemorial.
The blacklisting is presumably done by the wronged sex worker.  So instead have the worker whitelist a "good" customer by giving them a token.  The token is a one-time introduction to the next sex worker.  The tokens have unique id's that can be put in the database; they are paired with phone numbers or car plates.  The database stores that externally identifying information encrypted with the id, and the id hashed, so if you don't have the token id, there's no way to probe for the phone number/plate even if you steal the database.
Once used, the id is deleted from the database.  A worker under duress can give out an id, then cancel it later.
All the smarts for such a system can be in the server - the phone doesn't have to generate the tokens.  You can imagine a number of ways to get the token to the client - e.g., text it, or display a QR code on the worker's screen.
There remains the "enrollment" problem:  How do you get people in the first time?  The traditional technique is to allow someone who has an id to serve as introducer.  This gets you into all kinds of transitivity-of-trust issues in traditional systems - once you've let someone in, they're in.  But since here the introduction only gets you *one* token - and after that you are vetted at each interaction - it's not nearly as severe in this case.
For the real-world problem where there may not be an introducer, you can always have the first interaction take place in a well-controlled environment.
                                                        -- Jerry

@_date: 2017-03-13 18:20:49
@_author: Jerry Leichter 
@_subject: [Cryptography] USB firewall/condom HW/SW 
All the responses so far have been snarky comments about why you should trust this device.  If you don't trust *this one* - don't buy it.  Or wait until the code has been audited - or audit it yourself.  Sure, the underlying hardware might be hostile - but then that's the case for every USB device out there.  And on top of that, you can't generally trust the software either.
Can we consider the realities of such a device?  Can a useful device like this actually be built?  If I plug a memory stick into it and the firmware has been modified to actually report that it's a keyboard, and it starts typing commands into the console ... how could the USB firewall know?  It sees a device that identifies itself as a valid keyboard.  How can it know that it was supposed to be something else?
                                                        -- Jerry

@_date: 2017-03-14 08:09:02
@_author: Jerry Leichter 
@_subject: [Cryptography] USB firewall/condom HW/SW 
Answering my own question:  The designer of the thing provides some "Technical Details for the Curious" at   First off, internally the device consists of two STM32F microprocessors connected by a serial link.  So kind of an air gap.  (Given the "USB-killer" devices out there, I'd recommend an opto-isolator between them but that's a simple improvement.)  The two micros use a "simple serial protocol" so that what can be passed between them is controlled.
He enumerates three kinds of attacks:
1.  "USB Driver Exploits" involving malformed or unexpected inputs.  The receiving micro (which we assume has a carefully written USB stack) will simply block these.
2.  "Hidden Evil Functionality" involves things like USB memory sticks that also contain additional functionality like HID devices in addition to their memory device.  These are blocked by allowing only one device to attach at a time, and blocking run-time device class changes.  OK, but if you consider celebrated attacks like the "USB memory stick left on the ground in the parking lot" which may employees will pick up an stick in their machine ... if it acts *only* as HID or network device and attacks the machine that way, the employee will simply decide that it doesn't work, no big deal.
3.  "Evil Functionality in Plain Sight" involves using legitimate USB commans to perform malicious actions.  Defense requires "rules specific to the attached device".  For example, he suggests limiting the arrival rate of keystrokes from HID devices and encrypting blocks on the fly for mass storage devices.  But ... all this is still under development.
So ... a nice idea which has somewhat limited utility right now, and exactly how far one can go with it is unclear.  Nice to see someone thinking along these lines, but I don't see it ever having broad acceptance:  The systems that most need it shouldn't have open USB connections to begin with!
                                                        -- Jerry

@_date: 2017-03-17 07:28:40
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA says China's supercomputing advances put US 
OK, we're all properly cynical.  Just because someone will potentially make a great deal of money out of solving a "problem", doesn't mean there isn't an actual problem hiding in there somewhere.  And some of the people who would get the money actually *are* experts in the appropriate field.  (Of course, I guess you can subscribe to our President's attitude that "so-called experts" are all liars anyway.)
Actually ... they don't.  I can only speak to what Google builds.  It's indeed absolutely massive at an unprecedented scale that few on the outside can even begin to wrap their heads around ... but it's optimized for a large but certainly not universal set of problems.  Want to run a highly parallel, loosely-coupled, data-intensive, mainly integer and string manipulation computation?  We've got something perfect for you.  Want to run a massive, tightly-coupled, data-light (relative to computation) floating point computation?  There are better designs out there.
We can debate whether problems of this sort are important.  We can debate whether Google-like approaches, even if slower, are "good enough" by now to deal with them.  We can certainly debate, even if it's important to develop machines to solve such problems, whether government money should be invested in them - one can certainly argue that if there's actual demand, private money will be available.  (Note that if you look at the history of super-computing, even when commercial enterprises like CDC and IBM where involved, many, perhaps most, of their customers were relying on government contracts one way or another, so (a) the line is rather fuzzy; (b) it's not a trivial question.)
But just outright dismissing the whole issue as "more pandering for a bigger place at the government trough" just doesn't cut it.
BTW, I'm sure Baidu has Google-like data centers in China, too.
                                                        -- Jerry

@_date: 2017-03-18 06:58:10
@_author: Jerry Leichter 
@_subject: [Cryptography] Crypto best practices 
I think this actually is an instance of a deeper, much more difficult problem.
When we do cryptographic *theory*, we naturally try to "divide and conquer":  We look for secure pseudo-random stream generators, secure block ciphers, secure public key systems, secure signatures, and a number of other "primitives", which in turn we sometimes break down even further to simpler "primitives".  Having created a nice working collection of primitives, we find secure ways to combine them - e.g., we build HMAC out of one-way functions.
Similarly, when we program, we "divide and conquer", deliberately producing small reusable blocks that we can "mix and match" to produce larger programs.  In fact, a large fraction of "programming" today - most of it, in some domains - is done by searching the Web for a bunch of pieces and gluing them together.
But then programmers look at the collection of cryptographic "primitives" the cryptographic theorists have produced and view them as a set of reusable modules.  That's how we get the kind of combinatorics you see in SSL/TLS:  Pick your key agreement algorithm (historically since SSL2.0 there have been 20, according to  pick your cipher/mode of operation (one of 16; SSL/TLS hasn't let you pick cipher and mode of operation separately but, hey, why not).  Not even specified is how you pick your random number generator and nonce generator - that's considered private to each party, though really you might want to be able to validate your counter-parties choices.
Programmers look at the options and say "that's agile, that gives the user maximum flexibility - fantastic, thank you cryptographers".
But ... the problem is that cryptographic primitives *are not recombinable modules*. Many - likely most - combinations of secure primitives are insecure.  HMAC was developed because it was "obvious" how to combine the operations of "agree on a secure key" and "one way hash" to produce a MAC.  Except that the "obvious" combinations don't work, and proving HMAC secure was worthy of publication as a significant advance in cryptographic theory.
*Secure cryptographic primitives don't combine securely.*  If we could teach programmers that unfortunate truth, we'd get many fewer cryptographic disasters.  Choice here is *not* your friend.
By the way, this problem is in turn a particularly bad special case of a broader issue in security:  We don't know how to compose secure modules into larger systems and maintain their security.  Oh, there are some particular constructs that retain the required security properties, but often they don't meet other functional requirements.  The notion of a security boundary - e.g., a firewall through which all connections must flow - is great and allows you to generalize from the security of the firewall to the security of the system it protects - but only with certain assumptions about how the system will be used and how it will be attacked.  Unfortunately, your attacker isn't bound by those assumptions....
                                                        -- Jerry

@_date: 2017-03-19 09:09:47
@_author: Jerry Leichter 
@_subject: [Cryptography] Crypto best practices 
I would guess that this is an area where the three-letter-agencies are ahead of the open research community.  Actual safe use in the real world has always of necessity been a requirement for them.  Meanwhile, on the "break" side of the house, they've had decades of experience with underhanded means of getting into, through, and around systems.  Of course, their requirements are not exactly the same as those of individual or commercial users - they have much more of an ability to directly control how the systems they develop are actually used.
The earliest paper I know of that discussed this topic is a real oldie that I don't think I'll be able to track down now by Phil Rogoway.  He talked about the right practical abstractions for modes of operations, and suggested that the end-user call CBC(K, IV, data) not use the IV directly, but instead first encrypt it with K.  Then the IV would not have to be unpredictable, just non-repeating for a given key.  (E.g., a simple sequence number for the session would be fine.)
Unfortunately, it later turned out that this way of generating an IV IS NOT SECURE.  I don't recall the attack, but I think it was actually pretty simple.  Even the experts get it wrong sometimes!
                                                        -- Jerry

@_date: 2017-03-21 16:09:29
@_author: Jerry Leichter 
@_subject: [Cryptography] Crypto best practices 
Are we talking about *theoretical* security or *practical* security?
If it's *theoretical* security, it seems silly to be concentrating on lapses that generate the same IV twice.  How about easily guessed keys?  Or many other practical lapses?
If it's *practical* security, combining the user's offered IV with the time, to as high a precision as you can get it, pretty much eliminates the *practical* issue of IV reuse.  (Note that you don't need a very sophisticated combiner, but XOR is likely a bad idea as some bright user may decide to himself use the high-precision time as a quick way to generate a "unique" IV.)
Of course none if this helps if you are using a mode that requires an *unpredictable* IV, not just a *unique* IV.
                                                        -- Jerry

@_date: 2017-03-21 18:50:30
@_author: Jerry Leichter 
@_subject: [Cryptography] Blockchain scalability 
describes an IBM offering of a blockchain implementation in the IBM cloud.  It's intended as a distributed ledger for various kinds of transactions, not to support a crypto-currency.  It's based on the Linux Foundation's Hyperledger Fabric, whose development IBM helped finance.
I found the following quote from an IBM press release of interest:  "Fabric is built for enterprise-level systems, and it can handle more than 1,000 transactions per second among large ecosystems of users".
A large number - but if you're serious about using blockchain to support a widely used crypto-currency, you've got a *long* way to go.  And keep in mind that this is within IBM's cloud, which has much better connectivity than the larger Internet.
                                                        -- Jerry

@_date: 2017-03-23 21:36:28
@_author: Jerry Leichter 
@_subject: [Cryptography] encryption + authentication - waiting - chaining 
This brings to mind a design I came up with and implemented in a long-obsolete proprietary system years back.  I've never seen anything quite like it used elsewhere, so I pass it along in the hope someone might find it useful.
The system encrypted a data stream used for an RPC-like protocol.  Many calls simply returned a status, so many data segments were tiny - a byte or two of user data.   Others varied in size, though typically none were huge.  Because the underlying data stream was synchronous in some cases, you couldn't just wait for more data to accumulate; you had to deliver what you had promptly.
I chose to use AES-128 in CBC mode (yes, it was vulnerable to the attack published much later on IV's carried over from message to message), with a separate authentication step.  (The newer modes weren't around yet.)  I used an unusual approach to creating blocks to feed into AES, which allowed for "tuned" authentication.  Consider a 16-byte block of cleartext.  The value of the first byte B0, taken as an unsigned 8-bit value, falls into one of three categories:
- B0 > 15:  A full block.  The block contains 16 bytes of user data, including B0.
- 1 <= B0 <= 15:  A short block.  The next B0 bytes in the block contain user data.  The last 16-B0 bytes are available for other purposes.
- B0 = 0:  This is a metadata block, containing no user data.
As you create blocks, you also create a running MAC.  Whenever you emit a short block, you use as many bytes of it as you can to fill the available 5pace at the end.
There is also a security parameter R.  When sending the next block would cause the fraction M/N of MAC bytes sent to user data bytes sent to be less then R, you emit a metadata block containing MAC data.
The receiver does the same computation of M/N, of course, and reports an error if the sender doesn't send enough MAC data to keep M/N in bounds.
                                                        -- Jerry

@_date: 2017-03-29 18:29:30
@_author: Jerry Leichter 
@_subject: [Cryptography] escalating threats to privacy 
Tor is overkill for this particular application.  A decent VPN is sufficient:  What the ISP will see is that all your traffic - none of which it can read - goes to the VPN forwarder.  If the VPN forwarder has a nosy ISP on the far side, it would have to be able to correlate packets passing in and out of the forwarder.  Possible, but way beyond what these guys are currently planning on doing.
Yes, your VPN provider will know who you talk to.  But most of them exist by selling "we don't care who you talk to; we don't even log it."  You generally have no way to change ISP's but changing VPN's is easy.
You could also run your own VPN proxy out in the cloud.  The chance of anyone being able to correlate your encrypted data stream going into AWS with the immense volume of stuff flowing *out* of AWS is pretty low.  And except for someone at the scale of the NSA who wants to "collect everything", it's no longer a matter of adding some infrastructure and pulling in data from all your customers; it's now a bunch of individualized attacks which are highly unlikely to be economically worthwhile.
                                                        -- Jerry

@_date: 2017-03-29 19:57:13
@_author: Jerry Leichter 
@_subject: [Cryptography] DJI calls for drone IFF 
Snoop on *what* exactly?  The location of your drone?  The signal being sent by your drone?
So that they can send ads ... where exactly?  If I fly my drone over McDonald's I'll get ads from Burger King?  This makes little sense.
Why would they care to destroy *your* drone specifically, as opposed to "the (anonymous) drone that's flying over the White House"?  Just to piss you off?  They can piss you off much more effectively already.
While SIM cards can be cloned, SIM card cloning has not represented a significant attack - and cloning a SIM card has significant monetary value, beyond any "hacking for the lulls" value.
Sorry, these complaints make little sense to me.  We consented long ago to put license plates on cars because they are capable of doing significant damage and without plates identifying which car did the damage would be very difficult.  Drones have similar potential to do similar damage.
I just don't see a pressing need to have the ability to fly drones anonymously.  Do you have use cases that are significant enough to outweigh the potential abuses?
As you noted later in the quote, Denmark is one country already mandating this technology.  Freedom House ranks Denmark as one of the most free countries in the world.  There are a bunch of other rankings out there, depending on what exactly you want to measure, but Denmark always scores high.  Even that paragon of libertarian economic thought the Cato Institute ranks them as 5th.  Apparently there are many who believe that "freedom" and "you have to identify your drones" are compatible....
                                                        -- Jerry

@_date: 2017-05-02 18:03:31
@_author: Jerry Leichter 
@_subject: [Cryptography] Big ugly security problem in post-2008 Intel 
Published details remain vague.
One could also ask what level of *network* access is required.  If simply the ability to send a packet to the machine is enough, perhaps you could break into one VM's host from a different VM - since VM's you own can send each other packets.
                                                        -- Jerry

@_date: 2017-05-06 09:25:10
@_author: Jerry Leichter 
@_subject: [Cryptography] Escrowing keys 
Citation?  No description I've ever seen of Apple's approach to encryption involves such a key *to the devices*.
If you're saying that there's only one Apple *signing key* to sign legitimate updates - yes, that's true.  (Well, there's some small number, used for different things.)  The situation with that key - or the theoretical alternative, a database of unique signing keys for each device produced - involves interesting tradeoffs on the legal side.
Well, yes, that's potentially a problem for the *government*, not Apple or its users.  It's been a common strategy to entangle stuff you want to protect with a whole bunch or other stuff belonging to others so that you can later claim a demand to produce something would inevitably let out a whole load of other stuff to which the government (or a private party in a lawsuit) is not entitled.  It's not clear how effective that strategy is.  Certainly, the majority of governments in the world don't much care about who's privacy is compromised along the way of getting what they want.  And even in countries like the US, with reasonably strong protections, we have by now an extensive history of successful claims by the government that incidental *collection* of masses of stuff that "is never looked at" doesn't invade anyone's privacy for legal purposes.
                                                        -- Jerry

@_date: 2017-05-07 21:04:31
@_author: Jerry Leichter 
@_subject: [Cryptography] Blockchained code signing. 
That was me, in the context of compelled actions by the provider.
No help at all.  If the US government - or another government with the power to enforce it's desires - comes to Apple and ordera "build us a distribution with the following modifications, and sign it just as you would any other distribution", no technological measure can possibly help.
Of course, in the US, that would lead to some interesting legal arguments; but that's a separate issue.
                                                        -- Jerry

@_date: 2017-05-18 06:12:44
@_author: Jerry Leichter 
@_subject: [Cryptography] Repeated Salts? 
I've never heard of this happening, though it would certainly not surprise me if it has.  Obviously, it defeats the purpose of a salt, but that doesn't mean someone who programmed by rote, without any real understanding, hasn't done it.
Returning to your first question:  The earliest implementation of a salt, in Unix 6 or thereabouts and for a while after, used a 12-bit salt.  Obviously this resulted in duplicate salts, certainly within the space of all Unix installations and likely even at some individual installations.
True story:  At Yale University back in the early '80's, when salts were 12 bits and shadow password files hadn't been invented, some undergrad noticed that his salt and encrypted password exactly matched that of root on a CS department Unix system.  Not as impossibly unlikely as it sounds:  The root password was a dictionary word, something like "dolphin".  (Hey, those were much more innocent days.)  So of course he now had access to root.  He played some games with the system - nothing really harmful - and eventually got caught.  That's how the story of how he got the password to begin with came out.  Given, again, the more innocent era, nothing much was done to him; he may even have been hired as a student employee.
                                                        -- Jerry

@_date: 2017-05-19 15:26:48
@_author: Jerry Leichter 
@_subject: [Cryptography] Password rules and salt 
Many years ago, David Wittenberg and I developed and patented (for DEC; the patent wasn't renewed after 17 years and has lapsed) a solution to this problem:  Keep track of used passwords in a Bloom filter, *using cryptographically secure hash functions* to prove the filter.  This has the following properties:  If a password has been used, it will definitely be caught by the filter; if the password hasn't been used, there is a small probability it will be caught anyway; even someone who sees the individual updates to the filter can't determine what's been added to it.
                                                        -- Jerry

@_date: 2017-05-23 05:45:41
@_author: Jerry Leichter 
@_subject: [Cryptography] "Post-quantum RSA" 
"Abstract: This paper proposes RSA parameters for which (1) key generation, encryption, decryption, signing, and verification are feasible on today's computers while (2) all known attacks are infeasible, even assuming highly scalable quantum computers. As part of the performance analysis, this paper introduces a new algorithm to generate a batch of primes. As part of the attack analysis, this paper introduces a new quantum factorization algorithm that is often much faster than Shor's algorithm and much faster than pre-quantum factorization algorithms. Initial pqRSA implementation results are provided."
The proposed parameters lives in a curious area somewhere between theory and practice.  It isn't "theory" because it can't exclude the possibility of incrementally faster quantum algorithms for factoring that would destroy the tradeoffs.  (Then again, the same could be said of traditional pre-quantum RSA!).  On the other hand, it's not really practical because the recommended key sizes are around a terabyte - and they estimate an encryption/decryption time of about 5 days.
Still, an interesting exploration of limits.
Oh - the authors include Daniel Bernstein and Nadia Heninger. :-)
                                                        -- Jerry

@_date: 2017-05-23 08:00:22
@_author: Jerry Leichter 
@_subject: [Cryptography] "Post-quantum RSA" 
Oops, forgot to include the link:                                                          -- Jerry

@_date: 2017-05-27 16:27:57
@_author: Jerry Leichter 
@_subject: [Cryptography] key lengths in different places 
Thought to be the same.
There's some "reasonable guesswork" and approximation going on here.  There's no actual proof that the work factor of 3K3DES is 112.  We know it can't be *more* because of meet-in-the-middle, but then things get a bit vague:  We assume that there are no attacks better than brute force on DES, but what exactly does that mean?  It only talks about *complexity*, not what goes into the attack.  Perhaps there's an attack that takes as much work as brute force but gives you some advantage over chains of DES encryptions.  Tough to formalize what's included in "all attacks equivalent to brute force".
The best *analytic* result I know is actually about DES-X, which is the much simpler:
That is, replace the inner and outer DES encryptions with encryption by XOR'ing with a fixed key.  Looks too simple to work, but  shows that it gives you "about" 112 bits of security against brute-force attack.  (The actual security depends on the number m of oracle queries - I think chosen plaintexts; it's been a while since I read the full paper; the security is 118-lg m bits.)
The result itself, of course, doesn't really tell you what's going on with 3DES, but the proof techniques may (probably do - it may well be discussed in the full paper) carry over.
Anyway ... the interesting thing here is that if you're only concerned about brute-force attacks - which are historically the concern when it comes to DES - there's no solid reason to prefer 3DES over DESX (which saves you two DES encryptions per block).  Still - it doesn't give people "the warm fuzzies", so you'll still see 3DES all over the place, but DES-X seems to be used by ... no one.
                                                        -- Jerry

@_date: 2017-11-01 19:34:49
@_author: Jerry Leichter 
@_subject: [Cryptography] How Google's Physical Keys Will Protect Your 
Well, there is an exception - because they came at it from a different direction:  Apple.  Apple started of with Apple Pay, now very widely accepted by both providers and users and growing continuously.  More to the point - the underlying biometric authentication technology, which so far is used for Apple Pay and a couple of other things having mainly to do with payments (e.g., iTunes purchases) - but this is starting to shift.
It's possible for web sites to accept Apple Pay.  Current MacBook Pro's already allow unlock/login using the same fingerprint technology.
And of course the latest generation of Apple watches can already authorize Apple Pay payments and unlock your Mac.  If you already have an appropriately-capable watch on your wrist, why both to also carry around a Yubikey or similar device?
Apple could turn this around any time and provide a generic service.  If a web site accepts payments through Apple Pay - why not use the same mechanism for logging in to begin with?
We've had a number of attempts by companies to be the central authenticator for the Internet.  Google and Facebook have tried to get there; neither gained much traction.  I can see Apple heading that way - and unlike the others, Apple prefers to take a slow, steady approach:  Wait for the necessary technology to mature; provide a few basic but compelling uses; wait for a critical mass of users to join the ecosystem - then take the big leap, from a point so far ahead of others that they are instantly left with a huge catch-up job.
Exactly what plans Apple has in this direction, no one by Apple knows of course - but I'd guess we're getting close to seeing them revealed.
                                                        -- Jerry

@_date: 2017-11-13 05:47:48
@_author: Jerry Leichter 
@_subject: [Cryptography] Is ASN.1 still the thing? 
For protobuf, I'm pretty sure the answer is yes.  It would take a careful reading of the specs to be sure there are no corner cases, and it depends on proper implementation:  protobuf representations of some datatypes are transferred in a compressed format.  For example, integers use a varying-length representation that can drop leading zeroes.  So you *could* represent an integer in multiple ways - though you're *supposed* to use the shortest representation (which is unique).  Whether a receiver would reject a non-canonical representation, I don't know - probably not.
Then again, one could say the same thing about ASN.1.
                                                        -- Jerry

@_date: 2017-11-13 16:26:47
@_author: Jerry Leichter 
@_subject: [Cryptography] Is ASN.1 still the thing? 
I haven't seen a direct comparison, but I very much doubt ASN.1 would be competitive from a CPU-usage point of view.  Maybe it was well matched to CPU's at the time it was designed, but CPU's have changed significantly.  There are a couple of competing formats - Thrift (I think done by Facebook) and Avro (not sure who designed it) are two I know of - which do somewhat better on speed, compression, or maybe even both, so people are still learning new tricks - but the differences aren't very large any more.  Here's one 5-year-old, somewhat out of date, comparison (e.g., for protobufs, Google released an RCP implementation and added more supported languages.  I'm sure Thrift and Avro have improved as well.):   .  Performance Comparison of Data Serialization Schemes ... - ThinkMind  does seem to indicate that ASN.1 does better than protobuf - but it's a long paper that I don't feel like reading through and while it does show that in some cases (but not others, with differences I don't understand) ASN.1 produces significantly smaller encodings, I couldn't quickly tell how CPU times compare.
At Google, protobuf's are *the* medium of exchange for data.  Everything is done through RPC - I/O calls that use the Google file system turn into RPC's - and those RPC's are implemented as protobufs.  Protobuf values are also define a "record format" for saving data to files.  So there's really good reason to optimize for some combination of CPU and size - though *what* combination is really going to be application- and technology-dependent.  As has been discussed here in the past, ASN.1 is rather difficult to implement correctly.  The protobuf wire format (which is what we're really discussing) is quite simple by comparison.  Granted, these days there are libraries that (allegedly) get both right.
In the end ... the best answer for which is better seems to be "it depends" - which is exactly why we are still inventing new serialization formats.
                                                        -- Jerry

@_date: 2017-11-22 21:37:43
@_author: Jerry Leichter 
@_subject: [Cryptography] Intel Management Engine pwnd (was: How to find 
Why would you expect a hardware implementation to be more secure?
The problem is that the management engine has very complex functionality, including a complex interface.  We don't have ways to ensure the security of systems with that level of complexity - no matter how it happens to be implemented.
In fact it's unlikely a system that complex *could* be implemented directly in hardware for any reasonable price, if at all.  That's why microcode was invented.  And ... there's tons of it in any x86 implementation.  Is that hardware or software?  Is that distinction even meaningful?
                                                        -- Jerry

@_date: 2017-11-24 21:41:43
@_author: Jerry Leichter 
@_subject: [Cryptography] WIPEONFORK in Linux 4.14 
VMS had this many years ago:  A volume (disk) could have the "erase on delete" property.  A file on such a volume would be erased by the file system regardless of how it came be be deleted.  Since the erasure was done as part of adding the blocks to the free list, if the system crashed before all the blocks were erased, they would get erased during the next mount operation as the equivalent of fsck cleaned up incomplete operations.
I don't know if the file names were erased, but because of the way VMS directories were organized - with all free space immediately pushed to the end of the block - this would have happened implicitly for most (but not all) files as file names after them were "slid over them".  I don't recall if the free space at the end was filled, but it may have been zeroed as a way to mark it as free - in which case all traces of deleted file names would be zapped.  (Because VMS directories were always kept sorted, the Unix trick of renaming the file to a random name of the same length would not have worked.)
I will note that this option wasn't used very much because it caused noticeable slowdowns.  Today's disks (and certainly SSD's) and CPU's are much faster - but today's files are much bigger, too.  I had a vague recollection that "erase on delete" could be turned on for individual files, but apparently not.
Of course, an easier solution is to encrypt all files with a file-specific key.  Zap the key and the data is effectively gone.  iOS has done something like this for a number of releases.  Android has the capability to do something similar, but it's not clear whether it's turned on by default now - when the capability was first released, it wasn't.
                                                        -- Jerry

@_date: 2017-11-24 21:54:12
@_author: Jerry Leichter 
@_subject: [Cryptography] Intel Management Engine pwnd 
Reports from those who've hacked at this are that some parts can't be turned off or the system just doesn't work.  (You can get the chips to boot, but they shut down within half an hour.)
Whether influence was applied to get the chips into this state, who can say; but the switch that *is* there - which turns *most* of the stuff off - was apparently added for our friends at NSA.
The problem with this kind of setup is that once the capability to include complex algorithms in the infrastructure of the chip is there, people will use it.  Modern Intel chips have some very sophisticated power management algorithms.  Turning those algorithms off would require running the chips at well below their rated capacities or they can overheat.  So whatever may have started Intel down the path of making some of this stuff mandatory ... by now, the requirement is embedded deep in the designs.
                                                        -- Jerry

@_date: 2017-11-24 21:56:15
@_author: Jerry Leichter 
@_subject: [Cryptography] Intel Management Engine pwnd 
...which of course leads you to the exact opposite of the assertion I was responding to:  That the properties of the chip should be implemented in hardware, not in software.
                                                        -- Jerry

@_date: 2017-11-25 11:08:48
@_author: Jerry Leichter 
@_subject: [Cryptography] WIPEONFORK in Linux 4.14 
None of these technologies existed, even in labs, at the time the VMS feature was developed.  At the time, disks were disks and writes went to where you put them - with, granted, rare exceptions for bad blocks (though for at least some of the disks of that era, bad blocks were handled in the driver, not in device firmware).
I have no idea what's happened to the feature in the intervening years.  Storage hardware today is, indeed, very hard to erase - to the extent that Apple dropped support for "secure erase" of disks in recent versions of its MacOS Disk Utility since it no longer ships any disks, just SSD's, and there's generally no way to securely erase them.
Going back to the main point, though:  Making secure erasure *the responsibility of the file system* gives you the best chance of ensuring it will be done in whatever way the media involved support.  User-level code - even aside from the problem that it's easy to forget to use it - is insulated by so many layers of abstraction that it's unlikely to continue to succeed as technologies evolve.  Granted, for some media, the only alternative may be file-specific encryption (which reduces, but doesn't eliminate, the problem:  You still need to ensure that when you erase the key, it's *really* gone).
(Beyond that, you could well assert that secure erase should be the responsibility of the firmware, or whatever is close enough to the raw hardware to know what's *really* doing on.  Which is true - but of course puts that responsibility in code you have no access to and no way to audit.)
                                                        -- Jerry

@_date: 2017-11-27 12:25:29
@_author: Jerry Leichter 
@_subject: [Cryptography] Is ASN.1 still the thing? 
One can easily find arguments going exactly the other way.  I happened to be looking at a book from a couple of years back called Bitter Java today.  It's in the "Antipatterns in XXX" vein, though not in that series.  An antipattern it describes is exactly what you recommend:  Using a transfer mechanism where things are "too tightly defined".  The example they give has a zip code field that's exactly 5 digits long (which breaks with the introduction of 9-digit zip codes); and of a "country code" represented as an enumerated type, which breaks when a new country comes into existence.  Changing these kinds of things is extremely difficult in a system composed of many components that evolve separately, often on very different schedules; and especially components owned by different authorities.
The fix they recommend is to use XML, because of its flexibility.  (Today, the recommendation would probably be JSON - mentioned in passing in the book, but fashions change.)
Of course, just exactly what a client that receives a country code it doesn't recognize is actually supposed to *do* about it is an interesting question.
There are some very difficult issues here.  We tend to love one-size-fits-all solutions, but sometimes this gives us solutions that fit all badly.
At one extreme, we have cryptographic protocols, whose bit representation need to be very tightly constrained, which need to leave no free play (unspecified bits), and which on the one hand need to be rigorously checked by the receiver; but on the other hand, that receiver had better not send back detailed error reports for fear of acting as an oracle.
At the other extreme, we have Internet protocols designed for large numbers of players to use, which are expected to change frequently in unpredictable ways, whose messages often should not be checked *too* closely by receivers for fear of failing to support "important" senders that "get some silly technicality wrong", and which should be very verbose and descriptive when they *do* report an error in the hopes that the sender can figure out what it needs to change.
And ... there are tons of things in between.
One of the things I find ironic in this whole discussion is the status of ASN.1 and its encodings.  On the one hand, ASN.1 is the product of an era when we thought rigorous definition of protocols was a necessity so that independent implementations could reliably communicate.  On the other, ASN.1 was also intended to support emerging styles of networked communications.  It's extremely general because of the perceived need to support all kinds of applications.  But ... no one at the time anticipated - or *could have* anticipated - what the Internet, the Web, Web services, and that whole ecosystem would really look like, and what the emergent properties of systems of that scale and organic growth, rather than explicit design and construction, would be.
So ASN.1 - and other designs of that era, like CORBA - end up in an uncomfortable middle ground:  They would be great for tightly-coupled, small-to-medium-sized systems that implement individual services in parallel and distributed pieces; but are more general than needed, hence appear way too complex.  Meanwhile they are a poor match to the very-large-scale, open-ended, organic "systems" of the greater Web.  (In the end, even XML faced the same problem for these systems:  Too complex to justify the benefits it claimed to offer.  Hence the victory of JSON.)
It's an old saw that every OS problem (and solution) was already present in Multics.  One could probably argue the similarly that every distributed system problem (and solution) was already present in CORBA.  And ... we're seeing explicit arguments on this thread that every problem (and solution) in serialization was already dealt with in ASN.1 and one or another of the xER's spawned to represent the data it describes.
But ... while historically fascinating and instructive, Multics isn't coming back.  CORBA isn't coming back, though it continues to be used in a some specialized, existing applications.  And ASN.1 and the xER's aren't coming back either.
                                                        -- Jerry

@_date: 2017-11-28 23:17:28
@_author: Jerry Leichter 
@_subject: [Cryptography] Intel Management Engine pwnd 
As I understand it, the ME's TCP stack is integrated into the Ethernet engine.  When it sees a packet attempting to connect to the management port it simply grabs it - and all the packets that are part of the resulting connection.  The implementation in the OS simply never sees those packets.  If you were to write code listening on that port, you'd probably never see an incoming connection.
This is really no different in most details from any stateful firewall - except that the firewall generally discards the undesirable packets, while the ME acts on them.
You could block this entire mechanism by using an external Ethernet interface and not connecting the built-in one to anything - or connecting it to a separate management network fully isolated from the Internet and carefully controlled.  But not many will choose to do that because of the additional cost and likely performance implications.
                                                        -- Jerry

@_date: 2017-10-16 14:16:45
@_author: Jerry Leichter 
@_subject: [Cryptography] Severe flaw in WPA2 protocol leaves Wi-Fi 
Instant summary:  It's possible (via a replay attack) to force a party to a WPA2 "reset its session" information - to the information it was already using.  This includes the key and the nonce and other initialization.  This is deadly, because the protocol uses AES as a stream cipher.
There are variations in particular configurations that make things even worse.
                                                        -- Jerry

@_date: 2017-10-19 09:43:32
@_author: Jerry Leichter 
@_subject: [Cryptography] Severe flaw in all generality : key or nonce 
There are modes that do this - going back to Rivest's package transforms - and there are fairly natural definitions of security that end up, after analysis, requiring it. The problem, of course, is that such a mode cannot be on-line:  You have to have the entire plaintext available before you can emit a single bit of ciphertext (and perhaps the other way around as well, though I don't immediately see an argument for why that must be so).  In a world of multi-GB/sec streams of data some of them tens of GB long this is not workable.
An alternative is to work in blocks of some fixed length, with the property that ever bit of block i of the ciphertext depends on every bit of blocks 0 ... i of the plaintext.  There are modes like that, too.
This is discussed in the introduction to                                                         -- Jerry

@_date: 2017-10-21 06:57:52
@_author: Jerry Leichter 
@_subject: [Cryptography] Severe flaw in all generality : key or nonce 
Careful here.  The point of the security definitions and modes is to provide strong *semantic* security over the message *as defined by the user of the system*.  You can't arbitrarily break the message into pieces and say the *pieces* meet the definition individually.  Take this to the limit - send one bit at a time.  You now have a simple stream cipher.
                                                        -- Jerry

@_date: 2017-09-05 19:29:17
@_author: Jerry Leichter 
@_subject: [Cryptography] Anti-counterfeiting microchip 
Hmm.  A couple of years back, when Intel proposed to add a unique ID to each chio there was a flurry of objection from privacy advocates.
Have we collectively completely changed our views on the hazards here?  Or will the objections come back as soon as someone starts to implement this?
                                                        -- Jerry

@_date: 2017-09-25 17:30:21
@_author: Jerry Leichter 
@_subject: [Cryptography] Altcoin volume 
Why doesn't Gresham's Law apply?
                                                        -- Jerry

@_date: 2018-04-01 11:06:28
@_author: Jerry Leichter 
@_subject: [Cryptography] Password entry protocols 
That was something a bit different.  Old IBM equipment was half-duplex, controlled entirely from the system side:  It filled the screen, then you filled in some data and sent it back.  This fit well with another property of these systems:  Their terminal interactions were block mode.  Rather than sending individual keystrokes, they sent a bunch of filled-in fields that the system had previous provided.  It was because of the block mode transmission that mainframes of the day - with tiny memory and very slow CPU's by today's standards - could handle tens or even hundreds of terminals.
The keyboard was normally locked/dead when the system wasn't expecting a block to be sent.
Sometimes, though, you need a way to initiate (or terminate) an action from the user side.  Hence, the ATTN key - which was always live, and simply asked for attention.  Login was a natural use, as it meant the system didn't need to have requests outstanding at every currently-unused terminal just in case someone wanted to log in.
Oh, there are plenty of them still around.  But in general mainframes today don't talk to "terminals"; hell, hardly anyone talks to "terminals" any more.  It's much more cost-effective to put a PC where you used to put a terminal - and the protocols are of course now entirely different.  (Though if you think about it, running a front end program on the PC is an expansion of what block mode used to be.)
But I'll bet you can still find old block-mode applications in use if you look hard enough.
Interesting; never heard of that.
APL\360 solved a related (but different) problem:  It had a kind of "raw mode" input which accepted every legal character and sent it to a user program.  IBM Selectrics (used as terminals) didn't have anything like "CTRL/C", or even an ATTN key.  So ... if you had a bug in a program that caused it to prompt in "raw mode" forever ... what to do?  The APL\360 character set had many characters created by over-striking - none of which I can reasonably demonstrate with the ASCII character set.  So they O  U  T as a character - except it was the only one that "raw mode" would *not* read.  Instead, it immediately suspended the currently executing code and popped out to the Read-Eval-Print loop that was APL's interface.  (From there you could discard the previously-running context.)
                                                        -- Jerry

@_date: 2018-04-01 11:19:42
@_author: Jerry Leichter 
@_subject: [Cryptography] Password entry protocols 
Are we talking about inherent architectural/design issues or implementation issues/bugs?
No system I've used in many years has, by design, allowed one program to grab control of terminal input from another program.  (An old, old exception would be RSX-11 - which by default was much like Unix with an implicit "&" at the end of *every* command line - but *without* job control.)
Output to the screen is more complicated because of the desire to write all kinds of programs to enhance the output of *other* programs, but the intent of all windowing systems these days is to make it pretty clear where one window ends and another begins, and the only way to write over a window ought to be to obscure it.
Yes, attacks like "clickjacking" exist, but if you look at them, they rely on the ability to fool the user by putting an *invisible* window over the window he thinks he's clicking on, not the ability to change the visible contents of some other window.  And the ability to do this kind of thing is universally seen as a security bug and is fixed.
Can you show an example of an attack of the sort you have in mind?  All kinds of things are possible "in general principle"; not so many are actually realizable.
                                                        -- Jerry

@_date: 2018-04-20 06:09:20
@_author: Jerry Leichter 
@_subject: [Cryptography] The Bob Morris worm 
Oh, no, you don't get it.  Mirai showed just how incredibly far we've come.
You see, the Morris worm broke into systems by logging into the accounts of actual human users, who were themselves often choosing passwords from these very short lists.
Mirai, on the other hand, depends on automatically-set default passwords, delivered in stand-alone smart devices by the manufacturers.  We've eliminated the human bad choice of passwords - it's all automated now.
*So* much more advanced!
                                                        -- Jerry :-)

@_date: 2018-08-04 07:41:31
@_author: Jerry Leichter 
@_subject: [Cryptography] Perfect Integrity? 
That depends on what security requirement you think "integrity" actually corresponds to.
I've been thinking about this from a slightly different point of view.  Suppose we fix the message size at n bits.  What's the strongest possible cipher on n bits with a k-bit key?  The answer is pretty clear:  From the space of all 1-1 functions from n bits to n bits, choose 2^k distinct functions f_i at random, numbering them from 0 to k-1.  To encrypt with key K, apply function f_K.
These are immensely strong (in a completely meaningless, from any practical point of view, sense) ciphers.  The limit, when k is n, includes all the one-time pads as a tiny fraction, where the chosen f_i are not just 1-1 but are actual Cartesian products of n 1-bit 1-1 functions (of which, of course, there are exactly two:  Identity and complement).
Speaking loosely, we describe block encryption functions as "looking like they were chosen at random" - but again it's not from the space of all 1-1 functions, but of those that are Cartesian products of 1-1 functions on 2^b bits, where b is the block size (and, of course, not even *all* such 1-1 functions).
The "strongest possible encryptions" are way stronger than they need to be because they focus on the wrong thing, the functions.  Perfect security for the encryption of an N bit message means exactly that given an encrypted message E, for any n-bit message P, the probability that a key sends P to E is the same across all keys.  One-time pads already achieve this, of course, so there's "no need to go further".
So what would the equivalent for authentication?  Authentication requires some additional redundant information along with the message.  Suppose in addition to the n bits of message, we send a bits of authentication.  The strongest possible such authentication is then produced by choosing, at random, a function Auth from the set of all functions from n bits to a bits; the pair (M,A) is then considered authentic if Auth(M) = A.
Clearly some choices of Auth are bad - e.g., constants.  As a start, let's restrict the space of functions from which we chose to include only onto functions - an nice dual to the restriction to 1-1 functions for encryption.
In the limit, when n=a, we then have just the permutations.
Now let's consider the strongest possible authentication requirement:  It should be "as difficult as possible" for an attacker to construct a pair (M,A) which will be seen as authentic.  (We can actually state this in a way that's similar to the condition for encryption, though it doesn't quite capture what's happening:  Given M, across all choices of possible Auth's, any A is equally likely.)
In this framework, Mr. Hallam-Baker's idea of sending along a copy second copy - i.e., setting a=n and Auth the identity - is insufficient:  It's trivial to forge any "genuine" message M as (M, Auth(M)) == (M, M)!
Unlike the case with encryption, where there's nothing to be gained by a key longer than the message, here a larger than n actually makes sense (though you need to replace "onto" with some appropriate condition):  If I construct a random (M, A) pair, the chances that it will appear authentic is 1/2^a, regardless of n.  (This was exactly (Matt Blaze's?) attack that showed the pointlessness of Clipper's key escrow mechanism.)
This kind of attacker is easier because the "existential forgery" attacker has much more freedom.  The corresponding attack against encryption would be the ability of an attacker to find P, E, and K such that E=Enc(P,K).  This isn't an interesting attack because K is the secret and we consider the encryption function public.  In the authentication case, Auth itself is the secret.
Now, if we look at *keyed* authentication, things are different - though it's interesting to note that a cryptosystem secure against that "existential attack" would inherently provide both secrecy and authentication.
                                                        -- Jerry

@_date: 2018-08-09 17:02:17
@_author: Jerry Leichter 
@_subject: [Cryptography] Perfect Integrity? 
I provided one possible definition - which is similar to the one you described - in a message a couple of days ago.  It seems to have dropped in to the bit bucket - if it wasn't received, I can re-send it.
                                                        -- Jerry

@_date: 2018-08-20 08:50:31
@_author: Jerry Leichter 
@_subject: [Cryptography] Rescuing Encrypt-then-Sig 
This has exactly the problem the original poster set out to solve:  You have to decrypt a message whose provenance you can't be sure of.
It's certainly true that *if the message was actually produced using E/S/E*, then the contents of the inner message are essentially random.  But that's hardly something an attacker who is going after a vulnerability in the decryption engine (like the recently described, though very old, problems in PGP) has to do....
                                                        -- Jerry

@_date: 2018-08-30 07:11:43
@_author: Jerry Leichter 
@_subject: [Cryptography] WireGuard 
WireGuard - white paper at  - is a new secure IP technology.  Perhaps the best quick summary is that it's IPSec with all the complexity drained out - to the point that the implementation (without the actual crypto) comes to about 4000 lines of code.
The paper talks about the Linux implementation.  There has since been a BSD implementation, which is also available - all this is open source - on MacOS.
The white paper reveals what appears to be really good and clever design and engineering.  Some of the basic principles are things we've discussed (and argued about) repeatedly here - e.g., *one* choice of crypto configuration, no "algorithm agility", no negotiation at startup.
I'm wondering if others here have looked at WireGuard and have any insight into the reality.
Metacomment:  We seem to be in a new phase for public cryptography.  The first phase was the pre-history, when crypto was available only from a few companies - especially IBM.  Then we had a burst of public standardization, from algorithms (AES) to protocols (SSL on the ad hoc side; IPSec on the de jure side).  The standards had two features:  In general, beyond some of the base algorithms, they were extremely complex and difficult to get right (in many cases, we now know or strongly suspect, due to "enemy action"); and for years they "froze the market":  It was difficult to get "approval" for any crypto not based on these standards - from government, from industry, and even in discussion groups like this one, where we've generally told people "don't try to roll your own, just use the established standards".
Over the last couple of years, this has started to change.  DJB (certainly not alone, but his name keeps showing up) with new algorithms and some new base protocols.  OTR was able to establish itself because there really was no "standard" competitor.  ssh has always been there in the background, but its notion of "endpoint continuity" for secure key exchange - as a replacement for the "standardized" certificate authority model - has seen increasing acceptance.  And now we are seeing WireGuard, which is actually built on a number of other "non-standard" components.
Computer technology goes through these kinds of cycles.  It was not so many years ago when it was "obvious" that certain things were fixed forever:  The Intel x86 ISP was the end of CPU evolution; C was the low-level language; Windows was the OS; VB was the high-level language; desktops were the form factor.  All those moments lost in time, like tears in rain.
Phillip Hallem-Baker:  It seems that your Mesh work may be arriving at the right point in history.  :-)
                                                        -- Jerry

@_date: 2018-12-09 19:35:59
@_author: Jerry Leichter 
@_subject: [Cryptography] What if Responsible Encryption Back-Doors Were 
At one time, I thought that the solution was limiting key lengths.  The idea would be to make small numbers of targeted attacks on encrypted messages possible, but make mass attacks impossibly expensive to fund; or, if funded, to hide.
I gave up on that idea long ago.  First of all, technology has just advanced way too fast.  Any number you pick for a reasonable cost for one decryption will, in a few years, buy you many thousands - and not long after that, millions.
But more to the point, the encryption genie is long out of the bottle.  Perhaps you can force the major vendors to include back doors.  But it's easy to add another layer of uncontrolled encryption inside whatever envelope the majors give you.  So LE goes to the vendor, orders the stuff decrypted - and finds stuff encrypted with an algorithm they can't break.  Granted, most people won't bother with the second level - but it's exactly the ones you're most concerned with who will.
The whole thing is security theater - and the worst kind of security theater, in that not only doesn't it buy you any additional security, it destroys what was there.
Australia, for better or worse, is about to demonstrate to the whole world the futility of trying to make unbreakable encryption magically go away.
                                                        -- Jerry

@_date: 2018-12-28 11:01:06
@_author: Jerry Leichter 
@_subject: [Cryptography] Amongst the requirements for digests... 
There are simpler "deranged" hash functions.  For example, you could take:
H_Leaky(x) =  First k bits of x || SHA2(x)
You can modify this to pre-pend any fixed-length boolean function of x instead.  (Granted, this has "the wrong work factor" as it's only as hard as SHA2, while it "should be" 2^k times harder.  But the requirement of minimality - i.e., that an n-bit hash output should have difficulty 2^n for pre-image resistance (2^(n/2) for second) isn't in the explicit requirements anywhere - and in fact we accept cryptographic functions that are "a few bits shy" of the theoretical maximum.
The conditions of first and second pre-image resistance relate to the *cryptographically strong* part of the notion of *cryptographically strong hash function*.  Historically we don't write down a definition for the "hash function" part - which is where SHA2-Yuk and H_Leaky fail.  I'd guess there's a good definition out there, but it's not obvious.  People often try "changing any one bit, on average, changes half the bits of the output", but I don't think that works, though a counterexample isn't clear to me.
                                                        -- Jerry

@_date: 2018-02-10 15:32:20
@_author: Jerry Leichter 
@_subject: [Cryptography] RISC-V branch predicting 
Yes ... and no.
The Javascript is *intended* to be in a separate security domain from the rest of the browser.  The browser tries to maintain the separation in software.  Spectre proves once again - this has happened repeatedly, for decades - that pure software enforcement of security boundaries is very problematic.
The nature of the breaks - this one is typical - is an indication of why this remains a fool's quest.  All the fancy proof techniques we've developed, all the language invariants - all *assume the underlying model of the hardware that they build upon actually corresponds to the real hardware on which the resulting code runs*.  What Spectre illustrates, yet again, is that real hardware isn't the same as hardware models - and *any* variance has the potential to destroy your alleged proofs.
Is something like memory access mode protection, enforced by the hardware, really different?  Well ... perhaps not in principle, as Meltdown demonstrates.  But at the hardware layer, the interfaces and assumptions are much simpler (well, I'm not sure you could describe anything about x86 *hardware* security at this point as "simple" - which has lead to an entirely different set of problems), so there's a greater chance of getting it right.  Further, if you're analyzing and defining properties at the level of hardware, you have an easier time representing and getting a handle on hardware-level properties, like precise timings and cache interactions, something that's typically completely missing from the hardware models on which software security proofs are based.
The whole notion that you can safely execute hostile code within a single hardware security domain is one we need to get away from.  You want to run someone else's Javascript?  Run it in a separate address space and process.  We have years of experience in protecting separate OS processes and address spaces from each other.  Yes, there are periodically new bugs - in fact, periodically, new *classes* of bugs, like Meltdown - but we've done much better at keeping those under control than we have on pure software isolation.  It turns out that the Unix approach - in which process creation is assumed to be very inexpensive - is probably better than the approach of other OS's, where processes are more expensive to create, thus longer-lived and more likely to be subdivided into software-enforced security domains.  If even Unix processes are too expensive - which will likely be the response of browser makers to the notion that each individual piece of Javascript should be segmented off into its own process - then perhaps we should look at hardware and software models of very cheap hardware isolation.
                                                        -- Jerry

@_date: 2018-02-12 15:10:37
@_author: Jerry Leichter 
@_subject: [Cryptography] Spectre again (was Re: RISC-V branch predicting) 
That's not "cheating".  eBPF is *exactly* a mechanism to execute user-written code *in kernel mode*.
The original "BPF" was restricted enough that there are no (known! - or perhaps I should say "published") attacks based on it.  As always seem to be the case, since BPF was a "success" and "didn't cause any security problems", it got extended to eBPF - which granted enough power that it opened the door to Spectre.
This is not Spectre crossing hardware privilege domains.  I haven't seen any examples of such attacks, though they may well exist.  However, if they are found they can be mitigated much more cheaply than same-mode Spectre attacks, because the number of mode-crossings is way smaller than the number of instructions executed within a given mode.
                                                        -- Jerry

@_date: 2018-01-05 06:34:13
@_author: Jerry Leichter 
@_subject: [Cryptography] Speculation re Intel HW cockup; 
Let's follow this thought through all the way.
For the last 75 years, a driving force in the computer design business has been:  Computer hardware is expensive, we can't afford to dedicate a machine entirely to one person/program; let's find a way to share the expensive machine among multiple simultaneous users.  Once we did that, of course, we bought into the problem of keeping those uses safely isolated from each other.  The solutions have gotten more and more complex, but the promised land of "effective sharing with complete isolation" hasn't moved - and we show no signs of having reached it.
Perhaps the solution is to avoid the root cause.  Imagine a system with a "physical hypervisor".  It has a large number of *completely isolated* cores:  Each with its own caches, branch predictors, and paths to memory.  The "hypervisor" assigns *physical* cores to processes, not logical/virtual ones.  Sharing of resources is serial, with a complete hardware reset between assignment of processors to different processes.  We're getting to the point where something like this would perhaps be practical for high-security domains.
A sort-of-a datapoint:  Apple has a "secure enclave" in its ARM chips that's supposed to be inaccessible to any normal code.  Of course, it's really just a logically partitioned off portion of the chip itself, sharing resources with the rest of the chip.  Attacks on these kinds of implementations have appeared in the past.  But consider the newest iMac Pro.  This has an Intel chip that runs all the normal user code, including the OS; and a completely separate ARM chip running various support functions, including all kinds of security related functions.  No user code gets in there, ever.  Obviously, this will have its own problems and vulnerabilities - but it may be a path to the future. (Then again, it may point to the deep past:  As I discuss this, the image that comes back to me is of the CDC "super-computers" of the 1960's and 1970's:  User code in the "CPU"; OS/control code in completely separate "PPU"'s.  Forward into the past?)
This doesn't deal with all problems, of course.  The current issues have been rendered much more serious because we also persist in related illusion:  That we can allow someone potentially hostile to run code within our own security domain safely if we restrict the language he can use.  The same pattern shows up *twice* in these attacks:  The language starts out really restrictive, but as it appears "safe" it grows and eventually becomes powerful enough to allow big attacks.  One example is eBPF, allowing anyone to run "safe" code in kernel context - initially, in BPF, with very limited capabilities; now, in eBPF, with enough power to make use of these resource sharing bugs to leak kernel data at high rates.  And, of course, there's Javascript - the product of an evolution from simple display (HTML) to full Turing-equivalent programming language which "we can safely isolate".  We've been burned on *that* front since at least the 1960's, when Burroughs built machines in which the only user-accessible programming interfaces were in "safe" Algol variants.  "Safe", until they weren't.  (The published attack on these systems was to take a tape with an executable on it to a system written by someone else which allowed diddling directly with the binary and changing the verified executable to "break the rules".  Side channels have always been there!)
When you're at the bottom of a hole, the first thing to do is to stop digging.  If particular approaches have failed so consistently for so long ... it's time to start looking elsewhere.
                                                        -- Jerry

@_date: 2018-01-06 18:41:19
@_author: Jerry Leichter 
@_subject: [Cryptography] Speculation considered harmful? 
John Levine already pointed out the root of the problem - and the right solution:  Speculated code must run *in exactly the same way as non-speculated code*.  In particular, a speculated path needs to stop immediately if it attempts a forbidden memory access.  There's absolutely no point in continuing down this path, as it can't possibly be committed in any case:  It will terminate at this point with a memory access exception.
If the inaccessible data is never read into the cache (because the request to load it never makes it to memory), there's nothing to forget.  If other side channels might leak information about that data ... they would leak it along non-speculative paths as well - which would obviously be an ever more serious bug!
Reading this data into the cache has to go through the same mapping hardware whether it's during speculation or not.  The same access information is potentially available.  The access has to be checked before the speculative path is committed anyway.
It's not clear to me exactly what gets saved in the hardware design by handling memory accesses in speculative mode this way.  Maybe the idea is to allow speculation across mode changes - a potential speedup, though a very dangerous one.
The design of the speculation mechanism has to carefully list all state that may be modified along the speculative path so that it gets discarded if the speculative path is discarded.  Clearly the contents of cache was not considered to be part of this state because it can't be read except by going through the access checks.  But these attacks show that was false reasoning:  Timing analysis *does* let unprivileged code effectively read the contents of the cache.  If you get the resources you need to protect wrong, your whole protection regime crumbles....
                                                        -- Jerry

@_date: 2018-01-09 14:42:55
@_author: Jerry Leichter 
@_subject: [Cryptography] Speculation considered harmful? 
I would say that Spectre is fundamentally not soluble within our current system designs. Meltdown is a way for one security domain known to the hardware to access information from a distinct security domain that it should not have access to.  It's a failure of the hardware to fully maintain the properties it promises for the separation of security domains.
Spectre involves code within one hardware security domain gaining access to information *within the same security domain*.  There *are* two security domains at play here - but they are enforced only by software.  No matter how many times we get hit over the head by attacks that get around software-only security enforcement, we keep convincing ourselves that *this time is different*.  Yes, Javascript has grown from a fairly simple scripting language to a Turing-complete monstrosity with access to all kinds of low-level resources.  But we can stop Javascript code (from doing nasty things) any time we want.
We could stop the Javascript attacks by aligning the software security domains with the hardware-enforced security domains:  Continue the splitting up of browser responsibility in order to run each untrusted piece of Javascript code in its own process, carefully ensuring that there is no sensitive information anywhere within that process's address space.  The performance implications - not to mention the likely functionality implications - are hard to know without trying, but if you want safety from Javascript-based attacks, you need to *strongly* sandbox all untrusted JS code.  Let's stop pretending that there's any safe way to grant attackers such powerful access inside of our own security domain.
There are likely other places where untrusted code runs within the same security domain as trusted code.  In fact, the entire sandboxing effort arises from the recognition that the old days, where the code I ran was code I could trust, so there was no point in limiting what it could do, are long gone and are not coming back.
Perhaps it would be useful to make the kinds of hardware mechanisms that OS's use to enforce the separation of OS-defined security domains from each other inside of processes, accessible to normal user code.  The VAX had four privilege levels, and sort of kind of did this, but in the end it was very limited and not in any way accessible to ordinary user code.  Capability-based systems are the ultimate development along these lines, but represent a radical departure from current system designs.  Perhaps we can get some of their power without giving up all compatibility with existing code.
                                                        -- Jerry

@_date: 2018-01-09 14:59:32
@_author: Jerry Leichter 
@_subject: [Cryptography] [FORGED] Re:  Speculation considered harmful? 
There's more to it than that.
Multiflow knew from the beginning that its compiler would be very slow, and would need feedback from multiple runs with real data to help it make the correct decisions.  But the theory was that this was fine for scientific code that would be run hundreds, maybe thousands, of times, unchanged, once fully optimized.
But it turned out that there isn't quite enough code like that - or at least not enough to support machines that required such a radically different development cycle to really show their strength.
There have been other efforts to use feedback to drive better levels of optimization.  HP's C++ compiler (for their Precision RISC machines) had a mode that did that - perhaps driven by the technology they acquired from Multiflow.  I remember trying it.  Compilations took *forever* - sometimes literally, as the compilation sometimes showed every sign of never terminating - and the gain in performance was nothing particularly notable.
The payoff from feedback-driven compilation was never worth the disruption of development and build methodologies, and faded away - to re-appear, years later, in JIT compilers, which finally made the process invisible and non-disruptive.  (I haven't seen any solid evidence on how much feedback actually helps in producing faster/smaller code.  The only specific example I know of in Java, for example, has to do with optimizing virtual dispatch when it appears that the run-time type is fixed, or close to fixed.  Probably significant but a rather limited case - and given branch prediction, it may actually gain almost nothing.)
Meanwhile, moving all this stuff into hardware that discovers what's happening fully on the fly usually can't do as well as a *theoretical* feedback-driven compiler *with good feedback*, but (a) it comes pretty close; (b) it can actually do better in some cases; (c) the biggest plus:  It doesn't require changes to languages, libraries, development methodologies, what have you.
                                                        -- Jerry

@_date: 2018-01-09 15:54:12
@_author: Jerry Leichter 
@_subject: [Cryptography] Own on install. How grave it is? 
It depends on what you install.  For example, the only version of iOS you can actually install is the most recent.  (Well, after a new update, the previous version may also be installable, typically for couple of weeks - though Apple tends to shorten the period when the new version fixes serious bugs.)
In MacOS, you can generally get something with most of the recent patches, though no always.  Again, I think Apple tend to cut a new master when serious security bugs have been fixed.
If the low-level code to download the updates had vulnerabilities, you'd be screwed - but I've seen no reports about that happening.
Apple is actually moving to (so far, optionally) a setup in which *only* the most recent version (or an earlier "approved" version) can be installed.  (So far only the new iMac Pro can actually enforce this, but it's clear that this is the future.)
In any case, the issue is not a significant one for Apple OS's.  (Let's not get into the question of whether it's *appropriate* for Apple to outright *forbid* downgrading.  It's what they do on iOS, for better or worse - and it does avoid the "known attack" problem, as well as downgrade attacks.)
These days, antivirus programs arguably introduce more vulnerabilities than they avoid!
I don't know of any smartphone where there are *any* "installation media" in the traditional sense.  Off hand, I can't think of any wireless routers either:  They come with some version of their software and all further updating is on line.
If the problem is:  To re-initialize a device, I'm forced to install an older, insecure OS - which may well be attacked before I can upgrade it:  That *used to be* a severe problem with Windows (someone found, with some old version of Windows, that if you installed from CD and then connected to the internet to get updates, you'd be infected within half an hour at most - while the updates took several hours to install).  I doubt this is an issue any more, though I don't keep up with the Windows world.
The big problem with many devices - and certainly many IoT devices - is that they have *no* mechanism for updating, not that they may be compromised before you can updated them.
                                                        -- Jerry

@_date: 2018-01-09 21:44:52
@_author: Jerry Leichter 
@_subject: [Cryptography] Speculation considered harmful? 
Cross-process attacks could be fixed at the hardware (or OS) level by, for example, flushing the branch history on process switch.  (There's no direct hardware support for this right now so OS's are getting the same effect by executing a bunch of chosen branches - which do nothing directly useful - during process switch.)  Since process switches are relatively infrequent, the extra cost - even with the current hack-arounds - should be acceptable.
In-process mitigation uses the same ideas with the compiler generating the extra branches.  On its face, this is much more expensive because it's in the code that the process normally runs - likely frequently.  But there seem to be clever ways to accomplish it that have relatively small overhead.  The hardware, though - beyond providing direct ways to flush the branch buffers and such - can't help much here.
                                                        -- Jerry

@_date: 2018-01-10 06:35:23
@_author: Jerry Leichter 
@_subject: [Cryptography] Speculation considered harmful? 
has a nice description of some of the hackery needed to mitigate Spectre.  My reaction to it is: Very clever; looks like it should work against the published attack; isn't this yet another example of "Oh, *this* time we plugged all the holes?".
BTW, note this down at the end:
    Recommendations For App Developers
    Spectre means that secrets in the same address space as untrusted JavaScript are
    more vulnerable than ever before. Based on this, we recommend:
          protects your app by running untrusted JavaScript in another process.
Yup.  Let the hardware/OS help.  Of course that doesn't guarantee *they* get it right, but relying on in-process, same-mode trickery has failed repeatedly, and will fail again.
                                                        -- Jerry

@_date: 2018-01-12 16:22:09
@_author: Jerry Leichter 
@_subject: [Cryptography] Caches considered harmful 
It depends on what kinds of attacks you're talking about.
If you're talking about a mass attack that knows nothing about the individual cars, but, e.g., you set up a situation where they will all make the same decision putting them all on the same bypass - which is then promptly overloaded - then you don't need *probabilistic* algorithms:  You need *variations within the algorithms*.  So each car might find the best 5 alternatives (all perhaps reaching the same decisions), but then choose one of them based on the car's serial number.  Perfectly predictable, given the additional information.
Of course, if the attack you're talking about assumes the attacker knows all the serial numbers and can tailor the attack to herd all the cars together, or something of that sort, you're in a different domain.
Note that having variation among instances is basic to biological systems.  Since the species as a whole doesn't "know" the exact environments in which its members will need to survive, fielding a bunch of variations, each optimized for a slightly different environment, is the best approach.  This is particularly obvious in various "settings" that influence reactions to disease organisms.
                                                        -- Jerry

@_date: 2018-01-17 11:29:49
@_author: Jerry Leichter 
@_subject: [Cryptography] canonicalizing unicode strings. 
... and of course then you mix digits with letters from some script - which is common but not universal, as some scripts have their own representations, which they may or may not use depending on context.
Everything about natural languages is more complicated than you think.  The particular languages and representations you're familiar with - no matter how cosmopolitan you are - are pretty certain *not* to cover all the variations out there.
                                                        -- Jerry

@_date: 2018-01-25 14:23:05
@_author: Jerry Leichter 
@_subject: [Cryptography] =?utf-8?b?IkRlYXRoIG5vdGljZTogTW9vcmXigJlzIExhdy4g?= 
"Done in by the weaponisation of optimisation, and now 2017 may be as good as it ever got"
                                                        -- Jerry

@_date: 2018-01-26 20:50:06
@_author: Jerry Leichter 
@_subject: [Cryptography] I'll give the right answers to the right 
I worked on intelligent network management software back in those days.  Solaris SPARC was our favorite platform.  (We supported many.)  But Niagra - well, they were then the T series processors I believe - were a disaster.  Customers kept asking us to run our code on them, as they were much cheaper than traditional high-end SPARC boxes - and if you just looked at the total specs the T series were way more powerful.  Our software had tons of threads - a basic server started up with 80 or more of them - but they were used for structuring, not for parallelism.  It was hard to get the effective parallelism above maybe 4.  So on a T series performance was disastrous.  (It didn't help that early T series boxes used FP units that were shared - something like on FP unit for every four processors.  We didn't do a huge amount of FP computation - but we did enough this bottleneck made already bad things much worse.)
                                                        -- Jerry

@_date: 2018-07-21 00:44:49
@_author: Jerry Leichter 
@_subject: [Cryptography] "Public Accountability vs. Secret Laws: Can They 
Fields of inquiry define their own standards for what makes an acceptable paper.  Mathematics long ago separated itself from practical applications.  (Though there have been some partial exceptions.  During the Soviet era, there was a push to show that work could actually be applied.  I recall once seeing a translation of a Russian text on, I think, functional analysis.  This is highly theoretical stuff - and most of the book was as well.  But then the last couple of chapters applied some of the theory to engineering analyses of concrete shells.  How widespread this kind of thing was, I don't know - but there was some really good mathematics done in the SU in those days.)
There's a whole field of "applied mathematics" (which overlaps with "numerical analysis" in uncertain ways) in which work is expected to have practical application - though of course sometimes that's a stretch anyway.  (I know of at least one respected practitioner who refuses to call himself an "applied mathematician" because he thinks most of the work done under that rubric is neither usefully applied nor good mathematics.)
Cryptography as a field started off heavy on the practical applications - but any field with a strong mathematical component will inevitably feel a pull toward abstraction which will result in papers judged on some mathematical notion of elegance rather than practical application.  The same occurs, of course, throughout theoretical computer science.
Two stories, both from many years back:
1.  I knew a bunch of guys working on functional languages.  They were working on some algorithm for a compiler - I think some kind of type resolution - and were very happy one day to announce that they'd gotten the algorithm down to only doubly exponential time complexity.
2.  At some STOC or FOCS, there was a "rump session" talk announced with the title "Application of VLSI Theory to VLSI Practice".  The speaker came up, put up a slide with that title, stood for a bit, moved to his next slide - which was completely blank; stood for a bit more; then bowed, thanked the audience and sat down.  Much laughter and applause ensued.
                                                        -- Jerry

@_date: 2018-07-28 07:05:44
@_author: Jerry Leichter 
@_subject: [Cryptography] Maybe those million-bit-key cryptosystems have 
A common theme of amateurish or over-hyped cryptosystems - at least a couple of years back; this seems to have faded - was the use of super-long keys for "higher security".  Realistically, all a longer key gives you is security against brute force attack, and 256 bits is already way beyond anything that can be attacked by any foreseeable technology.  And yet....
Consider  (contains link to the technical paper) which describes a Spectre-like attack that can be carried out over a network, without downloading any code to the system under attack.  It's one of a class of slow side-channel attacks - very slow; 1-3 bits per hour attacking a system in the Google Cloud over the Internet.  (They got it up to a bit per minute over a local network.)
There are very few secrets worth stealing at these rates.  Except, of course, for keys.  Keys serve as information concentrators:  Leak 256 bits and in the right circumstances you've effectively leaked Gigabytes.
A workaround would be to stretch a 256-bit key in memory however long you want it - e.g., store each bit as the parity of a(n almost) random 64-bit value.  But to use any standard AES implementation, you need to synthesize the 256-bit form - and it can then be leaked.  An implementation that pulled in one bit at a time might be possible, though likely impossibly slow.
                                                        -- Jerry
                                                  With at least half a :-)

@_date: 2018-07-31 20:42:23
@_author: Jerry Leichter 
@_subject: [Cryptography] Perfect Integrity? 
============================== START ==============================
A discussion in a separate thread caused me to wonder about the following:  A one-time pad provides provably perfect security (for an appropriate definition of security, but that's not that hard to come by).
Is there an accepted definition, and algorithm implementing it, for perfect integrity?  I don't recall seeing any work of this sort.
Just as a one-time pad isn't really practical except in very limited situations, I would expect a perfect integrity solution to be similarly of limited practical use.
                                                        -- Jerry

@_date: 2018-06-01 11:26:16
@_author: Jerry Leichter 
@_subject: [Cryptography] Security weakness in iCloud keychain 
Nice description.  Only one - no so much disagreement as shading:
It would be bad practice to store the two kinds of secrets in the same place.  Checking the time-based password is exactly the kind of thing that should be done in dedicated hardware (something like a key store); or if you want to get close to that, in an isolated machine or VM providing TOTP validation as a service.  This kind of thing is very cheap these days.
There's an additional level that could be implemented (though I don't know of anyone who's done it):  Have the token sign the one-time password.  The token has the signing key; the server has only the verification key.  Now there's no shared secret at the server to be stolen!  (Actually, a better protocol would be to have the client send the signed pair .)  The downside of this approach is that the one-time password will necessarily be much longer than is typical - today it's usually 4-6 digits - so this isn't practical when the user is expect to type in the OTP, a common mode of operation for retro-fitted systems.
                                                        -- Jerry

@_date: 2018-06-04 20:27:54
@_author: Jerry Leichter 
@_subject: [Cryptography] Odd bit of security advice 
In another thread, Jason Cooper quoted the following:
[1]  "The most important security consideration in implementing this
  document is the uniqueness of the nonce used in ChaCha20.  Counters
  and LFSRs are both acceptable ways of generating unique nonces, as is
  encrypting a counter using a 64-bit cipher such as DES.  Note that it
  is not acceptable to use a truncation of a counter encrypted with a
  128-bit or 256-bit cipher, because such a truncation may repeat after
  a short time."
It's that last comment - that the bottom 64 bits of a counter encrypted with a 128- or 256-bit cipher may repeat after a short time - that really puzzles me.  Surely any 128- or 256-bit cipher with this property would immediately fail certification, as it would be easily distinguishable from a random permutation.
What am I missing?
                                                        -- Jerry

@_date: 2018-06-06 06:51:45
@_author: Jerry Leichter 
@_subject: [Cryptography] Apple Group FaceTime and end-to-end and encryption 
Apple's Facetime uses end-to-end encryption, with Apple's servers unable to decrypt the message streams.  Facetime has also only supported one-to-one connections, for which it's been criticized.  The explanation I had heard - and have given - is that effective group communication requires fairly sophisticated *mixing* of the data streams from all the participants in a group connection.  At the least, you need to balance the different audio streams to a consistent volume level, and typically you want to do some kind of focus, in which the user providing the loudest audio is emphasized somewhat over the balance of the others.  But you also want to suppress noise if possible.  This requires quite a bit of computation, and has traditionally been the domain of the server.
Video is typically not mixed, but streams are selected and composited.
But ... to do this sort of stuff in the server, the server needs access to the actual audio and video, not just the encrypted streams.
Now Apple has announced that the next release of iOS will support group connections with up to 32 members.  I'm wondering "how they plan to do that".
There are two components to the problem:
1.  Doing the necessary computations.  Recent iPhones certainly have the necessary compute power to handle mixing of 32 audio streams.  They also have GPU's that should be able to handle the compositing and other video processing.
2.  But ... you can only do computation *on data you actually receive*.  Sending all 31 audio streams to all phones at all times seems plausible.  Thirty one full video streams seems unrealistic.
I can imagine various tricks in which phones send servers some basic metadata - e.g., average loudness and both a full and a low-data-rate video stream.  The server could then pick the loudest audio streams and forward high-data-rate video for that one and low-data-rate video for the rest.  That does, of course, leak some metadata to the server.  (Then again, given that the audio almost certainly uses some kind of delta encoding, the rate of the encrypted audio is probably already leaking information about the rate of change of the audio, and louder audio is typically more changeable so could probably already be identified.)
Anyone know anything yet about how Apple is doing this?  (One would think they would have patented the techniques needed....)
                                                        -- Jerry

@_date: 2018-06-19 13:39:44
@_author: Jerry Leichter 
@_subject: [Cryptography] How to make rowhammer less likely 
Rowhammer is a targeted attack against a specific memory technology - and even, if you dig into it, against particular hardware implementations of that specific memory technology.  It seem highly unlikely that *this particular attack* would work against a very different memory technology like STT-MRAM or Optane.
For the matter, it's very unlikely that it will continue to work against next-generation memory chips which will be designed specifically to resist it.
Of course, saying "rowhammer won't work against XYZ" should absolutely not be taken as equivalent to "XYZ is secure".  Now that the seed has been planted, attackers will be looking for other attacks against other technologies.  Some will perhaps be similar; others very different but inspired by our emerging appreciation that our reliance on hardware based on its external specs, when operating in "typical" environments, has to stop.  Attacks for the last couple of years have increasing gone "down the technology stack", piercing the abstractions that we've built our trust upon.  Defenses will have to do the same.
                                                        -- Jerry

@_date: 2018-06-20 11:15:58
@_author: Jerry Leichter 
@_subject: [Cryptography] Ah, the wonders of complexity 
We have to build on the abstractions handed to us by, e.g., our hardware designers.  When those abstractions are too complex, no one can understand the implications.
The following description is from CVE-2018-8897 (  It reminds me, in a general way, of classic security bugs going back to the 1960's, e.g., a Tenex bug in which a user-written page fault handler could turn an O(N^k) search for the characters in a k-character password into an O(Nk) search by using page faults as an oracle for individual matching characters.
"A statement in the System Programming Guide of the Intel 64 and IA-32 Architectures Software Developer's Manual (SDM) was mishandled in the development of some or all operating-system kernels, resulting in unexpected behavior for  exceptions that are deferred by MOV SS or POP SS, as demonstrated by (for example) privilege escalation in Windows, macOS, some Xen configurations, or FreeBSD, or a Linux kernel crash. The MOV to SS and POP SS instructions inhibit interrupts (including NMIs), data breakpoints, and single step trap exceptions until the instruction boundary following the next instruction (SDM Vol. 3A; section 6.8.3). (The inhibited data breakpoints are those on memory accessed by the MOV to SS or POP to SS instruction itself.) Note that debug exceptions are not inhibited by the interrupt enable (EFLAGS.IF) system flag (SDM Vol. 3A; section 2.3). If the instruction following the MOV to SS or POP to SS instruction is an instruction like SYSCALL, SYSENTER, INT 3, etc. that transfers control to the operating system at CPL < 3, the debug exception is delivered after the transfer to CPL < 3 is complete. OS kernels may not expect this order of events and may therefore experience unexpected behavior when it occurs."
                                                        -- Jerry

@_date: 2018-03-21 19:44:28
@_author: Jerry Leichter 
@_subject: [Cryptography] Avoiding PGP 
Hmm.  So now you've contributed *your* idea of how this "should" behave.  :-)
And yet most mail today uses some of those features, so by leaving them all out you're limiting the scope of your "secure mailer" to a rather small set of people.  Not that the problem isn't real!
The takeaway from your comment, though:  If there's anything you *must* have before you implement a security-relevant piece of code, it's a careful definition of *what security actually means for that piece of code*.  And it appears, that for "secure email", we really don't.  What we have is essentially "I want it to do everything regular email does - but, you know, securely".  The fact that pretty much all the implementations we have out there are plugins or extensions to existing mail programs makes this "security definition" very hard to improve upon.
It's interesting that secure messaging has developed very differently.  There are formal definitions of what it means for a messaging application to be secure, and as far as I know all the secure messaging apps were written ab initio - no one tried to graft security on top of existing apps or even existing messaging protocols.
Now, there are multiple real-world reasons for this distinction, ranging from what appears to be a more tractable problem (no long-term storage, no need to support things like search) to history (email standards have been there for decades and there's an immense "open" installed based; previous messaging applications, while they followed some standards, were often proprietary and incompatible, so the notion of a new messaging app that couldn't interact with existing ones didn't bother people much).
But ... the evidence of many years of experience is that a usable, useful, secure (with an acceptable definition) encrypted email system that would be widely adopted is highly unlikely to *ever* emerge.
Well, maybe during The Year of Linux on the Desktop.  :-)
                                                        -- Jerry

@_date: 2018-03-24 09:49:18
@_author: Jerry Leichter 
@_subject: [Cryptography] Does RISC V solve Spectre ? 
You're forgetting the joke "back derivation" or the acronym RISC:  Relegate Important Stuff to Compilers.
The idea pretty much failed in the market - and not just in the market of money, where the x86 won for extrinsic reasons.  If you look at the idea as expressed in the earliest RISC designs - the IBM 801, which started the whole "RISC revolution", and the MIPS, which stood for Microprocessor without Interlocked Pipeline Stages - they really did rely on the compiler to do "unnatural" things to accommodate the hardware, like dealing with the load delay slot on the MIPS.  But designs that grew from them - the IBM Power, which is not at all a RISC according to the original design criteria; and the MIPS II, which eliminated the load delay slot - all trended in the direction of putting more of the stuff back into hardware.  (SPARC could never get rid of its branch shadow because that had visible semantic effects; whether it actually *helped* much in later incarnations, I have no idea.)  A kind of side-evolution, mentioned by others, was VLIW machines, which also left all sorts of stuff to compilers.  They didn't do well either.
Exactly why would make for an interesting dissertation.  I suspect a big part, however, was that if you expose all the innards and corners of the hardware to the compiler, you end up having to compile separately for each instantiation (or you need to control the details of instantiations so tightly that the hardware guys choke).  The VLIW guys explicitly acknowledged this from the beginning, and chose to attack an area (large-scale scientific software, then the realm of traditional "super-computers") where they thought it wouldn't matter.  But even there, as it turned out, it did.
If you look at late RISC designs, like the Alpha, the real "Relegate" part settled on areas like control of details of paging.  This put it squarely into the area of the OS, which always had to deal with variations among instantiations anyway.
Of course, with the rise of JIT compilation, perhaps this is no longer an issue.  It would make for an interesting experiment.  The JIT compilers I'm aware of do little to accommodate hardware variations - though perhaps they feel they don't need to, because those details don't much matter in the machines and for most code we actually use today.
But, OK, enough history:  Suppose there were a "load speculative" instruction available to the compiler.  *Would it actually help* to block the attacks in question?
The answer to that is, of course, depends on where compiler issues them.  At one extreme, it *never* issues them and the code is safe but slow.   At the other extreme, where it issues them based on actual projected *possible* need for the data - it's really doing nothing more "secure" against these attacks than the hardware itself.  In fact, it arguably makes the job of an attacker easier, as one can now look at the code and tell, without guessing about details of internal hardware prediction models, and completely deterministically, exactly where data of interest might get loaded into the caches.
So I guess the theory is:  The compiler is smart enough to know where "sensitive" data is present, and *not* request speculative loading of such data.  Exactly how it determines that ... who knows.  But even if that problem is solved:  The only way you can hope to get significant performance out of speculative loading is if *the vast majority of data* can be safely speculatively loaded.  Otherwise, you're doing work that rarely actually gains you anything - so why do it at all.
But if *most* loads can be speculated safely; and we assume we can tell the difference at compile time; doesn't it make more sense to have an instruction to *disable* speculative loads in the rare locations where appropriate than one you have to insert in every piece of code - significantly decreasing instruction density and hence pressure on the instruction loading path, already a bottleneck?
And indeed, *that's exactly what the hardware designers are doing*:  Adding ways to force non-speculation where appropriate.
                                                        -- Jerry

@_date: 2018-03-31 17:55:59
@_author: Jerry Leichter 
@_subject: [Cryptography] Password entry protocols 
You're leaving one thing out:  Computers don't just randomly ask you to type in your password.  In a well-designed system, password requests are *responses to user actions*, such as connecting to a web site.
Back in the old days, we worried about spoofed system login pages.  If you think about it, this is in fact one of the few circumstances where the user *doesn't* take an explicit action to initiate the sequence:  He sits down at the keyboard, but that in and of itself is not an action that triggers anything; the login prompt is already there.  So the classic fix - the "secure attention keystroke", which is guaranteed to take you to the actual login program - if used appropriately, does convert a login request to a response to a particular sequence of actions.  It's interesting, though, that among commonly used systems, only Windows retains this feature.
There are other equivalent sequences of actions that may be a greater threat.  For example, in MacOS, you can get an unsolicited popup telling you that some piece of code (Java is a common case) needs to be updated.  *Most* such prompts come when you actually start the code in question (e.g., Microsoft's updater for MacOS works that way); but the Java prompt comes when *it's* ready, regardless of what you are doing.  Agreeing to the download is fine in and of itself - but along the way, you'll be asked for your admin password.  You're expecting this, because you initiated the update - except that you didn't *really*, you responded to a prompt and believed it was what it claimed.  I've never heard of an attack using this mechanism but it seems quite plausible.
I'm not sure what attack you're describing here.
For browsers, it's been suggested there be a mechanism to prompt for passwords which would create a window with some kind of distinctive border decoration that could not be mimicked from code running in the browser.  MacOS has something vaguely like this, in which password prompts "fold out" of the browser window chrome.  Whether one could simulate this effect well enough to fool people, I don't know.  In any case, most web sites do their own password prompting.  No matter how good the mechanism, it helps no one if it isn't used.
Extending this kind of thing to arbitrary programs seems very difficult.  Perhaps we need something akin to a Secure Attention key that you can hit in a password prompt which will tell you exactly who's asking for the information.  Of course ... you have to get people to use it.
                                                        -- Jerry

@_date: 2018-05-02 22:48:29
@_author: Jerry Leichter 
@_subject: [Cryptography] Security weakness in iCloud keychain 
The only way they would get into iCloud that I know is if, at some point, they were uploaded from some other device.  While it's possible there's a bug that causes uploading even when you don't explicitly enable it, it's also possible that at some point in the past you had it turned on.
That's really bizarre.
I actually *do* have iCloud synchronization for passwords turned on, but I get something very different - and consistent with the documentation:  The only passwords on the device are the ones saved for specific web sites.  I have tons of others, and they don't show up.
Apple has a whole document describing this.  What it comes down to is that when the passwords are on one of your devices, they are encrypted using a key derived from the device password and a long device-unique value; that value cannot be exported from the device.  When stored in Apple's servers, they are stored encrypted with a key derived from the iCloud password and a unique value stored in the server, which cannot be exported from the server.  (Both the server and each device have an internal key manager implemented in hardware designed to keep the unique key within them secure.)  So *at the moment you are uploading or downloading a password*, Apple does indeed have access to it; but "at rest", it doesn't.
New to me.  It seems to me that something odd happened during the history of your devices.  Definitely a bug, but what *kind* of bug, and who it might affect, isn't clear.  Apple accepts reports of security bugs.  There's a description at  - read past the stuff at the top about *getting* security patches.
                                                        -- Jerry

@_date: 2018-05-09 17:59:13
@_author: Jerry Leichter 
@_subject: [Cryptography] secure authentication ... as opposed to passwords 
They have, of course, learned the lessons we've taught them well.  Every website wants to have its own login screen, configured the way it likes.  There is no expectation that any one can legitimately have other than "if it wants a username and password I should hand them over".
But there's actually an opportunity here, if we were to choose to seize it.  Imagine that the major browser makers coordinated on the following steps:
1.  Define a standard mechanism by which servers could ask for authentication information.  How and what form would be delivered would be specified in the request.  To ease adoption, a returned username and password would be available; but better methods would be included:  PAKE, some kind of challenge/response - not a long list of possibilities, but other *good* methods that we would hope to evolve to.
2.  Define a standard UI to be used for authentication requests.  The elements of the UI should be common across browsers and OS's, even if small details vary.  The UI must be independent of language - it mast be clear to someone familiar with the standard that they are looking at an authentication prompt even if they can't read the textual fields within it.  (They might not be able to determine what to type where if the language is different enough from anything they are familiar with, but they'll still know what, in broad terms, the prompt is about.)  There might need to be a small-screen version of the UI for phones and such.  Obviously, whatever the standard form is, it must be clearly distinct from anything that could be generated through HTML, Javascript, what have you.  There should be extensive testing with humans to make sure this really is distinctive.  An interesting and difficult challenge is figuring out how to do this for users who have visual impairments or other difficulties with standard UI's.  Designing this UI would make for a very interesting international contest.
3.  Get the new standards out there in browsers, and start to encourage web sites to use them.
4.  Browsers are already pretty good at spotting username/password prompts.  They have to be - and producers of web pages make it easy for them - to allow autofill of authentication information.  Use the same mechanisms, over time, to start warning users that the site they are on is using an obsolete and dangerous method for requesting sensitive information.  The warnings wouldn't start appearing until the new standard was broadly available, and would initially be restrained; but over time they become more strident and disruptive.  The precedent here is the warnings for self-signed certificates and Chrome's upcoming warnings about certificates that are not registered for certificate transparency.  (Note that one can argue whether those uses of such warnings are appropriate.  That's a separate issue from the fact that they've been used and, over time, have forced changes in behavior all over the Web.)
5.  An even broader step would be for Google (and other search providers - but this is the kind of thing Google has spoken about more than others) to start down-rating sites whose pages contained old-style authentication prompts.
6.  The goal is to make the new-style authentication so common that anything that doesn't use it will stand out as "wrong".  Only that way can we get away from the current user bias to enter usernames and passwords wherever they are asked for.
This is a way forward.  Whether we (collectively) choose to take it remains to be seen.
                                                        -- Jerry

@_date: 2018-05-16 12:01:02
@_author: Jerry Leichter 
@_subject: [Cryptography] Attacks on PGP (and allegedly S/MIME) 
If you look at this more closely, there's a fundamental violation of basic cryptographic principles involved here.  Think of the old red (unencrypted "secret" world)/black (encrypted "sanitized" world) distinction.  It was in the past often implemented as actual physical separation:  "Red" material could only be accessed within a secure, isolated facility.  All that could flow in and out of that facility was "black" material.
In these attacks, we take black material (the encrypted messages); send them into the secure facility (the PGP or S/MIME implementation); convert it to red material (the decrypted messages); *and then allow arbitrary red-material dependent messages to be sent out of the the secure facility to the outside world*.  This relies on the "exfiltration mechanisms" that the paper refers to - and getting a URL followed is just one such mechanism - they identify others.
If the "secure facility" were properly isolated, the attacks couldn't work.
There's another, broader issue involved here as well:  Improper design and implementation of the underlying protocols.  A least some of the attacks depend on the fact that we have multiple interacting message syntaxes - MIME encapsulation of different message parts; PGP marking of message boundaries; HTML bracketing - which are not appropriately parsed.  It makes no logical sense for an HTML element to cross over two MIME message parts, but the two layers are handled independently and nothing enforces proper syntax.  It's all very reminiscent of the many attacks on protocols that use length prefixes but don't check, for example, that an inner length isn't larger then the remaining length of the next out element.
The individual layers here have reasonably well-defined syntactic structures (well, HTML is always problematic - but at least at the level of individual elements, it's pretty simple); but they aren't so much combined as all thrown into a blender, with the hope that something sane emerges.  Well, surprise surprise, it doesn't.
                                                        -- Jerry

@_date: 2018-05-28 16:29:29
@_author: Jerry Leichter 
@_subject: [Cryptography] Quicksand 
Interesting and scary analysis of the "state of play" for Javascript code in the wild.  Quick summary:  Pretty much every page out there relies on a variety of libraries, but almost no one keeps up to date with the (frequent) security patches those libraries need.  And the library maintainers don't even bother to publicize the security updates they make, so keeping up to date is extremely difficult.  Since it's difficult, no one does it.  Lather, rinse, repeat.
"Thou Shalt Not Depend on Me:  A look at JavaScript libraries in the wild"
                                                        -- Jerry

@_date: 2018-11-22 07:27:32
@_author: Jerry Leichter 
@_subject: [Cryptography] Buffer Overflows & Spectre 
I think it's a bit more subtle than that.
A number of assumptions combined to get us where we are:
1.  "Hardware is expensive; we can't afford to give everyone his own computer".  This is what led us to invent timesharing.
2.  Since I have to share the hardware, I need to protect different entities from each other.
3.  I can make the protection between entities good enough that I can share it between actively hostile entities.
4a.  Since I can make the protection good enough that I can share the hardware safely, I can build a business model in which I rent out "slices" of my hardware to arbitrary customers (initially time-shared hosts, these days Cloud machines).
4b.  Since I can make the protection good enough, I can take a personal machine dedicated only to me and let arbitrary others run code on it - the Web/Java Applet/Javascript model.
In parallel - and these you can find explicitly written down in the early literature:
A.  There's no practical way to close down all covert channels.
B.  If we slow the covert channels down enough (perhaps a few bits/second), they don't matter because individual bits aren't that important - an attacker can't get enough of them to be a threat.
X.  We can safely do cryptography on a general-purpose, shared computer.  Note that back when DES was first coming into use, the NSA refused to accept this assumption.
The problem today is that assumptions 3 has turned out not to be quite true, because assumption X has rendered assumption B false:  Pretty much all the concern about side channels centers on leaking keys, which pack huge value into a very small number of bits.  Oh, I'm sure you can construct examples of other "highly valuable" bits, but they tend to be specialized and particular to individual programs, and often only at particular times.  Stealing encryption keys, though, is the "killer app" of side channel attacks. There are two root assumptions to these chains:  Hardware is expensive so must be shared; crypto can be safely done on a general-purpose shared computer.  Neither of these is true - the first due to Moore's law, the second ... by observation.  And we're just beginning to see the first hints of changes in approach by companies that at least "talk the talk" (we can argue forever about the degree to which they actually "walk the walk") of security and privacy:
.  Apple has moved some of the basic encryption operations off the main CPU into their T2 chips where nothing but their own code runs;
.  At the other extreme, Apple has eliminated most browser extensions in Safari, even though their extension architecture always provided much more limited access than some other versions out there; and they've begun to choke off certain accesses that Javascript/HTML have (e.g., they deliberately lie in response to certain queries about capabilities to make machine fingerprinting harder).  This isn't much, but it's among the first mainstream moves that say "OK, there's a Javascript standard which was designed to make advertisers happy, we don't see why our own customers should fulfill their requirements."
.  Oracle has an interesting little marketing line about their Cloud architecture:  Why should you run your vendor's software on your machine?  So they propose that the machine they rent you has none of their database code on it - the database runs off in a separate server (with none of *your* code in it), and the two communicate only over a link.  (There's some discussion of related stuff at   Whether this is real or not isn't as important as that we're beginning to talk about it seriously.
Given the realities of today's hardware, one could imagine a "Cloud" in which you don't rent a VM:  You rent an actual piece of hardware (so-called "bare metal"), which runs your code and your code only.  (There are "public Kubernetes container services" that are approaching something along these lines.)  When you're ready to relinquish that hardware, you do a hardware reset back to a fixed state.  Making such a thing real requires re-thinking of the hardware organization:  Rather than individual "bigger, faster" chips that share hardware resources and then mainly get divided down into small VM's anyway, you want smaller, cheaper individual chips sharing as little as possible.  Maybe - it's a direction we haven't explored - though in some sense, that might be what "edge computing" (the latest buzzword) will be about.  (Again, Apple is arguably already there:  Its bias is to put huge processing capabilities in each iPhone and do as much as possible in each phone, rather than in the Cloud - the exact opposite of Google, which plays to its own strength in the Cloud by providing tightly integrated off-loading of processing.)
The emerging hardware/software world of 10 years from now may look very different frame what we've been accustomed to.  This level of change is difficult; there's a huge amount of inertia to overcome.  But something will have to give.
                                                        -- Jerry

@_date: 2018-11-22 12:10:48
@_author: Jerry Leichter 
@_subject: [Cryptography] Buffer Overflows & Spectre 
The same you tell if any vendor is lying:  You put auditing requirements along with big legal penalties in the contract.  Nothing new here, for sophisticated buyers.  Sure, rubes will get taken - but there's nothing new there either.
BTW, there are a number of tricks to distinguish between "running in a VM" and "running on raw hardware".  The point of a hypervisor is to be able to run programs of interest, not to fool programs that specifically want to check.  In fact, OS's that run as VM guests tend to have special support for it - it's easier to make small modifications to the OS than to put major work into the hardware and hypervisor.  (Simple example:  OS's tend to have built-in expectations about how long the hardware will take to respond to some simple requests; they react to the hardware taking "too long" by assuming the hardware is hung, and force a reboot in an attempt to clear the problem.  A VM may be delayed for arbitrarily long by the hypervisor, so these timeouts cause crashes. OS's have been taught to ignore this particular "hardware problem" when running as a guest OS.)
                                                        -- Jerry

@_date: 2018-11-25 06:47:57
@_author: Jerry Leichter 
@_subject: [Cryptography] Hohha Protocol : 1. Key renewal review 
It provides neither, in the usual use cases for key renewal or forward security.
You have to think about the attack models that both of these are supposed to be protecting against.  The second is easier:  Suppose an attacker gains control of some endpoint and is thus able to extract K1 and K2.  What forward security is supposed to guarantee is that even given this information, the attacker cannot go back and decrypt previously recorded messages.
But the date is public information.  Assuming he wasn't so dumb (or somehow crippled in his attack) that he didn't record the time the messages were sent, he can now go back and apply the same algorithm to compute all the earlier shared secrets - and hence read all the previous messages.
The usual response is to use more bits of the time than just the year and date, but there's no help there:  Even if you assume that somehow the endpoints can synchronize to more bits of the time than the attacker, there can't be enough more accurate to matter.  Let's take and extreme case.  Suppose the endpoints have atomic clocks that can synchronize to the nanosecond *and* they know the distance between themselves to less than a foot (speed of light is on the order of a foot a nanosecond).  And the attacker for some bizarre reason can only record time to the nearest second.  So the maximum error the attacker has to consider is 2 seconds.  That gives him 2 billion guesses for the time field to try - effectively a 31 bit key.  Not useful for serious cryptography in this day and age.
As for key renewal:  Well, there are two reasons to do key renewal.  One is concern that the key was somehow compromised by some completely unknown means.  But if the session key was compromised by completely unknown means ... who's to say that K1 and K2 weren't compromised as well?  In which case, we're back to the forward security case.
The second, more common reason splits into two equivalent ones:  The session key leaks in a way we *assume* doesn't provide any access to K1 and K2; or the encryption algorithm's security depends on not have more than N blocks encrypted with the same key, and we're getting too close to N.  If you use the week number, you're going to recompute the same shared key for up to 7 days.  With modern crypto algorithms and link speeds, that's going to be *way* longer than the time typically assumed to reach the critical N blocks (usually a couple of hours; sometimes less); and if you have some reason to suspect the key leaked on Monday ... are you really going to wait 6 days to rekey?
Again, you can use a more precise notion of time ... which will probably work well for these cases.  But they work just as well with only a single shared secret.
Oh, and another thing to consider:  You have "two shared secrets" K1 and K2.  But how is that different from having a single shared secret:
        (length of K1) || K1 || K2
where the || is concatenation?  This in and of itself should tell you that you haven't actually changed anything by describing your algorithm in terms of two secrets....
                                                        -- Jerry

@_date: 2018-09-03 07:16:28
@_author: Jerry Leichter 
@_subject: [Cryptography] WireGuard 
I think much of the confusion is based on not identifying the problem to be solved.
The whole mechanism of CA's attempts to solve a very specific problem:  Establishing a secure channel between parties that have never communicated before.  Or, drilling down a bit more, ensuring the identify of a counterparty you've never communicated with before.
We all know the limitations of a key agreement protocol like (anonymous) DH:  Alice can establish a channel to Bob that's secure against anyone else, but without some way to identify Bob as the party at the other end, that's not a very useful thing to do.
The whole issue in SSL came down to:  I want to connect to this Amazon company I've heard about, but how do I know it's *the* Amazon company and not someone faking it?  Well ... Amazon gives me a certificate attesting that it is indeed Amazon.  And I trust that certificate because a CA certifies that it's valid.
In fact, if you look at the other side of the connection, it works pretty much the same way.  Amazon is willing to send me goods in return for my presenting a "certificate" in the form of a name and credit card number, which Amazon trusts because some CC clearing agency house certifies that it's valid.
Could this really have been otherwise?  All remote business transactions work in exactly this way.  They always have:  Someone already trusted acts as an introducer.  The only alternative is direct one-to-one contact and barter.  (How is actual cash different from a counterfeit other than in having a "signature" from the government....)
How did you know, when you went into a real-world Macy's, that it really *was* a Macy's?  Well, building a whole store to look like Macy's would be very expensive - and someone would notice and Macy's would come after the fake.  So it wasn't worth doing - most of the time.  (There were fake Apple Store's, selling fake Apple goods, in China a number of years back.  In some cases, not even the employees in the stores knew that the store was not, in fact, associated with Apple.)
You can distribute the introducer using something like blockchain - though if you think about it, you're still relying on whoever put the blockchain entry saying "you can get to Amazon with this key" - and you really don't know who that might be.  What a public attestation like this does is allow Amazon to notice if someone is acting in its stead.  Enter Certificate Transparency.
I've previously suggested that since you must inherently trust your browser maker - if he spikes your code, it doesn't matter how you get keys - so it would make sense for browser makers to also be "CA"'s - though if they do that they could as easily hard-code in actual keys.  That's not how the Web evolved - for many reasons - and is unlikely now.
Once an initial contact and exchange of identities has occurred, sure, you can exchange secrets and on the next contact be assured that you're talking to "whoever it was I trusted to be Amazon the last time we spoke".  Different problem, different (potential) solutions.
Where one might criticize SSL/TLS is its reliance on certificates *exclusively*.  The second time I contact Amazon, I might well prefer to rely on my records from the first time.  Why bring in a CA?  If you drill down into this, all kinds of interesting issues emerge.  Yes, it requires both me and Amazon to keep state around.  In fact, Amazon does want me to have an account, and it's keeping a whole bunch of state around - but it in some sense doesn't trust that state:  Even if it has my CC information, it will check it again the next time I try to buy something.  And every browser since the very earliest ones has kept around a memory of sites to which I've connected in the past.  All modern ones can also keep around the authentication information (account name and password) so that I can authenticate to Amazon - but, because SSL/TLS relies on certs exclusively, *not* the information needed for Amazon to authenticate to me.  Curious, when you look at it that way.
BTW, I run into this issue in a real way in my day job.  I work on software that runs within datacenters.  Within a datacenter, most systems don't even have DNS entries - they just have 10.* addresses.  Customers want connections between their systems to be "secure", which in policy terms ends up requiring TLS.  How do you set up TLS connections between systems with no DNS entry and a 10.* address?  Who would give you a certificate for such a thing?  (If you can find someone, they shouldn't be in the business!)
In practice, everyone uses self-signed certificates.  This is getting more and more interesting "on the edge", where browsers are making it increasingly annoying and difficult to connect to a server using a self-signed certificate....
SSH-like protocols give you the raw data needed to solve the first connection problem - the fingerprint of the remote party - but leave all the rest to you.  In practice, no one checks - the problem is "solved" by ignoring it.  Has this led to attacks?  Certainly, though I don't recall any being reported or discussed publicly.  The whole thing is viewed through the lens of "caveat emptor" - "I have only myself to blame" for not checking.
Are SSH-like protocols more or less secure than CA-based protocols?  It depends.  An SSH-like protocol requires trust in some kind of external (perhaps third party) verification on first connection.  The trust extended (and perhaps breached) at that point covers all subsequent communications.  The SSL/TLS model requires trust in the entire certificate system for each connection, forever.  Which is "more secure"?
                                                        -- Jerry

@_date: 2018-09-05 06:45:19
@_author: Jerry Leichter 
@_subject: [Cryptography] WireGuard 
The real question isn't whether all these things are potentially useful, it's whether people actually do use them.  The takeup of IPSec is pretty small.
There was a potential really interesting use case just a little while back:  When you're using containers, you generally run them on an overlay network that only the containers have access to.  There was an IPsec implementation for Docker which made that overlay network secure completely transparently - kind of the perfect use case.  Unfortunately, for reasons having nothing to do with IPsec - the overlay network implementation had all kinds of problems - it was very unreliable.  Meanwhile, Kubernetes seems to have taken on the mantel of "the way containers are done" - and no one seems to be interested in implementing an IPsec overlay network for it, at least the last time I looked.  What instead seems to win - what *always* seems to win - is TLS-based VPN's.  Five years (more?) ago, you could actually find IPsec-based VPN's.  Those faded, too (though, again, not so much because of IPsec itself as that the use of a separate protocols rather than TCP or UDP lead to endless problems with cheap routers).
The market has spoken, and the demand seems to be for Remote Access VPN's built on top of, not beside, TCP/UDP.  WireGuard addresses that market.  Whether it will succeed in displacing existing VPN's, I have no idea - but IPsec's additional features are not going to help it, any more than they have in the last two decades.
                                                        -- Jerry

@_date: 2018-09-18 11:45:33
@_author: Jerry Leichter 
@_subject: [Cryptography] "The Women Code Breakers Who Unmasked Soviet Spies" 
"In 1995, when Venona was declassified, the public face of the project was male. The most celebrated name was that of a man, Meredith Gardner, a linguist who deciphered names and words, working closely with FBI agent Robert J. Lamphere. But in the cryptanalytic unitwhere the tough analytic math was done, where the messages were prepared and matched, where the breakthroughs happened, where the numbers were so painstakingly strippedthe face of Venona was different: Most of the people working on it were women, says Robert L. Benson, a retired historian for the National Security Agency."
                                                        -- Jerry

@_date: 2019-08-16 09:07:27
@_author: Jerry Leichter 
@_subject: [Cryptography] Well, that only took ten years 
No matter how good your CA, no matter what you do to secure the whole CA system, in the end your security is much more fundamentally dependent on your browser maker.  And there are way fewer browser makers out there than CA's.
It's all very nice that many of the browsers are partly, or even entirely, open source, but the fraction of the population who builds browsers from source is vanishingly small.  And even among that crew, browsers are immense, and immensely complicated, supporting tons of protocols and full of all kinds of strange "compatibility" hacks.  Every browser out there gets broken into regularly - and that's for attacks through the surface they intentionally open to the outside world, and thus intentionally strengthen as much as they can.  Stuff hidden in the implementation that leaks your keys under the right conditions?  It could stay there for years with no one noticing.
Browser makers ship an absurdly long list of "trusted CA's" because ... a browser without *some* list of "trusted CA's" is worthless.  The makers are effectively vouching for the CA's by including them - though they are very, very careful to disclaim any responsibility here, which is why they end up accepting pretty much anyone who claims to be a CA.
There's a common mistake in reasoning about security composition:  I don't want to rely on my browser maker for both the code and the certs - it's safer to split the responsibilities.  Splitting security, however, only works that way if  the multiple "splits" must cooperate to attack you.  Here, each can independently attack you.  You're more vulnerable than before.
In fact, as I've discussed on this list before, it would make sense to do away with the whole CA infrastructure (for most uses) by simply having the browser makers ship the actual public keys for, say, the top 100,000 sites.  Even taking the most expensive approach and assuming 4Kb (512B) RSA keys and doubling for overhead, that's 100MB of data, less the size of a typical web page these days!
                                                        -- Jerry

@_date: 2019-08-16 09:18:09
@_author: Jerry Leichter 
@_subject: [Cryptography] Well, that only took ten years 
Ah, but EV certificates *do* have value; and in fact the GlobalSign employee is perfectly correct:  If their customers didn't see that value, they wouldn't pay.
The error is in assuming that how *you* assign value matches how the *GlobalSign customer* assigns value.  You are looking at the value as coming from greater security.  That value is so close to $0.00 that it's not worth thinking about.
The GlobalSign customer, on the other hand, has, or believes he has (based on industry chatter), evidence that presenting a EV certificate on his eCommerce Web site increases sales by x%.  On top of which, his security auditor is telling him this is an industry standard and his rating will be cut if he doesn't get an EV certificate.  Or ... he just wants to keep his manager or his board off his back (and continuing to give him nice bonuses) by showing he's doing "the right thing."  All of these have economic value, usually way beyond the cost of an EV certificate.  And that value is every bit as real as any other value that people find in the things they pay for.
                                                        -- Jerry

@_date: 2019-08-24 20:50:54
@_author: Jerry Leichter 
@_subject: [Cryptography] "Entropy as a Service: A New Resource for Secure 
OK, this one has me puzzled.  I can't figure out if they are talking about better entropy generators running within individual machines, or some kind of centralized entropy generation service (secured how?) or ... what, exactly.
I guess everything the becomes a buzzword is someone's business opportunity....
                                                        -- Jerry

@_date: 2019-12-09 12:24:41
@_author: Jerry Leichter 
@_subject: [Cryptography] "[CVE-2019-14899] Inferring and hijacking 
"We have discovered a vulnerability in Linux, FreeBSD, OpenBSD, MacOS,
iOS, and Android which allows a malicious access point, or an adjacent
user, to determine if a connected user is using a VPN, make positive
inferences about the websites they are visiting, and determine the
correct sequence and acknowledgement numbers in use, allowing the bad
actor to inject data into the TCP stream. This provides everything that
is needed for an attacker to hijack active connections inside the VPN
tunnel."  This is a neat attack.  It's based partly on data outside the encryption in TCP - e.g., sequence numbers; partly on using the lengths of encrypted messages to turn a remote party into an oracle.  It works against all of TLS, WireGuard, and IKEv2/IPSEC - the details of the encryption itself are unimportant.
I've commented before on this list that when we talk about semantic security, but neglect message length leakage, we are opening ourselves up to attacks.  One basic feature of these attacks is that while the cryptography remains, in some sense, completely secure, its interaction with particular protocols makes the combination unsafe.  (Classic example:  Vocoded/compressed speech encrypted with a packet-length-revealing encryption mode allows pretty accurate guesses about the underlying phonemes simply based on the length of successive packets.)  Not only is it hard to make security assertions about the composition of security mechanisms - it can be hard to make such assertions about the composition of security mechanisms with apparently non-security-sensitive layers.
It's probably time, in most contexts, to stop worrying about saving bits in network messages.  Most networks today are fast enough and have enough capacity that rounding everything up to 1K or 2K packets won't have any noticeable effect.  These numbers are *total guesses*.  It would be useful to understand the actual data leakage rate for different packet sizes for different data sources - e.g., digital audio, video, Web sites.  We thought that modern cryptographic techniques would get us away from the old world where the safety was in the crypto, regardless of any characteristics of the underlying data.  Well ... it appears to be time to admit that's not true.
BTW, if you're going to fill out a large block with random noise ... maybe an encryption mode natural to that block size makes sense.  Such things have been built in the past, but have fallen out of favor.
What exactly to do in cases where every bit still *is* important - particularly for low power devices - is an interesting question.
                                                        -- Jerry

@_date: 2019-01-25 13:52:44
@_author: Jerry Leichter 
@_subject: [Cryptography] Stupid question on S-boxes 
Responding more directly to Henry's comment:  Small S-boxes, if the code is properly arranged, can stay entirely in the cache - you can access every entry in the table up front to get them all in there, for example - so are less likely to leak information through cache timing attacks.  Large S-boxes, on the other hand, are more likely to get partially loaded/knocked out of the cache, giving more purchase to cache timing attacks.
Then again, it's not just S-boxes and it's not just caches.  TLBleed instead uses the TLB to grab EdDSA keys.  I'd say the ability to safely do crypto on shared hardware is very much an open question at this point.  Completely isolated co-processors - whether fixed-algorithm (now fairly common) or loadable (I don't know enough about the internals of Apple's T2 - it does crypto in such a co-processor, but whether the crypto algorithms it runs are hard-wired or in replaceable firmware I don't know) - may be the only way forward.
                                                        -- Jerry

@_date: 2019-01-25 18:20:52
@_author: Jerry Leichter 
@_subject: [Cryptography] Stupid question on S-boxes 
You wouldn't share the co-processor:  At any one time, it should only be accessible to a single security context.  And you'd reset it to a constant state between security context switches.
The issue here is side-channel attacks.  If there are no channels between the crypto processing and code controlled by attackers, there is no attack against the crypto processing.  The problem, of course, is that "channels" is extremely open-ended.  But a co-processor with its own private memory does limit the possible attacks.  Of the ones that have already been discovered, we could look at differential timing attacks (which we've pretty much learned to handle) and differential power analysis, which can be dealt by careful hardware design if nothing else.
That's not so say someone won't find another "channel" to attack, but at least all the ones we already know about are either irrelevant, or can be made very difficult to exploit.
                                                        -- Jerry

@_date: 2019-01-25 20:13:42
@_author: Jerry Leichter 
@_subject: [Cryptography] Stupid question on S-boxes 
There is an irony to this:  Allegedly when DES was first proposed, the NSA was skeptical of software implementations of cryptographic algorithms.  In fact, they influenced DES to make it harder to implement in software (the initial and final permutations).
Of course, without the ability to use this stuff in software, the public development of cryptography would have been completely stunted.  So the NSA clearly had other motives for pushing for hardware-only implementations.
We of course don't know exactly NSA does these days, though it is interesting that the FIPS standards for cryptography, for example, are clearly written with an eye to hardware implementations.
While the crypto hackers may not want to admit it, we appear at least for the reasonably foreseeable future to be at the end of the road for practical asymmetric cryptographic algorithm development:  Nothing is likely to supersede AES in widespread practical use.  We're probably converging on SHA2, with a gradual move to SHA3, for hash functions.  Which makes putting those directly into hardware sensible.  There's a lot of paranoia around trusting the hardware implementations, though if you do a careful analysis of realistic attack models, you're probably safer using the hardware implementations than relying on software - especially when you're using shared infrastructure (if, as you point out, security on shared infrastructure is a particularly meaningful concept anyway - though economics keeps pushing us toward it).
An interesting question I haven't seen specifically attacked:  Are there usable side-channel attacks against software random number generators?  (Particularly the algorithms actually in use in modern systems.)  These have seen many algorithmic attacks and defenses, but I don't recall anything like, say, a DPA attack against the stirring algorithms.
                                                        -- Jerry

@_date: 2019-01-29 11:22:51
@_author: Jerry Leichter 
@_subject: [Cryptography] Introducing the world's worst hash function 
An interesting bit of history:  VMS needed a way to introduce short delays at certain points (especially in drivers).  The clock's resolution was way too coarse for this purpose; what was needed was a delay loop.  But since VMS would run on a variety of processors - including processors that might not exist when the code was compiled - it needed to adjust dynamically.
So during boot time, some code was run that timed very large numbers of iterations of a simple loop, enough that you could actually get a useful timing value from the available clocks.  From that, the number of iterations needed for the loop to reach the desired interval was computed.  Then the code being loaded was actually searched for markers pre-inserted to show where a delay loop was needed, and the necessary code was patched in.
Today we might have problems with a patch-at-runtime approach as it's inconsistent (without a lot of extra work) with verified boot mechanisms.  But it was an elegant approach, back in the day!
                                                        -- Jerry

@_date: 2019-07-05 07:13:25
@_author: Jerry Leichter 
@_subject: [Cryptography] graph theory and sybil attack 
Two comments:
- Finding groups of nodes that are strongly-connected to each other but weakly connected to the rest of the graph is a generalization of the clique-finding problem (finding maximal groups of nodes that are fully connected to each other).  Finding cliques is NP-complete.  This very strongly suggests that the problem you suggest is as well.
- It's not clear that this algorithm really solves the problem you set out to solve!  There are going to be a lot of naturally-occurring, legitimate groups of nodes in a network of buyers.  Consider fans of a particular not very widely known musician in a network that sells recordings and allows reviews of songs and shows.  Can you distinguish a group of legitimate fans from an artificial group trying to make the musician look more popular?
                                                        -- Jerry

@_date: 2019-07-11 06:17:47
@_author: Jerry Leichter 
@_subject: [Cryptography] graph theory and sybil attack 
However, some NP-complete problems are not approximable - a celebrated result a number of years back.  Unfortunately, the clique problem is one of them.  (  has a discussion if you really want to get into the details).
Only experiment will determine if this is really true, and to what degree - but my guess is you'll have plenty of false positives.  In the end this will be a ROC problem - you have to trade off among sensitivity, specificity, and so on.  Since we're talking about an active opponent here, not random noise ... expect this to be very hard.
                                                        -- Jerry

@_date: 2019-07-22 06:33:07
@_author: Jerry Leichter 
@_subject: [Cryptography] Our leader opines on cryptocurrencies 
(Getting far away from crypto...)  The interesting thing is how little the tradeoffs are talked about.
The USD use as the global currency was a deliberate decision as part of the creation of the modern international economic system.  It provided - and continues to provide - huge benefits for the world as a whole by introducing a measure of stability into the global economic system:  Pricing agreements in dollars removes one element of price risk.  (There's always a risk relative to your own currency, but at least you don't have to worry about your counterparty's currency at the same time.)
On the one hand, this grants the US immense power over the entire system, because in the end it and it alone controls the printing of dollars.  On the other, the whole system only works as long as the US forgoes use of that power - or exercises it in the direction of stability rather than its own gain.  For the most part, things worked that way.  The US permitted - indeed, encouraged - what could be described as the growth of economic competition to its own post-WW II supremacy, first in Europe, later in Asia.  It could instead have worked to suppress both.  (Certainly there are times and places where it did so, but overall ... imagine where the rest of the world would be if the US had maintained a mercantilist policy.)
Our current Dear Leader is trying to move in exactly a mercantilist/America First direction - and one of the first reactions is the first stirrings of a move toward the possibility of other international currencies and banking systems that are not so heavily under US control.  The Euro is certainly a quickly-available alternative.  I'm sure the Chinese have all kinds of long-term plans for the renminbi.  Certain elements of our political system see this as a positive - getting the rest of the world to "stop taking advantage of us."  They may get their wish - and discover that the new system they create - controlled either by a consortium that takes forever to make any decisions, or a much more decisive power that sees itself as the US's natural successor - isn't quite what they asked for.
Note that no one is suggesting, say, the ruble as the next international standard currency.  To play this role, people have to have faith in the economy behind the currency.  It has to be large enough not to be readily manipulable in the markets.
Much of this is a confidence game - not in the negative sense, but in the need for confidence that the stability of the selected currency can be, and will be, maintained.  Does anyone really see Facebook and a new "currency" out of nowhere gaining that kind of confidence?
                                                        -- Jerry

@_date: 2019-06-10 23:12:12
@_author: Jerry Leichter 
@_subject: [Cryptography] Should the demarcation point for a viable 
You've jumped between two different kinds of attacks:  A "viable" attack and a "distinguishing" attack.
It's reasonably clear what a "distinguishing" attack is - it can readily be formalized as the ability of a machine of some sort gaining an epsilon advantage at guessing which of a pair of oracles - one implementing a random permutation, one implementing the encryption - is the random one.  Fill in details as you like; they don't change things by very much.  This is the weakest possible criterion for judging an encryption algorithm as vulnerable:  Any other attack that succeeds in and of itself provides a distinguishing attack.
A "viable" attack, on the other hand, has no clear meaning.  Is an attack that reduces the complexity of AES to 126 bits "viable"?  It would certainly be very interesting, but if "viable" has something to do with "of practical significance", it's hard to come up with situations where there's any practical difference between 128 and 126 bits.  It's not even clear where to draw the line.  How about 120 bits?  Hard to say. A factor of 256 is real - and if we're talking about a difference between 1 year and 256 years, it can be very significant.  But how about a difference between 1 hour and 256 hours?  That hardly seems worth discussing.
Saying "anything better then brute force is a viable attack - regardless of how *little* better" is likely to eliminate every algorithm out there.  I'm not sure we actually use that as the demarcation line.  Oh, we great attacks that achieve it with interest - but we need more detail about *how much better* before we really worry about the system.
There's tons of mathematics behind cryptography, but there are also ultimately practical tradeoffs to make.  "Viability" seems to get at those tradeoffs.
As for birthday bounds and rekeying ... now we're off into something entirely different.  We're no longer even talking about attacks on the crypto algorithm as such.  All that matters is the block size.  A random permutation on the same size blocks would reveal as much, or as little, about the cleartext.
It's really not clear where you are trying to go with this.
                                                        -- Jerry

@_date: 2019-06-19 22:16:48
@_author: Jerry Leichter 
@_subject: [Cryptography] =?utf-8?q?Shamir=E2=80=99s_secret_sharing?= 
Shamir Secret Sharing is information-theoretically secure:  For n total shares required, given n-1 of the shares, for any possible secret, there is a unique n'th share that produces exactly that secret.  That is, knowing n-1 shares gives you no information at all about the secret.  Quantum computation doesn't help in recovering what isn't there to begin with.  There's no difference in "resistance" regardless of n and t.
                                                        -- Jerry

@_date: 2019-11-26 17:45:54
@_author: Jerry Leichter 
@_subject: [Cryptography] WebSocket masking under TLS? 
You have to look at the details of the attack.  The attack involves a client talking to a hostile server through an (innocent) caching proxy.  It server sends back data that will get incorrectly cached and then returned to both this client, and perhaps subsequent ones.
To be effective as a WebSocket-level cache, the proxy would have to terminate the TLS session.  So protection between client and proxy (and even between proxy and server) would be ineffective:  This is an attack on the semantics of the WebSocket layer, which would be exposed at the proxy.
Known plaintext is not relevant here since all three of the participants have access to the actual session and know what the plaintext really is - they don't need to guess it.
Granted, it's a kind of ugly fix - but the attack is an interesting example of the non-composability of security assertions.
                                                        -- Jerry

@_date: 2019-10-15 15:54:29
@_author: Jerry Leichter 
@_subject: [Cryptography] "Planting tiny spy chips in hardware can cost as 
About a year after the widely-criticized Bloomberg reports of chip attacks on Supermicro motherboards ... two different proof-of-concept demonstrations that such attacks are feasible and don't even require huge amounts of money.
"Elkins and Hudson both emphasize that their work isn't meant to validate Bloomberg's tale of widespread hardware supply chain attacks with tiny chips planted in devices. They don't even argue that it's likely to be a common attack in the wild; both researchers point out that traditional software attacks can often give hackers just as much access, albeit not necessarily with the same stealth.
But both Elkins and Hudson argue that hardware-based espionage via supply-chain hijacking is nonetheless a technical reality, and one that may be easier to accomplish than many of the world's security administrators realize. "What I want people to recognize is that chipping implants are not imaginary. They?re relatively straightforward," says Elkins. "If I can do this, someone with hundreds of millions in their budget has been doing this for a while.""
                                                        -- Jerry

@_date: 2019-10-31 06:11:24
@_author: Jerry Leichter 
@_subject: [Cryptography] 0xFFFFFFFF is the loneliest number that you'll ever 
============================== START ==============================
Or the randomest number.  At least according to the AMD RDRAND instruction on some chips.
Microcode bug:  CPUID on AMD Ryzen 3000 chips reports that RDRAND is implemented, and invoking it reports that the value returned is valid - but the value returned is always 0xFFFFFFFF.
The bug was fixed in an AMD microcode patch in July, but board makers have been ... random about issuing BIOS updates.  At one point Asus had BIOS updates dated well after the microcode patch - which didn't include it.
All sorts of merriment ensued.  Some Linux distributions fail to boot because systemd checks the RDRAND results and hangs if they fail.  (There's a workaround patch.)  Wireguard loops on a second connection.  Other system features and programs may have workarounds using different randomness sources or may proceed blindly with unfortunately results.
Quite the mess.  Interesting writeup at                                                         -- Jerry

@_date: 2019-09-12 10:35:27
@_author: Jerry Leichter 
@_subject: [Cryptography] TRNGs as open source design semiconductors 
I think you have to consider how realistic a target an on-chip random number generator actually is.  Modern Intel chips come with a built-in generator.  Let's assume that as designed, it's actually unpredictable.  Now imagine an attacker with effectively unlimited funds who wants to attack cryptographic or other protocols based on that generator.  Given that the designed generator is "good," there are two ways to attack it:  Leak the values, or slip a change into the hardware that makes the generator predictable.
But consider:  If I can slip a change into the hardware, why attack the RNG?  I can attack anything at all!  In fact, there are already published examples - among many - of attacks on Ethernet interfaces such that simply seeing a particular series of bits in the input causes it to do arbitrary nasty things - like sending the contents of selected memory locations to an attacker.
Against an attacker who can slip changes into hardware, defending a single part of the hardware is no defense at all.  In fact, we currently don't *have* any techniques for defending against such attackers, beyond the physical-world techniques of maintaining visibility into all of the supply chain.  That was, at one point, something available to, say, the US military.  Today, it's unlikely even the US military can afford it - a major concern, given how much the US military relies on computers.  Perhaps the NSA can, for relatively limited numbers of chips that it can build in its own "black" fabs.  For some kinds of uses - a sealed box between a computer and a network that manages all the encryption - it's possible, certainly with NSA levels of funding, to ensure that the stuff that traverses the wire can't be attacked.  But once the decrypted data is delivered to a general-purpose computer with "modern" levels of performance, the story is very different.  (The same goes, for example, to all kinds of modern sensors, which contain quite a bit of computation within them, massaging the data before your "secure" machine ever sees it.)
Designing and building cool RNG's is a nice project.  They are, by today's standards, very simple circuits.  If you know what you're doing, you can make solid physical arguments that the circuits, if implemented properly, are really unpredictable.  Since they are so simple, you can build them out of discrete components which are themselves so low-level that no hardware attack is practical.  (If I let you add a hack to every AND gate synthesized anywhere on the planet, how what exactly would you change to allow a hidden attack against some uses of those AND gates?)  And there will always be people willing to pay for such a thing, either "just in case" or because ... hey, *my* encryption algorithm has a 50,000-bit key, it's *so much more secure* than you sissy 256-bit key.  I mean, 50,000 is *way* larger than 256, so it's just obvious!
But does one of these things actually add to your security under realistic attack models?
                                                        -- Jerry

@_date: 2019-09-16 14:31:37
@_author: Jerry Leichter 
@_subject: [Cryptography] "Exclusive: Russia carried out a 'stunning' breach 
Too long to try to summarize.  It looks as if the Russians, starting in roughly 2010, managed to crack the encryption used on FBI tactical radios.  "A former senior counterintelligence official blamed the compromises on a ?hodgepodge of systems? ineffective beyond the line of sight. ?The infrastructure that was supposed to be built, they never followed up, or gave us the money for it,? said the former official. ?The intelligence community has never gotten an integrated system.?"
                                                        -- Jerry

@_date: 2019-09-21 15:55:50
@_author: Jerry Leichter 
@_subject: [Cryptography] "How Long Will Unbreakable Commercial Encryption 
"Most people who follow the debate over unbreakable, end-to-end encryption think that it?s more or less over....In fact, this complacent view is almost certainly wrong. Enthusiasm for controlling encryption is growing among governments all around the world and by no means only in authoritarian regimes. Even Western democracies are giving their security agencies authorities that nibble away at the inviolability of commercial encryption. Equally importantly, unbreakable user security will increasingly conflict with the commercial and political interests of the big Silicon Valley companies that currently offer encryption as a mass market feature?especially as technology companies take a more aggressive role in content moderation."
The article is by Stewart Baker, who's been involved in this debate for a very long time.  The quote above shows the general theme.  His basic point is that only the US has shown solid resistance to mandated access, and once any of the large no-US economies - and perhaps the entire EU - require Silicon Valley companies to provide access, they will quickly fold since providing different software to different parts of the world will prove too complex.
Worth reading for the perspective.  Of course, what's always left out of these is the question:  Just what will they gain access too?  William Barr apparently repeats a story about Mexican drug cartels using a WhatsApp group to coordinate multiple murders of police officials.  But what would have stopped them from using any number of open source alternatives instead, if they knew WhatsApp wasn't secure against law enforcement?  You'd need much deeper hooks into phones than just restrictions on commercial encryption to make a dent in all the parade of horrible stories that get repeated in its favor.
                                                        -- Jerry

@_date: 2019-09-22 07:19:43
@_author: Jerry Leichter 
@_subject: [Cryptography] "How Long Will Unbreakable Commercial Encryption 
I'm not sure what you mean by taking it at face value.  I think it's a very accurate portrayal of the arguments and moves that are now emerging in the crypto debates. "Scare 'em with horror stories of criminals and terrorists" doesn't seem to be working here in the US - the FBI and a few noted local prosecutors have been trying for years.  So ... two new attacks:  Outflank US protections by starting overseas; and argue that weakened crypto is necessary to protect us from all the "bad" stuff on the Internet.  The two approaches actually support each other well, since the problem of "bad" stuff is viewed much more seriously in a number of other countries.
It also doesn't hurt to present an image of inevitability to discourage the opposition.
Know thy enemy!
                                                        -- Jerry

@_date: 2019-09-22 07:51:41
@_author: Jerry Leichter 
@_subject: [Cryptography] "How to Subvert Backdoored Encryption: Security 
The world of academic cryptography has gotten highly theoretical and rather abstruse and often far removed from practice.  But ... not entirely by any means.  The pre-print  considers the following problem:  The Clipper chip (well, an improved version) won.  Only one acceptable encryption algorithm exists and the attacker knows all the keys.  It's illegal to send any message whose plaintext is not immediately meaningful - i.e., sending pre-encrypted text into the chip will land you in jail.  Can you communicate securely and safely?
The answer turns out to be yes, by a clever bit of judo that turns a security assumption - that the single cryptosystem supported provides semantic security, because the government wants a strong NOBUS guarantee - against the government.  The basic idea is as follows:  It's long known that semantic security requires that (a) the output be indistinguishable from random if you don't know the key; (b) the cryptosystem must choose randomly among many possible cryptotexts for a given plaintext.  So the output of a hash function independent of the key on a pair of successive encrypted messages is itself indistinguishable from random.  By repeatedly apply the encryption function to the same "cover" inputs, I can force the bottom bit (say) to encode a bit of a real message.
Yes, the government can apply the same (public) hash function.  If the real message is *not* random, he can recognize it.  But if the real message *is* random, every possible pair of messages carries a possible message, and there's no way to tell.
Now consider a DH key exchange.  The messages exchanged are, indeed, random.  Further, they have the property that they can be public:  Even an attacker who can observes them doesn't learn the agreed-to key.  So Alice and Bob can, in this system, establish a secret key between themselves while sending messages that cannot be distinguished from legitimate messages encrypted using the "legal" system:  *Every* exchange of encrypted messages potentially hides a DH key exchange.
Once they have their shared key, Alice and Bob can turn their real messages into strings of bits indistinguishable from random and those, in turn, can be transmitted subliminally using the same technique.
I've grossly over-simplified even parts of the system I *do* understand.  Why does h() look at two successive messages?  Why not just use the bottom bit directly?  This gets into the theory of randomness extractors and a theorem that says no randomness extractor from a single biased source is possible, but one from two biased but independent sources can be safe.  And extractors further allow you to send log(k) bits (k a security parameter) at a time, rather than just one, which makes the system much more practical.
Cool stuff.
                                                        -- Jerry

@_date: 2020-04-07 10:20:10
@_author: Jerry Leichter 
@_subject: [Cryptography] "Zoom's end-to-end encryption isn't 
It turns out this report wasn't quite correct - though Zoom's lack of a full description made things worse.  Zoom now has a blog post describing how the system works.  What actually happens is that connections *between endpoints running Zoom software* are actually encrypted end to end.  However, Zoom also supports the ability to bring other kinds of participants in - voice calls, various room conferencing systems, and so on.  These can't support Zoom's encryption - or, in some cases, *any* encryption.  Zoom handles these by using a "connector," which acts as a virtual participant in the end-to-end encrypted network of Zoom clients while also acting as a virtual client for the third-party systems.  The connectors, of course, do run on Zoom's servers.  So as soon as you add any connector to the conversation, you lose the end-to-end property.
Then again, it's hard to see how to avoid that, if you want to support systems that don't otherwise "play the game."
Since the keys used for the end-to-end connections come from the Zoom servers, Zoom *can* decrypt any of these conversations.  Also, since the connectors are completely transparent to participants, the architecture for a law enforcement or other tap is already right there.  Zoom claims that no such tap exists, and they've never been asked to create one; and that, other than in the connectors, they never decrypt the contents of calls.  You can, of course, choose to believe them or not.
Zoom does allow you to run your own servers, which in theory takes their servers out of the picture entirely.  But I'm not sure how much detail they've provided about this.  (In particular, are such servers *really* wholly disconnected from the Zoom infrastructure.)
There are some interesting crypto questions and design issues raised in all this:
1.  A single key is used for all the connections.  Under most circumstances the use of a single key leads to issues.  Here, given the broadcast nature of the conversations - everyone hears everything equally - it's not clear there's a problem.  One exception is when a participant is booted from the conference.  He still has the key and might have other ways to get the contents.  It would probably be good to rekey any time anyone leaves the conference, for whatever reason.
2.  If you kick someone out, you need to rekey over a connection to which the evictee still has the key.  This requires something like authenticated DH.  Probably the right way to do the initial key setup, rather than leaving it up to the server.  Doing n-to-n key setup when not all the parties are present at all times should be doable, but complicated.  Perhaps this starts to overlap with distributed agreement protocols like Lamport's Part Time Parliament.
3.  They probably should signal when a connector joins - or allow you to lock out connectors entirely.  But how many people would understand the implications?  Would this just be yet another little indicator that almost everyone ignores?
4.  They apparently do use AES in ECB mode.  In practical terms, when you are encrypting a compressed video stream ... how much does this really matter?
5.  Related to his:  The connections don't appear to provide authentication.  Since packet loss is to be expected and has to be tolerated in an application like this, the common authenticated modes won't work.  Is there something that will?
We like to say that properly done security is *not* the enemy of good user experience.  (Poorly done security very often is, and people end up working around it.)  Zoom provides a wonderful test case:  How much of the existing really simple and easy to use UI to you have to trade for better security?
                                                        -- Jerry

@_date: 2020-08-25 15:53:39
@_author: Jerry Leichter 
@_subject: [Cryptography] any reviews of flowcrypt PGP for gmail? 
This is a problem worth thinking about.  And there are ideas already out there.
1.  Make sure *everyone gets the same upgrade.*  This can be done by cross-checking checksums in some broad, public way.  A poisoned release can, of course, act differently for different users - something we already see - but you're raising the bar.  And the more poisoned copies there are out there, the greater the chance that someone notices.
2.  Use reproducible builds, in which anyone building from source will end up with artifacts with the same checksums.  These individually-compiled versions must, of course, have checksums that match the ones of the pre-compiled versions.  Again, this can be hacked around - "Reflections On Trusting Trust" applies if you want to go all the way there.
3.  Of course, you'd better be sure that there are plenty of sufficiently-skilled, independent eyes on any changes.
4.  What else?  This is an important area for further research.
                                                        -- Jerry

@_date: 2020-12-21 13:02:24
@_author: Jerry Leichter 
@_subject: [Cryptography] Solar Winds hack 
FireEye, a security company, publicly reported that they had been breached on December 8th.  Apparently as they dug into how they breach occurred, they discovered that it was through Solarwinds.  Determining this took significant effort.  (A public report by the WSJ says FireEye initially got an alert about an unexpected new device connected to their VPN, and then a team of 100 employees scanned through 50,000 of code to figure out the cause.  I take those numbers with a huge grain of salt - if the vulnerability was in Solarwinds they would have no code to scan.  Most likely, they scanned log files.  And while 50,000 lines sounds like a lot to those unfamiliar with this kind of thing, I've personally scanned many hundreds of thousands of lines of log files myself to track down anomalies.  Obviously, that doesn't mean *reading* all those lines - you use various automated tools to winnow out all the noise.  I used to joke at one point that my main job qualification was knowing how to start with a million lines of logs and quickly tease out just the hundred or so that actually had useful information.)
Tracking down this kind of thing is something the SEC is really good at, and they love to nail people for it.  It's one of the few white-collar crimes that gets successfully pursued quite frequently.  There are more subtle ways to profit from this kind of inside information ... if these guys were stupid enough to actually trade directly on it, they're going to be very sorry.
There are two interesting points about this that I haven't seen discussed:
- Did this password allow writing to the download server, or only reading from it?  If the latter ... it is, after all, a *download* server which 18,000 customers have to have access to.  A password would only be intended as a minor hindrance to non-customers.
- Even if the password *did* allow uploading, that in and of itself, while bad practice, was not was not much of a vulnerability:  The files there were signed and so any fakes should have been ignored by the update software.  The *real* problem was that Solarwinds managed to lose control of their private signing key.  At that point, everything is lost.
                                                        -- Jerry

@_date: 2020-12-21 17:26:05
@_author: Jerry Leichter 
@_subject: [Cryptography] Cryptographic archive format 
Ah, blacklists.  Always an issue.
Concerning your list as it is so far:
- ~ isn't a feature of the file system, it's a shell expansion.  You don't need to worry about it on any system I know of - as long as you are sure you never put one of the filenames in the archive on a shell command line.  But then again, if you allow that you've opened the doors of hell anyway.
- Forbidding leading / and \ - as well as, not A: but likely
: - just says "relative paths only".
- Yes, you need to forbid upward traversals - i.e., the ".." rules.
- You should probably forbid following soft links when restoring.  There are ways to trick people into creating a soft link to a sensitive directory, and then (perhaps much) later getting them to open an archive that uses that link.
- But ... I can do a lot of mischief by simply overwriting .login or .bash or many other such files.  Only works when unarchiving in the login directory - but (a) that's likely pretty common; (b) there may be similar "hazardous" files that appear elsewhere, though they are less common.
- Consider forbidding the creation files marked executable.  Many people include "." in their PATH - and even if they don't there are likely some common places where executables live (e.g., in ~/bin).
I'm sure with some thought one can find other attacks - especially when you want to support a variety of OS's each with their own quirks.  This is why blacklists are always so problematic.
Jon Callas's suggestion of always forcing the creation of a subdirectory and then unarchiving into it simplifies things considerably.  Perhaps less convenient, but safer.  And if you make it a chroot jail, the OS will enforce the inability to escape and you don't need to worry about .. in paths or all kinds of other stuff.  But I don't know what the analogue would be in Windows.
                                                        -- Jerry

@_date: 2020-02-11 08:31:22
@_author: Jerry Leichter 
@_subject: [Cryptography] =?utf-8?q?=22=E2=80=98The_intelligence_coup_of_th?= 
The Washington Post reports -  - that "[Crypto AG] made millions of dollars selling equipment to more than 120 countries well into the 21st century. Its clients included Iran, military juntas in Latin America, nuclear rivals India and Pakistan, and even the Vatican.... [The CIA] rigged the company?s devices so they could easily break the codes that countries used to send encrypted messages."
No kidding!  How long has this been widely known?  Apparently not very long, to WaPo.
                                                        -- Jerry

@_date: 2020-02-15 20:25:10
@_author: Jerry Leichter 
@_subject: [Cryptography] 'The intelligence coup of the century' 
I first heard ?The Flight of the Bumblebee? performed by an IBM 1620 in the late 1960?s. The machine was slow enough that you listened to its EM emissions on an AM radio. Making music this way was a fairly common trick on the machine. Eventually people wrote ?music compilers? if increasing sophistication. There was one that accepted quite a bit of notation and would produce programs to play the music and use the paper tape punch to produce percussion for accompaniment. There were many similar games played. Someone figured out how to modulate the sound from a line printer (by careful choice of characters sent and line lengths) to produce tones as the band of letters went back and forth.  Now *that* could really weird out an operator!
                                          -- Jerry

@_date: 2020-02-23 05:59:07
@_author: Jerry Leichter 
@_subject: [Cryptography] Apple's 13-month certificate policy 
This is a very telling comment, for anyone to whom it actually applies:  They are willing to trust their CA to attest to their identity to everyone with whom they do business - who in turn must trust that attestation - but their credit card info ... well, no.  Rather revealing of what the CA business actually is.
                                                        -- Jerry

@_date: 2020-01-06 19:24:20
@_author: Jerry Leichter 
@_subject: [Cryptography] Recent factorization of RSA-240 & DLP 
In the past, the real limit on factorization wasn't the very highly parallelizable sieving, it was the non-parallelizable, extremely memory-intensive matrix phase.  Has this changed?  I guess something has since they obviously didn't run this single-CPU for 100 years!  But there's still no indication of the memory requirements.
I guess I'm out of touch with recent advances in GNFS factorization....
                                                        -- Jerry

@_date: 2020-01-08 20:29:04
@_author: Jerry Leichter 
@_subject: [Cryptography] retro crypto 
This is a neat design, though it illustrates how security is a property of the system, not of one part of the system.  You have a black box that implements a cryptographic algorithm of some level of security, and does little else.  It can run at the speed of a reasonable comm link.  How do you get keys into it?  With the speed of modern comm links, almost any cryptographic algorithm really will need the keys changed on a reasonably frequent basis.  And how do you handle the plaintext?  The plaintext that flows into and out of that black box these days goes to a general-purpose computer, vulnerable to all that malware you've carefully excluded from the black box.
I know you aren't suggesting that this has much practical use - it's just a nice exercise.  It strikes me as attractive to the same kind of people who won't use the built-in AES instructions on their hardware because "maybe the NSA bugged it" but proceed to use the same hardware to handle the keys, the plaintext - or even implement AES "securely" themselves.  The list of threats such approaches actually protects against is rather short....
                                                        -- Jerry

@_date: 2020-01-12 17:20:12
@_author: Jerry Leichter 
@_subject: [Cryptography] improved identification of non-targets 
This is overkill.  All state armies (pretty) follow the long-agreed-upon laws of war.  Among these laws is the requirement that combatants wear their official uniforms.  Anyone engaging in combat not wearing uniform is classified as a spy and can be shot on sight.  A civilian airliner is "not in uniform" and should be treated the same way.
You'd have to get a lawyer familiar with all the complex details of the laws of war to say exactly how *existing* law applies, but a law in the same spirit could be based on a simple standard for broadcasting "I'm a civilian plane and not a legitimate military target" and should not be hard to agree to.  Anyone shooting down such a plane, or a combatant broadcasting such a signal, would be committing a war crime.
Perfect?  Hardly.  But state actors would generally obey the rules.  Non-state actors are not addressed by the laws of war and might not - but then again they might equally well shoot at a plane regardless of all the correct crypto codes that it broadcasts.  So you're left with the rather small case of non-state actors flying a plane that claims to be innocent but is not.  Of course, whatever system you might have had in place, the planes that were flown into buildings on 9/11 *were* legitimate civilian airliners, and would have been broadcasting legitimate codes.
If you think back at the real shoot-downs that were false positives - and non-shoot-downs that were false negatives (i.e., apparently legitimate planes that were used for attacks) - you'll see that the false positives were much more frequent.  In fact, I can't off-hand think of *any* false negatives:  9/11 doesn't qualify since I know of no evidence that anyone spotted the planes (even the one headed for the Pentagon), selected it as a target, but then decided to leave it alone it because it was a civilian airliner.
Yes, civilian planes could use some more protection - they aren't shot down all *that* often, but it's still way more often than it should be.  Almost all of the problem could be addressed fairly quickly and effectively by diplomacy and treaties - which a crypto solution would need *anyway*.
                                                        -- Jerry

@_date: 2020-01-22 19:28:30
@_author: Jerry Leichter 
@_subject: [Cryptography] Proper Entropy Source 
We have the classic Knuth dictum (and example) telling (demonstrating to) us why we should not create (pseudo)random number generators "randomly."
Apparently we need a similar dictum/example for "true random number generators."
                                                        -- Jerry

@_date: 2020-01-29 14:23:30
@_author: Jerry Leichter 
@_subject: [Cryptography] Proper Entropy Source 
This is curiously parallel to the traditional approach to leakage channels:  We can't practically close them down so we get "plausible bounds" on the rate at which the channel can leak bits and thus on "the attacker's ability to [get useful information]."
Of course, what we now know is that even a very, very low bandwidth leakage channel can do really big damage if it's used to leak a cryptographic key.  The key effectively compresses a huge amount of information (in combination with the encrypted data, which of course we assume is available) into just a small number of bits....
Similarly, bounds on "how far away we are from random/unguessable" can be really problematic in some situations, where giving the attacker even a fairly small amount of known bias to work with may actually give him a significant "in" to the system.
                                                        -- Jerry

@_date: 2020-07-09 08:57:36
@_author: Jerry Leichter 
@_subject: [Cryptography] "Home router warning: They're riddled with known 
Shocking.  And there's gambling going on, too.
All but one small German maker embedded private keys in their firmware. "The Netgear R6800 router contained 13 private keys."
A third are running Linux kernel version 2.6.36 or older.  The latest security update for 2.6.36 was in February of 2011.  One Linksys router was running 2.4.20, released in 2002.  There are 579 high-severity CVEs affecting that.
One thing I find disturbing is how little you can trust what you think you know about the companies.  I would have considered Netgear as high end, more expensive, probably trustworthy.  On the other hand, ASUS always came across as a cheap Chinese (actually Taiwanese) clone.  Both are at the top of the rankings in this report (not that that's much to be proud of).  I thought of Linksys as good because they were owned by Cisco.  Not so much - but then again, I didn't realize that Cisco sold them to Belkin (also pretty good?) who then sold them to Foxconn - which, as it happens, also owns ASUS!  You just can't tell.
                                                        -- Jerry

@_date: 2020-07-09 09:04:39
@_author: Jerry Leichter 
@_subject: [Cryptography] iOS apps peeking at contents of clipboard 
Perhaps off-topic - no direct cryptographic content other than perhaps a warning not to cut and paste your credential - but as has been widely reported, quite a number of iOS apps have been found to be grabbing the contents of the clipboard for no clear reason.
What I found interesting - and here it does overlap with the kinds of excuses we often see from all kinds of vendors - is the explanation from (I think LinkedIn, perhaps others) that they looked at the clipboard "only to see if it matched what the user was typing."  That's about as much of an explanation as looking at the clipboard "because we're running on a ARM processor."
Has anyone come across an explanation of why a app might want to check - at each character entered, mind you - whether what was being typed matched the contents of the clipboard?  The only case I can come up with is trying to prevent a user from cutting and pasting a password (or sometimes an email address) into both the primary and "confirmation" fields.  I could imagine some bizarre security rule forbidding that - just as some web pages, to this day, try to prohibit pasting into password fields.  But I've yet to see any webpage or app actually allow it and then complain....
                                                        -- Jerry

@_date: 2020-07-13 15:38:04
@_author: Jerry Leichter 
@_subject: [Cryptography] Terakey, 
Principles
Gee, sounds like an old idea to use a high-resolution photo of the moon.  The way that was set up, the underlying random-looking database - in this case, the brightness values of the picture - didn't even have to be private.
                                                        -- Jerry

@_date: 2020-07-16 10:29:32
@_author: Jerry Leichter 
@_subject: [Cryptography] Terakey, 
Principles
This is a really bad analogy.  A skyscraper isn't built on the basis of mathematical theories.  It's based mainly on physical measurements of various properties of materials, combined using mathematical approximations.  The measurements and approximations are all validated empirically.  Mathematical proof plays a surprising small role.
Case in point:  A story relayed to me by someone who started out as an architect/structural engineer and had this case presented as a warning in a class at MIT, taught by a faculty member who was actually involved.
A number of years back, an unusual skyscraper was constructed in Manhattan.  Typically, the weight of a building is carried primarily (at ground level) along its corners.  But the architect of this building wanted a large open area under the building itself and wanted to avoid big, heavy supports at the corners.  So he came up with the idea of rotating the weight-bearing supports by 45 degrees:  The weight was carried by supports along the centers of each side.  Of course, this was fed into all the standard structural analysis programs and it passed the tests, including for stability in the face of "worst case" wind loads.
A class learning how to do this kind of analysis was given the assignment of using the standard tools to repeat the wind analysis for the building and show that it was, indeed, safe.  One student in the class came to the professor and said that her analysis showed that the building would actually collapse under the design wind loads, and she couldn't figure out why she was getting "the wrong answer."  The professor looked closely at what she was doing, found that it was correct ... and finally understood what was going on.  It turns out that one can mathematically prove that the worse case for wind loading is when the wind is normal to one of the sides of the building.  So the standard analysis is just run for the four normal directions; if those are good, the building is fine.
The student, for some reason, decided to also run the analysis for other wind directions - and found the hole in the proof.  The proof is valid *for buildings supported at the corners.*  It isn't valid for an unusual building supported at the midpoints of the sides.  In fact, the worst case for such a building is at some angle to one of the corners (probably depending on the ratio of the orthogonal sides).  A good hurricane coming from just the wrong direction would cause the building to collapse.
So much for structural design based on proven mathematical theory.
(In case you're wondering what happened to the building:  The professor involved was very well connected and quickly gained the attention of the architects and structural engineers involved in the building's design. They agreed, once they saw the analysis, that something needed to be done.  The building - fully occupied by then - was retrofitted with various braces and other devices to make it safe; as far as I know, it's in active use to this day.  The work was all done quietly; the actual reasons were not publicly explained.  Today, this story has become part of the education of architects and structural engineers, a caution alongside such classics as the collapse of the Tacoma Narrows bridge, aka "Gallopin' Gertie.")
It was quite some time ago, and I can't locate the reference.  As I recall, it was related to a book code, using a sequence generator of some sort to choose bits (yes, low-order bits which are very random) from what was at the time an unusual example of a publicly available immense database:  A very high resolution picture of the Moon produced by NASA.  Again, I don't remember the details and as a result can't speak to the actual system or its properties.
                                                        -- Jerry

@_date: 2020-07-31 11:10:51
@_author: Jerry Leichter 
@_subject: [Cryptography] Cryptographically securing a two-phase commit 
============================== START ==============================
This whole problem "feels like" distributed consensus; even the example you give of the attacker simply failing to send the last message is exactly the way the classic FLP result on the impossibility of consensus in a fully asynchronous system even with a single fail-stop process is proved.
Let me see if I can construct an argument.  The introduction of cryptography here is irrelevant to the underlying problem, as is the use of existing file formats.  2PC is exactly a consensus protocol:  Normally, what's important is that all parties agree on a single bit, COMMIT/ABORT.  There's an additional layer here that is only relevant given that we've decided on COMMIT:  Whether we've all agreed on the value.  But that's just yet another consensus problem.  Let's ignore it and just look at the COMMIT/ABORT problem.
What are the assumptions about the environment in which COMMIT/ABORT consensus must be reached?  What's the definition of a correct protocol?  If you assume (a) all parties that follow the protocol correctly must agree on the result; (b) both COMMIT/ABORT are possible outcomes (i.e., ignore the trivial case that everyone always decides on ABORT); (c) you must always reach a decision in some bounded amount of time; (d) all communications is fully asynchronous - you can't make decisions based on timeouts - then you're already in FLP territory, and even in the case of a process that simply stops (or maybe a message that is lost - subtly different; there's *tons* of work on exactly now all this plays out) no solution is possible.
If you change things to the Byzantine General's variation, solutions without cryptography exist if and only if the number of hostile processes is less than 1/3 of the total.  With cryptography (signatures, in particular) you can actually tolerate n-1 hostiles out of n.  Again ... many, many variations; this is all exquisitely sensitive to the detailed assumptions.
                                                        -- Jerry

@_date: 2020-06-05 21:56:56
@_author: Jerry Leichter 
@_subject: [Cryptography] [FORGED]  Cubbit 
If it actually worked, I'm not sure I would care that my encrypted bits were in a gaming machine or wherever.  Why should I?  What matters to me is the global guarantee that I can get my bits back within some reasonable time.  If the system can do that, I don't care if some of the storage is spray-painted graffiti on walls all over town, read by nearby video cameras.
There is an underlying issue that's clear with Cubbit but is present in all such systems.  Cubbit specifically says that the storage in each attached node is split 50/50:  Half is used to hold local pieces of your data; half is made available to the system.  (I think they actually split that into space used to hold chunks of other people's data and space to hold metadata that glues the system together).
Now, if half my space is my data, and half is shared to provide redundancy for other people's data ... over the entire system, the redundancy can't possibly be more than two.  At best, overall, the effect has to be that losing just the wrong two copies loses the data entirely.
The description Cubbit provides says that they distribute 36 copies in a Reed-Solomon code that allows recovery from any 24 copies.  But one way or another ... 36 copies requires that, somewhere, there be 35 times the space of the original copy to provide the redundancy.  Now, perhaps they are assuming that most people will use (well under) the "private" capacity of their local storage for their own data.  If on average people only use half their local storage, then the "public" half now has 4 times the used capacity, and the redundancy starts to get interesting.  But 36 times the used capacity?  Hardly something to rely on if the system catches on.
                                                        -- Jerry

@_date: 2020-06-07 06:41:49
@_author: Jerry Leichter 
@_subject: [Cryptography] [FORGED]  Cubbit 
It doesn't.  Dumb analysis on my part - not actually thinking it through.  I ought to know how error-correcting codes work....
                                                        -- Jerry

@_date: 2020-03-04 06:52:36
@_author: Jerry Leichter 
@_subject: [Cryptography] Ex-CIA Joshua Schulte Describes His Data/Crypto 
There are at least two interesting comments in that manual:
1.  "Solid state storage devices ... Power Removal: Sanitize DRAM (dynamic random-access memory), SRAM (static random-access memory), and Volatile FPGA by removing the power, including backup batteries. Once power is removed, sanitization is instantaneous."
Apparently some of the reported efforts at recovering RAM state are not considered meaningful threats.
2.  Smart cards:  "Strip Shredding: A strip shredder with a maximum width of 2 millimeters will destroy the microchip, barcode, magnetic strip and written information on the Smart Card. Smart Cards must be inserted diagonally into the strip shredder at a 45-degree angle for proper sanitization.
NOTE: A CROSS CUT SHREDDER WILL NOT SANITIZE SMART CARDS."
I don't recall any published information about recovering information from damaged cards.  Clearly the NSA has done some work here.  Note that cross-cut shredders *are* acceptable for diskettes and optical media.
Also worth noting that the NSA still mainly insists on physical destruction of hard disks.  (An NSA-approved - likely very high powered - degausser is sufficient but even then they recommend physically damaging the disks themselves.)
                                                        -- Jerry

@_date: 2020-03-04 14:53:34
@_author: Jerry Leichter 
@_subject: [Cryptography] Possible reason why password usage rules are 
I've actually heard comments to this effect.  I can't recall the context; it was years ago, when the whole field was new and was trying to develop best practices by incorporating "proven ideas," often without looking at them too closely.  (The whole Rainbow Book series is like that:  Some good ideas, some ideas that really make no sense outside of the military/intelligence context in which they were conceived.)
Much of this stuff is also due to CYA:  If I require this policy that others are requiring, it may inconvenience users, but that's someone else's problem.  If something goes wrong, I can show that I followed "best practices."  On the other hand, if I apply my own thinking and things go wrong, the shit will all land on me.
A number of years ago, a product I worked on had a protocol in which there were a pair of TCP connections.  One was essentially a control channel, used during setup and teardown but otherwise almost not at all.  The other transferred large amounts of encrypted data.  The protocol engine, if either connection went down, would shut the other one down as well.
At some customers, we'd find these connections shutting down every couple of hours.  Apparently some network admins, following "best practices" from the days of dialup connections, configured their routers to close any TCP connections that had been idle for more than x hours.
The question I always wanted to pose to these admins was the following:  Here are two TCP connections.  They connect the same two hosts - in fact, the same processes on the same two hosts - and have been up for the same amount of time, within a second or two.  In the last 2 hours (say), one of them has seen no data at all transmitted; the other has seen a couple of hundred MB of encrypted data which you cannot see into.  Which one is a potential security threat?
                                                        -- Jerry

@_date: 2020-03-04 18:03:25
@_author: Jerry Leichter 
@_subject: [Cryptography] Ex-CIA Joshua Schulte Describes His Data/Crypto 
All of this has been true on spinning rust drives for many years.  One common approach was to leave one sector free on each track.  If a sector goes bad, remapping is trivial - basically you just "slide the track around" so that the first sector is the one just after the bad block.  This requires minimal memory - the physical "first block" - and CPU power, important back in the day.
These days, the processing power and memory embedded in a disk drive renders the cost of doing more sophisticated remapping irrelevant.  It's generally pretty much impossible to know what's going on inside of a modern disk drive.  (Google put a lot of effort into understanding the performance implications, but even Google couldn't get much info out of the drive makers.  They had to do some sophisticated testing to figure stuff out.)
                                                        -- Jerry

@_date: 2020-03-05 05:11:40
@_author: Jerry Leichter 
@_subject: [Cryptography] Possible reason why password usage rules are 
When Unix first became publicly available, there was a population of Unix dweebs who would bow down to the immortal words and code of Thompson and Morris and Ritchie - typically with no understanding of what those guys actually *did* and how much care and work went into it; or the particular environment in which they worked.  I read a series of messages on a Unix Usenet newsgroup which argued against the notion of a private encrypted password file - after all, the gods of Unix had PROVEN that THE WAY was to make that stuff public.
I responded that if you read the article, you could see that they had to go through multiple iterations, all but the last of which were successfully attacked; and that in fact it was really an argument that this is a cool idea but the advance of technology renders any such mechanism vulnerable, and fairly quickly.
Many people jumped on me for not understanding The True Way of Unix - until Ken Thompson, in a Marshal-McLuhan-steps-out-from-behind-the-movie-poster moment, replied with a brief message:  "Leichter is right".  :-)
                                                        -- Jerry

@_date: 2020-03-05 16:37:31
@_author: Jerry Leichter 
@_subject: [Cryptography] Possible reason why password usage rules are 
There are arguments to be made for requiring certificate renewals, such as making sure that people aren't using 512-bit RSA keys any more.  But I'll agree the arguments are weak.  Mainly, this is a way to ensure the CA's have a viable business model:  If certs are "one and done" how long would CA's be around?
There is another valid argument for having certs expire:  When the expiration dates are short enough, you get an effect that's equivalent to CRL's, but without some of the headaches.  However, this hasn't been a practical use case until automated renewal of certs became easy.
This has been around *forever*.  I remember that my sister tried to provide her college ID to someone, but they rejected it because she'd just graduated and it had expired.  Her comment at the time was "it may have expired, but I haven't!"
Meanwhile, if you've traveled in the last 5-10 years (I don't know when this changed), not only must your passport not be expired - it must not expire within 6 months of when you start your trip.  The only explanation I can come up with that is that it's a stealth way of reducing passport lifetimes by 5%, forcing you to pay the fees again.
And don't get me started on the completely absurd paperwork requirements for "enhanced ID" driver's licenses.  The first time I tried to get one, I had three pieces of (computer printed, non-canceled) mail (what does that prove?) to prove residence - but one was more than 3 months old, and another had my first name as Jerry rather than Jerrold.  So sorry, you lose, come back again later.
                                                        -- Jerry

@_date: 2020-03-06 09:55:17
@_author: Jerry Leichter 
@_subject: [Cryptography] IDs and licenses, 
Until relatively recently, way after every other state, New York used a plain cardboard computer-printed license with no photo on it.  (Politics:  The office that printed and mailed them was up in Albany and well-connected.  They were not about to lose jobs to every DMV office in the state issuing licenses when they took a photo.  They eventually reached a compromise that I believe holds to this day:  The photos are indeed taken at each DMV office, but they are then uploaded to the central office in Albany which prints and mails the licenses.)  My original driver's license was from New York.  Later, I moved to Massachusetts and got a license there.  They looked at my NY license, but gave it back to me. So I had two of them.  For years after, New York let me renew my license - all I had to do was pay the fee.  No photo so no visit to a DMV office required.  At some point (well before they actually started requiring photos) I decided there really wasn't any point; but for many years I had licenses from multiple states, and no one seemed to care.
Meanwhile, I noted earlier that when I last renewed my Connecticut license, I couldn't get an Enhanced license because my envelopes weren't good enough.  Since my license had actually expired, they issued me a new not-good-for-Federal ID one, telling me I could convert it to a Federal one later (for an additional fee, of course).  I did so a couple of weeks later.  *They didn't collect my previous, perfectly good for another three years, license.*  So I now have three licenses:  My expired one (which they also didn't bother to keep); and two valid ones, one "plain" and the other "Federal", both covering the same validity period.
There's a *lot* of security theatre involved in this stuff.
Connecticut *did* finally eliminate one bit of held-over nonsense, the biannual sticker for your license plate to prove you'd renewed your registration.  They were always checking your registration on-line anyway, so the sticker was pointless.  However, you *do* still have to have a card from your insurance company.  I recently learned from a cop that in fact the card must have a date within the last year or it isn't valid.  Never even thought of that - other than the date, my insurance cards have been identical for many years as I've kept the same carrier.  And ... "issued no more than a year ago" means, literally, nothing:  Some policies are only good for six months.  And any policy can be canceled.  (Not to mention that anyone who cares to can print an exact duplicate saying whatever they want with little effort.)
This is something that, again, is checked on-line anyway.  But, hey, it's a nice violation to justify "reasonable suspicion" with....
                                                        -- Jerry

@_date: 2020-03-07 07:29:46
@_author: Jerry Leichter 
@_subject: [Cryptography] IDs and licenses, 
I'm sorry but the reasoning here is getting increasingly specious.
How long is the term of a driver's license?  That's actually quite difficult to answer!  The AAA summarizes all the state laws at   It's all free text and there is no simple chart - but then there really can't be.  Consider Iowa:
Except as otherwise provided, a driver?s license will have a random expiration date of 5, 6, 7 or 8 years from the issue year and expires on the licensee?s birthdate, and not to exceed the licensee?s 74th birthday. If the licensee is under the age of 17 years 11 months, the license expires 2 years from the licensee?s birthday in the year of issuance. If the licensee is 72 years of age or older, the license expires 2 years from the licensee?s birthday in the year of issuance. Licensees age 70 or older must appear in person for each renewal.
A person has 60 days to renew his or her license after the expiration date.  Electronic renewal is permitted every other renewal.
Applicants with vision or other physical restrictions may be required to renew their license every 2 years.
A vision test or vision report signed by a licensed vision specialist is required.
The expiration date for persons who enter military service while holding a valid Iowa driver?s license is 6 months after separation from active duty.
The licenses of active duty military personnel may be extended until 6 months after separation from the military. Active duty military personnel must obtain a military service extension. A 5-year extension is available for military personnel and their families.
All clear?  The reasoning for the shorter expiration dates mentioned here is clear:  To retest *the ability to drive* for older people; and to get new pictures from very young people whose appearance may change drastically.  It has nothing to do with the validity of the identification.  In fact, except when I converted from a "standard" to an "enhanced" license or moved from state to state (where, as I recall, they simply wanted proof of residence) the previous expired license always served as sufficient documentation to get a new one.  (This is true for the ultimate identity document, the passport, as well.)
Meanwhile, here in Connecticut, "New driver?s licenses are valid for 5 1/2 or 7 years and expire on the driver?s birthday. Renewals are valid for 6 years."  Renewals are generally done at a DMV office - they require a new picture.  However, you can apply for a renewal up to 6 months before your license expires.  They give you your new license immediately and I've never seen them take back the old one.  So they, by their own operation, allow me to have two non-expired licenses for 6 months out of eery 6 years.  (Or, as I noted, in my case because I converted to an "enhanced" license after the fact, almost the full 6 year period.)
Besides ... much of the population *already has two forms of ID suitable for airplane travel*:  A driver's license and a passport.  Military personnel probably have a third, their military ID.  If they want to travel on one while letting their evil twin brother travel on another, checking expiration dates will do precisely nothing to stop them.
And that's just for the relatively high bar the TSA sets.  There are many other places that accept all kinds of lower-grade ID - a local employer's badge, for example.  They buy into the same nonsense.  If I remember right, my sister's "expired college ID" wasn't accepted at some store for something or another, details long forgotten.
                                                        -- Jerry

@_date: 2020-03-08 08:24:45
@_author: Jerry Leichter 
@_subject: [Cryptography] IDs and licenses, 
When I mentioned this I did a quick scan for the most complicated one I could find (Iowa) but missed the simplest - Arizona:  "Upon issuance, a driver?s license is valid until the applicant?s 65th birthday and is renewable thereafter for successive 5-year periods."
Somehow Arizona seems to manage without all the hassle of periodic renewals, except when actually retesting eyesight and such becomes a significant issue:  "Persons 70 and older may not renew by mail. Persons 65 or older renewing by mail must submit a vision test verification form, or a verification of an eyesight examination conducted not more than 3 months before."
All kinds of systems grow up around ostensible purposes and are retained with explanations that sound reasonable but aren't tested.  (The *real* reason they get retained is that getting rid of them costs someone money or their job.)  Then along comes someone else - sometimes another state - and shows that in fact the requirement was never needed to begin with.  Every state I've even lived in had periodic renewal requirements, and until this moment, I frankly never questioned that there might be a need for renewals - it was just "the way things were."
BTW, Iowa, the bizarre case I brought up earlier, is a case study in the opposite direction.  Ever buy land, or a house?  Have to pay for a title search?  Be forced to pay for title insurance?  Doesn't it seems strange that it's complicated to determine who owns a piece of land?  The reason is that land records are kept sorted *by owner*, not by property.  The reason goes all the way back to 1086, when King William the Conqueror ordered the "Great Survey" to determine what nobles controlled what land all over England.  (Rough approximation.)  The emphasis was on a small number of nobles, not the land itself, since the purpose was to determine what they each owed William.  And ... that's the way real estate recording has been done in England to this day.  The method was, of course, brought over to the original Colonies, retained by the new United States, and spread from the original 13 to the new states as they were incorporated.  The system is based on recording transfers between named individuals.  "Clearing title" - ensuring that the person selling you some land really owns it - requires a complex historical search.  It can go wrong - sometimes due to historical inequities.  All kinds of land on the East Coast has turned out to actually have been invalidly transferred from native peoples back in 18th century.  There's no inherent limit on how long such rights can be re-asserted, though courts ignored them until late in the twentieth - when many towns discovered that they didn't actually own the land they were built on and had to settle with the descendants of the people it was stolen from.
Anyway, the complexity of the whole system leads to having to pay for title searches - usually a few hundred dollars - and, in some cases, for title insurance, which can be quite a bit more expensive.  (Mortgage issuers - who rely on the property as collateral - have often required title insurance, sometimes with no good cause.  There are strong claims that title insurance is basically fraudulent - it ends up being a way for the mortgage writer to steer some more of the buyer's money to an associated entity.)
All this is the case ... except in Iowa.  Iowa keeps track of information by plot of land, not by owner, and by law completely clears title at every transfer.  It's actually illegal to write title insurance in Iowa.
Why don't the other 49 states do things this way?  For that matter, since all these records are computerized now anyway - does it really matter how the old paper documents used to be sorted?  Well ... they've always done it the good old-fashioned way, it works just fine for the people who run the system, it only annoys people on the relatively infrequent occasions when they transfer real estate - and then their lawyer deals with it anyway.
                                                        -- Jerry

@_date: 2020-05-04 19:50:32
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA security guidelines for videoconferencing 
Let's separate concerns here.
Ignoring for the moment the way they choose and distribute the session key - which should change - Zoom provides the equivalent of a room in a building.  The room itself is (reasonably well) constructed so that outsiders can't observe or here the meetings you hold in there.  You can control who gets in the room by:
 o Controlling who you tell about the room.  This was their default mode of operation in the past, but it failed because everyone knew where all the rooms were and could wander the halls and peak in the rooms to find interesting meetings;
 o Put someone at the door and tell all your invitees a secret word.  The guy at the door only lets people who whisper the word in his ear into the room.  This is what their passwords amount to.  This works if you can trust that those you give the password to won't give it to people who shouldn't have it.  In many situations, this is a reasonable approach.
 o Make a list of people who are invited and have the guy at the door only accept *them*.  This is one thing the "waiting room" feature is for - but it's hard to operationalize in an automated fashion without some pre-agreed form of identification - i.e., an on-line identity.  If Microsoft were to implement a Zoom-like system, since they are already in the identity business, they would simply use their own identity system.  (Hey!  Guess what!  They are and do.)  Zoom is crippled here because there simply is no universal identity system out there they could rely on - and they're not about to go out and build one.
If you want to build a system that (a) supports authenticated control of access to meetings; (b) supports large meetings (beyond the point where a human can check everyone on entry); (c) would actually be usable by pretty much anyone in the world who has access to an Internet connection and a fairly pedestrian computer - well, I don't think the underpinnings exist today.
BTW, note that if you want a *public* meeting - where you put the time and room number on posters around town and then let anyone in - well, that makes sense sometimes - but complaints that "the wrong people" get in are a bit nonsensical.
Note that Zoom will let you (for sufficiently rich "you") run your own Zoom server.  In that case, if you have an identity service, Zoom could probably add an extension easily enough so that you could control who can get into what meetings.  (And in that case, only your organization, not Zoom, has access to the keys - modulo bugs, accidental or otherwise, in the server.)  But all kinds of things are much easier in a closed, controlled system than in the greater Internet.
                                                        -- Jerry

@_date: 2020-05-31 08:07:22
@_author: Jerry Leichter 
@_subject: [Cryptography] Cubbit 
Anyone looked at/into Cubbit (cubbit.io)?  This is a "distributed cloud" implementation - you buy a piece of hardware that joins all the others to form a distributed file storage network.  The site has a basic overview of the approach which says all the right stuff - they encrypt locally with AES-256, split the result into chunks, Reed-Solomon encode, then distributed the pieces across the hardware boxes.  Meanwhile the AES key is encrypted based on a password and also distributed so that you can access your stuff from anywhere.  They use a Bittorrent variant to allow fast access to multiple chunks in parallel.
Of course, the devil - especially for the crypto - is in the details.  I found references to two talks (which I didn't follow up) but no published papers with more details -though there is a published paper comparing the energy usage of their implementation to traditional cloud implementations, with theirs winning easily.  (I must admit to some skepticism about that one.  There has to be spinning rust tied to some compute elements and a network *somewhere*, and the amounts needed, and the energy used, has to be supplied - and it's generally more efficient to centralize that stuff, especially if you can - as the big cloud providers have - centralize it in areas where you can get local solar power or free atmospheric cooling.  But ... I do need to read that paper in detail.)
                                                        -- Jerry

@_date: 2020-11-17 14:01:56
@_author: Jerry Leichter 
@_subject: [Cryptography] Possible reason why password usage rules are 
The one-rotor box was the original crypt command.  iang will certainly answer with his take on it, but I suspect the "DES mistake" was, in NSA's view, that DES was delivered as a software solution to a broad audience.  NSA at the time believed in hardware implementations and was hard at work trying to control the "leakage" of good cryptography into the general community.
                                                        -- Jerry

@_date: 2020-11-23 11:00:46
@_author: Jerry Leichter 
@_subject: [Cryptography] Possible reason why password usage rules are 
A suggestion I've been making for years combines a small regulation with self interest:  Any system maintained by any corporation that stores user passwords - hashed or whatever you like - must also store, under pre-defined usernames, authentication information for the bank accounts, investment accounts, and other important financial and personal data, of the company president and everyone in the executive chain to whoever chooses, manages, or determines the funding for that system.
You'd be amazed at what could be accomplished if the security of those systems *actually mattered* to those in charge.
                                                        -- Jerry

@_date: 2020-10-01 22:08:11
@_author: Jerry Leichter 
@_subject: [Cryptography] Exotic Operations in Primitive Construction 
I don?t know of any hardware supporting this as a primitive, though of course that can change. (The main trend these days is to provide direct hardware support for the most commonly required algorithms, not more primitives.)
I think Galois multiplication is typically done using a lookup table.
                                         -- Jerry

@_date: 2020-10-03 12:14:02
@_author: Jerry Leichter 
@_subject: [Cryptography] Exotic Operations in Primitive Construction 
This is highly inefficient.  There's a classic trick:  To reverse an n-bit value, split it into two n/2-bit values; reverse each; and swap.  Repeat the recursion until you have a "small enough" remaining value, then look up the reversed value in a table.  A table mapping every 8-bit byte to its reverse is only 256 bytes long and will certainly fit in the cache of any modern CPU.  So:
uint8_t reverseByte[256] = {0x00, 0x80, 0x40, ...};
uint16_t reverse(uint16_t v) {
  uint8_t left = (v >> 8) & 0xFF;
  uint8_t right = v & 0xFF;
  return (reverseByte[left] << 16) | reverseByte[right];
The win is obvious even for 16 bits, and (assuming you open-code it, rather than writing a loop or doing a recursion) is even greater as you deal with longer words:  The loop is linear in the bit size; this is logarithmic.
If you want to deal with really long bit strings, they won't be represented as C primitive types - they'll be arrays of uintX_t's.  Most of the cost in this algorithm is splitting the uint16_t into its constituent bytes.  If the input is an array, most of the work is already done for you.
This will need to deal with bit strings that are not a power of two in length, the code gets more involved, but the same idea applies.
                                                        -- Jerry

@_date: 2020-10-16 05:45:40
@_author: Jerry Leichter 
@_subject: [Cryptography] Secret sharing for family members 
Neither writable CD's nor SSD media are good for long-term storage.  Both will deteriorate and become unreadable in a fairly small number of years.  Given the underlying nature of the task here - making data available to heirs - the limits on the lifetime of the media is significant.
Others have suggested, for other reasons, going with paper.  Paper, if reasonably carefully stored, should remain readable for many decades.  Go with acid-free paper and store it in a safety deposit box and it should be good for many centuries.
I wouldn't go fancy with QR codes - a technology that might fade.  Just use a couple of randomly chosen worss - easy for anyone to type, easy to get enough entropy for a portion of a key.
I don't know of any stock program to do this kind of secret splitting and recombining, but the algorithms are simple enough.  You could include a listing of such a program on the sheet of paper just in case the program itself isn't readily available years from now.  I'd suggest FORTRAN as it's likely to survive us all. :-)
                                                        -- Jerry

@_date: 2020-09-23 16:42:15
@_author: Jerry Leichter 
@_subject: [Cryptography] World's oldest security RFC published 
In 2000, they spelled "Enrollment" correctly.
In 2020, the *actual RFC* spells "Enrollment" incorrectly.
                                                        -- Jerry

@_date: 2020-09-30 16:59:21
@_author: Jerry Leichter 
@_subject: [Cryptography] Exotic Operations in Primitive Construction 
Cryptographic algorithms tend to have two parts:
1.  Something that maintains and mutates an internal state.  A cryptographic random number generator is an obvious example; it's pure mutation, with no input.  A cryptographic checksum looks like a random number generator except that it keeps feeding in the information to be checksummed.  Most block cipher algorithms mutate a key-based state to create round keys.
2.  A combiner that takes output from the mutator and combines it with an input stream to create an output stream.  Random number generators don't have a combiner, but if you use one to build a stream cipher, it's right there.  With block ciphers the combiner is determined by the mode.
Rotates, in particular, are common in the mutator.  You want to combine a series of simple, fast primitives to make a primitive that's hard to invert.  A basic idea going all the way back to Shannon is to alternate primitives that don't commute.  (In fact, it's pretty obvious that primitives the *do* commute are a bad idea, because an attacker can simply move them around to the most convenient form to attack.  Often, chains of repetitions of a simple primitive can simply be folded together.)
Using something like rotates and additions is a good choice because not only don't these commute, but they come from entirely different groups so their interactions are complex.  The same is true for shifts, but rotates have an advantage:  They don't lose any information.  A shift always discards some bits.
XOR is also widely used, but it's just addition in a different group.  Logical negation is just XOR with a constant.  Other logical operators all lose information.
Lookups have certainly been used - RC4 is an obvious example - but access to memory is way more expensive on modern CPU's than computational operations, so you have to keep them small.  And as differential cryptography shows us, just because a lookup table looks disordered doesn't mean it's strong.
Multiplication has been suggested for some special uses (a long discussion on this group of using multiplication in random number generators in a way that guarantees uniform distribution), but doesn't otherwise seem to be used often, perhaps for performance reasons.
Division is very expensive and has no obvious advantages.
It's not clear what other "exotic" operations you might use.  The only other primitive not in any of these classes I can think of is bit count, which loses so much information it doesn't seem useful.
AES has an internal structure that is more amenable than many to expression in  clean mathematical terms, but that hasn't helped anyone prove much of significance about its security properties.  We really don't have much in the way of "proofs of security" for cryptographic algorithms.  Rather, we are still at the point of showing immunity to various known attacks - though the attacks we understand today are very general and powerful.  So amenability to neat mathematical characterization doesn't really help - and in fact there were early arguments that AES was rendered *weaker* by its clean structure.  That hasn't panned out ... perhaps illustrating how little we actually understand about the sources of security and insecurity in cryptographic algorithms.
                                                        -- Jerry
