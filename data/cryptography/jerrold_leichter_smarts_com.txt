
@_date: 2003-12-01 11:35:56
@_author: Jerrold Leichter 
@_subject: lockable trapdoor one-way function 
You're going to have to be much more explicit about the details to have any
hope of getting at this.  If you take too naive an approach, even a simple
trapdoor function is impossible.  "Given f(X), it's impractical to compute
X".  However, whoever computed f(X) can of course give you X!  So one sets
it up as something like:  If we have any probabilistic Turing machine T
(subject to some constraints on size and running time) which is given an input
tape containing f(X), then over all X and all runs of the machine, the
probability that it halts with the tape containing X is less than (some limit
"not much bigger than" the probability of just just guessing X correctly).
This is the only way I know of to capture the notion of T "is only given f(X)".
But this whole approach can't possibly model the notion of "forgetting" X,
since the only way you can *prove* you've forgotten something is to display
that *all* possible memory you have access to, or will have access to in the
future, is (say) zeroed.
You could certainly spread the trust out.  For example, one could imagine a
trap door computed using shared-secret techniques:  The trap door information
is shared among K participants.  The K participants jointly compute the trap
door function.  Even *after* the computation is complete, it still takes all
K participants to compute the function.  So if we now tell all the participants
to forget their portion of the secret, if you trust *any* of them to do so,
the trap door information is gone.
Even this has a subtlety:  What's to prevent a participant - or anyone else? -
from storing  pairs?  So, on the one hand, you may need to state the
requirement in terms of being able to compute f(X) or its inverse on data that
had never been presented to the K participants.  Alternatively, you could
insist that the K participants individually never see X or f(X).  (But then
who *does* see them to distribute and finalize them?)
There's the seed of an interesting primitive here, but exactly what it is
seems difficult to pin down.  What application do you have in mind for such a

@_date: 2003-12-06 17:55:58
@_author: Jerrold Leichter 
@_subject: Additional Proposed Hash Function (Forwarded) 
I'd guess that this is part of an effort to define hashes "equivalent in
strength" to various standardized ciphers.  Because of birthday attacks, in
some sense the security of an n-bit hash is comparable to that of an n/2-bit
cipher.  So SHA-256, -384, and -512 are intended to match the three supported
AES key sizes of 128, 196, and 256 bits.  SHA-224 then comes out to match
2-key 3-DES.

@_date: 2003-12-06 18:37:42
@_author: Jerrold Leichter 
@_subject: safety of Pohlig-Hellman with a common modulus? 
The question seems equivalent to:  Is P-H with a *known* modulus safe?  The
only difference, from the point of view of an attacker, is that with a shared
modulus, he gets to see a whole bunch of values X^e_i mod P, including (if he
is a member of the system) some where he knows X and e.  But with a known
modulus, he can easily generate as many of these as he likes.  The safety of
P-H had better depend on the difficulty of solving the discrete log problem
mod P, not on keeping P secret.  (Keeping P secret would require great care in
the protocol, in particular in how you respond to messages that are greater
than P.  Otherwise, an attacker can guess the rough size of P - based on the
maximum sizes of messages he observes - and then try to do a binary division
by sending messages of differing sizes.)
The situation is different in RSA since there are *two* secrets:  The
factorization of N, and the correspondence between public and private keys.
These secrets are so closely related that revealing one reveals the other as
well.  We usually only consider the implication "factoring N lets you get the
private key from the public", but the other one is present as well. Giving
someone a public/private key pair for a given N is *not* zero knowledge!

@_date: 2003-12-07 11:34:01
@_author: Jerrold Leichter 
@_subject: Additional Proposed Hash Function (Forwarded) 
Standards bodies are supposed to deal with existing practice.  If you accept
the need for hash functions with sizes "matched" to standardized encryption
techniques, then it's appropriate for them to do this.  (Of course, it raises
the question of why there isn't an SHA-112 to go with DES - but I guess that's
officially deprecated now.)
NIST isn't an implementor, its a standards body.  It can't say "just truncate
SHA-256" - it has to say *how*.
In fact, the real question here is why they bothered to change the initial
values.  It does make the SHA-256 and SHA-224 values independent in some
sense, but but I'm not sure exactly *how* independent.  That is:  If I give
you the SHA-256 and SHA-224 hashes on some piece of data, have I given you
something that's effectively a 480-bit hash?  Changing the initial value is
equivalent to prepending some initial fixed block.  The fixed block is
unknown, unless the initial value was *calculated* as the result of one
iteration of SHA over some value.  That's why it's important to know just
where the intial value came from.
Well, this is one place that secure hash function theory is rather weak. All
the hash functions in use have an iterative structure, which implies a prepend
property (i.e., if I know the hash of X, where X is an even multiple of the
block size in length) I can calculate the hash X || Y for any Y, without
knowing X.)  Also, the constructions are size-specific - there's no obvious
way to change the block size or output size - except by keeping a fixed block
size and discarding some part of the final block to produce a smaller output
I think there are some constructions out there without these problems, but
none seem to be practical.  This remains an area waiting for a clever design.
The goal of "a hash to match each encryption", in the abstract, sounds great.
Whether it actually makes practical sense is debatable.  However, once a
standards process gets rolling, the original purposes can fade from view.

@_date: 2003-12-07 11:35:29
@_author: Jerrold Leichter 
@_subject: origin of SHA 224 initial hash values 
BTW, it hadn;t occured to me until now, but the 160-bit SHA-1 was presumably
designed to go with the 80-bit Clipper encryption!

@_date: 2003-12-14 19:21:48
@_author: Jerrold Leichter 
@_subject: example: secure computing kernel needed 
...which is, I would think, the only reasonable approach!  In fact, even
TCPA takes exatly this approach.  And it's hardly new, from identification (a
SecureID token is a separate, secure box, not a piece of software to run on
your PC) to IP protection (dongles being exactly tamper-resistant secure
computing kernels).
In no case does you, as the end-user, have any real say over what the secure
kernel does or how it is used.  By its nature, it operates, for the most part,
outside of your control.
Which brings up the interesting question:  Just why are the reactions to TCPA
so strong?  Is it because MS - who no one wants to trust - is involved?  Is
it just the pervasiveness:  Not everyone has a smart card, but if TCPA wins
out, everyone will have this lump inside of their machine.
I think many of the reasons people will give will turn out, on close
reflection, to be invalid.  Sure, you can choose not to buy software that uses
dongles - and you'll be able to chose software that doesn't rely on TCPA.
(In both cases, depending on the kind of software, you may find that your
choice is "run it our way, or do without".)  You can choose not to use a bank
that requires you to have a smartcard - but in practice you would be chosing
less security.
We've met the enemy, and he is us.  *Any* secure computing kernel that can do
the kinds of things we want out of secure computing kernels, can also do the
kinds of things we *don't* want out of secure computing kernels.  if the
kernel can produce *our* unforgeable signature, it can produce someone else's
as well.  Sure, we can decline to allow our secure computing kernel to be used
for that purpose - but someone else may then choose not to do business with
I think the real threat of TCPA is not in any particular thing it does, but in
that it effect 'renders the world safe for dongles".  MS *could* today require
that you have a dongle to use Word - but to do so, even with their monopoly
power, would be to quickly lose the market.  Dongles are too inconvenient, and
carry too much baggage.  But when the dongle comes pre-installed on every
machine, the whole dynamic changes.

@_date: 2003-12-15 19:02:06
@_author: Jerrold Leichter 
@_subject: Difference between TCPA-Hardware and a smart card (was: example: 
However, this advantage is there only because there are so few smart cards,
and so few smart card enabled applications, around.
Really secure mail *should* use its own smart card.  When I do banking, do
I have to remove my mail smart card?  Encryption of files on my PC should
be based on a smart card.  Do I have to pull that one out?  Does that mean
I can't look at my own records while I'm talking to my bank?  If I can only
have one smart card in my PC at a time, does that mean I can *never* cut and
paste between my own records and my on-line bank statement?  To access my
files and my employer's email system, do I have to have to trust a single
smart card to hold both sets of secrets?
I just don't see this whole direction of evolution as being viable.  Oh,
we'll pass through that stage - and we'll see products that let you connect
multiple smart cards at once, each "guaranteed secure" from the others.  But
that kind of add-on is unlikely to really *be* secure.
Ultimately, to be useful a trusted kernel has to be multi-purpose, for exactly
the same reason we want a general-purpose PC, not a whole bunch of fixed-
function appliances.  Whether this multi-purpose kernel will be inside the PC,
or a separate unit I can unplug and take with me, is a separate issue. Give
the current model for PC's, a separate "key" is probably a better approach.
However, there are already experiments with "PC in my pocket" designs:  A
small box with the CPU, memory, and disk, which can be connect to a small
screen to replace a palmtop, or into a unit with a big screen, a keyboard,
etc., to become my desktop.  Since that small box would have all my data, it
might make sense for it to have the trusted kernel.  (Of course, I probably
want *some* part to be separate to render the box useless is stolen.)
Why?  If I'm going to use a time-shared machine, I have to trust that the
OS will keep me protected from other users of the machine.  All the other
users have the same demands.  The owner of the machine has similar demands.
The same goes for any shared resource.  A trusted kernel should provide some
isolation guarantees among "contexts".  These guarantees should be independent
of the detailed nature of the contexts.  I think we understand pretty well
what the *form* of these guarantees should be.  We do have problems actually
implementing such guarantees in a trustworthy fashion, however.
Part of the issue with TCPA is that the providers of the kernel that we are
all supposed to trust blindly are also going to be among those who will use it
heavily.  Given who those producers are, that level of trust is unjustifiable.
However, suppose that TCPA (or something like it) were implemented entirely by
independent third parties, using open techniques, and that they managed to
produce both a set of definitions of isolation, and an implementation, that
were widely seen to correctly specify, embody, and enforce strict protection.
How many of the criticisms of TCPA would that mute?  Some:  Given open
standards, a Linux TCPA-based computing platform could be produced.
Microsoft's access the the trusted kernel would be exactly the same as
everyone else's; there would be no basis for complaining that they had left a
back door into your system.  However, it would do absolutely nothing to block
the "nightmare scenarios" that get raised - like a Word version that stores
all data in encrypted files that can only be read by "the real Word".
I remember looking at this paper when it first appeared, but the details
have long faded.  It's an alternative mechanism for creating trust:  Instead
of trusting an open, independently-produced, verified implementation, it
uses cryptography to construct walls around a proprietary, non-open
implementation that you have no reason to trust.  Kind of like deciding
there's no way you can ever have a mix you trust completely, so you chain
them together so that all have to be corrupted before any information is
That certainly may have its role, but it leaves all the "nightmare scenarios"
exactly as they were.  (BTW, I don't recall if it addresses the issue of
how I have multiple mutually-suspicious Observers all on line at the same

@_date: 2003-12-17 10:30:25
@_author: Jerrold Leichter 
@_subject: Difference between TCPA-Hardware and other forms of trust 
You're confusing policy with mechanism.  Both sides have the same notion of
what a trusted mechanism would be (not that it's clear, even today, that we
could actually implement such a thing).  They do, indeed, have different
demands on the policy.
The system owner's policy is that each user be charged for *at least as much
time* as he used.
The individual user's policy is that they be charged for *no more than* the
time he used.
A system trusted by both sides would satisfy both constraints (or report that
they were unsatisfiable).  In this case, a trusted system would charge each
user for exactly the time he used!
Exactly:  *You* had a system you trusted.  Your users had to rely on you.
The situation you describe is hardly new!  Auditing procedures have been
developed over hundreds of years in order to give someone like the user of
your system reason to believe that he is being treated correctly.  Like
everything else in the real world, they are imperfect.  But commerce couldn't
exist unless they worked "well enough".  (I once dealt with a vendor support
charge of around $50K.  It was sent with no documentation - just the bald
assertion that we owed that money.  It tooks weeks to get the vendor to send
their phone conversion and back-room work logs.  They contained all kinds of
interesting things - like a charge for a full 8-hour day to send a 5-line
email message.  Yes, *send* it - the logs showed it had been researched and
written the day before.  We eventually agreed to pay about half the billed
amount.  In another case, someone I know audited the books - kept by a very
large reseller - on which royalties for a re-sold piece of software were
based.  The reseller claimed that no one ever bother to check their books - it
was a waste of time.  Well... there were *tons* of "random" errors - which
just by chance all happened to favor the reseller - who ended up writing a
large check.)
I'm not defending TCPA.  I'm saying that many of the attacks against it
miss the point:  That the instant you have a truely trustable secure kernel,
no matter how it came about, you've opened the door for exactly the kinds of
usage scenarios that you object to in TCPA.
Let's think about what a trusted kernel must provide.  All it does is enforce
an access control matrix in a trusted fashion:  There are a set of objects
each of which has an associated set of operations (read, write, delete,
execute, whatever), a set of subjects, and a mapping from an object/subject
pair to a subset of the operations on the object.  "Subjects" are not just
people - a big lesson of viruses is that I don't necessarily want to grant
all my access rights to every program I run.  When I run a compiler, it
should be able to write object files; when I run the mail program it probably
should not.  (You can identify individual programs with individual "virtual
users" and implement things setuid-style, but that's just an implementation
technique - what you're doing is making the program the subject.)
Subjects have to be able to treat each other with suspicion.  However, we
assume that the trusted kernel is *above* suspicion:  It's been verified by
those who use it.  One aspect of this is that it can give trusted reports:
I - or anyone with appropriate authorization - can ask for a list of the
access rights for some object, and trust the answer.
Given this setup, a music company will sell you a program that you must
install with a given set of access rights.  The program itself will check
(a) that it wasn't modified; (b) that a trusted report indicates that it
has been given exactly the rights specified.  Among the things it will check
in the report is that no one has the right to change the rights!  And, of
course, the program won't grant generic rights to any music file - it will
specifically control what you can do with the files.  Copying will, of course,
not be one of those things.
Now, what you'll say is that you want a way to override the trusted system,
and grant yourself whatever rights you like.  Well, then you no longer have
a system *anyone else* can trust, because *you* have become the real security
kernel.  And in a trusted system, you could certainly create a subject that
would automatically be granted all access rights to everything.  Of course,
since the system is trusted, it would include information about the
override account in any reports.  The music company would refuse to do
business with you.
More to the point, many other businesses would refuse to do business with you.
If the system were really trusted, it could store things like your credit
balance:  A vendor would trust your system's word about the contents, because
even you would not be able to modify the value.  This is what smart cards
attempt to offer - and, again, it would be really nice if you didn't have to
have a whole bunch of them.  The bank records stored on your system could
be trusted:  By the bank, by you - and, perhaps quite useful to you, by a
court if you claimed that the bank's records had been altered.  (Today, you
can save and later produce a paper copy of a bank statement, and it will
generally be believed.  An on-line statement is worthless for this purpose.
Individual messages from the bank may be signed, but they are much less
convenient for this purpose.)
Yes, you can construct a system that *you* can trust, but no one else has
any reason to trust.  However, the capability to do that can be easily
leveraged to produce a system that *others* can trust as well.  There are
so many potential applications for the latter type of system that, as soon
as systems of the former type are fielded, the pressure to convert them to
the latter type will be overwhelming.
Ultimately, TCPA or no, you will be faced with a stark choice:  Join the
broad "trust community", or "live in the woods".
That being the case, I don't find attacks on TCPA as such fruitful.  Take it
as a given.  Attack any parts of it that are kept secret.  When/if it's open,
attack any faults.  Attack the actual - and potential - abuses.
PS  All the above starts with the working assumption that one *can* actually
produce a trusted kernel.  TCPA itself starts from the same assertion.  I'm
skeptical about how far we can go - and in fact the general access control
mechanism I used in my illustration is known to give rise to fairly obvious
questions about who can access what that are NP-complete (or is it actually
non-computable?)  If it can't be done - the implications of it *being* done
are irrelevant.

@_date: 2003-12-23 10:15:10
@_author: Jerrold Leichter 
@_subject: example: secure computing kernel needed 
The question is not whether you *could* build such a thing - I agree, it's
quite possible.  The question is whether it would make enough sense that it
would gain wide usage.  I claim not.
The issues have been discussed by others in this stream of messages, but
lets pull them together.  Suppose I wished to put together a secure system.
I choose my open-source software, perhaps relying on the word of others,
perhaps also checking it myself.  I choose a suitable hardware base.  I put
my system together, install my software - voila, a secure system.  At least,
it's secure at the moment in time.  How do I know, the next time I come to
use it, that it is *still* secure - that no one has slipped in and modified
the hardware, or found a bug and modified the software?
I can go for physical security.  I can keep the device with me all the time,
or lock it in a secure safe.  I can build it using tamper-resistant and
tamper-evident mechanisms.  If I go with the latter - *much* easier - I have
to actually check the thing before using it, or the tamper evidence does me
no good ... which acts as a lead-in to the more general issue.
Hardware protections are fine, and essential - but they can only go so far.
I really want a software self-check.  This is an idea that goes way back:
Just as the hardware needs to be both tamper-resistent and tamper-evident,
so for the software.  Secure design and implementation gives me tamper-
resistance.  The self-check gives me tamper evidence.  The system must be able
to prove to me that it is operating as it's supposed to.
OK, so how do I check the tamper-evidence?  For hardware, either I have to be
physically present - I can hold the box in my hand and see that no one has
broken the seals - or I need some kind of remote sensor.  The remote sensor
is a hazard:  Someone can attack *it*, at which point I lose my tamper-
There's no way to directly check the software self-check features - I can't
directly see the contents of memory! - but I can arrange for a special highly-
secure path to the self-check code.  For a device I carry with me, this could
be as simple as a "self-check passed" LED controlled by dedicated hardware
accessible only to the self-check code.  But how about a device I may need
to access remotely?  It needs a kind of remote attestation - though a
strictly limited one, since it need only be able to attest proper operation
*to me*.  Still, you can see the slope we are on.
The slope gets steeper.  *Some* machines are going to be shared.  Somewhere
out there is the CVS repository containing the secure kernel's code.  That
machine is updated by multiple developers - and I certainly want *it* to be
running my security kernel!  The developers should check that the machine is
configured properly before trusting it, so it should be able to give a
trustworthy indication of its own trustworthiness to multiple developers.
This *could* be based on a single secret shared among the machine and all
the developers - but would you really want it to be?  Wouldn't it be better
if each developer shared a unique secret with the machine?
You can, indeed, stop anywhere along this slope.  You can decide you really
don't need remote attestation, even for yourself - you'll carry the machine
with you, or only use it when you are physically in front of it.  Or you
can decide you have no interest in ever sharing the machine.  Or you can
decide that you'll share the machine by using a single shared secret.
The problem is that others will want to make different choices.  The world is
unlikely to support the development of multiple distinct secure kernels.
Instead, there will be one such kernel - and configuration options.  You want
a local "OK" light only?  Fine, configure your machine that way.  You want
a shared-via-shared-secret common machine?  OK.  In that sense, you're
absolutely right:  You will be able to have a secure kernel that doesn't, as
you configured it, support the TCPA "baddies".
But once such a kernel is widely available, anyone will also be able to
configure it to provide everything up to remote attestation as well.  In
fact, there will be reasons why you'll want to do that anyway - regardless of
whether you want to do business with those who insist that your system attest
itself to them.  This will make it practical - in fact, easy - for businesses
to insist that you enable these features if you want to talk to them.
Consider browser cookies.  Every browser today supports them.  Everyone knows
that.  Some sites don't work correctly if cookies are disabled.  Some (Ebay
is, I believe, an example) are quite explicit that they will not deal with you
if you don't enable cookies.  If enabling cookies required you to load and run
some special software on your machine, sites would have a much harder time
enforcing such a requirement.  As it is, they have no problem at all.
In the same way, I claim, if secure kernel software becomes widely used, it
*will* have a "remote attestation-shaped hole" - and the block to fill that
hole will be readily available.  You will have the choice not to put the
block in the hole - but then you have the option of disabling remote
attestation in TCPA, too.  Whether that choice will be one you can effectively
exercise is an entirely different question.  My opinion is that we are all too
likely to find that choice effectively removed by political and market forces.
It's those forces that need to be resisted.  The ubiquity of readily available
remote attestation is a probably foregone conclusion; the loss of any choice
in how to respond to that might not be.

@_date: 2003-12-29 11:24:11
@_author: Jerrold Leichter 
@_subject: Repudiating non-repudiation 
Ian's message gave a summary that's in my accord with how courts work.  Since
lawyers learn by example - and the law grow by and example - here's a case
that I think closely parallels the legal issues in repudiation of digital
signature cases.  The case, which if I remember right (from hearing about it
20 years ago from a friend in law school) is known informally as the
Green Giant Peas case, and forms one of the bases of modern tort liability.
The beginning of the 20th century lead to the first mass production, distri-
bution, and marketing of foods.  Before that, you bought "peas".  Now, you
could buy a can of "Green Giant Peas", sold by a large manufacturer who sold
through stores all over the place, and advertised for your business.
Someone bought a can of Green Giant Peas at a local store.  The can contained
metal shavings.  The purchaser we injured, and sued Green Giant.  One of the
defenses Green Giant raised was:  Just because it says Green Giant on the label
doesn't *prove* Green Giant actually packed the stuff!  The plaintiff must
first prove that these peas really were packed by Green Giant.  Such defenses
had worked in the past - there are many of the same general flavor, insisting
that no recovery should be possible unless plaintiff could reach a level of
proof that was inherently unreachable.  In this case, the courts finally
threw out this defense.  I can't find the actual case on line, but at
 (a curious site -
it seems to be mainly in Chinese) the following text appears:
"Self-authenticating" here seems very close in concept to what we are trying
to accomplish with digital signatures - and the Green Giant example shows how
the law grows to encompass new kinds of objects.  But it's also important to
look at how "self-authentication" is actually implemented.  Nothing here is
absolute.  What we have is a shift of the burden of proof.  In general, to
introduce a document as evidence, the introducer has to provide some
proof that the document is what it purports to be.  No such proof is
required for self-authenticating documents.  Instead, the burden shifts to
the opposing console to offer proof that the document is *not* what it
purports to be.  This is as far as the courts will ever be willing to go.

@_date: 2003-12-29 12:38:22
@_author: Jerrold Leichter 
@_subject: I don't know PAIN... 
That's true of RSA!  The public and private keys are indistinguishable - you
have a key *pair*, and designate one of the keys as public.  Computing either
key from the other is as hard as factoring the modulus.  (Proof:  Given both
keys in the pair, it's easy to factor.)
Merkle's knapsack systems (which didn't work out for other reasons) had the
property that the public key was computed directly from the private key.
(The private key had a special form, while the public key was supposed to
look like a random instance of the knapsack problem.)
Obviously, a system in which the private key could be computed easily from the
public key would not be useful for encryption; so we've covered all the
meaningful "is computable from" bases....

@_date: 2003-12-29 16:28:49
@_author: Jerrold Leichter 
@_subject: I don't know PAIN... 
This doesn't work for RSA because given a single private/public key pair, you
can factor.

@_date: 2003-12-30 10:05:01
@_author: Jerrold Leichter 
@_subject: Difference between TCPA-Hardware and a smart card (was: example: 
All of this is fine as long as there is a one-to-one association between
machines and "owners" of those machines.  Consider the example I gave
earlier:  A shared machine containing the standard distribution of the
trusted computing software.  All the members of the group that maintain the
software will want to have the machine attest, to them, that it is properly
configured and operating as intended.  We can call the group the owner of the
machine, and create a single key pair that all of them know.  But this is
brittle - shared secrets always are.  Any member of the group could then
modify the machine and, using his access to the private key, fake the "all
clear" indication.  Each participant should have his own key pair, since
attestation using a particular key pair only indicates security with respect
to those who don't know the private key of the pair - and a member of a
development team for the secure kernel *should* mistrust his fellow team
So, again, there are simple instances where it will prove useful to be able
to maintain multiple sets of independent key pairs.
Now, in the shared distribution machine case, on one level team members should
be mutually suspicious, but on another they *do* consider themselves joint
owners of the machine - so it doesn't bother them that there are key pairs
to which they don't have access.  After all, those key pairs are assigned to
*other* owners of the machine!  But exactly the same mechanism could be used
to assign a key pair to Virgin Records - who we *don't* want to consider an
owner of the machine.
As long as, by owner, you mean a single person, or a group of people who
completely trust each other (with respect to the security problem we are trying
to solve); and as long as each machine only has only one owner; then, yes, one
key pair will do.  But as soon as "owner" can encompass mutually suspicious
parties, you need to have mutual independent key pairs - and then how you
use them, and to whom you grant them, becomes a matter of choice and policy,
not technical possibility.
BTW, even with a single owner, multiple independent key pairs may be useful.
Suppose I have reason to suspect that my private key has been leaked.  What
can I do?  If there is only one key pair around, I have to rebuild my machine
from scratch.  But if I had the forsight to generate *two* key pairs, one of
which I use regularly - and the other of which I sealed away in a safe - then
I can go to the safe, get out my "backup" key pair, and re-certify my machine.
In fact, it would probably be prudent for me to generate a whole bunch of
such backup key pairs, just in case.
You're trying to make the argument that feature X (here, remote attestation for
multiple mutually-suspicious parties) has no significant uses.  Historically,
arguments like this are losers.  People come up with uses for all kinds of
surprising things.  In this case, it's not even very hard.
An argument that feature X has uses, but also imposes significant and non-
obvious costs, is another thing entirely.  Elucidating the costs is valuable.
But ultimately individuals will make their own analysis of the cost/benefit
ratio, and their calculations will be different from yours.  Carl Ellison, I
think, argued that TCPA will probably never have large penetration because the
dominant purchasing factor for consumers is always initial cost, and the
extra hardware will ensure that TCPA-capable machines will always be more
expensive.  Maybe he's right.
Even if he isn't, as long as people believe that they have control over the
costs associated with feature C - in this case, as long as they believe that
they can choose *not* to allow remote attestation where it isn't it their
interest - they will assign low costs.  The problem isn't with the argument up
to this point - it's in the potential future world where the weight of
concensus and standards and even law effectively removes the theoretical
choice to "opt out".

@_date: 2003-12-30 10:41:19
@_author: Jerrold Leichter 
@_subject: [camram-spam] Re: Microsoft publicly announces Penny Black  PoW 
(The use of memory speed leads to an interesting notion:  Functions that are
designed to be differentially expensive on different kinds of fielded hardware.
On a theoretical basis, of course, all hardware is interchangeable; but in
practice, something differentially expensive to calculate on an x86 will remain
"expensive" for many years to come.)
In fact, such things are probably pretty easy to do - as was determined during
arguments over the design of Java.  The original Java specs pinned down
floating point arithmetic exactly:  A conforming implementation was required
to use IEEE single- and double-precision arithmetic, and give answers
identical at the bit level to a reference implementation.  This is easy to do
on a SPARC.  It's extremely difficult to do on an x86, because x86 FP
arithmetic is done to a higher precision.  The hardware provides only one way
to round an intermediate result to true IEEE single or double precision:
Store to memory, then read back.  This imposes a huge cost.  No one could find
any significantly better way to get the bit-for-bit same results on an x86.
(The Java standards were ultimately loosened up.)
So one should be able to define an highly FP-intensive, highly numerically
unstable, calculation all of whose final bits were considered to be part of
the answer.  This would be extremely difficult to calculate rapidly on an
Conversely, one could define the answer - possibly to the same problem - as
that produced using the higher intermediate precision of the x86.  This would
be very hard to compute quickly on machines whose FP hardware doesn't provide
exactly the same length intermediate results as the x86.
One can probably find problems that are linked to other kinds of hardware. For
example, the IBM PowerPC chip doesn't have generic extended precision values,
but does have a fused multiply/add with extended intermediate values.
Some machines provide fast transfers between FP and integer registers; others
require you to go to memory.  Vector-like processing - often of a specialized,
limited sort intended for graphics - is available on some architectures and
not others.  Problems requiring more than 32 bits of address space will pick
out the 64-bit machines.  (Imagine requiring lookups in a table with 2^33
entries.  8 Gig of real memory isn't unreasonable today - a few thousand
dollars - and is becoming cheaper all the time.  But using it effectively on a
the 32-bit machines out there is very hard, typically requiring changes to
the memory mapping or segment registers and such, at a cost equivalent to
hundreds or even thousands of instructions.)

@_date: 2003-11-13 10:13:30
@_author: Jerrold Leichter 
@_subject: Cryptography as a component of security 
That's not true.  Consider a stream cipher:  It "stretches" your original key
into a much longer stream.  By your argument, as soon as there is sufficient
generated stream to uniquely determine the key, no additional entropy is being
introduced, so all the rest is pseudo-security!  But in fact the unicity point
must be reached very quickly, or the generated stream would be too *indepen-
dent* of the key, and an attacker could guess most of the stream without even
knowing the key.
Or consider the following construct:  Suppose we have two encryption functions
E1 and E2, E1 is very fast, but breakable - there is an attack of concern to
us that becomes viable after an attacker has seen (or been able to partially
specify, if you want to think about chosen plaintext attacks) N plaintext/
ciphertext pairs.  E2 is slow, but we consider it to be secure against the
attacks of concern.  To encrypt the stream of plaintext P_i to ciphertext C_i,
Even if an attacker breaks into a number of segments and gets those S_i's
(which, BTW, is an assumption about the form of the attack:  Some attacks
reveal the plaintext without revealing the key), what he's left with is a set
of plaintext/ciphertext pairs with which he now has to attack E2 - which is
assumed to be difficult.  (Even worse for the attacker, even if he could mount
a chosen ciphertext attack against E1, he has no control over the plaintexts
fed into E2.)
Note that we assumed E2 was actually very strong.  But even if we assume E2
has the same effective strength as E1 - it can be broken after seeing N
plaintext/ciphertext pairs - then the security guarantees (such as they are;
one would have to be more precise about the nature of "attackable after N
pairs are seen" vulnerability) are pretty much the same until about N^2
plaintext/ciphertext pairs have been sent.

@_date: 2003-11-16 18:46:28
@_author: Jerrold Leichter 
@_subject: A-B-a-b encryption 
Not at all.
What's required here is that A and B commute - i.e., that A(B(X)) = B(A(X)).
(This is needed for the 3rd step.  If they don't commute, then decrypting
B(A(x)) with A gives you garbage.)  That's a relatively rare property for
cryptographic algorithms - but there are examples.
No modern cryptosystem should be vulnerable against such an attack.  (Security
against chosen-plaintext attacks is a given these days.  If the attacker can
choose the plaintext, he can generate as many examples of this kind of things
he likes.)
Yes, older systems (pre-DES) - and toy systems - may be vulnerable.
Stream ciphers used in the standard mode - XOR the keystream with the
plaintext - have the property that encryption with any two keys commutes.
*However*, as your example shows, this particular encryption technique *must
not* be used for this key exchange.
Not necessarily.  Diffie-Helman has a form much like this (though it's not
exactly the same).  It's easy to construct a secure (but not useful!) variant
using RSA:  We give everyone a private/public key pair with respect to a fixed
N = pq, but we discard p and q.  Then, since encryption is just exponentia-
ation mod N, it's easy to see that encryption with any two keys commutes.
Further, the system is exactly as secure as the underlying RSA.  (This isn't
particularly *useful* because given a single private/public key pair, you can
get p and q back - so any participant can read and forge everyone else's
messages.  It is secure against any non-participant, though.)
Going back to the original question:  This idea has been around for a *long*
time.  I think a paper that was released a while back showing work done by the
British government cryptography organization that anticipated RSA's notion of
asymmetric crypto by several years mentioned it.  I think it goes by the name
"two-pass protocol".  The real-world analogue is nice to use in describing DH,
though again it's not *exactly* the same:  Alice takes a box, puts her message
inside it, and attaches her lock to the hasp.  She sends it to Bob, who adds
his own lock to the hasp, and sends it back to Alice.  Alice removes *her*
lock, and sends the box - now secured only by Bob's lock - back to him.  He
can remove his own lock and read the message. Alice and Bob need share no
secrets (i.e., each has a private collections of locks and keys; neither can
unlock the others' locks), but they can securely exchange a message since the
box, with the message inside, is never "out in public" without at least one
lock on it.

@_date: 2003-11-17 14:56:12
@_author: Jerrold Leichter 
@_subject: Are there... 
There's something seriously wrong here, however.  There are many close, but
not identical, definitions, of a one-way hash.  While none of them explicitly
say so, many *uses* of one-way hashes assume that the output of a hash looks
like the output of a random function.  However, for (say) RSA, this fails
very badly, since RSA is multiplicative.  Suppose, for example, that we used
the well-known technique of generating working keys K1 and K2 from a master
key K by applying a one-way hash function H:  K1 = H(K || "K1"),
K2 = H(K || "K2").  With H=SHA or any reasonable one-way hash revealing K1
provides no useful information about K or K2.  But now suppose instead of
concatenation we, for some reason, used multiplication:
For H as before, this is just as secure.  But for H = RSA with a known modulus
N and public key, this is completely insecure:  It's easy to compute 3' = the
inverse of 3 mod N (Euclidean algorithm), at which point H(3') * K1 * H(5) =
H(3') * H(K * 3) * H(5) = H(3' * 3 * K * 5) = H(K * 5) = K2.
A useful one-way hash function shouldn't have any simple algebraic properties.
(The existing ones do have a prefix property due to their iterative nature,
and it would be nice to avoid that, though I don't know of any work in that
direction.  Note that this prefix property does mean that the K in "K || "K1")
should probably be significantly less than the block size.  Personally, I favor
XOR'ing the distinguisher - "K1", here - into K, avoiding the whole potential
interaction with the prefix property.  Note of course that this assumes that
H does *not* have some "bad" interaction with XOR - as RSA does with simple

@_date: 2003-10-02 10:29:47
@_author: Jerrold Leichter 
@_subject: Reliance on Microsoft called risk to U.S. security 
While I agree with the sentiment, the text/code distinction doesn't capture
what's important.
Is HTML text or code?  Well ... it's actually neither.  There is a set of tags
in HTML that is perfectly safe.  Setting fonts and font colors, describing a
table ... all kinds of stuff like that isn't text, but it can't hurt you
either.  (Whether these are *useful* in mail is a whole other question....)
On the other hand, embedded references that reach out over the network are
inherently dangerous, since the very act of autonomously opening a connection
to a system of someone else's choosing is something I, personally, don't want
my mail reader doing, ever.
Text is code, too!  A plain ASCII message received by Pine is run through the
Pine Interpretation Engine - the Pine MUA under another name.  Some character
sequences cause entries to be made into a list of subject lines.  Others
may trigger automatic refiling.  Most cause bits to be written to a terminal
window.  Those bits in turn are really code, interpreted by my xterm (or
whatever), causing it do to all kinds of complicated things that ultimately
light some pixels on my screen.
The real distinction is:  Which of *my* resources are *you* able to manipulate
by sending me mail?  I'm willing to give you complete access to send characters
to the terminal in which pine runs ... well, almost.  There's a long history
of security holes *in terminals* - e.g., escape sequences that load the "WRU"
buffer, and then a control character that causes the "WRU" to send back
"qyy|rm *\n"!  You have to carry this analysis *all the way through all the
levels of interpretation*.  Way back, when I worked on designing terminals at
DEC, we were aware of the security issues and it was a design rule that there
was no way to send characters to the terminal and have exactly those read back
automatically.  (Usually, you couldn't read it back at all.  Otherwise, they
were wrapped up in a format that would be innocuous to all but very strange
programs - e.g., as a hex string.)  It was fun evaluating competitor's
terminals against that design rule - almost all of them failed.  (It was not
so much fun telling marketing that no, we would not implement that particular
feature even though the competition had it.)
Anyhow ... if you consider the entire *system*, then the rule is that I will
give you complete access to write a certain "safe" subset of ASCII characters,
or a "safe" subset of ASCII characters plus escape sequences.  That's why mail
clients have long filtered out certain characters.  (An MUA that talks direct
X calls has no reason to filter anything, since writing a text string directly
with X goes into an interpreter that can change *only* the bits on the
A resource-based model gives you some basis for deciding which parts of HTML
are acceptable, and which are not.  Given such a model, it's perfectly
possible to write a safe HTML interpreter.  Of course, you lose some features -
that's life.  Life would also be much easier if you never had to lock your
door because no one would ever think of stealing.
BTW, a resource model has to change with time.  The traditional view has been
that I grant anyone full write access to the end of mail file - i.e., anyone
can send me as much mail as they like.  These days, I really *don't* want to
grant that access to spammers - and spam blocking techniques are all about how
to control the "SMTP interpreter" I present to the outside world to protect
my disk resources.

@_date: 2003-10-03 14:16:22
@_author: Jerrold Leichter 
@_subject: anonymous DH & MITM  
No; it's false.  If Alice and Bob can create a secure channel between them-
selves, it's reasonable to say that they are protected from MITM attacks if
they can be sure that no third party can read their messages.  That is:
If Alice and Bob are anonymous, they can't say *who* can read the messages
they are sending, but they might be able to say that, assuming that their
peer is following the protocol exactly (and in particular is not releasing the
shared secret) *exactly one other party* can read the message.
Note that if you have this, you can readily bootstrap pseudonymity:  Alice
and Bob simply use their secure channel to agree on a shared secret, or on
pseudonyms they will henceforth use between themselves.  If there were a
MITM, he could of course impersonate each to the other ever afterward.
The Interlock Protocol doesn't provide this - it prevents the MITM from
modifying the exchanged messages, but can't prevent him from reading them.
It's not clear if it can be achieved at all.  But it does make sense as a
security spec.

@_date: 2003-10-03 15:28:22
@_author: Jerrold Leichter 
@_subject: anonymous DH & MITM  
But Mallet is violating a requirement:  He is himself passing along the
information Alice and Bob send him to Bob and Alice.  No notion of secrecy
can make any sense if one of the parties who legitimately *has* the secret
chooses to pass it along to someone else!
Yes - but an interactive form of it.
As long as Mallet continues to interpose himself in *all* subsequent sessions
between Alice and Bob, he can't be detected.  But suppose each of them keeps
a hash value that reflects all the session keys they think they ever used in
talking to each other.  Every time they start a session, they exchange hashes.
Whenever Mallet is present, he modifies the messages to show the hash values
for the individual sessions that he held with each party seperately.  Should
they ever happen to form a session *without* Mallet, however, the hashes
will not agree, and Mallet will have been detected.  So the difference isn't
just notional - it's something the participants can eventually find out about.
In fact, if we assume there is a well-known "bulletin board" somewhere, to
which anyone can post but on which no one can modify or remove messages, we
can use it as to force a conversation without Mallet.  Alice and Bob can:
If not, Mallet was at work.  (For this to work, the bulletin must have a
verifiable identity - but it's not necessary for anyone to identify himself to
the bulletin board.)

@_date: 2003-10-03 16:51:33
@_author: Jerrold Leichter 
@_subject: anonymous DH & MITM  
I didn't say I had a protocol that would accomplish this - I said that the
notion was such a protocol was not inherently self-contradictory.
There's nothing to be true or false:  It's a definition!  (And yes, DH does
not provide a system that meets the definition.)
The reference was missing; I'd be interested in seeing it.
I have no idea!
If Alice and Bob wish to establish pseudonyms for future use, they can.  No
one says they have to.  On the other hand, "linkability" is a funny property.
If Alice and Bob each keep their secrets, and they each believe the other
party keeps their secrets, then if there is *anything* unique in their
conversations with each other that they keep around - like the sessions keys,
or the entire text of the conversation - they can use *that* to link future
conversations to past ones.  (No one without access to the secrets can do
that, of course.)  If you define anonymity as complete lack of linkability,
even to the participants, you're going to end up requiring all participants to
forget, not just their session keys, but everything they learned in their
conversations.  Perhaps there are situations where that's useful, but they
strike me as pretty rare.

@_date: 2003-10-03 18:41:23
@_author: Jerrold Leichter 
@_subject: anonymous DH & MITM  
Why not?
They can't be *enforced*, but violations can (perhaps) be detected.
The shared secret is the session key.  Assuming the encryption is sufficient,
the security of this shared secret implies the security of all data exchanged
on the link.
"Every being able to communicate over an unintermediated channel" can mean
two things:
Case 2 is much weaker than case 1, but is sufficient to detect that Mallet
has been playing games.  In fact, even case 2 is stronger than needed:
Suppose that there are multiple Mallet_i playing MITM.  Any given connection
may go through any subset of the Mallets.  Any time a connection happens
not to go through a particular either Mallet that "usually" talks directly
to Alice or Bob, the game is up.
This is no longer MITM as such.  At what point do we say that Mallet is
simply having independent conversations with Alice and Bob?
In any case, if Alice and Bob use the Interlocked Protocol, they can tell
each other what the hash is supposed to be, and Mallet can't change the
The bulletin board isn't anonymous - it's well-known, in the sense of having
a well-known public key.  Mallet can't interpose on anyone's connection to the
bulletin board.  However, it's not a CA - it knows nothing about anyone else's

@_date: 2003-10-03 20:19:33
@_author: Jerrold Leichter 
@_subject: anonymous DH & MITM  
OK, let's set up two different scenarios:
If, indeed, you have a full "brain in a jar", and Mallet *always* manages to
interpose himself, then, yes, this situation is almost certainly undetectable.
I've learned not to make snap judgements on stuff like this - too many
"clearly impossible" things turn out not to be.  In fact, I find the
distinction between cases 1 and 2 quite surprising!

@_date: 2003-10-05 06:25:39
@_author: Jerrold Leichter 
@_subject: how to defeat MITM using plain DH, Re: anonymous DH & MITM 
[Using multiple channels on the assumption that the MITM can't always get all
of them.]
This is starting to sound like some very old work - to which I don't have a
reference - on what was called the "wiretap channel".  Basic idea:  Alice and
Bob wish to talk; Carol can listen in to everything, but her tap isn't
perfect, so she gets a BER that's slightly higher.  Alice and Bob can then
choose a code (in the information-theory sense, not the crypto sense) that is
fine-tuned to exactly match their BER - and also has the property that if you
have one more bit error than the code supports, you can't decode at all.
They get through, Carol gets nothing.
The same idea has been revived in creating CD's that work in audio players but
not PC's (which hvae CD drives that typically are not willing to tolerate as
high an error rate.)

@_date: 2003-10-05 07:29:06
@_author: Jerrold Leichter 
@_subject: Protocol implementation errors 
I think there's a bit more to it.
Properly implementing demarshalling code - which is what we are really talking
about here - is an art.  It requires an obsessive devotion to detailed
checking of *everything*.  It also requires a level and style of testing that
few people want to deal with.
Both of these are helped by a well-specified low-level syntax.  TLV encoding
lets you cross-check all sorts of stuff automatically, once, in low-level
calls.  Ad hoc protocols scatter the validation all over the place - and
some of it will inevitably be overlooked.
A couple of years back, the place I work decided to implement its own SNMP
library.  (SNMP is defined in ASN.1)  We'd been using the same free library
done at, I think, UC San Diego many years before, and were unhappy with many
aspects of it.  The guy we had do it had the right approach.  Not only did he
structure the code to carefully track and test all "suspect" data, but he also
wrote a test suite that checked:
The proof of the pudding came a couple of years later, when a group a OUSPG
(Oulu University Secdure Computing Group) came up with a test that ripped
through just about everyone's SNMP suites, leading to a CERT advisory and a
grand panic.  Our stuff passed with no problems, and in fact when we looked at
OUSPG's test cases, we found that we had almost certainly run them all in some
form or another already, since they were either in our fixed test list or were
covered by the randomized testing.
OUSPG's efforts were actually directed toward something more on point to this
discussion, however:  Their interest is in protocol test generation, and their
test cases were generated automatically from the SNMP definitions.  This kind
of thing is only possible when you have a formal, machine-readable definition
of your protocol.  Using ASN.1 forces you to have that.  (Obviously, you can
do that without ASN.1, but all too often, the only definition of the protocol
is the code.  In that case, the only alternative is manual test generation -
but it can be so hard to do that it just doesn't get done.)
BTW, the OUSPG saga is another demonstration of the danger of a monoculture.
There are, it turns out, vey few independent implementations of SNMP around.
Just about everyone buys SNMP support from SNMP Research, which I believe
sells a commercialized version of the old UCSD code.  Find a bug in that, and
you've found a bug in just about every router and switch on the planet.

@_date: 2003-10-07 11:37:33
@_author: Jerrold Leichter 
@_subject: Simple SSL/TLS - Some Questions 
This sounds nice in principle, but if you follow that where it goes, you are
left with GPL - not even LGPL.  There's no way for a library to protect itself
against code it is linked with.  (Where's TOPS-20 when you need it....)  Even
if you let me completely rebuild the crypto library you've shipped with your
product, why should I trust it?
This is not really a soluable problem.  Going full GPL will render your project
useful for just about any commercial use.  Anything less leaves even someone
with the necessary skills, desire, and time in a position where they can't
say anything meaningful about the system in which the code is embedded.
Check your employment contract carefully.  Many contracts these days basically
say "if you invented/developed/wrote it while you worked for us, it's ours".
(Sometimes there are provisions like "if it's related to what we sell", but
when you look more closely, they will define "what we sell" so broadly - and
then add the ability to change their minds later anyway - that it doesn't
change anything.)
Lawsuits about this sort of stuff get ugly and *very* expensive.  Just because
your current employer is reasonable doesn't mean it won't be acquired by
someone who isn't.  Doing a bit of up-front checking is only prudent.
BTW, someone remarked in another issue that you could put of your choice of
license from among a variety of possibilities until later.  Well ... yes and
no.  Once something goes out the door under less restrictive terms, you can't
add restrictions later.  Let a copy go out with the phrase "released to the
public domain" and it's no longer yours - it's everyone's.  Let it out under
a BSD license, and you can't apply GPL terms later.  (You can apply stricter
terms to new versions, but if you try that, the resulting confusion will kill
any possibilities of broad acceptance:  You'll end up with a competition
between your restrictive versions and evolved versions of less-restrictive
ones that other people do because they aren't willing, or can't accept your
later restrictions.)

@_date: 2003-10-07 13:12:32
@_author: Jerrold Leichter 
@_subject: Open Source (was Simple SSL/TLS - Some Questions) 
No commercial software is ever going to use your software based on those
requirements.  Oh, sure, the software vendors all write language into their
contracts that try to dump all the risk on the buyer - and the current
attempts at a class action suit against Microsoft may determine the degree to
which the courts let them get away with it.  But no one is going to let you
impose *your* language on the way they talk to their customers - unless you
make your requirement so mealy-mouthed that it's pointless, or you allow them
to hide the language so deep that no one ever sees it.
At some point, you need to decide whether you are doing this in service to
some kind of idealism about perfect cryptography for the masses, or whether
you want to improve the state of the world by making it easier for honest
vendors to easily provided good crypto.
I'll let my bias show:  If the vendor of your software wishes to attack you,
there's not a hell of a lot you can do about it.  It's just too hard to hide
things, if you set your mind to it.  Even accidental bugs take huge effort to
find - whether you believe that "all bugs are shallow to sufficiently many
eyes" or not, there's no bug-free open-source software out there any more than
there's bug-free closed-source (...which may simply mean that there aren't
sufficiently many eyes - but there likely never will be.)  *Deliberate* bugs
in a system of significant size?  I can't even imagine the effort necessary to
gain assurance that there aren't any, given the current state of the art.
(Sure, in an eventual world with multiple independent proved-secure compilers
and libraries for safe languages, verifiers, proof-carrying code or some such
technique ... maybe.  But the necessary infrastructure is currently way beyond
the state of the art for all but very specialized, limited domains.)

@_date: 2003-10-13 11:07:10
@_author: Jerrold Leichter 
@_subject: Software protection scheme may boost new game sales 
You should read the article - the quote is misleading.  What they are doing is
writing some "bad data" at pre-defined points on the CD.  The program looks
for this and fails if it finds "good" data.
However ... I agree with your other points.  This idea is old, in many
different forms.  It's been broken repeatedly.  The one advantage they have
this time around is that CD readers - and, even more, DVD readers; there is
mention of applying the same trick to DVD's - is, compared to the floppy
readers of yesteryear, sealed boxes.  It's considerably harder to get at the
raw datastream and play games.  Of course, this cuts both ways - there are
limits to what the guys writing the protection code can do, too.
The real "new idea" here has nothing to do with how they *detect* a copy - it's
what they *do* when they detect it.  Rather than simply shut the game down,
the degrade it over time.  Guns slowly stop shooting straight, for example.
In the case of DVD's, the player works fine - but stops working right at some
peak point.  Just like the guy on the corner announcing "first hit's free",
they aim to suck you in, then have you running out to get a legit copy to
save your character's ass - or find out how "The One" really lives through
it all.  This will probably work with a good fraction of the population.
Actually, this is a clever play on the comment from music sharers that they
get a free copy of a song, then buy the CD if they like the stuff.  In effect,
what they are trying to do is make it easy to make "teasers" out of their
stuff.  There will be tons of people copying the stuff in an unsophisticated
way - and only a few who will *really* break it.  Most people will have no
quick way to tell whether they are getting a good or a bad copy.  And every
bad copy has a reasonable chance of actually producing a sale....

@_date: 2003-10-13 12:46:52
@_author: Jerrold Leichter 
@_subject: NCipher Takes Hardware Security To Network Level 
I think this is a bit of an exaggeration, even if the net effect is the same.
Very few real efforts were made to actually produce a "provably correct" OS.
The only serious commercial effort I know of was at DEC, which actually did
a provably-correct virtual machine monitor for VAXes.  I knew some of the
guys who worked on this.  It was difficult, expensive, required a level of
integration that only a company like DEC - the did both the OS and the hard-
ware, which had to be modified to make the VMM run decently - was in a position
to provide.  But it was quite possible.
The whole thing died because by the time they finished it, the VAX was being
replaced by the Alpha, and you basically would have had to start over.
There was also an effort in England that produced a verified chip.  Quite
impressive, actually - but I don't know if anyone actually wanted the chip
they (designed and) verified.
The technical obstacles to actually doing a verified OS appear to be less
of an issue than is commonly supposed.  Unfortunately, as is often the case,
the economic factors are what kill you.  The VAX VMM designers were willing
to accept a factor of 10 slowdown to get provable security - and believed
their customers (mainly military) would too.  However, this makes for such
a small market that it's just not worth the investment.  Further, maintaining
your provable security is also difficult, expensive and time-consuming -
slowing you down in a market where everyone else is just speeding up.  And
you're certainly going to lose any time people start counting features -
which in turn means you're always going to be "non-standard", since you'll
never be able to keep up even with those standards that don't have insecurity
built in.
Could we have, say, provably-secure systems to run our water supplies and
nuclear power plants and aircraft carriers?  Technically, the answer is
probably yes.  (The question of what classes of attack you are provably
secure against is, of course, always present.  The Orange Book implicitly
defines the kinds of attacks of interest.  It's rather limited by today's
standards - a great deal has changed over the years - but it at least gives
you some solid ground to stand on.  About all you can say about today's
systems is that they are provably *in*secure against tons of well-known
attacks.)  Economically and practically, however, the answer is probably no.

@_date: 2004-04-09 16:30:27
@_author: Jerrold Leichter 
@_subject: voting 
The VoteHere system is really quite clever, and you're attacking it for not
being the same as everything that went before.
Current systems - whether paper, machine, or whatever - provide no inherent
assurance that the vote you cast is the one that got counted.  Ballot boxes
can be lost, their contents can be replaced; machines can be rigged.  We
use procedural mechanisms to try to prevent such attacks.  It's impossible to
know how effective they are:  We have no real way to measure the effectiveness,
since there is no independent check on what they are controlling.  There are
regular allegations of all kinds of abuses, poll watchers or no.  And there
are plenty of suspect results.
a)  Receipts in the VoteHere system are *not* used for recounts.  No receipt
that a user takes away can possibly be used for that - the chances of you being
able to recover even half the receipts a day after the election are probably
about nil.  Receipts play exactly one role:  They allow a voter who wishes to
to confirm that his vote actually was tallied.
b)  We've raised "prevention of voter coercion" on some kind of pedestal.
The fact is, I doubt it plays much of a real role.  If someone wants to coerce
voters, they'll use the kind of goons who collect on gambling debts to do it.
The vast majority of people who they try to coerce will be too frightened to
even think about trying to fool them - and if they do try, will lie so
unconvincingly that they'll get beaten up anyway.  Political parties that want
to play games regularly bring busloads of people to polling places.  They
don't check how the people they bus in vote - they don't need to.  They know
who to pick.
However, if this really bothers you, a system like this lets you trade off
non-coercion and checkability:  When you enter the polling place, you draw a
random ball - say, using one of those machines they use for lotteries.  If the
ball is red, you get a receipt; if it's blue, the receipt is retained in a
sealed box (where it's useless to anyone except as some kind of cross-check of
number of votes cast, etc.)  No one but you gets to see the color of the ball.
Now, even if you are being coerced and get a red ball, you can simply discard
the receipt - the polling place should have a secure, private receptacle; or
maybe you can even push a button on the machine that says "Pretend I got a
blue ball" - and claim you got a blue ball.  The fraction of red and blue
balls is adjustable, depending on how you choose to value checkability vs.
In VoteHere's system, you can't possibly verify that every vote that went into
the total was correctly handled.  You can verify that the votes *that the
system claims were recorded* are actually counted correctly.  And you can
verify that *your* vote was actually recorded as you cast it - something you
can't do today.  The point of the system is that any manipulation is likely to
hit someone who chooses to verify their vote, sooner or later - and it only
takes one such detected manipulation to start an inquiry.
Whether in practice people want this enough to take the trouble ... we'll have
to wait and see.
On what basis should an average voter trust today's systems?  How many people
have any idea what safeguards are currently used?  How many have any personal
contact with the poll watchers on whom the system relies?  Could *you* verify,
in any meaningful sense, the proper handling of a vote you cast?  Could you
watch the machines/boxes/whatever being handled?  Unless you're in with the
local politicians, don't bet on it.
Actually, it makes no difference at all.  The algorithms are public, and all
the data that goes into the calculations are published after the election.
Anyone can implement the algorithms themselves and re-run all the calculations.
There are conceivable attacks on the various random number generators, which
could be used to reveal information that the system is supposed to keep secret
- but I don't think they can be used to change the election results.  This is
one place where the system could use some kind of "hardening", but it seems
very amenable to procedural fixes - e.g., each major party contributes a
"randomization module" that it trusts, and the results are combined.  Each
randomization module is also allowed to say "I want this result checked", at
random every k votes or so, *after* the combiner has produced its value.  When
any randomization module says that, all the inputs and the combiners output
are printed, then not used.  These values are published after the election,
and a bad combiner will quickly reveal itself.

@_date: 2004-08-22 09:32:29
@_author: Jerrold Leichter 
@_subject: First quantum crypto bank transfer 
Not to attack you personally - I've heard the same comments from many other
people - but this is a remarkably parochial attitude.
Quantum crypto raises fundamental issues in physics.  The interaction of
information and QM is complex and very poorly understood.  No one really knows
what's possible.  This is neat stuff, and really nice research.  New results
are appearing at a rapid pace.
Will this end up producing something new and useful?  Who can say?  Right now,
we're seeing the classic uses for a new technique or technology:  Solving the
old problems in ways that are probably no better than the old solutions.  If
the new technique or technology is really good, it will solve *new* problems
we haven't even thought of yet.
The press will always focus on things people understand, and which seem to
have short-term relevance.  If you're objecting to researchers blowing their
own horns ... well, that's the way the world works.  It's certainly been the
way physics has had to work since it became impossible for an individual, and
ultimately even an institution like a university. to be able to fund the
experiments necessary to move forward.  Without public support, research will
starve to death.
Anyone raising objections to work on quantum crypto today might ask themselves
why they had no objection to work on traditional crypto when DES seemed, as a
practical matter, to be unassailable except by brute force that was then 15
years in the future (by which time the same "sources" who provided DES could
provide something new).  As a practical solution to practical problems, there
was no need to go further.  And all this stuff about bit commitment protocols
and BBS generators and all that impractical stuff - why bother?
Alternatively, how anyone can have absolute confidence in conventional crypto
in a week when a surprise attack appears against a widely-fielded primitive
like MD5 is beyond me.  Is our certainty about AES's security really any
better today than was our certainty about RIPEM - or even SHA-0 - was three
weeks ago?

@_date: 2004-08-23 06:50:09
@_author: Jerrold Leichter 
@_subject: First quantum crypto bank transfer 
I don't really disagree.  *This particular* announcement; indeed, many of the
recent announcements, may well rise to the level of lying - though it's always
important to separate what (a) the researchers said; (b) what some PR flak
turned it into; (c) the hash the press made of it.  However, the comments I've
seen on this list and elsewhere have been much broader, and amount to "QM
secure bit distribution is dumb, it solves no problem we haven't already
solved better with classical techniques."  Even if some snake-oil salesmen
have attached themselves to the field doesn't say research in the field is
worthless.  Should we attack Shannon because some snake-oil salesmen use his
proof of security of one-time pads to "prove" that their (non-OTP) systems are
Also, there is a world of difference between:
For QM key exchange, step 1 goes back maybe 10-15 years, and most people
thought it was a curiosity - that you could never maintain coherence except in
free space and over short distances.  Step 2 is a couple of years back, the
first surprise being that you could actually make things work through fiber,
then through a couple of Km of fiber coiled on a bench.  Step 3 is very
recent.  Yes, everyone - including some frauds - are jumping on this right
now.  The first couple of experiments are interesting; beyond that, it's
nothing new.  But look at this from a different point of view:  What are being
solved now are some of the *engineering* problems of distributing coherent QM
states across multi-Km distances in the dirty real world.  Yes, the people who
solve *that* problem usually have no particular strengths at the crypto side;
so, yes, what they are producing today isn't useful, and is oversold.  But it
*may* provide a basis for some so-far-unseen next step.
BTW, if we look at QM *computation* in comparison, we've barely made it
through Step 1.  There are still plausible arguments that you can't maintain
coherence long enough to solve any interesting problems. Some of the papers
I've seen solve the problem only in their titles:  They use a QM system, but
they seem to only make classical bits available for general use.   The
contrast between this work and QM key exchange is striking.  Will the
engineering successes with key exchange somehow feed back to solve some of the
problems with computation?  Perhaps not, but after all, transistors were
invented to build phone lines, not computers!

@_date: 2004-08-23 07:06:51
@_author: Jerrold Leichter 
@_subject: More problems with hash functions 
It strikes me that Joux's attack relies on *two* features of current
constructions:  The block-at-a-time structure, and the fact that the state
passed from block to block is the same size as the output state.  Suppose we
did ciphertext chaining:  For block i, the input to the compression function
is the compressed previous state and the xor of block i and block i-1.  Then
I can no longer mix-and-match pairs of collisions to find new ones.
Am I missing some obvious generalization of Joux's attack?
(BTW, this is reminiscent of two very different things:  (a) Rivest's work on
"all or nothing" package transforms; (b) the old trick in producing MAC's by
using CBC and only sending *some* of the final encrypted value, to force an
attacker to guess the bits that weren't sent.

@_date: 2004-08-23 11:38:53
@_author: Jerrold Leichter 
@_subject: First quantum crypto bank transfer 
Perhaps we hear them differently.
I'm not sure what "the" key distribution problem would be or what "solving" it
would mean.  As we all know, the real problem with OTP systems is that you
have to distribute as much keying material, securely, as you have material to
protect.  So OTP pretty much comes down to leveraging a single secure channel
to produce another.  In all practical instances I know of, the two channels
are separated in time and space:  You leverage the security of your diplomatic
pouch today to get secure messages from a spy tomorrow.
QM key sharing lets you build an OTP with a shared transmission medium and an
arbitrarily small time separation.  This is new.  It gives you guarantees that
the bits sent have not been intercepted.  That's new. Certainly, it doesn't
solve MITM attacks, as mathematical abstractions. What it does is reduce
protection from MITM attacks to protection of physical assets.  All crypto
ultimately has to rest on that - if you can't protect your keys, nothing
works.  The nature of the system that must be protected, and the kind of
protection, are somewhat different than in traditional systems, but the
inherent problem is neither eliminated nor made inherently worse.
Here, I'll pretty much agree with you.
The thought experiments on this always involve simple pictures in free space.
I agree, actually *doing* anything in free space over macroscopic distances is
a non-starter.
I think that's obvious now, but might not have been so obvious 20 years ago.
(For that matter, just how long have we had usable multi-km single-mode
Actually, they started off pointing out that error correction couldn't be
done in QM systems without unmixing the states, thus losing the essense of the
computation.  Well, it turned out that things are more subtle than that.
Don't take this as a criticism of those who sayd quantum error correction was
impossible!  This is all new, complex physics.  We're wrong before we're
You miss my point.  Papers have been published _ there's not much point
dredging them up - whose title and abstract implies that they are providing a
way to store and manipulate qubits, but when you look at what they actually
end up providing, you can't *use* them as qubits, just classical bits.  (What
a surprise:  There are poor papers published that miss the point of the
On the contrary.  The point is to show that, for whatever reasons, quantum key
sharing (or whatever you want to call it; quantum *cryptography* it isn't any
more than a quantum-based random bit generator is quantum cryptography) has
progressed to the point of engineering practicality much faster than quantum
computation.  (Personally, I'm not surprised - it strikes me as an inherently
simpler problem.)  Note that "engineering practicallity" doesn't in and of
itself imply usefullness!  (The converse *is* true:  What can't be reduced to
engineering practicality will probably never be very useful.)
Again, not at all.  Quantum computation has a long way to go, and will likely
tell us significant, interesting things about the world even if it never
produces any practical results.  Quantum key sharing is pretty much a closed
book as far as theory and even physics is concerned.  What there is to be
learned from it was probably learned years back in analyzing the thought
experiment first proposed by, as I recall, Einstein, Podolsky, and Rose.
(They used the same basic setup as is used in quantum key sharing and thought
it could transfer information faster than light.  It doesn't, and why and how
it doesn't is pretty fundamental.)
You've missed my point.  Transistors were developed for message transmission
purposes.  (So, for that matter, were vacuum tubes.)  They turned out to be
the basis for modern computation, a result no one could have forseen.
We may eventually get to the point where we try to build practical quantum
computation devices.  At that point, what was learned in the process of
building quantum bit agreement systems may prove useful.  Or it may not; we
really can't say.  But certainly the history of technology shows that
techniques often turn out to be useful outside the field they originally grew
up in.

@_date: 2004-08-24 07:18:14
@_author: Jerrold Leichter 
@_subject: On hash breaks, was Re: First quantum crypto bank transfer 
...because it's been seen as giving too short a hash, and because of a minor
weakness - widely described as "certificational" - in the compression function
that no one ever showed lead to an attack.  (While the details of the current
attack aren't yet completely clear, the fact that it worked on so many
functions strongly indicates that the particular weakness in the MD5
compression function has nothing to do with it.)
The advice may have been prudent, but it doesn't rise to the level of a theory
for distinguishing good from bad hash functions.
because the NSA said so.  It turns out they were ahead of public crypto by a
couple of years.  I will grant you that this is indirect evidence that NSA
has no attacks on AES, since this is now the second time that they've
strengthened a proposed primitive against which no publically-known attacks
existed.  It tells us little about how strong AES actually is - and absolutely
nothing about any other system out there, since NSA has no reason to comment
on those and every reason not to.
...but not because anyone thought there was a weakness.  MD5 happened to be
widely used, SHA-1 had standards pushing it; little room was left for another
Moving to a larger hash function with no underlying theory isn't very far from
the "million-bit key" algorithms you see all over the place.  Bigger probably
can't be worse, but is it really better?
Real good business practice has to make judgements about possible risks and
trade them off against potential costs.  I quite agree that your advice is
sound.  But that doesn't change the facts:  Our theoretical bases for security
are much weaker than we sometimes let on.  We can still be surprised.
Suppose a year ago I offered the following bet:  At the next Crypto, all but
one of the widely-discussed hash functions will be shown to be fundamentally
flawed.  What odds would you have given me?  What odds would you have given me
on the following bet:  At the next Crypto, an attack against AES that is
substantially better than brute force will be published?  If the odds were
significantly different, how would you have justified the difference?
Let's update the question to today:  Replace "widely-discussed hash functions"
with "SHA-1 and the related family".  Keep the AES bet intact.  But let's got
out 5 years.  Now what odds do you give me?  Why?

@_date: 2004-08-24 07:32:04
@_author: Jerrold Leichter 
@_subject: More problems with hash functions 
Not true.
Joux's attack says:  Find single block messages M1 and M1' that collide on
the "blank initial state".  Now find messages M2 amd M2' that collide with
the (common) final state from M1 and M1'.  Then you hav four 2-block
collisions for the cost of two:  M1|M2, M1'|M2, and so on.
But even a simple XOR breaks this.  M1 and M1' have the same hash H, but the
state being passed is now very different:   in one case,  in the
other.  So M1|M2 and M1'|M2 are completely different:  Both started the second
step with the compression function in state H, but the first compressed
M1 XOR M2, and the second M1' XOR M2.
All I'm left with, unless there's some cleverer attack I'm missing, is:
Then for the cost of finding two block-level collisions, I have a collision
between two-block messages (M1|M2 and M1'|M2').  But why bother?  It's always
been sufficient to find a collision on the *last* block, with an arbitrary
initial state.  (It would be nice to eliminate *this* generic weakness, but
it's not clear you can and still have an on-line algorithm.)

@_date: 2004-08-27 08:51:01
@_author: Jerrold Leichter 
@_subject: More problems with hash functions 
There's a more general problem with the algorithm:  It isn't on-line.  An
implementation requires either an unbounded internal state, or the ability to
re-read the input stream.  The first is obviously impossible, the second a
problem for many common uses of hash functions (in communications, as opposed
to storage).
However ... *any* on-line algorithm falls to a Joux-style attack.  An
algorithm with fixed internal memory that can only read its input linearly,
exactly once, can be modeled as an FSM.  A Joux-style attack then is:  Find
a pair of inputs M1 and M1' that, starting from the fixed initial state, both
leave the FSM in state S.  Now find a pair of inputs M2 and M2' that, starting
from S, leave the FSM in state S'.  Then for the cost of finding these two
collisions, we have *four* distinct collisions as before.  More generally,
by just continuing from state S' to S'' and so on, for the cost of k
single collision searches, we get 2^k collisions.
The nominal security of the FSM is its number of states; for k large enough,
we certainly get "too many" collisions compared to a random function.
What this shows is that our vague notion that a hash function should behave
like a random function can't work.  On-line-ness always kills that.  (In
fact, even given the function random access to its input doesn't help:  An FSM
can only specify a finite number of random "places" to look in the input.
If the FSM can only specify an absolute location, it can't work on arbitrarily
long inputs.  If it can specify a location relative to the current position,
you can re-write the machine as a linear one that gets repeated copies of
blocks of a fixed size.)
This is really just the prefix property writ large.  Define an on-line
function f:{0,1}*->{0,1}^k as one with the property that there exist
infinitely many pairs of strings Si, Si' with the property that, for any S in
{0,1}*, f(Si||S) = f(Si'||S).  (In particular, this is true for S the empty
string, so f(Si) = f(Si').)  Then the most we can expect of an (on-line)
hash function is that it act like a function picked at random from the set of
on-line functions.  (This, of course, ignores all the other desireable
properties - we want to choose from a subset of the on-line functions.)
Getting a security parameter in there is a bit tricky.  An obvious first cut
is to say an on-line function has minimum length n if the shortest Si has
length n.
Actual hash functions don't follow this model exactly.  For example, MD5 needs
512+128+64 bits of internal storage* - the first for the input buffer, all of
whose bits are needed together, the second for the running hash state, the
third for the length.  So there are 2^704 states in the MD5 FSM, but the
output only has 2^128 values - only 128 bits of the "state number" are used.
That in and of itself isn't an issue - it doesn't hurt, in this particular
analysis, to throw bits away in the *final* result.  What does limit it is
that every 512 input bits, the FSM itself logically "mods out" 512 bits of the
state (the input buffer), and ends up in one of "only" 2^192 possible states
(and if we work with "short" strings, then only a small fraction of the 2^64
states of the length field are actually accessible).  Because of this, MD5 can
at best have been chosen from on-line functions of minimum length 128, rather
than those of a minimal length up to 704.  That's why keeping more internal
state *might* help - though you have to be careful, as we've seen, about how
you do it.
* Actually, you need 3 more bits because the MD5 length is in octets, not
bits. This memory is hidden in any real implementation on byte-oriented

@_date: 2004-08-28 07:08:11
@_author: Jerrold Leichter 
@_subject: More problems with hash functions 
It all depends on how you define an attack, and how you choose to define your
security.  I explored the "outer edge":  Distinguishability from a random
function.  For a random function from {0,1}*->{0,1}^k, we expect to have to do
2^k work (where the unit of work is an evaluation of the function) per
collision.  The collisions are all "independent" - if you've found N, you have
... N.  The next one you want still costs you 2^k work.  However ... no on-
line function can actually be this hard, because a Joux-style attack lets you
combine collisions and find them with much less than the expected work.
Because a Joux-style attack works exponentially well, the cost for a very
large number of collisions is arbitrarily small.  Note that this has nothing
to do with the number of internal states:  One can distinguish random on-line
functions (in the sense I defined) from random functions (My criterion of
"infinitely many extendible collision pairs" may be too generous; you may need
some kind of minimum density of "extendibile collision pairs".)
You can certainly "de-rate" such a function by discarding some of the bits
of the output and considering the range of the output to be {0,1}^l, l < k.
But I don't see how that helps:  In the limit, collisions become arbirarily
cheap, and making collisions easier can only decrease the cost of finding
If the question is how close to the ultimate limit you can get a function
*constructed in a particular way*, then, yes, passing more internal state may
help.  In fact, it's the only thing that can, if you want an on-line
algorithm - you have nothing else available to vary!  A full analysis in this
direction, though, should have *two* security parameters:  The current one,
the size of the output; and the amount of memory required.  After all, MD5
(for example) is only defined on inputs of up to 2^64 bytes.  If I allow 2^64
bytes of internal state, then the "on-line" qualifier becomes meaningless.

@_date: 2004-01-04 17:09:18
@_author: Jerrold Leichter 
@_subject: Difference between TCPA-Hardware and a smart card (was: example: 
Precisely - though see below.
Smart cards are intended to work this way, too.
This kind of thing goes *way* back.  In the '70's, there was a company - I
think the name was BASIC 4 - that sold a machine with two privileged levels.
The OS ran at level 1 (user code at unprivileged level 2, of course).  There
were some things - like, probably, accounting - that ran at level 0.  Even
with physical access to the machine, it was supposed to be difficult to do
anything to level 0 - unless you had a (physical) key to use in the lock
on the front panel.  The machine was intended as a replacement for the then-
prevalent time-sharing model:  An application developer would buy machines
from the manufacturer, load them with application environments, and sell
application services.  Users of the machines could use the applications with
fast local acceess, even do development - but could not modify the basic
configuration.  I know the company vanished well before networks got fast
enough, and PC's cheap enough, the the business model stopped making any
sense; but I know nothing of the details.
There's no more reason that the manufacturer has to be trusted than that the
manufacturer of a safe has to be trusted (at least in the sense that neither
needs to know the keys/combination on any particular machine/safe).  If
machines like this are to be built, they should require some special physical
override to allow the keys to be configured.  A key lock is still good
technology for this purpose:  It's a very well-understood technology, and its
simplicity is a big advantage.  A combination lock might be easier to
integrate securely, for the same basic reason that combination locks became
the standard for bank vaults:  No need for an open passageway from the outside
to the inside.  (In the bank vault case, this passageway was a great way to
get nitroglycerin inside the locking mechanism.)  In either case, you could
(like a bank) use a form of secret sharing, so that only a trusted group of
people - with multiple keys, or multiple parts of the combination - could
access the key setup mode.  Given this, there is no reason why a machine fresh
from the manufacturer need have any embedded keys.
Will machines like this be built?  Probably not, except for special purposes.
The TCPA machines will likely require you (and the people who want to do
DRM on them) to trust the manufacturer.  The manufacturers will be
"trustworthy" to Disney and friends because of licensing agreements and such.
You would have to trust them because you would not be given the choice.
BTW, David Wagner's comment - that you ultimately have to trust root - is a
sentiment echoed in some Microsoft quote about how your security is only as
good as your administrator.  This illustrates the primitive approach to
security that we've all been forced to live with.  Bank vaults are designed so
that even the manufacturer's representative doesn't have to be trusted (much).
ATM's are designed so that neither the people who regularly load or unload
them, nor the repairmen, nor even the programmers who develop the applications
that talk to them, have to be trusted.  (The degree to which they meet such
goals is another question, of course.  People do break into bank vaults, even
today - there's no such thing as perfection in the real world.)  Separation of
responsibilities is the basic technique - but for some reason we don't seem to
apply it to our computers.  Mainframe environments *do* try to apply this
principle - you can divide things so that operators, system programmers, and
security officers have separate domains of control, and no one of them can
subvert everything.  Again, how well this works is subject to debate - I have
no first-hand knowledge, but suspect we'll hear from the Wheelers.
A great example of how little understood this idea:  VMS has for years allowed
you to have two passwords on an account.  You need to provide both to log in.
Virtually everyone who sees this thinks it's just a way to have longer
passwords.  A few people will comment that it makes password guessing harder -
since you are always prompted for the second password, you can't guess the two
sequentially.  Almost no one seems to understand that this has to do with
separation of responsibilities:  You can give one password to each of two
people, and know that both have to be present for the account to be used.
The Unix "root can do anything" philosophy on one side, with the "it's a
*personal* computer, the owner/administrator can do anything" philosophy on
the other, has really warped our thinking on security.  (Example:  Does the
guy who minds backup tapes need to be trusted?  On virtually all systems, yes
- what's to stop him from duplicating or just stealing a tape?  On a securely
designed system, no:  His account runs a captive backup procedure.  It, not he
individually, has access to all files on the system.  It, no he, securely
provides an encryption key that cannot - without significant cost - be
extracted.  The tapes are written encrytpted,  They can be duplicated or
stolen - but the information on them will be secure.

@_date: 2004-01-05 13:02:49
@_author: Jerrold Leichter 
@_subject: Walton's Mountain notaries  
For all the joking about this ... ever see what's involved in getting a
security clearance?  You have to provide a record of everywhere you every
lived.  FBI agents will (sometimes) go out and talk to people there about
you.  The implication is clear:  Paper (much less on-line) records can be
faked.  If you *really* want to be sure that the Carl Ellison you are giving
a clearance is really who he claims he is, go for physical evidence and
people's memories.  *Much* harder to fake.  (Of course, that's also why it's
so expensive to get a security clearance.)

@_date: 2004-01-07 10:14:58
@_author: Jerrold Leichter 
@_subject: [Fwd: Re: Non-repudiation (was RE: The PAIN mnemonic)] 
Now that we've trashed non-repudiation ... just how is it different from
authentication?  In both cases, there is a clear technical meaning (though as
with anything in mathematics, when you get right down to it, the details are
complex and may be important):  To produce an authenticator/non-repudiable
signature, you must have access to the secret.  There isn't, at this level,
even any difference between the requirements for the two.  Where we get into
trouble is in attempting to bind the real world to the mathematics.  In each
case, the receiver wants to be able to say:
     1.	I can rely on the fact that X sent me this data, because it came
What he *really* needs to say is:
     2.	I can rely on the fact that X sent me this data, because it came
To go from 2 to 1, the receiver must also have:
     3.	I can rely on the fact that only X knows X's secret.
In ordinary English usage, there is little difference between "I've authenti-
cated this message as coming from X" and "X can't deny that he wrote this
message."  We've learned that "non-repudiation" is a concept with relatively
little use in the legal system.  However, authentication (of a signature,
document, whatever) is quite common (even if for the usual kinds of objects
that need authentication, there is generally little to discuss).  If the
ultimate question is whether, as a legal matter, X is bound by some writing
or whatever, authentication gets at the same basic question (which is only
part, usually a small part, of the relevant legal issues).
The problems that we've been discussion here are clear from 2 and 3:
So in fact we can't even begin to get 3; at best, we have:
    3'. I can rely on the fact that, if X has shared his secret with Y (where
This is now so bizarre and removed from ordinary notions that it should be
clear why it's unlikely be of much real-world use!

@_date: 2004-01-07 14:37:22
@_author: Jerrold Leichter 
@_subject: [Fwd: Re: Non-repudiation (was RE: The PAIN mnemonic)] 
There is absolutely *no* cryptographic or mathematical content to this
definition!  It could as well apply to key locks, to signatures on paper,
or whatever.  It's in no way a property of a cryptographic system, or of
*any* system.  Nor, as written, is there even any possible set of evidence
that could be adduced to prove this:  After all, someone might, just by
chance, have guessed the private key.
Granted, there are significant issues with non-repudiation - so significant
that it probably isn't a very useful concept.  But it there *is* some
cryptographic content behind it!  Otherwise, what are we to make, for example,
of the various "evolving signature key" schemes that detect stolen keys?

@_date: 2004-07-09 18:52:04
@_author: Jerrold Leichter 
@_subject: EZ Pass and the fast lane .... 
It would, in principle, be relatively easy to query these boxes yourself, or
listen in near a station.  You could quickly build up a database of valid
ID's, and could then build/sell a clone box, perhaps a "tumbler" box that
would rotate among valid ID's.
The actual money involved can be substantial - in the NY area, a cross-Hudson
-River commuter spends at least $5/day through EZ-pass, and you can now charge
things like parking at airports - $25/day or more.  So ... you'd think there
would be an active market in rigged EZ-pass boxes by now (as, for example,
there has been an active market for counterfeit monthly passes on the commuter
rail lines in the New York area.)  Curiously, if there is such a thing, it's
so far on a low enough scale that the press hasn't picked it up.
The basic protection mechanism involved is apparently quite simple:  Every
time you use EZ-pass, a photo of your license plate, and of the driver, is
taken.  The photos are kept for quite some time.  So cheaters can be tracked.
In addition, where there are high-value charges, there is usually a gate.  If
your EZ-pass is invalid, you're stuck in what is effectively a man-trap,
waiting for the cops on duty to check things out.  You'd better have a valid
EZ-pass to show them.  I don't know how much info they can get out of the
system, but it could easily tell them if, when they scan your "good" pass,
it shows a different ID from the one registered before.  (On the other hand,
high-speed readers - where there is no gate - are spreading.  Several were
recently installed at the Tappan-Zee Bridge, where the toll is $7.)
All in all, the system seems to depend on what I've heard described as the
"bull in the china shop" theory of security:  You can always buy more china,
but the bull is dead meat.

@_date: 2004-07-12 09:44:07
@_author: Jerrold Leichter 
@_subject: EZ Pass and the fast lane .... 
EZpass actually went in the opposite direction.  When I got my EZpass a number
of years back, they provided such a bag, along with instructions on use. These
days, they no longer provide the bag, and indirectly they strongly discourage
you from using any such thing:  According to the rules, EZpasses must be
mounted on your windshield:  They provide a variant on Velcro strips, which
make the box a pain to remove while driving.  (For commercial vehicles,
there's an external, permanently-mounted version).  People used to just keep
the thing loose inside the car and wave it at the sensor, which apparently
caused to many misreads, leading to traffic backups.  Now, if they catch you
doing that, there's a substantial fine.

@_date: 2004-07-12 09:56:38
@_author: Jerrold Leichter 
@_subject: EZ Pass and the fast lane .... 
A local TV station here in the NY area did a show about a lower-tech version
of the same thing:  A plastic cover for the plate that is supposed to cause
enough glare in a camera that the plate is unreadable when snapped by the
various automated speed traps and red-light-running traps out there.  These
things are apparently advertised in all the car magazines.  According to the
TV show, they vary in effectiveness, from quite effective for some kinds of
cameras in certain uses to pretty much ineffective.
A universal feature of all such devices is that they are illegal.  At least
around here (and I think in most if not all states), license plates may not be
covered *at all*.  If any kind of device emerged that was effective at
actually making plates unreadable, I can easily see municipalities make using
one into a parking violation - a quick source of revenue, at least until most
people figured out that it wasn't worth it to buy these things.
How long before license plates have transponders built into them?  After all,
it's long-established law that you can be required to place an identifier on
your car when it's on the public roads - why's there a difference between one
that responds at optical frequencies and one that responds at a couple of
gigahertz?  (For that matter, even if you want to stick to optical and you
can't get plate reading accurate enough, the technology for reading bar codes
from moving vehicles is well-developed - it's been used for years to identify
railroad cars, and many gated communities use them to open the gates for cars
owned by residents.)

@_date: 2004-07-19 10:25:49
@_author: Jerrold Leichter 
@_subject: dual-use digital signature vulnerability 
Fascinating.  They are trying to re-create the historical, mainly now lost,
role of a notary public.
Notary publics came into being at a time when many people were illiterate,
and only the wealthy had lawyers.  A notary public had two distinct roles:
The first role is long lost.  Notary publics don't look at the material being
signed.  We lost our trust in a "public" official explaining contracts - the
assumption now is that everyone gets his own lawyer.  The second role
remained, even with universal literacy.  A notary public is supposed to check
for some form of "good ID" - or know the person involved, the traditional
A traditional "notary public", in modern terms, would be a tamper-resistant
device which would take as inputs (a) a piece of text; (b) a means for
signing (e.g., a hardware token).  It would first present the actual text
that is being signed to the party attempting to do the signing, in some
unambiguous form (e.g., no invisible fonts - it would provide you with a
high degree of assurance that you had actually seen every bit of what you
were signing).  The signing party would indicate assent to what was in the
text.  The notary might, or might not - depending on the "means for signing" -
then authenticate the signer further.  The notary would then pass the text to
the "means for signing", and verify that what came back was the same text,
with an alleged signature attached in a form that could not modify the text.
(E.g., if the signature were an actually RSA signature of the text, it would
have to decrypt it using the signer's public key.  But if the signature were
a marked, separate signature on a hash, then there is no reason why the notary
has to be able to verify anything about the signature.)  Finally, the notary
would sign the signed message itself.
We tend not to look at protocols like this because we've become very
distrustful of any 3rd party.  But trusted 3rd parties have always been
central to most business transactions, and they can be very difficult to
replace effectively or efficiently.

@_date: 2004-07-19 12:02:42
@_author: Jerrold Leichter 
@_subject: dual-use digital signature vulnerability 
...which makes for an interesting example of thw way in which informal
understandings don't necessarily translate well when things are automated.
The law school professor of a friend of mine told a story about going to rent
an apartment.  The landlord was very surprised to watch him sign it with only
a glance - not only was this guy a law professor, but he had done a stint as a
Housing Court judge.  "Aren't you going to read it before signing?"  "No -
it's not enforceable anyway."  (This is why there have been cases of landlords
who refused to rent to lawyers - a refusal that was upheld!)
If you are offered a pre-drafted contract on a take-it-or-leave it basis -
the technical name is an "adhesion contract", I believe - and you really need
whatever is being contracted for, you generally *don't* want to read the
thing too closely....
When you buy a house these days, at least some lawyers will have you initial
every page of the agreement.  Not that there is anything in there you want to
read too closely either.  (The standard terms for the purchase of a house in
Connecticut have you agree not to "use or store" gasoline on the property.  I
pointed out to my lawyer - who had actually been on the committee that last
reviewed the standard form - that as written this meant I couldn't drive my
car into the garage, or even the driveway.  His basic response was "Don't
worry about it.")
The black-and-white of a written contract makes things appear much more
formal and well-defined than they actually are.  The real world rests on many
unwritten, even unspoken, assumptions and "ways of doing business".  It's
just the way people are built.  When digital technologies only *seem* to match
existing mechanisms, all kinds of problems arise.  Despite such sayings as
"You can't tell a book by its cover", we trust others based on appearances
all the time.  Twenty years ago, if a company had printed letterhead with a
nice logo, you'd trust them to be "for real".  Every once in a while, a con
man could abuse this trust - but it was an expensive undertaking, and most
people weren't really likely to ever see such an attack.
Today, a letterhead or a nice business card mean nothing - even when they are
on paper, as opposed to being "just bits".  It's really, really difficult to
come up with formal, mechanized equivalents of these informal, intuitive

@_date: 2004-06-08 11:35:46
@_author: Jerrold Leichter 
@_subject: Passwords can sit on disk for years 
... until you hibernate your laptop, at which point all of memory gets written
to disk.
Or until the program gets run under a virtual machine, and the entire VM gets
swapped out.
Or ...
I agree that locking pages containing sensitive data into memory is prudent,
but Garfinkel's point is well taken:  The model of memory presented by existing
general-purpose OS's is just too complex for anyone to understand the security
implications.  OS support for "red pages" - whatever exactly that should mean -
would be a useful thing.  (Even then, the VMM scenario is complex to handle.
Really, a "red page" needs to be "red" all the way through all levels of
virtualization.  Very low level, or even hardware, support might even prove
useful - e.g., if for whatever reason the data in the physical page frame
needs to be copied (after a soft ECC error?), zero the previous page frame.)

@_date: 2004-06-15 18:08:37
@_author: Jerrold Leichter 
@_subject: Is finding security holes a good idea? 
I don't find that argument at all convincing.  After all, these bugs *are*
being found!
It's clear that having access to the sources is not, in and of itself,
sufficient to make these bugs visible (else the developers of close-source
software would find them long before independent white- or black-hats).
Something else accounts for their surfacing.  I would guess that at least two
factors are involved:
To the extent these are true, a white-hat could reasonably argue that a
newly-found bug is unlikely to be rediscovered only if it is neither closely
related to previously-found bugs; nor found using a new technique.  But these
are exactly the cases in which you would *want* to publish!

@_date: 2004-10-21 14:09:19
@_author: Jerrold Leichter 
@_subject: Printers betray document secrets 
Actually, they say they can identify the make and model - which is about all
you could do with a typewriter.  Going further, in either case, means tying a
particular piece of text to a particular writing instrument to which you have
gained access.
Changing printer cartridges will certainly work, but then again simply replac-
ing the typewriter will, too.  Any identification of physical objects can only
work as long as the physical object isn't replaced.  In practice, there's a
great deal of inertia in replacing physical objects, for cost, convenience, and
other reasons.  So such identifications may still be useful.
A bullet can't be tied to a gun's serial number, but that doesn't make it
useless to examine bullets.
The technique is based on variations in dot pattern that ultimately come down
to small variations in mechanical parts, usually the gears that drive the
paper.  Laser printer cartridges are deliberately designed so that (just
about) all moving/wearing parts are part of the cartridge.  So most variations
in the results are necessarily tied to the cartridge.  That's not true for ink
jets. While the paper describing all this isn't yet available, from what is
published I don't think they are making any claims about inkjets, just laser
printers. However, they seem to believe the same general approach - look for
variations due to variations in manufacture that don't produce artifacts that
are visible to the naked eye, so don't need to be and hence are not controlled
- would work.  Whether the source of the variation would be in the ink
cartridge or in the fixed mechanicals, who can say at this point.
Actually, this would probably be noticable in certain pictures.  But slight
variations in pixel spacing - which is what these guys look for - is not
visible.  (In fact, the origin of this work seems to have been work in the
opposite direction:  Early laser printers had a problem with banding, due to
periodic variations in paper movement causing variations in pixel spacing.
The trick was to find out how much variation you could allow without visible
artifacts and then get to that level cheaply.  But there is still plenty of
variation left for appropriate software to find.)  You could probably play
games with pixel sizes, too.
One could say the same about most physical objects that end up being used
for identification.  You would think that fibers would be useless for
identification, for example - you can always throw out the clothing you were
wearing and buy a new tee shirt.  Still ... the real world has a great deal
of inertia.

@_date: 2004-09-01 18:05:24
@_author: Jerrold Leichter 
@_subject: "Approximate" hashes 
I had a look at the code (which isn't easy to follow).  This appears to be a
new application of Bloom filters.

@_date: 2005-08-04 16:28:57
@_author: Jerrold Leichter 
@_subject: [Clips] Escaping Password Purgatory 
Hmm.  I came up with the same idea a while back - though with a different constraint:  I think it's reasonable to trade off the one-wayness of the
hash for the ability to work out the password with pencil and paper when
necessary.  Various classic pencil-and-paper encryption systems can be bent
to this purpose.  Since the volume of data "encrypted" is very small and it's
hard for an attacker to get his hands on more than tiny samples - a given
web site only sees its own password - you don't need much strength to give a
reasonable degree of protection.

@_date: 2005-08-04 17:07:45
@_author: Jerrold Leichter 
@_subject: Query about hash function capability 
Rotate the input string until it has the smallest possible value among all possible rotations.  "Possible rotations" are those that you want to consider equivalent under the hash - if you want just "ab" and "ba" as ASCII strings to be equivalent, then allow only rotations in units of bytes.  If you also want 0xc2c4 - the result of rotating that pair of bytes left by one bit - to be equivalent, include bit rotations in the "possible rotations".  Finally, hash using any standard algorithm.  (What this is doing is partitioning the set of inputs into equivalence classes - where two inputs are in the same equivalence class if they are intended to hash to the same value - and then replacing the input string by a unique representative of the equivalence class.  You can define equivalence classes any way you like, as long as you can compute a unique representative.  For example, a hash that ignores upper/lower case
distinctions is trivially realized by replacing all letters in the input with their lower-case equivalents.  That just chooses the all-lower-case version as the representative of the set of "case-equivalent" versions of the string.

@_date: 2005-02-02 10:19:45
@_author: Jerrold Leichter 
@_subject: how to tell if decryption was successfull? 
Without some additional information, there's no way to tell anything!  Take
the limit case:  I generate a Megabyte of random bits, encrypt them, and
send them to you.  You decrypt what you receive and see - a million random bits.  Are they the *right* random bits?  How could you possibly know?  Any key at all gives you - a million random bits.
The only way to know you decrypted correctly is to have some way to recognize a correct decryption.  Any bunch of *random* bits looks like any other - there's no possible way to tell.  What you need is a *non*-random set of bits.
If I sent you English text and what arrived looked like English text, you
can recognize that - exactly because English text is non-random at many
different levels.  The 8-bit bytes, if it's standard ASCII, all have the top
bit set to 0.  For 100 random bytes, the chance that all the top bits are 0 is 1 in 2^100 - already a pretty good test!  Beyond that, most of the characters
are going to be lower-case letters.  Certain control characters won't appear at all.  There will be statistical properties - e much more common than j.
And, of course, the letters will form words, which fit together into meaningful sentences, etc.
The way this is usually described is that English text has tons of redundant
information.  If you see a q, you can pretty much guess that u follows.  If
you see th, you can be very certain that a vowel, a space, or a punctuation
mark follows.  Etc.
The same kind of redundancy - predictability - appears in other file formats.
If you expect rot-13, rot-13 it and check for English.  If it's a ZIP'ed
file, check for the ZIP header and directory.  Etc.
In practice, if this is of concern, you *add* redundancy.  For example, you do
a checksum and append it to the message before encrypting.  Decent cryptosystems - anything worth consideration today - will, with the wrong key, produce something effectively indistiguishable from random bits.  So you can
model what you can expect from, say, a 64-bit checksum as:  What's the chance
that it will match just by pure chance?  Answer:  1 in 2^64.
Note that I'm specifically dealing only with how you know you decrypted with the right key.  Looking for inherent redundancy, and even a typical checksum, does *not* provide a solid indication of authenticity:  There are deliberate attacks that can, for example, change a message "under" the encryption but
leave the checksum intact.  There are more sophisticated techniques that
protect against such things as well.  In practice, "solid" systems use those -
and as a side-effect, end up detecting things like using the wrong key "for

@_date: 2005-02-07 10:25:42
@_author: Jerrold Leichter 
@_subject: Is 3DES Broken? 
Picking nits, but:  ECB mode is "unsafe at any speed" to encrypt an arbitrary data stream.  If the data stream is known to have certain properties - e.g., because it has undergone some kind of transform before being fed into ECB - then ECB is as good as any other mode.
After all, CBC is just ECB applied to a datastream transformed through a
particular unkeyed XOR operation.
There's a paper - by Ron Rivest and others? - that examines this whole issue,
and carefully separates the roles of the unkeyed and keyed transformations.
(I think this may be the paper where all-or-nothing transforms were

@_date: 2005-02-07 10:43:49
@_author: Jerrold Leichter 
@_subject: Is 3DES Broken? 
Many people have tried to do this.  I know of no successes that are really
practical.  (I've played around with many "obviously good" ideas myself, and
have always managed to break them with a little more thought.  Everything that gives you the desired security ends up costing much more than twice
the cost of the underlying block algorithm for a double-size block.)
The block size appears to be a fairly basic and robust property of block
ciphers.  There's probably a theorem in there somewhere - probably one of
those that isn't hard to prove once you figure out exactly what it ought to

@_date: 2005-02-08 18:43:40
@_author: Jerrold Leichter 
@_subject: Can you help develop crypto anti-spoofing/phishing tool ? 
The code Thompson describes would produce equivalent outputs[1] for all but a
very small number of specially-selected strings.  If this test caused you to
change your level of trust in the code ... it did you a great disservice.
"N-version programming" - which is what you are proposing here - can increase
your level of trust against random errors[2], but its of no use at all against
a deliberate attack.  (Recall the conversation here a couple of months ago
about how difficult - to the point of impossibility - it would be to use
external testing to determine if a crypto-chip had been "spiked".)
[1]  Since Thompson was talking about a compiler, the mapping from inputs to
outputs is a partial function.  Just because two compilers produce different
sets of bytes doesn't mean either one is wrong.  Determining the equivalence
of two programs is a very hard problem; in fact, even *defining* the equiva-
lence seems intractable.  Suppose the only difference is that the "spiked"
compiler introduces enough data-dependent speed variation to leak information
through a timing channel?
[2] BTW, test have shown that the "random error" model for bugs isn't a very
good one.  Certain kinds of code are just more likely to contain errors - and
the errors produced by completely independent programmers are often quite
strongly correlated.  So the simple-minded analysis that says that if one
program fails with probability p then the chance that of of n different
versions fail with probability p^n is way off.

@_date: 2005-02-09 10:54:47
@_author: Jerrold Leichter 
@_subject: Can you help develop crypto anti-spoofing/phishing tool ? 
All you've again proved is that you're misapplying the results.
Consider again the attack we are discussing:  There is one very particular
"magic" input that produces special, pre-determined results.  In the case of
the ACM paper, a particular input program produces slightly different code.
But let's take a simpler example:  A password-checking program that has a
magic passcode that, when entered, is always accepted.
Since you weren't clear how you were going to use your N versions, let's try
two models:
If we do indeed assume the second model, things get more difficult for the compiler.  What do you verify?  The object code from the N compilers all differ.  You can't verify anything useful at that level.  Maybe you have some kind of symbolic execution environment, and get use it for checking.  Do you trust *it*?  What compiler was *it* built with?  In practice, you have to keep the N versions of each of your input programs all compiled by the N different compilers running in parallel forever, and compare their outputs.  (And how many different linkers do you have?  Run-time loader?  Presumably you don't keep around N^2 or N^3 or N^4 versions - somehow you pick a representa-
tive set.  How?  Doesn't sound easy, against an intelligent attack.)
Anyway, suppose you've chosen your redundant set and can afford to run all the copies indefinitely.  For something like a password checker, you have Boolean outputs and can easily compare.  But let's try something more complex, say an implementation of a signature algorithm like DSA.  You have N different versions which have to be different "all the way down" - including their random number generators.  No two of your programs *ever* produce the same output for identical inputs.  What will you compare?  What does it mean to say that one of them produced an "incorrect" result?
As noted above, if randomization is used, the "spike" might be there but not
be detectable even in principle.
Even without randomization, many programs can produce all kinds of "equivalent" outputs.  Compilers producing different by "acts the same" object are an expected example - but see Les Hatton's classic paper on errors in scientific programs (  Hatton's other papers are also well worth a visit - they are some of the best papers on software engineering I know of.  Of particular note for this discussion:  "Are N average software versions better than 1 good version?"

@_date: 2005-01-05 10:57:17
@_author: Jerrold Leichter 
@_subject: Cryptography Research wants piracy speed bump on HD DVDs 
The new mechanisms aren't for use with current DVD's - already a lost cause because of the current installed base - but for next-generation stuff (Blue Ray or whatever the other HD standard is called).  On the one hand, the players for these will, initially, be much more expensive.  On the other, there is no inherent reason why they will *stay* expensive - the cost of electronics is driven by implementation and experience and volume, so within a couple of years, one can reasonably expect an HD player to have a cost comparable to an HD disk.
Besides, all this ignores another reality:  Many DVD's are played on general- purpose computers, whether legally (on Windows and Apple boxes) or pseudo- legally (on Linux boxes).  Why would one expect the story to be any different for HD disks?  The economics of software distribution make it unlikely that there will be a unique table of keys for every copy of the software - and even if someone does that, the cost of the software is likely to be nominal at best: If one copy of the software gets locked out, I can just get another.
In fact, if you follow the logic through, if the threat to be dealt with is from file sharing, then necessarily the disks are being read on a general-
purpose machine!  (I suppose we could assume that there is no software codec
and decryptor for these things, and you have to get a hardware codec/decryptor for your PC.  Given the popularity of the software-only solution for DVD's, is
this a realistic scenario?  In any case, a internal HD drive would be even cheaper than a stand-alone one.)
This whole approach strikes me as a clever solution in search of a problem.

@_date: 2005-01-07 09:32:35
@_author: Jerrold Leichter 
@_subject: entropy depletion (was: SSL/TLS passive sniffing) 
The argument you are making is that because the one-way function isn't
reversible, generating values from the pool using it doesn't decrease its
"computational" entropy.  (Its mathematical entropy is certainly depleted,
since that doesn't involve computational difficulty.  But we'll grant that
that doesn't matter.)
The problem with this argument is that it gives you no information about the
unpredictablity of the random numbers generated.  Here's an algorithm based
on your argument:
By your argument, seeing the result of a call to getRandom() does not reduce
the effective entropy of the pool at all; it remains random.  We certainly
believe that applying SHA to a random collection of bits produces a random
value.  So, indeed, the result of getRandom() is ... random.  It's also
Granted, no one would implement a random number generator this way.  But
*why*?  What is it you have to change to make this correct?  Why?  Can you
prove it?  Just saying "you have to change the pool after every call"
won't work:
This (seems to) generated 512 random values, then repeats.  Just what *is*
good enough?

@_date: 2005-01-07 12:15:24
@_author: Jerrold Leichter 
@_subject: entropy depletion 
You're using Kolgomorv/Chaitin complexity here, but unfortunately that's a
bit hard to apply.  *Any* string has a shorter representation - if I get to
specify the model *after* you choose the string.  K/C complexity is robust
when you talk about sets of possible strings, because playing around with
the machine can only shorten a fixed number of them.  Turning that into a
notion useful for cryptography is a bit tougher - and in any case, if you
want to get at PRNG's, you need to get at K/C complexity "with trapdoor
information":  The bit stream may *look* uncompressible, but given the
internal state, it is easily produced.
More generally:  We're talking about "stretching" entropy here.  There are
certainly theoretical results in that direction, the one usually mentioned
being the BBS bit generator, which takes k bits of entropy and gives you
p(k) (for some polynomial p) bits that are polynomially-indistinguishable
from random bits.  But you (a) need some significant work to set this up and
prove it; (b) BBS generators are slow.
A simpler approach that does work (in some sense) is:  Choose a random
starting value R, and a number k.  Compute SHA^i(R) for i from 0 to k.  Emit
these values *backwards*.  Then given the first k-1 outputs, an attacker
cannot determine the next one (under the standard hypotheses about SHA).
Unfortunately, this is useless as, say, a key generator:  If you send me
the k-1'st output for use as a key, I can't determine what your *next* key
will be - but I can trivially read your preceding k-2 sessions.
The idea that revealing just the hash of random bits "doesn't reduce the
effective entropy" sounds great, but it's naive.  It's like the argument
that if H is a good hash function, then H(K || message) is a good MAC.  Not

@_date: 2005-07-08 14:43:16
@_author: Jerrold Leichter 
@_subject: Why Blockbuster looks at your ID. 
The article also mentions that the loss rate for 1992 was 15.7 cents per $100.
Something doesn't add up.  Combining the dollar values above with the loss
rate per $100, I calculate that the total charges handled in 1992 was about
$165 billion - which seems a bit low, but reasonable.  However, the
corresponding calculation for 2004 shows a total charges of about $16 billion,
which is clearly nonsense.
I don't actually see the $2.6B figure anywhere in the article.  Where did it
come from?
In doing this calculation, be careful about the assumptions you make about
how effective the countermeasures will be.  The new systems may be more secure,
but people will eventually come up with ways to break them.  The history of
security measures is hardly encouraging.  There have been a couple of articles
in RISKS recently about the fairly recent use of a two-factor system for
bank cards in England.  There are already significant hacks - and the banks
managed to get the law changed so that, with this "guaranteed to be secure" new
system, the liability is pushed back onto the customer.
It's a continuing battle, and the banker's approach is really the only one that
works over the long run:  Keep the loss rate low enough that you can live with
it while keeping the system easy enough to use that you don't lose customers.
(Of course, bankers also try to externalize their liability - an effort that
society must watch and control carefully.  The liabilities must always be
put on those in a position to actually do something about the risks.)

@_date: 2005-07-10 00:25:15
@_author: Jerrold Leichter 
@_subject: payment system fraud, etc. 
It's very difficult to get a "clean" experiment on something like this.
There is no doubt that going from NAMPS to digital cellphone networks raised the cost of phone cloning or related methods for getting uncharged/mischarged service considerably.  However, at the same time, the cost of *legitimate* cellphone service fell dramatically.  When you can get 500 minutes of free calls to anywhere in the US for around $40/month (with various hours or calls to customers of the same carrier free on top of that), just how much does it pay to clone a phone?  Overseas calls probably provided some incentive for a while, but soon their prices dropped radically, pre-paid, cheap phone cards became widespread (and were probably stolen) - and more recently services like Skype have reduced the cost to zero.
The only remaining reason to clone a phone is to place untraceable calls - but
you can do as well by buying a pre-paid phone and the number of minutes of
airtime you need, paying cash, then tossing the phone.  (Using a clone phone
for this purpose was getting rather dangerous toward the end of the NAMPS era
anyway as the providers started rolling out equipment that recognized the
transmission signatures of individual phones.  Generally, this was aimed at
preventing clones from operating, but it could as well be used to recognize a
given clone regardless of the identification info it sent.)
A better history to look at might be satellite TV subscription services, which
took many generations of allegedly secure cryptography to get to wherever they
are today (which as far as I can tell is a non-zero but tolerably low rate of
fraud - the cost of entry to satellite TV subscription fraud these days is
very high).

@_date: 2005-07-14 19:05:42
@_author: Jerrold Leichter 
@_subject: ID "theft" -- so what? 
In effect, what you've done is proposed a digital model of *checks* to
replace the digital model of *credit cards* that has been popular so far.
In the old days, paper checks were considered hard to forge and you were
supposed to keep yours from being stolen.  Your physical signature on the
check was considered hard to forge.  Checks were constructed in a way
that made made alteration difficult, binding the details of the transaction
(the payee, the amount being paid) and the signature to the physical
There was a time when checks were accepted pretty freely, though often with
some additional identification to show that you really were the person whose
name was on the check.
Credit cards and credit card transactions never bound these various features
of the transaction nearly as tightly.  Stepping back and looking at the
two systems, it seems that using the check model as the starting point for
a digital payment system may well be better than using credit cards, whose
security model was never really as well developed.  When I handed a check to
a vendor, I had (and to this day have) excellent assurance that he could
not change the amount, and (in the old days) reasonable assurance that he
could not create another check against my account.  (Given digital scanners
on the one hand, and the "virtualization" of the check infrastructure, from
the ability to initiate checking transactions entirely over the phone to
check truncation at the merchant on the other, this is long gone.  It would be
nice to recover it.)

@_date: 2005-07-21 15:02:20
@_author: Jerrold Leichter 
@_subject: ID "theft" -- so what? 
If this is all you need, then using a 1-way hash of the card number for
identification and the card number itself for security would give you much
of what you need.  There are databases out there which identify customers
by their CC numbers, not because they are willing to use the stored CC
number for charging, but just becauses it's a good unique ID.  If what were
stored were the 1-way hash, there would be nothing worth stealing in the
This kind of thing is actually implemented, though people rarely think of it
in these terms.  You can see it on many printed charge slip these days:  Your
CC number no longer appears, being replaced by a one-way hash that produces 12
X's and the last 4 digits of your number.  Hardly cryptographically strong in
the usual sense, and not generally applicable, but for ID purposes - letting
you identify which of your cards you used when making the charge - this
particular one-way hash is perfectly good.  (It's also common on Web forms
that tell you which of your cards a charge will be applied to.)

@_date: 2005-07-21 19:13:50
@_author: Jerrold Leichter 
@_subject: ID "theft" -- so what? 
I just had that experience.  My bill contained one charge that made no sense
at all, and one for a local store whose name I don't know.  I called the
issuing bank.  They could provide essentially no useful information.  All they
could offer was to send me a copy of the original charge information - whatever
that is - which would take 4-6 weeks.  (How can *anything* take 4-6 weeks?  If
they had to get this from backup tapes out at Iron Mountain, OK - but for
charges made in the last month?)  I could tell that for one transaction, this
wasn't going to help at all:  The company was a computer supplier that works
mainly on-line or by phone.  (They may have a physical store in California.)
What "original records" could they have?
Anyhow ... I called the computer vendor, and they were able to search their
database on the CC account number.  It was, indeed, a fraudulent order -
shipped to a town 30 miles from me.
The other order - on the same day - remains a mystery.  It's at a record and
DVD store, but the only name they have is something like "Record and DVD
Store".  It *could* be a DBA for the local Tower Records; it could be some
unrelated store where the same guy who ordered the computer stopped in for
a couple of DVD's.
It's clear that the CC companies really don't care very much.  They are happy
to issue me a new account and dump the charges back on the vendors.  If I
choose to do the research and find out where the charged merchandise ended
up ... they'll take the data from me, but probably not do anything with it.
It's not costing *them* anything.
It's also clear that they don't expect customers to look closely at, or
question, their bills.  If they did, they'd make sure that meaningful merchant
names appeared on the bills, or at least were available if you called to ask
about a charge.

@_date: 2005-07-22 13:47:36
@_author: Jerrold Leichter 
@_subject: ID "theft" -- so what? 
The banks, operating through the clearing agents, could if they wished impose a requirement on the way names appear in billing statements, regardless of how the names appear on contracts.  Alternatively, they could at least require that an end-user-familiar name be made available in whatever database records all merchants, which the banks obviously have access to.
This is yet another situation where the banks are really the only ones who can fix something that hurts the customers and merchants.  In the past, I denied a charge to a company whose name I didn't recognize.  It turned out the charge was legitimate - it's just that the name and location listed for the company were completely unrelated to anything that appeared on their website. Once I denied the charge, the bank was done.  The vendor had to go through the expense of getting in touch with me, and then I had to write a letter to the bank authorizing the charge.  Meanwhile, the vendor was out the money.

@_date: 2005-06-08 14:25:20
@_author: Jerrold Leichter 
@_subject: AmEx unprotected login site (was encrypted tapes, was Re:  
If you look at their site now, they *claim* to have fixed it:  The login box has a little lock symbol on it.  Click on that, and you get a pop-up window discussing the security of the page.  It says that although the page itself isn't protected, "your information is transmitted via a secure environment".
No clue as to what exactly they are doing, hence if it really is secure.

@_date: 2005-06-10 18:39:42
@_author: Jerrold Leichter 
@_subject: analysis of the Witty worm 
The links in the paper no longer work - they go to restricted pages.  The (or an) HTML version is in the Google cache at:

@_date: 2005-06-10 18:42:36
@_author: Jerrold Leichter 
@_subject: analysis of the Witty worm 
Oops.  I should have read it more closely first.  The only thing in Google's cache is the intro page, with an abstract.  The paper (pdf and ps) and a slide show are inaccessible, and are not in Google's cache.
Anyone saved a copy?

@_date: 2005-06-10 19:15:10
@_author: Jerrold Leichter 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?)  
DEC sold a product that added encryption to its standard backup utility back in the early '80's.  They also sold a full solution for encrypted Ethernet - KDC, encrypting Ethernet adapters, associated software.
None of this stuff went anywhere.  People just weren't interested.  Of course, the inherent cost of the hardware in that era for the Ethernet solution, and the complexity of dealing with export regs, didn't help.

@_date: 2005-06-14 06:37:14
@_author: Jerrold Leichter 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
I'm not sure where you heard this, but I find it hard to believe, for several In any case, this was a very expensive device.  Even regular Ethernet adapters were quite expensive in those days.  Doing DES at 10 Mb/sec was a significant engineering challenge.  Avoiding RF leakage was a significant engineering challenge.  As I recall, the boxes were physically tamper-resistant.  Put it together and I would guess you had a significant cooling challenge.  The DESNEC was aimed primarily at DoD, and I think had some sales in the government market, but didn't go anywhere commercially.
Interesting fact I just ran across:  In the late '40's/early '50's, Ford decided to make a move to safer cars.  They introduced seat belts, and used the safety they provided as the basis of advertising and promotion.
The public hated it.  Ford's sales actually fell significantly, and they were forced to drop the seat belts.
Fifteen years later, cars were being described as death traps, and auto makers lambasted as putting profits over the lives of their customers.  Seat belts were required by laws that were designed to punish the auto makers for their sins as much as to protect the public.
I suppose this is consistent with your story about DEC, so maybe I shouldn't be telling it here :-), but it's certainly worth pondering in the greater security space.

@_date: 2005-06-21 18:00:50
@_author: Jerrold Leichter 
@_subject: AES cache timing attack 
It strikes me that block ciphers in *communications*get used in two different contexts:
Usage in first of these may be subject to Bernstein's attack.  It's much harder to see how one could attack a session key in a properly implemented system the same way.  You would have to inject a message into the ongoing session.  However, if the protocol authenticates its messages, you'll never get any response to an injected message.  At best, you might be able to observe some kind of reaction to the injected message.  But that's a channel that can be made very noisy, since it shouldn't occur often.  (BTW, if you use encrypt-then-authenticate, you're completely immune to this attack, since the implementation won't ever decrypt the injected message.  Of course, there may be a timing attack against the *authentication*!)
By their nature, the first class of uses don't usually require the ultimate in performance.  Since they receive and respond to small messages involving the encryption of a small amount of data, the encryption portion of the what they do is rarely the major time cost.
If this dicotomy holds up, it leads to a simple recommendation:  Use a constant-time implementation in the first role; use the highest-performance implementation in the second role.

@_date: 2005-06-22 07:19:32
@_author: Jerrold Leichter 
@_subject: AES cache timing attack 
That's basically it.  You need to pair up packets that have known (or at least
recognizeable) plaintext - or, more accurately, input to the encryptor.  (In
CTR mode, what matters is the value of the counter, not the plaintext.)
Unless the protocol required some externally-visible response to a keep-alive,
it would provide you with no information - *nothing* would pair with the keep-
alive packet.  (Well, I suppose one can imagine an implementation that uses absolute time on its system to a very high degree of precision to determine when to send a keep-alive, and then posit an attacker who can get access to the system time.  But given any reasonably typical keep-alive mechanism, this seems pretty unlikely.)
A low-level ack to a received message might work.  Any higher-level response - one that came from semantic processing of the actual data, not from the low-level bits - is likely to contain so much noise that pulling useful timing out will take more paired, guesssable packets than an attacker will ever see (yet another reason for periodic rekeying, of course).
This does point out some interesting interactions.  For example, a mode like
CBC is harder to attack because you need to know more actual plaintext to
determine the input to the encryptor.  On the other hand, a mode like CTR may
be particularly vulnerable since the input can be determined independently of
the actual plaintext.  Pre-whitening, even with a (secret) constant (for a
particular connection) - something that generally seems pointless - would

@_date: 2005-06-23 07:36:38
@_author: Jerrold Leichter 
@_subject: Optimisation Considered Harmful 
This is an excellent point, as it reveals a deep and significant parallel
between cryptography and compiler/hardware optimization.
Consider what it means to create an optimizing compiler (or some kind of opti- mization in hardware - the issues are the same, but I'll talk in terms of a compiler for definiteness.)  The input is source code; the output is a bunch of instructions.  A compiler's transformation is correct if it's semantics- preserving:  The source has some meaning, and the object code correctly represents that meaning.  There are an infinite number of possible object codes that preserve the input semantics.  Some are "better" than others with respect to some objective function, say size or speed.  An optimizing compiler simply chooses a "better" object code than some reference choice.
The big issue in compiler optimization - and even more so in some hardware
optimization - is defining exactly what semantics has to be preserved.  For
example, must computations be done even if the compiler can determine that
they cannot affect the output?  Can the rules of algebra be used to rearrange
expressions (possibly breaking carefully crafted error management strategies,
since floating point arithmetic doesn't actually obey the rules of algebra)?
Do writes to two variables written in order in the source code have to occur
in that order in the object code?  If two writes are issued in order to the
hardware, do they have to hit memory in that order?
An understanding of what semantics has to be preserved, and what semantics is
"side-effect" and can be tossed to gain performance, has gradually emerged in
the hardware and software communities.  There have been, and continue to be,
missteps along the way.  Some of the weaker memory consistency models might
have helped the hardware guys, but were just too hard for the software guys
to deal with.  Newbies to multi-threaded programming are caught by the not-so-
obvious memory access semantics present even at the language level in all
common programming languages.  (Java tried to pin this down exactly and got it
completely wrong for several tries.)
Enter cryptographic algorithms.  On their face, these are simple mathematical
transformations.  You have to really abuse the implementations (e.g., having
multiple side-effect-producing operations in one statement) to get into area
where a programmer or hardware developer's warning bell would sound "watch the
semantics".  And, in fact, from the point of view of input/output transforma-
tions, there really are no semantic issues.  The problem is that these side-
channel attacks broaden "the meaning of the program" to something that has
never been considered in previous work that I know of.  (The closest you are
likely to find is in complaints by real-time programmers that modern machines
give you no way to determine how long an instruction sequence will really
take:  You might take a page fault, or a cache miss, or who knows what along
the way, and in some real-time code, you have to *know* that that won't
happen.  Such code really is sometimes run only on machines like Z80's!
What can be done?  Well, the first thing that's clearly needed is a more
complete specification of the semantics of cryptographic algorithms.  Just
the input/output transformation - which is all we write down in most analyses
- is insufficient.  We sometimes state things informally, almost in passing -
as in the comments on AES that "table accesses take constant time".  We
certainly assume things like "the time to add two numbers is independent of
the number of carries" - which is probably true on machines today, but may
actually have been false at one time.  Without a way to write down what
matters, we have no way to judge whether a particular compilation/hardware
approach is safe.  There's some work on abstract program interpretation that
might help here (though it's mainly aimed in other directions).
Ultimately, performance is likely to suffer.  Software and hardware optimiza-
tions are essential to get good performance today, and they are all done under
the assumption that it's the *average* case that matters:  Increasing the
variance (up to a certain - fairly generous - point) is fine as long as you
drop the mean.  All performance benchmarking is on loops that are repeated
thousands of times.  I can't recall ever seeing a benchmark that reported the
variance of timing across instances of the loop.
There are only a couple of roads forward:

@_date: 2005-06-25 05:49:53
@_author: Jerrold Leichter 
@_subject: Optimisation Considered Harmful 
thus bringing us back to an observation that goes back maybe 30 years:  Timing
channels are usually impractical to eliminate, but you can lower the bit rate
(or decrease the S/N ratio, which comes down to the same thing).  For the case
at hand, we've lowered the bit rate by a factor of 20.
If you're willing to change the protocol a bit, you can do better:  Never send
a block whose encryption crossed the boundary.  Instead, send a special "ignore
me" block.  (Obviously, the "ignore me" marker is *inside* the encrypted
envelope.)  Depending on the nature of the protocol - is the size of a response
invarient, or is the number of exchanges invarient? - you then continue either
by sending the real block, or by the sender returning an "ignore me" block
which you "reply to" with the encrypted block that crossed the boundary.  This
still doesn't completely remove the timing channel - there are differences in
the data stream depending on whether you cross the boundary - but they are
much subtler, so again you've decreased the bit rate.  (It's not clear how to
quantify in this case, though.)

@_date: 2005-03-03 17:31:28
@_author: Jerrold Leichter 
@_subject: I'll show you mine if you show me, er, mine 
The description has virtually nothing to do with the actual algorithm proposed.  Follow the link in the article -  - for an actual - if informal - description.

@_date: 2005-03-20 10:52:12
@_author: Jerrold Leichter 
@_subject: Non-repudiation 
With all the discussion we've seen on this topic, I'm surprised no one has mentioned "Non-Repudiation in Electronic Commerce", by Jianying Zhou.
I haven't read this book, but Rob Slade gave it a good review in a year-old RISKS that I happened to stumble across.  Any comments from list members?
 							-- Jerry

@_date: 2005-03-21 07:07:18
@_author: Jerrold Leichter 
@_subject: Do You Need a Digital ID? 
This is a rather bizarre way of defining things.  "Something you have" is a
physical object.  On the one hand, any physical object can be copied to an
arbitrary degree of precision; on the other hand, no two physical objects are
*identical*.  So a distinction based on whether a replacement is "identical"
to the original gets you nowhere.
A digital signature is just a big number.  In principal, it can be memorized,
thus becoming "something you know".  As a *number*, I don't see how it can, in
and of itself, *ever* be something you *have*.
difference among the different authentication modalities.  A house key can be represented as a fairly short number (the key blank number and the pinning). Even a very fancy and elaborate key - or any physical object - can, in principle, be represented as a CAD file.  While "something I am" is difficult to represent completely this way (at least today!), it doesn't matter:  A "something I am" *authentication element* has to ultimately be testable for veracity on the basis of information the tester has access to.
The meaningful distinction here has to do with possible modes of attack, constrained by the *physical* characteristics of the system.  An authentication element is "something you have" if an attacker must gain physical possession of it to be able to authenticate as you.  The "closeness" and length of time the attacker must possess the element form the fundamental "measures of quality" of such an element.  A house key is a prototypical "something you have".  Duplicating it requires the ability to physically hold it.  (One can, of course, imagine taking very detailed photographs from a distance, or using some other kind of remote sensing technology.  While possible in principle, this would be a very expensive and difficult attack in practice, and we generally ignore the possibility.)  Keys with restricted blanks are relatively difficult to duplicate even if you have physical possession.  We generally assume that you can take a key back, thus revoking access.  This is also a general property of any "something you have" authentication element - and is truely present only to some degree.  Still, one can meaningfully ask of such an element "How many copies are in existence?  Who has access to them?"
Conversely, "something you know" can, in principle, only be learned by you
revealing it.  Once revealed, a "something you know" element cannot be
revoked.  It can be copied easily, and determining who might know it is
usually impractical once there is any suspicion of compromise.
A key card by itself is like a blank house key.  It becomes "something you
have" when it's encoded with a password, a digital signature private key, or
some other secret that's, say, part of an interactive zero-knowledge proof
system.  The quality of the key card depends on how easy it is to extract
the information and produce another key card that can be used in its place.
Of course, quality is a *system* property.  A house key "reveals its secret"
when placed in a lock - any lock.  While I could easily enough build a lock that
would read off the pinning of any key inserted into it and send it to me on
the Internet, this doesn't at present appear to be a threat that needs to be
defended against.  We generally assume that locks are simple physical devices
that don't leak any information.  On the other hand, a key card by its very
nature sends information into a digital system, and protecting information
once it is in digital form is challenging.  If I could know to a sufficient
degree of certainty that my keycard would only be used in "secure" readers
which would send the information no further, there would be relatively little
difference between a key card with a simple password encoded on a magnetic
strip, and a house hey.  Both would provide a "something you have" element.
A "digital signature" isn't an authentication element at all!  We incorrectly analogize it to a traditional signature, because inherent in the notion of the latter is a whole system embodying assumptions like (a) a signature instance is physically created by the party being authenticated; (b) we can effectively distinguish an instance thus created from a duplicate.  I can photocopy a signature perfectly.  If it were impossible to distinguish a photocopy from an original - based on pen pressure on the paper, ink vs. toner, etc. - signatures would be completely worthless as authentication elements.  To decide whether a "digital signature" is "something you have", "something you know", or perhaps even "something you are" - a signature based somehow on biometrics; or not a reaonable authentication element at all; requires knowing how the abstract bits that define that signature are actually used in the total physical system.

@_date: 2005-03-21 18:52:49
@_author: Jerrold Leichter 
@_subject: Do You Need a Digital ID? 
I don't think the 3-factor authentication framework is nearly as well-defined
as people make it out to be.
Here is what I've always taken to be the core distinctions among the three
This classification, useful as it is, certainly doesn't cover the space of
possible authentication techniques.  For example, an RFID chip embedded under
the skin and designed to destroy itself if removed doesn't exactly match any
of these sets of properties:  It's not "something you have" because it can't
be transferred, but it's not "something you are" because it can be changed.
Attempting to force-fit everything into an incomplete model doesn't strike me
as a useful exercise.
Yes, this *entire system* - a private key embedded in a device that protects
the secret key embedded in it - is properly described as "something you have".
But people use the phrase "digital signature" to describe other systems as
well.  The laws on acceptable "digital signatures" are broad enough to include
way more than this - in fact, way more than is really reasonable.  If my
secret key is stored en clair in a file on a general-purpose computer that
provides no protection against copying, it still acts in some ways like
"something I have", but it lacks the "cannot be copied" attribute that seems
central to the notion.  On the other hand, if the secret key is stored in a
file encrypted using a pass phrase that I memorize, the entire system takes on
the security properties of "something I know", even though it has a physical

@_date: 2005-03-23 09:57:47
@_author: Jerrold Leichter 
@_subject: Do You Need a Digital ID? 
I business rule or law can choose to define anything it wishes.  "Separate but
equal" was the law of the land for many years.  Many people don't understand
what that meant.  It wasn't in the law as a *goal*; it was in the law as an
*assumption*:  If a state said it was creating two school systems, one for
blacks, one for whites, courts took their word for it that the two were
"equal", regardless of any actual facts.  What we are discussing is what rules
might be reasonably consistent with the real security properties of the
objects and methods involved.
Of course.  These classifications are idealizations.  In fact, *any* security
property, as applied to a real-world system, is an idealization.  One of the
errors of the early burst of enthusiasm about RSA was the assumption that
mathematics could prove things about the security of real-world systems.  It
can give you upper bounds - the system is breakable by a mathematical attack
so it's no stronger than this - but that's pretty much all.  (We continue to
see a bit of the same hyperbole concerning quantum cryptography, though most
people have learned their lesson by now.)
When we say a smart card "can't be copied", all we are really saying is that
we will not be considering copying as a mode of attack to be defended against.
If you want to be a bit more quantitative, you might instead say that the
calculated cost of copying a smart card significantly exceeds the value that
an attacker could put at risk through that attack modality.
At a lower level of abstraction, someone *did* consider copying attacks, and
added defenses to the card, its reader, how it was used, etc., exactly to get
us to the point where we could stop worrying about that particular threat.
(And, of course, many such security decisions ultimately come down to ... very
little.  We can reasonably well rate the security of a safe because we have
many years of experience with attacks and defenses; and further safes are
fairly simple physical objects subject to well understood physical attacks.
The same can't be said of smart cards or really any digitally based system.  I
expect to see a continuing see-saw between attack and defense for the rest of
my life.  That's not to say nothing is changing:  The barriers to entry for
attackers are much higher than they used to be.  But they can still be
I think this concentrates too much on the implementation technique rather than
on the goals.
Again, of course.  The question is:  What security properties do I want my
system to have?  What security properties can some physical realization
actually deliver, to an appropriate degree of assurance against an appropriate
set of attacks, and at an appropriate cost?
This is mainly restating the names of the modalities, without adding any real
In designing a system, I certainly agree that the security properties are
not the only ones of relevance.  The distinctions as you draw them say nothing
about the security properties, but they certainly are relevant in other ways.
"Something you know" can't be a 100-digit string, because humans can't
generally remember such things.  "Something you are" runs into all kinds of
social acceptability issues - e.g., fingerprint identification systems have
an association in many people's minds with their use to identify criminals.
"Something you have" is likely to be very tightly constrained at a bar on a
nude beach!
Using two-factor authentication *can* give you "the best of both worlds" in
terms of security attributes, but it can never do anything but give you the
*worst* of the acceptability/usability properties of the two factors.  I
challenge you to argue for a two-factor system *simply on the basis of the
definitions you have above, without reference to the differences in security
properties I list above.  A couple of years ago, Citibank (I think) started
adding photos to their credit cards.  Their ads focused on the security; the
general argument was something along the lines of "because no one but you
looks like you".  Even *advertising* discussed - in friendly terms - the
non-transferability of "something you are"!
Yes.  The distinction is only meaningful as a description of how a person
retains the authentication information.  To *use* it to authenticate, you
have to transfer *something* to another party, and they have to be able to
verify it, based on *something* they have stored.  None of those later
actions can meaningfully be described by the have/are/know trichotomy.
(Well, I suppose if you are authenticating to another person, he may KNOW
what you look like, HAVE a picture of you, BE your twin brother.  :-) )
That's fine for *describing* the system, and useful for analyzing its usability
or acceptability.  But it's not the whole story.

@_date: 2005-03-25 11:25:39
@_author: Jerrold Leichter 
@_subject: Secure Science issues preview of their upcoming block cipher 
They don't claim that:
I can come up with a cipher provably just as secure as AES-128 very quickly....
(Actually, based on the paper a while back on many alternative ways to
formulate AES - it had a catchy title something like "How Many Ways Can You
Spell AES?", except that I can't find one like that now - one could even
come up with a formulation that is (a) probably as secure as AES-128; (b)
actually faster in hardware or simpler to implement or whatever...)

@_date: 2005-03-25 12:07:51
@_author: Jerrold Leichter 
@_subject: Secure Science issues preview of their upcoming block cipher 
I was responding in jest to the text Adam actually quoted - and indeed was
refering to:
[Remind self once more:  Ironic humor doesn't work in mail....]
I didn't see that claim on their site, but then again I only glanced at it
quickly.  Unless they have some entirely new kind of reduction, I'm guessing
that what they are really claiming is that the same proofs of security that
are available for AES - against generalized differential attacks, for example -
are also available for CSC2.  *That* much is certainly possible.

@_date: 2005-03-25 18:16:03
@_author: Jerrold Leichter 
@_subject: Secure Science issues preview of their upcoming block cipher 
They don't claim that:
I can come up with a cipher provably just as secure as AES-128 very quickly....
(Actually, based on the paper a while back on many alternative ways to
formulate AES - it had a catchy title something like "How Many Ways Can You
Spell AES?", except that I can't find one like that now - one could even
come up with a formulation that is (a) probably as secure as AES-128; (b)
actually faster in hardware or simpler to implement or whatever...)

@_date: 2005-05-31 09:43:55
@_author: Jerrold Leichter 
@_subject: Papers about "Algorithm hiding" ? 
There was a paper published on this a while back.  The question was posed as essentially:  Can one produce a "one-way trapdoor compiler"?  That is, just as an encryption algorithm takes plaintext and converts it to cryptotext, with the property that the inverse transformation is computationally intractable without the key, we want something that takes an algorithm and a key and produces a different but equivalent algorithm, such that asking some set of questions (like, perhaps, whether the output really *is* equivalent to the input) without the key is computationally intractable.  The result of the paper was that no such compiler can exist.
Can prove *any* algorithm is an "encryption algorithm"?  If so, I think there are some big prizes coming your way.
On a more general note:  This court case is being blown out of proportion. Screwdrivers, hammers, and a variety of other implements are "burglar's tools".  If you are caught in certain circumstances carrying burglar's tools, not only will they be introduced into evidence against you, but the fact you have them will be a criminal violation in and of itself.  The way the law deals with all kinds of ambiguities like this is to look at intent:  If I carry a screwdriver to repair a broken door on my own house, it's not a burglar's tool.  If I carry it to break the lock on my neighbor's house, it is.  Determining intent is up to a jury (or judge or judge's panel, depending on the legal system and the defendent's choices).  It's outside the realm of mathematics, proof in the mathematical sense, or much other than human judgement.  If an expert witness testifies something is an encryption algorithm, and the jury believes him more than the defense's expert witness who testifies it's a digital controller for a secret ice cream maker ... that's what it is.  If the jury further believes that encryption algorithm was used in the furtherance of a crime ... the defendent is in trouble.
