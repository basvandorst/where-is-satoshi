
@_date: 2001-08-01 08:10:24
@_author: Ray Dillinger 
@_subject: moving Crypto? 
It is time to move the conference because it is no longer safe for cryptography researchers to enter the USA.

@_date: 2001-08-01 09:55:29
@_author: Ray Dillinger 
@_subject: moving Crypto? 
The Domestic Terrorism Act of 2000 makes London as dangerous for crypto researchers as the DMCA makes the USA.  Remember, in England, you can be arrested for having "information useful to

@_date: 2001-08-13 13:46:16
@_author: Ray Dillinger 
@_subject: Danish police: Not Safeguard Easy but passwords were weak 
And we're back to the easy chunk of cryptanalysis.  That 128-bit key doesn't do you a darn bit of good if it's derived from one of the two million most common words in your language.
In Finnish and/or German, I believe the working vocabulary isn't even that large; even in English, which has a huge vocabulary, two million words will include words that have been out of style for centuries.
There is no help for people who are not willing or able to store real entropy in their brains somehow. "Password: swordfish" just ain't gonna cut it when the rubber meets the road. And here is where we get to the cryptanalytic uses of those high-powered clusters some folk here have been admiring:  The fact is that the ability to chew through about two million words plus forty million variations as possible passwords, will get you a substantial number of decrypts no matter how good the system is.  No need for an exhaustive search of the huge keyspace until you've finished your exhaustive search of the relatively tiny vocabulary of the user's native language.

@_date: 2001-08-30 10:19:02
@_author: Ray Dillinger 
@_subject: Outreach Volunteers Needed - Content Control is a Dead End  
This is not how I see things being developed.  In fact, I'd say you only get an odd number of {privacy, copyright} -- either I control what happens on my machine, or someone else does.  Right now, the copyright technologies we've been seeing are actively invasive of the information purchaser's privacy.
The whole "you can display it but you can't print even a little bit of it" thing that Adobe is doing, for example, is damned offensive: by what right do they take copyright to exclude fair use?  And while we're at it, let's look at the RIAA, the MPA, etc.  Does copyright allow them to control what software I can and cannot run in the so-called "Privacy" of my own home?  I don't think so. Shakespeare had a rich public domain to work with because there were no copyright laws in his era.  I'd say the world has been enriched by the fact.  So yes, even as a software author, writer of short stories, musician, and several other things, I'll cheerfully give up copyright in favor of privacy and think the world is a better place for it.  Copyright has gotten way too oppressive lately, it's time to enrich the public domain and allow creative people to do what they want with it.

@_date: 2001-12-25 08:52:30
@_author: Ray Dillinger 
@_subject: Stegdetect 0.4 released and results from USENET search available 
Actually, statistically significant sample sizes have more to do with
the probability distribution than the sheer number of objects being
analyzed.  One million images has the same statistical significance
whether it's one day of traffic or one year of traffic.

@_date: 2001-12-25 09:35:25
@_author: Ray Dillinger 
@_subject: CFP: PKI research workshop 
Inherent conservatism of the financial business world.  The only
way you're going to get a PKI going fast is if people can use it
to do financial transactions they couldn't do before.  I mean,
there are other uses for PKI, but money is the heart and soul of
it because money is usually the only application people have where
security is important to them personally.  And you're not going to
get people to use it for money unless you can do it while all the
bankers and all the merchants get to do business with the same
companies they're doing business with now, the relatively few
"people" whom they trust with their money.
So far, PKI's have been mostly advanced by new companies which don't
have hooks into the infrastructure of financial services yet.  So
you get failure of interoperability, and a certain amount of FUD.
The ones that have been advanced by the companies who are among the
trusted elite, have been incomplete or flawed on technical grounds
(although that may be less of a barrier to adoption than initially
It's not like these barriers are going to last forever; they're
getting used up, and companies like VISA are now trying to develop
some kind of authentication scheme.  But they're not used up yet.
Hey, don't be too discouraged; have a little perspective. It's
going a lot faster than the adoption of paper money went.

@_date: 2001-12-26 11:03:31
@_author: Ray Dillinger 
@_subject: CFP: PKI research workshop 
Yep.  So far, that's true.  Financial stuff is the only killer app
in sight for a PKI, and the financial services sector is conservative
and heavily regulated.  There is a substantial barrier to entry: just
try to imagine running off a few thousand PKI-backed credit cards and
going into business competing against mastercard/visa/amex.  Vendor
acceptance is slow and the regulatory hurdles are high.
Oh, I can.  If it's any good, you ought to be able to offer cards
with lower interest rates/fees, and people will go for that. The
whole idea of PKI in financial services after all, is to reduce
the amortized cost of transactions by reducing fraud.  If there's
a significant cost savings, you make more money even if you pass
part of it on to the consumers.
But nobody wants to be the first -- they all want to be able to look
at the business case built by some "bleeding-edge" financial-services
company that adopted and deployed PKI-based infrastructure in some
market and got measurable results, and they all want any and all
kinks in the technology to get worked out by someone else before
they touch it.  In financial services, they want mature technology
that's cheap and reliable to produce and use -- and they will roll
their own in order to make it cheaper instead of paying some "outside"
PKI company.
That's happening now, in fits and starts, with various
products internationally and in various closed markets.  If the
business case is good, the financial services companies will be
starting to pick it up for more mainstream use in a few years.
Odds are, however, that each and every one of them is going to want
their own PKI -- where P stands for Private, or Proprietary, rather
than Public.  A Public Key Infrastructure happens when the chaotic
situation which that brings about gets consolidated and standardized,
so don't look for that for at least a decade.  Basically we have no
chance of getting a Public Key Infrastructure in place right now
because we don't have enough different Private Key Infrastructures
in place for it to have started to hurt yet.  People won't go for
the PKI until they are in some kind of pain that it relieves. And
if financial services businesses are involved, they will do it in
such a way that no PKI vendor ever makes a profit they could possibly
have made themselves.  Look for them to be buying regulations that say
PKI is part of financial services and can only be provided by licensed
financial services corporations sometime in the next few years.
Like I said, don't get too discouraged -- these things happen slowly
and it's very much a matter of stages of development.  People don't
do things until the pain of not doing them gets worse than the pain
of doing them.  Public Key comes about when Private Keys have been
common for several years and their multiplicity causes pain.  That
in itself will take several years after the Private Key structures
are fully adopted. The Private Key structures get adopted several
years after the profit margins, split between consumers, vendors, and
financial institutions, each overcome the pain of changing infrastructure.
That will take several years after the initial offering.  The initial
offerings are happening now in very restricted markets, but don't
look for it to happen in domestic consumer markets until the results
of the restricted-market offerings are several years old and the
technology involved hasn't changed AT ALL for several years. They
are looking for a technology that's been in use long enough to
establish a baseline and get results that look stable and repeatable.
That's when financial services companies will begin to take them
seriously enough to consider that the pain of deploying new
infrastructure may overcome the painof absorbing losses due to
These are just network effects: PKI will trickle through at the end
as surely as water runs downhill, because it's a better solution.
It's just going to take a decade or two, or maybe four or five
decades if there's a substantial monopoly somewhere in the industry.

@_date: 2001-12-26 11:31:51
@_author: Ray Dillinger 
@_subject: CFP: PKI research workshop 
In fact, that may be exactly it.  PKI, as espoused by vendors,
once established, will become an indispensable monopoly, like
AT&T before the breakup. Investors love the fantasy of buying
a kajillion shares for cheap today and then having them be
shares in an indispensable monopoly next year, so they are
inclined to believe.
The problem is that none of the vendors are offering anything
that someone who has significant volume (like a financial-services
company might) cannot provide for themselves.  The FS companies
can easily wait to adopt, because the margins offered by PKI are
fairly small and the initial investment required is fairly large.
Perhaps the margins will remain too small until royalty payments
can be eliminated entirely (until any patents expire) and the
FS companies can roll their own.  Whether or not the margins
are too small, The FS companies can wait that long easily.
But the PKI vendor cannot wait.  S/he will be out of business
in three or four years if nobody adopts.  The patents will be
for sale then much cheaper than the royalty payments s/he is
offering, and the FS negotiator across the table knows it.  The
PKI vendor therefore is going to get the worst end of the deal
every time s/he goes to financial services vendors, because s/he
is not dealing from a position of strength, and had best learn
the harsh lesson sooner rather than later.
A PKI will happen, eventually, but nobody is going to get into
a position where the financial-services sector depends on them
and has to pay them.  That's as fundamental in business as the
second law of thermodynamics in physics, and chasing the dream
of becoming an indispensable monopoly to the financial services
sector promises to be as frustrating to the seekers as the quest
for a perpetual motion device.

@_date: 2001-12-28 13:29:04
@_author: Ray Dillinger 
@_subject: CFP: PKI research workshop 
The only case in which the PKI solution is not redundant is in
offline clearing.  But getting your point-of-transaction online
is easier than paying attention to PKI.
I happen to like offline clearing -- it opens up the possibility of
new transaction types and doing transactions in places you couldn't
before.  But the practical issue is, everybody who's interested in
electronic transactions of any kind is also interested in getting
online, and when PKI's were deployed in "developing" areas (south
africa) they got dumped just as soon as the area was developed
enough for communications to support online clearing.
On the principle of people refusing to adopt something until
it relieves pain, maybe we won't see a real PKI deployed until
we need to serve markets where speed-of-light delays make online
clearing impractical.
Mars, for example, is 3 to 22 light-minutes away.  I don't imagine
someone using an ATM on Mars is going to want to wait 12 to 88
minutes for online clearing (more if the protocol is talky or the
bandwidth is busy...).  So a martian colony might be the first
practical application of PKI and/or digital cash, assuming the
colonists want to do business with Earth companies.  But a colony
looks pretty distant right now: we haven't even got an outpost
there yet.

@_date: 2001-07-02 08:38:08
@_author: Ray Dillinger 
@_subject: blocking chinese domains? 
has been mirrored a while as
When people are doing what they can do to evade censorship in a repressive state, it is best not to draw attention to them.  Often their continued existence is largely a matter of how well they can maintain a low profile.

@_date: 2001-07-11 13:37:09
@_author: Ray Dillinger 
@_subject: pseudonymous decentralized marketplace 
I've been attempting to design a decentralized auction/
exchange system that permits pseudonymous participants.  By 'decentralized', I mean that NO central server, or subset of individual servers, controls access to any resource the system cannot work without; that there is no single point of failure. A consequence of this is that every ability that exists in any node, must exist in every node.  So the whole problem of currency issue gets the slightly weird solution of "everybody has to be able to print their own money."  The sticking point is that this basically means the system will be without any single universal "currency".
A lot of E-cash techniques are usable, but what you wind up trading is certificates that represent goods or services offered by individuals in the system -- Alice the Farmer might issue certificates for bushels of wheat, while Bob the Carpenter might issue a bunch of certificates that say "collect a thousand of these and I'll redeem them for a new 10x10 meter deck on your house" and Carol the moneychanger might promise to redeem hers for one US dollar each, just for the amusement value of "redeeming" something in a system where hard currencies are the norm with a fiat currency. So these would be  effectively a sort of digital merchants scrip, reducing back down to barter.
Exchange rates between the currencies issued by different participants would fluctuate according to trust and commodity values, and I'm okay with that.  Given the nature of the trust/reputation thing, I'd expect only a very small percentage of the participants to *actually* issue their own currency, as they wouldn't get good acceptance/exchange values until widely known, but everybody would have the ability.
The problem I'm running into is that while all kinds of e-cash protocols exist that protect the anonymity of the buyer and a lot protect the anonymity of the seller, there are none that protect the anonymity of the currency issuer, which would be ideal in this circumstance.  With the techniques I know of, the issuer can have only "Nym" protection. The basic problem with anonymizing the issuers (beyond technique alone) would be how the scrip gets redeemed when you don't necessarily know whom the issuer is.
Can anybody recommend appropriate reading?

@_date: 2001-07-29 16:35:27
@_author: Ray Dillinger 
@_subject: A pattern emerges... 
Consider the DMCA (US law) as compared to the Terrorism Act of 2000 (UK law).  Both make it effectively illegal for ordinary citizens to own, use, or distribute any software capable of performing decrypts by exploiting a weak cryptographic system. The US and UK, not coincidentally, are the two governments with the largest known investments in SIGINT -- the famous Echelon System. If people started using strong cryptographic systems, Echelon would be effectively useless.  Therefore it is in the best interests of these two governments to make weak cryptographic systems the norm insofar as they are able. This is possible by providing an additional layer of legal protection to users of weak cryptographic systems -- with software capable of exploiting such weaknesses effectively illegal to own or use, the developers of such products have drastically reduced incentive to develop strong cryptographic systems to replace them. The DMCA and the Terrorism Act appear to provide exactly such laws. What has been passed recently by the other signatories to the UKUSA agreement that created Echelon?

@_date: 2001-09-10 09:08:51
@_author: Ray Dillinger 
@_subject: SSSCA = Digital Rectal Thermometer Security Act ? 
You can also read title II as creating a "guild" of people who are allowed to know this stuff or practice it.  I don't like that idea, myself.  I think that any time you have a class of people who are the only ones allowed to make a living on certain information, you wind up in a situation where there is an effective prohibition on anyone else *knowing* the information. And, let's face it, if knowing how to program becomes a guilded profession, we're going to lose about 90% of our programming talent -- 'cause the really good ones are doing stuff a long time before they get actually trained.  If they're not allowed to mess with computers when they're fifteen and manic, and if they're not allowed to get over, around, and all through the system software the way they do, they'll never develop the interest and become CS majors and then the really good programmers they become now. Reading title I as outlawing general-purpose computers, title II appears to outlaw members of the general non-guilded public knowing how to program them.

@_date: 2001-09-10 09:08:51
@_author: Ray Dillinger 
@_subject: SSSCA = Digital Rectal Thermometer Security Act ? 
You can also read title II as creating a "guild" of people who are allowed to know this stuff or practice it.  I don't like that idea, myself.  I think that any time you have a class of people who are the only ones allowed to make a living on certain information, you wind up in a situation where there is an effective prohibition on anyone else *knowing* the information. And, let's face it, if knowing how to program becomes a guilded profession, we're going to lose about 90% of our programming talent -- 'cause the really good ones are doing stuff a long time before they get actually trained.  If they're not allowed to mess with computers when they're fifteen and manic, and if they're not allowed to get over, around, and all through the system software the way they do, they'll never develop the interest and become CS majors and then the really good programmers they become now. Reading title I as outlawing general-purpose computers, title II appears to outlaw members of the general non-guilded public knowing how to program them.

@_date: 2001-09-24 11:44:51
@_author: Ray Dillinger 
@_subject: <nettime> "Pirate Utopia," FEED, February 20, 2001 
Actually, dictionary attacks reveal about sixty percent of passwords, so for every six passwords you find on a dictionary attack, you can infer ten actual stegotexts times the ratio between your analyzed and discovered (possibly-false) positives.  While he has analyzed only two percent of his sample, that's a sufficient number that if even even a tenth of one percent of his positives were real he'd have discovered at least a few passwords. The paper is solid statistical methods; lack of any dictionary-yeilding passwords in that big a sample is very strong evidence that the sample is overwhelmingly made up of false positives.

@_date: 2001-09-26 09:06:29
@_author: Ray Dillinger 
@_subject: [FYI] Did Encryption Empower These Terrorists? 
A few problems:  1) in a typical credit card transaction, the seller neither knows,    nor cares, what bank issued the credit card.  Thus, connecting    to the correct gateway becomes a minor problem. 2) No provision for dispute resolution.  What happens in a month    and a half when george gets his credit card bill back and says    "I've never been there and never done any business with this     person"?  The bank generates a chargeback and sends it to the     merchant who, in the absence of knowledge about the buyer's     identity, has no recourse.  George may or may not be the person     who made the transaction; but the merchant has no way to even     begin to try to find out. In general, "anonymity" and "credit" are immiscible.  If you want anonymous transactions, you absolutely cannot abide by the laws that require chargebacks, merchant and/or bank liability in case of fraud (instead of consumer liability), etc.  Compliance with those laws requires the merchant and banks to have the very information that anonymity prohibits them from having.
For anonymous transactions, you want something a whole lot more like cash, with the same failure modes (ie, no shift of liability in case of fraud) as cash.  That requires infrastructure which will not be allowed to be built.

@_date: 2002-04-16 17:14:10
@_author: bear 
@_subject: Schneier on Bernstein factoring machine 
Because Lucky Green is a well-known paranoid who has no business
requirement to put up with second-class crypto for the sake of
compatibility and can reasonably control other methods of accessing
his important stuff.  Conversely, your typical businessman has few
or no business secrets not known to at least half-a-dozen employees
and after trusting that many people, better crypto would add
essentially nothing to the businessman's security.
For a handy metaphor, you can think of a kilobit-keyed cipher as
a potentially weak link in Lucky's security (worth the attention)
and probably the strongest link in a typical businessman's security
(not worth the attention).

@_date: 2002-08-02 09:55:00
@_author: bear 
@_subject: Skeleton Keys for Palladium Locks. 
It occurs to me that the Palladium architecture relies on control
of the data paths between the memory and CPU.
In order to spoof it and read Palladium-protected content, all I
have to do is provide another path in and out of memory.
Dual-ported memory has been manufactured for video and DSP systems
for decades, and is frequently faster than that used for main
It should be possible to construct a memory unit ("skeleton key")
using dual-ported memory, which looks to the palladium motherboard
exactly like an ordinary memory module.  The second memory port
would be hooked up to a simple hardware "blitter" -- a standard
video-system chip that scans through the bits on the memory chip
and writes them to another memory chip, or other device.
The skeleton key would have exactly two control inputs: The first
would cause the data in memory to be copied "out" of the
palladium-controlled architecture.  The second would cause the
memory that had been copied "out" to be copied back in.  You
could hook them up to the two positions of a double-throw switch
on the front of the case if you liked, so they'd require no
software which could be detected by the Palladium motherboard.
Now, with appropriate selection of devices so that data stored
on the skeleton keys are persistent across boots, Palladium
control is circumvented. That requires a battery, but no big
deal; you can mount the battery with the switch.
The skeleton key module, if fabbed in bulk, would (wild guess
alert) probably cost about ten times what ordinary memory modules
cost. It is a simple device with a schematic that someone could
work out in an afternoon, far simpler than a PCI card. I could
have a working mask for you within a week. It could be fabbed
by a very small shop, using ordinary chips and PCB boards.
Now, I have chosen the name "skeleton key" advisedly.  Skeleton
keys are perfectly legal, necessary tools that every locksmith
must own in order to do business.  There is a legitimate market
for them, and if they were unavailable, nobody could afford the
risk of locking stuff up in a hard safe because they might not
be able to unlock it if they lose their key (if their hardware
fails or a drive crashes). Similarly, in a world where the
"locks" look like the proposed Palladium architecture, then
every "Locksmith" is going to have to have some skeleton keys
in his or her toolbox, just in order to do legitimate business.

@_date: 2002-08-05 08:32:56
@_author: bear 
@_subject: dangers of TCPA/palladium 
This is unacceptable.  If the vendor is so ashamed of his code
that he won't let anyone see it, I do not want it running on my
So the "file format prison" of software without backward compatibility
becomes completely absolute.  This is going to kill it in corporate
IT environments.
Offhand, it looks like a computer in this mode is just a sort of
inferior media player.  People will still need real computers.

@_date: 2002-08-06 07:47:06
@_author: bear 
@_subject: An authentication question 
No.  The "signin" form makes you a little more resistant to DOS attacks
based on sucking up all your bandwidth.  Allowing anyone to upload files
makes you a little more resistant to some kinds of "web tracking" that
anyone may be doing on your correspondents.  You just have to decide what
you're most paranoid about.
In both cases, the files are encrypted over the pipe, so you needn't
worry too much about eavesdroppers on the file content.

@_date: 2002-08-06 08:29:59
@_author: bear 
@_subject: dangers of TCPA/palladium 
The TCPA hardware and Palladium Software make it possible.  It's not
in the spec per se, but given the possibility, it will be done.
Uh-huh.  At the expense of their "trusted machine" status and causing
every last bit of TCPA-disabled software they've got to quit operating
correctly, and locking them out of their own confidential data which
they've got stored in sealed areas on the machine.  To say that this
gives them the choice to "avoid installing it" is at best fatuous.
Moreover, we're not that worried about *obvious* bad things... we're
worried about very, very *subtle* bad things.  Keyboard sniffers, screen
dumpers, web-cache readers, and other snoopware, if it has a "sealed"
data space to hide its malicious code and stolen data, runs without a
single detectable trace.  And, if it has an unmonitorable encrypted pipe
to the outside world (which it gets every time someone remote-authenticates
your machine) it can deliver that stolen data to untrusted parties.
The hardware supports installing such snoopware remotely as part of a
"bug fix".  Nobody can tell whether the content of a "bug fix" is or
isn't what's claimed.  Why should we assume that these businesses,
*knowing that nobody can find out*, won't screw everybody to the max?
Wow.  You must really be an idiot.
It may be worth noting that I haven't installed flash BIOS upgrades,
and won't until I can compile them myself. My machine still works
fine.  You're talking about a system where failure to install an
"upgrade" will cause loss of all the system's sealed data, which makes
it something other than voluntary.  The word "extort" comes to mind.
Assuming they are given a choice.
Okay, the changes are:
1. Somebody implements Clipper in software.
2. A "bug fix" for TCPA hardware is announced.
3. The Clipper application is made available for download.
4. The software checksums are changed to require it to be loaded at bootup
   for "trusted computing" status.
5. People find they can't get at their own data unless they install it.
6. People download it and install it.
7. Sure people can still boot linux.  But if they boot in
   "trusted" mode they'll have clipper installed on their system.
Note that this requires lying about what it is.  Note that we're
talking about companies and agencies that don't have a history of
avoiding lying.

@_date: 2002-08-11 19:58:17
@_author: bear 
@_subject: Challenge to TCPA/Palladium detractors 
I don't like the idea of a "trusted compiler".  No matter who makes
it.  People should choose compilers based on the compiler's merits
and make optimization and configuration decisions when compiling
based on their particular hardware, not in order to match some other
machine's or other user's ideal of trustable code.  The minute a
compiler becomes a "standard", for any reason, it becomes a target
for people to subvert.
People who are likely to be a source of malicious clients will also
hack hardware if the data is sufficiently valuable to warrant it.
We have already seen how a relatively simple and inexpensive hardware
hack can be used to defeat palladium security, so while it may provide
suitable infrastructure if the attacker's motivation is just the price
of a movie ticket, it is not at all trustable as a structure if the
value of the data being "protected" rises above prices that justify
hardware hacking. Moreover, the same simple hardware hack defeats
every piece of palladium-protected content or software, so the cost
of hardware hacking can be amortized over many "breaks".
I think you are trying to solve in hardware, problems which are
properly protocol-design problems.  This looks like the easy way
out because protocol design is hard, but the fact is that if there
is data you really want to protect which is more valuable than movie
tickets, what you want is a protocol that ensures no one using the
data ever has sufficient information to reconstruct more of it
than their particular licit use of it requires.

@_date: 2002-08-11 20:34:29
@_author: bear 
@_subject: adding noise blob to data before signing 
This is true.  Cyclopedia Cryptologia has a short article detailing
some of the attacks against direct use of RSA.
is a good URL if you want to read it.

@_date: 2002-08-14 08:19:01
@_author: bear 
@_subject: Overcoming the potential downside of TCPA 
The problem with this idea is that TCPA is useless.  For all the *useful*
things you are thinking of, you need TCPA plus an approved key.  The only
way you are going to get an approved key is inside a tamper-resistant chunk
of hardware.  If you should manage to extract the key, then yes, you'll be
able to create that CD.  But the idea is that you, the hardware owner, are
not authorized to extract the information contained in your own hardware.
I find the idea of "owning" something without having the legal right to
open it up and look inside legally dubious at best, but I'm no lawyer....
The idea is that you shouldn't get anywhere without hardware hacking. The
people doing this have decided hardware hacks are acceptable risks because
they only want to protect cheap data -- movies, songs, commercial software,
whatever.  They are sticking to stuff that's not expensive enough to justify
hardware hacks.
However, if this infrastructure does in fact become trusted and somebody
tries to use it to protect more valuable data, God help them.  They'll get
their asses handed to them on a platter.

@_date: 2002-08-14 14:50:30
@_author: bear 
@_subject: Overcoming the potential downside of TCPA 
Is that the one where the first elephant decides that
blind men are flat and squishy, and all the other elephants
check the blind men after he's done, and they agree?
I would say that it has been more impeded by misrepresentation.
The spec is designed to be hard to read and M. AARG, the
one who has been talking about the advantages of the proposal,
has been (hmmm) either terribly naive or deliberately
misleading.  As people actually work through the spec and
find the things s/he's been claiming aren't there or couldn't
be done with it, the odds of his/her being a mere paid shill
increase and his/her credibility decreases in direct
Yes, there has been some extremism.  The forum has a high
incidence of paranoia and distrust of authority -- but there
are few fora where people *KNOW* as many good reasons to be
paranoid and distrust authority as the posters to these
lists, having seen backpedaling, power grabs, and outright
misrepresentations and lies beyond counting as regards
security.  The distrust you see from the extremists here
has been well earned by those whom they distrust.  You
should expect extremism -- including mine.
But in the end, if a point is wrong, regardless of whether it
was made out of extremism, paranoia, or a deliberate will to
mislead, the truth comes out as people work through the
spec and find out what the heck is actually there.  No shill
can mislead people who are actually willing to go to source
documents and do the work, so putting a paid shill here is
just a waste of money.  In other words, the "Extremists"
will do the work to back up their points for free, whereas
the shill, if his/her points are wrong, cannot no matter how
much s/he is paid.
Shakespeare couldn't possibly have produced his body of work
if copyright laws had been in effect.  All the characters would
have "belonged" to someone else.  I'll go with patents about
technical things, although I think the duration is maybe too
long, but art, poetry, music, words -- they properly belong to
the public and I applaud the amateur writers, artists, and
musicians who do what they do because they love it instead of
doing what they do to get paid.  The professionals certainly
aren't producing work that's any better than the best amateurs
any more - and I doubt they ever have.
I am an extremist.  That's me under the banner that says
"Real Artists Have Day Jobs and Real Computers Can Copy Files."

@_date: 2002-08-14 20:54:17
@_author: bear 
@_subject: Overcoming the potential downside of TCPA 
... wide ... open ...  <>  Do you even know
what Kerchoff's Principle IS?!  Look, I've been trying
to hold back on actual ranting, but... but...  you
really don't get it, do you?
On the contrary, I have one box with all the protection I want:
it's never connected to the net at all.  I have another box
with all the protection that I consider practical for email
and web use.  Both run only and exactly the software I have
put on them, which I obtained from sources trusted to me and
which do not and CAN NOT require any further interaction or
authorization whatsoever to run their software.  I have selected
software, on purpose, which denies someone who is not me even
the bare possibility of restricting my ability to run it at
some future time.  I have the source code.
That is what I mean when I talk about trusted computing.  I
trust that my software does what it says it does, is completely
open to inspection and verification by first, second, and third
parties, and cannot be denied to me at any point after I have
come to depend upon it, whether or not I have a net connection
at the moment to interact with its creators.  I trust that I
cannot be singled out remotely to have a sniffer installed on
my local machine through a backdoor.  I trust that code I can
inspect at the level of machine instructions is code that cannot
keep secrets from me or forever conceal malign intent.  I trust
that I cannot be forced to depend on any single commercial
software provider, whether it be for the OS or for the "trusted"
compiler which can, of course, come from only one source.  I
trust that coercive "upgrades" done for the sake of incompatibility
with existing code,  do not and cannot happen automatically given
my system configuration.
That is trusted computing sir, and TCPA/Palladium is a huge
step *backward* from it.  Anything that runs *ANY* code that
cannot be inspected, or which keeps data that cannot be inspected,
is running code that cannot be trusted.
TCPA/Palladium does not provide trusted computing.  At least
not computing that I can trust in the way I trust what I've
got now. In fact, from my POV, TCPA/Palladium look like ways
to enable the running of software which I *CANNOT* trust.
Imagine my excitement at the prospect.
This is a cryptography mailing list.  Do we even want to count
the number of times commercial software providers have come up
with some crap and claimed it was secure, and been just lying
to us?  Do I have to recount all the proprietary, snake-oil
encryption systems that relied for its security on not being
inspected?  Do I have to recount the number of ways that unstudied
security designers have given us their best efforts and had them
shot down by professionals in seconds?  Do we have to speculate
about what can happen if the clowns who employ them think they'll
never be caught?!  We've all been around the block enough to know
this by now:
Kerchoff was absolutely right.  A hundred and twenty years since
he elucidated the Principle has not changed a thing.
NOBODY has the manpower or time to find all their bugs in-house.
If you can't inspect the machine code (and preferably the source)
then you are looking at something that is not and can never be
made secure.
Now, you're talking about a system that gives people the opportunity
to HIDE THE CODE, and telling us that's security?!  What the hell
are you smoking?! You are confusing real security mistakes with the
ability to DETECT real security mistakes!

@_date: 2002-08-29 10:00:16
@_author: bear 
@_subject: Palladium and malware 
If it provides the protections that copy-protection groups want
(ie, it can be used to prevent keys in their software from being
read by other software) then yes, it can be used to prevent any
code from being read by any software.

@_date: 2002-08-29 10:07:51
@_author: bear 
@_subject: Palladium and buffer over runs 
It's my understanding of Palladium that it can enforce a separate
data space for applications by creating a memory space which is
encrypted with a key known to only that application.
Given that, I think a cracker could subvert IE normally, but that
wouldn't result in any access to the protected space of any other
applications.  And as long as IE is actually separate from your
OS (if you're running it on your Mac, or under WINE from Linux,
for example), it shouldn't give him/her access to anything
inside the OS.

@_date: 2002-08-30 09:45:44
@_author: bear 
@_subject: Palladium and buffer over runs 
No, I wasn't forgetting that.  But that doesn't make it
any less silly.
Sigh.  It may be the case that laws are really a bad idea,
because they seem to make so many people feel that they are
obligated to say things that just aren't true.
Unfortunately, I don't have any better ideas on how to create
an orderly and capable society.  For a while I was thinking
cryptographic protocols based on provables, self-interest,
and the laws of mathematics (which are relatively constant
and unbiased) might be a superior organizing principle for a
society to laws. Under careful scrutiny and after acquiring
a better understanding of protocol design, however, I
concluded that such a society has probably as many problems
(opportunities for tyranny and oppression) as the current
nation-state concept.  Its only definite advantage might be
lower legal fees.

@_date: 2002-08-30 10:27:30
@_author: bear 
@_subject: Quantum computers inch closer? 
The papers I've been reading claim that feistel ciphers (such as
AES, DES, IDEA, etc) are fairly secure against QC.
But I don't see how this can be true in the case where the
opponent has a plaintext-ciphertext pair.
Because most feistel ciphers (and most other ciphers) can
be implemented as linearized hardware in a fairly small
(<1M) gate-and-wire count, and the "combinatorial explosion"
can be prevented if you have a plaintext-ciphertext pair,
I don't understand why you can't set up the entangled state
on the wires, gates, and key register, collapse the
eigenvector on the eigenstate that gives the known ciphertext
and plaintext, and read the key out of the collapsed state.
Similarly, if you can build a function that returns "1" if
a plausible plaintext is found and "0" otherwise, in hardware
gates, you have a specification for breaking feistel ciphers
using a Qchip from ciphertext only.  And there are a lot of
functions that return "1" for plausible plaintext -- you can
check for a zipf distribution of bytes to detect natural-language
plaintext for example, or check for headers and binary-formatted
data to detect compressed files.
In this case you'd need to set up the wires-and-gates model
in the QC for two ciphertext blocks, each attached to an
identical plaintext-recognizer function and attached to the
same key register.  Then you set up the entangled state,
and collapse the eigenvector on the eigenstate where the
ciphertext for block A and block B is produced, and the
plaintext recognizer for both block A and block B return
"1", and then you'd read the plaintext and key out of the
appropriate locations (dots?) in the qchip.
I figure two blocks because with keys about as long as blocks
there will be *some* key that produces just about any
pattern that will trigger your recognizer -- but with two
blocks running off the same key, a key that triggers the
recognizer in both blocks is highly unlikely unless it's
actually the correct key.
The papers I've been reading are concluding that these ciphers
are secure against Qchips in the unknown-plaintext case,
because for any round of the cipher there are a lot of
prefiguration states that could produce the same output.
keeping track of these prefiguration states introduces
additional "virtual" bits to the internal state that the
Qchip has to collapse, and of course each prefiguration
state was output by another round of the cipher so in turn
it produces its own family of prefiguration states, and
there's an exponential explosion in the number of qbits
needed in the eigenvector to keep track of them all.
But if you can design a plaintext-recognizer in gates,
I think that this argument is out the window.  Even
with an unknown plaintext, if we assume that the plaintext
is english words (or a zipfile, or a GIF file, or whatever)
then we can design a plaintext recognizer that will be
highly reliable.  And if we have the plaintext recognizer's
assumed output anchoring one end of the entangled model,
we don't need all the qbits devoted to the virtual bits
to keep track of prefiguration states.
I'm not a quantum physicist; I could be wrong here.  In
fact, I'm probably wrong here.  But can anyone explain
to me *why* I'm wrong here?

@_date: 2002-12-08 13:03:51
@_author: bear 
@_subject: [mnet-devel] Ditching crypto++ for pycrypto (fwd) 
Actually, I've found the reverse to be true.  Anything that gets
ported a lot eventually gets all the portability crap straightened
out so that porting it becomes just a matter of providing a few
definitions in a well-documented file.
If something still has porting problems, I'd say it hasn't been
ported enough.

@_date: 2002-12-08 14:07:34
@_author: bear 
@_subject: DBCs now issued by DMT 
I thought about this problem for several months.
The problem I kept running into and had no way around is that if the
holders are truly anonymous, then there is no way for them to seek
redress for fraudulent issue or fraudulent transactions.  If the
banker goes broke, people want to be able to make a claim against the
banker's future earnings for whatever worthless currency they were
holding when it happened, and they cannot do that from a position of
anonymity.  People want a faithless banker punished, meaning jail time
or hard labor, not just burning a nym.
The sole method for any truly anonymous currency to acquire value is
for the banker to promise to redeem it for something that has
value. So the banker, if it's to have a prayer of acceptance, cannot
be anonymous.
And the minute the banker's not anonymous, the whole system is handed
on a platter to the civil authorities and banking laws and so on, and
then no part of the system can be reliably anonymous because the
entire infrastructure of our legal system requires identity.
Look at the possibilities for conflict resolution.  How can the
anonymous holder of an issued currency prove that he's the beneficiary
to the issuer's promise to redeem, without the banker's cooperation
and without compromising his/her anonymity?  And if s/he succeeded in
proving it, who could force an anonymous banker to pay up?  And if you
succeeded in making the banker pay up, how could the banker prove
without the cooperation of the payee that the payment was made and
made to the correct payee?
We use a long-accepted fiat currency, so we're not used to thinking
about the nitty-gritty details that money as an infrastructure
requires. It is hidden from us because our currency infrastructure has
not broken down in living memory.  We shifted from privately issued
currency to government-issued currency largely without destabilizing
the economy.  Then once people were accustomed to not thinking of a
promise to redeem as being the source of value, we went off the gold
standard.  Our economy hasn't broken yet, but you have to realize that
this situation is a little bizarre from the point of view of currency
issue.  We're not thinking anymore about the promise to redeem
currency for something of value, and the implications of failure to
honor that promise, because we live in a sheltered and mildly bizarre
moment in history where those things haven't been relevant for a long
time to the currency we use most.  But any new currency would have to
have a good solid solution for that issue.
The only way I found to decentralize the system, at all, was the model
where all the actors are pseudonymous rather than anonymous, each user
has the power to issue currency, and different issued currencies were
allowed to fluctuate in value against each other depending on the
degree of trust or value of the underlying redemption commodity.
Money becomes a protocol and a commodity and labor exchange in raw
form, rather than a simple sum - it's back to the barter system.
An interesting idea, but it more or less prohibits offline
transactions involving a currency issue.  It also means the entire
market must be finite and closed.
I do not think that there are profits to be made as an issuer of
anonymous or hard-pseudonymous money.  That's one of the reasons I
advocate the "everyone is potentially a mint" model -- the expenses of
issue, and the cost of doing business uphill against trust until one's
issue is trusted, should be shared in something like equal proportions
by people who undertake it voluntarily.

@_date: 2002-02-05 14:47:27
@_author: bear 
@_subject: biometrics 
You could, I suppose, create an algorithm that takes as inputs
your "single" PIN/password and the name of the entity you're
dealing with, and produces a "daily use" PIN/password for you
to use with that entity.
It wouldn't help much in the daily use arena -- you'd still
have to carry all the daily use PINs around in your head -
but in the scenario where you forget one, it could be used to
recreate it, and it would be a bit more secure than carrying
around the sheet of paper where your 20 PINs are all written

@_date: 2002-02-21 11:10:46
@_author: bear 
@_subject: Cringely Gives KnowNow Some Unbelievable Free Press... (fwd) 
I really want to read this paper; if we don't get to see the
actual mathematics, claims like this look incredibly like
someone is spreading FUD. Is it available anywhere?

@_date: 2002-02-25 11:49:26
@_author: bear 
@_subject: Cringely Gives KnowNow Some Unbelievable Free Press... (fwd) 
[Moderator's inquiry: Any third parties care to comment on this? --Perry]
Holy shit.  The math works.  Bernstein has found ways of
using additional hardware to eliminate redundancies and
inefficiencies which appear in any linear implementation of the
Number Field Sieve.  We just never noticed that they were
inefficiencies and redundancies because we kept thinking in
terms of linear implementations.  This is probably the biggest
news in crypto in the last decade.  I'm astonished that it
hasn't been louder.
Note that there have been rumors of an RSA cracker built by a
three-letter agency in custom silicon before this, but until
analyzing Bernstein's paper I had always dismissed them as
ridiculous paranoid fantasies.  Now it looks like such a device
is entirely feasible and, in fact, likely.
This work demonstrates a lack of security in a bunch of PGP Keys.
All previous estimations of security level as a function of bit
length, should be applied as though the bit length were one-third
of its actual length.  This means that effectively all PGP RSA
keys shorter than 2k bits are insecure, and the 2kbit keys are
not nearly as secure as we thought they were.
I remember there was one version of PGP that allowed RSA keys
longer than 2kbits - I don't remember what version it was right
now, but someone is sure to remind us now that I've said so. :-)
Anyway, probably very few people are using 4kbit or 8kbit PGP
RSA keys anyhow, due to lack of cross-version compatibility.
The "secure forever" level of difficulty that we used to believe
we got from 2kbit keys in RSA is apparently a property of 6kbit
keys and higher, barring further highly-unexpected discoveries.
Recommendation to all implementors:  Future applications should
not offer to create RSA keys shorter than 2048 bits, and should
allow users to specify keys of up to *at least* 8 kbits in length.
Remember, backward compatibility is inappropriate where it
compromises security.
Recommendation to all crypto users: discontinue use of RSA keys
shorter than 2048 bits, NOW.  Issue a revocation if the software
you use allows it.  If the software you use is restricted to
RSA keys shorter than 2048 bits, get rid of it and find something
I predict that Elliptic-Curve systems are about to become more

@_date: 2002-02-26 08:40:40
@_author: bear 
@_subject: Cringely Gives KnowNow Some Unbelievable Free Press... (fwd) 
Right.  I'm not very comfortable with Elliptic-Curve yet, either.
I haven't been able to work out exactly how, but I have a gut
feeling that there may be some translation or transformation of
the Elliptic-Curve problem that simplifies to integer factoring,
and as a result I'm not comfortable with EC key lengths shorter
than factorable numbers. However, I'm just a hobby mathematician.
I'm going to let the real mathematicians pound on it for a decade
or so and see what they come up with.
This is probably a good idea - but independent keys for those
systems are going to make the keys *long*. Still, disk space is
cheap now, so yeah, that's probably the way to go.
Isn't Elliptic-Curve patent-encumbered?

@_date: 2002-02-27 15:40:52
@_author: bear 
@_subject: Cringely Gives KnowNow Some Unbelievable Free Press... (fwd) 
Hmmm.  According to Bernstein,  It's better and worse than
it first appeared.  On the one hand, the "o(1)" term may
be quite large and cancel much of the speedup for keys of
practical size, and even with reduced costs, that's still
a lot of single-purpose hardware to build for a practical
keysize.  On the other hand, RSA is not the only system
affected.  The technique may work on Elliptic Curve systems
as well. Which of these sides is "better" and which "worse"
is something that you will have to work out depending on
your own perspective.

@_date: 2002-07-02 11:04:10
@_author: bear 
@_subject: Montgomery Multiplication 
It's kind of an exotic technique, but it's one I happen to know...
Montgomery Multiplication is explained for the computer scientist
in Knuth, _Seminumerical Methods_.
Briefly: The chinese remainder theorem proves that for any set
A1...AN of integers with no common divisors, any integer X which is
less than their product can be represented as a series of remainders
X1...XN, where Xn is equal to X modulo An.
if you're using the same set of integers with no common divisors
A1...AN to represent two integers X and Y, you have a pair of series
of remainders X1...XN and Y1...YN.
Montgomery proved that Z, the product of X and Y, if it's in the
representable range, is Z1...ZN where Zn equals (Xn * Yn) mod An.
This means that, for integers A1..AN that are selected to be small
enough for your computer to multiply in a single instruction, you
can do multiplication of extended-precision integers less than their
product in linear time relative to the size of the set of integers
A; just multiply each pair of remainder terms modulo the modulus
for that term, and the result is the product.  As a technique,
it's a useful method when you know ahead of time what approximate
size your bigints are.
With positionally-notated integers, You need time N squared, or
at least N log N (I seem to remember that there was an N log N
technique, but I don't remember what it was and may be mistaken),
relative to the length of the integers themselve.
There are a few optimizations available so you can select a set A
that's just big enough to do the job, but they require some degree
of foreknowledge of the job that you often don't have when writing
The major problem with Montgomery representation of integers is
that it makes division hard. If you can arrange your problem so
you don't need division, and you know the approximate size of
the bignums you'll be working with, it can speed things up

@_date: 2002-07-04 09:30:06
@_author: bear 
@_subject: crypto/web impementation tradeoffs 
Without more knowledge of the parameters of the system
(especially the threat model), it's hard to say -- however,
this sounds like a case for the Diffie-Hellman key agreement
protocol.  Have the client and server each pick a random
number, and then use those numbers to generate a key

@_date: 2002-07-18 13:08:55
@_author: bear 
@_subject: It's Time to Abandon Insecure Languages 
Agreed.  And I particularly like Scheme.  However, it's also not
hard to compile your C code with bounds checking turned on if you're
willing to sacrifice maybe a few things you shouldn't be using anyay,
so it's pretty inexcusable IMO to still be having buffer overflows.

@_date: 2002-07-19 16:08:28
@_author: bear 
@_subject: It's Time to Abandon Insecure Languages 
You may regard pointer arithmetic as fundamental, but I've
written lots of programs without using it.  As far as I'm concerned,
the only spot for numbers and pointers to mix directly is when
you need to set pointers at the absolute addresses of hardware
i/o ports and the OS provides no mechanism for discovering the
addresses of those ports.  Likewise, sprintf, in its raw or
traditional form, is one of the things you need to give up,
and which you should have given up already as part of good
programming practice.  It is badly designed.
It is possible to write a function that tells exactly
how many characters will be needed (at maximum) to carry
out a format command.  Any "good" definitions of sprintf ought
to use such a function, along with an access to the allocation
database maintained by malloc, or probably an enhanced malloc
that knows more about variable boundaries inside the stack,
to ensure that they are in fact writing into a buffer that's
big enough, and throw an appropriate error otherwise.
You've got a point about pointer arithmetic -- if it's really
widely used, there's just no hope for that body of code.  But
I still say that other than that, most of the problems with C
could be solved, transparently to most existing code, by an
appropriate refactoring of its malloc, I/O, and string libraries.

@_date: 2002-07-19 20:58:56
@_author: bear 
@_subject: Maybe no stego on eBay afterall 
And then again, there's the undeniable impulse, when you read about
someone hunting stegotexts on ebay, to go and post one there just to
give him something to worry about.  I have to wonder whether one or more
punks somewhere are having a guffaw over this establishment type having
gotten taken in by the stegotext they posted.
Wanna bet that, if decrypted, it just contains a small drawing of the
author flipping off the researcher, or something equally stupid?

@_date: 2002-07-19 21:09:24
@_author: bear 
@_subject: Maybe no stego on eBay afterall 
If the pictures are scaled ("digital zoom" anyone?) or smoothed, the
pixel values (including the LSB's) turn into a system of fairly simple
linear equations relating adjacent pixels.  This might be the effect
you're seeing.  Of course, you could also be seeing hardware that
creates this very correlation as some side effect of the way it works,
too.  But in a scaled or smoothed image, where the pixels are literally
the output of simple linear equations, any "randomness" sticks out
like a sore thumb.

@_date: 2002-07-23 08:01:45
@_author: bear 
@_subject: building a true RNG (was: Quantum Computing ...) 
Depends on the data and how much entropy you suppose it
has, really.  An irreversible compression function that
I use when extracting entropy from text (for other
purposes) is to have a counter.  Each time you process
a character, you add the character code to the counter,
then multiply the counter by 2.4 rounding down.  This is
based on estimates of 1.33 bits of entropy per character
in english text, and requires an "initialization vector"
(in this case an initialization value) twice as long as
the character code to prevent you from taking too many
bits from the first few characters alone.
For something like a lava-lamp picture, your compression
function might be first converting it into a 4-color image,
editing out the constant parts (eg, the lamp base and edges),
compressing that using PNG format, and then taking some
similarly counter-based function of those bits. Using a
time series of pictures of the same lava-lamp, you'd have
to adjust for lower entropy per byte of processed PNG (by
using a lower factor), because it could be redundant with
other frames.
You are talking, specifically, about cryptographic hash
functions.  The diagram specifies a simple hash function.
The distinction between cryptographic hashes and simple
hashes is, a simple hash is supposed to produce evenly
distributed output.  A cryptographic hash is supposed to
produce evenly distributed *and unpredictable* output.
A simple hash, plus a whitener, is about what you're
thinking of for a cryptographic hash function.
Roughly.  People talk of "digestion" of a datastream, or
"distillation" of entropy, or "irreversible compression",
etc.  It's roughly the same thing.
That's one bit per 8k. I guess it just depends on which
8k comes through and how much your opponent can make of
one bit.
I think you may be right about that -- whitening protects you
from errors in an overly-simple distiller such as I described
above, but if you've got a really fine-tuned one, it doesn't
help much.
Again, it violates the requirements of a cryptographic hash
function, not a simple hash function.

@_date: 2002-07-30 18:54:28
@_author: bear 
@_subject: building a true RNG 
The above sentence is unsound.  You cannot take an assumption
(If SHA-1 is indistinguishable....) and then use its negation
to prove something else (... then we would like to prove that
... the attacker would need to be able to distinguish...),
unless you prove your something else for both the TRUE and FALSE
values of the assumption.
Euclid was all over that, and George Boole after him.
Now, for the TRUE value of your assumption, which I
think you may have meant:
IF (A) SHA-1 is indistinguishable from a random oracle without
       prior knowledge of the input,
   AND
   (B) We can prove that in order to succeed in an attack, an
       attacker would need to distinguish SHA-1 from a random
       Oracle without prior knowledge of the input,
   THEN
   (C) The attacker will not succeed in that attack.
But for the FALSE value of your assumption A, we get
   IF (B) AND (~C) THEN (~A)
   IF (B) AND (~A) THEN (C) OR (~C).
So if we take B as an axiom, then we did not prove
anything about A unless given ~C and we did not prove anything
about C regardless of our assumptions about A.

@_date: 2002-06-18 10:12:31
@_author: bear 
@_subject: Randomisation - IBM's answer to Web privacy 
I hope these guys didn't get a patent on this.  This is an
old statistics trick, where you know your data has a known
measurement error and you "correct" it in aggregate by
analyzing the distribution curve you actually got and
comparing it to the set of "natural" distribution curves
you suspect it must conform to -- normal or "bell-curve"
distribution when dealing with population demographics,
in most cases.  Random variations will make the distribution
curve proportionally different, and with elementary calculus
over distribution equations you can find the solution that
restores a "true" bell curve -- which is going to be within
epsilon of your true distribution in most cases.
Nice application to privacy, though.

@_date: 2002-06-18 10:20:00
@_author: bear 
@_subject: Randomisation - IBM's answer to Web privacy 
On further reflection, IBM's special sauce here may be a
different flavor entirely if the randomized data are linkable.
If you introduce a random error in measurement with a range
of say 5 years, and IBM does business with linkable customers
over a sequence of several orders, they can home in on the true
values for individual customers ages over time and literally
correct the database information based on variants recieved
with subsequent orders.  Let's say they'll get the customer's
age as 28 the first time -- and know it's between 23 and 33.
Then next time say they get 32.  Now they know it's 27 to 33.
The time after that, they get 27, and narrow it to 27 to 32.
Etc....  by the time the guy has placed 20 orders they're
probably going to know his age to within one year.

@_date: 2002-06-21 10:12:41
@_author: bear 
@_subject: Shortcut digital signature verification failure 
It's already been thunk of.  check the literature on "hash cash".
Basically, the idea is that the server presents a little puzzle
that requires linear computation on the client's side.  (same
algorithm as minsky used for his "time-lock").  The client
has to present the solution of the puzzle with a valid request.
To extend the idea to signatures, all you really have to do is
program the server to create puzzles that will take at least as
much computation to solve as it requires to check the signature.
And of course it checks the solution to the puzzle (using a single
modular-power operation, which is relatively cheap) before it
checks the signature itself.

@_date: 2002-06-24 18:54:56
@_author: bear 
@_subject: Ross's TCPA paper  
What they are doing is, in one sense, "bundling" the proprietary
media content with the proprietary operating systems; trying to fix
it so you can't have one without the other. And in this, they may
well succeed.
I think the DRM//media thing needs to be fixed, at the root.  By which
I don't mean denying these clowns their sterile fantasy; I mean just
creating open source alternatives that leave them behind.  We don't
just need open source programmers anymore; we also need open source
bands, singers, actors, directors, animators, and misc. other artists.
Information may want to be free, but it's not going to happen, not for
long, unless the people who create the information *set* it free and
fight to *keep* it free.  Software artists, being sufficiently in demand
to get not-very-oppressive contracts and less financially desperate
on the whole than other artists, have been able to lead the curve in
creating art for the public -- but other kinds of artists need to
follow or the open-source movement is not going to get past this DRM

@_date: 2002-06-25 10:22:46
@_author: bear 
@_subject: Ross's TCPA paper  
Yep.  It was a very similar frustration that moved Stallman to
start working on GNU.  At that time, no matter what programs
you wrote, you wound up relying on a proprietary operating system,
and nobody could run your programs without paying the reichsgelt.
Since Stallman wanted to share stuff even with poor people, he
launched what was then considered a lunatic's crusade, duplicating
the functionality of every damn proprietary thing, starting from
zero, under a free license.  None of which mattered much until
Torvalds wrote the linux kernel and it finally became possible
to actually run Stallman's free stuff without paying the reichsgelt.
An open-art movement faces a very similar problem, but from a
cultural rather than a technical perspective.  'happy birthday'
is copyrighted, so somebody needs to write a song capable of
being a cultural replacement and develop a free-play license
making it possible for other artists to do covers and adaptations
as they like provided they license their performance likewise.
With freeplay songs, bands could still make money from
performances and club dates.
Ever notice restaurants where the waitstaff sings happy birthday
to customers never use the copyrighted version?  I bet you could
get them to use the freeplay version.  I bet bars would go for
a jukebox full of songs they didn't have to pay ASCAP for.  And
musicians would really like a bundle of stuff they could do
covers of without worrying about that crap.  Picture distributions
of a dozen DVDs covered with a hundred thousand MP3's (and
MIDI files so people can do their own remixes) by a thousand
ObCrypto: Could we use the DRM/trusted platform stuff they plan
to put into the computers to "enforce" the freeplay license?  ie,
make sure all derivative works also bear the freeplay license?

@_date: 2002-06-26 10:01:00
@_author: bear 
@_subject: Ross's TCPA paper  
You are fundamentally confusing the problem of
privacy (controlling unpublished information and
not being compelled to publish it) with the
problem of DRM (attempting to control published
information and compelling others to refrain
from sharing it).  Privacy does not require
anyone to be compelled against their will to
do anything.  DRM does.
As I see it, we can get either privacy or DRM,
but there is no way on Earth to get both.
Privacy can happen only among citizens who are
free to manage their information and DRM can
happen only among subjects who may be compelled
to disclose or abandon information against
their will.
Privacy without DRM is when you don't need anyone's
permission to run any software on your computer.
Privacy without DRM is when you are absolutely free
to do anything you want with any bits in your
posession, but people can keep you from *getting*
bits private to them into your posession.
Privacy without DRM means being able to legally
keep stuff you don't want published to yourself,
even if that means using pseudonymous or anonymous
transactions for non-fraudulent purposes.
Privacy without DRM means being able to simply,
instantly, and arbitrarily change legal identities
to get out from under extant privacy infringements,
and not have the new identity easily linkable to
the old.
Privacy without DRM means people being able to
create keys for cryptosystems and use them in
complete confidence that no one else has a key
that will decrypt the communication -- this is
fundamental to keeping private information
Privacy without DRM means no restrictions whatsoever
on usable crypto in the hands of citizens.  It may
be a crime to withhold any stored keys when under a
subpeona, but that subpeona should issue only when
there is probable cause to believe that you have
committed a crime or are withholding information
about one, and you should *ALWAYS* be notified of the
issue within 30 days.  It also means that keys which
are in your head rather than stored somewhere are
not subject to subpeona -- on fifth amendment grounds
(in the USA) if the record doesn't exist outside
your head, then you cannot be coerced to produce
Privacy without DRM means being able to keep and
do whatever you want with the records your business
creates -- but not being able to force someone to
use their real name or linkable identity information
to do business with you if that person wants that
information to remain private.

@_date: 2002-06-28 19:49:52
@_author: bear 
@_subject: Ross's TCPA paper 
The problem is that the "analog hole" is how we debug stuff.
When our speakers don't sound right, we tap the signal, put
it on an oscilloscope so we can see what's wrong, correct
the drivers, and try again.  When our monitor can't make sense
of the video signal, it's different equipment but the same
idea.  When you encrypt all the connections to basic display
hardware, as proposed in Palladium, it means nobody can write
drivers or debug hardware without a million-dollar license.
And if you do fix a bug so your system works better, your
system's "trusted computing" system will be shut down.  Not
that that's any great loss.
Likewise, encrypted instruction streams mean you don't know
what the hell your CPU is doing.  You would have no way to
audit a program and make sure it wasn't stealing stuff from
you or sending your personal information to someone else.
Do we even need to recount how many abuses have been foisted
on citizens to harvest marketing data, and exposed after-the-
fact by some little-known hero who was looking at the assembly
code and went, "Hey look what it's doing here.  Why is it
accessing the passwords/browser cache/registry/whatever?"
Do we want to recount how many times personal data has been
exported from customer's machines by "adware" that hoped not
to be noticed?  Or how popup ads get downloaded by software
that has nothing to do with what website people are actually
looking at?
I don't want to give vendors a tunnel in and out of my system
that I can't monitor.  I want to be able to shut it down and
nail it shut with a hardware switch.  I don't want to ever
run source code that people are so ashamed of that they don't
want me to be able to check and see what it does; I want to
nail that mode of my CPU off so that no software can turn it
on EVER.
I'll skip the digital movies if need be, but to me "trusted
computing" means that *I* can trust my computer, not that
someone else can.

@_date: 2002-03-06 12:24:18
@_author: bear 
@_subject: [CYBERIA] Open Letter to Jack Valenti and Michael Eisner 
[Moderator's note: No, I don't want to open up the floodgate, but this
has a genuinely new idea in it among some others -- the notion that
perhaps the good of the entertainment industry isn't as important as
general purpose computing. That said, this is far afield from
cryptography (I'm only interested here because of the technological
copy protection politics angle) and I'm not going to entertain
followups unless they're genuinely interesting. --Perry]
Perhaps the time has come.
Copyright was necessary in earlier times because so few people
had the time to think and produce new ideas -- novels and songs
were rare, valuable to society, cost a lot of time and effort
to publish and distribute, and the people who made good ones
had to be supported and protected.
But these days a talented hobbyist can make really great music,
do all the mixing digitally on his or her home system, and release,
and there are hundreds of thousands of talented hobbyists.  The
publishers and studios can add no value.  Graphic artists can work
at home now.  Pixels don't care a bit whether they're produced in
a studio.  Publishing houses have more good novels available than
they can ever publish, even not counting the professional novelists.
And it is now possible for a hobbyist writer or musician to publish
entirely on the net at very little cost to themselves - or if
someone mirrors the work, at no cost to themselves whatsoever.
So, a few random ideas to keep in mind the next time you hear
someone arguing that computers must be crippled:
Society no longer needs copyright.
Society *DOES* need general-purpose computers.
To the extent that copyright threatens general-purpose computing,
it is harmful.
With the Internet, we no longer need publishers and distributors.
Go to a site like MP3.com and see how visibly redundant they have
Good musicians can play club dates and get a percentage of the door,
and sell signed disks they burn themselves to the people at the
Good authors can go on lecture tours, or get paid by bookstores for
promotional appearances.
or, maybe, we can just leave it at "real artists have day jobs."

@_date: 2002-03-20 23:00:16
@_author: bear 
@_subject: crypto question 
The answer is in fact "no."
However, your actual needs may be amenable to some slightly different
question, or a protocol-based method.  What are you trying to accomplish?

@_date: 2002-03-28 16:07:06
@_author: bear 
@_subject: authentication protocols 
"Authentication" relative to what?
All identity, and therefore all authentication, derives from
some kind of consensus idea of who a person is.  With no third
party, it is hard to check a consensus.
Usually authentication comes down to checking a credential. But
that implies some third party that issued the credential.
So, the pertinent question becomes, what is identity? For purposes
of your application, I mean -- no point to go off on philosophical
tangents.  Answer that, and maybe there'll be a protocol that you
can use.

@_date: 2002-05-13 09:45:49
@_author: bear 
@_subject: objectivity and factoring analysis 
One thousand years = 10 iterations of Moore's law plus one year.
Call it 15-16 years?  Or maybe 20-21 since Moore's seems to have
gotten slower lately?

@_date: 2002-05-13 10:16:10
@_author: bear 
@_subject: Pact Reached to Stop Pirating Of Digital TV Over the Internet 
The proposed system is of course broken.  It's defeasible in
software on any general-purpose computer for as long as general-
purpose computers are legal, as has been proven already.
But you know, I really don't give much of a crap about commercial
content anymore.  Will this system get in my way if I try to
make and distribute (and play and copy on standard hardware) a
nice digital-video, digital-audio recording of a family wedding,
or an original computer-generated movie, or a demo video for my
buddy's band?  'Cause really, that's the problem as far as I'm
concerned; if the system prevents people from making and
distributing our *own* content with compatible hardware, then
it has to be destroyed.

@_date: 2002-11-05 17:15:25
@_author: bear 
@_subject: patent free(?) anonymous credential system pre-print 
There is a possibility that you have neglected.  And, evidently,
so have most of the patent-filers.
Twenty years is not so long.  Patents expire.
It's not terribly helpful for someone to lock up an idea for twenty
years, but honestly it may be at least that long before the legal and
cultural infrastructure is ready to fully take advantage of it anyway.
You, like most engineers, are thinking of technical barriers only;
it's entirely reasonable to suppose that you could deploy the
technical stuff in two to five years and rake in money on your patents
for the next fifteen to eighteen.  That's a valid model with computer
hardware, because its value to business is intrinsic.  Bluntly, it
enables you to do things differently and derive value within your own
company regardless of what anyone else is doing.  But here we are
talking about something whose value is extrinsic; it affects the way
mutually suspicious parties interact.  For changes in that arena to
happen, they have to be supported by the legal system, by precedent,
by custom, by tradition, etc.  These are barriers that will take a
*hell* of a lot longer to overcome than the mere technical barriers.
The fights over liability alone will take that long, and until those
fights are settled we are not talking about something that a
profit-motivated business will risk anything valuable on.
I remember having exactly your reaction (plus issues about patenting
math and the USPTO being subject to coercion/collusion from the NSA
and influence-peddling and so on...) when the RSA patent issued - but
RSA is free now, and RSA security has not made that much money on the
cipher itself.  And frankly, I don't think that having it be free much
earlier, given the infrastructure and implementation issues, would
really have made that much of a difference.  Note that there are
*still* a lot of important court decisions about asymmetric encryption
that haven't happened yet, and it was only profitable (due to
e-commerce) for the last couple years of the patent's run.
These patents are being filed in an industry and application which is
NOT part of how the world does business today.  They may or may not
turn out to be enabling items, but the world will have to learn to do
business in a different way before they become relevant.  That's not
going to happen in time for the dog-in-the-manger crowd to make any
money off the patents they're filing, so unless they can mobilize
*BILLIONS* of dollars for infrastructure replacement, education,
marketing, lobbying, court cases about legal validity for their
digital signatures and credentials, etc, etc, etc, there is no chance
of them withholding anything of value from the public domain.
It will take twenty years or more just for the *legal* system to
adjust to the point where a credential system or "non-repudiation
property" might possibly become useful to business.  Add another five
or ten years at least for acceptance and custom to grow up around it.
Another five or ten years for court cases and precedent and decisions
about liability to get settled so that it can become standard business
practice.  By that time the patents will be long gone.
Check history.  There is a long list of companies that made cipher
machines or invented ciphers, patented them, and went broke.  It isn't
a coincidence, nor a recent development.

@_date: 2002-11-08 08:36:56
@_author: bear 
@_subject: Did you *really* zeroize that key? 
I remember this issue from days when I wrote modem drivers.
I had a fight with a compiler vendor over the interpretation
of "volatile".
They agreed with me that "volatile" meant that all *writes*
to the memory had to happen as directed; but had taken the
approach that *reads* of volatile memory could be optimized
away if the program didn't do anything with the values read.
This doesn't work with the UARTs that I was coding for at the
time, because on those chips, *reads* have side effects on
the state of the chip.  If a read of the status register
doesn't happen, then subsequent writes to the data buffer will
not trigger a new transmit.
The compiler vendor had not foreseen a situation in which
reads might have side effects, and so the compiler didn't
work for that task. I wound up using a different compiler.
Although the bastards never admitted to me that they were wrong,
I noted that in their next patch release, it was listed number
one in the list of critical bugfixes.

@_date: 2002-11-28 11:27:38
@_author: bear 
@_subject: 'E-postmark' gives stamp of approval 
Hmmm.  Spam wasn't as big a problem six years ago, either.  And
businesses weren't looking at email as an avenue for legal and
commercial communication six years ago, either.
I dislike microsoft, but if this service is available without them,
and available for use by free software makers (yes, I know the
postmarks will still cost money, but the software to get them from
USPS doesn't have to be as proprietary or restricted as microsoft is
undoubtedly making theirs) it could become very useful.  If it becomes
widespread, I might start discarding unread all email from parties
unknown to me that doesn't bear a postmark, in the same way that I now
discard unread all email from parties unknown to me that doesn't have
my email address on it or that comes from untraceable hosts.
And the reason of course would be no different than the reason I throw
away all paper mail from parties unknown to me that doesn't bear at
least first-class postage; If somebody I don't know didn't pay enough
to show he gives a shit about talking to *me*, as opposed to any
random pair of eyeballs, then he's a bulkmailer and not worth my time.
The vulnerability of SMTP is a known problem. SMTP traffic, by
default, is easy to subvert, easy to eavesdrop on, easy to forge, easy
to divert, and easy to obfuscate.  SMTP is a playground for spammers
and con artists, and sufficiently unreliable and subvertible that no
legally binding or important documents can be safely trusted to it.
Business really wants a reliable electronic communications medium for
legally binding content, and SMTP is a spectacular failure on that
front. We have needed a better standard email protocol for a long
time, but the only real entries in the race have been locked up by
licensing costs and interoperability issues and so we've been hanging
ugly bags on the side of SMTP without fixing its fundamental issues.
We need a better protocol, for authentication, message integrity,
privacy, portability, and lots of other reasons. This product failed
six years ago, but I think that the SMTP problem, both as an open
wound into which spammers have been rubbing salt and as an
impossibility for confidential or legal-process communication, hurts
worse now than it hurt six years ago.  It may catch this time.
Not that I consider the US Postal service, or Microsoft, as players
likely to make anything *less* capable of being eavesdropped on or
subverted.  But authenticated senders, verifiable message integrity,
and reliable return-receipts for authenticated readers would be a step
forward, and I can't get any of them reliably with SMTP.
Sigh.  Ideally, I'd prefer the idea of a bond rather than a toll.  If
I could get email through some channel that guaranteed someone would
lose $1 *if* I designated their email as spam, I'd open every last
letter I got through that channel because I'd be confident that no
bulkmailer would *EVER* use it. I don't actually want corresponding
with me to cost money, I just don't want to be a "free target" for

@_date: 2002-10-03 07:41:52
@_author: bear 
@_subject: What email encryption is actually in use? 
I consider that state perfectly well defined -- it is the
"no connection" state.  The only reason any protocol works
is because people prefer abiding by its rules and the
policies each other set up in it to having no connection.
The essence of a protocol is to detect situations where
one party or the other prefers "No connection" over the
rules, and enforce that such detection happens before any
confidential data is shared.  According to this rule, I
would say that the protocol you say is in an "undefined
state" has in fact functioned perfectly.  It detected a
rule that the other was not willing to abide by and dropped
the connection *before* risking any confidential data.
That's precisely what it was supposed to do.
But if you are willing to abide by the sending-plaintext
protocol in the first place, this is perfectly reasonable
too. Protocol termination for lack of willingness to trust
single-DES is no different than termination of protocol
for lack of willingness to send (or receive) plaintext.
Where our protocol design fails is in considering "plaintext"
to be something other than "a particularly unreliable and
ineffective encryption algorithm."  Certainly nobody who's
willing to reject a connection for a self-signed certificate
should be willing to accept plaintext, because obviously
plaintext is not as secure as the minimum security they are
requiring.  But experience shows that people willing to
reject self-signed certs and poor ciphers always seem to
be willing to accept the even poorer cipher named plaintext.
This is completely irrational; either you need security or
you don't.

@_date: 2002-10-03 07:48:16
@_author: bear 
@_subject: Gaelic Code Talkers 
Neal Stephenson probably ran into a similar story; he inserts it
(in fictionalized, names-changed form) in his novel Cryptonomicon.
You can probably find some references to the historical precedences
by googling starting there.
[Moderator's Note: I think Stephenson's story didn't seem to involve
code talkers, and appeared to be entirely fictional... --Perry]

@_date: 2002-10-13 08:42:17
@_author: bear 
@_subject: Microsoft marries RSA Security to Windows 
Good grief.
This is an old, old story by now, and it's starting to really
piss me off. It seems like every last attempt to implement
security of any kind in a commercial product gets compromised
for the sake of convenience/marketability, etc.
A system that is *actually* secure is inconvenient, or requires
mental effort to manage keys, or offline key storage, or won't
interact transparently with known insecure programs, or some
other basic fundamental constraint they're not willing to live
with -- so they take a component (RSA in this case) that could
have been used to build a secure system, use its presence as a
point to *claim* that that's what they're building, and build
something else.
It's irresponsible.  It makes *actual* security into a rare,
specialized, and arcane field.  It creates expectations that
you can do insecure things with "secure" software.  It gives
users a *FALSE* sense of security and deters them from getting
products that are actually secure.  It uses fraudulent (or, to
be very charitable, perhaps "mistaken") claims of security to
compete unfairly with actual secure software which, of course,
has constraints on its operation.
I think somebody needs to start assigning security grades
based on the theory that it's the weakest link (PRNG with
state value out in the open) rather than the strongest (we
use whizbang patented strong encryption algorithm!) that
determines security. It's basically a matter of consumer
protection, and it's really something that security and crypto
people need to do within the industry.  It has to be within
the industry, because this is stuff that is well outside
a layman's ability to judge.

@_date: 2002-10-18 11:47:32
@_author: bear 
@_subject: QuizID? 
Actually, it looks like a fairly good idea.  The idea of a
standalone token (ie, not requiring any electronic interface
to the machine) eliminates some hardware barriers that would
otherwise hinder the device's acceptance, and it really *is*
a lot more secure than password authentication.
It could be made better -- you could have the server take the
user's password and issue a challenge for that user's device,
which the user would then punch into the device, and enter the
device's response back to the server.  In fact that may be how
this thing works - I couldn't tell for sure through all their
marketroid-speak whether there is a unique challenge from the
server or whether the user enters the same use-code into the
device every time.
But, even though that would be more secure, it could also end
up in a slightly less desirable position on the security-
versus-annoyance curve. I think the major target here is
consumer-grade security - while it would be nice if these
devices were secure enough to control access to fort knox,
they can't afford to annoy users enough (or require them to
think enough) to get that level of security.

@_date: 2002-10-22 11:09:41
@_author: bear 
@_subject: Why is RMAC resistant to birthday attacks? 
This is a point I don't think I quite "get". Suppose that I have
a MAC "oracle" and I bounce 2^32 messages off of it.  With a
64-bit MAC, the odds are about even that two of those messages
will come back with the same MAC.
But why does that buy me the ability to "easily" make a forgery?
Does it mean I can then create a bogus message, which the oracle
has never seen, and generate a MAC that checks for it?  If so
In protocol terms, let's say Alice is a digital notary.  Documents
come in, and Alice attests to their existence on a particular
date by adding a datestamp, affixing a keyed MAC, and sending
them back.
Now Bob sends Alice 2^32 messages (and Alice's key-management
software totally doesn't notice that the key has been worn to
a nub and prompt her to revoke it).  Reviewing his files, Bob
finds that he has a January 21 document and a September 30
document which have the same MAC.
What does Bob do now?  How does this get Bob the ability to
create something Alice didn't sign, but which has a valid MAC
from Alice's key?

@_date: 2002-10-23 07:50:44
@_author: bear 
@_subject: Why is RMAC resistant to birthday attacks? 
This is interesting, but there are two easy ways to prevent it:
1) First, it doesn't work unless the MAC algorithm is sequentially
   linear (ie, unless its internal state never depends on nonlinear
   interactions between earlier and later parts of the message).
   Sequential linearity, for this reason, seems like a very bad
   property for a MAC algorithm to have.
2) Second, it only works if the MAC generated discloses the *entire*
   internal state of the MAC algorithm -- which it should not, any
   more than any other pseudorandom number generated should disclose
   the entire internal state of the PRNG that generated it.  The
   MAC algorithm should have *FAR* more internal state than the number
   of bits disclosed in the MAC.  If the algorithm has N bits of
   internal state and the MAC is M bits where M < N, and the algorithm
   is secure, then the attacker cannot determine with a better than
   1/((N-M)^2) probability that the MAC algorithm's internal state
   is in fact the same after each of two messages that generate
   identical MACs.
And these are both very basic points, and ought to be easy things
for MAC designers to avoid.  So, if I found a MAC algorithm that
were subject to the attack you describe, I would cheerfully toss
it out with the morning trash as a Bad Idea.

@_date: 2002-10-25 10:22:43
@_author: bear 
@_subject: more snake oil? [WAS: New uncrackable(?) encryption technique] 
The implication is that they have a "hard problem" in their
bioscience application, which they have recast as a cipher.
But in most cases, when someone - especially someone without a
crypto background - tries to transform a hard problem into
a cipher, the break on the cipher comes at some point in the
transformation, rather than on the hard problem itself.
I think it's not unlikely that the cipher can be broken and
not unlikely that the break will not help them at all with
their hard problem.
One thing that strikes me about it is that it doesn't seem to
be a practical cipher in any case.  It is too slow when implemented
in software to be competitive with known-good ciphers that we
have today, so it has little value as a cipher even if it does
turn out to be as unbreakable as the best we've got.  A good
cryptographer would spend time optimizing the snot out of it
and abstracting away operations that don't add security, in
order to make it fast enough to be competitive - after
which it might bear only a dim resemblance to the hard problem
that inspired it anyhow.
Offhand, I'd say that since it isn't a practical cipher to use
anyway, it's probably not a good use of time for professional
cryptographers to try to break.  On the gripping hand, if
there's a pro out there who wants to donate some specialized
mathematical expertise to biosciences, with or without compensation
from the benefactors photonics, this may be a nice
way to do it.
On the gripping hand, since they've patented the method rather than
placing it in the public domain, you have to realize that it's a
donation to a single company rather than a donation to "human
knowledge" or biosciences in general.  There're plenty of worthwhile
things to work on that are truly public if you're feeling like
donating time and expertise on a charitable basis.

@_date: 2002-09-16 17:33:11
@_author: bear 
@_subject: Cryptogram: Palladium Only for DRM 
You know what?
Any designer of financial services software should know better than
designing something which stores sufficient information on the local
machine to enable a signon to the account. Simply speaking, it is a
fundamental of financial services software that there must not be
adequate information on the machine to enable authentication without
an explicit user action. If posession of the machine alone is
sufficient to authenticate with financial services, then we are
back to the situation where thieves would break into the houses of
the wealthy to steal all their money -- except now they would do
it by stealing the computer instead of by stealing a big box of
jewelry, silverware, and gold coins.
Machine-based signon makes the PC into a thing that has to be locked
up in the safe at all times, just like a big pile of gold coins
sitting on the desk as far as thieves are concerned.  It would
remove all the protections normally associated with bank vaults
and Federal Deposit Insurance.
If anyone builds a system that allows authentication based solely
on information stored on the machine, they should not be surprised
when insurance companies which are sane refuse to insure
transactions mediated thereby, and refuse to ensure accounts
subject to such transactions.
Pd does not change this fundamental fact.  If it's possible for
software, including Palladium-dependent software, to sign on and
authenticate with the bank without having human input to the
authentication step, then it's possible for a thief in posession
of the machine to create a situation where the bank is fooled
into thinking that something is a legitimate transaction, when
it is not.
And if it is *not* possible for software to authenticate with the
bank without human input, whether or not it is palladium-dependent,
because there is not sufficient information on the machine to
enable an authentication, then it is not possible for the "next
nimda" to empty your bank account.  That is what we refer to as
a sane design, with or without Palladium.
Thus your argument defeats itself.  You can't have it both
ways.  This is why "single signon" is insane from a security
point of view, along with any software that permits it.
Machine-based authentication is suicidal for financial
applications, with or without Palladium in the mix.  It
gives thieves a motive to break into people's homes, which
makes those people less safe as well as risking their money.
Schneier reached exactly the same conclusion about Pd that I did,
for exactly the same reasons.

@_date: 2002-09-18 18:41:41
@_author: bear 
@_subject: Cryptogram: Palladium Only for DRM 
In emergencies, yes.  Remember the people trying to deal with
and organize the WTC rescue efforts, whose software kept rebelling
because of inappropriately-enforced license issues?  Care to even
estimate the liability for lives lost due to that?  You want to
create a system where they'd have *NO* way to override copyright
in a real emergency, *NO* way to save lives?  No. That's cut and
dried, because Copyright is never an emergency.  Copyright infraction
never costs lives.
I for one don't give a flaming shit whether someone has the
"legal rights" to equipment he has to use in an emergency to
save lives.  When putting automatic enforcement in place
means that lives will be lost, it is a Bad Idea. A company
that did it might (and IMO should) be held liable in court.
Furthermore, if you think that Pd will only be used for legal
purposes by the software vendors and manufacturers who control
it, I strongly suggest you revise your trust model....  I have
seen no indication anywhere that these people are any more
trustworthy than those whose actions you decry. The only
difference is that the scale of abuses which can be perpetrated
by them is staggeringly large compared to the minor abuse of
someone copying a song or running a program out of license.

@_date: 2002-09-22 11:46:06
@_author: bear 
@_subject: unforgeable optical tokens? 
Here's a potential application: consider it as a door key.  Every
time the user sticks it into the lock, the lock issues two challenges.
The first challenge is randomly selected; the lock just reads and
stores the result.  The second is for authentication: it issues the
same challenge it issued for the first challenge last time, reads
the result and compares it to the result it stored last time. If
it's a match, the lock opens.
This is not really applicable to remote authentication, because
in *remote* authentication, someone has to be *signalled* that
the authentication succeeded, whereupon the *signal* becomes just
another message that has to be protected using conventional crypto
and protocols.  But for *local* authentication, it's got some
good stuff going for it.
But consider the door lock application: There's no way for the
attacker (or the key-holder either) to know what challenge out
of zillions has been issued or what response out of zillions
has been stored. The door never had to send any of that
information over a network, so Eve can't get it and Mallory
can't replay or duplicate it; presumably it is stashed inside
tamper-resistant hardware somewhere in the lock.
Superficially, this resembles a smartcard key where the challenge
is a string and the response is the string encrypted according to
a key held on the smartcard.  But it's not subject to side channel
attacks like power measurement to extract its key for the encryption
operation the way smartcards are. And it is far more resistant to
duplication, even to an attacker who knows its internal structure
("key") and has the fab infrastructure. And it is many orders of
magnitude faster.  You shine lasers on it at particular angles
and at particular points on its surface for a challenge; its
response is at your sensors in a nanosecond or less.  No smartcard
is anywhere near that fast. And you can go swimming with it, which
you can't do with a smartcard; no need to ever have it out of your
posession, even when you're in the shower.
If you want to make whole computers that are tamper-resistant,
you could extend the "door key" metaphor to the computer itself;
with your key in it, it can read its hard drive and do computer-
like things.  Without your key in it, it's just a sealed lump
of metal and glass with some buttons on it. In an operating system
for such a machine, everything would be encrypted.  The boot sector
would be encrypted using the same protocol as the "door key" above,
with a different key for every bootup.
For the rest of the machine, instead of storing any encryption or
decryption keys anywhere, you'd store challenges for the token
and use its responses for the keys.  And every (say) tenth time
you touched something, you'd generate a new challenge, get a new
key from the token, and re-encrypt the plaintext with the new
key. That way even if a thief gets your machine, they can extract
zero information from it unless they get your keytoken too.
If your machine ever goes missing, and you still have the keytoken
in your posession, you have no security worries; likewise if the
keytoken ever goes missing, but you still have your machine.  It's
only if *both* of them go missing that you have a problem.
hmmm.  It becomes more rococo, but of course, it also makes it
easy to create a machine that can only be used with *all* of
two or more keytokens inserted; just the thing for mutually
suspicious parties to store confidential shared data on.
Anyway; it's nothing particularly great for remote authentication;
but it's *extremely* cool for local authentication.

@_date: 2002-09-24 10:13:40
@_author: bear 
@_subject: Sun donates elliptic curve code to OpenSSL? 
Read it again.  The first two words of the second sentence you
quoted are, "In addition..."
As I understand it, this means the donated code is available under
the OpenSSL source license.  So you *can* use it, whether or not
it's patented by Sun.
*In addition* to that, *if* you have software patents and you
promise not to sue Sun over them because of an infringement you
find in the donated code, then Sun promises that it won't sue
you either.  Sun does not forbid people from using the donated
code on the basis of whether or not they make this promise.
Basically, they're offering something they didn't have to offer
in order to release it under the OpenSSL license; if they'd
simply released it under the OpenSSL license, you'd have fewer
options, not more.

@_date: 2003-04-16 15:24:51
@_author: bear 
@_subject: FW: DMCA used to shut down campus ID security talk 
If you're as fed up about the DMCA et al as I am, please
do what I do.
Write a letter to your congresspeople asking for their
help in stopping copyright abuse.  It helps if you have
articles to cite; it helps if you have credentials; it
helps if you make a living doing security or working with
the ever-more-endangered public domain or fair uses of
copyrighted material.  It especially helps if you have
fundamental research you can't do or civil rights you
cannot exercise as a result of copyright abuse.
Today, two cases of copryight-abusing laws with darkly
ridiculous consequences were posted on this list.  I took
the opportunity to point out the articles to my senator,
along with a plea to help stop the madness of copyright
abuse.  I'm asking you to do the same, or similar.
If a lot of people write letters, they actually do notice.
But it has to start with a surge in the number of letters
received on a given topic, to kick over whatever mechanism
they use to decide to start tracking it.  Maybe we should
coordinate a "write your congresscritter" day on a wide
basis, just to make sure they get, at least once, the big
rush of messages that it will take to get the issue on the
radar screen.  After that, coordination with others doesn't
matter so much, as long as people do write letters and
they've got a bucket to put them in.
Thank you all.  I'll shut up about this now.  But I felt
that it needed to be said.

@_date: 2003-04-21 16:01:51
@_author: bear 
@_subject: DRM technology and policy 
I just want to chime in with a datapoint on DRM.
My favorite bands don't tend to have big labels behind them.  Every CD
but one that I've bought in the last six years, I've bought at a live
performance. The one that wasn't a live performance was put together
by a guy with a MIDI synthesizer over the course of fifteen weeks of
work, alone.
So far many schemes for DRM I've seen rely on people not having access
to the technologies available to major labels, or create marketplaces
restricted to songs owned by the major labels with DRM stuff built
into them.  Now, my favorite artists are already frozen out of the
major streams of the business by the consolidation of radio and their
consequent inability to get played on the airwaves.  I really *really*
don't want to move forward into a future where the keys they'd need to
put out CD's in a "standard" format that the market expects and that
music stores will have to keep track of differently if they sell it,
cost more than they make in a year.
I'll admit to being something of a copyright extremist; I think that
copyrights are fundamentally not as important as general purpose
computers that can do anything with any data, including copying it,
and I don't want anybody else telling me what I can do with bits I've
bought or what platforms I can play them on.  But as a music fan, I'm
also looking at a bunch of copyright schemes that would freeze my
favorite bands out of the market entirely.
Despite the fact that it's *technologically* possible in most cases
for them to go on making their own CD's, they wouldn't be on an equal
footing or subject to the same policy in record stores as the
major-label offerings, and most store managers are going to pull them
instead of trying to keep minimum-wage employees trained on more than
one procedure.  And new markets where downloadable music is kept
online are going to have massive legal liabilities if non-DRM'd stuff
is downloaded; you think they'll risk carrying something from bands
that can't afford an "authenticated" key?  These bands already have a
marginal existence; if major marketplaces that they can't get into at
all become the business standard, I don't know if they'll still be
around at all.
This is the same thing that happened with DeCSS, except from the
band's side of it instead of the Operating System's.  There's no good
reason why it ought to be illegal to play DVD's that you own on a
Linux box, but there wasn't some entity that stood to make its money
back on a million-dollar key investment, so it is.  Linux got frozen
out of the "legitimate" DVD player arena.  Now we're looking at a lot
of systems that run the risk of creating standard channels for the
music market which will freeze out all the small bands.  I don't want

@_date: 2003-04-21 16:46:47
@_author: bear 
@_subject: DRM technology and policy 
Okay, I'll start with the big problems as I see them. I have
basically six problems with the DRM systems that have been
proposed so far.  These are public-policy requirements, that
a DRM system has to meet in order to benefit society at all.
So far, no systems proposed have met more than one or two of
them.  In order to be a good idea, a DRM system must:
1) Empower individuals to use its protection for their private
   information.  That means that if some company wants my email
   address, I should be able to give them an email address that
   they *CANNOT* turn around and sell to a spammer.   Likewise if
   my attorney wants my financial records I should be able to
   satisfy the requirement by giving him or her financial
   records which he *CANNOT* forward without my knowledge to
   someone else.  If the DRM system has legal support, then
   accepting "protected" information should be a legal
   requirement for businesses - otherwise they'll just insist
   on plaintext and the DRM will not protect individuals.
   Frankly, I don't think this is possible - especially given
   that a lot of these businesses will have budgets to pay for
   hardware circumvention devices.
2) A DRM system must not freeze small producers out of the
   information marketplaces.  It must be equally accessible to
   all information producers, regardless of whether they have
   a budget to spend on an "authenticated" key.  Joe Blow's
   garage band must have the same protection as Metallica,
   and neither may have a key that can be authenticated
   through a "shortcut" that won't work for the other.  That
   means the root certs have to be public and there cannot be
   a list of root certs that get built into machines or installed
   by default in software unless  at least one of them represents
   an agency that gives out certs for free.
3) A DRM system must not freeze cooperatively-developed systems
   like Linux out of the player market just because there's no
   specific software supplier who has a budget for certs.  That
   means _EQUAL_ footing with commercial operating systems, not
   just a theoretical possibility that will never see use in
   practice.
4) A DRM system must not prevent me from accessing files which I
   have a legal right to access, even on a machine that is never
   hooked up to any network  This is not theoretical, by the way;
   I work with several natural-language databases on my development
   box, which for security reasons is never hooked up to any
   network.  If I were required to hook it up to a network in
   order to use those databases, I would not be able to meet
   security demands due to lack of budget for proving an
   operating system secure.
5) A DRM system must allow legal fair use, such as printing a
   few pages of a copyrighted document, quoting, time-shifting
   of broadcast content, and editing one's personal copy or
   producing derivative works if done without redistributing
   the edited or derivative version.
6) A DRM system must not introduce some "unknowable" part of
   my computer or operating system wich could be used to house
   code which cannot be examined.  This is particularly true if
   that code may or must run with sufficient priveleges to
   destroy information, facilitate compromise of the system, or
   transmit private information (such as what programs I'm
   running and when I'm on my machine) to other parties.

@_date: 2003-04-22 14:36:56
@_author: bear 
@_subject: DRM technology and policy 
Here we run into an interesting thing.  Material which is coyrighted
must enter the public domain when copyright expires.  Material which is
private must not.  Right now DRM systems are not distinguishing these
cases in any meaningful way.

@_date: 2003-04-23 10:47:08
@_author: bear 
@_subject: DRM technology and policy 
That's what copyright protection is supposed to
buy for the public.  If stuff never entered the
public domain, there would be exactly zero motive
for us to provide copyright protection for it at
all.  We extend copyright protection so that the
Tolstoys and the Shakespeares of the world will
bequeath to us their literature instead of locking
it in a damn box somewhere and showing it only to
personal friends.  That's the deal.  The artist
gets paid, and the public eventually gets the art.
Copyright wasn't necessary in this case.  What
you don't disclose doesn't become public, and
doesn't need to eventually become public property.
In fact, I'd consider this a "privacy" protection
rather than a "copyright" protection, because
you're not seeking payment for those notes (ie,
you are not selling that work to the public via
the copyright transaction). But in the same way
laws against sexual harassment are often couched
in terms of sexual discrimination (and therefore
don't apply to bisexual harassers?), the laws
protecting privacy are couched in terms of
It's not clear to me that copyright is necessary
any more.  Amateurs seem perfectly willing to
expand the public domain, and have even taken to
using copyright laws to prevent their work from
being appropriated by the intellectual-property
industry.  There are some very good writers who
put their work on the web for free, and I respect
the hell out of those guys.  ObCrypto, I have to
cite the Handbook of Applied Cryptography as one
example of available content, but there are a lot
of producers who go even further and put their stuff
all the way into the public domain. But the point
is, the public domain is being expanded - with
really good stuff in a lot of cases - without the
need for the people to extend copyright protections
or pay for the expense of enforcing them, so what
exactly justifies the continued existence of
copyright is no longer clear.
I reiterate; copyright protections were justified
solely because they were necessary to expand
the public domain with good stuff at one time.
If they're no longer necessary for that purpose,
then they're no longer justified.  If there are
a lot of really good artists who expand the
public domain for free, then what the hell does
it buy us to pay good money to facilitate the
greed of others who seem to want to lock their
stuff away from the public domain forever?
  Clearly, you weren't on the web
before 1995, when it truly was an all-amateur
effort.  The suckage of the web has increased
exponentially the more people are trying to
make money off of it.  Think of the web without
ads, popups, spam, and "pros" who want to use
horrible colors or make everything *&%^ *wiggle*
when you're trying to read it.  Amateurs produced
a lot of pages that sucked, but *really* horrible
design, and serious disrespect of the people
accessing the content so far has been nearly
the sole province of the so-called professionals.
It used to be that people were more or less equal,
here; colleagues accessed each other's sites for
information, and people did entertaining things
to provide fun for each other because they liked
each other.  There's no way commercial motivations
can compete with the atmosphere that we had then.
Copyright has been actively bad for the web.
I think and hope it *will* turn out okay,
but then I suspect that I may be what you
call a barbarian.

@_date: 2003-04-24 09:30:15
@_author: bear 
@_subject: DRM technology and policy 
And would you feel morally entitled to be paid if your audience
noticed that amateurs better at it than you were doing it for free?
Hint: It's hard to compete with free.
This is the dilemma that software houses are facing now.  And if you
go check out some big internet music sites, you'll see that musicians
are beginning to get the same treatment that free software is giving
I'm sorry, but given the rise of really good amateurs voluntarily
working for free, and competing with the commercial art and
distribution industry, I believe that the revenue stream of that
industry is doomed for simple business reasons of competition. When
a competitor is offering a better product than you for free, it's
time to close the books on the business and decide whether you want
to pursue it purely for the sake of art.
I just don't see a need to expend effort to protect the payment stream
of an industry that's doomed anyway.

@_date: 2003-04-24 15:26:24
@_author: bear 
@_subject: DRM technology and policy 
All information is software.  Some software is programs and
some is music and some is literature.  It makes no difference.

@_date: 2003-04-24 16:30:54
@_author: bear 
@_subject: DRM technology and policy 
Yeah.  Microsoft just *knew* its products were better than
Linux too.
Absolutely.  And the access to people who *are* providing their
work free is one of the things I really love about the internet.
The monopoly of paid artists, formerly enforced through the
distribution channels, is finally something we can get around,
and hallelujah.
And of course, if you want to be paid, you are absolutely free to do
your damnedest to keep your stuff out of the free channels.  I'll
support your choices, but I'm not going to knock myself out to
create paid channels when my needs for art are met spectacularly
well through the free ones.
The web has been good for art.  It's provided a route around
censorship for strips like "doonesbury" when major city papers refused
to carry something controversial.  It's allowed comic book artists a
way around the Diamond Media stranglehold on their business.  It's put
artists like Nellie and the Drummers in touch with an audience when
what they had would never appeal to a broad enough market in any one
city to merit radio play.  It's put microtonal music that I could
never have gotten from major labels back in my stereo.  It's freed
comic strip artists, once confined to single-strip gags by their
syndicates, to get seriously creative.  They can publish according to
their own schedules and they can make plotlines as complicated as they
damn well want, and they can even deal with situations that syndicates
don't want to touch, like  gay characters or rape survivors.
The sheer diversity of what's available now eclipses by a factor of
ten, or fifty, everything that commercial art has produced in the last
fifty years, and a lot of it is better quality.
That's mostly because the distribution channels can't be monopolized
on the web, they're available even to people who aren't making much
money, and I LIKE it this way!
The artists who are doing well on the internet aren't businesses who
"sell" their art and withhold it from people who don't pay.  They're
amateurs who put it out there for everybody and put a paypal button on
their site for people who like it enough to support them with tips.
They're comic strip artists who are selling mugs, buttons, tee-shirts,
and plush dolls of characters that were brought to life and made
desirable solely by their art.  They're bands who get club dates on
the strength of tracks that club owners have freely downloaded and
listened to.  They don't get paid in the multimillions of dollars, but
they're making enough to continue to do their art, and that's the
payment they care about.
Now, if you can come up with a channel where you can withhold your
stuff from people who don't pay you big money, that's fine with me.
But you're going to have to compete with these guys getting their
stuff out for free, or for really cheap, and frankly I don't think
the channel you want is going to make enough money for me to care
about building it.
So next time I listen to my "Live at the Slaughterhouse" album from
Nellie and the Drummers, I'll think of you.  Next time I buy my
girlfriend a Kiki the ferret plushie off of Fluggy Freelance, I'll
think of you.  Next time I tip R.K. Milholland a buck for another good
year of "Something Positive", I'll think of you.
It's a shame if you don't want to make your art in this new world, or
maybe it's a shame if you couldn't sucker anybody into making your
money-losing distribution channel that can't compete with the web for
you, but artists come and go, and art lives on. That's how it's always
Have a nice day.

@_date: 2003-04-25 16:17:53
@_author: bear 
@_subject: DRM technology and policy 
I wanted to apologize for ranting on the crypto list.  I'm
sorry.  I'm letting personal feelings, and passions about art,
get away from me here, and I guess I'd like to explain myself.
I have fundamental issues with a business model where a hundred or a
thousand promoters and distributors make a living off of every artist
who's on the short list of artists they consider it profitable to
I believe that it means heavily promoted artists, in order to make a
living at all, have to occupy market share that could otherwise
support hundreds if not thousands of indie artists.  And I'd like to
see those indie artists all have the money instead of the hundreds or
thousands of promoting flacks working with the work of just one
artist, because I like hundreds or thousands of different kinds of
What we've got in the web, with piracy, is finally a place where the
heavy promotion expenses work against this model of business.
Unpromoted indie artists are losing nothing to piracy, because
nobody's ever heard of them except their fans anyway, and there's not
enough margin or volume to make them worth a pirate's risk.  But
artists with heavy-rotation airtime and marketing blitzes are losing
margin to pirates, because the pirates move in to take advantage of
the inflated value and inflated sales volumes established by the
marketing blitz.
That doesn't make pirates good, and I'm not going to defend piracy;
But I've really liked that big commercial artists keeping their music
the hell off the web has created a surge in indie art that's
unparalleled since the invention of radio.
So, when I see someone looking for a way to distribute even heavily
commercialized music without losses to piracy, I get a bit irrational
because I see the hook coming.  Before it's all over, someone's going
to put some kind of hook in it that will try to freeze out competition
from indie artists again and sweep these thousands of good bands back
under the rug.  And I really don't want that.
Anyway; I'll try to behave a little better in the future, and not just
go off on somebody when this kind of question comes up.
(would love it if music stores were more like bookstores, and you
 could go into any music store and find albums from fifty thousand
 artists you'd never heard of before....)

@_date: 2003-04-30 13:31:21
@_author: bear 
@_subject: eWeek: Cryptography Guru Paul Kocher Speaks Out 
Mmm, no.  In principle, this is not a bad idea.  He needs to use enough
bits to make it resistant to the birthday paradox, and he needs to
fix it so *every frame* will have subtle differences based on its key.
But this isn't dismissable the way most of these systems have been;
it doesn't give the attackers an oracle to tell when they've been
successful at erasing their tracks.
The issue is that it takes work, and that the work can't easily be
automated, and that it takes a reasonably substantial investment.  If
you "hack" fifty or sixty players for fifty or sixty keys, you get
fifty or sixty different versions of the work, which you can combine
in some way to eliminate most, or maybe all, of the watermarks.  But
its going to cost you substantial money to acquire those players, so
you're not going to do it for no profit and you're not going to do it
casually.  And if someone is paying you money, there's a money trail
to follow back to you.
On the other hand, any one of those fifty or sixty different versions
of the work can serve for fair use or archival, watermarks and all, so
in principle we have here the first example of a DRM scheme that
doesn't necessarily, at least in principle, deprive the public domain
from eventually inheriting the protected work, nor prevent people from
exercising fair use.
I don't think it's quite technically what he's claiming, but I do think
it's less actively harmful than previous DRM proposals.

@_date: 2003-08-22 15:55:17
@_author: bear 
@_subject: Abit's SecureIDE 
Interesting.  Can these chips be usefully cascaded?

@_date: 2003-08-22 16:22:50
@_author: bear 
@_subject: PRNG design document? 
The accepted (?) terminology among the relatively
few writers who are distinguishing between all three
classes of RNG's is that
- "True random number generators" require entropy
density of 100%,
- "Semirandom number generators require entropy
densities between 99% and 1%,
- "Pseudorandom number generators" do not consume
entropy at all and eventually (disregarding such
intervening events as the heat death of the universe)
In practice however, many writers aren't distinguishing
between semirandom and pseudorandom generators at this
time, despite the fact that their properties are drastically
different (semirandom generators for example are unsuitable
for applications such as stream ciphers). When they're not
distinguished, folk tend to use "pseudorandom" as nomenclature
for both.

@_date: 2003-08-26 14:26:23
@_author: bear 
@_subject: FYI: The size of a bit of entropy 
The event horizon of a black hole is a very special case of
"physical world" -- interesting article though; I read it in
the paper edition.

@_date: 2003-08-26 14:49:18
@_author: bear 
@_subject: blackmail / real world stego use 
It is interesting to speculate about whether the FBI served
surfola.com with a warrant.  If the anonymizing service is
"transparent" after the fact to the details recorded during
the FBI's ordinary daily monitoring of the internet, then we
live in interesting times indeed.
That would imply packet recording and correlation on a level
greater than we've ever considered to be in the arsenal of
cryptographic threats, implying the emergence of forces (and
inevitably of forces other than governments) that have
eavesdropping capabilities that cannot be defeated except with
time-delayed packet relay through many hosts and re-encryption/
redecryption at each step of the way.
That is a model that does not permit realtime communication,
meaning that monitoring may be impossible to escape for
realtime activities such as web browsing.

@_date: 2003-08-27 09:52:00
@_author: bear 
@_subject: blackmail / real world stego use 
The problem being here access to the website's logs. Getting the logs
via a warrant and due process, which seems like a minimal exercise for
a privacy server, is hard to do inside 24 hours.  It's much easier to
believe that the FBI is keeping its own logs at hubs, routers, and
switches connected to surfola, thereby eliminating the need for
warrant service.

@_date: 2003-12-07 12:01:25
@_author: bear 
@_subject: yahoo to use public key technology for anti-spam 
This is generally how I work it.  I sit down at any hotspot and I
get network connectivity.  But all the hotspot is ever going to see
of my browsing, email, and anything else I like to keep private is
SSH packets to my home machine, or encrypted X packets running
between the X server on my laptop and X clients on my home machine.
A bit of lag is acceptable. Sending private mail via untrusted
SMTP servers is not.

@_date: 2003-12-15 15:25:23
@_author: bear 
@_subject: example: secure computing kernel needed 
It is because this lump which we have no control over (aside from
the trivial degree of control implied by simply refusing to use it
at all) is proposed for presence inside machines which we use for
doing things important to us.
Most of us have a relatively few applications for such a device,
and we want to keep those applications completely separate from our
other use of our computers.  A dongle is more acceptable than the
TCPA hardware because it can be detached from the computer leaving
a usable machine, and because in order to reach a broad market you
cannot write software assuming its existence.
I would not object to a tamper-resistant stainless-steel
hardware token that I needed to carry with me in order to access
financial transactions (or whatever).  That's a hardware token
with a single application, which is not at all mixed up with or
involved with the fundamental hardware or software that I depend
on for all my other applications.
But I do object, in strongest possible terms, to the proposal to
weld some device into my personal computer, give it the highest
privelege mode, allow it to read or write arbitrary data on the
bus or the network interface, forbid me from looking inside it
or altering its contents, and allow it to communicate on my behalf
to unknown hosts over the internet.
I like to think that I am the person who owns my machine and that
ownership carries with it the privelege of deciding what to run
or not run on it.  TCPA assigns to others the privelege of blocking
basic, ordinary functionality if they don't know or like some
program I'm running.  But what programs I'm running on my machine
in my home is not their business unless they are trying to literally
take control of my machine away from me.
If they've got stuff that needs to be done in a secure environment
and they don't trust me to run a machine to do it on, let them run
it on their own machines rather than taking mine over by proxy.
Fair's fair; *I* own this one; *They* own that one.  What either
of us doesn't trust the other with, we must run ourselves.
I believe that if TCPA or something like it is adopted, vendors
will respond by ceasing to make any applications that are at all
useful on machines where it is not present, enabled, and loaded
with some specified default configuration that basically gives
them all ownership rights to my machines.  In a world where basic
functionality depends on such applications, no one has any choice
any more about whether to enable it or what to run on it.
I do not believe that the long-term goals of the TCPA partners are
consistent with the continued feasability of operating machines
that don't rely on TCPA.
Indeed.  I cannot comprehend that you have such a complete grasp of
the problem but don't find that a very compelling argument *against*
the TCPA mechanism.
Remember that the world suffered through seven centuries of imprimatures
before freedom of the press was recognized as fundamental to liberty. I
think that freedom and self-determination in computing applications is
equally important and that the TCPA is a step toward a technology that
would enable the same kind of struggle over that freedom.
A secure kernel is a kernel that the *owner* of the machine can trust.

@_date: 2003-12-18 10:48:32
@_author: bear 
@_subject: Difference between TCPA-Hardware and other forms of trust 
I think that if the music company wants that much control
(which is, btw, in clear violation of the First Sale Doctrine),
then the only legal way for them to achieve it is to provide
a player specifically for the music which they own, in exactly
the same way that banks retain ownership of the credit cards
and smartcards we use.  As long as the player is not their
property, they can't do this.
The major problem I want a trusted kernel for is because I
don't want to trust binaries provided by closed-source software
houses.  I want my trusted kernel to tell me exactly what
priveleges they're asking for and I want to tell it exactly
what priveleges it's allowed to provide them.  I want it to
be able to tell me exactly when every executable file appeared,
and as a result of running which other executable file (all
the way back to whichever command *I* gave that resulted in
its being there).  I want it to tell me exactly how the daemon
listening on any tcp port got installed and what priveleges
it has.  I want my trusted kernel to keep tamper-proof logs;
in fact I'd go so far as to want to use a write-once media
for logfiles just to make absolutely sure.
A trusted kernel should absolutely know when any program
is reading screen memory it didn't write, or keyboard
keystrokes that it then passes as input to another program,
and it should be possible for me to set up instant notification
for it to alert me when any program does so.
A trusted kernel should monitor outgoing network packets and
sound an alarm when any of them contains personal information
like PINs, passwords, keys, Social Security Number, Drivers
License, Credit Card numbers, Address, etc.  It should even
be possible to have a "terminate-with-prejudice" policy that
drops any such packets before sending and terminates and
uninstalls any unauthorized application that attempts to send
such packets.
I really don't care if anyone *else* trusts my system; as
far as I'm concerned, their secrets should not be on my
system in the first place, any more than my secrets should
be on theirs.  The fact is I'm building a system out of
pieces and parts from hundreds of sources and I don't know
all the sources; with an appropriate trusted kernel I
wouldn't have to extend nearly as much "black box" trust
to all the different places software comes from.
I do not think so.  People want to retain ownership of their
computer systems and personal information, and a system that
is made for *others* to trust would be used to take that
ownership and information.
No.  Lots of bands release music and encourage sharing, as promo
for their main revenue source (concert tours).  I see those bands
getting a leg up as their released music becomes popular while
music only available with onerous conditions languishes.  Lots of
other artists do graphic or animation work just for the chance to
be seen, and some of them are quite good.
You may consider it "living in the woods" to listen to stuff that
isn't the top 20; but I think lots of people will find that the
"woods" is a friendlier and more trustworthy place than a world
full of weasels who want to control their systems.

@_date: 2003-12-18 10:48:32
@_author: bear 
@_subject: Difference between TCPA-Hardware and other forms of trust 
I think that if the music company wants that much control
(which is, btw, in clear violation of the First Sale Doctrine),
then the only legal way for them to achieve it is to provide
a player specifically for the music which they own, in exactly
the same way that banks retain ownership of the credit cards
and smartcards we use.  As long as the player is not their
property, they can't do this.
The major problem I want a trusted kernel for is because I
don't want to trust binaries provided by closed-source software
houses.  I want my trusted kernel to tell me exactly what
priveleges they're asking for and I want to tell it exactly
what priveleges it's allowed to provide them.  I want it to
be able to tell me exactly when every executable file appeared,
and as a result of running which other executable file (all
the way back to whichever command *I* gave that resulted in
its being there).  I want it to tell me exactly how the daemon
listening on any tcp port got installed and what priveleges
it has.  I want my trusted kernel to keep tamper-proof logs;
in fact I'd go so far as to want to use a write-once media
for logfiles just to make absolutely sure.
A trusted kernel should absolutely know when any program
is reading screen memory it didn't write, or keyboard
keystrokes that it then passes as input to another program,
and it should be possible for me to set up instant notification
for it to alert me when any program does so.
A trusted kernel should monitor outgoing network packets and
sound an alarm when any of them contains personal information
like PINs, passwords, keys, Social Security Number, Drivers
License, Credit Card numbers, Address, etc.  It should even
be possible to have a "terminate-with-prejudice" policy that
drops any such packets before sending and terminates and
uninstalls any unauthorized application that attempts to send
such packets.
I really don't care if anyone *else* trusts my system; as
far as I'm concerned, their secrets should not be on my
system in the first place, any more than my secrets should
be on theirs.  The fact is I'm building a system out of
pieces and parts from hundreds of sources and I don't know
all the sources; with an appropriate trusted kernel I
wouldn't have to extend nearly as much "black box" trust
to all the different places software comes from.
I do not think so.  People want to retain ownership of their
computer systems and personal information, and a system that
is made for *others* to trust would be used to take that
ownership and information.
No.  Lots of bands release music and encourage sharing, as promo
for their main revenue source (concert tours).  I see those bands
getting a leg up as their released music becomes popular while
music only available with onerous conditions languishes.  Lots of
other artists do graphic or animation work just for the chance to
be seen, and some of them are quite good.
You may consider it "living in the woods" to listen to stuff that
isn't the top 20; but I think lots of people will find that the
"woods" is a friendlier and more trustworthy place than a world
full of weasels who want to control their systems.

@_date: 2003-12-22 16:26:39
@_author: bear 
@_subject: Difference between TCPA-Hardware and other forms of trust 
This underscores an important point.  In security
applications limitations are often a feature rather
than a bug.  We are accustomed to making things better
by making them able to do more; but in some spaces
it's actually better to use a solution that can do
very little.
Much of the current security/cryptography angst can
be summed up as "small, limited, simple systems work,
but big, complex, general systems are very hard to
get right or have unintended drawbacks."  Often the
very generality of such systems is a barrier to their
wide adoption.
I would say that if you want to make any money in
cryptography and security (and make it honestly) you
should pick one business application, with one threat
model and one business model, and nail it.  Add no
features, nor even include any room in your design,
that don't directly address *that* problem.  When
you are able to present people with a solution to
one problem, which has no requirement of further
involvement than solving that one problem and introduces
no risks or interactions other than those flatly necessary
to solve that one problem, then they'll pay for it.
But when we start talking about multi-function cards,
it becomes a tradeoff where I can't get anything I want
without getting things I don't want or risking network
effects that will lead to markets dominated by business
models I don't want to deal with.  It makes the buy
decision complicated and fraught with risk.

@_date: 2003-12-29 13:24:29
@_author: bear 
@_subject: Difference between TCPA-Hardware and a smart card (was: example: 
It's not the parties who are interested in security alone we're worried
about.  There is an advantage in profiling and market research, so I
expect anyone able to effectively subvert the protocols to attempt
to hide the content of remote attestataion.

@_date: 2003-02-03 19:13:16
@_author: bear 
@_subject: question about rsa encryption 
RSA is subject to blinding attacks and several other failure modes if
used without padding.  For details on what that means, read the
cyclopedia cryptologia article on RSA.

@_date: 2003-02-09 09:48:34
@_author: bear 
@_subject: Columbia crypto box  
Actually, I don't think it does.  It's been my experience that the
decision-makers never even *KNOW* whether their systems are secure.
They've been sold snake-oil claims of security so many times, and,
inevitably, seen those systems compromised, that even when responsible
and knowledgeable engineers say a system is secure, they have to
regard it as just another claim of the same type that's been proven
false before.
So I can easily imagine them just not knowing whether the link was
secure, thinking that the NASA engineer's job of "securing" uplinks
might be no better than Microsoft's job of "securing" communications
or operating systems, because they've had it demonstrated time and
again that even when they hear words like "secure", the system can be
The fact is that the NASA engineer has a huge advantage; s/he's not
working for a marketing department that will toss security for
convenience, s/he's not working on something whose code has to be
copied a million times and distributed to people with debuggers all
over the world, s/he's not trying to "hide" information from people on
their own computer systems, and s/he's not complying with deals made
with various people that require backdoors and "transparency to law
enforcement" in every box.
So the NASA engineer's actually got a chance of making something
secure, where the Microsoft engineer didn't.  Microsoft has to claim
their junk is secure, but in their case it's just marketing gas.  But
all this is below the notice of the decision makers; they *LIVE* in a
world where marketing gas is indistinguishable from reality, because
they don't have the engineer's knowledge of the issues.
So having the decision makers get real nervous was likely to happen,
whether the link is secure or not.  There's no information there
except that the decision makers have finally realized they don't
really *know* whether the link is secure.  That's progress, of a sort.
Battlefield systems have been that way forever.  Battlefield
information only has to remain secure for a few seconds to a few
hours, and they exploit that to the max in making the systems flexible
and fast enough for actual use.  You want appalling?  In the civil
war, they used monoalphabetic substitution as a "trench code" -- on
both sides.

@_date: 2003-02-10 11:47:31
@_author: bear 
@_subject: Columbia crypto box 
Actually, it is re-using a pad, exactly.  It's just a pseudorandom
pad (stream cipher) instead of a one-time pad.
And while WEP had problems, it didn't have that particular problem.
New messages with the "same" key would use a later chunk of the
cipherstream pad under WEP.

@_date: 2003-01-07 21:09:22
@_author: bear 
@_subject: DeCSS, crypto, law, and economics 
This makes an interesting point.  While the argument that market
segmenting may increase the ability to provide material in all
markets, the fact is that given region coding, the producers of
this stuff *DON'T* provide the material in all markets.
If their argument, that the increased market size available with
region coding enables economies of scale, were actually the driving
force behind region coding, there should be no such thing as content
available in one region that is unavailable in another.
Thus their actions betray that they have a different motive. Therefore
the public skepticism regarding the truth of their assertions about
their motivations seems fairly solidly grounded on fact.
( who likes a fair amount of stuff that is only available
  coded for region 6 ).

@_date: 2003-01-08 08:58:01
@_author: bear 
@_subject: DeCSS, crypto, law, and economics 
Ah, but you're forgetting the whole "globalization" issue.
Governments aren't answering to their own people any more; they're all
striving to become a part of the "new world order" where a norwegian
can be brought to court for a supposed violation of american copyright
laws or where the Russian Dmitri Sklyarov can be jailed in the USA for
DOING HIS JOB IN RUSSIA.  We're moving forward into a glorious new
world where governments can impose laws upon their own people, not by
the fickle and divisive will of those governed, but rather in response
to international treaties and agreements with other nations promoting
global unity and harmony.
Cryptography is a part of that wonderful vision...  if the people of
different nations can be prevented from communicating effectively with
one another, or exercising their freedoms in ways that affect one
another, then effective opposition to global unity may be reduced, and
we can all become better servants and markets to our corporate
All power to the dromedariat!
PS.  If you happen to be mentally defective, you may not recognize

@_date: 2003-01-20 18:28:30
@_author: bear 
@_subject: Key Pair Agreement? 
Key Pair Agreement already means something though, I thought.  In
Key Pair Agreement, Alice and Bob want to interact so that each
generates one-half of a key pair for an asymmetric encryption system.
The requirements are:
Alice does not know and cannot compute Kbob (bob's key).
Bob does not know and cannot compute Kalice (alice's key).
Each has enough information to assure that the keypair is novel.
Each has enough information to assure that the keypair is not
   contain a "weak key" if the encryption algorithm has weak
   keys.
Encrypt(Encrypt(P, Kbob), Kalice) = P
Encrypt(Encrypt(P, Kalice), Kbob) = P

@_date: 2003-01-25 01:39:53
@_author: bear 
@_subject: [IP] Master Key Copying Revealed (Matt Blaze of ATT Labs) 
There are several types of devices that can convince a keylock
to open.  One of them is a kind of spring-loaded bar, usually
on a handle.  The bar is inserted into the keyhole, and then the
spring is released and a weight whacks the bar fairly hard.
This transmits the shock to the pins resting on the bar, and
thence to the other side of the pins resting across the cut
from the shocked side.
The result is that the pins fly apart momentarily against the
retaining springs.  If your timing is good, you can turn the lock
immediately after the 'snap' of the spring slamming shut.  It
usually takes an experienced user no more than three or four
tries to get the timing right.
This is actually a very simple device to construct.  I ran
across it in a book on locks and mechanisms.  Some folks call
it an automatic lock picker, but it's really just a snap
mechanism.  I've never actually seen one in person, but I
can give you the name and publication date of the pamphlet I
saw it in if I can find it around here.

@_date: 2003-01-27 09:44:17
@_author: bear 
@_subject: [IP] Master Key Copying Revealed (Matt Blaze of ATT Labs) 
I'm actually not sure of that.  I think that an organized
case-by-case study of "social engineering" breaches would
be valuable reading material for security consultants, HR
staff, employers, designers, and psychologists.  It's not
actually the study of cryptography, but it's a topic near
and dear to the heart of those who need security, just as
Matt's paper on locks.

@_date: 2003-01-27 09:57:43
@_author: bear 
@_subject: EU Privacy Authorities Seek Changes in Microsoft 'Passport' 
The widespread acceptance of something as obviously a bad idea as
passport really bothers me.  I could see a "password manager" program
to automate the process of password invalidation where you discovered
a compromise; but the idea of putting everything you do online on the
same password or credential is just...  stupid beyond belief.
Why are single-sign-on systems even legal to sell without warnings?
Why don't Msoft and the other members of the "Liberty alliance" have
to put a big warning label on them that says "USE OF THIS PRODUCT WILL
DEGRADE YOUR SECURITY"?  Because that's what we're looking at here;
drastically reduced security for very marginally enhanced convenience.
But what really gets me about this is that it's totally obvious that
that's what we're looking at, and people are buying this system
anyway.  That's hard to swallow, because even consumers ought not to
be that stupid.  But it's even worse than that, because people who
ought to know better (and people who *DO* know better, their own
ethics and customers' best interests be damned) are even *DEVELOPING*
for this system.  It just doesn't make any damn sense.

@_date: 2003-07-01 09:48:37
@_author: bear 
@_subject: Attacking networks using DHCP, DNS - probably kills DNSSEC 
I tend to agree...  I don't think "zero-configuration" networking has
a real possibility to create any safety zones beyond the immediate
physical machine.  After all, if you can plug it into any network and
it just works, you can plug it into an insecure or subverted network
and it'll just work.
At the very least you've got to have a file of keys.

@_date: 2003-07-16 10:52:12
@_author: bear 
@_subject: Announcing httpsy://, a YURL scheme 
But any single search engine is itself a single reference, regardless
of how many times and contexts it uses to print the reference on the
IOW, if you go to Mallory's search engine, then no matter how many
references you find, they're all coming to you through the same
channel and you have to trust Mallory.

@_date: 2003-07-16 11:10:57
@_author: bear 
@_subject: Looking for an N -out-of-M split algorithm 
Perry has it exactly right, although he was pretty brief and gave no
Let's say you want to be able to reconstruct a message given any
two out of three splits of the message.  What you do is plot the
message as the y-intercept on a cartesian graph, and draw a line
in some random direction.  (where random != straight up)
Now, the line you've drawn crosses the x=0 vertical line at the
message, and it crosses the x=5000 line at some other point whose y
coordinate is A, and it crosses the x=10000 line at some other point
whose Y coordinate is B, and it crosses the x=15000 line at yet some
other point, whose Y coordinate is C.
A, B, and C are your three secret splits.  Unless someone has at least
two of them, they have no information about the slope of the line and
they can't project it back to the (x=0, y=M) point to get the message.
If you want two out of four, or two out of six, or whatever else, it's
the same thing; two points establish a line, so you can just pick more
points along the line.
If you want a 3-out-of-N secret split, you need to use a curve that
requires three points to establish (such curves include functions
of X^2). And so on...

@_date: 2003-06-02 10:51:58
@_author: bear 
@_subject: Maybe It's Snake Oil All the Way Down 
Well....  I do understand how it can look that way.  And getting away
from that problem is really hard.  The problem is, if you really want
to cut past recieved wisdom, you have to have your own wisdom to judge
it by.  That means you have to get an idea of what the threats are out
there.  And that means not only understanding hundreds of different
algebraic attacks and mathematical patterns that have been brought to
bear on various ciphers, but also understanding the underlying
mathematics that give rise to these attacks well enough to see if
you're just inviting a variation on something well-known that you
think you're defending against.  Crypto is a very context-intensive
business, and a "working knowledge" is actually more than can really
be expected of anyone except specialists.
I am not a crypto specialist.  I have studied protocol design, mostly
on my own, for about two years, and I still miss stuff but I'm still
getting better.  I have also studied cipher design, and mostly come to
the conclusion that it is wizardry beyond my ken to design a cipher as
secure as existing ciphers which can be used with as small an
investment in CPU power.  I like to think I came to it reasonably well
prepared; My professional background is in Artificial Intelligence,
and I've had a *LOT* of discrete mathematics and statistics in order
to get there.  But I still have to draw the line at cipher design; it
is for people who eat, sleep, and breathe crypto and crypto attacks,
and I just cannot do it.  I could build a secure cipher, but it would
look something like GOST; a long-keyed cipher based on a ridiculously
high number of rounds of a feistel network.  It would be slower than
3DES and nobody sane would use it.  *I* wouldn't use it if 3DES were
So....  the people who can function at the level of cipher designers
are rare, and mostly they've devoted their lives to it.  The rest of
us pretty much have to accept what they say as recieved wisdom.  I've
learned enough that I can tell what they're saying, or maybe even see
how it would work, but usually it's stuff I wouldn't have thought of
trying in a hundred years, or whose existence as a risk is well-known
to them but unknown until that moment to me.

@_date: 2003-06-03 08:53:17
@_author: bear 
@_subject: New vs Old (was Snake Oil) 
Well - Hans Dobbertin found hash collisions in MD5 and while I haven't
heard much more, that's a toehold that somebody might be able to use
to break it, and makes it vulnerable in some applications.  SHA-1 is
now considered better.
IDEA is still a good cipher as far as I know, but PGP has been driven
away from it in the US due to intellectual-property issues.  Rather than
continue with incompatible versions for use inside/outside the USA, they're
switching to CAST (although this is causing more, rather than less, version
RSA is still good, as far as I know, and has been in the public domain
worldwide since September 2001.  But it had the same kind of IP issues
as IDEA until that point, and several versions of PGP had to be
produced that used a different asymmetric cipher for that reason.  I
don't know enough about DH/DSS specifically to comment further on its
relative security, but RSA has had several scares and people are
concerned that custom hardware (such as a million-qubit quantum
computing device or Bernstein's matrix hardware factoring device)
might cause insecurity in RSA _and_ be possible for someone to keep
secret.  And lots of people quit using RSA because they don't like
the "big block of key" that it requires.
for a good complete description of SHA-1 and a few others, try
(warning: link may be outdated).
I don't have pointers to the other two offhand.

@_date: 2003-06-04 16:53:29
@_author: bear 
@_subject: Maybe It's Snake Oil All the Way Down 
Too right.  The problem is that your priest, sister, shrink, lawyer,
etc, aren't technical people. They may be concerned about privacy, but
as long as they don't understand how and why this stuff works - and
as long as there is some level of functionality they can get without
doing it - they aren't going to understand what they need to do, or
necessarily even know if they're doing it wrong or know what the risks
They already remember a shared value to talk to you - your phone number.
They might be annoyed if the phone number were fifteen digits longer
(extended by a password), but they'd at least "get it" if they had to
enter the extra fifteen digits to talk to you. They wouldn't, however,
manage it like a password - it would be all over their autodial systems
and jotted down on postit notes etc.
If you wanted your end of the conversation encrypted calling from your
cell you could call a service that takes encrypted cell phone calls
and "forwards" them on a fiber trunk unencrypted for the benefit of
your sister who won't get a better phone...  but if she takes the call
on her cell, or on her wireless handset, it's going to be unencrypted
on the air again at her end.
There doesn't seem to be a good solution that's fully interoperable
with the current technology.

@_date: 2003-06-05 23:07:15
@_author: bear 
@_subject: Nullsoft's WASTE communication system 
AES is public infrastructure.  It's available for everybody, worldwide,
without copyright or license or patent issues; that was one of the
conditions for even being allowed to enter the AES competition.

@_date: 2003-06-11 10:27:25
@_author: bear 
@_subject: Keyservers and Spam 
That is the theory.  In practice, as long as the PGP "web of trust"
depends on connections made through signers not personally known to
the person depending on the security, it hardly works.  There is
very little verification done in the web of trust, not even for
consistency.  There's no way for it to propagate negative information,
(such as Bob's mention of having observed Alice verifying keys to
people not known to her) nor, where nyms are easy to come by, any
way for negative information to attach to a given person.
In order for the web of trust to work, it would have to be better
for your trust profile to be a known spammer and fraudster than to
be an unknown person.  Because as long as known spammers and fraudsters
can become unknown people just by grabbing another nym, there's no
I don't particularly like the commercial certs, but the thousand
bucks or so ought to serve as a "bond", in that if people untrust
the keys, there is real value that will be lost.  That makes it
require some expenditure of resources to grab a new nym.  However,
even when provoked - even when root certs have been **SOLD** -
people still don't untrust them, because the news of the compromise
doesn't propagate around triggering revokes on individual systems.
I consider it to have already disintegrated, long ago.  Trust extended
to unkown people is a bogus concept.
It's worthwhile, in some sense, to attach a key to a nym.  It doesn't mean
the key is bogus, it just means it's a nym instead of a name.  When I
correspond with entities known to my mailbox as "Madame Ovary" and
"Guadalupe de Loop"  it's not because I believe that those are their actual
names.  However, there is a certain level of trust, because I've been
corresponding with these entities for over ten years.  They are no longer
"unknown people" to me, regardless of the fact that I couldn't link either
of them to a particular email address, a legal name, or a photo.  Would I
sign Madame O's key, attesting that he/she/they/it are/is a persistent
pseudonymous entity known to me more than ten years and never observed to
be part of a scam or fraud?  Yes, I would. Would that be meaningful to anyone
else?  I dunno.
It's actually not too difficult.  If keys were stored by a one-way hash on
the email address, rather than by the address, there'd be no need for the
keyserver to even know the email addresses.  You'd query it by sending it
the hash of the email address, and it would respond by sending you the
associated key.
You could prevent keyservers from being used for address verification
with a "blind query" where the Keyserver sends back a key whether or
not there is a key for that address.  The "key" would be pseudorandom
bits based on the query if the address is not listed, or the actual
key if it is.  Then there'd be no way for someone to obtain or verify
an email address from a keyserver, but they could still use the email
address to get the key, if it existed, from the keyserver.  The
downside would be that you'd run the risk of sending encrypted mail to
someone with no key, but that doesn't cause too much of a problem.

@_date: 2003-06-25 16:35:18
@_author: bear 
@_subject: Draft Edition of LibTomMath book 
One thing that I've noticed for a long time is that there
are *VERY* few math libraries that don't leave whatever
numbers they're working with in memory when deallocating
(deallocating heap via free() or deallocating stack via
returning from a procedure call or deallocating swapspace
by getting paged back in off a disk).
And numbers that an application leaves lying around in
whatever working memory or media it's using, can be
discovered and exploited by other programs - frequently
by unauthorized ones.
Windowing systems have the same kind of leakage, but you
can avoid using windowing systems with a crypto program;
there's no need to put sensitive information like keys
or passwords on the screen ever.  Admittedly, I'd like
to have a secure windowing system, but it seems unlikely.
But I think Math is indispensable to crypto, and there
ought to be a secure mathematics library.

@_date: 2003-06-30 09:10:24
@_author: bear 
@_subject: Attacking networks using DHCP, DNS - probably kills DNSSEC NOT 
I think that the problem would be somewhat ameliorated if there
were a DNS cache on the laptop itself.  It would still use DNS
servers, but if it got a different IP number for the same address,
it should notify someone.
This can happen without an attack going on, if the legitimate
addressee's DNS record changes because the IP address of that
service actually changes - but with sites like Yahoo, Paypal,
etc, they've got a lot of infrastructure and momentum there.
Those IP addresses don't change on a whim. And those are the
major targets for a DNS spoof.

@_date: 2003-03-06 08:25:54
@_author: bear 
@_subject: Scientists question electronic voting 
blink, blink.
you mean *MORE* widely available than photoshop/gimp/illustrator/etc?
Let's face it, if somebody can *see* their vote, they can record it.
and if someone can record it, then systems for counterfeiting such a
record already exist and are already widely dispersed.  If the
republicans, democrats, greens, libertarians, natural law party, and
communist party all offer you a bottle of beer for a record of your
vote for them next year, there's no reason why you shouldn't go home
without a six-pack.

@_date: 2003-03-14 13:23:31
@_author: bear 
@_subject: Encryption of data in smart cards 
I've seen this too -- a little card that has its own 10key pad so
you can enter your key directly to the card, and a little "purge"
button next to the zero so you can tell it to forget the key you
entered after each use. Also a red LED to tell you that it was
"up" with a key entered, and that you needed to purge it before
sticking it back in your wallet.  The guy would enter his PIN,
stick the card in the PCMCIA slot, and the machine would unlock.
Slick little device, actually.
Now can we get one that uses more than 5 digits for a key?

@_date: 2003-03-15 00:40:31
@_author: bear 
@_subject: Diffie-Hellman 128 bit 
If they're using 128-bit primes, you don't really need to look for
breaks - just throw a cpu at it and you're done.

@_date: 2003-03-24 08:32:00
@_author: bear 
@_subject: Face-Recognition Technology Improves 
The problem is that's exactly the sort of barrier that goes
away over time.  We face the inevitable advance of Moore's
Law.  The prices on those cameras are coming down, and the
prices of the media to store higher-res images (which plays
a major part in how much camera people decide is worth the
money) is coming down even more rapidly.  Face recognition
was something that was beyond our computing abilities for a
long time, but the systems are here now and we have to
decide how to deal with them - not on the basis of what they
are capable of this month, but on the basis of what kind of
society they enable in coming decades.
Also, face recognition is not like cryptography; you can't
make your face sixteen bits longer and stave off advances
in computer hardware for another five years.  These systems
are here now, and they're getting better.  Varied lighting,
varied perspective, moving faces, pixel counts, etc -- these
are all things that make the problem harder, but none of them
is going to put it out of reach for more than six months or
a year.  Five years from now those will be no barrier at
all, and the systems they have five years from now will be
deployed according to the decisions we make about such systems

@_date: 2003-03-24 08:42:28
@_author: bear 
@_subject: Face-Recognition Technology Improves 
I'm reasonably certain that a 'whole-face geometry scan' is a
reasonable thing to expect to be able to extract from six or eight
security-gate images.  If you've been through the airport four or five
times in the last year, and they know whose boarding pass was
associated with each image, then they've probably got enough images of
your face to construct it without your cooperation.
And if they don't do it today, there's no barrier in place preventing
them from doing it tomorrow.  Five years from now, I bet the cameras
and systems will be good enough to make it a one-pass operation.  I'd
be surprised if they don't then "scan" routinely as people go through
the security booths in airports, and if you've been scanned before
they make sure it matches, and if you haven't you now have a scan on
file so they can make sure it matches next time.

@_date: 2003-03-24 16:26:08
@_author: bear 
@_subject: Who's afraid of Mallory Wolf? 
There have, however, been numerous MITM attacks for stealing
or eavesdropping on email.  A semi-famous case I'm thinking
of involves a rabid baptist minister named fred phelps and
a topeka city councilwoman who had the audacity to vote against
him running roughshod over the law.  He set up routing tables
to fool DNS into thinking his machine was the shortest distance
from the courthouse where she worked to her home ISP and
eavesdropped on her mail.  Sent a message to every fax machine
in town calling her a "Jezebellian whore" after getting the
skinny on the aftermath of an affair that she was discussing
with her husband.
And as for theft of credit card numbers, the lack of MITM
attacks directly on them is just a sign that other areas of
security around them are so loose no crooks have yet had to
go to that much trouble.  Weakest link, remember?  No need
to mount a MITM attack if you're able to just bribe the data
entry clerk.  Just because most companies' security is so
poor that it's not worth the crook's time and effort doesn't
mean we should throw anyone who takes security seriously
enough that a MITM vulnerability might be the weakest link
to the wolves.
These, technically speaking, are impostures, not MITM attacks.  The
web makes it ridiculously easy.  You can use any linktext or graphic
to link to anywhere, and long cryptic URL's are sufficiently standard
practice that people don't actually look at them any more to notice a
few characters' difference.
On the occasions where people have actually spoofed DNS to route the
"correct" URL to the "wrong" server in order to get info on people's
accounts, that is a full-on MITM attack. And that definitely has
happened.  I'm surprised to hear someone claim that credit card
numbers haven't been stolen that way. I've been more concerned about
email than credit cards, so I don't know for sure, but if credit cards
haven't been stolen this way then the guys who want them are way
behind the guys who want to eavesdrop on email.
This is a simple consequence of the fact that the main market for SSL
encryption is financial transactions.  And no credit card issuer wants
fully anonymous transactions; it leaves them holding the bag if
anything goes wrong.  Anonymous transactions require a different
market, which has barely begun to make itself felt in a meaningful way
(read: by being willing to pay for it) to anyone who has pockets deep
enough to do the development.

@_date: 2003-03-24 19:32:08
@_author: bear 
@_subject: Keysigning @ CFP2003 
Right, but remember that knowing people personally was supposed
to be part of the point of vouching for their identity to others.
"I know this guy.  We spent a couple years working on X together."
is different in kind from "I met this guy once in my life, and he
had a driver license that said his name was mike."

@_date: 2003-03-25 09:07:40
@_author: bear 
@_subject: Who's afraid of Mallory Wolf? 
Wait.  Time out.  Setting aside the increased monetary
cost of her reelection campaign in a fairly conservative
state capitol, and setting aside the increased difficulty
of raising money for that campaign, the main costs here
are intangible.
On a professional level, she had reduced power in office
because of the scandal this clown created publishing her
personal email, but the intangible costs go both directions
from there.
Toward the personal end of the spectrum, discussing the
aftermath of an affair with one's husband is sensitive and
personal, and making that whole thing public can't have done
either of them, or their marriage for that matter, any good.
In the public sphere, this is a case in which information
gained from an attack on email was being employed directly
for undeserved influence on government officials.  Being timed
to interfere with her reelection makes it a direct means of
removing political opponents from office,  and it has
probably had a "chilling effect" on other council members
in that benighted city who might otherwise have voted in ways
Phred didn't like.  What he did was nothing less than a
direct assault on the democratic process of government.
I don't think mere monetary costs are even germane to
something like this.  The costs, publicly and personally,
are of a different kind than money expresses.  And we're going
to continue to have this problem for as long as we continue to
use unencrypted SMTP for mail transport.

@_date: 2003-03-25 09:25:57
@_author: bear 
@_subject: Keysigning @ CFP2003  
An interesting idea: At one extreme you could view the whole
universe as having a finite amount of trust and every
certification is a transfer of some trust from one person to
another. But then companies like verisign, after the first
thousand or so certs,  would have nothing left to sell.
At the other,  you could view verisign as providing a fairly
reliable indication, not necessarily of who X is, but certainly
of the fact that somebody was willing to spend thousands of
dollars to claim to be X and the financial records are on file
if you absolutely need to figure out who that was, so they
"create" trust in a way that most keysigners don't.
Neither model is perfect, but the latter one seems to have more
appeal to people in protecting financial transactions and the
former to people who are more concerned about personal privacy.

@_date: 2003-03-25 09:28:58
@_author: bear 
@_subject: Who's afraid of Mallory Wolf? 
Of course the consumer gets to make that choice.  I can go into my browser's
keyring and delete root certs that have been sold, ever.  And I routinely
do.  A fair number of sites don't work for me anymore, but I'm okay with

@_date: 2003-03-25 12:09:24
@_author: bear 
@_subject: Who's afraid of Mallory Wolf? 
You honestly haven't heard of Fred Phelps?
He has thirteen children and nine of them are
lawyers.  Estimated costs to sue the guy are in
the hundreds of thousands of dollars.  Estimated
costs for him to defend are near zero.  Plus the
instant you file that civil suit you'll have his
zombies loudly picketing your home (that's right,
your private residence) 24/7 until you stop.
ISP's don't want to support encrypted links
because it raises their CPU costs.  And mail
clients generally aren't intelligently designed
to handle encrypted email which the mail servers
could just "pass through without decrypting and
I think a new protocol is needed.  The fact
that SMTP is unencrypted by default makes it
impossible for an encrypted email form to be
built on top of it.

@_date: 2003-05-09 18:05:49
@_author: bear 
@_subject: A Trial Balloon to Ban Email? 
I rather liked the suggestion someone made a while ago that involves
paying the recipient when sending email to them.  If they reply, you
get your money back.  But if you spam, it would rapidly become
However, that involves financial payments again, and nobody is willing
to do financial anything in a way that allows anonymous players.  So
if we care about the ability to have anonymous email, we can simply
eliminate from consideration anything that requires a paid email
license or financial payments to be made in exchange for the right to
send mail.
There is a better way, of course. But it may not be as profitable for
the people who want to sell certs, so nobody's pushing it right now.
Remember the "hashcash" proposal from a few years ago? It basically
involved the recipient setting some computational task that would take
a couple of CPU seconds to complete and demanding the results (from
the sending machine) before it would accept an email.  IIRC, it was
proposed with a probabilistic task, but there's no reason why it
couldn't be done with a more precisely controlled linear task such as
repeated squaring under a modulus.  Or maybe you could ask
distributed.net to find a way to use CPU cycles beneficially and
provably, and require some number of work-packets to be completed
before the mail is delivered.
The computational task can get arbitrarily larger, if the recipient
system doesn't like the look of the mail.  I can picture the MDA
going, "wow, I decrypted this one, but it scores 9.2 on my procmail
filter scale, so I better ask for and get fifteen MIPS-minutes of CPU
time before I actually deliver it."
Stuff like this can be done anonymously, can be done on the recipient
and sender machines, can depend on filters (the MDA sees it after it
arrives and gets decrypted) and limits the per-machine rate at which a
spammer can send spam.
It requires no central keying authority, no registrations or controls,
allows random email from people you don't know or haven't heard from
in a while to reach you, is a barrier that's fully customizable at the
recipient site, can be implemented purely in software (meaning nobody
has to get a licence or a subscription or vouched for by someone else
to send mail), and if someone really *does* care enough to dedicate
fifteen MIPS-minutes of CPU to getting an advertisement through to
you, it probably means he's got a specific reason to believe that it's
actually something you'll be interested in, rather than just being a
"bottom feeder" who sends out a million emails in the hopes of one
SMTP is a hole, and needs replaced. We have the technology.  It'll

@_date: 2003-05-12 10:27:21
@_author: bear 
@_subject: A Trial Balloon to Ban Email?  
I submit that if Joe Lunchbox is not spamming, he is unlikely to
need to change his habits regarding having his machine available
for a computational burden.  The mail he sends to people known to
him will not ordinarily trip spamfilters at the recieving end that
would make such requests.
Likewise, all the people who use remailers to send anonymously.  As
long as what they're sending isn't identifiable as spam, the remailer
won't get a CPU-time request.

@_date: 2003-05-12 16:43:11
@_author: bear 
@_subject: A Trial Balloon to Ban Email?  
The question here is not about whether *you* get anything worth value;
the question is whether the economics of email can be changed in such
a way that spam is no longer profitable.
When I get something in the USPS papermail, it means that someone
cared at least enough about talking to me to spend money on a bulkmail
stamp.  When I bring my mail in, I routinely throw away anything that
the sender spent less than 29 cents on, because I know the postal
rates and the only way to go below 29 cents is to be sending a bunch
of stuff which you don't even care who gets it.  It doesn't mean I
actually get that 29 cents; it just means I'm absolutely sure I don't
need to care what's in anything that someone spent less than 29 cents
If someone is serious about contacting me, he sends me a 37 cent
letter, with a first-class stamp - or maybe a second-class letter if
they're a business with a mail room that has a lot of customers.  But
bulk rate, I can ignore.
I think we have a chance to build a better system electronically.
Instead of imposing a cost on all mail, we could just impose a cost on
the mail that our mail delivery agents (that's the MUA, not the MTA)
think smells spammy.  No cost in the basic simple case, but
prohibitively expensive to spammers.
We have already seen that 50 to 80 percent of all paper mail is junk
even in postal systems where there is a financial cost for sending
mail.  That means that, if we made *every* email cost a quarter to
send (in CPU time, or whatever) it would evidently be worthwhile for
at least some spammers to just buy enough machinery to send it anyway,
and those spammers would continue to generate enough spam to be 50
to 80 percent of our mail.
Therefore, we need the power to impose higher costs on spammers than
we want to pay for personal mail, and that requires us to be selective
about what mail we impose costs on.  The only reasonable way to do
that is at the MUA, where encrypted mail is already decrypted and
where decisions about it can be made depending on the individual
user's preferences and filters.  We don't want to impose a cost on
individuals sending stuff to their friends from underpowered handheld
units, and if what they send doesn't smell spammy to your filters,
why should we?
No matter what an MTA filter looks for, it will serve some users
badly.  Remember AOL's filters killing a breast cancer support group's
mail because the word "breast" kept being mentioned?  Think what
happens if writers of fiction about terrorism try to exchange stories
for critique and run into filters intended to catch terrorists.  Or
what happens when porn actors form a support group, or when students
in an ad copy-writing class send their assignments to their
instructor.  If we are to discriminate among emails, then the
discrimination *MUST* be on the level of individual preferences and
settings, and therefore *MUST* be done at the MUA rather than at the
Also, if we make discrimination among emails part of the job of the
MTA, we implicitly require all email to be plaintext in order for the
system to work, and that is a very bad design decision of SMTP which
we do not want to repeat.  If we intend to support mail that's
end-to-end encrypted, we must make any content-based filtering on it
filtering that's done at the MUA.
If we stick to a store-and-forward model for email, we need to stick
to a store-and-forward model for the hash-stamps and requests for
same.  Delivery of mail may be delayed several days if it smells
spammy to the MUA, until the request for a stamp propagates back and
the originating system fulfills it.  If the mail smells spammy and the
originating system can't be found, or ignores the hash-stamp request,
then after several days of not getting a hash stamp, the MUA should
just drop the suspicious package on the floor.  But if the mail
doesn't smell spammy, it shouldn't even matter whether the originating
system can be found; this allows anonymous email to people who need
it, as long as anonymity is not used to cloak spammers.  And it allows
underpowered devices to send mail, as long as the mail isn't spam.

@_date: 2003-05-16 09:32:43
@_author: bear 
@_subject: Payments as an answer to spam 
That's a good point, but remember TargetFirst?  They were
the first movers on the "get paid to surf the web" business
model.  Basically they paid people for the right to put
advertising on their desktops, and their business died
a horrible death.  I know because I scored some good
deals on laptops at their liquidation sale.
What happened to them, as I understand it, is that the
per-ad revenue got driven down to the point where it was
more expensive to process the payments than the payments
were actually worth.
Whenever anyone writes a check, it costs at least a nickel
for the merchant to clear it.  Clearly you don't ordinarily
use checks for such tiny purchases because (a), almost
nothing can be sold for less than a nickel, and (b), it
would be more time and bother for you to write the check
than you probably consider a nickel's worth of trouble.
If you posit a system for making very small payments,
you've got basically three problems: (1), consumers
will never make micropayments because a decision about
whether to spend money on something involves stress -
in this case stress that's out of all proportion to
the payment itself - and will be resented.  (2), Each
and every consumer system will need configuration and
a relationship with a "bank" somewhere to cash such
tokens made to them - and that requires the attention of
the consumer again to maintain a business relationship
which he won't bother to do unless there's a significant
(more than a few dollars a month) revenue stream. (3),
the cost of processing payments must be kept small
relative to the payments themselves.
I've seen half-a-dozen papers in the last year touting
the ability to keep per-transaction costs on micropayments
down to less than 20% of gross.  All of them did so at the
expense of precision and accountability.  These "solutions"
are non-starters because insofar as they operate without
human attention at any part of the system, if they are not
precise and accountable then they are subject to fraud.
And insofar as they require human attention, people will
ignore them in droves because the payments are too small
to be worth the hassle.
No paper, so far as I know, addresses the basic consumer
psychology issue, that payments of less than ~$1.00 or
revenue streams of less than ~$10.00/month are seen as
too annoying to bother with, especially if they come with
business attachments, contracts, advertising, emails
from the bank about additional services, monthly statements
that incur storage overhead and mental bandwidth, etc...
Every bit of that stuff is a teeny bit of stress.  And
humans will forego revenue streams that come with more
stress than they are worth.

@_date: 2003-05-30 08:25:44
@_author: bear 
@_subject: "PGP Encryption Proves Powerful" 
Aside from the whole governments-and-people-and-terrorists thing,
I will say that there was an event last year at my former employers'
that made us very glad we were using PGP.
An engineer's laptop got stolen. With the entire source tree of an
enterprise application that licensed for $25K a seat on it.  Fortunately,
since it was in an encrypted archive, we didn't need to worry too much.
I don't know how many "incidents" like this happen every year.  I don't
think governments care that much about the kind of risk companies not
using crypto to protect their livelihoods take.  They don't become aware
of crypto when it averts trouble.  They become aware of crypto when it
causes trouble.

@_date: 2003-05-30 16:33:33
@_author: bear 
@_subject: Nullsoft's WASTE communication system 
Excellent point.  Sequence numbers aren't the only way to do
this, but they are probably the simplest.  Without them you
need to worry about replay and other integrity attacks.
If MD5 itself is to be trusted as a hash function, this is not
particularly scary.  They are using MD5 over encrypted data which
includes a participant identifier; that means that in order to defeat
message authentication, Mallory would need to be able to forge and
encrypt a message with the known participant identifier, such that the
encrypted message hashes to the same MD5 code.  I think this is not a
bad fundamental design, if MD5 is to be trusted as a hash function.
Using a keyed hash like HMAC here in a way that relies on its keyed
property would introduce key management issues, with the attendant
risks of getting them wrong, which as far as I can tell are
unnecessary in this application.
However, MD5 may not be an example of a hash function that is still
trustworthy at this point. Dobberton (who is known to work for German
Intel) published a paper about collisions in MD5 that looks like it
might have been the 'toehold' that someone could climb to a full break
on, and then, conspicuously, did not publish any more papers about
MD5.  If he did manage to work it into a full break, he would not have
been allowed by his employers to publish.  And if he proved that a
full break based on that toehold was not possible, he would not have
been allowed by his employers to publish.  And if he is still working
on it, there'd have been nothing *to* publish.  All three ways, we see
the same story.  And since his paper came out, there are excellent
odds he's not the only one trying to climb on that toehold.  So using
MD5 these days is largely a matter of whether you feel lucky or not.
I'd suggest a different hash function, or HMAC with a constant key.
Another excellent point.  Is there a good way to reset PCBC counters
without requiring a key agreement protocol for the new counter value?
PCBC mode, I'm guessing, was an attempt to simplify the security
equations of the system by making it hard for Eve to pick up the
thread of a communication in the middle.  The authors are relying on
TCP/IP's error correction to prevent bit-errors in transmission, which
makes this system unsuitable for any non-Internet applications.
Blowfish has been around longer than Rijndael; I think AES may not yet
have gotten as much cryptographic attention as Blowfish's several-year
headstart has given it.  I think that a "perfect cipher" of Blowfish's
block size would necessarily be less secure than a "perfect" cipher of
AES' block size, but I'm not aware of any work demonstrating either to
be an example of a "perfect cipher". (Nor any methodology such work
could employ, for that matter).
Note, I'm using "perfect cipher" to mean that there is no method for
recovering a plaintext block or key from ciphertext that involves less
work than attempting decryption with all possible keys - a property
which, of course, cannot in practice be proven but which is useful to
consider as a lower constraint on key sizes, block sizes, etc.

@_date: 2003-05-31 09:28:34
@_author: bear 
@_subject: Nullsoft's WASTE communication system 
I haven't gone source diving, but from the doc, I've been assuming
it's the third case.
That's reasonable.
Hmm.  I had missed that too.  I suppose in that case they would
lose nothing by using PCBC since there'd be no remaining message
for a bit error to screw up, but it's still a strange choice, and
not one that demonstrably gains them much either.
Ow.  Now hat _IS_ scary.  Making a strange choice is one thing, and
I tend to assume unless proven otherwise that strange choices don't
mean incompetence.  But not _knowing_ what choice you made is
entirely different.  Where the doc and the source diverge, the
explanations that involve competence get a lot harder to believe.
Okay, I was out of date.  My bad.  And you're definitely right about
future work.
Just BTW, I don't often have good things to say about the intel guys,
but I think that the hands-off policy during the AES selection process
was _EXACTLY_ the right thing for them to do.  It gives users of AES
a kind of confidence about being its being tamper-free that DES never
had, clears their agencies of tampering suspicion which helps foster
trust, and places responsibility for public security infrastructure in
public forums where it, IMO, belongs.  I'd agree with you about using
AES going forward, or if CPU time's not a serious issue, 3DES.

@_date: 2003-11-16 09:44:16
@_author: bear 
@_subject: Roundtrip Blinding (was: A-B-a-b encryption) 
This is a roundtrip blinding message protocol.
First of all, you mean asymmetric crypto (where encryption
key != decryption key).
The problem with this is that there are very few encryption
algorithms that this will work with and all the ones I know
have serious problems in modes where this is possible. In
decrypt(a, encrypt(b, encrypt(a, M))) != encrypt(b, M)
in most secure cipher systems.
RSA will do this - but in modes where stunts like this are
possible, it means you're using "straight" RSA -- ie, without
padding the blocks with randomness.  And this leaves RSA open
to some types of attacks that are very difficult to allow for
in a secure system.  Where RSA is used in this mode (for blinding
digital cash, etc) it is used in a very stylized and restricted
way, blinding "tokens" whose interpretation and use is very

@_date: 2003-10-01 13:08:47
@_author: bear 
@_subject: Reliance on Microsoft called risk to U.S. security 
I think part of the point is that that expectation is a substantial
Data that moves between machines is inherently suspect; and if it can
originate at unknown machines (as in SMTP or NNTP), it should be
regarded as guilty until proven innocent.  There ought to be no way to
send live code through the mail.  Users simply cannot be expected to
have the ability to make an informed decision (as opposed to a habit)
about whether to run it, because its format does not give them enough
information to make an informed decision.
The distinction between live code and text is crucial.  While both are
just sequences of bytes, text has no semantics as far as the machine
is concerned.  Once you start sending something that has machine
semantics - something that contains instructions for the machine to
run and running those instructions may cause the machine to do
something besides just displaying it - then you are dealing with live
code. And live code is handy, but dangerous.
There is pressure to stick live code into any protocol that moves
text; SMTP sprouted 'clickable' attachments.  Java, javascript, and
now flash seem to have gotten stuck into HTTP. But I think that live
code really and truly needs a different set of protocols; and for
security's sake, there really need to be text-only protocols.  It
should be part of their charter and part of their purpose that they do
*NOT* under any circumstances deliver live code.
"Can be relied on to _only_ deliver text" is a valuable and important
piece of functionality, and a capability that has been cut out of too
many protocols with no replacement in sight.
Separating it by protocol would give people practical things that they
could do.  You could, for example, allow people to use a live-code
mail protocol inside a company firewall or allow a live-code browsing
protocol inside the firewall, while allowing only a text mail protocol
or a text browsing protocol to make connections from outside the
company.  We approximate this by trying to make smarter clients that
have different trust models for different domains, but that's always a
crapshoot; you then have to depend on a client, and if the client can
be misconfigured and/or executes live code it can't really be relied
on.  It would be better to have separate protocols; Ideally, even
separate applications.
Indeed.  I think that there ought to be simpler, text-only protocols
for the use of people who don't need to send and recieve live code, so
that they could be effectively protected from live code at the outset
unless they really need it.  Others, of course, disagree.

@_date: 2003-10-01 13:32:02
@_author: bear 
@_subject: Monoculture 
That's not the money they're trying to save.  The money they're trying
to save is spent on the salaries of the guys who have to understand
it.  Depending on what needs you have, that's anything from
familiarity with setting up the certs and authorizations and servers
and configuring the clients, to the ability to sit down and verify the
source line by line and routine by routine.  The price of computer
memory is a non sequitur here; people want something dead-simple so
that there won't be so much overhead in _human_ knowledge and
understanding required to operate it.
Crypto is not like some game or something that nobody has to really
understand how it works; key management and cert management is a
complex issue and people have to be hired to do it.  Code that has so
much riding on it has to be audited in lots of places, and people have
to be hired to do that.  Every line of code costs money in an audit,
even if somebody else wrote it.
So, yeah, they'd rather see a lot of stuff hard-coded instead of
configurable; hard-coded is easier to verify, hard-coded has less
configuration to do, and hard-coded is cheaper to own.  We get so busy
trying to be all things to all people in computer science that we
often forget that what a lot of our clients really want is simplicity.
And in a lot of places that's exactly what they do.  If the shop
requires a full code audit before taking any new software, going to
the new version can cost tens of millions of dollars over and above
the price.  And the bigger the new version's sourcecode is, the more
the audit is going to cost.
You wouldn't.  But the people who have to slog through that tarball of
code for an audit get the jibblies when they see  all over the
place, because it means they have to go through line by line and
routine by routine again and again and again with different
assumptions about what symbols are defined during compilation, before
they can certify it.

@_date: 2003-10-01 19:02:00
@_author: bear 
@_subject: Reliance on Microsoft called risk to U.S. security 
Heh. You looked at my mail headers, didn't you?  Yes, I use pine -
primarily *because* of that property.  It treats all incoming messages
as text rather than live code.
A protocol for text (as opposed to live code) requires compliant
clients (ie, clients that don't do anything other than display the
recieved messages).  As such, it's at least somewhat a social issue.
No, it is not.  You can make a hyperdocument that is completely
self-contained and therefore "text", but that is not how HTML is
normally made.  HTML can cause your machine to do things other than
display it, and to that extent it is "code", not text.
It can cause your machine, specifically, to make network connections
to get graphics, style sheets, etc, and will not display correctly
unless the network is available, making it impossible (or at least
difficult) to properly read mail offline and introducing dependencies
that you as reader may or may not be aware of.  Someone at a site a
thousand miles away can change out a graphic in their server, and
suddenly your "text" is different.  A machine you've never heard of in
a location you don't know can go down, and suddenly your "text"
becomes undisplayable.  You can't rely on "saving" an HTML document
and being able to read it years or decades later, because with
hypertext, maybe the part you're interested in (or need for evidence)
isn't even on the page you saved.  Hypertext creates the illusion of
being a single document; but hypertext documents are code directing
interaction with a network, not just text storing information.
The fact that sending HTML (and other code) through SMTP was not
considered a violation of SMTP has allowed a generation of mail
readers to become common that encourage mail viruses, macroviruses,
worms, and other malicious code.  If we are interested in security, we
need some kind of protocol where we as a group just draw a line and
say "nothing but text through this port."

@_date: 2003-10-01 21:14:00
@_author: bear 
@_subject: anonymous DH & MITM 
DH is an "open" protocol; it doesn't rely on an initial shared
secret or a Trusted Authority.
There is a simple proof that an open protocol between anonymous
parties is _always_ vulnerable to MITM.
Put simply, in an anonymous protocol, Alice has no way of knowing
whether she is communicating with Bob or Mallory, and Bob has no way
of knowing whether an incoming communication is from Mallory or from
Alice.  (that's what anonymous means).  If there is no shared secret
and no Trent, then nothing prevents Mallory from being the MITM.
You can have anonymous protocols that aren't open be immune to MITM
And you can have open protocols that aren't anonymous be immune to
MITM.  But you can't have both.

@_date: 2003-10-02 11:48:53
@_author: bear 
@_subject: anonymous DH & MITM  
If it's an anonymous protocol, then "credit" for being a good chess
player is a misnomer at best; the channel cannot provide credit to
any particular person.
Public key? I thought we were talking about an open protocol between
anonymous entities.  If Alice and Bob can identify each other's public
keys, we're not talking about anonymous entities.  If there is a
trusted authority to say "these keys are okay" without identities
being known to each other then we're not talking about an open
protocol.  And if there's neither, then there is room for Mitch.
If this is an open protocol between anonymous entities, then Alice and
Bob can be using asymmetric keys, but must be using key pairs neither
part of which is known to the other at the beginning of the protocol.
In that case nothing prevents Mitch from deriving two new key pairs
and using one in communication with Alice, the other in communication
with Bob, and forwarding their moves to one another.
Hmmm.  I'll go read, and thanks for the pointer.  But I'm confident
that if Mitch can be kept out, then either it's not fully anonymous
participants, or it's not a fully open protocol.

@_date: 2003-10-02 14:16:51
@_author: bear 
@_subject: anonymous DH & MITM  
Wait.  That's not anonymity, that's pseudonymity.  And yes, you can
have pseudonymous open protocols that are immune to MITM.  My
contention was that you can't have anonymous open protocols that are
immune to MITM.
Okay, so the keypair is fresh-made and we are talking about an
anonymous protocol.  In that case Alice can't tell Mitch's key from
Bob's key and Bob can't tell Mitch's key from Alice's.
Perhaps I spoke too soon?  It's not in Eurocrypt or Crypto 84 or 85,
which are on my shelf.  Where was it published?

@_date: 2003-10-02 16:38:53
@_author: bear 
@_subject: anonymous DH & MITM  
Ah.  Interesting, I see. It's an interesting application of a
bit-commitment scheme.
Hmmm.  The key to this is that synchronous communications have to
happen.  When it's your turn to move, you create a message that gives
the move, then pad it to some unsearchable length, encrypt, and send
half.  MITM can't tell what the move is without seeing the second
half, so either has to make something up and send half of that, or
just transmit unchanged.  The second half is sent by each player when
the first half has been recieved, and includes a checksum on the first
half that was actually recieved.
Mitch hast the choice of playing his own game of bughouse against each
of the contestants, which just turns him into a third contestant.  Or
he has the choice of allowing the first two contestants to complete
their game without interference.
Why should this not be applicable to chess?  There's nothing to
prevent the two contestants from making "nonce" transmissions twice a
move when it's not their turn.

@_date: 2003-10-03 15:05:47
@_author: bear 
@_subject: anonymity +- credentials 
The state of the art, AFAIK, is Chaum's credential system.
One important thing to remember about any pseudonymous credentials is
that they can't possibly say anything bad about the holder more
important than what they say that's good.  If it isn't better to have
them than not have them, the holder will just abandon them.
This applies most strongly to pseudonymous credentials, because
pseudonymous systems are typically a lot easier to create a new
credential with and the cost of credential abandonment is lower.  But
this doesn't just apply to pseudonymous credentials.  People treat
even the "absolute identity" credentials exactly the same way, when
"is-a-citizen" and "is-a-person" and other fundamentals are no longer
more important than "is subject to involuntary military service" or
"is wanted by the FBI" or "Convicted an abortion clinic bomber" or
"Testified against the Mafia" or "Was one of the protesters at
Tiannanmen Square."
Basically, when your credential gives people (enemies of the state or
servants of the state, makes no difference) a reason to want to kill
you, or otherwise do you harm, you have to analyze keeping that
credential in terms of risks and benefits. Pseudonymity brings this
aspect of identity credentials to the fore, but it doesn't begin and
end with pseudonymity.

@_date: 2003-10-03 16:56:27
@_author: bear 
@_subject: anonymous DH & MITM 
Not "faster" per se, but I do happen to know the solution to that
problem.  :-)
Suppose Alice picks a nonce A(zero).  Then for n=one to a thousand
(presumably no chess game will last 1000 moves) she calculates A(n) =
hash (A(n-1)).  Note, this has to be a ONE WAY hash function rather
than any kind of encryption that can be decrypted.  I'd suggest
seventeenth-power mod K where K is prime, but lots of good
irreversible hashing functions that aren't so expensive in CPU cycles
are around.
Bob also picks a nonce B(zero) on his side, and produces the same kind
of sequence of B(one...one thousand) using the same hash function.
Now let the moves of the chess game be numbered from 1000 down to 0.
(ie, the first move they play will be move 1000, the second will be
move 999, etc.)
When it's Bob's turn, he sends his move padded with B(n), and Alice
sends a random move padded with A(n).  When it's Alice's turn, she
sends her move padded with A(n) and Bob sends a random move padded
with B(n).
Bob can rapidly check to make sure that the A(n) recieved with each
message has the right relation to the A(n+1) he recieved with the
previous move, but there is no way he (or Mitch) can possibly predict
A(n-1) to know what he'll get in the next move.
Likewise Alice can rapidly check to make sure that the B(n) recieved
with each move has the right relation to the B(n+1) she recieved with
the previous move, but there is no way she (or Mitch) can predict
B(n-1) to know what she'll get the next move.
The only change to the rules of chess this requires is that if they
ever exhaust the finite sequence of generated nonces, they have to
call that game a draw.  But a thousand moves, really, shouldn't be a
problem for chess, and if it is you can just make the sequence longer
and start a new game.

@_date: 2003-10-05 09:58:56
@_author: bear 
@_subject: anonymous DH & MITM 
Yes, although I hadn't immediately realized it would be necessary:
Timing information.  If you require 30-45 seconds between packets,
Mitch's game dies a rapid death.
T:0 - Mitch sends first half of cyphertext of MA(1000) (to Alice)
T:30 - Alice sends first half of cyphertext of her move + A(1000) (to Mitch)
T:60 - Mitch sends second half
T:90 - Alice sends second half
Mitch can now decrypt Alice's move.
T:60 - Bob sends first half of cyphertext of B(1000) (to Mitch)
T:90 - Mitch sends first half of cyphertext of Alice's move + MB(1000) (to Bob)
T:120 - Bob sends second half.
T:135 - Alice times out waiting for Bob's response because it's 45
        seconds since her last packet. Mitch must commit to a move
T:150 - Mitch sends second half of Alice's move to Mitch
Bob decides on his move.
You could fiddle the intervals, within limits, or allow the players an
"I need more time to think" move, but if they're not allowed to use it
more than one time in three, then mitch isn't going to be able to make
more than two moves.

@_date: 2003-10-13 14:27:11
@_author: bear 
@_subject: NCipher Takes Hardware Security To Network Level 
I think this is just a question of where the target is this year.
As long as systems continue to become more complex, the individual
parts of those systems are going to have to become more reliable.  The
alternative is that the complex systems die under a mass of tiny bugs
that interact in bad ways.
As memory space becomes larger and systems that take advantage of it
become more complex, we're going to see ever-increasing reliability
requirements of the individual components.  And, at some point,
"proofs of correctness" will be necessary as sales points for the
individual components.
Right now, we're not seeing it much yet.  But I saw a proof of ext3
journaling fileystem software, buried in one of the design documents.
It demonstrated that there is no possible order in which the
filesystem interface routines can be called that will leave the system
in an undefined state.  Of course, that proof assumed that the
read/write operations in the hardware were error-free, which is not
the case.  They come close though, with the sector checksums an error
correction codes.
Filesystems are a nice microdomain for proofs of software correctness,
partly because operations on them are fairly constrained and partly
because they are subject to hardware errors; if you want to say with
assurance that some crash isn't the filesystem drivers' fault you have
to prove it.
Crypto is another nice microdomain for proofs of software correctness,
because it is also constrained, but operates under assumptions of
malicious attack or efforts at subversion, and is relied upon to
protect valuable information.  If you want to say that a particular
security breach isn't the crypto software's fault, you have to prove
But, as systems grow more and more complex, and you have many
thousands of subsystems and components interacting, you're going to
see more and more of these little microdomains, because when it gets
to that level you have to have some *very* stringent requirements for
correctness in all those little subsystems.  I see correctness proofs
for many key infrastructure components of the OS as likely to be
fairly common in another ten years, or half that if the OS market
shows signs of actual competition.
OTOH, I don't think proofs of correctness for user-level applications
is likely to ever happen, because the potential loss of value tends to
be less for an application crash than for a hosed OS.

@_date: 2003-09-08 16:54:32
@_author: bear 
@_subject: fyi: bear/enforcer open-source TCPA project 
You cannot.
The correct security approach is to never give a remote machine
any data that you don't want an untrusted machine to have. Anything
short of that *will* be cracked.

@_date: 2003-09-10 09:46:54
@_author: bear 
@_subject: Code breakers crack GSM cellphone encryption 
Of course the NSA's satellite and embassy based cellphone interception
capability isn't primarily targeted against - US - calls; that would
be illegal.  The snooping in the US is done by others and then handed
over to the NSA instead.  And of course the NSA does the same for
them.  This is what the UKUSA agreement is all about.
Bluntly, no matter who does the actual interception work, in the
modern world every intel agency's analytic and correlative resources
are targeted against everybody in the world.  To say that some
particular agency doesn't do intercepts in some particular country is
irrelevant; It's all just data. Remember lawmakers learning that the
internet treats censorship as damage and routes around it?  Well,
we're looking at the same phenomenon here: the worldwide intel
community treats privacy laws and operational restrictions as damage
and routes around them.  It's exactly the same thing.
I'd be willing to bet most nations even get intel on their own
citizens that's gathered by actively hostile countries: An actively
hostile nation, let's say, snoops on american citizens.  Then they
share the intel product with someone they've got a treaty with, and
then that country shares it with somebody they've got a treaty with,
and they share it with the US.  It's all just routing.  Someone has
information somebody else wants, somebody else has money or intel to
swap for it.  It doesn't take a genius to figure out, it's just going
to happen.  Anything an intel service shares with anybody, it's
putting into the network, and it's going to get around to everybody.

@_date: 2003-09-10 10:49:15
@_author: bear 
@_subject: fyi: bear/enforcer open-source TCPA project  
The problem with this is Moore's law.  By the time your high-end
coprocessor is widely adopted, most of the actual units out there will
no longer be high-end.  And the kid who has the latest hardware will
always be able to emulate an older secure coprocessor in realtime, the
same way they used to use hacked printer drivers to simulate the
presence of hardware dongles on the parallel port. So this doesn't
work unless you put a "speed limit" on CPU's, and that's ridiculous.
Yes.  Protocol designers have been explaining how to do them for
decades.  There is usually a protocol that allows untrusted machines
to only have data suited for handling by untrusted machines, while
still providing the appropriate benefits.
There are things you can't do that way, of course; a machine cannot
display information to a human that it does not have.  But a
remote-and-therefore-untrusted machine is in front of a
remote-and-therefore-untrusted human, and therefore ought not do such
a thing anyway.
Designing applications that use protocols to achieve design goals
without ever transmitting information that an untrusted machine ought
not have is hard.  But it is possible, and until it's done we're going
to see a parade of cracked applications and hacked hardware destroying
every business plan that's built on it and every life that depends on
it.  Depending on a solution that lets "remote but trusted" hardware
handle information that the remote machine shouldn't have in the first
place is an invitation to be hacked, and an excuse to avoid the hard
work of designing proper protocols.
Online credit-card purchases are ten percent fraudulent by volume.
Crypto is widely deployed for credit-card purchases, but stemming
fraud seems to be like trying to dig a hole in water.  Points made
here recently about who has a motive to stop fraud seem applicable.
And, significantly, much of this fraud is done by people who manage to
crack the merchants' databases of credit card numbers and accounts
which are kept in cleartext.  I don't think any crypto infrastructure
is going to stop "personal" card fraud by someone who's got your card
out of your wallet.  Boyfriends, Girlfriends, roommates, and siblings
commit a lot of fraud on each other.  But a better protocol design
should at least put credit card numbers in merchant databases out of
reach of crackers - by never letting the merchants get them in
A merchant should wind up with a unique "purchase code" - a blob from
which the bank (and no one else) ought to be able to tell the payee,
the amount, the source of funds, and the date of the transaction.
This is fairly simple to do with asymmetric encryption where the bank
has a published key.  A merchant should NOT wind up with a cleartext
credit card number for an online purchase.  Someone hacking the
merchant's database should NOT wind up with information that can be
"replayed" to commit frauds.  This isn't a matter of transmitting
priveleged (sensitive) information to a "remote but trusted" machine;
this is a matter of finding an appropriate (non-sensitive) form of
information that a remote machine can be trusted with.  No special
hardware is required, it's just a matter of using the appropriate
Frankly I don't know enough about how medical records are handled to
say much about them - I couldn't even make a good assessment of the
operational requirements.  But the information has huge economic value
as well as huge personal privacy value.  Its inappropriate disclosure
or misuse can destroy lives and livelihoods.  It ought to be
considered, and protected, as a target for theft.

@_date: 2003-09-11 00:42:58
@_author: bear 
@_subject: fyi: bear/enforcer open-source TCPA project  
You propose to put a key into a physical device and give it
to the public, and expect that they will never recover
the key from it?  Seems unwise.

@_date: 2004-08-10 11:28:30
@_author: bear 
@_subject: How a Digital Signature Works 
Standard stuff, really.  The reporter misunderstands something, or
"simplifies the explanation" for the article, or it gets edited
for length by the publisher,  until the explanation given is --
well -- no longer true.
This happens with almost every technology article that attempts
any depth; we should be unsurprised.

@_date: 2004-08-18 08:32:52
@_author: bear 
@_subject: RPOW - Reusable Proofs of Work 
I'm wondering how applicable RPOW is.  Generally speaking, all
the practical applications I can think of for a proof-of-work
are defeated if proofs-of-work are storable, transferable, or
reusable.  Once they're storable, tranferable, and reusable,
aren't we restricted to applications already nailed down by
digital cash schemes?
So, even if it works flawlessly, this seems like an exercise
of cleverness that makes a cryptographic entity *less* generally
useful than the primitive from which it's derived. It probably
has a few specialized applications that normal POWs won't serve;
I just haven't been able to distinguish them from the applications
served by digital cash.
Why doesn't this scheme give rise to a "POW server" that just
sits there, generating proofs-of-work in advance of need and
dispensing them at request?  Or even to a company that sells
POW's to people who can't be bothered to run their own server?
And doesn't such a device or service defeat the use of POWs
for real-time load balancing, traffic control, etc?

@_date: 2004-08-25 17:06:41
@_author: bear 
@_subject: More problems with hash functions 
It seems like, for every "obvious" thing you can do to get around
it, it can be rearranged and applied in a different way.  And the
less obvious things, which actually do work, are all CPU-expensive.
One interesting idea which I came up with and haven't seen a way
past yet is to XOR each block with a value computed from its
sequence number, then compute the hash function on the blocks in
a nonsequential order based on the plaintext of the blocks.
In concrete terms, you have a message of n blocks, p1 through pn.
you xor each block with a value computed by a nonlinear function
from its sequence number to get q1 through qn.  Now rearrange
q1 through qn by imposing a total ordering on p1 through pn: for
example if p4 sorted before p7, you put q4 in front of q7.
Finally, you compute the hash value on the blocks q1 through qn
in their new order.
Now the attack doesn't work; collisions against individual blocks
don't combine to produce a collision on the sequence because the
colliding values wouldn't have been fed into the hash function
in the same order as the actual blocks.

@_date: 2004-08-25 21:06:35
@_author: bear 
@_subject: More problems with hash functions 
You are correct.  Thank you for your quick and clear thought
regarding the proposal.  Increasing the attacker's work by
a factor of n^2 where n is the number of blocks preserves the
strength only where n^2 >= (2^k)/2 where k is the block size.
So, for a 160-bit hash, (2^k)/2 is 2^159, and that means the
proposed method preserves nominal strength only for messages
longer than 2^80 blocks.  Which in practical terms will not
happen; I don't think 2^80 bits of storage have yet been
manufactured by all the computer hardware makers on earth.
I guess I momentarily forgot that with a hash function the
attacker has access to the plaintext and can therefore tell
unambiguously the result of the permutation of the blocks.
I'll continue to think about it.  Maybe I'll come up with
something better.

@_date: 2004-08-27 14:07:08
@_author: bear 
@_subject: More problems with hash functions 
"Avoided" is too strong a term here.  What you're doing is making
the course twice as long because now the runners are twice as fast.
The Joux attack still "works" in principle, in that finding a series
of k block collisions gives 2^k message collisions; All that you're
doing by making the internal path wider is making the block collisions
harder to find.
When you make them harder to find in such a way as to compensate
exactly for the advantage from the Joux attack, you're not "avoiding"
the attack so much as "compensating for" it.
Still, it looks like you're exactly right that that's one good way
to have an online algorithm that the Joux attack doesn't give any
advantage against; if you want a 256-bit hash, you pass at least 512
bits of internal state from one block to the next.
I had another brainstorm about this last night, which was to use
an encryption algorithm for the block function; that way there are
no block-level collisions in the first place.
Unfortunately, that doesn't cut it; even if there are no block-level
collisions on the state passed forward, there can still be collisions
on state passed forward for every two blocks.  Instead of searching
for a collision of internal state in single blocks, the attacker
would then be searching for it in block pairs.  Given the same amount
of internal state passed between blocks, the search would be no more
difficult than it is now; for the price of finding k block-length
collisions, the attacker gets 2^k colliding sequences.
So far, your suggestion of passing twice as much internal state
between blocks looks like the only sound and simple solution yet

@_date: 2004-08-28 21:09:36
@_author: bear 
@_subject: ?splints for broken hash functions 
The conjecture is false.
The Joux attack will still work to find multicollisions,
and it will not increase the work factor at all.  In
fact in some cases it will actually decrease it.
I think you're trying to use the encrypted form of the
message as a redundancy code for tamper-resistance;
presumably the collisions found will be highly unlikely
to share the structure of being a message concatenated
with its encrypted form under AES.
This will help in some applications:  If used for
bit commitment, for example, a "collision" when
eventually presented would clearly *not* be the
initial message because it would lack the redundant
structure of a message followed by its own AES
encrypted form.
However, that's not the same property as collision
resistance.  The attack is derived from the structure
of the conventional hash function and the way it handles
blocks one at a time.  More blocks does not make it
any harder.
A given block of the message to be hashed is normally
one of to inputs to a particular round of the hash
function.  The other is the "internal state" of the
hash function.  These are operated on to produce the
new "internal state" that is used along with the next
block of the message.... until the end of the message,
whereupon the internal state is typically read as the
value of the hash function.
An attacker using the Joux method would start knowing the
message and the initial state of the function.  He would
feed one block of the message into the hash function,
read the new internal state, and search for another block
that, given the same starting internal state, produces
the same ending internal state.  He also knows the starting
internal state of the hash function before the second
block, and can equally search for another block that,
starting from that internal state, produces the same
internal state as the state after the second block.
(and etc through the message, if he feels so inclined).
Now, if our attacker finds *two*  one-block collisions
in different blocks, he can "mix and match" them to get
*four* colliding whole messages, of which the original
message is one and two would have come from his search
in a random function.  That means he got one for "free".
If he finds a collision in each of eight different blocks,
he can mix and match them to get 256 colliding messages,
of which one is the original message and eight are what
he would have gotten from a collision search for a fully
random function, so 247 of them are "free."
In fact, the more blocks he has to work with, the more
different places he can find collisions in.  So in that
sense appending the ciphertext of the message makes it
*less* secure.  Consider the results of searching for
sixteen one-block collisions in an eight-block message;
with only eight blocks, you get the most free stuff if
you find two block collisions per block; now you have
3^8 colliding messages.  But if the message is extended
to sixteen blocks, for the same effort you get one
collision per block and 2^16 collisions.  2^16 is a lot
more than 3^8, so extending the message gives the
attacker more freebies per block collision found.
One-block messages are completely immune to the attack.
Hal's proposed solution is to double the length of the
internal state, which makes the block collisions much
harder to find and thereby cancels out the advantage
of recombining block collisions.

@_date: 2004-08-28 21:19:57
@_author: bear 
@_subject: ?splints for broken hash functions 
You realize that passing two internal-states forward at each
step is entirely isomorphic to Hal's suggestion of passing
twice as large an internal state forward at each step?
Effectively, you're agreeing with him.  Good hash functions
*do* evidently cost twice as much work and (at least) twice
as much internal storage as we thought before now to compute.
Your proposal triples the amount of state required through
the process (two one-block internal states, and the initial
block), whereas Hal's proposal only doubles it.

@_date: 2004-08-28 22:28:17
@_author: bear 
@_subject: A splint for broken hash functions 
Here's a handy way to build Joux-resistant hash
functions with existing tools.
This construction takes slightly more than twice as
much work and twice as much internal state as a
conventional hash function (embrace it guys, I don't
think there's a way around it).
   H2(H1) = H1( H1(M)  xor  H1( TT( M)))
 TT denotes some trivial transformation
 (I propose swapping high and low halves
 of the first block).  H1 is a conventional
 hash function and H2 is, I believe, fully
 resistant to the Joux attack.
Or, more precisely defined in the
scheme programming language:
;; joux-resistance takes a conventional
;; hash function and returns a joux-resistant
;; hash function derived from it.
(define joux-resistance
  (lambda (H1)
    (lambda (message)
      (H1 (xor (H1 message)
               (H1 (TT message)))))))
(define JR-MD5 (joux-resistance MD5))
(define JR-SHA1 (joux-resistance SHA1))
;; ... etc...
This is based on John Denker's second proposal.
But this proposal uses a trivial transformation
to force divergence in internal states rather
than using a reordering of blocks.
Now the attacker has to find blocks that have
internal-state to internal-state collisions in
both H1(M) and H1(TT(M)) in order to use the
attack, which is the same work factor as finding
collisions in both mappings of start-to-end
states (Denker's reordering proposal) or finding
state-to-state collisions on an internal state
twice as long (Finney's wider internal path
I think this is a refinement on Denker's
proposal because we don't have to store the
initial block while we're doing it and don't
have to make changes as deep into the existing
applications in order to implement it, and
more immediately usable than Finney's proposal
because we don't have to wait for the next
generation of hash functions to be written.

@_date: 2004-08-29 11:36:30
@_author: bear 
@_subject: A splint for broken hash functions 
Actually, it was intended to take a hash function
as an argument and define a new hash function which
is Joux-resistant in terms of it.
    JR(H1)(M) = H1( H1 (M) xor H1( TT( M)))
would have communicated the intent more clearly.
Anyway, functions on functions that return functions
kind of need lambda calculus for clear expression,
which is why there was scheme pseudocode restating
the definition following this.
Good point.  I don't want to mess with the IV's though,
because some hash functions may depend for their security
on the exact value of the IV.
That's compensated, I think, by the outer application of H1.
But Hal Finney's response proposes using a 2n-bit to n-bit
hash function on the concatenation of the two H1 results,
which is definitely better.
It still requires finding a double collision, so the work
factor should be the same.
Okay, if your H1 and H1prime are nonidentical hash
functions, I don't think you're getting any added
security from the trivial-transformation here; Its
purpose in my construction was to force divergence
of the internal state between the two hash functions;
why is it included in yours?
:-)  You're right about that; a trivial transformation
makes it too easy to construct an attack by constructing
a block that reads the same under transformation as it
does plaintext.  Instead of swapping first and second halves
of the block, probably the first block of the message should
be replaced by its hash.
Responding to your issue about xor being too insecure, by
taking Hal Finney's suggestion of using a wider hash function
to combine the outputs,  And redefining TT for greater
strength, the construction becomes
H2(M) = Hnew (H1 (M) + H1( TT(M)))
Where H2 is the Joux-resistant version of H1, which is our
desired objective.  TT now means replacing the first block of the
message by its hash value under H1, and + denotes concatenation.
However, this requires implementing an Hnew, which must take a
*single* large block of input (the size of two H1 blocks) and
produce a single H1-sized block of output.
Since we all seem to agree that an Hnew is needed to make the
suggestion concrete, what definition of Hnew do you prefer?
Why?  Divergence in internal state ought to be complete after
the first block; constructing a message that would force the
internal states of the two hash functions to ever reconverge
would require the same work factor either way.  I don't think
a long-range permutation buys us anything.
It's cheap and it doesn't hurt. We can include it if it makes
us feel any more secure; but why should it make us feel any
more secure?

@_date: 2004-07-10 13:07:06
@_author: bear 
@_subject: identification + Re: authentication and authorization 
I have always thought that "credential fraud" would make a better
description than "identity theft."  The crime about which we are
concerned, literally, is the use of your credentials by someone
else in the commission of a fraud.
"Theft" would imply to me that he simply walked into the bank (or
wherever) and took the money (or whatever) at gunpoint, directing
them to charge your account; an image I find more than a little
preposterous.  There has to be some kind of fraud or subterfuge
for the proposed crime to even be credible.

@_date: 2004-06-08 12:32:40
@_author: bear 
@_subject: Security clampdown on the home PC banknote forgers 
Probably not; instead, the banknote detection stuff will probably be
pushed out to tamper-resistant hardware ROMs in the printers, where
it's *NOT* under the control of anything running on a general-purpose
computer.  Because, really, nothing prevents someone from building
their own electronic device from scratch and attaching it to the
printer. The logic has to be something you can't use the printer
without, and that means built into it.
This is actually a lot less annoying than something like Palladium,
where people want remote restriction on a general-purpose PC.  If
it's pushed out to the printing hardware, there's no need to restrict
the architecture of a general-purpose machine.
Of course, there is such a thing as money that really and truly
*can't* be counterfeited.  Elements such as gold, or other rare
commodities, for example, cannot be faked; something either is gold,
or it isn't.  Also, useful objects and consumables in general cannot
be faked; something either is useful, or it isn't.
Fiat currencies are based on artificially imposed rarity, and
increasingly people are able to overcome the artificial impositions.
Wouldn't it be a stitch if nations were forced to re-adopt the gold
standard (or adopt the chocolate standard) because all their bills
(and SmartCoins, and RFID tokens, and ....) could be counterfeited?

@_date: 2004-06-22 12:46:58
@_author: bear 
@_subject: cryptograph(y|er) jokes? 
Bob and Alice routinely discuss bombs, terrorism, tax cheating, sexual
infidelity, and deviant sex over the internet.  They conspire to commit
crimes, share banned texts and suppressed news, or topple tyrannical
governments whose agents eavesdrop on their every communication.  They
do all this with utmost secrecy and unbreakable codes.
However, Alice and Bob do not even trust each other.
A protocol designer then, is someone who does not think that Alice
and Bob are insane.

@_date: 2004-05-28 09:46:03
@_author: bear 
@_subject: The future of security 
I tend to agree with Mr. Stiglic.
Cryptographic techniques can provide a few partial solutions to spam.
What cryptography *can* do is limit the possible senders to a known
list.  This has positive, but limited, utility.  If there's a single,
general list that more than a few people all use, then spammers will
be on it (or at the very least people whose machines spammers use will
be on it) and the situation is generally unchanged.
If everybody maintains their own list of people whom they will accept
email from, then email becomes much less valuable because it's no longer
a way to reach anyone who hasn't put you on their "good senders" list
or hear from anyone whom you haven't put on your "good senders" list.
Another thing cryptography can do is make it much harder (perhaps even
impossible) to spoof mail headers.  Imagine, for example, a protocol
where your machine recieves a "can I mail you?" message from some machine
out there in untrusted space, responds by sending a unique password or
key to the address in the "can I mail you?" message, and then recieves
email using that password or key.  This ensures that every piece of spam
you get must correspond to a password or key that you know where you
However, this is also of limited utility.  It hasn't actually stopped
any spam; it's just fixed it so you know whence a message comes.  How
can you use that knowledge?
If you know where spam comes from, you can send a spambounce message
that names a particular machine.  It's probably not the spammer's
machine.  It's probably just a machine out there that was running
windows or something so the spammer took it over and is sending
email from it.  The owner of the machine has no knowledge whatsoever
that his machine is trying to email you.  What will your spambounce
Here's where it all breaks down.  In some cases, we've seen people
trying to claim they'll arrange it so spambounces cost the sender
money.  But here we get to repudiation of charges; if a thousand
spambounces cost fred a thousand dollars, and all he did was run
windows and connect his machine to the internet, fred's going to
fight the charges.  He may win.  And whatever happens at that point,
it's not going to be costing the spammer any money.
In other cases, we've seen ideas for fred to post a separate bond for
everyone he sends email to; the idea being that his "can I mail you?"
message contains the address of some bank somewhere that can be
checked for the existence of the appropriate bond before the "okay you
can mail me" response goes back.  The idea here is that if fred does
not actually want to mail you, then fred will not have put up money
for the privelege of mailing you, so you will simply reject his
request.  The problem here is twofold; first, it means you have to put
up some money (amount indeterminate) for every email address you send
mail to.  This doesn't fly real well in countries with a steep
currency exchange rate.  It stops a spammer who can't get into fred's
wallet from using fred's machine to send you spam, but invites the
usual suspects to develop "integrated" mail clients that will automate
the bond-posting, enabling the spammer to get into fred's wallet.  At
that point, email fraud has escalated to financial fraud, and fred is
the victim.  The spammer who is able to get fred's machine to post
bonds can clean out fred's wallet.
There are partial solutions.  Each has problems.  As Mr. Gutman
writes, it's a social problem and doesn't really admit purely
technical solutions.  What technology can do is shift the problem
around a little, and *try* to shift the problem onto the spammers -
but the successes are always partial and in some way unsatisfactory.
Spam won't stop until spam costs the spammers money.

@_date: 2004-05-28 16:02:14
@_author: bear 
@_subject: The future of security 
Perspective on things...
Where I grew up, safety equipment inside your car (or on your head on
a motorcycle) was limited to that which prevented you from becoming
more of a hazard to *OTHER* drivers.  Motorcyclists didn't need
helmets, because helmets don't prevent crashes or change the
consequences of crashes for anyone who's not wearing them.  But they
did need eye protection, because eye protection reduced the
probability of crashes that could be dangerous to others.
I thought this was actually a well-considered system.  The law
required us to take whatever reasonable precautions we needed to
protect others from our actions, but it was entirely up to us whether
we attempted to protect ourselves from our own actions.
Now, in most states, law doesn't work this way any more -- protecting
people from each other has gotten fuzzed into the idea of protecting
"the people" (monolithic unit) from "themselves" (monolithic unit).
But I think there is some wisdom here that may apply to the spam
situation. Have partial solutions been getting rejected because we're
seeing that we can't protect users against their *own* stupidity?
What we actually need is systems to protect *other* users from their

@_date: 2004-05-30 12:36:53
@_author: bear 
@_subject: The future of security 
The bigger problem is that webs of trust don't work.
They're a fine idea, but the fact is that nobody keeps
track of the individual trust relationships or who signed
a key;  few people even bother to find out whether there's
a path of signers that leads from them to another person,
or whether the path has some reasonably small distance.
I have not yet seen an example of "reputation" favoring
one person over another in a web of trust model; it looks
like people can't be bothered to keep track of the trust
relationships or reputations within the web.

@_date: 2004-05-31 20:27:49
@_author: bear 
@_subject: The future of security 
============================== START ==============================
Does it?  If there were meaningful reputation accounting
happening, we'd be getting feedback and value judgements
from the system on the people we were corresponding with.
Have you ever seen any?
Has there been *ANY* instance of negative consequences
accruing to someone who signed the key of an entity which
later defected?  Machine-moderated or not, the web of
trust fails.
Have you seen any web-of-trust implementation that even
*considers* the trustworthiness of the key servers?  Have
you seen any web-of-trust implementation that works to
cut out defectors, but couldn't be "autospammed" to cut
out anyone you didn't like?
Sorry; but the fact is no web-of-trust implementation to
date works, or even comes close to working.

@_date: 2004-09-01 11:12:27
@_author: bear 
@_subject: Compression theory reference? 
For example, loading the message into a Linear Feedback Shift
Register and iterating until it produces one of two predetermined
messages 0 or 1?
For a nontrivial message, the last star will burn out before you
get that many iterations.  Also, as you say, in order to decode
the message you have to know how many iterations you made - a
number which will, on the average, be the same length as the
It hardly seems a worthwhile example.
That is inconsistent with the advancement of knowledge.  Any
university relying on such a principle has abandoned its duty.

@_date: 2004-09-01 11:30:04
@_author: bear 
@_subject: Compression theory reference? 
Actually you don't need to take it all the way to the
reductio ad absurdum here.  There are 1024 10-bit
messages.  There are 512 9-bit messages.  You need
to point out that since a one-to-one mapping between
sets of different ordinality cannot exist, it is not
possible to create something that will compress every
ten-bit message by one bit.
Or, slightly more formally, assume that a function C
can reversibly compress any ten-bit message by one bit.
Since there are 1024 distinct ten-bit messages, there
must be at least 1024 distinct nine-bit messages which,
when the reversal is applied, result in these 1024
messages.  There are exactly 512 distinct nine-bit
messages.  Therefore 512 >= 1024.

@_date: 2004-09-01 11:43:43
@_author: bear 
@_subject: Implementation choices in light of recent attacks? 
I believe that SHA256 with its output cut to 128 bits will be
effective.  The simplest construction is to just throw away
half the bits.

@_date: 2004-09-01 20:30:09
@_author: bear 
@_subject: ?splints for broken hash functions 
The birthday paradox does not apply in this case because H1 is fixed.
The above construction is in fact secure against the Joux attack as
stated.  2^80 work will find, on average, one collision.

@_date: 2004-09-07 08:14:51
@_author: bear 
@_subject: Spam Spotlight on Reputation 
For what it's worth, do you remember a device that was marketed on
American television called the "Ronco Pocket Fisherman?"  It was
a sort of folding fishing rod with a built-in, tiny, tacklebox,
and the idea was that here was a complete fishing rig that you
could toss into a suitcase and still have room for all your
clothes and stuff.
The fact is, as fishing gear, it was astonishingly bad.  But, as
the owner of a bait shop once explained to me after someone who
had come in with one tossed it in the trash and walked out with
a real fishing rod, "It's not made to catch fish.  It's made to
catch fishermen."
Similarly, the current generation of anti-spam technology isn't
made to catch spammers; it's made to catch ISP's and software
companies and get them to part with their money.  Alas, unlike
the Ronco Pocket Fisherman, there is no proven technology that
people can go back to after getting fed up with it not working.
It has been clear from the outset that all the solutions to spam
consisting of "building a fence around the internet and keeping
the spammers out" aren't going to work, any more than the old
anarchist-cypherpunk dream of "building a fence around our
cryptographic networks and keeping the government out" was going
to to work.  The problem in both cases is that if the information
needed to join the network is available to members of your
intended in group, it's also available to members of your
intended excluded group.
I have two patents in natural language, and a fair amount of
experience engineering in the field.  But that's a fairly recondite
skill, and these days most folks are looking for engineers for much
more prosaic tasks like interfacing their middleware with their
databases.  In the last year, I have been unemployed.  I've
turned down two job offers, though -- from software companies
with "bulk mail products", looking for natural-language guys
to build "paraphrase engines" to bypass spam filters or "copy check"
functions to estimate the likelihood of a particular message body
being filtered.  That's the level of commitment these guys are
showing.  They're actually willing to hire engineers at specialist
salaries to build new ways to bypass filters.
We should not be at all surprised, when we offer a way to
"auto-whitelist" email and therefore bypass filters at a lower
cost than hiring engineers, that they're leaping onto it at
a much higher rate than legit senders.
there that are solving some trivialized version of the problem or
some not-very-crucial aspect of the problem.  There are a lot of
systems that have a threat model that's very peculiar, and which
can be solved, however meaninglessly, while their customers still
get lots of UCE.  Indeed, there are a lot of systems out there
that don't have any published threat model.  These are failures
of protocol design, though not necessarily failures of
marketability.  But to the extent that they allow bypassing
filters, the spammers are the biggest customers.

@_date: 2004-09-11 14:53:59
@_author: bear 
@_subject: [anonsec] Re: potential new IETF WG on anonymous IPSec (fwd from 
That's pseudonymity, not anonymity.
This is just plain not true.  When operating under a pseudonym,
you are making linkable acts - linkable to each other even if
not necessarily linkable to your own official identity.  Anonymous
actions or communications are those which cannot be linked to any
other no matter how hard someone tries.
We can expect the public to fail to grasp the distinction, but
on this list "anonymous" is a very strong claim.  Anonymity is
*HARD* to do, not something that results from failing to check
a credential.

@_date: 2005-12-02 10:08:21
@_author: bear 
@_subject: Proving the randomness of a random number generator? 
"Randomness" is a quality that, intrinsically, cannot be proven.  Period.
You can take an urn with a hundred numbered balls and pull them out one
at a time -- a truly random process -- and the sequence from one to a
hundred by ones is just as likely as every other sequence.  If it happens
to come up, even that doesn't prove that it wasn't a random process.
On a practical note, I would test the PRNG's output against pattern detectors.
Spectral Analysis software is quite good at detecting patterns in PRNG output.
Then there are the pattern detectors built into various file compression
tools.  If gzip or winzip or arc or arj or (etc) can find a pattern, it
will succeed in producing a shorter file than the original.
Before you do any of that, however, check the literature to see if it's
already been done.  If you're using a commercial or cryptological PRNG
that's been studied, read the papers of the people who studied it, and
the papers of people who studied competing products.  See if they found
anything usable or any useful property that you can use to support a
claim of "randomness."  (note: it won't actually *be* randomness, for
the simple reason that that can't be proven.  But some systems have
proofs that someone who has access to the entire output of the PRNG so
far has no strategy better than random guessing for determining the
next and subsequent outputs, and that may be "random" enough for your

@_date: 2005-02-02 10:32:53
@_author: bear 
@_subject: Is 3DES Broken?  
I think you meant ECB mode?
whichever it is, as you point out there are other and more secure
modes available for using 3DES if you have a fat pipe to encrypt.

@_date: 2005-06-25 09:46:16
@_author: bear 
@_subject: Optimisation Considered Harmful 
Yep.  As a sometime compiler weenie, I'll give literal answers
to your next few questions.
No.  There's no language higher than assembly that requires that.  A
lot of compilers will do the redundant computations anyway because
the authors of those compilers considered that it would be too much
work to prove what can be eliminated, and in imperative languages
like C there's often little benefit in doing so.  But there are
transformations and techniques you can do (autoconverting to laziest
semantics allowed by the language spec) to absolutely determine what
can be eliminated, and emit code that *only* evaluates that which
does affect the output.  And increasingly, compilers are doing them.
If it changes the output, it's considered an error for C compilers to
do this.  That said, it's considered a minor error, and since a lot of
people are in a great hurry to get wrong answers, GCC will knowingly
and willingly commit this error for you (as an "extension" to the
language) if you give it a -O option that's too high.  And several
other compilers (including Borland's and Microsoft's) have committed
this error in the past without giving anyone the ability to turn it
off, or "by default" with a --fpstrict option to turn it off.
That said, in languages other than C you have to look to the language
spec to see whether this is allowed or not.  If the spec is silent on
the topic then rearranging in a way that changes output is still
usually considered a bug, but it becomes open to debate whether it is
an actual bug or not.
Some language specs (including Common Lisp, IIRC) and several
single-implementation languages (including Mathematica, IIRC) allow
arbitrary rearrangement of floating-point operations, provided that
the compiler can prove that *at least* as much precision is preserved
as would be gained by doing it in the order it appears in the source.
This is explicitly allowed to change output, provided the compiler can
prove that the changed output is a closer approximation to the
algebraic solution.  Even in languages where this is allowed, many
implementors consider it tasteless and try to avoid doing it,
conforming thier compilers' behavior to a tenet of the "principle of
least surprise."
In C, the answer depends on whether the variables are declared
volatile.  If they are not declared volatile, then the writes need not
appear in any particular order, and if the compiler can prove the
values are not used, need not appear at all.  In languages without
volatile variables, the answer is usually no.
Uh, I think the short answer is no, but you'll never be able to
tell the difference in normal circumstances.  It's common for a
superscalar chip to buffer memory writes in its L1 Cache, and
write a cache line to memory at a go (which may reorder the writes
to memory that's off the chip).  But no software running on the
same chip will be able to tell it by "reading" those addresses,
because it will snag the "written" values (even though they
haven't hit phyiscal memory yet) out of the L1 cache.
Where this can break down with the possibility that you'd be able to
tell it's breaking down, is volatile variables in multiprocessor
machines.  But these are normally protected by mutexes, the
implementation of which must trigger explicit L1 cache flushes on
multiprocessor machines.
Um.  Yup.
Certainly I haven't seen any discussion of these problems anywhere
along my four-foot bookshelf of compiler books.
Interestingly, although the timing attack is obsolete, there is a
potential power attack here.  Since power is consumed when gates
switch from 0 to 1 or vice versa, if you know the machine is
performing an add instruction and you know how much power it's eating
at that precise instant, you can probably tell how many carries it
took, because the gates that hold the carry bits are normally 0 when
the add instruction starts and switch back to 0 before it's over.
I dunno if you can separate this from the other stuff going on at
the same time, and when you got it it would be combined with the
power for changing the destination operand to the answer - but you
might be able to eliminate a lot of possible keys that way.
Right now there is no computer language (and no mathematical notation)
for writing this kind of stuff down.  Our compiler theory is mainly
based on math, our idea of program semantics almost wholly so based,
and these issues are not now modeled or tracked in any formal
semantics we use.
Further, the chip architecture used by most desktop machines is
not equipped to treat such semantic requirements correctly, and
even if we developed notation and semantic models that took these
issues into account, it would be difficult to get them to run
correctly on desktop machines.
I think the easiest way to achieve such hardware would be to
make it *constantly* leak side-channel information based on
several chaotic processes - in the hopes that attackers couldn't
isolate which parts of the information were related to your

@_date: 2005-11-19 18:03:18
@_author: bear 
@_subject: "ISAKMP" flaws? 
Actually this is a good thing.  Separation of the key distribution channel
from the flow of traffic encrypted under those keys.  Making key distribution
require human attention/intervention.  This is treating key distribution
seriously, and possibly for the first time in the modern incarnation of the

@_date: 2006-04-19 11:53:18
@_author: bear 
@_subject: Unforgeable Blinded Credentials 
Um, if it's anonymous and unlinkable, how many certificates do you
need?  I should think the answer would be "one."

@_date: 2006-01-17 10:31:33
@_author: bear 
@_subject: quantum chip built 
This is not necessarily so.  In order to factor a 1024-bit
modulus using Shor's algorithm, you would indeed need a 1024-
qbit machine.  But we haven't seen what fruit may be borne by
algorithm research and hybrid machinery; it seems plausible
that a hybrid machine may be able to use, say, 16 qbits to
divide the work factor of factoring large numbers in general
by approx. 65536.
In general, I think that until QC is a mature field, cryptographers
and cryptologists ought to assume that some QC or hybrid algorithm
or machinery that may be discovered "any minute now" can
simultaneously exploit the strengths of both QC and classical
computation.  And that means, in general, that I'd want to *add*
the number of bits factorable by Shor's algorithm in the foreseeable
future to the number of bits factorable by classical brute-force
In fact, maybe we ought to be worried about synergistic effects
and multiplying the figures together, although I can't imagine
where such effects would come from.  Let us say simply that Quantum
Computing is far from mature, and at this moment we are only
beginning to understand it.  I remember all the mechanical engineers
who proved that no heavier-than-air flying machine could exist
back in the 19th century, back when knowledge of mechanics and
materials was less precise than now...  And these guys knew what
there was to know about it.  I'm chary of people "proving" that
no n-bit factoring machine can be built just because the way they
already know to build one (Shor's algorithm, which requires n qbits)
won't work.  Given that our knowledge of QC is nascent, our
ignorance of QC's practical limits is likely staggering, and
caution is to be advised.

@_date: 2006-01-26 18:09:52
@_author: bear 
@_subject: thoughts on one time pads 
It's far easier and less error-prone to hand them a CD-ROM
full of symmetric keys indexed by date.
The problem is that most people will not take the care needed
to properly use a one-time pad.  For text communications like
this forum, they're great, and a (relatively) small amount of
keying material, as you suggest, will last for many years.
But modern applications are concerned with communicating *DATA*,
not original text; someone on the system is going to want to
send their buddy a 30-minute video of the professor explaining
a sticky point to the class, and where is your keying material
going then?  He wants to be ignorant of the details of the
cryptosystem; he just hits "secure send" and waits for magic
to happen.  Or if not a 30-minute video, then the last six
months of account records for the west coast division of the
company, or a nicely formatted document in a word processor
format that uses up a megabyte or two per page, or ...
whatever.  The OTP is nice for just plain text, but the more
bits a format consumes, the less useful it becomes.  And
fewer and fewer people even understand how much or how
little bandwidth something is; they think in terms of "human
bandwidth", the number of seconds or minutes of attention
required to read or listen to or watch something.
An OTP, as far as I'm concerned, makes a really good system,
but you have to respect its limits.  One of those limits is
a low-bandwidth medium like text-only messages, and in the
modern world that qualifies as "specialized."
Given a low-bandwidth medium, and indexing keying material
into daily chunks to prevent a system failure from resulting
in pad reuse, you get 600 MB on a CD-ROM.  Say you want a
century of secure communications, so you divide it into 8-
kilobyte chunks -- each day you can send 8 kilobytes and
he can send 8 kilobytes.  (Note that DVD-ROMs are better).
That gives you a little over 100 years (read, "all you're likely
to need, barring catastrophic medical advances,") of a very
secure low-bandwidth channel.
Of course, the obvious application for this OTP material,
other than text messaging itself, is to use it for key

@_date: 2006-01-27 12:22:06
@_author: bear 
@_subject: thoughts on one time pads 
You did not miss anything; I confirmed the OP's supposition
explicitly, and I agree with it in principle.

@_date: 2006-11-11 20:05:19
@_author: bear 
@_subject: Can you keep a secret? This encrypted drive can... 
Yep.  When costs are equal (and in this case computing power is so
cheap as to make that approximately true) any competent manager will
always pick the method which is "superior" to the other in any way.
The facts are that with AES128 or AES256, the cipher itself will *NOT*
be the weakest link in security, so the theoretical superiority of
AES256 doesn't matter much.
Anybody who is making a serious attack will have to do pretty much
exactly the same thing -- social engineering, rubberhose attack,
subpoena, password guess, protocol flaw exploit, Van Eck monitor
exploit, keyboard monitor, software backdoor exploit, DLL substitution
attack,  mem device exploit by a trojan running at the same time as
the encryption software, audio interferometry to determine keystroke
sequences, audio-frequency carrier wave interference from some metal
thing in the same office as the transmitter vibrating to the voice
that's being encrypted, etc...  There's a million different links
all weaker than the cipher itself.
Conversely, it harms nothing to have them pick the stronger cipher,
given that both ciphers are sufficiently strong that their strength
has nothing to do with the mimimum effort required to attack their

@_date: 2007-07-21 04:46:51
@_author: bear 
@_subject: How the Greek cellphone network was tapped. 
Halfway, I think.  ISTR there are laws against manufacture for sale,
sale, purchase, or most usage of such gear - but no laws against
manufacture without intent to sell, posession, or some exempted
types of use of such gear.
Basically, owning such devices is not a crime, nor is using them
provided the "target" has been duly notified that their call will be
or is being intercepted.  So you can build the gear, and you can demo
the gear you've built on a call made for purposes of demo-ing the
Consult a lawyer first, but I believe it may also be legal to monitor
calls made in a given location provided you first put up a sign that
says "all cell calls made on these premises will be monitored" etc.
But you can't legally buy or sell the equipment to do it.
The technical requirement was for a TV with a UHF analog *tuner* as
opposed to a digital channel-selection dial.  The channels that the
cellular network used (still uses?  I don't know) were inbetween the
channels that were assigned whole numbers in TV tuning.  So you could
pick up some cell traffic if you tuned, for example, to UHF TV
"channel" 78.44.  But not if you tuned to channel 78 or channel 79.

@_date: 2007-07-21 10:40:20
@_author: bear 
@_subject: How the Greek cellphone network was tapped. 
Hm.  Okay, we're looking at the same law, and I am not a lawyer
either; but I read "knowing or having reason to know ... that such
device or any component thereof has been or will be sent through the
mail or transported in interstate or foreign commerce" as a limiting
clause on what would otherwise be an unconstitutional law.
In the case of someone who manufactures and posesses such a device,
but never sends it or its components through the mail nor transports
it in interstate or foreign commerce, I don't think this law gets
broken.  Despite intimidation tactics that do their best to try to
spread the opposite impression, this is explicitly *not* forbidden by
this law.
And the statute on using such a device, IIRC, also has a limitation,
in that it bans using such devices *surreptitiously* - which I think
permits non-surreptitious use such as demonstrations.
Still, it's a case of two reasonably educated people being able to
look at the same statute and draw different conclusions: Sooner or
later it will have to be decided in a trial to see who can pay the
best lawyers^H^H^H^H^H^H^H^H^H^H^H^H^H^H^H^H^H^H^H^H see which
interpretation of the statute best serves justice.

@_date: 2007-07-24 19:51:55
@_author: bear 
@_subject: A secure Internet requires a secure network protocol 
I don't think I agree that a fully encrypted net is for a "security"
network only.  Most of the attacks we see on protocols require one
of the following properties:
  * packets can be inserted into the network that do not come from the
      machines they claim to have come from (spammers exploit SMTP)
  * packets are readable somewhere besides their intended destination.
      (criminals eavesdropping on "secure" transactions and logins)
  * packets can be easily modified in-flight
      (unethical ISPs or others exploit HTTP by inserting ads into
      documents that aren't supposed to have them).
  * authorization information is available in plaintext packets
      (killed telnet)
These are (or were) protocols that EVERYBODY uses; not just security
applications.  If someone develops SIP (Secure Internet Protocol,
in which all payload packets are encrypted end-to-end and completion
of a secure key agreement protocol is required to initiate every payload
packet stream)  then running our network on TCP/SIP solves all of these
Further, merely encrypting a network doesn't make it suitable for a
"security" network.  Although that would solve a lot of problems with
common protocol attacks, it doesn't really address authorization issues.
It makes them easier to address, but by itself it doesn't do it. We
think of security problems as being associated with machines, but
security problems are really associated with users.  What keys &c are
stored on the machine, in a "security" network, is not really
appropriate information to rely on if a different user is sitting in
front of it.
A "security" network (or VPN done right) needs two more things:
someone issuing fully revocable is-a-person credentials (indicating,
roughly, that the bearer *IS* authorized to use this particular
secure-net), and strict standards for how secured applications must
work. Every packet needs to be traceable to the is-a-person credential
(not just the machine) that was used to enter it on the network.  I
would be happiest if this credential were hardware: a passcard that
you stick into a slot in the side of the machine in order to enable
the secure-net, for example.
But you still need support from secure applications.  The applications
must guarantee that the is-a-person credential is NEVER stored on the
hard drive; your participant should have to enter it each time they
enable the secure-net.  And all the "user profile" information,
browser settings, etc, should key off the is-a-person credential.  If
someone who is not using that credential changes a setting, it should
have absolutely no effect on someone who is using that credential.
And of course, 100% standard checking is required.  Rejecting any
non-standard-conforming packet or payload is flatly necessary.
Once you have a secure-net, you kill another common problem:
  * problematic users can't be effectively and selectively banned -
    they just come back under a new pseudonym/account.
     (killing NNTP, SMTP, creating severe problems for SSL)
The secure-net, without further modification, is then effectively an
"island" within which NNTP, SMTP, FTP, SSL, HTTP, etc, can only see
other systems on the same secure-net.  I'm going to let other people
think hard about the problems of establishing and controlling gateways
between secure-nets.
SSL is dying because the trust model implemented by its key
certification process is horrifically clumsy from the user POV and the
choices it presents are not meaningful to most people nor based on
distinctions they can practically make.
No information about the signing or trust policies in use by different
signing authorities is generally available, and sadly, this is largely
because there _is_ no information to convey about it.
The sole thing that an SSL key establishes is that someone paid the
signing authority money.  Further, the signing authority is often some
random unknown person whose name doesn't even appear on the cert, but
who bought the key on a secondary market.  Further still, the SSL keys
themselves are bought and sold -- an SSL key is frequently in use by a
person other than the person whose name appears on the cert, and the
signing authorities do not track these further transfers nor revoke
transferred keys.
There is no root key whose further key-signing is controlled by any
process related to security, nor is keysigning halted or signed keys
revoked in the case of users who have perpetrated or permitted known
security abuses.
It should therefore be no surprise that SSL is nearly useless.

@_date: 2008-01-18 11:04:17
@_author: Ray Dillinger 
@_subject: Death of antivirus software imminent 
I see your point, but I can't help feeling that it's a lot like requiring all houses to be designed and built with a backdoor that the police have a key to, in order to guarantee that the police can come in to investigate crimes. The problem is that the existence of that extra door, and the inability of people to control their own keys to lock it, makes crimes drastically easier to commit.  You think police don't use DMV records to harass ex-girlfriends or make life hard for people they don't like?  You think Private investigators and other randoms who somehow "finesse" access to that data all have the best interests of the public at heart?  You think the contractor who builds the house will somehow forget where the door is, or will turn over *all* copies of the keys? And stepping away from quasi-legit access used for illegitimate
purposes, you think there're no locksmiths whose services the outright criminals can't buy?  You think the existence of a backdoor won't inspire criminal efforts to get the key (by reading a binary dump if need be) and go through it?
That is a very petty class of criminal.  While the aggregate thefts (of computer power, bandwidth, etc) are impressive, they're stealing nothing that isn't a cheap commodity anyway and the threat to lives and real property that would justify the kind of backdoors we're talking about just isn't there. Being subject to botnets and their ilk is more like the additional cost of doing business in bad weather, than it is like being the victim of a planned and premeditated crime with a particular high-value target.  Moreover, we know how to weatherproof our systems.  Seriously.  We know where the vulnerabilities are and we know how to create systems that don't have them.  And we don't need to install backdoors or allocate law enforcement budget to do it.  More than half the servers on the Internet - the very most desirable machines for botnet operators, because they have huge storage and huge bandwidth - run some form of Unix, and yet, since 1981 and the Morris Worm, you've never heard of a botnet composed of Unix machines!  Think about that!  They do business in the same bad weather as everyone else, but it costs them very little, because they have ROOFS!
I submit that the sole reason Botnet operation even exists is because so many people are continuing to use an operating system and software whose security is known to be inferior. A(nother) backdoor in that system won't help.
The criminals whose activities do justify the sort of backdoors you're talking about - the bombers, the kidnappers, the extortionists, even the kiddie porn producers and that ilk - won't be much affected by them, because they *do* take the effort to get hard crypto working in addition to standard protocols, they *do* own their own machines and get to pick and choose what software goes on them, and if they're technically bent they can roll their own protocols.

@_date: 2008-05-04 08:00:02
@_author: Ray Dillinger 
@_subject: User interface, security, and "simplicity" 
Let me restate things just to make sure I understand the problem.
You're talking about "binding IPsec credentials to a user," but I want to look at it from the point of view of exactly what problems this causes, so is the following an accurate position?
The problem is that we're trying to have entities with different security needs share a common set of authentications.  When user 'pat' and user 'sandy' have different security needs (different authorized
or trustable communication partners, to start with) we can't give them IPSEC because IPSEC operates on channels between machines rather than on channels between trusting/trusted entities.   Even if 'pat' and 'sandy' both have a trusted/trusting entity on a given
remote machine from theirs, IPSEC fails them because it cannot
differentiate between the various entities (users, agents, services)
using that remote machine, when 'pat' and 'sandy' need it to.  Similarly, it fails the entities on that remote machine because it cannot differentiate between 'pat', 'sandy' and any other entities using the local machine, when trust relationships might exist only
for some subset of those entities.

@_date: 2008-11-05 21:14:37
@_author: Ray Dillinger 
@_subject: Bitcoin P2P e-cash paper 
I think the real issue with this system is the market for bitcoins.  Computing proofs-of-work have no intrinsic value.  We can have a limited supply curve (although the "currency" is inflationary at about 35% as that's how much faster computers get annually) but there is no demand curve that intersects it at a positive price point.
I know the same (lack of intrinsic value) can be said of fiat currencies, but an artificial demand for fiat currencies is created by (among other things) taxation and legal-tender laws.  Also, even a fiat currency can be an inflation hedge against another fiat currency's higher rate of inflation.   But in the case of bitcoins the inflation rate of 35% is almost guaranteed by the technology, there are no supporting mechanisms for taxation, and no legal-tender laws.  People will not hold assets in this highly-inflationary currency if they can help it.

@_date: 2008-11-14 18:20:23
@_author: Ray Dillinger 
@_subject: Bitcoin P2P e-cash paper 
Okay.... I'm going to summarize this protocol as I understand it. I'm filling in some operational details that aren't in the paper by supplementing what you wrote with what my own "design sense" tells me are critical missing bits or "obvious" methodologies for First, people spend computer power creating a pool of coins to use as money.  Each coin is a proof-of-work meeting whatever criteria were in effect for money at the time it was created.  The time of creation (and therefore the criteria) is checkable later because people can see the emergence of this particular coin in the transaction chain and track it through all its "consensus view" spends.  (more later on coin creation tied to adding a link). When a coin is spent, the buyer and seller digitally sign a (blinded) transaction record, and broadcast it to a bunch of nodes whose purpose is keeping track of consensus regarding coin ownership.  If someone double spends, then the transaction record can be unblinded revealing the identity of the cheater.  This is done via a fairly standard cut-
and-choose algorithm where the buyer responds to several challenges with secret shares, and the seller then asks him to "unblind" and checks all but one, verifying that they do contain secret shares any two of which are sufficient to identify the buyer.  In this case the seller accepts the unblinded spend record as "probably" containing a valid secret share. The nodes keeping track of consensus regarding coin ownership are in a loop where they are all trying to "add a link" to the longest chain they've so far recieved.  They have a pool of reported transactions which they've not yet seen in a "consensus" signed chain.  I'm going to call this pool "A".  They attempt to add a link to the chain by
moving everything from pool A into a pool "L" and using a CPU-
intensive digital signature algorithm to sign the chain including the new block L.  This results in a chain extended by a block containing all the transaction records they had in pool L, plus the node's digital signature.  While they do this, new transaction records continue to arrive and go into pool A again for the next cycle of work. They may also recieve chains as long as the one they're trying to extend while they work, in which the last few "links" are links that are *not* in common with the chain on which they're working.
These they ignore.  (?  Do they ignore them?  Under what circumstances would these become necessary to ever look at again, bearing in mind that any longer chain based on them will include them?) But if they recieve a _longer_ chain while working, they immediately check all the transactions in the new links to make sure it contains no double spends and that the "work factors" of all new links are appropriate.  If it contains a double spend, then they create a "transaction" which is a proof of double spending, add it to their pool A, broadcast it, and continue work.  If one of the "new" links has an inappropriate work factor (ie, someone didn't put enough CPU into it for it to be "licit" according to the rules) a new "transaction" which is a proof of the protocol violation by the link-creating node is created, broadcast, and added to pool A, and the chain is rejected.  In the case of no double spends and appropriate work factors for all links not yet seen, they accept the new chain as consensus. If the new chain is accepted, then they give up on adding their
current link, dump all the transactions from pool L back into pool A (along with transactions they've recieved or created since starting work), eliminate from pool A those transaction records which are already part of a link in the new chain, and start work again trying to extend the new chain. If they complete work on a chain extended with their new link, they broadcast it and immediately start work on another new link with all the transactions that have accumulated in pool A since they began work.  Do I understand it correctly?
Biggest Technical Problem: Is there a mechanism to make sure that the "chain" does not consist solely of links added by just the 3 or 4 fastest nodes?  'Cause a broadcast transaction record could easily miss those 3 or 4 nodes and if it does, and those nodes continue to dominate the chain, the transaction might never get added.  To remedy this, you need to either ensure provable propagation of
transactions, or vary the work factor for a node depending on how many links have been added since that node's most recent link.   Unfortunately, both measures can be defeated by sock puppets.  This is probably the worst problem with your protocol as it stands right now; you need some central point to control the identities (keys) of the nodes and prevent people from making new sock puppets. Provable propagation would mean that When Bob accepts a new chain from Alice, he needs to make sure that Alice has (or gets) all
transactions in his "A" and "L" pools.  He sends them, and Alice sends back a signed hash to prove she got them. Once Alice has recieved this block of transactions, if any subsequent chains including a link added by Alice do not include those transactions at or before that link, then Bob should be able to publish the block he sent Alice, along with her signature, in a
transaction as proof that Alice violated protocol.  Sock puppets defeat this because Alice just signs subsequent chains using a new key, pretending to be a different node. If we go with varying the work factor depending on how many new links there are,  then we're right back to domination by the 3 or 4 fastest nodes, except now they're joined by 600 or so sock puppets which they use to avoid the work factor penalty. If we solve the sock-puppet issue, or accept that there's a central point controlling the generation of new keys, then generation of coins should be tied to the act of successfully adding a block to the "consensus" chain.  This is simple to do; creation of a coin is a transaction, it gets added along with all the other transactions in the block.  But you can only create one coin per link, and of course if your version of the chain isn't the one that gets accepted,
then in the "accepted" view you don't have the coin and can't spend it.  This gives the people maintaining the consensus database a reason to spend CPU cycles, especially since the variance in work factor by number of links added since their own last link (outlined
above) guarantees that everyone, not just the 3 or 4 fastest nodes,
occasionally gets the opportunity to create a coin.
Also, the work requirement for adding a link to the chain should vary (again exponentially) with the number of links added to that chain in the previous week, causing the rate of coin generation (and therefore inflation) to be strictly controlled.  You need coin aggregation for this to scale.  There needs to be a "provable" transaction where someone retires ten single coins and creates a new coin with denomination ten, etc.  This is not too hard, using the same infrastructure you've already got; it simply becomes part of the chain, and when the chain is accepted
consensus, then everybody can see that it happened.

@_date: 2008-11-14 23:04:21
@_author: Ray Dillinger 
@_subject: Bitcoin P2P e-cash paper 
Okay, that's surprising.  If you're not using buyer/seller identities, then you are not checking that a spend is being made by someone who actually is the owner of (on record as having recieved) the coin being spent.  There are three categories of identity that are useful to think about.  Category one: public.  Real-world identities are a matter of record and attached to every transaction.  Category two: Pseudonymous.  There are persistent "identities" within the system and people can see if something was done by the same nym that did something else, but there's not necessarily any way of linking the nyms with real-world identities.  Category three: unlinkably anonymous.  There is no concept of identity,
persistent or otherwise.  No one can say or prove whether the agents involved in any transaction are the same agents as involved in any other transaction. Are you claiming category 3 as you seem to be, or category 2?
Lots of people don't distinguish between anonymous and pseudonymous protocols, so it's worth asking exactly what you mean here.  Anyway:  I'll proceed on the assumption that you meant very nearly (as nearly as I can imagine, anyway) what you said, unlinkably anonymous.  That means that instead of an "identity", a spender has to demonstrate knowledge of a secret known only to the real owner of the coin.  One way to do this would be to have the person recieving the coin generate an asymmetric key pair, and then have half of it published with the transaction.  In order to spend the coin later, s/he must demonstrate posession of the other half of the asymmetric key pair, probably by using it to sign the key provided by the new seller.  So we cannot prove anything about "identity", but we can prove that the spender of the coin is someone who knows a secret that the person who recieved the coin knows. And what you say next seems to confirm this: Note, even though this doesn't involve identity per se, it still makes the agent doing the spend linkable to the agent who earlier recieved the coin, so these transactions are linkable.  In order to counteract this, the owner of the coin needs to make a transaction, indistinguishable to others from any normal transaction, in which he creates a new key pair and transfers the coin to its posessor (ie, has one sock puppet "spend" it to another). No change in real-world identity of the owner, but the transaction "linkable" to the agent who spent the coin is unlinked.  For category-three unlinkability, this has to be done a random number of times - maybe one to six times?  BTW, could you please learn to use carriage returns??  Your lines are scrolling stupidly off to the right and I have to scroll to see what the heck you're saying, then edit to add carriage returns before I respond. Mmmm.  I don't know if I'm comfortable with that.  You're saying there's no effort to identify and exclude nodes that don't cooperate?  I suspect this will lead to trouble and possible DOS attacks. Okay, when you say "same" transaction, and you're talking about transactions that are obviously different, you mean a double spend, right?  Two transactions signed with the same key?
Until.... until what?  How does anybody know when a transaction has become irrevocable?   Is "a few" blocks three?  Thirty?  A hundred?  Does it depend on the number of nodes?  Is it logarithmic or linear in number of nodes?  But in the absence of identity, there's no downside to them if spends become invalid, if they've already recieved the goods they double-spent for (access to website, download, whatever).  The merchants are left holding the bag with "invalid" coins, unless they wait that magical "few blocks" (and how can they know how many?) before treating the spender as having paid.  The consumers won't do this if they spend their coin and it takes an hour to clear before they can do what they spent their coin on. The merchants won't do it if there's no way to charge back a customer when they find the that their coin is invalid because the customer has doublespent.
So there's a possibility of an early catch when the broadcasts of the initial simultaneous spends interfere with each other.  I assume here that the broadcasts are done by the sellers, since the buyer has a possible disincentive to broadly disseminate spends. Okay, that's a big difference between a proof of work that takes a huge set number of CPU cycles and a proof of work that takes a tiny number of CPU cycles but has a tiny chance of success.  You can change the data set while working, and it doesn't mean you need to start over. This is good in this case, as it means nobody has to hold recently recieved transactions out of the link they're
working on.
Right.  That was the misconception I was working with.  Again, the difference between a proof taking a huge set number of CPU cycles and a proof that takes a tiny number of CPU cycles but has a tiny chance of success.
It's like a random variation in the work factor; in this way it works in your favor. I don't understand how "transaction fees" would work, and how the money would find its way from the agents doing transactions to those running the network.  But the economic effect is the same (albeit somewhat randomized) if adding a link to the chain allows the node to create a coin, so I would stick with that.
Also, be aware that the compute power of different nodes can be expected to vary by two orders of magnitude at any given moment in history.

@_date: 2008-10-31 14:33:09
@_author: Ray Dillinger 
@_subject: Who cares about side-channel attacks? 
============================== START ==============================
But isn't that the attacker's job?  We will never arrive at anything
secure - or even *learn* anything about how to build real security - if attackers leave any part of it untested or consistently fail to try particular approaches.  As far as I can see the "acid tests" of the real world, hammering away with anything they've got, are exactly
the kind of environment that security pros have to design for in the long run.  We should be trying to identify products and implementations that hold up under this kind of assault, and then publishing books about the design processes and best practices that produced them.  Knowing full well that Kerchoff's Principle is alive and well, and that the people doing the attacks will be first in line to buy the books.  The point is that if the material in the books is any good, then having the books shouldn't help them. Cipher suites and protocols and proofs and advanced mathematics are well and good, but we have to recognize that they are only a small part of actually building a secure implementation.  Holding up under diverse assault *is* the desired property that we are all supposed to be working toward, and this kind of diverse assault is exactly the sort of test we need to validate security design processes.

@_date: 2009-08-19 21:11:21
@_author: Ray Dillinger 
@_subject: Client Certificate UI for Chrome? [OT anonymous-transaction 
[Moderator's note: this is getting a bit off topic, and I'd prefer to
limit followups. --Perry]
No.  This juvenile fantasy is complete and utter nonsense, and I've heard people repeating it to each other far too often.  If you repeat it to each other too often you run the risk of starting
to believe it, and it will only get you in trouble.  This is a world that has not just cryptographic protocols but also laws and rules and a society into which those protocols must fit.  That stuff doesn't all go away just because some fantasy-world conception of the future of commerce as unlinkable anonymous
transactions says it should.  In any transaction involving physical goods, the seller also wants to know to whom to ship the product.  Since the laws in most nations do not require the recipient of an erroneous shipment to return the goods and *do* require the seller to give back the buyer's money
if the shipment doesn't go where the buyer wants it, sellers really care that the correct recipient will receive the package and really need some way to contact the buyer in case there's a mistake about the recipient address or identity.  Otherwise you'd get people playing silly buggers with the shipping address to get out of paying for million-dollar equipment.
The law usually requires that the recipient of defective goods or services has the ability to return those goods for a refund or obtain a refund in the event of seller nonperformance of services or nonshipment of goods.  Since such returns can be used to launder money from illegal enterprises, laws usually restrict anonymous returns. Therefore the seller needs the buyer's (or client's) identity in order to comply with the law.
In information-based transactions involving IP that's subject to copyright or trade secret protection (which is effectively all of them since other IP can be had for free) the seller also wants to know who is the licensee that's bound by the terms of the license and who now poses a "risk" of copyright breakage.  In both cases this is a liability taken on by the buyer, and not something that his "money being good" for just the transaction price can ameliorate.
In financial transactions The seller also wants to know that s/he can comply with, eg, "know your customer" laws and avoid liability
for gross negligence in, eg, money laundering cases.  In many transactions the seller wants the buyer's identity and a liability waiver signed by the buyer so as to keep track of or avoid liability for what the customer is going to do with his/her
products.  Most sellers want the ability to offer the buyer credit terms,
especially when large sums are involved.  And even where money is supposedly firm (like the money Bernie Madoff's clients had in their accounts) it is subject to catastrophic vanishment in
extraordinary circumstances.  The seller needs to know whom to sue or at least whose name to put on the forms for their insurance claim if contrary to expectations the buyer's money turns out not to be good.
If the cert authority does not provide the identity of the buyer but asserts that the buyer's money is good, and this turns out not to be true (as in the case of Madoff's clients), then in most legal systems the cert authority is either liable, or can expect to be sued in a very expensive empirical test of liability.  So the cert authority doesn't want to be in the business of vouching for the ability of anonymous people to pay.  The only way for the money to be truly firm for these purposes is that the cert authority has it in escrow.  This makes the cert authority a financial institution and therefore subject to "know your customer" mandatory reporting, data retention laws,
subpeonas, and so on.  Also, it introduces a needless delay and complication to the transaction that legitimate buyers and sellers would mostly rather not have. Also, in any large transaction the seller or cert authority or both must retain buyer identity information in order to be able to comply with subpeonas, inquests, or equivalent writs, for periods ranging from zero in a few undeveloped african nations to five years in much of the rest of the world. In most of the nations on earth, there is such a thing as sales tax or use tax on goods or services, and any transaction involving
more than a tiny sum must be reported (with the names of buyer and
seller) to relevant tax authorities.  Even tiny transactions must be reported in aggregate, although these usually don't require the buyers' names. Since the seller has the legal obligation to report, s/he also has the legal obligation to collect identity information from his/her clients.
Most nations are very sensitive about cross-border money flows,
have tax laws that apply specifically to international transactions, and want to know such things as the buyer and seller identity.  In this case it is the legal obligation of both buyer and seller
in international transactions to collect whatever information their particular nation requires them to have and report it according to their particular nation's laws.
And so on.
Maybe in a cypherpunk world where there are no laws other than the natural laws of mathematics, no physical world in which goods have to be manufactured and delivered, no national borders or third parties having a tax or legal interest in transactions, no information other than valuable secrets subject to no post-sale copyrights or licensing, no liability laws or customers-rights laws whatsoever, no taxation, and a bunch of other bizarro-world conditions, the seller would not need anything more than the knowledge that the buyer's money was good.  But that's like proving that a pig can fly starting from an assumption of an ideal, spherical pig of zero mass.  It is not the world in which we live, unless we are black-marketeers in international waters, not subject to the laws of any nation.
If you make it "optional" - where people can request a true name etc when they need it to comply with law, but don't have to request it otherwise - you will find that the number of sellers willing to do business with anonymous buyers, and the number of transactions in which they legally can do business with anonymous buyers, starts low and then drops rapidly as legal troubles and scams of various kinds,  as well as new laws designed to prevent those troubles and scams, catch up to the sellers.
Anyway, nothing's preventing you from building your "unlinkable" cert
system to compete with other forms of commerce.  But in the presence of any other system whatsoever, I expect almost no one to use it and
predict that using it or running services that allow people to use it will rapidly become illegal in all developed nations.
Perhaps there are other ways to achieve all of the requirements for a
system that people can use while complying with applicable laws.  I
cannot think of a simpler or more useful one.
Aside from being irrelevant because ebay does not function as a buyer or seller, and only minimally as a cert authority in their client's auctions (in particular they do NOT vouch for anyone's ability to pay), this is blatantly false.  Ebay cares about true names, and linkable information such as bank account
numbers.  Without them it won't let you use its payment system. Also, try funding an ebay seller's account using just cash somehow and tell me how it goes.  It used to be possible but it's been several years since the law bounced on ebay for allowing that and
commanded them to collect true name information from all sellers.
Also remember ebay has to collect its fee from somebody and until the auction's conclusion doesn't know how large that fee is going to be.  They insist on knowing who that somebody is.

@_date: 2009-02-25 10:04:40
@_author: Ray Dillinger 
@_subject: Security through kittens, was Solving password problems 
There is no such thing as a "benign" web cache for secure pages.
If you detect something doing caching of secure pages, you need to shut them off just as much as you need to shut off any other

@_date: 2009-01-30 13:47:23
@_author: Ray Dillinger 
@_subject: UCE - a simpler approach using just digital signing? 
I have a disgustingly simple proposal.  It seems to me that one of the primary reasons why UCE-limiting systems fail is the astonishing complexity of having a trust infrastructure maintained by trusted third parties or shared by more than one user.  Indeed, "trusted third party" and "trust shared by multiple users" may be oxymorons here. Trust, by nature, is not really knowable by any third party, not the same for any set of more than one user, and in fact the people most willing to pay for it at least where UCE is concerned, experience shows to be usually the _least_ trustworthy parties.  So why hasn't anybody tried direct implementation of user-
managed digital signatures yet?
A key list maintained by individual recipients for themselves alone could be astonishingly simpler in practice, probably to the point of actually being practical.  In fact, it is _necessary_ to eliminate third parties and shared infrastructure almost entirely in order to allow mail recipients to have the kind of fine-grained control that they actually need to address the problem by creating social and business feedback loops that promote good security.
As matters stand today, there is no protection from UCE. If I know there is a user account named 'fred' on the host
'example.com', then I have an email address and I can send all the UCE I want.  And poor fred has the same email address he gives everybody, so he gets spam from people who've gotten his address and he has no idea where they got it.  All his legitimate correspondents are using the same email address, so he can't abandon it without abandoning *all* of them, and he doesn't know which of them gave his address to the spammers.  What if email accounts weren't that simple?  Consider the implications of a third field, or "trust token," which works like a "password" to fred's mail box.  Your mailer's copy of fred's email address would look like "fred at example.com" where "token" was a field that was your own personal password to fred's mailbox.  Your system would still send mail to "fred at example.com" but it would include a "Trust:" header based on the token.
The simplest solution I can think of would be a direct application of digital signatures;  the trust token would be (used as) a cryptographic key, and the headers of any message would have to include a "Trust" field containing a digital signature (a keyed cryptographic hash of the message, generated by that key).  Messages to multiple recipients would need to contain one "Trust" field per recipient. Its use would follow simple rules:  Each time Fred gives out his email address to a new sender, he creates a trust token for that sender.  They must use it when they send him mail.  So fred gives his bank a key when he gives them his email address.  If fred were willing to recieve mail from strangers, he could publish a trust token on his webpage or on usenet or whatever - it would be painless to revoke it later, so why not?  If fred trusted someone to give out his email address, he could give that person multiple trust tokens to pass along to others.  Again, an error in judgement would be painless to revoke later.
Fred can revoke any trust token from his system at any time, and does so whenever he gets spam with a trust token he issued.  In UI terms there'd be a button in his mail reader that works as, "this message is spam, so revoke this trust token because now a spammer has it."  Other messages sent with the same trust token would disappear from his mailbox instantly. Fred might not push this button every time, but at least he'd know what spam he was getting due to (say) his published trust token
on his webpage or usenet, and what spam he was getting due to his relationship with a bank, and he'd have the option of turning any source of spam off instantly. In the short run the .aliases file on the mail host would need a line so it would know to deliver mail to "fred at example.com"
to fred.  This is not because a legitimate email would ever include
the literal key, but for purposes of alerting fred's MUA to protocol
breaches, so it could do key management.  Fred's MUA could then be upgraded to use tokens without affecting other users on the
system.    In later MDA's that handle trust tokens directly, this forwarding would be automatic. Whenever Fred gets email sent by someone using a trust token, his system tells him which token - ie, what sender he gave that trust token to.  So email sent to fred using the trust token he gave his bank will show up in his mailbox under a heading that says "this was sent by someone using the trust token you gave your bank."
Whenever fred gets email for "fred at example.com" and that's still a legitimate token, his system revokes the token, sends him an automatic note that says which trust token was revoked, and bounces the email with a message that says,     "Your mailer is not using trust tokens.  Your mail has not been
    delivered and the trust token you used has been automatically     revoked because your mailer sent it in cleartext instead of using
    it correctly.  Please update your mailer and contact fred via     some other channel to obtain a new trust token."  Whenever fred gets email for "fred at example.com" and it's not a legitimate token anymore, it bounces with the same message, but doesn't generate another note to fred.  Fred can see the status of
all his tokens (live or revoked) when he looks at his address book.
Whenever Fred gets email with no trust token, his system bounces it with a message that says     "Contact fred using some other means to get a trust      token for email." Whenever Fred gets email with a revoked trust token, his system bounces it with a message that says,    "Sorry, this token is known to have been compromised; if     you're a legitimate sender then either you've sent mail     using a mailer that doesn't handle trust tokens correctly     or somehow a spammer has read your email database. If     you are a legitimate sender and you've updated your mailer,     repaired your data security, or stopped selling your mailing     list to spammers, whichever is applicable to your situation,
    then please contact fred using some other means to get a new     trust token."
If fred gets UCE under his bank's trust token, then he can revoke that token, and have a few words with the bank about their information security and how some spammer was able to get his hands on that trust token.  This would promote early detection of breaches of information security and suborned machines, and also rapid detection of institutions that sell email information to spammers in violation of the users' trust.  The bank would have a motive it now lacks to keep spammers out of its email database;  every address/token combination used to send spam would instantly stop working, requiring them to resort to (relatively) expensive paper mail or use the phone in order to contact their legit customer.  And if it happens again, and again, and again, then fred is likely to do his banking elsewhere in the future. To me, this feedback is the invaluable missing link that all the third-party trust and shared-trust schemes I've seen lack.  When fred maintains his own trust keyring, both fred and the bank know where the spammer got the trust token. Also, both of them know the other will know where the spammer got the trust token.  A spammer getting the trust token fred gave the bank will cost the bank money, customer goodwill, and potentially business.  And fred and the bank must have contact with each other about it if there is still a mutual wish to reestablish email communication.  Machines suborned by spammers would be detected instantly because all the trust tokens would be revoked within minutes of the spammer using them.  Spammers could not share trust tokens around to get thousands of spam delivered on each
one, because fred would revoke the token the first time he got spam on it and all the other spam would disappear. This would destroy spammers' motive to cooperate by selling each
other spamlists.  And when fred did revoke a key, none of his legitimate email still arriving under non-compromised tokens would be affected.  Literally *EVERY* piece of spam would be evidence of an information security breach traceable to a single entity that fred gave his address to, and every time spam were recieved there would be a single action that fred could take to stop getting spam derived from that particular information security breach.
This is basic digital signatures; it would work.  We know how to do it.  We've known how to do it for a long time.  People would need to add one field to their email information to keep trust tokens.  Everything else you could implement in the Mail User Agents.   My inner cynic says nobody's implemented it because nobody can figure a way to make money by selling trust when users manage their own keyrings rather than paying a third party to manage keyrings for them.  Can anybody else think of a better reason, or are we really just lazy greedy bums who can't be bothered to work on something unless we can "make money fast" doing it?

@_date: 2009-07-06 14:59:46
@_author: Ray Dillinger 
@_subject: MD6 withdrawn from SHA-3 competition 
I think "resistance to attacks" (note absence of any restrictive
adjective such as "differential") is a very important property (indeed, one of the basic defining criteria) to demonstrate in a hash algorithm.  If someone can demonstrate an attack, differential or otherwise, or show reason to believe that such
an attack may exist, then that should be sufficient grounds to eliminate a vulnerable candidate from any standardization competition. In other words, the fact that MD6 can demonstrate resistance to a class of attacks, if other candidates cannot, should stand in its favor regardless of whether the competition administrators say anything about proving resistance to any particular *kind* of attacks.  If that does not stand in its favor then the competition is exposed as no more than a misguided effort to standardize on one of the many Wrong Solutions.

@_date: 2009-03-04 22:15:30
@_author: Ray Dillinger 
@_subject: Judge orders defendant to decrypt PGP-protected laptop 
The law is not administered by idiots.  In particular, the law is not administered by people who are more idiotic than you.  You may disagree with them, or with the law, but that does not make them stupid.
On the one hand there are (inevitable) differences in profile
between a partition that sees daily use and a partition that doesn't.  If a forensics squad had a good look at my laptop, they'd see that my (unencrypted) Windows partition has not been booted or used in three years, whereas file dates, times, and contents indicate that one of the other partitions is used daily.
If he decrypts a partition that clearly does not get used frequently, and more to the point shows no signs of having been used on a day when it is known that the laptop was booted up,
then he is clearly in violation of the order.
More to the point, you're arguing about a case where they have testimony from multiple officers who have *SEEN* that the images are on the computer, where both defense and prosecution agree that they do not enjoy fifth-amendment priveleges, and where the testomony of multiple officers gives the partition name ("Z drive") in which the images were found.  If the decrypted partition does not match in these particulars, and especially if it does not show any evidence of usage while the laptop is known to have been powered up during the initial search, then the defendant is clearly in violation of the order. Now, I think there is a legitimate argument to be made about whether the defendant can be compelled to *use* a key which he has not got written down or otherwise stored anywhere outside his own head.  It's generally agreed that people can't be compelled to produce or disclose the existence of memorized keys, but can be compelled to produce or disclose the existence of any paper or device on which a key is recorded.  But regardless, if the order to use the key is considered legit, then failure to comply with the order (by using a different or "wrong" key, unlocking a different volume) is direct violation of a court order.  People go to jail for that.
Keep in mind that the right to be secure from search and seizure of one's documents has always been subject to due process and court orders in the form of search warrants.  The right to privacy is not an absolute right and never has been, and obstructing the execution of a lawfully served warrant is not a viable strategy
for staying out of jail.
            (neither a lawyer, nor, usually, an idiot)

@_date: 2009-05-01 17:29:29
@_author: Ray Dillinger 
@_subject: [tahoe-dev] SHA-1 broken! (was: Request for hash-dependency in 
I cannot derive a realistic threat model from the very general
statements in the slides. In the case of, for example, the Debian organization, which uses SHA-1 keys to check in code so that it's always clear with a distributed network of developers who made what changes, What threats must they now guard against and what corrective measures ought they take?
Can a third-party attacker now forge someone's signature and check in code containing a backdoor under someone else's key?  Such code could be loaded on a "poisoned" server, downloaded, and executed on millions
of target machines with devastating effect and no way to catch the Can a rogue developer now construct a valid code vector B, having the same signature as some of his own (other) code A, thus bypassing
the signature check and inserting a backdoor?  The scenario is the same with a "poisoned" server but, once detected, the attacker would be identifiable.
Is it the case that a constructed hash collision between A and B can be done by a third party but would be highly unlikely to contain any executable or sensible code at all?  In this case the threat is serious, but mainly limited to vandalism rather than exploits.
Is it the case that a constructed hash collision between A and B can only be done by the developer of both A and B, but would be highly unlikely to contain any executable or sensible code at all?  In this case the threat is very minor, because the identity of the "vandal" would be instantly apparent.

@_date: 2009-05-26 13:37:49
@_author: Ray Dillinger 
@_subject: consulting question.... 
At a dinner party recently, I found myself discussing the difficulties of DRM (and software that is intended to implement it) with a rather intense and inquisitive woman who was very knowledgeable about what such software is supposed to do, but simultaneously very innocent of the broad experience of such things that security people have had.  She was eager to learn, and asked me to summarize what I said to her in an email. So I did.... And it turns out that she is an executive in a small company which is now considering the development of a DRM product.  I just got email
from her boss (the CEO) offering to hire me, for a day or two anyway, as a consultant.  If I understand correctly, my job as
consultant will be to make a case to their board about what hurdles of technology and credibility that small company will find in its path if it pursues this course.
So now I need to go from "Dinner party conversation" mode to
"consultant" mode and that means I need to be able to cite specific
examples and if possible, research for the generalities I explained over dinner.  I'll be combing Schneier's blog and using Google to fill in details of examples I've already cited to get ready for this, but any help that folks could throw me to help illustrate and demonstrate my points (the paragraphs below) will be much
I explained to her that the typical experience of "monitored" or
"protected" software (software modified for DRM enforcement) is that
some guy in a randomly selected nation far outside the jurisdiction of your laws, using widely available tools like debuggers and hex editors, makes a "cracked" copy and distributes it widely, and that current efforts in the field seem more focused on legislation and international prosecutions than on software technology.  Software-
only solutions, aside from those involving a "Trusted Computing Module"
(which their proposed project does not - She seemed unaware of both the Trusted Computing Platform and the controversy over it) are no
longer considered credible.  I cited the example of DeCSS, whose crack of players for DRM'd movies used techniques generally applicable to any form of DRM'd software. I explained that in the worst case, such software works by making unacceptable compromises of security or autonomy on the machines where
it is installed, citing the infamous and widespread Sony Rootkit, (and IMO also the TCM system, but I didn't go into that messa worms at
dinner) and that these compromises usually become public and do serious damage to both the credibility of DRM systems generally and
the cash flow of the companies that perpetrate them (ISTR Sony wound
up losing something over 6 million in the US judgement alone on that one, and spent considerably more than that on legal fees in the US and several other nations). Finally, I explained the "cheap" attacks available to a sysadmin who does not want his DRM'd software reporting its usage statistics; for example having a firewall that filters outgoing packets. Does anyone feel that I have said anything untrue?
Can anyone point me at good information uses I can use to help prove the case to a bunch of skeptics who are considering throwing away their hard-earned money on a scheme that, in light of security
experience, seems foolish?

@_date: 2009-05-27 01:07:44
@_author: Ray Dillinger 
@_subject: consulting question.... (DRM) 
It's a software company. Its customers would be other software companies that want to produce "monitored" applications.  Their product inserts program code into existing applications to make those applications monitor and report
their own usage and enforce the terms of their own licenses, for example disabling themselves if the central database indicates that their licensee's subscription has expired or if they've been used for more hours/keystrokes/clicks/users/machines/whatever in the current month than licensed for.
The idea is that software developers could use their product instead
of spending time and programming effort developing their own license-
enforcement mechanisms, using it to directly transform on the
executables as the last stage of the build process.
The threat model is that the users and sysadmins of the machines where the "monitored" applications are running have a financial motive to prevent those applications from reporting their usage.
They are in the US.  Their potential customers are international.
And their customers' potential clients (the end users of the "monitored" applications) are of course everywhere. You're taking a very polarized view.  These aren't "DRM fanatics"; they're business people doing due diligence on a new project, and
likely never to produce any DRM stuff at all if I can successfully
convince them that they are unlikely to profit from it.

@_date: 2009-05-27 09:12:10
@_author: Ray Dillinger 
@_subject: consulting question.... 
This matches my experience as well.  "Have any exploits of this particular scheme been found in the wild?" is always one of the first three questions, and the answer is one of the best predictors
of whether the questioner actually does anything.  For best results one must be able to say something like, "Yes, six times in the last year" and start naming companies, products, dates, and independent sources that can be used to verify the incidents.  To really make the point one should also be able to cite financial costs and losses incurred.
Because companies don't like talking about cracks and exploits involving their own products, nor support third parties who attempt
systematic documentation of same, it is frequently very hard to produce sufficient evidence to convince and deter new reinventors of the same technology. This failure to track and document exploits
and cracks is a cultural failure that, IMO, is currently one of the
biggest nontechnical obstacles to software security.

@_date: 2009-11-22 11:18:28
@_author: Ray Dillinger 
@_subject: Crypto dongles to secure online transactions 
So the model of interaction is:     Software displays transaction on the screen (in some
         predetermined form)
    Device reads the screen, MACs the transaction, and          displays MAC to user
    User enters MAC to confirm transaction
    Transaction, with MAC, is submitted to user's bank.
    Bank checks MAC to make sure it matches transaction
       and performs transaction if there's a match.
Malware that finds the user account details lying around on the hard drive cannot form valid MACs for the transactions
it wants to use those details for, so the user and the bank are protected from "credit card harvesting" by botnets. Malware that attempts to get a user authorization by displaying
a different transaction on the screen is foiled by not being able to MAC the transaction it's really trying to do.  etc.
But a four or six digit MAC isn't nearly enough. You see, there's still the problem of how you handle fraudulent transactions.  If the black hats start submitting transactions
with random MACs in the certain knowledge that one out of ten thousand four-digit MACs will be right, all that happens is that they have to invest some bandwidth when they want to drain
your account.  They will do it, because it's more profitable than sending spam.  If there is some "reasonable" control like freezing an account after a thousand attempts to make fraudulent transactions or not accepting transaction requests within twenty seconds after an attempt to make a fraudulent transaction on the same account, then you have created an easy denial-of-service attack that can be used to deny a particular person access to his or her bank account at a particular time of the attacker's choosing.  Denying someone access to their money makes them vulnerable at an unexpected time - they can't get a hotel room, they can't get a cab, they can't get a plane home, they can't buy gas for their car and get stranded somewhere, and they become easy pickings for physical crimes like assault, rape, theft of vehicle, etc. That's not acceptable.
In order to be effective the MAC has to make success so unlikely
that submitting a fraudulent transaction has a higher cost than its amortized benefit.  Since the botnets are stealing their electricity and bandwidth anyway, the absolute cost to black hats of submitting a fraudulent transaction is very very close to zero.  What we have to look at then is their opportunity cost.  Consider the things a botted machine could do with a couple kilobytes of bandwidth and a couple milliseconds of compute time.  It could send a spam or it could send a fraudulent transaction to a bank with a random MAC.  It will do whichever is considered most profitable by the operator of the botnet.  Note: with spamming there's almost no chance of arrest.  Receiving
money via a fraudulent transaction submitted to a bank *MIGHT* be made more risky, so if that actually happens then there's an additional risk or cost associated with successful fraud attempts, which I don't account for here. But ignoring that because I don't know how to quantify it: In late 2008, ITwire estimated that 7.8 billion spam emails were generated per hour. (
Consumer reports estimates consumer losses due to phishing at a quarter-billion dollars per year. Check my math, but If we believe those sources, then that puts the return on sending one spam email at 1/546624 of a dollar, or about one point eight ten-thousandths of a penny. If we can make submitting a fraudulent transaction return less than that, then the botnets go on sending spams instead of submitting fraudulent transactions and our banking infrastructure is relatively safe. For now.  (Just don't think about our email infrastructure - it's depressing). If a fraud attack seeks to drain the account, it'll go for about the maximum amount it expects the bank to honor, which means, maybe, a couple thousand dollars (most checking
accounts have overdraft protection that allows a fraudster to drain a credit card up to its limit).  Assume that, with a correct MAC, half these transactions will still fail due to insufficient funds.  So if the botnet has a choice between sending 546624 spams to make a dollar and submitting 546624 fraudulent attempts to take $2000 out of a bank account, which is more profitable?  The profit on an attempt to take $2000 out of a bank account is $1000 x the MAC false-positive rate.  A 4-
digit MAC has a false-positive rate of 0.0001, which leaves the profit level at about ten cents per attempt.  A 6-digit MAC reduces profit to a tenth of a cent per attempt - which is still a vastly more profitable use of a botnet than sending spam.  A nine-digit MAC will get the profit down to one ten-
thousandth of a cent; that's the smallest MAC that puts the profit of submitting fraudulent transactions to banks lower than the profit of using the same botted machine to send a spam. I would add a couple of digits just in case the sources
I cited were wrong, or talking about different subsets of data, and the return on spam is actually lower. Still, there are qualitative differences.  In order to be profitable, spam requires human attention, and fraudulent transactions have to evade human attention.  There's only so much human attention per hour available in the world.  An infrastructure of robots accepting or rejecting transactions does not require human attention to defraud.  The resources it requires are essentially electricity and bandwidth, which the botnets have in vast quantities because they steal it.  So we can expect the costs of submitting both spam and fraudulent transactions to drop with Moore's law. But the return on spamming will rise only with the amount of available human attention that isn't already captured by other spam, whereas the return on fraudulent transactions would approach
a relatively constant ratio as available human attention to combat fraud relative to the number of fraud attempts drops.
At some point the two curves intersect, fraudulent transactions become more profitable than spam, and the following week half the botnets in the world are attacking banks instead of spamming.  I think we'd want to add another three digits and make a 14-digit MAC just to keep that day at least 15 years in the future.
Also, we wind up in the unenviable position that if we do anything that successfully reduces the return on spamming, and it reduces the spamming rate of return below the rate of return on fraudulent transaction attempts, then within the space of just a week or so, the botnets will be refocused on fraudulent transactions as having a higher rate of return, and the banks won't be ready for the million-fold or billion-
fold increase in bandwidth that they have to sift through in order to find their legit transactions.  On the bright side, I'm sure they'll be able to buy or lease the required networks cheap, from people who've been handling email....

@_date: 2010-08-15 09:34:23
@_author: Ray Dillinger 
@_subject: Has there been a change in US banking regulations recently? 
I'm under the impression that <2048 keys are now insecure mostly due to advances in factoring algorithms that make the attack and the
encryption effort closer to, but by no means identical to, scaling with the same function of key length.  This makes the asymmetric cipher have a lower ratio of attack cost to encryption cost at any given
key length, but larger key lengths still yield *much* higher ratios of attack cost to encryption cost.  At 2048 bits, I think that with Moore's law over the next decade or two
dropping attack costs and encryption costs by the same factor, attack
costs should remain comfortably out of reach while encryption costs
return to current levels now practical for shorter keys. Of course, this reckons without the potential for unforseen advances in factoring or Quantum computing.
That's probably a good idea. We've placed a lot of stock in public-
key systems because of some neat mathematical properties that seemed to conform to someone's needs for an online business model involving the
introduction of strangers who want to do business with each other.  But
if you can handle key distribution internally by walking down the hall
or mailing a CD-ROM preloaded with keys instead of by trusting the
network the keys are supposed to secure, you really don't need
Public-key crypto's neat mathematical properties.

@_date: 2010-07-30 19:40:49
@_author: Ray Dillinger 
@_subject: About that "Mighty Fortress"...  What's it look like? 
Assume, contra facto, that in some future iteration of PKI, it works, and works very well.  What the heck does it look like?
At a guess....  Anybody can create a key (or key pair).  They get one clearly marked "private", which they're supposed to keep, and one clearly marked "public", which they can give out to anybody
they want to correspond with. Gaurantors and certifying authorities can "endorse" the public key
for specific purposes relating to their particular application.
Your landlord can "endorse" your keycard to allow you to get into the apartment you rent, the state government can "endorse" your key when you get a contractor's license or private investigator's license or register a business to sell to consumers and pay taxes,
etc.  There are no certifying agencies other than interested parties and people who issue licenses/guarantees for specific reasons. You can use your private key to "endorse" somebody else's key to allow them to do some particular thing (you have to write a short note that says what) that involves you, or check someone else's key to see if it's one that you've endorsed.  If you've endorsed it, you get back the short note that you wrote, telling you what purpose you've endorsed it for. Anybody who's endorsed a key can prove that they've endorsed it by publishing their endorsement.  You can read and verify public endorsements using the public keys of the involved parties.
And you can revoke your endorsement of any particular key, at any
time, for any reason.  The action won't affect other endorsements
of the same key, nor other endorsements you've made.
Finally, you can use your private key to prepare a revocation, which can be held indefinitely in some backup storage, insurance
database, or safe-deposit box. If you ever lose your private key, you send the revocation and everybody who has endorsed your public key gets notified that it's no good anymore.
I think this model is simple enough to be understood by ordinary people.  It's also clear enough in its semantics to be implemented in a straightforward way.  Is it applicable to the things we want to use a PKI for?

@_date: 2010-10-06 11:13:36
@_author: Ray Dillinger 
@_subject: Computer "health certificate" plan indistinguishable from Denial 
Microsoft is sending up a test balloon on a plan to 'quarantine' computers from accessing the Internet unless they produce a 'health
certificate'  to "ensure that software patches are applied, a firewall
is installed and configured correctly, an antivirus program with current
signatures is running, and the machine is not currently infected with
known malware."
Apparently in a nod to the fact that on technical grounds this is
effectively impossible, the representative goes on to say "Relevant legal frameworks would also be needed."
as though that would make lawbreakers stop spoofing it.  Existing malware already spoofs antivirus software to display current patches,
in order to prevent itself from being uninstalled.
It is hard to count the number of untestable and/or flat out wrong
assumptions built into this idea, and harder still to enumerate all the
ways it could go wrong.
The article is available at:

@_date: 2010-10-06 11:57:26
@_author: Ray Dillinger 
@_subject: English 19-year-old jailed for refusal to disclose decryption key 
a 19-year-old just got a 16-month jail sentence for his refusal to disclose the password that would have allowed investigators to see what was on his hard drive.  I suppose that, if the authorities could not read his stuff without the key, it may mean that the software he was using may have had no links weaker than the encryption itself -- and that is extraordinarily unusual - an encouraging sign of progress in the field, if of mixed value in the current case.
Really serious data recovery tools can get data that's been erased and overwritten several times (secure deletion being quite
unexpectedly difficult), so if it's ever been in your filesystem
unencrypted, it's usually available to well-funded investigators without recourse to the key.  I find it astonishing that they would actually need his key to get it. Rampant speculation: do you suppose he was using a solid-state drive instead of a magnetic-media hard disk?

@_date: 2013-08-25 10:37:52
@_author: Ray Dillinger 
@_subject: [Cryptography] PRISM PROOF Email 
> an attack by the Iranians, I am not just worried about US interception.
 > Chinese and Russian intercepts should also be a concern.
Observation:  Silent Circle and Lavabit both ran encrypted email services.
Lavabit shut down a few days ago "rather than become complicit in crimes
against the American People."  I would say that's about as close as you can
skate to "We're facing a court order that we're not allowed to tell you
about."  Maybe even closer; we'll be forbidden to know whether anyone
prosecutes them for violating the presumed gag order.  Silent Circle shut
down soon after, saying, "We always knew the USG would come after us."
Which perhaps a little less clearly indicates a court oder they can't talk
about, but that's certainly one interpretation.
Egypt, Oman, and India refused to allow Blackberry to operate with their
end-to-end encrypted devices.  In cases where Blackberry is now allowed to
operate in those jurisdictions it is not at all clear that they are not
doing so using compromised devices whose keys shared with those governments.
Chinese military teams spent so much effort hacking at gmail and facebook
accounts, in order to ferret out dissidents, that Google was eventually
forced to cease doing business in China, and now gmail and facebook both
have some end-to-end encrypted clients.
My point I guess is that we have some evidence that Governments across the
world are directly hostile to email privacy.  Therefore any centralized server,
CA, or company providing same may expect persecution, prosecution or subversion
depending on the jurisdiction.
And it can never, ever, not in a billion years, be clear to users which if
any of those centralized servers or companies are trustworthy.  Google now
implements some end-to-end encryption for gmail but we also know that google
is among those specifically mentioned as providing metadata access to the
US government.  The exact details of Blackberry's keys in Oman, UAE, & India
are now subject to largely unknown deals and settlements.
Therefore, IMO, any possible solution to email privacy, if it is to be trusted
at all, must be pure P2P with no centralized points of failure/control and no
specialized routers etc.  And it can have no built-in gateways to SMTP.  Sure,
someone will set one up, but there simply cannot be any dependence on SMTP or
the whole thing is borked before it begins.  It is time to simply walk away
from that flaming wreckage and consider how to do email properly. S/Mime and
PGP email-body encryption both fail to protect from traffic analysis because
of underlying dependence on SMTP.  Onion routing fails to protect due to timing
So I say you must design your easy-to-use client completely replacing the
protocol layer.  No additional effort to install because this is the only
protocol it handles.
 > My solution is to combine my 'Omnibroker' proposal currently an internet draft and Ben Laurie's Certificate Transparency concept.
I would start from a design in which mail is a global distributed database, with
globs that can be decrypted by use of one or more of each user's set of keys, and
all globs have expiry dates after which they cease to exist.  Routing becomes a
nonissue because routing, like old USENET, is global.  Except instead of timestamp/
message ID's, we just use dates (because timestamps are too precise) and message
hashes (because message IDs contain too much originating information).
No certificate, no broker, no routing information unless the node that first hears
about the new glob has been compromised.  Each message (decrypted glob) optionally
contains one or more replyable addresses (public keys).
If we need more 'scalability' we could set up "channels" discriminated by some
nine bit or so substring of the message hash, and require senders to solve hashes
until they get a hash with the "right" nine bits to put it in the desired channel.
Still no routing information as such. Now Eve can tell what channel/s a user is
listening to, but the user has each of those channels in common with thousands
across the world most of whom s/he has no connection with.
Zero-trust anonymous email.

@_date: 2013-08-26 10:16:09
@_author: Ray Dillinger 
@_subject: [Cryptography] Good private email 
> You need the client to be
I know who has that - in spades!
The bitcoin network is a public transaction record of bitcoin transfers.
The individual accounts are not quite fully anonymous to a determined
observer, but nothing we've discussed here would be more anonymous.
Anyway, a bitcoin client already generates key pairs, and every transaction
stores them in the database.  The database is distributed to all "full node"
clients, and kept (reasonably) secure using Nakamoto's proof-of-work protocol
for the byzantine-generals problem.  The maintainers of the database have a
vested (monetary) interest in keeping the database secure.
Anyway, each "address" is a relatively short high-entropy string (ECC
crypto) -- and each client already has an "address book" of public
"addresses" (public keys where people can be sent bitcoin payments --
or private messages) and "accounts" (private keys which represent
bitcoin that can be sent).  In addition, you can ask the client to
generate a new "address" (keypair) for you at any moment.  The private
key goes into your "accounts" as an account with zero balance (and no
message history) and a new public key for you goes into your "addresses"
as a place where you can receive payments (and messages).
There are smartphone clients that don't maintain the full database, but
which do maintain the address book, accounts, and address-generation bits
for you.  There are already solutions for transferring public keys
directly between smartphones via bluetooth, which is a convenient channel
outside the sphere of Internet eavesdropping.  And there is already
software that can preprint N business cards (with or without your name/etc
on them) that all have different "addresses" on them, so you can hand them
out to anyone whom you think may have a reason to send you money (or
messages), one address per person.
In practice, people need to key in an address for someone once if they
are handed a card.  Keying it is about the same difficulty as a VIN
number on an auto insurance form.  Subsequent new addresses for the same
person can be sent in a message encrypted, along with any bitcoin
transaction, and automatically replace the address you already have
associated with that account for your next payment (or message).  If
Alice doesn't have preprinted cards, she has her smartphone and it can
generate an address for her on demand -- She will have to read it off
her smartphone screen if she wants to scribble it on a napkin.
If we build further email infrastructure on top of this, A side effect of
this is that every user has a choice about whether or not s/he will accept
messages without payments.  You can require someone to make a bitcoin
payment to send you an email.  Even a tiny one-percent-of-a-penny payment
that is negligible between established correspondents or even on most email
lists would break a spammer.  Also, you can set your client to automatically
return the payment (when you read a message and don't mark it as spam) or
just leave it as a balance that you'll return when you reply.
In short, a private email client can be built directly on top of the
bitcoin network.  In practice, I think it would be useful mainly for
maintaining the distribution and updating of keys, rather than for
messages per se, because the amount of "extra" data you can send along
with a bitcoin transaction is quite small (3k?  I think?).  Anyway, it
couldn't handle file attachments etc.

@_date: 2013-08-26 10:40:17
@_author: Ray Dillinger 
@_subject: [Cryptography] Email and IM are ideal candidates for mix 
My main issue with this proposal is that somebody identifiable is going
to manufacture these boxes.  Maybe several somebodies, but IMO, that's
an identifiable central point of control/failure.  If this is deployed,
what could an attacker gain by compromising the manufacturers, via sabotage,
component modification/substitution at a supplier's chip fab, or via
secret court order from a secret court operating according to a secret
interpretation of the law?

@_date: 2013-08-26 11:01:43
@_author: Ray Dillinger 
@_subject: [Cryptography] Email and IM are ideal candidates for mix 
Absolutely agreed; the most reliable things are the least complex.
 > That's 20th century thinking:  The computer is expensive, keep
My thinking is more like: The computer has a multitasking OS.  Whatever
else it needs to be doing will be in another process.  So you lose nothing
if you keep each process simple.  Or if it's a single-purpose box intended
to provide security; don't dilute its purpose.  Keep it simple enough that
even installations of it in the wild, after unknown handling and in all
possible configurations, can be unambiguously, easily, and exhaustively
tested so you know they're doing exactly what they should be and no more.
Also agreed; online patches are the number one distribution vector of
malware that such a device would need to be worried about. Firstly
because whoever can issue such a patch is a central point of control/
failure and can be coerced.  So send it out with an absolutely sealed

@_date: 2013-08-26 11:54:32
@_author: Ray Dillinger 
@_subject: [Cryptography] Good private email 
Possible, but Doubtful.  The bitcoin "wallet" is extraordinarily secure
as software goes. Once you've chosen a keyphrase, It NEVER gets saved in
decrypted form to the disk, and even in the client software, cannot be
decrypted except by explicit command and will not remain in memory for more
than a few seconds in decrypted form. Furthermore, the client software
does not invoke other programs (like Word or other scriptable attack
vectors) under any circumstances.  Furthermore any "extensions" like
clickable URLs in messages or javascript execution etc or other methods
by which external possibly non-secure applications could start up with
information from inside the client would be soundly rejected as
untrustworthy extensions.  People design for and demand an altogether
different level of security when you're talking about their own money,
and handle the "complexities" of key management with no difficulty.
In short, no possibly naive user could convince the developers to do
the stupid things that email clients do for coolness or convenience in
the context of a financial client.
If there were a vulnerability or exploit discovered that allowed a spammer
to take control of a bitcoin account, it would be regarded as a MAJOR
DISASTER by the community and prompt a fix within minutes, not hours
days or months as is the case with "mere" email clients.
Consider that *every* *last* *developer* stands to lose at least
thousands or tens of thousands of dollars of real, personally owned
money if confidence in the network falters.  In some cases literally
millions.  This is not some hypothetical loss to "the company" that
they can be ordered to do by some boss even though they think it's
a bad idea, nor some hobby that they can allow to fall by the wayside;
these people are deeply and very literally invested in the security
of the code, and flatly will refuse to do anything that might
compromise it.
If some company did issue a client with security holes, the usual
shrink-wrap "not liable" crap would be completely unacceptable, the
lawsuit exposure would be somewhere in the trillions of dollars,
and the legal costs to even try to defend a mealymouthed claim of
"not liable because of our shrink wrap license" from the resulting
firestorm would probably break the company.  There are *dozens*
of serious, litigous, investors who hold millions of dollars in
bitcoin these days, including, among others, the Winkelvoss
brothers who spent ten years or more pursuing their infamous
Facebook lawsuit.  Even if you win that legal fight you're going
to lose.
The fact that the client is also highly usable is an excellent example
of interface design.

@_date: 2013-08-30 01:11:32
@_author: Ray Dillinger 
@_subject: [Cryptography] Functional specification for email client? 
User-side spec:
1.  An email address is a short string freely chosen by the email user.
     It is subject to the constraint that it must not match anyone else's
     email address, but may (and should) be pronounceable in ordinary language
     and writable with the same character set that the user uses for writing.
     They require extension with a "domain" as current email addresses do,
     but not a "domain name" in the IETF sense; just a chosen disambiguator
     (from a finite set of a million or so) to make name collisions less of
     a problem.
2.  An email user may have more than one email address.  In fact s/he can
     make up more email addresses at any time.  He or she may choose to associate
     a "tagline" -- name, handle, slogan or whatever -- with the address.
3.  When an email user gets an email, s/he is absolutely sure that it comes
     from the person who holds the email address listed in its "from" line.
     S/he may or may not have any clue who that person is.  S/he is also
     sure that no one else has seen the contents of the email.  The "tagline"
     and email address are listed in the "from:" line.
4.  A user has an address book. The address book can be viewed as a whole or
     as seen by just one of the user's email addresses.  IOW, if you have an
     email address that you use for your secret society and a different email
     address that you use for your job, you can choose to "be" one or the other
     and your address book will reflect only the contacts that you have seen
     from that address or have been visible to under that address.
5.  A mail client observes all email addresses that go through it.  When a
     user receives mail from someone who has not directly sent them mail before,
     the client opens a visible entry in the address book and makes available
     a record of previous less-direct contacts with that address, for example
     from posts to mailing lists, from CC: lists on emails, etc.  The client
     also makes visible a list of possible contact sources; places where the
     correspondent may have seen the address s/he's writing to.  However, often
     enough, especially with cases where it's a "scribbled on a napkin" address,
     the client just won't know.
6.  When a user sends mail, s/he knows that no one other than the holder of
     the address/es s/he's sending it to will see the body of the mail, and also
     that the recipient will be able to verify absolutely that the mail did in
     fact come from the holder of the user's address.
7.  Routing information once obtained for a given domain is maintained locally.
     This means routing information for each email address is public knowledge,
     but also means that no one can tell from your address queries who specifically
     your correspondents are more precisely than knowing which domains they are in.
     This also means that other users may obtain routing information for that
     domain from you. You can update your routing information (ie, set the system
     to route messages for your address to the network location where you actually
     are) at any time, via a message propagated across all peers serving the domain
     of that address.  Also, your client "keeps your addresses alive" by periodically
     sending out a message that is propagated across all server peers for that domain.
     This happens at intervals you set (a few months to ten years) when you create
     the email address.  If that interval goes by without a keep-alive or a routing
     information update, the servers will drop the address.
8.  Emails are "mixed" on your machine locally, then sent out onto the network.
     The "mixing" means creating packets of a uniform size, planning a route for
     each, encrypting them once for each 'hop' on the route, and sending them.
     Routing is constrained to average less than ten 'hops'.  The packet size
     should be selected so most text emails are one packet or less.  Larger messages
     will be sent as a set of packets and reassembled at destination.  Packets will
     be released at a rate of one every few seconds; very large file attachments
     may take days to send and are discouraged.
9.  Your machine, while connected, is collecting your email.  It is also in the
     business of packet forwarding:  ie, it gets a packet, decrypts it, reads the
     next hop, waits some random number of seconds, and sends it to the next hop.
10. Finally, your mail client will occasionally create one or more packets and
     send them via some randomly selected route to another point on the network,
     where they will be received and ignored.  It will do this just about as
     often as it sends original content-bearing packets, and about five percent
     as often as it forwards packets.  This generates 'cover traffic' equal to
     about three quarters of the total network volume. Generation and receipt
     of cover traffic is completely invisible to the user.

@_date: 2013-08-30 16:50:44
@_author: Ray Dillinger 
@_subject: [Cryptography] Functional specification for email client? 
More generally, I was hoping for feedback as to whether this is a design
useful to and usable by ordinary people.
It's fairly straightforwardly implementable as a fully distributed system,
given the notion that "routing information" includes public key.  The only
slightly tricky issue is maintaining the coherence of the per-domain
routing databases among mutually suspicious clients, and there are existing
techniques for that.
It's also compatible with current standards for email payloads, so existing
infrastructure can easily be adapted; in the protocol stack, it looks like
any other MTA.
It can also fairly easily gateway to SMTP, but is not dependent on it.

@_date: 2013-08-31 11:02:00
@_author: Ray Dillinger 
@_subject: [Cryptography] NSA and cryptanalysis 
I have been hearing rumors lately that factoring may not in fact be as hard
as we have heretofore supposed.  Algorithmic advances keep eating into RSA
keys, as fast as hardware advances do.  A breakthrough allowing most RSA keys
to be factored could be just one or two more jumps of algorithmic leverage
away (from academics; possibly not from the NSA).  It could also be the case
that special-purpose ASICs that accelerate the process substantially may
have been designed and built.
We know about Shor's algorithm for factoring in NlogN time.  It requires a
quantum computer to run though.  We have heard rumors of quantum computers
being built, and I recall a group of academics who actually built one nearly
eight years ago.
That seems to be the sort of thing that would attract attention from a lot
of three-letter agencies, and efforts to scale it up would be intensely
supported with all the resources and brainpower that such an organization
could bring to bear.  How far have they come in eight years?  It is both
interesting and peculiar that so little news of quantum computing has been
published since.

@_date: 2013-12-13 08:28:59
@_author: Bear 
@_subject: [Cryptography] An alternative electro-mechanical entropy source 
It would also be a source of vibration which is deadly over the long run to hardware, and annoying as hell to work in the same room with. Sorry, but the cost of the components is irrelevant when it annoys your staff and takes a year off the five year lifetime of your $N000
servers.

@_date: 2013-06-29 02:04:22
@_author: Ray Dillinger 
@_subject: [Cryptography] Snowden "fabricated digital keys" to get access 
I read it to mean that the NSA is using some sort of defeatable
cryptography in its own communications with contractors, presumably
to enable internal snooping for purposes of monitoring contractors.
If a contractor then discovers this system, and manages to cryptanalyze
it (or somehow obtain a copy of the snooping software, though that's
not strictly necessary to cryptanalysis) to figure out the corresponding
method of how the snoopers from the NSA generate keys out of thin
air for it, then he might use that method himself to get access to
all the material that other contractors on that system are working
It would be a ridiculously stupid methodology for the NSA to manage
its security affairs this way, but if "fabricated keys" isn't a flat
out lie, then it's the only thing I can think of that makes sense.
And if it is a flat out lie, then lying to congress is fairly serious.
'Tho it wouldn't be the first time that's happened, either.

@_date: 2013-11-08 14:33:52
@_author: Ray Dillinger 
@_subject: [Cryptography] randomness +- entropy 
Is there any functionality now in the linux system that provides a
"pure" PRNG?  ie, if I know or have set the complete state, I can
then read from it an infinitely-long stream of bits which can be
repeated by initializing it with the same state (and which will
repeat after some finite but very long output stream) but which
is otherwise unpredictable?  Obviously this would be per-process;
sharing state would allow other processes to draw outputs and
therefore alter the sequence visible to the current process. This
used to be random(3). I believe this is still what random(3) does.
Is there any functionality now in the linux kernel that provides
a "pure" Random Number Generator?  IE, it would trap, or set an
'invalid' flag, or suspend the caller, or something, but won't!
ever! return a number when there is insufficient entropy to be
sure that that number is less than completely unpredictable?
This is what /dev/random was supposed to be, right?
Because, honestly, the 'hybrid' state of both of /dev/random and
time when /dev/random was supposed to deliver NOTHING but hard
randomness you could absolutely rely on for keying material, and
the latter but would rather return a (possibly somewhat)
predictable result than fail or slow down your process. Knowing
for sure which was what made designing robust and secure systems
straightforward, if sometimes hard.
The discussion here makes me want to craft something that gives
two simultaneous results; first, the "random number", and second,
a "confidence score" that indicates how certain I can be that the
"random number" is unpredictable.

@_date: 2013-11-11 13:23:20
@_author: Bear 
@_subject: [Cryptography] SP800-90A B & C 
I have actually more concerns about the first design, because recent
events force us to consider hardware manufacturers as adversaries or as being possibly complicit with adversaries.  A special-purpose
device which we cannot see inside to verify that it works in the way
it is being described to work is unacceptable as a sole source of
entropy because it represents a single manufacturer who must therefore
be given total trust. It is a good design, but we have no way of assuring that it is the design which is actually implemented.  Therefore we need a different good design, and I think that systems of the second kind with diverse sources of entropy are better because they contain multiple sources  which can be verified.  The fact that they also benefit from sources which cannot be verified if those sources are in fact good, and with respect to adversaries to whom those sources are good even if there are other opponents to which they are transparent, is also important.
In fact even if every source of randomness available is compromised, but they are compromised by nine different opponents none of whom is trusted with the compromises by all of the others, it is still
possible to build a system secure against all of these nine adversaries
using a mixing approach.

@_date: 2013-11-11 15:23:29
@_author: Bear 
@_subject: [Cryptography] SP800-90A B & C 
But if we have no way of verifying that it is designed to the spec
without manipulation we have no way of verifying that any security exists.  I have a problem with that. And it must.  There absolutely must be a requirement for sources of entropy whose nature and functioning are verifiable.

@_date: 2013-11-12 13:36:00
@_author: Bear 
@_subject: [Cryptography] Practical Threshold Signatures 
For me, it always made sense to think of threshold schemes in terms of geometry.  For example, if you want a situation where any two of some number of people must cooperate in order to produce a key, you're looking for an entity which can be defined by any two of some other class of entities.  In geometry, a point can be defined by the intersection of two lines. So if you give each of your N participants a subkey that corresponds to a geometric line, and all the lines intersect at some point, then the point can be taken as the key to unlock the secret.  However, this
particular construction gives each participant the ability to reject all false keys that don't appear at points along his "line"; given any proposed key and any two subkey holders, it is certain that unless it is the correct key, one of them will be able to tell it is the incorrect
key without the cooperation of any other.  You may want that property
for some specialized protocols, but not for the most general use. So, instead, you could let each subkey represent a point along a line,
and then let the full key be the Y-coordinate at which the line
intersects the x-axis.  This has a different property in that every
possible full key remains possible as far as any subkey holder knows, regardless of the value of their own subkey.  And that's a 2-holders
scheme suitable for a different (and more general) set of protocols. Likewise, if you're looking for a key that can be determined using any three subkeys, you can let the subkeys represent lines, and then let the key represent the center point of the circle tangent to all three lines.  But again, this provides the holders of any two subkeys with the power to discriminate between a large set of impossible keys and a set of possible keys, which, again, is only useful in some limited protocols.  Generalizing again, you can let each subkey be a point along a circle, and put them together indicating the full key at the point where that circle intersects the X-axis.  In this way you again create the situation where given any two subkeys, you cannot eliminate any
possibility for the value of the full key until you are given a third
subkey.  Alternatively, you can go with the first scheme (in which each subkey is a line and you're determining a circle tangent to each of three lines
to find the full key) except that the full key is the *radius* of the circle instead of the *centerpoint* and you'll have the same desirable property of requiring all of three keys to eliminate *any* possible value for the full key.  Anyway, I don't know how many people's brains work in exactly this way, but this has always been an easy mental tool for me to envision and
develop secret-sharing schemes.  It gets a bit less obvious as it goes
into higher share numbers, but there's a very natural set of geometric
relationships where a line can be defined by two points, a plane or a
circle by three, a sphere or conic by four, etc, and it isn't at all
difficult to generalize into higher dimensions.

@_date: 2013-11-12 15:11:06
@_author: Bear 
@_subject: [Cryptography] randomness +- entropy 
I'm inclined to agree.  IMO the kernel ought to simply terminate any
process that attempts to read /dev/random before the boot process is complete.

@_date: 2013-11-13 08:17:41
@_author: Bear 
@_subject: [Cryptography] randomness +- entropy 
I think I'm not buying it.  Hard drive encryption doesn't need *randomness* early in the boot process; it needs *A KEY* early in the boot process.  A machine with an encrypted hard drive has to be able to read
and write sectors encrypted with an existing key before boot can proceed.  IMO that means it either halts during boot and the BIOS asks for someone to type in the passkey (the option I'd prefer on a "secure" machine) or it has the key stored unencrypted somewhere (obviously less secure but probably more manageable).  Randomness for keying material is needed when creating a *new* key, but does not help us get the existing key/s we need to read the boot sector.  Why would an encrypted drive really need a *new* key during bootup?

@_date: 2013-11-19 19:52:39
@_author: Bear 
@_subject: [Cryptography] Moving forward on improving HTTP's security 
True.  Still, the fact that something *IS* a cryptography device makes the manufacturer a target for anyone who wants to subvert security, and all manufacturers are located in and subject to the
demands of countries.  Countries have an interest in subverting
security. Therefore such devices simply cannot be trusted unless you
build them yourself, with off-the-shelf parts whose manufacturer has no idea that you're going to assemble a cryptographic device.
Honestly, I think the best we can do for secure crypto devices is to develop and publish schematics and parts shopping guides for
build-your-own kits.  Along with parts testing guides and software so you can be absolutely sure each component of the device is doing
exactly what it's supposed to do.

@_date: 2013-11-20 14:17:07
@_author: Bear 
@_subject: [Cryptography] Moving forward on improving HTTP's security 
True.  Still, the fact that something *IS* a cryptography device makes the manufacturer a target for anyone who wants to subvert security, and all manufacturers are located in and subject to the
demands of countries.  Countries have an interest in subverting
security. Therefore such devices simply cannot be trusted unless you
build them yourself, with off-the-shelf parts whose manufacturer has no idea that you're going to assemble a cryptographic device.
Honestly, I think the best we can do for secure crypto devices is to develop and publish schematics and parts shopping guides for
build-your-own kits.  Along with parts testing guides and software so you can be absolutely sure each component of the device is doing
exactly what it's supposed to do.

@_date: 2013-11-23 17:30:52
@_author: Bear 
@_subject: [Cryptography] Moving forward on improving HTTP's security 
I am particularly concerned about foreign governments, especially those in developed nations where chips and consumer electronics are fabricated.  After all, if the NSA is an example of what
intelligence agencies are doing, then we may assume that it is not the only such example, nor likely even the one with the most
comprehensive program of surveillence.  Nor is anyone else likely to limit themselves to so called "Metadata," even though Metadata is equivalent to long-term close surveillence in a way that the texts of our mails are not and if anything even more of an invasion of privacy. As long as we leave the infrastructure capable of being suborned, we will not know who nor how many are reading everyone's mail or
collecting information files on every human being in the world. Bruce Schneier posted the other day about detected MITM attacks, some of them quite large-scale and others quite closely targeted, that have been diverting http and mail traffic by suborning DNS.  I first learned of such an attack over 20 years ago when a local foaming-at-the-mouth you're-all-going-to-hell style minister used it against a local government official, whose mail to her husband (with whom she was having marital problems) he then reprinted and faxed to thousands of people under the headline "Jezebellian Whore!"  That was 20 years ago.  Why hasn't it been fixed by now??  Why is this attack still possible, if the entire world hasn't been derelict in their duty, wilfully complicit, or asleep at the wheel?

@_date: 2013-11-29 22:22:58
@_author: Bear 
@_subject: [Cryptography] (no subject) 
I'm pretty firmly of the opinion that your grandparents ought not be
required to understand asymmetric key crypto in order to use it. They need a little appliance next to the computer that's shaped and painted like a little mailbox with a USB socket inside a great big keyhole on the front and another USB socket inside the mail slot.  Then you give them a USB stick with a picture of a key printed on one side and a picture of a stamped envelope on the other. "When you want to read your mail, unlock the mailbox by putting this
key into it...  then you can bring the mail into your computer, just
like bringing paper mail into the house.  You read and write your mail on the computer ... and finally you take the mail out of your
computer and put it into the mail slot to send your mail...."
"Oh, yeah, if you want to get your email anywhere else, you need to take the mailbox and the key with you.  And when you're done remember
to clear your mail off their desktop, you don't want to leave a mess."
You and me and the guy who manufactures those mailboxes know there's public-key crypto going on inside the appliance, and a whole lot of filtering to make absolutely sure no 'scripty' stuff is going onto that USB stick.  Trying to explain it to anybody else is a waste of time.

@_date: 2013-10-01 02:34:43
@_author: Ray Dillinger 
@_subject: [Cryptography] Sha3 
What I don't understand here is why the process of selecting a standard algorithm for cryptographic primitives is so highly focused on speed.?
We have machines that are fast enough now that while speed isn't a non issue, it is no longer nearly as important as the process is giving it precedence for. ?
Our biggest problem now is security, ?not speed. I believe that it's a bit silly to aim for a minimum acceptable security achievable within the context of speed while experience shows that each new class of attacks is usually first seen against some limited form of the cipher or found to be effective only if the cipher is not carried out to a longer process. ?
-------- Original message --------
If you want to understand what's going on wrt SHA3, you might want to look at the nist website, where we have all the slide presentations we have been giving over the last six months detailing our plans.? There is a lively discussion going on at the hash forum on the topic.? This doesn't make as good a story as the new sha3 being some hell spawn cooked up in a basement at Fort Meade, but it does have the advantage that it has some connection to reality.
You might also want to look at what the Keccak designers said about what the capacities should be, to us (they put their slides up) and later to various crypto conferences.? Or not.? The cryptography mailing list
cryptography at metzdowd.com

@_date: 2013-10-01 08:03:38
@_author: Ray Dillinger 
@_subject: [Cryptography] Sha3 
Okay, I didn't express myself very well the first time I tried to say this. ? But as I see it, ?we're still basing the design of crypto algorithms on considerations that had the importance we're treating them as having about twelve years ago.?
To make an analogy, it's like making tires when you need to have a ten thousand mile warranty. ?When rubber is terribly expensive and the cars are fairly slow, ?you make a tire that probably won't be good for twelve thousand miles. But it's now years later. ?Rubber has gotten cheap and the cars are moving a lot faster and the cost of repairing or replacing crashed vehicles is now dominating the cost of rubber. Even if tire failure accounts for only a small fraction of that cost, ?why shouldn't we be a lot more conservative in the design of our tires? A little more rubber is cheap and it would be nice to know that the tires will be okay even if the road turns out to be gravel.?
This is where I see crypto designers. ?Compute power is cheaper than it's ever been but we're still treating it as though its importance hasn't changed. More is riding on the cost of failures and we've seen how failures tend to happen. ?Most of the attacks we've seen wouldn't have worked on the same ciphers if the ciphers had been implemented in a more conservative way. ? A few more rounds of a block cipher or a wider hidden state for a PRNG, or longer RSA keys, ?even though we didn't know at the time what we were protecting from, would have kept most of these things safe for years after the attacks or improved factoring methods were discovered. ??
Engineering is about achieving the desired results using a minimal amount of resources. ?When compute power was precious that meant minimizing compute power. But the cost now is mostly in redeploying and upgrading extant infrastructure. ?And in a lot of cases we're having to do that because the crypto is now seen to be too weak. ?When we try to minimize our use of resources, ?we need to value them accurately.?
To me that means making systems that won't need to be replaced as often. ?And just committing more of the increasingly cheap resource of compute power would have achieved that given most of the breaks we've seen in the past few years.?
To return to our road safety metaphor, ?we're now asking ourselves if we're still confident in that ten thousand mile warranty now that we've discovered that the company that puts up road signs has also been contaminating our rubber formula, sneakily cutting brake lines, ?and scattering nails on the road. Damn, it's enough to make you wish you'd overdesigned, isn't it?
-------- Original message --------
If you want to understand what's going on wrt SHA3, you might want to look at the nist website, where we have all the slide presentations we have been giving over the last six months detailing our plans.? There is a lively discussion going on at the hash forum on the topic.? This doesn't make as good a story as the new sha3 being some hell spawn cooked up in a basement at Fort Meade, but it does have the advantage that it has some connection to reality.
You might also want to look at what the Keccak designers said about what the capacities should be, to us (they put their slides up) and later to various crypto conferences.? Or not.? The cryptography mailing list
cryptography at metzdowd.com

@_date: 2013-10-02 20:13:34
@_author: Ray Dillinger 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
Leaving aside the question of whether anyone "weakened" it, is it
true that AES-256 provides comparable security to AES-128?

@_date: 2013-10-04 09:20:22
@_author: Ray Dillinger 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
>>
So, it seems that instead of AES256(key) the cipher in practice should be
Is it not the case that (assuming SHA256 is not broken) this defines a cipher
effectively immune to the related-key attack?

@_date: 2013-10-04 09:24:12
@_author: Ray Dillinger 
@_subject: [Cryptography] encoding formats should not be committee'ised 
Well, yes, as a matter of fact DCOM was always incomprehensible
and extraordinarily inefficient.  However, it wasn't so much of
a security hole in the "remotely crashable bug" sense.  It made
session management into something of a difficult problem though.

@_date: 2013-10-06 11:15:10
@_author: Ray Dillinger 
@_subject: [Cryptography] Sha3 
I believe you are right about this.  The problem with AES (etc) really  is that people
were trying to find *ONE* cryptographic primitive for use across a very wide range of
clients, many of which it is inappropriate for (too light for first-class or long-term
protection of data, too heavy for transient realtime signals on embedded low-power
I probably care less than most people about the low-power devices dealing with
transient realtime signals, and more about long-term data protection than most
people.  So, yeah, I'm annoyed that the "standard" algorithm is insufficient to
just *STOMP* the problem and instead requires occasional replacement, when *STOMP*
is well within my CPU capabilities, power budget, and timing requirements.  But
somebody else is probably annoyed that people want them to support AES when they
were barely able to do WEP on their tiny power budget fast enough to be non-laggy.
These are problems that were never going to have a common solution.

@_date: 2013-10-06 21:57:49
@_author: Ray Dillinger 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
Is it just me, or does the government really have absolutely no one
with any sense of irony?  Nor, increasingly, anyone with a sense of
I have to ask, because after directly suborning the cyber security
of most of the world including the USA, and destroying the credibility
of just about every agency who could otherwise help maintain it, the
NSA kicked off "National Cyber Security Awareness Month" on the first
of October this year.
[Slow Clap]  Ten out of ten for audacity, wouldn't you say?

@_date: 2013-10-07 09:21:03
@_author: Ray Dillinger 
@_subject: [Cryptography] Politics - probably off topic here. 
But when I see politicians passing laws to stop people voting, judges deciding that the votes in a Presidential election cannot be counted and all the other right wing antics taking place in the US at the moment, the risk of a right wing fascist coup has to be taken seriously.?
Well, yes. ?That is my main concern as well. ?The recent tactics of the Republican party are more an attack on the process of constitutional government than ordinary tactics of the sort that make sense within the process. ?This sort of issue used to get solved with simple and relatively harmless horse trading and pork barrel deals where the minority members would "sell" their votes for something to bring back to their constituents. Shutting down the whole system is mad - and horribly damaging - compared to just taking the chance to bring home some pork.
And this concerns me more than it otherwise might because of the recent economic trouble we've been having. ?When economies go bad is when fascist parties tend to come to power. Or more to the point in our case , when existing parties veer further in a fascist direction. ?I'm seeing the golden dawn party in Greece gaining popularity that it could never have gotten when its economy was in better shape. ?I see the recent elections in a few euro zone nations giving seats to far-right parties, and I just can't help starting to worry.
Most of the history I'm aware of regarding genocides and the emergence of dictatorships is that everywhere from Rwanda to Germany to Haiti, ?they have always followed close on the heels of a particularly severe and long sustained economic crisis.?
I don't want the country I live in to become another example. And I don't want the world I live on to suffer through more examples.?

@_date: 2013-10-07 09:45:10
@_author: Ray Dillinger 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
? Really, you are talking more about the ability to *remove* algorithms.? We still have stuff using MD5 and RC4 (and we'll probably have stuff using dual ec drbg years from now) because while our standards have lots of options and it's usually easy to add new ones, it's very hard to take any away.
Can we do anything about that? If the protocol allows correction (particularly remote or automated correction) of an entity using a weak crypto primitive, that opens up a whole new set of attacks on strong primitives.
We'd like the answer to be that people will decline to communicate with you if you use a weak system, ?but honestly when was the last time you had that degree of choice in from whom you get exactly the content and services you need?
Can we even make renegotiating the cipher suite inconveniently long or heavy so defaulting weak becomes progressively more costly as more people default strong? That opens up denial of service attacks, and besides it makes it painful to be the first to default strong.
Can a check for a revoked signature for the cipher's security help? That makes the CA into a point of control.
Anybody got a practical idea?

@_date: 2013-10-08 18:18:02
@_author: Ray Dillinger 
@_subject: [Cryptography] P=NP on TV 
As I see it, it's still possible.  Proving that a solution exists does
not necessarily show you what the solution is or how to find it.  And
just because a solution is subexponential is no reason a priori to
suspect that it's cheaper than some known exponential solution for
any useful range of values.
So, to me, this is an example of TV getting it wrong.  If someone
ever proves P=NP, I expect that there will be thunderous excitement
in the math community, leaping hopes in the hearts of investors and
technologists, and then very careful explanations by the few people
who really understand the proof that it doesn't mean we can actually
do anything we couldn't do before.

@_date: 2013-10-10 14:20:21
@_author: Ray Dillinger 
@_subject: [Cryptography] prism-proof email in the degenerate case 
Wrong on both counts, I think.  If you make access private, you
generate metadata because nobody can get at mail other than their
own.  If you make access efficient, you generate metadata because
you're avoiding the "wasted" bandwidth that would otherwise prevent
the generation of metadata. Encryption is sufficient privacy, and
efficiency actively works against the purpose of privacy.
The only bow I'd make to efficiency is to split the message stream
into channels when it gets to be more than, say, 2GB per day. At
that point you would need to know both what channel your recipient
listens to *and* the appropriate encryption key before you could
send mail.

@_date: 2013-10-11 10:38:20
@_author: Ray Dillinger 
@_subject: [Cryptography] Broken RNG renders gov't-issued smartcards easily 
Saw this on Arstechnica today and thought I'd pass along the link.
More detailed version of the story available at:
Short version:  Taiwanese Government issued smartcards to citizens.
Each has a 1024 bit RSA key.  The keys were created using a borked
RNG.  It turns out many of the keys are broken, easily factored,
or have factors in common, and up to 0.4% of these cards in fact
provide no encryption whatsoever (RSA keys are flat out invalid,
and there is a fallback to unencrypted operation).
This is despite meeting (for some inscrutable definition of "meeting")
FIPS 140-2 Level 2 and Common Criteria standards.  These standards
require steps that were clearly not done here.  Yet, validation
certificates were issued.
Taiwan is now in the process of issuing a new generation of
smartcards; I hope they send the clowns who were supposed to test
the first generation a bill for that.

@_date: 2013-10-13 09:55:50
@_author: Ray Dillinger 
@_subject: [Cryptography] Key stretching 
There is a completely impractical solution for this which is applicable
in a very few ridiculously constrained situations.  Brute force can
be countered, in very limited circumstances, by brute bandwidth.
You have to use random "salt" sufficient to ensure that all possible
decryptions of messages transmitted using the insufficient key or
insecure cipher are equally valid.
Unfortunately, this requirement is cumulative for *ALL* messages that
you encrypt using the key, and becomes flatly impossible if the total
amount of ciphertext you're trying to protect with that key is greater
than a very few bits.
So, if you have a codebook that allows you to transmit one of 128 pre-
selected messages (7 bits each) you could use a very short key or an
insecure cipher about five times, attaching (2^35)/5 bits of salt to
each message, to achieve security against brute-force attacks.  At
that point your opponent sees all possible decryptions as equally
likely with at least one possible key that gives each of the possible
total combinations of decryptions (approximately; about 1/(2^k) of the
total number of possible decryptions will be left out, where k is the
size of your actual too-short key).
The bandwidth required is utterly ridiculous, but you can get
security on a few very short messages, assuming there's no identifiable
pattern in your salt.
Unfortunately, you cannot use this to leverage secure transmission of
keys, since whatever key larger than the initial key you transmit
using this scheme, once your opponent has ciphertext transmitted
using the longer key, the brute-force method against the possibilities
for your initial short key becomes applicable to that ciphertext.

@_date: 2013-10-13 10:04:28
@_author: Ray Dillinger 
@_subject: [Cryptography] Broken RNG renders gov't-issued smartcards 
Either way, it boils down to "tests were supposed to be done or conditions
were supposed to be met, and producing the darn cards with those certifications
asserted amounts to stating outright that they were, and yet they were not."
All you're saying here is that the certifying agencies are not the ones
stating outright that the tests were done.  I can accept that, but it does
not change the situation or result, except perhaps in terms of the placement
of blame. I *still* hope they bill the people responsible for doing the tests
on the first generation of cards for the cost of their replacement.

@_date: 2013-10-15 00:57:06
@_author: Ray Dillinger 
@_subject: [Cryptography] "/dev/random is not robust" 
That was my takehome message as well.  But theirs is not the first
construction to address this, nor even really the best.  I recall
that Schneier's most recent PRNG recovers well from compromise too,
and I think it does so in a way that addresses the most common cases
of compromise faster than this one and the total compromise that
these authors are concerned about not much slower.

@_date: 2013-10-15 01:05:39
@_author: Ray Dillinger 
@_subject: [Cryptography] please dont weaken pre-image resistance of SHA3 
Oddly enough, Bitcoin is built on no such assumption.  The standard
hash used in Bitcoin is SHA256(SHA256(text)), both for authentication
and proof of work.  I had wondered whether there was any rationale
for that choice and figured Nakamoto was just being paranoid about
possible future cryptanalysis.  But if considered as a drop-in
replacement, the analogous choice would be fully justified with a
(strength at half-length) SHA3.

@_date: 2013-10-21 12:27:07
@_author: Ray Dillinger 
@_subject: [Cryptography] "Death Note" elimination for hashes 
SSL and similar "extensible" protocols are often plagued by a plethora of ciphers and
hashes, some of which are insecure.  The problem of *disabling* ciphers and hashes at
a later date when they become known to be insecure is difficult.
But I think there is at least the beginnings of a way to do it. I will walk through it
for hashes, but a similar logic applies to ciphers as well.
One observation is that from a practical point of view, it is very much necessary to
tell naive users two things:  First, what the practical result of a failure in protocol
negotiation is, and second, whose FAULT it is that something that was working yesterday
is no longer working and who bears responsibility for fixing it.  Naive users do not
care in the slightest what stage of protocol negotiation failed or why; they want to
know what they can and can't do, and who must fix it so it will work. The rest is
just vaguely threatening technobabble to them.
So when a cipher or hash function is eliminated, it needs to be done in a way that
makes that information absolutely detectable and provable, not just talking about a
"protocol failure."  But if we require evidence of breakage to disable a hash or cipher,
that information is readily available;  when a cipher or hash function has been proven
invalid, the party who cannot switch to any valid hash or cipher in protocol negotiations
is solely responsible for the protocol failure.
Prima Facie evidence of an insecure hash can take two forms:
Therefore, I propose that either of these forms can be used as a "death note" against
hash functions.  Essentially, part of any new protocol designed using a plugin hash
function ought to have the ability to recognize "death notes" as a special message in
hash negotiations, meaning, "I refuse to use this hash and here is why you should also
refuse to use it at all points in the future -- and pass the word on to others as
So when some entity in a protocol negotiation suggests using SHA-1, the other ought to
respond with a SHA-1 death note, saying, in essence, "no, here are two messages that
hash to the same value using SHA-1," or "no, here is a message that hashes to the
'public death note' vector for SHA-1."  The 'public death note' value is just a special
case of a test vector whose preimage value is not published (nor, most likely, even kept
after initial production of the vector).
This requires that each hash function have a 'public death note' vector for the preimage
note, so part of protocol negotiation ought to be demonstrating knowledge of the same
'public death note'  known by the other party to protocol negotiation.  This vector
would need to be distributed as part of the hash implementation, and protocol negotiation
should fail if the vector is different between the two ends of the negotiation.  (ie, no
implementation is allowed to be a "working" implementation that can interface to deployed
"working" implementations if it has a different 'public death note' vector for that hash).
The other case, a hash collision, doesn't require any special value to be distributed
with the hash implementation.
The point of having the 'public death note' vector held in common across all users of a hash
is that a valid "death note" preimage once known to any client or server anywhere is reusable
as a proof against use of that hash across all clients and servers that attempt to contact it.
So if someone discovers a working attack, it doesn't matter who they are or where they are
or what credentials they do or don't have;  The death note is self-evidently true or false
and cannot be faked, so the ability to convince others that you are right (and the trust in an
authority that could be subverted in order to implement such an ability) is a nonissue.
In order to protect against DoS using false "death notes" it has to be purely a PROOF of
breakage, so it has to follow all the way through to a form of evidence so solid as to be
indisputable.  Therefore anyone who can formulate a valid 'death note' against SHA-1, can
use it, once, in their communication with their local ISP, and thereafter it will spread
like wildfire across the Internet, every time someone attempts to use that hash function
in protocol negotiation with someone who knows it's been killed.  I'm just guessing here,
but I think with a widely-used insecure hash this would be essentially finished (all
backbone sites and ISPs) within an hour.
So across the Internet, millions of clients and servers with this protocol extension would
never use SHA-1 again, and because there are an unfortunate few who will then be left with
no valid hash function at all, some people will need an error message which says something
"Whenever you try to do anything financial over the Internet using this browser, or attempt
to change your account settings with your ISP, or anything else that requires secure internet
communications, it will fail.  This is because a cryptographic function used by this browser
has recently been proven to be wrong.  You will need upgraded or different software to
continue using secure communications."
And some others will be looking at an error message the next day that says something like,
"Whenever you try to do anything financial over the Internet using this ISP, or attempt to
change your account settings with your ISP, or anything else that requires secure Internet
communications, it will fail.  This is not a problem or error with your software or settings.
This is something your ISP needs to fix, and you will not be able to use secure communications
until your ISP fixes it.  A cryptographic function used by your ISP has been proven to be
invalid and your ISP now needs to upgrade servers, routers, or switches for you to be able
to continue using secure communications."
And a few others will be looking at error messages that say something like,
"The party you are trying to communicate with over the Internet is not currently able to
use secure communications.  You will be unable to do anything financial using their website
until this is fixed.  This is not a problem with your software or settings.  This is something
that the company hosting this website needs to fix, and you will not be able to use secure
communications with them until they fix it."

@_date: 2013-10-28 17:53:05
@_author: Ray Dillinger 
@_subject: [Cryptography] "Death Note" elimination for hashes 
John Kelsey Wrote:
True.  Is this a bug?  After all, if DES and RC4 are both broken, then
the fact is that something which has no other choices really and truly
DOESN'T work.  If it continues to give the impression that it works,
and other parts of the system continue to deign to interact with it as
though it works, then those other parts of the system are also broken.
It looks like a trust issue, unfortunately.  That is a 'break' which is
in terms visible to an average user only hypothetical.  If you can find
a way to formulate an unambiguous machine-checkable proof of this break
that the software clients out there can reliably evaluate as true or
false, then more power to you.  But if not, then you have to appeal to
some recognized authority - and that puts the authority, whoever they
may be, in the line of attack and also at risk of issuing a death note
that is in fact wrong.  I very intentionally framed the proposal in
terms of "prima facie evidence" or the kind of proof that ANYONE can
verify is correct.  If anyone can verify that something is correct, then
it doesn't matter who points it out or whether that person is trusted.
That kind of very simple proof we can handle in software, and we can
reliably implement a robust proof checker for it.  But as the breaks
get less total that kind of simple proof is less applicable.  By the
time you've gotten as far as the attack you describe, I despair of having
a trustworthy non-buggy automaton capable of checking that your proof
is correct, and that puts us on shifting ground where people debate things
and reach conclusions for reasons that most of those who depend on them
don't understand, and ignore things that most most who depend on them
cannot tell whether they ought to be ignoring or not.  The trust issue
becomes much murkier.
If you require humans to verify the correctness of the proof, then
as far as most people are concerned it has become a matter of opinion
anyway.  That is, the IT professional can't tell the difference between
a death note that is true and a false death note that has a con job
behind it good enough to fool the pros. Nor can he tell the difference
between a death note that is false and a death note and one that the
pros simply aren't treating seriously enough to even bother checking.
Aaand this.  This is one of the reasons why you really can't place trust
in the hands of a few authorities with regard to this kind of thing.  It
makes them into targets.
That's completely aside from the equally difficult question of whether
they are in fact trustworthy, and whether they and their heirs in the
position will remain trustworthy over a long period of time.
So, yes, "death notes" as I see them are very much limited to cases
where there is a truly unambiguously broken primitive such that everyone
can verify that it is broken.  I would love for them to be more widely
applicable, but I don't know how to do that.

@_date: 2013-09-01 10:14:25
@_author: Ray Dillinger 
@_subject: [Cryptography] Functional specification for email client? 
True.  I shouldn't have muddied up user-side view with notes about
packet forwarding, mixing, cover traffic, and domain lookup, etc.
Some users (I think) will want to know that much in general terms,
in order to have some basis to evaluate/understand the security
promises, but it's not part of the interface. Only the serious crypto
wonks will want to know in more detail.
{note: how strange! my spell checker thinks "crypto" is a typo, but
has no problem with "wonks"!}
As I consider it, I'm thinking even that promise needs to be amended
to include the possibility of leaking from the recipient.  For example
email forwarding, unencrypted mail archives found by hackers, etc.
Eggs Ackley.  I believe every user in the world is familiar at this point
with the idea of an "email alias," and that the concept maps reasonably
well to "holder of a key" for crypto purposes.  To promise any more
than that about "identity" requires centralized infrastructure that
cannot really exist in a pure P2P system.
If you want to gateway secure mail into the same bucket with
insecure mail, I guess you can do that; I would far rather have
separate instances of mail clients that do not mix types. eg,
this is Icedove/P2P, and this is Icedove/SMTP, and they are not
expected to be able to interchange messages without some gateway.
That said, all you need to gateway secure mail into an SMTP system
is easy to construct.  Consider if the peer mail system has an
address structure like "name**domain":
You have a machine with DNS/SMTP address like "secure.peermail.
com" to reserve the name and provide bounce messages that prompt
people to get a peer mail client and send a message in that
client to "name**domain" for whatever address someone tried to
reply to. Mail imported from the peer mail client with a
name*domain mail format, could show in an SMTP client as
"name**domain at secure.peermail.com."
Alternatively, or additionally, you could have a machine with
an address like "insecure.peermail.com" that actually does
protocol translation and forwards SMTP mail onto the secure
network and vice versa, and allow peer mail users to choose
which machine handles their SMTP-translated address. But
this has the same problems as Lavabit and Silent Circle,
which recently shut down under duress.
Dual-protocol mail clients could use "name**domain" on the
peer network directly.  Mail imported from the SMTP network
on a dual-protocol client or on a peer mail client could
appear as "name at address.com**INSECURE-SMTP" or similar, and on
the dual-protocol client a direct reply would prompt use of
the insecure protocol after a warning prompt.  On a secure-
protocol client it would simply prompt the user to use an
insecure mail client, same as the bounce message on the
other side.
I see the Big Wide Internet's address space as a simple tool to
implement it, not as a conflicting thing that needs reconciled.
The "domain lookup" as I envision it would associate mail peer email
addresses with a tuple of IPv6 address and public key.  The public
keys are stable; the IPv6 address may appear and disappear (and may
be different each time) as the user connects and disconnects from the
system.  The presumption is that the mail peer daemon on the local
machine sends a routing update message when starting up, and
possibly another (deleting routing information) in an orderly
As stated earlier, the system makes no effort to actively hide the
machine where an email address is located.  It could be a machine
designated to receive and keep mail for that address until it gets
a "private address update" that tells it where to send the messages
but which is not propagated; even in that case, the designated
"maildrop" machine if not controlled by the holder of the address
cannot be considered to hold any real secrets.
Routing update messages propagate across the network of relevant domain
servers, which check the sig on the update against the public key,
update their table, and pass it on.  Routing messages could propagate
worldwide within minutes of a logon. Anybody holding a packet whose
next hop is an email address which doesn't currently have an IPv6
address, simply holds it until the address appears or the packet
expires.  In a disorderly shutdown, other peers attempting to contact
the IPv6 address still in the table should get a routing error from
the protocol layer and update their own tables (only) to reflect the
current absence of an IPv6 address.
Peers should communicate only if their routing tables match (have
matching hashes).  In the event they don't, either one or both
have one or more routing updates that the other does not, or one or
both of them has detected a dropped connection that the other has
not.  In either event, the differences can be quickly and
authoritatively resolved by identifying and forwarding the missing
routing update/s, and by pinging the "dropped" machine/s to verify
whether they're actually (or still) gone.
Anyway; too much detail, too early.  Need to consider more and
plan less for now.

@_date: 2013-09-03 12:29:38
@_author: Ray Dillinger 
@_subject: [Cryptography] Three kinds of hash: Two are still under ITAR. 
If I recall the most recent revision, the above requirement is true
for keyed hashes whether they are "signatures" with public-key crypto
or "secret hashes" with private-key crypto) but not for "fingerprint"
or unkeyed hashes like FIPS or SHA-XXX.
The distinction among the three types:
"Signature" hashes:  Alice produces a "signature" hash using her
private key.  Because her public key is common knowledge, everybody
can tell that Alice (or at least someone with her private key)
really did sign it.
"Secret" hashes:  MIB or some similar group share knowledge of a
secret key.  A, a member of the group, produces a secret hash
using that key, and when they check, every member from Bea to Zed
knows know that some member of the organization (or at least
someone who has the secret key) did sign it. But even if the
message and hash are public or in an insecure channel like email,
nobody who doesn't have the key can prove a thing about the
signer. Or at least, not from the signature itself.  Server logs
and "security" video surveillence of public terminals etc, are
an entirely different thing. A would be worried about those
if she had an official "identity" for someone to find.
"Fingerprint" hashes:  Anybody can apply a fingerprint hash to
something, and it proves nothing about who signed it because
the hash is completely public knowledge and has no particular
key. Anyone who applies a fingerprint hash to something will get
exactly the same hash code for the same thing. The point of a
fingerprint hash is that it is a fixed-length probably-unique
identifier that can be checked in constant time.  If the
fingerprint of two documents are not equal, the documents are
guaranteed to be dissimilar.  If the documents are dissimilar,
the signatures are *almost* guaranteed to be dissimilar.  This
is very useful for looking up documents in a hash table or
tree, for example, using the fingerprint hash as a key.
Usually when cryptographers use the word "hash" they are
talking about a fingerprint hash.

@_date: 2013-09-07 11:36:13
@_author: Ray Dillinger 
@_subject: [Cryptography] Bruce Schneier has gotten seriously spooked 
Here is another interesting comment, on the same discussion.
Schneier states of discrete logs over ECC: "I no longer trust the constants.
I believe the NSA has manipulated them through their relationships with industry."
Is he referring to the "standard" set of ECC curves in use?  Is it possible
to select ECC curves specifically so that there's a backdoor in cryptography
based on those curves?
I know that hardly anybody using ECC bothers to find their own curve; they
tend to use the standard ones because finding their own involves counting all
the integral points and would be sort of compute expensive, in addition to
being involved and possibly error prone if there's a flaw in the implementation.
But are the standard ECC curves really secure? Schneier sounds like he's got
some innovative math in his next paper if he thinks he can show that they

@_date: 2013-09-07 12:54:04
@_author: Ray Dillinger 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
If it isn't, then you haven't considered its likely effects.
First of all, it makes CA's visibly redundant.  If people stop using
CA's that multiplies the number of channels that must be compromised
in order to eavesdrop.  Furthermore, it makes those channels parties
actually interested in the authenticity of the communications, such
as the companies whose keys are being authenticated.  In short, it
means the NSA would have to deal directly with the people they want
to eavesdrop on. That makes reaching a covert deal to expose keys a
bit more difficult, I'm thinking.
Secondly, it is the case that a DNS cache poisoning attack is an
occasionally useful technique allowing attackers to access things
that some people would rather they didn't access.  Such attackers
may or may not, apparently, include the NSA themselves, and if they
depend on that capability, then DNSSEC could be seen by them as a
threat against a useful channel for obtaining information.

@_date: 2013-09-07 13:01:53
@_author: Ray Dillinger 
@_subject: [Cryptography] Why prefer symmetric crypto over public key 
I think we can no longer rule out the possibility that some attacker
somewhere (it's easy to point a finger at the NSA but it could be
just as likely pointed at GCHQ or the IDF or Interpol) may have
secretly developed a functional quantum computer with a qbus wide
enough to handle key sizes in actual use.
And IIRC, pretty much every asymmetric ciphersuite (including all public-
key crypto) is vulnerable to some transformation of Shor's algorithm that
is in fact practical to implement on such a machine.

@_date: 2013-09-07 19:19:09
@_author: Ray Dillinger 
@_subject: [Cryptography] Suite B after today's news 
Given some of the things in the Snowden files, I think it has become the case
that one ought not trust any mass-produced crypto hardware.  It is clearly on
the agenda of the NSA to weaken the communications infrastructure of American
and other business, specifically at the level of chip manufacturers.  And
chips are too much of a black-box for anyone to easily inspect and too much
subject to IP/Copyright issues for anyone who does to talk much about what
they find.  Seriously; microplaning, micrography, analysis, and then you get
sued if you talk about what you find?  It's a losing game.
Given good open-source software, an FPGA implementation would provide greater
assurance of security. An FPGA burn-in rig can be built by hand if necessary,
or at the very least manufactured in a way that is subject to visual inspection
(ie, on a one-layer circuit board with dead-simple 7400-series logic chips).
It would be a bit of a throwback these days, but we're deep into whom-can-you-
trust territory at this point and going for lower tech is worth it if it means
tech that you can still inspect and verify.

@_date: 2013-09-07 20:14:10
@_author: Ray Dillinger 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
I've seen this assertion several times in this thread, but I cannot
help thinking that it depends on what *kind* of backdoor you're
talking about, because there are some cases in which as a crypto
amateur I simply cannot see how the construction of an asymmetric
cipher could be accomplished.
As an example of a backdoor that doesn't obviously permit an
asymmetric-cipher construction, consider a broken cipher that
has 128-bit symmetric keys; but one of these keys (which one
depends on an IV in some non-obvious way that's known to the
attacker) can be used to decrypt any message regardless of the
key used to encrypt it.  However, it is not a valid encryption
key; no matter what you encrypt with it you get the same
There's a second key (also known to the attacker, given the IV)
which is also an invalid key; it has the property that no
matter what you encrypt or decrypt, you get the same result
(a sort of hash on the IV).
How would someone construct an asymmetric cipher from this?
Or is there some mathematical reason why such a beast as the
hypothetical broken cipher I describe, could not exist?

@_date: 2013-09-08 07:45:49
@_author: Ray Dillinger 
@_subject: [Cryptography] Why prefer symmetric crypto over public key 
You've answered your own conundrum!
Of course the idea of remembering keys established in previous
sessions and using them combined with keys negotiated in the next
session is a scalable way of establishing and updating pairwise
shared secrets.
In fact I'd say it's a very good idea.  One can use a distributed
public key (infrastructure fraught with peril and mismanagement)
for introductions, and thereafter communicate using a pairwise
shared secret key (locally managed) which is updated every time
you interact, providing increasing security against anyone who
hasn't monitored and retained *ALL* previous communications. In
order to get at your stash of shared secret keys Eve and Mallory
have to mount an attack on your particular individual machine,
which sort of defeats the "trawl everything by sabotaging vital
infrastructure at crucial points" model that they're trying to
One thing that weakens the threat model (so far) is that storage
is not yet so cheap that Eve can store *EVERYTHING*. If Eve has
to break all previous sessions before she can hand your current
key to Mallory, first her work factor is drastically increased,
second she has to have all those previous sessions stored, and
third, if Alice and Bob have ever managed even one secure exchange
or one exchange that's off the network she controls (say by local
bluetooth link)she fails. Fourth, even if she *can* store everything
and the trawl *has* picked up every session, she still has to guess
*which* of her squintillion stored encrypted sessions were part
of which stream of communications before she knows which ones
she has to break.

@_date: 2013-09-08 10:11:20
@_author: Ray Dillinger 
@_subject: [Cryptography] [tor-talk] NIST approved crypto in Tor? 
> that the parameters cannot be predetermined ... and no trapdoors can
Eugene has a very strong point. Clearly we need to replace deployed
instances of those curves.  And just doing that is going to be a
years-long project that takes hundreds of people and won't be fully
(re)deployed for decades. If then. Can we rerun the code starting
at a more reasonable place and see what curves develop?
Good god, we need to re-evaluate *EVERYTHING* that's deployed in the
last 20 years for safety and security, from the standards level down.
This is critical public infrastructure we're talking about here and
we don't even know how much of it has been sabotaged.  By people we
usually trusted, whose mission was to enhance communications security.

@_date: 2013-09-08 10:15:10
@_author: Ray Dillinger 
@_subject: [Cryptography] MITM source patching [was Schneier got spooked] 
Why is that a problem?  GIT is open-source.  I think even *I* might be
good enough to patch that.

@_date: 2013-09-08 10:24:54
@_author: Ray Dillinger 
@_subject: [Cryptography] Suite B after today's news 
Depends on the operation.  If it's linear, somewhat certain.  If it's
parallizable or streamable, then very certain indeed.
But that's not even the main point.  It's the 'assurance of security' part
that's important, not the speed.  After you've burned something into an
FPGA (by toggle board if necessary) you can trust that FPGA to run the same
algorithm unmodified unless someone has swapped out the physical device.
Given the insecurity of most net-attached operating systems, the same is
simply not true of most software.  Given the insecurity of chip fabs and
their management, the same is not true of special-purpose ASICs.

@_date: 2013-09-08 10:38:17
@_author: Ray Dillinger 
@_subject: [Cryptography] [cryptography] Random number generation 
Y'know what?  Nobody has to accuse anyone of anything.  The result,
no matter how it came about, is that we have a chip whose output
cannot be checked.  That isn't as good as a chip whose output can
be checked.
A well-described physical process does in fact usually have some
off-white characteristics (bias, normal distribution, etc). Being
able to see those characteristics means being able to verify that
the process is as described.  Being able to see also the whitened
output means being able to verify that the whitening is working
OTOH, it's going to be more expensive due to the additional pins of
output required, or not as good because whitening will have to be
provided in separate hardware.

@_date: 2013-09-11 20:37:03
@_author: Ray Dillinger 
@_subject: [Cryptography] Impossible trapdoor systems (was Re: Opening 
True.  A universal key that uses the same decryption operation as
a normal key is clearly stupid.
I guess the thing I was thinking of is that the "attacker" knows
a method that allows him to decrypt anything if he knows the IV,
but cannot recover the key used to encrypt it.
Which is of course a public-key system, where the decryption
method is the "private" key and the IV is the "public" key.
The thing I was thinking of as a "key" functions as a "nonce"
or subkey which allows people unrelated to the private key
holder to communicate semi-privately by shared secret, but
the private key is a backdoor on their communication.
Duh. Sorry, just wasn't thinking of the right "parallel mapping"
of what I described. For the cipher itself to function as a key
sort of escaped my attention.
Sorry to waste time.

@_date: 2013-09-19 10:02:35
@_author: Ray Dillinger 
@_subject: [Cryptography] A lot to learn from "Business Records FISA NSA 
SeLinux seems to be targeted mostly at organizational security,
whereas the primary need these days is not organizational, but
That is to say, we don't in practice see many situations where
different levels and departments of an organization have complex
and different rules for how and whether they can access each
other's information and complex requirements for audit trails.
What we see is simpler; we see systems used by people who have
more or less uniform requirements and don't much need routine
auditing, except for one or two administrators.
More useful than the complexity of SeLinux would be a relatively
simple system in which ordinary Unix file permissions were
cryptographically enforced.  If for example read permissions on
a file are exclusive to some user or some group, then that file
should be encrypted so that no one else, even if the bytes are
accessible to them by some means, should be able to make sense
of it, and the configuration options should include not storing
the key to it anywhere in the system -- let the user plug a
USB stick in to give the key for his session, and let the user
remove it to take that key away again whenever he's not using it,
rather than leave it around on the hard drive somewhere potentially
to be accessed by someone else at some other time.
We have spent years learning to protect the operating system from
damage by casual mistakes and even from most actual attacks,
because for years control of the computer itself was the only
notable asset that needed to be protected.  It is still true that
control of the computer is always at least as valuable as
everything else that it could be used to compromise, but with
unencrypted files it can compromise far too much.  And the value
of what is stored in individual accounts has gotten far too high
to *NOT* give protecting them at least as much thought as
protecting root's access rights. Photographs, banking records,
schedules, archived mail going back for years, browser histories,
"wallets" that contain many other keys, etc, etc.  This is far
different from old days when what was on a user's account
was basically a few programs the user used and some text or
code that the user had written.  We need to catch up.

@_date: 2013-09-20 11:08:00
@_author: Ray Dillinger 
@_subject: [Cryptography] RSA recommends against use of its own products. 
More fuel for the fire...
RSA today declared its own BSAFE toolkit and all versions of its
Data Protection Manager insecure, recommending that all customers
immediately discontinue use of these products.
The issue is apparently the Random Number Generator that these
products use, the rather amusingly named "Dual Elliptic Curve
Deterministic Random Bit Generator." *1
And according to more of the Snowden Files released to (or by)
the New York Times last week, that pseudorandom generator is
deliberately flawed in order to allow it to be sod...  um,
excuse me, I should have said, to permit backdoor penetration.
RSA was truly between a rock and a hard place here as I see it.
With the deliberate weakness now made public, they took a terrific
blow to their business.  But failure to follow up with a
recommendation against their own products, no matter how much
additional financial pain that action entails, would have
destroyed all trust in their company and prospects for future
business.  As best I can tell, they have lost $Millions at least
due to the tampering of their products, and American security
and software companies taken as a whole are in the process of
losing $Billions to foreign competitors for the same reasons.
I wonder, would a class action suit seeking compensation for this
wholesale sabotage be within the jurisdiction of the FISA court?
*1 "Anyone who attempts to generate random numbers by
     deterministic means is, of course, living in a
     state of sin." -- John Von Neumann

@_date: 2013-09-20 12:14:22
@_author: Ray Dillinger 
@_subject: [Cryptography] RSA recommends against use of its own products. 
That said, it seems that most of these attacks on Pseudorandom
generators some of which are deliberately flawed, can be ameliorated
somewhat by using a known-good (if slow) Pseudorandom generator.
If we were to take the compromised products, rip out the PRNG's,
and replace them with Blum-Blum-Shub generators, we would have
products that work more slowly -- spending something like an
order of magnitude more time on the generation of Pseudorandom
bits -- but the security of those bits would be subject to an
actual mathematical proof that prediction of the next really is
at least equal in difficulty to a known-size factoring problem.
Factoring problems apparently aren't as hard as we used to think
but they *are* still pretty darn hard.
Slow or not, I think we do need to have at least one option
available in most PRNG-using systems which comes with a
mathematical proof that prediction is GUARANTEED to be hard.
Otherwise it's too easy for people and businesses to be caught
absolutely flatfooted and have no recourse when a flawed PRNG
is discovered or a trust issue requires them to do something
heroic in order to convince customers that the customers' data
can actually be safe.
We've been basing our notion of security on the idea that others
don't know something we don't know -- which is sort of nebulous
on its face and of course can never be provable. We can't really
change that until/unless we can say something definite about
P=NP, but we're a lot more sure that nobody else has anything
definite to say about P=NP than we are about most crypto
Do we know of anything faster than BBS that comes with a real
mathematical proof that prediction is at least as hard as
$SOME_KNOWN_HARD_PROBLEM ?

@_date: 2014-04-08 12:12:54
@_author: Bear 
@_subject: [Cryptography] TLS/DTLS Use Cases 
To be fair, keep-alive was not part of the design.  Http was initially a completely stateless protocol, and actually a fairly well designed one.  The reason keep-alive is not well supported is because it's got nothing to do with the original design and was bolted on as an
afterthought. Is there a take-home lesson there?  Only that if we engage in elegant design we should not trust those who come after us not to screw it up.

@_date: 2014-04-08 13:21:48
@_author: Bear 
@_subject: [Cryptography] OpenPGP and trust 
For a while now I've been considering 'continuity' as an (adhoc)
approach to security.  It could be a supplement to the web-of-trust
or a supplement to X.509, or even something that provides a certain
level of trust on its own for particular purposes.
In a continuity-based system, you'd be associating each key with an entity.  An entity presenting a different key/cert, without some continuity measure such as new-key-signed-by-old plus revocation
of old key, should not be assumed to be the same entity regardless
of what they or their cert claims.
And revocation of old key is a very important part of it, because with revocation you get a situation where an impersonator is forced to either use a key that the impersonated person can decrypt (leaving
possible evidence) or break the impersonated person's key (leaving
possible evidence). Anyway this is the sort of thing that approach would address.  You have bought a cert for your site, some other guy has bought a cert for his site, and when people log on, they get a cert - but not the one they've been getting up to that moment. This is a continuity violation, and absolutely nothing that their software
knows is associated with your site should be working for or with
this other site which has a different key.  My vision of UI for this would be that each site should come up next to (or under, or over) a banner that lists previous interactions
with that site, and that a completely new previously-unseen cert, regardless of who it's signed by or what it says, should come up with a banner that says, "YOU HAVE NEVER ACCESSED THIS SITE BEFORE TODAY - NEW KEY HAS NO
PREVIOUS HISTORY." Obviously if the user "knows otherwise" there's something wrong.  If it's important, and there's no continuity, then the user should be making a phone call and getting his bank to verify that yes, it did in fact change its key since last time he connected.  The bank doesn't want the hassle of answering phone calls?  Then it should do the continuity dance, sign the new cert with the old one, and

@_date: 2014-04-10 15:49:54
@_author: Bear 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
'Strewth, but most folks don't even know how to zero memory reliably before the allocator gets it back, because many compilers now include optimizers that eliminate even explicit writes to memory when they can show it will not be read again before it is deallocated (and also eliminate reads that don't affect output, etc).
"Doesn't affect program output: therefore is a waste of work"  is the underlying assumption, correct for most software, which is utterly, utterly wrong for crypto.
In C and C++, variable locations are stable (and mostly have to be because those languages use explicit pointers) but you have to declare them 'volatile' to be absolutely sure that the compiler will never move them (without zeroing the old location) and that writes to them prior to deallocation will happen as the code commands.  In most languages, there is absolutely no standard way to be sure of getting an optimizing compiler to leave final writes alone.  After a conversion to single-assignment form or continuation passing style, which lots of compilers do as a *first* step, it isn't even likely that a write of a new value for a variable, if performed at all prior to the variable going out of scope, will be written to the same memory address that held a previous value.  Nor is it guaranteed that what you think is a value you assigned
once and never copied, isn't getting copied to new locations on a regular basis by a compacting or 2-space garbage collector.
Unless the language standard guarantees that variables have stable locations, and provides a way to guarantee that final writes to those locations are honored regardless of whether they are read again affecting program output, the best you can do is to analyze and test a single version of a single
implementation.

@_date: 2014-04-10 16:11:03
@_author: Bear 
@_subject: [Cryptography] Preliminary review of the other 
DNSSEC correctly solved a different and much smaller set of problems. It would not be called SEC today; it makes no attempt to keep secrets of any kind from anyone.  It's merely better than what it replaced at
foiling phishers.
In the fullness of time, somebody will have to further secure DNS.  It really shouldn't be apparent to any observer what someone is looking
up nor what response they got from the nameserver.  It really shouldn't be apparent to any observer exactly which traffic on the system needs
to be modified if they want to make an active attack.  It shouldn't be possible for any observer to even know exactly which nameserver they
have to compromise to spoof a particular target (ie, which DNS server
someone is likely to use for their next lookup). et cetera.
But it will not happen this year, nor this decade.  DNSSEC took over 20 years to develop and deploy, and is only about 90% fully deployed
today, despite being a genuine security improvement against phishers and enjoying unambiguous political support. A potential SDNS system would address things that most consider much smaller problems  and would lack political support besides. It is not likely to arrive sooner, even if we all get out and push.

@_date: 2014-04-11 19:07:38
@_author: Bear 
@_subject: [Cryptography] Preliminary review of the other 
Actually, a quite clever, well-secured P2P DNS system has been created, and IMO ought to be known and used more widely.  It's blockchain secured (developed from Bitcoin technology), except
instead of keeping track of who owns the output of particular transactions of a circulating pool of imaginary nerd money, it keeps track of the association (and occasional revocation) of
keys/addresses with domain names.
It is strictly first-come first-served, so there's no second-party
revocation if, for example, skeptics claim 'scientology', nor any procedure for mediation if the World Whack-A-Mole Fund and the Wildlife Wrestling Foundation can't agree on who gets 'wwf'. *1
Also, it makes no effort to take up existing DNS information from the domains administered by major TLD's like .com, .org, and so on.
I think that it ought.  But the people who administer those TLD's don't like and would not abide by the limitations of its decentralized aspects, so it would quickly go out of sync, because for example they couldn't kick domain squatters off of a potentially lucrative trademarked name without getting the claimant to use his/her own key to revoke the old cert, etc.
*1 I think that's a fundamental limitation of a fully decentralized system, and actually more fair.  I could live with it, and  be happier with that arrangement than with the current one.  It would force people to resolve these decisions by resorting to buying and selling for the market value of the names rather than by simply using force against the less powerful or less privileged
claimant.

@_date: 2014-04-12 09:41:32
@_author: Bear 
@_subject: [Cryptography] Preliminary review of the other 
It absolutely does. Tor, I2P, and etc -- essentially all services that
use keys but not CAs -- are entirely compatible with namecoin and have
clients which have been adapted to use it.  Cert Authorities are visibly
redundant in namecoin-based domains, so there's no call to serve those keys also.

@_date: 2014-04-15 15:14:05
@_author: Bear 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
Indeed, the real issue is that security software cares about "outputs" that are important in no other kind of programming.  All your dev tools and language implementations are written under the assumption that the program's outputs are that which the program is directed to write to a place where other programs (or people) are *expected* to read it - that is, to output ports, to files, to the console, to the screen, etc.  All other effects of executing the program, it is presumed - those you do not command, and which are not left in places where they are *expected* to be read - are not "outputs" and do not matter.
But security is all about ensuring that nobody can take advantage of any effect of executing the program to disclose information that the program was supposed to keep private.  All the other effects of executing the program that someone can find *any* way to read, ARE therefore outputs that matter to the correctness of
your program.
That includes but is not limited to, the traces on the swap partition, the stale values in memory, the bits left over in deallocated inodes in the file system, the amount of time the program is taking between two known system calls, etc. This violates the fundamental assumptions under which development tools are written.  With security software, you're suddenly caring
about outputs that the tools do not acknowledge and are programmed to assume you don't care about.  In trying to restrict the information
available on those channels, IOW, you're making what the author of your dev tools would have regarded as a spectacularly nonsensical
requirement, and one which has nothing to do with any kind of objective or goal that his programming language left you any way to explicitly state.
I myself have implemented a nice lisp.  It's memory safe, mostly functional, has a beautiful lock-free threading model with zero
contention because it uses copy-on-write semantics with shared memory between threads.  A lot like Clojure, but it has memory versioning so that if something doesn't work out you can just rewind and try it again, which is neat. It gives you beautiful
expressive power, but absolutely no control over any channels other than the explicit channels you might be *intentionally* writing.  And that's the norm as regards explicit vs. implicit Memory safe languages that give you nice abstractions and guaranteed initialization, etc, are making no promises to you
other than the promise that your code will produce *explicit* outputs indistinguishable from those it would produce if those initializations/finalizations/etc were actually performed.
As far as I know, C (and C++) with its 'volatile' directive are the only language standards that give you a promise about being able to force something to never get swapped out and never have a read or write specified by the code omitted - which means a way to control many of the *implicit* channels you have to care about in security code.

@_date: 2014-04-17 11:22:18
@_author: Bear 
@_subject: [Cryptography] I  don't get it. 
Bounds checking should be part of array and string semantics.
ALL string semantics including strcopy, strcat, etc, should never
have had any bounds-unchecked versions in the first place.  Zero-terminated strings were a Bad Idea.  Strings should have had explicit lengths from the beginning.
Bounds checking should be part of stack semantics.  Stack scribbling should get a guaranteed halt with a frame error as opposed to continuing until 'return' lands in a scribbled frame or, even worse, follows a scribbled pointer to 'return' to an entirely different location.
Bounds checking should be part of the semantics for all reads and writes to dynamically-allocated memory.  'char' should never have been any kind of integer, and should never have been assignable nor comparable to integers without a cast. It could make sense for 'char' to have a value indistinguishable from a length-1 string, but never to have a value indistinguishable from
a length-1 integer.
NULL, unless cast to some particular type, should never have been a valid value for comparison or assignment.  Documentation for system calls that rely on NULL, in particular, should have to tell you exactly what type of NULL they rely on.  Errors should have been a type.  Routines that return 'success or failure' should be returning (errortype)NULL for success or an error value (not just setting 'errno' but returning a value) for failure.  Errors should not be comparable to nor assignable to
any other type without a cast.
There should have been a proper boolean type from the beginning.
Booleans should never have been assignable nor comparable to another type without a cast.
Eh.  I don't have a problem with them, although I'd want to see users provided with *other* methods of doing dynamic memory that make them less attractive;  in particular, there should be library calls for building and managing dynamically allocated data structures that tuck the malloc/free stuff under the corners where the user doesn't need to do it themselves.  Sometimes the user needs sharp tools; but if you can anticipate the need, it's nicer to give him what he needs so he doesn't need to use the sharp tools to carve it for himself.
This is already true, isn't it, with the caveat that you also have
to rely on it being the same set of system libraries?

@_date: 2014-04-18 15:47:56
@_author: Bear 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
There are many things that TLS is purported to be a solution for.
Depending on which context I want to replace TLS in, I would be using any of several different designs.
Each such thing is far simpler than TLS itself and much much easier to implement well.  But none caters for all the use cases people believe that TLS serves.

@_date: 2014-04-18 15:58:46
@_author: Bear 
@_subject: [Cryptography] bounded pointers in C 
C++11 and later mandate the existence of a type "intptr_t" which is
defined as an integer large enough to hold the value of any pointer.  You get access to it if you include the stdint library.
In a system with fat pointers intptr_t may be larger than long long
int.

@_date: 2014-04-19 16:50:21
@_author: Bear 
@_subject: [Cryptography] Are dynamic libs compatible with security? was: 
I have to say the OpenSSL guys really and truly had a point when they pointed out that dynamic linking is not fully or easily compatible with the goal of security libraries.  Inconvenient as it was for Apple (and inconvenient as it is for others) we really shouldn't be relying on dynamic linking to load security modules.  Remember, in security we're in the business of guaranteeing that things we haven't quite planned for do *NOT* happen.  Dynamic
linking is a great way to get things to happen even though you haven't
quite planned for them.  In fact that's its main selling point; dynamic
linking means you automatically link to versions of the libraries other
than the ones you had available when you shipped, even if they contain code you don't quite know about and haven't quite planned for.  We assume this new code is better in the usual case; but in trying to secure systems against attacks, we should be regarding that new code as an attack vector and assuming that it is written by Shaitan.
The other mindset, where we're busily trying to get good things we haven't quite planned for to happen, values the hell out of dynamically linking to new bug fixes.  But in security we have to look at it from the POV of professionals in the art of being certain that bad things (like dynamically linking to new bugs!) do NOT happen, and I simply do not see that as being compatible with dynamically linking to, and therefore trusting, newly updated code sight unseen.
The other thing about dynamic linking is that from the POV of the people who are updating the code that you're linking to, YOUR software
is at least possibly an unknown quantity.  They probably haven't tested with it unless yours is one of the four or five best-known applications
that uses their stuff.  Even if the update isn't part of an attack and isn't written by Shaitan, it's a bit much to trust that your usage pattern which worked with the previous release absolutely is guaranteed
not to expose a flaw in the current release.  Hard linking guarantees that *somebody* tests the library together with the product before the new behavior created by their combination is unleashed on the public.
Is this something that Apple can explain to its users?  I don't know.
Apple has a history of cultivating a user culture that puts a negative value on complex technical explanations, and makes a point of not requiring them to sit through those, so it really was sort of between a rock and a hard place there.  To some extent though, everybody with a modern user community that expects everything to work the same way faces the same problem.

@_date: 2014-04-19 16:59:38
@_author: Bear 
@_subject: [Cryptography] It's all K&R's fault 
There are different aspects of security that are important here.
I am more confident that someone will not be able to get an application written in Haskell or ML to do other than behave as its writer intended.
That is important in terms of security.
I am more confident that someone writing in C will be able to create an application that doesn't leave copies of information you want private sitting around the system in public places.  That is also important in terms of security.  Now, if only I could be confident that the person writing in C had
*BOTHERED* to write an application that doesn't leave copies of
confidential information lying around, I'd feel better. But the person
writing in Haskell or ML, literally CANNOT avoid leaving copies of data values lying around in memory.

@_date: 2014-04-20 09:53:44
@_author: Bear 
@_subject: [Cryptography] Are dynamic libs compatible with security? was: 
Agreed, and possibly in an even more radical form than you were considering.  I'm more and more sold on the idea that ABI's are a bad idea.  This is kind of a radical notion, and would not be as performant, but hardware is faster now and, especially for the desktop, we still have security needs our current methodology doesn't address and underutilized hardware capabilities we can devote to them.  This wouldn't work for compute-bound or I/O bound servers, but on the desktop the current hardware represents an embarrassment of riches for doing things better than we have.
Consider as an alternative a system in which pretty much everything is a discrete application with no shared address space, and if something needs a service provided by other code it needs to ask for it via the sockets layer - which is locked down hard and possibly encrypted, does its own bounds checking, etc.  Depending on the service and the timeframe, that request is either a request to a running daemon that handles it just like it handles any other request, or a request to the kernel to start an instance of a process just to serve the current program's needs.
Procedure calls are just too unchecked and mapping things into the same address space where machine code can read data you wanted private is not something we have to do.  So why are we doing it?
It can also be done trivially with separate processes.  But let's face it; an entry level machine these days has a terabyte drive, and development boxes are a dozen terabytes.  Saving space is no longer as important as design simplicity, because design simplicity has gotten cheaper with current hardware.  And the most complex machines - where security issues hurt the most - are desktops, which have underutilized hardware capabilities.

@_date: 2014-04-24 16:43:57
@_author: Bear 
@_subject: [Cryptography] GCC bug 30475 (was Re:  bounded pointers  in C) 
Such a compiler would be a very specialized product, but I believe that it would be useful and extraordinarily valuable to some subset
of programmers.  I have certainly searched for it before now.
But as far as I know, there is absolutely no compiler in which every
instance of "implementation-defined" or "undefined" is translated as
"stop the compiler immediately with an error message."  If someone knows of such a C or C++ compiler I would be glad to pay good money for it.

@_date: 2014-04-24 17:04:14
@_author: Bear 
@_subject: [Cryptography] GCC bug 30475 (was Re:  bounded pointers  in C) 
There are in fact conformant checks for signed overflow. For example, you can say if (MAXINT - al >= be) halt(2); // overflow check - not using assert()
                                // because we want this check in
                                // production code.
ce = al + be;                   // addition guaranteed not to overflow.
What you can't do is check for overflow *AFTER* the operation that might commit an overflow.  The instant you actually perform an operation that might commit an overflow, the compiler is in fact free to create any old evil code it wants including ignoring your subsequent check.
This is why signed-overflow detection is one of the hardest things about writing a compiler that actually catches and warns about implementation-dependent behavior.  You have to take the condition
in the assertion and use it to make a proof that overflow cannot happen in the case of that addition, in order to avoid warning on the assignment of ce to the sum of al and be.

@_date: 2014-04-25 19:14:24
@_author: Bear 
@_subject: [Cryptography] Heartbleed and fundamental crypto 
Unfortunately, no.  Or at least, not by default on many compilers.  Yes, it is a C library function, and each conforming C library has to provide it in a linkable form - but the standard does not require that to be the only form in which it's provided, nor require that the library version be linked when the call is
made.  In many compilers there is also a 'builtin' version - that is, by default these compilers treat certain calls as though they were C-language statements that the compiler knows how to compile, rather than as calls to a library.
In order to force the compiler to use the 'library' version where there is an actual procedure call that you can trace or debug, you usually need to take at least a couple more steps.  For example, on GCC, you'd have to invoke the compiler with the -fnobuiltins option to suppress builtins which replace
several calls (mostly string functions) in a way that the compiler can inline and, it hopes, optimize or eliminate.
Even if you do that, you still have the possibility of an
optimizing compiler inlining any call, which will get you the same issues as a 'builtin' - once the code is local, it's nothing more than a variable write, and a variable write is usually eliminated unless the value is later read from that variable and used.  In fact, if it's a write of a constant value, constant propagation optimizations can make the variable write be eliminated and the later read replaced by the load of that constant value.
The only way to be sure is the 'volatile' directive; that is intended to mark something at an absolute address that some other process may be reading or writing, which means that skipping a read or write would actually cause bugs in that other process.

@_date: 2014-04-25 19:36:13
@_author: Bear 
@_subject: [Cryptography] GCC bug 30475 
Hey, that's useful!  I have no idea why I had never spotted that in the compiler docs before.  So, I've added that to my default options list, we'll see if it catches anything!
Incidentally, in the hopes that this might be useful for others and in hopes that someone can give me a hint to add something security-relevant and useful, my default options lists for gcc are now....  # I want warnings about essentially everything which can # produce a linguistic ambiguity under the standard or which # probably indicates bad coding style, if gcc can warn me.
# this is the compiler invocation for c++
# for c++ see also 'scan-build' options for static analyzer
g++ -fstrict-overflow -fmax-errors=10 -fno-nonansi-builtins
-fsanitize=undefined -fuse-cxa-atexit -Wall -Wextra -std=c++11
-Wpedantic -Wformat=2 -Wctor-dtor-privacy -Wmissing-include-dirs
-Wswitch-default -Wtrampolines Wfloat-equal -Wundef -Wswitch-enum
-Wstrict-overflow=4 -Wshadow -Wcast-qual -Wcast-align -Wconversion
-Wzero-as-null-pointer-constant -Wlogical-op -Wstrict-null-sentinel
-Wold-style-cast -Wnormalized=nfkc
# and this would be for C code.  # for C, see also 'splint' options for static analyzer.
gcc -fstrict-overflow -fmax-errors=10 -fno-nonansi-builtins
-fsanitize=undefined -Wall -Wextra -std=c11 -Wpedantic -Wformat=2
-Winit-self -Wmissing-include-dirs -Wswitch-default -Wswitch-enum
-Wunused-local-typedefs -Wstrict-overflow=4 -Wtrampolines -Wfloat-equal -Wtraditional-conversion -Wundef -Wshadow
-Wbad-function-cast -Wc++-compat -Wcast-qual -Wcast-align
-Wwrite-strings -Wconversion -Wzero-as-null-pointer-constant
-Wlogical-op -Wold-style-definition -Wnormalized=nfkc
This elicits just about every warning that gcc knows how to give.  I get a few more from static analyzers.  I could have cranked up -Wstrict-overflow another level but the level-5 warnings are pretty much all useless there.  Now, in relation to the particular issue we've been discussing -- the first option, -fstrict-overflow - forces an exception on any signed overflow.  -Wstrict-overflow causes a warning when code is seen that is likely to cause an overflow (ie, if the compiler cannot prove that it won't).

@_date: 2014-04-25 19:58:15
@_author: Bear 
@_subject: [Cryptography] GCC bug 30475 (was Re:  bounded pointers  in C) 
This is one of the reasons I usually write to the c++2011 standard
and tell the compiler to be absolutely pedantic about it.  Int32_t and int64_t are NOT optional anymore, and the code is portable to anything that supports c++2011.  You are correct.  Once your code is beyond a sequence point where it is committed to perform some 'undefined' operation, whether it has reached that operation or not, its semantics are undefined and you have invoked nasal demons.  The result, however, is the same; the correct response is to assure
that your code will NEVER commit itself to performing an 'undefined'
operation, if you possibly can identify all such operations. The fact that undefined or "just plain wrong" behavior can start even *before* the operation that renders the semantics of your code
meaningless has even been committed is counterintuitive and annoying, but true.
I think you're absolutely right about this.  I used to assume
2's-complement semantics, because that was what every compiler in the world did and I hadn't actually studied the standard yet.  Until somebody quit doing it and I got bit by this very same bug.
I'm mostly annoyed with the standard writers for not specifying 2's complement signed overflow semantics (this is something Java got
right). I'm only a little bit annoyed with the compiler writers though; that which is not in the standard, however much it should be, is not something we can *rely* on from any other compiler either, so it behooves us to fix the code anyway.  Still, failing
to support that widespread assumption made a lot of legacy code break, or silently skip checks, etc, for what, by rights, *shouldn't* even have been a bug in the first place if the standards people had gotten it right.
After reading the standard and the documentation, I wound up putting -fstrict-overflow -Wstrict-overflow=4 in my compile options (calling for an immediate halt on signed overflow and for warnings on any code that the compiler thinks might ever produce it) in order to make absolutely sure that I don't create code that fails to notice it or relies on it having some particular semantics ever again. Along with various conversion warnings that warn me about conversions
between signed and unsigned types.  The litany of safety options and warnings grows long.

@_date: 2014-08-02 11:50:21
@_author: Bear 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
I don't think "WF" means anything in the way you're using it.  As I understand the classes of attacks here the "Work Factor" according to best available attacks is not related to the number
of bits in the way you are arguing.
The actual security - or 'work factor' of the best available attacks - is proportional to some base raised to the power of the number of bits, and the base is (I believe) around 1.32 at
this point.  So something 16 bits shorter is, really, only a work factor of about 2^5 easier, not 2^16.  I want to see a curve selected based on hardware constraints of bus and memory width according to an absolutely nothing-up-
my-sleeve set of rules.  So far, all these performance arguments
have big floppy sleeves.

@_date: 2014-08-04 10:19:23
@_author: Bear 
@_subject: [Cryptography] You can't trust any of your hardware 
This will not get fixed until some virus or other using it to steal something important becomes widespread.  That's what I really hate about the situation; in order for it to be worth anyone's time to fix it, someone first has to use it to perpetrate a major ripoff.  Which means, if the black hats don't pick it up and run with it and use it to actually hurt people, nobody ever gets a secure machine.
Meanwhile, can anybody come up with the firmware for an update-
blocking USB hub?  I have a feeling that when somebody finally gets around to wanting one, they'll want it yesterday.

@_date: 2014-08-14 19:43:24
@_author: Bear 
@_subject: [Cryptography] Encryption opinion 
NIST has been recommending against the use of 1024 bit RSA since 2010.  People are right to be worried about it, although
the cause for worry is not immediate.  It's good enough for data in flight and no 1024-bit modulus has yet been factored (at least not publicly), but it is no longer adequate to protect data at rest.  IOW, your clients don't need to be worried about someone listening in on the traffic in real time unless you're making a key management, padding, or protocol mistake somewhere.  On the other hand, if your clients assume the encrypted text messages get recorded somewhere, they are reasonable to be worried that someone who cares a whole lot and is willing to devote substantial resources to it might break them a relatively short time (less than five years) in the future.

@_date: 2014-08-16 09:18:45
@_author: Bear 
@_subject: [Cryptography] Cost of remembering a password 
Whatever your password manager runs on, is a trusted system - ie, one whose compromise could absolutely destroy your security.  And if it is a conventional system running software, then it is running something invisible and modifiable which I cannot fully inspect, ie, it is not trustworthy.  We must never create trusted systems which are not trustworthy.  A computer system which can reasonably be trusted not to be hiding a security breach cannot execute anything from RAM at all and has the only sofware it runs in firmware consisting of diodes (non-rewritable ROM).  Further, it uses the kind of embedded CPU that has no state, no cache, no firmware, and no bootup sequence.
Until you build one of those, I'm going to continue using an
iron box for a password manager.  Integrating a software password manager is a failure because it puts trusted software
on non-trustworthy hardware.

@_date: 2014-08-18 11:37:07
@_author: Bear 
@_subject: [Cryptography] Cost of remembering a password 
I have to inquire here whether TCG did in fact understand "Trusted" to have its literal meaning -- ie, a system whose compromise can destroy security?  Because, honestly, we have acres and acres of trusted systems running on untrustworthy hardware and untrustworthy OSes.  While we really need serious work on Trusted Computing -- ie, we need to figure out how to have far fewer trusted systems and put them on genuinely trustworthy (utterly simple inspectable tamper-evident)
hardware, that isn't what the Trusted Computing Group was doing.  I have never gotten the impression that all these people really wanted to be manufacturing devices whose compromise could destroy users' security. I mean, I know what they worked on - the whole TCM and infrastructure
for creating software for the TCM that cannot be inspected or altered
without a key.  And yes, that would in fact be a hardware system whose
compromise could destroy security, ie, a trusted system.  But it seems
to me that it had nothing to do with the problem of too many trusted systems proliferating, did nothing to reduce the problem, and did not
even demonstrate an understanding of what the problem with trusted
systems is and why they are bad.
So I have to ask, what understanding of "trusted" did they have?

@_date: 2014-08-18 12:24:39
@_author: Bear 
@_subject: [Cryptography] Cost of creating huge theft targets [Was: Cost of 
This.  This is exactly why I will never, ever, use this feature.
In order for this password to 'sync' across other devices, it has to be stored, in clear or with cleartext recoverable, nonlocally at the site of a trusted service where it is part of
an aggregated theft target having massively greater value than my password alone.
If it were okay to use a trusted system controlled by someone else, there would be no need for a password in the first place.
My bank account password is mine, damnit, must be stolen from me individually, and is not worth any more effort to steal than my own bank account is worth.  Systems like this aggregate hundreds of thousands of bank account passwords creating billion-dollar theft targets that can be stolen in bulk.  Even if the security is a thousand times better than I can do myself, the reward per criminal effort ratio still favors attacks on the aggregate once the aggregate contains more than a thousand bank passwords.  These systems are a net loss in security because they drastically
lower the criminal effort required per dollar stolen.
This is a special case of a 'monoculture' problem; any system where millions of people are vulnerable to the same exploit lowers the amount of criminal effort required to exploit them all systematically.

@_date: 2014-08-22 11:19:08
@_author: Bear 
@_subject: [Cryptography] On 40-bit encryption 
Actually, I think the crypto export security theatre was probably responsible for teaching a huge swathe of the general public what
proxies were and how to use them, who otherwise would never have Because, after all, the procedure for anyone outside the US to download the 'strong' encryption version of any US product, during that era, was to go through any proxy server that happened to be located in the US. Anyway, it was nuts.  It was a US law that cut the profitability of US companies by a noticeable percentage, and simultaneously gave foreign companies an advantage in fielding cryptography products (and therefore in doing cryptography research and acquiring cryptography expertise) - at a time when cryptography was recognized to be important to US defense.  Simultaneously, foreign citizens were given a free object lesson in the use of proxies, acquiring yet more skills that the US would probably have preferred not to be so widespread.  So, yes, the US was shooting itself in the foot as far as policy is concerned.

@_date: 2014-08-22 12:08:42
@_author: Bear 
@_subject: [Cryptography] Cost of creating huge theft targets [Was: Cost 
I guess my real issue is that I can't tell whether or not that's what it's doing. If a compromised device can be made to behave exactly like an uncompromised device to the best of my ability to observe, why shouldn't I assume it's compromised?

@_date: 2014-08-25 11:16:17
@_author: Bear 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
Is there any evidence that CAST5 is in any way inadequate?
People are upset with use of an "Antique" algorithm?  Why?  I would be upset with the use of an "Insecure" algorithm or an "Untested" algorithm.  Into neither of which classes CAST5 falls. So, I say the burden of evidence falls on those requesting a change here.  What is wrong with CAST5 that people want to get
rid of it?

@_date: 2014-08-25 11:47:56
@_author: Bear 
@_subject: [Cryptography] Encryption opinion 
I think the most effective class of phishing attacks right now in fact can easily be beaten.  This is the message that purports to be from someone with whom you have a pre-existing security relationship: your bank, your ISP, a business from which you buy things online, or your email provider, or whatever.  The problem here is that the email client checks keys and returns "TRUE" or "FALSE" rather than returning "IS WHO IT CLAIMS TO BE" or "IS NOT WHO IT CLAIMS TO BE" because right now email clients don't have any concept of who the correspondent claims to be.  They're just
checking that, yes, such a certificate exists rather than checking that it actually matches the most recent certificate seen from this But an email client certainly can have such a concept.  The receipt of even a single authenticated message should establish the concept of an authenticated correspondence identity to which any later email either clearly does, or clearly does not, belong.  So any correspondence that comes from your bank ought to show up in the email client, in a secured bin containing nothing except other email to and from the same bank.  And if it doesn't, clearly that is UI that can give people more of a clue that someone is trying to phish them.

@_date: 2014-08-25 14:52:33
@_author: Bear 
@_subject: [Cryptography] Encryption opinion 
Please don't muddy technical terms.  We need the precision
your argument would destroy here if we're going to have a meaningful technical discussion.  Once we start confusing one thing with another the discussion ceases to be precise and all-too-easily ceases to address what we intend to Phishing is a social attack taking advantage of human confusion.  We can combat it by creating UI's that reduce human confusion.  MITM is a technical attack taking advantage
of protocol weakness.  It needs an entirely different means to combat it, and needs to be considered separately.
This is not to say that phishing is unimportant; it is just as important as MITM and needs just as much to be addressed.
But addressing it does not affect and is not affected by the need to address MITM and conflating the terms in any way
is counterproductive. MITM is precise; if your bank is not trying to communicate
with you, and the phisher is not intercepting the bank's communication in flight, then a phisher pretending to be your bank is not engaging in an MITM. MITM is by nature three-sided.  There is you, your correspondent, and the adversary is someone between you in the communications channel.  If you don't have all three, then you don't have MITM.

@_date: 2014-08-26 12:21:33
@_author: Bear 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
(Massive snip)
See, those are much better reasons for proposing a change.  When someone wants to change and advances no better reason than "It's Old," or "it's an antique from the 90's" or such
that doesn't make sense.  Math doesn't go stale. "Old" just means that it has remained in place with no discovered weaknesses through more cryptanalytic effort, and ought therefore be considered more, rather than less, reliable. But when you come out and admit that cryptanalysts, especially those using more modern methods and attacks, have been actively ignoring it such that it has now withstood LESS cryptanalytic effort than some more recent cipher, that makes a bit less nonsense.  The problem is not its age, the problem is that despite it having been around for more cryptanalytic effort it has actually withstood less. The 64-bit blocksize being too short is also a legitimate technical issue and a good reason for requesting a change, especially when use for file encryption offers so many 64-bit blocks whose contents are completely known to an attacker.

@_date: 2014-08-26 15:15:57
@_author: Bear 
@_subject: [Cryptography] Encryption opinion 
MITM is easier to defend against because in order to do it the attacker must fool both communicants, not just one.  HTTPS is laughable. If you have interpreted me as saying it's adequate, you are dead wrong.  It isn't adequate; its only virtue is that it sucks *VERY SLIGHTLY* less than nothing.
First, you speak truly in saying that it provides only technical protection.  It provides not nearly enough of that to protect against MITM, and even if it did that would still be no effective protection against phishing. HTTPS in fact does not ensure that you talk to your bank.
It ensures only that you are talking to someone who has some certificate issued by one of the umpty CAs recognized
by your browser, and your browser isn't even going to bother mentioning it to you if it isn't the same cert (or the same CA) that your bank uses.
HTTPS also fails to ensure that your bank is talking to you. It doesn't even ensure to them that they're talking to the same person who has most recently pretended to be you.  It ensures only that your bank is talking to someone who knows your password, including both the isochronous phishing attacker who learned your password without your bank's participation,  and my synchronous MITM attacker who got between us WHILE my bank and I were both attempting to communicate with each other.  HTTPS is NOT an effective protection against MITM.  Furthermore, MITM is easier, not harder, to address than phishing, and even if HTTPS were effective protection against MITM it still would not be an effective protection against phishing.
The only reason real MITM attacks aren't widespread in this laughable protection regime is that phishing is so damn much easier for the crooks to do.
There can be no protecting consumers until consumers learn to manage and reason about keys with their conscious minds.  Efforts to spare them from key management and automate it for them will lead only to helplessness and incomprehension in the face of failures.

@_date: 2014-08-26 15:29:15
@_author: Bear 
@_subject: [Cryptography] Encryption opinion 
Phishing is isochronous.  Phishers can get your credential from you without the bank's participation.  Later, they can use that credential
with your bank without your participation.  That is not an MITM; that is simple credential fraud.  The reason we're not seeing MITM right now is because credential fraud
can be simply and easily accomplished without bothering to set up an
MITM.  HTTPS will not be revealed for how horrifyingly inadequate a protection against MITM it is, until we tighten security so far that the fraudsters actually are forced to resort to MITM.

@_date: 2014-08-28 12:28:21
@_author: Bear 
@_subject: [Cryptography] Encryption opinion 
It becomes clear why you are eager to reclassify it as something that the IETF is interested in stopping, but motivation is subjective and facts are objective. The disconnect remains. The IETF is a protocol group; if it isn't a problem that can be solved via protocol, they regard it as being outside their mandate.  Even if there were general acceptance of the *idea* that phishing is an MITM, it would be an MITM of a kind that cannot be solved via protocol, and therefore an MITM that the IETF still would not be interested in.  Whether or not it is called an MITM doesn't matter; your problem is that the IETF mandate is restricted to protocols.
At most, if you got a broad consensus, you'd force them to qualify their statement and say more specifically what *kind* of MITMs are and are not their purview.  In my opinion they've failed to provide a working remedy even for those, but at least they are interested in them. What you want sounds like an appeal to the (as yet unformed) IUTF, Internet Usability Task Force, whose mandate specifically is to recommend remedies to problems - particularly security problems - caused by poor user interface.  That mandate definitely includes
phishing - and many other security concerns, most notably including my own pet peeve that there is no UI indication of the continuity (or lack thereof) of counterparty identities in any cryptographically
secured communication.

@_date: 2014-08-28 12:33:42
@_author: Bear 
@_subject: [Cryptography] cryptography Digest, Vol 16, Issue 26 
I've got one.  It's an envelope lined with copper foil. I get the pass out when approaching a toll booth, and put it back (and put the 'chip clip' back on the envelope to ensure that the foil makes good electrical contact) as I pull away from the toll booth.  A toggle switch would be nice, but we can be fairly confident
that a Faraday cage is working as designed.

@_date: 2014-12-02 13:44:05
@_author: Ray Dillinger 
@_subject: [Cryptography] Construction of cryptographic software. 
It occurs to me that the craft of designing and writing cryptographic
software so that it doesn't leak sensitive information is, at best, a
black art, and most of the necessary techniques aren't widely known,
documented, or shared.
I bet there are a thousand tips and tricks that most of us have never
written down because they are grotty details that aren't very
interesting mathematically.  I would love to hear the coding techniques
that others have developed to control side channels and write software
that is both reliable and doesn't leak information.
Here are a few of mine.
First, I write unit tests for everything.  Often even before writing
the things.  It's astonishing how often crypto code can look like it's
doing the right thing while it's actually doing something very subtly
different that you'd probably never notice was different. Like having
the wrong period for your implementation of some LFSG-based thing like
the Mersenne Twister - get it wrong by one bit, and the output still
looks good, but it'll be insecure and have a period way shorter than you
expected, giving your software  an exploitable bug.  Or checking for
certificate revocation, then accepting a certificate that's been revoked
Or getting encrypt-decrypt that reproduces the plaintext, but doesn't
produce the same output on test vectors as the standard implementation
you're trying to be compatible with (and if it's not producing the same
output, very likely that's the standard implementation that's more
secure than yours....)  Or finding the key server unavailable and then
falling back to accepting a certificate without displaying the error it
was supposed to display about not knowing whether the certificate is
good.  Anyway, write lots of unit tests.  Not only will they make sure
you know what your routines are doing, they'll also tell you when you
break something.
The debug build calls the unit tests, and they  write a file of test
results. The test results file, rather than the executable, is the usual
makefile target, and the makefile instructions for building test results
end with 'cat results.txt' which appends the test result output (ie, a
blank line followed by listings of any unit test errors) directly to the
compile output, just like any other errors that need fixing.
If I get any choice about it, I try to do all the crypto in a
single-threaded, dedicated executable that does very little else.  It's
much easier to analyze and debug a single-threaded application.
Know your compiler options.  Use the diagnostic and standard-enforcing
ones heavily.  Use those that enable security extensions peculiar to
that particular compiler if it helps (such as stack canaries or an
option to zero all memory allocated to your program on exit)  but do
your best to write code that does not rely for its security solely on
those extensions, because sooner or later someone will need to compile
it on a different compiler.  Eschew any standard-breaking extensions
that won't work (especially if they will also cause errors) in different
I write in C using the standard libraries and the GMP bignum library.
GMP has some not-very-widely known primitives that are specifically for
cryptographic use and leave nothing on the stack  or in buffers.
They're a little slower than the "normal" set of calls, but that's okay.
The C standard libraries can mostly be trusted to do exactly what they
say and nothing more.  This is kind of a shame because other languages
have nice facilities, less "undefined" behavior, and considerably more
protection from programmer mistakes, but there is no
way to get around it because AFAIK no other language allows me to
absolutely control when and whether copies are made, when and whether
writes to variables actually happen, etc, as well. Which isn't high
praise for C, because it's still hard and grotty and error prone.  But
at least it's possible.
The runtimes, templates, and libraries that come with most languages
(and here I include C++) can be trusted to do what they say, but without
going over a million lines of difficult, heavily  template code
with a fine toothed comb I can't trust that they do nothing more.
Therefore I don't know how to write secure software in those languages
and be certain that it won't leak information. They accomplish their
semantic goals well, but do so while leaving copies, and fragments of
copies, of everything they touch in their objects, in unused areas of
their allocated buffers until those buffers are actually used for
something else, and on the stack.
A lot of crypto software makes extensive use of global variables for
sensitive values.  They are fast, never get deallocated or
(accidentally) copied during runtime, new values always overwrite
previous values instead of landing at new places in memory possibly
leaving copies of the old values somewhere, and they avoid indirections
that might leave pointers to them lying around.  The only thing even a
little bit subtle is making sure that they get erased as soon as the
program doesn't need them anymore, and again before the program exits,
and that's not hard. A pattern emerges with a dedicated 'eraser' routine
that sets them all to bytes read from /dev/random before program exit.
The read from /dev/random can't be elided by the compiler because it is
a system-level side effect.  But if you read from /dev/random into a
buffer and then copy from the buffer to the sensitive variables, the
compiler can elide that because those are writes to dead variables.
What's necessary is to read bytes from /dev/random *directly* into the
global variables, one at a time if necessary.  It helps if you define a
singleton record type that holds them all; that way you can just
overwrite the record instead of doing one at a time.
That pattern is fine for globals that you clear once or twice per
program run and again on program exit, but I/O, even from /dev/random,
is too slow for using on return from every subroutine that handles
sensitive variables. I kind of don't like using global variables. I
don't like the idea that every part of the program has access to them.
But I have certainly used them.
I use the 'volatile' keyword a lot, to designate local variables so that
writes to those variables will never be skipped, even if writing to them
is the last thing that happens before the procedure they're allocated in
returns. I know of no other language besides C that allows that.
'volatile' allows me to easily use local variables and avoid leaving
anything sensitive on the stack, but do not use it for anything that's
not sensitive.  For example if you use a volatile variable to index a
loop, the loop will run slow because the code has to write that variable
to cache every iteration rather than just mapping it to a register. It's
better to just have a regular auto variable that you use for that.
A very good solution to the problem is to allocate a large 'security'
buffer as a local variable in main(), and have the program manage its
own stack for sensitive variables.  The way that works is that when
main() calls anything, it gives it a pointer into the security buffer,
and the size of the buffer. A called routine checks that the buffer is
large enough and uses as much of the buffer as it needs for its own
sensitive locals by casting the pointer at the buffer into a pointer at
a record type that contains its sensitive locals.  If it calls anything
else that has sensitive local variables, it does so with a pointer just
past the end of its record, and the size it got for the security buffer
minus the size of its own sensitive-locals record.   As with globals,
before program exit you call an 'eraser' that overwrites the entire
buffer with bytes from /dev/random.
The benefit of this is that main() retains control of the buffer.  It
doesn't make the system slower the way 'volatile' does.  And if multiple
routines both read and write in the buffer, the compiler can never elide
writes into it - so the routines can easily and reliably clear any
sensitive vars that they *aren't* passing back to their caller before
they return.  It's probably safer than using 'volatile' local variables,
because it's possible to forget to clear a sensitive 'volatile' before
returning but it is NEVER possible to forget to clear the security
buffer before exiting - that would definitely break your unit tests.
The downside of this is that handling the buffer tends to be a pain in
the  which is why I tend to use 'volatile' instead.  I do use a
designated buffer for VERY sensitive variables, such as passwords.
Absolutely no routine, anywhere, gets to make its own copy of a password
that lives outside the buffer, and main() writes over that as soon as
the key is set up.
I've seen crypto software where sensitive locals were allocated using
the 'static' keyword to ensure that local variables with sensitive
information are not deallocated when the function returns.  This
prevents leaks during runtime, and with static variables, the compiler
can't USUALLY elide writes, so the called subroutine can usually
clear the values of its sensitive locals before returning. But it's  not
a technique I trust, because compilers are ALLOWED to elide final writes
to static variables if it can prove that the initial values of the
static variables don't matter to the routine whose scope they're in, and
static locals are strictly worse than globals for leak prevention
because main() has no way to overwrite all those before it exits. Every
one of them gets released when the program exits, and you just don't
know what's going to be the next program to allocate the block of memory
where they were contained.
I always use unsigned integers.  In fact this is something I learned
rather recently, due to an issue raised on this list.  The basic
mathematical operators (addition, subtraction, multiplication) can
overflow, and overflow on signed integers (with the usual wraparound
semantics that can give a negative result of adding two positive
numbers) is undefined behavior.  If you must use signed integers and
check afterward for overflow/wraparound, you must add them as though
they were unsigned integers, like this:
(unsigned int)z = (unsigned int)x + (unsigned int)y;
because overflow on unsigned integers *is* defined behavior.
You can't even check to see if undefined behavior has happened, because
the compiler will go, "oh, that couldn't happen except for undefined
behavior, and I can do whatever I want with undefined behavior.  I want
to ignore it."
It will then cut the checks and everything that depends on them out of
your program as 'dead code'.  So you can have an integer that is in fact
negative because of a wraparound that occurred while adding two positive
numbers, and the program will jump straight past a check for a negative
value without triggering it.
Crypto coding has taught me to use a few other unusual bits of coding
style. In crypto, we tend to allocate buffers, permutations, etc that
are 'round' numbers like 0x100 bytes or 0x10000 16-bit values.  Because
we're using unsigned integers anyway, indexing into these buffers using
uint8_t or uint16_t variables gives us an automatic range safety check.
 But it is hard to write 'for' loops that exit if we're iterating over
the whole buffer, so instead of 'for' loops I tend to
use do/while loops.  If I want to do something like initializing a
permutation with every value of a uint16_t for example, my usual idiom
is to write something like
count = 0; do {
  permutation[count] = count;
}while (++count != 0);
This is also an example of a habit that suits me when in full-on
paranoia mode to never leave any local variable (such as count) with a
nonzero value if I can help it, whether *I* think that variable is
sensitive or not. If I leave something with a nonzero value on routine
exit, I try to leave the same constant value in it on every exit.
So that's a few bits about writing non-leaking crypto code.  I've been
more concerned with preventing memory-based data leaks, obviously, than
controlling other side channels like timing or power use.
Would anybody else here like to share some of the techniques they use?

@_date: 2014-12-02 14:38:23
@_author: Ray Dillinger 
@_subject: [Cryptography] [cryptography] Underhanded Crypto 
Yes. That was sort of my thought as well.  Using uninitialized memory
as *input* to add to a generator that had a good amount of entropy
before you input the bytes, and which also gets lots of randomness from
other sources, isn't harmful. But relying on uninitialized memory alone,
or even mostly, to produce a good PRNG state is crayzee.
It is far better to initialize it by writing over it from /dev/random.
I've been writing PRNG's recently, but I don't want randomness to
initalize them. In fact I start out by zeroing the state buffers.
I want a key setup routine instead, so as to produce the *SAME* sequence
every time when started with the same key.  It can also  be an
interesting problem to make sure that key entropy is not wasted and is
spread evenly over the whole buffer.  A hash function is the right way
to go to do that.  Passwords generally do not have nearly the 256 bits
of entropy I want to spread over a PRNG state, so key management
(probably with keys loaded into the PRNG state by something that only
requires a password, damnit) will become important.

@_date: 2014-12-02 22:39:11
@_author: Ray Dillinger 
@_subject: [Cryptography] Construction of cryptographic software. 
Thank you both for that pointer.  I wasn't aware that it existed.
I observe that they say don't use the Mersenne Twister.  My response
is don't use it for a secure generator because it's not.  However, it is
an excellent source of bytes to mask with the output of a secure
deterministic sequence generator (the sort of thing that's used for a
stream cipher).  Not because it will make a secure PRNG any less
predictable, but because of the mathematical property that it provably
generates every possible output sequence 19937 bits long or less.
MT - or the LFSG it's based on, skipping MT's trivially reversible
output scrambling function as a feature that consumes CPU energy while
adding no security, sequentially generates every last bit pattern of
that length or less.  So, sooner or later, masking bytes from it will
match up with your secure PRNG to produce absolutely every output that
length or less.
This is a little bit important, because many SPRNGs have not very
much state, and a generator with not very much state will have much
shorter unproduceable sequences.
As an example, Rivest's new pseudorandom generator, Spritz (which is an
update of RC4) has a 256-element permutation, plus 3 bytes of state.  A
256 element permutation is about 1683-and-a-fraction bits of state
(because 2^1683 < 256! < 2^1684) and adding 24 more for the bytes gives
us 1707 bits.  So because it has only 1707 bits of state, it can only
start in 2^1707 different states, and there are provably sequences a
minimum of 1707 bits long that it absolutely cannot produce, because
math.  Because it is a CSPRNG, and must keep at least half its state
hidden from attackers, it's a good bet though I can't prove it, that
there are shorter sequences that it cannot produce, if its states are
distributed over several "orbits" or cycles.
Mask it with the output of an LFSG that has exactly one cycle and
provably goes through every permutation of its possible states on one
orbit, like the Mersenne twister, and suddenly, without losing the
security or nonlinearity of the SPRNG, you jump from sequences of no
more and probably considerably less than 1707 bits length being
impossible, and having it be very difficult to genuinely know exactly
how long the sequences that can't be produced are, to EVERY sequence of
19937 bits or less guaranteed to be a possible output.
This doesn't only matter if the CSPRNG eventually gets sufficiently
broken that an adversary can *TELL* which sequences of outputs are
impossible and use that information to attack a system.  It matters when
we consider the security of a set of 4096-bit keys for an asymmetric
crypto system.  If all the keys were generated by a CPRNG like Spritz,
and I know that there are only 2^1707 possible states that Spritz can
start in, then there are 2^1707, not 2^4096, possible output sequences
that could have been considered as key candidates.  With most of those
being eliminated by prime search or whatever, the field can be a lot
thinner than the 2^4096 would lead you to believe, and the odds of
collisions, birthday paradoxes, etc and the effect of those things on
security, have to be realistically adjusted.

@_date: 2014-12-03 09:10:41
@_author: Ray Dillinger 
@_subject: [Cryptography] [cryptography] Underhanded Crypto 
Using uninitialized memory
Don't wanna pick a fight here, but I gotta point this out.
If that's what's going on, then zeroing the memory before
doing it won't cause a vulnerability.

@_date: 2014-12-03 09:21:16
@_author: Ray Dillinger 
@_subject: [Cryptography] Why Alexander Hanff won't be using "Let's 
Honestly?  I think that may be a good thing.  TLS certificates
are not a good security solution; I think people need to not
be confident in them.
In fact, if there are any well-meaning extremists on the project,
(and there are) that may have been the whole point of the Let's
Encrypt project in the first place.  While people still believe
in TLS there is no possibility of progress in security because
everyone just points at TLS and thinks it's taken care of.  If
you wanna build anything better, and something is in the way, it
has to be destroyed.
Public (over)confidence in TLS was in the way.  Let's Encrypt
is the easiest way to destroy it.

@_date: 2014-12-03 09:31:10
@_author: Ray Dillinger 
@_subject: [Cryptography] Construction of cryptographic software. 
No desktop OS in wide use.  But within the last ten years, I have worked
with Solaris and AIX kernels that did not zero pages. I
have no idea about phones and things running on ARM, so I try not
to assume it.

@_date: 2014-12-03 12:23:10
@_author: Ray Dillinger 
@_subject: [Cryptography] Mapping numbers to permutations 
Lately I've been working with permutations, because they are at the core
of a family of deterministic pseudorandom bit generators I've been
One of the things I used was a bijective numbering scheme that
identifies each permutation with a unique integer.  It's pretty
straightforward.  Say you've got a number between 0 and 720, and you
want to map it to a permutation of six objects.  You load the number
into a register, then proceed.
Your first choice is among 6, so you take the register mod 6 to get the
choice, subtract the choice from the register, then divide the register
by six.  (and you get a choice 0..5)
The second choice is among the 5 remaining, so you take the register mod
5 to get the choice, subtract the choice from the register, then divide
the register by 5. (and you get a choice 0..4)
The third choice is among the 4 remaining, so you take the register mod
4 to get the choice, subtract the choice from the register, then divide
the register by 4.  (and you get a choice 0..3)
And so on. I've applied this technique in reverse before to create
permutations for S-boxes based on pseudorandom numbers in an
experimental block/stream Feistel cipher that uses different
pseudorandom S-boxes for each block.  (That cipher is very secure, but
VERY slow, so not competitive).
What the numbering scheme means for analyzing permutation based RNG's is
that a set of a permutations can be mapped to a state machine, with the
states numbered by the above scheme and transformations of those
permutations (such as the swaps in RC4 or the newer Spritz generator)
becoming edges in the state graph. Then if you can prove things aboout
the state graph you can make meaningful statements about the generators.
This procedure is very much like the procedure for converting bases.
You have this number, and you are taking the number modulo some value,
subtracting whatever you get from the number, and dividing by the same
value. Repeated getting a digit modulo ten then dividing by ten is the
classic machine-code way to get a base-ten representation, one digit at
a time, out of a base-2 register.  The only difference here is that the
base changes each digit.
Now here is an insight new to me, though someone else has probably
already had it.  This is also a bijective way to convert bases in the
output of nonbinary deterministic PRNG's to binary.  Let's say you have
a deterministic bit generator based on a permutation of 5 elements which
produces output (0..4) each iteration, and you want to transform the
sequence of pseudoentropy from that base-5 stream into an unbiased
binary pseudoentropy stream with a 1-to-1 mapping from the base-5 stream
to the base-2 stream.
That's fairly easy.  You fill a pseudoentropy register by repeatedly
multiplying by 5 and adding the generator output.
Then you can take binary output by taking the low bit of the
pseudoentropy register and shifting right to divide by 2 (taking base-2
digits from the integer).
Whenever the pseudoentropy register gets to be small enough to hold
another base-5 digit, you multiply it by 5 and add the output of the
generator (essentially you'd be 'replacing' the base-5 digit you most
recently took, had you been taking digits in base-5 rather than base-2).
What is interesting about it is this: Most conversion schemes for base-n
pseudoentropy to binary pseudoentropy have one of two problems: either
there can be multiple input streams that produce identical output, or
there is bias in the output stream because some inputs count for more or
less pseudoentropy than they're worth.  This scheme introduces no bias,
but preserves  the property that there is a one-to-one mapping from
input streams to output streams.  And this is desirable when trying to
show that there are no 'aliased keys' that produce or converge on the
same keystream.
Anyway, this isn't interesting for  streams of real entropy, because if
you're working with real entropy, then repeatability and one-to-one
mappings aren't important.  But if working with pseudoentropy - mostly
keystreams emitted from deterministic generators - it's interesting.
Just a little math I thought it would be interesting to write up and
pass along....

@_date: 2014-12-03 22:25:17
@_author: Ray Dillinger 
@_subject: [Cryptography] new PRNG family 
It is interesting, but you're relying on the exact format of
floating point numbers, correctness of the hardware implementation
of operations on them (*cough* FDIV bug *cough*), and rounding
modes which you must control exactly in order for your code to
work the way you think it will. Honestly, these are things
that vary much *less* from system to system and machine to
machine than they used to, but they are still a cause for
concern.  I would worry about using an FP-based generator on
any platform other than the one it had been developed and
tested on, or even on any platform where the default rounding
mode can be set with a system call.
Anyway, I don't want to get sufficiently elbows-deep in float
format and representation to do that analysis.  But if you want
to analyze this, Here are some things to check for.
Given your assumptions about IEEE float representation and
the default rounding mode, you already know that each state
has a unique successor.  Now try to prove that each state also
has a unique predecessor -- ie, for each total state your
generator can be in, there is only one possible prior state
which could have produced it. If the unique-predecessor
property is not true, then one of the predecessor states
(along with all of its predecessors, etc) is not on any cycle,
and that is bad.  You want as many states as possible - in
fact, all of them - to be on cycles.
A cycle is a set of states, each leading to the next in sequence.
Because you have a finite number of states, and each state has a
unique successor, every traversable path through the states of
your generator must eventually comes back to whatever state you
started in if your starting state is even *ON* a cycle - which
it probably won't be if your states don't have unique predecessors.
That is why the unique-predecessor property is important.  States
that aren't on cycles are unreachable, and don't count toward
security. Ideally, every possible state is on the same huge cycle.
If your generator has several equal-length cycles, then no keys
are weaker than others - but none of them is as strong as it would
be if all the states were on the same cycle. If it turns out that
some cycles are much shorter than others, your PRNG will have a
class of weak keys for every such short cycle.
How many bits of state does your generator have?  That is, let
S be the number of valid, reachable states your generator can
be in.  Log2(S) is the number of bits of state your generator
has.  IE, if you map all the valid states to numbers, you
would need this many bits to represent a number that size.
If your adversary knows your PRNG's state, the game is all over
because it's deterministic.  You're producing pseudoentropy with
no input of real entropy.  And each output reveals state to
the adversary.
Your objective is that your generator should change its
state faster than the bits it reveals allow an adversary to
determine the state.  By the time an opponent sees a bit,
the bit should be a function of every part of your state.
And every part of your state should depend on that bit
(or on the transition that produced it) by the time he
has seen enough further bits to reveal that state.
For a generator with N bits of state, it takes N+1 bits of
output for an analyst with Godly computer power to determine
the total state.  But most analysts have merely mortal compute
power at their disposal, and if they can explain the most
recently observed N-1 bits of output in terms of any of
2^128 or more starting states which they would have to check
through one at a time, the N+1 bit isn't going to do them
any good.
The security of a PRNG fails when an adversary doesn't have
to check all those possible prior states one at a time. For
example, in an LFSG, each output tells an adversary a whole
bit about the future state of the generator which won't
change until another output reveals what it's being
replaced with, so after he's seen N+1 bits of output he
knows all of the current and therefore all future states.

@_date: 2014-12-04 11:02:48
@_author: Ray Dillinger 
@_subject: [Cryptography] Mapping numbers to permutations 
Eggs Ackley.  A random number between 0 and some power of 2
yields that many random binary bits, but a random number
between 0 and some power of 5 yields that many random binary
bits only if it is also between 0 and a power of 2 less than
the power of 5.
And the usual solution is....
To throw away any base-5 number you get that isn't also in the
range of the highest binary power that's less than the quintenary
power. But that throws away those base-5 digits, and the result
is that you get a mapping from input stream to output stream which
is a many-to-one mapping, ie, it's not invertible, so there are
a bunch of useful operations that don't work with it as a keystream
or a couple of other things.
The technique I posted about was a way to get an unbiased stream
of binary bits that has a one-to-one mapping to the input stream,
ie, there is only a single sequence of base-5 digits that could
result in a particular binary stream.

@_date: 2014-12-04 13:20:36
@_author: Ray Dillinger 
@_subject: [Cryptography] new PRNG family 
Well, I don't mean to be dismissive, but rounding means information loss
from your generator, and every time you lose information from
your generator you get a halfbit closer to being in a known state.
To put it another way, your generator will gravitate fairly quickly
onto cycles where there is no rounding at all (more precisely onto
cycles where the rounding happens in the same direction every time
through the cycle).
Every possible state leads into one of those cycles, but only some
of your states are actually *ON* one of those cycles.
That means an adversary looking at a stream of output doesn't have
to consider most of the states your generator could be in.  If he
assumes it's in one of the states that is on a cycle, he'll be right
unless your machine happens to be on one of its first few hundred outputs.
So the question becomes, how much of your state is useless for purposes
of security?  There is a way to at least guess.  If every state has
exactly one successor, but some states have multiple predecessors,
then you have at least as many states with no possible predecessors.
Those are "leaf nodes" in a state diagram, and you have to find a
way to identify them.
Then initialize your generator and run it forward past a few
hundred points where you lose information to rounding, then try
running from the state you reach, backward along all those paths.
What you will probably discover is that within a  dozen such
decision-point branchings, the vast majority of possible paths
that could have led to the reached state, will be shown to begin
in leaf nodes that could not have been reached from any state.
Then you can form an estimate at least of the ratio of unreachable
to reachable states.  Every state along a path that begins in a
leaf node state is unreachable.  Every state along a path that
begins in a branching node beyond which both paths are unreachable,
is also unreachable.  States you have actually traversed, may or
may not be reachable.  You should assume that the first N states
you traversed, where N is the length of the longest unreachable
path you've discovered, were on an unreachable path.
This matters because the ratio of unreachable to reachable states
is likely to be very large.  If it turns out that most of your
possible states are unreachable, then your generator necessarily
has much shorter cycles than it could, because none of those
unreachable states are on the cycles.  And we still haven't
considered how much the periods are shortened by the states
being divided up among multiple cycles  or how to tell how
many different cycles there are.
This is all general statistics, not real analysis of the specifics
of your particular generator.

@_date: 2014-12-04 14:25:48
@_author: Ray Dillinger 
@_subject: [Cryptography] converting one base to another 
True.  Hmm, I thought I had gotten there, but what I got
was actually more on the lines of *LESS* biased and *MORE*
bijective than other solutions I'm aware of.
I read your "float" solution and it's mathematically correct,
but conceptually, I think I like a slightly different formulation
of it better, with two different registers:  One to hold power
information, and the other to hold bits.  You'd initialize both
with the value '1'.
The power information is a double float, and you multiply by
5 every time you add a base-five digit then divide by 2 every
time you take out a bit.
The bits register would hold an integer value, and you'd
multiply by 5 and add the input every time you added a base-5
digit, then take the bottom bit and divide by 2 every time you
took out a bit.
You would add a base-5 digit every time the power register
showed less than 2^61, and take a bit out every time the power
register showed less than 2^61.
You're right that it can never be perfect; bijection and
bias avoidance both fail once every time accumulated rounding
errors cause the magic value of 2^61 to fall between the
theoretically correct and calculated value in the power
Errors in bias (on the order of 1 in 2^61) would occur in
both directions as the process continued and be overall
quite negligible, but failures of bijection would accumulate,
and make most of the 'nice' properties I had in mind valid
only on streams of a few tens of thousands of digits or
less.  Darn.

@_date: 2014-12-05 11:30:32
@_author: Ray Dillinger 
@_subject: [Cryptography] new PRNG family 
Yes.  Actually, for starters just see if you can do it for a single
state.  Can you find all the different states that could be *immediate*
predecessors of any single state?
For example, if I have an integer "generator" with one word of
state, and the transition function is S=S/2 + 6, when I know
that the current state is S=9, then I know that the immediate
prior state was either S=15 or S=16.
But because there can be at most one cycle that includes the
current state, I know that therefore at least one of these
prior states (and all of its prior states, etc) is unreachable
(ie, not on any cycle).
Indeed, the above is an unfair example because S=S/2 + 6 has
only a single one-element cycle - at 6.  So even the current
state, where S=9, is not reachable on any cycle.
This is a degenerate "generator" because we lose a whole bit of
information due to rounding, *every* iteration, from every
possible state, until we have zero bits of state left when we
get to the cycle S=6.
Most generators - especially FP generators -- that people come
up with have a similar problem, though not that bad a case of
it.  There is some theoretically huge number of possible states
as measured by bits devoted to representing state information,
but as the generator iterates forward, its path is gradually
constrained by rounding errors as information is lost, until
finally it settles into a cycle where it approaches each
potential branching point on the cycle from the same single
prior state every time.
Often this stable cycle is shorter, by a factor of billions
or more, than the designer of the generator expected, because
of all the bits that have been lost due to rounding.  So
the *effective* state of the generator measured as log2(number
of states on a stable cycle) is much less than the number
of bits devoted to *representing* state.
To figure out how many bits you lose due to rounding, you
need to get some idea of how much effective state your
generator has.  So follow a path forward through a few
hundred such decision points, then try to go backward
along paths that *could* have led to the state you've
reached, and see how far you get on average before hitting
dead ends -- that is, states that no possible prior state
could have produced.  You don't have to find them all,
but try to get enough for statistical sampling.
If you can go backward through, say, ten (2-way) decision
points on an average path before hitting a dead end, that
means that at least 2^10 of your states are unreachable for
every state that is reachable.  That would mean your
generator's effective amount of state, for purposes of
security, is at least 10 bits less than the number of bits
required to enumerate the valid starting points.
It would also mean that every possible key or starting state
has at least 2^10 alias keys or starting states whose output
will converge within a few hundred transitions, to the same
point on the same cycle.  And finally it would mean that
your generator's cycle is shorter by a factor of 2^10 than
you might have hoped.
The bad news is that it's common for generators that people
design on their own to lose literally hundreds of bits of
state before settling onto their cycles.
The good news is that if you lose only three or six or ten,
and you've managed to get all the reachable states on the
same cycle, you're doing way better than most people's first

@_date: 2014-12-09 19:41:50
@_author: Ray Dillinger 
@_subject: [Cryptography] North Korea and Sony 
The hell of this is effective security doesn't have to be anywhere
near this hard.  We are fighting things that shouldn't even be on
the  field in the first place because they were bad ideas from the
beginning. Everybody who understood them already knew what
vulnerabilities were implicit in their operation, and JUST DIDN'T
GIVE A DAMN because OOH, NEW FEATURE, NEAT!!
I guarantee that if software and hardware manufacturers could be
and routinely were successfully prosecuted for insecure products,
we would see within two years a generation of *DRASTICALLY* more
secure systems.  They would have reduced feature sets, but if
insecurity became a real expense for the people producing it,
the marginal additional revenue from increased feature sets would
be offset.
We shouldn't have to work out how to PREVENT mail clients from
opening executable attachments; we should be establishing legal
frameworks for recovering the entirely forseeable losses from
the criminally negligent entities who make mail clients which

@_date: 2014-12-11 12:44:45
@_author: Ray Dillinger 
@_subject: [Cryptography] North Korea and Sony 
Right now there do not seem to be any capability-based secure
Operating systems that have reached a level of development
making them viable as real options for real companies to be
using for everyday work.
Could this be fixed?

@_date: 2014-12-11 15:53:01
@_author: Ray Dillinger 
@_subject: [Cryptography] Cipher having a universal polymorphic-decryption 
I have developed an interesting and peculiar cipher. I don't believe
that there's any proper application for it because it is very slow, but
it has at least one rather astonishing and potentially underhanded
property, so I thought I would describe it and see if anyone here has
any idea of a good (or evil) use for it.
This is now my entry for the "underhanded crypto" contest recently
announced here, although I can't immediately think of any terribly
underhanded use for its astonishing property.  I'm confident that it has
an underhanded use, but I just can't think of it.  Perhaps that's just
because I am not sufficiently devious.
There was an earlier version that had security problems, but on
polishing up the code, I found the mistake I had made (a mathematical
proof which I had misinterpreted) and was able to fix it.  The current
version is now less "Fatally Flawed" than the original, but definitively
has more potential for "Underhanded" use.
In construction it is a Feistel block cipher of four rounds. The S-boxes
are constructed by (uniformly distributed) pseudorandom selection from
among all possible sets of S-boxes. Each block encrypted uses a
different set of S-boxes.  The P-boxes are fixed and utterly generic:
one bit from each S-box to the input of each S-box of the next round.
There is a version with 32-bit blocks using 4-bit S-boxes (permutations
of 16 bitpatterns), and a version with 128-bit blocks using 8-bit
S-boxes (permutations of 256 bitpatterns).
The selection of S-boxes is driven by a key stream, generated from a
cryptographically secure pseudorandom generator initialized using a key,
as for a stream cipher. In fact this whole thing began as an experiment
in how to protect stream ciphers from bitflipping and
repeated-keystream vulnerabilities.
The security of the cipher rests on the unpredictability of the key
stream.  If the keystream is unbiased and unrepeated, as with stream
ciphers, I know of no conventional cryptanalytic technique that will
make any inroads into it. In order for any conventional cryptanalytic
technique to be meaningful, it is necessary to have a repeated
keystream, giving the adversary more than one block encrypted using the
same set of S-boxes. In that case I believe that it is at least *AS*
difficult to make inroads against blocks having a single keystream
location in common, as against any cipher having its block length, and
that discovering the complete permutation (key) of any keystream
location  should give an attacker no insights against other blocks of
the same keystream.
Feistel ciphers of four rounds have been proven capable of implementing
literally every permutation of bit blocks depending on the S-boxes used,
so this gives the peculiar effect, in common with stream ciphers, that
if the pseudorandom generator used can be used to generate any sequence
of the length required to construct the S-boxes for a given  sequence of
blocks, a given plaintext of that length or less can be encrypted as
literally *ANY* ciphertext of the same number of blocks.  Conversely,
given a ciphertext, it can correspond to literally
any possible plaintext, depending on the initialization of the PRNG.
Therefore if used with with a PRNG that permits one to find an
appropriate initial state to "decrypt" to a selected message, you could
find keys that permit you to decrypt any short ciphertext to a chosen
plaintext message if someone with a court order, rubber hose, or $5
wrench requires you to give them a key.  This is the (at least
potentially) underhanded bit.
I have implemented it with a Completely Ridiculous CSPRNG that meets
these requirements: it is provably capable of producing absolutely any
sequence of bits up to 850Kbytes or thereabouts, *AND* you can find a
key/initial state that permits any chosen initial sequence of that
length or less to be output.  So you can find a set of S-boxes required
to encrypt/decrypt a given input to exactly match a desired output, one
block at a time, and set the PRNG to produce the output, in sequence,
that will produce exactly the desired S-boxes to transform a given input
to your selected output.
The Completely Ridiculous CSPRNG is brutally simple in construction and
requires just as much memory as you'd expect that it would.  It takes a
good known CSPRNG and masks its output with a massively large Linear
Feedback Shift Register.  The LFSR is in no way cryptographically
secure, but using its output to mask with, provides a proof that the
resulting PRNG can (and eventually will) produce every possible sequence
of bytes up to the length of the LFSR state.  The CSPRNG can be run, its
output recorded, and the contents of the LFSR state selected to
complement the CSPRNG output stream to produce any chosen output stream
up to the length of its state.  Because it is possible to find a set of
S-boxes that will cause a given block to be transformed to any other
block, it is possible to "gimmick" the PRNG state; one can find an
initial PRNG state which will decrypt any input to some chosen output.
Nevertheless, the PRNG output is otherwise just as secure as the output
of the CSPRNG used to drive it. The current implementation uses a
conservative (indeed, excessive) version of Rivest's "Spritz" generator,
with its state information widened to a permutation of 16-bit values,
but retaining 8-bit output per state transition.
Now, as to the reason why it is slow....  It requires 54 bits of
pseudorandom keystream per bit encrypted (when implemented in 128-bit
blocks) or 40 bits of pseudorandom keystream per bit encrypted (when
implemented in 32-bit blocks).  In addition to the time spent generating
pseudorandom numbers, time is spent using that pseudoentropy for
constructing the S-boxes for each block and then actually performing the
encryption and decryption.  And the PRNG is enormous defeating on-chip
cache.  So in terms of speed, it is a snail among racehorses.
As you may have guessed, the length of a "key" that can decrypt a given
ciphertext to any chosen plaintext is therefore 40 or 54 times the
length of the ciphertext/plaintext pair, and therefore use of such a
"key" would be rather obvious unless it is the case that *ALL* keys
used with the system are that length.  Predictably, the size of keys
indicated is a bit over 850K, enabling messages of up to 14Kbytes or
so to be encrypted (or decrypted) to any chosen ciphertext (or
plaintext) by selection of a particular key.
This makes key distribution and management, if this particular
capability is desired, just as large (in fact 54 times larger) a problem
as it is for one-time pads.  On the other hand, if initialized from a
key of 128 bits, it is as secure as any other cipher of 128-bit security.
Obviously, the Completely Ridiculous PRNG can be used to create a stream
cipher having the same universal polymorphic decryption property
applicable to much longer messages, but in this application you would
have the usual problems of stream ciphers, in messages becoming entirely
insecure in the case of repeated keystreams and in vulnerability to
bitflipping attacks - necessitating MACs, initialization vectors, etc.
So that's a very secure but ridiculously slow block cipher having a
potentially devious univeral polymorphic encryption property.  Can
anyone think of a single sane use (or a suitably underhanded use) for it?

@_date: 2014-12-12 14:57:58
@_author: Ray Dillinger 
@_subject: [Cryptography] Cipher having a universal polymorphic-decryption 
It's actually even crazier than that.  Because each block is subject to a
chosen *complete permutation*, you can create a single key which maps
many different chosen plaintexts to different chosen ciphertexts,
subject only to the limitation that no block is repeated in position -
easy to satisfy with 128-bit blocks.
So you could construct a key that transforms, say, a boring stock
performance report into an entirely opaque ciphertext, and then on
encrypting again using the same key (rather than decrypting) the
ciphertext would become a "sensitive" letter outlining corporate
strategy - and on encrypting again, still using the same key, a
message that says where the missiles will be launched from and what
time the bombs that have been planted in your enemy's nuclear plant will
You'd need to have all of these texts in hand when creating the key; the
distinction between message and key is reversed except when talking
about messages greater in length than the number of blocks to which the
PRNG's output can be preselected.
Mathematically, I'm fairly certain that you could embed at least 30 such
chained messages to be "decrypted" from a given text, before the math to
find the desired set of S-boxes to set the initial PRNG state starts
getting Hard.
Alternatively you could create a key that would transform each of 30
different plaintexts into different chosen ciphertexts; you can use
that key as a normal symmetric key for encryption and decryption with
all other messages, but at certain moments, if you need to transmit
exactly one of the 30 preselected messages, you can do so using a
ciphertext that looks exactly like a completely innocent preselected

@_date: 2014-12-18 09:23:21
@_author: Ray Dillinger 
@_subject: [Cryptography] OneRNG kickstarter project looking for donations 
In fact it is known that they have done so - but as far as can be told
from the Snowden papers this only happens as part of TAO (Tailored
Access Operations).  And TAO is used only as a last resort - when
there is a *specific* target whose information they have a pressing
need to get for some very specific reason, but whose computers they
can't otherwise break into.
TAO requires deploying agents in the field to get to single targets,
so it is VASTLY too risky and expensive for the kind of ubiquitous
surveillance that constitutes the threat for ordinary users.
We can hope that at some point TAO requires better controls than
expense, such as actual search warrants.  But I'm not holding my
I'm vastly more worried about the fact that most computer hardware
is manufactured in countries whose human rights records are worse,
and whose governments are more corrupt and ruthless, than even the
United States.  A few forced modifications in basic chip designs
at the major manufacturers are very easy to cover up. Get to the
chip mask, and not even the people making the chips have to know
about your backdoor, let alone the people who are actually
putting together the routers and switches, much less the trusting
souls who buy them.  That would be very cheap (effectively no
marginal cost once the chip mask is substituted), effectively
risk-free, very hard to detect, and *DOES* lend itself to
ubiquitous surveillance.
So it would not be at all surprising to find that Chinese agencies
have even more capability to break into anything they want to and
perform mass surveillance than the NSA.
And that is the sort of threat, if you remember, that the NSA was
supposed to *defend* us from.

@_date: 2014-12-18 10:03:42
@_author: Ray Dillinger 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
Darn right they should.  So should we.  Given what we've learned
about hardware compromises, no nation should be trusting its
critical infrastructure to hardware manufactured in any other
As long as nations believe they can get foreign secrets by
compromising the security of domestic production, no producer
anywhere will be allowed to produce secure hardware.  The
only way to take away that belief is to make the fact false.
I would fully support a UN treaty whose signers agree to charge
at least a 100% tariff on all imports of chips, routers, switches,
and computers. And probably operating systems too while we're at
it, even though that's a huge moneymaker for the US.
It would mean everybody uses mostly domestically produced network
infrastructure, and thereby remove a huge motive for all
governments to force manufacturers to produce insecure network
And, at the same time, it would provide a motive (ie, self-
defense) to have their domestic manufacturers producing secure
In the interests of security *EVERYWHERE IN THE WORLD* every
nation should be manufacturing its own infrastructure.  To do
otherwise is to provide governments with perverse incentives
that compromise everyone's security.

@_date: 2014-12-18 10:08:35
@_author: Ray Dillinger 
@_subject: [Cryptography] Any opinions on keybase.io? 
Dude, that's just plain wrong.  Johnny's never going to be secure until
Johnny learns that he has to manage his own keys. If his email provider
can switch it on and off, his email provider will be *FORCED* to switch
it on and off based on the interests of someone who isn't Johnny.

@_date: 2014-12-19 10:58:16
@_author: Ray Dillinger 
@_subject: [Cryptography] OneRNG kickstarter project looking for donations 
I would say that "one of the EFF team working on TOR" is a fairly
specific target as opposed to ubiquitous surveillence, and the
fact that he found out about it means that the risk of exposure
involved must have been nonzero.
Now, whether such a person *ought* to be a target for TAO is an
entirely different debate, but that is a fine example of deploying
actual human effort to compromise a single specific target that
they apparently couldn't get to with less risk and effort.
And, in fact, it seems very likely that they intended to leverage
whatever they stole from and installed on his laptop for purposes
of making it easier for them to do ubiquitous surveillance on
TOR users.

@_date: 2014-12-19 11:20:55
@_author: Ray Dillinger 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
You got any proposals for convincing governments to at least try
to resist sabotaging the security of network infrastructure?
I'm listening.
No, it would be jingoist if it promoted one nation at the expense
of all others.  It doesn't.  My point is that EVERYBODY should
have no reason to compromise the security of their products. I
concentrated on nations for the sole reason that when someone
wants to make secure hardware, their government (ie, the nation
state in which they work) is the only power that can (and
evidently does) legally forbid them from doing so.
Governments, by sabotaging infrastructure at production, have in
fact made themselves into the primary security threat facing
citizens all over the world.

@_date: 2014-12-19 20:41:24
@_author: Ray Dillinger 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
Okay, you're right.  It's a stupid idea, and it would cripple
the hi-tech economy.
Now, propose ANY other idea that will convince governments that
they have more to lose than to gain by compromising security at
the factory in the production of network infrastructure.
I'm listening.  I'll withdraw my support for the stupid idea the
minute I hear something even a single millidumb *less* stupid
which would actually work.

@_date: 2014-12-22 09:11:12
@_author: Ray Dillinger 
@_subject: [Cryptography] Certificates and PKI 
Honestly, I think pinning is necessary to the customers because
they *NEED TO KNOW* when the keys (ie, the website) change hands.
If I've been doing business with, say, Amazon, and one day I go
to amazon.com and I see "this site is using a different certificate
than last time you came here" I think that's valuable information,
and I will be absolutely sure that the site knows things that only
Amazon should know - such as my order history - before I'll think
of trusting it for small orders.  And it means I'm not going to
be doing any large orders (say, more than $10) with Amazon that
day.  Before spending major money, I'll be watching the news for
a couple of days.  When Amazon calls a press conference and
announces they lost their key and are now using a new one, then
I will start trusting that the new cert is legit, and pin it. Or,
if I have no large purchases to make, I'll pin it after a few
small ones go through and get delivered, provided no major news
stories break about a someone putting up a fraudulent fake Amazon
site in the meantime.
If Amazon thinks that's unacceptable, or aren't willing to announce
it publicly when they lose their key, then they'd damn well better
keep track of their key.
You put up your site with a new certificate, publicize the fact
that your old key is now void, EAT the expense of a few days of
lost business while your customers check it out, and continue.
Replacing a key you failed to keep confidential, including the
lost business you incur because you failed to keep it confidential,
is part of the cost of doing business.
This kind of announcement to the attention of actual consumers
rather than all behind the scenes and invisible, is how the CA
business should have worked from the start. The decision to
trust a new party should never be invisible to the people who
are trusting the new party with their money.
Again, if that is unacceptable, then people need to start taking
security seriously, and implementing serious computer security
infrastructure so they don't get their keys stolen.
Only once big business has real money on the line, will we
see the whole new infrastructure of security in software and
operating systems that is needed to add value to every business
and protect every individual!

@_date: 2014-12-24 12:30:23
@_author: Ray Dillinger 
@_subject: [Cryptography] floating point 
All this discussion makes me wonder how many who consider
themselves "experts" are surprised and baffled by languages
wherein numerics DON'T work in any way resembling IEEE
One implementation of common lisp has bignum integers by
request, and otherwise their numbers have numerator and
denominator both of up to 64 bits, plus binary and decimal
exponents, both of up to 32 bits.  Oh, and it gets yet
weirder because if the denominator is negative, that signifies
that the number is inexact (ie, rounding errors or inexact
functions such as logarithm have been used in obtaining it)
and that instead of the rational interpretation, the value
of the other 63 bits of the denominator should be read as
extending the numerator.
It's all very clever, high precision, and gives exact results
for a lot more of the cases where people expect them, but the
prospect of doing numerical analysis and determining error
bounds in that environment gives me hives.
The PRNG using floating-point operations recently discussed
here absolutely depends on particulars of IEEE binary
representation; if a straightforward translation to common
lisp were run on the above numerics, or a straightforward
translation to Cobol were run on the base-10 numerics
specified for that language, it would produce an entirely
different sequence with different security properties.

@_date: 2014-12-27 12:06:43
@_author: Ray Dillinger 
@_subject: [Cryptography] floating point 
The IEEE floats are, mathematically speaking, are a set of
numeric values which approximate (though not very closely)
a logarithmic distribution.
This has always annoyed me somewhat.  If you're going to
approximate a logarithmic distribution, why not just make it
BE a logarithmic distribution?  Define your numeric value as
the result of raising some root to the power of the bit
representation, and you get a logarithmic distribution that
minimizes relative error better than the system IEEE is using
now.  Further, you get efficient and accurate(!) FP
multiplication and division using the same hardware you use
for integer addition and subtraction.
Of course you'd still have inaccurate addition and subtraction,
but heck, you've got that now.  You could at least get
multiplication and subtraction right, which is better than
IEEE does.
You still need an analogue of "denormalized" floats right around
zero because it breaks down there for the same reasons the IEEE
mantissa+exponent system does - but you need fixedpoint
representation anyway! You've got to have it for things like
modeling software and accounting, etc. where you're trying to
minimize or eliminate absolute error rather than relative error,
so using it for numbers near zero doesn't really increase
overall requirements.

@_date: 2014-12-27 13:41:22
@_author: Ray Dillinger 
@_subject: [Cryptography] General security infrastructure suggestion: where to 
This is not specifically cryptographic, although an implementation
of it in a secure file system might be.  Any suggestions about
what would be a better venue for bringing this up would be
The Unix permissions convention of root:group:user is IMO no
longer really adequate for the security issues we're facing at
this point, and should probably be replaced with a convention
of root:usergroup:user:exegroup:executable.
The evolution I propose changes the structure of file permissions
to keep track of a program/program group having privileges over
that file, and gives the users access to tools for administering
the privileges of programs/program groups running under their
own login.
Regulating privs by root:group:user was the right answer when
the only thing we really needed to protect was the system
integrity, and the question was simply "whom can we trust?"
But that's no longer the case.  Users (not just root) now value
system integrity mainly (or only) because system integrity is
necessary to protect assets far more valuable to them than the
system itself.
But, while necessary, it is not sufficient. The primary threat
to those assets comes not from other users, but from software
running under their own account, with their own privileges, that
does things using their privileges which they did not anticipate
and would not approve of that software doing.
The trust issue between individual users' assets and the
privileges given the executables that they run is now as severe
in terms of preventing losses to individual users, as the trust
issue between system integrity and the privileges accorded
individual users.
Accordingly, I think a future evolution of Unix probably ought
to have users able to control delegation of their privileges to
executables running under their login, in exactly the same way
that root controls the delegation of privileges to user accounts.
So, a user who has just downloaded "SOOPERGAME.SWF" may have
no problem with it having privileges to read and write in its
own directory in his home folder, but has no real reason to
trust its authors and absolutely would not give it the
'executable group membership' or however this works, that it
would need to read/write in his customer database, mailbox,
or bitcoin wallet.
Which would mean that securing operations should check the
privileges of both user (to be sure the user has the authority
from root) and the executable (to be sure that the executable
has the authority from the user) before allowing an operation.

@_date: 2014-12-27 20:04:58
@_author: Ray Dillinger 
@_subject: [Cryptography] Cryptonomicon and Solitaire 
First look at
for some information about a bias in the output of Solitaire.
I haven't found any indication that someone has leveraged this
into an attack, but it exists.
Moving on from there, pretty much all the card decks sold in the
US support games that call for a "high" joker and a "low" joker,
so the different jokers are not something Stephenson (or Schneier)
just made up.  Different decks do it in various ways - some have
suns on one joker and moons on the other, some are printed color
on one joker and b/w on the other, etc.  So check the backs
carefully because that's always a good idea, not because two
distinguishable jokers indicate that someone is pulling a fast
The most ubiquitous brand sold in the US, "Bicycle" decks from
the United States Playing Card Company, has the same graphic
printed larger on one joker and smaller on the other.

@_date: 2014-12-29 10:27:22
@_author: Ray Dillinger 
@_subject: [Cryptography] Of web form passwords etc 
This isn't the cookies/remote website etc, this is Firefox
being "helpful."  It keeps a cache for repetitive forms you
fill out, indexed by the fieldname of the entry box and the
URL of the site.
It will politely ask whether you want it to store your
password for this site before storing something for a
fieldname of "password" or "Password" etc, but if the
field is named something else, it's just another string....

@_date: 2014-02-06 19:34:43
@_author: Bear 
@_subject: [Cryptography] cheap sources of entropy 
That is true, but a source of random bits is not very much like a cryptographic primitive.  A source of "random" bits has value if
there is *any* subset of attackers to whom its output is unknown
and unpredictable.  I would be scared to death of using anything marketed as a "well-
calibrated well-defended well-monitored entropy source" -- and nothing else -- because such sources and the organizations that produce them are single points of failure.  Also, they are high-
value targets and it is known that adversaries both national and
criminal, both foreign and domestic, routinely attack the manufacturers of high-value targets with tactics including but not limited to court orders, gag orders, blackmail, extortion, bribery of plant workers, covert die alterations, etc.  By all means, put the darn thing in my system.  There will be a lot of attackers whom its output is not known to, so it'll
positively help.  But, I'm also going to be using the sound card, camera, hard drive delays, A/D converter noise, thermal noise, local wi-fi static, keyboard and mouse timing, USB power loads, and everything else I can think of along with it, because however limited they are compared to what the device you advocate *is purported* to be, there will be other attackers whom those outputs are not known to.  The objective is to have at least one source in the system that is unknown to every attacker.  It doesn't have to be the same source in each case; any one will do.  If the guy who can monitor local wi-fi static is not the same guy who can monitor the local audio environment AND predict a potentially pwned hardware RNG, I win even if all three sources can be monitored or predicted.

@_date: 2014-02-06 19:47:53
@_author: Bear 
@_subject: [Cryptography] who cares about actual randomness? 
This essentially duplicates the "mental poker" protocol in which each
participant provides his/her own RNG, committing in advance to its output, and an agreed-on combination of all is used for the game. This works because each player's RNG (or OTP, if one or more of the participants wants to go there) is unpredictable *to the other players* and therefore the output of combining them is provably unpredictable to all the players.  Your scenario achieves this by having each participant provide a part of the seed for an RNG, and being satisfied that it is a large enough part that the remainder cannot be 'brute forced' from the output. If the condition is true, it assures that the RNG is not
predictable to any players, even though each knows the "random" bits they contributed themselves.
But you knew that.

@_date: 2014-02-14 20:53:17
@_author: Bear 
@_subject: [Cryptography] BitCoin bug reported 
Summary:  The protocol is secure in terms of protecting against double payments actually taking effect, but someone is injecting bogus transactions (fiddled copies of real transactions) onto the
network. The clients in use at Gox are unable to distinguish bogus
copies from real, and other clients in use are still *SHOWING* the copies (which will never confirm) as "pending unconfirmed" payments
leading to confusion.  Details:  Bitcoin transactions have an "ID number" derived from a hash of the transaction.  The problem is that the transaction may be expressed in any of several ways, resulting in different hashes. This problem is called transaction malleability, and work to eliminate it has been underway for months.
What is cryptographically secured about a transaction is which unspent coins are being spent, what public key they are being spent
to, the fact that the correct privkey (whose pubkey they were previously spent to) has signed off on the spend, etc.  This set
of information is globally unique.  But the "ID number" itself could be any of several values depending on details about how these things are expressed.  It has never been possible for more than one version of the same transaction to confirm; that is prevented as a double spend of the same inputs.  The problem of issuing noncanonical forms of transactions was fixed months ago in the Bitcoin clients themselves - the standard tools now emit transactions in a "canonical form" that can only have a single hash/ID number.  But until quite recently the network still accepted transactions having other forms, and if multiple forms of the same transaction somehow got on the network
at the same time, it was essentially a cointoss as to which one would be accepted.  And Mt.Gox, which had failed to update its own software when the rest of the network updated, was still occasionally emitting such noncanonical transactions.  Less than one-thousandth of Mt.Gox's payments were noncanonical, but they still existed.  Recently, after many warnings, the network reached a threshold and stopped
accepting noncanonical transactions in new blocks, which resulted in occasional payments made by Mt.Gox failing to confirm.  Gox was "confused" by its outdated software into thinking that it had made these payments, but because such payments were not accepted by the network, the coins were still at Gox.  Or, one of
the miners would be accepting their noncanonical transaction, transforming it into canonical form, and including it in a block, whereupon it would have a different ID number than the ID number
Gox was looking for a confirmation of.  That was bad enough for Gox, but it was a relatively minor problem; Gox was reviewing these on a case-by-case basis, discovering that the transactions had in fact never been accepted by the network (or had been accepted with a different ID number), and issuing new
transactions or updating accounts with the corrected ID numbers.
Then someone started a DDoS attack on the Bitcoin network. There's some botnet now that's taking transactions off the network, fiddling
them into a noncanonical form, and re-emitting them onto the network.
The standard clients of course are refusing to accept these
transactions, and whenever a miner who has failed to update software
attempts to put one or more of them into a block that block is rejected.  But they're causing confusion and load, and the standard
clients are still seeing them as "pending" payments that haven't been accepted into a block yet.  For this reason, people are seeing
multiple copies of payments in their clients, with different ID numbers.  Only one copy of a payment will ever confirm, but for a while (until the payment confirms and the noncanonical version of it is then dropped as a double spend of the same coins) it looks as though multiple copies of payments have been sent.
Gox, which had relied on the "ID numbers" instead of more stable cryptographically secured identifying information about the
transactions, has been completely crippled by this attack.  Some users, seeing "unconfirmed" payments in addition to the ones
they expected or made, are being confused into issuing more transactions to try to "correct" the multiple copies of payments.

@_date: 2014-02-17 10:17:08
@_author: Bear 
@_subject: [Cryptography] Another Bitcoin issue (maybe) 
As soon as I understood the full-blockchain storage requirement, I had the same concern.  Especially if facing widespread adoption, the
blockchain will grow far too fast for everybody to store a copy of it.
The community is working on a solution they call the "mini-blockchain" which allows for periodically dumping blocks older than X. That
addresses some data storage requirements, but I'm still concerned that
bandwidth requirements do not scale well.
I have an idea how to split up bandwidth requirements, but I have no proof-of-concept implementation yet.  My idea is to use multiple blockchains.  Let's say that we partition the users up into six proper subsets which I'll arbitrarily call S1, S2, S3, S4, S5, and S6.  Each pair of subsets shares a blockchain.  And each blockchain shared by a subset is part of that subset's
"channel."  Additionally there is one shared blockchain that's part of every channel. I believe that it would be most useful (have least overhead costs) if the user subsets were divided according to geographic region, but in the future that may be wrong.
So subset S1 has a channel consisting of blockchains B12, B13, B14, B15, B16, and BS. Subset S2 has a channel consisting of blockchains B12, B23, B24, B25, B26, and BS. Subset S3 has a channel consisting of blockchains B13, B23, B34, B35, B36, and BS.  etc.  If you partition the users into N subsets, you wind up with 1 + ((N^2 - N) / 2) blockchains.  Each blockchain appears in 2 channels, except BS, which appears in all channels. Now transactions between actors in any two different subsets can take place in the blockchain shared between those two subsets, while transactions internal to any subset can take place in whichever
blockchain in that channel is under least load, or in whichever blockchain in that channel the payee finds it strategically useful to have the txouts appear in. The shared blockchain common to all channels is used for transactions
that take place among users divided among 3 or more subsets.  However, it must also be used for reconciliation.  IE, if you have money to spend but don't have money in one of the blockchains you share with the person you want to spend it to, you have to make a preliminary tx transferring that money to the shared blockchain.  This is
essentially the overhead cost of separating the users into channels.

@_date: 2014-02-17 10:26:49
@_author: Bear 
@_subject: [Cryptography] Another Bitcoin issue (maybe) (was: BitCoin bug 
For whatever it's worth, I don't think anybody was buying the idea that players with serious skin in the game (at the very least La Cosa Nostra and various foreign and domestic TLA's) hadn't already been developing
ASICS and FPGA's for password cracking.
So now that there are cryptocurrencies, such things are being developed
for broader audiences.  Good.  That only means that people can't ignore them anymore.  Our security has been compromised by them ever since the
TLA's and crooks started using them; now that we can't ignore them,
maybe we can start designing security that takes them into account.

@_date: 2014-02-17 10:39:00
@_author: Bear 
@_subject: [Cryptography] Another Bitcoin issue (maybe) (was: BitCoin bug 
In practice, in the most recent downturn of bitcoin prices thanks to the Mt.Gox vulnerability, the idled bitcoin mining rigs went to work
mining altcoins.
Miners set their rigs to work on whichever coin shows highest current
profitability, and if the price of bitcoin plummets, that's an altcoin
that uses the same proof-of-work.  So instead of diverting mining rigs that are otherwise doorstops, the 51% attack has to divert rigs from a profitable activity.  The picture is not quite as bleak as you paint it, but I fear that you are absolutely correct that it is not stable.

@_date: 2014-01-11 15:35:27
@_author: Bear 
@_subject: [Cryptography] defaults, black boxes, APIs, 
My problem with the entire category is that they are the products of people whose whole business model is to sell information about their
customers.  I daresay if any of these things don't share every bit of information they can get about you with the software companies who provide the browser and those who subscribe to their information
services, some engineer somewhere will be getting fired for failing to do the assigned job. Security actively flies in the face of the "web 2.0" business model
which is *BUILT ON* surveillence of as many people as possible.

@_date: 2014-01-12 09:21:43
@_author: Bear 
@_subject: [Cryptography] Advances in homomorphic encryption 
This is true.  Proof of "less than" is not simple.  It winds up needing
to take the form of a zero-knowlege proof.  Of course there are other possibilities like discrete integral fields for year, month, day, hour, minute, that could be checked quickly in succession -- first by stepping through years looking for equality,
then months, then days, then hours, then minutes.  You could figure
out whether a particular transaction is within a given date range by making a relatively quick and simple series of equality checks. Most other things you'd be interested in making comparisons about can also be reduced to discrete integral fields.

@_date: 2014-01-14 10:36:00
@_author: Bear 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
I tend to agree.  If RSA doesn't go down in flames over its utter failure, then people will learn from that fact that security is a joke industry.  That's a problem we already have badly enough with the failure after failure after failure revealed by the Snowdon
I don't think that there is any real hope of building a secure infrastructure for the world if the world learns by this example that an industry leading security company can completely fail in its primary mission without consequence.  That would be a vote of no confidence in the entire security industry, like an acknowledgement that there can never be security and there's no point in even trying. That said, I don't think a conference boycott is specific enough. A conference boycott hurts everyone at the conference.  And most of them have not been complicit (or merely incompetent, which is nearly as bad) in betrayal of the public.

@_date: 2014-01-14 11:20:28
@_author: Bear 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
I may be excessively cynical here, but when you wrote "etc" I think you might have been misspelling "RSA."
Why should we ask someone go to an RSA conference to get products to protect themselves from RSA itself?  Considering the conflicts of interest, that doesn't seem likely to be fruitful, does it?
While I think that a simple boycott is too broad, there has to be an alternative course of action.  The industry needs conferences (for sales) but would prefer not to have RSA's name plastered on them (for credibility).  The public needs conferences (for security
information and products) but would prefer not to be going to a conference organized by the very same people they have discovered that they need to secure their business against.  Both groups would be well-served by a parallel series of security
conferences, scheduled very deliberately to conflict with RSA's conferences and ideally not open to companies who are also presenting the same papers or products at the RSA conferences.  That would allow people to actually *do* the boycott of the RSA conferences without missing all possibility of making sales or all possibility of securing their infrastructure.  It would reduce the level of collateral damage and allow the very targeted kind of boycott that I think everyone could support. Can we get a University (one with a hell of a good math program, or who offer graduate degrees in cryptography) to organize one, or
if they won't organize it could we get them to host it?

@_date: 2014-01-16 16:18:19
@_author: Bear 
@_subject: [Cryptography] [cryptography] Boing Boing pushing an RSA 
PRNG's have nothing to do with one-time pads.  Compromised PRNG's affect stream ciphers, but one time pads do not use PRNG's.

@_date: 2014-01-16 16:54:00
@_author: Bear 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
See, that is precisely the same problem that the NSA is up against. How can they know that terrorists *aren't* operating in a given theatre unless they know *EVERYTHING* that goes on in that theatre?
That is the rock, the demand on them, on which the waves of ethics and morality have broken.  The only way to 'prove' a negative proposition:  You have to know
absolutely everything about the universe in which it takes place.
And there is no way to do that.  There is no way to even approach
it other than by doing evil and betraying the trust of everyone. We'll never be able to 'prove' that something didn't happen unless we do the same kind of unethical crap and pervasive monitoring that is so repugnant.  So clearly we cannot make lack of evidence into a standard of trust.  We have to evaluate what is known and what isn't, and then reach and act on our conclusions even if we can't have direct evidence of malfeasance.  Or we have to descend into the kind of amoral
backstabbing behavior and pervasive monitoring that the NSA is now seen to have done.  The only alternative is sticking our fingers in our ears and going, "la la la" and pretending we don't have to think about it at all.

@_date: 2014-01-17 19:04:15
@_author: Bear 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
That is exactly the definition of a stream cipher.  Use a deterministic (Pseudo-random) number generator and seed to produce a stream of bits to encrypt with, and you have a stream cipher, which is NOT a one-time pad.  A one-time pad uses a fairly reliably *random* number generator, like this:  Unfortunately "real" random generators are difficult to implement in software, but thermal noise sensors, lava-lamp and aquarium cams, and
microphones pointed at freeways give pretty good results.  And yes, so do more amusing implementations like DiceOMatic. Actually I think that the *noise* produced by DiceOMatic, sampled a second at a time and sent through a hash function, would be a much higher-bandwidth source of randomness than the dice reading that the machine is actually built to do. Hmm.  It should not be too difficult to equip many servers in the same
room with $10 USB cameras, and have them all pointed at a cheap,
known-chaotic physical system like an aquarium with a bubbling filter,
moving aquarium toys, and swimming fish -- all from different angles -- running the resulting video, a half-second at a time, through a hash
function, and using the results for "real" random numbers.  And it amuses me that the sysadmin's job could legitimately include feeding
the fish.

@_date: 2014-07-04 10:47:15
@_author: Bear 
@_subject: [Cryptography] Bitcoin, litecoin, vertcoin, and derivatives 
Please be more specific.  Which proof-of-stake problem does it solve, and why is it a solution for that problem?

@_date: 2014-07-11 13:30:08
@_author: Bear 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
I think that is true only for the agency or company that actually does the research into someone's background and approves the security clearance, because of the Law Of Trust: (all together now)      "Trust is neither transferable nor transitive. "
A security clearance is evidence that someone trusts this person; not evidence that any specific additional person has reason to.
That's very much to the point, because a security clearance represents
a possible conflict of interest; that is, the holder may value their
relationship with the issuer of the clearance more than the integrity
of their  contribution to the project.  I have seen this in practice in multiple places, and heard it advocated by people whose business is to know the law.  As a condition of employment (or a condition on making contributions) the employee or contributor is required to positively declare any security clearances, employment relationships, potential
reserve activation obligations, potentially conflicting contractual obligations, etc.  And then, when turning in any changes, positively and specifically declare that each member of this list of potential conflicts of interest had neither knowledge of nor input into the current or previous revisions from that employee or contributor. The rationale as I understand it is that someone with a clearance who does something silently or passes along information silently has a different legal status or poses a different organizational risk than someone who does these things and then untruthfully swears on record that they have not.  The first way you have a silent break or a suspicion that can never really be proved; the second way you have the risk of a major scandal, legal challenge, or even legal precedent creating permanently changed laws, at some point when those specific sworn statements are shown to have been lies.
That said, I'm not a lawyer.  Further, I'm not sure American citizens without some kind of clearance are even permitted to know the law on this topic, which is disturbing on a whole different level.
I recently ended a job with a security clearance myself; it was with a government contractor whose employment agreement asserted
"we own any code you publish even in your hobby time", so I'd been not contributing to FOSS projects during the time I worked there. It is important to note that that feature of the employment
agreement had nothing specifically to do with the security clearance; employment agreements with government contractors are a separate consideration under law.
I was relatively sure that if push came to shove and a court could show no connection with company business or security or my duties there or information gained there, that clause wouldn't hold up in court.  I was also relatively sure that if push came to shove I was going to have to hire a lawyer and spend a stupid amount of money to bring a court to that conclusion.  So it was just much less trouble to not do anything that would bring the question to court.

@_date: 2014-07-11 14:25:51
@_author: Bear 
@_subject: [Cryptography] hashes based on lots of concatenated LUT lookups 
Not as hard as it is to accomplish a specific purpose by doing so.
What are you trying to do?  Why do existing ASIC-hard hashes not serve your purpose?  Not quite the case, but close; you can have as many parallel threads using the same lookup table as you like.  The bottleneck is in the form of limited bandwidth between CPU and memory, because each of
those parallel threads is going to ask to see a thousand memory locations that no other thread will be asking to see.  Right... For what purposes have you not just answered your own question?  Seriously, do you want an all new hash function, as opposed to going to going with Scrypt-J or Cuckoo?
If so, what needs to be different from those functions?

@_date: 2014-07-18 10:41:26
@_author: Bear 
@_subject: [Cryptography] Steganography and bringing encryption to a piece 
Okay, I have a hypothetical.  Let's call it the "Voynich alternative."  Redirecting intellectual effort from cryptography
as such to linguistics could plausibly result in an arguably practical system of storing handwritten information privately.  It would be a system of limited utility at best because you'd have to actually spend up to a year or two internalizing the system in your own squishy brain before it would be usable to
you, or your correspondent.  Let's imagine that there is a person who is a conlang hobbyist and has a diary which he keeps in an entirely made-up language.  It has grammar that doesn't (much) resemble the grammar of English, its own vocabulary most words of which are not direct substitutions for English words and are ambiguous in different ways, its own
morphology (derived from three earlier made-up languages) and system of affixes and infixes, and its own set of a few thousand made-up idiomatic phrases.  Some of these made-up idioms are "linguistic fossils" from earlier made-up languages, which don't make sense according to the rules of the current language's grammar.
Its only relation to words in existing languages are via proper nouns, which are handled via a sequence of syllable substitution and sound-change rules that result in pronounceable but apparently
unrelated strings that are (usually) longer or (sometimes) shorter and otherwise conform to the lexicographic conventions of his made-up language.  Further, the transformation rules are not reversible; while there may be only one 'image' in the constructed language for a given proper noun, the constructed word could be the result of applying the process to any of billions of possible preimage strings - of which possibly only one or possibly as many as a few dozen are genuinely proper nouns from which it might have been derived.  And, to make matters worse than that, almost every *other* word in the
language could also result from the same set of substitution rules, each with billions of possible preimages which might include zero,
one, or as many as a few dozen completely unrelated proper nouns.
As a conlang weirdo, he's fluent in his constructed language.  But he's the only person who has ever used it, and has not published it anywhere.  It represents a monumental amount of very much enjoyed but arguably wasted intellectual effort on his part, in much the same way that Tolkien's middle-earth languages did prior to the
publication of his books.  While not "encrypted" as such, I doubt that anyone who got their hands on his journal could, in any reasonable timeframe or possibly
ever, read it.  With no illustrations or passages in English to relate to the written words, the proper nouns are the only relationship it has to the real world, and that relationship is itself tenuous. To those who had not spent time learning the language from someone who, ultimately, learned it from the guy who made it up, it should be  as impenetrable as the Voynich

@_date: 2014-07-28 16:48:17
@_author: Bear 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
Hmm, let me monkey together a tiny bit o' code: Expmod, in scheme:
(define expmod
  (lambda (base exp mod)
    (remainder
     (cond ((= exp 0) 1)
       ((even? exp) (square (expmod base (/ exp 2) mod)))
       (else (* base (expmod base (- exp 1) mod))))
     mod)))
;Value: expmod
and now that I've got expmod, the Fermat primality test:  (yes, I know you have to verify the results to rule out carmichael numbers). (define fermat-prime? (lambda (n m)
  (let fermat-tests ((i m))
     (if (= i 0)       (let ( (a (+ 1 (random (- n 1)))) )
        (if (= (expmod a n n) a) (fermat-tests (- i 1))
        ;Value: fermat-prime?
And now that I've got fermat-prime, findpr to find the first prime less than a given number:  (define findpr (lambda (x n)    (let ((fp (fermat-prime? x n))          (over (> n 100)))
       (if (and fp over) x            (if fp (findpr x (* 2 n))                   (findpr (- x 1) n))))))
;Value: findpr
And now that I've got findpr, let me see just how far below 2^512 you have to go to find a prime:
(- (expt 2 512) (findpr (expt 2 512) 5))
;Value: 569
So the first fermat-test prime number below 2^512 is 2^512 - 569? It's a nice nothing-up-my-sleeve number, anyhow. What's the problem with it?
Are there some requirements I don't know?

@_date: 2014-07-30 13:42:30
@_author: Bear 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
Despite the non-exceptional speed, 512 = 2^9 is THE next nothing-up-my-sleeve number for a bit width, and 2^512 - 569 is therefore THE next nothing-up-my-sleeve number for an exponent modulus.  The minute there is debate, there is reasonable suspicion that someone is trying to influence the debate for purposes of subverting security.
Gratz to Microsoft, whatever their past sins, for making a
recommendation that marks them out as DEFINITELY not playing that role in the current round.  The minute that there is suspicion that someone might be trying to influence the debate for purposes of subverting security, it raises a hard question....  one which sounds like a paranoid wearing a too-tight hat, but which must be raised....  Is at all reasonable to suspect that the same properties that make a given calculation faster might also make it easier to analyze or reverse?  IOW, when we look for 'fast' curves, is there a "reasonable to  suspect" chance that we're thereby looking for 'weak' curves by some mathematical attack that we may finally notice next week or the week after - but which the hypothetical parties attempting to influence the debate may be aware of now?

@_date: 2014-06-03 09:05:13
@_author: Bear 
@_subject: [Cryptography] Is it mathematically provably impossible to 
This provides identifiable points of attack for adversaries of the system.  what the NSA &co have been teaching us is that if agencies
with legal authority are permitted to command breaches of trust and keep those breaches of trust secret, there can be no trusted authority.  So, one day, your 32 independent notary log maintainers all get secret orders and have no legal recourse but to submit to jail if they do not participate in betraying or subverting the protocol.  Further, they are forbidden or prevented from even sounding an alarm.  "Warrant canaries" are an endangered species; if they are identifiable to their audiences they are identifiable to their predators. I don't like the bitcoin proof-of-work system; it guarantees that the expense of creating bitcoins will hover right around equal to their market value (ie, the point at which the marginal profit of
bitcoin farming reaches zero), and constitutes an outright theft of subsidized electrical utilities, etc.  But the problem it solves
is how to proceed with zero-trust, and a solution that requires identifiable trusted authorities is no solution.

@_date: 2014-06-03 09:55:51
@_author: Bear 
@_subject: [Cryptography] What has Bitcoin achieved? 
One issue that is driving Bitcoin is the excruciatingly slow uptake of new technology into legitimate financial mechanisms. Finance in general is littered with obsolete crap that doesn't work all that well, supported by entire ecologies of rent-seekers
and fee-eaters luxuriating in a protected-species status thanks to very well-intentioned conservative laws intended to minimize fraud and theft which have the effect of making any way other than the ways now known to be obsolete and inefficient compared to new technology, illegal.  The experience of technologists is that more effective ways of solving financial problems, often with solutions that close significant security problems, are usually not permitted to succeed.  Any improvement requires the approval of legions of people whose job security would be threatened by the improvement. Even if the approval can be gained, it then requires literally hundreds of millions of dollars in licensing and permissions and infrastructure to conform with many of the regulatory processes, so startups are frozen-out.  Bitcoin, unlike most other fintech offerings, is financial technology that can exist independently of that regulatory structure.  You don't need anyone's permission to start using Bitcoin, and nobody else has to get regulatory approval to open a bank or become a credit card issuer etc before you can.  Entire legions of rent-seekers and fee-eaters are cut from the process by using cryptographic/mathematical/physics-based rather than institutional/legal/trust-based security. And if the failure of financial institutions to take up new technology has been driving bitcoin's adoption, the major factor
holding it back has been the appalling failure of institutional security wherever people dealing in bitcoins have been allowed or required to do so in ways that do not take advantage of the cryptographic security features of bitcoin. The major pain points have all occurred at the interfaces, such as brokerages and exchanges, where mathematical security and institutional security ought to be working together and are not.  What this says to me is that the more business we can find a way to do cryptographically, without ever touching those interfaces, the better off we will be.  How do you achieve the tech result, where the threat model includes Eve, Sybil, and Trent working together?  Heck, I'll give you a freebie and say we don't need to worry about Eve because we're not going to get financial privacy anyway.  But you still have to deal with Sybil and Trent.  If you can deal with Eve too, that's pure win.  All the good solutions to byzantine-generals I've seen require communication that scales with the square of the number of participants.  I've been trying to think of a way
to leverage that into something practical using overlapping cells, but I haven't found one yet.

@_date: 2014-06-05 11:20:18
@_author: Bear 
@_subject: [Cryptography] Is it mathematically provably impossible to 
No, it isn't a theoretical possibility.  It *IS* the threat model, because we're now talking about adversaries with the capability to *FORCE* that collusion regardless of whether the entities involved
are willing to collude.  Basically, what you're betting on is that someone on your list is both willing to go to jail and able to defeat professionals who have had time to prepare their tactics around the possibility and
who can choose their time and place of intervention to limit the
ability of any such willing person to get the warning out.

@_date: 2014-06-06 12:08:27
@_author: Bear 
@_subject: [Cryptography] Vote of no confidence. 
"A secure computer is one that is powered down and not connected to any network."  We've all heard that before, yeah?  I have a confession.  I believe it.
I realized I believe it when a financial services firm asked me to install a password manager on my phone.  On my android phone, which shares information with people whom I don't trust on a regular basis, where every "upgrade" to anything asks for ever-more access to personal information, contact lists, location, etc.  An application written by people I don't know.  Who don't seem to give out any guarantees.  And who are
very reassuring that if my phone is lost, my passwords
won't be...  meaning they're storing a hell of a lot more than a hash.
And I said no.  I understand that the current wisdom
is that password managers are a good thing, but.... I just cannot trust the people who develop them and the environments they run on.  The complexity runs off beyond the horizon and I just can't say, for certain, that nothing else can see this thing in memory which this particular app is using.  I do business with that company now, on the basis of a sixty-character password, which is complicated and slow to type and not stored in any electronic form anywhere.  It's stored on a "computer that's powered down and not connected to any network," along with a bunch of my other important passwords. But maybe "computer" is the wrong word.  It's actually an iron box with a padlock.  Also known as a computer whose security model is simple enough to understand and whose
operating system is known completely enough to trust.
And when I log in using that password, the company sends
my phone (which NEVER syncs on my computer) a nonce via SMS which I then enter to finish the login. There is no automatic authentication when the stakes are high.  That which is automatic, in an environment
where complexity runs beyond the horizon, I just cannot
guarantee will never admit someone else.  There is no
"password sync" between phone and computer...  because
I don't want the attack surface that comes with any electronic script-detectable association between the two.  I don't want to have to secure phone information
on my computer, and I don't want to have to secure computer information on my phone.  There is no "password wallet" in my browser, because I don't want my browser to store passwords.  Anywhere.  Ever.  Because I don't believe I can keep anything accessible to, or especially
managed by, a browser secure.  My cryptographic keys (to bitcoin savings, SSH tunnels,
and some other high-stakes things) are no more complex than many of my other passwords, and I save them in the same way.  With ink.  On index cards.  In the iron box.  With a padlock.  I don't worry about a trojan horse program or a worm stealing my passwords when I'm not using them, because I'm reasonably confident that the restricted computing environment inside a padlocked iron box with no power supply, no CPU, and an index-card memory isn't complex enough for such a program to run.  I could worry about burglars, I guess.  But a burglar would actually leave evidence - he might get something but I'd know he'd got it.  Further, a burglar has to spend time and effort and personal risk on each and every target, instead of writing some program to rip off the thousands of people who didn't patch the hole it exploits, leaving no visible evidence of the breach.
And then launching it anonymously from some Internet
cafe in a jurisdiction with no extradition treaties.  It just seems to me like simple burglary is a more direct and detectable and therefore more acceptable risk than the activities of seven billion apes and software complexity that goes out beyond the horizon, out there somewhere in the universe. That leaves me slightly worried about keyloggers when I'm actually entering passwords, but I have one trusted software source (linux distro) and seven applications in total that come from any other source.  Of those seven applications, for five I have compiled from source and for two I have taken the trouble to obtain binary hashes of public repositories using machines in other places with separate connections to the network.  And
then I've brought those binary hashes home - on paper - to make sure they match the software I downloaded.  And I run with the 'bin' directory mounted readonly, so I'm not all that worried about keyloggers.  Ultimately, I believe in security.  But what I believe about security leaves me far from the cutting edge; my security environment is more like bearskins and stone knives, because bearskins and stone knives are simple enough that I can *know* they won't do something I don't want them to do.  Smartphones and computers simply cannot
provide that guarantee. The parts of their security models that I do understand, *won't* prevent any of the things I don't want them to do. An iron box with a padlock on the other hand is a simple enough security model to understand, and does provide  strong guarantees about what that environment won't do. Just a musing, I guess....  the point is that the industry
is now building security models which want to provide collaboration, and single sign-on, and synchronization, and interoperation, and 'cloud storage' and so forth - but in doing so simply do not and can't provide good reasons for trust nor solid mathematical proofs of
how the things I don't want them to ever do have been rendered impossible.  In fact, most of them simply refuse to enumerate things
they render impossible.  Security means guaranteeing that
certain things are impossible.  Nobody's even trying to do that because doing the minimum to achieve meaningful guarantees that meaningful kinds of abuse are impossible, would also mean that features like password wallets where they can guarantee password 'recovery' are also impossible. They're selling the set of things that are enabled rather than the things that are prevented.
Good computer security could be built.  But maybe it can't be sold.
And because that's what computer security is like these days  ...  I'm forced to use an iron box.  With a padlock.

@_date: 2014-06-06 18:35:42
@_author: Bear 
@_subject: [Cryptography] What has Bitcoin achieved? 
True.  Cost of executing an "undo" - zero.  Cost of getting peers to agree on who can execute an "undo" and for what reasons - prohibitive. I've been working on a blockchain protocol extension that would allow "undo's" in some limited circumstances and could possibly exist in cooperation with regulations and laws and courts.  It's strictly opt-in, but I figure the "crypto anarchist" contingent will hate it anyway - because it becoming possible means some people will start refusing to do business with them unless they "voluntarily" abandon some privacy.
The first piece of the puzzle is the "subordinate claim" on blockchain assets.  An entity owning some asset can issue a subordinate claim on that asset to some other entity rather than transferring ownership of that asset to that other entity.  A subordinate claim in the asset can be used just like its superior claim could be used (bought, sold, transferred, etc), except that the holder of the superior claim has the ability to revoke either the subordinate or superior claim at any time.
The subordinate claim is revoked at any moment if the superior
claim is transferred to a new address. The superior claim is revoked if it is transferred to the current address holding
the subordinate claim. The second idea is that of a persistent identity, which really is just a species of PKI published to the blockchain. An entity could publish a key asserting its identity and/or identifying itself in transactions, so that subsequently people could use the blockchain to see how much and exactly what business that identity has done and how long it's been around.  Yes, it's an abandonment of privacy.  It is also strictly voluntary. Nothing compels anyone to link a particular transaction to their persistent identity, nor forbids them from having more than one, etc.   The reason why some entities might choose do this is to firmly establish their identity and trustworthiness, or at least their collateral, both as issuers and recipients of subordinate claims that might be revoked.  The idea is that regulated assets could have sovereign claims (that is, the top-level claim that is not subordinate to anyone or anything else) held by the regulatory agency, and then ordinary trade in those assets would be trading the subordinate claims.  Thus, someone with a publicly traded stock would hold his claim subordinate to the agency with authority on that stock, which would be someone like the SEC in the US, or equivalent national authority elsewhere.  If it's a privately held stock, the stockholder would hold a claim subordinate to the company's claim, which would in turn be subordinate to the sovereign claim held by the SEC (or whatever regulatory authority has jurisdiction
in that particular asset). That gives the regulatory agency the option to 'freeze trading' in a particular asset, or even claim it pursuant to court action, etc, without the cooperation of the holder or even after the death of the holder if necessary.  It gives the courts a way to recover assets that have been burgled or embezzled, or the assets of someone who died intestate or who has failed to pay fines, child support, or income taxes, etc -- provided those assets were held subordinate to court control in the first place.  It is still strictly opt-in, in that nothing could compel you
to do anything you don't want to do with a _sovereign_ claim, and if you don't *want* any subordinate claims, you don't have to buy any, and if you don't *want* to issue any subordinate claims under any circumstances, you don't have to issue any, and if you don't *want* to establish a persistent identity that
would doubtless quickly be associated with your legal identity
nothing would compel you to, and you'd always know exactly which persistent entities have the superior claims before you acquired a subordinate claim....  but I still figure the crypto
anarchists will hate it.

@_date: 2014-06-07 20:34:59
@_author: Bear 
@_subject: [Cryptography] Back door competition for TrueCrypt fork? 
I think the idea is that when people are looking for the new backdoors and bugs, they'll find backdoors and bugs.... including some existing ones that they otherwise wouldn't.
It's all about moving from an assumption of trustworthiness
to an assumption of untrustworthiness - definitely worthwhile
for a security product.

@_date: 2014-06-09 10:49:06
@_author: Bear 
@_subject: [Cryptography] Aggregate signatures 
My immediate thought was that it could be applicable to Byzantine Generals.
Nakamoto's proof-of-work protocol has some practical limitations
and waste that a better solution might not.  An aggregate signature - by a weighted majority of the keys of the txouts that existed at the time a block was created - would be proof positive that a block is backed by enough users
to constitute a majority of stake - hence, it could 'cement' a block in the blockchain as being immune to displacement.

@_date: 2014-06-09 12:07:40
@_author: Bear 
@_subject: [Cryptography] Yet more formal methods news: seL4 to go open 
We shall need to.  Many critical pieces of infrastructure already have a suite of "acceptance tests" - if you introduce a change and the result does not pass the acceptance tests, it does not become part of the code.  So that infrastructure is already there; the new bit is that the verification will have to be run as part of the
acceptance tests. I think that's a "layers of abstraction" problem.  You can verify an application at a higher level of abstraction by relying on verified
properties of lower levels. Things at a lower level of abstraction should be building barriers that things at a higher level of abstraction cannot breach.  Thus, if the "microkernel"  that does memory and process management is
verified to implement a memory protection policy that says so, then it should be the case that no memory written by one process ever becomes visible to another process without first being zeroed.  Which doesn't completely defang something like Heartbleed, but at least limits what OpenSSL can leak to data owned by the process that OpenSSL is running in.  With such barriers in place, building verifiable higher-level security that relies on those barriers ought to be relatively easy to achieve:  If the server starts a separate process to talk to each user, it can guarantee that none of those child processes can ever see data belonging to any other.  And at that point Heartbleed is starting to have trouble getting its hands on things
you need to protect.  Indeed.  But it's also completely necessary at this point.

@_date: 2014-06-12 14:25:15
@_author: Bear 
@_subject: [Cryptography] Swift and cryptography 
Indeed.  Language hacking is, I think, a fundamental part of learning to master one's tools for a significant fraction of
programmers. There's a certain -- temperament? -- anyway, there's a set of very good coders that would never fully develop its skills without an opportunity to wrestle with programming language design as an art.  They're not the majority, and they have no exclusive on being highly skilled - but they're some of the visionaries and once in a while they do come up with a good idea.
'Safety and formal verification' are well studied semantic issues valuable to all langauges and not more valuable to a crypto/security
language specifically than they are in general.  But if you're creating a cryptography and security language you need more than safety guarantees.  You need security guarantees that are ignored by (indeed irrelevant to) current models of formal semantics, and which might be very hard to define in Coq or its equivalent.
Arguably these aren't 'semantic' requirements at all because you
could violate them hard and produce an implementation that always
produces the same visible results given the same input, and that's why modern formal semantics models cannot be defined in terms of them.  So I'm going to use the term 'quasi-semantic' to describe them.  Any cryptography/security language needs absolute control over
the lifetime and location in the machine of data.  Specifically,
a crypto language needs a quasi-semantic requirement that any operation rendering data unavailable to the program, including
but not limited to subroutine returns, freeing of dynamic memory, overwriting values, deletions from or replacements in data structures, last use of a namespace, scope, or environment, or releasing the last pointer to a garbage-collected structure, etc, overwrites that data in memory making it unavailable to any tool that can be run on the machine including binary examinations of the running program image or disk image, and does so immediately, before the next program step is executed.  And of
course that includes also program exit and/or abort, where the next step is handing control back to the operating system.  It also needs a quasi-semantic guarantee that program data is never copied anywhere unless the program itself directs the copy to be made.  A language implementation allowing the operating system to do "normal" paging and swapping of running programs would be non-compliant with the spec.  No 'debug' environment allowing inspection of variable values could also be a compliant implementation, although it could otherwise be a simulation of one. This means a whole class of optimizations will be things you can't do, and will make any garbage collection agonizingly slow.
But that's the price you pay for knowing that confidential data that you're trying to protect is in fact being protected.

@_date: 2014-06-12 14:50:56
@_author: Bear 
@_subject: [Cryptography] Languages, 
Indeed, the only way to do formal verification of imperative systems using existing frameworks requires one to develop a formal semantics for those languages - which is specifically
a way of rendering the content of any 'imperative' program in
a functional way.

@_date: 2014-06-14 10:50:01
@_author: Bear 
@_subject: [Cryptography] What has Bitcoin achieved? 
And every part of the banking system arose in response to needs that have not gone away.
Maybe.  But even if that is the purpose of Bitcoin, it need not be the purpose of every implementation of a blockchain protocol.  Nor will it continue to be the purpose of Bitcoin itself if Bitcoin ever spreads beyond the crypto anarchist contingent.  Indeed, well over 90% of the trade that might ever be done in cryptocurrencies will not be done if they remain unregulatable.
So let the crypto anarchists continue to use Bitcoin as it is.  Serving the people at large has a different set of requirements
because most people want the security of operating within the framework of laws and authorities that provides the context of their societies. I know that it will not be welcomed by anarchists.  But that's a tiny, tiny bunch of people;  Most who are doing serious trade want
to be trading in assets that police can trace and courts can recover, because for most purposes and most people that's safer.  A blockchain "Open ledger" guarantees that the powers we give our regulatory agencies cannot be exercised secretly, and that if exercised in violation of law or due process, no one will be able to pretend otherwise.

@_date: 2014-06-15 11:37:21
@_author: Bear 
@_subject: [Cryptography] What has Bitcoin achieved? 
A.  You are ranting.  Most of the things you are reacting to are not things I said.  Please calm yourself and consider what has actually been said instead of jumping nine squares ahead to the worst of what you fear it might possibly mean.
B.  I am not talking about taking things people have and handing
them over to governments.  I am talking about a blockchain protocol
that allows people to voluntarily create and use subordinate claims on assets.  I expect that a lot of people will voluntarily
use this capability to create things that comply with existing
regulations in whatever jurisdictions they require - things with value, which could not otherwise exist.
C.  I was not talking about Bitcoin.  Bitcoin is controlled by the very community that I expect to most emphatically reject the idea of allowing identities, authentication, and the capacity to work within regulatory structures.  I think that they are fools in this rejection, and I think that they are limiting the prospects of Bitcoin to actually be adopted and used, but that is the behavior I expect of them.  The "Cryptocurrency anarchist" community will be displaced over the next few years by a "Cryptocurrency  plutocrat" community that better understands the opportunities to create value, so the foot-dragging is at most temporary.  Still,
it'll last far longer than I intend to wait, and if it goes on long enough, it will cause Bitcoin to fail because the plutocrats
will be adopting something else that better suits their purposes.
D. Therefore, I'll be looking at an altcoin implementation.  It is the market, and not any discussion between you and I, that will decide which is eventually dominant.  Perhaps both will be.
E. If an altcoin allowing authentication, identity, and working with regulatory structures is adopted by the plutocratic crew, then
the crypto anarchist crew will remain in control of Bitcoin - but will recede into economic insignificance.  If Bitcoin is adopted by the plutocrats instead, then Bitcoin will acquire these features
because the plutocrats need them and will get them built immediately
once they are in control - but the crypto anarchists will be displaced from control and will hate what Bitcoin has become.  Either way I expect that the crypto anarchists will be gnashing their teeth over their "loss" because what they want is, simply
speaking, destined to lose, one way or another.
A.  Comma splices are a sign of ranting.  Learn to control them.
B.  None of this follows from the idea of subordinate claims.  No
mechanism for keeping blockchain transactions secret has been introduced.  Not quite.  At that point we have a system that cuts a whole ecology
of rent seekers and parasites out of the money transfer business, and a way to do international and intranational business more efficiently.

@_date: 2014-06-15 21:32:02
@_author: Bear 
@_subject: [Cryptography] Dispelling some myths about Bitcoin, 
I think you're missing a point about distributed crypto-ledgers.  Because they are designed to be safe against attacks by governments, they provide a way for *governments* to agree on facts recorded in a blockchain, without fear that the facts as recorded at the time have since been distorted, hidden, or misrepresented by other parties
including other governments, or that third parties, including third-
party governments, might believe or credibly pretend to believe that they have been.
I don't know yet how that plays out in practice, but I do know that it's a first.  A blockchain protocol is truly capable of being
an impartial witness, even on the international playing field between governments, where the most routine plays are to gag, compel, assassinate, impersonate, censor, license, or otherwise control the "impartial" witnesses.  I think that's bound to mean
something new, and probably something good, for the future of
international relations.

@_date: 2014-06-19 12:52:47
@_author: Bear 
@_subject: [Cryptography] Shredding a file on a flash-based file system? 
I have never understood the threat model that these drives supposedly protect against.  Under what circumstances would an attacker have the drive platters in hand, but not have access to the key which is stored in the drive hardware?  If there were a worthwhile "encrypted drive" technology, the decryption routine, but not the key, would be built into the BIOS and the key would not be stored anywhere outside a volatile register - preferably one physically incapable of being read or written by any hardware at all other ythan a hardware channel from the keyboard (opened by the BIOS chip
only during bootup) and a hardware channel to the disk driver (opened by the BIOS only after bootup).
Under that system you could not do so much as read the boot sector without first entering the key at the local keyboard.
Of course, even if something that acts like that appears, which is unlikely, it will be a fake. Various agencies simply will not allow manufacturers to make a system that crooks cannot break into.  This is kind of like trying to keep houses and shops safe in a city where lockable secure doors are illegal.

@_date: 2014-06-20 09:54:00
@_author: Bear 
@_subject: [Cryptography] Shredding a file on a flash-based file system? 
Could you please explain what you mean, in hardware-oriented terms, what you mean when you use the words "wrapped with"
in this context?  Because clearly the key is stored unsecured if the machine can boot up the operating system without me entering a key.  To boot
the operating system requires reading the supposedly encrypted drive.  If the key doesn't come from outside, then it is stored in the same machine the attacker would presumably be stealing. If it doesn't require someone to demonstrate their credentials by entering a secret, then it is stored unsecured.  So if there's meaningful protection attached to the words "wrapped with" I don't understand how it can possibly work.

@_date: 2014-06-20 10:32:37
@_author: Bear 
@_subject: [Cryptography] Shredding a file on a flash-based file system? 
That would be an improvement over all systems I have actually encountered that are built around these supposedly encrypted drives. To hear that such systems exist is encouraging.

@_date: 2014-06-24 13:00:14
@_author: Bear 
@_subject: [Cryptography] What has Bitcoin achieved? 
What you are describing is called a "proof-of-stake" system, as opposed to a "proof-of-work" system.  Several cryptocurrencies have implemented proof-of-stake, but, as yet, the versions of it that they have implemented are subject to attacks based on what has been called the "nothing-at-stake" problem.
I outline the problem and a partial solution below.
In order to distinguish a "correct" blockchain from an orphan (or attacking) blockchain, proof-of-work relies on the provable
expenditure of a finite resource - compute power.  The nothing-
at-stake problem arises when there is no ability to prove expenditure of a similarly finite resource to distinguish a correct from an orphan blockchain in a proof-of-stake Proof-of-stake as implemented in existing cryptocurrencies treats "coin days" as the finite resource whose expenditure distinguishes correct from attack blockchains.   The idea behind this is that when a txout is used in a transaction, the time since it was created is multiplied by the number of coins it represents to get a number of "coin days."  And the assumption is that whichever blockchain has created (or used up, depending on how you think of it) the greatest number of coin days is accepted as legitimate.
Nothing-at-stake arises because "coin days" are not a finite resource
in the way we need a resource to be finite in order to resist attacks.
When there is a fork, stake is duplicated on both sides of the fork. This leads to attacks based on the use of stake that the attacker has already spent in the other fork.  Also, the same unspent txout can generate more or fewer "coin days" depending on when it is spent, so an attacker can spend coins on both sides of a fork while choosing which side to generate more "coin days" in.  Also, under proof-of-stake, there is no need for a miner to forsake
one side of a fork in order to support another;  The miner has the same stake on both sides of the fork, and would be an idiot to stop
generating blocks supporting either side until he is absolutely certain which side which will be orphaned.  That makes the support  of miners effectively useless for deciding which side is to be
Finally, new unspent txouts are generated by transactions, including
coinbase transactions, on both sides of the fork, and expenditures of these txouts are 'noise' that an attacker (in an attack which gives
the attacker control of a higher proportion of the coinbase txouts on one side of a fork) can take advantage of to generate more coin days in his attack chain than in the legitimate chain. A partial solution to "nothing at stake" is to focus on a resource
which really is finite in the way we need it to be:  Uncontested expenditures of txouts that existed at the moment when the blockchains
When comparing two sides of a fork, instead of counting coin days, we should count the expenditure of txouts that existed at the moment of divergence.  But we should count only those transactions that do
not conflict with (do not use any of the same txouts as) transactions
on the other fork.  And we should count them strictly for the number
of coins spent, rather than varying it by block height as in counting "coin days". This means it is transactions, and not mining, that supports the security of the blockchain.   In order for transaction support to be finite (necessarily count for only one side of the fork) it is necessary for transactions to give a block hash from the blockchain
they support.  Any transaction that gives a pre-fork block hash can be replayed into either side of the fork, thus cancelling its support
for the other side.  Any transaction that gives a post-fork block hash can be counted as support only for the fork in which that block hash appears.  Thus, transactions that name more recent block
hashes (within the last 1-3 blocks) are more valuable for securing the chain than transactions that name later block hashes (within the last 4-7 blocks), and if compensated via proof-of-stake 'interest' payments for securing the chain, should be compensated more. Transactions giving block hashes older than 8 blocks are not terribly useful in securing the chain, and should not be accepted.  Because this solution is not subject to nothing-at-stake, at the very least attackers have to use real as opposed to already-
spent stake to attack it, and cannot support their attacks by
making transactions using the same coinbases they are trying to steal via their attacks.  But this is still a partial solution.  There is still a flaw in
that someone making a transaction can easily make it in both sides of a fork, therefore supporting neither.  Further, there is some motive for them to do so, unless such transactions can be demonstrated based on information to be recorded in the main branch and their proof-of-stake payment for securing the chain withheld.
I believe that this is possible, but complex and possibly unnecessary.

@_date: 2014-06-24 16:12:56
@_author: Bear 
@_subject: [Cryptography] What has Bitcoin achieved? 
TAPOS?   Transactions as Proof of Stake....
Okay. Cool name. Hmm, I wonder which of us thought of it first.  Could be they got it from my first article on Bitcointalk.  The attack you describe works if the attacker waits for a fork, then spends txout A for (say) 100 coins, in one branch of the fork and spends txout B for (say) 10000 coins in the other branch, which if accepted will 'unspend' his 100-coin transaction.    If the blocks are averaging substantially less than 10K coins in legitimate transactions per block, the 10K spend supporting
the fork is likely to get the fork accepted.  The 100-coin spend, unless attached to a block prior to the fork, cannot be replayed into the other fork, and so the coins are 'unspent'.  OTOH, the prospect of discounting large transactions, even a little bit, to attempt to correct that problem would open up new avenues of attack exploiting the "correction."
The alternative is to make sure that transactions turn the money over with high frequency, assuring very large transaction volume
per block.  One could structure proof-of-stake incentives so they
work hardest when turning the entire money supply over every 24 hours, and in that case the attacker would have to do something pretty amazing to overcome the volume.  I don't buy that as a solution;  Anything that bounds the weight of each block or constrains its weight to be close to the bound invites attacks via deliberate invocation of the regulatory mechanisms.  As far as I can see the only practical constraint on block volume for maximizing security with TaPoS is "as much as
practical".

@_date: 2014-06-25 15:01:22
@_author: Bear 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
Whether agility in the cipher is or is not desirable depends on whether the protocol is more or less secure with cipher agility. As typically implemented, where either party may refuse to use a
particular cipher, an attacker can choose whichever cipher s/he has the best attack on and anyone who hasn't got it locked out will get screwed by "automatic cipher negotiation".  In that case cipher
agility actively subverts protocol security and it is better to not do it.

@_date: 2014-03-03 11:04:49
@_author: Bear 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
Okay, I have to say this despite the complaints on this list about how common TERRIBLE security practices may be.
This is completely over the top.  There is no way that this
could possibly be accidental.
In point of fact, I know of no commonly used or commercially sold compiler that fails to emit unreachable-code warnings by default.  Therefore I do not believe that this could be anything but deliberate.  I would be willing to state exactly that in a court of law.
It is unthinkable that ordinary process in a professional software shop should silence all code warnings.  Routines for which a normal code warning should be silenced, for a particular known reason, should be placed in files containing no other routines and subject to silencing of no other warning, with a comment next to the line that triggers the warning stating exactly why this code had to be written this way and why it is better to silence the warning than fix the code.  Alternatively, if the compiler supports inline pragmas
that turn off particular warnings for given regions of code, it's acceptable to pragma that single line, with the same comment about why the pragma has to exist. That isn't even special security practice, or special for security code, that is absolutely routine for all software in every shop I've ever worked at.
Make no mistake, this was not an oversight.  This was a deliberate sabotage.  If working as a manager, I would want to know not only who checked in the code with the change, but also who checked in the build control changes that had to be added to silence the warning.  The code change itself merits termination for negligence, incompetence, or sabotage. If whomever checked in the build file changes did so at a time when that was not a (mal)practice already years established, or other than under direct orders from a(n incompetent) manager, that would also merit termination for deliberately violating basic professional practices.
It may or possibly may not be enough evidence for a court conviction, but it certainly is enough for probable cause and a trial, and certainly justifies a termination for incompetence or negligent practice.
PS.  Any language that allows "goto" without use of a keyword that can be searched for project-wide without knowing the label gone-to is at best suspect.  It should be terrifyingly easy (as easy as "grep -r goto *") to find all uses of a Dubious Practice.

@_date: 2014-03-04 14:38:01
@_author: Ray Dillinger 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
Crap. No unreachable code warning in gcc anymore.?
I think that is an astonishing failure.?
As a developer, I rely on that warning because unreachable code is ALWAYS a mistake. ?
-------- Original message --------
* Bear  [2014-03-03 11:04 -0800]:
Ho about gcc?
The cryptography mailing list
cryptography at metzdowd.com

@_date: 2014-03-08 15:27:15
@_author: Bear 
@_subject: [Cryptography] End-to-End Protocols and Wasp Nests 
I like your "wasp nest" idea; a sort of crowdsourced set of remotely accessible test cases would be very useful, because most developers never come up with more than a few dozen.  It seems to me that most "interesting cases" are made up of unexpected combinations of Ops & Circs (Operations & Circumstances), and that by the time humans have written a thousand bug reports, they've probably discovered eighty or a hundred different Ops & Circs of interest -- each of the thousand bug reports/future test cases being composed of combinations of two or three or four of them. I think it would be a worthwhile methodology to start trying to abstract the Ops & Circs - each 'aspect' that is normally handled well but combines with some specific
other thing to reveal a bug, is a candidate.
You can write a brute-force algorithm that just keeps trying
combinations of different known Ops & Circs of interest -- but the problem there is that if you've got forty of them, you're exploring a forty-bit keyspace, with an "operation" that takes a hell of a lot longer than a decryption.  And if you've got sixty of them, you have to restrict your search somehow.  I think you get most of the coverage if you just tell your brute-force test generator to limit the number of bits it flips
"on" -- most bugs are a combination of two or three or four things, not a combination of twenty or thirty or forty.  There's a simple algorithm to generate numbers from lowest setbit count to highest, so that would probably be the test enumerator for this hypothetical brute-force testing tool.  Start from zero, and work your way up, as they say.  It's not a fully rigorous proven-complete test suite (because you can't ever prove you've found all the ops & circs of interest) but it surely would generate and test a nearly-infinite number of cases, and whenever a particular new op or circ were discovered, it would very quickly delineate the circumstances in which it becomes a problem.
And unlike most automated testing proposals, the method is simple enough for ordinary developers to understand and use.

@_date: 2014-03-08 15:37:28
@_author: Bear 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
Oh, that is so not the way it works.  "Unreachable code" by the sense the compiler looks for is code that cannot be reached
in the course of normal control flow.  Buffer overflows can work just as well with code that *can* be reached by normal control flow, so the "Unreachable code" warning won't touch them. On the other hand, using a language with boundary checking on parameters and variables (which is most modern languages, even modern compiled languages) is a fine way to disallow buffer overflow bugs completely.

@_date: 2014-03-08 16:17:22
@_author: Bear 
@_subject: [Cryptography] See??? Satoshi Nakamoto Smeared 
And let's not forget the possibility, that when he said "I no longer have anything to do with that and I can't talk about it," he might have been talking about his prior
work for the US gummint, and the reporter simply quoted him with an implication that he was talking about Bitcoin. "Can't talk about it" is some pretty specific language, I think.  It sounds to me like an NDA, not a project now being
carried on by others. I'd never have believed how much reporters misquote folks
or misapply the quotes they reproduce correctly, until I actually got some first-hand experience with it.

@_date: 2014-03-09 12:10:35
@_author: Bear 
@_subject: [Cryptography] End-to-End Protocols and Wasp Nests 
It is exactly combinatorial testing.  I've never seen any really good frameworks to do it though.

@_date: 2014-03-14 11:04:46
@_author: Bear 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
+1.  If something has a local management interface (ie, there's an rs232 port on the side of it that says "Management interface"
and is visible (visibly unconnected except when actively updating
the system) behind a keyed panel, but no remote interface (ie, someplace out on the net, if somebody has the right keys, they can make changes to the device invisibly and without me knowing about it), I feel that I can keep something secure -- or at least that if I fail to it's my fault. That makes it all about physical
security and my ability to keep unauthorized people away from my physical devices and to know exactly which people have had access to them.  Which, bluntly speaking, is a smaller attack surface than networked software security at this time.  Bonus if the local management interface still requires you to have
the right key, and you can reset the key.  That gives me some degree of software security as well as physical security.  Remote management interfaces look to me like a security flaw that can be easily exploited from a central location and without the knowledge of the people whose security is being compromised.

@_date: 2014-03-14 18:21:30
@_author: Bear 
@_subject: [Cryptography] How can I make use of the AES hardware on new CPUs? 
I am writing some software for which I would like to take advantage of the built-in AES instructions supposedly now included in AMD64 CPUs. So I have two questions.  First, how do I get at them?  Do I have to insert conditional  assembly code directly to use that, or is there a linkable library? Hopefully a reasonably portable one since I need to make both windows and Linux clients?
Second, is there a way to detect whether or not that hardware is present, so that I can create something that may run more slowly but at least doesn't crash horribly if run on an earlier CPU without an AES instruction built in?

@_date: 2014-03-15 18:07:12
@_author: Bear 
@_subject: [Cryptography] How can I make use of the AES hardware on new 
Just wanted to say thanks to those who gave me pointers.  Much appreciated.

@_date: 2014-03-16 20:41:49
@_author: Bear 
@_subject: [Cryptography] How can I make use of the AES hardware on new 
I only "mostly" trust the silicon.  What I'm writing will do bisimulation about 1% of the time to make sure the dedicated and non-dedicated silicon come up with the same answers.  If there is any broad class of keys or ciphertexts on which they do not, then that will be an interesting thing to know -- and will become visible as heralded by the appearance of an error message, which I will get calls about.

@_date: 2014-03-16 21:05:03
@_author: Bear 
@_subject: [Cryptography] Focus 
His claims for "decoding" are hardly better than random association of
nouns with repeated sequences of characters.  If he can't come up with
sentence structure and identify some sequences as verbs and modifiers, I don't think I believe.  You could take an entirely random sequence and identify some repeating
subsequences as nouns.  I don't think his claims mean anything that
hasn't been claimed before. And all such previous claims have led nowhere.

@_date: 2014-03-16 21:14:55
@_author: Bear 
@_subject: [Cryptography] Apple's Early Random PRNG 
LCGs are NEVER a good idea for any cryptographic purpose.  IIRC you can pretty much always derive their internal state and predict them forever after given some shockingly small (maybe just a dozen?) number of outputs.  The real problem is that they're starting parts of the OS that need secure RNG outputs too early in the boot process.
The idea that you need random output early in the bootup sequence is just plain wrong.  Even if you want to download a boot image over the network securely, you can darn well start the process by booting something else and gathering entropy for a minute before you open network connections.

@_date: 2014-03-18 11:18:22
@_author: Bear 
@_subject: [Cryptography] How to build trust in crypto 
Bruce's point there is "Trust is not Transitive."  It's a fundamental flaw in every business-oriented key infrastructure
so far enacted.  There is a problem in associating such keys with official identity, but they are not entirely useless in the absence of an automated universal way to do so.  First of all, when you meet Bruce at a conference, he'll hand you his card and it has that key printed on it.  You can trust that, including for business purposes, whether or not you got it automatically through some signing authority.
And even if you can't associate a key with an official identity,
it still isn't useless. When you get a message from the same key, you know (subject to theft or compromise of keys, etc) that it's from the same correspondent who last used that key.  So there is a persistent identity there, whether or not you can tie it to an "official" identity.  You know at least that
a MITM isn't going to start impersonating that correspondent
halfway through your conversation, unless the MITM has been there from the very beginning.
These "pseudonymous" identities are not useful for business purposes because with business you need recourse to courts etc.  They are not useful for exchange of secrets because you need to control exactly who secrets are exchanged with. (cf, definition of "secret" etc....)
But they are useful to journalists and scholars who are interested in information that can be supplied by people of indefinite identity.  They are also useful for people who know each other IRL and can handle establishing the correspondence of keys to identities for themselves - while wishing to avoid any publicly available association of these private keys with their "official" identities or with each other's keys.  For example, a sexual minority or dissident community might have people who don't want their keys associated with the community or each other in any public registry, but who know each other IRL and do want a private way to exchange social correspondence. Likewise they are useful in "pseudonymous" social interaction - on a private chat channel it might be that no one knows who "Guadalupe De Loop" or "Captain
Kitty" are in terms of official identity, but it's still meaningful to the community that no one else can pretend to be them. So anyway, it isn't quite right to claim that keys are
entirely useless in the absence of an infrastructure to
automatically establish correspondence to official It's my opinion that we haven't yet even attempted to just put a reasonable UI on people entering keys that they've discovered in the real world (from business cards, etc) into their system and thereafter using them.

@_date: 2014-03-18 12:34:38
@_author: Bear 
@_subject: [Cryptography] How to build trust in crypto (was:recommending 
One could however do D-H over the Interlock protocol, and reduce our MITM to a simple endpoint.
If Bob and Alice are using D-H over Interlock, then Mallory can have an interaction with either of them while pretending to be the other, but he cannot find any way to allow them to have an interaction with
each other which he can observe - ie, he cannot "pass through" messages.
And this is true whether or not Bob and Alice have ever met.
I believe that is very important for privacy purposes.  It raises the
bar substantially for the attacker.  It doesn't solve authentication, but it establishes forward security and goes a long way toward
eliminating eavesdropping.

@_date: 2014-03-20 11:32:00
@_author: Bear 
@_subject: [Cryptography] Mathematically, how do proxy signatures work? 
I hope that I have at least gotten "proxy signatures" correct as the proper name of what I want to ask about...  If not, then please correct I have recently seen a feature in Bitcoin, called BIP38 -- but despite
reading their wiki pages and linked materials I can't figure out exactly how it works. The effect is that given a particular type of persistent public key belonging to a Alice, Bob who has no other information about or from Alice, can combine that key with a nonce, or with a hash, to produce a unique derived asymmetric key.  He can then use this derived asymmetric key to encrypt a message that can only be decrypted by Alice's private key.  Carol, Dave, Eve, etc, even given all information except Bob's nonce or hash and Alice's private key, cannot identify Bob's derived key as being derived from Alice's persistent public key , nor identify Bob's message as being one that is encrypted to Alice's persistent private key, nor decrypt the message. It acts like Alice's persistent public key is one side of a
non-interactive Diffie-Hellman key exchange, except that I have no idea what "non-interactive" and "Diffie-Hellman key exchange" could possibly mean when used together.  So it seems magical. I would like to understand the mathematics that allow these derived keys to be created and the mathematics that allow Alice to identify and decrypt messages encrypted using these derived keys.  I would like to know what Hard mathematical problem prevents Carol, Dave, etc, from being able to identify Bob's derived key as being part of the set derived from Alice's persistent key.  And if there are any special properties that the persistent key Alice publishes must have in order to work with this scheme, I would like to understand
those too.  I do not know whether Alice can recover the nonce Bob used when decrypting the message, but that would also be an interesting thing to know for security purposes. Should the nonce be treated as a shared secret between Bob and Alice?  If so then what are the
downsides of later using the nonce as a shared secret for other
protocol steps?
Anyway, any help you can offer will be appreciated.

@_date: 2014-03-23 12:40:21
@_author: Bear 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
Right now there are very few things that come with a very good reason to trust them.  After NSA/Snowden revelations about how standards bodies are targeted
for interference and caused to fail, one can no longer point at a standards body and cite it as a reason to trust.
That leaves you with two options; first, there is the opinion of working
professionals, and that still comes down to "why trust this professional
over that one?"  Second, there is mathematical proof.  The problem with mathematical proofs at least so far is that to the extent we can cite meaningful mathematical proofs, we usually can cite them only about a few primitives whose performance is abysmal. If you truly don't care about speed, there are solutions such as Blum-Blum-Shub for stream ciphers, which have a profound proof backing
them up in terms of security.  But even in hardware, I think you have to
care more than that. Still, the future may be different.  It may turn out that nothing we know about now *except* that unwieldy, finicky set of provably-hard functions is actually truly hard.

@_date: 2014-03-24 13:36:57
@_author: Bear 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
Almost true, but not quite completely true anymore.  At Eurocrypt in 2012, Reschberger published  an attack on full IDEA.  It exploits narrow bicliques in order to get an attack with complexity of  2^126.1 against a 128-bit key - a 2-bit break.  Not nearly enough for practical deployment against a 128-bit key, but it
demonstrates a tiny chink in the armor. It could happen tomorrow that someone figures out how to broaden the attack and get another 2 bits....

@_date: 2014-03-26 12:45:29
@_author: Bear 
@_subject: [Cryptography] Dark Mail Alliance specs? 
I don't want to divert you from what may be an entirely useful course,
but I'm firmly of the opinion that interoperability with present email infrastructure, or even the attempt at it, is fatal to privacy.  In
fact even the promise of such interoperability is a strong reason to
NOT trust a new encrypted email application. By the time you have something that allows email to "sync" seamlessly across several devices, allows file attachments, allows clickable URLS to invoke browsers that can execute scripts, can show attached file contents to a browser so people can use a (script-executing!)
browser to view it, links to external libraries to resolve MIME types, and uses plugins created for unencrypted systems, you have
introduced at least a dozen gaping holes that some black-hat can and will drive a tank through.
It does no good to encrypt messages in flight (or even on disk!) if the
application that can read those messages sprays access to them around
indiscriminately to whatever happens to be installed on the user's

@_date: 2014-03-26 12:56:25
@_author: Bear 
@_subject: [Cryptography] Dark Mail Alliance specs? 
If the machines in a cloud are secure, that means that they do exactly what their owners want and intend them to.  The problem arises because the users are not the owners.
Without some very strong reason to trust that the owners' security interests are in fact the users' security interests there is no reason to even consider security on such machines
to be an advantage for the user.

@_date: 2014-03-26 16:59:22
@_author: Bear 
@_subject: [Cryptography] Dark Mail Alliance specs? 
It's true, I trust cloud server companies less than I trust car rental companies.  But there's a good reason for that.  Cloud server companies are effectively immune to contract law with respect to user-oriented security of the machines.
There is no risk, from my point of view, that the car is not doing what I want it to.  As a human being actually driving it, I can see where I am and I can tell that the car is responding to the controls, etc. Similarly, when I rent cloud servers, I can tell that network requests are being served, that compute jobs are getting done, and that all the positive results I want are actually happening.  But security is a negative result. If you want to demonstrate security you want to show that something *didn't* happen, and that is much much more difficult. I don't know whether the cloud machine I rented is secure in my interests as a user or just a VM sitting there logging all the packets and memory writes, until suddenly I'm seeing my
customers' credit card details being sold at black hat sites.  Because the machine owners can in principle break the machine users' security with impunity, with no evidence visible to the user, and then later deny all knowledge of how that customer database got out
there, I'm never going to be able to prove it if the security of the cloud fails me.  In the absence of a reliable way to have evidence of breach of contract, any reliance on contract law is likely to be long drawn out, expensive, and ultimately fruitless.

@_date: 2014-03-27 10:34:00
@_author: Bear 
@_subject: [Cryptography] Dark Mail Alliance specs? 
I do assume that.  I can set up a virtual machine in userspace and give someone root access to that virtual machine at any moment.  And while they have root access, I can still go in and commit live edits to their running image, invisibly modifying crucial software on their virtual disk, performing actions that ought to require 'root' while leaving no logfile traces, etc.  Doing this doesn't even require me to have root access on the host machine.  Why do I have a reason to assume that the people running these data centers, who are in the business of setting up virtual machines, and who *do* have root access on their local machines, can't do the same thing?  Indeed, why do I have a  reason to assume they're not routinely being compelled to?  I may be excessively cynical, but hearing the words "cloud" and
"security" in the same sentence, if the speaker wants me to believe him, is only likely to make me angry.

@_date: 2014-03-28 13:15:03
@_author: Bear 
@_subject: [Cryptography] Dark Mail Alliance specs? 
A lack of confidence.  This is a tautology.
Those are literally the two alternatives; one trusts in something that cannot be secured, or one does not trust in something that cannot be secured.  The absence of effective technical means to secure something or detect a security failure implies the absence of effective legal means to secure that thing or punish that security failure.
Hmm.  As I read it, I think maybe the sentence above is one of
those fundamental truths like Kerckhoffs principle that ought to be internalized in the entire industry.

@_date: 2014-03-28 13:29:34
@_author: Bear 
@_subject: [Cryptography] Dark Mail Alliance specs? 
The threat under discussion, at least from my POV, was the conflict between security requirements of the machine owners and the machine users.  As such the entity we're worried about as users is the machine
owner - who does in fact have physical access to the machine and who can choose at any moment regardless of what the user believes he's getting whether it's a dedicated machine, VPS, or shared hosting. By being the owner of the machine, with physical access, ability to reboot as needed, ability to emulate in software if that facilitates my attack, etc. The machine user is nowhere around to make sure that he's actually getting the running dedicated server he paid for, and
as the machine owner I can fix it so absolutely every packet he gets
is indistinguishable from what he'd get if he were. No evidence means
no crime.

@_date: 2014-03-31 12:09:42
@_author: Bear 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve numbers 
If you want "nothing up my sleeve" numbers start with a source that has long been published and do a simple repeatable manipulation to it.  For around 2^14 bits, I think you could take a long novel (say, the Gutenberg Press copy of Fyodor Dostoyevsky's book _The Idiot_).
Separate it into sentences. Eliminate any duplicates.
Take all combinations of two sentences in a deterministic sequence. Produce a SHA256 block for each. Then publish the code that munged the book into the bitblock, publish the exact version of the book you used, and everybody can verify that the bits you used are in fact derived from that book in a straightforward way and that you didn't manipulate the text to get any particularly-desired results.

@_date: 2014-05-01 12:16:13
@_author: Bear 
@_subject: [Cryptography] GCC bug 30475 (was Re: bounded pointers in 	C) 
At this point it's hard to say that C's treatment of such things as 'undefined' is better.  Having semantics that can't be known or checked is nearly as bad as working on a "written off" class
of hardware.
Given the history of bugs, I would rather have a language be cripplingly slow on some hardware (doing all math via emulation, yuck!) than have important parts of mathematical semantics unknown, unknowable, or unenforceable on all hardware.  If I want to use Java on a 1's complement machine, performance will suck but it'll have no math-related bugs that it didn't have on the original.

@_date: 2014-05-05 21:46:56
@_author: Bear 
@_subject: [Cryptography] crypto software design advice; 
I'm coding a peer which does roughly the job of an encrypted inetd +
onion routing through a fully decentralized p2p network. The basic security design is that I'm avoiding it having the ability to
decrypt anything at all that it doesn't absolutely need to do its job.  It has a 'listener', which listens on a known port and, when it gets an incoming connection, accepts a new peer (with an ephemeral port), negotiating a link key, updates its peer list, and then handles messages normally.
with that peer, and then it has a decrypted header that gives minimal
information - at least a protocol number - and a payload which, despite
being decrypted with the link key, is still encrypted with its end2end
key.  The end2end key depends on the protocol and for protocols not
handled by the peer isn't available to the peer.
If the protocol number indicates that the payload should be handed off
to a program on the local machine, that's all the information that the
peer ever gets.  In the more usual case that the received packet isn't
for the local machine, the protocol will be 'route' or 'broadcast' or
'ping' or something, and there will be minimal additional information
such as a key/address, an expiry time, a list of IP+port addresses for
other peers, and/or a ZKP puzzle or solution (to prevent someone
announcing the receipt of a message and thereby cancelling its further
propagation, without actually having the key/address to which the
message is addressed).  Anyway, if another message (or more than one)
needs to issue as a result of processing the incoming packet, the peer
changes the header information as needed, then uses one (or more) of its
other link keys to encrypt the new header and reencrypt the payload, and
sends to whichever other peers are appropriate.  Rinse, repeat, until it
gets shut down.
Now, my basic question here is, is there a good reason for this to have
a separate  administrative interface? Someone could use the administrative interface to do things we don't want strangers across the net to do, such as accessing link keys by
port, dumping our (limited) routing information, shutting down the peer
or something. The usual method of securing this would guard against that
by having the admin interface on a separate port.  But the fact of the matter is that nobody who isn't the real admin ought
to be able to get an admin command accepted, because absolutely every
packet is encrypted and nobody else ought to have the admin's key. So
the admin can (and maybe should?) just connect using the listener, like
any peer. I like this because it would make the program simpler; no separate admin
interface means not as many special cases and not as much complexity to
set it up or administrate it.  But is it reasonable to accept
(encrypted) admin commands on a regular port accessible to random remote
peers?

@_date: 2014-05-20 09:35:46
@_author: Bear 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
Purely aside from technical issues about micropayments, it was always my impression that micropayments as a method of doing business failed, not because of any implementation or security barriers but rather because people loathe them.
Purely aside from the decision that it *is* worth 20? to listen to an Aerosmith album, it is stressful to people to have to *make* the decision.  Money decisions are stressful regardless of the amount under consideration.  When the amount under consideration is too small to justify the stress, people justifiably resent being forced to make a money decision.
When you embrace a model that subjects people to stress every twenty seconds, they will never relax and enjoy the experience.  And they're going to go off and do something they enjoy.  Somewhere else.

@_date: 2014-05-23 13:56:16
@_author: Bear 
@_subject: [Cryptography] The proper way to hash password files 
You're not going to sell them on that idea, but I've got one that seems like a much easier sell and a very simple good idea to me; Why not make 9/10 (or, heck, 99/100) of the entries in a password file correspond to fake accounts that simply ring an alarm and shut down access to the legit accounts from that file if their passwords are ever actually used?
It's still never a good thing for password files to be stolen, but
since no method of preventing the theft will be perfect, we should at the very least make the theft harder to exploit.

@_date: 2014-05-23 14:03:01
@_author: Bear 
@_subject: [Cryptography] New attacks on discrete logs? 
Would anyone like to clarify what exactly they mean by "small" and "large" characteristic here?  Please?

@_date: 2014-05-23 15:50:40
@_author: Bear 
@_subject: [Cryptography] The proper way to hash password files 
I note that this paper has been published for years, contains a good and simple idea, does not materially affect any other aspect of system operation, and that I have never seen or heard of an actual implementation being deployed in the wild. One might almost conclude that people don't want it known when their password files are stolen....  Oh, wait .....

@_date: 2014-05-24 11:47:45
@_author: Bear 
@_subject: [Cryptography] New attacks on discrete logs? 
Thanks Jerry, for that lesson on group theory.  It was genuinely
helpful.  It permits me, at least, to formulate the question in a more specific way. What I really wanted to know was how to adjust security estimates for modular groups as a function of the bit length
of the prime factor of the modulus.
Using modular addition as the operation, modular groups in the integers are, for reasons you explained, necessarily integers modulo some number which has exactly one prime factor.  And if that factor is "small" this algorithmic advance makes operations in those groups somewhat easier to reverse, and if that factor is "large" this algorithmic advance does not make operations in those groups significantly easier to reverse.
Because reversing group operations is provably at least as hard as factoring, (though in practice, at least as far as we know how to do it now, much harder) I could conservatively interpret "large" as meaning >4096 bits, or out of range of current factoring technology, and "small" as <2048 bits.  But that's a very dire interpretation, and would effectively
destroy Elliptic Curve cryptosystems outright, which no one so far is claiming that this advance does.  So what bit lengths (of the prime factor of the modulus) are we talking about when we say "small" and "large" in this

@_date: 2014-05-27 10:17:26
@_author: Bear 
@_subject: [Cryptography] client certificates ... as opposed to password 
This is worth a note:  Good security design is compartmental.  We should be writing software that has strictly defined information
inputs and outputs, and does specific, narrow things with them.  Ideally, most of the "sensitive" pieces should start, run, and exit without ever putting up any UI. A client-side proxy is a much better idea in the first place than a plugin, because a client-side proxy has much more narrowly defined information input and output and a much more well-defined job to do.  Its design need
not be warped by conforming to conventions or standards driven by non-security considerations.

@_date: 2014-05-28 15:20:02
@_author: Bear 
@_subject: [Cryptography] client certificates / client-side proxy 
Its broad scope for mischief is much more well-defined and monitorable than the broad scope for mischief of a plugin.  If you want security, IMO, you want simple components that each do one thing, work with publicly specified interfaces, and have minimize 'implicit' ways for information to be passed between them.  If something isn't in the HTTP stream, then a proxy isn't where it can see that thing.
OTOH, a Plugin can see browser settings, browser history, cookies, and probably contacts and appointments and your mail and news preferences depending on how many other plugins you've got and how they're configured.  With a plugin you have to secure every bit of that additional stuff against it, AND everything in the HTTP stream as well.  With a proxy you just have to secure the HTTP stream.  Further, a plugin has a much more complicated life.  It has
to coexist with an unknown number of other plugins, many of
which will appear in combinations its never been tested with
and which may be doing other things with whatever it's trying
to secure (or whatever it's trying to steal, depending on who wrote the plugin and why).

@_date: 2014-05-29 15:30:55
@_author: Bear 
@_subject: [Cryptography] Truecrypt removed by authors 
Why dignify such claims by calling the people making them "geeks"?
Geeks are people who actually know something, although their knowledge
may be specialized.  When geeks make a claim, it's generally true although it may not mean what they think it means. What you were dealing with here are ignoramuses.
Salt means you have to hash against each account separately, but it doesn't ever increase the work factor of a brute force attack by more than the number of accounts. Length of a password hash prevents makes getting in on a hash collision less likely, but collision passwords were never the issue in the first place.  The vast majority of the time a collision password wouldn't even be something someone could type.  Further, it's vastly easier to anticipate the real password than find any collision, regardless of the hash length stored.  In practice storing
a 128-bit hash is more than adequate protection against 'collision'
Any "geek" talking about password security would know this.  So these
definitely weren't geeks.

@_date: 2014-11-04 19:25:17
@_author: Bear 
@_subject: [Cryptography] $750k Fine for exporting crypto 
Y'know what?  If they popped up for no reason other than "they broke the law" -- even when I think that particular law is stupid -- I'd
really prefer that situation to one where people get picked out for enforcement based on political connections or degree of covert
cooperation. I think the US is drifting dangerously far from the ideal of a rule of law.  You know, where the law is the same for everybody, no matter
how much you contributed to somebody's political campaign, no matter
whether you're sitting in an expensive government office, no matter how much money you make, no matter whether you're cooperating in
attempts to subvert the law against other citizens and no matter whether you're saying something unpopular?  Increasingly, we're behaving more like China, where they have a rule BY law instead; where the law is a tool for the powerful corrupt to
extract rents, to extort perks, to take revenge, to enforce covert or personal policies by selectively enforcing overt public law, etc. I know there's always some of this kind of slime under the corners when you have too close a look at any government, and always has been.  But really, a rule of law IS achievable, and when it's working it means that shit is the exception rather than the norm, and that
abusing the law that way is a crime which entails a genuine risk of getting caught, being publicly prosecuted, and going to the same jail where you've been putting your victims.
A stupid law can be oppressive, which is bad enough, but corruption is a pure poison that can suck the life out of an entire country, breeds domestic terrorists like flies, and generally drives us slowly down the road toward bloody revolution.  Stupid laws make me roll my eyes, file amicus briefs, write congresspeople, etc... Today it made me vote for several minor-party candidates where incumbents who favor stupid policies had no major-party
opposition. But corruption is altogether darker and the slowly rising tide we've been on for the last couple of decades makes me fear for my long-term safety.
I say enforce it uniformly or repeal it.  Every law that isn't uniformly and promptly enforced, all the time, contributes to public corruption because selective enforcement means someone is using it for blackmail or extortion -- and probably thinks "that's what it's for" meaning blackmail and extortion have become the norm. So yes, the whole damn thing from prostitution to labor laws to export restrictions to monopoly busters.  If you're not going to enforce it for everybody, all the time, then get rid of it so you don't have the temptation to become corrupt as you use it for blackmail, extortion, revenge, or for the harassment of dissenters.

@_date: 2014-11-13 11:01:21
@_author: Bear 
@_subject: [Cryptography] ISPs caught in STARTTLS downgrade attacks 
End-to-end email encryption solutions such as PGP do not protect crucial elements in the headers.  STARTTLS was supposed to do so but can only be run by the parties that run the mail servers.  Since most correspondents rely on mail servers operated
by their ISP's (and most ISP's block customer mail servers as
non-negotiable policy in order to limit spam sending) STARTTLS was never practical for end-to-end use. The plaintext of STARTTLS email is normally visible to the sender's ISP and receiver's ISP.  Unfortunately, the ISPs do not risk substantial losses from failures of STARTTLS and can subvert or fail to implement it in ways not immediately visible to those who do. Predictably some have therefore been subverting or failing to implement Sigh.  One more round of "Internet Mail, Privacy Fail."
I'm increasingly of the opinion that there is no protocol that can be derived from SMTP and compatible with it that can provide the practical privacy of a paper letter in a paper envelope.

@_date: 2014-11-18 16:18:44
@_author: Ray Dillinger 
@_subject: [Cryptography] IAB Statement on Internet Confidentiality 
This is true.  Additionally, MITM is not a risk-free passive attack.
Someone doing MITM must make an active attack on a channel
whose "legitimate" content is known to the parties at the endpoints.
Occasionally they will notice that there is a mismatch.  MITM risks
discovery and exposure in a way that someone passively listening
does not.
So even if MITM is feasible in some instances and in the short run,
it is not something that can be done at large scale without the
discovery and knowledge of the people whom it's being done to.
I think one of the biggest fallouts of opportunistic encryption
would be all the different "snoops" becoming more able to detect
each others' attacks.  I bet Chinese eavesdropping is going on in
hundreds of places in the US where the NSA would be intrigued
to find evidence of it, and I bet CIA eavesdropping is going on in
hundreds of places in China where the equivalent Chinese agencies
would be similarly intrigued to know.  The risks undertaken by these
agencies when operating MITM attacks in each others' territories are
not inconsiderable, and the consequences of detection are such that
the threat of discovery by each other is likely to reduce at least the
number of snoops operating and the frequency of snooping in the
affected areas.
There is some comedy to be played out there because the most
common means of detection that an MITM attack is in progress
would be having it interfere with one's own attempt to make an
MITM on the same channel.  But that never happens if both (or
all) parties are merely eavesdropping passively, so an MITM-able
protocol is definitely a step up from an eavesdroppable protocol.

@_date: 2014-11-20 18:18:24
@_author: Ray Dillinger 
@_subject: [Cryptography] FW: IAB Statement on Internet  Confidentiality 
At the very least such a proposal would force us to confront the usability
and key management issues of the software and at least put us in a
position to become ware of its security interactions with various mail
I'm agnostic on the matter though; I've never really felt the need to
encrypt, or sign, on this very public forum.
The 'Outlanders' list and similar venues are a different matter entirely,
but they are not open-subscription, the encryption they use is not
public-key, and obviously they do not have the usability and security
issues of a PKI to worry about.

@_date: 2014-11-23 16:33:58
@_author: Ray Dillinger 
@_subject: [Cryptography] encrypted list mail, 
Having the list re-encrypt and re-sign each message is a good idea
if the list of activities you are seeking to forestall includes
traffic analysis.
If the server re-encrypts and re-signs each message, and the
moderator forwards messages to other recipients in batches
only once a day or so, then it becomes less obvious which
sender was responsible for writing which message.
Of course it's only a small part of the job.  If you really
want to defend against traffic analysis, you have to generate
cover traffic sufficient to (and in the complementary pattern
to) make the real traffic indistinguishable from noise.

@_date: 2014-11-24 17:24:47
@_author: Ray Dillinger 
@_subject: [Cryptography] Blogpost: CITAS, a new FBI security program proposal 
Note to list participants: check the CC line of the original message
before responding.  We are aware that this list is always monitored, but
this time I have explicitly invoked monitoring and explicitly invite
response.  Hello Agent Chesson; feel free to join the (list and)
discussion if you have something to add or correct.  It's a moderated
and usually very polite list, although events in the last couple of
years have caused some resentment and a great deal of distrust here
toward American Three-Letter agencies.
Brief: The FBI is proposing a security service to assist American
companies in achieving network security. It is called CITAS, for
"Computer Intrusion Threat Assessment System."  It is not an active
program yet; My impression that it is the proposal and brainchild of
special agent John B. Chesson and that he is actively trying to raise
support for it both within the agency and among its potential clients.
This is one of very few proposals I have seen from any US agency that
genuinely seems likely to support rather than subvert security, in the
strict sense of owners retaining control of the assets they own.  It
does not require backdoors, it does not require keeping insecure
plaintext traffic on the network, and it does not propose to compel
What it proposes is that companies who join the service allocate an IP
address on their company's subnet for the use of the FBI, and the FBI
can then set up a honeypot at that IP address. Routers and switches in
the company's DMZ would direct traffic to the honeypot just as though it
were a company machine, leaving no clues to the contrary in route traces
or DNS, but the traffic would tunnel over some other channel, probably a
VPN, to a location controlled by the FBI.
The honeypot would be physically located at and controlled by an FBI
data center. This does not imply that the FBI gets any
"behind-the-firewall" view of a company's network; the company's
firewall can distrust the honeypot just as much as it distrusts unknown
IP addresses out in the wild.
The FBI would monitor the honeypots in real time for threats and
attacks, and when any "significant" threat or breach is detected, share
the information immediately with the subscribing company.
Less briefly:
 ?
This arrangement strikes me as likely to be highly effective in terms of
security, because the FBI could leverage manpower and monitoring effort
across a huge pool of honeypots truly indistinguishable to attackers
from genuine targets.  Effort spent by an FBI agent to understand and
script a log checker for a new threat would instantly apply to thousands
of companies via the honeypots sharing software, where the equivalent
effort spent by anyone else takes weeks to months to achieve wide
adoption, and never achieves wide adoption until after it is redone for
the nth time by many open-source volunteers.
This arrangement also strikes me as problematic in that it would also
allow the FBI to set up a huge pool of Tor, Gnutella, Bittorrent, etc,
nodes truly indistinguishable to users from genuine nodes run by people
who support anonymity, uncensored journalism, whistleblowers, and free
speech. The data would, of course, be shared across all the usual
law-enforcement, espionage, and security agencies of the US. Although to
be honest, these services are already so heavily monitored that there is
little left to lose.
Although Agent Chesson, whose presentation I attended, did not mention
these other uses, I would expect widespread adoption of this system to
mean effectively the death of "anonymous" P2P services such as Tor, due
to the simple fact of most of the gateway nodes being FBI-operated
sockpuppets.  While Tor or something like it remains the only way in
most of the world to use the Internet for uncensored journalism or
whistleblowing, the FBI cannot possibly ignore that as a channel it is
also used by criminals.
There is also some risk to the companies involved in the existence of
machines which they do not control but which have addresses publicly on
record as belonging to that company's subnet.  They could experience
adverse public perception if a honeypot became publicly known as
someplace where an unsavory or criminal activity were happening and its
address were traced back to the company's IP block.
Ray "Bear" Dillinger

@_date: 2014-11-24 20:16:44
@_author: Ray Dillinger 
@_subject: [Cryptography] Blogpost: CITAS, 
Space, power, cooling == tax dollars at work.  I share your
concern for reducing gov't waste, but I don't think this is
particularly wasteful, nor does it lean heavily on any small
set of taxpayers. The FBI has to pay for a datacenter to run
the honeypots.  Access is just a matter of the FBI datacenter
being inside a government-owned building, yes?  The system
as described doesn't require FBI access to assets owned by
the networks the honeypots are pretending to be on, aside
from an IP address and having routers configured to let them
send and receive packets in the first place, and that is really
That is a good point.  In buying machines for an FBI datacenter,
it's likely that they'd get a lot of very similar machines (same
processor, same speed, same disk, same network interfaces etc)
and keep them on the same software (same OS, same servers, etc)
just to keep the admin expense controllable.  It could become
a signature set (or a few sets) of hardware known to/identifiable
by sophisticated intruders and markedly different from that run
by the company whose address space the honeypot were using.
I would not be worried about the device discovering enough
about attacks to be effective.  I see hundreds of attacks per
hour on any DMZ box, and I don't have time to figure out more
than a couple a day, let alone try to track them down. The
idea that somebody could devote the time and effort to
correlating the evolution and spread of those attacks across
a baseline created by honeypots listening in many places
would be a richer information source about attackers than
any single actor is now bringing to bear.
You are right that these machines could, and inevitably would,
serve as the source for attacks as easily as they could serve
as honeypots, and I'm concerned about that too.  The FBI will
attack Tor, Bittorrent, etc in a heartbeat, and as often as
the FBI itself has a security breach these machines will also
be the base for attacks against the companies whose network
address space they're using.
But in that regard the FBI is probably more accountable than
some random for-profit business in India. The random for-profit
in India wields just as much trust with a correctly configured
firewall as an FBI honeypot pretending to be on the firewall's
network. So I'm not seeing any added problem as far as that
That's actually one of the things I like about this; it DOESN'T
require the company to trust anything that, if misused, would
make breaking its security easier. There is no need for a
honeypot to have any trust relationship with the rest of the
Hah!  That one ought to get laughed out of court, I should hope.
But you're right that if MS were to make secure operating systems
they'd also make enemies in the current security industry -- as well
as making enemies of crooks across the world and making our own
three-letter agencies angry at them for depriving them of intel
sources.  Never mind that it would also deprive all their enemies
of intel sources and avenues of attack, they'd be angry anyway.
It's a problem.
But it would be so worth it - to the whole world - in increased
economic output if it didn't cost so damn much to keep MS boxes
secure and if the expense of security failures could reduced at
the same time that would be nothing short of great.  So, yeah, I
share your dream.

@_date: 2014-11-25 08:31:01
@_author: Ray Dillinger 
@_subject: [Cryptography] Blogpost: CITAS, 
Sounds like you didn't read it.  This isn't the usual LE
proposal that everyone should buy guns, load them with bullets,
point them at their own heads and hand over the triggers.  Though
given the history I can understand why you'd stop reading after
the first sentence; that's kind of what all of them have been
up until now.
In this case, for the first time, the gun is pointed AWAY from
the people whose network it's sharing address space with; there
is no requirement that the network extend trust to the honeypot.
To the extent that the gun can be pointed at the network, it's
still got to shoot through the firewall, so it's no more
dangerous to the cooperating businesses than the guns everybody
on the whole wide Internet already has.
Trust is the destructive force that security people are quite
correctly scared of creating. A trusted system is defined as
one that will destroy you when it fails.  This is the first
proposal I've seen from any branch of the USG that doesn't
require the creation of more trusted systems, which means
the first proposal I've seen that indicates somebody out
there at least understands what security is.

@_date: 2014-11-26 10:05:00
@_author: Ray Dillinger 
@_subject: [Cryptography] Blogpost: CITAS, 
Aw crap, they screwed the pooch again.
I thought maybe the FBI had remembered its mission, but
apparently you're right, there's no trusting them.  I guess
I'm done defending these guys.
At least people are still allowed to protest warrantless searches.
Found this when I went to confirm that this actually happened.

@_date: 2014-11-27 21:39:15
@_author: Ray Dillinger 
@_subject: [Cryptography] [cryptography] Underhanded Crypto 
Okay, I've got something....  though it's more
fatally-flawed than underhanded; I was trying
to make something secure when I came up with
it, but I failed. I don't have a way to actually
read the messages, but I did find a way to
distinguish encrypted messages from random
output, and a way to do better than random at
identifying messages encrypted with the same
key. And you know, attacks always get better,
not worse.  Still, it sounds like a great idea;
I could totally sell this.
It could be patched a little with complicated
cipher modes and things, but we don't like systems
that need to use complicated cipher modes and
I could "fix" it by increasing the block size.
But the construction is peculiar, and its good
properties only hold for a block sizes which
are exactly double the square of a natural number.
so I could totally make a 128-bit version, which
would not need such complicated cipher modes but
would still have encrypted messages distinguishable
from random.
The design is simple: generate a key stream from a
CPRNG, but instead of using it to XOR the plaintext
as in a typical stream cipher, you use the keystream
to generate an infinite key schedule of S-boxes for
a Feistel cipher.
Put on your best snake-oil salesman voice....
"I invented a four-round Feistel cipher on 32-bit blocks,
that's totally unbreakable! The sekrit is it uses totally
unpredictable secure S-boxes!  It generates a new,
random set of S-boxes every round, using a secure CPRNG
that you seed with the key.  It runs just four rounds,
because the S-boxes are totally unbreakable so you
don't need any more rounds than that!
'Cause there are about 2^48 permutations of all the
4-bit blocks, and each block uses four of them chosen
completely at random from all the possible sets,
That's 144 bits of randomness that goes into every
block, which is enough to generate every POSSIBLE
permutation of 32-bit blocks with a factor of ten
million or so left over for lunch!  And it totally
does generate that total set of permutations, with
just four rounds! Luby and Rackoff proved it, way
back in 1988, so I don't know why all those losers
haven't been using that result and making totally
secure ciphers ever since then!
You can't even analyze it, because every block is
encrypted with a totally DIFFERENT 32-bit permutation!
Selected randomly and with equal probability from ALL
the POSSIBLE 32-bit permutations!  The uncorrelated
sequence length is 12k bits, and it uses 256 bits of
output to generate S-boxes per 32-bit block, so up to
a message 48 blocks, or 192 bytes, long, there's
SOME key, not that anybody could ever find it, that'll
transform ANY message into ANY ciphertext!
Oh yeah, that reminds me, you can use any size key
you want!  The pseudo random generator I invented
for it has about 12.5k bits of state, so it won't
repeat until sometime AFTER THE UNIVERSE DIES.
Until you start getting keys six kilobytes long
or so, there's probably not ANY two keys that will
produce the same output, stream and that's including
the stoopid birthday paradox.  If you use a key
that's over 12 kilobytes long, then yeah, there's
probably some other key out there twelve kilobytes
long or less, that'll produce the same sequence of
S-boxes.  But there's no way anybody could possibly
find it!  Like anybody even has to worry about that,
when finding a key 256 bits long is the next best
thing to impossible!
Okay, no more snake-oil salesman voice.
Can you figure it out, or do you want me to post
the code?

@_date: 2014-10-08 08:57:27
@_author: Bear 
@_subject: [Cryptography] Best Internet crypto clock ? 
I have to point out here that there is absolutely nothing in the Bitcoin
protocol that prevents servers who solve a block from misreporting the time.
In fact, it has been a strategy in the past for parallelizing the
hashing. When the portion of the nonce that people could adjust seeking a winning hash was exceeded by hardware capacity, the time field was incremented even though the mentioned time had not yet arrived, and the search started over.  Because that was faster than rearranging the transactions to form a different base for hashing, I guess.  Anyway, the time reported in bitcoin blocks is approximate, and historically not even monotonically increasing.  It is testimony
that the Bitcoin network accepted the server's claim of what time it was, so probably, usually, within ten minutes of  the real time.  But it does not establish any precise notion of time.

@_date: 2014-10-08 09:20:24
@_author: Bear 
@_subject: [Cryptography] Creating a Parallelizeable Cryptographic 
True, but he has a point.  In most programming applications a hash drives a hash table in a context where we aren't at all worried about
an attacker.  For example, in a program that's keeping track of the actors in a
simulation, where each actor is assigned a GUID number when it's
created.  Then as the simulation continues, old actors are removed and
new actors are created, and the program keeps track of them with a hash table.  This app doesn't even communicate over any network and (if it's, eg, a single-player game) probably doesn't compute anything that any attacker cares about. All the programmer cares about for the hash function on that table is that a) you want aliasing (accidental collisions) to be minimized.
b) you want the hash to be fast to compute.
c) you want "random" distribution of GUID's in the hash table    (ie, no single part of your hash table should be *more*     full than the other parts, or at least not by any margin     distinguishable from statistical noise on random numbers).
I have in fact solved this problem in the past by using a counter
viewed through a linear congruential transformation to assign the GUID's, and then using the GUID modulo the size of the hash table as a hash function.  And that's considered to be an elegant and
completely standard solution. Table hashes, usually, are not cryptographic hashes.

@_date: 2014-10-09 14:01:34
@_author: Bear 
@_subject: [Cryptography] Sonic.net implements DNSSEC, 
Here is an amusing/infuriating example of an otherwise pretty good ISP getting security exactly wrong:
Sonic implemented and deployed DNSSEC - and put it on their shiny new servers along with an 'RBZ service' that censors supposed malware
and phishing sites.  And while they told their customers about DNSSEC, they didn't mention the 'RBZ service.'
They didn't get prior informed consent from their customers.  In fact
they didn't inform their customers, beyond quietly putting up a few mentions on webpages their customers normally have no reason to look at. They didn't provide a click-through link enabling customers to get the content anyway.
And they diverted traffic to a page that does not mention who is doing
the diversion, how, or why, or how to opt out. And they aren't providing DNSSEC in any form that doesn't have this 'service' (coughATTACKcough) imposed. Black hats immediately found a way to get sites they dislike onto the list of supposed malware and phishing sites. Among the blocked sites:   Local democratic party campaigners (first post).   Financial services and markets - at a crucial time. (page 4).   Software development sites (apparently some devs use the same      utility network libraries used by malware devs, so the      unknown-because-todays-compilation executables have code      in common with known malware and aren't on the whitelist...)
I had occasionally been annoyed by the 'mousetrap page' on software
dev sites, but never annoyed enough to finally eliminate all other suspects and track it down -- too much trouble, right?  But after personally taking a hit on the 'financial services' thing, I tracked this down to sonic.net -- I'd been assuming that it was some overeager plugin that had defaulted to 'ON' and I just hadn't
figured out which one and how to turn it OFF.  But it kept happening
even with all plugins uninstalled. It turned out to be the very same attack that I had switched to DNSSEC specifically to avoid.  And it was performed by the very same ISP that I'd been relying on to protect me from it. I have rarely been so angry.
As I understand the law, "common carriers" are protected from prosecution when crimes are committd using their services because they aren't in the business of determining what traffic moves via
those services.  But Sonic.net, by failing to conform to the standards of care for
filtering services (no prior consent, no clickthrough link, no identification of blocking agency, no basic notification, no provision of DNSSEC service without the blockage)  appears to me to have no claim to common carrier status for DNSSEC.  They DID make the decision, based on content, what traffic they would carry on DNSSEC.  As a result, didn't they become liable for damages from crimes committed by the abuse of that service?

@_date: 2014-10-09 14:24:19
@_author: Bear 
@_subject: [Cryptography] Creating a Parallelizeable Cryptographic Hash 
Yes, that is exactly the distinction.  A counter viewed through a linear congruential transformation is a perfect example of a
low-discrepancy or quasirandom function.  It will generate *every* possible value, *once*, before the counter rolls over and it starts repeating. As a reductio ad absurdum, if it's a sixteen bit counter and you've seen the last 65535 outputs, you know exactly what the next output is going to be -- it's going to be whatever value you haven't seen yet, and then the sequence you've already seen will repeat.  If it were a good cryptographic hash, you would have, as always, absolutely no idea what the next output will be just by knowing previous outputs.

@_date: 2014-10-21 10:02:33
@_author: Bear 
@_subject: [Cryptography] Best internet crypto clock 
IIRC a lot has been done to verify video and audio as having come from a certain moment in time or general location based on recovering
the precise 'drift' of the omnipresent 60-cycle (or 50-cycle if you're Australian) hum of the surrounding electrical system.  While it's fairly precise, it's not exact, and over very widespread areas the exact frequency and interference patterns recovered from a video or audio record have been used to determine exactly when (and to some extent where) the record was made.  Relevant law enforcement and Intel agencies are, yes, known to monitor and record the variances specifically for purposes of dating recordings
that later may become evidence.

@_date: 2014-10-24 10:56:35
@_author: Bear 
@_subject: [Cryptography] Uncorrelated sequence length, 
I think I'm going to take issue with this.  While 256 bits plus a CPRNG is enough to prevent known and practical means of predicting the stream of numbers created, it does not constitute proof that a stream of outputs of that length *CANNOT* be predicted.  It restricts the uncorrelated sequence length to be provably no more than 256 bits.  Actually, it forces the uncorrelated sequence length to be provably less than 256 bits assuming a CPRNG. In order to prove an uncorrelated sequence length, it is necessary to first accept some standard for correlation, and then prove that every sequence of a given length is produced by a set of the possible initial states of the PRNG none of which are larger or smaller than any of the other sets by a ratio larger than your
correlation parameter.  With a state of size N, there aren't even as many possible initial states as there are output sequences of length N+1, so some output sequences must be impossible - the set
of initial states that produces those sequences has cardinality zero, and zero will definitely fall outside any correlation That is, whether or not there's any *known* way to mathematically predict whether the next bit is a 1 or a 0, there are guaranteed to *be* sequences at minimum 257 bits long which can never be produced, and therefore we cannot prove that there is no *unknown* way to mathematically predict whether the next bit is a 1 or a 0. The distinction may not be important for most applications; but if an attacker knows some important technique for attacking the CPRNG that you don't know, and can eliminate whole classes of bit sequences as being impossible to produce from that CPRNG, the attacker may need to search a key space no larger than the uncorrelated sequence length -- and, for some ciphers such as RSA, a 256-bit search space is for other reasons relatively trivial.  Using a CPRNG proven to have an uncorrelated sequence length of N bits means that the key space he has to search even when he knows a key has been produced by it, no matter what he knows about which sequences are possible and impossible (or more-likely and less-likely) given your CPRNG, is still never smaller than N bits.  So, I would say that seeding with just 256 bits of state is only reasonable if for some reason you absolutely know that there can
be no *POSSIBLE* attack on your CPRNG.  Because math that enables new attacks will keep being discovered..... A relatively humble lagged-Fibonacci generator of the sort we would never use for cryptography, provably produces an uncorrelated sequence the size of its state.  So does a linear feedback shift register. But, as is true with every generator that *provably* produces an uncorrelated sequence the size of its state, they are trivially predictable thereafter and therefore not a candidate for use as a cryptographically secure PRNG.  Any PRNG must have some impossible sequences of outputs which are no longer than its state, and cryptographic PRNGs must have correlated sequences which are shorter. For a good CPRNG, I think it's important to prove a minimum length of uncorrelated sequences. A small state proves a maximum length
for uncorrelated sequences.

@_date: 2014-10-24 11:02:51
@_author: Bear 
@_subject: [Cryptography] In search of random numbers 
This is dumb.  This is bad design.
We don't need to be providing early boot-time entropy; we need to be educating people that any design which requires early boot-time entropy is a mistake.

@_date: 2014-10-25 12:16:43
@_author: Bear 
@_subject: [Cryptography] In search of random numbers 
In response to your immediate question: since four of the most popular current extensions to a light switch are a timer, a light sensor, a motion sensor and a microphone, it can just damn well pause the boot until it reads enough off the sensors to be ready to go.  The install manual *is* allowed to say "It will function as a simple light switch until the flashing blue LED turns off after about one minute; after that, the extended functions are ready to use."
But more to the point, why does a light switch need a full Von Neumann architecture so complex that new code can ever run on it?
Give it a ROM of executable code that's loaded at the factory and cannot be rewritten under any circumstances, some kilobytes of non-volatile configuration which comes with "reasonable" defaults and is mounted on a dedicated memory bus that can never be the target of an instruction fetch, and some volatile memory also on the dedicated memory bus, and what valuable light-switchy task could it NOT do that a viable attack surface If the code on the ROM is discovered to be flawed or someone finds an attack surface, it's product recall time. We're not talking about something terribly expensive that people can't replace here, nor about something so large it can't be mailed back and forth cheaply to get repaired or replaced. Seriously, justify this "Internet of Things That Can Be Targets".
"Things" are things people don't use as general-purpose computers, so while they need writable configuration memory, that writable memory doesn't need to be executable. "Things" don't need to be
reprogrammable as such for any reason relevant to the end user. If we're talking ubiquitous, we're talking simple and replaceable and cheap.  If we're talking simple and replaceable and cheap, we can get much better security by making the executable memory
completely non-rewritable than we can by any application of
cryptography.

@_date: 2014-10-25 12:42:15
@_author: Bear 
@_subject: [Cryptography] Arduino Enigma simulator 
Cute.  If there is sufficient interest from the community in paying some more "reasonable" rates for working mechanical replicas of these
antiques, I have a tiny little machine shop adjoining my study and would be amused/interested in constructing a few.  They'd still be expensive, you must understand; there are a lot of mechanical bits that I'd have to individually mill or cast.  It would entail a lot of very tedious hand work.  And I might be forced to use brass in a few places where the originals had steel due to limitations on the hardest metals my desktop CNC mill can handle. They wouldn't be as historically significant as the actual antiques
of course; but they wouldn't be as expensive either.  You'd be on your own if you wanted to add a parallel port or USB
interface.

@_date: 2014-10-25 17:08:54
@_author: Bear 
@_subject: [Cryptography] Uncorrelated sequence length, 
You're right that a CPRNG that is "hard" doesn't provide a point of attack.  But, like most of our ciphers, we don't have any real mathematical proof that a particular CPRNG is in fact "hard".  All we really know is that we haven't found the soft spots yet.  A provably long uncorrelated sequence length is the same kind of "hard" guarantee as a one time pad -- although, like a one-time pad, it applies only to sequences shorter than that length. I think that PRNGs should be able to prove a minimum uncorrelated
sequence length (hence require RNG state) that is longer than sequences (specifically keys) whose unpredictability we rely on for the security of our other components.

@_date: 2014-10-25 17:21:03
@_author: Bear 
@_subject: [Cryptography] In search of random numbers 
Also, if you don't connect to the network before you're finished booting up, you can't be attacked over the network until you're finished booting up.  And if you're not under attack yet, such things as stack canaries have a bit less urgency....
There is such a thing as booting an operating system before the network is connected!

@_date: 2014-10-25 23:40:46
@_author: Bear 
@_subject: [Cryptography] In search of random numbers 
Works for me.  If somebody wants entropy, he has to wait until there is some.  If that delays bootup, it's because bootup is wrong.

@_date: 2014-10-27 13:44:20
@_author: Bear 
@_subject: [Cryptography] Paranoia for a Monday Morning 
Tempting as it is to look around for someone to blame, I think this is simply a result of the browser wars of the '90s.  At that time leading browser manufacturers were deliberately
introducing features incompatible with other browsers, implementing
features introduced by other browsers in ways that were deliberately
incompatible or subtly different ("extended!"), creating HTML authoring tools that deliberately caused other vendors' browsers to stumble over the differences, and scrambling to play catch-up with each other which meant that the differences and incompatibilities
multiplied with every new version.  This festering swamp is the environment that the current browser
"standards" you're talking about grew out of.
It is no remarkable thing that they are horrendously complex,
inconsistent, and filled with labyrinthine masses of exceptions.

@_date: 2014-10-27 13:54:19
@_author: Bear 
@_subject: [Cryptography] In search of random numbers 
The randomness issue doesn't look bad to me.  You just boot a non-networked OS and don't load any networking software or generate any keys until something actually needs a network connection or a key.

@_date: 2014-10-31 11:31:12
@_author: Bear 
@_subject: [Cryptography] Uncorrelated sequence length, 
I am completely baffled by this comment. A provable uncorrelated sequence length of N or greater is a proof that it is NOT even theoretically possible to distinguish any
generated sequence having length less than N from a true random
sequence.  That is the opposite of being a way to distinguish a
generated sequence from a truly random sequence. This is "like a one-time pad" in that there are an equal number of possible initial states of the PRNG that could result in any output
sequence of the uncorrelated sequence length or less, just as in a one-time pad there are an equal number (in that case, one) of possible pads that could have produced the observed ciphertext
regardless of the plaintext. If we want protection from unforeseen mathematical insights into the PRNG, we can obtain it (to some extent) by using PRNGs which have an uncorrelated sequence length strictly longer than the length of any single output we intend to generate. In exactly the same way that a one-time pad is immune to any cryptanalysis no matter how
advanced but can protect only messages shorter than itself, a PRNG of long uncorrelated sequence length is immune to any possible way to distinguish a PRNG output sequence from a random sequence, but that immunity is limited to output sequences shorter than that length. If someone is using a PRNG with a 32-bit state to generate his
128-bit keys, my brute force search space for the key is 2^32 possible
keys, not 2^128, because there are only 2^32 values which are both valid keys _and_ could have been produced by that generator. If additional constraints on the output sequence eliminate a much larger fraction of the possible sequences (eg, if it has to be a valid RSA key) then we're talking about the intersection of two sets -- the
set of valid keys and the set of sequences that length which could have been produced from your generator.  And even if both the number of possible keys and the number of possible sequences are beyond the reach of brute force, the intersection of the two might be within it. An attacker with the right insight into the mathematics would only need to brute-force a set the size of the intersection, rather than a set the size of all possible keys or a set the size of all possible
PRNG-produced sequences.  An uncorrelated sequence length strictly
longer than the RSA key means that even for an attacker with omniscient mathematical insight there is no POSSIBLE attack that can consider less than the full set of valid RSA keys, because with that uncorrelated length, every valid key is provably equally likely to be produced by the generator. We are allowed to disagree about whether this is important, depending on how likely we consider attackers with greater mathematical insight than ourselves to be. But I believe that it is a significant property
for PRNGs, and that attackers with greater mathematical insight are a
more significant risk, comparatively speaking, than attackers with
detailed knowledge of the PRNG's internal state at a chosen instant or
attackers who influence our sources of entropy at a chosen instant, particularly when we are talking about the long-term security of data
at rest.

@_date: 2014-10-31 13:41:08
@_author: Bear 
@_subject: [Cryptography] Uncorrelated sequence length, 
============================== START ==============================
The property is necessary (IMO) but definitely not sufficient.  XOR with a repeating pattern has an uncorrelated length the size of the pattern - and is completely useless for cryptography if your message is so long that any part of the pattern is used more than once.  But it is also a completely unbreakable one-time pad if the message is not that long.
Anyway, this is a property that is relatively easy to add to a system without diminishing the security of any CSPRNG you're using; you can just "whiten" the output of a PRNG having a long uncorrelated
sequence length and VERY long repeat period (such as a lagged-
Fibonacci generator with several thousand words of state) by XOR with the CSPRNG output.  The result will be have the provably
uncorrelated  sequence length of the PRNG, and will certainly be no less unpredictable than the CSPRNG.
That said, for efficiency's sake I'd prefer to simply use a CSPRNG
that has enough bits of state that there are no sequences of any length remotely close to the size of the largest single output I'll ever need which are impossible (or significantly more or less likely) for it to produce.

@_date: 2014-09-05 12:16:00
@_author: Bear 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
NP problems are those for which we do not know whether any
polynomial-time solution exists.  In practice all successful modern ciphers are based on NP problems. For cryptography we want problems that are hard;  but it doesn't really matter whether they are hard in a way that is proportional to an exponential (non-polynomial)function or hard in a way that is proportional to some polynomial of very high degree. It only matters that they are too hard for solving them to be likely or
For example, if there is some problem whose difficulty is O(N^50)
for N-bit keys of at least some minimum size, then it isn't NP, but it could be suitable for cryptography anyway.

@_date: 2014-09-13 09:33:37
@_author: Bear 
@_subject: [Cryptography] Encryption opinion 
We have to get used to the idea that surveillance and correlating   surveillance data with data found on the Internet is not difficult. Heck, someone who wants to can just point a camera and an EZpass scanner out the window from a building near a freeway, hook up their own OCR software, and start crosslinking EZpass numbers with license plates.  No data theft is required.
In addition to the option of stealing a database to connect license plates to individual names and addresses, they can also buy the database from any number of sources of varying legitimacy, or build their own. Even if they don't get access to "private" records for addresses, there are all those youtube videos and facebook pages that contain images of cars belonging to identified people, with the license numbers clearly visible.  You could simply build a bot that trawls what google image search returns when you enter "car." It will get a lot of photos of cars whose license plates it can read, often on
pages identifying the owners by name, or if not then at least appearing on the pages of a lot of people who you can find
names/addresses for.  It's not hard to cross-correlate that data with what your camera and your EZpass scanner observe out your window. Nothing prevents the spammers and the phishers and the fraudsters from doing the same thing our governments and search engines do, unless perhaps it be the simplicity with which they can save the time and effort by stealing the data.  In fact nothing save distrust even prevents them from sharing the data they collect.

@_date: 2014-09-14 11:06:35
@_author: Bear 
@_subject: [Cryptography] RFC possible changes for Linux random device 
My 20 millibucks says it's important for cryptography that no sequence of observed outputs should allow anyone to predict future outputs.  I consider that if they can directly observe the state of the RNG or its power requirements or instruction counts, they have hardware access, which makes them at best an
uncommon threat relative to the ubiquitous problem of network That said, I find myself using the cryptographic RNG for non-
cryptographic purposes sometimes, specifically because the small state of the PRNGs provided by most systems means I cannot get a sequence of uncorrelated outputs more than a few dozen bits long out of them.  256 bits of state limits the uncorrelated sequence to just four int64's, which is nowhere near enough even for some of the simulation and statistics type applications. In my very strong opinion, a requirement for boot-time entropy can result only from bad design.  Systems that need boot time entropy can need it only because they are doing things at boot time which should not be done at boot time, and failure to correct this OS design failure is actively harmful to security. The issue is that if you are building a dependence on secure
communications into the boot sequence, that means that you are depending on external systems and their state of security.  If you build a need for external systems that deeply into the OS, you are exposing the booting machine to compromises and failures of both those external systems and the communications network. Fully boot a purely local operating system before you attempt to open any external communications, and open those external communications in process spaces where the kernel can monitor
and/or sandbox them.  In short, make sure your local operating system is fully instantiated and can protect itself from anything that can come over external communications. And for this reason my own personal opinion is that providing the people who promote this network-before-fully-booted failure of design with the early-boot entropy they desire contributes to a problem, in that it allows them to skate along for one or two or ten more years without correcting that fundamental failure, or even continuing in blithe denial of the idea that it needs to be corrected.
On the other hand failing to provide the early-boot entropy they
desire, in any context where they cannot be dissuaded from that failure of design, will result in yet worse breakage. So this really is kind of between a rock and a hard place. This seems like the sort of thing to test in very early boot.  Wake up, check a variable ARM-COUNTER from the disk, find it equal to zero, write one to it, flush the disk buffer, attempt to continue using the the counter.  If you don't crash, write a zero back to ARM-COUNTER and remember (in some other bit) that the counter is valid.  If you do crash, you wake up again in a forced reboot, but this time you find ARM-COUNTER equal to one - so you write a zero to it, remember (in your other bit) that the counter is invalid, and never ever use the counter.  The result is that boxes where the instruction counter is unimplemented have a slightly longer boot sequence, containing one early crash-and-reboot.  It's ugly as sin, but I used to do the same thing boot-testing for math coprocessors on 8088 boxes.

@_date: 2014-09-15 14:18:10
@_author: Bear 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
Wait.  If I have both members of an RSA key pair, can't I do Euclidean reduction (which is only O(logN)) to find the factors
of the key modulus?
And isn't that why RSA systems where a common modulus is shared
among more than one keypair are considered insecure?  In short, if we can use a solution to RSA to perform factoring, then isn't that a proof that factoring is no harder than RSA?

@_date: 2014-09-22 16:17:03
@_author: Bear 
@_subject: [Cryptography] Of writing down passwords 
Still, we're talking about the owner of a device accessing it to administer it.  The owner of the device is distinguished from most network attackers
by having physical access to it. We even lampshade this by having a little hole you can poke a wire into to reset it to the default password
in most cases, so it's clear we're not securing the devices against anyone who has physical access.  So if physical access allows admin privileges anyway, why don't we build these devices with a physical toggle switch?  Turn it ON, and with the password, you can get an admin interface.  Turn it OFF, and it won't respond with to any input with an admin interface no matter what.  Turn it ON and poke the wire into the hole, and you can reset it to the default password.  And you can see just by looking at the device which position the switch is in.
This is simple, easy to do, easy to check, costs no more than the toggle switch itself, and would stop 99.9% of all cases of unintended access to the admin interface cold, especially in the poorly-secured "home area network" markets that are the biggest security problem on the network today.

@_date: 2014-09-27 11:07:42
@_author: Bear 
@_subject: [Cryptography] The Trouble with Certificate Transparency 
But this fails because there is absolutely no way to prevent Alice from knowing that her key has not been accepted into the blockchain, nor from knowing that some other key is now associated with the name "alice".  Once Mallory has published his "alice" key, he has absolutely no way to get Alice to use She will never publish the name "alice" as a correspondence address if the key associated with it by the blockchain is controlled by someone else.  And if she's the one Bob wants to
communicate with, Bob will be using the name she gave him.

@_date: 2014-09-27 12:13:57
@_author: Bear 
@_subject: [Cryptography] The Trouble with Certificate Transparency 
I am no longer convinced that this use case is worth the security costs.  DNS records and CA certificates which can be altered by people who do not own the current ones, cause DNS cache poisoning attacks we've known about for a long time, but more recently we've been
discovering that these pieces of infrastructure are opening EVERYONE up for massive bulk MITM monitoring by anybody who can obtain compliance from ISP's, hardware manufacturers, or backbone sites. The NSA was only an example case; the real-world attacks are bound
to be as widespread as the vulnerabilities. Which, by the time you go down the list of actors, includes not just
lawful authorities who settle legitimate business claims in free nations, but also the large ISP's, hardware manufacturers, and operators of backbone sites themselves, many of whom have commercial
interests in monitoring traffic.  And it also includes every criminal
organization who is able to infiltrate them or, with normal levels of
corruption that vary worldwide, pay them an adequate bribe.  And
finally, it also includes pretty much every police and spy agency of every government of nations where hardware is manufactured and every petty dictatorship that traffic passes through.  Seriously, think about that "made in [[COUNTRY]]" sticker on most of the hardware in your home, and ask yourself what's the general level of corruption in those countries for purposes of bribing or infiltrating the manufacturers, what the government powers are for coercing those manufacturers, and how much we really trust all the
people whose fingers could be inside that box.
All told it's just not worth it.  It would be cheaper for businesses to just make the one-time payments to domain squatters to buy the
names.  Legitimizing domain-squatting for revenue is far less costly to business in the long run than the follow-on effects of the
infrastructure required to de-legitimize it.
Blockchains help to achieve consensus; but they do not distinguish defense from attack.  There may be a blockchain-based protocol that meets the requirements here.  If, for example, like Bitcoin somebody is willing to pay
65 thousand dollars per hour (at current prices) for security.  But be very careful about saying a blockchain solves the problem if you don't have a very clear idea about what motivates people to secure the chain and why that's more valuable than what motivates
other people to attack it.  Otherwise you have to develop an asymmetric advantage for the defenders over the attackers, which leads you into the problem of distinguishing defense from attack, which leads right back into the problem we have now of who can be bribed, coerced, infiltrated, or subverted.

@_date: 2014-09-27 22:21:27
@_author: Bear 
@_subject: [Cryptography] The Trouble with Certificate Transparency 
That is really hard to do.  A blockchain is public; it should be the same for everybody you
communicate with, and everybody *ELSE* communicating using the same structure.  Its caching structure and request structure reflect
this; the people who have it will cache, forward, and share it promiscuously. And to prevent your MITM target from seeing it, you have to shut all of them up.
He doesn't even need to identify yourself in order to get the current
top node of the blockchain, from any peer, at any time.  He can close
his connection, walk down to the coffeeshop, and reopen the connection.
You either abandon the MITM on him and then he knows instantly that he was on a bad fork, or somehow expand it in midstream to include all the customers at the coffee shop who are on the same wi-fi server, because the server will be serving all their requests for the same data out of the same cache. If he knows *anyone* using the protocol, and can't sync up with them on the same version of the blockchain, he knows there's something wrong.  So the attacker can't MITM one target; he has to block everyone that target knows as well.  And this isn't even a stable set; The guy you're attacking walks into a Wal-Mart, his cell phone connects with their free wifi, and if anybody else in the store is using the same blockchain, you lose, because he's going to see the version cached on the server you haven't been MITMing up to that point.  Or, if he doesn't because you can pwn Wal-mart's server so fast, on demand, that he never sees the legit blockchain, then a bunch of Wal-Mart customers you haven't been MITMing up to that point have to be transitioned from the
non-attack version of the blockchain to the attack version, which they're all going to notice. Because there's no need for anyone to identify themselves to get the
correct current version of the blockchain, I think it would be really
hard for an MITM to single out a target or small subset to show a
different version of the blockchain to -- or, especially, to prevent a particular target or small subset from seeing any other version of the blockchain anywhere.  So, yes, in some artificial universe, for an hour or two at a time, someone might be able to MITM a blockchain protocol.  But on an ongoing
basis?  You wind up making a continuing effort, that continues to be exponentially more costly and difficult as time goes on, requiring you
to MITM (and maintain the attack on ALL OF) an exponentially increasing
number of additional targets your real person of interest comes into contact with (and which THEY come into contact with) that you're not
even interested in, culminating in a requirement that you escalate it to a full 51% attack or else it will rapidly collapse.  In the
meantime, you are operating in the certain knowledge that *UNLESS* you achieve that 51% attack, one minute after you stop -- or fail --
your MITM target/s will know that they've been on a bad fork the whole time.  You might manage it for a while if your target sits at a desk and never moves, and nobody else using that blockchain drifts into or out of his network with a mobile or a laptop on wifi.  But it's hard to imagine carrying it on for any length of time.

@_date: 2015-04-03 12:10:11
@_author: Ray Dillinger 
@_subject: [Cryptography] Cipher death notes 
Right.  Before there's an actual attack that someone can really
perform, there is still time to consider options and reach
consensus.  That dam has not yet broken, and there are not
yet millions of tons of water sweeping down on the homes of
the innocent.
If the note can appear, then the dam has utterly and completely
broken.  The disaster has happened, and the foundations of
the homes of the innocents are already underwater.  And the
water's rising.
The note is not the broken dam, nor is it the flood.  The note
is not the disaster, the note is somebody who is both sounding
the flood warning sirens and trying to pile sandbags into the
breach.   At that point you are doing emergency response.
There is no more time to consider or plan or anything else. At
that point grit your teeth and pitch in with all the other
first responders and good samaritans to do everything you can
for the imperiled and the survivors.

@_date: 2015-04-03 12:24:01
@_author: Ray Dillinger 
@_subject: [Cryptography] Fwd:  OPENSSL FREAK 
Those are not yet full-scale emergencies.  You have time to plan
and implement a more reasonable and measured response in those
cases, so use of something as drastic as the Death Note would
not be inappropriate in such cases.  To go back to my dam metaphor
that's spotting weaknesses in the levee before it breaks, while
there's still time to make repairs and/or evacuate the area.
The Death Note is disaster management for full-scale emergencies
where time to plan and implement a more reasonable and measured
response has already, clearly, *provably* run out. You can't
deploy it without doing some damage, but the damage you'd do
by NOT deploying it is worse. Death Notes cannot and should
not appear until the cipher or whatever primitive has broken
so very badly that damage is unavoidable.

@_date: 2015-04-04 11:53:09
@_author: Ray Dillinger 
@_subject: [Cryptography] Fwd:  OPENSSL FREAK 
Every variety of "export mode" encryption could and
should have received a Death Note long before now.
They had been deprecated but, despite being a clear
and present danger, were still in use. A timely
Death Note could have stopped them.
Some whistleblower like Snowden or Manning inside
the NSA who actually knows the magic numbers behind
the Dual-EC DRBG and feels that it is a crime against
society (and would be right to believe so) ought to
have been able to publish a Death Note against that.
If they could do it without putting their life,
freedom, and families on the line, they probably
would have.
Mobile phone and wi-fi encryption that somebody
can break with a laptop in seconds?  That's not
an implementation flaw, that is a dead cipher.
Send it a Death Note and bury it.
These primitives are completely, unambiguously,
provably, *BROKEN!* Getting them the hell out
of public infrastructure is an obligation.  It's
basic disaster response like clearing away the
wreckage of a collapsed bridge.

@_date: 2015-04-04 11:57:17
@_author: Ray Dillinger 
@_subject: [Cryptography] how to put a password in an evidence bag? 
What I can't understand is how they got the admin password from
Ulbricht's computer, and *THEN* were able to use a text log file
(easily faked, authorship non-provable) from that computer as
evidence against him.  Nobody even knows that they didn't
*write* that log.

@_date: 2015-04-04 12:56:52
@_author: Ray Dillinger 
@_subject: [Cryptography] Fwd:  OPENSSL FREAK 
In the case under discussion, checking a cert from a
known-bogus Certificate Authority would be the simplest
implementation.  Public keys known, private keys destroyed.
If the cert checks, the cipher is broken.
Of course the drawback is that you have to trust at least
a little bit that the destruction asserted actually

@_date: 2015-04-04 19:12:54
@_author: Ray Dillinger 
@_subject: [Cryptography] Fwd:  OPENSSL FREAK 
Backward compatibility is just a name for downgrade attacks that
haven't cost enough money to stop yet.

@_date: 2015-04-06 15:08:02
@_author: Ray Dillinger 
@_subject: [Cryptography] Fwd:  OPENSSL FREAK 
Yes, the Internet of Things as it's being called is scary as hell
because you know that software will not be updated in any organized
way - especially when the company that sold the "thing" is out of
business or loses a patent lawsuit or something.  Or somebody
who makes a business model of "give away the thing, charge for the
updates" is going to have a bunch of customers are perfectly happy
with the way their toaster or thermostat or doorbell or whatever
works now and don't want to pay for a software update for it,
or whatever.
In the absence of timely, reliable upgrades, there really does
need to be some kind of "kill switch" to shut down discovered
vulnerable configuration options, or those "things" will become
the gateway for crooks to get into the rest of the owner's

@_date: 2015-04-12 12:56:11
@_author: Ray Dillinger 
@_subject: [Cryptography] upgrade mechanisms and policies 
I have a long history of failing at this, but the experience has
taught me to notice when something *isn't* a failure...  and I
have to agree that this is a nice neologism.
And that's IMPORTANT.  If you give something an official-sounding
name, people take it more seriously.  "Outroduction" sounds like
a positive action that is an agenda item in itself, whereas
"end-of-life" or "phase out" sound like just stepping back and
not paying attention to something until it dies.
And the whole point is that we DO have to get people to pay
attention.  Especially the kind of people who like big important-
sounding words that seem like they are positive agenda items
that must be planned and executed.

@_date: 2015-04-14 12:58:45
@_author: Ray Dillinger 
@_subject: [Cryptography] Fun and games with international transaction 
currencies)
How does the 34% attack work, exactly?
The problem is figuring out what known set of entities in a way
that diverse people can trust and are satisfied with.  So far
every attempt at digital cash which relied on a central authority
or central registrar or verifier or someone running a server at a
particular network location etc, has failed - some due to lack of
trust in that entity, some due to lack of trustworthiness or
reliability by that entity, and  some for other reasons.
In an era where an actor responding to a lawful order under
threat of imprisonment is indistinguishable from an actor
performing a breach of trust, there is no authority or group
whom the people interested in cryptocurrencies trust enough
to allow that authority, or that set of authorities, a
privileged or pivotal role with respect to securing their
The fact that demands for private information, which demands
may or may not be legal in the first place, now come with gag
orders to prevent anyone from talking about getting such a
demand on threat of prosecution, undermines everyone's ability
to trust *ANYBODY* *EVER*.

@_date: 2015-04-17 11:55:38
@_author: Ray Dillinger 
@_subject: [Cryptography] upgrade mechanisms and policies 
The crypto has been solid, except where deliberately sabotaged.
The protocols have been good.
The implementations have been problematic.
And the key management bites rocks.
Everybody says key management is too hard and complex, and then
because they think *that* is the problem, they simplify it until
it is neither hard, nor complex, nor useful.
Our so-called "authentication" protocols don't, by default, even
tell us whether there is any cryptographic evidence whatsoever
that someone is the same person we dealt with last time; only
whether some member of a huge world-wide, distributed set of
"Cert Authorities" whom we don't personally know, and who don't
even check, asserts that they are using the same name.
Cryptographic evidence of continuity is a relatively simple
thing to provide; but, oh horrors, somebody would have to manage
keys.... so we can't have that.
And to a first approximation, we don't even have any effective
privacy protocols that don't depend on the utterly helplessly
broken auth mechanism.

@_date: 2015-04-17 12:40:28
@_author: Ray Dillinger 
@_subject: [Cryptography] upgrade mechanisms and policies 
A private chat application that avoids this problem is simple,
really; use Rivest's Interlock Protocol while negotiating keys
with Diffie-Hellman key agreement protocol.  If Mallory
attempts to MITM by doing Diffie-Hellman with Bob on one side
and Alice on the other, Interlock then prevents him from passing
through any messages from Alice to Bob nor from Bob to Alice.
If he can't do that, then it's a simple matter for Bob and
Alice to authenticate each other with a challenge/response
that Mallory will utterly fail at.
Of course, this all seems more complicated than it needs
to be from an engineering perspective; if Bob and Alice have
a shared secret to auth each other, they could just
communicate using symmetric encryption with a shared secret
key.  But that simpler solution ignores the fact that Bob
and Alice are human.  If Bob and Alice were just machines,
that would be fine, but if they're human they need something
Bob and Alice aren't used to thinking of "shared secrets"
as boring high-entropy bit strings.  Bob is perhaps looking
for the same person who knows what wine they had with dinner
on their last date, and Alice is looking for the same person
who knows why her father doesn't get along with her cousin.
If you ask them to "authenticate" using bit strings as a
shared secret key, their eyes will glaze over.  But if you
ask them to "make sure it's the right person by asking
something only they would know" that's something they'll
understand and can do.
My point is that combining Interlock to prevent replays over
a DH-secured private channel, the "human-level" secrets
that they care about and are interested in (and will remember)
can be leveraged to provide the kind of cryptographic
authentication that they care about in that channel.
When doing transactions the way a human would do a transaction
with a *stranger* (at a checkout counter or whatever) use auth
methods (high-entropy bit strings) that machines are good at,
because as far as human perception is concerned one stranger is
the same as another. You fall back on what machines can do
well because humans are no help in that case.
But if people are doing personal interaction of any kind (such
as with a private chat channel) then they can and should use
auth methods that *people* are good at.  A DH/Interlock
private channel will allow them to authenticate using
personal secrets rather than hackable secrets.

@_date: 2015-04-22 14:10:35
@_author: Ray Dillinger 
@_subject: [Cryptography] Entropy is forever ... 
The use of "entropy" to denote unpredictability in CSPRNG's
and other cryptographic numbers has always been problematic.
It's a physics concept that isn't really all that well mapped
to what we're using the word for.
There is a comment in some code I'm working on that seems somehow
relevant:  It says,
MStart[0]=169;  // determined by dice roll, guaranteed random every time!
MStart[1]=74;  // "magic numbers" to distinguish our packets from
everybody else's
As the above comment points out, randomness isn't properly
a measurable or even meaningful quality.  The MStart bytes
are transmitted at the beginning of every message, and
they are the same in every message.  We do not think of
such things as "random" and yet, if the comment is accurate,
they were selected by a method well understood to give
"random" results.
Entropy in physics follows a different model from the
rather peculiar "physics" of how it is conserved and
distributed in cryptographic systems.
If you have a CSPRNG seeded with a 256-bit state unknown to
and unpredictable by an opponent, and an absolutely perfect
algorithm, and your opponent has infinite computing power
and an absolutely perfect algorithm, your oppponent can look
at 2^255 bits of output from the CSPRNG and derive sufficient
constraints on its state that he can eliminate half the
possibilities for your 256-bit state - meaning you now have
255 bits of unknown state as far as that opponent is concerned.
In practice we don't achieve that level of "unpredictability
physics" in real systems.  We have no guarantees that our
algorithms are good let alone perfect, and our opponents
certainly do not have infinite computing power.  And we
don't model anything like that kind of unpredictability
physics when we're tracking so-called "entropy" in
cryptographic systems.  More usually we use linear rather
than exponential models just because they're easier to
keep updated "accurately" and it's a conservative estimate
on the level of unpredictability both in our state and in
our algorithms.
Nevertheless, I trust a CSPRNG with a "good" algorithm
and 16Kbytes of state unknown to and unpredictable by
an attacker, to remain absolutely secure for any practical
length sequence of bits without further input.  Most
CSPRNG's are, IMO, misled by the linear entropy accounting
model they use: they often wind up with not enough state,
and having got enough state (or having been configured to
use enough) they do not trust it to remain secure.
In part what the designers distrust is the ability of the
system to keep the state unknown to an attacker, which is
probably a risk more linear-in-time than exponential-in-
state-size. In part they distrust their ability to get an
initial state unknown to and unpredictable by the attacker,
which can be mitigated by mixing in unpredictable bits in
big chunks.  But the linear-in-bits "entropy" model they
use is not a good model for either of these risks.
Anyway the conservation laws are very different for
cryptographic entropy than they are for physics entropy,
so probably using a different word such as "surprisal"
would be less misleading.

@_date: 2015-04-28 00:15:56
@_author: Ray Dillinger 
@_subject: [Cryptography] The Friedman Papers 
I have downloaded and zipped the recently declassified files
regarding William Friedman.
Then I made a torrent out of them and set up my machine to seed
the torrent.  Here is a magnet link.
Please download, disseminate, host, post to trackers, etc, as
you see fit.  Public information is public, and so on.
I made an account on thepiratebay.se and uploaded the .torrent
file, but as yet it does not show on a search of the site.  I
worry that I may have done something wrong?  Or maybe it's just
slow to update.

@_date: 2015-04-28 12:42:59
@_author: Ray Dillinger 
@_subject: [Cryptography] The Friedman Papers 
I wasn't able to get thepiratebay's tracker to pay attention,
apparently, so I just found a few at random that I could get
it onto.  I've never heard of any of these, but:
and the magnet link is now:

@_date: 2015-08-04 11:14:36
@_author: Ray Dillinger 
@_subject: [Cryptography] SRP for mutual authentication - as an 
I consider SSL session reuse to be a vulnerability.  It gives
an attacker additional time to break the SSL key before cutting
in with a "reuse".
We have already seen downgrade attacks that put SSL keys within
reach given an amount of compute power that can be achieved by
a modest cluster in a matter of a few minutes.  Session reuse
can give an attacker literally hours to break an SSL key.

@_date: 2015-08-06 09:46:00
@_author: Ray Dillinger 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
I'm guessing only around three significant digits.  A lot of
programmers are actually both conscientious and smart.  More
than one out of a thousand will actually do the research and
the homework and do a good job of protocol design.
Cipher or RNG design I'm less sure about.  It seems that people
invent three of these a week and I've essentially never heard of
anybody other than a math/crypto pro or a university researcher
inventing ANYTHING in that line that there is a good reason to
use.  Rarely, one of the better ones is secure AFAICT, but none
so far are both secure AFAICT and have any other advantage over
existing and well-examined secure algorithms.
This is true.  But it is true any time software is designed by
If one is trying to write one's very first crypto application,
I think designing with the specific goal of compliance with
any standards over about 10-15 pages long is likely to cause
enormous numbers of bugs.
The committees do not tend to produce anything simple enough to
fully understand all at once.  The usually-good programmer habit
of breaking things into subproblems and thinking about them
separately does not work well in this case.  In fact it is, in
crypto software, possibly the primary origin of most serious
failures. MANY attacks are perpetrated by using "distant" parts
of the system together whose interaction the programmer (or
sometimes the committee whose standard the programmer was
slavishly following) never considered. Fewer moving parts would
be fewer opportunities for that to happen.
So I'm with David here.  Standards are good design input but
mainly in terms of reminding you of the attacks that the standards
are designed to defend against.  If you read them merely as
instructions on how to build something secure, you will fail.
Design it carefully.  Then build it.  Be sure it's working as
designed.  Then if and ONLY if you can do it without compromising
the design, see if you can actually comply with those standards
without breaking its security.
If you can't, then either the standard does not describe what
you intend to do and need your software to do, or it was a bad
standard and you shouldn't be following it anyway.  In fact
in the case of a bad standard, following it slavishly to be
interoperable with all the other things that follow it, is
simply adding your application to a pile of toxic waste that
will eventually have to be cleared away for the sake of public
safety. (X.509 CA process, I'm looking at you....)
Indeed.  And that includes conscientiously not implementing any
which you know to be bad, insecure, inappropriate for your
specific application, or have too many details to hold in your
head all at once.  Sometimes standards documents are inapplicable,
dangerous, or just plain wrong.

@_date: 2015-08-07 18:41:25
@_author: Ray Dillinger 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
It seems counterproductive to me to specify a "hash" function that
can produce output longer than its security provides collision
resistance for.  People are going to make this mistake - and get
less collision resistance than they're designing for - because
the muddled use case and unfortunate terminology of calling this
a "hash" invite this mistake.
When people want a hash of some level of collision resistance
MANY of them are going to think they should be looking for a
hash function that produces a hash of some bit length.  There
shouldn't be a "hash function" they can select which gives them
that bit length without giving them that level of collision
resistance.  It invites avoidable design errors.
Keep the primitives simple so everybody knows exactly what they
do and more importantly what they don't do.  If you want a PRNG
initialized from a hash on some document, you take the hash and
use it to initialize a PRNG.

@_date: 2015-08-13 19:57:32
@_author: Ray Dillinger 
@_subject: [Cryptography] Why is ECC secure? 
One thing to point out is that RSA is at least as hard as factoring
because if you can solve RSA, you can use the solution to factor its
modulus.  IOW, there can be no shortcut that makes it easier than
But that doesn't rule out the possibility that factoring may still be
easier than the best way we know how to do it now.

@_date: 2015-08-13 20:06:36
@_author: Ray Dillinger 
@_subject: [Cryptography] Medieval Sword contains Cryptic Code. British 
18 characters isn't a lot to go on. Even for a monoalphabetic
substitution cipher, that's likely below the unicity distance.
We'll probably never know what the inscription says, unless we
find it as part of a longer text somewhere else.
I really have to admire the metallurgy though; the thing was
sunk in a river for seven centuries, is still sharp, still has
a very legible (if mysterious) inscription, and didn't even
significantly rust.
Seriously.  That's better than most of the stainless steel we
make today.

@_date: 2015-08-17 07:59:26
@_author: Ray Dillinger 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
I don't believe it's ridiculous.  I mean, yes, large, but still under
2k.  We already had keys of such a length that nobody was going to
enter them by hand, and 2k is near-epsilon with regard to today's
It probably lets the bottom tier devices have a decent excuse not to
implement it, but other than that it's fine.

@_date: 2015-08-17 08:28:45
@_author: Ray Dillinger 
@_subject: [Cryptography] Why is ECC secure? 
Actually we specifically don't know that about P256.
 raises a concern that the P256
curve may be manipulatable by an attacker. There is a large unexplained
input to the curve parameters, and it comes from NIST which has been
subverted by attackers before.
 raises a concern that the P256
curve has properties that make standard Weierstrass addition formulas
not work on this curve (fails doublings) and that there are identity
points (positive vs. negative values of identical absolute value) for
some parameters that produce the same results, which increases the
attack surface. (not by much if I'm reading it right, but it may be
one of the contributing factors to the next concern).
 raises a concern that the normal
method of making elliptic-curve strings indistinguishable from random
is not defined on NIST P256.  Distinguishability today leaves open an
increased probability of an attack tomorrow.
I highly recommend poking around safecurves.cr.yp.to a lot if you're
selecting elliptic curves for widespread use.  I think it's the most
comprehensive collection of specific information about the security
and efficiency properties of particular elliptic curves available.
They may (or may not) be overly concerned with relatively minor issues
that can be mitigated by careful coding.  But these "relatively minor"
issues do sometimes lead to later attacks. And even if the "lead" or
"killer" application driving adoption of the curve is carefully and
correctly coded, hundreds of people out there will be programming
things that are intended for compatibility with it, and some of them
won't be as careful or as capable.

@_date: 2015-08-18 13:54:52
@_author: Ray Dillinger 
@_subject: [Cryptography] Speculation about Baton Block Cipher 
Heh.  Is this a trick question?
The checksum size is of course ludicrous with respect to the key
and block size.
They don't need more than 4 bytes for a checksum, if that.  BATON
is implemented in hardware with a secret algorithm, so virtually
anything could be encoded in the remaining 16 bytes and nobody
would be the wiser.
The fact, however, doesn't lead me to any specific speculations,
except that it's probably some kind of deliberate side channel.
But it's not at all clear what such a side channel would be
useful for.  It's a Type 1 product.  Why do you suppose the
NSA would install a side channel on their own communications?

@_date: 2015-08-27 18:55:45
@_author: Ray Dillinger 
@_subject: [Cryptography] Augmented Reality Encrypted Displays 
No matter how hard you squeeze the snakes, you just can't get
enough oil to make it worth the effort.

@_date: 2015-08-27 19:45:48
@_author: Ray Dillinger 
@_subject: [Cryptography] 3DES security? 
3DES (with 168-bit keys) is considered to be 112 bit secure.
Per Wikipedia, the best known attack requires 2^32 known-
plaintext messages, 2^113 steps, 2^90 single-DES encryptions,
and 2^88 memory.  That's well out of reach, I think.
AES is very much more secure in raw numbers, but in practical
terms both AES and 3DES are in the "The Sun Won't Last That
Long" category, so it doesn't matter very much.  It can be
trusted to the extent that you trust your implementation of
it, your protocols using it, your key handling, your users,
etc.  The weak point in any system using either of the two
will not be the cipher.
The advantage of AES is bigger blocks and lower resource usage.

@_date: 2015-08-27 19:59:46
@_author: Ray Dillinger 
@_subject: [Cryptography] AES Broken? 
Is this guy blowing smoke, or did he find a real attack on AES?
The fact that he proposes a self-invented cipher in the same paper
makes me very suspicious that his attack on AES is imaginary.  It
matches too much past 'crackpot' behavior.

@_date: 2015-08-28 12:36:10
@_author: Ray Dillinger 
@_subject: [Cryptography] Augmented Reality Encrypted Displays 
My issue was that the proposed system requires both halves of the
secret to be present, in the same hardware, at the same time.
This nullifies the value of separating them at all.
It is no more secure than any other cryptosystem where the data
are present in toto, and will be attacked by the same methods.

@_date: 2015-12-04 14:52:40
@_author: Ray Dillinger 
@_subject: [Cryptography] JSC notifies on introduction of National 
As I read it, the objective is to monitor the use of *foreign*
Internet services by Kazakh citizens.  The drafters of the law
may presume that *domestic* services don't present a foreign
threat to domestic security and are therefore outside some
legal basis that would justify the passage of a law.
Which, honestly, isn't that unreasonable a distinction, although
this type of mandatory-monitoring tool will not help and instead
is likely to put them front and center in a long series of disasters
until they develop a better plan.

@_date: 2015-12-04 15:26:37
@_author: Ray Dillinger 
@_subject: [Cryptography] Anyone else seen some odd shipping delays? 
Oooh, interesting!
If you approach any of a number of researchers, I'm willing to bet
they'd offer to swap you a computer from some store near you which
someone buys that very afternoon, for your presumably-trojanned box,
straight across.
It's win-win; you get something clean (assuming something clean is
what's at the local store which didn't know you were going to be the
one using it) and some grad student with a masters thesis to write
and lots of electronic hardware gets to crack into the latest
What's your specific region?  Someone may be able to recommend a
few specific people to contact who are near enough to come do a
swap in person.
That said, the biggest targets in modern computer hardware are
the disk controller (integrated into the hard drive), the USB
controller (on the motherboard), the nonvolatile BIOS (on the
mobo but usually with an update channel that depends on the USB
controller), the network controller (usually on-board these days
with its own nonvolatile BIOS), and DSP controllers (possibly
on the mobo but these days usually on upgradeable video cards).
Nothing that can be fixed by wiping the OS and reinstalling is
still a target.
Of these the BIOS or disk controller are usually preferred if
the goal is to ensure persistent access or access to encrypted
files, the network card is preferred if the goal is access to
communications, and the USB controller is preferred if the goal
is to gain access to additional machines.  The video controller
can extend any of of these capabilities with communications
to nearby "airgapped" machines if those machines are also
compromised, or make radio-frequency signals that are much
easier to monitor than the usual TEMPEST style remote screen-
reading tricks.
I haven't yet heard of any microcode (in-CPU) compromises yet,
but that would seem like the obvious next place to go if you
want access to encrypted material in general.

@_date: 2015-12-04 15:37:04
@_author: Ray Dillinger 
@_subject: [Cryptography] Anyone else seen some odd shipping delays? 
Now *there* is a wrinkle I hadn't noticed yet.
I'm aware that every package which goes through the mail
(and several commercial shippers as well) is photographed
and the photos are archived "forever" digitally.  I had been
thinking of it in terms of "metadata" tracking - address and
return address, etc.
But if the MAC addresses are barcoded on the outside of
shipping boxes, those archived photos enable a whole different
degree of later traffic tracking, don't they?
Anybody received a computer or network hardware lately and
still have the packaging?  Any barcodes that don't normally
appear on other packages?

@_date: 2015-12-09 13:40:54
@_author: Ray Dillinger 
@_subject: [Cryptography] Who needs NSA implants? 
FWIW, the hard drive trojan can be remotely installed if given
admin privileges on a Linux box.  I presume the same is true of
Windows, given that the drivers use the same underlying mechanisms
to dispatch requests.

@_date: 2015-12-10 18:53:21
@_author: Ray Dillinger 
@_subject: [Cryptography] Satoshi's PGP key. 
Ding, we have a winner.  The way Satoshi survives is if nobody
EVER knows s/he was Satoshi.  If s/he ever gets unmasked, s/he's
going to be kidnapped, or subject to extortion, or in jail in
"protective custody" within hours.  And if s/he doesn't produce
keys I believe s/he probably no longer has, s/he or his/her
family are going to die.
I corresponded with Satoshi a bit, and just on impressions, I
expect that s/he will never spend those coins.  Wasn't in it to
get rich.  I think s/he was in it because s/he was angry at "the
system" for being rigged and corrupt and didn't personally give
a crap about the wealth.
Add to that the hunt for him that kidnappers and extortionists
would instantly mount if any of those coins ever got spent, and
the systems that read the blockchain looking to de-anonymize
transactions, and the other systems that listen to the network
to see exactly where every transaction originates - and the
fact is s/he'd be risking death if s/he ever spent a single one
of those known coins.  S/He could do everything perfectly, via
Tor or whatever, but if the person who got paid the coins s/he
spent wasn't equally careful, Satoshi would probably get killed.

@_date: 2015-12-16 14:22:19
@_author: Ray Dillinger 
@_subject: [Cryptography] Satoshi's PGP key. 
Most rich people don't have wealth that can all be instantly stolen
and hidden and carried in a tiny USB drive in someone's pocket.
They have assets distributed over a wide geographical area, most
of which aren't small, light, fungible or even mobile, and as
such aren't very high on the list of things easy to steal.
Their fungible assets are in institutions like banks or
exchanges, where there is "normal" security on guard to prevent
it from being stolen.  It's not a particularly easy target.

@_date: 2015-12-18 18:15:09
@_author: Ray Dillinger 
@_subject: [Cryptography] Satoshi's PGP key. 
At the moment, yes.  I made some money on them, but I sold out.
Satoshi has enough that s/he has a huge target painted on his/her
butt, but even if you don't own enough for that to be an issue,
it's still a bad idea.
According to my math the protocol promotes mining consolidation,
and the winning tactic for a consolidated miner amounts to a
partial DDoS.
Once anyone has more than 1 - 1/Phi of the mining power, and
there is enough *actual* demand to fill more than half of each
block on average, then even if everybody else behaves optimally
(which they don't) that miner gets rich faster than all other
miners (at a higher rate of return on expense) by keeping the
blocks, on average, half full of transactions in order to force
up the fees other people pay.  It will cost the consolidating
miner fees, of course, to prevent other miners from accepting
tx below that fee level - but he'll get whatever share of them
back that he's getting of the blocks, and he (and all other
miners too) makes more money on all the other tx.
In practice the tactic (durst call it an attack?) works even
better than that, for two reasons: first because doing it even
occasionally makes people reset their fees to "enough", and
then those settings stay high even when the miner isn't
inflating block chain traffic.  Second because the consolidating
miner has a cost advantage in being set-up where expenses are
lower than average competition.  Both factors dramatically
reduce the amount of mining power required to reach the
inflection point for consolidation and eventual monopoly.
In China bitcoin mining is essentially a way to launder stolen
tax money, (subsidized electricity is profligately used,
courtesy of the taxpayers, and then re-emerges as bitcoins
which the profligate users get to spend) so this stronger
centralization dynamic is definitely in effect.
Anyway, with the vast majority of bitcoin mining already being
done in China, and the entity whom I suspect is or soon will be
the consolidated miner present in that country masquerading as
multiple pools in order to prevent a panic, I think the Chinese
government is already in a position to allow or disallow pretty
much any transaction they care to.  When the masqerade fails,
or when China exercises the coercion option, I expect bitcoin
values to drop precipitously.
Until then, it has become a sham.  What Satoshi and Hal worked
so hard to bring about, has already failed, and it makes me sad.

@_date: 2015-12-18 19:02:07
@_author: Ray Dillinger 
@_subject: [Cryptography] What should I put in notifications to NSA? 
With legal codes, as distinct from computer code, these definitions
are made up *after* the fact, and are likely to include anything
that comes up in court as the subject of a search or subpeona.  The
courts will eventually decide whether whatever means were used
constitute the lawful exercise of a freedom from unreasonable search
and seizure, or an unlawful exercise of
1) evidence spoilation
2) accessory (before or after the fact)
3) obstruction of justice
4) harboring a fugitive
5) failure to comply with subpeona
6) contempt of court
7) conspiracy to commit a crime
8) whatever else the prosecution decides to call it.
So, bluntly speaking, there is no "definition" precise enough
to satisfy you, and likely never will be.  They literally decide
it case by case, and if it's fifth-amendment the developer of the
unregistered/unreported software is not likely to be prosecuted,
and if it isn't then there may (or may not!) be a problem.
I doubt that rsync will ever prevent prosecution (or defense) from
getting its hands on evidence, nor facilitate the commission of a
crime more than, say, a standard non-crypto utility like ftp would.
So the odds of it coming up in court or its makers being prosecuted
for failure to notify are essentially nil.  But nobody can rule it
out completely.

@_date: 2015-12-19 16:08:15
@_author: Ray Dillinger 
@_subject: [Cryptography] Satoshi's PGP key. 
Yep.  If you get a key logger installed on somebody's
machine and listen through their wallet login, and
manage to get a copy of their wallet.dat, that's all
you need to steal their coins.

@_date: 2015-12-19 16:14:28
@_author: Ray Dillinger 
@_subject: [Cryptography] Satoshi's PGP key. 
rs too) makes more money on all the other tx.
Well, sure, but what would be the point?  If you do that, then
other miners are still likely to accept tx with fees below the
premium level you want to charge, so people who don't pay the
premium will still have their tx go through and you won't
effectively drive up the fee price.
Heck, if it *doesn't* happen the partial-DOS works and the system
centralizes anyway.  Pick your poison.
This isn't political pressure, this is math.

@_date: 2015-12-20 01:55:10
@_author: Ray Dillinger 
@_subject: [Cryptography] Questions about crypto that lay people want to 
Okay, here's a few answers.  I skipped the one about physics and
quantum-based notions of "information" because I don't feel
like an authority on that.  Hope this helps.
Your basic citizens in earlier days needed privacy too, and
routinely used sealed envelopes to send their mail.  Crypto
is what a sealed envelope looks like when using digital
communications instead of paper.
Then as now, there is a lot of business you just plain can't
(or wouldn't want to) do without it.
It's happening again because the people who legitimately lost
last time around are cheating.  And it probably will be a
generational thing, until we fix it so they can't cheat.
Snowden, Manning, etc, let us know that although we unequivocally
won the right to privacy in the last crypto war, some agencies
have not been abiding by the people's decision.  Now those agencies
that want to continue the practices they've grown accustomed to
doing by cheating, have launched another crypto war trying to get
those practices declared legal.
Finally, partly because of things we haven't fixed yet and partly
because of things those same agencies have been actively subverting
efforts to fix, we have been losing billions of dollars to corporate
espionage, fraud, and theft, and many national diplomatic and
defense secrets via deliberate foreign attacks on our systems which
exploit the same vulnerabilities.
The short version is we're fighting another crypto war because
our nation is in danger again.
You need keys that nobody can guess, nor even narrow down
the search for.  Randomness is the very best (in fact only)
possible source of keys nobody can guess.
You can say it in more complicated ways and specialize it for
lots of individual cases and applications, but it all comes
down to the same thing.  One way or another, randomness in
crypto is for the basic requirement that you need keys nobody
can guess.
"Code" has several meanings.  In software engineering it usually
expresses procedure and occasionally mathematical relationships.
In cryptography it usually expresses mathematical relationships
and occasionally procedures.  The uses are related but not the
Also security code, even when it is programs, is very different
from ordinary software engineering code, because in software
engineering programs are defined only in terms of what they do.
Security code is also be defined in terms of what it must not
do and what information its execution must not reveal about what
it's doing.
The first difference is that one protects physical things secured
by a lock, and the other protects digital things secured by
The second difference is that your cryptographic keys also do double
duty for things that people have been doing for centuries with paper
documents but could usually only do in person.  They are the anti-
counterfeiting stuff in banknotes, they are the signatures on
contracts, they are the notary stamps on documents, they are the
locks on your roll-top desks that you used to use to lock down
your business at night, they are even the wax seals on old-timey
envelopes that let you know nobody else has opened a message since
it was sent.  And a bunch of other things besides.  We need those
things for digital documents as much as anybody ever needed them
for paper documents.
There are a lot of operations that are easily reversed when you
do them the normal way, but very very hard to reverse when you
do them on modular numbers.  A number of those operations, when
done on large numbers, form the basis of crypto operations -
when something is very very hard to reverse, people who don't
already know the answer can't solve the problem. And in this case
the problem is the code, and the solution is the key.
Your bank only accepts https because they have insurance that will
reimburse them if their certificate gets spoofed.  There isn't any
insurance that can give us back democracy, and the attackers are
willing to devote a lot more resources to stealing it than thieves
are willing to devote to stealing from banks.
To a large extent, it already has.  It's just that for most people the
digital currency is denoted in the same old units they've been using,
such as dollars (or francs, or pesos, or whatever).  Between direct
deposit, credit cards, bank transfers, and online bill payments, most
of our currency is digital, and secured by cryptography, already.
If you're asking about some special currency that isn't denominated
in one of the usual government-defined units?  Nobody knows yet.
Personally, I have my doubts that people will ever allow much of their
business to be done in completely irrevocable transactions that courts
can't recover when they get cheated or stolen from.  But that's not a
direct part of the short answer, and besides opinions vary - so "Nobody
knows yet."
"strong" crypto usually means you'd have to either use more energy
than the sun will ever produce, more time than the solar system is
expected to last, or have a fundamental mathematical breakthrough
to find a way to decrypt it without the key.  That said, people
trying to sell you something will use the word "strong" when they
aren't selling anything close to that.  Caveat emptor.
"Government quality" isn't specific enough to be meaningful.
The key size does not really mean much without knowing what system
it's for and what you're trying to protect against.
If we take seriously recent pronouncements about being safe from
quantum computing advances, then even the shortest, lowest-security
keys for the simplest systems should be at least 160 bits.
Public-key cryptography systems can require much more key material
than that to be safe, and most of the ones we have currently deployed
would fail even with very large keys if a large quantum computer is
available for the attack. Fortunately no such computer is available
at this time.
No key length will get you any security at all if you use a crypto
algorithm that has a mathematical flaw, or a key that someone can
guess, or a protocol that allows an attacker to bypass or extract
the key.
And no increase in key length will improve your security if there
are already any easier ways to break the security than an attack
on the encryption.

@_date: 2015-12-21 12:53:51
@_author: Ray Dillinger 
@_subject: [Cryptography] Questions about crypto that lay people want to 
That is an excellent point - and more to the point than the one I
made.  The requirements *are* different, in authentication and
privacy and in which endpoints are trusted.

@_date: 2015-12-22 14:58:14
@_author: Ray Dillinger 
@_subject: [Cryptography] Juniper & Dual_EC_DRBG 
Best explanations I've seen so far:
We got several things going on with Juniper firewalls apparently.
I'll list the ones I know about now.
First, they used Dual_EC_DRBG.  They knew it was broken
and they used it anyway.
Second, we can tell they knew it was broken because they
"compensated" for the possible backdoor by substituting
their own constants for P and Q rather than using the
NIST (and NSA) supplied constants.  But they give
absolutely no indication of where the constants they used
came from nor whether the math behind the ones they used
also came from (or were shared with) eavesdroppers or
Many people, including myself, think they just rekeyed
the lock on the NSA's backdoor so it would look different
if their customers checked.  Nobody knows whether they
gave the new backdoor key to the  NSA, or whether the
NSA just stole it from them.
Third, they also "protected" the universe from the possible
backdoor by using the Dual_EC_DRBG to generate keys for a
3DES-based PRNG implementation - after all, if an attacker
never gets to see ~30 consecutive bytes of raw D_EC_DRBG
output, they can't work out the RNG state even if they know
the backdoor.  The problem with this was that in the middle
of a routine prng_reseed(), a side effect set a variable named
prng_output_index to 32, and a for loop immediately following
the call to prng_reseed(), which is supposed to process the
buffer before it goes out, repeats only for as long as that
variable is less than 31.  IE, not at all.  Thus the buffer
that was supposed to be processed by their 3DES-based
generator is not in fact so processed, and what goes out
into the world is in fact 32 bytes of the raw D_EC_DRBG
output. Cue the fail whale.  This would be idiotic had
it not been done on purpose.
Fourth, somebody made "unauthorized" code changes in 2012,
changing the Juniper-selected mysterious P and Q constants
for a *different* pair of mysterious P and Q constants.
Apparently a crook found the backdoor and managed to insert
code to use his or her or their own constants turning that
backdoor to his or her or their own advantage.  There are
numerous conspiracy theories as to the identity of the
crook.  One of the most pervasive and credible would imply
that this particular crook draws a regular government
paycheck.  On the other hand this "unauthorized" change may
be another fig leaf for Juniper, much like the use of different
P and Q constants in the first place prevented them having to
admit they were using  parameters provided directly by the
NSA.  This "unauthorized" code is something they can point at
and claim wasn't their fault, but which wouldn't have altered
the functionality of their backdoor at all had they done it
Fifth, when this relatively complex Dual_EC backdoor was
discovered, Juniper immediately published another CVE about
a *different* and much simpler backdoor which allows anybody
who knows or can guess a legitimate account name to log
into it, with magical admin privileges, using the password
<<< %s(un='%s') = %u
.  The code that does this is comparatively blatant; the
string above, although it looks like the sort of string you'd
find doing something else, is a direct argument to strcmp.
The timing of this announcement is sort of silly; it implies
either that the code was finally reviewed when the news
about the Dual_EC_DRBG based backdoor came out, or that both
backdoors had been long known and the second was announced
in order to confuse the issue and draw attention away from
the first.
Sixth, their patch for the Dual_EC_DRBG backdoor is to
replace the "unauthorized" P and Q constants with the same
completely unexplained P and Q constants that Juniper
selected for unknown reasons years ago.  So it's a return
to "authorized" code, but somehow it fails to inspire my
Seventh, the bug that has them using Dual_EC_DRBG in the
first place has not, so far, been fixed by their security
Eighth, the much more damning bug that has them allowing
32 bytes of raw output from that horrible beast to go
directly out on the line has also not, so far, been fixed
by their security patch.
....  and I think that about sums it up.
My only remaining question is where did the bribe money

@_date: 2015-12-23 00:53:41
@_author: Ray Dillinger 
@_subject: [Cryptography] Juniper & Dual_EC_DRBG 
Not a bug??
I beg your pardon?
In what universe is any use of Dual_EC_DRBG not a bug?  It is
known to be flawed, it is known to be inefficient, and it is
known to inspire loathing and distrust of products which use it.
The supposed "theoretical guarantees" that NIST touted for it
are nullified, even if their design worked the way they made it
look like they wanted it to, by using it as a key generator for
a PRNG that does not provide those guarantees.  What possible
attribute could make it anything other than a bug?

@_date: 2015-12-28 11:45:31
@_author: Ray Dillinger 
@_subject: [Cryptography] Understanding state can be important. 
Remember systems where there was a physical write-protect slot on the
floppy disk?  And the hardware was physically constructed so that the
write head could not come down to write the media if the write-protect
slot was filled, so if you didn't trust something that was running you
could protect your stuff by just not letting anything get written to?
Maybe I've always been a suspicious bastard where software is
concerned, but I considered that ability fairly valuable.  Most of the
stuff I was running back in the day was from people I didn't trust,
including major software vendors and anonymous hobbyists - why should
I let those people have write access to anything I considered valuable?
Floppy drives came out a little later that simulated the physical write-
protect mechanism with code, but that seemed wrong to me; you needed
the write-protect in the first place exactly because you didn't trust
the code.  So I avoided buying those; When I shopped for floppy drives
I looked for the little lever that physically prevented the write head
from coming down instead.
So, way back in the way back, when I got my very first hard drive, I
was looking for the write-protect toggle switch.  A physical switch,
mounted on the face of the drive, that would cut the power needed to
write the drive when only reading was desired.  Simply to replace
the functionality lost, right?  Because hard drives were supposed to
be better, and let you do everything you could do with floppies?
Except nobody ever manufactured that.  And we have never stopped
needing it.
MFM hard drives still had stateless electronics and an LED that
would flash a red warning when anything actually performed a write.
So even if you couldn't stop running software from writing to
anything on the disk, you could still notice a write when none
was supposed to be happening, and if you suspected that something
had done something naughty, you could be sure of clearing it OFF
the disk if you popped the BIOS battery out of your motherboard,
rebooted from floppy, then re-entered the hard drive parameters
using DEBUG from ROM (which really WAS read-only back then) and
formatted the drive.  That was a huge pain in the neck compared
to having a write-protect you could engage when running an untrusted
program, but it was at least something.
Nothing since MFM has had even that capability.  We live now in a
world where our devices offer no physical security against being
written, and no physical certainty of the success of any erasure.
Running code can write things to permanent storage at any time
without us allowing it, and if we suspect that it has done so,
we can tell the code to erase it and the code tells us it's erased.
The very few drives that are still made with a write-warning
LED? That write-warning only lights up when the code tells it
to.  If we don't trust the code, too damn bad.
There is no physical basis for security we're truly certain of
to even be built any more.
And we allowed it to happen.

@_date: 2015-12-29 10:34:30
@_author: Ray Dillinger 
@_subject: [Cryptography] Understanding state can be important. 
Oh hell no, there's hundreds of running processes now.  I'm still a
suspicious bastard, but these days I feel utterly helpless to do
anything about it.  There is no practical alternative to complicated
OSes that start processes without permission and refuse to explain
what they are and why they're running.  I look at my process table,
I see $NAME I don't recognize, I ask "man $NAME" and there is no
such documentation.  One more mysterious thing that will crash my
system if I kill it but I have to look elsewhere to explain what it
is and why it's running.
This is one of the reasons I don't.  The other is that there is even
less explanation of what the hell is running, and you can't even get
the code to check what the hell it does.

@_date: 2015-02-03 16:19:47
@_author: Ray Dillinger 
@_subject: [Cryptography] crypto standards and principles 
No matter why someone fears releasing knowledge, fear reveals
Keeping keys secret is part of what the NSA is for.  Keeping
cryptosystems secret is pathetic, idiotic, and counterproductive.
If they are afraid others might break the ciphers, and they
have not, that's pathetic; it would mean they have lost the
lead and they know it.
If they are afraid others might learn how to make secure
ciphers from their designs, that's stupid; others already
know how to make secure ciphers.
If those systems are insecure, that's stupid; The longer
such secrecy keeps people from discovering that insecurity,
the more exposed they become and the more secrets they have
exposed to their enemies.
If those systems are secure, that's pointless and
counterproductive; in that case they've nothing to lose by
releasing them and gain nothing by keeping them secret.

@_date: 2015-02-09 19:55:41
@_author: Ray Dillinger 
@_subject: [Cryptography] What do we mean by Secure? 
"Security" means the parts of the system which can be used against
anyone who doesn't understand them by anyone who does.
The fact that anyone is trying to answer all these questions AT THE
SAME TIME, and without discussing each with the car owners for an
extended period first, is probably an indication that failure is
BMW's failures with ConnectedDrive are nothing short of comical, but
that's the only possible result, in security terms, of trying to
change a hundred different things at once.  The complexity of the
security model changed too fast and exceeded the understanding of
the users.
With most systems, features that the user does not understand and
does not personally use are harmless.  Security on the other hand
is more or less defined by the fact that every last feature the
users do not understand can be used against them by someone who
And that applies to ALL the users.  The car owners had no idea, of
course, so the system they didn't understand could be used against
them by thieves who did.  But they're just the tip of the iceberg.
The implementers didn't understand their use cases, either, or
they'd have realized that knowledge of the VIN is not something
that identifies an authorized user.  The people making the error
messages did not understand even that (admittedly laughable)
security model, or they'd have known that disclosing the VIN
in every error message amounted to making an authentication token
available.  And that's two more cases of people who didn't
understand something, so it could also be used against them by
anyone who did.
The people to whom this authority was being extended didn't
understand it either - if they'd understood how their "app"
worked to extend that so-called authority to literally everyone
in possession of a cell phone and an understanding of the app,
they'd have known that the authority they had just gotten meant
nothing and was in fact dangerous, and would have demanded its
immediate removal.  And that which they did not understand, once
again, could be used against them by someone who understood it.
BMW tried to change a hundred things all at the same time. People
with no experience (no bone-deep understanding) of newly-changed
parts of the system were trying to design, implement, and use other
new parts of the system.  The result was fore-ordained.
Understanding as measured across such large groups, changes only
gradually, so if you change a security model suddenly or by a
hundred things at once you will fail just as hard as BMW did.
The existing security model is - one person has access, and is
authenticated via a mature technology using a physical token at
least moderately hard to duplicate (a key).  That person may
extend access to others by deliberately and knowingly having a
key made for them.
This is a security model that the users UNDERSTAND!
That understanding puts BMW ahead of most software companies,
and they can arrive at a good solution much faster in the long
run if they move slowly enough to keep that advantage.
Make *one* change.  Wait until all the people who need to understand
it do.  Wait for the thieves to point out the problems with it and for
people to understand how thieves interact with it.  Wait until people
have had time to make whatever adjustments in behavior (or demands
for limitations to it) that they need to make.  Then, you can make
another change.  Repeat. It may seem excruciatingly slow, but trust
me, it's faster this way.
Every security "feature" is a potential hole until its users fully
understand it.  And every security feature designed in the presence
of or depending on features not yet fully understood, compounds
the problem.

@_date: 2015-02-18 14:35:45
@_author: Ray Dillinger 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
That which is available to any organization is available
to criminals, because every organization includes criminals.
"We have met the enemy, and he is us." -- Walt Kelly.

@_date: 2015-02-18 16:18:08
@_author: Ray Dillinger 
@_subject: [Cryptography] [cryptography] Equation Group Multiple Malware 
Ironically, I do in fact recommend postal mail over any software-
controlled process or public-key infrastructure as a means of
distributing initial keys (the seldom-changed keys that access
a key distribution channel).
Pretty much every PKI that exists is either pwned or useless as
far as I can tell.  Some due to enemy action, others due to
mismatch in threat model or bluntly wrong assumptions about
trust.  I have the good fortune to be working usually with
something more manageable than financial crypto:  The problem
I usually am solving is not "How can I be introduced to anybody
at any time" but "How can I exchange secure messages with my
selected group of fifteen to fifty known correspondents?"  And
the answer, of course, is not a PKI of any kind, it is a KDC.
Is someone monitoring the email?  Almost definitely.  Is
someone monitoring the text messages?  Yup.  Is someone
monitoring the phone call?  "Metadata" sure but not likely
storing the audio.  But postal mail?  Someone opening and
reading the postal mail is unlikely, and if someone is, and
they're not an unambiguously criminal organization to start
with, that's a huge legal risk on their part.
The nice thing about postal mail, is that a letter is a physical
object that some human being with hands and eyes and a mind,
drawing a paycheck, would have to devote at least a few paid
seconds to opening and reading and understanding.  Another
nice thing is that there are still laws and court precedents,
left over from a previous era, that should give any would-be
eavesdroppers at least on the government side pause about
opening and reading it, and most non-government organizations
simply don't have any expertise in intercepting it selectively.

@_date: 2015-02-18 16:38:17
@_author: Ray Dillinger 
@_subject: [Cryptography] phishing attack again - $300m in losses? 
And then you have to have another round of security to prevent anyone
from DoSing a legitimate site by getting it put on that list which
your software is considering equal with alwaysmalware.com.
I recently had an experience with just this attack; a well-meaning
fool had put a "safety" feature on http accesses, and then when some
evildoer wanted a site knocked off the network visible to the people
whose access was subject to that safety feature, all he had to do
was submit a bogus "attack software" report.

@_date: 2015-02-22 10:12:37
@_author: Ray Dillinger 
@_subject: [Cryptography] Quarter-million dollar bounty for key crack.... 
Okay, anybody out there who has an idea for special hardware that
can break an ECDSA key, or a way to do better than random guessing,
you should be aware that somebody is offering a quarter-million
dollar (at current bitcoin prices) bounty for doing so.
I'm pretty confident that this prize will never be claimed, but
if anybody's got anything that they've been considering out-of-budget
for an attack, it could help.
Details at:
No idea who this guy is, but he's been around that forum for a
while and has a verifiable history of flinging wealth around
(usually in ways that make him more; he's clearly not stupid).
So the bounty is credible.  Although I'm relatively certain
there's no way within the laws of math and physics to claim it,
I could be wrong.
I'm actually more interested in the key he wants cracked.  It
doesn't show up on any casual searches, but could it possibly
be a mathematical transformation of a key that stores large
wealth somewhere in the bitcoin blockchain, such that if someone
cracks *this* key for him, he can use the answer to derive
a *different* key that unlocks a large amount of bitcoin?

@_date: 2015-02-22 10:24:35
@_author: Ray Dillinger 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
This is a point I make from time to time.  You will never have good
security unless you have full and enthusiastic cooperation from your
people.  And you won't have that if you're doing anything that your
people don't believe is RIGHT.
And, you know, that "Constitution" thing -- a whole lot of Americans
believe in that.  It's going to be really really hard to do decent
security with Americans on your staff if you're going up against it.
Larger groups always increase the odds of a non-cooperative member.
But those odds start out an order or two of magnitude different
depending on how much your employees believe in what you're doing.

@_date: 2015-02-22 11:35:02
@_author: Ray Dillinger 
@_subject: [Cryptography] Lenovo laptops with preloaded adware and an evil 
The current page of "Cybernetic Entomology" is about this whole
Shameless Plug: Cybernetic Entomology is my own series of blog
posts.  As it develops it may become a book.

@_date: 2015-02-25 19:11:05
@_author: Ray Dillinger 
@_subject: [Cryptography] trojans in your printers 
In the first place I am not usually printing anything, so it spends
99%+ of its time powered down and unplugged.
In the second place my printer has no need - ever - to send or recieve
a packet outside my home area network, and therefore its address does
not have a hole in the firewall, in either direction, on any port.
In the third place I did not install a wireless interface.  'Cos
back when I got it, those were separate parts that the guys tried
to sell as a premium add-on, and I didn't want it.
But since BADUSB came about, I can't even fully trust that.

@_date: 2015-02-27 09:42:12
@_author: Ray Dillinger 
@_subject: [Cryptography] The Crypto Bone's Threat Model 
Here is how I would try to do it if I were designing hardware.
FWIW, this is exactly what I'd like in a desktop or laptop
computer too.  But then I'm one of those curmudgeons who
misses diskettes with write-protect tabs and wishes hard
disks (and solid-state drives) came with write-protect toggle
Essentially this amounts to separating all firmware updating
from all normal operation low a level as possible, while
providing a "hardware reset" capability to restore a known
firmware configuration.
1) The Device has in ROM (ie, not rewritable AT ALL) the
    required code to disable the device's external connections,
    then "flash" all the rewritable firmware in the device
    back to a known configuration.  Any updates to firmware
    beyond the default configuration, must happen again
    after any time this code is run.  Absolutely no on-
    device memory image anywhere, save only the non-rewritable
    ROM itself, survives the autoflash.
2)  The "autoflash" action described above happens when and only
    when a physical switch on/in the device is triggered by a
    human.  It cannot be triggered nor prevented in any way
    from software, including firmware.
    I'd suggest a switch with a good mollyguard; maybe a
    tiny button located behind a 12mm-deep 1mm-wide hole, such
    that someone would have to stick a wire into the socket
    to reset it or something. Or maybe a DIP switch mounted
    behind a snap-out panel.  Up to you, just don't leave it
    hanging out where someone could hit it accidentally.
3)  When in its "autoflashed" mode it can copy new firmware
    images from the mSD card into its own RAM, but does not
    actually install any of it, on any device, at all,
    until....
4)  When the operator flips the switch back to "normal operation"
    mode, a different chunk of the  ROM code runs.  It flashes
    the new images to all on-device firmware, disables all
    firmware updating, and THEN re-enables network communications.
    At no time until it's finished running does it call any of
    the firmware it is installing or has installed.
Any such device which is physically unmodified can be fully
configured (including being restored to REAL factory settings)
by anyone who can control what contents are on the mSD card.
At the same time there's no "signature checking" phase that
would mean a key has to be built into hardware and that
users have to trust a single potentially-subvertible source
for updates.
This preserves the principle that trust is subjective;
ie, a human who subjectively experiences trust can check
signatures or whatever that information comes from a source
s/he personally, subjectively, trusts.  But a device with
no subjective experience within which concepts like "trust"
have a meaning, cannot trust others on behalf of its owner.
Of course, anyone can make a device which *looks* like it
works exactly that way, and *claim* it works in exactly that
way, even if it doesn't.  And that is including the original
manufacturer as well as anyone who interdicts the device in
shipment or anyone who gets access to it via a black bag job.
So we're now talking about trusting trust, which is an
infinite regress.  But, because trust is purely subjective,
the regress breaks at the level of the user.  If he
subjectively experiences trust, then for better or worse
he trusts the device.
Electronic devices *can* be made as trustworthy as hammers
and wrenches. The problem at the bottom level is that almost
no one can check that they truly have one of the trustworthy
But that's also true of mechanical devices. Someone can
sabotage a hammer or a wrench, too -- or a nail gun, an air-
wrench, a car, a bridge, a high-rise building, an airliner,
or a hydroelectric dam.  As per the usual pattern with
electronic devices, the more large and complicated a
mechanical device is, the easier it is to hide a sabotage.
The more powerful it is and the more who depend on it, the
worse the consequences of the sabotage.

@_date: 2015-01-04 18:13:40
@_author: Ray Dillinger 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
The Lorenz cipher was breakable largely because it used ten rotors
to generate keystream material, but encrypted each bit of the (5-bit)
telegraphy code in use using only two of its wheels, allowing them
to be attacked as five two-rotor ciphers with additional constraints
provided by the combination of the five bits into sensible (german)
Its ten rotors were driven in two synchronized groups of five rotors
each, with one group advancing one space for each letter input and
the other group advancing zero or one space depending on a
combination function of the outputs of two "motor" wheels.
Each of these groups of rotors consisted of five rotors which were
a different (relatively prime) number of units in circumference,
such that one wheel went through a complete revolution every 41
times it was advanced, the next every 43 times it was advanced,
the one after that every 47 times it was advanced, etc.
It would have been considerably harder to cryptanalyze if each bit
had been encrypted using a combination of several different rotors
rather than just two (and always the same two).  But it's hard to
come up with a good way to do that without increasing the parts
count, bulk, expense, and unreliability.  If the rotor combination
used for each bit were not to change during the message, it could
be part of the key and set up using a switch panel of five toggle
switches per rotor.  That would probably have secured it in WWII,
but wouldn't be secure against modern methods.
It would have been *much* harder to cryptanalyze if there had
been a simple monoalphabetic substitution of each five-bit
"character" encoded, followed by the Lorenz encryption, followed
by a different monoalphabetic substitution. In terms of parts count,
that would mean adding an Enigma style plugboard or "steckerbrett"
to the input and output of the Lorenz machine.  That would have
provided diffusion by making the encryption of each bit depend not
just on the two rotors in its bit position but also on the values
and settings affecting the encryption of all the bits in its
"character." That would also have kept it secure in WWII, but still
would not do so today.
Considering that both the Lorenz machine and the Enigma had
the capability to read and write paper tape, they could have
been used to superencrypt their own (or with slight adaptation
each other's) output in a way that added no bulk, weight, or
expense and very little time or unreliability.  It would
double (or more) the amount of paper tape with which the signal
corps needed to be supplied, but AFAIK that was never a real
The real problem with all three of these approaches would be
that they would complicate the job of the cipher clerks, and
those guys making mistakes is a far bigger liability than
cryptanalysis of the devices without them making a mistake.
If I were attempting to make a cipher secure against modern
methods using WWII technology.... it's easy to conjure up some
device which they could have built, but which would have been
bulky, heavy, expensive, slow, complicated to use correctly,
and prone to breaking down. Much harder to come up with
something light, compact, reliable, swift, easy/foolproof
to use, and no more expensive in bulk to produce than, say,
the Naval Enigma.
We're always caught up in the Bletchley Park story and the
cryptanalysis of Enigma and Lorenz; at the time the Allies were
using a device called the M-209, which was successfully
cryptanalyzed in the 1970s (though publication was suppressed
by the NSA) by Dennis Ritchie, James Reeds, and Robert Morris.
That's a far more subtle attack, and actually probably a more
instructive example for modern cryptanalysis.

@_date: 2015-01-06 00:54:09
@_author: Ray Dillinger 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
The Enigma actually had two fatal weaknesses.  First, as you
correctly point out, the allies exploited the can't-map-to-
itself property to great effect; t was how the Bombes
eliminated possibilities in great quantities once they were
Second, there were not nearly enough different rotors.  The
Germans never deployed more than eight different movable
rotors, plus a couple (beta and gamma) that, though they
could be hand set to any position, were immovable, could
only be deployed in a single known location, and could
not be combined.
The Allies eventually worked out the wiring of all these rotors,
and this formed the set of possibilities that the Bombes started
from.  Once you know that the message is encrypted using a
selection of four out of just eight different movable rotors,
whose geometry you know, and and you know the geometry of the
input/output signals, and you know that there can be one of only
53 remappings of the signals at the reflector (beta and gamma
"rotors" in each of 26 positions, or just plain reflector)...
you have a limited set of possibilities to check.  Limited
enough that they might have been able to run through them
even without the cant-map-to-itself property.  It would have
been much harder to break and, on the relatively rare breaks
would have given much less timely information - but it would
be possible.
The steckerbrett helped the security a lot by rearranging the
geometry of the input-output signals. The "beta" and "gamma"
rotors did the same basic job at the reflector end, but they
could only be used to make a very limited number of remappings.
Fixing the cant-map-to-itself property would be the highest
In terms of electromechanical parts count/intricacy, that
could be done by doubling the amount of wiring and contact
points in each rotor.  Providing separate electrical paths to
and from the reflector would permit each letter to map to any
other letter including itself, giving the Germans the
cryptographic mileage they *believed* they were getting out
of their reflector mechanism.
A possible implementation of this would have two sets of
contacts in concentric rings in each rotor, so that signals
going *to* the reflector passed in one ring and signals
coming *from* the reflector passed via the other.
That would have slowed down the Bombe's drastically, because
they would then be unable to eliminate possibilities automatically
based on the can't-map-to-itself property.  But you still have
the not-enough-different-rotors problem.  Bluntly, with only
eight distinct movable rotors, and the rotors each having a
geometry known to the allies, a brute-force attack would still
be somewhat feasible - though much more expensive and usually
providing untimely results.
It would be beneficial to have about half the rotors map paths
from the inner to the outer ring of contacts and vice versa.
Cryptographically this would help with the not-enough-rotors
problem because downstream of such a 'swap' the rotors have the
effect of reversed inner and outer permutations, so now there
are four possible ways a rotor can "face" instead of just two.
Depending on the disks closer to the I/O than it is, it can be
"inside out" or "right side out" as well as "facing left" or
"facing right".
If I wanted to secure the Enigma with WWII tech (against
WWII tech), I'd use the doubly-complex rotors described
With the doubled contacts and paths, each steckerbrett would
have to be 26 rather than 13 sockets, which would double the
complexity of the steckerbrett but extend the key by 26!
possibilities as compared to 13! possibilities, which is  bad
for our parts count but good for our key length.
Next, I would get rid of the "beta" and "gamma" non-movable
rotors entirely and replace them with a second steckerbrett
on the reflector side. That would do the same job of remapping
electrical paths at the reflector, but would do it in a much
more versatile way, with 26! possibilities (again) rather
than 53.
At that point you'd have effectively doubled the physical
complexity and added a factor of probably at least 20% to
the manufacturing costs of each machine.  If at all possible
I would deploy it with 26 rotors to choose from instead of
8, which would probably increase manufacturing costs again
by another 20% or so and also increase the effective weight
and bulk of each machine.  At that point though, I am
confident that it would be every bit as unbreakable in
practical terms (given WWII capabilities) as the Germans
believed it to be.
But it would still crumble (not easily, but given realistic
amounts of traffic it could be broken) in the face of
modern methods to use against rotor machines.
There are some additional things you could do to introduce
displacements of the positions at which rotor movements
take place from the wiring geometry, or introduce
displacements of the inner "ring" of contacts relative
to the outer "ring" on each rotor, thus adding bits to
the key -- but while they'd force the allies to run
through more guesses they would add greater complexity
(and potential mistakes) for not-very-much-greater
encryption strength relative to the above improvements.

@_date: 2015-01-06 12:57:32
@_author: Ray Dillinger 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
I had another thought about securing Enigma (or designing
a rotor machine) last night.  This follows from thinking
about the "not enough different rotors" problem.  The rotors
were actually somewhat intricate and difficult to manufacture;
deploying with 30 of them would have been difficult.  So the
problem is to come up with something simpler to manufacture
and fairly easy to use, but cryptographically harder to break.
Suppose that each rotor contains one or more sockets for
interchangeable subassemblies that can be added in any
of several different orientations. One possibility is an
inner 'disk' not very unlike the rotor itself electrically
speaking.  The advantage would be that they can be easily
repositioned relative to the rest of the rotor, according
to the key of the day.
But these inner disks could be mechanically very simple.
All the mechanical difficulty of making contact while in
rotation, and being driven, and for that matter rigidity,
is already taken care of by the construction of the
rotor itself.  These inner disks could be very simply
connected inside them, just by being screwed down in
position each day.
Manufacture would be simple; you could stamp electrical
traces out of thin sheet copper, stick them through
prepunched holes in a "middle" insulating  disk, bend
them so the ends reach contact positions on the edge,
bend them again so contact areas stick through slots in
an insulating 'cover' on each side, and then glue the
extending contact ends down on the outside of the covers.
With a clockwise spiral on one side, pass through at a
chosen point, and counterclockwise on the other side, you
make a crossbar configuration - so any arbitrary combination
of connections is possible, no fancy printed-circuit
routing or shapes required.
In fact you could completely replace all the wiring (other
than the external contacts) in the rotor with one of these
disks, and just allowing a rotation of the disk relative to
the driving teeth on the rotor would make the code breakers'
job harder.
But there's no reason to stop there.  The disks would be
thin and fairly flexible, so you could make a stack of
them, using the extra thickness at the electrical contacts
to make sure each disk in the stack makes contact with
the next.  The job of rewiring a rotor for each day's key
is then taking the cover off the rotor, building a stack
of disks into it like stacking a deck of cards, putting
the cover back on, and screwing it down to clamp the stack
in place.
So, instead of just figuring out which of 8 rotors is in
each of four positions, the codebreakers now have to
figure out how the rotor in each of four positions is
composed from a choice of some (say) four out of the 30
or so disks that have been distributed, each of which
may be in two different orientations and any of 26
different rotations relative to each other and relative
to the driving teeth.  This would extend the key space
to a point where I'd be confident of it even today,
and be easier to manufacture per machine than the eight
rotors (with all the sliding contacts and contact springs
and driving lugs and individual wiring etc that each
rotor had to have). People with screwdrivers would
still have to connect all the springs and contacts,
but on just four rotors per unit rather than eight.

@_date: 2015-01-06 13:05:53
@_author: Ray Dillinger 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
The steckerboard didn't fix the cant-map-to-itself problem, and the
allies were able to get so much mileage out of that limitation that
the steckerboard didn't matter much.  If that problem had been fixed,
the steckerboard would have added a lot more security than it
actually did.

@_date: 2015-01-07 12:23:46
@_author: Ray Dillinger 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
Steckering at the reflector end would have been much more effective
because that would have decorrelated the permutation between input
and reflector from the permutation between reflector and output.
The cyclometric attack wouldn't have worked at all, for example.
But that's relatively subtle math, and I think it's mostly something
that cryptographers have all learned since.  It's easy to understand
someone in WWII failing to realize that.
In reviewing the Third Reich's operational record with Enigma,
it's hard to tell whether they lost the war because of sheer
stupid arrogance (with the failures of training, overconfidence
in equipment and procedures, and systematic underestimation of
opponents that implies), or whether it just seems that way now
because we have the record of the cryptanalytical progress against
Enigma which depended so much on those mistakes.
Does every large-scale military organization make stupid mistakes
subordinating security to petty officiousness, redundant procedure,
personal ego, and just plain laziness?  Is this level of
operational failure something that people need to design for
if building systems for military clients?
I suppose a review would require gathering data about how often
warrant officers (those who have a warrant on account of expertise
with some particular crucial field) are overruled by commissioned
officers (those who are in the chain of command and have
commissions on account of military training). Seriously, part
of good military training ought to be a realistic assessment of
how much to trust nonmilitary training.
I mean, imagine a warrant officer cryptography clerk, saying to
Herman Goering: "Sir, it degrades operational security to repeat
this same greeting word-for-word with full honorifics etc, at the
beginning of each message...."  Odds of him getting overridden?
Odds of him being too afraid to even speak up in the first place
even though he knows it to be true?  Odds of him getting punished
for telling the truth?

@_date: 2015-01-07 19:27:07
@_author: Ray Dillinger 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
In at least one case, they were definitely deliberate signals
to the allies.  IIRC there was at least one double agent whose
entire task on behalf of the allies was retransmitting, word-for-
word, the same messages on different cipher systems.  Which he
was able to do, and often, without ever violating a single
Nazi security procedure.  :-o

@_date: 2015-01-07 21:30:02
@_author: Ray Dillinger 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
heh.  Of course, other people had similar ideas.
It's simple enough to make a rotor that can easily be
reconfigured.  I mean, if even my little stack of wiring
sandwiches would be too challenging to manufacture, distribute,
and manage, it's easy to just make rotors with a 26x26 grid of
screw holes.  Screw in 26 machine screws in at the beginning
of the day and you've got a rewired (crossbar) rotor, whose
electrical map depends on which 26 holes you filled.
I think maybe the biggest problem with re-keying like that is
the risk of getting it wrong and broadcasting something with
a "related" key and then later rebroadcasting it verbatim with
the "correct" key.
Operationally, each key would need to come with a test vector.
So in addition to the key, it would need to say something like,
"If you have key set up correctly, then the message BLACK CROW
IS ASLEEP should encrypt as DREEX CDNVX TWKFF PI"  and people
should be able to check that their key setup is correct without
ever transmitting or receiving anything.

@_date: 2015-01-08 22:18:54
@_author: Ray Dillinger 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
It's true though.  Restricting ourselves to exactly the same
parts list they were using with the Enigma, the single most
security-enhancing adjustment I can think of would be putting
the steckerboard between the reflector and the rotors rather
than between the keyboard/lights and the rotors.  That would
defeat the cyclometric attack that gave away which rotors were
being used, thereby making the additional rotor choices that
the nazis introduced after the first year much more effective
in creating a combinatorial explosion of possibilities that
the Allies would actually have to crunch through.
Restricting ourselves to the same mass, bulk, and expense,
two dozen printed-circuit wiring slugs, plus four rotors
that could have them inserted and use them, would have been
easier and cheaper to produce, not to mention lighter and less
bulky, than the set of eight fully-wired rotors with spring-
loaded contacts etc they were using by the end of the war.
For no additional expense they could easily be made left-right
symmetric so they could be mounted in the rotors "backwards"
drastically increasing their effectiveness in creating more
possibilities to crunch through, and radially symmetric so
they could be mounted in any rotation, which would have
saved the cost of introducing the movable alphabet rings.
Both of these measures would leave the "can't map to itself"
problem in place, so the Bombe would still work to eliminate
possibilities for rotor combinations based on plaintext cribs.
But without the cyclometric attack to tell them which
possibilities were worth checking they'd have had to crunch
through a whole LOT of possibilities. The steckerboard placement
alone would have required crunching through about sixty times
as many possibilities.  And two dozen reversible wiring slugs
plus four very slightly more complex rotors in which they could
be mounted, instead of eight fully wired rotors and movable
alphabet rings, would have increased the allies' difficulties
by a factor of at least tens of thousands.
The Naval Enigma which had a papertape reader/writer attached
could have been used to re-encrypt its own output, which would
have eliminated the cant-map-to-itself problem and placed it
completely beyond analysis with WWII methods - but it would
have complicated the job of the cipher clerk and therefore
made mistakes more likely as well, so that might not be a
good idea in practice.

@_date: 2015-01-08 23:22:28
@_author: Ray Dillinger 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
A nontrivial redesign of Enigma, but still using
mostly the same parts and the same budget for mass
and parts complexity, would be representing each
letter as a combination of *two* signals on a
selected *eight* I/O lines out of the 26 that the
manufactured wheels had. 8 x 7 (for some combination
of two out of eight) divided by 2 (because it can't
matter which order you get them in) gives an alphabet
of 28 symbols, which encompasses the 26 letters and
gives you two more - to use probably for "switch to
numbers" and "switch to numbers then look up the
number in a codebook" respectively.
Then, instead of having a reflector, you'd just
loop the other 18 connections through pluggable
patch cords from output to input.  The patch cords
would replace both the steckerbrett and the reflector.
So two signals enter from the keyboard on the eight
leads and pass through the 4 rotors. If one of them
reaches one of the eight selected contacts, that's
one of the two output signals, and if it doesn't it
just loops back through a patch cord to the input side
and passes back through the three rotors again, until
it comes out at one of the eight selected I/O contacts.
A signal can't get stuck in a cycle in the non-output
contacts, because the input and output contacts are
the only possible endpoints, and everything else loops
through without ending.  Because there's no way back
onto the same electrical path you've already traversed
except through an endpoint that you can't get mapped
back to, no electrical path can be repeated. Therefore
a path from any input contact MUST end at an output
Because a signal going in only has an 8/26 chance
of coming out in the range of I/O lines after a
single trip through the rotors, each of the two
signals would pass through the rotors something
between 1 and 18 times, with an average of about
3 times, getting redirected by a patch cord each
time through. This would be drastically better
than using the reflector plus steckerbrett.
This would fix the can't-map-to-itself problem,
and also the cyclometric attack.  It would require
the redesigned Enigma to add an encrypting/decrypting
switch (the effect of which could be as trivial as
reversing the direction of current flow with two
strategically placed diodes) but would replace the
reflector and steckerbrett with something much
simpler, so probably be less expensive overall to
produce and almost exactly the same bulk.
This would place it entirely beyond the analytical
capabilities of WWII, with pretty much the same
parts, bulk, etc, and nothing harder to manufacture
than they did manufacture.

@_date: 2015-01-09 20:51:54
@_author: Ray Dillinger 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
The "numbers" are digits 0-9, mapped to some set of ten symbols.
The first character that's not part of that set signals that
you're dropping back to letters. If you need to type a letter
that would otherwise be mapped to a number, you just insert a
space (well, an X since that's the way they did that) first.
Seriously, no other comment on the redesign?  I thought that
running both the positive and the negative connections through
the rotors via different electrical paths encoding the letter
by combination, and then using 18 of the electrical paths
through the rotors for feedback, was really clever!
It preserves the desirable property that the encryption depends
on (at least) two different paths through the rotors that the
reflector was intended to provide, and it does it while
keeping the parts count, mass, volume, and cost down. It
requires no new mechanical technology, and demolishes both of
the two greatest weaknesses of Enigma - the cyclometric attack
and the cant-map-to-itself property!
The fact that most of the time one or both of the paths will
go through some random-ish number *more* trips through the
rotors is a bonus!

@_date: 2015-01-09 21:02:07
@_author: Ray Dillinger 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
I have heard the same story, regarding U-boats listening to
telegraph traffic by hearing the relays clicking at underwater
repeater boxes. It's not too unlikely, because the sonar guys
were also the radio guys, and knew Morse Code.  I got no
references though.

@_date: 2015-01-13 13:28:48
@_author: Ray Dillinger 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
After finally doing a thorough analysis of the Enigma cipher, I
conclude that the biggest cryptographic weakness wasn't the
antireflexive property (though that was bad) or the way its
slow evolution allowed the easy extraction of cyclometric
information on the first rotor which allowed the wiring to be
worked out (though that was bad too). Those were both weaknesses
of the machine, which is what we're all trained to look for given
that we work with modern electronic ciphers.
But we've been thinking about the machine, and not about the
The biggest single cryptographic problem is in the way it was used -
specifically in the selection of the rotor settings as the part of
the machine state that was used for a message key.
There was a day key consisting of the sequence and selection of
the rotors and the ring settings, and a message key consisting
of the rotor positions.
The rotor positions.  The only thing that changed during the
transmission of the message.  And the thing whose change during
the transmission of a message were nothing but offsets on the
exact same cycle of states.
Had the axis used the ring settings instead of the rotor
positions for the message key, every message transmitted
with a nonidentical message key would have been on a different
cycle of states, and not subject to this devastating related
key attack.
The sequence of states produced by the message key AAC was
exactly the same as the sequence of states produced on that same
day by the message key AAA, offset by two positions in that day's
single cycle.  And the sequence of states produced by the message
key ABA was the same as the sequence produced by the message key
AAA, offset by twenty-six positions.
The Germans were transmitting the message keys at the head of
every message, encrypted with a common setting.  This allowed
the allied cryptographers to know immediately which keys were
related by differences only in the last rotor (ie, sequence
offsets within 26 positions of each other).  They could then
find the index of coincidence for those messages, correlating
the different letters of the ciphertext message keys in the
last position with offset distances.  Then they could repeat
the process on keys having only the first rotor setting in
common, knowing that the sequence offset in those cases would
be the offset derived from the last rotor, plus some multiple
of 26 depending on the second rotor - rinse, repeat.  This
made a related-key attack from hell!
With three rotors, there were only 17576 possible rotor
positions, meaning Enigma traffic could be completely broken ---
easily, knowing NOTHING ABOUT THE ROTORS WIRING OR THE
POSITIONS OF THE RINGS, using nothing but armchair pencil-
and-paper methods not much different than crossword puzzles

@_date: 2015-01-13 13:38:07
@_author: Ray Dillinger 
@_subject: [Cryptography] simple codebook for passwords 
Not too bad a plan, except for the part where you buy it from
someone else who's already seen what yours says, and then most
people will use it as directed on the site - meaning they use
a method that anybody who steals their wallet can discover by
looking up the URL politely printed on the card.
I know you say "for ordinary users" - but, seriously?  They
don't even fill out their own?

@_date: 2015-01-15 12:11:11
@_author: Ray Dillinger 
@_subject: [Cryptography] Summary: compression before encryption 
In general, compression algorithms ought to be beneficial in
cryptography.  Greater information density means less redundant
structure of the sort that results in "cribs" for the analysts
to use.  The problem with most compression programs is that
they have very distinctive headers that instead provide cribs
for the analysts in the first few hundred bytes of the file.
It would actually be fairly easy to make a compression format
that is safe to use with cryptography.  It is also easy to
encrypt in a way that is safe to use with compression.
So you could solve the problem in either of two ways.  On
the encryption side, absolutely regardless of where in the
file the predictable header information might be, you could
simply apply a whole-file transformation during encryption.
It would suffice to loop through the file twice in a block
cipher using CBC mode for example, making every bit of the
output depend on every bit of the file.
On the compression side, where you actually know exactly
which bytes are the header, you could do something that
would work with streamed data, not requiring access to the
whole ciphertext before someone can start decrypting. It's
essentially the same thing as above, but only applied to
the section of the file containing the header and some
immediately adjacent compressed data. So, when someone is
decompressing a file with a 1k header block for example,
he might start by scrambling blocks 1-3 of the compressed
file and XORing the result with block 0 to get the header
We don't need security in the compression side; he could
use a simple "scramble" here instead of a real cryptographic
hash. The compressor could use pretty much any method that
achieves the diffusion needed without worrying about adding
confusion.  Confusion is the job of the cryptography (in
the cases where the compressed format gets encrypted). But
the scramble would make the format safe to use with

@_date: 2015-01-15 12:24:29
@_author: Ray Dillinger 
@_subject: [Cryptography] Summary: compression before encryption 
Oh, Jerry, you skipped the punch line!
Of course a uniform string of zeros is amenable to a VERY tight
encryption scheme - you can express it as a one-place number using
hexadecimal notation. It works out to nicely compressed numbers
in other bases as well, thanks to some interesting properties of
the Chinese Remainder Theorem with respect to a universal-
remainder property that applies to all nonnegative integers
smaller than the least prime...

@_date: 2015-01-18 13:55:05
@_author: Ray Dillinger 
@_subject: [Cryptography] Summary: compression before encryption 
The idea that modern cryptographers don't use cribs is relevant
only when the keyspace cannot be reduced enough by mathematics
to actually begin a search for a key that yields a plaintext.
We may have reduced the keyspace we need to search down to a
60-bit equivalent in a cipher with an 80-bit key using pure
mathematics and on that grounds we would rightly consider a
cipher to be completely broken.
But when we actually attempt to decrypt an individual message,
we must recognize the plaintext as being plaintext.  That is
the "crib" we still use. It's gotten more abstract, and we
apply our "cribs" in gobs of millions at every file offset
without thinking about it, but with most ciphers, recognizing
the plaintext is the only way you know that what you finally
have is the message you decrypted - and of course, the one
and only key that decrypts that particular message.
It is also ridiculous to think that with any modern cipher a
one-bit difference in a megabyte file will yield a completely
different ciphertext with no way to tell which bit with any
probability greater than the file length in bits.
What modern ciphers, mostly, will tell you is what block
that bit occurred in.  In ECB mode you can tell which blocks
through the whole file have bit differences.  Worse, if an
insertion into the file happens to be a multiple of the block
size, you can tell exactly how long the insertion was and
which subsequent blocks are unchanged except for having an
insertion before them.  In CBC mode you can still tell
which block is the first block that has a bit difference.
If we never apply a whole-file transformation we do not
propagate an inability to guess where the bit difference
is beyond a single block. So, usually, an opponent who
knows there's a 1-bit difference has a 1-in-64 or
1-in-128 chance of guessing exactly where it is, even
if the file is 8 million bits long.
This is why I floated a compression mode that applies a
whole-file transformation making every output bit depend
on every input bit.  Specifically so we can achieve that
goal of having an opponent have no greater than random odds
of guessing where that one-bit difference is.

@_date: 2015-01-25 11:50:11
@_author: Ray Dillinger 
@_subject: [Cryptography] The Crypto Pi 
The problem with the idea of a single entropy_avail figure is
that "entropy" means information unknown to all opponents, and
different opponents, at least theoretically, may have access
to different parts of your state.
If you have 128 bits known to your hardware manufacturer, 128
bits known to the NSA, 128 bits known to your ISP, and 128 bits
known to MI6, then you have 384 bits of effective entropy against
*EVERY* opponent on the list *UNLESS* they are sharing information
with each other. If any three of them are sharing information you
still have 128 bits of effective entropy against those three.
But if you're aware that any given entropy source could be tapped
by one of them, you can't count anything at all toward entropy_avail.
The question of how much entropy you effectively have is largely
a question of how many sources you get it from and whether any
single opponent has access to *ALL* of those sources.  Many
sources of entropy at least some of which are local is
*drastically* more secure than any single source of entropy
no matter its apparent quality, because its apparent quality
could be a sham or turn out to not be so great in a way an
opponent will eventually figure out.

@_date: 2015-07-09 14:03:01
@_author: Ray Dillinger 
@_subject: [Cryptography] Are Momentum and Cuckoo Cycle PoW algorithms 
I tried to implement cuckoo cycle and momentum myself, but I was
unable to develop a "strong" construction for them.  It's easy to
develop problems that require resources FOO, but for all the problems
I developed, as soon as I thought deviously enough about ways to
solve them, FOO for problems as memory-hard as I can make them is
always expressible as the simple product of CPU and memory - double
one and you can halve the other.
Trying to come up with anything that valued memory *more* than CPU
(ie, FOO expressed as the product of CPU and some >1 power of
memory) I completely failed at.
I didn't go so far as to extend these observations into
published attacks on existing implementations - but I did examine
a few implementations and decided that none were suitable for
my purpose at the time because they valued memory the same as
CPU, rather than valuing it more than CPU.
Now that I've thought of explaining it, I'll probably organize
the arguments (and devious strategies for solution with smaller
memory and larger CPU) and put them in a blog post or something.

@_date: 2015-07-09 19:13:04
@_author: Ray Dillinger 
@_subject: [Cryptography] "Office of Personnel Management Says Hackers Got 
And the most recent hack is not just federal employees.  It's
people who've had federal background checks run by the OPM.  There
is an important distinction that the press tends to gloss over.
I know, because I got the billet-doux from this one, thanks to
my own most recent security clearance being one of those in the
date range that this attacker managed to steal.  I am not and
was not a federal employee during this time period.  For the
job involving that security clearance I was employed by a
private contractor.

@_date: 2015-07-11 11:43:02
@_author: Ray Dillinger 
@_subject: [Cryptography] Is there a better way to discuss/publish new 
Crypto-practicum is a fully moderated free mailing list devoted
to discussing and analyzing attacks on cryptographic systems.  It
is hosted on sonic.net, the only ISP to get top grades for security,
transparency and privacy for all of the last six years.  If you
want to get people to look at, critique, refine, or possibly
extend your attacks, please subscribe and post.
A good URL for it is:
Full disclosure:  I am the moderator of the list, so this post is
promoting something that I personally am involved with.
I am especially hoping to attract people who want to discuss and
refine attacks to a point where they can be journal-published papers.

@_date: 2015-07-14 09:27:19
@_author: Ray Dillinger 
@_subject: [Cryptography] Super-computer project wanted 
An investment of supercomputer time should produce something of
lasting value, IMO.
Maybe you could apply new randomness tests to long sequences of
outputs from supposedly secure pseudorandom number generators
and see if you can discover anything worth adding to Big Crush.
Something that spots patterns in an existing PRNG believed to
be secure would be an important new algorithm for detecting
bad PRNG's and/or breaking stream ciphers.
Or you could apply Big Crush to long sequences of outputs from
deployed implementations of supposedly secure pseudorandom
generators such as those in popular operating systems, devices,
CPU chips, deployed routers and wireless networking devices,
etc, and see who failed to use anything worthwhile or whether
the new random number instruction in Intel CPU's is worth a
Or you could apply static code analysis software to huge
masses of existing operating system, device driver, plugin,
email-client or god-help-us browser code in wide use and
see if you can't spot instances of dangerous vulnerabilities
like buffer overflows.  A list of known errors would be
very helpful in getting code up to 'bulletproof' reliability
and no one runs ALL the possible static analysis we know
about on large bodies of code because it takes too long on
regular computers.

@_date: 2015-07-14 09:59:08
@_author: Ray Dillinger 
@_subject: [Cryptography] Ad hoc "exceptional access" discussion at 
Excuse me, but that's a load of crap.
I have no issue with law enforcement's desire to execute search
warrants - you know, the kind where they have to go to court and
get an actual judge to sign off on a warrant to search for a
very specific thing in a very specific place, and that warrant
then becomes public record. With a warrant they have, in addition
to the general access allowed to the public, the right to enter
premises and seize property, or compel the cooperation of the
person whose stuff they're pawing through before actually
pawing through it (or face arrest), or violate what would
otherwise be laws applicable to the general public, such as
laws against trespass or burglary or violation of electronic
security measures, in order to execute that warrant.
A search warrant is not a surveillance order.  Surveillance
means COVERTLY observing someone, and what our constitution
(though in many cases unconstitutional laws exist) allows for
COVERT observation on people is that law enforcement gets
exactly the same access to someone that members of the general
public have.  When you do surveillance, you don't get the keys
to someone's house.  Meaning you don't get cryptographic keys
handed to you either.  Also, if there are laws against cracking
security, you do not get to violate those laws unless you have
a real warrant - if you repeal such laws so it's the same
access the general public has then, if you can hack it you
can have it.  But the police are not (constitutionally)
permitted to break the law for purposes of covert surveillance.
And while surveillance may be done secretly, serving a warrant
must (constitutionally) create a public record for accountability.
I really hate unconstitutional laws.  How long until they are
struck down?

@_date: 2015-07-15 16:08:22
@_author: Ray Dillinger 
@_subject: [Cryptography] The Golden Ratio Attack on Bitcoin. 
You may have heard of the recent "stress test" on Bitcoin.
It is an attack.  I guess I get to name it, so I'll call it the Golden
Ratio Attack.
It is not strictly cryptographic, but it's a serious attack on a
cryptographic system and it's being executed.
Any miner who controls more than one over the Golden Ratio of the mining
power makes a profit while paying the fees it takes to maintain a
permanent backlog of transactions, for as long as the blocks are more
than half full of other transactions.
The following assumes expenses roughly equal for miners relative to the
amount of hashing power they control.  This is not exactly true, because
a miner someplace where electricity is subsidized (like China) has
substantially lower expenses.  In such a place the fraction of mining
power required to make it profitable would be even less.
The *initial* "stress test" was a test to see whether the miner
controlled sufficient hashing power to make a profit by doing this.  We
can assume that test was successful, because now this miner is doing it.
 Probably permanently.
The miner decides how much they want per transaction (anything that the
traffic will bear, as long as it keeps blocks more than half full of
real transactions), then keeps the backlog sufficiently full with bogus
transactions to prevent any tx that pay less than that from going through.
Maintaining the backlog subsidizes other people's mining as well as
their own, but means they don't need to compete with miners willing to
process transactions for less money in fees because those miners aren't
willing to process transactions for less fees when any transactions with
more fees are available.
Let's work the math.  If 2/3 of the transactions actually processed are
"real", then whoever is maintaining the backlog is paying the tx fees
for 1/3 of every block. If this is someone with half the mining power
then they get half of their third back, so their average cost per block
is the fees for 1/6 of the block.  If we are talking about someone with
half the mining power, their average return per block is 1/2 the fees
for a block.  Because 1/2 is greater than 1/3, they are making a profit.
The breakeven point for the biggest miner was when his fraction of the
mining power plus the fraction of each block devoted to legitimate
transactions was equal to one.  We can conclude that whoever is doing
this, if he started the instant it was profitable, controlled 2/(1 +
sqrt(5)) of the mining power at that time.  This happens to be the
inverse of the Golden Ratio.
It will continue to be in the financial best interests of any miner
controlling more than 2/(1 + sqrt(5)) of the mining power for as long as
blocks are more than half full with legitimate transactions.
This does not affect, and is not influenced by revenue from block
subsidies AT ALL.
All miners will see increased fee revenues in the competitive market.
They will respond to more revenue by  investing more in equipment.
Those miners are still competing fairly with each other, though they
will make less on their investment than whoever's maintaining the
backlog.  It is not in their best interests to add bogus transactions to
the queue because with a smaller fraction than 2/(1+sqrt(5)) of the
mining power they would lose money on the fees they invest.
But any miner for whom this IS profitable, will make additional revenue
that the fair market among miners does not.  What percentage more,
depends on what fraction of the hashing power he controls. Any such
miner is competing at an advantage and will eventually drive all other
miners out of the market.
Miners for whom this is profitable must control at least 2/(1+sqrt(5))
of the mining power.  Therefore there can be no more than two miners
doing it at a profit.  And it's got positive feedback, so those two
cannot compete fairly. Assuming there are two, the instant one of them
has more hashing power than the other, he has a competitive advantage
over the other (gets back as revenue a greater fraction of all fees
spent) than the other miner) and will eventually drive him out of the

@_date: 2015-07-15 17:16:10
@_author: Ray Dillinger 
@_subject: [Cryptography] Correction re: Golden Ratio Attack - 38% not 62% 
I misspoke.  I said 1/phi without remembering exactly what the
ratio measured.  I should have said 1 minus 1/phi.
The required hashing power is 38% of the total.  Considerably
less than the 51% people have been scared of.

@_date: 2015-07-17 20:18:42
@_author: Ray Dillinger 
@_subject: [Cryptography] Nasruddin Cryptographic Function (99% finalized) 
mmmm, nope.  You have to support claims like that with math.
Rijndael has a proof of security against linear and differential
cryptanalysis.  You can't make that claim without producing
equally strong proofs for yours. That's before we even talk
about block boundary attacks, alias attacks, other types of
group theory attacks and related-key attacks, chosen-plaintext
and chosen-ciphertext attacks, cyclometric attacks, timing
attacks, power attacks, and .... all of these things are stuff
you have to provide very convincing arguments about before
you can claim anything is better than something whose creator
did provide those convincing arguments.
One thing that SIMD ciphers are particularly prone to is
Tempest attacks.  You get a whole lot of processors doing
the same thing at the same time, and they act like a little
phased array of tiny radio transmitting antennas.  Have you
noticed that CPU's are operating at frequencies in the radio
range?  A three hundred dollar gadget can read them from
fifty feet away and get the keys.  If you're introducing
a SIMD cipher, you have to explain why that won't work.
Specifically you have to arrange it so that no part of the
key or plaintext, nor anything identifiably derived from
them, goes through many processors at the same time.
After that, justifying the use of a new cipher requires
demonstrating exactly what weakness existing ciphers have
that makes this one better.  And yours has to be a whole
lot better (ie, Rijndael has to be BADLY broken) in order
to justify something that takes many times as long to
compute on large classes of machines.
Finally you seem to be mistaken about which end of the
market is in sufficient pain to be willing to consider
adopting something new.
Right now everybody who isn't trying to figure out what
business a feeking thermostat has accessing the Internet in
the first place, is trying to connect them to the Internet
and then figure out how do do anything on the tiny ten-
cent processors they're putting into the thermostats. Then
people are trying to steal as much private information as
possible via those thermostats. Now that burglars are
ACTUALLY stealing the information in order to find out
when people aren't home, they want cryptography that runs
on those ten-cent processors. Few to none of the existing
ciphers are a good fit.
Come up with something like that, and people will pay a lot
more attention than they will to something that runs on a
CPU which would add more than a dollar a unit to the cost
of their devices.
Usually proving the required properties (or the absence
of undesired ones) involves heavy lifting via group theory,
because by definition no supercomputer is fast enough to
iterate over all keys and demonstrate that some undesired
property never happens.
The heavy lifting can be some other kind of math as
well, but however you do it, you have to have proof of
a bunch of claims, and in order make them you have to
know what the attacks are and therefore what mathematical
properties protect against them.
Seriously, best of luck.  But if you want to do this,
you have to actually DO it, not just claim somebody

@_date: 2015-07-17 22:56:10
@_author: Ray Dillinger 
@_subject: [Cryptography] Hypothetical WWII cipher machine. 
Someone on wattpad (where I do some recreational writing) noticed
that I've done some cryptography, and read my 'fixing Enigma'
articles, and asked me to design a Plot Device.
He's writing a WWII spy thriller, and the MacGuffin is a
super-duper cipher machine that the protagonist and his little
group have to prevent from falling into the hands of the Axis.
Having broken Enigma, the Allies don't want them adopting
something that's actually secure.
He asked me for a detailed design for something that (A) could
realistically have been built around WWII, (B) is not a rotor
machine, (C) is very much more secure than Enigma, (D) that he
could do a cool, interesting, understandable illustration of,
(E) whose basic operation could be explained in one page or
less, and (F) would not make real cryptographers laugh if they
read his book.
So I thought pretty hard about what's wrong with Enigma again -
you can read my blog posts from times I've thought about it
before. I admit I've obsessed somewhat on the topic of old
cipher machines.
One of the problems with Enigma, and most rotor machines, is
that too much of their state is static.  While their setup can
have a lot of crucial bits, most of them are things that don't
change during encryption/decryption, and most of them make
the machines very prone to related-key attacks (which was how
Bletchley Park worked). In fact the only thing that changes
during encryption is the rotor positions, and those only
describe a small fraction of the total state.
So I obsessed for a couple of days and came up with something
whose state is almost entirely dynamic state, as well as
having a much larger state space than Enigma.  It has what's
effectively a 154-bit keyspace and it's chaotic, so it will
fall into repeating cycles of about 2^76 states, with each
state representing a different monoalphabetic cipher, and the
relationships between adjacent states being far more chaotic
and obscure than Enigma ever made them.
And it does have a cool diagram and one-page basic explanation.
Now, the final test:  Will it make real cryptographers laugh?
Have a look, if you feel like thinking about the security of
a Plot Device and possibly having a laugh and correcting my
mistakes if I've made any obvious ones. The diagram and one-
page explanation are at
My only objections to it are practicality, not security; key
setup would be a big pain in the butt, and during the
process small removable parts could become lost making the
machine useless.

@_date: 2015-07-18 14:31:11
@_author: Ray Dillinger 
@_subject: [Cryptography] Hypothetical WWII cipher machine. 
Yah, on consideration it's sort of like a weird, flattened,
three-cornered, hexagonal-faced version of a Rubik's cube
with no immobile central faces.
The three circular regions rotate in chaotic sequence, and
because any two of the circular regions have six parts in
common and all three of them have four parts in common,
they swap parts depending on which one's rotating, very
much in the style of the way edge and corner cubes get
swapped around on a Rubik's cube. Within a few turns, any
part can move from anywhere to anywhere else or be
reoriented in any direction or both.
Sequences of rotations continually swap and reorient parts,
and have chaotic feedback because which rotations happen
depend on which 'boats' get lined up with the driving wheel
and what direction they're pointed in when they do.
The inputs are a positive and negative electrical impulse
routed independently through the maze.  It's very difficult
to even predict how long the paths they'll take are other
than in statistical terms, and unlikely that the path lengths
will be equal, so a cyclometric attack or anything like it
doesn't have any obvious way to work.  And at least as far
as I can tell there is no obvious similarity to the
alphabetic mapping produced by adjacent states to hang
a related-key attack on the way there is with a single-
wheel step of a rotor machine.

@_date: 2015-07-21 12:10:39
@_author: Ray Dillinger 
@_subject: [Cryptography] [Crypto-practicum] Hypothetical WWII cipher 
Yeah.  A non-electrified version - no circuits, input board,
output board - would be very easy to make (mill, laser cutter,
injection mold, 3d printer, whatever), could be pocket sized,
and with alphanumerics on the bits it would make a pretty good
"secure password" generator.  Give everybody stickers instead
of stamping the alphanumerics directly onto the bits, so they
all have unique mappings of movement to characters.  Then
they can assemble the name of whatever they're securing around
the outer edge, turn the thing fifty times, or whatever, and
then use the new outer edge for the password.  And, importantly,
there are no potentially-compromised electronics to steal
anything from.  There is no way for an electronic or software
breach in security to steal passwords from a purely mechanical
device. Stealing them from the sites where they're used, alas,
remains a problem.
It would be fine for that use. It would drastically improve
password security for people that use it, and the number of
passwords most people use wouldn't create enough traffic to
break it.
But I wouldn't be nearly as confident of it as a cipher for
encrypting reasonable amounts of traffic as I am of the
electrified version where two impulses go through a torturous
path to make a circuit and encrypt. The problem is that I
can understand how to get a handle on the raw movements
using group theory.  The "torturous path" with chaotic
feedback and uncertain lengths is needed to put it beyond
the abilities to analyze that I personally understand, so
that someone doing group theory wouldn't have a reasonable
way to initially detect the groups.
That said, it's still true that "anybody can make a cipher that
he himself cannot break."  I believe the electrified version
to have about 80-bit security, but someone else might have an
idea I don't.

@_date: 2015-07-21 12:52:34
@_author: Ray Dillinger 
@_subject: [Cryptography] Hypothetical WWII cipher machine. 
I believe that might have been part of the motivation behind the
Solitaire cipher from Cryptonomicon by Neal Stephenson.  Solitaire
is no longer considered secure by modern standards, but it is a heck
of a lot more secure than Enigma.
And software runs on a device that communicates with other
devices invisibly, which can have other software running on
it.... and which has a long history of the combination of those
two things being exploited to steal passwords and plaintexts.

@_date: 2015-07-23 09:37:27
@_author: Ray Dillinger 
@_subject: [Cryptography] Whitening Algorithm 
You are absolutely right.  Thanks for not having to explain that one.
Also, I had used the Von Neumann algorithm
If you whiten input from a biased source, you're always going
to get less output than you had input.  So yes, whitening
inherently must slow output significantly.
Eventually transistors burn out.  When that happens
they start producing all 1's.  Or all 0's.  And they
won't announce it ahead of  time.
So consider how it breaks down when your input is biased in
particular ways.  For example, if it's all zeros or all ones.
In the all-zeros case especially, you quickly run into what
most hardware either responds to with NaN or traps as an
error (zero to the zeroth power equals?)  In the all-ones
case, your loop goes on quietly producing highly predictable
output, which is exactly what you don't want it to do.
When a whitening algorithm is run on a repeating string of
zeros (or a repeating string of ones) the "correct" result is
no output, because there is no unpredictability in the input.
If you're getting any other response, you're doing something
besides whitening the input, at least in a cryptographic
If you need a high ratio of output to somewhat-biased
input, on modest compute requirements, use your somewhat
biased input to seed a cryptographically secure pseudorandom
number generator.  But be sure to use enough of it so
that it actually has at least the same number of
unpredictable bit transitions that the state of the PRNG
has bits.  In practical terms, if you want to seed the
PRNG all in one go, use the output of the Von Neumann
whitener as a seed.
Once you've seeded a CSPRNG, you can run it and use its
output directly. Although it won't really be random, these
algorithms exist because it's very hard to tell they aren't.
Or you can use its output to XOR-mask your biased input:
this still won't be "whitening" your input in the cryptographic
sense, but it'll be indistinguishable from random for
practical purposes.
As a lightweight CSPRNG suitable for devices of modest
compute power, I recommend the 'Spritz' algorithm developed
by Rivest & Schuldt.  Although it's fairly new, it seems
quite good.

@_date: 2015-07-24 14:24:24
@_author: Ray Dillinger 
@_subject: [Cryptography] Whitening Algorithm 
All this is true, but if you are implementing it for a device
that natively speaks 8-bit bytes (which a lot of embedded devices
do) it ain't bad.  And it's among the easiest cryptographic things
for a programmer to get right.

@_date: 2015-07-24 23:19:50
@_author: Ray Dillinger 
@_subject: [Cryptography] Whitening Algorithm 
It retains the protocol particulars that RC4 had prior to its major
breakage - Initialization takes some time so it's got poor key
agility, should be used with an IV because it's a stream cipher
and becomes broken if a full key is reused, etc.  Those problems.
Not (at least not yet) the cryptographic break problem.

@_date: 2015-07-25 02:43:20
@_author: Ray Dillinger 
@_subject: [Cryptography] Whitening Algorithm 
I dislike using stream ciphers in general because they
don't achieve diffusion; that complicates key management
and introduces possibilities for making encryption
mistakes.  So I evaluate Spritz as a Random Number
generator rather than a cipher, and find it ... okay.
There are faster ones on desktop hardware of course,
but they're easy to get wrong if you're coding.  I
like Spritz' simplicity, because that makes for robust
If you do the fiddly key management and IV things
correctly to use it for a stream cipher, you can do
that.  But I treat it as a simple, robust CSPRNG.
I would be okay if confident that it's well initialized,
using it to generate a key for a solid block cipher.

@_date: 2015-07-29 10:41:50
@_author: Ray Dillinger 
@_subject: [Cryptography] Windows...  Your choice but make it informed. 
What Microsoft is up to these days...
Short version:  "All your data are belong to us."
This is about Windows 10, not the one you probably
have installed on your machine now. Anyway it's where
MS is apparently going.
Crypto Relevance: If you turn on device encryption,
the new Windows automatically encrypts the drive and
backs it up to Bitlocker.  And then backs up your
Bitlocker key to your OneDrive account.  In the clear.
And their new "non-privacy agreement" gives them
permission to vacuum up and use whatever is on your
OneDrive account.  What could possibly go wrong?
The rest of this is privacy-relevant, not specifically
cryptographic.  But it shows what your cryptography
protects (or more accurately doesn't) in the new
When you log in, it immediately syncs all your settings
*and data* to the company's servers.  Including such gems
as browser history, saved app data, documents, mobile
hotspot passwords, wi-fi network names and passwords....
Also, there's a virtual assistant. Remember Clippy?
It's gotten much better.  Now it's called 'Cortana'
because everybody got used to hating it when it was
still Clippy.  Know why it's gotten so much better?
Because ....
from the new non-privacy agreement....
... and more.  Don't you just love that?  "And more."
Geez, what else could they possibly collect?  You know
what?  I think I still hate Clippy.
I also like the bit about Windows generating a unique
"advertising id" for every user on every device, and
automagically sharing it with every advert you encounter
online, in email, or anywhere else.  Just, you know, so
every advertiser in the world, and anybody else who
accesses that service, knows exactly who you are and
exactly what other websites you've ever visited,
whether or not you delete cookies or do any of that
other inconvenient "privacy" behavior.  Don't you just
love knowing that when you access your company website
all the advertisers (and your boss too if she cares)
will know about the porno site you visited last
Wednesday?  Don't you just love the idea of the
"personalized" ads that might pop up during a demo
of a new web app, when everybody in the conference
room can see your screen?
Uh, guys?  Considering as Windows users no longer
control their own passwords, even their wi-fi and
network passwords, and pretty much can't no matter
how sensitive anything they're working on might be
without going through a lot of pretty non-obvious
contortions and registry editing and settings changes,
which judging from past behavior will probably get
silently undone during some service pack or patch
or other "upgrade", from now on Windows users get
their own segment on my networks.  I'm firewalling
them off from everything important.

@_date: 2015-07-29 16:52:08
@_author: Ray Dillinger 
@_subject: [Cryptography] Graphs for asymmetric crypto? 
Zamir Bavel published a tractable method for finding isomorphisms
in automata (directed graphs in which transitions are driven by
inputs) some years ago.  In general directed graphs (in the
absence of inputs that drive transitions a ) I think it's
probably the best available algorithm but it's still at least
potentially exponential.  In practice though?  Usually faster.
You have to construct some deliberately pathological graphs to
get any exponential behavior with a "base" higher than about
I was in one of his grad classes at University and we covered it
there. It was in this book:  (our text for that class)
He labeled it "Introduction" but don't kid yourself. It's pretty
chewy, even for grad students. I think that these days it's
considered the Standard Reference on automata theory.  It used
to be two volumes, one for graph theory and one for automata;
I think the one for sale now might be both volumes bound in a
single cover. The page count given would support that theory

@_date: 2015-06-01 13:05:38
@_author: Ray Dillinger 
@_subject: [Cryptography] If diffusion is perfect how much confusion do you 
(warning: cipher design post)
Let's say, in theory, that I have a "perfect" diffusion mechanism
that can be defined for blocks of many different sizes.  I'm denoting
it as a "D-box."  I think that I may ACTUALLY have one (defined
using modular multiplication and some bit-slicing), but right now
I'm not asking whether mine is actually a perfect D-box; I'm asking
what the value of a perfect D-box would be.
If I send a block of data through a D-box, the result is an output
the same size as the block, in which every bit of input has an
absolutely equal and nonlinear influence on every bit of the
output.  Flip any nonempty subset of input bits, and roughly half
of the output bits will change.
But this mechanism which provides perfect diffusion, provides no
confusion whatsoever.  Like P-boxes, it is trivially reversible,
and anyone given the whole set of output bits can easily derive
all the input bits. I'll denote the reversed D-box as a "Xob-d".
Now, I believe, but could be wrong here, that if you have such a
"perfect" diffusion mechanism, you need very little confusion to
create a secure cipher.
in catenative notation, using no S-boxes whatsoever and xor as
a trivial, reversible method of combining message with key, I
can't think of a single attack that applies to this utterly
simple, fast, three-and-a-half round construction:
M > ^key | D-box | ^key | D-box | ^key | D-box | ^key = C
And the decryption is in fact the very same operation using the
reversed D-boxes:
C > ^key | Xob-d | ^key | Xob-d | ^key | Xob-d | ^key = M
So far my cipher design ideas have fallen into very few categories:
A) Sent to /dev/null when I figured out how to break them.
   (by FAR the majority)
B) Broken by people smarter or more knowledgeable than me.
   (relatively few)
C) secure (provably as secure as some off-the-shelf
   PRNG or other cipher I used as a primitive) and with
   interesting properties, but too slow or requiring too
   much state to be advantageous over existing ciphers.
   (mostly thought experiments)
D) Built out of off-the-shelf components and suitable for a
   protocol with odd requirements but too specialized and
   not fast enough to be of general use (what people very
   occasionally pay me for doing).
But I think this is different.  This is a construction I'm
actually confident in for mathematical reasons, and it's
fast, and it requires very little state, and it's general,
and it could be ridiculously fast (and small) in silicon,
and it's so simple that it would be easy to get right.
Am I hallucinating here?  Can anybody put this firmly in
category B, or should I start writing a paper and some code
to submit for the next design competition?  Surely this is
too simple to've not been considered and debunked by someone
smarter than me.

@_date: 2015-06-01 20:22:39
@_author: Ray Dillinger 
@_subject: [Cryptography] If diffusion is perfect how much confusion do 
... re: "D-box" as a perfect but reversible diffusion
    mechanism and the construction....
I rejected one-round constructions because of known-plaintext attacks
recovering the key (same problem as one-time pad or stream cipher),
and rejected two-round constructions because of meet-in-the-middle
attacks using a known plaintext, and rejected 3-round constructions
with a D-box at either end because D-boxes are defined as trivially
reversible so they'd add no security, besides which XOR is
ridiculously cheap.
Which is how I wound up with the odd 3-and-a-half round construction.
"Secure" in one round would, I believe, apply only to an adversary
with no oracle and no known-plaintext attack, which leaves some
very sharp edges exposed to any mistake in protocol or use. The
3-and-a-half is secure against a far more general class of
And in the last few hours, I've actually implemented and tested
recursive D-box and Xob-d definitions which are valid for blocks
of every size which is a power of 2 greater than 8.  Although I
don't know why anybody would want an 8-bit cipher, I *HAVE*
sometimes wanted a cipher with a 32-Mbyte block size.   And now
I have one.  :-)
One drawback, I guess, is that my implementation does require
up to twice the block size as working space.  But that's no
worse than most; I can live with it.
Now I'll go read up  about Even-Mansour, and see if he came up
with the same D-box construction I did.  Thanks for the pointer,
although I'm disappointed that someone else came up with it first.
I was hoping to finally publish something.

@_date: 2015-06-01 21:16:29
@_author: Ray Dillinger 
@_subject: [Cryptography] open questions in secure protocol design? 
I'm going to claim ignorance rather than FUD.  As I said, I've
been not using windows for a LONG time.  I believe the last
Microsoft OS I used windows NT. Or, wait, there was one place
I worked that was still using Windows 98 after that.  But I
haven't had any Windows installed on any of my own daily-use
machines for well over a decade.
So, the problem I was thinking of was introduced with XP, not
after.  What then is the compelling disincentive preventing
people from moving to a newer OS?  Is it still resource
hoggery? I remember a lot of complaining about that at a
major Windows release sometime in the last few years, and
a lot of places that didn't want to change OS's until their
computer replacements came around, either on schedule or
as old machines died.

@_date: 2015-06-02 14:17:56
@_author: Ray Dillinger 
@_subject: [Cryptography] Model K 4 Rotor Enigma Machine 
Wow.  That's in perfect condition, it's one of the rarer models,
and it has all the bits, even the original manual.  Somebody
who isn't me is going to pay a lot of dough for that.

@_date: 2015-06-07 12:14:39
@_author: Ray Dillinger 
@_subject: [Cryptography] let's kill md5sum! 
We're already trying to stop it from spreading.  The relevant
section from the md5sum man page already says:
So anybody who uses it in a new application is making a well-
known, documented, mistake.
Killing it is a harder proposition because it involves getting
rid of existing uses with an entrenched set of applications.
But Zooko is absolutely right; painful or not, hard or not, it
does need to be done.
Probably the first major step that need be taken is to get
the utilities that the operating system depends on fixed.
Once that is done, it will become possible for a linux distro
to simply NOT INSTALL md5sum by default.
Debian's licensing policy strongly deprecates anything whose
license is "non-free" as they understand freedom, making a
hard requirement that nothing installed by default may depend
on it.  A very similar "security policy" should be written and
used regarding anything that depends on known-broken features
like MD5sum.  "Insecure" is at least as bad as "non-free."

@_date: 2015-06-13 12:52:40
@_author: Ray Dillinger 
@_subject: [Cryptography] let's kill md5sum! 
But the problem with 'hiding' the hash algorithm behind a toool
named hashsum is that if the algorithm behind it ever changes,
then a bunch of big userland software archives, repositories,
filesharing systems, and databases will immediately break.
Breaking existing userland stuff isn't something you can fix
by hiding the change behind a generic name suitable for scripts
etc.  Any change means that all the existing checksums are no
longer good, and all the data in those vital applications is
suddenly useless.
What's needed is a way for migration of userland applications
from one hashing algorithm to another to happen.  That means
additional functionality has to be added to all that database,
archive, and repository software:  It needs to be able to take
a one-time command to replace (all and _only_ the correct)
checksums of the current algorithm, with checksums that are
correct according to the new algorithm.
And executing that command?  On large databases, trying to do
that all at once could take DAYS of downtime they don't have
to spend.  So this is non-trivial, because it needs to happen
as a background process and the software has to be able to
keep track of what's changed and what's not so it knows how
to check which checksums.

@_date: 2015-06-16 14:16:29
@_author: Ray Dillinger 
@_subject: [Cryptography] Round Robin Cipher Design Group 
I would like to form a mailing list whose participants design
and implement cryptographic code and then break each other's
We will work on encryption, signing, authentication, privacy,
anonymity, algorithms, protocols, applications of cryptography to
data at rest and data in flight, design challenges, etc - and then
let everyone on the list who has any idea how to tear into our
proposals, rip those algorithms, protocols, and proposed
applications to shreds.
This is primarily a study group devoted to learning cryptographic
and cryptanalytic techniques, improving our ability to write
papers about them using standard semantic notations, and improving
our cryptographic design abilities.  It is not likely (or at least
not anytime soon) to result in proposals that see widespread
adoption anywhere, but should at least educate the participants
about the "simple" mistakes in cryptographic engineering that
are so easy to make, without putting massive infrastructure at
risk for the sake of our education.  The hoped-for result is that
in the future, if and when we *do* need to design and implement
such things, we will have the chops to design and implement them
If anyone is interested in joining such a study-and-practice
list, please reply.  If anyone knows additional cryptography
interest groups or lists where people interested in joining such
a group might be found, please forward this message.
This will be a moderated list, allowing posts from members only.
Messages which are off-topic, content-free, political rather than
cryptographic in scope, smell like spam, contain advertisements,
or appear to be deliberately offensive will be blocked.

@_date: 2015-06-17 11:56:53
@_author: Ray Dillinger 
@_subject: [Cryptography] password fatigue;  was: Lastpass 
SRP is a way for phishers to leverage a single password retrieved
from you by whatever means, into access to pretty much every site
you go to.  That's all you really need to know.

@_date: 2015-06-17 12:19:28
@_author: Ray Dillinger 
@_subject: [Cryptography] Round Robin Cipher Design Group 
directly to Ray and not to the list, I will be winnowing replies. Once
the list is all set up, I will happily let the announcement go thru on
this list.
Well, I guess this is the announcement.
I've been startled at the enthusiastic response to what I
thought was going to be a tiny and highly specialized list
announcement.  More than twenty people have already indicated
their interest and/or subscribed, and I expect a "second wave"
when word reaches more lists and communities.  So it looks
like this is going to be a real thing, and I'll do my best
to keep it working well.
The mailing list 'Crypto-Practicum' is now up.  You can
subscribe, or manage your subscription, at
In case it wasn't clear from the original message, the mandate
here is not limited to algorithms and cipher design only; it
includes implementation strategies, application design, and
integration as well. I mentioned basic algorithms specifically
because most cryptography communities tend to treat them as
either off-topic or as mere curiosities, because "Amateurs
shouldn't ever design ciphers, they'll get it wrong."
Crypto-practicum specifically intends to NOT treat them as
off-topic, because doing it and getting it wrong is really
the only way to learn how to do it and get it right.
So, Crypto-Practicum is about all aspects of designing,
implementing, analyzing, breaking,and fixing pretty much
anything that needs to be secured with cryptography.

@_date: 2015-06-30 12:03:58
@_author: Ray Dillinger 
@_subject: [Cryptography] Dilbert on WiFi password security 
============================== START ==============================
Why?  Did you forget what "trusted" means?
"Trusted" means that if there's anything wrong with it, you're screwed!
Is it easier to understand now?

@_date: 2015-03-10 21:52:28
@_author: Ray Dillinger 
@_subject: [Cryptography] Securing cryptocurrencies 
It is hard to create something that meets the needs of block
chain security which is also more generally useful. Cooperation
between nodes is limited both by bandwidth and by the fact that
the nodes are fundamentally competing.  Solutions need to be
easily checkable but difficult to find.  They have to be subject
to a measurable degree of successfulness or difficulty because
block chain security requires a meaningful difficulty adjustment
to regulate its block rate.  The problems that are being solved
must be unknowable until the previous block is published, because
otherwise someone could subvert block chain security by preparing
an 'attack sequence' of blocks in advance,
Simulation problems are mostly too long-running (meaning the
fastest computer wins *ALWAYS* rather than just at a rate
proportional to its speed advantage) and as hard to check as
they are to compute.
There is a cryptocurrency secured by an algorithm for finding
large prime numbers, but large prime numbers (with orders of
magnitude many times the order of magnitude of primes that are
beneficial for cryptography today) are only marginally useful.
What other useful calculations yield the properties needed for
block chain security?  Factoring is out, because for the problem
to exist there needs to be someone who knows what the factors
are before the contest starts.  Protein folding maybe?  But where
does a useful protein folding problem come from in the context
of being unknowable until the previous block is revealed and
having a solution verifiable in terms of the block chain?
What class of problems is "bottomless" in terms of being
capable of producing useful instances to solve, easy to check
but hard to find solutions for, has uniformly distributed
probabilistic solution time, has solutions gradable on
degree of correctness or can produce problems of graded
difficulty on demand, where the next instance to solve can
be revealed or specified by the previous block, and where the
finder of the previous block does not get to pose a special
form of the problem giving himself (or herself) an advantage
in finding the solution?
I agree with you in principle about the undesirability
of converting electricity into heat and otherwise-useless
hashes, but I just can't come up with anything.

@_date: 2015-03-12 21:32:19
@_author: Ray Dillinger 
@_subject: [Cryptography] Securing cryptocurrencies 
The usual problem with proof-of-stake systems is that in a chain
fork, the attacker has stake on both sides of the fork.  He can
use this stake on both branches of a fork rather than having to
commit a finite resource (such as hashing power in the Proof-of-
work coins) to one branch or the other.
The usual method of determining proof-of-stake (as "coin days
destroyed" or similar) allows the attacker to manipulate the
timing of his double spends on the attack chain to generate more
stake than the spends of the same coins on the main branch, or
to spend his coins (to himself) multiple times generating more
proof-of-stake each time.
Finally, the usual protocol has a weakness in that the
transactions made on one branch of the fork, unless specific
conditions are met which ordinary users will never meet, can be
replayed into another branch of the fork and contribute to stake
in that branch - meaning nobody other than the attacker chooses
any preferential generation of stake on one branch or the other
because they're making transactions that can be included on both
sides.  In fact, if the attacker moves some transactions to a later
block of the attack branch, they typically generate more stake
in that position, so the attacker can manipulate the priority of
the attack chain in another way by using a replay attack.
I have dreamed up a potentially workable protocol variation
which hybridizes proof-of-work with proof-of-stake, addressing
some of these problems.   It uses transactions as proof-of-
stake, but addresses the replay attack by having each transaction
'stake' a recent block in the block chain it is contributing
security to.  Transactions are not valid in any block chains
in which the staked block does not appear. Thus, an attacker
cannot replay transactions that staked a block he wants to
disappear in a reorganization, into the same chain he intends
to replace it with.  The 'stake' of a  block chain branch (ie,
the sum total of txouts that existed before the fork and are
used in tx that are staked in a block after the fork) are
used for choosing between branches in a proof-of-stake
The downside of this, of course, is that now staked transactions
are annihilated by any chain reorganization that occurs before
their staked block.  But it should be very hard to achieve such
a reorganization without relying on the replay attack to get
stake support from other users' transactions and also unable
to manipulate the 'stake' generated by your own spends by spending
them earlier or later or more times in one branch vs. another.
This makes it impossible to prepare a *long* attack chain in
secret - everybody else will be staking all transactions in
the chain they can see, so unless you have a substantial
fraction of the money, your would-be attack chain will have
stake that's pathetic by comparison to the total volume
everybody else is doing.
But it's not so good for preventing *short* reorgs, because
spending is, in the very short term, a lot more bursty and
occasional than the constant, steady effort of hashing which
remains nearly constant at the miner's capacity.  As a
result, using spends to determine branch priority is very
sensitive to the exact timing of large spends.  When combined
with the possibility of annihilating staked transactions by
removing the staked block in a reorg, this opens the field
to a class of short-term timing attacks that are better
prevented by proof-of-work systems.
So this type of proof-of-stake protocol could work at large
scale (where the law of large numbers smooths out the volume
of transactions or the rate of flow of money to something
steadier) but in smaller scales would have to be part of a
hybrid protocol with proof-of-work.

@_date: 2015-03-13 11:57:31
@_author: Ray Dillinger 
@_subject: [Cryptography] Securing cryptocurrencies 
Nope.  Chaining many unconfirmed transactions in the same block
will only spend txouts that existed *BEFORE* the staked block
once - hence will add priority for those txouts only once.
The observation is that txOuts that existed before the fork are
the only finite resource that is limited in a useful way.  The
protocol I advanced values only those txOuts for stake, and
values them each at most once in each branch.  The problem
addressed was in fact *specifically* to remove the ability of
attackers to manipulate spends of the same stake to contribute
more priority to one branch than it contributes to another.
And yes, TaPoS alone (in whatever variant) is subject to short-
term forks which someone can DoS the system by cultivating.
That is why, as I already said, it needs to be a hybrid system
with proof-of-work until it reaches the kind of very large
scale where the law of large numbers smooths out the rate of
tx and spending.

@_date: 2015-03-13 17:51:04
@_author: Ray Dillinger 
@_subject: [Cryptography] IBM looking at adopting bitcoin technology for 
I think that the "who has control of the ledger" question is a serious
hot potato when central banks deal with each other.  This isn't about
their clients and businesses, this is about the fact that they don't
trust each other and want an open ledger that no single one of them, nor
any of the governments they work with/for, can control.
So, no, I don't expect the businesses and ordinary people to ever
see it; this sounds to me like a settlement system to be used
internationally between central banks.

@_date: 2015-03-14 14:21:11
@_author: Ray Dillinger 
@_subject: [Cryptography] Looking for sequential-memory-hard hashing 
I have an application that requires a proof-of-work function
depending on some data plus a nonce - (possibly a hash but
needn't have all the properties of a good cryptographic hash)
which needs to be fast to verify on all machines, but
drastically harder to compute on machines with small memory
than on servers with large memory.
The best I've come up with is a "triplet birthday search" -
a search for 3 nonces that yield a collision in the first
N bits under some conventional hash function.  (I think N
is around 30 bits).
It's memory hard because an efficient implementation
requires a large (2^N entries) cache of nonces indexed by
hash to efficiently find triplets, and triplets don't
invoke the birthday paradox nearly as quickly as twins
do. Any implementation with a smaller table should be
able to find triplets at a speed that depends on the
ratio of their table size to the size of the whole table.
I can limit the effect of ASIC parallelism by forcing
parallel threads to have contention for locks in the table.
This can be accomplished by requiring that the nonces be
within a limited numeric range of each other - the proof-
of-work would be presented as A,B,C, where A is a 64-bit
number, and B and C are 32-bit "offsets" from A. That
will require constantly updating the table so that its
entries stay within the set numeric range.
My question for the list is, can anybody think of a way
to make this worse for ASICs and small-memory systems?
Can I force more contention for the table?  Is there
any construction anyone is aware of that's better than
a triplet search?

@_date: 2015-03-14 18:58:56
@_author: Ray Dillinger 
@_subject: [Cryptography] Fun and games with international transaction 
currencies)
Bitcoin is essentially protocol over policy.  Some of the advocates
think that since nobody can be trusted to set policy, protocol is the
best a cryptocurrency can do.  The ideologues claim that anything
resembling policy or political input into monetary policy is
inherently evil and that protocol is superior because it is "fair."
The ideologues have forgotten the endless, predictable, painful
boom/bust cycles of the eras in which there was no such thing as
modern monetary policy.  While it's true that nation states still
mess up occasionally and produce a hot steaming deposit, those
of us who do not live in kleptocratic states are generally better
off for having a government that sets monetary policy, including
mild inflation.  By contrast, Bitcoin's monetary policy is a
dead hand on the tiller - it does not foolishly steer toward
cliffs as kleptocracies and noticeably stupid nations sometimes
do, but it will not change course either, regardless of whether
its economy is heading for a cliff.
Ultimately, the dollar is backed up by the productivity of the
American people and the ability of the government (via exercise of
its monopoly on legitimate use of force) to tax them.  There is no
productivity whatsoever associated with gold, or Bitcoin, or any
other asset which is valued only because of its rarity. No agency
is able to implement anything like monetary policy with respect
to it in order to protect participants from a boom/bust cycle,
or to stabilize or uphold the value of Bitcoins - hence, even
in the best case of complete adoption, wild fluctuations in value
relative to government-issued currency can be expected.
Also, it's a bit amusing to hear the cryptocurrency people
referring to government-issued fiat currency as "fiat" and not
noticing that Bitcoin itself is also fiat.  The difference is
only whether it is the issuing governments or the issuing
users whose fiat power (the ability to create value by simply
asserting that something has value) is being exercised.
And it is at this time still an open question whether someone
can meaningfully have fiat power at all if they do not have
the ability to tax or otherwise derive value from broad-based

@_date: 2015-03-15 23:08:15
@_author: Ray Dillinger 
@_subject: [Cryptography] Looking for sequential-memory-hard hashing 
It looks like the "momentum function" is very much the same thing
as the "triplet search" I had proposed.  This is good, because it
means it's in published literature and I can search published
literature for attacks on it now. I'm a little worried about
susceptibility to some application of Bloom Filters for example.
The cuckoo cycle construction -- wow that's neat.  Nice algorithm
for it, supralinear memory-to-difficulty curve, sequential
memory hard, and even more contention for memory bandwidth and
write locks on its table than the triplet birthday/momentum
hashing.  It's a *LOT* heavier to compute though.  Hmmm.  I'll
have to run simulations and see how much of a burden it is.
There is a unifying notion that these memory-intensive algorithms
have in common.  What we're all doing is looking for sets of hashes
that have some relationship to each other, where the appearance
of the new members of that relationship is not predictable.  That
requires saving previous results because new results are equally
likely to join *any* of the already-computed results to make the
relationship.  But, wow, I was only looking for sets of three.
Tromp's Cuckoo cycle construction is looking for a similarly
precise relationship on sets of forty-two.
"Kitten hockey has rules that every kitten understands.  They are
subtle, but we humans at least know that goals are scored when
things go under furniture where they cannot be retrieved."

@_date: 2015-03-17 21:14:06
@_author: Ray Dillinger 
@_subject: [Cryptography] Kali Linux security is a joke! 
I see that they also haven't heard about cryptographic attacks on MD5.

@_date: 2015-03-20 16:53:26
@_author: Ray Dillinger 
@_subject: [Cryptography] FFS 
I feel compelled to ask what purpose is served by the adjective placed
before the term "Forward Secrecy?"  Because I've been preferring to use
it with no adjective for as long as I've been using the term.
The only circumstance in which an adjective is needed is in describing
partial forward secrecy - and in that case the 'default' form of forward
secrecy still needs no adjective.

@_date: 2015-03-24 20:41:02
@_author: Ray Dillinger 
@_subject: [Cryptography] NSA Pre-Releases 24 William Friedman Docs, 909 pp 
That is awesome.  Friedman is absolutely the final authority on
"classical" or pre-computer cryptography.  I have his "Military
cryptanalysis" books (5 volumes, tho each is sort of thin) and
they really are a complete guide to breaking virtually every kind
of pen&paper cipher that was in use at that time.
This looks like a lot of the same material, (some of it in draft
or pre-release versions) plus a lot of context about the war and
other circumstances that motivated it.

@_date: 2015-03-24 21:02:52
@_author: Ray Dillinger 
@_subject: [Cryptography] "Most Americans Don't Mind Being on Candid 
This.  Privacy as we knew it is a memory.  Information is too easy
to record, transmit, organize, and search for there to ever be real
privacy again.  But there can be real fairness in sharing it, and
at this point I think that's the only possible thing that could
preserve democracy.
Information which is available to those in power needs to be
available to everyone, in exactly the same way that democracy
demands that every kind of power be shared with the people whom
it is power over.  If the police can record you, then you can
record them.  If they can demand copies of your video, you can
demand copies of theirs.  And if they get to go through your
email looking for incriminating crap, you get to go through
Cameras that record the public should have URLs printed on them
that allow any member of the public to download their video
stream.  If the government (who at least in theory serve the
public) can use it, and public taxes pay for it, then that
information is a goddamn public resource.  So, if a crime is
committed against you in view of a camera, you get to download
the footage yourself, whether immediately or the following day.
Even if the crime is committed by the police themselves.
And if a recording of you in that video stream would result in
your arrest and be used in court against you should you do
something, then a recording of the mayor or the police chief or
your senator doing that same thing in that video stream should
have exactly the same result with respect to them. And every
last member of the public needs to have the same ability that
they have to search for such recordings and bring them to
Otherwise democracy dies right alongside privacy.

@_date: 2015-03-24 21:13:15
@_author: Ray Dillinger 
@_subject: [Cryptography] "Most Americans Don't Mind Being on Candid 
I don't make any distinction myself.  Aim a gun at your
ex-wife's head and pull the trigger?  One count of murder.
Aim an airliner with full fuel tanks at the world trade
center and push the throttle forward?  Several thousand
counts of murder.  Plan, fund, and train people to do
several instances of that very act?  Several times several
thousand counts of murder.
Murder isn't political.  It isn't "war" unless it's a
dispute between nations.  Random yahoos with some islamic
jihad, unless they're acting in support of a specific
nation, and on orders from or with the support of a
specific nation, are merely ordinary criminals (albeit
sometimes on a scale that makes Ted Bundy, David Berkowitz,
and Charles Manson look like a bunch of boy scouts) and
should be dealt with as ordinary criminals.

@_date: 2015-03-25 09:01:21
@_author: Ray Dillinger 
@_subject: [Cryptography] How to crypto secure speed limit signs 
Spoofing could be a real problem.  Although I haven't heard of
any cases where somebody made trouble by replacing a speed limit
sign yet, most people simply ignore speed limit signs that are
obviously wrong.  If it's a six-lane interstate highway with
miles between offramps and no construction, a human would know
darn well that a 20-MPH speed limit is a fake.  An automatic
system?  Not so much.  It'll take at least a couple instances
of some joker putting up a 20-MPH sign on a highway just to
see the ensuing chaos before Ford gets the "ignore speed limit
signs when...." rules built into their systems, along with a
GPS database that tells the car what the real speed limits are.
It should be easier to build speed-limit information into
the GPS database that all the new cars use as a map.  I would
think that actually relying completely on the sign would be a
temporary measure until they can reliably get timely info
about changes in speed limit into their geographic database.
I would totally like to have that system on my car.  I haven't
had a speeding ticket in years, but I'd be happier knowing I
didn't need to worry so much about getting one accidentally.
And it's just making another baby step toward the self-driving
car.  We've been taking those steps for a while now; new
commercial trucks, and some cars, already have forward and
backward looking radar with automatic braking, and lane sensors
that can sound an alarm if they're drifting out of their marked
lane.  A few more increments like this and we'll be getting
to commercially available self-drive systems.

@_date: 2015-03-25 21:37:56
@_author: Ray Dillinger 
@_subject: [Cryptography] "Most Americans Don't Mind Being on Candid 
I think it's not okay to give these clowns the respect that enemy
soldiers deserve.  That's my main objection to classifying these
murders as a war.
But this drifts far afield from crypto, and if we go much further
I fear I will be prone to getting ranty.  So, we should stop now.

@_date: 2015-03-27 17:55:49
@_author: Ray Dillinger 
@_subject: [Cryptography] Drop Zone: P2P E-commerce paper 
Your first mistake is in thinking that testnet is for the purpose of
persisting messages; it is not.  Testnet gets rebooted from time to
time - most recently when it came to the attention of the devs that
people were buying and selling Testnet coins.
Right now most of the Bitcoin devs are trying to avoid activities
that make them look shady, and testnet is much more closely associated
with the devs than bitcoin itself. if a type of business starts
being done via testnet that threatens the legal status of Bitcoin,
you may expect them to gimp testnet immediately.
Your second mistake is in thinking that nobody's monitoring bitcoin
or testnet to see exactly where each transaction originates, how
combining coins reveals the other coins in the wallet, what goes in
and out of mixers, and how all of these clues correlate them with
real-world identities.  Trust me when I say there are well funded
people doing exactly that.  In fact it would behoove you to assume
that a lot of the Tor nodes and coin tumblers are run by such
people strictly for the purpose of information gathering. Bitcoin
and testnet are by no stretch of the imagination anonymous.
Your third mistake is in thinking that anything which is visible
to random people who want to participate in it, is invisible to
law enforcement.  You must know that some fraction of the people
who participate in this, will be law enforcement people specifically
looking for people to arrest.
If you want a short radius from a user's geographic location that isn't
closely monitored, you should be using encrypted packet radio rather
than the Internet.  With packet radio they at least have to send
a human to triangulate to see where you're originating.  But be aware
that the FCC frowns on encrypted radio channels.  Maybe you could work
over bluetooth or wi-fi?  They're not nearly as heavily monitored
as the Internet itself.

@_date: 2015-03-31 10:47:34
@_author: Ray Dillinger 
@_subject: [Cryptography] Fwd:  OPENSSL FREAK 
Honestly, this was exactly the scenario I had in mind when I proposed
implementing "death notes" for ciphers.
It was a simple idea then, and is still simple.  A death note is
simply a proof that the encryption has been broken, (such as, in
this case, a cert issued by a known-bogus "negative cert authority"
whose keys were publicly destroyed immediately after creation).
Everything that gets the death note (and has implemented the
feature, sigh) responds by permanently disabling that crypto
primitive (in this case erasing all certificates that use that
cipher), permanently storing the death note, and thereafter
passing on the death note to anyone who later tries to use
the dead crypto primitive.
Seriously.  This is fairly simple to implement.  Yes, it would
cause outdated servers FROM TWENTY YEARS AGO to stop being
compatible with anything that has the death note protocol
implemented.  That isn't merely an acceptable loss, that is in
fact the desired result.

@_date: 2015-05-04 14:50:01
@_author: Ray Dillinger 
@_subject: [Cryptography] "Trust in digital certificate ecosystem eroding" 
Absolutely true.  I worked for several years at a company
where all the machines on the local network had installed
versions of browsers etc that knew exactly one root CA -
which was managed by the company's firewall.
Literally all encrypted traffic was decrypted "for log
maintenance purposes" at the firewall, re-encrypted and
sent onward to the machines on the company network.  All
requests for a certificate were intercepted at the firewall
and got a certificate auto-issued by the firewall's own CA,
which it would then use to re-encrypt that traffic.
All this of course was done in the name of "security..."

@_date: 2015-05-04 15:15:59
@_author: Ray Dillinger 
@_subject: [Cryptography] "Trust in digital certificate ecosystem eroding" 
If it's ever going to work automatically, then it has to be
semantically consistent enough for automated systems to make
meaningful decisions about.
What we're used to building as engineers - indeed, what we strive
to build as engineers - is systems where the same technical means
can be used to express many different kinds of semantics.  But
that deprives any automated system of knowledge of what semantics
are being expressed.  And trust is all about semantics.
The things we can monitor automatically - that is, when a site
changes CA's, how many CA's assert that a site's certificate has
been paid for, whether we are seeing the same cert for a site that
other clients see, etc...  are semantics-free.  Monitoring them
does not give us means to make trust decisions, because every one
of those things could arise for reasons having nothing to do with
whether a particular web site ought to be trusted.
If you want an automated system, you have to make a system where
particular technical aspects which can be monitored (and which
can't be selected for convenience by an attacker) have definite
semantic meanings.   And you essentially can't get there unless
you create a situation in which your network admin sometimes has
to say "no, we can't make X configuration change without going
through six-month notification and migration procedure Y, because
that would invalidate our trust cert."

@_date: 2015-05-07 16:50:44
@_author: Ray Dillinger 
@_subject: [Cryptography] Is there a good algorithm providing both 
In principle, what you want to do is compress, then encrypt,
before you transmit or store.  If you do it any other way, then
any compression achieved is an indication that your encryption
algorithm is broken.
I deployed a compress/encrypt function like this for a
specifically anti-auth compression on English Text in an
encrypted messaging system used by a small organization.
As an anti-auth system, it's somewhat lossy by design to
limit authorship information leakage: other than seeking
double carriage returns as an indication of paragraph ends,
for example, it compresses any whitespace to a single space,
strips oxford commas, smashes CamelCaps, coalesces/compresses
common alternate US/UK spellings and common misspellings,
It's VERY good compression for English Text; most words
and many common whole phrases become single code numbers.
But that's partly because of the anti-auth features.  And
of course if makes complete hash of ASCII art signatures,
expands UUEncoded gif images or Base58 keys by a factor
of three or so, etc.
It's not much of a win IME for performance purposes unless
bandwidth is precious; the design constraints are such that
you have to do compression and encryption as separate
computational steps anyway so you're not saving CPU cycles.
What you're saving is I/O steps and bandwidth.  You can
implement them in the same module mainly to enhance security
somewhat by NOT storing the compressed-but-not-yet-encrypted
form while writing/sending or the decrypted-but-not-yet-
decompressed form while reading/receiving.
There are a couple of different approaches you can take
to make a more full-featured/bit-preserving function:
First, the decompression dictionary(ies) may be a shared secret
with you and your recipient, making it a very long low-entropy
part of the key.  Possibly the message itself, in this case, has
occasional "control codes" inserted that suspend/enact different
parts of the decompression dictionaries/algorithms, so that for
example PNG image bytes are compressed differently from rich-
text bytes are compressed differently from plain-text bytes.
This is functionally equivalent to rolling MIME types with
compression method for each type, and then encrypting the
resulting byte stream.  Compression happens before encryption,
or else Eve can read your control codes in the transmitted
stream and deduce a lot of information about what you're
Second, the decompression dictionary may be "contextual" - either
transmitted in encrypted form as a sort of extended IV, and/or
transmitted along with the encrypted/compressed content via the
use of occasional "control codes" in the decrypted stream that
modify, add, and replace as well as suspend/enact various aspects
of the compression dictionary on the fly. Decryption also needs
to happen before decompression here, for the same reason: If
the control codes or IV are not also encrypted, then Eve can pick
them out of the stream and deduce a lot of information about
what you're transmitting by the kind of decompression functionality
it needs.  Assuming you start with a reasonably complete shared
"dictionary," this is really essentially the same as the first
scenario except that the dictionary itself is mutable.  It can
be worth it if you need to be able to add compression methods
for new MIME types on the fly and they correspond to highly
structured compressible formats which aren't in your starting
dictionary.  The drawback is that it is very hard to look
ahead in your input stream to determine whether transmitting
dictionary updates is or is not a waste of bandwidth, without
creating side channels which a savvy opponent can exploit.
I have not seen this done with a "secret compression dictionary"
 - Kerckhoff's Principle usually inspires designers to limit
the secrecy to a single relatively short key so the starting or
default dictionary is known to the opponent.

@_date: 2015-05-07 17:15:22
@_author: Ray Dillinger 
@_subject: [Cryptography] Is there a good algorithm providing both 
In the first place, if compression is perfect, then the
compressed text, like encrypted text, is indistinguishable
from random noise.  A compressor, after all, works by removing
redundancy from the input.  When there is no redundancy there
is no way to tell whether an equiprobable sequence of bits
means one thing or another thing -- only that both things were
within 50% of equally likely to be expressed because they both
mapped to the same bit length.  The fact is that compression
we actually have is never perfect but that's what perfect
compression would mean.
In the second place, while given a practical, not-very-good
compression function that symbol-repetition information may
be available in the compressed text, it _certainly_ won't
be available in the encrypted compressed text unless your
encryption is completely broken when considered separately
from your compression.
There is absolutely nothing wrong with the "compress then
encrypt" construction. As folks have pointed out, you must
never rely on imperfect compression to SERVE AS encryption,
but encrypting compressed text using a good encryption
primitive is not worse at hiding its contents than
encrypting uncompressed text using the same good encryption
We usually accept the opponent knowing approximately how long
a message is because the encrypted message is that long plus
the length of a known-length IV and rounded up to a block
boundary.  So I'm not concerned that message length information
may be only as well hidden as it is with any kind of encryption.

@_date: 2015-05-08 13:24:25
@_author: Ray Dillinger 
@_subject: [Cryptography] Is there a good algorithm providing both 
Sorry, I was missing the point. You're right about that.
If you want to deploy compress-then-encrypt, you have
to avoid adaptive compression algorithms which create
that side channel (detectable adaptation creates a side
channel about repetition).  Which means your compression
needs to be as simple as Huffman Coding with a pre-built
dictionary, like what I did in that anti-auth chat system.
It has a compression that works well for exactly one kind
of data and does no adaptation.
Interesting....  I had a sort of "vague feeling" that
adaptive compression would be a Bad Plan when I was coding
that but hadn't actually developed a fully-expressible
argument of exactly why.  So, ten-out-of-ten for having
"Holistic Zen Understanding" working for me, but that
Zen schtick is absolutely NOT what reliable, repeatable
engineering is based on, so minus ninety-nine for missing
the attack with my fully conscious mind.
Conventional encryption does usually leak message length
information: a 2 Gbyte plaintext file becomes a ~2 Gbyte
encrypted file.  If the attacker knows the length of the
original payload, the length of a compressed payload
reveals how well it compressed - meaning it reveals how
well or badly its contents matched the compression algorithm
in use, and becomes a side channel.  So it matters to NOT
use compression where the attacker knows or can control the
length of the payload - and that eliminates a lot of
uses in protocols.

@_date: 2015-05-09 18:09:50
@_author: Ray Dillinger 
@_subject: [Cryptography] A Fun Trick: The Little MAC Attack 
So this common construction for message authentication codes,
which with vector2 = 0x363636... and vector1= 0x5c5c5c....
constitutes the usual definition of HMAC:
   hash(key XOR vector1) XOR hash(key XOR vector2 XOR message)
allows the ability to create collisions in the hash to be extended
to the ability to create collisions in the MAC, when the attacker
knows the key.
I don't see any significant damage here; if the attacker knows
the key, the point of a MAC function (particularly a symmetric
MAC function in which knowledge of the same key needed to verify
the MAC is the requirement for generating a bogus MAC) is hosed
I have never understood, though, why the above form is preferred
to the more straightforward construction:
    hash(Encrypt(key, message))
.  The only reason I know why the first form was EVER preferred
has to do with crypto export regulations back in the bad old
days when, in certain contexts, certain authorities permitted
hash functions to be part of exportable software but not
encryption functions.
Why it would still be preferred today is a mystery.

@_date: 2015-05-09 18:15:01
@_author: Ray Dillinger 
@_subject: [Cryptography] Is there a good algorithm providing both 
Which, sorta, defeats the initial purpose of compression.
Unless you simply state a priori, "all messages will be compressed
by exactly 30%" or something, add padding if compression is better
than that, and reject messages for which compression is worse.  Now
the only thing an attacker can tell is that the original message
was exactly 30% longer, AND was at least 30% compressible.
Hmmm.  That's actually not a bad construction when working with
data that's fairly reliably compressible (text, etc).

@_date: 2015-05-10 10:48:12
@_author: Ray Dillinger 
@_subject: [Cryptography] A Fun Trick: The Little MAC Attack 
If I map || onto XOR, then (key || message || key) == message, which
makes the use of the key pointless.  If I map || onto OR, then
(key || message || key) == (key || message), which makes the second
use of key pointless.
Was the above MD5(key || message || key) a typo for
MD5(key || message) || key
The above form (even if MD5 were secure which it ain't)
is still brittle as hell in terms of what its security is
applicable to, but it at least isn't obviously broken, and
makes less nonsense than what you typed?

@_date: 2015-05-10 17:45:50
@_author: Ray Dillinger 
@_subject: [Cryptography] A Fun Trick: The Little MAC Attack 
Ah.  I see; they used of a copy of the key on both ends as a
defense against extension attacks.  That makes more sense now.
Unfortunately it still doesn't do a darn thing for colliding-
block substitution in long messages.
Until Unicode characters are a darned sight easier to type, we won't.
There are Unicode characters for most bit operations, and I can be
driven to use them if necessary.  But they're a pain in the tush to
type, and like you I usually use ASCII substitutes.
There is a standard math notation for sequence concatenation
using U+2322, which is named FROWN and written .  This is
decimal 8994.   We could use it for string or blob concatenation,
but it's a pain in the tush to type.
I've used double slash in the past for concatenation but I have
no idea whether that's understood without providing context; I
always feel that I have to explain it whenever I use it.  And
I've seen at least one person use $+ for concatenation, which
I suppose is reasonable if you read "string" for $.
Vertical bar reads as OR to me, but that's probably because
I'm a C programmer.

@_date: 2015-05-12 19:08:48
@_author: Ray Dillinger 
@_subject: [Cryptography] AEAD modes for signed ciphertext 
I note that the current wikipedia article lists Zooko's triangle as
They are wrong.  I am not willing to expend the energy to argue the point.

@_date: 2015-05-13 11:39:09
@_author: Ray Dillinger 
@_subject: [Cryptography] [cryptography] NIST Workshop on Elliptic Curve 
Well, maybe...
How about "The block size is exactly the same as the message
size no matter what the message size happens to be?"
I know, it's a laughable idea when talking about "lightweight"
cryptography.  I can't think of any way to actually do it that
wouldn't take at least LogN times longer than a standard block
cipher with fixed-length blocks. And the idea of encrypting a
"stream" is right out unless you have a higher level of the
protocol breaking the stream up into packets of known size,
in which case you have a standard block cipher again.
But in a brute kind of way, it's very interesting for just
plain freezing out most  of the attack methodologies I'm aware
No block boundaries inside the message, and every bit of the
ciphertext depending on every bit of the plaintext, means
entire classes of attacks just don't have anything to work
It's a random idea.  It may have occurred to me due to lack
of sleep, or because I'd been looking at C++ template code
for a block cipher that takes block size as a parameter.

@_date: 2015-05-13 21:58:11
@_author: Ray Dillinger 
@_subject: [Cryptography] [cryptography] NIST Workshop on Elliptic Curve 
I can think of a few ways to do it that would work and be secure,
but all are too heavy for anything that's supposed to compete with
well-studied ciphers on the basis of speed or gate count.  Here's
the top two in my opinion.
Option 1: A well-studied fast cipher like AES or something, used
twice in CBC mode:  Once from front to back, and then from back
to front at an offset of half the block size. So the second time
through, you start at the tail end and each block you encrypt is
half of two different first-round blocks.  This has the advantage
of being much more conservative design built on the security of
something that's already well studied, and "only" takes twice as
much time as that thing.  The end result is that every bit of the
output depends on every bit of the input.  If you insert
an additional block encryption with a customized amount of
overlap to make it come out even, you get ciphertexts exactly
the same size as plaintexts.
Option 2: A block-ish (one-block) cipher with multiple layers
of S-boxes and between them P-boxes calculated from message
size to achieve complete diffusion as rapidly as possible.
I actually prefer this option from an aesthetic point of
view, but it's got a lot of security questions.
Each layer of S-boxes could increase the amount of diffusion
by a factor of the size of the S-box in bits, so with 8-bit
S-boxes, one layer gives diffusion over 8 bits, 2 over 64 bits,
3 over 512 bits, 4 over 4096 bits, etc.  And to get Feistel's
security properties you'd want at least four times the number
of layers required for full diffusion.
But finding S-Boxes that are secure given any set of P-boxes
computed from the message size (or a way to compute sets of
P-boxes from message size which have properties that preserve
the analysis-resistance of the system) would be original work
which I don't trust my math skills enough to do, so I would
have to go brute-force with it instead of being able to discern
a "minimal secure" solution that would be efficient.
I could in good conscience deploy something like this, if I
use an absurdly compute-intensive key schedule to prevent the
opponent from being able to use any fixed information about
the S-boxes.  I would initialize a CSPRNG with the key material,
then generate ~= 1800 bits from a CSPRNG to create a uniform
choice among all possible 8-bit permutations for every S-box.
It would defy analysis assuming no complete break of the CSPRNG,
but it would be absurdly compute-intensive, and not lightweight
at all. Someone with better math chops than me can probably
come up with a *much* more performant key schedule and/or
set of s-boxes than my ridiculous-overkill plan while not
sacrificing security.

@_date: 2015-05-13 22:08:26
@_author: Ray Dillinger 
@_subject: [Cryptography] [cryptography] NIST Workshop on Elliptic Curve 
It doesn't have to be trivial to break; If you introduce as a
first step a transformation of the data (even a fairly trivial
transformation as long as it's got any nonlinear component)
that depends on both the key *AND* the number of rounds the
subsequent encryption will do, then the information that can
be gleaned from selecting the number of rounds gets a *lot*
harder to get at.
For example, say you decide to do a key transformation, where the
"real" key as known to the user's key management software etc is
K:  If 34 round foo uses F(K,34) internally as its key and 35 round
foo uses F(K,35) internally as its key, where F is nonlinear and
has no "special" properties w/r/t the cipher foo, then the
devastating round-reversal attack he was talking about ceases to

@_date: 2015-05-16 12:20:22
@_author: Ray Dillinger 
@_subject: [Cryptography] NIST Workshop on Elliptic Curve Cryptography 
You have just named the only known intervention by the NSA to
result in stronger civilian encryption.  Literally EVERYTHING
else they've done "for the public" has not been shown to
result in stronger cryptography, and in many cases has definitely
resulted in weakened cryptography.  Their strengthening LUCIFER's
S-boxes to make DES can only be regarded as a singular anomaly
which has never been and will never be repeated.
Further, the improvement in resistance to differential analysis
came at the same time as a reduction in its resistance to brute
force, making DES equally vulnerable to less sophisticated
opponents.  Given that they (and IBM) had the advantage of
differential cryptanalysis at the time this is a direct
violation of their current "NOBUS" policy, and definitely
would not be repeated today.
The NSA, since people were still taking them seriously at the
time, could as easily have extended LUCIFER's rounds to justify
its (pathetic) 64-bit key with 64 bits of security, but they
chose instead to shorten the key to an (even more pathetic) 56
bits reflecting its security level (given its improved S-Boxes)
to an opponent aware of differential cryptanalysis - making it
no more resistant to joe sixpack coding a brute-force attack
than it was to sophisticated opponents using differential
The only real benefit to that was that it didn't have a key
size that actively misled people about its security level,
and the "obvious" attack was in fact the best attack.  Under
the NOBUS policy they would happily have left the key length
deceptively long allowing people to believe their encryption
64-bit secure, probably with the "benefit" of holding off
3DES' adoption for an additional couple of years.
So whatever reputation capital they got for strengthening
DES, they spent long ago. And overspent. Their reputation
on strengthening civilian crypto at this point?  They are
the adversary, not the defender.  That's how deep the hole
they've dug themselves into has gotten.

@_date: 2015-05-16 14:11:44
@_author: Ray Dillinger 
@_subject: [Cryptography] [cryptography] NIST Workshop on Elliptic Curve 
I think I have something.  An algorithm which can be fast in
hardware or software, is adaptable to operations on data of
different power-of-2 sizes (larger powers will be doing mostly
table lookups on precomputed tables to duplicate what smaller
powers would achieve in more steps) and is well-defined on
cipher blocks of any size that is a multiple of 8 bits.  It
can be done using custom hardware on predefined block sizes
using a reasonably small gate count for the round function.
Key agility will be terrible, but it won't be as bad as what
I came up with initially.  Given a block and a key it will
still take at least ten times as long as if given a block
and reusing an already set up key.
I will code it up, write a detailed description, post the
code somewhere, and wait for you guys to demolish it. :-)
Hmmm.  How bad, in theory, would ECB mode really be on
512Kbyte blocks when doing disk encryption?  Because after
setting up the key, 512Kbyte blocks could be encrypted/
decrypted in custom hardware comfortably faster than
disk speed.

@_date: 2015-05-16 17:55:37
@_author: Ray Dillinger 
@_subject: [Cryptography] NIST Workshop on Elliptic Curve Cryptography 
Checking into the history it looks as though several different
ciphers (or the same cipher at different block and key lengths)
have borne that name.
If I remember correctly the one considered in the DES competition
had a 64-bit key.  But I may be confused here.

@_date: 2015-05-20 10:13:41
@_author: Ray Dillinger 
@_subject: [Cryptography] NIST Workshop on Elliptic Curve Cryptography 
According to the histories I've read, differential analysis
was known at IBM.  They called it the "Tickle attack" and
had not published a paper about it because the NSA was
working with them on DES and had asked them not to.
Also there was not much prestige in publishing crypto papers
at the time; as you note, the civilian crypto community was
almost nonexistent. As far as I know cryptography wasn't a
category in which papers were accepted by journals at that
time, although the fundamentals behind a particular new
crypto attack would sometimes get published in a math journal.
It certainly wasn't a category in which papers were sought
or awards were given, nor in which civilian conferences and
symposia were occurring.

@_date: 2015-05-21 16:58:07
@_author: Ray Dillinger 
@_subject: [Cryptography] NIST Workshop on Elliptic Curve 
Well-known fact to pediatricians and family-practice doctors,
but relatively unknown to the general public.  Adults hear up to
about 20 KHz.  More if they've avoided loud noises a lot during
their lives, less if they've regularly attended rock concerts or
used loud machinery at their primary job for any length of time.
But kids have both less accumulated inner ear damage, and
smaller ears.  They can often hear sounds in the 30 KHz range.
Any engineer attempting to design an ultrasound device should
keep this in mind.
Sometimes this has security implications:  I can think of two cases
that have already happened and one that came intriguingly close and
may have been developed more since.
  A few years past there was a ringtone that became popular
amongst the younger set because it was audible to them but not
to most adults.  They called it "Mosquito" as I recall.  And
sometimes used it in very minor security breaches like sending
signals each other covertly in the presence of adults who
attempted to forbid that sort of thing.
In another case, some asshole, not too long before that time,
had come up with a "child repeller" he tried to market, which
worked specifically by making loud annoying noises that most
grownups can't hear.  Fortunately for all concerned, his
business venture failed miserably when it turned out that
roughly 10% of grownups (who had mostly avoided rock concerts
and loud machinery) absolutely hated the loud annoying noises
too, and were repelled from businesses that had tried to
attract people by being free of annoying children.
And I recall at least one science project was undertaken by
a 13-year-old at the local junior high.  He was attempting
to electronically transpose voices up by six octaves where
grownups couldn't hear them.  But when the account I read was
written, he had a device that youngsters could hear, and
identify as speech, but could not understand.

@_date: 2015-05-22 16:06:44
@_author: Ray Dillinger 
@_subject: [Cryptography] I broke a cipher this week. 
Don't get excited:  It was not a major cipher and nothing
depends on it.  And I'm allowed to talk about it as long as I
don't mention the company or the project they had intended to
use it for.
And TL:DR; if you don't want to read details, just consider the
fact that a linear congruential transformation makes a hell of
a good way to distribute 64 bits of output from one layer of
S-boxes to the inputs of your next even if your S-boxes are a
lot smaller than 64 bits.  It's not crypto-secure as an S-box
transformation, but it provides a really good amount of
diffusion, and is computationally cheap on 64-bit machines
relative to the usual bit-slicing, and is massively parallel
on GPUs.
Now, for the people who are willing to listen to me ramble
instead of just reading the TL:DR, a little story.
I had mentioned to a friend that I was frustrated by a lack of
ciphers not yet broken that were within my own ability to break,
and he (and his boss) obliged me by submitting for my scrutiny
a proprietary cipher that an engineer at their workplace had
come up with, which thank goodness, had not yet been included
in a product.  My friend had been trying to convince his boss
that proprietary crypto algorithms were not a good idea, and
I'm pleased that I managed to give the strongest possible
evidence that the one they were about to use is in fact not a
good idea.
That said, its basic round function (what they wanted to use for
an S-box) would make a hell of a good P-box to distribute 64
bits of entropy among next round's S-boxes.  So I thought I'd
share it.
The cipher designer (professional programmer, not professional
cryptographer) was using a perturbed modular multiplication
transformation as an S-box.  It consisted of taking a 64-bit
input (C1) and two bytes from the key schedule.  The two
bytes (which I'll call A and B) were used to perturb the
output of the LCT like so.
C2 = ((C1 x A) + B modulo 2^64)
To generate C2 (output).  In fact a Linear Congruential
Transformation with key-schedule variables instead of
constants.  Then his P-boxes were slicing that up into
8 bytes and sending one to each of the 8 S-boxes of the
next round. (working on 64-byte blocks). Decryption was
with plain subtraction and a table of modular inverses.
The particulars of his cipher aside, a linear congruential
transformation is absolutely not a bad 64-bit P-box for a
real cipher, which is what I thought I'd point out here.
You can do your operations on any size S-box you want (that
divides 64) and use an LCT to split the bits up among the
next set of S-boxes.
But as for his cipher? Table of modular factors of each of
the 256 bytes the input might have been multiplied by, modular
division of set of numbers resulting from permutations of
the other 7 bytes plus 256 possible values for the unknown
input, statistical correlation of small factors under 16,
rinse, repeat, recursive search seeking the path with the
greatest bias in factors <23 available at each round (he
was using 4 rounds), program that can break the cipher in
the time it takes to get your finger off the return key.
And extends to *ANY* number of rounds with a constant
average work factor required per round.
I know that this is no tremendous feat in terms of crypto,
but I'm still pleased with myself; this is the first time
I've actually broken a cipher *PREVENTING* it from being
used in an insecure product, and it was a complete break
allowing the awesome demo of actual plaintext recovery
from a single block of encrypted output, which is highly
understandable even to those who do not speak math as a
native language.
Though I felt bad for the engineer; he actually made a
pretty good try for a beginner. If his 2-byte "perturbation"
at each round had selected from a better set of
transformations (had been a choice of 1024 different
64-bit XOR masks which exclude all rotations of each other
and the 64 possible bit-rotations for example rather than
an 8-bit multiplicand and 8-bit addend), or if his P-
boxes had sliced the output into bits instead of bytes
and directed them so that each byte of S-box input was
constructed from a different *combination* of bits from
the bytes of the previous S-box output? I could have
undone the trivial last round of P-boxes, but I don't
have math skillz to get further than that.
Anyway, someone else has better math skillz than me, so
instead of saying "this would be secure if..." and
almost certainly being wrong, I pointed them at two well-
tested and widely-scrutinized libraries.

@_date: 2015-05-22 16:41:25
@_author: Ray Dillinger 
@_subject: [Cryptography] I broke a cipher this week. 
There is a certain degree of Hubris and a certain
degree of "Not Invented Here" involved.  Which,
unfortunately, is typical of certain industries.
In fact I'm surprised they allowed anyone outside
the company to examine their cipher before
deploying it.  But glad they did.

@_date: 2015-05-25 20:24:07
@_author: Ray Dillinger 
@_subject: [Cryptography] I broke a cipher this week. 
That they hired a consultant and improved their cryptographic
security before launch based on the consultant's input?
That's not a bad story, IMO, it's a good one.  It's a
practice that more companies ought to do, for all kinds
of reasons.  Proprietary cipher algorithms aside, almost
everybody tries to implement proprietary protocols and
lots of them - even the ones that know proprietary ciphers
are mostly crap - get protocols wrong.   Hiring a
consultant to review cryptographic code before launch
should absolutely be standard practice IMO when preparing
to launch something important.
But you're right that it's probably not the story the press
would write if they hear the words "broken cipher".  Hence,
the very specific nondisclosure agreement. Which is fair.

@_date: 2015-05-25 20:39:58
@_author: Ray Dillinger 
@_subject: [Cryptography] I broke a cipher this week. 
There is a lot of Fear, Uncertainty, and Doubt involved.
Most companies lack the ability to really examine and vet
cryptographic code, and they've been hearing that the NSA
and other actors are regularly breaking suites that depend
on known, widely published, reviewed ciphers.  So there is
suspicion that these widely published ciphers are somehow
the point of vulnerability: some sort of trap to ensure
that their product is breakable.
Absent the ability to fully analyze and test cryptographic
attacks on these ciphers, there's little evidence to the
contrary that's really acceptable to them.
Crypto pros see that the breaks are in badly designed
protocols, in buffer overflows in completely different
programs giving an attacker root, in social engineering
or botnet hacking that gets customers' passwords, in
poor implementations putting stuff into unencrypted
buffers or an unencrypted swap partition, etc...  And
not in the crypto algorithms themselves.
But from the "birds eye view" of management they only see
"...used AES... got broken... customer records stolen ...
 ... company lost millions... "
And the press doesn't tend to help much.  They edit out
the technical details no one is interested in.

@_date: 2015-05-27 16:58:31
@_author: Ray Dillinger 
@_subject: [Cryptography] open questions in secure protocol design? 
Show me a means of pushing upgrades to users, and I will show you a
crook with a means of pushing downgrades deliberately mislabeled as
upgrades to users, in order to rip them off.

@_date: 2015-05-27 17:38:45
@_author: Ray Dillinger 
@_subject: [Cryptography] Guaranteeing that no distinct keys produce 
There's a tiny problem with a fair number of ciphers and
deterministic pseudorandom bit generators, which is that
two different keys or different sequences of entropy input,
can posssibly result in the same encryption or the same
sequence of pseudo-entropy output.  In fact there's a
related issue with DES, where the effect of composing
any two encryptions with different keys is the same as
a single encryption with a third key (hence 3DES with
a DEcryption in the middle rather than another ENcryption
that wouldn't actually add anything to security).
In fact this is the norm for pseudorandom bit generators,
because the design space favors a fixed-size buffer of
state and mechanisms to permit any amount of input to be
fed into it.  It is astronomically unlikely in PRNGs with
large state, but you can't actually prove that two different
sequences of input won't result in the same state.
I was thinking about this the other day and implemented a
cryptographic peculiarity; a deterministic PRNG (usable for
stream ciphers) that guarantees a different state (and
therefore output sequence) for every possible different key.
Its design is very simple:  It's the "Spritz" generator by
Rivest and Schuldt, with a state size depending on the size
of the key input and a key schedule which guarantees that no
distinct keys of the same length result in identical initial
states.  It is also impossible for any key to create a
state that "leads" or "trails" the state that a different
given key results in and produce the same sequence of
outputs shifted a finite number of outputs earlier or later.
Does anyone have a use for this cryptographic peculiarity?

@_date: 2015-05-31 09:55:36
@_author: Ray Dillinger 
@_subject: [Cryptography] open questions in secure protocol design? 
Has Microsoft provided a motive to refuse the upgrade?  I have
not used their software in years, but I'm under the impression
that XP is the last version which can be legally maintained in
a known state:  That is, without giving Microsoft Carte-Blanche
to implement any change in security or any other part of the
software that they want to change, for reasons that have nothing
to do with any threat model their customers may be facing.
If true, the people continuing to run XP have a point; it may
be the last version of Windows in which they have the option
to refuse a downgrade attack by anyone who can coerce Microsoft,
or even to wait until new software has been adequately tested
before adopting it.
Of course, there is at least one other motive that MS may have
provided to refuse the upgrade; is XP the last version of
windows that can legally be maintained without making continual
monthly payments to Microsoft?  If so, then companies may only
be running XP until they sort out their migration away from

@_date: 2015-05-31 12:06:06
@_author: Ray Dillinger 
@_subject: [Cryptography] Calculating Algebraic Complexity or Algebraic 
It is kind of arcane.  I'd be interested in any commentary
more knowledgeable than mine on this as well.  This list
does not talk as much as I'd like about actual cipher
design, since the consensus is that the vast majority,
even of professionals, should not attempt it.  So what I
know about cipher design I've learned mainly by studying
(and trying to break) existing ciphers.
Read this:
to get a good idea why cipher design is generally considered
to be something that most people should not attempt.
So the vast majority of us don't do it, and it doesn't
get discussed much. What I'm about to say is derived from
general knowledge and obsessive examination of existing
ciphers, not from any very deep theoretical knowledge of
their design methodology.  If anyone knows more, *please*
chime in.
That said, my impression, from studying existing ciphers,
is that good S-Boxes implement permutations which have
maximal period; ie, that 4-bit S-boxes from good ciphers
always seem to create a cycle 16 states long if iterated
on their own output, and that each bit in such an iteration
has a *different* 16-state cycle which is not an offset or
inversion of any other bit.  And that in order to describe
the operation that an S-box does, you have to use operations
from (at least) two different "group" relationships to
describe the relationship between the set of inputs to
the set of outputs. For example, combining addition-with-
carry, bitmasking via XOR, and bit shifting.
Avoiding the accidental creation of S-boxes whose behavior
can be explained simply in operations of a single group
relationship is a crucial feature of all good ciphers.
Especially when people are free to define new group
relationships.  Occasionally a new group relationship
defined by someone will be amenable to a new kind of
And this property remains true regardless of any S-box
modification due to the key used. This is why the influence
of the key is generally limited to bit-masking.  Any round
or S-box construction in which a possible keyschedule
breaks that relationship "doesn't count" in the number
of cipher rounds required by the properties I'll describe
S-boxes are never iterated on their own output.  The P-boxes
partition the output of all S-boxes into subsets (ideally
one bit subsets) which are fed to S-boxes (ideally to all
S-boxes) in the next round of the cipher.
Ciphers vary as to exactly what the P-boxes do with the
S-box output; from examination of good ciphers, there seems
to be a pattern that wherever S-boxes are repeated from one
round to the next, the bit assignment usually (but not always)
changes: for example, bit zero output of an S-box is rarely
directed to bit zero of the input of an identical S-box in
the next round.
All good ciphers of the Feistel construction have enough
rounds that what I call the "cascade depth" (that is,
sufficient rounds to assure that every bit of output
depends on every bit of input *and* every bit of key) is
repeated at least four times.  IE, if it takes four
rounds to achieve complete cascade, the cipher will have
at least 16 rounds.  This much I do understand the reasoning
for.  It has been proven that achieving cascade depth at
least twice suffices to transform *any* input into *any*
output.  And achieving *that* level of transformation twice
(meaning cascade depth is achieved a total of four times)
is required for a defense against meet-in-the-middle attacks.
Non-Feistel constructions (ie, constructions with reversible
S-boxes) can at least in theory have only sufficient rounds
to achieve complete cascade twice.  But I've never seen it
done, because reversible S-boxes always form a group
relationship.  Security in non-Feistel ciphers depends on
the P-boxes to ensure that the group relationship formed by
composition is defined only for inputs and outputs the size
of the cipher block *plus* the key.  And also for the
reasons I'll explain below.
Unless after the minimum number of rounds theoretically
required every bit of output depends *EQUALLY* on every
bit of input, the cipher must have more rounds.
Likewise unless after that number of rounds every bit of
output depends *EQUALLY* on every bit of the key, the cipher
must have more rounds.
Now here's the really arcane part, which I know is a
requirement, and observe in good ciphers, but have no idea
how to do:  in a Feistel cipher, the number of rounds must
be selected such that the influence of any particular
*algebraic relationship* between bits of input on any
particular *algebraic relationship* between bits of output
differs by less than one part in 2^n, where n is the block
size of the cipher.
Likewise in a Feistel cipher the number of rounds must be
selected so that the influence of any particular algebraic
relationship between bits of key on any particular algebraic
relationship between output bits is less than one part in
2^m where m is the key length of the cipher.
And in a non-Feistel cipher the number of rounds must be
selected so that the influence of any particular algebraic
relationship between any bits of key AND input on any
algebraic relationship between bits of output, is less
than one part in 2^q where q = length of key PLUS length
of block.
What that means is that under various groups for which
particular analysis methods are known the relationship
between input, key, and output is unpatterned.
But those last two properties are where even pros sometimes
fail.  It isn't really possible to check every possible
algebraic relationship over a large block and key size;
as far as I can tell it's as bad as trying to check every
key by brute-force. Cipher "breaks" are accomplished by
finding algebraic relationships on which these properties
fail.  Sometimes this can be done by studying the
construction of the cipher. I can sometimes do this to
ciphers designed by amateurs, but to date I've never
cracked a cipher designed by a pro.
Ensuring that finding such algebraic relationships of
particular kinds can't be done by particular methods of
analysis is a matter of advanced group theory: The best
ciphers achieve defense against all known particular
methods of analysis against all considered particular
types of algebraic relationships, at more or less the
same round.
But people are always coming up with algebraic relationships
of types or on groups that the cipher designer didn't
consider, and occasionally developing new methods of analysis.
Whether a cipher stands against these is mostly a question
of whether the designer got lucky or whether the cipher
designer included enough rounds "redundant or unnecessary"
by known relationships and attacks that the new thing even
though more effective on the cipher than any of the old things
is not more effective than brute force.
But these "redundant or unnecessary" rounds included for
insurance against new attacks, are balanced against a
requirement that ciphers must be fast to execute, and how
many "extra" rounds are needed is always a judgement call
by the cipher designer.  Only with a mathematical proof
that any analysis could be used to do [Hard Problem] in
[faster than Hard Problem is known to take] could a
cipher designer include *NO* extra rounds. For example,
solving RSA is at least as hard as factoring, because any
solution to RSA, no matter how achieved, could be used
to factor the modulus immediately.
Hope that helps:  And also hope anybody who knows more,
or can correct any possible misinformation I've spouted,
will chime in.  Anybody?

@_date: 2015-11-01 09:30:44
@_author: Ray Dillinger 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
But that ISN'T what the documentation said it would do!
It said it would warn you IF IT DETECTED null pointer use.
That is no guarantee that it will be able to detect all
cases of null pointer use - particularly not at compile
time! It's not even any guarantee that it will ATTEMPT
to do so!
The only way to detect ALL cases of null pointer use given
the rest of C's semantics is to insert runtime checks, and
as a rule C doesn't DO runtime checks. (except if you
use assert() statements and put them BEFORE anything that
depends on the occurrence of the potential error they check
If the intent had been to warn you UNLESS they could prove
that there was no null pointer use, the doc would have told
you that sometimes there was going to be a warning on good
code, or it would have warned you that it would slow down
your code with runtime checks.
Either way, it would NOT have said that it would make
optimizations based on the knowledge that arguments would
be non-null!  Any time the doc says "will make optimizations
based on" anything you give it, it means you better be
right about it or your code will crash!  That's ENTIRELY
different from "will insert runtime checks to make sure
you're right" or "promise to statically prove at compile
time that you're right" or even "warn you if we can't
statically prove at compile time that you're right."
You told them it would never happen.  They based optimizations
on the "knowledge" that it would never happen.  If you weren't
*sure* that it would never happen, then you should not have
made that promise.

@_date: 2015-11-01 19:18:25
@_author: Ray Dillinger 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
So about that 100% - I don't count?  I answered first without even
reading the doc, and then reading the doc absolutely confirmed it.
"will do optimizations based on the knowledge" of any declaration
or pragma that a programmer can add to a program ALWAYS allows
"whether or not the compiler can prove that the programmer is
right." If it were otherwise, it wouldn't even SAY anything
about optimizations because it is *already* allowed to do any
optimization whatsoever with anything it can actually prove.
You were making a promise to the compiler about your program.
You thought you were asking the compiler to make a promise to
you about how it checked your program.  But the words "will
do optimizations based on" are about as direct a statement as
it is possible to get that the compiler is making NO promise
to check what you told it.
In the case of gcc, it appears to be making no *effort* to check,
which is different and less useful - but fully consistent with
everything else about gcc.
The only thing even *slightly* misleading about the doc was
mentioning that if it HAPPENED to prove that you were wrong, it
would warn you about it. That sentence should probably be struck.

@_date: 2015-11-08 12:16:22
@_author: Ray Dillinger 
@_subject: [Cryptography] How programming language design can help us 
Indeed.  I do things that require getting *right* answers
from time to time, and use Scheme for the same reason.
Unbounded rationals, unbounded bignums.
One problem, even given software that handles potentially-
infinite representations is that you don't have potentially-
infinite memory to store them in nor potentially-infinite
CPU to manipulate them.
Another problem is that a fair number of the things you'd
*like* unbounded precision with, are fundamentally
irrational operations, so even if you get to pick how much
rounding error you'll allow, you can't just pick zero.  You
have to allow a rounding error somewhere if you're taking
square roots, because square roots are not closed over
rationals, for example.

@_date: 2015-11-09 18:50:01
@_author: Ray Dillinger 
@_subject: [Cryptography] Bitcoin blocksize limit can be removed 
Indeed.  Even in the brief message, Satoshi's "fist" of language
use patterns, line lengths, etc, are not followed.  This is in
addition to his signature keys not being used.
Satoshi walked away.  Not being outed means not preserving any ties
to that identity.  Therefore all the artifacts of that identity -
his old user names, email addresses, etc -- are up for grabs as
rapidly as the providers they tied him to allow reuse of lapsed
usernames etc.  Or, alternatively, as often as they lose control
of their user databases, which has already happened at least once.
We're going to see lots of fake Satoshi stuff.  This isn't the
first, and won't be the last.
Short version: don't believe it unless he's using the same keys.
And, bluntly speaking, the "real" Satoshi won't.  Ever.  He's not
going to take that risk.

@_date: 2015-11-09 20:01:30
@_author: Ray Dillinger 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
I feel compelled to respond to this one.  I have in the last year
dealt with a situation in which a client with homemade crypto was
attacked by a criminal organization intent on stealing bitcoins,
within which someone actually DID bother to learn and perform
linear or differential cryptanalysis.  (I don't know which one;
the system would have been vulnerable to either).   The protocol
gave them several known and partially-known plaintext blocks for
each key, so they had a starting point. Doing the math on about
2 hours worth of traffic got them from a 128-bit symmetric key
down to an effective search space of 28 bits, which they promptly
The client had been almost certain that "someone" was passing keys
along to people who oughtn't have them, but I saved "someone"s job
by putting up some fake traffic using keys that nobody but the client
and I had, and the attacker showed up immediately at the honeypot
that traffic revealed.  So, the attacker was definitely doing the
After that I changed some software and configurations and the problem
went away.
I like people who wade in and learn enough to do their own crypto well.
I admire the effort, and try not to discourage them, especially when
they're already feeling down because they've just had their first
"learning experience".  I don't want them to quit trying, I want them
to get better!
That said, when something is breaking, I have to look at the least-
tested stuff first because the negative correlation between still-
unsquished bugs and long testing is well established. In practical
terms that means I start by looking at the homemade stuff.
"As far as I know we've never had an undetected error."
   -- Howard Aiken, IBM Engineer.

@_date: 2015-11-14 21:32:01
@_author: Ray Dillinger 
@_subject: [Cryptography] Satoshi's PGP key. 
Recently was a request on the list for the Public PGP key
corresponding to the Private key used by Satoshi Nakamoto.
I wasn't inclined to post it at first, but ... well...
It can't be used to do mischief, and possession of the
corresponding private key is certainly something that
pretenders will never be able to fake.
It is:
-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: GnuPG v1.4.7 (MingW32)
-----END PGP PUBLIC KEY BLOCK-----
I don't believe the world will ever see another message
signed with that privkey though, so it's more a historical
curiosity than useful information.

@_date: 2015-11-14 22:01:07
@_author: Ray Dillinger 
@_subject: [Cryptography] Bear Bonds - a new crytpocurrency 
I am compelled to state for clarity's sake that I have nothing
to do with this new cryptocurrency.

@_date: 2015-11-16 13:50:07
@_author: Ray Dillinger 
@_subject: [Cryptography] Satoshi's PGP key. 
That depends on how you mean 'functionally subservient',
but as far as I know he (or she or they) never used the
Genesis Key for correspondence.
At least, not correspondence to me or anyone I've ever
heard from.
I don't think that widespread knowledge of the Genesis Key
would actually cause harm at this point.
Although one could generate another checkable Genesis
Block using it, that fact could not be used for theft now.
Aside from the Genesis block being checkable
cryptographically, it is widely known and distributed.  It
would not be displaced even if another checkable Genesis
Block generated by the same key showed up.
And the hashing proof-of-work has the interesting property
of being bidirectional;  We have hashers working away to
produce a partial collision with the hash of the previous
block.  You could produce an arbitrarily-long fake block
chain of _perfect_ hash collisions working backward, but
a chain of _partial_ collisions is just as hard to produce
backward as forward.  So a new Genesis block couldn't be
the root of a longer (in hashing power) block chain.
So the Genesis Private Key has value only as a historical
artifact at this point.

@_date: 2015-11-16 14:10:57
@_author: Ray Dillinger 
@_subject: [Cryptography] ratcheting DH strengths over time 
I am a fan of the "rip out and replace" methodology.  I dislike
systems where everything is so intertwingled that it cannot easily
be done.
When I hear words like "suite" or "toolbox" or "framework" I am
often pleased in other endeavors, but usually consider myself warned
of a hazard in the context of security.
Excessive integration means that in order to rip out and replace
whatever's actually causing the problem you'll probably have to
rip out and replace a lot of things that are working just fine.
It's collateral damage; more work, and more opportunities to get
something wrong.

@_date: 2015-11-17 16:47:35
@_author: Ray Dillinger 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
It is true.  The new smoke alarm units flatly presume that
all ranges are electric and all ovens are either electric
or microwave.  Any gas range or gas oven will set them off
immediately.  If they are actually in the same room, they
will even refuse to coexist with pilot lights.
As a result they cannot be used by anyone who prefers cooking
with gas.  As most "foodies" in fact do.
Since the people willing to own and (very) occasionally use
an electric range or oven, to a first approximation don't
cook at all, that means the new units are most unusable
where they are most needed. The sort of non-foodie who
rarely cooks at all hasn't much need for protection.
This is an excellent model for security to NOT follow.  A
security device that only works for those who don't need
it is a non-starter.

@_date: 2015-11-17 17:38:45
@_author: Ray Dillinger 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
The real problem is that people want to build all this stuff
without a self-destruct timer. Things that don't wear out, get
folded into infrastructure and forgotten rather than becoming
a routine part of infrastructure maintenance.
All of these IoT devices need dead-man switches to assure that
their software does in fact get updated occasionally as the
security issues get worked out.
If you want an automatic update, that's a fine way to prevent
an expiry from happening in the usual case.  But not an excuse
to build a device with no expiry, because automatic updates
can be prevented by accident (firewall configuration by someone
who didn't think of it) or on purpose (Site Policy says no new
software enters the Tempest secured area, period, without an
audit trail, a purchase order, a signed authorization from our
security department, the signature of a company representative
on behalf of the software provider, a receipt that we can use to
establish the provider's culpability in case of malfeasance or
criminal negligence, etc....).
I for one wish that a particular crappy "wireless J" router had
just plain stopped working three years after it was purchased,
prompting me to update its software or buy a new one, instead of
continuing to present vulnerabilities.  But it never called attention
to itself, and I didn't notice it until doing an audit.
Let IoT devices, and routers, and switches, etc, come with
expiration dates molded into plates in the cases in large, deep
letters that can't be filed off or altered while leaving the
cases at all intact, and countdown timers with nonvolatile
registers inside that will flatly shut them down on that date
unless the software has been updated before then.
Manufacturers could "refresh" the devices for a nominal fee or for
free, fitting new expiry plates right along with the new software,
or count them toward "exchange" for new devices and resell them
refurbished.  People who don't care about keeping the "expiry
plates" current, or who don't want the turnaround time, can be
allowed to just download the patches and install them themselves.
But the important thing is that the devices oughtn't continue
to work indefinitely unless somebody *does* install new software.

@_date: 2015-11-18 12:13:36
@_author: Ray Dillinger 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
Which is different from blindly trusting them in the
first place when you buy the brand-new target you're
painting on your chest, how?
I *know* expiry a bad idea - I'm mooting it mainly
because people are talking about auto-updates and there
is ABSOLUTELY NO OTHER WAY you're ever going to get auto-
updates in place. Therefore that idea is just as bad as
this one because it ENTAILS this one.  In fact expiry is
probably a less-bad idea, from the consumer POV, than
buying IoT devices in the first place.
There are no Internet-of-Targets devices in my home,
specifically because no vendor is accepting legal and
financial liability for the long-term security of their
No Trust = No Sale.
But that's me.  Most consumers are following them like
sheep to the slaughter.
Homer Husband and Harriet Housewife do not understand the
need for software updates and WILL NOT GET THEM.  They
didn't buy a computer, they bought a frikkin water heater.
NOT hooking it up to the Internet where the stupid auto-
update idea could work, is probably the best security
they are likely to get.  But even that won't work if some
yahoo can exploit it via its even-stupider-to-have-
installed wireless interface.
The fact that if they don't patch a vulnerability someone
can now use their water heater to plant viruses on their
computer is nonsensical to them.  They not only have no
idea why these systems should be related, odds are that
they have no idea THAT these systems are related.  They
will ignore everything that it is possible to ignore.  The
*only* way to get their attention on the fact that updates
are needed, is with an expiry.
If you're serious about updating IoT devices, then
devices that are not getting updates must somehow call
attention to themselves ("Huh?  Our water heater can send
us messages?!")  And then educate them, at least a little
bit, about what they have to do ("Our water heater says
it's going to stop working next month if we don't get
this little dime-size thingie attached to the thermostat
replaced with a new one.  I guess the hardware store has
IMO, the Internet-of-Targets is a bad idea in the first
place.  There is NO way my thermostat or my refrigerator
has any business talking to the Internet or listening to
the Internet.  If I want or need adaptive learning software
to set the optimum temperatures, save electricity, or
remind me I need to buy cheese, I'm happy to run that
software locally on a machine that has absolutely no
bridge to the Internet-at-large and absolutely no
relationship with any particular vendor of cheese.

@_date: 2015-11-18 19:06:05
@_author: Ray Dillinger 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
Absolutely.  The thermostat doesn't have to shut down; what
has to shut down is every functionality it has that involves
a network.
As far as the network is concerned, such devices should be
"write-only" after their expiry: No response whatsoever and
no local behavior prompted by any network message, no matter
what gets sent to them.  No way to even tell there is any
device attached to the far end of the wire.
But that doesn't mean they have to stop being useful devices.
A thermostat whose network software has expired can still be
a fine (locally) programmable thermostat without being on any
network. An expired refrigerator can still keep food cold.
And a car whose IoT functions expire can still be a perfectly
good car even if its doors stop responding to remote-unlock
commands from cell phones and start requiring you to use the
physical key instead.
In fact, cars are extra-easy because they require maintenance
anyway. Pick something that gets changed regularly anyway, like
a car's air filter or something, and make the software module
(remember, they cost <20 cents each in bulk) part of that.  no
extra steps, no extra technician time.  Take the old one out,
drop in the new one, and the software is part of what got
physically replaced.  "Maintenance Mechanic doesn't even notice
she's doing it and it adds nothing to the jobs she already does"
is exactly the level of automatic update you're looking for.

@_date: 2015-11-20 10:36:39
@_author: Ray Dillinger 
@_subject: [Cryptography] Unencrypted SMS 
Why do they think that was specifically about the attacks, in
the sense of being certain enough to consider it admissible
evidence?  It contains no information about the attacks.  It
sounds just as much like somebody who just bought a new
smartphone letting people know she's coming to a dinner party
with a few friends, and then throwing away her obsolete old
cell.  If there are no fingerprints on the phone, could it
be because it's in the middle of November and Paris is cold
enough that hands inside gloves are more comfortable?
Finally double encryption with ROT13, as Bruce put it, is
not really characteristic of this kind of operation.  In
fact it's not really characteristic of *ANYTHING* - it's
a default that anybody might use for any casual message.
So I just don't get why that phone is considered "evidence."
A clue certainly, and possibly even a relevant clue.  But
that's not the same as evidence.

@_date: 2015-11-21 17:38:15
@_author: Ray Dillinger 
@_subject: [Cryptography] Unencrypted SMS 
Ah.  THAT is crucial information that links the phone much more
strongly with the attacks.  That, taken together with the message,
is MUCH more specific and incriminating than the text message
Still not enough to stand up as evidence in court though;
a map of the place isn't useful or desirable ONLY to people
planning an attack.  But it's pretty strong for merely
circumstantial evidence, and very likely to be a very
relevant clue.
Thank you for the additional info.

@_date: 2015-11-22 13:48:44
@_author: Ray Dillinger 
@_subject: [Cryptography] Dan Bernstein has a new blog entry on 
Known plaintext is more common than you'd guess.
For example, the plugins for phbb that render user avatars produce
files (gif or png or jpg) in a standard format, size and color depth.
Those plugins, with their default settings of 256-color, 150x150-pixel,
gif get used across a lot of sites.  The pictures look different to
us, but the headers are identical when seen as data.
Each time a user avatar pic gets transmitted from any of those sites,
it's transmitted as its own packet and the first few blocks of the
payload will be identical because identical headers.
So, pick a block of that gif header as your known plaintext, and you'll
have millions of examples of it being used 'live' every day.

@_date: 2015-11-22 14:47:50
@_author: Ray Dillinger 
@_subject: [Cryptography] [FORGED] Re: ratcheting DH strengths over time 
How difficult is it to create and validate a set of curve
parameters that are secure, if we are using straight,
unoptimized bignum math to do our ECC rather than tuned
and tweaked code that is hardcoded for moderately faster
computation on some particular curve?
Because I think it's silly for very many people to all be
using the same curve for long periods of time.
As I see it, many people using the same curve all the
time makes that curve into a target.  You may be able to
make a tuned software implementation %30 faster, and
that's a huge net LOSS in security.
If we take djb's premise at face value, and frankly I think
he's right, our attacker model should be the cost PER KEY
of breaking any key in a very large batch.  If everybody's
using the same curve, the same computation searches for
everybody's keys.
A tuned-code implementation of a single curve that is 30%
faster, is a net loss in security because it gives an
opponent a stationary target (which doesn't change for
DECADES) that has a billion times the value of any one
user's traffic.  It gives an opponent means and
justification to have already done extensive precomputation
and compiled large rainbow tables, and justifies for your
opponent the expense of creating hardware implementations
that are %4000 percent faster than your tuned software
implementation and buy truckloads of them.
It is far better to force the opponent to use general-purpose
hardware and software and give them no opportunities or
justification for calculating and storing huge data required
to speed up their work to break keys on a single curve.
So why aren't the users all coming up with a new curves on
a regular basis, especially when creating new high-value
keys?  It may be too compute-intensive to do as part of a
handshake for a transient key, but for LONG-term keys, or
for a server key that lasts a week or more?  Why should
any two of them be on the same curve?

@_date: 2015-11-25 15:35:52
@_author: Ray Dillinger 
@_subject: [Cryptography] basic cryptography ... was: key breaking 
I actually developed one of these.  It's three full rounds
of a Feistel cipher on 128-bit blocks, with all S-boxes
generated by a cryptographically secure pseudorandom number
generator.  The key is put through a secure hash to get
the initial state of the PRNG, because otherwise related
keys producing PRNG output offset by a known number of
blocks would be easy to generate.
That said, it's a moderately silly exercise - more a
design and coding exercise than a real thing that people
might want to use.  It would never be used in a real
design because its encryption/decryption speed *and*
its key agility both suck harder than rocks.  It sucks
up six (If I remember right) times the number of bits
of PRNG output that it encrypts, and spends the time
to do Feistel rounds on top of that.
If you're interested in it, I can send you the C
implementation I made.
I like a lot of its properties;  Like a stream cipher,
it can produce literally ANY ciphertext from ANY
plaintext, with a different permutation of plaintext-
to-ciphertext in effect for each block. All ciphertexts
for a given block are provably equiprobable for a given
plaintext in fact, if the PRNG is good.  It can also
be used without an IV, or with repeated use of the
same key, to at least the same extent that any 128-bit
block cipher can, and can be used ECB in lots of places
where "normal" block ciphers can't.  If you have the
same 128-bit plaintext encrypted with the same key, a
normal block cipher in ECB mode will always give you
the same ciphertext.  This one will do so only if the
identical block is ALSO at the identical offset in the
plaintext.  Its key space is essentially the size of
your RNG's state or your hash output, which can be as
much or as little as you want.
In fact 3 rounds is massive overkill for the "equiprobable
transformations" thing if the PRNG is secure - the third
round is just to insure it against any PRNG correlation
that is detectable at all (looking at you, RC4).

@_date: 2015-11-25 19:37:07
@_author: Ray Dillinger 
@_subject: [Cryptography] Security of a permute-only system? 
All of it, I think.  If you mean "whitened" like a stream cipher, and
then put a permutation per-block on top of it, you get at least as much
security as the stream cipher and then deny the opponent the opportunity
to take advantage of the bit-masking properties of stream ciphers.
If you mean "whitened" like XOR a LFSG, I think it would still be pretty
impossible to take any advantage of the linear properties of the LFSG
if the bits were randomly scrambled across a large block.
To be sure of real resistance to attack, however, at least one of
those things - the whitening stream or the permutation selection -
really does need to be cryptographically secure.

@_date: 2015-11-29 21:49:12
@_author: Ray Dillinger 
@_subject: [Cryptography] Security of a permute-only system? 
I forgot to mention this, because at first glance I thought
it was obvious.  However, on reflection it may not be.
You must not apply the SAME permutation to each block;
if you do, then you win absolutely nothing w/r/t security
because your opponent can then undo your permutation
via parallel anagramming.
Like a stream cipher means XORing with a random string of
bits but you NEVER repeat that random string, any security
gained from permutations only holds as long as you don't
repeat the same permutation.

@_date: 2015-11-30 15:51:56
@_author: Ray Dillinger 
@_subject: [Cryptography] Security of a permute-only system? 
The only thing that the permutation protects against is the
XOR class of attacks against stream ciphers.  But the attacker
is still able to apply XOR attacks against the (repeated)
permutation and observe the precise ways in which this messes
things up.  In many protocols that would mean he can first
identify the permuted locations of first and last bits due to
block boundary issues, then other bits relative to them to the
extent that different field boundaries within the block or
protocol result in different hashes being violated or different
protocol invariants failing, etc.  The exact point at which
the destination machine stops responding reveals the moment
at which it detected the error.  If it gets a wrong bit in
the middle of a nonce, it will not know until a later step
in the protocol when the derived key doesn't match, vs. getting
a wrong bit in the middle of an amount, in which case it will
know immediately because accounting invariants are violated.
To know to exactly what extent a particular application is
vulnerable to this type of field-matching attack you'd have
to see the entire protocol it uses.  And a carefully coded
application might be able to detect any bit-error immediately
if there is, eg, a checksum on each block, or otherwise force
any failure in a given block to cause failure-of-response at
exactly the same time regardless of which field/s in the
block contain/s the detected error.
The point is that the necessary security measures (checksum
on each block) are identical to the necessary security measures
for an unpermuted stream cipher - hence the permutation was of
limited use.

@_date: 2015-10-03 14:31:41
@_author: Ray Dillinger 
@_subject: [Cryptography] Hyper-V claims to protect tenant secrets ?? 
Agreed in terms of financial and economic risk.  If governments
were the only malefactors whom we had to worry about, then we
wouldn't actually need to worry about keeping clients secure
nearly as much as we do - nor mostly from the same set of
A bit of corporate espionage by government agencies in China
and Japan, the odd attack by North Korea on someone who
embarrassed them, routers and chips compromised for access
to the nations that manufactured them,  sabotage of uranium
enrichment plants by the US, etc, are *probably* less damaging
to infrastructure and economies in real terms. Sure, they cost
a few billion dollars, mostly from American companies, but
that's nowhere near the amount lost to plain old criminal
fraud and theft each year.
But not agreed in terms of the risk to freedom.  What we're
looking at right now is essentially our ONE opportunity to
establish a precedent that says nations respecting human
rights to any extent do not deliberately weaken security,
do not require their manufacturers to produce deliberately
defective hardware and software, and do not create total
surveillance societies nor tolerate mass surveillance of
their citizens by others.
If we can't get the USA and some other first-world "free"
nations to hold the line on this point, then there is no
differentiation (or at least none on this point) between
free and unfree nations, and will never be.
The fifth amendment, and the first, like the Magna Carta and
the UN Declaration on Human Rights, are there for good reasons
and in order to be a free nation, we need them to apply to
individual people's electronic data in exactly the same way
they apply to searching individual people's homes.
We're used to thinking of security in technical terms or about
cryptographic security, so we're thinking about ways to deny
such attackers the *ability* to steal or destroy information;
but in fact this fight can only be won if we win it in
political rather than technical terms.
TLDR: It is more relevant to write your representatives in
congress (or parliament/etc) about whether free nations have
the RIGHT to conduct such attacks on their own people, than
it is to consider cryptographic security against government

@_date: 2015-10-09 16:40:16
@_author: Ray Dillinger 
@_subject: [Cryptography] blockchain and trustworthy computing 
The problem with this is that if your block chain is restricted to
a single installation (vehicle) then it requires a trivial additional
expenditure in CPU onboard that vehicle to enable cheating.  So if
VW says, "and we're spending $10 per car on hashing power to prove
that this block chain thing is legit..."  but then actually installs
more or cheaper-per-hash hardware, or has understated the number of
hashes the hardware can do, then the car can produce fake block
chains on command that look just as good as the real one.
And this assumes customers are okay with the vehicle quickly running
its battery down continuing to hash all the time, whether running or
not, because if you allow it to quit hashing while it's not running,
then you allow a block chain to be faked by anything that can do
hashes while the car isn't doing any.
Also, what prevents a replacement block chain from being calculated
on much more capable hardware on demand and downloaded into the vehicle?
Finally, another problem with this is that even if you don't have the
capacity to fake a block chain built into the vehicle, what the block
chain in the vehicle shows at time B is whatever was put into it at
time A.  And the ability to prove at time B what was said at time A
has not much to do with proving that what was said at time A was true.
As Charles Babbage once said,
"I have been asked, 'Pray, Mr. Babbage, if you put into the machine
wrong figures, will the right answers come out?' I am not able rightly
to apprehend the kind of confusion of ideas that could provoke such a
People talking about using Bitcoin's block chain to secure information
are relying on the property that Bitcoin's block chain would be very
very difficult for anybody to fake, because there is FIERCE competition
to get the next block and in order to fake it you'd have to get a bunch
of blocks in a row.
But this property simply isn't true for a small block chain that's
running in an isolated instance with a single "competitor" for the
blocks. Without independent agents fiercely competing for the next
block, you are left trusting the sole agent that is getting ALL the
blocks, which is not better than the situation you started with.
What you're proposing is essentially the same as a 'black box' that
just keeps a log of things indexed by time and mileage.  It being a
block chain wouldn't make it any harder to fake than a good tamper-
proof seal on a hardware black box.

@_date: 2015-10-11 12:32:59
@_author: Ray Dillinger 
@_subject: [Cryptography] Usable Security Based On Sufficient 
Read those two paragraphs again and ask yourself why execute
permission as root would be required to reveal the secret.
The point is that the malware is already executing (as the
user); it doesn't need to get root execute privileges if
it has root read privileges.
If the malware can read with root privilege then it can read
executable code with root privilege.  And if it can read
executable code with root privilege, and it is already executing,
then it can execute that code, if in no other way, then by
using, eg, a Bochs machine or the equivalent built into
the malware.
What it comes down to is that you can't rely on execute
privilege alone to protect secrets.  For example, if
execution of some program as root can give the number 54,
then any process that has root-level read privileges can
perform the computation to get that number, whether they
run the computation as root or as some other user.
You may rely on execute privilege to prevent particular
usage of those non-secrets, however.  For example only a
root process could take that number, turn around, and
open a connection on port 54.

@_date: 2015-10-16 13:39:27
@_author: Ray Dillinger 
@_subject: [Cryptography] Fwd: freedom-to-tinker.com: How is NSA breaking 
Instead of wondering how long it'll be until it works with *longer*
re-used primes, why aren't you asking why primes are getting reused??
Isn't the central weakness here is the propensity of server
implementations to continue using the same prime factor for their
whole uptimes - or, indeed, for the whole of *every* uptime?
Isn't the appropriate fix making sure that different numbers get used
each time DH is performed?  And won't that be the appropriate thing to
do regardless of the key length being used?
I mean, yes, I'm all for moving to longer keys given that these
exhaustion attacks are possible in the first place.  But shouldn't
we first be fixing the dead-wrong implementation that makes the
brute-force attacks feasible?

@_date: 2015-10-21 15:27:56
@_author: Ray Dillinger 
@_subject: [Cryptography] How does the size of a set of target results 
This is true, but expressed in a way that's ambiguous enough
that someone who doesn't already know what you're saying could
take the wrong impression of it.
Clarification:  MD5 now lacks strong collision resistance.  It
is feasible to generate a pair of files, or a pair of sets of
100 files, that have the same hash.
MD5 still has weak collision resistance. Meaning, given a file
or set of files whose creation you don't get to control, it is
still difficult to generate a different file or set of files
that has a matching hash.
There is a third category of collision resistance that we
don't usually talk about, but when we do the name "multicollision
resistance" is often used.
A hash algorithm has multicollision resistance for as long as
it is hard to generate sets of MORE than two files (or sets of
files) that have the same hash.  As it happens MD5 lacks strong
collision resistance but still has multicollision resistance,
meaning it's not feasible to generate a set of three files (or
100) that all have the same hash.
Multicollision resistance is not usually interesting because
we're usually only interested in whether or not collision resistance
is strong.  Multicollision resistance, like weak collision
resistance, is automatically satisfied when functions have
strong collision resistance, and we don't really usually care
whether "weak" collision resistance includes multicollision
resistance or not.

@_date: 2015-10-21 15:37:18
@_author: Ray Dillinger 
@_subject: [Cryptography] multicollision resistance in protocol design? 
As everyone on this list is aware, MD5 is no longer suitable for use
as a general purpose cryptographic hash function because it lacks
strong collision resistance.  It is now known how to generate pairs
of files having the same MD5 hash.
However, MD5 still has weak collision resistance (meaning it is hard
to generate a file that has a predetermined hash) and still has
multicollision resistance (meaning it is hard to generate sets of more
than two files which all have the same hash).
We don't usually think about these weaker properties, but there are
some peculiar circumstances in which multicollision resistance could
be useful in protocol design.
For example, we can present a file and then when we later disclose a
different file with the same MD5 hash, that can serve as a proof
that the already-committed file has no MD5 collision with anything

@_date: 2015-10-21 16:10:09
@_author: Ray Dillinger 
@_subject: [Cryptography] Other obvious issues being ignored? 
The sort of threat analysis that might detect FREAK / LOGJAM
downgrade attacks in progress, or "Application Proxy" firewalls
that fail to do proper security?
Sign me up.

@_date: 2015-10-21 17:13:39
@_author: Ray Dillinger 
@_subject: [Cryptography] Other obvious issues being ignored? 
C and C++ do have the "volatile" keyword which does exactly what
you are asking for (data located outside register, always written
when the code writes it even if the code never reads it again).
I know of no other language that has anything equivalent.  As far
as I'm concerned it is impossible to write secure code in Pascal,
(because any optimization that is guaranteed to produce the same
output result is permitted without let or hindrance), it is impossible
to write secure code in Java (because there is no way for a program to
control whether the JVM is modified), it is impossible to write secure
code in Lisp or Java (because garbage collection can leave sensitive
stuff lying around indefinitely until collected) .....
It's a problem.  But when I'm trying to write secure code I do in
fact usually wind up writing it in C.  In an excruciatingly tedious
process to make sure every last damned buffer write is checked for
overflows, and that every last damned malloc at a known line creates
something that is absolutely guaranteed to end at a free at a
different known line, that every last "constant time" algorithm
has timing loops and exit statements to force the program to
fail immediately if the compiler "optimizes" the routine breaking
that property, and etc....
And further excruciating process to ensure that mathematical
operators leading to 'undefined' behavior on overflow are not
used (eg, integer addition/subtraction is allowed only on
unsigned integers, or values cast to unsigned, etc).  I
frequently wind up reading the standard "adversarially" in terms
of specifically finding ways to prevent things it would ordinarily
allow, or accomplish things in ways the system isn't allowed to
redefine.   It is long, ugly, and sometimes the resulting code is
horrifying.  It is often clearer and simpler when subroutines are
coded in assembly language, but the kicking-dead-whales version
has to be done in C because portability.
Usually it will compile just fine with any optimization setting
but if compiled with -O3 it will print a message about what got
"optimized" in an unacceptable way and exit instantly if run.

@_date: 2015-10-21 18:26:15
@_author: Ray Dillinger 
@_subject: [Cryptography] Other obvious issues being ignored? 
Yes, I am that guy who is compiling the checklist of things of such
boneheaded obviousness that you couldn't get anyone to come up with them
And, your help is appreciated.  When you refer to a serious security
failure in hardware or software (even - or especially! - one that's
so mind-breakingly obvious it seems trivial), please give me a point
to start researching from.  At least the name of the system affected
and something like a timeframe. I try to do research into how things
came to be, but I have to have someplace to start.
The only way to justify asking checklist questions of such boneheaded
obviousness is to demonstrate that this is a real mistake that has
already been made at least once by real people, and hopefully also to
dcocument that it had real costs.  So each and every checklist
question has to be preceded/explained by a real episode that happened
to real people and resulted in a real bug.
Also, the stories add interest and "teachable moments"; if it were
*just* a checklist I couldn't imagine anybody reading more than ten
pages of it, nor retaining the information in a form meaningful enough
to occur to the poor human brain at an appropriate moment.
The series of articles entitled "Cybernetic Entomology" (and, yes, I
know that other writers have used the same tag).  ATM it is a series
of blog posts. Hopefully eventually it will become a book.
I intended this at the outset to be more generally about bugs, but
as time went on it seems to have become focused more specifically
on security bugs. Here are some of the articles, in case you're
interested:  If these remind you of other things I ought to write
about, or if you have corrections of fact to share, please let me

@_date: 2015-10-21 23:25:18
@_author: Ray Dillinger 
@_subject: [Cryptography] Other obvious issues being ignored? 
Do you really want to go up on an airplane with a pilot and crew who
have to check things off on a safety checklist with stupid crap like
making sure the landing gear works and the cabin pressure is good?
Even, you know, as a reminder that it's worth checking?
Aviation checklists do not mean that the people who use them are
incompetent.  Neither do software development checklists.
But as I said, I have to make a compelling case as to why something
that may seem boneheaded is being included, so I need a story
illustrating its reality and the value of asking it, to go with
every darn question.

@_date: 2015-10-22 06:33:29
@_author: Ray Dillinger 
@_subject: [Cryptography] Other obvious issues being ignored? 
Welcome to my quote file.
The assumption that writing crypto code requires no particular
training is at the root of SO MANY of the kind of boneheaded
problems we've been talking about.

@_date: 2015-10-22 18:28:25
@_author: Ray Dillinger 
@_subject: [Cryptography] How programming language design can help us 
Okay, I'm going to use an illustration that has already been talked
about on this list.  In C, the meaning of a signed integer addition
where the sum is greater (or less) than the signed integer type can
represent is unspecified.  And it matters whether your compiler treats
unspecified as meaning literally "behavior doesn't matter at all" or
unspecified as meaning literally "doesn't specify the return value
of the expression."
If you have code that adds two positive numbers and then checks for a
negative result intending it as a check for overflow, like this:
if (y < 0 || z < 0 ) halt(1);
z = x + y;    /* undefined in case of overflow */
if (z < 0){
    printf("overflow at line %d\n", __LINE__);
    halt(1);
    }
printf("positive result is %d\n", z);
In the case of gcc, the compiler goes through saying "I don't
have to care what the code DOES in case of undefined behavior."  It
doesn't even generate the check for z < 0 because that check can
never succeed in any case where behavior is fully defined.  That
leaves the consequent as dead code and the whole check and response
get stripped.  At runtime your code may say "positive result is -32760,"
because undefined behavior did in fact give z a negative value, but
it will never check for that negative number or halt, because the code
that would perform that check and that call to halt has been stripped.
In the case of MSVC, the compiler goes through saying  instead,
"I don't have to care what VALUE an expression returns if the
expression invokes undefined behavior."  The addition result may
be undefined but the addition will have a result. That result
will either be greater or less than 0, and the check could turn
out to succeed if an undefined result was returned.  So in that
case the test and the consequent are not meaningless and code to
do those things will be in there.  Your code may (or may not)
detect a negative result and halt, but it will never print
"positive result is -32760".
And that's the essential difference between the "undefined value gets
returned" and the "undefined behavior if" schools of how the compiler
treats code whose exact meaning is not specified by the standard.

@_date: 2015-10-23 17:06:15
@_author: Ray Dillinger 
@_subject: [Cryptography] Other obvious issues being ignored? 
The Debian RNG bug was a particularly screwed-up case that's been
oversimplified by virtually everybody I've seen write about it.
People go on about how stupid it was, and by implication how they'd
never ever have been so incredibly stupid, and... okay, it wasn't a
bright moment, but it was really not something that required truly
incomprehensible levels of stupidity.  It was understandable, and
in a careless moment a whole lot of the people who make fun of it
could have done it.
There was a routine that took a pointer at a buffer and mixed
the buffer's contents into the RNG state.
There were two calls to it in the same file:  One in a routine that
was the primary means of adding randomness read from physical sensors
and other "good" sources to the RNG state, and one in a routine that
was supposed to fill a buffer with RNG output.
The latter was just a way of opportunistically harvesting whatever
happened to be in the buffer prior to filling it with RNG output.
It was  Valgrind and marked with "// valgrind complains".
The former was a vital part of how the RNG got implemented and updated.
It had no  and no comment about valgrind.
Valgrind REALLY didn't like the one that harvested whatever was in
the buffer prior to filling it with RNG output.  It made nonsense to
programmers that a call to fill a buffer with RNG output would cause
valgrind to complain that the buffer hadn't been initialized first,
so they didn't do that, and whenever it got called with a
newly-allocated buffer (a very common usage pattern) it looked exactly
like the sort of thing Valgrind was supposed to warn about.
Completely harmless, and occasionally beneficial when programs
reused buffers - but it generated a storm of warnings and for
some logistical reason or other having to do with efforts
toward reproducible builds, they didn't reap the benefits of
the primary maintainers' " valgrind".
Eventually the Debian crew became afraid that somewhere in the
storm of bogus warnings they might be missing a real bug.  They
talked about it, about the benefits or problems of opportunistically
harvesting entropy from uninitialized or reused buffers, on the Openssl
development list.  At this time, the Openssl development team had
silently abandoned that list, going elsewhere to have their
discussions in a lower-traffic, less stressful environment, so
they never heard about the discussion.
Eventually, without their feedback, input, or review, a Debian
maintainer went in there to delete the line that was generating
all the warnings.  But it turned out both lines were generating
So he deleted them both.
They were both generating warnings because the Debian crew were
actually right that there was in fact a real bug buried somewhere
in the storm of warnings.  The routine containing the *IMPORTANT*
call was also being called from at least one place with an
uninitialized buffer, which hadn't been intended.  This was
a VERY minor bug given that all the other calls were correct,
but it was a real bug.
But instead of going, "ah-hah, there really was a bug there and now I
can find it", the maintainer went "ho-hum, I'm here to delete the call
that causes all the warnings, and there are still warnings, and
therefore I'm not finished until I delete this other call too."
And nobody noticed for two years.  Hello, code review?  The guy
I'm talking about did *NOT* make the only serious mistake here!
It was boneheaded, yes.  But it's not the kind of utter blunt stupidity
that people ridicule when they talk about it.  It's not something
that they themselves could never ever have done because they're not
complete idiots.  It was a moment of human weakness, of losing sight
of the forest because you're momentarily distracted by one tree. And
it's understandable why it happened.
And this is the sort of thing you find out when you research bugs
and how they came about.  The dumber something seems, the more
likely that there is an understandable reason why it happened.
And I think people need to understand how these things happen
if we're ever going to do any better, and that's why I'm doing
the 'Cybernetic Entomology' book.

@_date: 2015-10-24 15:00:16
@_author: Ray Dillinger 
@_subject: [Cryptography] Making secure devices. 
I found a supplier for PROMs!!
Sometimes we have discussions here about the difficulty of finding
or manufacturing trustworthy devices.  The number of places - firmware,
non-volatile memory, boot records, etc - where malware might be hiding
in a full-on computer seems to multiply every year, and the "Internet
of Things" appears to be designed mostly to serve as attack vectors.
It becomes more and more important to do security on devices which are
so simple that it is possible to insure that they are doing NOTHING
else, and of course any such device becomes a high-priority target
for an attacker to access at any point along the supply chain or by
finding a way to get a program wedged into some hidden spot in its
BIOS, etc.
So ....  I've recently found a supplier for PROMs.  Non-erasable,
write-once devices whose idea of a software update is to literally
throw them away and get a new one.  And also a programmer for them,
which proceeds by the old-fashioned expedient of breaking undesired
circuits by physically melting selected traces.
With this, I think I can do things like, eg, replace disk drive
firmware with firmware that cannot be modified to hold a trojan.
With work, I think I could replace a motherboard BIOS and make
sure that malware can't be stored there.
I think I can make a USB controller whose administrative interface
is capital-C Closed and can be relied on to remain so no matter
what infectuous crap gets plugged into it.  I think I can put
executable code for a simple PRNG or a block cipher on this, hook
it up with a processor that has absolutely no nonvolatile storage
and is incapable of addressing its volatile storage on its instruction
bus, and make a network device that I can trust to NOT be remotely
subverted even though it contains some network software.  Heck,
I could make my own keyboard hardware and be absolutely certain
there isn't a little password-stealing program running on the
NVRAM chip I see on that circuit board (WHY does a keyboard need
a 256k non-volatile memory?!  WHY??!!)
Heck, I could even build a trustworthy router.  One where I KNOW
that no "trusted" manufacturer has been forced to include a back
door or pwned into putting one there.
Of course, since I'm the guy who's writing these write-once chips,
I'm the only guy who can trust them.  Trustworthy devices for
customers would pretty much have to be programmed on site and
require each customer to have their own prom writer.
Otherwise they'd have to be "trusted" rather than "trustworthy" -
which I do not want, because "trusted," which used to mean only
capable of doing harm by betraying people, has been taking on an
even darker meaning.  Increasingly it means COMPELLED to do harm
by betraying people.

@_date: 2015-10-24 18:35:16
@_author: Ray Dillinger 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
Sufficiently useless compliance to a standard is indistinguishable
from having a useless standard.
I would be in favor of an extended standard for "Crypto C" such
that all code whose behavior is specified in C would be specified
identically in Crypto C, no new syntax or keywords would be
introduced, and most of things that are left unspecified in the
C standard would be either guaranteed to be compile-time errors
or specified with an exact semantics.
If we had that we could at least point at as a coherent,
implementable standard when we try to get compiler writers
to "be reasonable and consistent" from the point of view of
cryptography development.  If they didn't want to make it the
default mode, or if they hid it behind obscure command-line
switches, that would be okay.  Specifying all the undefined
crap would even make it a valuable mode for those general
developers who value the specified behavior more than the
The gcc developers have a policy of delivering the most useless
possible conformance to every language standard they implement,
on the presumption that "good" code shouldn't rely on anything
unspecified ever.  Under this definition the C standard makes
"good" code very difficult to write, and by that criterion isn't
a very good standard.  A "crypto c" standard would make things
a hell of a lot better just by being less possible to conform
to in an utterly useless way.
That said, I truly believe the same thing the gcc developers
do about "good" code and whenever such a facility is available
I use whatever compiler switches and options force an immediate
abort or compile failure on unspecified behavior.
The "expected but not specified" stuff that the gcc devs have
failed or refused to do has played a huge part in exposing the
flaws of previous language standards which needed to be improved
or clarified.  Most of which, sigh, *STILL* need to be improved
or clarified.

@_date: 2015-10-25 11:46:36
@_author: Ray Dillinger 
@_subject: [Cryptography] Other obvious issues being ignored? 
But writing code that's acceptable to gcc is important for two
reasons.  First, if you don't have a particularly "adversarial"
compiler, you will never find the problems in your code where a
silly or utterly useless interpretation of what you wrote, is
allowed by the standard.  gcc specializes in finding the silliest
or most utterly useless permissible interpretation of your code,
so making it your usual compiler makes that your testing baseline.
Gcc is a sort of worst-case nightmare scenario of the language
standard.  Code that works on gcc will need very little maintenance
in the future, because later language standards are likely to make
a difference only to code that the current standard allows silly
or useless interpretations of - which you can't write if gcc is
one of your regular test compilers - and no other compiler
working within the current standard will ever find more excuses
to produce silly or useless executables.
Second because when you release code to clients, they're
going to use "whatever" compiler to compile it, and they're
going to blame YOU, not the compiler devs, if their executable
does silly or useless things. No matter if they've signed off
on the build process and makefiles you provided, this will
still happen.  Pointing at that part of your contract can get
you paid, sure, but they will still be miffed at you about it.
Because they're not likely to find a compiler that will
produce a sillier or more useless interpretation of your code
than the one you test every time you compile with gcc, the
odds are in your favor.
Alas, they may find one that is *differently* silly or useless
and thus exposes bugs that gcc didn't.  But that's why you test
builds with different compilers before you deliver code, and
that's why your contract includes the build process as well as
the source.
Anyway, my problem is not with compilers that bite me whenever
the standard allows them to, because every one of those bites is
legitimately a bug in my code given that wimpy damn standard;
my problem is with wimpy damn standards that allow them to bite
me a lot.

@_date: 2015-10-26 19:40:07
@_author: Ray Dillinger 
@_subject: [Cryptography] How does the size of a set of target results 
Oh damn!  I hadn't seen that!  That's important!
And that pretty much wraps it up for ANY USE AT ALL of MD5!  It
is time for it to go to the great bit bucket in the sky.
Thank you, Zooko!

@_date: 2015-10-28 15:00:45
@_author: Ray Dillinger 
@_subject: [Cryptography] portable, 
I think it's worth looking at both the false negatives *and*
the false positives, in all the combinations you can get.  In
the hopes, of course, that they come along with a bunch of
diagnostics that are neither.
I wonder if it's worthwhile to monkey together a makefile
that selects a different compiler, optimization level,
warning levels, error options, and defined-behavior build
flags EVERY TIME ....  or decides 'lint' or some other
analysis tool output is a necessary precondition for
compiling, *that particular time*.  It would sort of
automate something I do haphazardly and by hand every
so often to see what a different compiler or tool thinks
of a particular bug I'm trying to figure out, or of code
I *think* I've successfully debugged.  It's too easy to
be wrong about that.
I don't know if even I'm that masochistic in terms of
testing, and *my* development process is actively a FIGHT
against the compilers to force them to do exactly what I
intend and nothing else.  But it would definitely be
"the right thing to do" w/r/t the goal of producing
portable code.
It might make some things easier because it will
provide different error messages one of which might be
right or useful while others are obscure, wrong, or
"mysterious", or a couple or three of which might
each give ne one necessary "clue" to the same error
while none are informative on their own, making it
possible to figure out more easily by seeing the
different messages in rapid succession.
On the other hand bugs that are exposed by only one or a
few of the builds, or exposed differently by different
builds, will be maddeningly inconsistent in testing, and
I'll definitely want to have a "compiled by" function
that will allow me to ask a buggy version of the program
exactly which compiler (and what version of that compiler
and with what build flags, etc) created it.
Hell, I guess I'll try it. No point doing it haphazardly
and by hand; I might miss something I could have caught.
If it's too obnoxious to cope with, I'll stop.
The *USER*?!  Oh HELL no.  The user gets the most
non-silly executable I know how to build and a
makefile that builds nothing else, with build
options that make a hell of a lot of "undefined"
behavior explicit and checks a hell of a lot of errors
the standard doesn't say you have to check for, like
with stack canaries and array reference ranges, etc.
It's me as the *DEVELOPER* who wants the silliest
executable (or the ten silliest executables) my code
allows; that way whenever one of the compiles or
test scripts fail I know I have to fix the code.
Oooh, I like that one.

@_date: 2015-10-29 10:51:05
@_author: Ray Dillinger 
@_subject: [Cryptography] Hiding parties identities 
This is the one I implemented for a peer-to-peer message
delivery service (college class project).  Each peer
shared keys with only a small(ish) number of other peers,
so it wasn't that much of a burden for a peer to try its
dozen-to-thousand or so shared keys on incoming requests
one at a time.
Requests were of the form E(K $+ nonce, K) indicating
that a following protocol message was encrypted in key
But one of my classmates pointed out that in order to
take the message delivery service down, she could just
flood the peers with bogus incoming requests made of
pure binary noise, which would multiply the decryption
work by an arbitrary factor.
I never implemented the modified form, but proposed to
fix it by altering the form of the requests to be
M $+ E(Knext $+ M $+ Mnext, K)
where M is a message-ID, Mnext is the next message-ID
intended for use in the same session, and Knext is the
next ephemeral key intended for (ONE) use in the same
In that circumstance the peer could just lookup the
Message-ID and if it wasn't expecting that Message-
ID it would just toss the request.  If it was, it
could read the expected key for that message-ID from
the table, decrypt, check for validity (M from the
payload matches M from the header?) and toss the
request otherwise, delete M and K from its table of
incoming messages, add Mnext and Knext to the table,
and process the protocol message.
It shifted the (theoretically) DoS-able resource from
CPU to memory, and memory for a hash table of message-IDs
and ephemeral keys with limited times to live, is a *lot*
cheaper and less practically DoS-able than CPU for
attempting decryption on incoming noise.
Edit:  The notation $+ above is for string addition,
aka concatenation, and E(P,K) is for for "plaintext P
encrypted with key K."  Different authors use different
notation, but $+ is ascii-friendly and visually distinct
from other common notation such as + which is confused
with mathematical addition and space or comma both of
which are confused with argument separation.

@_date: 2015-10-29 22:54:30
@_author: Ray Dillinger 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
I have never used that annotation in earnest yet, but
I'll give it a guess....
You claimed to be passing it a pointer to an integer,
with an annotation that told it that a null pointer
was an error.
Therefore gcc developers will claim it is entitled to
assume the pointer is Non-Null. (or rather, that the
program has no semantic requirements in the case where
the pointer is non-null).
Therefore I'm guessing it elides the test for null and
its consequent return statement as dead code, then crashes
(if you in fact HAVE called it with a null pointer)
when it tries to dereference the pointer in order to
multiply it by 2.
Am I wrong?

@_date: 2015-10-30 10:20:33
@_author: Ray Dillinger 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
I think that the possibility of pointers being null
is not wrong.  *WILD* pointers, at storage that has
been deallocated or, worse, reallocated for something
else, are an abomination, but null pointers are a
simple and useful thing.
To me a null pointer is a semantically valid entity
meaning "value not yet determined" or "no such value
is possible" or "the structure had no data matching
the criteria you searched for," etc.  If you didn't
use null for that, you'd still need a way to express
I mean, sure, they could be better.  If I were to
attempt a type-safe language then each type, including
but not limited to pointer types, would have its own
*distinct* null to indicate "no valid value," and
dereferencing a null pointer would have a defined,
non-crashing semantics as returning the null value
of the associated type - but that wouldn't change how
I use them.  It would just mean I needed to type a
bit less.
I simply don't understand all the rage and gnashing
of teeth directed at them.

@_date: 2015-10-31 11:41:53
@_author: Ray Dillinger 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
Well, you're right that I was assuming most useless possible
conformance.  I hadn't read that specific doc, but that's
exactly what I'd have seen it saying that gcc will do if I had,
even on the most straightforward reading.  I wouldn't have
been surprised, or even annoyed.  My reaction would be, "yep,
exactly what it says on the tin."
This conformance, as usual for gcc, is _technically_ correct.
"The compiler may choose to make optimizations based on the
knowledge that certain function arguments will be non-null... "
is EXACTLY what happened to your code.
A pragma like that is a promise you are making to the compiler
about the correctness of your code.  "Cool," gcc will say,
"this knowledge is something I get for free directly from the
programmer, I don't even have to prove it!"  It'll take it as
gospel, do optimizations based on it, and produce smaller,
faster code.
As a bonus it'll even warn you, as the doc mentions, if it
*happens* to prove that you're wrong.  But why would it attempt
such a proof?  It has your word on it, and the spec says it's
entitled to assume you're right. Otherwise it wouldn't be
allowed to do optimizations based on the knowledge.
Reasonable compilers given that pragma might make an honest
effort to prove what you said and issue a warning if they can't
prove that you're right.
But gcc isn't reasonable.  gcc is like the dev who follows your
order to "fix all the bugs you find" by playing minecraft all
day.  Hey, why would you complain?  He fixed all the bugs he
"The most likely way for the world to be destroyed, most experts
  agree, is by accident. That's where we come in. We're
  computer professionals. We cause accidents."
                - Nathaniel Borenstein

@_date: 2015-09-02 11:50:22
@_author: Ray Dillinger 
@_subject: [Cryptography] Checking for the inadvertent use of test keys 
clip
In fact I have such a file available.  It contains ten million
UserID/Password pairs from a banking system which leaked the
information to crackers about eight years(?) ago. The file
was later recovered and entered the public domain as trial
evidence, long after all the customers involved had been
required to change their passwords. I can make it available
to you if you'd like.
In many cases the UserIDs are more apparently-random than the
passwords.  Evidently obscenities of four to twelve characters
are better bets than most for a password guesser; perhaps
people find them more memorable.

@_date: 2015-09-03 17:09:12
@_author: Ray Dillinger 
@_subject: [Cryptography] Checking for the inadvertent use of test keys 
Don't even joke about sending keys off to a third-party who's going
to have to have them in the clear to provide meaningful results.

@_date: 2015-09-10 22:16:39
@_author: Ray Dillinger 
@_subject: [Cryptography] millions of Ashley Madison bcrypt hashes cracked 
They've cracked 11.2 million accounts "so far".  I'm completely stunned
that Ashley Madison had 11.2 million accounts in the first place, and
that counts only those who had signed up *before* they switched to more
secure methodology.  That would be approximately one for every 30
people in the US, and Ghu alone knows how many new accounts since
then and how many more insecure accounts remain to be cracked.  I
just didn't imagine that such a skeevy "service" would attract so
many clients.
I guess I haven't been reading the news closely enough; I've been
treating it as 'ho hum more of the same.' But I guess it has the
scale to be significant after all.

@_date: 2015-09-11 09:57:55
@_author: Ray Dillinger 
@_subject: [Cryptography] millions of Ashley Madison bcrypt hashes cracked 
A Sybil attack by any other name would still be as much of a
vulnerability.  Right?
It occurred to me this morning that many system design flaws can
be traced to unwarrantedly anthropomorphizing the user. -- Steven Maker

@_date: 2015-09-13 23:53:31
@_author: Ray Dillinger 
@_subject: [Cryptography] millions of Ashley Madison bcrypt hashes cracked 
IIRC, the law that made SSN's into as nearly a requirement as
they are now, also provides that anyone can get a new SSN if
they want to - it's in the same paragraph with the bit about
issuing a new card if the name changes.
However, people actually doing this is so rare that you probably
won't find an office where any SS employee actually remembers that
it's possible, much less how to do it. I remember reading someone's
account of basically forcing the local SS office to comply with the
law and go through the process back in the 1980's, but he fought
about it for a couple of years before he convinced them they really
had to because that's what the law said, and even when they discovered
they really had to, they had no idea how to do it until they got some
long-delayed interaction with some authoritative source that probably
made up the procedure they used on the spot. Ultimately I think the
FBI had to be involved - not because it's in any way criminal, but
because their Witless Protection Program is approximately the only
office where there is a known procedure to issue new valid SSN's to
people who already have one.
And I don't know if the social security account into which he'd been
paying taxes for years and which more or less determines his SS
benefit on retirement came with him when the number changed. If
not, then there's a definite downside to getting a new number
attached to a new, completely empty social security retirement

@_date: 2015-09-14 00:33:11
@_author: Ray Dillinger 
@_subject: [Cryptography] [FORGED] Re: millions of Ashley Madison bcrypt 
Hmmm.  What set of services are needful to provide?
Obviously people want to be able to use their browsers to see
HTML pages that get regularly updated.  And they want deniability,
which is specifically an avoidance of anything like authentication,
so SSL certificates on the server side are okay but not on the
client side.
But those HTML pages, images, etc, can be on their own filesystems,
yes?  So they could get a big encrypted package daily-or-so that
contains the site updates - probably via bittorrent or equivalent,
which gets unpacked directly onto their filesystem in encrypted
form.  Then they can browse using their browser with a local-system
proxy that decrypts the material without ever writing the plaintext
to the filesystem.  If and as they answer or write ads, their proxy
uploads a relatively tiny update, no more than once per hour, to
the server - possibly via Tor or encrypted and tucked into an ICMP
Obviously this won't work if the site services need to include very
time sensitive things like live chat, and very heavy datastreams
like streaming movies, but it would certainly work for a relatively
simple private website a heck of a lot more resistant to traffic
analysis and account hacks than AM ever was.
If the central server sees your searches in plaintext, then it could
tailor an update for you depending on your searches - but there's a
privacy issue with the trusted central server where someone can
demand to see those searches.  Otherwise, users would need to select
a subsite to browse (a torrent seed, basically) based strictly on
statistical data or general interests, and have searches be local-
Obviously anybody will be able to subscribe and get the daily update
bundles, so the ads people place should not be considered to be
private material.  But who placed which ad, I believe, is information
that is legitimately private and should be withheld from anyone who
declines to answer the ad and meet face to face.

@_date: 2015-09-16 15:28:38
@_author: Ray Dillinger 
@_subject: [Cryptography] Microsoft's new, free, 
That's -- Highly uncharacteristic of Microsoft.  Perhaps
things are changing in Redmond.  Or perhaps it's just for
this library because someone  has finally convinced them
of the truth about security software being utterly different
than other software in terms of whether proprietary code is
an advantage or a hindrance.

@_date: 2015-09-16 15:33:14
@_author: Ray Dillinger 
@_subject: [Cryptography] Comey: targeted ads => plaintext access 
But why wouldn't the ad broker engage in a sibyl attack?  It's in
his economic interest to know EXACTLY who's getting his ads, because
that list of names/addresses is one of the things he sells.  In this
era the consumer IS the product.
Heck, he'll even serve up ads that prompt internet requests to his
own servers to get one-pixel graphics just to see where the requests
come from, if we don't give him an easier way to find out.

@_date: 2015-09-18 14:33:18
@_author: Ray Dillinger 
@_subject: [Cryptography] An Open Source Analysis of NSA Cryptologic 
At IBM, if I recall correctly, it was known at the time, and called
the "tickle attack."
The NSA shortened DES keys, in part, because they had a mandate to
recommend a secure cipher for commercial use, and have a very specific
institutional definition of a "secure cipher:"  IE, one which is no
easier to break by any other means than it is by guessing keys.  They
saw a cipher which (once strengthened against differential
cryptanalysis) still had a 2^56 work factor to attack, and therefore
in order to make it a "secure" cipher they reduced the key to 56 bits
without telling anyone about the relevant attacks.  They could also
have strengthened it with more rounds to support the original key size
to create a "secure" cipher under this criterion, but they were able
to achieve their particular definition of "secure" by reducing the
key length instead.
It's arguable, IMO, that had they known about the linear cryptanalysis
attack they'd have reduced the key to 54 bits in order to satisfy the
same technical criterion.
Either way, I have a different notion of "secure cipher" than they
do, and it is more related to the work factor of the best attack than
it is to a match between the best attack and length of the key.  That
is, I count something with a 2^64 work factor attack as having 64-bit
security and something with a 2^56 work factor attack as having 56-
bit security, even if the former has a 128-bit key and the latter a
56 bit key. Either, of course, is clearly inadequate today.
Today I prefer 128-bit security, and if I take the possibility of large
quantum computers seriously (a point on which I am not yet decided), I
will start to prefer 256-bit security instead.
That said, I distrust ciphers that were thought to have a higher level
of security when designed than they are known to have under the best
attack developed to date.  A discovered vulnerability, especially if
relatively recent, will often be extended to reduce security further.

@_date: 2015-09-18 16:07:19
@_author: Ray Dillinger 
@_subject: [Cryptography] Microsoft's new, free, 
Interesting.  That's about the same time I stopped using Microsoft
products.  In fairness, I have been seeing progressively less
evidence as time goes by of widespread and idiotic security failures
on their part; for the last while the windows security failures have
been rarer and apparently have required more-than-just-trivial effort.
The GNU license is largely a reaction to the proprietary culture
he fostered and the relative impossibility of realizing a profit
from anything if you weren't part of the Microsoft hegemony.  It
represents a lot of people just plain giving up on the possibility
of personal profit. They wouldn't have done so had Microsoft not
been frequently, deliberately, and unfairly barring or disrupting
their access to the market.  As far as I can see its emergence as
a significant force affecting the market is the direct consequence
of Gates' own actions. His dislike of it therefore deserves neither
sympathy nor respect.

@_date: 2015-09-18 16:21:24
@_author: Ray Dillinger 
@_subject: [Cryptography] Comey: targeted ads => plaintext access 
Doubts about whether ad brokers would voluntarily co-operate aside,
I'm interested in the mechanics of a protocol to do this.  It may
have other applications where the motives of the participants align
- such as matching up people with aligned interests while maintaining
some degree of privacy for them greater than simply advertising those
interests publicly.
So how does the protocol puzzle work out if we want to target ads
based on the contents of encrypted emails without having the ad
broker have access to the plaintexts or know who is getting which ad?

@_date: 2015-09-23 11:09:08
@_author: Ray Dillinger 
@_subject: [Cryptography] Follow up on my password replacement idea 
The attack surface is reduced by the user's hamfisted and careless
handling of keys and sensitive data as they fumble them from one
device into another - possibly with the "help" of software they've
downloaded from the attacker on an unsecured connection.
Yes, people do fall for that.  I recently had to clear malware out
of a machine where someone had downloaded "helpful" software to
copy all their financial data into a new format -- a new format
on a server somewhere in Belgium as best I can tell.  Along with
their key manager and all its data files, because of course they
let it run with the admin privileges it requested.
Just plain NOT asking users to share keys across devices is a really
good plan - you can't get them to develop secure habits for something
if it isn't something they do by habit every few hours of every day.
Not doing  it at all is better.  If you really want to have the same
accounts on many different devices (which I do not!) then let keys
for different devices get handled on the server side, because the
professionals on the server side *are* going to be doing it every
few hours of every day so they're going to develop a procedure
that's at least consistent, and they'll review it for security at
least as often as they get pwned.

@_date: 2015-09-23 13:04:05
@_author: Ray Dillinger 
@_subject: [Cryptography] Follow up on my password replacement idea 
It is difficult to imagine an IP replacement that doesn't leak a
huge amount of traffic analysis fodder.  Difficult, but fun.  On
a small scale you can use something like the DC-net software.
On a larger scale there are things like Tor.  So, at least in
theory, you could have a Tor-like system with gateways into
smaller DC-nets rather than IP exit nodes, and then use
something like namecoin for DNS and key infrastructure. It
would be chaotic, hard to route anything, high overhead,
and considerably higher latency than we're used to .... but
it could work if enough people really wanted it to.
Unfortunately a lot of the biggest stakeholders have a vested
interest in tracking that traffic information.  The people who
actually make a living from advertising and ad brokering are the
same people who invest in development and work on standards, and
they absolutely do not want a world in which they cannot get at
the traffic information.
Oh, any of them would be happy with protecting you from everybody
ELSE's access to your traffic information.  But not from their own.
It reminds me of when the guys from Google tout the privacy and
security of their gmail system, while simultaneously selling targeted
ads based on its content.  Yep, they protect your privacy from
everybody except Google.  and Google's customers, partners,
brokers, clients, contractors, licensees, pets, employee's
ex-spouses, three-letter agencies, etc.
I don't see how a password manager would help with the problem of
third-party cookies that enable advertisers to track web browsing
behavior.  I mean, seriously - I hate to be picking on Google so
much, but they're the best example. Go to almost any site and you'll
get cookies from fonts.googleapis.com, from googletagmanager.com,
from googlesyndication.com, etc....
How does it matter that you have a different password for each of
those sites?  The servers in Redwood City know exactly where each
of those cookies gets set and retrieved.  To stop that you'd have
to ban cross domain  requests entirely, and the entire web
infrastructure would then grind to a halt amid much screaming
and gnashing of teeth.  For about three days, until they work out
forwarding on the server side with all information going back to
wherever content is getting forwarded from.  And then you're in
exactly the same position except you wouldn't even be able to see
the domain names anymore.
I think far too many people place far too much faith in the idea of a
PKI.  The reality has never yet measured up to the faith.  CAs get
compromised, certificates get stolen or forged, people assuming
trust is transitive in a "web of trust" find out it isn't, people
sign certs on the basis of bogus paper ID, and people blindly accept
ALL attestations from ANY CA - even a completely bogus one on the
other side of the planet - if it means they can watch cat videos.
As far as I can see trust relationships are binary.  That is, every
pair of parties has a separate trust relationship, and attempts to
mediate it all through a universal trust brokerage such as a PKI are
at best difficult and at worst counterproductive.  If trust is
binary, we might as well be using symmetric-Key crypto and all of
our key managers darn well ought to support symmetric key crypto.
I -- _really_ hate that.  I keep hoping that there's a way for
some protocol to take information like that into account without
trusting it to someone who has the capability of disclosing
it without my knowledge or permission.  But I haven't come up
with such a protocol yet.
Objectively, you're probably right.  If I bought gasoline and a
burrito in Pratt Kansas an hour ago, I'm not likely to be buying
lobster thermidor in Monaco right now.  For one thing I wouldn't
be hungry again yet.  And I'd be happier if the bogus attempt to
use my account failed.
But, darnit, I don't want someone looking to be able to tell
that I bought a burrito and gasoline - OR lobster thermidor, if
it turns out that it was the previous rather than subsequent
purchase that was bogus.  I also really hate the idea that
someone can look at something and see that I was in Monaco -
or Pratt Kansas, whichever - at a particular date and time.
And yet I can't deny that if I ever used my phone to access my
bank account (which I don't, but which most people do) then the
operator of a wi-fi hotspot with a DNS proxy and a "clever"
bit of malware performing a downgrade attack and an MITM, could
sell my bank info to somebody who went on a holiday in Monaco,
or somebody on a road trip across Kansas, and I don't want to
get stuck paying for purchases I didn't make.  It would only
require that I did something ordinarily stupid.  Something
ordinary people do every day.
Right, but I absolutely hate it when "trusted," as in this case,
means only "capable of doing harm by betraying others." Is there
*ANY* way we could use an untrusted party instead?  Computation
on undisclosed information?

@_date: 2015-09-23 16:29:36
@_author: Ray Dillinger 
@_subject: [Cryptography] Follow up on my password replacement idea 
The difference is that you have a genuine trust relationship with
the lawyer in the first place and you evaluate his introduction in
the context of that trust relationship.
Just to pick one at random from my browser, I have absolutely no idea
who "TRKTRUST Elektronik Sertifika Hizmet Salaycs" is.  I have no
reason to trust them.  No trust relationship whatsoever.  And yet my
browser is configured to accept their word that people are trustworthy.
Supposedly because I trusted the people who make Firefox to make the
introductions to CA's, but get serious.  There's a big difference
between "default configuration of ubiquitous software" and "actually
has a reason to trust."
My point is that trust is flowing the wrong way.  I have a reason
of long business experience, FDIC regulation, federal oversight,
personal  experience and contractual liability, etc, to trust my
banker.  If my banker introduced a CA, then I would consider myself
to have some reason to suspect that that particular CA isn't bogus.
Now run it the other way.  I have almost no relationship with the
people who make the browser, and that relationship expressly disclaims
all liability and gives me no recourse under contract. Further, they
aren't liable even for a refund because their software is free.
Finally, it's open-source so if they screw it up there isn't even
anybody to sue. That isn't a real trust relationship.  They have no
relationship with "TRKTRUST Elektronik Sertifika Hizmet Salaycs"
that I'm aware of, beyond having probably exchanged one or two
letters and being afraid that some user somewhere might not be able
to watch a cat video without a certificate from them.  Again, that's
not a real trust relationship.
And on the basis of the word of someone whose relationship with me
isn't contractual and disclaims all liability, I'm accepting the
word of someone whose relationship to them I'm completely ignorant
of but suspect to be minimal or nonexistent, to introduce complete
strangers whom I am supposed to trust for financial transactions.
What am I, crazy?

@_date: 2015-09-24 18:11:35
@_author: Ray Dillinger 
@_subject: [Cryptography] Wrongware: was VW/EPA tests as crypto protocols ? 
I'm making up a new term.  The term is wrongware.  Wrongware
means software that is deliberately wrong, which is provided
by the exact people from whom a faithful (rather than wrong)
implementation is expected, specifically to cause misbehavior
that they find desirable for whatever reason.
There are more than a few cases of wrongware that have
already been discovered.  Because the security challenges
are different from other categories, I think we need a
distinct named category for it as a security issue.
"Wrongware" would encompass the misbehaving VWs that deliberately
cheat on EPA tests.  It would also describe, eg, deliberate
backdoors installed by a router manufacturer to enable
surreptitious access, deliberate vulnerabilities in operating
systems, USB or disk controllers, deliberately installed
vulnerabilities in computer BIOSes, "updates" released by a
software vendor that secretly disable or cripple its own
product, etc.
As to the case in point:  I think this particular wrongware can
be reasonably easily defeated.
Develop a self-contained exhaust probe that can live on batteries
for a few days.  Stick it to the tailpipe using aluminum speed
tape with tamper-resistant seals, with its tip sticking into the
tailpipe.  Let the owner drive around normally for a few days,
then remove the device and download the data.
It would make it very hard to hide misbehavior, especially on a
very large scale.  Yes, some car owners would deliberately cheat,
but a lot of them would get caught cheating, and it would require
only a small fraction of the car owners to cooperate with the
testing regime to conclusively reveal any deliberate misbehavior
caused by wrongware.
In fact you could reveal wrongware by using this "extended testing"
regime on only a randomly selected one percent of the cars.
You'd want to do a few other things too, like NOT making the exact
date of the next test predictable or publicly available, because
if the wrongware can tell when it's being tested by reading the
time and date, its attack succeeds.

@_date: 2015-09-30 11:54:58
@_author: Ray Dillinger 
@_subject: [Cryptography] Future GPG/PGP 
Which sort of reminds me of Adam Back's hashcash scheme for limiting
email rates, except that it more specifically limits *connection* rates
instead and email is not usually from the party who bears the cost of
Hmmm.  This could be used as a tactic against some forms of sybil
attack where the attacker has to contact *MANY* nodes in a short time
in order to be successful. Considering that SSH is a pre-existing
part of many protocols whereas it's really difficult to add support
for something like Hashcash to a protocol.
Instead of the asymmetry of calculating vs checking a hash partial
collision, you have the asymmetry of defenders getting to split up
the work among themselves and attacker not.

@_date: 2016-04-04 21:48:35
@_author: Ray Dillinger 
@_subject: [Cryptography] [cryptography] Secure universal message 
I'm increasingly unConvinced that a good email system really
requires broadly distributed  public keys at all.  Consider
the advantages of a system where email addresses are secrets.
Where you encrypt a message to someone using a secret key
which that person gave you.  It may be a symmetric or asymmetric
key, but first and foremost there are huge advantages in it
being secret, and once it's secret there's little harm in using
plain old symmetric cryptography.
In the first place, if your address isn't a public key you've
distributed, then spammers don't, by default, have it.  That's
kind of the right thing for starters.
In general I don't want people who have one of my private email
addresses sharing it around randomly. When they do share it around,
on the likelihood that I don't agree with them that the person they
shared it with ought to have it, I want the ability to cut that
address off.  If some troll pisses on my reputation and tosses my
email address to 4chan, why should that be a disaster that requires
me going through fifteen thousand hate mail just to find the ten or
twelve real messages I get that day?  And why shouldn't I know
exactly which of the people I correspond with is responsible for
the attack?  If it's a secret key, handling the problem is simple;
I just delete that key from my profile, and and look at the messages
I have left.
As another case in point, when, inevitably, my dear nontechnical
brother gets pwned yet again and a spammer gets his addresses
including his address for me, I want to be able to cut off
the spammers' access to my mailbox.  In order to do that I need
to be able to identify which key is compromised - a step which
is useless if everyone you correspond with has the same key.
I need to contact *THAT* person, and nobody else, to give them a
new key.  And I need to be able to cut it off without cutting
myself off from other correspondents and without giving the spammer
access to my inbox - again, much harder if everyone has the same
key or the key/address is available from a public registry.
Finally, the privacy is better.  Secret keys can be revoked, just
by erasing them.  Once they're gone that address is not one that
can be used to contact you, period. You never had to publicly
identify yourself with it because you never had to distribute a
pubkey.  Odds are that no snoops know you were ever associated
with it, unless you, or a person whose privacy it would have
compromised along with yours, told them.
So, in summary, email addresses as secret keys are better privacy
in the first place and better damage control when someone has your
contact information and you'd rather they didn't.  So why does
everybody keep trying to build a PKI and revocations and all that
jazz for a mail system?

@_date: 2016-04-06 16:47:53
@_author: Ray Dillinger 
@_subject: [Cryptography] [cryptography] Secure universal message 
I would like to point out that nothing about a "secret-key" system
prevents anyone from publishing an address to the world at large
which they allow anyone to make first-contact with them using.
That would work with exactly the same kind of spam-dynamics etc.
we have now.  But it would only apply to people who wanted it
to apply to them, and only for as long as they wanted it to
apply to them.  And anybody who contacted them via that address
would acquire a new address for them in the course of the
protocol, so cutting it short wouldn't cut off people who'd
already contacted you with it - all the ones you'd already
replied to would already have a personal address for you.

@_date: 2016-04-17 10:50:19
@_author: Ray Dillinger 
@_subject: [Cryptography] USB 3.0 authentication 
Y'know, I don't think I believe in the idea of cryptographic overkill as
such.  Not, at any rate, where there is a real need for that level of
crypto *in any application.*  For two reasons.
The first thing that makes me doubtful about "overkill" arguments is
that experience shows that the number of security flaws is proportional
to the number of choices among different moving parts in crypto
implementations.  And implementations of different security levels means
different moving parts.
Somebody who needs 128-bit security, if they build it in an environment
that has a lot of these "wrong" parts lying around for the supposed need
to build lower-security products, is likely to end up with the classic
"vault door on a straw hut" situation. 128-bit encryption algorithm with
keys protected by passwords stored with 40-bit salt. Or 128-bit
encryption in a protocol that relies on 64-bit hash collision
resistance. Or, even worse, 128-bit encryption with a choice for one end
of the protocol to unilaterally opt down to 64-bit encryption. Or
whatever.   Why should all those wrong choices be available in the first
place given that even an ARM microcontroller is fast enough to do
full-strength real encryption at full USB3.0 speeds?
Second, the crypto somebody thought was overkill for a mere USB device
takes on a whole new set of requirements when someone uses a USB device
to secure high-grade secrets.  The people who really do need to rely on
this stuff sometimes don't have the practical ability to switch to
something that's not universally available or compatible with everything
else out there.  Overkill isn't overkill when we don't know the most
sensitive thing it will ever be used for.
And taking the recent noise about Quantum attacks seriously (jury's
still out, but security means being a pessimist) means 128-bit
encryption isn't such horrifying overkill as we used to think. In fact
if we really take the Quantum stuff seriously I'd be gritting my teeth
and setting 160-bit as the lowest acceptable bar.

@_date: 2016-04-17 11:43:38
@_author: Ray Dillinger 
@_subject: [Cryptography] Simple IoT sensor encryption ? 
I think he was saying that the adversary has physical access to examples
of the device that came off the same assembly line with the same
firmware - not that the adversary has access to the instant device.
This is the classic case of the Nest Thermostat - the first few examples
of which had been installed for only a couple of weeks before burglars
figured out how to read their signals in order to find out when the
owners were away and burglaries could proceed.
The burglars weren't already inside with access to that particular
thermostat; if they had been, they could have just looked around for
themselves to see if anyone was home, or installed their own IR sensors
and motion detectors alongside the Nest in order to duplicate the
service it was providing to them.  But they had been able to access and
analyze Nest thermostats, and learned the language in which it was
blabbing its owners' secrets.

@_date: 2016-04-17 12:26:57
@_author: Ray Dillinger 
@_subject: [Cryptography] Simple IoT sensor encryption ? 
I think the device needs to have a unique identity, but it can get one
easily: It is after all a sensor.
All it requires is that it's manufactured with a charged battery.
Let's say it's a thermostat.  Even though it's probably only accurate to
within a tenth of a degree, it's a safe bet that its thermometer has
a resolution of one percent of a degree.  So if it keeps making
full-resolution temperature readings every minute, XORing them with its
internal register, and hashing - from the time it leaves the factory to
the time its factory charge runs down, storing the low bits in
nonvolatile memory, then when it gets (re)charged and installed by the
user, it already has a bunch of bits that make it different from all the
other thermostats of its class out there.  When the user turns it on for
the first time, these bits are hashed to form the device's unique ID,
which it stores permanently.
Now, assuming you want the device to use a different key every hour
for the anticipated ten years of its installation, you need ~90K keys.
So you hash that unique ID 90K times, remembering the last 24 hashes,
which are your keys for the first day. The following day it derives
24 new keys, by hashing its secret 90K-24 times, again storing the
last 24 hashes to use as keys.  Rinse, repeat, and each key is the
hash preimage of the key used the previous hour.
The server knows that the device using the key that is the hash
preimage of one used the previous hour, is the same device.  Or
if it hasn't heard from a device in three or four hours, then it'll
be the third or fourth preimage of the last key the server heard.
The server can tell it's the same device, but a spoofer can't derive
the credential to impersonate it without taking that particular

@_date: 2016-04-17 15:32:56
@_author: Ray Dillinger 
@_subject: [Cryptography] Is "drivers for foo" a major malware vector? 
I recently went to the Internet to search for a possible
replacement for a device one of whose virtues was that
there has never been any driver requirement whatsoever.
It has a microcontroller with firmware that makes it look
like a bog-standard keyboard from the computer's point
of view.  So of course it works regardless of what OS
you have (regardless, in fact, of whether you have one -
you can use it to interact with the BIOS).
No drivers for this device have EVER been available from
the manufacturer.  The whole job is handled in firmware;
There would be nothing for drivers to do.
Nevertheless, when I searched for a possible replacement,
I came up with hundreds and hundreds of sites that were
offering free downloads of the drivers.
And the same sites come up with offers of free downloads
of software drivers for literally any piece of computer
hardware ever manufactured, by any manufacturer, with a
recognizable model name.
I can imagine absolutely nothing that these downloads could
be other than malware.  And of course if the dupe thinks
that they're drivers, of course they'd be installed with
root privileges.
Seriously?  That many sites?  That blatantly?  That
UNIVERSALLY?  With essentially no *OTHER* responses to
queries about any fairly obscure piece of hardware? And
I've never seen a specific "malware drivers" warning,
over and above the "don't install random downloads"

@_date: 2016-04-18 13:39:38
@_author: Ray Dillinger 
@_subject: [Cryptography] Is "drivers for foo" a major malware vector? 
Hah!  I'm not.  I'm looking for a replacement for my smart
keyboard.  The issue is that the only people who seem to
have even heard of it are offering drivers which don't exist.

@_date: 2016-04-18 16:13:33
@_author: Ray Dillinger 
@_subject: [Cryptography] Is "drivers for foo" a major malware vector? 
I don't know how I can be any clearer about this. What is the
cognitive barrier that is making the actual issue here go straight
past people?!  Okay one more time for people who aren't paying
I was not looking for a device driver.
I do not want a device driver.
I do not need advice about where legitimate device drivers can be
The device I am trying to replace was specifically designed to have
no requirement of any driver.
The manufacturer never made any such driver.
No such driver exists.
No such driver ever existed.
What I'm asking -- the real issue here -- is why nobody has
been saying anything at all about this enormous malware vector
operating right out in the open?!  There are literally
*hundreds* of sites out there brazenly offering downloads of
software they do not have - which they cannot possibly have,
because there is no such software!
There is something which they are pretending is software
that some people will want.  They have SOMETHING they want
people to download and install.  With admin privilege, of
It buggers my imagination that all of these hundreds of sites,
operating openly and with brazenly transparent lies, are
representatives of an entire industry spreading malware and
that NOBODY SO FAR HAS SAID ANYTHING ABOUT THEM!

@_date: 2016-04-22 14:11:46
@_author: Ray Dillinger 
@_subject: [Cryptography] Security on TRIM for full-disk encrypted SSDs 
Actually?  Downloaded one today.  3.5 gigabyte git repository.

@_date: 2016-04-27 16:02:35
@_author: Ray Dillinger 
@_subject: [Cryptography] Darpa wants a secure messaging app based on 
Eh.  Not all weapons are or should be government-exclusive.  If someone
developed something that was usable to restrain or immobilize a
combatant but would be really unlikely to do them lasting damage or
even cause significant pain, I think that would be preferred to
firearms and knives for civilian self-defense use.
Of course, like mace and all other "non-lethal" weapons, criminals
would also use them against victims.  But it's still nice if people
the criminals just want to rob, don't end up also dead.
PS.  And of course there are uses not intended by the manufacturers.
I was really glad when they switched from mace to pepper spray; that
made a potent, if sort of flavorless, hot sauce widely available,
where up till then I'd had to either make my own, buy by mail, or
pay outrageous prices at rare specialty stores.  But then crooks
started using it on people they were about to rob.  So now they're
regulating access to pepper spray in my jurisdiction, and I've gone
back to raising scorpion peppers and making relish.  Much less
convenient, but on the other hand it has a better flavor and it's
less expensive.

@_date: 2016-04-28 23:40:28
@_author: Ray Dillinger 
@_subject: [Cryptography] WhatsApp: Why asymmetric key instead of 
I keep hearing rumors about this "breakthrough." I don't
know how seriously to take them, but I suspect that if it
exists it's more likely to be deliberate sabotage at the
hardware/software/firmware level than it is to be the
often-implicated Quantum Supercomputer or major mathematical
I have no doubt that AT LEAST until very recently, and likely
through today, snoops have been able to get virtually
everything by systematically exploiting operating system,
protocol, application, and firmware vulnerabilities.  A
"major breakthrough" in those terms would just mean that
something that easy to exploit and very hard to detect or
secure came along.
Or something widespread was deliberately created that way.
It's hard to pick out which of the exploits security people
deal with are due to compromised hardware, or even know how
many of the crooks who've discovered security flaws have
discovered them by bribing, blackmailing, extorting, or
social-engineering the hypothetical SIGINT people responsible
for putting them there. Or, even of the crooks who haven't
done such things, how many of the flaws they've discovered
and exploited were deliberate.
But I keep hearing noises about a fundamental breakthrough
in cryptology, with the strong implication that it's some
kind of new cryptanalytic technique, mathematical insight,
or design principle for special-purpose custom hardware.
So let us look at the hypotheses.
Assuming they can get four orders of magnitude of hardware
efficiency for purpose-built AES cracking silicon, and back
it up with scores of billions of dollars per year investment
in constantly updating overwhelming volumes of this custom
hardware -- I still don't see anybody cracking AES-128 any
time soon without either a mathematical insight so profound
as to be completely unexpected, or a fundamentally new
computing technology like large scale Quantum Computers.
I mean, otherwise you'd have to pour enough raw electrical
power into the effort to boil an ocean, for even a tiny
chance of success.
If the QC threat is real (and recent guidance in ciphersuite
selection says to plan for the possibility) then we ought to
be using 256-bit keys for symmetric crypto.  And I don't have
a problem with doing that.  But most physicists are claiming
they consider it highly unlikely.
If the fundamental mathematical breakthrough is real, it's
very surprising that it hasn't leaked or been duplicated yet,
but in that case it's only a matter of time before one or the
other or both occur.  Speculating about the effect of a
fundamental mathematical breakthrough is at best hard to do
meaningfully; on the supposition that there are some things it
does and some things it doesn't apply to, maybe the best
insurance we can make against the day it becomes a widely
exploited attack is to superencrypt using multiple unrelated
encryption algorithms with independent keys.  But that
complicates protocol and application design and key management
in a way likely to create more vulnerabilities.  OTOH, doubling
key sizes - which we do *anyway* if we take QC attacks seriously
- is likely to de-fang most math breakthroughs as well. Even
the most serious have not been reducing security by the huge
orders of magnitude that would make up for doubling the key
And if, as I consider more likely than either, the
"breakthrough" is  mostly just a way of getting ubiquitous
firmware and/or hardware installed that contains deliberate
flaws or trojan horse backdoors, then most likely the noise
about encryption breakthroughs is a ruse or misdirection and
they're bypassing people's attempts to encrypt all together.
In that case it's there for crooks and foreign intelligence
to discover and exploit, and they certainly will.  The
question is when, and what kind of damage control we'll need
to do when it happens.  And, if we take deliberate firmware
or hardware sabotage seriously as a threat, we need to be
worried first and foremost about the nations where the chips
and devices are manufactured rather than (or in addition to)
America's NSA.
Does anybody with better math chops than me have even any
potential speculation about what a math breakthrough could be
about?  People I trust with good knowledge of physics are very
doubtful about the scale of QC but can't completely rule it
out.  So doubling key size is the conservative move on the QC
front, and may (or may not) be effective against a hypothetical
math breakthrough as well.
But the protocols and software are notoriously difficult to get
right, and we have new flaws discovered every week.  And as for
the firmware and hardware, that's turtles all the way down and
most of it can't easily be inspected.  Flaws there, especially
if deliberately hidden or designed to avoid discovery, could go
undiscovered until an attack that causes MAJOR losses.  I can
easily think of a half-dozen types of sabotage that a "major
cryptological breakthrough" could be in terms of something in
the software/firmware/hardware stack where the behavior does
not match the expectation.  So here's how I break it down.
"A large-scale quantum supercomputer is very doubtful."
"A mathematical insight of such magnitude is very doubtful."
"Sabotage, or just plain mistakes, in software, firmware, or
hardware are EXTREMELY plausible."
On the physics or math front, doubling key sizes is the
conservative move and ought to be adequate protection in
most "imaginable" scenarios.  But those scenarios aren't
all that plausible.  If we keep hearing rumors about a
major cryptological breakthrough....  then IMO we should
be looking for firmware and hardware sabotage.

@_date: 2016-04-29 17:04:38
@_author: Ray Dillinger 
@_subject: [Cryptography] More speculation on cryptographic breakthroughs. 
So, I'm sure this is no news to anyone here, but it's recently
been demo'd (again) that if you have somebody's phone number
you can go through the telco's billing systems to track and
eavesdrop on them.
And we already know that mobile phones can be used as passive
microphones/cameras without their owners' knowledge or consent.
So it leads me to wonder how many surveillance targets sit down
with their cell phone in their pocket and type passphrases to
decrypt things.  In fact, many might speak or whisper the
passphrase out loud as they're typing it, but that's not necessary
for this to work.
Give me a decent recording of a keyboard in use for a few minutes,
then let somebody type a passphrase on that keyboard and I bet I
can take the sound of that typing and tell you what it is.  Give
me decent recordings of a few thousand keyboards in use for a few
minutes each, and I bet I don't even have to get a baseline for
the exact keyboard in use first.
But remember, this crypto breakthrough is supposedly working in
favor of an organization known to like collecting "big data".
So give me audio recordings of millions of people using their
computers every day, and I'm betting it wouldn't take more than
a few afternoons to train a machine learning system that recognizes
which bits of the recordings are people entering their passwords
and stores the audio in a giant database.
The "major crypto breakthrough" that we keep hearing about, may
be just a giant database of audio recordings of people typing
passwords.  Everybody's passwords.  From terrorists to truckers
and from robbers to ranchers. It would explain most of the
claimed effects.

@_date: 2016-08-06 12:30:06
@_author: Ray Dillinger 
@_subject: [Cryptography] Generating random values in a particular range 
This is dumb, and if they ever try to extort money from anyone based
on this classic algorithm, shooting them down will be horribly easy.
If the court so desires I can show them a computer science textbook
printed in _1982_ that specifically gives that very algorithm and
specifically warns against the modular construction as being biased.  I
think I call that "prior art."
In following pages it even goes on to show how to calculate the exact
degree of bias so you can tell within engineering constraints when the
modular construction is acceptable.  Then it gives a six different tests
for predictability that many common pseudorandom generators at the time
were failing, four 'whitening' algorithms (one of which even looks a lot
like cryptographic hashing) to extract bit streams passing those
statistical tests for predictability given bit sources that do not, a
couple of pseudorandom generators that were passing all the tests known
at the time and comparisons of their efficiency, and algorithms for
calculating the period of lagged-fibonacci and LCG type generators.
Did RIM try to patent all of those basic well-known algorithms, too?

@_date: 2016-08-07 11:45:16
@_author: Ray Dillinger 
@_subject: [Cryptography] Generating random values in a particular range 
You still wind up with sets of different possible numbers having
different probability.  The set sizes and the differences in probability
are unchanged.  The only difference is that now the sets are less
obviously correlated.  ie, you don't have "all numbers less than x" and
"all numbers greater than x" with different probabilities, you have
sets the same size but with the set membership now spread across the
full range of possible outputs.
This will make a primitive magnitude-bias test pass (in the long
run the average of all outputs does not drift away from the
expected mean nearly as fast).  But it will still fail any
selection-bias test, because it doesn't actually make the choice
among numbers in a way that's any less biased.

@_date: 2016-08-07 22:49:01
@_author: Ray Dillinger 
@_subject: [Cryptography] Where to Find PQC Crypto Libraries? 
On enormous mathematical objects -- I don't really know what mathematics
types consider "enormous" but I recall a proposed hashing scheme for
an altcoin block chain, intended to be horribly slow, which required
random accesses into a large object (bigger than any computer memory)
that could only be generated sequentially.  And it was a different
instance of such an object for every thing being hashed.
The intent of the author (earnest, bright, and terribly nave) was that
people would have to generate the object up to the points of references
many times.
The reality of course would be that people would do their hashing using
rainbow tables instead. Generate the object once saving the
(rather large but but reasonable for hard drives) generator state at
a hundred different checkpoints along the way, and implement the random
accesses by loading and regenerating starting from the checkpoint
previous to the indicated location.
Which, thanks to the regulated difficulty to produce blocks at
predictable intervals, wouldn't have been all that much of a
problem for the block chain - it mostly would mean the miners wore
out hard drives in addition to burning CPU hours.

@_date: 2016-08-11 10:45:54
@_author: Ray Dillinger 
@_subject: [Cryptography] BBC to deploy detection vans to snoop on 
Generally speaking though that wouldn't work unless people happened
to be watching the shows at the exact same time they were being
While that was a reasonable assumption in pre-VCR days, and not too
unreasonable in the days when stolen cable was mostly directly plugged
into the TV set, it's becoming VERY unreasonable when people are
watching programs with their computers. First, there's a variable
delay while it's downloaded.  Second, it's downloaded in the default
case directly onto storage media.  For someone to be watching that
stream, and for their playback to be at the exact point of being
caught up to the broadcast, is both unlikely, and from the POV of
the watcher, undesirable.  It would mean, among other things, not
being able to skip commercials and other unwanted content delays.
It would also mean that the watcher hasn't, even once, jumped back
a minute to watch something again after being interrupted by, say,
the telephone.
Seriously, if that's how they're working, I'd expect it to not work
at all.
Surely they've got some better technology than that.

@_date: 2016-08-16 14:02:55
@_author: Ray Dillinger 
@_subject: [Cryptography] Electronic currency revived after 20-year hiatus 
I consider Chaum's scheme to be a weak form of digital
currency for several reasons.
These notes cannot circulate.  Alice and Trent (the Bank)
cooperate to create one, Alice spends it to Bob, and then
there is absolutely nothing Bob can do with it except
bring it back to Trent.  If Alice spends the note
multiple times, the spends cannot be detected until the
at least two copies of it are seen by Trent. So in
practice Bob isn't going to take the note unless he
has a 'live' connection to Trent.  And that puts the two
parties who can cooperate to de-anonymize the note (not
cryptographically, but in practice Bob will know who's
making the purchase and will tell Trent) directly in
contact with each other at the moment of the spend.
At the very least Trent will know Bob redeemed it and
if the amount is at all significant, compliance with
most accounting laws will require Bob to know Alice
spent it.
The Trusted party, even if they don't cooperate with
Bob to deanonymize Alice, is also able to steal if
they so choose.  This is normal with banks and checking
accounts, but not with any kind of cash.
And IIRC, the existence of a Trusted party was a significant
problem with most of the early attempts to deploy Chaumian
e-cash.  In fact, the weakness and/or larceny displayed by
most of the issuing institutions was one of the main reasons
for failure at that time.
It doesn't have to be that way; a real bank handling these
digital notes with the same reliability that they handle
transactions in other accounts would be, in theory, no
more susceptible to such failure than it would in its
other accounts.
And there's a salient point here; Bitcoin-like solutions
with a block chain, while reasonable for some applications,
simply won't scale to (say) 20 billion transactions a day.
We still need to be thinking hard about e-cash, because it
is not a solved problem.  If it's ever going to be
mainstream, something has to replace block chains.

@_date: 2016-08-16 17:47:05
@_author: Ray Dillinger 
@_subject: [Cryptography] Electronic currency revived after 20-year hiatus 
That is an interesting assertion.  I do not believe it.
20G transactions.  Assume 100 bytes per, and that's terribly generous
in terms of how small you think you can get them.  2T bytes per day.
You have a data plan that will allow you to download 2T bytes daily?
You think you'll have one by the end of a year? You have a hard drive
that can store 700 Tbytes for even one year's worth of block chain
history? You think you'll have one by the end of a year?
The only parties with that kind of connection and storage will
perforce act as Trusted Parties to all other users.  That means
they'll either Gox the users over and over, or they'll become
just as heavily regulated, insured, and monitored as any
conventional bank.  A banker is a banker is a banker.
The vast majority of people are already letting strangers hold
their keys and trusting that they'll get their bitcoins back.
That means the online-wallet people and the exchanges function
as banks.  And these banks are Trusted. When they embezzle
(or even if they just get robbed), it leaves people broke.
Ending the existence of Trusted parties and their ability to Gox
people, if you recall, was sort of the point of the block chain.
It failed.

@_date: 2016-08-18 11:40:58
@_author: Ray Dillinger 
@_subject: [Cryptography] Electronic currency revived after 20-year hiatus 
You're wrong.
Existing solutions for the Byzantine Generals Problem provide for any
two correctly functioning nodes to reach agreement regardless of what
any or all other nodes do.  Sybil attacks do not affect the correctness
of the solution.
The problem with them is that required bandwidth per peer scale with the
square of the total number of all other peers.  And sybil attacks DO
affect the scale of the problem for bandwidth purposes.
With block chains the required bandwidth per peer scales linearly with
the number of all other peers (well, technically with the size of the
transactions done by all other peers), making it more scalable (but
still not scalable to the level of a widely used payment network).
The "Lightning Network" solves a problem for people who do many
transactions per day with each other.  There is no merchant nor client
with whom I do more than one transaction per day, so absolutely none of
my transactions would wind up using it.  I expect it to have some real
impact on the bandwidth and size of the block chain because some
business models are absolutely pathological in terms of number of
transactions per day by individual customers (like casinos or
speculative markets) but not nearly as much as its proponents hope
because it simply isn't applicable to ordinary tx (like grocery
stores, office supply vendors, restaurants, or investment markets).
And it still doesn't solve the problem that Trusted parties have emerged
in the form of online wallets and exchanges etc, and they are failing
and/or goxxing people on a regular basis.
Multimillion dollar losses, thefts, and swindles impose public as well
as private expense.  Legislators, Law enforcers, and civil courts cannot
ignore them because the people who get goxxed when they happen won't
allow them to.  Legislation, law enforcement, and both civil and
criminal prosecutions in court are damned expensive, and will
accordingly attract taxation and regulation as the taxpayers
underwriting that expense use their governments to seek to limit their
Being a self-sufficient or self-regulating system does not include
placing fiscal burdens on the public without taxation. Nor does it
include freeloading on public law enforcement, legislative, and judicial
resources without regulation.

@_date: 2016-08-18 12:09:47
@_author: Ray Dillinger 
@_subject: [Cryptography] Generating random values in a particular range 
I note that the no-increment solution creates cycles whose period
resists specific calculation but which on average have one half the
order of magnitude of the state. The with-increment solution creates a
single cycle whose period can be easily calculated and has the same
order of magnitude as the state.  Increasing the size of the state to
achieve the same longer period would increase the cost of hashing by an
amount greater than the cost of a single increment.
In practical terms, whether this is significant or not probably depends
on other engineering concerns, assuming your RNG state is at least
quadruple the size of any keys you might generate using it.  If your
state is only twice the size of any keys you might generate using it,
the with-increment method is to be preferred for security reasons.  If
your state is less than twice the size of any keys you might generate
using it, your system is simply wrong and the method doesn't matter.

@_date: 2016-08-18 13:25:36
@_author: Ray Dillinger 
@_subject: [Cryptography] Shadow Brokers :: powerful NSA hacking tools 
If we're talking favorite conspiracy theories, I have a couple to share.
Some actor has been hacking the DNC in what is apparently an effort to
either influence the outcome of the US election or cast doubt on the
legitimacy of the result.  Russia and China have both been mentioned -
Russia more often and loudly - as the probable actor behind these hacks.
Clearly whoever stole this cache of malware three years ago has had
plenty of time to develop tools to detect and track its use.  It is
likely that they now have LOTS of dirt on US intel operations during the
last, say, 2 1/2 years.  An implied threat that this dirt might be
released, could be considered as an implied warning to the US that
attempts to loudly blame anyone, or impose sanctions against anyone, for
the election hacks could be responded to via further and more sensitive
Russia and China have both again been mentioned as the probable source
of the NSA malware release.  Russia, as with the DNC hacks, has been
mentioned more often and loudly.
Interestingly, Snowden at most recent update is residing in Moscow.
The most recent dates in the cache are from some months AFTER Snowden's
departure from the NSA's data center, so they clearly didn't come out
with him.  It is at least dimly possible that knowledge of how to
acquire them did, although the idea that such knowledge might still be
relevant after the inevitable revamp of security following the leaks
buggers the imagination. However, it is *extremely* likely that
knowledge of someone able to execute such an acquisition was already
present in Moscow.
Of course it is also possible, as Jerry says, that this release is a
deliberate and calculated decision by the NSA.  I seriously doubt this
possibility, because the NSA, which used to be extremely secret, is now
extremely reputation-conscious. They would be very unlikely to
voluntarily pretend that their own security had been compromised.
Still, there are reasons for them to do it.  They must have been
furiously developing new malware since the Snowden leaks three years ago
and may now have an effective new suite of tools.  There is
circumstantial evidence that other nation states have been using this
malware or the security flaws it exploits in hacks against the US.  A
release can motivate people to fix security flaws that were once seen as
assets but which have now become an active vulnerability. Simultaneously
a release from an anonymous source creates a pretext for which Snowden
and/or Russia can be blamed to people in the US at various levels, at
least in covert, implied, or unofficial ways.  Or they may be trying to
motivate intended targets to buy a new set of routers, switches, and
servers that have been compromised in new and interesting ways as yet
unknown to other actors.
History teaches that organizations with more than two or three
employees, operating on a more than occasional basis, cannot keep
secrets.  They can, however, distribute misinformation in hopes of
confusing people about which of the discovered secrets are real.  They
can deliberately lie to each other in order to try to get others to make
mistakes giving their secrets away.  And they can exchange threats,
blackmail, extortion, and warnings of varying degrees of implied,
covert, and overt, in an effort to deter unintended parties to those
secrets from disclosing them.
A wilderness of mirrors, indeed.  Working in security, even
occasionally, already turns me into a screaming hair-triggered paranoid.
 As someone prone to thinking too much, the environment in which they
live would likely drive me insane. If the ability to spin this much
speculation and conspiracy theory within a half-hour or so of hearing
the news is any indication, maybe I should have said "even more insane."
I would probably be less paranoid, overall, if my most paranoid
fantasies turned out to be wrong more often.

@_date: 2016-08-19 21:01:33
@_author: Ray Dillinger 
@_subject: [Cryptography] Robust Linked Timestamps without Proof of Work. 
So you have Trent instead of Sibyl.  Technically either is just as bad,
and nobody wants to take the time and trouble to deal with Trent.  The
whole point of the Proof-of-work thing is that Sibyl can't do damage
for free.  If you're using a Trusted system with gatekeepers who can
screw it over or keep people out, then you don't have Sibyl in the first
place.  But that doesn't mean you have a problem that's any smaller.

@_date: 2016-08-20 12:12:28
@_author: Ray Dillinger 
@_subject: [Cryptography] Robust Linked Timestamps without Proof of Work. 
If there isn't a limit on the number of trent, then the system will
appear to be working fine until people realize that 90% of all the
trent are actually Sybil.
If one must establish trust relationships with established nodes,
that just means that it's easier to break in by being ten thousand
fake nodes gradually allowing the legit nodes to join the network,
than it is by being one fake node that's trying to quickly join a
legit network.
I don't think Proof-of-work systems are the only way.  But 'mumble-
mumble-trust-relationships'  is not sufficient.  Exactly why will
Sibyl will have more difficulty establishing her ten-thousandth
fake node (given a "trust relationship" with 9,999 nodes already
established) than Alice has in establishing her first real one?
Remember: Sibyl doesn't have to break in in a single afternoon.
She will be there while you're building the walls, when you're
putting hinges on the doors, and when you're sharing the keys
around among "yourselves" days, weeks, or months before any hint
of 'disorderly' or 'noncooperative' behavior begins.
The problem is that you're setting up a system where you want a trust
relationship but there is a dead hand on 99%+ of the switches that
decide whom to trust.  The failure of the CA role in the PKI process
taught us exactly in what ways that doesn't work.
Sibyl will just meet whatever static criteria the dead hands find
acceptable and get her thousands of fake nodes (all trusted by each
other) trusted by some of them - then by extension trusted by all the
others whose dead hands use the trust relationships of that 'some' as
a criterion.  And she will be there from the beginning, while the
system is being set up.  Legit nodes will trust her fake nodes because
they want to join the network.
If the system isn't implemented in a way that allows Sibyl to do this,
then it will be effectively impossible to set up legitimate nodes
without individually, personally, contacting thousands of other human
beings one at a time - rather in the way that CAs were supposed to make
sure that people were who they said they were, but don't.

@_date: 2016-08-22 10:42:04
@_author: Ray Dillinger 
@_subject: [Cryptography] Real-world crypto/PRNG problem: Bridge 
A bridge hand contains more than enough information to uniquely
determine the state of the PRNG at the time of its creation.
There is a bit more additional snark than you mentioned.  First
the addition to the seed after multiplication appears to be 11
rather than 13.  Second the RNG output is being used to select
the next card from *each* of 2000 decks before going back to
the first deck, so when you see any single hand you are looking
at every 2000th output of the the PRNG rather than sequential
outputs of the PRNG.  The guy setting up the system probably
thought this made it more secure.  However, with an LCG this
is easy to solve.
The decks before shuffle are sorted in sequence ace through king
of each suit in order hearts, clubs, diamonds, spades.
I've cracked this same problem before in the context of an online
poker system that was losing money and the operator didn't
understand why - except he was only using a 32-bit seed and
the PRNG built into the MSVC compiler.
FWIW, There are a bit more than 8 x 10^67 different permutations
of a 52-card deck.  Which is to say, about 226 bits of state.
If you want fair shuffles - even remotely fair - use at least
256 bits of state for a PRNG to deal a single deck.  That will
mean there isn't enough information in any single hand to
narrow down the state of the RNG when it was dealt in any way
useful to a player.
If you want every possible set of 2000 deals, you will need
to start with a PRNG having 512kbytes of state - which is
a bit ridiculous but easy to do.  In practice one would use
a CSPRNG with 2k or 4k state - This could deal only a tiny
fraction of all possible combinations of 2000 hands, but by
any means we currently understand nobody could work out which
If you regard it as really important (on the same order as
nuclear launch codes), you'd use a TRNG such as the one
built into /dev/random on modern computer systems.  The TRNG
could deal all possible sets of 2000 hands, but on most
systems demanding 512kbytes of random bits at once would
mean it might block for a while (more than a second, less
than an hour) gathering bits from hardware sources like
internal temperature fluctuations, bus, disk, and cache
latency, turbulence inside hard drives, etc.
If the machine has been running for at least an hour since
bootup, the output of /dev/urandom ought to be indistinguishable
for any practical purpose from the output of /dev/random.
Sigh.  The more I deal with this the more I hate the word
"Random."  Trying to define it rigorously is like trying to
nail jelly to a tree and the phrasing leads people to believe
it's a property of a number rather than of a sequence.
"Entropy" in the non-physics sense is even worse.  I think
I prefer talking about "Predictability" rather than
"Randomness"  because it at least leads to the right concept
(property of a sequence) and the right questions: predictable
to whom, by what means, and with what available information?

@_date: 2016-08-24 14:06:59
@_author: Ray Dillinger 
@_subject: [Cryptography] Retire all 64-bit block ciphers. 
So it turns out that on average after you've sent 32 Gbytes using a
64-bit block cipher in CBC mode (ie, normal for TLS) there's a
plaintext recovery attack that starts working.  Even if the attacker
hasn't the key.
Time to up the game to ciphers with 128-bit blocks.

@_date: 2016-08-24 14:18:18
@_author: Ray Dillinger 
@_subject: [Cryptography] Insecure email might be an even bigger problem 
On the one hand:
Oh good.  Some other people are reaching the same 'paranoid
conspiracy theory' I spun the other day.  So I'm at least no
crazier than you.
On the other hand:
This.  Is.  BAD.  An attack on the integrity of elections by
a power foreign to the nation in which those elections are to
be held, cannot be allowed to stand. Regardless of who the
attacker is.
On the gripping hand:
F***in' called it.
PS.  I still wish my most paranoid conspiracy theories would
     turn out to be WRONG sometimes!

@_date: 2016-08-25 15:23:22
@_author: Ray Dillinger 
@_subject: [Cryptography] Say 'unguessable' not random 
The Poker Protocol keeps getting reinvented.  I thought I invented it,
but Adam Back got there before me and Bruce Schneier got there before
Each participant chooses a key and derives a long sequence from it
by hashing.  Play starts with each player revealing the last element
of their sequence.  After that, "random" elements are determined by
having all the participants reveal the hash preimage of the number
they most recently revealed.  These elements are combined (usually
via XOR) to determine the choice.
Because nobody can find a preimage of anybody else's hash function,
nobody can predict what the forthcoming numbers from anybody else's
sequence of preimages will be (and therefore discover what the next
'random' element is).
Because nobody can prevent the other players from noticing any
number they present which isn't a correct preimage of the last
number they presented, nobody gets a chance to pick a number
giving their preferred output, even if they have already seen
the numbers presented by all the other players before they give
Because everybody knows when placing bets (or whatever) that they
haven't yet revealed the number that will be used for *their*
contribution to the next choice, yet, nobody else has looked at
it before placing their own bet (or whatever).
And after the fact anybody can audit the preimage chains and make
sure they all match up.
No Trent is required; bandwidth requirements are minimal.  The
requirements for cheating amount to finding the preimages for
everybody's most recent revealed sequence element.  Or breaking
into everybody's computer and stealing the next preimage without
their knowledge.  But if there is a single player out there who
can keep you out of their system and whose hash you can't
preimage, it doesn't work.
It's even suitable for a pure serverless P2P implementation.

@_date: 2016-08-25 15:30:33
@_author: Ray Dillinger 
@_subject: [Cryptography] ORWL - The First Open Source, 
And the first thing that happens if you go to the site is that
it tries to invoke Flash to play video.  The people trying to
design a secure computer don't know about the problems with
having Flash enabled?
This does not inspire confidence.

@_date: 2016-08-26 18:54:26
@_author: Ray Dillinger 
@_subject: [Cryptography] programming languages and the people who (don't) 
was "NSA-linked Cisco exploit poses bigger threat than previously thought"
I use C when writing crypto code because there exist compilers for
C that allow me to control whether or not "redundant" copies of
sensitive data get made, whether things get overwritten before
they get released, exactly how representations work, allow me to
assure that there are no bits unaccounted for (and potentially
not overwritten) when a structure is built in memory, etc, and
all that dangerous stuff.
All that dangerous stuff which causes exploitable security flaws
in languages where you CAN'T do them.  C is a language that in
practical terms and in most implementations allows you to control
side effects and therefore side channels.  It makes it terrifyingly
easy to screw up in some ways, but I simply have no way to avoid
screwing up in other ways unless I use it.
I can write C code, compile it using TCC with no optimizing, and
go check the machine code to make sure that buffers get erased when
they're supposed to get erased, that unsecured copies don't made
just because that would make some operation faster, that the compiler
didn't substitute an algorithm subject to a timing attack for the
one I intended to get, that the fields in a structure don't get
realigned in ways that might leave sensitive bits unerased between
fields, make sure that variable writes that clear sensitive
bits happen immediately when they're supposed to happen, including
those parts the compiler can prove won't affect later computation
affecting the things it considers the important outputs, etc. etc.
C even gives me tools like 'volatile' that mean I *DON'T* usually
have to go all the way down to machine code to get it done.
To be fair the C standard no longer guarantees all these things, but
at least in many C compilers they are still available.  In almost no
other language save assembly is it possible to write code where you
know what it *WON'T* do, or that it WILL do what you tell it regardless
of whether it affects a side-channel output the compiler won't believe
anyone cares about.

@_date: 2016-08-26 19:01:53
@_author: Ray Dillinger 
@_subject: [Cryptography] Insecure email might be an even bigger problem 
To be fair when three recent zero-day flaws affecting the iPhone were
discovered, (when an Isreali/American information weapons supplier's
malware was used to attack the phone of a human-rights attorney) Apple
fixed them and pushed out a patch within ten days.
So it isn't *ALL* doom-and-gloom; there are some people who take it
seriously and fix security bugs fairly quickly.

@_date: 2016-08-29 14:51:58
@_author: Ray Dillinger 
@_subject: [Cryptography] tail recursion in C [was Re: "NSA-linked Cisco 
FWIW, for the first several iterations of the standard,
function calls were literally flow-of-control operation
in scheme.  People have since added looping constructs,
in an effort to check off "doesn't have X" arguments by
newbies against using the language, but looping structures
are usually defined as macros - they are syntactic sugar
for flow-of-control via function calls.  Direct use of
recursion is still the easiest and simplest way to express
Guaranteeing an unbounded number of open active tail
calls has the same importance in scheme as supporting
loops and other flow-of-control structures in imperative
languages. It is not optional.  It is not an optimization.
It is a necessity.
The very simplest interpreters always allocate call frames
on the heap, and garbage collect them like any other data.
More sophisticated compilers use the hardware stack when
possible, but this requires some program analysis for
figuring out when it's possible.

@_date: 2016-08-30 15:14:56
@_author: Ray Dillinger 
@_subject: [Cryptography] N. Korean radio broadcasts string of random 
Whenever somebody does this, everyone around the world sits up and
takes notice, writes news stories, etc etc etc....  have we ever
considered the possibility that it's just somebody with a homemade
transmitter who's trolling them for giggles?
I could pick up an amateur radio tomorrow and read the output
of a CPRNG out loud into a microphone.  Would that cause an
international incident?  Would I wind up talking to the guys with
the expensive sunglasses and cheap shoes again?

@_date: 2016-08-30 15:20:15
@_author: Ray Dillinger 
@_subject: [Cryptography] Strength of 3DES? 
'Tis true, but the SWEET32 attack still requires a buttload
of ciphertext.
Key grinding can get plaintext even from just a few kilobytes
of ciphertext.  But yeah, if you've really got 126 gallons
(honest, that's how much a buttload actually is!) of ciphertext,
then the SWEET32 attack is much much easier.

@_date: 2016-08-30 15:33:14
@_author: Ray Dillinger 
@_subject: [Cryptography] Key meshing (Re: [Crypto-practicum] Retire all 
Depends on how much and how often.  Servers in big server clusters
are pretty precisely tuned in terms of how many connections they serve;
what doesn't impose a detectable penalty on a desktop machine, still
matters in terms how many servers you need to handle ten thousand
simultaneous sessions.
In response to your question, almost every Feistel-type cipher has
poor key agility.  The construction you mention, if deployed and
used for general purposes as a symmetric cipher, would multiply
the cost of servers by a non-trivial amount.
For a few brief years, the advance of CPUs was vastly in advance
of the available bandwidth. The desktop machine that pegged its
CPU for a few seconds of each day was the design target, and CPU
costs didn't really matter.  But now it matters again because
bandwidth allows people running servers to use all the CPU they
can afford.
For a few brief years, the advance of CPUs didn't permit them to
be battery-powered under normal circumstances. The desktop machine
with a plug in a wall socket was the design target, and CPU costs
didn't really matter.  But now it matters again because CPUs have
gotten small enough to run on batteries, but battery power still
limits CPU cycles.

@_date: 2016-12-01 14:05:57
@_author: Ray Dillinger 
@_subject: [Cryptography] Gaslighting ~= power droop == side channel attack 
Yes, growing your own at home is explicitly legal according to state
law, like brewing your own hooch.  Unless you're growing more than
anyone believes you could consume yourself, and not licensed to sell.  I
think the guidance is no more than 3 or 5 plants, but I could be wrong.
Nothing that would require a server room full of grow lights anyway.
Still, now that commercial farmers are moving into the market, I imagine
in a few more months a grow-light operation is going to be about as
commercially viable as my hobby of growing ghost pepper plants when I
can make my hotdog relish by buying habaneros by the pound cheaper than
the potting soil I use every year.
I imagine Trump will be concerned about the inevitable fact of immigrant
workers doing the picking and processing, while Pence will be upset
about the possibility that somewhere someone might be enjoying something
immoral.  Screw them both, says the California Legislature.
There's an odd reciprocal effect going on here; people have less and
less privacy, and other people (except for advertisers, I guess) give
less and less of a flying crap about what they're doing.
There was a time when people would have been ashamed for it to be known
they were buying, or selling, sex toys for example, and businesses would
have been boycotted etc; now they're on sale and recommended as
christmas gifts at Walmart.  Lack of privacy, or lack of caring about
privacy, has revealed how utterly normal this is, and most people just
don't care about it anymore because normal means boring.

@_date: 2016-12-01 14:19:55
@_author: Ray Dillinger 
@_subject: [Cryptography] RNG design principles 
I can picture it.  A tiny little ridge on the brim of a baseball cap,
the inside of which (where no one except the wearer can see it) shines a
randomly selected one of five low-power LEDs every 3 seconds or so.  Not
hard to build, and even a pretty simple pseudorandom generator  wouldn't
provide enough information in the course of a single game to reveal its
I figure it's not cheating.  It's legit sports equipment, like a
baseball cap to keep the sun out of someone's eyes or cleats for better

@_date: 2016-12-02 22:11:02
@_author: Ray Dillinger 
@_subject: [Cryptography] OpenSSL and random 
I don't think we need to provide a special device to interface with "low
quality randomness."  The system libraries of every development
environment that exists have got it covered and people do use those
library calls when they're not doing crypto.
When you need faux nondeterminism for non-security purposes, the normal
thing to do is to seed the PRNG from your standard library ("random" in
C++ for example) with some transient value like the time or whatever,
and get a sequence of numbers.
The Mersenne Twister, Lagged-Fibonacci generators, the modular sum of
three linear congruential generators, Arcfour, etc - whatever the
library provides.  These sources are repeatable and predictable (most of
them _easily_ predictable).  They are the wrong thing for cryptography,
but still entirely adequate for hash tables, simulations, etc.
And now that I think of it I haven't seen for at least a few years
anyone mistakenly using those generators for cryptography.  **
I used to have to gently inform people using the "random()" PRNG for
crypto purposes was a mistake about once a month, but I think this is a
lesson that has finally filtered down to the rank & file.  So, um, yay?
Also passwords are averaging 1.5 to 3 characters longer than they were
in 1985.  They're getting there in baby steps....
**There's been an instance where I had to tell someone that
   their online gambling games needed real randomness too,
   and then the bridge tournament shuffling program that got
   broken last year.  So we still see library PRNG's being
   misused in security contexts, but that's not exactly crypto.

@_date: 2016-12-04 12:16:44
@_author: Ray Dillinger 
@_subject: [Cryptography] OpenSSL and random 
And don't you need some magic to ensure that the owner of the machine
is the only one who has access to it?
The problem with anything that comes with the machine (or the OS) is
that it comes FROM someone else.  In fact it comes from ANYONE who has
had control of the machine before you get it or has control of the
OS installation disks before you have them in your hands, because
they may have read them or may have swapped them for something else.
So, no, I don't see this as a good plan.  They're certainly all things
to stir in, but they're not something we can pretend is bits unknown
to all possible adversaries suitable for "credit" to an RNG state.
And I'll reiterate the notion that generating keys, or taking/making
network connections, prior to being fully booted up is ALWAYS a bad
idea.  A machine is not fully booted up before it has RNG state that
can be relied on.
OTOH, the 'live disk' OS for an OS distribution disk could gather
entropy from the local machine for a few seconds before it makes its
first network connection to continue with the install process.
So it should have bits prior to downloading any software, prior to
writing anything at all on the hard drive (even prior to partitioning
the hard drive) and FAR prior to running any of the software has
downloaded.  And it should have another good random state to save, long
before the new OS boots.
This isn't a good solution unless it's the user whose secrets are to be
protected who does the installation, though, so it's out for all the
portable devices and computers that come with OS preinstalled.  And
that's the vast majority of devices.

@_date: 2016-12-04 12:37:23
@_author: Ray Dillinger 
@_subject: [Cryptography] Final words on RNG design 
It's my opinion that mentioning NOT using the CSPRNG in situations that
don't warrant its use (such as, eg, making ordinary data hash tables) is
warranted, in favor of relatively simple library PRNGs.

@_date: 2016-12-06 10:32:06
@_author: Ray Dillinger 
@_subject: [Cryptography] Anyone else seeing an uptick in infected IoT 
In the last few weeks I've seen something close to a doubling of the
number of botnet-infected IoT devices at sites around the SF Bay Area.
Or maybe it's just a doubling of the rate at which people notice them,
I'm not sure.
Anybody else seeing this?  Is it a local effect or worldwide? Anybody
know what's infecting them?  Mostly they got noticed when people found
themselves on spam blocking lists due to spam originating at the sites.
The increase in rate of infection seems to apply only to ARM devices
but that doesn't narrow it down much.
The reason why I'm wondering if it's mainly a local effect is because
there doesn't seem to be any IP address that they're "phoning home" to
and they're not doing DNS queries for any names.  Virtually all their
Internet traffic is outbound.  I don't see any inbound channel big
enough to be domain or email address lists, and they go silent if
powered up at a different location unless that location is also
infected. I'm wondering if the control channel (and possibly the
infection vector) is wi-fi. Commands jumping between local networks by
wi-fi would spread in most urban areas because most places have a dozen
overlapping wi-fi networks and wi-fi security is crap. But if so then
control would be local to a metro area.

@_date: 2016-12-07 14:20:24
@_author: Ray Dillinger 
@_subject: [Cryptography] Anyone else seeing an uptick in infected IoT 
Aw, crap.  Well, that explains it.
There is a certain irony in security cameras getting infected
by a malware worm.  Forgive me if I'm not laughing.

@_date: 2016-12-07 19:44:52
@_author: Ray Dillinger 
@_subject: [Cryptography] Anyone else seeing an uptick in infected IoT 
They talk about security and authentication, then go right into
how someone can use "shared secrets" ie, a default password, to
load new firmware on the device remotely.
   ~   ~
   0   0
    <_>
Yep, that's "calling all botnets" loud and clear.
Hooboy.  We need to focus more on educating the public, don't we?

@_date: 2016-12-15 18:43:36
@_author: Ray Dillinger 
@_subject: [Cryptography] Photojournalists & filmmakers want cameras to be 
Encryption, as such, is valuable but the value might be limited
by the reaction of people who do censorship.  It is hard to
imagine that the sort of people who confiscate cameras with
particular types of footage on them would not also confiscate
any camera specifically designed so that they couldn't tell what
kind of footage was on them.
If such a camera did exist, I think it would be confiscated at
*EVERY* checkpoint where anyone might seize an ordinary camera.
People protecting secrets will assume the worst (from their point
of view) every time such a device is found.
OTOH, a camera you can't see into probably wouldn't justify
the expense of jailing or torturing somebody - the response
would be limited to the seizure of the camera leaving the
journalists considerably safer.
Whether contents are encrypted or not, crypto hardware is very
much worthwhile to build into cameras, because such hardware
could also do crypto authentication for evidentiary purposes.
If the signing private key never leaves the camera, images and
video shot by it can be shown with a very high degree of
confidence not to be faked or altered.
A running signature scheme for video such that the signature on
every (hundredth or so) frame can be used to verify that none of
the preceding few thousand frames are missing or altered would
be pretty simple to design; it would just use a hash chain,
allowing detection of quick-cut edits and rearrangements that
might escape notice otherwise.
This is a serious issue in the age of digital editing.  If the
prosecution shows footage of the defendant kicking a puppy, the
defense can then claim it's faked and distribute footage of
every member of the jury doing likewise.

@_date: 2016-12-16 16:16:57
@_author: Ray Dillinger 
@_subject: [Cryptography] Photojournalists & filmmakers want cameras to be 
It's journos who are asking for this, and their cameras are definitely
part of that 1%.  A reporter who can show that her footage isn't
tampered can limit the degree to which people are able to credibly deny
or discredit her story.
Now think evidence again.  Outside of journalism and investigation
contexts, you don't know in advance which cameras are going to be used
to record something that becomes evidence. A random college student at a
demonstration takes footage of an incident of police brutality using
whatever camera they happen to be carrying, and either is, or isn't,
able to convince reporters/school officials that the incident wasn't
faked, or convince police higher-ups that it can't be credibly denied.

@_date: 2016-12-26 12:26:30
@_author: Ray Dillinger 
@_subject: [Cryptography] Are there random sources? 
This has been my contention for a long time.
We cannot measure randomness because it's an ill-defined idea. More to
the point, it is a concept that exists only in the abstract.  We should
not expect to measure randomness, any more than we can count angels.  It
is a word that fools us into thinking that it's a definite, concrete
thing. But we are using the term only to make statements about our (or
our opponents') ability to anticipate or understand a thing, not about
the thing itself.
    Eggs Ackley.
                   Bear

@_date: 2016-12-27 12:30:01
@_author: Ray Dillinger 
@_subject: [Cryptography] While we're on 'Randomness' again.... 
Worthy of note:  The 'dieharder' battery of tests for the statistical
properties of streams of bits is often used to reject any pseudorandom
sequence generator that shows detectable patterns in  output.
It can also be used as a rejection test of any encryption or hashing
algorithm anyone comes up with.  Proper cryptographic output of any kind
is indistinguishable from random bits.  So if the output fails
dieharder, then it most definitely is not secure encryption.  This alone
is sufficient to demonstrate a problem with  (or as a guide to where to
start cryptanalyzing) most kinds of homebrew cryptographic algorithms.
But things like the Mersenne Twister pass dieharder, and while it's fine
for simulations etc, it most definitely is not a secure source of bits
for cryptographic purposes. In fact as a GFSR I find the fact of it
passing at all to be counterintuitive and somewhat amazing.
Acceptance tests, both for cryptographic primitives and for supposed
sources of unbiased bits, are effectively impossible to come up with,
for reasons we've been talking about.

@_date: 2016-12-28 12:13:29
@_author: Ray Dillinger 
@_subject: [Cryptography] Should we always KeyWrap, even with Key Recovery? 
This is true, but so far I had been under the impression that it doesn't
matter for related-key purposes if the RSA key factors selected are
non-identical strong primes.
That said, my number-theory-fu is certainly much weaker than yours,
given your work at Verisign.  If you believe encrypting the same data
under different RSA keys could become a problem, you are far more likely
than I am to be correct about it.
Bearing this in mind, I devoted a few minutes to research to see if I
could find evidence to support your concern.  And it turns out that
there definitely is supporting evidence.
See this year's usenix paper at
Evidently an attacker is able to determine what implementation has been
used to generate primes, and therefore narrow down the possible set of
primes they need to check, based only on patterns detectable in the
public key.  This is yet another illustration of something whose
security has strong theoretical support being subject to implementation
constraints that reduce its security in practice.
Because detectable patterns in public keys based on prime selection
evidently exist, it seems very likely that many systems are selecting
from relatively small families of primes, and that therefore the
related-key attacks you're concerned about may become a real problem -
Especially given that Nancy's* pattern is to attack as many keys as
possible rather than investing all her power in attacking any single key.
Multiple-party DH is a known protocol.
See  But I do not know of any good implementations of it which are at a
trustworthy/vetted/tested stage.  You are one of relatively few people
who most likely could produce one.
*: Incidentally, I have added Nancy and Harold to my own set of dramatis
personae.  Together, the two are even more pernicious than Eve and Mallory.
Nancy is the Nation-State attacker interested in mass surveillance and
breaking as many protocols as possible rather than breaking any
particular single instance of a protocol.  Traffic analysis, attacks on
entire families of keys, hardware and expertise both well beyond other
attackers, subversion of cryptographic standards, using legal force to
make otherwise trustworthy Trents and Harolds act in bad faith, etc.
Harold is the manufacturer of hardware like routers and NAT boxes, etc.
He is often located in nations with adversarial interests in the
activities of the nations or companies where the hardware gets used.
PS. Because detectable patterns due to particulars of the selection of
primes for RSA exist, it seems possible that they may also be applicable
to the selection of primes for DH key agreement.

@_date: 2016-02-05 16:14:07
@_author: Ray Dillinger 
@_subject: [Cryptography] DH non-prime kills "socat" command security 
Well crap. The fermat test got this on the first iteration.
Before we worry about provable primes, we can implement much simpler
probabilistic primality tests that clients can use to at least try to
disprove primality.  This particular number yields instantly to every
probabilistic primality test that's been tried on it as far as I can
And honestly there's been no case ever of a nonprime discovered that
fools both the Fermat test and Maurer's algorithm for 100 iterations,
and at least in the civilian world as far as I know nobody has any
idea how to find one.

@_date: 2016-02-07 13:14:27
@_author: Ray Dillinger 
@_subject: [Cryptography] New block cipher competition 
I have sometimes thought it would be worthwhile to create a block
cipher that could be used on very large blocks. The 64-kbyte
block requirement is actually justified in some applications.
The requirement for a 64Kbyte key (and the insistence, on the blog
page, that that is the *EFFECTIVE* key size, not just the size of
input to a key derivation function), leads me to believe that
either the person proposing the contest does not understand the
relationship between key size and security in a symmetric cipher,
or he has some deeply peculiar protocol in mind which relies on
the keys serving some set of additional purposes - perhaps as
secret splits, or as encrypted messages, or as one-time pads,
or as stegotexts within which other keys can be hidden, or for
some reason he must be able to use output blocks including
the 64-kbyte blocks as keys for a different operation, or ...
It is interesting to speculate about what such a protocol entails
or what it would be trying to accomplish, but so far I can't think
of a blessed thing that it would be the *best* design for. The
requirement is clearly silly if the block cipher is all he's doing.
That said, working key management that handles 64Kbyte keys
(actually, arbitrary-size keys), while ridiculous in the context
of a block cipher, could be useful in conjunction with some
types of post-quantum asymmetric-key systems.

@_date: 2016-02-09 09:06:06
@_author: Ray Dillinger 
@_subject: [Cryptography] DH non-prime kills "socat" command security 
I think I disagree.  It's easier to get code that deals with
general parameters right - once - than to rely on individual
implementations that depend on specific hardcoded numbers
for each of a dozen different groups being used in different
And I think I'd be as happy with code that does a hundred
iterations of three or four *different* probabilistic primality
tests as I'd be with a proof of primality.  Proofs of primality
are likely only to apply to a very small set of numbers having
very specific properties some of which we don't necessarily
fully understand.

@_date: 2016-02-10 14:44:34
@_author: Ray Dillinger 
@_subject: [Cryptography] DH non-prime kills "socat" command security 
Isn't it more reasonable for /etc/cron/daily to generate the file
( which is /etc/ssh/moduli on most unices )?

@_date: 2016-02-11 12:58:26
@_author: Ray Dillinger 
@_subject: [Cryptography] Proof that the NSA does not have a quantum 
Your proof would be valid if the device were widely accessible to
people in the NSA.  But if it exists, then I'd be confident that
it is not widely accessible.  I would presume instead that if it
exists at all it's a high-level compartmental security item, with
"need to know" limited to at most a few dozen people, all of whom
have security dogs sniffing their butts all day long.
Knowing *that* it had been used would effectively alert the
higher ups to *who* had used it, and that destroys the illusion
of unaccountability and expectation of profit, limiting the
potential malefactors to political idealists who are in it for
other reasons.  It'd be more credible and harder to deny, for
example, than just leaking some docs to the press. But doing
it would mean never being able to spend the money and going to
jail for life.
Not saying profit-motivated abuse couldn't happen by any means,
but it'd be a heck of a lot riskier than "Use QC, collect $500M."
Along the way any malefactor who hoped to profit would have to
get a very solid alibi and plant some very compelling evidence
to convince absolutely everyone of the guilt of one of a very
small set of highly vetted coworkers.  Account for the fact that
both the malefactor and those coworkers are under scrutiny 24/7,
and that gets really difficult.
None of this means they have a QC of course.  It means only that
institutional corruption combined with a lack of easily perceived
abuse, isn't a sufficient proof that they don't.

@_date: 2016-02-11 13:27:44
@_author: Ray Dillinger 
@_subject: [Cryptography] Justify the sequence of operations in CTR mode. 
In considering attacks on disk encryption, I realized we've been doing
it wrong.  Our objectives with CTR mode are to obscure the relationship
between identical blocks while permitting random access (not requiring
reading/writing additional blocks when changing something in the
CTR mode encryption is defined as
Ciphertext = E(counter, key) XOR Plaintext.
This obscures the relationship between identical blocks and allows
random access, but leaves the data malleable.  If Mallory knows or
can guess the plaintext, he can XOR the ciphertext with a difference
vector to substitute a plaintext of his choice.
I propose instead "CXR mode", defined as
Ciphertext = E(counter XOR Plaintext, key)
Which also obscures the relationship between identical blocks and
permits random access, but leaves the data nonmalleable.  With CXR,
if Mallory has not the key, any change he makes to the ciphertext
block is overwhelmingly likely to result in garbage.
This is a particularly pointed question because Mallory usually knows
the plaintext at the crucial location which is the first software
that will be read for execution from the encrypted disk.  Its location
can be determined by looking at the non-encrypted portions of the
disk, or else bootup wouldn't work, and its plaintext can be guessed
by knowing what OS, distribution, or boot manager the user has
installed.  It will execute with root privileges early in bootup
with access to the encrypted filesystem, so substituting malware for
it is a ridiculously effective attack.
This is far too obvious for nobody to have thought of it before;
why aren't we doing it this way?

@_date: 2016-02-11 14:35:45
@_author: Ray Dillinger 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
In order to exploit any properties about counter XOR plaintext, the
attacker would have to be able to spot patterns in plaintext by
looking at the ciphertext, or predict patterns in ciphertext based
on known properties of plaintext.
If the attacker can do either of those things -- at all -- then
your cipher is no damn good.
So CXR is fine on at least that count; it maintains the "at least
as secure as the cipher used" property, which is what we want from
cipher modes.  And the hash operation isn't all that cheap
compared to the encryption operation itself; it would slow the
combo down by at least 50%.
Well for one thing it's a hell of a lot simpler than an authenticated
encryption mode.
For another I'm happier if all attempts to modify the ciphertext are
guaranteed to produce garbage than I am if someone can modify
ciphertext in any way that results in the software being able to
determine what the attacker's intended modified plaintext was "but
it fails authentication." If that's possible, then a trojan horse
or an implementation failure will make it happen and fail to report
the authentication failure.
For a third thing it requires no authentication tag, which means
disk capacity doesn't get sucked up by something I don't care about
and I don't have to worry about handling extraneous bits in the
form of auth tags.
And for the last thing, this actually came up in a context where
each disk sector contains exactly one ciphertext block, so the
distinction between levels at which to achieve nonmalleability is
moot in the context of that discussion.

@_date: 2016-02-12 12:50:52
@_author: Ray Dillinger 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
You have a good point and you've convinced me of a necessary
revision of CXR - but the notion of "unintentional" is really
really funny when dealing with security.  You know that if CXR
as I first proposed it were to be deployed, this would happen
immediately and it would be as intentional as all getout.
But now we have to use a hash function on the counter, or at
least a nonlinearity of some kind, so the operation has gotten
significantly more expensive dangit.

@_date: 2016-02-12 12:58:00
@_author: Ray Dillinger 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
Agh, you're absolutely right about a chosen plaintext distinguishing
attack.  If the attacker knows (or can arrange) that some portion of
the plaintext differs by exactly sequential numbers in each block, they
can tell where in the ciphertext that plaintext actually is - thus
know, eg, where to target if they want to mess nearby blocks up.
So, crap, the op just got more expensive by a hash operation.  Doesn't
have to be a heck of a great hash operation but it has to obscure

@_date: 2016-02-15 09:36:45
@_author: Ray Dillinger 
@_subject: [Cryptography] =?utf-8?q?True_Micropayments_with_Bitcoin_?= 
It is my opinion that micropayment schemes will probably
continue to fail because consumers will probably continue
to loathe them.
Nobody wants the stress of making a money decision, when
only a half-cent is at stake.  Money decisions are still
stress, and bothering someone about piffling small change
is merely rude.
It is only when micropayments are redefined as, say, at
least half a US dollar, that the idea becomes psychologically
viable, regardless of the technical feasibility.

@_date: 2016-02-16 16:55:18
@_author: Ray Dillinger 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
It's a good construction in terms of security, but as you say it
costs an extra encryption.  Which, on the whole, makes the cost
not significantly worse than an encryption plus a hash, which is
the other primary option for equivalent modes.
On the other hand these encryptions definitely are not parallelizable.
you can't start the inner decryption until you've done the outer one,
and you can't start the outer encryption until you've done the
inner one.
I'm liking Cox's long-block extension of Blake2b for a fast hash
though, and I think I've come up with a fast cipher based on it
(or possibly some other hash - still studying options) that
doesn't need an extra hashing or encrypting step.
It's a three-round Feistel cipher using the extended hash for a
round function.  The state of the sponge is initialized by
processing the key and IV, and not reinitialized between rounds.
It really is exactly that simple.
I've written code for it (based on BLAKE-512) in C.  for 64Kbyte
blocks (yes, that's eight thousand times the block size of a
conventional 64-bit cipher like 3DES) It's a bit hard on my L1
cache latency, but it looks like that's the *only* reason it's
not as fast as AES.
There is a part of my brain that wants the size of the sponge
state (the state of the hashing process in progress) to be at
least as large as the block size being generated.  It's all
about a cipher having an effective key size at least as long as
its block length, which has repeatedly bitten newb designers
with short-block ciphers whose state is the same size as the
blocks or long-block ciphers with state significantly smaller
than the blocks.  So somebody has a cipher with "sensible" 128-
bit keys, and then someone has to point out that because the
state of his generator is only 64 bits, every key has around
2^64 equivalent keys that initialize the generator to the same
value.  Therefore an opponent needs to search only a 64-bit
keyspace to find *some* key that will have the same effect as
the one he's actually using.
In this case the impulse to have generator state the same size
as the block is a silly impulse.

@_date: 2016-02-17 12:38:56
@_author: Ray Dillinger 
@_subject: [Cryptography] Hope Apple Fights This! 
What they are asking Apple to do is to update code which is not
(AAUI) secured by that cryptography.  Specifically, they want
Apple to disable the OS code that counts the number of attempts
that the user has made to enter the password.  That OS code
is not customer data and isn't protected by the device encryption.
If Apple *can* do this, then it cannot legally refuse the order
unless it wins in court.  Which I hope they do, but in this
specific case - owner known to have committed multiple murders,
search warrant pursuant to an ongoing investigation, and compliance
opens a single device instead of all devices - I don't think they
If Apple *can't* do this, then it means they have given up not
just the ability to get to their customers' data remotely but
also the ability to update the customers' operating systems
without the customers' express consent.
Because having many different versions of the OS out there makes
software support into a huge expensive headache, I *REALLY*
doubt that Apple has given up the ability to push OS updates
regardless of customer consent.  I also don't believe they've
invested in the kind of hardware security module that would
protect any individual part of the OS from updates without the
customer entering the code, while allowing updates to the rest
of the OS.
So technically, it comes down to the OS update signature check.
If Apple is able to do the remote update, then it can produce
the weapon and sign it with their update key.  At that point
they have an ugly choice.  They can either revoke their update
key, which costs them $Millions because it requires an in-person
update to millions of devices, or it can leave all those devices
vulnerable to surreptitious installation of the fake OS update,
which costs them $Millions in lost customer trust and business.
The only other thing they could have done consistent with their
"everybody's running the same software so support costs are
limited" business model would be to have even "mandatory" OS
updates not take effect until the customer enters the passcode.
But that's a design refinement that I rather doubt they thought
of before now, so one way or another they're going to lose a
hell of a lot of money over this.

@_date: 2016-02-20 08:57:05
@_author: Ray Dillinger 
@_subject: [Cryptography] Apple 3rd Party dilemma 
The 3rd party doctrine has extra complications here; Remember that
we're talking about Farook's work phone which was provided by his
employer.  And that he worked for the county.  So there's an
argument to be made that if he placed any evidence of a crime on
that phone, then  he has *already* provided it to the government.
There's another argument to be made that the owner of the phone
(that is to say, the county) has consented to the search already
but is unable to provide the unlock code.  Because Apple's
developed channels for getting the unlock code reset for a
customer involve loss of data, the search that the *owner* of
the device wants carried out, cannot be done.
Anyway, it's a mess.  As far as I can see this is probably
the perfect case to use to create really bad law.
Remember how the law enforcement community was baying at the
moon about encryption after the Paris attacks, even though the
attackers had done their dirtywork in the clear on unencrypted
devices?  They hoped to get bad law passed because that crime
was heinous enough to make people fail to actually think,
even though *that* crime had nothing to do with the law they
To this day the majority of the uninformed public who experienced
that misinformation campaign believe that the Paris attackers
were using encryption, in the same way that the majority of
people who watch only Fox News still believe Iraq had those
WMDs that the US was just plain lying about when it was bullying
its European allies into agreeing to its ill-advised war.
Well, it looks to me like this is the case that they wanted
the Paris attacks to be.  It's so precisely tailor-made for
the purpose of getting bad law passed that if I didn't know
better I'd guess that it had been carefully planned for the
purpose, or that opportunities to prevent it might not have
been seen as being as useful to the interests of law enforcement
as letting it happen, or that stopping it might not have been
as useful as making sure that the device was encrypted in case
Farook had forgotten to do it.
But that's crazy conspiracy theory, right?  That would be
exactly the opposite of what law enforcement is supposed to
do.  That's so nutty it's like the crazy conspiracy theories
some of us used to believe about the NSA undermining the
defenses of the same citizens it was supposed to protect.
It could never actually happen, right?  Right?
 . . . ... crickets chirping . . .

@_date: 2016-02-20 18:32:14
@_author: Ray Dillinger 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
You know, after considering stuff, I think I have a conclusion
about whole-disk security.  And it's brutally simple. 256
bits is not an unreasonable size overhead for a 64kbyte block
cipher.  That's an overhead of 1/2048 the size of the data, a
bit less than 0.05%.
So fill (at least) 128 bits with a nonce, 128 bits with a
checksum or whatever metadata, like a logical block length
to know where to stop reading when the plaintext only fills
a partial block, and the other 2047/2048 of the block with
real data, and then encrypt.  And you can use plain old
ECB mode if you feel like it.
With the key, you can decrypt just fine and decrypting sorts
out your nonces and checksums/metadata from the real plaintext.
So you don't have to care about incrementing counters or IVs
or modes (though you could still use them if you feel like
it), etc.  You have no need to even remember the nonces, and
no possible chosen plaintext can reveal squat.
An opponent can never tell if two blocks match, because
they don't.  Ever.  When you write a file, even if writing
the exact same data with the same IV using the same mode
and encrypted with the same key, it never looks anything
like it looked before.

@_date: 2016-02-21 13:10:57
@_author: Ray Dillinger 
@_subject: [Cryptography] [Crypto-practicum] Justify the sequence of 
And so you'd need to write a whole new filesystem.  ... yes, I had
sort of thought that might be the case.  I'd been thinking of it
anyway because I'd like to extend file system functionality in some
other non-transparent ways as well.  It's annoying, but is there
any alternative that provides the same degree of protection and is
even close to being as simple and reliable?
Hmmm.  What happens if we use a cipher that takes a key 64 bits larger
than really required to achieve security - say 180 to 256 bits - and
the key used to encrypt any particular block is the salt (disk sector)
plus the user's 128-bit secure key?
No chosen-plaintext leak because no detectable relationship of
any ciphertext produced by full key A to ciphertext of the same
plaintext produced by full key B, assuming the cipher doesn't
leak to linear cryptanalysis.
No differential cryptanalysis because no plaintext/ciphertext
pairs can reveal or confirm a similarity between keys A and B,
short of brute-forcing the 128-bit security key.
This has the *tiniest* imaginable leak - if the same data is
written to the same disk sector with the same security key, it
will look the same.
There is some extra complexity, because awareness of the disk
sector parameters has to get out of the drive itself and into
the encryption module.
If the opponent has subverted the firmware on the disk (which we
have to worry about because we don't have a way to check it) is
there any way they can jigger the block and sector numbers used by
the drive to get identical "salt" used on sectors they care about
being able to compare? Hmm...  I'm going to say 'nope' because
the sector numbers the drive reports are also going to be used
to chain inodes together to make files - any misreporting two
sectors as having the same sector number will just cause the least
recently written one of them (and the file it's in, effectively)
to get lost by the filesystem.
We still get space-conserving encryption - same number of bits stored
as the length of the block, so filesystem code needs only minimal
updating.  Any change in plaintext cascades through the whole block
because basic property of block ciphers.  Every change in the
salt/IV results in a new "full key." The ciphertext of the same
plaintext encrypted by a different full key is indistinguishable
from the same plaintext encrypted by any other full key, or else
the cipher leaks on differential cryptanalysis.  So knowing the
first 64 bits of the "full key" does not give an opponent any insight
into 128 (or more) bit secure key.

@_date: 2016-02-22 12:20:26
@_author: Ray Dillinger 
@_subject: [Cryptography] eliminating manufacturer's ability to backdoor 
After finding out a bit more about the situation than I knew
before, I think I agree.  Apple is fighting the stupid fight here.
They left a bug in their device.  They are now being commanded
to write an exploit for that bug.  The exploit demanded *is* in
fact limited to a particular phone, and there is *no* requirement
that it be applicable to any other.  Nor can the exploit once
written be extended to any other (at least not without their
signing key) because the signature will not be valid.  And the
bug, once exploited, can be fixed the way any other exploited
bug gets fixed.
I don't think there's a leg left for Apple to stand on here. They
need to provide the damn exploit, then fix their damn bug so that
no similar exploit will work on upcoming generations of devices.
The big bad security issue here, and the place for battle lines
to be drawn, is if the FBI (or anybody) tries to prevent Apple
from fixing the bug, or demands disclosure of or steals its
software signing key for unmonitored, criminal, or secret use.
It's a pretty slimy way to do it and the legal repercussions of
the way it's done are deliberately calculated to be the worst
possible for consumer security.  It's dramatic, antagonistic,
and intentionally damaging to the company's reputation.  It's a
reputation attack serving as a deliberate and vicious punishment
against Apple for providing their customers with an expectation
of privacy. But ultimately... it's also a legitimate bug report
that points out a genuine exploitable flaw.
Had they pointed this flaw out to Apple previously, without
accompanying the bug report without the demand to write an
exploit and deploy it against a particular device, it would
have been an entirely value-neutral, even helpful, thing to
do.  As the matter stands, it's a calculated reputation attack
on the company and a tactic for getting adversarial precedent
into place.  But there's not shit Apple can do about it
because the bug is real and the warrant is valid and the
technical capacity to serve the warrant does in fact exist
because of the bug.

@_date: 2016-02-22 13:13:30
@_author: Ray Dillinger 
@_subject: [Cryptography] eliminating manufacturer's ability to backdoor 
I can think of a few things.  As far as I understand it, the
uniqueness-for-profitability argument argues for uniqueness of
content provided to the user and personal data snooped from the
user, but not for the uniqueness of the software that runs to
deliver the content and snoop the data.
Apple's profitability doesn't depend on providing different
versions of the operating system to different users.  Nor do
the app makers create individually tailored executables.  And
if they did they wouldn't be able to sell them in apple's
walled garden without getting Apple to sign off on every
release for every user.
Apple's profitability (and that of app makers) depends on
serving unique *content* to each device, and on snooping
identifiable individual data from each device, but how on
earth would that require unique per-user versions of OS
You're describing mutual blinded authentication.  Chaum worked
out a protocol for this decades ago.  Nothing argues against
using it for software distribution (as opposed to data snooped
from user devices or content delivery to them).

@_date: 2016-02-22 17:02:54
@_author: Ray Dillinger 
@_subject: [Cryptography] Secure software update protocol? 
Drifting back on topic and away from the business/politics
of the Apple/FBI situation , we do have a valid crypto question
How can software updates be secured so the distributor knows
which customers have which updates, not permitting cloned
devices, and not permitting the distributors to choose a
particular customer to get a special "joejob" update?
I propose the following: each device would need one
tamper-proof chip.  The chip would contain two keys baked
in: one for its series and one for its particular device
The chip can be fairly secure, because the keys never
need to leave the chip.  It just checks signatures, does
it on exactly two keys which are baked in at the factory,
and does *no* other crypto operations.
At one point the customer's side needs to *make* a
signature, but that's with a customer key, which is
presumably a different entity from the device keys which
would be on the chip.

@_date: 2016-02-22 18:39:57
@_author: Ray Dillinger 
@_subject: [Cryptography] Secure software update protocol? 
Well, absolutely.  If they're willing to distribute the evidence to
everyone, they can do it.  But it would be very hard to do it in
secret, and if caught it would be flatly impossible to pretend they
hadn't done it.

@_date: 2016-02-24 09:34:49
@_author: Ray Dillinger 
@_subject: [Cryptography] 1x pad: the elephant in the Apple/DOJ courtroom 
They get securely generated by drawing individual bits of metal
type out of a bin where they've been dumped together, and securely
printed by loading the metal type into a mechanical, manually
operated printing press where some poor sod has to ink the type
with a roller and then yank the handle to press the type plate
down on the paper.
If you're going to make secure crypto hardware illegal, that's
the kind of stuff you really have to worry about.  All this
focus on electronics and code is just grabbing the low-hanging
fruit that's easy and convenient to subvert and eavesdrop, and
doesn't address any of the *really* secure hardware that actual
paranoids use.
Like, you know, pens.  And paper.  and dice, and packs of cards,
and poker chips, and game spinners.  Or shotguns vs. graph paper
at twenty paces.  Or cassette tape recorders vs. popcorn poppers.
Or bingo ball cages.  Or cats.  Can't have any of that horribly
subversive stuff!  Gotta file special papers to take those across
a border, 'cause they're dangerous threats to national security!

@_date: 2016-02-24 10:10:08
@_author: Ray Dillinger 
@_subject: [Cryptography] RIP Claude Shannon 
? Why bother with voice encryption for secure communications during WW
II when all you have to do is wait, oh, 15-20
Whatever the practicality of voice encryption in 1944, I
utterly respect the man who had the out-of-the-box idea of
inducing a phase-locked loop between the frequency of AC
current driving an electric phonograph motor and the
frequency of the transmitter to *exactly* synchronize the
playing of identical phonograph records separated by 6000
Seriously.  You want a source of randomness?  Try playing the
same phonograph record at exactly the same speed for over
one minute at a time, twice.  Overcoming that to actually *do*
real-time one-time pad encryption on voice communications
over radio, based on additive vectors provided via phonographs,
was a hell of a good original piece of work.
Of course, why the hell the brass wanted *voice*, specifically,
so badly they'd go through an expensive delicate difficult
procedure that took international courier deliveries of keying
material to set up, and involved additional humint security
risks?  And required hiring a guy like Claude Shannon and
however many academic and army technicians to design and
develop new hardware?  It's a dubious priority. Possibly has
something to do with biometric proof of authentication (by
recognizing the individual voice on the other end). People
like Rich Little could spoof it of course, but that's a
fairly rare skill.

@_date: 2016-02-28 22:11:04
@_author: Ray Dillinger 
@_subject: [Cryptography] McAfee: NSA Juniper backdoor used by China to 
I have at least some anecdotal evidence that in some cases
at least, the USG has also been extremely rigorous about
software development procedures - not up to "the penalty is
death" levels, but certainly serious enough.
FWIW, back when I was working for NativeMinds (a provider of
development software specialized for making natural-language
interactive systems - alas, no longer funct), I occasionally
dealt with code intended to run on a secure USG system.  We
weren't the direct government contractors here - we were a
provider of very expensive (like, half a year's wages per
development seat expensive) development tools and support to
the contractor.
What I remember most about that codebase (at least that I'm
allowed to talk about) was the fact that EVERY changed line had
to be accompanied by a code comment that said who made the change,
when, and why, with a reference to the associated bug/enhancement
request in their issue tracker. Those were "hot comments" - they
were in a formal language of their own that was parsed and
audited by an automated system which verified that the date/
attribution data was in agreement with the version control system,
that the associated issue in the issue tracker had been updated
on that day, and that absolutely no undocumented changes had been
made.  Apparently there were auditors who got screamy if this
process (and apparently others which didn't concern our support
contract so I don't have to know what they may have been) was
not rigorously followed.
ISO 9001 was a big buzzword at the time (yes I know I'm
dating myself here) but this went WAY beyond the most anal
of ISO 9001 requirements.
A couple of us had to get security clearances because the
contractor was paying for support and in order to do the
support a couple of us had to be cleared to look at the code
they were using our tools to write.
Almost every line had one of these formatted comments.  I
was assured by the "real" contractors that the only lines
that did not bear such were those which had remained
unchanged since the initial checkin.
Yep, yep, yep....  And if our intelligence agencies can force
backdoors to be installed, of course they need to consider the
capabilities of the intelligence agencies in the nations where
it's being manufactured - or shipped through, or where components
are being made, or where firmware is being written, etc.  There
are any number of countries that could have forced a supplier
to place a backdoor the USG would know nothing about into a
typical android phone. And the USG is allowing those devices
to connect with its networks.
It's amusing to think that even when the FBI claims to be unable
to crack an iPhone, the Chinese may have a dozen ways the FBI has
never heard of because China can twist the manufacturers' arms.
Of course, that's paranoid crazy talk, like thinking the NSA
might be spying on American citizens rather than protecting them,
or that Volkswagen would write software specifically to cheat
on EPA tests rather than complying with them.  Nobody would
believe someone would do that, right?

@_date: 2016-01-02 15:04:50
@_author: Ray Dillinger 
@_subject: [Cryptography] Alice, Bob, Eve, Mallory, Maxwell ??? 
The name 'Mallory' is traditional for an MITM attacker.
(Because the data in flight is 'Malleable' if it can
be changed by an attacker, IIRC).
We know
Alice, Bob, Carol, Dave, Esther, Fred, Gina, Harold....
... Walter, Yolanda, Zebulon,
in alternating genders and increasing alphabetical
order for the honest participants.
The roster of bad guys, from various sources, are:
Betty, the Botnet Operator (often works for Daisy or Sam)
Daisy, the Denial-of-service attacker(wants the system unavailable)
Eve, the Eavesdropper (wants to snoop)
Ike, the Impersonator (tries to subvert signing)
Mallory, who can change data in flight (for virtually any purpose)
Trudy, the Intruder* (tries to suvert authentication)
Trent, the Trusted Authority (can screw you over by defecting)
Sam, the Spammer (wants to send spam)
I would call someone who can inject noise either 'Daisy' or
'Mallory' depending on their goal; probably 'Mallory' since
'Mallory' is traditionally involved in protocol attacks and
'Daisy' usually implies trying to overwhelm available resources
with excessive volume.
But sure, if you need a different name 'Maxwell' would work.
If I were to ask for additional bad-guy names, I think I'd
want one for providers of "legitimate" hardware/software
who make backdoored hardware/software instead.  'Baker'
*Mark Bitman's book uses 'Trudy' for every kind of attacker;
elsewhere it's been used specifically for 'Intruder' meaning
someone who tries to gain illicit access.

@_date: 2016-01-03 11:26:51
@_author: Ray Dillinger 
@_subject: [Cryptography] How can you enter a 256-bit key in 12 decimal digits? 
So I was looking for a new hard drive today and I came across this:
I'm pointing this particular drive out, but it's representative of
the offerings on display.  I don't want to single out Fantom, but
the description of this particular device very blatantly points
out a very common problem.
256-bit encryption it says, but it has buttons for entering
decimal digits and allows "up to 12-digit pass code combinations
to protect your data from unauthorized use."
Now, the last time I looked, 12 decimal digits equals about 40
bits, not 256 bits.  To enter a 256-bit key you need ~77 decimal
digits, not 12.
If someone can get at your data by brute forcing a key in a 40-
bit key space, why is it legal to call this 256-bit encryption?
Here's another thing I found which illustrates a different (also
common) problem.  Again, I'm not singling out this manufacturer;
I'm pointing at this particular device only because the problem
is very clear from its description.
"128-bit encryption" it says, but I notice it has no way to actually
enter a key.  Reading on, I see that the device is meant to be used
with a physical token that stores the key.  O....kay, but I'm not
happy about an object that can be stolen along with the drive being
the sole security, and I wonder how the customer sets the key. Is
there a grid of pins that you can put about a hundred jumper blocks
onto at random and then snap down a cover and fill the fob with
epoxy? It'd be easy to read that using an x-ray, but if you can
x-ray it you've already stolen it, right?  So no *additional* threat.
But then I read on, and it says "Bundled with 2 cipher keys."
2 physical tokens - both containing the same single key which is
known to the drive manufacturer, with no assurance that the key
is unique. Even if they are, brute-force attempts on all the keys
the manufacturer has ever issued is going to be an unacceptably
small (trivial) key space.  Moreover, the device comes with a
serial number, so an opponent who cracks (or compels) the manufacturers
database would be looking at a zero-bit key space.  How in the
world can a device with a zero-bit key space be referred to as
having 128-bit encryption?
I can easily remember 40-digit sequences using mnemonics that are
completely opaque to others; I figure that entitles me to use
128-bit key spaces.  Nobody appears to want me to be able to do

@_date: 2016-01-04 17:57:09
@_author: Ray Dillinger 
@_subject: [Cryptography] How can you enter a 256-bit key in 12 decimal 
the point is though that if they steal the device, then they
steal the high-entropy on-device secret along with it.  They
only have to hook up their serial port to the wires that
the buttons connect to, and try the 10^12 combinations.  They
never have to try to work out the high-entropy secret.

@_date: 2016-01-07 17:54:21
@_author: Ray Dillinger 
@_subject: [Cryptography] Chaum Has a Plan to End the Crypto War 
golden key." The classic golden key approach is to provide to
authorities a copy
of someone's private key, encrypted with the golden key. The fragmented
appears to split the key in N fragments, each encrypted with the
specific golden
key of a different "council member," so that the N council members have to
cooperate to recover the private key. It is not clear to me whether the
calls for unanimity or "M out of N," but both variations are obviously
key approach would be any more secure than the single golden key. It is
more complicated, which increases the probability of bugs and
compromises. And it
also provides a really big attack surface.
As I see it, it shifts the attack surface slightly away from
ubiquitous surveillance (although that could still happen) and
opens up a huge attack surface for various DDoS attacks.
Any actor who wants to shut this down can easily do so by
disrupting communications between any two of the nine servers -
making 72 different ways to kill it.
I am far less optimistic than Chaum about its ability to protect
privacy.  As I see it geographically distributing the servers to
different countries mainly insures either a "gift to" or an
"operation against" each of those countries from a cartel of
nations interested in ubiquitous surveillance.  At least insofar
as the servers are located outside those nations in the first
place.  He says he intends to keep one in the US?  Might as well
write that one off in terms of protecting the privacy of anyone.
And, if in fact these nine servers are free to cooperate (rather
than compelled to cooperate) to unmask malefactors, then all that
a particular malefactor has to do to be immune to such unmasking
is bribe or blackmail a single server operator.  Among nine they
can surely find one who will defect if suitably induced.
Finally, you have actions performed with anonymity by the selfsame
government agencies who can control the server in their country
in order to assure that they themselves are not unmasked.  This
creates an asymmetry of power in which government agencies can act
with impunity in terms of anonymity whereas anyone else runs the
risk that nine servers will cooperate to unmask them.
I normally respect David Chaum, but this is a pile of shit.

@_date: 2016-01-09 12:52:11
@_author: Ray Dillinger 
@_subject: [Cryptography] Verisimilitrust 
This whole thing (browser makers deciding which certs to
trust and distrust) occurs for the simple reason that the
CAs have always been visibly redundant, and to the extent
that they had a legitimate role, they have abdicated their

@_date: 2016-01-13 14:20:09
@_author: Ray Dillinger 
@_subject: [Cryptography] OpenSSL minimal "safe" configuration? 
I would argue that MD5, single-DES, and export-grade crypto are
so poisonous at this point that anything depending on them needs
to be terminated with extreme prejudice.  If it's required for
TLS versions before 1.2, that is an indication that compatibility
with TLS versions before 1.2 must be dropped like a hot rock.
If you want to make a fork named InsecureSSL that supports that stuff
for backward compatibility, that would be fine. But it shouldn't
be in the same tool people are using for secure operations.
I support Henry in his goal and would vastly prefer an OpenSSL that
flatly refused to use known insecure algorithms.

@_date: 2016-01-13 15:32:23
@_author: Ray Dillinger 
@_subject: [Cryptography] Verisimilitrust 
So - what worthwhile applications do we need another public key
infrastructure for? And what requirements does it have beyond or
different from the X.509 PKI?
The model with browser vendors deciding which root keys to put
into browsers (the browser vendors delegate trust to the CAs)
was necessary because nobody else was effectively doing it.  We
wanted to keep Mallory out so we trusted Trent, but it turns out
that we failed to give Trent a reason not to cooperate with Eve.
Certificates aren't a bad thing.  The cryptography was sound.  But
we got the business relationships into a different configuration
than the trust model, and that failed.  We set up CAs who were
in a position to sell the trust of people who never trusted them.
They had no contractual or product liability to the people who
depended on their services.  That was unsustainable, and very
predictably, it's dying.
Moving the trust root to the browser vendors is a step forward.
The people whose trust is being delegated/sold are the ones who
use the browsers, and they are making a choice. They could make
a different choice if they decide that a browser vendor is
untrustworthy.  And possibly the browser vendors could even be
held liable to the users if their product contains deliberate
malware in the form of untrustworthy certs, which was something
the users never had with mere CA's.
So now we have Trent II.  Unlike Trent I, he has at least some
potential motive or business reason to be a trustworthy actor.
Unfortunately I fear that Eve (and/or Mallory) will offer even
greater incentives to become an untrustworthy actor.  And Trent I
isn't dead yet, which will cause confusion at least until the
CA model finishes dying.
Anyway, that's it in the nutshell.  Nobody is in a position
to sell the trust of consumers unless the consumers trust them.
CAs who had not established a trust relationship or at least a
commerce relationship with the users were never worthwhile. It
was the browser vendors all along who could operate in that
capacity, and the recent steps making that more clear are merely
a long-overdue clarification.
But the browser vendors can't secure trust for anything except
web browsers.  And the PKI for web browsers, to a first
approximation,  doesn't work or at least hasn't been made to
work for much beyond E-commerce.  Authenticating the user (as
opposed to the server) is strictly out-of-band.  The user is
unauthenticated in the protocol, because the user has no
certificate.  All the server knows is that the user provided
them with the information they need to access a bank account
or credit card out of band to their interaction with the user,
and that's all the user authentication they get.
This brings us up to the present day.  Browser vendors are
now the only CA's that matter.  Web servers are authenticated
to users but user authentication is either out of band (e-commerce)
or not done (everything else).  Without mutual authentication the
applications beyond E-commerce are limited.  With browser vendors
as CAs the applications beyond web browsing are limited.
So, what's the payoff to overcome these limitations? What worthwhile
applications do we need another public key infrastructure for?  What
is the trust model and how can we avoid the mistakes of setting up a
business model that doesn't follow it?  And what requirements does
it have beyond or different from the X.509 PKI?
In short, where is the new work that we still need to do?

@_date: 2016-01-13 16:14:13
@_author: Ray Dillinger 
@_subject: [Cryptography] OpenSSL minimal "safe" configuration? 
That may be, but can't people actually be required to consciously
*turn* it off?
If someone really wants to be compatible with an insecure old product,
I have no problem with them being able to go and change the calls to
"OpenSSL" in their scripts into calls to "InsecureSSL" and move
"InsecureSSL" into their $PATH where scripts can invoke it.  That is
a conscious decision to turn off security.
But I really, really object to the idea that being compatible with
insecure crap should be the *DEFAULT* configuration, or that scripts
invoking insecure operations in OpenSSL should continue to work after
those operations are discovered to be insecure.
When things are discovered to be insecure, or computing resources
advance to the point where they are no longer secure, it should
require a conscious decision and configuration action to continue
using them, not mere inertia.
Finally I object to the idea that the choice to use a tool supporting
known insecure operations (and have that tool available in secure/bin
where "Secure" scripts can invoke it in insecure modes) should be the

@_date: 2016-01-15 22:45:12
@_author: Ray Dillinger 
@_subject: [Cryptography] OpenSSL minimal "safe" configuration? 
That is what "InsecureSSL" is for.  If you can't secure something
you shouldn't be using the same tool you use for secure data on it,
nor should you be misled into believing that it's secure because
something you think is security software continues to work with it.

@_date: 2016-01-17 11:25:29
@_author: Ray Dillinger 
@_subject: [Cryptography] Verisimilitrust 
This ... is ... brilliant.  Establishing trust in person at the
place of business, by scanning it off the wall.  Totes simple,
people will get it.  Add certificate pinning, and you get keys
that matter into consumers' devices and (with sync) home
computers.  The beauty of this is that it's out of band to the
Internet itself.  It's authenticated by physical presence in a
space controlled by the securing party.
There are a lot of things it doesn't cover, of course; folks never
actually go to an Amazon storefront. But ... Banks.  Attorneys.
Accountants.  Health-care providers.  Brick-and-mortar merchants.
This handles a bunch of really important auth problems.

@_date: 2016-01-17 11:55:06
@_author: Ray Dillinger 
@_subject: [Cryptography] OpenSSL minimal "safe" configuration? 
This.  Paranoia or not, the attack surface those insecure ops leave
is an unacceptable risk.  If it won't build without MD5, then a
temporary fix would be to make a version of MD5 wherein every
routine contains a call to assert(false).  But a far better fix
would be to just clean up the code so it doesn't call those routines
at all.
Henry, are you still having trouble getting a clean build without
all the insecure legacy stuff?  Maybe I can help, or if you
haven't made much progress, maybe I should just start working on
it myself?

@_date: 2016-01-21 08:52:43
@_author: Ray Dillinger 
@_subject: [Cryptography] OpenSSL minimal "safe" configuration? 
Thank you for doing that, and doing it at the build level
rather than in some way a mere shell script passing it
command lines could ignore.
I'm still not happy about MD5 (which is even more broken
than SHA1) being in the compiled code at all for fear that
a stack stomp attack could get at it or something, but it's
hard to imagine coordinating a stack stomp with a protocol
downgrade to MD5 in a way that would result in valid protocol
Not the sort of thing I can dismiss as impossible, after
FREAK and POODLE and a few more advanced-persistent-threat
things I've been exposed to. But hard to imagine how to do.

@_date: 2016-01-21 10:51:34
@_author: Ray Dillinger 
@_subject: [Cryptography] TRNG related review: rngd and /dev/random 
In case anyone is interested, there is a modified 'rm' command
(maintained by Arch or Kali distribution, I forget which) that
copies every file being deleted to /dev/random (where it gets
zero entropy credit but still renders the pool immune from the
vast majority of potential attackers).  Including all the stuff
that gets deleted or overwritten daily by rsync while doing
backups. And including the file *names*, so even the empty
'mutex' files that get scattered around in working directories
and temp directories have a small contribution.
I remember it mostly because it was necessary to alter several
shells, including Bash, because their default configuration is
to have the NON-extended version of 'rm' and a few other
common command lines compiled in.
IIRC, the same distro redirects everything sent to /dev/null
into /dev/random as well, and has a configuration option that
makes its extended 'rm' do 'shred' by default.  But shredding
addresses a different attack model, of course.
Anyway, it's hard to imagine an attacker keeping up with all
of that, even just through boot time and the virtual filesystem
that gets used to load initial drivers and things.  A few bits
here and there that the attacker can't guess, and it's over -
even if the attacker knows the state of the file on disk
*after* the previous shutdown, which is quite difficult for
an attacker who has not physically disassembled the machine.
And once you've taken a picture of your cat....

@_date: 2016-01-26 15:07:30
@_author: Ray Dillinger 
@_subject: [Cryptography] Anyone have information on Export 1024 RSA? 
I think it would still take them at least weeks to break a 1024
bit RSA key.  Export Grade, when that was a thing, restricted RSA
to 512 bit keys.
With the usual "worst realistic case" compute capabilities ascribed
to state-level threats, it would take years to break a 1024 bit key.
The problem here is that state-level threats deal in bulk, and
the amount of compute power required per RSA key broken is much
less if you're committed to breaking a bunch of them.
The major expense is the sieving step, which can be done once.
And it's a "reasonable" assumption, IMO, that the NSA, or their
counterparts in China or Russia or wherever else, started
sieving within a couple of months after the RSA cryptosystem was
first announced and have never stopped.  This would require
factoring algorithms that were way ahead of the curve back in
1980, but state-level threats could be considered to be the
most likely to have had such capabilities and not announced them.
If that only-somewhat-paranoid bet is in fact true, then by this
time, it isn't terribly unlikely that someone has built the data
structure needed to use similarly staggering compute resources to
take 1024 bit keys in a few weeks.
And they're telling us to look out for Quantum Algorithms now.  If
there's a good reason for that concern, then RSA has a very uncertain
future regardless of key size.

@_date: 2016-06-30 21:33:52
@_author: Ray Dillinger 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
Okay, let's put it in a simpler way.
Stop your protocol right before the last message is sent by Bob.
At this point, Alice has nothing more to do to complete the
protocol.  Bob has the choice between sending his final message
and completing the protocol (putting in the hands of Alice a
contract that binds him) or just composing that final message
and never sending it.
Now, If the contract turns out to be beneficial to Bob, Bob can
later go to court and show the final contract (whether he did, or
did not, actually transmit it to Alice) and claim that Alice
is bound.
Contrariwise, if it turns  not to be to his benefit to have made
such a contract, and Alice never received the final message, then
Alice cannot show the final message to the judge and claim that
Bob is bound.
In court, Alice will claim she isn't bound because she never
received that final message completing the protocol.  There is
no way for the judge to know whether or not she's telling the
truth about receiving it, nor whether Bob is telling the truth
about having sent it.
With the channel being unreliable, there is even the possibility
that both of them are telling the truth.  Maybe Bob really DID
send the message and Alice really DIDN'T receive it.
Unless there is some way to irreversibly publish the completed
contract in an archive where it remains 'on display' in a way
that either participant could have checked at any time since
(and where later someone can show that it has been present since
a particular time), and such publication is required as a step
to complete the contract, it comes down to Bob's word against
PS.  This is a very silly argument under "reasonable man"
provisions as real courts use -- A contract with signatures
from both, and acknowledging signatures of the document plus
both signatures from both, will stand up in any court.
Requiring an infinite regress of acknowledgements of the
acknowledgements of the acknowledgements of the signatures
is, well, childish and silly and would get you laughed at.
So, yes, in some technical "pure" sense it's impossible under
the proposed conditions, but it doesn't matter.
Also, public archives from which neither can delete something
and which either could publish in or check at any time, and where
either could later prove that something was or wasn't published
before a particular time, are easily available.  So the fair
contract problem is easy to solve because the assumptions on
which this impossibility theorem rests don't even apply.

@_date: 2016-07-01 19:58:11
@_author: Ray Dillinger 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
If your fairness definition only deals with a completed
protocol, but one of the participants has the power to
stop the protocol short of completion, in possession of
the EXACT SAME evidence of the contract that they would
have had it been completed, and is able to PREVENT the
other from having such evidence - then your protocol
conforms to your fairness definition without being fair
in any meaningful way.
If we had the power to force people to *COMPLETE* protocols
then designing security and cryptography applications would
be easy.  We don't.

@_date: 2016-07-01 20:53:34
@_author: Ray Dillinger 
@_subject: [Cryptography] I have an archive of WWII cryptography documents for 
The archive is one gigabyte (mostly PDFs of scanned paper documents)
of papers relating to US cryptography efforts during a span from the
middle 1930s to the late 1950s - including the duration of WWII.
The information has been declassified and reclassified at least twice
now.  I don't know its current status, but it was legal for a
civilian to get it when I got it, and I'm not bound by an oath or
contract requiring me to prevent others from getting it.
I immediately put it up as a torrent to prevent reclassification from
making it unavailable - but I see that the original torrent has gone
So here's a new magnet link for those who are interested.
If you have received this message via a cryptography list, be sure
to check the to: field if you choose to reply.  You'd like to not
crosspost anything irrelevant to one of these lists.
If you have received this via BCC, note that you will miss any
followup commentary unless you pay attention to one or more of the
lists to which it's been addressed.
If you want to follow up to me personally, my public key and email
are attached to this message.

@_date: 2016-07-02 12:36:51
@_author: Ray Dillinger 
@_subject: [Cryptography] [Crypto-practicum] I have an archive of WWII 
And as a helpful soul has pointed out offlist, the magnet link in my
original message referred to a badly corrupted zipfile.  Apologies to
all: this was my mistake, and I'm sorry to've wasted your time and
Here is a magnet link to a corrected torrent; I have stopped seeding
the other.  The corrected torrent is just over 2 Gbytes.

@_date: 2016-07-07 14:15:46
@_author: Ray Dillinger 
@_subject: [Cryptography] What to put in a new cryptography course 
If one is to teach ECC at the undergrad level, one must have students
who have already studied group theory.  That's reasonable for math
majors and mathematical cryptographers, but in computer science ECC is
almost the only thing it will enable.  CS needs set theory, logic,
statistics and algebra absolutely.  It benefits greatly from geometry,
trigonometry and calculus/differential equations. But outside of
cryptography though it gets almost nothing for number theory, and
outside of ECC it gets almost nothing for group theory.  The closest we
get to group theory in ordinary CS is modular mathematics, and we never
do modular geometry or trig.
I get the _mechanics_ of ECC as "geometry of lines tangent to curves in
modular coordinate systems"  but what makes a particular curve secure or
insecure in a particular modular coordinate system?
THAT, and that alone, is the crucial question for understanding ECC, and
that is the question most resolutely ignored in all the introductory
material I've seen in ECC.
Where do the modular coordinates, geometry, and trigonometry boil down
to an algebra problem in a form I know a good reason why we can't easily
solve? And can we show that any simpler formulation if one exists, would
enable an Easy solution to something we firmly believe is Hard?
It's like that old problem with RSA;  You can use any solution to
factoring to solve it, so we know it's no harder than factoring.  If we
could also use any solution to RSA to solve factoring, we'd also know
that it's no easier.  What are the analogous arguments in ECC?
I don't really know, and as a result I'm suspicious and superstitious
about ECC.  About curves selected for efficiency for example.
I get ECC mechanics but without understanding the mathematics of ECC
security very well, I worry about those "efficient" compute properties
and whether they may pertain to unanticipated attacks.  No one has ever
tried to explain to me why there's nothing to fear in these "efficient"
curves, beyond making an appeal to authority or a bald assertion that
there isn't.   Without answers no one seems willing to give to the
crucial questions I posed above, I can't check  that assertion.
So my suspicious-bastard side required to do cryptography runs up
against my fear-of-of-the-unknown in group theory and results in
suspicion of the seeming shortcuts in ECC.
Were you planning a course that addressed the arguments-for-security of
ECC, or just another course on how to do the mechanics?  Because I don't
think we actually need another course on how to do the mechanics; that's
out there already.

@_date: 2016-07-08 15:16:42
@_author: Ray Dillinger 
@_subject: [Cryptography] What to put in a new cryptography course 
The site contains lots of assertions that curves with particular
properties are more secure than curves that don't have those
properties.  I've read it before. It is good practical advice
for selecting among known curves, and it is greatly appreciated
as a resource. However, it is yet another resource which
resolutely ignores the basic question I would want out of a
course on ECC and had intended to ask.
In fact evaluating the rationales given for most of the advice
would require that basic question to be answered first.
Having reread the whole site, I still have not discovered an
explanation of exactly how the mechanics of geometry on modular
coordinate systems are transformed in some cases into a specific
algebraic formula whose solution would require mathematical
operations believed to be Hard.  It seems to be treated as
something that everyone except maybe a few specialists is
simply expected to assume "because we say so."
What is the specific equation someone has to solve to break a
particular case of ECC, how exactly is that equation derived
from the geometry of particular curves on particular sets of
modular coordinates, and why do we believe that equation is
hard to solve?  IOW, don't just tell me how to pick parameters
believed secure; tell me enough to see exactly why parameters
satisfying those criteria lead to a hard case of that equation.
Lacking that understanding, I cannot evaluate the practical
advice given at the site about what kind of curves are and are
not recommended, nor know what forms and transformations and
special cases of the "Hard" equation that advice is intended
to promote or avoid.  I can only memorize the advice, as I
have already been doing, and continue to worry about what
potential attacks lurk in the gaps between the things I've

@_date: 2016-07-10 13:36:08
@_author: Ray Dillinger 
@_subject: [Cryptography] Time(keeping) and Crypto-Economics 
FWIW, I've already published code that keeps block height
(multiplied by target block interval) roughly synchronized
with consensus time (time elapsed since genesis block as
measured by consensus time established by the timestamps
of recent blocks).
This would allow any script (in a script language able to
detect block height or query intervals in block height) to
calculate things like interest or contract performance
deadlines etc, and would keep events like block subsidy
reductions happening on a more predictable schedule
(ie, predictable with much greater precision than by
block height in a block chain without such regulation)
over the long term.
It is, obviously, just an approximation. IIRC, the numbers run
against simulation were something like "spends 99.9%+ of its
time within a week of clock-correct and altering difficulty
by 5% or less," and "spends 50%+ of its time within 4 hours
of clock-correct and altering difficulty by 0.22% or less."
It speeds or retards block creation by making small adjustments
to block difficulty (with adjustments capped at 10% for any
block height more than 2 weeks out of sync with the consensus
It does this while simultaneously regulating difficulty to
keep block intervals consistent despite immediate addition
or reduction of hashing power (because it was developed for
altcoin chains, which have much higher hashing rate variance
than the bitcoin chain).
With the expectation of block-height-to-time adjustable for
the immediate periods (people know if the current block height
is off-sync and can calculate beginnings of contract periods
starting from it) and expectations of time as deadlines
approach visible in final periods (people know the actual
block height so they have warning of the rough variance in
deadlines), I think that kind of "light-touch" regulation
addresses most of the practical problems with block height
as a proxy for time.
Or at least it serves adequately for purposes of contracts
between humans who accept its roughness as part of their terms.
It also ought to be acceptable for purposes of events whose
precise timing is only of minor significance as opposed to
their effect as measured in months or years (such as regulation
of coin creation).
Code publicly available, usable without fee, modifiable and
redistributable under the terms of the MIT free-software
It may not do *exactly* what you want but I think it's a
way to make block height into a decent if rough proxy for
wall (calendar) time.

@_date: 2016-07-13 15:16:38
@_author: Ray Dillinger 
@_subject: [Cryptography] The Laws (was the principles) of secure 
I've come up with a few enlightening definitions:  Sometimes
people make better security decisions if they look at them.
Some of them could be laws I suppose.  Here are the ones I
put on the one-page version of the handout.
"Backward Compatible" -- Vulnerable to Downgrade Attack.
"Backup" -- Additional Target.
"Big Data" -- Whatever people didn't secure.
"CA"  -- Some clown in Uzbekistan who paid somebody a bribe.
"Cloud"  -- Computers that someone else can access at will or
            take away from you at will.
"Common Carrier" -- Compromised Carrier.
"DRM" -- A technology which gives the key to the attacker.
"Firmware Update" -- A reminder that the software you can see
                     is not the only software that contains holes.
"IoT" -- Internet of Targets.
"Keyboard" -- A device for entering mistakes into a computer.
"Password" -- If a human can remember it a computer can guess it.
"Proprietary" -- Unreviewed.
"Protocol" -- A procedure ANY part of which may not be followed.
"Routine" -- With absolutely the minimum possible amount of
             attention or checking required to usually get results.
"Secure" -- Less valuable to a crook than something else they
            could steal or break with the same effort.
"Social Media" -- Surveillance As a Business Model.
"Switch"  -- listening post.
"Trusted" -- Capable of screwing you over.
"USB"    -- Un-Secure Bus.
"Virtual" -- Looks like something that it isn't.

@_date: 2016-07-13 17:25:20
@_author: Ray Dillinger 
@_subject: [Cryptography] The Laws (was the principles) of secure 
Oooh, I particularly like that one.  With Sony's invention as
a prime example even.  I think I will update my handouts now.

@_date: 2016-07-16 10:47:24
@_author: Ray Dillinger 
@_subject: [Cryptography] The Laws (was the principles) of secure 
About this:  I see the point of it, in that one kind of
security flaw can often be leveraged by attackers into
attacks on other systems.  So the point from a security
POV is to remind people to isolate systems that come
under attack both from each other and from the rest of
their intranet.  Internal firewalls are your friend.
But it does promote the kind of all-or-nothing thought
that leads to entirely unrealistic security measures
that will, in practice, fall by the wayside as soon as
you try to get real people to do them, and/or be too
expensive in time and money to implement, and as stated
it is besides false.
*Absolute* security is, if not completely impossible,
at least extremely difficult and expensive, and you
don't want to lead folk to believe that anything less
than absolute is useless.  Perfectly ordinary, relatively
inexpensive and simple security measures are _very_much_
worthwhile for most businesses - even large businesses.
A more appropriate way to get the point across, in my
opinion, would be stating the real issue more directly.
Something like:
Law 11: Whatever is successfully attacked becomes
        a means of making further attacks.

@_date: 2016-07-18 10:18:06
@_author: Ray Dillinger 
@_subject: [Cryptography] Sandia storing information in encrypted DNA 
But, wouldn't that overwrite the valuable messages left for
us by the ancient progenitors of life in the universe?
Oh, wait.  That's not the real progenitors.  That's just a
spammer who got into their mail server.  Nevermind....
PS.  Can I just mention here that I adore this artist's work?
     I wish his comic updated more often, but he makes his
     living doing commissions so the comic is second priority.
     Still, it's worth the wait every time.

@_date: 2016-07-22 08:45:37
@_author: Ray Dillinger 
@_subject: [Cryptography] DMCA lawsuit by Matt Green 
Of interest to a bunch of people on the list, Matt Green has
filed a lawsuit specifically addressing the elephant in the
If anybody's going to fix this stuff, we have to find out what
it is.  If anybody's going to make reasonable assessments of
security when selecting products to use, they have to know the
risks.  If anybody's going to defend anyone from crackers, they
have to know what security problems they're trying to keep the
crackers from exploiting.
So.  Um.  Yeah.  People need to be able to publish information
about security flaws.  On the topic of the stunningly obvious,
the sun rises in the east, too.

@_date: 2016-07-24 10:51:58
@_author: Ray Dillinger 
@_subject: [Cryptography] Entropy of a diode 
"Remember, kids!  The only difference between a diode and a friode
is too much voltage or too many reverse breakdowns!"
                        -- "mr. chips," public TV show circa 1983.
Dang, I liked that show.  It taught electronics hobbyists,
among other things, to actually test their circuits and keep
watch for burned out parts.  So there was the Big Red Trash
Can (with smiley face and happy munching noises) for friodes,
write-only memories, desistors, dark-emitting diodes, and
chips that the magic smoke had leaked out of. He always acted
so happy to find one, because it meant he could fix the darn
problem and go on!
Actually I sort of got the impression he might have been a little
bit toked most of the time, although that might just be the way
he talked naturally.
But yeah, if you're generating noise by reverse breakdowns it
won't take all that long for your diode to turn into a friode.
And friodes just aren't all that chaotic.

@_date: 2016-07-27 00:44:33
@_author: Ray Dillinger 
@_subject: [Cryptography] Code is Cruel -- The DAO 
Yah.  Bitcoin doesn't have accounts though, just transactions.
The basic entity that stores Bitcoin on the block chain is a
"txOut" -- literally, just one of the outputs of a previous
What is actually stored on the block chain are transactions that
say something like, "this set of existing txOuts was used in a
transaction by one or more people who, between them, demonstrated
knowledge of the keys and/or scripts needed to spend all of them.
The transaction creates a set of new txOuts each of which will
probably be spent in some subsequent transaction sometime."
Transactions are "atomic" in the sense that they are either
recorded in a published block that is part of the current best
block chain, or they are not.  You can't do just part of a
transaction.  It is DONE (old TxOuts destroyed, new ones created)
in any version of the block chain that contains it.
Locking is something Bitcoin would have had to deal with if
there'd been any kind of accounts that could have coins deposited
into them or withdrawn from them more than once.  A txOut on the
other hand is non-divisible, and is created exactly once and spent
exactly once. The Bitcoin design didn't have to deal with locking
as such because it was built on "atomic" primitives.
Locking issues (and potential division attacks) arise when people
try to force complicated things that involve multiple sequential
transactions to happen.  The design of such interlocking sets and
sequences of transactions is fraught with all the usual troubles
of protocol design and can be got wrong even if the individual
steps are properly atomic.
Ethereum, unlike Bitcoin, provided persistent accounts with balances
that could change more than once.  So, yes, they needed locking.
Apparently a bunch of transactions were done with a script that
failed to execute the locking mechanism correctly.
By nature such a thing can't be automated.  There has to be community
support for the hard fork to happen. You can't do it yourself unless
you've built in a technical stop that would make you "trusted" in the
evil sense; someone whose failure, coercion, or bad faith can screw
everyone else over.
Contract or not, if people don't actually *do* the hard fork
when you call for it, you can't pick up your toys and go home
because they're not yours anymore. Not unless the user community
allows you to keep playing with them.  All the alternatives involve
drawn out proceedings in an old-fashioned analog courtroom, with
expensive lawyers and a judge and jury who're going to have to study
cryptocurrency ideas and protocols starting from scratch in order
to even understand what the contract they're being called on to
enforce is about.  The eventual result of such a proceeding is
something I'd prefer to neither rely on nor wait for nor pay for.
As John Boehner said, "without followers a leader is just a man
taking a walk."

@_date: 2016-07-27 09:16:22
@_author: Ray Dillinger 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
If an "attack" presupposes the application of brute force,
that's not an indication that a cipher is insecure. That's a
secure cipher functioning as intended.
Remember when the NSA shortened the Lucifer keys from 64 bits
to 56 bits when selecting an algorithm for DES?  That's because
there is a formal definition, at least for symmetric ciphers,
of security.
A secure cipher is one which cannot be broken by any means
_more_efficient_ than brute force.  They saw a cipher that gave
56-bit security (due to an attack the public didn't know about
at the time) and unless the key were shortened to 56 bits, it
would not be secure.  If they knew everything we knew today,
they'd probably have lowered the key length to 54 bits. A 2-bit
break on a cipher that's been in use for 40 years and heavily
studied isn't bad for practical security (in the sense of
securing systems, 3DES is still a very reliable choice). But
it means that neither DES nor 3DES meets the definition of
a secure symmetric cipher.
A cipher with an 8-bit key would be formally secure if there
were no way to find a key more efficient than enumerating 8-bit
keys until a solution were found.  The same is true of a cipher
with more possible keys than there are atoms in the visible
universe multiplied by the number of Planck times remaining
until its heat death.

@_date: 2016-07-27 10:14:38
@_author: Ray Dillinger 
@_subject: [Cryptography] Code is Cruel -- The DAO 
This is highly relevant to Ethereum.
It would be relevant to Bitcoin if accounts existed.
On the block chain, txOuts disappear from the consensus
history and new txOuts appear and everybody checks that
the new ones add up to the same number of coins as the
old ones.
It isn't like a check register, because every transaction
records both the txOuts destroyed AND the txOuts created.
As with double-entry, the money disappears from somewhere
and appears somewhere else.
There is no recorded label as to their purpose or owner.
If someone does account-based double-entry bookkeeping to
keep track of which ones are owned by whom and what they
intend to do with them, that requires accounts and hence
must happen off-chain.  Essentially, Bitcoin has the same
"accounting" requirements as physical cash.  As long as
a particular physical dollar bill can't exist in more than
one instance, can't be copied, and won't vanish from my pocket
due to someone else's non-performance, holding and spending
it doesn't require me to do accounting.  The block chain is
just a mechanism for ensuring that it can't exist in more
than one instance, can't be copied, and can't vanish due
to someone else's nonperformance.
Also the Bitcoin block chain has no model for debt, hence
no role for more sophisticated forms of bookkeeping to
track debt.  Debt would require accounts.  If you want
to borrow Bitcoin from somebody, or allow them to borrow
Bitcoin from you, you have to do that in some venue that
has accounts, hence off the block chain.  And such borrowers
have a history of melting down, as anyone who's gotten
Goxxed (at any of them! eMpTy Gox is the largest, not the
only, example of Goxxing) - can tell you.
This ignores the fact that Bitcoin itself does not measure
an investment that can be recovered in any other form. It's
just as much a fiat (unsecured or "debt" asset) as any other
form of fiat money, the only distinction being the identity
of the entity by whose fiat it exists.  With Bitcoin it's the
holders' fiat rather than any sovereignty's.  But don't tell
them that. They are confused on this point and happy about
it NOT being fiat, and about the block chain not being
capable of recording debt.

@_date: 2016-07-27 10:30:21
@_author: Ray Dillinger 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
In general, I refer to the fact that enumerating all
possible proofs is the same method for finding whether
a cipher is secure, as enumerating all possible keys
is a method for finding an encryption key.  (an
encryption key, after all, is a particular bit
string which we can prove has some property with
respect to another bit string).
One could in theory base a secure cipher on the
problem of finding mathematical proofs that the
equations governing particular problems are secure,
just as we normally do with ciphers which are proofs
that particular numbers have particular properties
relative to the boolean equations that comprise
the cipher algorithm.
But how would you prove that such a hypothetical
cipher were secure?  Oh, wait (Wait forever, in
fact).  Didn't we say that the Halting problem was
one we wouldn't necessarily need to address?

@_date: 2016-06-06 18:29:29
@_author: Ray Dillinger 
@_subject: [Cryptography] "Physical Key Extraction Attacks on PCs" 
It's a common effect of people using the GMP bignum libraries.
There exist constant-time multiplication routines etc in that lib, but
they are not the default.  So using them requires that you
(a) realize there is a distinction which may matter.
(b) realize that the default GMP multiplication routines put you
    on the wrong side of the distinction, and
(c) read the documentation enough to find that the constant-time
    multiplication routines you want exist and how to invoke
    them.
The last step isn't at all hard, but most implementers in the
field, especially if inexperienced in coding crypto security,
miss one or both of the first two.
It is likely to be an issue with the default config of bignum
libs which I haven't worked with as well.

@_date: 2016-06-09 10:34:29
@_author: Ray Dillinger 
@_subject: [Cryptography] GNU's "anonymous-but-taxable electronic payments 
Honestly, it would be better if software designers hadn't taken that
"ubiquitous network connectivity" thing to heart.  There is nothing
better for security and privacy (and in many cases functionality!)
than avoiding unnecessary connections.
Example 1:  GPS does not require a network connection.  It is an
application for a passive radio receiver.  But, when I used a GPS
on my phone, it loaded the network every damn time downloading the
map.  Not just checking for map updates, but downloading the whole
damn map.  Every time.  And if it didn't get network connectivity
it wouldn't use the map it had downloaded last time, it just failed.
Maybe people don't want their location monitored in real time,
would enjoy a GPS application that doesn't waste bandwidth, or are
somewhere out in the boonies with GPS reception but no cell towers.
This is one of several reasons I no longer use smart phones.
Example 2:  Help does not require a network connection.  When I
go to a conference, it's often about code and I sometimes want to
write or review code.  But it's always got a hotel Wi-Fi, and I
don't usually want to bother to try to be secure on a hotel wi-fi,
so I don't hook up the network.  The immediate result is that none
of the applications help systems work.  And there's not even an
option to download a snapshot of the current help database for
offline use.  Maybe people don't want what software they're using
and what they're doing with it monitored.  Maybe they're someplace
where they don't trust the network.  Incidentally none of these
help systems are encrypted, so MITM and content monitoring is
direct and simple.
Example 3:  Document preparation softare that wants to save a
copy every fifteen seconds and wants to save it over the network.
It becomes completely unusable (because you can't turn this "feature"
off) if not network-connected because it pops up an annoying dialog
every fifteen seconds that you have to click away.  And maybe I
didn't want people monitoring what software I was using for document
prep or what I was writing.
Example 4:  Device backup and sync systems that are completely
incapable of using local media.  Maybe I don't want people at your
data center pawing through all my contacts, emails, appointments,
applications, and documents.  Or maybe I want to sync my devices
while I'm at a & hotel, coffee shop, or other place where the
wifi is insecure, or even while visiting my brother who lives
where there is no cell service.  I'm fine with using a USB stick
to do sync, thanks.  Why can't I?
In general, any software which is incomplete or non-functional
in the absence of a network connection, ought to be software that
is utterly useless in the absence of information from a remote
location that becomes irrelevant within one minute.  Otherwise,
just check for updates - or better yet, check configuration for
how often to check for updates.  I'm happy checking for help
and map updates once a month, and if I turn off real-time traffic
flow monitoring, I shouldn't be bothered about the absence of
network connections.

@_date: 2016-06-10 12:49:56
@_author: Ray Dillinger 
@_subject: [Cryptography] GNU's "anonymous-but-taxable electronic payments 
Absolutely.  That's a prime example of an application which is
completely useless lacking information less than ten seconds old,
so it doesn't fall under the heading of "unnecessary."  It does
however fall under the heading of "necessarily private."

@_date: 2016-06-10 13:52:07
@_author: Ray Dillinger 
@_subject: [Cryptography] Rumor has it that AES-256 is broken (again!) 
I don't think anyone can *officially* debunk with absolute certainty,
because by definition we don't know what breakthroughs have yet to be
discovered. But this seems about as likely as the spontaneous
teleportation of a large solid body via simultaneous quantum
tunneling of every subatomic particle in that body and having it
arrive three feet away and still solid.  You'd need at least three
huge breakthroughs, one of which is extremely unlikely and two of
which are as close to impossible as nevermind.
1)  Major breakthrough in factoring reduces factoring time by many
    orders of magnitude.  Unlikely at best, and would qualify someone
    for a Nobel prize in mathematics. Factoring has gotten better,
    and may get better yet, but short of a 256-bit quantum computer
    (which they definitely didn't do, or it would have made front
    page headlines all over the world) nobody's getting that much
    of an edge in one jump.
2)  Unsuspected relationship of factoring to AES allows factoring
    to help anyone recover AES keys and/or decrypt AES.  This is
    so out-of-left-field that it would be unrelated to all prior
    mathematical work and knowledge. I would cheerfully bet one
    penny against life imprisonment on this, and call the guy who
    put up the penny a sucker.
3)  Hypothetical Factoring-based attack reduces AES256 by over 200
    bits of security. It is either 70 orders of magnitude faster
    than the biclique attack due to Bogdanov/Khovratovich/Rechberger,
    or 69.5 orders of magnitude faster and can be used together
    with it.  The biclique attack is the best known attack on
    AES, and it reduces AES256 complexity by about a factor of 4.
    It was a major breakthrough and used math that's actually
    known to be related to AES.
I would say that it's completely safe to ignore this rumor.  The odds
that the reporter just plain got the story wrong are googolplexes of
times greater than the odds that it's actually significant.  Reporters
getting stories just plain wrong, after all, has happened at least
once in the history of the universe.

@_date: 2016-06-10 17:28:39
@_author: Ray Dillinger 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
Contracts as legal commitments are not operative until signed
by all parties.  Software commitments, when competently implemented,
are the same.
The fairness issue arises, to some extent, if there is a period
of time during which Bob may have the *option* of signing or not,
after Alice has already signed.
But this is easily prevented - by incremental signing, as you
suggested, but secret splits, including visual-cryptography
splits, are not appropriate, because for any such split one can
fabricate a corresponding split that combines with it to make
a different contract.  Thus the commitment fails, because
Alice has signed a split of contract A, and Bob provides a
specially constructed split that combines with it to form
contract B and claims Alice agreed to contract B when she
Instead, incremental signing starts with the hash of the whole
document, forming a bit commitment, and continues by signing
the document itself.  It's impractical (hopefully impossible)
to create a meaningful-looking contract that has the same hash,
so Bob doesn't get the option of fiddling it.
To prevent contract substitution using visual cryptography,
you'd have to start with a contract containing a signature
using a key Bob doesn't know, so he couldn't substitute a
different validly-signed contract - but if Alice has to create
a signature (a signed hash) anyway why not let the protocol
use the operation of signing a hash in the first place?

@_date: 2016-06-11 13:32:41
@_author: Ray Dillinger 
@_subject: [Cryptography] GNU's "anonymous-but-taxable electronic payments 
I guess I resemble that remark.  So I'll respond.
As I said at USC last year, "yes you can completely secure a computer
for DRM purposes.  But then it's not a computer anymore."
I don't care about DRM as such; I'm pretty much uninterested in
mass-market entertainment.  There's approximately nothing out there that
I'd even bother to steal.  I would cheerfully abandon everything I might
ever gain by breaking copyright, and would STILL remain a hardcore
opponent of anything that could be used to enforce DRM, because
copyright has nothing to do with why DRM is unacceptable.
What I care about is non-encrypted data which, nevertheless, can't be
used in a general purpose way.  At the bottom level, there is nothing
different about playing media and doing any other set of operations on
digital data.  Provide me the logic and math functions I need to write a
decent spreadsheet, and I can use them to modulate digital information
for output over a speaker.  Provide me the disk drive I need to read and
write backups on my computer, and I can use it to read and write media
files as well.  Binary data is binary data and operations on data are
operations on data.
In order to implement DRM, then, somebody has to interfere with my
ability to do math and logic I'd need to implement spreadsheets, or my
ability to use the disk drive I'd need to read backups.  Suddenly I
can't be trusted to write code that uses these facilities anymore, or I
might write a media player.
I don't really give much of a crap about media players, but if someone's
FEAR of them interferes with me using my machine as a general purpose
computer, that is an unacceptable outcome.
If you want your media to be opaque to general-purpose computers, then
distribute it in encrypted form and put the decryption key in
tamper-resistant hardware in your dedicated media players.  I'm fine
with that.  You can even put one of your dedicated media players in the
same box with my computer, and fix it so only one of the two gets power
at any moment, so the computer has absolutely no access to the media
player or to the data it decrypts.  That's fine too, as long as the
media player also has no access to the computer.  And when media
playing software runs on the computer, it will have no access to your
media because of encryption, and that's still fine.
But if you intend to interfere with my ability to use the CPU's math and
logic functionality, or read or write the screen pixels or the audio
stream or the disk drive _while_the_computer_is_running_, then we're
going to have a fight.

@_date: 2016-06-13 11:49:20
@_author: Ray Dillinger 
@_subject: [Cryptography] GNU's "anonymous-but-taxable electronic payments 
Incidentally, DRM no matter how well or completely it's implemented
always fails.  Whatever can be played can be recorded. People mod
speakers to record the electrical activity of the drivers, and point
thirty-megapixel cameras that record sixty-four-bit color at every
one-megapixel area of a screen that displays thirty-two-bit color, rip
media then distribute it all over the planet.  It doesn't even require
cryptanalytic cleverness or delicate hardware hacks to extract keys from
tamper-resistant modules.  The so-called "analog hole" is sufficient to
recover every last digital bit of current formats, and enough bits that
no human will be able to tell the difference of any future format.
So, really, if you don't want people to be able to read your media in
unencrypted form on general purpose computers, you can't allow them to
play it even on secured dedicated media players.  And if they can't play
it you have no copyright business anyway.
As I said, breaking copyright protection is unimportant to me.  But if
something doomed to failure, it is pointless to make any sacrifice for
it, let alone a tremendously valuable one like general purpose computers.

@_date: 2016-06-13 13:01:25
@_author: Ray Dillinger 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
I do not understand how that is an issue.
Surely, if each message contains the hash of messages received so far,
then the last message (at any time) contains the root of a Merkle tree
demonstrating the complete correspondence record.  There is no way for
Bob or Alice to continue correspondence while keeping the other ignorant
of what is and isn't on the official record of their correspondence, and
either can prove the existence and receipt of any message ever responded
to "on the record" by the other.
At any moment, both correspondents have a complete record of everything
the other has acknowledged "on the record", and anything not
acknowledged is inoperative as part of the record anyway because the
other party might not have even received it. It can be resent, and any
further messages originating from the sender will include its hash, or
else the correspondence can just plain stop until it's acknowledged.

@_date: 2016-06-13 13:10:50
@_author: Ray Dillinger 
@_subject: [Cryptography] Determining TLS session keys from the hypervisor 
This is not just hypervisors.  This attack can be carried out  against
nearly every kind of VM and interpreter.
People are doing a heck of a lot of crypto in things like Java, Python,
etc.  People are doing crypto in Javascript inside browsers.  It's all

@_date: 2016-06-13 23:02:30
@_author: Ray Dillinger 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
If Bob has not made some response to the contract, then there is no
reason to believe that he received it.  If he has sent anything else
to Alice in the meantime not including the contract in the Merkle
root, Alice knows he's not claiming to have received it and can
resend.  If Alice has sent anything else to Bob in the meantime,
he cannot respond to it without going on record as having received
or not received the contract.
Anyway, until he's gone on record as having received the contract,
the correspondence stops.  Likewise, until Alice has gone on record
as having received the *signed* contract, there's no reason to
believe she received it.  Until she goes on record as having received
or not received the signed contract, correspondence ceases.
In practice, I'd expect follow-up confirmation messages to be part
of the protocol, and if it takes more than an hour or so for either
party to acknowledge the receipt of the contract or the signed
contract, consider it inoperative.  Bob who didn't get Alice's
ack on the signed contract has no contract.  Alice who didn't get
Bob's ack on the contract itself has no contract.  There is no
infinite regress because neither needs an ack on the other's ack.
And there is certainly no coming back a month later and claiming
Bob believed he had a contract when he can't produce Alice's ack
for that signed contract.

@_date: 2016-06-13 23:24:29
@_author: Ray Dillinger 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
Suppose that Alice decides she wants to rescind her offer.  Shes not
actually allowed to do that, but she can claim that she never received
Bobs acceptance, and Bob cant prove otherwise.
It doesn't work.  Both of them know within an hour that they either do,
or don't, have the other's acknowledgement of the contract (or the
signed contract).  Alice who made the offer and does not have Bob's
ack on the offer is not bound.  Bob who signed the offer but received
no acknowledgement of the receipt of his signature, is not bound.
Expiration of an hour without those conditions being met, means the
contract is not operative.
I have the impression that your proof that this is insoluble relies on
preconditions which would be laughed at in any court of law - such as
the notion that correspondents are not aware of the passage of time or
can consider time to be a meaningful part of the protocol.  Or on the
parties playing ridiculous mathematical games like a child asking "why"
iteratively forever, insisting on an infinite regress of acknowledging
the acknowledgements.
Such antics and requirements for absolute mathematical purity do not
impress judges and juries, any more than a child's demand to know the
ultimate cause of the universe.

@_date: 2016-06-14 08:43:52
@_author: Ray Dillinger 
@_subject: [Cryptography] Music players that don't promote eavesdropping? 
Just out of curiosity, is there any such thing as a desktop music player
(for Linux) these days, which can be configured to NOT use the Internet
at all?
Music playing is high on my list of things that have absolutely no need
for an Internet connection.
Every damn thing, even those considered "minimalist", wants to download
f&*(%^ album covers from this, lyrics from that, track lists from
somewhere else, internet radio listings from another place, etc etc etc,
by unencrypted channels, and doesn't seem to have options to turn this
obnoxious behavior off.  Even if I never ask it to do these things.
And then next time I go to Amazon, it's clear that they know exactly
which tracks I've been listening to within the last 24 hours, because
they try to sell me more stuff by the same artists. Even terribly
obscure artists whom I have played only on that one day out of the last
six months.
Maybe I would rather *NOT* allow every advertiser and snoop in the world
to know what kind of music I listen to and when I'm listening.
Just ... seriously, is there any way to shut these damn things up and
still have a music player on the desktop, or do I have to abandon the
desktop and use a console-only music player if I care?

@_date: 2016-06-14 08:59:29
@_author: Ray Dillinger 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
But protocols insofar as anyone cares about them are used for reasons
that people care about.  If the problem is signing a contract, the legal
binding that people are asking about is decided by a court.
If communications are unreliable, obviously there's a bottom case in
which communications don't exist at all and you can't make a contract no
matter what.  So what even was there to prove?  That missed messages
such as happen when communications don't exist at all can cause a
protocol to fail?  Was that a surprise to anyone?
Anyway, there are plenty of public, irrefutably timestamped venues where
neither participant controls the timestamp and neither participant can
rescind a message.  Usenet, bitcoin blockchain, etc.  Commitment or lack
thereof can be based on what appears on those channels within a time
limit, and there's no way either party can claim it's there when it's
not, or that it's not there when it is.
And if communications are unreliable, then maybe one party can't check
the venue on the first try.  But when they do check, they can see that
the terms were met and the contract is operational.
You can consider such a venue to be a third party, if that preserves
your proof of impossibility and you care about the proof.  But there's
no application your proof is meaningfully applicable to in the sense
that the practical application is impossible.  That would be like
claiming that the proof that the halting problem is insoluble in the
general case means nobody can prove that the specific case of "hello
world" terminates.

@_date: 2016-06-16 16:27:58
@_author: Ray Dillinger 
@_subject: [Cryptography] Music players that don't promote eavesdropping? 
I have discovered the issue.
Now switching OFF the "install-recommends" option which some
dweeb interpreted to mean "install every plugin you can."
And Wireshark is now happy with my Rhythmbox install; it
suddenly emits no packets.

@_date: 2016-06-23 15:14:42
@_author: Ray Dillinger 
@_subject: [Cryptography] Code is Cruel -- The DAO 
Yes.  Among other things, locking is one of the points that Hal Abelson
cited when he and Satoshi were deciding which opcodes should be allowed
in the Bitcoin scripting language.  IIRC, Satoshi wanted to enable more
programmability but Hal demonstrated some ways that recursion and/or
looping operators could be used to attack the system - or enable legit-
looking scripts that could be attacked later, as apparently happened
with ETH.  In the end they wound up removing all the potentially
backward-branching control structures.
IOW, every "later" instruction executed in a Bitcoin script is also a
"subsequent" instruction in the serialization of that script; control
can jump-over sections but can't jump backward to execute anything
more than once, nor call anything that might be called from more than
one place.  ETH didn't have the same structural constraints.

@_date: 2016-06-23 16:29:08
@_author: Ray Dillinger 
@_subject: [Cryptography] Digital currencies 
Ultimately the efficiency (and expense) of any payment system must
be measured by the resources it uses to conduct each transaction.
Block chain based currencies have high expenses in the 'salary'
paid to miners, most of which is inevitably passed on to electric
utilities for power that could have been used for more obviously
valuable purposes.  And although it's mostly 'donated' at this
point, the bandwidth, computing, and storage costs are fairly
substantial. Other cryptocurrency schemes could possibly reduce
the mining expenses substantially, but the bandwidth and storage
costs will always be orders of magnitude greater than those paid
by conventional payment systems.
Conventional currencies, on the other hand, bear a different but
not inconsequential set of expenses.  Conventional payment systems
have to measure the salaries and maintenance costs of people and
institutions whose function is to keep track of transactions and
prevent deception and fraud.  Also, they must measure the expense
of wholesale deception and fraud that they pay (the 'bezzle' that
drops out of the system whenever large-scale fraud and unsound
practices are corrected, whether by collapse or regulative action).
Wholesale fraud and deception can be terrifyingly expensive.  When
regulators have been on the take, hobbled by unbelievably stupid
laws or lack of laws passed or not passed by bought-and-paid-for
legislators, incompetent, or otherwise spectacularly ineffective,
as in the 2008 crisis, the bezzle dropped due to wholesale fraud
can be measured in hundreds of billions, or globally trillions,
of dollars.
Both systems, unfortunately, are subject to retail fraud and
deception - the activities of individual criminals, which
amount to a more or less constant drag on both systems. But
that amounts to a fairly small expense, and for conventional
payment systems it is still shrinking as financial security
gets harder to penetrate.  For block chain based systems, I
don't see it dropping much - people are not getting much harder
to phish, their passwords are only marginally better than fifteen
years ago, home computer security isn't getting (much) better
anytime quickly, and smartphone app security is laughable at
this point.
The ability to levy fines, reverse transactions, and/or seize
assets is a huge reduction of the expense of retail fraud on
conventional systems - but to some extent it only transfers the
expense to the wholesale fraud that occurs when these powers
are abused for that purpose.
If that were the full basis of comparison, it might very well
be hard to pick which system is more efficient.
Unfortunately, as far as I can see, in any universe where the
block chain based solution scales, transactions actually on
the block chain become so expensive that they are limited to
very large transactions.  Smaller transactions would have to
be handled "off-chain," which ultimately opens up the system
to the traditional model of banking - lending at fractional
reserve, closed ledgers, individual transactions not recorded
anywhere they can be checked or proven immutable, etc.
In short, you cannot scale the block chain solution without
reenabling wholesale fraud and deception, and the expense it

@_date: 2016-06-23 16:34:24
@_author: Ray Dillinger 
@_subject: [Cryptography] Code is Cruel -- The DAO 
I am wrong.  I said 'Abelson' when I meant 'Finney.'  Apologies to
anyone confused by my misstatement.

@_date: 2016-06-26 10:11:17
@_author: Ray Dillinger 
@_subject: [Cryptography] 40 years of "Diffie-Hellman" 
Maybe, but it's arguable whether it counts if he didn't publish.
The world at large didn't benefit from his invention of it, but
we all benefit from Diffie and Hellman's (re)invention of it. I
say credit where credit's due means give the credit to the guys
who did something that gives the world a benefit. It's not like
they stole it from him.

@_date: 2016-06-26 22:39:59
@_author: Ray Dillinger 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
Hmmm.  There's this weird property of error-correction codes based on
CRC redundancy checks that the error-correction code identifies the
location of the bit errors in both the document *AND* the error-
correction code itself.
The Diffie-Hellman protocol allows two parties to communicate
non-secret information pertaining to secret nonces, and derive from
it a shared secret.
And then there's public-key/private-key transformations, some of
which are commutative and therefore useful in blind signatures, as
in unpadded RSA where Da(Db(Ea(Eb(Plaintext)))) == Plaintext and it
doesn't matter whether you decrypt in the reverse of the key sequence
used to encrypt.
And then there's secret splitting, where you have 1-N of the required
set revealing nothing about a secret but any N of the required set
allowing it to be fully determined...
All of these things seem to have properties relevant to or involved
with a hypothetical signature type that violates the assumptions which
the proof of impossibility is based on.
If you do deeper math than I do, figuring out a solution would be a
short trip to at least as much fame (and as much value contributed
to the world) as Diffie and Hellman got out of their key agreement
So here's the conundrum; is it possible for Bob and Alice to pick secret
nonces, and then communicate in a protocol creating a shared
secret, that allows Alice to produce a signature on [Document plus
Bob's signature] while Bob produces a signature on [Document plus
Alice's signature], before either learns the exact bits of the other's
The idea being that neither signature can be checked or verified by
anyone else as relating to a document lacking the other signature.
Alice would be able to check her own signature knowing the secret
nonce she picked (obviously - since she can derive it) but she'd
never have to publish the nonce.  Bob likewise would be able to
check his own signature knowing the secret nonce he picked, but
he'd never have to publish that nonce.  And either of them could
immediately check the other's signature immediately when they receive
it.  But nobody else could check either signature without the other,
and absolutely anybody could check them both given both signatures
and the document plus Alice and Bob's public keys.
If that particular math problem has a solution, then it doesn't matter
what order they're revealed in or who communicates their signature
first.  Revealing enough to show the other's commitment would require
revealing enough to show one's own commitment, and vice versa.

@_date: 2016-06-27 11:10:08
@_author: Ray Dillinger 
@_subject: [Cryptography] RFC: block cipher randomization 
The proposal is the addition of *random* salt to the text to be
encrypted, with the locations of each octet of random salt determined by
a stream of *pseudorandom* bits generated based on a secret key;
possibly the same key used in the encryption.  The recipient of the
message can generate the same stream of pseudorandom bits and therefore
has the ability to subtract the random data from the decrypted message.
It's a sound construction on the whole for security purposes, and it
might be practical as a way to protect against certain types of protocol
attacks which are far more prevalent than mathematical cryptography
Otherwise it won't be used because it makes encryption take longer and
makes the transmission of encrypted text require more time and battery
power.  Cryptographers at present are mostly satisfied about
mathematical cipher security, and therefore more concerned about
bandwidth and CPU time on servers and bandwidth and battery life on
portable devices.
At this point pretty much anyone who understands Feistel cipher
construction can make a *secure* block or stream cipher, simply by
applying brute force and using some ungodly number of iterations on
pretty much any nonlinear round function executed on large blocks. But
it requires serious math chops to make a secure cipher which is also
efficient and flexible for use in protocols.
Second; what you propose is a *very* good protection against
chosen-plaintext attacks (due to salt destroying patterns in the chosen
plaintext) and a fairly good protection against a large common class of
protocol attacks that relies on knowing the locations of particular
pieces of plaintext in the datagrams (because the randomization of salt
locations makes the exact location of plaintext w/r/t block boundaries
of block ciphers and/or the locations of particular pieces of plaintext
in structured messages in stream ciphers unpredictable).
The protection it affords against the latter class of attacks makes them
probably only about one order of magnitude more difficult to exploit,
more on large messages.
Counting against its security is the fact that it causes the size of the
cryptogram to be increased by a factor of two.  This increases the
amount of ciphertext an opponent has to work with, making a difficult
class of attacks that relies on distinguishing statistical properties of
a ciphertext stream slightly easier.  Offsetting the fact though is that
using such statistical properties to recover any information about a
ciphertext stream half of which is random, is more than twice as hard.
Therefore the only class of such attacks that really matters are
key-recovery attacks against long-term keys that will be used in other
messages.  Key recovery attacks are the most rare and difficult subclass
*of* the aforementioned difficult class, useful only against a type of
key which has become quite rare given public-key encryption and
Diffie-Hellman key negotiation.
In security terms, the marginal weakening against key-recovery attacks
loses less than is gained by the strengthening against chosen-plaintext
cryptographic attacks, and FAR less than is gained by strengthening
against known-plaintext-location protocol atttacks.
It could be strengthened probably by another order of magnitude against
chosen-plaintext attacks on ciphers and known-plaintext-location attacks
on protocols, with a modification.  As your proposal stands you're using
the stream 'gamma' bit-by-bit to decide whether to insert one byte of
salt or one byte of plaintext.   The attacker still knows that the
alternation between plaintext and ciphertext will be on byte boundaries
only.   Instead, consider using gamma a byte at a time: five bits to
decide how many *bits* of salt to use before switching to plaintext, and
three bits to decide how many *bytes* of plaintext to send before
switching back to salt.  Counting against the modification, however,  is
the primary reason why this method won't be used much in the first
place; bit shifts are an extra step that make the modified method take
even more time and use even more battery power.

@_date: 2016-06-27 15:18:52
@_author: Ray Dillinger 
@_subject: [Cryptography] RFC: block cipher randomization 
Jerry had a fine point too;  Any "extra" data sent, if your
correspondent can't check it, is an excellent way for malware to
exfiltrate data.  And malware will exfiltrate stolen data in any way
Grrf.  Recently dealt with malware exfiltrating data in DNS queries
of all things, where the botmaster was intercepting traffic at the
DNS server using Wireshark. It's a particularly annoying technique,
because *NOBODY* firewalls DNS, most security software ignores DNS, and
a bunch of applications  CRASH if you firewall DNS.  Networks with
sensitive contents now need to be running their own DNS servers
in-house and preferably deploying DNScrypt.
But in this case I think it's a nonissue, because whoever hopes to get
at exfiltrated data has to be able to first decrypt the ciphertext, and
that requires them to have the session key that your *legitimate*
correspondent is using.
With access to the plaintext, it probably isn't necessary to be able to
generate the pseudorandom stream to find the salt/plaintext boundaries;
it's fairly easy to separate interleaved datastreams, especially if one
or both of them has a known format.

@_date: 2016-06-29 20:28:23
@_author: Ray Dillinger 
@_subject: [Cryptography] RFC: block cipher randomization 
Except in this case, as described, it's not XOR'd - it's interleaved
with the "real" data, and then the mixture encrypted using an
ephemeral secret key shared with the legitimate correspondent.
The only creature in creation that's able to exfiltrate stolen data
over *that* channel, is the legitimate correspondent.  So while
uncheckable data in general provides an exfiltration channel, that
channel in this case is extraordinarily narrow.
It's certainly no more troubling than the use of any PRNG as a stream
cipher, and that's a very common construction.
One could substitute PRNG output for the "random" numbers in this case,
with the N-bit initial PRNG state sent as the final N bits of the
interleaved data.  The problem, of course, is that the "fix" comes at
the expense of yet more CPU cost, and this construction is already too
expensive in CPU for any non-niche use.
That's interesting as a simple-ish primitive for multiplexing and
demultiplexing garlic-routed messages. Admixture of many encrypted
channels in a single stream, where in order to separate and decrypt the
stream you need the stream key, but in order to read any part of a
channel you'd also need the channel key.  The cutover points between
different data channels would be determined by PRNG generated from the
shared stream key.  The endpoints, knowing the stream key because they'd
established the stream in the first place, could separate the
encrypted channels, remix them in different combinations, and
redistribute them using different streams.
Of course it's recursive; any "channel" in a given stream might contain
a stream of its own.
That mutation of the original idea might be a very good one; it could
enable continuous-streaming garlic-routed channels, with cutin/
cutout of particular channels on a realtime basis becoming stupidly
simple.  And it doesn't waste CPU/bandwidth on encrypting bits we don't
actually care about; it's just a simple way to multiplex/demultiplex
channels over a shared encrypted stream and doesn't spend (much)
more CPU/bandwidth than stream encryption normally would.

@_date: 2016-03-01 08:36:56
@_author: Ray Dillinger 
@_subject: [Cryptography] DROWN attack on SSLv2 enabled servers 
And this - though I didn't know exactly what shape it would take
at the time - is why I patched all those god damned export ciphers
with "assert(false)" statements and recompiled after the protocol
downgrade attacks of last year, and advised everyone I know to do
the same.  Yes, I will take the DoS of having my server just
plain crash if someone gets to them.  That's better than having
data stolen.
Good gods, how much has that idiotic requirement cost the USA by
this time?  And the rest of the world, for that matter?  How much
more damage will it do before it is killed absolutely dead with
a mouth full of garlic cloves in its decapitated head and a
stake through its heart?
I didn't absolutely *KNOW* that someone would find a way to get
to them, but it was complicated enough that I couldn't be sure
they wouldn't.  Hence, "assert(false)."
I HATE being right about crap like this.  It's hard to get over
paranoia when it keeps being right.

@_date: 2016-03-02 13:41:22
@_author: Ray Dillinger 
@_subject: [Cryptography] The FBI can (almost certainly) crack the San 
Which brings us to a point.  The last three times I've bought a
laptop, the BIOS has been configured with a password that I as a
customer was not given, which twice I had to get from a confused
tech support rep (pointing out that damnit I OWNED the device and
could install what the hell I wanted), and the third time I just
went screwit and took debugging leads to the board to reset it.
All this, so I could install a non-microsoft OS.  And I had to
pay for the microsoft OS (which I never booted) all three times
regardless because the laptop is not sold without it.
If these devices were supplied with TPM chips, may we suppose that
their initial configuration would be any more generous to those
of us who do not choose to use the "approved" software?  By analogy,
is the iPhone useful, AT ALL, to anyone who does not choose to run
iOS?  To anyone who does not use Apple's storage solutions?  To
anyone who gets software from places other than the Apple Store?
MS is selling Windows phones and tablets that literally *CAN'T*
run any other operating system, exactly the same way. Good luck
running the supposedly "open" Android system without being part
of Google's carefully-contained ecosystem, too.
To put it bluntly these tablets and phones are not general-
purpose computing devices; they're rented plots inside the
service providers' walled gardens.  And for my laptop and
desktop devices I need general-purpose computing devices.
Would the TPM really have been used any differently? The
Secure Enclave *could* have been used differently, but it
isn't.  Ever.  Is the way the Secure Enclave is actually
being used any different from the fears the EFF had about
the TPM?
I want the security.  But I don't want the system I'd get if
that security were a commodity that software or service
providers could use to create captive markets in their walled

@_date: 2016-03-02 13:53:35
@_author: Ray Dillinger 
@_subject: [Cryptography] LibreSSL unaffected by DROWN 
Yep, complete non-story.
Backward compatibility is Backward.  No surprise whatsoever.
Those who keep Stupid features in the codebase, get damaged
by Stupid attacks.  Rinse, repeat.  Nothing new except, maybe,
eventually, the learning of the lesson.

@_date: 2016-03-03 08:50:30
@_author: Ray Dillinger 
@_subject: [Cryptography] A question on Shannon's entropy 
If in a sample of n things you find a strictly increasing order,
then the generator from which the sample is drawn is not
producing independent outputs - each is constrained by the
previous one.  Because Shannon Entropy is not defined on
sources unless they are producing independent outputs, this
source cannot be said to have an entropy measure.
That is not the same as saying the measure is low, or high, or
that the measure is any particular value - the measure simply
doesn't exist.  It would be like talking about the mass of
the color green or the aroma of an array.

@_date: 2016-03-03 09:31:36
@_author: Ray Dillinger 
@_subject: [Cryptography] Side channel attack on OpenSSL ECDSA on iOS and 
OpenSSL considers itself busy enough patching other holes, I
suppose.  It still looks rather !clue of them though.  And it's
starting to look !clue of other software maintainers who depend
on them.
Apple's iOS kernel IIRC is based on BSD sources, and BSD cut the
support for SSLv2 and export ciphers before this appeared. So I
would assume the answer is "no it isn't."
Even if it were, getting the key would not serve the FBI's
objective here. They don't give a rat's ass about what's actually
on the phone. What they are seeking is a precedent that says they
have the right to force a private company to decrypt things for
them by any means possible.  Secondarily they are interested in
forcing Apple to bring into existence a software artifact that
would enable it to do so easily and instantly to other devices.
That would devalue Apple by undermining Apple's current
marketing position, but it's not clear whether that's intended
as a punishment and an important part of the FBI's goal.  It
might be merely collateral damage. And pigs might fly.
Much of the precedent in cases so far has been based on an early
ruling that smartphone customers have "no expectation of privacy"
in the contents of their devices.  I don't think the response of
consumers to marketing based on Apple's "we don't even have the
key" policy, and the fact that Apple is successfully distinguishing
itself as a premium brand based on customers' ability to keep
their data private, is consistent with that early ruling. I'm
no lawyer, but wouldn't it be sensible to revisit that ruling
in the light of current evidence?
"So come on over tonight, and we'll sit out on the swing
 And watch the pigs fly by, flapping their brand new wings.
 Just sit back and relax, and watch me eat my hat,
 As the old oak tree sprouts dollar bills and looks you in the eye
 And a big old bolt of lightning strikes me not just once but twice.
 See the premier performance of Demons On Ice...."
   --Brad Paisley

@_date: 2016-03-06 08:38:38
@_author: Ray Dillinger 
@_subject: [Cryptography] EFF amicus brief in support of Apple 
The fundamental tension is that the people who want security
and the people who want to secure systems, are not the same
The difference is that one is a noun - something you can get -
and the other is a verb - something you can do.
The current state of the art is that providing secure systems
for people who do not participate in working to secure their
systems is extremely hard.  To the extent that it can be made
easier, there is a further tension: People whose software effort
is for the purpose of making things easier for nonparticipants
and those uninterested in technology are mainly profit motivated.
The profit motive does not normally promote the same security
interests as people who want to secure their personal data
because that personal data is a profit center.
It is possible that a profit center can be identified outside
of selling customer data to advertisers.  Such a profit center
could bring the security interests of software providers into
alignment with those of people who want to secure their
personal data.  But if we're talking about making things easy
for nonparticipants uninterested in technology, that is a
market segment mostly unaware of or apathetic about the invasion
of their lives and the sale of their personal data.
Finally there are powerful entities other than advertisers
who have a motive to keep the same information the advertisers
want available.  By any means necessary. China, for example,
passed laws mandating backdoors in all encryption software
and devices, and people are still getting their encryption
devices - from chips with built-in AES function to smart-
phones fully assembled with network hardware and cell phone
radios - manufactured there.
What are the odds that the manufacturers whose devices are
shipped off to the round-eyed foreigners the government is
most interested in spying on get an exception to that law?
I'm calling zero.  But anyway, with governments generally
against the network "going dark", we can expect that any
business model that involves securing personal data will
be strongly discouraged in many jurisdictions and that those
entities will not be interested in telling each other or
us what holes in security they have created.
We can expect, in fact, that they will do whatever they can
to prevent the emergence of any methodology that would make
securing personal data easy for those unwilling to make
much effort or those uninterested in the technology.  Which
brings us back to the beginning.
The way forward is not at all clear.

@_date: 2016-03-07 18:08:29
@_author: Ray Dillinger 
@_subject: [Cryptography] EFF amicus brief in support of Apple 
Ah.  Yes.  By "Systems" I meant - and should have said -
"general purpose computers."
If instead you just want a media player, that's relatively
easy to secure.  Software-as-media with no ability to create
anything?  Sure, we can secure that.  But it's not a generative
system.  It's not something that can give rise to anything new
or be put to different uses by different people.
I had a smart phone for a while.  It was boring.  I let the
contract lapse to see exactly how useless it became without
the provider.  Yep, it was that useless.  Didn't even retain
access to the things I'd told it to store locally, which
pissed me off.  It's still around here somewhere probably;
I haven't taken it to the thrift store yet.

@_date: 2016-03-12 17:23:40
@_author: Ray Dillinger 
@_subject: [Cryptography] Govt Can't Let Smartphones Be 'Black Boxes, 
It would be a damned sight easier to take a view that allowed
exceptional access with warrants, etc, if in the past the USG
had abided by the notion that such access is in fact exceptional
rather than routine, and needs to be provided for with warrants,
When the precedent has been set that you're in the business of
hoovering up absolutely everything regardless of privacy or
invasiveness, that you're in the business of leaving security
holes open for our enemies to exploit, and that you have no
respect whatsoever for the right of people to be secure in their
persons and papers from unreasonable search and ...
Seriously, how can anybody trust the USG when now it's saying
that we're wrong to defend ourselves from the kind of exploits
that it's been in the forefront of creating and abusing?  Which
its enemies have repeatedly used against us as well?
Show us any reason to think that this is to be reserved for
individual instances with publicly acknowledged search warrants
subject to due process, or any reason to think that any hole
left for the USG would not also be exploited by Russia, China,
and North Korea, and I'd consider it.

@_date: 2016-03-16 13:02:47
@_author: Ray Dillinger 
@_subject: [Cryptography] Lavabit's and Snowden's Solos 
I think this case actually has a lot to do with the Lavabit episode.
Lavabit received an order asking IIUI to enable the FBI to access a
*single* account. The operator, fearing that their motive might be
persecution and vengeance rather than prosecution and justice, took less
than 48 hours to consult with an attorney. Consulting with an attorney,
as a constitutionally guaranteed right, seems reasonable to me.
He may or may not have voluntarily provided access to that account -
we'll never know, because by the time he had finished consulting an
attorney the FBI had, apparently because of his less than instant
compliance, made a completely intolerable demand instead for the keys to
the entire site. That would have enabled covert, real-time access to the
communications of subscribers who were specifically purchasing the
service of privacy. Deprived of the ability to sell what his subscribers
were buying, he shut the business down rather than engage in fraud or
the provision of pretended services.
Then as now, the FBI made a demand for access so broad in scope,
burdensome to provide, or contrary to the basic principles of the party
from whom it was demanded, as to be offensive.
The major distinction as far as I can tell is that Apple has the money
to hire a herd of lawyers and fight it in court, and Lavabit didn't.
If Lavabit had fought the demand in court and lost (very likely given
its meager legal budget and the infamy of the single account that the
FBI originally demanded access to) it would have established a precedent
which the FBI would now be trying to leverage in further cases. So the
decision by Lavabit to shut down rather than have a court battle was
probably the only available way to avoid the creation of a harmful
precedent.  Leaving the arena means you don't win, but it also means
denying victory to your opponent.
In much the same way the FBI now seeks a precedent in the Apple case,
and in much the same way, compelling compliance with their order seems
more important to them than the data from the single instance that the
case is ostensibly about.
So I have to wonder if an attempt to replace the precedent they planned
to get from a Lavabit case, which they failed to obtain because it never
came to court, may be part of what motivates the FBI in its battle over
the Apple case.
Of course if they had that Lavabit precedent, they would certainly be
using it against Apple right now.

@_date: 2016-03-17 11:59:24
@_author: Ray Dillinger 
@_subject: [Cryptography] Clinton asked for a secure email Blackberry in 
I thought maybe I was the only one taking the "quantum crypto"
noise seriously.  Guess not.  Although RSA keys suffer even more
from quantum crypto than they suffer from advances in "standard"
factoring algorithms.

@_date: 2016-03-17 12:09:37
@_author: Ray Dillinger 
@_subject: [Cryptography] Trust & randomness in computer systems 
Suppose that instead of leaping to conclusions about murder
or political assassination, you were to take the quoted
material in context as it was presented, as a possible way
to avoid subverted electronic devices from subverting more
electronic devices.
In other words, if someone cracks my IoT thermostat I
would rather have the thermostat go out ("commit suicide")
than have it be used to make further attacks on my home
Not that I would ever own an IoT thermostat, but there
exist people who would, they live on the same larger
Internet with the rest of us, and compromises in their
systems impair everyone else's productivity and security

@_date: 2016-03-18 11:45:59
@_author: Ray Dillinger 
@_subject: [Cryptography] Formal Verification (was Re: Trust & randomness 
If the system can fail because of something that was not
formally verified, then even if the formal verification
was perfectly implemented and proved perfect compliance
with the specification...
It does not matter because the specification itself
contained a bug.
This is the real issue with formal verification.  You
can prove that software complies with a specification,
but you cannot prove that the specification itself is
bug-free.  The specification is also code and may
also contain bugs.
The benefit is that the specification language is usually
less susceptible to misunderstanding or unsuspected bugs
because it has much cleaner semantics which would take a
compiler at least days to compile code for (and which in
some cases cannot be directly translated into machine
code but which can be checked)  and nobody really cares if
the theorem prover (as opposed to the compiler) runs for
days.  Another benefit is that even if the specification
does contain a bug, a bug in the program being checked
usually does not coincide with it.
But no language has ever been created which makes it
difficult to write buggy code, and that includes the
representations of software specifications that the
formal verification systems prove compliance to.

@_date: 2016-03-19 14:27:35
@_author: Ray Dillinger 
@_subject: [Cryptography] Formal Verification (was Re: Trust & randomness 
You're right.  My point was that it is a bug in the formal specification
of cars.  And it's cars whose correctness people rely on.

@_date: 2016-03-21 15:53:00
@_author: Ray Dillinger 
@_subject: [Cryptography] Q: Status quo of key exchange via neural 
As far as I know "neural" and "cryptography" are words that are
never used together except in the context of snake-oil.
The issue is that neural systems are a heuristic optimization.
Cryptography means systems that are specifically designed so that
no possible heuristic (nor, indeed, any known computing algorithm
whatsoever, in the absence of the key) can make useful headway.
If neural anything can identify a single statistical trend to train
itself to find, then any cryptography it can find such a pattern in
is hopelessly broken.

@_date: 2016-03-23 19:47:05
@_author: Ray Dillinger 
@_subject: [Cryptography] Unicity distance of Playfair 
Okay, here's your terse recipe:
Keybits/Redundancy = Unicity.
Keybits = number of bits required to count all the possible keys.
Redundancy = RepBits - InfoBits
RepBits = Number of bits required to count the units of the encoding
InfoBits = Amount of information conveyed by each unit of the encoding
Using Playfair we either can't distinguish IJ, or we can't
distinguish YZ.  That makes it a 25-character alphabet.
Log2 of 25 is RepBits, the number of bits required to count the
units of encoding.  It's about 4.64.
InfoBits is the number of bits of information conveyed per unit
of the alphabet used.  In English, it's estimated to be about 1.5.
This is unaffected by the indistinguishability of an IJ pair
or a YZ pair because there are approximately no words you could
confuse for each other due to ambiguity between those pairs -
ie, those particular distinctions don't measurably _contribute_
to the information conveyed per character.
Redundancy = (RepBits - InfoBits) = (4.64 - 1.4) = 3.24.
Playfair has 25! possible keys because there are 25 different
boxes in the grid to fill with letters (one of them gets to be
both I and J, or else one of them gets to be both Y and Z).  It
takes about 83.7 bits to count 25! different keys, so keybits
is 83.7.
83.7 / 3.24 = about 25.8333.  Rounding to the nearest whole
number, therefore Playfair has about 26 character Unicity
This means that, *on average*, given a Playfair key and 26
letters of ciphertext, you can tell whether or not it's the
right key.
Given the ciphertext, though, it's a lot harder to *find* the
correct key with Playfair than it is with a number of other
'hand' ciphers whose Unicity distance is larger. Unicity
distance is not a measure of security.

@_date: 2016-03-23 20:01:18
@_author: Ray Dillinger 
@_subject: [Cryptography] FBI engages Isreali company to crack IPhone 
I think it's hilarious.
"Hey, NSA!  Here we have a private company in a FOREIGN NATION,
able to crack the security of the very same AMERICAN CITIZENS
your charter specifically says you're responsible to protect
from foreign nations!  Egg on your face much?"
This grudge match between the FBI and the NSA is heating up!
I wanna get some popcorn and a ringside seat to see what the
NSA does to spite the FBI for making them look like idiots.
"Better'n Pro Wrestling!" -- Peter Puppy

@_date: 2016-03-24 14:41:34
@_author: Ray Dillinger 
@_subject: [Cryptography] Unicity distance of Playfair 
It doesn't matter.  The cipher produces one output letter per
input letter.  The letter in this case is the basic unit of
encryption.  This is true with almost all hand ciphers.

@_date: 2016-03-24 14:45:38
@_author: Ray Dillinger 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
Monoculture is wrong, but it's less wrong than overcomplicated
and brittle.  Overcomplicated and brittle has been the *only*
thing that non-monoculture solutions have shown themselves
capable of producing so far, so this is a vote of no confidence
in design by committee until the people on committees learn
how important it is to design simple and solid.

@_date: 2016-03-24 17:38:17
@_author: Ray Dillinger 
@_subject: [Cryptography] Unicity distance of Playfair 
You don't have to know how a particular cipher works to calculate
the Unicity distance.  Every cipher that has the same key length
will have the same unicity distance when used to encrypt English.
So thinking of pairs is strictly a distraction here.
Using different units will not affect the unicity distance any
more than the difference between measuring a distance in miles
or kilometers affects the distance.  I got my answer in letters
because I worked the problem using bits of information per letter
to calculate the redundancy of English text.
The number of different possible keys is easiest to calculate
using letters, and the redundancy of English text is available
as a measure per letter.  That made working with letters easiest.
If you want, You can measure the unicity distance in bits
instead, and at 4.64 bits per Playfair character, it's going
to be about 120 bits.  If you want to measure it in pairs it's
going to be about 13 pairs.  The number of possible keys does
not change in either case.
In each case it's the same amount of information because the
redundancy of English scales exactly according to the size
units you're measuring it in.

@_date: 2016-03-24 17:47:23
@_author: Ray Dillinger 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
If you are eliminating the fragility and brittleness that has attended
protocols for determining which thing to use and in which form and at
what key length, etc, you wind up eliminating things from most
protocols.  If you're eliminating things from most protocols it
behooves you to eliminate the same things from all of those protocols,
so as to leave fewer places for bugs or sabotage to hide.
You can argue about which things to eliminate, or claim that DJB made
the wrong choices about it. But that is the slippery slope that leads
to design by committee - so far an unmitigated disaster for security.

@_date: 2016-03-25 15:24:01
@_author: Ray Dillinger 
@_subject: [Cryptography] Unicity distance of Playfair 
It's subject to a few assumptions.  As others have pointed
out, Playfair is a block cipher of two-letter blocks, so
calculating its unicity distance in terms of letters is
valid for an even number of letters.  The Unicity distance
will always be a multiple of the block size, so if the
calculation in letters had come out to, eg, 26.8 or something
you'd have to round up to 28 instead of 27.
Also the redundancy measure I used was a very specific figure
applicable to most hand ciphers:  It was the redundancy of
English text represented in letters, over the space of all
possible strings of letters the same length.
But if we're talking about the redundancy of English text as
represented in a 676-character alphabet (digraphs) over the
space of all possible digraph strings the same length, the
sizes of the units (and therefore the redundancy per unit)
are the only thing that's changed.
You have the same ratio of non-English strings to English
strings at a length of N digraphs, that you have at a length
of 2N letters.  So your redundancy per digraph is twice as much
as the redundancy per letter.
You get a unicity distance half as big as it was with letters
because you doubled the redundancy in the denominator without
changing the number of possible keys.  So you get unicity =
13 digraphs, which happens to be 26 letters.
Sure, you could calculate any cipher's unicity distance in
pairs instead of letters.  But ultimately all it will tell
you is that unicity measured in digraphs is half (rounding up)
the unicity distance in characters.  Because each digraph
is twice as much plaintext as each letter, that's the same
amount of plaintext either way.
Anyway, Viginere (26x26 or 676x676) can have any key length,
so a unicity measurement requires making an assumption about
the key length.  If the sequence across the table is not known
to your opponent (ie, if that's part of the key) then the
676x676 version would have a much larger practical unicity
distance just because there are MANY more keys.

@_date: 2016-03-25 16:06:49
@_author: Ray Dillinger 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
Okay, just in case you're new to the crypto-code game I have to
bring this up.  Encrypt-then-MAC really *is* superior to MAC-then-
encrypt, but you've got to be careful not to fall in one narsty
little pothole next to the road.
That pothole is this:  Alice prepares a message for Bob, which
she MACs, then encrypts.  She sends it to Bob, and he strips
her MAC off of it, puts his own MAC on it, re-encrypts, and
sends it to Carol pretending it's a message to Carol from Bob.
Now this sounds like a dumb thing that could never happen because
Carol wouldn't really be fooled by a message from the wrong origin
bearing the wrong MAC, even if the origin and MAC she actually gets
it from do go together.  But, assuming Carol is running an honest
node (your code?) look at the code for it.  Would she be fooled?
How would she react?
When we're talking about protocol messages, they tend to be a heck
of a lot simpler and harder to tell apart aside from the MAC. Stunts
like this are at the top of the list when people are working out how
to MITM or DoS various protocols or provoke informative responses
(or non-responses!) from other nodes, or block other nodes from
replying to each other, or etc.
So it's a pothole, and you need to be sure that nothing you build
is vulnerable to it - just using encrypt-then-mac solves some
problems but doesn't solve every problem.  You also need to build
in a good reason why nobody will ever be fooled by the *wrong* MAC,
and know exactly how the same message from different users will
be detected and dealt with.

@_date: 2016-03-25 18:41:37
@_author: Ray Dillinger 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
Ergh.  I babbled, of course. The first sentence above is just
plain wrong. Parts of the rest are muddled. Let me clarify.
Briefly:  The misattribution attack on Mac-Then-Encrypt allows
Bob to redirect messages originally sent to Bob (because he can
decrypt those, then replace the MAC, re-encrypt, and resend them).
In security terms this is a pothole.  It can be harmful if Alice
is sending to Bob anything Bob should not be able to produce
himself, but is otherwise harmless.
The misattribution attack on Encrypt-Then-Mac allows Bob (or
Mallory) to intercept an encrypted message from anybody to
anybody, and with no need to decrypt it substitute his own
MAC for the original.  In security terms this is a missing
bridge.  You have to find a different way to get where you're
going.  This is the good reason why Encrypt-Then-MAC ought
to be avoided.
In the Encrypt-then-MAC world attackers can substitute MACs
on messages regardless of whether they can decrypt them -
With a lot of protocols it's a pretty easy guess what's being
said, so inability to decrypt is frequently inadequate defense.
 				Bear

@_date: 2016-03-26 17:57:21
@_author: Ray Dillinger 
@_subject: [Cryptography] Unicity distance of Playfair 
The redundancy is an estimate of how much information is available
from all those sources taken together.  Distribution, pairs, triples,
word frequencies, the whole business.  English conveys about 1.5 bits
per letter - the remaining representation bits, ie, the redundancy,
are 'redundant' because that information is both explicit in those
bits and implicit in that you could predict them on the basis of
all that other information.
I don't think so - a mixed-alphabet Alberti cipher seems a *lot*
harder when solving it by hand.  I believe the mixed-alphabet
multiplies the number of different keys effectively. I've never come
up with a solution to one that used a mixed-alphabet different
from the one the encryptor used, and if it were redundant I think
I would have.
But it often happens with classic hand ciphers that a mixed-alphabet
substituted for a plain one doesn't actually increase the number
of possible keys.  For example when you compose two monoalphabetic
ciphers you don't get a cipher with 26! times as many different
keys (and longer unicity distance), you just get a cipher with
26! different ways to write down each key (and the same unicity

@_date: 2016-03-26 19:15:06
@_author: Ray Dillinger 
@_subject: [Cryptography] More Bad Govt Shit To Fight (Burner Verizon FBI) 
I'm also in California but the main use I see is at halfway
houses and womens' shelters.  There are a lot of scared battered
women out there who can't disentangle exes from their financial
crap fast enough to establish independent phone service without
them, and a lot of shelters that go through them like tissues.
It's a pretty big deal for them because there are *LOTS* of violent
loons who are trying to find those places, and everyone who winds
up there is completely inexperienced in the art of Not Being Seen.
Lots of chances for one person's mistake to become everybody's
problem, and phones are like the "ablative surface" that can be
gotten rid of for mutual protection when someone makes a mistake.
I see their IT manager at Urban Ore, buying a basket full of
old used phones, almost every time I'm over there.

@_date: 2016-03-28 14:09:42
@_author: Ray Dillinger 
@_subject: [Cryptography] Mixing public key crypto systems? 
You could in principle build a system that didn't trust in the security
of any particular ciphersuite.  You could even do it with good security,
although by most people's standards the resulting system would suck.
The issue is that if the whole is to *NOT* be as weak as the weakest
link, then it will certainly be far less efficient than the strongest.
The protocols would be complicated (and therefore subject to bugs in
design and implementation) in a way different from all the complexities
introduced by ciphersuite negotiation in the committee-designed systems
we've seen, exactly because they must be explicitly designed to DISALLOW
ciphersuite negotiation.
BOTH sides in every protocol would pick ciphers and keys completely
independent of the other.  Security must be preserved even if only one
participant selects secure ciphers.
As far as protocols are concerned you just have one cipher suite. Its
keys, hashes, signatures, ciphertexts, etc, are completely opaque blobs
which can be any length.
A key might be formatted something like  AES512:key:foo or
RSA8192:pubkey:foo or EC256kp1:privkey:foo instead of just being
presented as binary string foo after protocol negotiation.  Internally
to the system you dispatch on methods per cipher, but this is at a
completely different layer than the protocol itself, and the choice of
cipher must not affect the protocol in any way.
This allows someone to present a stupid key for a stupid cipher, to
someone else who has security requirements greater than that stupid key
with that stupid cipher provide.  The only way to ensure that the
transaction is not done using whichever cipher is the weakest link is to
ensure instead that BOTH systems are used, that all protocol messages
are subject to superencryption, and protocol transactions which require
both participants to be using the same cipher system must be done twice;
once using A superencrypted with B, and once using B superencrypted with
A. Naturally both must succeed.  And this "wasteful" thing must happen
even if both clients want to use the SAME system because otherwise you
get cipher negotiations into the protocols.
That's horrible because under absolutely every circumstance it will more
than double the compute, memory, time, and, most threateningly, the
complexity overhead of the  resulting system.  But it's the only way
AFAIK to foil the attacks on the weakest link that will happen whenever
a weakest link is discovered, while also not trusting any particular
It is also less robust to noisy channels, means that protocols
between clients with different sets of ciphers implemented or enabled
will fail hard whenever someone wants to use one on which the clients
disagree, and makes it completely futile to choose a cipher in hopes of
limiting the overhead involved. If someone presents a 64kbit RSA key to
your mobile device or Internet-of-targets thermostat, then it either
uses a 64kbit RSA key, in addition to whichever key it picked, or it
fails hard.
Finally of course it opens the way to DoS attacks where compute-
expensive hashes and ciphers are used as a force multiplier.

@_date: 2016-03-29 20:08:22
@_author: Ray Dillinger 
@_subject: [Cryptography] On the 'regulation proof' aspect of Bitcoin 
That was pretty much when I decided it was doomed.  The
Nakamoto Consensus protocol, as implemented, requires
propagating all transactions in a public transaction
register to every node.
Nobody except mass-surveillance snoops has the bandwidth
to receive nor the space  to record that much data, if
the system starts getting any real traction.
The block size limit is, strictly speaking, optional to
the protocol.  But removing it first opens the system to
a DoS attack and second results in a system that, if it
does catch on, does not scale.
I've seen several technically feasible propositions to
address it.  Just removing the block size limit is the
first, but it's a non-starter in terms of scalability.
The next option would be leaving the micro-transactions
to be settled offchain by money exchangers and use the
block chain for a settling mechanism handling inter-dealer
transfers.  But that breaks the basic design of bitcoin
as a trustless system, as many users discovered to their
detriment after treating empty-gox as a trusted party.
A third option is to drop the transaction records from
the block chain.  You are left with a block chain that
contains the merkle root of a tree of unspent tx outputs,
and a different merkle root of all transactions in the
block.  This way when someone wants to do a transaction
they can produce a transaction record and a merkle branch
to prove that transaction record is part of the block
chain, and a txOut with another merkle branch to prove
it's among the unspent tx outputs. The problem with fixing
it so people can check to make sure the blocks are valid.
Either you don't get the ability for ordinary people to
check that a block is valid, or you get no real savings
in bandwidth because whether in the block or not the tx
still have to be propagated to every node interested in
checking blocks.
The fourth option is "side chains" where you have numerous
block chains operating at once and settling-mechanism
tx can move coins from one block chain to another.  The
problem here arises when you ask whether someone can
really deal with general transactions and customers if
they are downloading and verifying absolutely anything
other than ALL of the block chains.  Which brings you
right back to the original bandwidth/data size problem.
A fifth option is to have the blocks themselves distributed
rather like files during a bittorrent download, rather than
sent to and replicated at every node.  You get a fraction
of the blocks and "spot check" that fraction for validity.
And when you need a specific block, you can get it from
somebody eventually. But getting it from somebody
eventually does not satisfy realtime transaction
People have kicked it around for a while.  Lots of ideas.
Most of them won't freakin' work.  The few that will
leave the bitcoin network slower, more expensive, and LESS
private than just using Visa.  Anybody got other ideas?

@_date: 2016-03-30 15:10:19
@_author: Ray Dillinger 
@_subject: [Cryptography] Gates are cheap. Should cipher design change? 
How about "any multiple N of 64 bits, for N >= 7" ?
There's a "twist" construction Bill Cox invented in the context of doing
fast hashes on large documents without enabling block-based collision
I've been intending to nail down an algorithm for generating the twist
sequences and verify that it can be applied to any multiple >7 of the
subblock size, because it's also applicable to round-based ciphers
including Feistel ciphers.
It is based on a fixed-blocksize hash function.  It slices the plaintext
into subblocks the size of the hash's primitive block, then does chained
partial hashes in a "twist" sequence on the blocks using a single
iteration of the hash's round function on each block. The total
iterations of the hash's primitive round function is min(2N, N+M) where
N is the length of the block measured in subblocks, and M is the number
of rounds in the underlying hash.
So far I've proven that it achieves complete diffusion across blocks
whose size is a multiple of a "primitive" block size, and shown that
particular constructions of it are *at least* as strong in terms of
confusion as a corresponding round-based hash that's applied to it. I
haven't yet shown that it can be applied to *every* multiple of the
subblock size, but it can be applied to the ones I've tried that are
seven or greater.
What I haven't done yet is fully formalize an algorithm for generating
"twist" sequences and prove that twist sequences can be generated for
any number of subblocks >7. I have an algorithm for generating twist
sequences which is easy to do by hand (build a graph with certain
properties and find an Eulerian path) but not immediately obvious how to
code. I also have an easy-to-code test/definition for whether something
is a correct twist sequence.
I think the short block sizes are deliberate for two main reasons:
The first is purely technical; They want to build vector-processing
chips that do one block per clock cycle, and that means strict lower
limits on the pin count required per chip for I/O.  Bigger block sizes
require disproportionately larger chip dies in order to have the I/O
pins needed for vector processing, but still can't have more blocks "in
flight" through the processor at a time than the number of gates in the
maximum-length gate path required to do encryption/decryption.  The
number of nanoacres used goes up linearly with pin count, whereas the
number of nanoacres paid for goes up with the square of die size.  You
also wind up with less data throughput per board because you can only
fit sqrt-n times as many chips on the same size board, and get n-times
as much data throughput per chip.  So all told they'd  rather use
smaller, cheaper chips with a "more reasonable" ratio of size and gates
to I/O pins, for reasons of cost and bandwidth per board. And that means
smaller block sizes.
The second is security-related, and is the main reason why we want
bigger cipher blocks; There are a whole lot of attacks that allow
information to be extracted from ciphers, or ciphertext/plaintext to be
manipulated, based on knowing (or guessing) where the block boundaries
are. Smaller blocks give more opportunities to find such attacks.

@_date: 2016-03-30 18:04:42
@_author: Ray Dillinger 
@_subject: [Cryptography] [Crypto-practicum] Faster hashing with twisted 
Coming back to Bill Cox's "twisted hashes."
I got the basic algorithm for generating N-cycle graphs of degree 4.
(graphs in which every node is connected to four edges, and there are
no cycles of length greater than N).  An Euler path in such a graph
creates a twist sequence of the nodes.
So the "twisted hash" construction means taking a twist sequence and
visiting blocks of a plaintext in that order - meaning you visit each
block twice, but the sequence in which you visit blocks makes
block-based attacks on the hash effectively impossible because no blocks
that are adjacent, or near neighbors, in the first visit, are also
adjacent, or near neighbors, in the second visit.
I think I've figured out how to make a block cipher with an arbitrary
block size, using the Feistel cipher construction on a twist sequence in
the same way the twisted hash uses it on text blocks.
However, twist sequences have a very definite set of sizes. The 4-cycle
graph has 8 nodes, the 6-cycle graph has 26 nodes, the 8-cycle graph has
80 nodes, the 10-cycle graph has 136 nodes, etc.  If I start with
a fixed block size and my twist sequence is on a number of nodes that
doesn't match the number of blocks I'm working with, then mapping each
element of the twist sequence to a block isn't exactly how to do it.
I can get to an arbitrary block size by taking the N-cycle graph,
finding a set of nodes that are maximally separated (N-1 edges apart)
and not mapping those nodes to text blocks.  Thus I can produce, say, a
twist sequence on 20 nodes by figuring out which 6 nodes of the 6-cycle
graph to not map to blocks, and still guarantee no cycles shorter than 5.
Perhaps I shall monkey together an implementation, with test vectors and
so on.  First, does anyone see a problem with this?  Second, is anybody
who understands it well enough to say anything fairly confident of the
absence of problems?

@_date: 2016-03-31 12:24:20
@_author: Ray Dillinger 
@_subject: [Cryptography] On the 'regulation proof' aspect of Bitcoin 
want to have a look at the the BitTorrent protocol, or even rsync.
Cost of introducing parties with a privileged role w/r/t the
block chain:  Technically speaking, limited. Probably no
worse for security than the existing threat of collusion
among miners to execute the "selfish miner" attack.
Cost of selecting the parties to occupy those positions?
Politically speaking, impossible. Especially considering
how dysfunctional the community around Bitcoin is.
Rolling back a multi-notarized block after it's published
would certainly alert everyone to the fact that the notaries
as a group are cooperating to effect the rollback.
It is unclear whether the governments of the world are too
dysfunctional to cooperate in forcing all notaries to
perform such collusion at the same time.  Precedent so
far says they probably are.  Further, it is hard to imagine
a situation in which they have sufficient mutual motivation
to force a rollback; if they cooperate in forcing all
notaries to do anything, it probably won't be for anything
less important than something that would convince them
they needed to shut it down completely.  And as John
pointed out, if they were going to cooperate in an effort
to shut it down, it would not be difficult for them to
do so with the current mining scheme.
In fact as matters stand either of two different governments -
The US and China - whose markets and exports give the stuff
most of its licit economic value - could shut it down
It would be more accurate to say git is very similar to
rsync.  ;-)

@_date: 2016-05-01 13:40:58
@_author: Ray Dillinger 
@_subject: [Cryptography] USB 3.0 authentication: market power and DRM? 
That would be deeply impractical in terms of materials costs.
Increasing amperage means you have to use heavier cables.  To
the extent that copper ain't cheap and space inside walls for
wiring is often limited, a higher voltage/lower amperage is
always a more effective use of materials.
What you're talking about may be put off for USB 4.0 or something
when negotiating services includes negotiating which of a dozen
or so standard voltages is desired.
I'm not sure I like that idea though, because now it means every
last outlet in your home has electronic devices listening to it
and people have to trust that that's all they're doing.  Of course
this whole discussion is about establishing that trust, but it's
still true that it's easier to trust a copper wire screwed into
a terminal socket with a hardware-store screw than it is to trust
a chip with logic you can't see, made by somebody you don't know.

@_date: 2016-05-02 10:08:21
@_author: Ray Dillinger 
@_subject: [Cryptography] USB 3.0 authentication: market power and DRM? 
I'm seeing this whole thing as an attempt to prop
up CA's which are otherwise essentially looking at
a failed business model. Even if CA's did what
they're supposed to do there would be no way for
that business to function in the market of USB
CA's were supposed to verify identities, respond
to authentication attacks, handle revocations, etc.
The race to the bottom and their business "need"
to support stupid security decisions ("compatibility"
means, if someone is stupid once, therefore everybody
must be stupid forever!) meant, inevitably, that
they only verify that their payments clear.
Certification of USB equipment doesn't even
pretend to have key revocation capabilities or
any way of responding to authorization attacks.
By design it pretty much can't. Which means that
there is literally nothing CA's can contribute
to it. You can tell some piece of kit presents
a key which was valid, for somebody, once.  Woo.
Does that, in some way, help?

@_date: 2016-05-19 14:14:51
@_author: Ray Dillinger 
@_subject: [Cryptography] NSA Crypto Breakthrough Bamford [was: WhatsApp 
Well, after reading, I suppose you and Bamford are probably right about
what the breakthrough here probably is, but I strongly dispute NOBUS in
this case.
An attack based in known mathematical technique will have been deployed
by many state-level adversaries elsewhere, and besides, once the
database the precomputation generates exists, it can be stolen, bought,
shared through diplomatic channels with other nations, or otherwise
acquired through extortion, blackmail, or bribery by criminal actors.
The potential financial payoffs to a criminal organization of having
that database are immense.  It might even justify the expenditure needed
to do the computation themselves.
It doesn't even have to be the USA that gets compromised.  Even if one
supposes that the USA may have kept its database secret, it is
unreasonable to expect that several governments have done so - or that
they will continue to do so in the future.   China builds good
supercomputers and undertakes such giant projects, so they've probably
built this database without any NSA help.  And they might not guard
theirs so well against crooks, or might even willingly share it with
business interests.
In unrelated news, I read that SWIFT has been cracked yet again, and
that a database of some millions of LinkedIn IDs is available in the
black market this week.
It is characteristic of these agencies, worldwide, that they pursue all
available avenues of information compromise, never just one and never
just a few dozen.  I have no doubt that even as I type this, China is
putting backdoors in chips, and North Korea is building a database of
audio recordings of people typing passwords, and Iran is repurposing
Stuxnet to attack facilities in other nations, and England is examining
public video recordings to extract security codes whenever people
publicly enter them into smartphones, and Venezuelan government hackers
are examining the guts of Microsoft TLS implementations looking for
holes.  And on, and on, and on in every possible combination.
NOBUS is a fiction, and none of these agencies are ever satisfied with
any number of sources less than ALL OF THEM.
Meanwhile crooks are busy ripping off bitcoin from online poker games
that use good encryption but shuffle their decks using 32-bit random seeds.
Crooks are in many ways more reasonable people; if they get one break
that makes them money, they're usually happy with that until it gets
shut down.
I'm pretty sure that won't happen.  The US and China, indeed, are the
leaders of the two primary political coalitions contending  for world
domination, and as such the two most likely adversaries in espionage or
in any future large-scale warfare.

@_date: 2016-05-19 17:49:07
@_author: Ray Dillinger 
@_subject: [Cryptography] NSA Crypto Breakthrough Bamford [was: WhatsApp 
Oh crap.  I could be wrong here but I think maybe it's worse than
The Number Field Sieve algorithm for finding vectors of coefficients
for index calculus has a lot of sub-parts which don't depend on the
particular modulus being considered.  Those intermediate results can be
applied to precalculations on multiple moduli.  Some fraction of the
work you do when precomputing for modulus x can be reused when doing
precomputation for another modulus y.
It's a small fraction, but an opponent trying to build these
databases for a large number of different moduli (on the order of a few
thousand groups) could eventually realize a benefit from many such
small fractions that reduces the compute time required by ???  Uh,
back of the envelope says maybe two orders of magnitude? The costs
in data storage and the degree to which your calculations
get I/O bound, get steeper the more of a speedup you ask for.
I don't quite know if this is a practical technique; it depends on
whether the I/O requirements are light enough that the computation can
proceed at speed. It might be so I/O bound for significant advantage
that it's not worth it.  Also I'm still trying to figure out whether
the intermediate data storage requirements for a 2-order speedup are
merely very large (a few exabytes or less) or ludicrous (larger than a
few yottabytes).
ObNothingInParticular, we're getting close to needing more SI prefixes
to describe our storage media.

@_date: 2016-05-23 11:09:46
@_author: Ray Dillinger 
@_subject: [Cryptography] Entropy Needed for SSH Keys? 
This is very close to true.  It is certainly true if one trusts the
algorithm and coding of one's DRBG and intends to produce less than
a few trillion keys.
But, honestly, I sincerely question the idea that you need random
numbers "early" in the boot process.  It's like thinking that you
have to be in the middle of a long-distance call before you can
hook up your phone.  We were building operating systems that could
finish booting up without network connections a long time ago.
Thinking that we've lost that technology is silly. A non-networked
operating system on a machine with sensors can run a program
capable of gathering entropy, gather entropy, and *then* start
using the network.
So, if you're looking at a situation where anything is asking for
key generation before bootup is even complete, you're looking at
a design failure.  It is bad design to do something the hard way
when there is an easy way that is more reliable.

@_date: 2016-05-23 11:34:00
@_author: Ray Dillinger 
@_subject: [Cryptography] Entropy Needed for SSH Keys? 
"Anyone who attempts to generate random numbers by deterministic
means is, of course, living in a state of sin." -- John Von Neumann
Even if the crypto is perfect, you still want an extra bit
of state every time you double the amount of output you're
going to produce.  So, if making a few trillion additional
keys, you'd want ~50 or so extra bits of state.
Also, if making bigger individual reads of /dev/urandom.  If
you've got anything that's reading 2Kbytes at a time of output,
then you want an extra 2Kbytes of RNG state.
Try redirecting /sys/log/* to /dev/random, like TAILS does,
if you're really concerned about topping up state.  But for
TAILS that's more about not writing sys/log/* than it is
about keeping the RNG pool topped up.
"The real problem is not whether machines think but whether men do."
 -- B.F. Skinner

@_date: 2016-05-23 11:52:22
@_author: Ray Dillinger 
@_subject: [Cryptography] FBI Gripes About Crypto on 
Um.  One billion people is 14% of the world.  Meaning, one
person out of every seven.
Assuming for the moment that there are more than seven
criminals or terrorists, it seems likely that someone will
in fact use it to hide crime or terrorism-related activities;
otherwise criminals and terrorists would be underrepresented
in the sample.
On the other hand, there are vastly more people who will use
it to secure information that criminals would otherwise steal
and keep secure information that terrorists would otherwise
use to plan attacks.  Because there are vastly more people
engaged in those activities than there are criminals and

@_date: 2016-05-27 13:17:24
@_author: Ray Dillinger 
@_subject: [Cryptography] Entropy Needed for SSH Keys? 
IIRC, one nanosecond was once defined to me as the approximate
amount of time it takes for light to travel fifteen centimeters.
So, something just across the room from you is probably fifteen
to thirty light-nanoseconds away, depending on the size of the
room.  Does this matter much, in terms of creating useful
interference patterns?

@_date: 2016-05-31 13:44:17
@_author: Ray Dillinger 
@_subject: [Cryptography] Blue Coat has been issued a MITM encryption 
============================== START ==============================
  (attributions unclear so I left them out).
Symantec keeping the Blue Coat private keys is an interesting
twist for a certificate at that level.  In theory that should
prevent them from issuing a cert for a different domain.  In
practice it depends on what kind of cert they were issued.
If the crypto is weak enough they could crack the private
key themselves and I don't know how much software still accepts
that weak-ass crypto. More likely, given their business
model, they'll have bribed somebody who got them the key
"off the record," so they can now issue keys that software
will recognize as being from Symantec, and Symantec, who
didn't "officially" provide the key, isn't liable for what
they do with it. Nice arrangement.
Alert, I'm making standard "must be an excessively
suspicious paranoid to do security" assumptions here; it may
not be this bad. But actually, I think it's worse than that.
I think this is probably the system operating as designed.
The CA cert system used by x.509 was proposed in response
to the need for an "introduction" system for customers and
merchants with no prior relationship.  But what emerged from
committee is self-evidently not designed to do exactly that
If it were designed to do that job then CAs would be used
when doing introductions (or establishing new keys on a
new device) and key management would be done by both the
parties to the contact after that, with certificate
pinning on both sides.
I don't doubt that this protocol was debated by people
the majority of whom acted in good faith. But I have long
assumed that the debate was unduly influenced by, and the
swing votes cast by, actors who didn't have that job as
their primary concern.  Among these actors were CAs who
intended to profit from increased dependency of everyone
on their services, but I don't think they were the only
I have, for a long time, considered the CA system to be,
first and foremost, a tool for concentrating the ability
to perform MITM attacks to a set of known and controllable
CA's, and thereby make certain that MITM attacks are
available to government actors while protecting ordinary
customers from all but the most influential crooks.
Unfortunately the set of CA's rapidly became uncontrolled,
with the effect that many CA's are now frankly run by crooks
and the ability to perform MITM attacks is available on
the black market for crooks with no political influence
and only a little money.
We've been patching X.509 for its many holes and failures
for a long time, most recently with (FINALLY) certificate
pinning - the beginnings of key management by the parties
to the contact.  Still conspicuously missing are persistent
customer keys/self-certs that the businesses can "pin"
and associate with particular accounts on their side.
Maybe if we keep patching we'll eventually get to something
that does the job that X.509 was supposed to do.  But we
could have started with something a hell of a lot closer
to fulfilling the requirements.

@_date: 2016-11-01 16:08:57
@_author: Ray Dillinger 
@_subject: [Cryptography] How to prove Wikileaks' emails aren't altered 
If they don't believe you you're probably not charging them
enough money.
I mean, yes, having the notebook of source material for the
things you say helps, and sometimes they ask to see sources.
But honestly?  People usually decide to believe what you're
telling them for no reason other than knowing that you get
paid a lot of money to tell people about these things.
It seems dirty, like exploiting one bug in order to fix another,
but it's a bug in people's brain software that you can exploit
to get them to believe things that are actually true.
Or which certain candidates can exploit to get voters to believe
things that are actually false....
Grrf.  Without going into partisanship, I encourage everyone who
is eligible to vote in the US to please do so.

@_date: 2016-11-02 10:42:10
@_author: Ray Dillinger 
@_subject: [Cryptography] Using AI to identify state secrets. 
So some researchers picked over recent dumps of unclassified and secret
information with a neural network, training it to identify features
more likely to occur in documents marked secret.  This turns out to be
about 90% predictable, with a few caveats about some categories being
far more predictable than others.
Interesting article.
Obviously, this could also be used to pick over unclassified information
like newspaper articles and identify things ("false positives") that
some state actor *ought* to have made secret or might have *preferred*
to have made secret but for some reason didn't or couldn't.  IMO that
could turn it into a really productive intelligence asset.

@_date: 2016-11-08 11:53:43
@_author: Ray Dillinger 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
I disagree vehemently with this.  There are numerous actors
who could easily believe that they'd have a freer hand to
dominate and exploit others in their area, should one
candidate or another take office in the US.  There are
numerous actors who could easily believe that they'd have
better chances of negotiating more favorable treaties etc
if one candidate or another should take office.  There are
numerous actors who have strong preferences for or against
a candidate who questions the legitimacy or desirability of
NATO, NAFTA, the TransPacific Partnership, military aid to
Israel, etc.  There are numerous actors who could easily
believe that should the US military commander-in-chief be
one candidate or the other, that there would be a greater
or lesser chance of them achieving (or being left alone
while achieving) their military goals.  Virtually everybody
in the world is evaluating the American election in terms
of its potential for danger of financial or military
crisis in their part of the world.
The US elections face an array of persistent, motivated
sophisticated, and highly resourced attackers - nation
states, criminal organizations, downballot candidates for
other offices, and the list goes on and on.
And what have we built for security?  The voting machines
are still suspect but have been mostly audited by now. But
after the elections they process those ballot counts on
f***  windows boxes.

@_date: 2016-11-09 10:54:57
@_author: Ray Dillinger 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
There is a fairly simple protocol to solve a decent chunk of this
problem at a reasonable scale, using two parallel Hash Chains.  A
national election with 300M voters would require Merkle Trees instead,
but it's easily do-able.
If Trent wants to jigger the vote, Trent has to do it in real-time
rather than after the fact, by inserting additional bogus votes between
pairs of legitimate votes.  But in order for Trent to do that, a
conspiracy right to the highest levels must exist, issuing 'extra'
ballots with bogus IDs and faking their distribution to real voters -
and then the people inserting votes in realtime in the field must guess
correctly which voters are not going to vote in the time remaining
before the polls close, because any voters whose ballots do get replaced
would notice that verification fails because their real ballots are
missing from the Merkle trees.
One problem is that voters could prove to third parties afterward how
they voted, by proving the preimages match values they know from their
own ballot, opening up the possibility of vote selling or vote coercion.

@_date: 2016-11-14 14:45:43
@_author: Ray Dillinger 
@_subject: [Cryptography] On the deployment of client-side certs 
In response to recent discussion on the list:
Right now we have infrastructure in place to authenticate the host side
of most connections.  Predictably we still have massive fraud
perpetrated through credential theft followed by use of stolen
credentials as an unauthenticated client.
There is a technical standard for client-side certificates, which was
part of the original SSL specification.  However, because CAs demanded
that people pay for every certs, it was utterly ignored and never
deployed; Homer Husband and Harriet Housewife were not going to spend a
four-digit number of dollars (or equivalent) on a certificate, and no
sane vendor or banker was going to require them to.  Hence the
half-fast* job of one sided authentication that has become the norm.
This sounds like a proper use case for self-signed certs (on the
consumer side) with pinning (on the host side) to prevent fraudsters
from impersonating consumers.
That's within practically everybody's capability, (in the sense that
code to create self-signed certs is all over the place and most SSL
implementations allow certs to be used on both sides) but nobody's about
to do it (consumers) or require it to be done (bankers, merchants, etc)
or facilitate doing it (servers, browsers, mail clients, etc).
How can we change that?  What can we do to make it easier to do, provide
a transition path toward, and get pinning, certificate checking, and
revocation list checking integrated on hosts and cert generation/use
integrated into clients?  And make it simple and easy for consumers to
share their certs from multiple devices, reliably remove them from
devices before loan or sale, and revoke-and-replace whenever one of
their devices with a copy of their cert gets stolen?
* "half-fast" is the earlier form, as in tying something down with
   loose or wrong knots so it wasn't actually fastened (made fast);
   But it sounded like "half assed", and as 'fast' to mean secured
   has fallen out of use we rarely see the earlier form anymore.
   It seemed appropriate in this case to recall that 'fast' used to
   mean 'secure.'

@_date: 2016-11-15 13:37:43
@_author: Ray Dillinger 
@_subject: [Cryptography] highlights of crypto history 
And lest we forget, it had significant implications for Mexico as

@_date: 2016-11-15 14:18:49
@_author: Ray Dillinger 
@_subject: [Cryptography] On the deployment of client-side certs 
This is actually a quite good idea.  The mental model of a keyed
lock, with a physical key, works reasonably well for at least some
plausible implementations of client-side authentication.
If you want your device to do secure transactions with the bank, you
have to 'unlock' the bank secure channel by sticking the bank key into
it.  If you want your device to do secure transactions with Amazon, you
have to 'unlock' the Amazon secure channel by sticking the Amazon key
into it. And so on.  The USB port is also the keyhole.
I think most people could compass the idea that you have a single device
(your e-keychain) which is all of these keys but only one of them at a
time, and that you have to turn a knob on it (or whatever) to make it
produce the "right" key. I think they could also handle 2FA that way;
the key opens the secure channel, then the bank, or amazon,
or whatever, asks for your password to make sure it's you using the key.
I think that amount of key management, with a physical device, is within
reach of most people.  Under the hood, the device contains a
set of certificates and can do cryptographic operations to prove
possession of each of them, negotiate session keys, etc.  But we want to
make sure it's "each" rather than "any."  When Alice has her 'bank key'
in the keyhole, we don't want Bob to be able to spoof a form and get her
to unlock a secure channel between Bob-impersonating-Alice and Amazon.

@_date: 2016-11-17 08:37:00
@_author: Ray Dillinger 
@_subject: [Cryptography] On the deployment of client-side certs 
It remains to be seen whether they will bite the bullet and fully
divorce the hardware for security from the hardware that actually runs
their OS.
I REALLY doubt that they will give the security hardware its own I/O on
the first iteration though; they really hate to put visible external
bits and bobs on their hardware.  And as long as they don't, it'll be
possible for something running on the OS to pretend to be the
security-hardware interface, making all their efforts at separation
They're more likely not to fix that until after the first few exploits
come in, and maybe not even then.
At an absolute minimum, there needs to be an externally-visible LED or
something that isn't even connected to the hardware that runs the OS.
The user should be assured that she is interfacing with the security
hardware if and ONLY if the light is on.

@_date: 2016-11-19 15:42:06
@_author: Ray Dillinger 
@_subject: [Cryptography] cattle branding and rustlers 
I've lost the message I'm responding to, so this will appear outside
that thread.  But there were several protections for cattle (and other)
First of all, if you were in a state as opposed to a territory you had
to pay to register your brand at the county seat, and they would flatly
refuse it if it looked anything like, or had any lines that looked like,
another registered brand in the same county - so they were acting, sort
of, as a CA.  The benefit was that you knew nobody in the county had a
brand that yours could be modified into, and vice versa. If you wanted
to be sure, or just had a big ranch, you could register the same brand
in several nearby counties.  Ranchers could inspect the brand registries
of surrounding counties at the county seat, so that they could pick
brands that were mutually exclusive in lines from anybody nearby, or
figure out whom to return strayed animals to.
In the territories they didn't have county seats/CAs so things were a
bit more uncontrolled.
The US army had a brand for its animals (cattle, and oddly enough,
camels) in the territories that as far as I know no one ever
successfully modified to resemble a different brand.  In fact it's one I
can type.  It said "US ARMY".
The letters were smaller than the letter parts of most brands but the
brand overall was of similar size.  Greater complexity probably meant
that animals would be more prone to developing infections or otherwise
suffering more during and after branding, but also would have made it
effectively impossible to modify undetected.
I remember reading about it and seeing it in old photos, but I get so
many fake hits when I do an image search that I can't find an example
There was a similar but not identical thing that got used by the postal
service on pony express horses.  It was simpler: "US."  You can find
plenty of examples online of that one, because pony express is an iconic
image but the fact that the army used to raise the beef for its own mess
halls is considered an insignificant detail and almost forgotten.
As an additional precaution the US postal service branded horses on the
left front flank (shoulder); everybody else who branded horses was
branding them on one of the rear flanks (hip). So somebody could for
example put a "G" on there so the brand said "GUS" or something, and
that might have worked if the pony express horses were branded on the
rear flank.  But nobody would believe that rancher Gustav both had that
brand AND was branding on the left front flank - even if he had never
touched a postal service horse, having the same peculiar brand location
and the "US" substring would cause everyone to assume he had, or that he
intended to, and that would be Very Bad for Gustav.

@_date: 2016-11-21 13:40:20
@_author: Ray Dillinger 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
Don't do that.  It is folly to rely on any *single* source as a
seeding mechanism.
People talk about "entropy" as though it were one thing (same against
all adversaries) but it's not.  When you need 128 bits of "entropy" you
aren't measuring (or at least shouldn't be) measuring some abstraction;
what you want is that for every possible set of attackers with *some*
inside knowledge, to make sure you're getting at least 128 bits that
that particular set of attackers wouldn't know or be able to determine.
If every last possible source of entropy on your box is known by or can
be determined by *someone*, but for each possible *someone* there's at
least one that they can't, then you can have some security as long as
your adversaries don't share information, but your security depends on
128 bits from every source in the set that is unknowable to at least one
of your adversaries.
RDRAND is provided by a single entity and can't be audited.  Therefore
that entity (in this case Intel and anybody they have possibly shared
information with) should be assumed to know exactly what bits RDRAND
provided.  Even if RDRAND is secure against everybody else, you still
need 128 bits that can't be determined by anybody in that set before you
call it 128 bits of "entropy".
For all we know, RDRAND is a deterministic generator keyed by CPU ID and
reseeded by the time each millisecond.  I do not claim that this is the
case; I claim only that it COULD be and nobody could tell the difference
from the outside.  That would be a worst-case scenario deliberately
designed as a backdoor.  If that were the case, then someone from the
"In group" who knows about the backdoor, knows what time your seed was
generated, and knows your CPU ID or MAC address (MAC can usually be
correlated with what CPU that device was installed on the same
motherboard with) can eavesdrop on your output, test a few hundred
thousand possibilities which takes less than a second, and determine
your key.
Doesn't matter.  ChaCha is deterministic, and therefore adds no real
randomness.  If someone knows the seed material they can predict your
ChaCha outputs forever, or if someone can see (or determine) the ChaCha
outputs they can tell whether it was seeded with a particular sequence.
"Volunteer work" means that you should be prepared for the possibility
that some of your "volunteers" are paid employees of your adversaries.
If I were part of the "in group" that can determine RDRAND outputs, it
would definitely be worth my money to pay someone to "volunteer" and
make sure that anything I want to crack uses RDRAND exclusively.

@_date: 2016-11-22 11:15:15
@_author: Ray Dillinger 
@_subject: [Cryptography] combining lots of lousy RNGs ... or not 
First: NO single nest is worthy of absolute trust.  Absolute trust does
not exist.
You will note that birds do not, in general, all trust the *same*
basket.  A particular family of owls might trust that nest that they
built under the eaves of a barn where the squirrels can't get at it,
while a different family of owls, even in the same species, might trust
that nest they built in the hollow tree where the humans won't notice
it.  Even birds have different threat models, and besides it would be
really really stupid for them to trust just *one* bird to build a giant
nest for the entire species.  It would even be stupid for them all to
use the same method of selecting nesting sites, because a new predator
might come along making one of the choices a deadly mistake, and they
can't tell in advance which choice, and they'd really like at least a
few families of owls to survive.
Trent should not get to put himself in the position of the one bird that
builds the giant nest for the whole species, because Alice or Bob or
both may not trust Trent.  Did we learn nothing from the Debian RNG fiasco?
I really and truly do want non-squish.  But we're talking about
virtualized machines and pseudorandom bit generators.  It is very hard
to tell which sources are non-squish, so I prefer to use all of the ones
that MIGHT be, including a bitcoin-mining USB stick that stays powered,
retains its internal state, and keeps hashing across reboots, and
obscure them with multiple squishing algorithms to prevent attacks that
see through some of them.
If every opponent finds at least one squish to be opaque, even if it's
not the same one for every opponent and even if I'm wrong about which
ones, then my pile of squish is opaque against every opponent. I am
*absolutely* confident of no implementation of any algorithm, so with
confidence factors thrown in, my equation is
non-squish(99%) XOR non-squish(99%) = non-squish(99.99)
But in the absence of ANY 100% trusted source, combining 99% trusted
RNGs makes as much sense as trusting 99% trusted ciphers, ie a hell of a
lot of sense.
Also?  The days when one cipher might accidentally undo the security of
another are gone.

@_date: 2016-11-22 15:50:09
@_author: Ray Dillinger 
@_subject: [Cryptography] combining lots of lousy RNGs ... or not 
In practical terms, an old Bitcoin mining stick, which continually
hashes its state plus a nonce, is a nice thing to feed to your RNG.  I
picked one up after the Debian Uninitialized RNG fiasco.  It uses a
fairly trustworthy hash algorithm, its output absolutely can be audited,
it's too high-frequency for someone more than ten feet and a stucco wall
away to easily track by electronic means, it produces output at an
easily adjustable rate, and it can remain powered avoiding loss of state
across reboots.
So I'm pretty confident that it helps, and I 'tail -r /dev/antminer >
Forget mining Bitcoin with it; those USB stick miners are now utterly
useless for that, and so can be had cheap.  But they're still "pretty
good" sources of bits.  There is a good chance that it's EM-loud and
broadcasts its state or nonce or both, (whether because backdoored or
just not manufactured for security) but it still makes me happy,
especially because the /dev/random mixer keeps stirring the pot.  Even
if it's EM-loud, its contribution to the mixer makes me about 10
decimals more confident that the mixer's output is unpredictable to all
If we use 128-bit keys (ie, ~37 decimals) and we want the same security
from our RNG that we get from our ciphers, it follows that I want bit
sources which *I* believe with ~37 decimal confidence to be
unpredictable in combination to all adversaries.  The Bitcoin stick
alone doesn't get there, but it sure helps.
It eliminates from consideration many attackers that could easily and
remotely take advantage of a  backdoored RDRAND or a
failure-to-properly-initialize mistake in implementation.

@_date: 2016-11-22 16:47:04
@_author: Ray Dillinger 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
It's okay with me.  I want 128 bits of confidence and RDRAND gives me
about 5 (sorry, but for more than 5 you have to be publicly auditable).
5 bits is a net gain, so I'm glad it's there.  I use much smaller
samples of it (tho still bigger than the OS's RNG state that I'm feeding
it into) because given only 5 bits of confidence much larger samples of
output are useless.
It doesn't matter to me whether I get those 5 bits of confidence in an
800MByte sample or a 16-bit sample, they are still only 5 bits of
confidence.  That's because this isn't about the unpredictability of
some unknown physical process.  It's about the good faith of Intel,
Intel's freedom to act in good faith, and the quality of the
implementation.  Either it's golden, or it's broken, and absent an audit
I'm giving it about 31-to-1 odds of being good.
I have 20-bit (or million-to-one) confidence that acting in good faith
Intel may have provided exactly what it claims to, and with a proper
audit I could trust it 20 bits.  After a detailed check into the quality
of the implementation of a randomly-drawn sample of CPUs I could trust
it about 40 bits (trillion-to-one). But I have only 5-bit confidence
that Intel is free to act in good faith, so obviously RDRAND makes only
a 5-bit contribution to my confidence no matter how much of its output
gets used.  Make sure I read some of it before generating a key?  Yes,
because 5 bits are better than none.  Make sure I read something else
too?  Yes, because 5-bit keys are a stupid concept.
There is no ratio of bits to "entropy" from any unauditable source, and
any mixer that claims that there is, is wrong.  For something that can't
be audited, there are only two questions. How much do we trust the
implementing entity and its team? and: What fraction of the adversaries
we care about can't take advantage of any failure of the implementing
entity and its team?

@_date: 2016-11-23 10:24:49
@_author: Ray Dillinger 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
A proper audit is one that's sufficient for anybody with a copy of the
audit to notice if there's a mistake in the claimed implementation.
Therefore, it would include a discussion of the exact design specs, the
design process, and why the design is expected to meet those specs.
Then it would show both images of the relevant parts of the chip die and
the source code that resulted in that layout, so that people can check
those things against the claimed implementation.  And it would be
publicly available so security researchers and random grad students
anywhere in the world can inspect it, publish papers about it, freely
quote it, etc. without need of an NDA and without worrying about getting
If this document existed we'd know about it because researchers and grad
students in Shanghainese and Russian universities, and everybody else
whose governments or sponsors are suspicious of American companies and
would budget a whole lot of university research, would be publishing a
firehose stream of papers about it.
Lite Verification means making the pre-whitening random-process output
available from the chip and letting people verify that that the RDRAND
output does indeed correspond to those bits and the whitening process
claimed in the audit. Likewise, we'd know about it, and there'd be
another firehose stream of papers coming out.
Full Verification involves decapping randomly selected chips that have
been sold to the general public, and inspecting them under an electron
microscope to make sure the implementation claimed in the audit is
what's actually there.  And to make sure that it's actually hooked up so
people can tell that it is the part of the die which is used to handle
that particular instruction. Full verification could *in principle* be
repeated by as many people, in as many different countries, as there are
chips sold, with notice to or participation by the manufacturer neither
required nor given nor expected.
Publishing the audit would be simple and easy.  We assume that document
already exists, it just hasn't been published.  Lite verification would
require additional chip design, possibly additional output pins, and
would raise the cost of the chip, but designing to make it possible
would have been well within Intel's capabilities. Full Verification
would be an absolute bugger for anybody to do, very expensive, and IMO
is unlikely to be done by anyone save nation-states who will never
publish their findings. Nor, indeed, even admit they've done it.

@_date: 2016-11-23 11:46:47
@_author: Ray Dillinger 
@_subject: [Cryptography] combining lots of lousy RNGs ... or not 
It's a completely valid argument.  TT's not speaking math, he's speaking
engineering.  Our public policy is unsound.  Compliance of hardware with
public policy is unverifiable.  Our engineering policy has to deal with
that unsoundness and unverifiability as ground truth.
There exists no single source that can be fully trusted.  There exists
no manufacturer whom we can assume will ever be allowed to create such a
source.  If a manufacturer makes an honest attempt to create such a
source we cannot assume that manufacturer has not failed in design or
implementation.  If a manufacturer has made an honest attempt to create
such a source, and has succeeded in design and would have succeeded in
implementation, we cannot assume that manufacturer has not been sabotaged.
The best we can do is make sure that all these untrustworthy sources are
transformed into output unrecognizable and unpredictable to anyone who
can recognize or predict the sources.  And that's what the mixer does.
If I'm to put all my eggs in one basket, that basket is a mixer fed from
as many diverse sources as I can find.
We can have no expectation that any of them are ever going to stop, even
if a day comes when "public policy" says that they must and all have
claimed to.  To assume we can ever have compliance with public policy
from anyone who could possibly get away with cheating, is bad
engineering and bad security policy.
We can use their bits, in the expectation that at least some of them
haven't cracked or colluded with the others.  We can use combinations,
hashes, and encryptions of their bits, in the expectation that combining
them with each other in hard-to-reverse and uncorrelated ways will
render them unrecognizable even to people who could predict or recognize
any one of those sources. And we can go right on gathering all the bits
we can get from sources whose unpredictability we're more certain of,
both because our public policy is deplorable and because it remains bad
security policy to assume compliance with public policy.
I have completely given up on the ideas of "random" and "entropy."  In
practical engineering terms, the crucial properties are
"unpredictability" and "unrecognizability".
The mixer is what ensures unpredictability and unrecognizability, when
no single source can be trusted to be unpredictable or unrecognizable to
the entities that provided it.
Air turbulence inside an old-fashioned hard disk drive was pretty
unpredictable. Once people started relying on it for unpredictable bits
there was a motive to sabotage the firmware, intercept calls that were
being used to detect turbulence, and provide predictable answers.  We
don't know if anybody actually did, but the source went from trusted to
mostly-trusted because someone could have.  These days people are using
SSDs and they provide no unpredictable bits at all.
RDRAND seems unpredictable.  It's claimed to be unpredictable.  It is
almost certainly unpredictable to non-state actors or I'd have seen the
cracks on the dark side of the net by now.  But there's a motive to
sabotage it, and we can't be certain nobody did, so it's mostly-trusted,
not trusted.  It's still useful, however, because even if sabotaged, it
can be rendered unrecognizable to the saboteurs as long as we encrypt or
hash it along with bits they can't predict or recognize.
Using it otherwise would be bad engineering, for the same reason that
protocols requiring a fully honest and fully error-free Trent have been
revealed to be bad engineering.
We can't assume Trent is honest and error-free.  We can't even assume
Trent will be ALLOWED to be honest. Even if Trent is both honest and
error-free Trent can still be deliberately sabotaged and adversaries
have demonstrated willingness to do exactly that.  Especially if Trent
manufactures hardware, because hardware is even harder to audit than
software. Cheating, failure, or sabotage are therefore harder to detect
and correspondingly more likely to happen.

@_date: 2016-11-23 18:55:01
@_author: Ray Dillinger 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
Exactly.  And that's why "the claimed implementation".  Verifying that
the claimed and actual implementations match is a whole different level
of difficulty, but fortunately unnecessary if we run the bits through a
good mixer with a bunch of other bits.
Fully trusted sources of completely unpredictable bits don't exist.  I
wouldn't trust one without mixing even if I built it myself, because I
can make mistakes and besides there are attacks nobody knows about yet.

@_date: 2016-11-26 18:34:02
@_author: Ray Dillinger 
@_subject: [Cryptography] Is Ron right on randomness 
The best use for an HWRNG, in my opinion, is with a driver that writes
its output at some "reasonable" rate directly to /dev/random.  OpenSSL
would be supported automatically because it normally reads that device.
But that's just my opinion.  Others may have a different opinion because
that would require the /dev/random mixer to be run on all those bits,
which on some resource-constrained platforms might raise objections.
If you're serious about OpenSSL having direct hardware-generator support
to address those objections, I suggest you implement this yourself and
submit it as a patch.
Your self-interest in it is completely obvious, of course, but doesn't
represent a conflict of interest as such; there is no valid reason for
OpenSSL to NOT support hardware randomness sources, and by virtue of
being there first, any free code you submit could well define a common
interface to "standard OpenSSL-compatible" hardware random sources.
A standard interface for hardware bit generators is badly needed;
offerings from different vendors being incompatible is an unnecessary
source of difficulties and requiring custom code to be installed for
each is an unnecessary source of trust issues.
To address legal trust issues, it will at the very least be necessary
for you to make it clear that anyone is welcome to build compatible
devices and may do so without incurring liability for any kind of
intellectual property claim.
To address crypto trust issues, don't freeze out other sources. Write a
patch that would use the HWRNG as one of multiple bit sources rather
than as the exclusive source.

@_date: 2016-11-27 11:03:42
@_author: Ray Dillinger 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
The OS has another job and it's very important:  Even if it were getting
its bits from RDRAND exclusively, it runs a whitener on the numbers
before it gives them back out, to make it harder for everybody - even
the vendor who produced that chip which we can't see into - to predict
or recognize the output.
I call that reducing attack surface, not adding it.  To the extent that
it reduces performance, I'm entirely happy with paying a few CPU cycles
to reduce the attack surface.
Remember, as far as I'm concerned, RDRAND output is only worth five bits
of vendor trust, no matter many bytes of RDRAND output there is.  I
require at minimum 256 bits of trust before I start producing keys.

@_date: 2016-11-27 11:09:06
@_author: Ray Dillinger 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
Did Intel make it possible for us to do that?
Did Intel make it possible for us to tell whether the power droop attack
is doing anything?
And what do you suppose actual history would show if RDRAND were as open
to review and audit as the source code for /dev/random?  You can't claim
that something we can't see and audit is superior to something we can
and have.  That's comparing a known with an unknown, and the result of
that comparison is NaB.

@_date: 2016-11-29 19:15:53
@_author: Ray Dillinger 
@_subject: [Cryptography] randomness for libraries, e.g. OpenSSL 
Well.  My advice was to trust it about five bits, on the assumption that
Intel was both allowed and motivated to act in good faith, and didn't
get sabotaged when it tried to do so.
But it's still valuable because the RATE at which it produces
apparently-random bits is a worthy contribution.  If you trust other
sources enough to make even *one* good key (ie, one with greater
security than anything you're going to produce based on the derived
stream) you can encrypt RDRAND output at high speed for days.
Even if Intel didn't act in good faith or just plain got sabotaged,
inside knowledge of exactly how will do nothing for an attacker trying
to predict or recognize an encrypted stream.

@_date: 2016-11-29 19:27:20
@_author: Ray Dillinger 
@_subject: [Cryptography] Is Ron right on randomness 
Check the ioctl() to ensure that a read of /dev/random wouldn't block,
and then read from /dev/urandom.
The ioctl() makes sure that the pool is properly initialized.  The
urandom read can be performed by as many processes as need it, as fast
as it's needed, without causing a block.
Remember that once the pool has its ~4k "real" bits of randomness, you
don't need to worry, at all, about how much or how often you read from
If the ioctl() reveals that /dev/random will in fact block, then
either read /dev/random (ie, deliberately block waiting for the pool to
be initialized) for your bits, or bail with an error message.

@_date: 2016-11-29 20:29:58
@_author: Ray Dillinger 
@_subject: [Cryptography] Gaslighting ~= power droop == side channel attack 
Sure.  Attach between the meter and the breaker switches a high-current
bridge rectifier, and on the other side of the bridge rectifier put the
biggest supercapacitor you can get.
It'll absorb transient spikes and droops leaving only the very tiniest
fluctuations for the snoopy device to look at.
Upside: bonus, some of your appliances may last longer.  Smoothing out
little spikes and droops helps avoid some kinds of damage.
Downside: it'll make enormous transient current available, so having
your breakers between the capacitor and the house is very important, and
an electrician may even recommend specialty high-speed breakers to avoid
Downside:  Attached as I suggested between the meter and the breakers,
it will remain HOT, so don't go grabbing it unless you like the taste of
lightning.  A licensed electrician could probably suggest a better way
to attach it, so that it can drain its charge when you shut down the
power but still have your house protected by your breaker box.
Disclaimer:  I am not a licensed electrician.  If you're doing anything
like this, you should be doing it in collaboration with a licensed

@_date: 2016-11-30 10:23:06
@_author: Ray Dillinger 
@_subject: [Cryptography] OpenSSL and random 
I know it wasn't me you were replying to, but thank you.  I did not know
that.  I have (and up to now have been recommending) code which does the
same thing by checking to see if /dev/random is blocking then reading
For those who don't chase email links ever, or who do but didn't
recognize the above host so refrained, the short version of the story is
that getrandom() blocks unless sufficient initial entropy has been
collected, but is nonblocking forever after.  Which is, IMO, the right
thing.  I would happily alias urandom to this, but as TT pointed out in
the checkin, changing the behavior of urandom could break userspace
As written, it regards 128 bits as sufficient initial entropy.  That's
striking a "reasonable" balance between being usable very early and
having enough bits to generate secure keys.  I can think of about three
reasons why I'd prefer to see it at 256 or 384 bits instead (256 at
minimum if we are moving into a post-Quantum Cryptanalysis world), but
the 128 bits is sufficient at this time for generating that very first
key, and after that (in the nonblocking state) the pool does continue to
collect bits from a variety of sources.  Subsequent keys will benefit
from the unpredictability of those additional bits.

@_date: 2016-11-30 10:57:37
@_author: Ray Dillinger 
@_subject: [Cryptography] randomness for libraries, e.g. OpenSSL 
5 bits is my worst-case estimate that the compromise exists but requires
at least a work factor of 32 to exploit.  IOW, that an attacker with
inside knowledge still has to try at least 32 possibilities based on
some visible output before your stream is pwned. One could also say, I
am confident of the community's ability to spot any correlation in a
single source that would require a work factor of less than 32, and
confident that nobody designing such a compromise would choose one with
a smaller work factor on the grounds that it would be immediately noticed.
Your own estimate, of a couple hundred billion starting points for the
sequence, was much more generous.

@_date: 2016-11-30 16:30:58
@_author: Ray Dillinger 
@_subject: [Cryptography] OpenSSL and random 
John.  You're telling people to do things they'd really like to be able
to do.  Things that they would do, immediately and without you even
mentioning it to them, if they could.  Things that some of us have
bought, or built, special hardware to do because we can't find any other
way.  And you're telling them not to rely on any special hardware to do
it, even though the extant standard hardware does not provide a
trustworthy way to do it.
This may explain why your oft-repeated demand is not getting much
traction here.
If this blocks, it will block so early in boot that the machine does not
reach runlevel. On any machine that has already booted up, it will never
block and running it will never cause anything else to block.
If it does block before runlevel is reached, that is a plain
configuration error that the distro people designing the boot scripts
will need to fix. It will mean, for the very same reason you just cited,
that nobody will use their broken distro.  This is, in many ways, an
ideal solution.

@_date: 2016-10-01 22:55:25
@_author: Ray Dillinger 
@_subject: [Cryptography] Insecure email might be an even bigger problem 
There's a difference in kind though.  A domestic crook who wants
to steal high office, at least wants there to be a functioning
nation that gives the high office some power.
A foreign actor has no such constraint on its set of desired
Of course considering the US election news this year, maybe
there really isn't as much of a difference as I'd have thought.

@_date: 2016-10-03 16:32:39
@_author: Ray Dillinger 
@_subject: [Cryptography] Insecure email might be an even bigger problem 
I must confess that I've given up on mobile phone security
completely.  I no longer carry a smart phone.  I will probably
get a stupid one (voice only, no sensors other than microphone)
one day soon; until then I am available only via land line.
A stupid phone will go along with my dedicated GPS, which
has no outgoing communications ability whatsoever, does not
update its map (except when I change its little memory card)
and does not use any network service.
Sometimes opting out is a reasonable choice.

@_date: 2016-10-05 12:07:13
@_author: Ray Dillinger 
@_subject: [Cryptography] French credit card has time-varying PIN 
I do not believe this is necessarily true.  There are any number
of online outlets from which a 'bot can place an order instantly.
In some future iteration of the security arms race, we're going
to see scripted attacks which do exactly that.  And then automated
stolen-card marketplaces on the other side of the net where card
information is brokered by bots in seconds rather than by humans
in hours, not unlike the automatic systems for trading stocks.
Just because it's harder and riskier than what they're doing
today, doesn't mean the price (in effort and risk) can't be
driven that high in a scarcity market once the easy pickings
are gone.  But I really like the idea of a scarcity market;
high prices in effort and risk mean a much lower demand volume.
Indeed.  This system is at least three orders of magnitude
better than what's out there now; crooks won't try to cope
with it until the easy-pickings are getting scarce and the
cost in effort and risk meets a much higher demand curve.

@_date: 2016-10-09 13:01:17
@_author: Ray Dillinger 
@_subject: [Cryptography] BP in the media again 
I love reading about the old-style cipher machines and the
institutions that surrounded them (and surrounded breaking
They do have one crucial feature that modern systems do not;
inspectability.  They are made of electromechanical components
none of which is too small to see, all of which are simple
enough to inspect and replace.  When you're operating a SIGABA,
you needn't worry about whether someone has subverted the
device on a chip level, because no chips.  I think it is
possible to design and build an operable machine that would
resist modern cryptanalysis, too.
Alas, they are not applicable to most modern problems.  If you
tried to operate one fast enough to handle net traffic, it would
Oddly enough, the only thing they'd really work for would be
text correspondence, such as we're doing right now. It is,
just barely, possible to imagine an application in diplomatic
correspondence or the no-computers secure areas in the
modern world, but I think you'd go broke trying to market
them.  Hagelin poisoned that ground.

@_date: 2016-10-10 17:10:10
@_author: Ray Dillinger 
@_subject: [Cryptography] Would anybody like to participate in a Bletchley 
I have the idea of a wargame reenacting an alternate version of WWII,
with some of the players playing the part of the Axis, only with
different and better crypto machines to compensate for the codebreakers
having real computers. It would start with comically bad use and
management of those machines, but get less bad the longer it goes on.
The rest of the players would be playing the part of Allied codebreakers.
The cipher machines would be in 'easy', 'intermediate', 'advanced', and
'impossible' categories - judged against modern, rather than period,
state of the art, where the allied codebreakers all have modern
computers. Just for nostalgia value, we could even use the actual naval
Enigma from late in the war as the opening 'easy' crack. Everything
above the 'easy' category would be out of reach of an ordinary
exhaustive key search; you'd have to actually have an insight to get them.
They would all be straightforward, buildable electromechanical devices.
I'd cheerfully provide diagrams and schematics and simulations and test
vectors of about two-thirds of them to the Allied players before the
exercise starts, reflecting real Allied knowledge at the opening of the
war. A few would be rotor machines, but most would not. In this version
of the universe the Axis powers contracted different designers for their
main crypto devices. A few would be mysteries until/unless someone
figures them out.
We'd need some people to game WWII under some fairly detailed wargame
rules to give us a 'reality' for the messages to be about. The Allies
would be aware of newspaper stories and intel about troop deployments
etc. The idea is to make the details of that game into the basic input
for the one we'd be playing - making lots of cribs available including
names of officers, unit designations, names of ships, etc...

@_date: 2016-10-11 12:24:39
@_author: Ray Dillinger 
@_subject: [Cryptography] 
=?utf-8?q?rapdoors=E2=80=9D_in_millions_of_crypto_keys=22?=
So there is now a potentially very large undetectable class of
weak keys.
I suppose the prudent thing to do would be to behave as if there
has been a breakthrough in factoring such that primes now require
about twice as many bits length to achieve the same level of
security against factoring.  For primes whose origins we don't
know anyway - but that pretty much includes all 'ephemeral' DH
primes, as well as the primes used to construct RSA keys created
by others.
Am I right in thinking that this affects pretty much all pubkey
crypto operations performed on a modular field -- RSA, DH, ECC,

@_date: 2016-10-28 14:36:26
@_author: Ray Dillinger 
@_subject: [Cryptography] How to prove Wikileaks' emails aren't altered 
Mostly I think because the leaked email seems a likely source of
supplementary context such as key people's judgments or evaluation of
that public domain information, or the development of tactics leveraging
that public domain information.  The information itself is 'cold' but
the story of what humans felt about it or hoped to do with it is 'warm.'
 Most people can't pay attention to something without some kind of
emotional focus.
It's flatly amazing what's in the public domain, and vitally important,
but simply hasn't been found or noticed yet by people whose job is
bringing public attention to focus on important things that affect their
present and future lives, for want of that kind of focus.  Public domain
or not, information included in those leaked emails may have never
previously received the attention it merits.
But first you have to decide what you mean by 'security.'  What is your
threat model?  There are a lot of security requirements for email, and
many of them are in direct conflict, as we can see by the passage of
retention laws.  The people doing mail delivery have one set of
requirements, the people relying on mail delivery have a different set,
and the people drafting laws seem to have a third set.  You need buy-in
from all three groups to fix this.
Not so long ago, it would have been considered extremely poor security
practice for emails to remain on the server, at all, once they'd been
read.  What isn't there can't be stolen.  But not very long before that,
the decision was mostly about how much precious disk space could be
allocated to the mail spool, and that former security norm was a
follow-on from common practice established then...
The post office doesn't keep a copy of every piece of paper mail that
passes through the system, after all.  People would be shocked and
offended by the idea that it ought to.
People are shocked and offended when they learn that images of every
piece including sender and recipient names and addresses, and MAC id's
of network-capable devices, are retained by publicly available delivery
services.  It wasn't *THEIR* security requirement that drove that decision.

@_date: 2016-10-29 22:39:16
@_author: Ray Dillinger 
@_subject: [Cryptography] How to prove Wikileaks' emails aren't altered 
Yes, I was aware.  And yes, when I let people know, they are
indeed shocked and offended.  As yet however the text of the
contents of paper mail is not routinely recorded.  Although
that could now be done with the appropriate spectrum of microwave
scanning and a sufficiently high-resolution receiver, I don't
think it could as yet be done without burning out the RFID's
that an increasing number of shippers are including for package
tracking, so there are conflicting goals there.
The same 'metadata' recording happens in all common publicly
accessible delivery services.  Fedex, UPS, etc do it too.
This is especially interesting in light of rules requiring the
MAC address of network-capable devices to be printed on the
outside of the packaging.  If they can get a device's MAC, they
can review delivery records to see to whom that device was sent.

@_date: 2016-09-07 13:04:17
@_author: Ray Dillinger 
@_subject: [Cryptography] Secure erasure in C. 
When possible, I do operations that can be done in finite
bounded memory using a static volatile buffer to hold
sensitive data.  Design the program right so you absolutely
know how much memory the operations need, and the static
volatile buffer solves many problems.
Writes and reads aren't optimized away regardless of whether
you're going to read that space again before exiting.
Transient copies and virtual-memory caches don't get made
because if the compiler can't assume the copies remain in
sync to the buffer, then making copies is useless.  Finally
out-of-memory errors don't happen at runtime (and can't be
forced by an attacker to happen at runtime) if you've
preallocated the secure buffer that you keep using.
That is definitely the preferred solution. As far as I know,
the desired behavior is absolutely required by all versions of
the C standard and the availability of static volatile buffers
is the main reason why I write security code in C despite all
of C's sharp pointy bits, vicious free-swinging hooks and
bloody blades.
But every so often I need to securely delete a buffer that I
get from somewhere else, or flatten a stack image before exiting
a routine, and that turns out to be harder.
Non-volatile variables and buffers have MANY problems that
mean they should usually be avoided for secure data. The
compiler making copies in order to parallelize operations or
relocating data for whatever other reasons, or the OS deciding
that it needs to make a disk cache for virtual memory on a
machine whose cache partition is unencrypted, etc.  Those
problems can only be managed with careful configuration of
the machine, runtime environment, and OS.
But aside from that,  it's actually hard to get a non-
volatile buffer deleted.  The short version of the story is
that most languages don't have volatile buffers, and the
one that does has a standard that says unless something
in the "officially observable" output is required by the
standard, the compiler doesn't have to do it.  We want to
delete non-volatile buffers when we're done with them, but
when we're done with them we don't read them anymore, and
the compiler often concludes that therefore the writes
don't have to happen.  We often rely on observed behavior
instead of the language standard, but in many cases observed
behavior is just the optimizer not being able to figure out
that the language standard will allow it to get away with
skipping something.
In response to a compiler that made a completely unexpected
pessimization of my code, I've recently been forced to revisit
secure erasure of non-volatile buffers.
For years I'd been defining 'erase' using
static void *(*const volatile deleter)(void*,int,size_t)=memset;
static void erase(void *buf,size_t len){deleter(buf, 0, len);}
which, IIRC, I originally copied from the PGP source code
back when Hal Finney was working on it.
It's a reasonably clever bit and has been standard in crypto
code for a long time. Declaring the pointer to the routine
as volatile  means the program is absolutely obligated to
access that pointer when the routine is called.  It also
means that the compiler can't make static assumptions about
what the routine does because the pointer might change
outside of program control.  And we've been assuming that
because the compiler isn't allowed to guess what the routine
does, it has to call it in order to make sure that if it
does anything "observable" that thing actually happens.
And this has been true through all of computing history,
Up to now.
Unfortunately while the standard absolutely obligates the
program to access the function pointer and doesn't allow it
to assume that the pointer value (and therefore the routine
it points at) is unchanged, it does not obligate it to
actually execute the routine.
And someone recently discovered some obscure combination of
mind-boggling optimization levels, compilation environment,
and system settings under which an Intel compiler actually
doesn't.  Usually this kind of thing is discovered w/r/t
gcc: Intel is something of a surprise.
When static analysis reveals that calling the function which
'deleter' is initialized with, does not in fact change the
contents of volatile memory, nor non-volatile memory that
the program will later access, nor cause other un-skippable
side effects, the program can cache a copy of the function
pointer.  Then when the call to 'deleter' is made, it must
read the volatile pointer - which completes its obligation
to the standard. But then, instead of just executing the
procedure, it is allowed to compare the pointer's value to
the cached value, and if they are identical may conclude
that it need not actually execute the routine.
So.  I'm redefining 'erase.'  I think that these steps
will ensure that secure deletion happens and would like
feedback if anyone can think of any way for a compiler
compliant to the language standard to pessimize this
   1) Make calls to the RNG to get a key and IV, allocating
      the space for key and IV out of a static volatile
      buffer where the normal secure-deletion procedure
      will work on them.
   2) Encrypt the buffer in place using that key and IV.
   3) Calculate a CRC of the encrypted buffer.
   4) Write the CRC to a volatile variable.
   5) Clear the key and IV from the volatile buffer.
   1) Calls to the RNG cause side effects that can't be
      elided (changes to RNG state), and produce results
      which a static compiler cannot predict.
   2) Encryption in-place in CBC mode using a random key
      and IV should leave no recoverable information about
      the original contents of the buffer once the key and
      IV are secure-deleted. (Note; in-place encryption
      requires avoiding creation of ciphertext longer
      than the original buffer.  The IV should not be
      written to the buffer, and I can encrypt the last
      block starting at a different offset so it overlaps
      the previous block placing its end is at the end
      of the buffer).
   3) The compiler can't elide the encryption because
      the encrypted value will be used to calculate a
      CRC.
   4) The compiler has to calculate the CRC because it
      can't otherwise figure out what it has to write
      to a volatile variable and isn't allowed to guess.
   5) The volatile variable containing the CRC is not a
      risk because of random key and IV; it contains no
      recoverable information about the original contents
      of the buffer (and anyway it's in the volatile
      buffer so it can be secure-deleted normally).
   6) Although some "observable" effects are data-
      dependent, there are no possible buffer contents
      that could affect its operation for purposes of
      an attack.  It is not vulnerable to timing attacks
      because the time taken for encryption and CRC
      does not depend on the buffer data.
   7) Copies of the buffer will not be made because I'll
      be calling a competently implemented encryption
      library. I know this is the big assumption but
      encryption implementation flaws are a much more
      serious threat which must be fixed for other
      reasons and fixing them for other reasons will
      fix them for secure deletion as well. Therefore
      they are out-of-scope as a problem specific to
      secure deletion.
Can anybody think of any way that a standard-compliant
compiler is allowed to mess this up?

@_date: 2016-09-07 17:08:28
@_author: Ray Dillinger 
@_subject: [Cryptography] [Crypto-practicum] Secure erasure in C. 
everything that starts with this line doesn't work, because
passing a (non-volatile) 'char *' to a 'volatile char *' is
an error.  Something can't be made volatile by a cast; it
can only be done when declaring the variable (or buffer).

@_date: 2016-09-07 18:09:10
@_author: Ray Dillinger 
@_subject: [Cryptography] [Crypto-practicum] Secure erasure in C. 
You can initialize a volatile pointer from a non-volatile
pointer, but that's because the pointer itself is volatile
when created.  You want to initialize a pointer at a
volatile _object_ from a pointer at a non-volatile _object_,
which is different.
I don't think that's allowed unless the non-volatile
object is 'static' in the first place, like a procedure
or a static variable.  IME nothing can become 'volatile'
unless its location can be fixed at compile time.  But
now I have to go read the standard again because I may
be remembering that incorrectly.  It would be very nice
if it's allowed.

@_date: 2016-09-08 18:49:18
@_author: Ray Dillinger 
@_subject: [Cryptography] Secure erasure in C. 
I have an encrypted swap partition for the same reason.

@_date: 2016-09-09 15:10:40
@_author: Ray Dillinger 
@_subject: [Cryptography] Secure erasure in C. 
I would never recommend multithreaded ANYTHING if
handling sensitive data.  I know you're right; if it's
a library some idiot will want to put it into a
multithreaded program.  But it's hard enough to
control information in a single-threaded program.
Even if you DO use it in a multithreaded program,
you'd put a page barrier around it so that anything
that runs in memory visible to any other thread
doesn't ever contain any sensitive information.
It doesn't prevent OS swapping unless the OS is aware of
what memory is mapped virtual, which outside of a few
weirdies like Capsicum, they're not.  Even in that case
it's just notifying the OS that caching it is completely
useless and wastes disk space.  To actually forbid caching
to disk, you have to use memlock().
Outside of that it's down to machine configuration; secure
workstations usually have no cache partition at all. If you
run out of memory, "crash is better than cache."
If they do allow cache partitions, they use encrypted cache
partitions. But that's still OS-level; the program usually
has no control over that.
What a volatile buffer does is prevent the compiler from
issuing a stream of instructions that make the variable
name refer to different locations in memory at different
times, or queue up writes to it somewhere before actually
committing them to the buffer.  It forbids mapping it into
cache and then writing the final result back out after a
bunch of operations that don't actually touch the buffer.
And it means that when the program calls for that value
it has to read it from that single memory location, so
there's no optimization goal that can be fulfilled by
making transient copies elsewhere.
This is all well below the level of the OS. This is about
how the stream of instructions from the compiler goes about
carrying out the program - variable relocations, transient
write caching, etc, are normal strategies for current
compilers even in carrying out single-threaded programs.
Hmmm.  "Static" in this case means internal linkage only;
the function pointer isn't supposed to be made visible by
the linker to anything outside this file, even if they use
the 'extern' keyword to try to get at it. But as a "volatile"
pointer the compiler is strictly forbidden to assume that
its value won't get changed from outside the program.  It
doesn't matter whether other compilation units can see it,
or whether the whole-program optimizations are turned on.
"const" means that the particular compilation unit represented
by the file won't be changing it.  Hmmm.  It should work the
same way if I drop 'const', I guess, because the optimizations
usually allowed by const (partial evaluation, etc) don't apply
to volatile variables anyway.
Maybe an adhoc kluge that drops 'const' and then actually
changes the pointer value occasionally during runtime would
foil this particular optimizer.  And maybe memset_s (the right
answers) will have more widespread support by the time that
kluge stops working.
Yeah.  The thing that surprised me was an 'optimization' based
on speculative caching.  The compiler didn't assume the value
would never change; it just checked whether it had actually
changed during runtime and skipped over the function call if
the pointer still had the same value.

@_date: 2016-09-09 15:22:07
@_author: Ray Dillinger 
@_subject: [Cryptography] Secure erasure in C. 
I really, really wish operating systems were implemented in something
else.  If we had Pascal discussions instead of C discussions at least
we'd be talking about something with array bounds checking.
But operating systems are written in C, so you can't get secure erasure
in anything else unless you can get it in C.  And it looks like you
can't get it in C unless you can get it in assembly language, and you
can't get it in assembly language unless you can get it in silicon.
Where's the bottom turtle?

@_date: 2016-09-09 15:42:45
@_author: Ray Dillinger 
@_subject: [Cryptography] Finding the least significant bit of RSA secret 
It is well known and one of the things that implementers of crypto
have to be careful about.  RSA as raw math is beautiful and simple;
RSA as a viable cryptographic primitive requires IVs, padding, and
very careful thought.
In principle the attack you describe works; in practice RSA crypto
pads values and transforms keys to prevent it.

@_date: 2016-09-09 17:28:32
@_author: Ray Dillinger 
@_subject: [Cryptography] Secure erasure 
Indeed.  That is in fact the main problem.  All these systems
are defined in terms of what they make visible or available,
and security is defined in terms of what is destroyed or kept
unavailable.  The result is that a lot of things are made
available by copying or caching or whatever, when we wanted
to assure that no copies are ever made.  But we don't have
the ability to express that, because NOT making copies isn't
defined in terms of making something visible or available.
Similarly, in the case of memset(), the intent of the code was
to make the data in that buffer UNAVAILABLE - ie, to destroy it
so an asshat trying to scrape it out of the system with a memory
snapshot or a force it to get dumped to disk cache would have a
harder time doing it.  But the compiler looked and said, hey,
this new data being written doesn't have to be available later,
so we can skip the write entirely.  It was designed, from the
ground up, to ignore any instructions except as they relate to
making things visible or available.  And if that visibility or
availability isn't needed, it wants to skip the work of
destroying the stuff it was our intent to destroy.  It's like
we trusted somebody to shred the stuff in the confidential
files in our office, and they didn't bother because they
didn't need the space for new files yet.
There's a fine line between working smarter not harder, and
being too damn lazy and stupid to be allowed to live.  If
an attorney, or a paralegal, or even a secretary, failed in
such a way they'd be fired immediately.  But we're not even
entitled to legitimately complain about the non-performance,
because we weren't even able to explicitly command them to
shred the damn files.  Shredding the files, you see, isn't
making something available or visible.
We have no way to even begin to EXPRESS a positive requirement
for making things invisible, destroyed, or unavailable.
Oh hell yes.  It would be really REALLY nice if I could declare
something with a keyword like 'secure' instead of 'static volatile',
meaning DON'T make caches of this, DON'T copy its value to anywhere
else as part of an optimization, DON'T allow the OS to swap it
out to disk, DON'T allow the program to exit without explicitly
destroying it, DON'T allow more than my one designated single
security-handling thread to read it, DON'T put it into L2 or L1
on-chip cache, DON'T elide writes to it ever, DON'T ever leave
part of its contents in registers where they might get spilled
into the stack on a call, DON'T allow the power manager to dump
the memory image of it to disk when the machine is going to
sleep (instead just plain EXIT when the machine is going to
sleep), DON'T allow the program to run in any kind of VM, etc
etc etc....
Every last one of these new pieces of functionality intended to
make things visible or available is another DON'T that I have to
add to the list because of that asshat who can set the exec bit
on his malware using the rowhammer bug, or the botmaster who
successfully phished Homer Husband or Harriet Housewife, or the
user whose root password is the same as the password they used
on the account in the un-hashed database that got stolen yesterday,
or whatever else is fresh and bloody this week.  I mean, there's
always going to be bugs because both coders and users are human.
I get that.  But the bugs could be a HELL of a lot harder to
leverage and exploit given tool support.

@_date: 2016-09-10 21:35:57
@_author: Ray Dillinger 
@_subject: [Cryptography] An historical document. 
In my long sift through the Friedman Files, I came across this
gem, dated 16 August 1937, which may very well signal the beginnings
of the modern era of cryptography and security.  I thought I'd share
it with any Pre-WWII cryptography buffs in the audience.
Historical Notes:  At this time the Coast Guard Cryptanalytical
Unit headed by Mrs. Friedman was in the news a lot because they
were daily breaking ciphers by rumrunners (still active 4 years
after the repeal of prohibition) and occasional human-trafficking
operations.  William Friedman himself was at that time chief
cryptanalyist of the Signals Intelligence Service working for the
War Department.  Joseph N. Wenger, who sent the memo, was Deputy
Director of the Armed Forces Security Agency.  He later went on to
become a Rear-Admiral of the Navy and vice director of the
National Security Agency.
The publicity Mrs. Friedman's exploits attracted seems to have
been a concern for Wenger with respect to keeping confidential
the first whiff of something entirely new and unprecedented in
the field of cryptanalysis.
... Something which has had a PROFOUND effect in shaping the
present in which we find ourselves.
Ref ID: A72632
Navy Department
Office of Chief of Naval Operations
(in reply refer to Initials and No. OP-20-GY)   16 Aug 1937.
Memorandum for 20A:
  1.  I have just learned through Mr. Friedman, head of Army
Intelligence Section, that the Coast Guard Intelligence Division
has been equipped with I.B.M. Tabulating Equipment for Cryptana-
lytical purposes.
  2.  The idea of applying this machinery to cryptanalysis
originated in the Communication Security Group of the Navy and
was revealed in confidence to Friedman.  The latter obviously
passed it on to his wife who is head of the Coast Guard Crypt-
analytical Unit.
  3.  The application of Tabulating Machinery, and in fact
any machinery, to cryptanalysis represents the greatest advance
we have made in this field in recent years.  It is in my opinion
an important weapon of national defense which if revealed to
foreign nations might deprive us of a great advantage in time
of war.
  4.  In view of the extensive publicity given to the Coast
Guard cryptanalytical activities I believe that immediate steps
should be taken to inform responsible authorities of both the
Army and Coast Guard that the Navy considers information per-
taining to the application of machinery to cryptanalysis is a
matter of national defense and should be withheld from disclosure
under provisions of the Espionage Act of 15 June 1917.  It might
also be well to take similar action with respect to the Inter-
National Business Machine Company through their representative
here in Washington.
  5.  I have taken it upon myself to present the foregoing
views to Friedman this morning and request that prompt action
be taken to give this action official backing.
 Respectfully, (signature) J.N. WENGER.
There you have it my friends.  That's the US Government first
noticing the possibility of using automatic machinery for
cryptanalysis.  1937.  Just two years later WWII would break
out and make it really really important to have noticed that.
It was declassified and approved for release by the NSA on
September 18 2013, pursuant to Executive Order 13526.
PS.  The Friedman Files are available as a torrent.  Reading them
is a lot of fun if you like historical primary sources.

@_date: 2016-09-10 22:09:44
@_author: Ray Dillinger 
@_subject: [Cryptography] Secure erasure 
Yes.  That is in fact the point.
Heartbleed is EXACTLY the kind of vulnerability I'm worried
about.  The OS allocated memory to the server for new buffers
out of memory the process had already allocated and released.
Because it was going to the same process, the OS "optimized"
and didn't bother to clear it before reallocating.  The source
code for the server program contained instructions to clear
it before releasing. But the machine code didn't. The compiler
had "optimized" and skipped those instructions on the grounds
that it was a write to dead data that was about to be released.
Then somebody comes along and tells the server they want to
see more buffer memory than the process had written since the
buffer was (re)allocated, and they get data that the programmers
thought had been erased, from buffers the programmers had
released, sitting there in the "unused" space of live buffers
allocated after that release.
My point is that the programmers weren't just idiots.  They
made a legitimate effort to be secure.  They may have even tested
and made sure the memory clear *DIDN'T* get optimized away at
the time the code was written, but they had no way to explicitly
specify destroying that data. And some later version of the
compiler, or some different compiler than they used, or some
different combination of environment, settings, or OS found a
way to rip them off.
   optimizers, two.
   Programmers, zero.
   System security, minus several million.

@_date: 2016-09-11 09:47:01
@_author: Ray Dillinger 
@_subject: [Cryptography] [Crypto-practicum] An historical document. 
I don't think that was very likely.  The whole point of
this memo was to make sure people *didn't* see this news
and take it to heart.
This is decades later and we're looking at declassified
documents.  This is not at all representative of what
someone on the ground reading the newspapers would have
seen.  At this late date we occupy a very privileged

@_date: 2016-09-11 10:28:08
@_author: Ray Dillinger 
@_subject: [Cryptography] Secure erasure 
Agreed?!  What the hell was Heartbleed, if not an attack
on memory that ought to've been secure-erased?  No.  NOT
agreed!  These are attacks we're dealing with in the field
right now, and we need ways to defend against them!
Sure, it got accessed through a buffer hack that invoked
undefined behavior and ought to've also been defended against.
But defense in depth means that not only should the buffer
hack not work, but also that even if the buffer hack does
work there should be nothing for it to take.

@_date: 2016-09-11 11:41:44
@_author: Ray Dillinger 
@_subject: [Cryptography] [Crypto-practicum] An historical document. 
Yeah.  It is exceedingly likely that Turing saw publicity
about the Coast Guard Cryptanalysis Group breaking the
codes of rumrunners and opium rings and traffickers and
smugglers.  I'm thinking it's very unlikely that he saw a
breath in the news about them using the IBM tabulating
equipment to do it.
The whole point here was to make sure that no mention
of the tabulating devices' use in cryptanalysis was to
make it into that publicity, and specific mention was made
of making it clear to IBM that no mention of the use of
those devices in cryptanalysis was to make it into their
publicity either.  They were probably allowed to say the
Coasties were using them, but they weren't to breathe a
word about what they were using them for.  If asked,
they'd have to talk about boring office stuff like
keeping track of billable contractor hours, distribution
of military mail to all the mobilized personnel, tracking
distribution of supplies and materiel and purchase orders
and accounting.  You know, normal things, right?  All
those other perfectly real logistical challenges where
their machinery would be useful, which were also
logistical challenges that their business customers
could relate to their own uses for the devices.
This is the MIB deciding to keep something confidential,
not the news story that could have only resulted from
their failure to do so.

@_date: 2016-09-11 20:59:09
@_author: Ray Dillinger 
@_subject: [Cryptography] Secure erasure in C. 
Honestly?  Yes, I cared.  Thank you for those resources.
memset_s() is the right thing, at least w/r/t this specific
issue.  I look forward to the day when it is widely enough
implemented and reliably available to be the simple solution.
That day has not yet arrived, as the libsodium code
Because I also went and looked at the libsodium code.  It's
good.  It checks for the availability of memset_s and calls
it if it can.  Then it checks for the availability of the
Windows not-quite-equivalent and calls that if it's available.
Then it falls back to the same thing I was doing with a
volatile pointer to memset.  If and when memset_s is more
widely implemented it will be a viable solution; as it is,
it's better than what I was doing.
However, I took another bit from the list and implemented it,
and expect to find nothing that reduces it.  The program
isn't allowed to cheat on I/O, and I can do transformations
on the buffer (such as XOR'ing it with /dev/urand output)
which destroy the information in it - and which can't be
skipped because I then make _use_ of the transformed values
when I turn around and write them to /dev/random.  A minor
variation on my initial idea, but simpler to implement.
Of course even these operations aren't *absolutely* required
to be done in the same buffer overwriting the old data, either,
as long as the "same" values get written back to dev/random.
But if the most *efficient* way to do these ops and get these
values is to in fact overwrite the buffer, I don't think I
should need to worry about an optimizer cheating its way
out.  At least not before memset_s is ready to take over.
Of course, before I call that I'll check for memset_s.

@_date: 2016-09-13 13:17:03
@_author: Ray Dillinger 
@_subject: [Cryptography] Secure erasure - Ada may be a solution. 
I want to thank you - from the bottom of my heart, truly! for
pointing that out.  The spec writers appear to have declared war
on undefined behavior, which was just a curiosity back when but
has become something we desperately need. And I had literally
forgotten that Ada actually has the kind of control that C
compilers have been taking away and which we need for that
1% of bit-groveling code that is all about plugging the tiniest
little leaks. I wasn't in need of those things back when I
actually used Ada (for 2 semesters in college) so they didn't
make much of an impression.
I used Ada, half a lifetime ago - I remember it being a lot
like a more stringently type checked Modula with a bunch of
specified error checking behavior which seemed silly and
overspecified at the time but doesn't seem at ALL silly now.
Plus a lot of bolt-on code verification features I wasn't
interested in at the time but am much more interested in now.
It has a lot of stuff to do things like specify binary layout
at the bit-grinding level, etc, which I didn't appreciate at
the time because C and Pascal implementations were doing all
that stuff very reliably without the added verbosity.  Which,
AGAIN, don't seem at all silly now.
I've been having a go at the language spec and tutorials and
firing up gnat.  There's a lot of this I'm really hazy about
remembering, but it's familiar....
There is noise in the Ada spec about being amenable to either
garbage collection or conventional (for the time) allocate-
and-free memory management, but I don't think I ever actually
attempted to use garbage collection back in the day and I'm
not terribly interested in it right now.  Maybe later.
Now I'm looking at it again, and finding - HOT DAMN!!! - here
is a language that supports volatile locations where it won't
EVER under any circumstances elide a write!!!  All this time
I've been using C for security stuff mostly for that reason
alone! And it has atomic operations I can use to counter race
conditions without locks that open timing attacks!  And it has
representation pragmas - standard, not as compiler extensions
of unreliable availability - that allow to eliminate gaps in
structures where data or state could exfiltrate.  AND absolute
layout control so I can code it to directly accept calls from
C or directly call C code - OR NATIVE LIBRARIES - via targeting
a specific ABI in code that is spec-compliant and ISN'T
implementation-dependent on the Ada compiler!!  And even
absolute addressing with mmap and munmap-like facilities!
You could write _device_drivers_ with this, even in an OS
whose ABI is based on a different language!
All that grotty low-level stuff that "modern" languages got
rid of, which has kept me writing in a flying-hooks-and-bloody-
blades language in order to control!  It's all here!  And it's
here in such a way that you can IGNORE it if that grotty
low-level stuff isn't what you're doing!  And it's here in a
language that DOESN'T ignore error-checking!
Ada throws a CONSTRAINT_ERROR exception when an array index
goes out of bounds.  The default error handler halts the
program.  That alone would be FAR better at least for security
purposes than most of the things that out-of-range array indexing
has been used to do in C. Crashes are bad but not as bad as
breakins, botnets, rootkits, trojans, and stolen data.
But if you don't like crashes you can catch the CONSTRAINT_ERROR
with an error handling routine that does something else.  Finally
the error handler isn't global; you can set different ones for
different record types or control modules.
The runtime bounds check is parallelized (by gnat) or eliminated
(at least by the gnat compiler) when static checks indicate it
won't ever fail, and can be deliberately suppressed by the
programmer (in standard compliant code!  No compiler-dependent
pragmas required!) if the programmer is micro-optimizing
(inadvisable) and has determined via other means that it will
never fail.
I'm with you on fighting the implementation; its type checking
is finicky and hard to satisfy. But I've been doing this level
of checking, with less consistent tools, forever, so Ada isn't
significantly a bigger fight.  All it means is that I wouldn't
be using cppcheck, frama-c, scan-build, lint, and splint so
much, and the equivalent of the -Wall -Werror I've been compiling
with is on by default.
Ada compilation was slower than C compilation in 1988 but doesn't
seem noticeably slower today.  It's about the same speed as compiling
C++ and that's reasonable because it has equally-expressive OO
features including templates, generics, and interfaces.  It's
definitely faster than I can get equivalent typechecking using
external static analyzers.
On several benchmarks, I do not experience the compiled
code being slower at runtime.
Bottom line:  looks and acts superior so far, has all the things
I need for writing grotty bit-groveling code including device
drivers and security code, appears to also be amenable to higher-
level programming, has WAY better error checking, is (and can be
FORCED to be) ABI compatible with anything, gnat appears equal
in quality to gcc, and performance is identical unless I specify
something definitely suboptimal like packed arrays of 6-bit
All that 'extra' stuff is about ensuring correctness.  That's
not at all the kind of meaningless, redundant and undirected fluff
that got into PL/1.  Instead, I'd say it's a big dose of exactly
what we have come to need.
It's no more complex than C++ (he said, praising it with faint
damnation) and far less gotcha-ridden (in the sense that most of
the 'gotchas' either throw exceptions you can't ignore but can
decide what to do with, are easy and reliable to detect at runtime,
or halt compilation).

@_date: 2016-09-13 13:30:26
@_author: Ray Dillinger 
@_subject: [Cryptography] Secure erasure 
It is very difficult to imagine a DRM system where you'd
need to attack the encryption rather than just reading
the key out of the player. DRM is a special case in that
the idiots who attempt it are putting the decryption keys
into the hands of the very people whom they're trying to
prevent from decrypting the content.
Essentially, if the device is a general purpose computer,
as opposed to a dedicated media player, then it is
GUARANTEED that someone will be able to stroll around
the encryption rather than attacking it.

@_date: 2016-09-15 19:54:57
@_author: Ray Dillinger 
@_subject: [Cryptography] True RNG: elementary particle noise sensed with 
One small-ish problem with electromagnetic noise is that it can
also be described using the word "radio."  If somebody points a
microwave antenna (or for that matter a tightly-focused transmitter)
at the device, would they be able to predict what "random" bits
it produces?
Still, whatever can be done with radio, they remain unpredictable
to anyone who isn't doing an expensive attack that requires human
time and attention (and risk) to monitor a single target.  That's
I think that, rather than building a complex device that
people then have to trust, we should be building very
simple devices whose entire functionality (and therefore
trustworthiness) can be determined by visual inspection.
Start with an RFC or a standard, that specifies a wire-
level serial interface for devices that generate "random"
And the wire-level protocol should be so simple that it
requires nothing on the board to be capable of running any
code and is easily within reach of a hobbyist to build.
That is, IMO, an IMPORTANT property.  If a paranoid is
supposed to trust the device, then either it will be
because s/he built it him/herself, or because an electronics
buff is able to tell from visual inspection of the components
alone everything that the device does.  Code is not evident
to visual inspection of the components and therefore must
not be a requirement for any part of the device.
Then you can publish schematics for a simple device
that meets that protocol and produces unpredictable bits,
and a dozen or so people can write device drivers for
their favorite operating systems that deal with any
device meeting that dead simple serial protocol.
Somebody else can publish schematics for a *different*
simple device implementing the same standard wire protocol
but producing unpredictable bits in a different way,
and the same device drivers would work for it.
Paranoids and hobbyists can build their own and know that
there is no backdoor.  Entrepreneurs can construct the
circuits on acrylic boards and then cast more acrylic
around them for durability, and people can inspect the
fully transparent devices they built to make sure they
used the same parts in the same arrangement as shown
in the schematic.
A different entrepreneur can do the same thing but embed
glitter in the acrylic, and sell a device that scans
the glitter to make sure it does the same thing to a laser
light that the glitter in the original module did.
Then people can be "reasonably" more certain that the
device they have is the same device they had last week
(ie, nobody switched it) and hasn't been dissolved or cut
and then recast (ie, nobody tampered with it).
A third entrepreneur might build an "enhanced" device
with a USB interface and an arduino or something on board,
and a camera that you're supposed to point at a lava lamp
or goldfish tank or something. The exact functionality of
this "enhanced" device couldn't be audited by visual
inspection alone so paranoids wouldn't buy it, but other
people would.  As long as it meets the dog-simple serial
interface spec, a device driver on the system doesn't
care how the thing at the other end of the bus does it.

@_date: 2016-09-16 14:29:55
@_author: Ray Dillinger 
@_subject: [Cryptography] True RNG: elementary particle noise sensed with 
The majority of users don't have a threat model that includes
sophisticated actors who are willing to devote time, attention, and
manpower to attacking their systems on an individual basis. Instead,
most people have to worry about untargeted, automated, attacks of
opportunity.  These attacks have the simple goal of suborning or
eavesdropping on as many systems as possible without regard to which
ones. Nobody is going to deploy expensive hardware and even more
expensive human agents in the field to further them with respect to any
particular system.
The majority of users who *do* have such a specific threat model (high
value targets) also have physical security that should make it extremely
difficult for such agents to put the equipment in place and then monitor
or control it.
And, as I see it, the device is valuable to both of these majority
classes of users regardless of any mild susceptibility to EM interference.
No.  What it requires is a specification of a wire-level protocol via
which such a device communicates with a computer.
Let the philosophers debate a definition of "randomness."  Let customers
and makers decide for themselves what definitions they accept and what
hardware they trust to fulfill  them.  A fully deterministic device
producing an infinite series of 1s could meet the communications spec.
It would have no value as a source of unpredictable or "random" bits,
but that is outside the scope of a communications protocol.
USB stands for Unsecured Serial Bus. Secure machines have their USB
ports filled with epoxy.  I would not buy a USB device for any security
purpose. I would prefer a device with a DB9 serial plug.  Preferably a
plug made of transparent plastic so I can see every wire and verify that
there is nothing else inside it, or one that I can cut off and replace
with a random DB9 plug that I take from a generic cable.
Serial devices are also simple enough to build with zero chips that
things can be hidden in, and USB devices are not.  If people insist on
using USB, adapters are widely available, but any built-in USB
functionality could be a place to hide things. The absence of a USB
interface would add significant value to any device intended to be
manifestly trustworthy.
And most people are willing to trust makers at the component level for
an extremely simple device whose electronic components are visible.
Those who are not willing, are the very same paranoids I mentioned who
will insist on building a device themselves from individual components
they buy and test themselves.
This, IMO, is why there needs to be at least one very simple schematic
calling for widely available standard parts.  For the paranoids who
won't trust it unless they build it themselves.  The scheme you proposed
(current noise) presents a reasonable method for building one.
It needn't be the only schematic for devices meeting the wire level
communication spec and purporting to provide unpredictable bits, nor
even the only schematic simple enough to self-build; but at least one
such schematic needs to exist and be public so anybody can build it.
Again, defining "random" is for philosophers.  It is the makers and
users who decide for themselves what definition they accept, and they
will not care what the philosophers say about it.
Every OS comes with a functioning randomness extractor built into
dev/random that is better than the one you could create for another
driver.  Rolling your own would be stupid.  The device driver should
just write the bits produced to dev/random.
If you're worried about counting how much "entropy" you get from the
device, you can test it. Pipelining through gzip can compensate for
simple bias and any easily detectable statistics.  Measure the ratio of
gzip's output to its input from the device, and that's (about twice) the
amount of entropy you should credit via an ioctl call per that amount of
device input.  You don't need to run gzip during runtime; just keep
feeding raw device output to /dev/random, credit it for the appropriate
compression ratio revealed by your gzip test, and dev/random will do the
right thing.
You might even want a cron job that repeats the test at intervals and
updates the entropy ratio or raises a security alert if it changes
significantly.  But for god's sake don't do anything more complicated
than that.  dev/random works, and gets audited routinely.  Every
component you introduce for this specific purpose will need its own
expensive security audit for any professional use, or be seen as a place
to hide things.
Most people, most of the time, aren't attempting to count "entropy" any
more.  Aside from being another squishy concept suitable only for
philosophers, it has not been found to add much value to the subsystem
in practice.  They should be calling dev/urandom for almost all purposes
provided that it's more than a minute or so after bootup.
Yes, it's a fine source of unpredictable output. IMO such sources could
be made much more useful useful by a "standard" way for computers to
read the output and a way to build it so that it is _manifestly_ and
_visibly_ trustworthy.
To me the value of your idea is that here is a way to build something
that is visibly trustworthy because it's made of simple components and
can be built in a way that every component and circuit trace can be seen
from outside the device.

@_date: 2016-09-17 10:25:48
@_author: Ray Dillinger 
@_subject: [Cryptography] Ada vs Rust vs safer C 
There was an effort called 'simple c' which attempted to do that IIRC.
But it became bogged down in the question: what do different developer
communities consider to be the _correct_ specification for various bits
of unspecified behavior?  There was far less agreement than expected.
As far as I'm concerned, most of 'undefined behavior' should be defined
either as 'compiler halts with an error' or 'compiled program contains
code to detect this and halt instantly if it happens.'
IE, code that is valid and has specified behavior in the new dialect
should for the most part be a subset of valid code with the same
specified behavior in C.
Otherwise I'd far prefer to have the new dialect absolutely require
something that flatly will not be accepted by other C compilers (which
could be just a 'dialect declaration' at the head of the file).
Otherwise people will use everything else in the world to compile it
because "It's just C code, right?"  and things will go awry because code
written to the new 'extended standard' will be silently depending on
things that other compilers will do differently or not at all.
There are several cases where 'compiler halt or program crash' is
clearly wrong, too - Integer overflow for example should not be
undefined and it should not be a crashing error.  I consider it
'obvious' that
C = A + B;  /* overflow is unspecified on integer addition */
should have the same semantics as
   and therefore this expression does "what you expect" of integer
   addition with both integers and unsigned integers represented
   in 2s complement; wraps around to the most negative value. */
C = (int)((unsigned int)A + (unsigned int)B);
but a compiler proceeding on the basis that either an accurate
(non-overflow) addition happened, or undefined behavior, can decide it
has the right to skip any code which could only be activated by the
undefined behavior - such as a check for inaccurate (with-overflow)
Not much, actually; tcc with optimizations turned off already meets most
of them with what people would consider 'obvious' semantics.  Because
it's not 'compiler halts' or 'program crashes' however, code developed
under tcc dies horribly or develops obscure and unexpected bugs when
compiled anywhere else.
I've been going the other way mostly; I've been developing under gcc
specifically to make any unspecified-behavior bugs as much worse as
possible, because that way I don't inadvertently depend on crap
assumptions so much.  gcc is cool like that.  Then I actually use
something else to compile any code I send to clients.  Porting the
code away from gcc sometimes reveals unspecified stuff too - gcc'isms I
accidentally depended on.  Once code is hammered down enough to do the
right thing on a couple of different compilers, it's potentially ready
for use.
True that.  Mostly I think what you want to do for this purpose is to
fork tcc and enshrine most of its default behaviors for unspecified C in
your new standard.  I would prefer compiler halt or program halt in most
cases, but if you want "obvious" instead, that's probably a very close
approximation to it.

@_date: 2016-09-17 12:50:35
@_author: Ray Dillinger 
@_subject: [Cryptography] True RNG: elementary particle noise sensed with 
Doubtless.  That is absolutely true.  But one can formulate an
expectation sufficiently demanding that it won't be met by anything.
Would that expectation help you keep a system safe?
My expectation is for "some" unpredictable output from a device that can
be visually inspected to assure that it contains nothing other than
the components and functionality that it is supposed to contain.  That
would help with my relentlessly practical security needs.
Ah.  Now I understand.  That device was not "biased" - it was in fact
deterministic.  He may express it using words that make you think he's
worried about a definition of randomness, but the security concern is
with a definition of trustworthiness.
Firstly there is the matter of the fixed seed.  The information about
the seed state was known to someone other than the current owner at the
time the device was manufactured, and that information can be used to
predict or reconstruct the device's whole output.  There is no such
thing as evidence that information once known to someone has not been
retained, or that it has not been sold to mobsters, crooks, and crackers.
Second, if a CPRNG-based device comes up in the same state each time
it's powered on, it will repeat the same sequence of output bits - and
that's also untrustworthy because it's trivially predictable if you've
seen it before, or trivially reconstructable if you're seeing it later.
Anyone who has possessed the device previously (including the
fabricator, shipper, manufacturer, wholesaler, retailer, somebody who
broke into a warehouse to make a substitution during storage or
shipment, etc) or gets hold of the device in the future (including
burglars, heirs, randoms who got it accidentally at an estate sale,
creditors who seize assets at a site, lawyers with subpeonas, etc) can
discover the output being used by the current owner.
Avoiding that obvious mistake increases the complexity (and degrades the
inspectability) of the device.
Third, if a malicious actor 'tweaks' the code for the CPRNG that gets
baked into the device and substitutes one s/he can easily analyze or
predict, that's equally dire, can't be revealed by visual inspection,
and has already happened several times so people now EXPECT it and want
direct evidence (manifest trustworthiness, visually inspectable
hardware, etc) that that is not what you're doing.
Finally, even if those aren't problems, there's a worry that the CPRNG
algorithm will get stale and eventually rot.  Had you implemented a
deterministic generator based on, say, Arc4, 10 years past, it would
have seemed "random" at the time - but someone equipped with modern
mathematical knowledge could analyze its output, detect that it was
Arc4, and start reverse-engineering its state.  Eventually, having seen
enough output and applied enough computing cycles, one could, in theory,
predict every output that would ever be made by the device from that
point on. And somebody could make a discovery next week that reduces the
amount of output needed or the number of computing cycles required to
process it.
Trustworthiness is a legitimate concern.  The output of a CPRNG-based
device mustn't be used directly, for any high-value keys or other
sensitive material.
I would be happy to feed such output into dev/random to be mixed with
everything else, but: first, it would do nothing that I can't do in
software so why am I buying the device? Second, it would be incorrect to
make the ioctl() call that said dev/random was entitled to produce
output based on no *other* bits so my system wouldn't get new
capabilities that satisfy a need for a given rate of dev/random bits.
Third, dev/urandom already implements such a CPRNG.
Work which I shall read with interest, but which I have no expectation
will require any change in my behavior regarding relentlessly practical
security concerns.  Given the choice between someone trying to sneak a
gimmicked, non-inspectable RNG into use, and someone making a huge
mathematical breathrough of equally dire import?  The gimmicked RNG is
billions of times more likely (and here "billions" is likely a
misspelling of a far larger number).  In fact it has already happened
several times.
It's especially dire if the output of the gimmicked RNG is given
privilege to bypass the mixing step in dev/random and get used directly
as keys etc, which is why there is ABSOLUTELY NOTHING to do with the
output of any such device, under any circumstances, other than feeding
it to dev/random.
If you try to do anything else with it or make its output privileged in
any way, alarm bells will go off.  Suspicious bastards like me, with
relentlessly practical security concerns, will think, "Hey, that's the
way they were telling us to use Dual-EC-DRBG!  And the way Intel wants
us to use that stupid chip instruction from the mask we can't inspect!
Somebody's cheating again!"

@_date: 2016-09-20 11:34:14
@_author: Ray Dillinger 
@_subject: [Cryptography] Ada vs Rust vs safer C 
I have gradually gotten into a habit that seems to help me (especially
when dealing with large-ish functions), although most people find it
peculiar and react badly to it on first sight.  I would have reacted
badly to it myself ten years ago.  But having gotten used to it it just
seems much clearer. I still restrain myself from doing it while working
with people who don't like it, but in a pinch I just pipe my  code
through a sed script to astyle to massage it into a format more familiar
to them before checking it in.
I find that I can more easily understand functions, even functions of
just as many statements, if they're on fewer lines.  This brings out
control structure on the left side without stringing code off the top
and bottom of the page.  And I can more easily see relationships between
functions if they're vertically compact enough to see both (or all) of
those functions at once.  The habits that made for readable code when we
were writing on 20x80 character monitors have, at least for me, become
obviously wrong in an era of 10-Megapixel displays.
My pages are now ~100 lines or so rather than 20 lines, and my editor
can be as wide as the whole screen. So, gradually, I've taken to writing
code in a 250-column format where I can read it on about 3/4 the width
of my monitor.  That way I can still keep on the right side a terminal
window where I can see it and use the command line (and see compilation
errors) while the code is still visible.  Most functions fit on <6
lines, and I *VERY* seldom have to write one that runs off a 250x100
So if-statements and loops that fit on one line, or even short function
definitions that fit on one line, go on one line.  Conceptually related
short statements and multi-statement idioms like the classic 'shuffle'
that exchanges two values, also go together on one line.  Statements
that are related in two different ways (like expressions that initialize
or manipulate elements of a 2d array, or doing 5-or-so related but
non-identical things with x and y coordinates while navigating a
structure) go into a grid that matches the structure of the operations.
That way I can see the differences in handling different elements by
contrasting statements that are vertically adjacent, and see the
"general" nature of the analogous operations involved by contrasting
statements that are horizontally adjacent. I can find relevant
statements more easily that way than when the code is vertically strung
out, and see more easily how it relates to other operations.
This flies in the face of what used to be the accepted advice for
writing readable code, and violates some assumptions that some of the
tools make (like GDB wanting to execute a whole line at a time rather
than a statement at a time).  Nevertheless I find it more readable and
easier to work with, and for GDB and similar, debug builds pipe the code
through my sed script to an intermediate file (to one-statement-per-line
form) and that works fine even though it screws up the relationship
between GDB's line-numbering and the code's line-numbering.

@_date: 2016-09-20 19:28:40
@_author: Ray Dillinger 
@_subject: [Cryptography] defending against common errors 
Short version:  Don't use gcc or icc with high optimization settings, if
you want to write anything just before deallocating it.
Long version:  Compiler is allowed to elide accesses to a non-volatile
object, even if the access is via a volatile pointer or via a pointer
with type pointer-to-volatile-object. *ALMOST* all compilers treat
volatile as qualifying both the pointer and whatever gets accessed via
such a pointer, (ie, treat 'volatile' as a qualifier of the actions done
via that pointer rather than a qualifier on the allocation alone) but
our usual problem child takes -O3 as license to make the distinction.
Even so it can only seldom actually prove that it's allowed to.
Declaring a pointer to be a pointer to volatile data should
(recommendation, not requirement) produce a compile-time warning at any
assignment statement that points it at known non-volatile data, but I've
never seen that warning.
Accesses via a volatile pointer are guaranteed/required to read the
pointer from main memory, but in the event that the data pointed at was
not allocated volatile, are not required to follow it and read or write
to any non-volatile data it points at.  Accesses via a
pointer-to-volatile are not required to go to memory for the pointer
value itself, but are required to write the object if their (possibly
cached) value for the pointer is the address of something allocated
If an object is about to be deallocated, the compiler can move the
(required) free-ing access of a volatile pointer above the write code,
and then skip the write code if the pointer on access contains the
address of a non-volatile object.
Until memset_s is widely and usefully implemented, I'm back to hacks -
XOR the buffer with /dev/urand output then write it to /dev/random
before releasing, on the grounds that I/O operations which change the
state of the pool can't be elided.

@_date: 2016-09-21 16:09:46
@_author: Ray Dillinger 
@_subject: [Cryptography] defending against common errors 
Okay, finally I have something definite about making assignments with
volatile semantics (assignments that won't be optimized away under any
circumstances) on objects that were allocated non-volatile.
And there is GOOD news!  There IS a way to guarantee it!
This is based on section 6.7.3 of the C language specification rationale
Assignment when treated as a primary expression simply returns the value
assigned, and casting that value does not affect the assignment operation.
If you have a non-volatile variable 'identifier' of type 'THING' then
(volatile THING) identifier = 0;
WILL NOT(!!) impose volatile semantics on the assignment.  It says, in
English, "Carry out the assignment normally, and then return the result
of the assignment expression as though that result were volatile."  In
this case the cast is a no-op and unless the compiler writer is just
being nice to you, it can be ignored.
But by using the address-of (&) operation as a primary expression, you
can say
* (volatile THING_T *) &identifier = 0;
Or using the variable reference itself as the primary expression, you
can say
((volatile THING) identifier) = 0;
This WILL impose volatile semantics on the assignment.  These say, in
English, "Make an assignment to memory at this location, as though the
memory at this location contains a volatile variable", or "make an
assignment to this variable as though it were a volatile variable,"
This is so easy to get wrong, so easy for someone cleaning up
'superflous parens' or 'convoluted expressions' to accidentally destroy
without even a second thought, so easy for a saboteur to screw up,
likely to go long undetected, and even when/if found he could credibly
claim it was an accident ....
There's a certain kind of poisonous purity about it. I've rarely seen
such a well-polished, efficient, and smoothly functioning footgun.

@_date: 2016-09-25 10:55:09
@_author: Ray Dillinger 
@_subject: [Cryptography] Yahoo is sued for gross negligence over huge 
What's happening to Yahoo is more or less exactly what we've been
telling businesses will happen if they don't start fixing their crap. I
hope that significant commercial losses will motivate a significant
widespread investment in security, because the consequences of failing
to make that investment go FAR beyond mere business losses.
This election cycle we've seen a huge spike in international
politically-motivated cracking, and that's problematic on a whole
different order.
Up to now it's been common for intra-national powers like political
parties and crooked politicians to penetrate opposing candidates or
parties in an effort to influence elections. But in the last two years
or so, the world has seen more international cracks by nations and
organizations actively trying to subvert electoral processes, and that's
a whole different order of concern.
No matter how sleazy an intranational political cracker is, a political
hack wants there to be a functioning nation to take control of.
International crackers such as North Korea, ISIL, China, the People's
Liberation Front of Lower Slobovia, etc who are fundamentally opposed to
democratic processes in the first place have no such constraint.  Should
Americans trust Russian hacks on the email of their political parties,
or Vladimir Putin's apparent interest in the Trump campaign?  Are the
British quite certain that the Brexit vote wasn't rigged by Mideastern
players deliberately working to undermine the EU?
Democracies across the globe now have sophisticated, persistent, highly
motivated security opponents whose goal is to weaken them in
negotiations, make them militarily vulnerable, or just plain burn them
down. We're not talking about business losses any more, nor even about
the straightforward attempt to take high offices fraudulently.  We're
now talking about hacks by people who see the destruction of nations as
either a primary goal or acceptable collateral damage.
If Yahoo loses a few hundred million in an apparently well-deserved
lawsuit, and financially motivated people around the world start paying
attention to software security?  I couldn't say that would be a bad
thing.  An improved security infrastructure would improve the ability of
nations to defend their political processes, and I for one happen to
like living in a world with some global stability.

@_date: 2016-09-28 08:49:48
@_author: Ray Dillinger 
@_subject: [Cryptography] Bamford: Over Eight Years, 
Apparatus in the World. To What End?
If any single presidency were to more to blame, it would imply that
construction of the surveillance state has significantly lagged
available technology during some other presidency.  I don't think that's
happened.  It hasn't happened in US government since the 9/11 attacks
legitimized the tactic of selling fear in order to buy back freedom, and
it hasn't happened in private-sector businesses since hard drives got
cheap and communications with laughable security got common.
Universal Surveillance, whether the data are gathered from primary
sources, purchased, "shared" from other governments, extorted, or
outright stolen, is the norm of modern government.  Surveillance as a
business model is the default form of today's private industry.

@_date: 2016-09-28 21:15:59
@_author: Ray Dillinger 
@_subject: [Cryptography] Use Linux for its security 
Um, yesterday.
I checked in a code patch in an opensource project shutting down a
possible buffer overrun attack where somebody was doing an unchecked
read-line from a config file into a fixed-size stack buffer.
It's not like the app is in any way significant yet; nobody's got it
installed except a few developers who are trying to get it up to
pre-alpha status.
But it's depressingly common even in "mature" code.  Depressingly
simple.  And not even remotely crypto-related, except in the sense that
the calls which result from running the app with an attacker's
stack-smashing config file is made with the target's privs, while the
target is logged in and therefore while the target's wallets, keyrings,
etc are open and encrypted volumes transparently mounted.
Oh, and if this mistake ever does get made in a crypto application the
attacker gets strpbrk, index, memchr, etc to poke around all the
application's allocated memory including its keys and so-called "secure"
No point attacking the crypto if you can stroll around it....

@_date: 2016-09-29 22:07:07
@_author: Ray Dillinger 
@_subject: [Cryptography] Use Linux for its security 
Given the writ to do a clean slate design, the mandate to design for
security, and a tolerance for performance costs implicit in secure
design, I think we really could fix most of the problem.
Here is how to build a drastically more secure operating system. Isolate
every process in its own address space sharing memory with zero other
processes, handle privileges with a capability system, and use a
microkernel to get as much stuff as possible to NOT run with default or
root privileges.
 There would be no ABI as we understand it.  All interprocess
communication, including what we now think of as calls into runtime
libraries, calls to DLLs or shared objects, and calls into the OS, would
be handled by daemons communicating with other processes via named
pipes.  Most of them (possibly every one that provides an accessible
service) would share zero address space with the kernel itself, or with
the processes requesting their services, or even with each other.
We could use a REAL encrypted filesystem; rather than just emulating an
unencrypted filesystem with the bulk storage  transparently 'mounted'
whenever the machine is running, we could enforce file level access
privileges using crypto.  IE, the filesystem daemon accepts and returns
only ciphertext, you don't have the key to decrypt anything you don't
have the capability to read, and you don't have the key to encrypt
anything you don't have the capability to write.
A real encrypted filesystem could do other things as well; for example
you could have ciphertext-only read capabilities to enable people to do
routine backups etc without ever gaining the ability to read the bits
they're making backups of.
Even without support from an encrypted filesystem, logfiles could
contain blockchain-type sequential hashes to make modification of
logfiles after the fact extraordinarily easy to detect or else damned
hard to do.
It would have very poor performance relative to other OS, in terms of
responsiveness.  Sequential communications rather than ABI calls means
lag.  It would use a lot more memory and CPU to get its work done.  But
it would absolutely shut down most existing types of privilege
escalation and access insecurities, and shut them down HARD.

@_date: 2016-09-30 11:34:03
@_author: Ray Dillinger 
@_subject: [Cryptography] Use Linux for its security 
It was equivalent.  It was fscanf(file, "%s", charpointer).
I did the immediate security fix:  fscanf(filename, "%179s",
charpointer)  where 180 is the hardcoded size of the buffer.  The more
comprehensive design fix with a parser that is treats newline as just
another whitespace character is underway.
Getting rid of gets() is a no-brainer, because gets() CAN'T be used with
buffer length checking.  But fscanf is also a problem, because
the EASIEST way to use it to read a line does no buffer length checking.
That's the wrong way round.  It should be the dangerous use that's
harder and the safer use that's easier.  That way people would (usually,
probably) do the safer thing by habit.
I would REALLY like to see a guarantee in the specification of fscanf
and friends that "%s" when not length-qualified is considered equivalent
to "%255s", or that using them to read more than 64k characters at a
time is an error, or something, just to guarantee that there's a clearly
"safe" amount of buffer to allocate.

@_date: 2017-04-02 11:37:37
@_author: Ray Dillinger 
@_subject: [Cryptography] Regulations of Tempest protections of buildings 
Huh.  I know half a dozen companies in the bay area that have meeting
rooms with such shielding, and at least one public library.  I assume
there are at least hundreds more.  I don't think it even occurs to
anyone that there might be a rule requiring them to ask for a license.
They just go, "Oh, this could be a problem," and assign some EE on their
staff to fix it.
The idea that you'd need someone's permission to build something that
doesn't attack anybody else or violate anyone else's rights is just
plain weird.
Especially when you need it for just plain business purposes.  "Oh,
look, our competitor is driving around with cars that scoop up traffic
from wireless networks, and we already know that WEP is just plain
broken.  Not having done this already is pretty irresponsible and could
get us sued by our shareholders....."  Or even, "Look, radio
interference from lightning etc makes the power supplies in our server
room wear out faster and in extreme cases could put other hardware at
risk. We can save money on our hardware budget by shielding them...."
I mean, seriously.  Just plain ordinary business purposes.
"I have been asked, 'Pray Mr. Babbage, if you put into the machine wrong
figures, will the right answers come out?'  I am not able rightly to
apprehend the kind of confusion of ideas that could provoke such a
question." -- Charles Babbage

@_date: 2017-04-24 15:14:59
@_author: Ray Dillinger 
@_subject: [Cryptography] Internet synthetic traffic generator or ad 
Or the difference from a Distributed Denial Of Service Attack for that

@_date: 2017-04-29 02:25:18
@_author: Ray Dillinger 
@_subject: [Cryptography] 
=?utf-8?q?_AES_Counter_Mode=E2=80=A6?=
Mmmm.  It is my opinion that counter mode is (and stream ciphers in
general are) a mistake.  We've discussed the limitations of stream
ciphers and counter modes before on this list. It allows opponents who
know what is stored at a given location to alter it by bitflipping, in
such a way that the decrypted message will be their chosen text rather
than the text that was originally written there.
There are ways to protect against this (checksums, hashes, etc) but all
of those ways add complexity to the implementation.  Protecting yourself
from bitflip attacks with hashing, in particular, destroys one of the
most valuable properties of counter modes, because with a hash rewriting
any block requires the entire document (or disk sector) to be hashed again.
Complexity is to be avoided where possible in security software:  it's
directly proportional to the number of opportunities you have to make a
A reasonable protection from bitflipping, if you want something that
works like a counter mode, doubles the work of encryption/decryption.
That is to separately encrypt the plaintext and the counter, and let the
ciphertext be the XOR of the result.  To decrypt, one encrypts the
counter, XORs the result with the ciphertext, then decrypts the result
to get the plaintext.
Ideally one ought to encrypt plaintext and counter using uncorrelated
keys, (effectively doubling the key length) but that ought only matter
in cases where the cipher in use is somewhat broken anyway.  AES, and
any other standard ciphers that a knowledgeable person might recommend
in good faith are not broken in that way, or at least not yet.
Doubling the work of encryption/decryption is a compute cost that many
are unwilling to pay, but it achieves the desirable property that a
known plaintext cannot be easily manipulated into a known modified
plaintext, while retaining the desirable properties of counter modes
that they do not require reading and decrypting another block to make
sense of the current block and do not require a document to be rehashed
when a change is made in the content of a single block. (Though you may
need to keep document hashes anyway for other reasons, and in that case
you would still need to rehash...)
Otherwise secure modes don't really allow random-access; they all depend
on the previous block and rewriting any block usually means it's
necessary to rewrite the remainder of the file or re-hash the entire
file (or the disk sector depending on what level the encryption works
on) to protect data integrity.
Entropy tests only measure *anticipated* departures from perfect
entropy, such as statistically correlated bits at given offsets, bits
correlated with anticipated sequences, etc.  Given something that
departs from perfect entropy in any unanticipated way, a given entropy
test is useless.  Possibly worse than useless since it can give a false
sense of security.
If the attacker has partial knowledge (like a restriction to the members
of a 16-bit-enumerable set) of the IV, a departure from perfect entropy
that no entropy test could detect in terms of statistical correlations
is simple for the attacker to discover by checking each possibility.
This would give him 65536 possible values for the un-masked ciphertext,
meaning you wouldn't get very much of even the limited benefit of having
used a counter mode.  Any entropy test written without that specific
knowledge would never detect any departure from perfect entropy on that

@_date: 2017-08-06 21:02:42
@_author: Ray Dillinger 
@_subject: [Cryptography] Finding undocumented opcodes 
This opcode calculates Hamming Weight.  It is useful in providing
an additional dimension for rapid location of data in large multiply-
indexed data structures.  I have synthesized it myself using binary
comparisons for that purpose.

@_date: 2017-08-17 14:58:56
@_author: Ray Dillinger 
@_subject: [Cryptography] 2008 revision of Bitcoin whitepaper - original 
Yes.  The hash matches.  That's definitely it.  And this time I'm
grabbing a copy and putting it in each of the first three places I
looked for it.

@_date: 2017-12-01 17:29:46
@_author: Ray Dillinger 
@_subject: [Cryptography] Cryptocurrency: CME Approved, Coin Paychecks, FED, 
I am concerned.
A lot of money is going into Bitcoin.  This is harmless - even good for
the crypto market, as long as the investors are solvent.
But the current crop aren't. A lot of that money is borrowed. They're
borrowing money to get in.  They're trading futures.  They're buying
bitcoin on their credit cards and paying 20% interest.  They're
mortgaging their houses.  And now banks are borrowing to hold bitcoin in
their portfolios....
A lot of the institutions holding that bitcoin for their customers don't
have anything like the amount of USD on hand that they'd need to cash
the customers out in case something happened that shook their confidence
even a little bit and, say, 10% of them wanted out.
In order to lay hands on that amount of USD, of course they'd have to
sell Bitcoin.
Bitcoin is thinly traded.
A large sell-off in a thinly traded asset would cause a market shock.
The customers would get less money than expected for their coin due to
the market shock.
The market shock, and the cries of anguish from customers who feel that
they were "cheated", might prompt more customers to want to sell.
 Bitcoin was a reaction (If I understand Satoshi's intent correctly) to
the 2008 debt market crash, intended to be something where debt wasn't
But when people are in debt to own something and want to sell the thing
to pay off their debts and the people holding it for them are in debt
and would have to sell it to get the money to pay them, and the people
they'd have to sell it to are banks who have leveraged positions that,
in a falling market, would want to sell it themselves.....  It's
starting to sound a lot like those funny mortgages people were writing
in 2007.

@_date: 2017-12-04 14:17:52
@_author: Ray Dillinger 
@_subject: [Cryptography] Cryptocurrency: CME Approved, Coin Paychecks, FED, 
Since that situation is when "long" owners of the shares are
It most certainly is not;  In fact many of those most in need of the
warning will instead see the downward movement of the price caused by
the short sell, and take it as a signal to buy more.  Because that's
the way they (don't) think.
The evidence supports the notion that most investors make up reasons
for their decisions long after the decisions are made, thinking backward
instead of forward.  Those whose decision is "buy", will believe after
the fact that the price shift downward was their reason for deciding that.
Market information becomes accurate, not because the investors know what
they're doing, but because the performance/valuation metrics of the
companies themselves continually redistributes wealth toward an accurate
distribution in the long run, regardless of how the individual investors
allocate their assets. The market information would be nearly as
accurate if the investors allocated money randomly, as long as they
didn't change the allocations much more often than they do now.

@_date: 2017-12-16 16:02:48
@_author: Ray Dillinger 
@_subject: [Cryptography] High volume thermal entropy from an iPhone 
Have we any reason to suspect that whatever amount of thermal noise may
be present in dark frames, is not also present in photographs of actual
scenes with light and subjects?
If the question at issue is whether we expect thermal noise to randomly
flip some number of low-bits in the dark frames, I can't imagine a good
reason why that would not also happen to a camera in the course of its
intended use.

@_date: 2017-12-16 17:54:54
@_author: Ray Dillinger 
@_subject: [Cryptography] High volume thermal entropy from an iPhone 
In some accounts, I suppose that I may be considered an expert....
but I don't really consider myself to be one.
That said there are bits and pieces of code I can write and be quite
confident in.  Unfortunately, most of them aren't the kind of thing
you'd want in a standard.
I can write deterministic pseudo-random number generators that are
reasonably efficient and whose resistance to cryptanalysis I'm as
confident of as I am in the more widely published generators.  They are,
however, "reasonably" efficient rather than "very" efficient - the math
is cool, the coding is fun, and I get to make generators that have
particular, peculiar properties that for whatever reason I happen to
want in a particular application (Reversibility, variable state size,
parallel keying, other esoterica).  But they're not going to win any
design competitions or get adopted in any standards, and I well know it,
and I don't expect anyone who's not me to trust them.
Of course a pseudo-random number generator can be used as a mask
generator for a stream cipher.  But friends don't let friends use stream
ciphers.  Seriously, just don't.
A mildly peculiar cipher I created when I was thinking about everything
that's wrong with stream ciphers used a reduced Feistel cipher - just
four rounds, which is the minimum for the security proof - with a new
key for every block, drawn from the next bits of the stream-cipher mask.
 This is a construction that might actually work with an efficient
stream generator and sufficient key agility; if you can generate a
128-bit key and use it to encrypt a 512-bit block with a reduced (FAST!)
block cipher, then you get what might amount to a two-to-one discount on
encrypting the 512 bit block because speed from round reduction, and
have to pay some of it back (hopefully less than you gained) for
generating a new key and rekeying for the next block. I'm still toying
with this.  It's about the only idea I've had that might actually be a
good one worth using.
I could use one of my deterministic PRNGs for a very good cryptographic
hash. A couple of the PRNG designs have arbitrary-length keys/states; I
could simply take the document to be hashed - all of it - as the state,
produce and throw away several times the document's length in bits for
the cascading side effects in the state, and then produce the hash. This
approach absolutely eiminates length extension attacks where a prefix or
subsequence collision can be leveraged into longer or other collisions.
But it also absolutely eliminates important properties that a good
number of people want in a hashing algorithm, such as a small constant
memory footprint and a streaming mode that works on arbitrary-length
streams as they arrive, without even knowing how much data is in them at
the outset. And it's slower than most hashes and there's no possible way
to implement it in hardware, which leaves it completely out of the
running for most design standards.
I've implemented Feistel ciphers whose security I'm confident of, but
they're brutish.  They're mostly an exercise in "find the best efficient
round function I know how to find, then account for my pessimism about
people figuring out more math about round functions than I know, by
iterating them for so many rounds that the cipher becomes ludicrously
inefficient."  I know a lot about Feistel round functions but there are
a lot of people who know so much more that I can't be confident that
none of them could break it.
I've coded software simulations of WWII and cold-war rotor cipher
machines - and a few hypothetical machines that nobody ever built - just
for fun.
And I've created some real peculiarities like a "timelock" cipher that
allows a document (or stream) to be encrypted, "reasonably" efficiently
(about an RSA block's worth of calculation, which is many times slower
than a block cipher), in a way that will take some arbitrary,
predetermined amount of time per block to decrypt - and you can give
different people keys with different work factors.  It's based on
Rivest's Timelock, so credit where credit's due.
Another peculiarity, this one for an underhanded crypto contest, was a
"polymorphic" cipher in which, with some work, one could create
gimmicked keys, otherwise quite secure and normally usable although
ridiculously long for keys, that would encrypt twenty to thirty
preselected messages into preselected ciphertexts (It's an exponential
curve; 20 preselects is "instant" and happens when you take your finger
off the return key.  A key for 30 preselects took an all-night run on an
8-core machine to find). The minimum key length of a gimmicked key was a
multiple of the length of the longest message you wanted to work into
the gimmick. As a cipher, it was brutally inefficient - took 1800% or
more of the encryption/decryption time of a standard cipher as I recall.
But you know what I have absolutely no confidence in my ability to do,
however brutally or inefficiently?  What I have never even had the
temerity to experiment with?
There is absolutely no way I would ever trust code I write myself to do
key management.  Key management is hard.

@_date: 2017-12-17 21:39:36
@_author: Ray Dillinger 
@_subject: [Cryptography] How good random number generator is the human 
Pretty terrible.  When they asked humans for random numbers distributed
between 0 and 16, put one number in each byte, and compressed using
gzip, the output was 2.76 bits per byte input rather than the nominal 4
bits per byte input.  This simple empirical test means that an humble
statistical tool was able to eliminate 2.48 bits of redundant
information from every human-generated "random byte."
That's biases, repetitions, and patterns detectable by an off-the-shelf
compression program that DOES NOT detect all possible (nor even all
likely) biases, patterns, and repetitions.  So the 5.52 bits observed is
a maximum estimate. If you build a more exhaustive test, you can almost
certainly demonstrate an upperbound smaller than that.  Hell, the
current version of gzip likely demonstrates an upperbound smaller than
that; somebody ought to repeat the experiment.

@_date: 2017-12-19 16:15:29
@_author: Ray Dillinger 
@_subject: [Cryptography] Rubber-hose resistance? 
You really want unguessable credentials you can hold in your head?  You
can do it; it just takes some effort.
You can easily memorize a nonsense phrase that talks about, eg, the
fictitious exports of a fictitious city near a fictitious landmark,
inhabited by fictitious animals, where the fictitious ethnicity
inhabitants eat fictitious food.... Or about something from an entirely
different category that involves a similar number of fictitious nouns or
verbs or adjectives....  and make up a good original pseudo-word for
each of these things and arrange the whole so it rhymes. Or maybe so it
scans like a surreal passage from your favorite cyberpunk author or a
conspiracy theorist on an acid trip.  Or so it's funny. Or something,
anyway, that makes it easy for you to remember.
Got something over a hundred characters that you can say in eight
seconds or less?  That's easy for your audio loop memory to hold as a
single chunk.  That's probably good.
You know a dozen different desktop ciphers that have popular methods
for constructing keys from words, just because you've read books on
the history of crypto.  You can use seven-or-so of the made-up words in
the phrase as keywords.
Now come up with a mnemonic that links at least one of the desktop
ciphers and at least one of the words with each of the places you have
to log in.  If your mnemonic doesn't guarantee uniqueness, you should
also prepend the name of the place to each keyword when constructing
your cipher keys.
Now encrypt your phrase according to the derived instructions, do it
again to make sure you get the same ciphertext on the second try, do a
test decryption to make sure you get the original phrase as plaintext,
and when you're sure, change your password or key to the ciphertext.
Then you'll want to throw the paper you worked it out on into a blender
with a quart of water and puree it (or whatever; office shredders don't
secure anything any more).
That gives you enough underlying (and shared) structure to keep it
entirely in your head if you want to have the credentials not recorded
on anything.  But the structure (and sharing of structure) is made
sufficiently obscure that password guessing programs are never going to
find it.  And when you do work out those credentials, you are entering
them into your password manager or keyring only.  You never have to
communicate them to another human being, which drastically reduces the
odds that anybody is going to crack your desktop ciphers and figure out
what you're doing.
It's hard enough to do that you'll want to keep the times you have to
actually re-derive the passwords rare, because it'll take a half-hour
per password.  But if you really want to go "clean" for border
crossings, you can do it with good solid keys/passwords stored
completely inside your head.

@_date: 2017-12-20 15:39:51
@_author: Ray Dillinger 
@_subject: [Cryptography] Rubber-hose resistance? 
Many filesystems, on catching sight of a bunch of logical sectors all
filled with the same content, will happily store all of them on the
same physical space.
For several reasons this is especially true with zero; the filesystem
may be keeping a list of sector ranges that are zero, and just update
one of the ranges to include the sector you wrote, or add a new range if
neither of the adjacent sectors are already part of an existing range.
Obviously, when this happens the information is still there on the
physical media, and it doesn't take a national lab level attack to get
it out; it takes somebody poking the zero-sectors list.
Random bits are incompressible, and therefore no file manager is going
to discover that it can double them up or compress them to avoid the
need to overwrite the stuff you intend should be in fact overwritten.

@_date: 2017-12-22 13:45:07
@_author: Ray Dillinger 
@_subject: [Cryptography] Bitcoin, fork you very much 
This would not work.
Modified blocks would not have a hash proof-of-work.
That is, if you put the original hash in a modified block, people
will hash the block, say, "hey that doesn't match", and reject it.
If you hash the modified block, people will hash the block, look
at the hash, say "hey, that hash isn't below the work threshold",
and reject it.
The alternative of course is to modify the block, do some really
stupid amount of hashing on a computer made of unbelievium, find a
nonce to stick into the block that makes the modified block hash
low enough, and send it on.  That would be Hard.

@_date: 2017-12-23 13:38:07
@_author: Ray Dillinger 
@_subject: [Cryptography] paragraph with expected frequencies 
Actually the longer it is the easier it gets, because the "law of large
numbers" in statistics makes the frequency counts more accurate.
An alphabetic substitution is a very simple program; one you've probably
already written to produce the puzzle.
Letting your daughter edit the key, run it, see the changed message, and
edit the key again, interactively, would be fun, a rapid way for her to
get a feel for substitution ciphers, and you could let her look at a
whole page of text while she does it.

@_date: 2017-12-24 14:01:53
@_author: Ray Dillinger 
@_subject: [Cryptography] Bitcoin, fork you very much 
A lot of the rules of Bitcoin come from the simple fact that people
agree that those are the rules of Bitcoin.  There are thousands of nodes
out there checking every would-be new block, and if it violates one of
the rules that people have agreed on, that node dumps the block on the
floor and refuses to have anything to do with it.  When lots of nodes
dump your new block on the floor and refuse to have anything to do with
it, it doesn't get added to the block chain.  Or, if you have made a
special node that adds it to the block chain, all the other nodes drop
that version of the block chain on the floor and refuse to have anything
to do with it.
The finite limit on coins is a feature that results from just such a
rule.  Everybody has agreed that it is okay for someone who successfully
creates a block to include a special transaction that creates coins at
an address which they can later spend.  But the number of coins they are
allowed to create with each block is being halved every so often as time
goes on.  That's not a special cryptography procedure, so much as just a
rule about what blocks people will and won't accept as a legitimate part
of the Bitcoin block chain.
Each period between halvings lasts 210000 blocks.  Each block during
the first period, 50 coins were created.  That was half the number of
coins that would have been created if there were 100 per block, which
I'm pointing out because that would have been 21 million coins, an
important number.
During the period before the first reward halving, half of 21 million
coins were created.  Then there was a period where half as many as that
were created, after which we had 3/4 of 21 million.  Now the reward has
halved again, and we are in a third period.  During this period one
eighth of 21 million coins will be created.  At the end of this period
we will have 7/8 of 21 million.  And so on.  We approach 21 million
coins by Zeno's limit, not because of any special cryptological property
or mysterious thing that you need advanced knowledge to understand, but
just because the rules of the protocol establish a procedure that
approaches it.

@_date: 2017-02-05 12:05:23
@_author: Ray Dillinger 
@_subject: [Cryptography] Why is a short HMAC key zero-padded instead of 
In the case of longer keys, I can argue in favor of hashing, but only on
the basis of oddball protocols where there is key derivation from
multiple sources with different origins or security properties, and
that's properly protocols rather than primitives.
In the case of shorter keys, zero-padding and hashing have equivalent
security properties as far as I can see; if a hashing functionality is
provided to cope with longer keys, then hashing should be preferred on
the basis of code simplicity. Secure code minimizes the number of choice
points and different execution paths.
On the other hand if a fixed-size buffer is zeroed before the key is
written to it, then truncation of longer keys and zero-padding of
shorter keys are also the natural consequence of a simplest code path.
One might argue in favor of hashing longer keys on security grounds and
zero-padding shorter keys on efficiency grounds, but I don't buy
complicating the execution path as a security choice.  This would IMO be
poorer practice than either of the cases above.
But, IMO all three of these are the wrong thing to do on consistency
grounds. It's inconsistent because it means that the same operation -
adding a bit to the key size - doesn't mean the same thing at different
key sizes.  If a bunch of different longer-than-standard keys are
members of equivalent-key groups, then a primitive simply oughtn't
accept keys that size.  And if a short input results in a long key that
has a short work factor, then hashing has disguised a problem without
fixing it.
That is, if adding the 128th bit to a key increases the attacker's work
by a factor of two, then so should adding the 65536th bit, and if it
doesn't then the primitive ought not be accepting 65536-bit keys.  When
we start talking about hashing or truncating to get a particular key
length, there can be good reasons to do that but any such reason we
invoke has to be considered in the context of a particular usage, and
therefore moves from primitive design to protocol design.

@_date: 2017-02-10 10:42:53
@_author: Ray Dillinger 
@_subject: [Cryptography] A little history, 
This in fact stems from one of Bitcoin's central problems: scalability.
The distributed block chain system is simply inadequate to ever become a
widely-used payment system.  It can be used for interbank settlements
(ie, coinbase and empty-gox make a single bitcoin transaction when their
respective userbases have created a net movement of so many coins), but
if that's how it's used, the consumer is no better off than when using
any other bank-issued currency or bank-mediated payment system.
Also, bitcoin's privacy provisions are inferior in most ways to credit
card transactions.  Pretty much anybody can analyze the block chain but
bankers reveal that information only to people who are recognized by law
as having "right to know."  You don't have to like the way the right-
to-know determination is made with no control by the consumers actually
using the system, but making the information public doesn't really
enhance your privacy.
Bitcoin may allow privacy in theory, but practically nobody's got good
enough opsec to actually receive and spend bitcoin without every
transaction being traceable. If you follow the rules you'd have to
follow to be untraceable, you wind up in a completely closed subset of
the bitcoin economy populated exclusively by other people following
the same rules.  And, bluntly speaking, the cardinality of that subset
is zero.  There is nobody you can trade with whose opsec will allow
you to keep your privacy.
So you have a choice between bank-issued currencies and bank-settled
transactions, which allow you *some* privacy from *some* people, and
block chain transactions which don't.  And that's when you can actually
make block chain transactions, and there simply isn't enough room in
the block chain for a consumer-level application do that.

@_date: 2017-02-15 11:57:29
@_author: Ray Dillinger 
@_subject: [Cryptography] detention and/or seizure if you don't give your 
4) Suppose you mail your device home (or ship it) before going to the
5) Suppose you buy a simple 'dumb' cell phone and don't program it with
any contact numbers.

@_date: 2017-02-20 13:24:52
@_author: Ray Dillinger 
@_subject: [Cryptography] HSMs or Intel SGX? Which is harder to hack? 
They're a near-necessity in training modern neural networks.
How the heck are they useful for phone fraud?
Just because something doesn't do what you planned it to do doesn't mean
it's useless.  -- Thomas Edison
When in doubt, use brute force. --Ken Thompson

@_date: 2017-02-20 13:35:43
@_author: Ray Dillinger 
@_subject: [Cryptography] HSMs or Intel SGX? Which is harder to hack? 
I am being foolish this morning.  There was a discussion about
poolGSM and I replied about the pooled GPU servers they use at
render farms and now in AI.  Apologies for any confusion caused
by my mistake.

@_date: 2017-02-27 11:36:19
@_author: Ray Dillinger 
@_subject: [Cryptography] Schneier's Internet Security Agency - bad idea 
I would not expect an IoT device to even be *able* to connect to
the Internet until I configure it with the key for my house-area
network, and with the certificate it needs to communicate with
the proxy server to get packets across my outbound firewall.  The
fact that many IoT devices expect this is laughable.  The firewall
on outgoing packets tells me what devices I need to disconnect
and destroy.
Perniciously, it is the case that some devices, especially cameras
and printers, which are not marketed as Internet-enabled, still
attempt to send outbound packets.  Many routers which are configured
for local network only still attempt to send outbound packets onto
the wide open Internet.  My desktop mill's goddamned CNC controller
made a DNS request the instant I plugged an ethernet cable into it
to transfer G-code to it! It got replaced with an arduino board.
Even if I wanted an Internet-enabled device, and even if it *had*
the wifi and proxy info to connect from anywhere in my house, I
wouldn't want it to attempt to connect to the Internet before I
told it what certificates it should use, exactly where to connect
to, and exactly what certificates its only valid connection partners
have.  If it connects to anything else, or communicates with
anything that does not present that certificate, or communicates
with anything at all using any other certificate besides the one I
give it or communicates at all via unsecured protocols, then the
busted pieces of it go into the trash.
Until somebody starts selling devices whose architecture implements
that standard of behavior, I'm not buying IoT devices.  But sometimes
I discover that I have bought one unintentionally.  That's one of
the reasons I keep a fire axe handy.

@_date: 2017-02-28 14:13:46
@_author: Ray Dillinger 
@_subject: [Cryptography] jammers, nor not 
In the early 90s when cell phones started to invade public spaces, and
people started being unmindful of the rudeness of being loud in places
whose patrons and purposes needed quiet,  several theaters and libraries
that were remodeling anyway quietly and cheaply put up conductive
layers.  I know of at least two, anyway.  In most cases not true Faraday
cages, but certainly enough to disrupt cell service. A layer of foil
backing the carpet pads, wall paneling, and ceiling sound tiles isn't
perfect, but with a bit of attention to contact at the edges it goes a
long way, especially when the doors and ducts are also metal.
However, there were lawsuits about people with pagers related to public
safety or job performance being impossible to contact, and libraries in
particular got sold on wireless Internet connectivity, and people
started texting more and talking loudly less in quiet places.  The
impulse or willingness to expend effort faded after a while.
About Hollywood, need I remind you of the ongoing difficulties between
paparazzi and the celebrities on whom they prey?  A lot of the celebs
have a good housing budget and a genuine practical threat model. That's
not just tinfoil hats there.  They also have good reasons to put safe
rooms in the basements of their houses.
"Ideas won't keep; something must be done about them." - Alfred North

@_date: 2017-02-28 14:27:48
@_author: Ray Dillinger 
@_subject: [Cryptography] jammers, nor not 
That is not the case, at least for private citizens/companies.
I have been in several meetings that were held in such rooms.
There's a particular way to interpret the public meetings
act that could be construed to make such a requirement
on rooms used by public officials where blocking, eg, radio
and TV live coverage would be "unreasonable."  But I am no
attorney, and haven't heard of any actual cases testing
that hypothesis.

@_date: 2017-02-28 14:52:21
@_author: Ray Dillinger 
@_subject: [Cryptography] XTS mode IV generation 
It is really no different from the risk of being able to observe the
same disk locations over time when XTS is used in the traditional full
disk encryption case.  Early disk encryption systems used CTR, which
failed catastrophically if an attacker ever saw two different
ciphertexts at the same location.  XTS and related systems mitigate that
difficulty by properly encrypting the data rather than just masking it
with an encrypted counter.
However, that is not at all the same as saying the ability to observe
the same disk locations over time does not weaken the system.
XTS is still subject to replay attacks; someone can revert your changes
at chosen locations by replacing blocks on your disk with old values.
In a copy-on-write system, they don't have to change the (encrypted)
block pointers to point at the old block; they have to break the
copy-on-write logic by putting the old block image in place at the
block's current location (where the block pointers they can't get at are
This is why ZFS keeps a Merkle tree of checksums. The only way for
a Merkle-tree checked filesystem to be reverted is for every last
sector to be reverted. There is no reversion at selected locations.

@_date: 2017-01-02 12:13:32
@_author: Ray Dillinger 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
I thought ripple signalling was used for streetlights everywhere, up
to the advent of LED streetlights at least.
If you're standing in the right place when rippled streetlights go
on, you can just barely see that they're coming on in a row, at
increments something like 1/20 second apart due to ripple signalling.
I thought it was that way everywhere because it's the cheapest/
easiest way to get a relatively gradual ramp-up of a power demand
that would otherwise bugger things up by coming on all at once.
Of course the new LED streetlights are using something else; they
come on all at once.  Some of them even have motion or IR sensors
and only come on when people are actually walking nearby.

@_date: 2017-01-02 12:15:59
@_author: Ray Dillinger 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
It's a valid threat.  If someone turned *off* the power of a large
part of a city, all at once, I think most of the transformers at
the substations would explode.

@_date: 2017-01-05 18:21:12
@_author: Ray Dillinger 
@_subject: [Cryptography] Internet of things - can we secure it by going 
If we put no more software and complexity in these devices than they
actually need, don't we suppose that they could be made secure?
Web interfaces on these devices only need to be able to understand eight
or so different HTML tags.  Skip implementing the rest.  A device that
never executes anything downloaded, ever, can't easily be recruited into
a botnet. Command lines on these devices (if they expose command lines
at all) should only understand three or four different commands.  They
should not even have access to utilities like "ls" and "cat" and "mv"
and "copy", let alone interpret pipes and things.  I mean, is there an
explicit reason, for each and every one of those tools, why it's needed
for device configuration?.  Because device configuration is the only
thing these command lines exist for.
If someone breaks into a thermostat and can install shell scripts on it
- what the hell was a thermostat doing with a command shell capable of
running scripts?  If someone can use it for a reflector to poke around
your network - what the hell did a thermostat need with a repertoire of
utilities like 'mount' and 'rlogin' and whatever else would get used to
do that?
The complexities of protocol conformance are sometimes beyond the
capabilities of simple IoT devices, and secure implementations for them
are, well, no better than secure implementations for other platforms -
you're still hosed if you haven't got monthly updates.
But the manufacturers of these devices don't DO monthly updates. They
want to forget about a device as soon as it's sold.  It's not a contract
or a subscription, it's just inventory.  Their goto model for service or
updates is "get a new one," whether that's by another sale or a product
recall or a guarantee fulfillment.
So we need to build devices which need security updates no more often
than the length of time a typical customer goes before getting a new one.
That's not so difficult as the complex protocols we're using on desktops
makes it.  These are simple devices and the protocols and capabilities
they actually need are also very simple.  They could use protocols with
two moving parts instead of fifty.   And I think elementally simple
protocols, running on simple single-tasking machines can in fact be
implemented securely enough not to need updates more than, say, once in
six years on average.

@_date: 2017-01-09 10:51:45
@_author: Ray Dillinger 
@_subject: [Cryptography] Guess what got declassified this morning? (Russia 
US Intelligence report on Russian hack of the DNC and
Russian intent to influence the election.
It spends a fair amount of time on Kremlin propaganda
efforts (attributing motive).  The technical stuff
comes mostly in the last third of the report.

@_date: 2017-01-12 13:21:37
@_author: Ray Dillinger 
@_subject: [Cryptography] nytimes.com switches to https 
A standard I've seen applied elsewhere is "a power of two, optionally
multiplied by three."  It gives the usual sequence of "Computer Science
round numbers"  ie, 1,2,3,4,6,8,12,16,24,32,48,64.....
Maximum overhead is 50%.  I think that's in the "reasonable" range for
most purposes.
But size-padding schemes aren't enough to narrow down the identity
of the article accessed, unless the request/response is condensed into
a single entity with all text and graphics embedded in one response.
There's an obvious difference, to a traffic analyst, between a page that
triggers further requests for 20 different images and a page that
triggers further requests for 21 different images.  If you add in
image sizes, whether padded to a "round number" or not, the whole thing
is quite transparent.
So what you need is a server-side plugin that packages all the stuff
that's supposed to be on a page into a single http response, and a
browser plugin that unpacks these mini-archives and displays them
correctly. I think this is standard methodology for IPN protocol,
because it minimizes roundtrips and IPN is all about minimizing
roundtrips.  But there are no web browsers and damned few servers
that speak IPN, and so far it hasn't been applied to IP networks.

@_date: 2017-01-15 10:31:38
@_author: Ray Dillinger 
@_subject: [Cryptography] Cryptocurrency Exchange without a trusted third 
"Trusted" means "Someone who can screw you over by acting in bad faith."
People prefer hash chain solutions because that would mean that a bunch
of people (more proportionate to time since transaction they're
screwing with)  would have to act in bad faith to screw people over,
and the maximum practical size for a conspiracy is rapidly exceeded.
People prefer block chain solutions because that sharply limits the time
window during which someone acting in bad faith could screw them over,
and imposes significant (possibly insurmountable in practice) hardware
requirements and expense on bad-faith actions.
Block chains mean that someone cannot act in bad faith without making a
substantial investment that is not justified by the rewards of the
action.  Block chains, in fact, mean that the "trusted" party cannot
even be INDUCED OR COMPELLED to act in bad faith by blackmail, bribery,
extortion, or force of law.
Yes, the block chain is a "distributed trusted third party."  No, it
isn't free.  Unlike most "trusted" parties, however, there is a good
reason to believe that it can be trusted.

@_date: 2017-01-15 10:42:21
@_author: Ray Dillinger 
@_subject: [Cryptography] 33C3: cash :-) attacks ! 
How viable is this idea anyway?  Has anyone ported Linux to, say,
the Samsung Galaxy hardware, without sticking it on top of the
Kernel/JVM that underlies Android?
I can handle not having Android applications available if I can
compile OS code from Linux distros.

@_date: 2017-01-15 12:59:27
@_author: Ray Dillinger 
@_subject: [Cryptography] ZK meeting scheduling protocol? 
You mentioned zero-knowledge proofs but I do not see one in your
protocol requirements.  I'm assuming there isn't a ZKP going on
here. If I'm wrong, then who are Peggy and Victor?  What is
Peggy trying to prove and what is Victor trying to verify?
Otherwise this looks like the classic "Byzantine Generals"
problem, with additional security/privacy requirements.  The
additional security/privacy requirements look a lot like multi-
party Diffie-Hellman key exchange, or possibly look like they
might be accomplished using a Dining Cryptographers' Network.
At first blush I'd say something like "multi-party Diffie-
Hellman exchange or Byzantine-Generals protocol, either with
message blinding, or carried out over dining cryptographers'
network," but that's before analyzing the problem carefully,
and the communications overhead for either of those over a
DCN would be pretty cumbersome.
So start by reading up on those classic protocols.  But first
get a firm idea of the requirements in your mind so you can tell
when and how those protocols do and don't match them.
Now.  When attempting protocol design, you start with three
basic questions.
Let's consider.  First, what are your objectives?
1.  Before the protocol, Alice, Bob, Carol .... Xavier,
    Yvette, Zebulon want to schedule something but each
    has constraints on the time slots available during
    which they can schedule it.
2.  After the protocol, either they have determined some
    common time slot that doesn't violate any of their
    constraints (protocol success) or have determined
    that no such time slot exists (protocol failure).
Second, what are your security requirements?
1.  If anyone, including the participants, learns anything
    specific about the schedule constraints of any
    participant, beyond the fact that all have the agreed
    timeslot available or that no such time slot exists,
    then the protocol violates the privacy requirement.
2.  If a nonparticipant can determine what time slot has
    been agreed on, or determine that no such time slot
    exists, then the protocol violates the confidentiality
    requirement.
3.  If a nonparticipant can influence or choose which of
    several possible time slots are chosen, (for example
    by redirecting or resequencing protocol messages
    between participants) then the protocol violates the
    authenticity requirement.
4.  If any two participants can be led to have different
    beliefs about whether a common time slot is available or
    about which that time slot is, then the protocol violates
    the consistency requirement.
Third, what's your threat model?  ie, what are the capabilities
of the attackers trying to cause the above failures?  All the
attackers are assumed to have public keys but no private keys
of all participants.
1.  Eve (eavesdropper) is assumed to be present and will
    monitor the channels hoping to learn things about the
    participants' schedules, including but not limited to
    the event they're trying to schedule in this protocol.
2.  Eve's Big Sister Nessa (traffic and metadata analyst)
    will be monitoring traffic patterns and will learn
    something about schedules violating confidentiality if
    the sequence or direction of messages depends on any
    participant's (or any subset of participants') match
    or mismatch with a proposed schedule.
3.  Mallory (who can also intercept, change, block, redirect,
    or fake messages), is also a fairly constant threat.
    There's really no way to prevent Mallory from performing
    a Denial-of-Service, but that's essentially the same as
    saying you have to be able to communicate at all.
    Mallory must never get the opportunity to present an
    arbitrary string of bits that will get signed by a
    participant's private key, (not even in a protocol that
    will be aborted due to later detection of protocol
    violation) and no message from any participant, including
    messages exchanged in a previous or aborted iteration
    of the protocol, should be meaningful if Mallory
    misdirects it to a different participant or replays it
    to its original recipient.  Either of these conditions
    can lead to virtually any security violation.
4.  Sybil (malefactor pretending to be several participants)
    appears to be excluded because all participants have
    each others' private keys, and if they're scheduling
    meetings and know which private key corresponds to which
    participant they'd notice if the relation isn't 1-to-1.
    This is important because Sibyl can easily mount an
    undetectable denial-of-service in any agreement protocol.
5.  Trent (who normally does something beneficial but can
    screw people over by acting in bad faith) appears to be
    excluded from consideration here because if a Trent were
    tolerable this protocol would be trivial and you wouldn't
    even be thinking about it.
Those are the big five; if there are any other relevant
threat models for this protocol who are they?

@_date: 2017-01-23 11:30:35
@_author: Ray Dillinger 
@_subject: [Cryptography] Oracle discovers the 1990s in crypto 
FWIW, the 'diff' utility as frequently deployed in software
shops ignores whitespace.  A change to the code modifying
whitespace only would often pass without a 'blip' on change
control.  Someone who has malicious code could modify its
source *and* the source of some innocuous program to have
the same hash, in such a way that the innocuous source would
not register as "changed" to a set of tools that might be
in use at the shop involved.

@_date: 2017-01-27 12:08:58
@_author: Ray Dillinger 
@_subject: [Cryptography] HSM's to be required for Code Signing 
Yeah, I'm still chuckling over the use of "secure" and "cloud based"
in the same sentence.  It's going to remain utter nonsense no matter
what people pretend.  "Cloud" means you have given up control of
your security.
In fact for the level of security needed for key signing keys, I'd
be shaking my head doubtfully over a combination of "secure" and
"network connected."

@_date: 2017-01-30 11:10:05
@_author: Ray Dillinger 
@_subject: [Cryptography] Is Factoring of large RSA moduli using 
For anyone who misses it this is a dig on the Trump administration and
the ridiculous "Alternative Facts" line that recently dropped out of his
press secretary.  Right after the line nobody else seems to be howling
about, where "the press should just be quiet."

@_date: 2017-01-31 11:25:08
@_author: Ray Dillinger 
@_subject: [Cryptography] Great IoS quote from LCA 2017 
Okay, I recognize that device, and there is actually a good reason why
it's a cloud device.  In fact, if it's the one I'm thinking of, I'm one
of the people who recommended networking it.  And contrary to your
earlier point, the data it collects is in fact used for the primary
purpose of making the device better for the USERS, not just for the
sellers or the advertisers.
Neural networks (and other machine learning stuff) are one of the things
I do when I'm not doing security or software QA.  I didn't know she had
gone public with that device, but I spoke with her about neural-net
technical issues during its development.
It's applying neural networks to control the devices, with feedback from
embedded conductance, pressure, and temperature sensors, using "presets"
from training different utility functions - maximizing orgasms per hour,
maximizing total time spent having orgasms, and keeping someone within
an estimated ten seconds of having one without quite going there.  Those
three settings, and presumably some others, are things that it's
supposed to learn how to effectively do for individual users, although
the process is very noisy and nonlinear.
Thing about that is, training neural networks needs data.  The more
data, the better.  Training itself via feedback for a single user is
obviously the way to get an effective personalized device for that user;
but it takes an individual user a LONG TIME to generate that much data.
When she spoke to me she was complaining about a learning curve where
the device took *weeks* of semi-regular use to learn to optimize its
interactions with a particular person, and apparently the people who had
volunteered to try out the completely untrained devices (to establish
their basic as-sold presets) went through several months of depressingly
mediocre or distractingly random performance before things started to
get better. "The first three months they were swearing at them - but
then six months after that they were swearing by them,"  is how she put
Obviously a personalized device that takes 9 months to learn the basic
per-user customization isn't terribly marketable, but starting with
basic initial training from the volunteers' data it was already down to
just a few weeks by the time I spoke with her.
The obvious thing to do was to make it a "cloud" device, in order to
have more data available to train with.  This is directly contributing
to its effectiveness and usefulness, the same way Google has to log all
those road hours recording all that data to train with, before their
self-driving cars are ready for prime time.  Use of "big data" should
enable much more effective out-of-the-box settings and much more rapid
self-customization for individual users, at least to the extent that
their reflexes and responses are similar to those of other users in the
training data set.
Ethically that data ought to be statistical-only; anonymized data would
be just as effective for neural-net training purposes.  Practically
speaking there are trust issues, because the users of IoT devices never
know what data is gathered.
The social-network, leaderboards, and program-sharing are a twist I
hadn't yet heard about yet though. From a technical standpoint they're
an absolutely great idea for gathering the needed amount of data,
establishing baselines and comparisons.  From a privacy standpoint ...
???   !!!     I guess the users are doing this voluntarily....
Program-sharing will allow the device to "see" exactly what works for
its individual user, in terms of a performance that doesn't need
training to deliver, without taking a chance on annoying the user by
doing something suboptimal. At least if something is suboptimal then
some other user can be blamed for it instead of people deciding before
it has had time to learn customized responses that the device is no
good. Even better, it allows recording the responses of MANY users to
the same programs, permitting baseline comparisons that will accurately
help predict and classify features of the user's profile.
"Security is an illusion.  It's what keeps you from curling up into a
little ball and screaming until the universe inevitably betrays and eats
you." -- Hunter Cressall

@_date: 2017-07-02 12:01:45
@_author: Ray Dillinger 
@_subject: [Cryptography] Satoshi's Trump Card 
What puzzles me about these discussions is that people assume, with no
evidence and apparently for no reason, that Satoshi would choose to play
his (or her, or their) so-called "Trump Card" in some way that agrees
with their own particular agenda.
Satoshi, as I recall, was very responsive to design feedback. When Hal
pointed out potential problems with the scripting language, the
scripting language got changed, immediately.  I remember saying to both
of them I thought they didn't need to cut that much from the scripting
language (ie, that they could afford to include more of FORTH). But then
Hal outlined a DoS based on the cost of key checking (which I had
underestimated, badly) and all the flow-of-control stuff stayed cut.
The point of this is, right now, Satoshi would be looking at Bitcoin's
current problems and evaluating the available options.  There's no
reason to think that a prejudice in favor of "NO CHANGE" would be
considered to be the correct response.  The Satoshi I remember would be
evaluating the alternatives pretty much objectively, and relating them
to the current problems rather than the problems faced by the original
design.  And might choose either, based on its merits.  NIH wasn't part
of how he (or she, or they) worked.
And anyway, the use of this particular so-called "Trump Card" in this
particular way wouldn't have the effect the OP thinks it would have.
They are considering the incentives of the users, when in fact the fate
of a block chain is determined by the incentives of the miners.
The fact that the interests of miners and users are not necessarily
aligned, I have always regarded as one of Bitcoin's flaws.  It opens the
potential for mining that is completely useless to the users, or the
possibility of giving miners incentives to cooperate with DoS attacks
against the users, etc. The block chain records many blocks that come
from miners apparently indifferent or malicious to the users' interests.
They are empty blocks more than five minutes after the previous block,
at times when transactions were averaging more than thirty a minute.

@_date: 2017-07-02 12:15:59
@_author: Ray Dillinger 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Arc4random should not be mistaken for a CSPRNG.  It's a good PRNG, but
at this point there are enough attacks on it that it's not really good
enough for cryptography anymore.  So it should not be certified as
cryptographically secure - there is no "pretending" about it, and it is
not "gaming" the auditing process.
On the other hand, given some particular permutation of 256 elements,
its stream of output is entirely repeatable.  So it's fine as a PRNG for
repeatable sequences.

@_date: 2017-07-03 09:59:39
@_author: Ray Dillinger 
@_subject: [Cryptography] OpenSSL CSPRNG work 
This ... okay, I get it, because you want to leverage code that
was written to call a generator that turned out not to be secure.
But this still bothers me.  It takes a name that has a *known*
meaning, and uses it to mean something else.  It seems almost
like lying.
In this particular case, it is badly misleading.  I would absolutely
never have called a function named arc4random expecting to get a
cryptographic number generator. And I would never have looked at the man
page, because I know exactly what ARC4 means and I know that it
is NOT a cryptographic number generator.
We should have a better way of updating code when a CPRNG needs to
be downgraded.

@_date: 2017-07-03 12:17:21
@_author: Ray Dillinger 
@_subject: [Cryptography] Satoshi's Trump Card 
I would not say hairshirted ideologue - he actually talked more about
engineering than ideology. But yes, I would agree that he seemed
profoundly uninterested in personal wealth.  He mentioned the topic of
money seldom outside of technical writing about Bitcoin, but when he did
it was never in terms of personal wealth. Money only mattered to him in
the abstract - in terms of its effect on society and economics and the
interaction between wealth and power.
I doubt that he is the sort who would have mined the blocks into the bit
bucket.  I remember him once responding to someone who had deleted an
expired key with evident near-disbelief saying "why would anyone ever
delete a key?"
I don't understand.  Nobody knows who Satoshi is.  Moving his known
blocks would risk someone finding out.  He's not vulnerable to extortion
etc until someone finds out who he is.  And of course the instant any of
his known coins move, a thousand alarms go off across the world, causing
a disruptive effect I bet he'd rather avoid.
Everyone is allowed a theory.  But it's likely that we will never know
for sure.
Hal expected to wake up from cryogenic suspension a decade or a century
from now; if he *were* Satoshi, why would he spend a Bitcoin fortune to
wait through an agonizing few extra weeks, which is all that the most
expensive currently available treatments in the world could have provided?
Right now, I think the biggest risk that Hal faces (if Hal can be said
to even exist) is that people who believe he's Satoshi will block his
resuscitation in order to prevent the possible financial panic when
Satoshi's coins start to be spent. If resuscitation turns out to be
possible in the first place.  I expect that risk is not very great,
because it's predicated on Bitcoin solving its scale problem.

@_date: 2017-07-06 17:37:18
@_author: Ray Dillinger 
@_subject: [Cryptography] OpenSSL CSPRNG work 
So far?  The complete invalidity of that assumption, regardless
of all efforts to make it true.
Do Not Generate Keys Before The Machine Has Booted Up!

@_date: 2017-07-06 17:41:49
@_author: Ray Dillinger 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Yes.  Absolutely it does.
Anything using bits that MUST be unpredictable, can either wait until
after the machine is booted up far enough to have them, or else it is
badly designed software and SHOULD cause boot to hang, every time,
unconditionally, either until it is fixed or moved out of the critical
early-boot path.

@_date: 2017-07-11 13:36:57
@_author: Ray Dillinger 
@_subject: [Cryptography] [FORGED] Attackers will always win, 
I have sometimes argued that for security purposes we need semantic-
level guarantees about things which are completely inexpressible
in existing semantic frameworks.  Where 'semantics' has been taken as
expressing everything intended to be observable about program output,
I've labeled these additional requirements as 'quasi-semantic', on the
grounds that they affect or determine 'quasi-output' or side channel output.
A lot of quasi-semantic requirements are about particular kinds of
things NOT happening; most of the rest are about the operational
properties of particular things (usually low-level things) that DO happen.
Once you introduce quasi-semantic requirements about particular kinds of
things NOT happening, the ability to do many kinds of optimizations or
rewrites vanishes, in ways that cascade through the code often down to
levels where the effects reach into tight loops where you wouldn't
expect it.  I'd argue that that's an acceptable cost of doing business.

@_date: 2017-07-11 17:24:00
@_author: Ray Dillinger 
@_subject: [Cryptography] Creepy correlation noted 
Pretty sure you'll get spamcalls at home and not on voicemail whether
there is any bandwidth use or not, because the spamcallers detect when
they are being connected to a voicemail system and autohangup before it
takes any kind of a message.
There is no need to suspect your ISP of being an accomplice in this one;
there are plenty of other things to suspect them for already.
Is the only result of the "do not call list" that they outsource it
to a boiler room in a different jurisdiction where the callers aren't
technically breaking the law, or do they just plain ignore it?

@_date: 2017-07-12 12:28:23
@_author: Ray Dillinger 
@_subject: [Cryptography] [FORGED] Attackers will always win, 
The issues with trusting electronic hardware have become so intractable
that for some applications it seems that building mechanical cipher
hardware should be reevaluated.
No, I am not joking.  :-(
No significant bandwidth, no easily automated integration with anything,
not really applicable to any type of data except text, absolutely
doesn't do public-key encryption ... and is absolutely the only thing
you know isn't deliberately manufactured to be electronically broken on

@_date: 2017-07-14 17:23:37
@_author: Ray Dillinger 
@_subject: [Cryptography] Defeating timing attacks 
Cooperative timesharing (software driven as opposed to the current,
sane, standard of preemptive hardware-driven timesharing) has very low
context-switch overhead, and allows very fine granularity of thread
changing.  It allows an application to determine its own CPU slice,
and gives the scheduler no compelling reason to ignore the application's
instructions.  Cooperative timesharing under BeOs(?) moved tens of
thousands of threads per second, per core, a few instructions at a
An application under cooperative timesharing that wants to defend
itself against emitting leaked information via side channels would
just yield the CPU, often, after processing very few instructions.
It should take time slices so small - say, 5 to 10 machine instructions
at a time - that the sequences are sequences that could be found in
pretty much any program. Low-overhead context switching would allow
it to work.
If you tried to do the same with a pre-emptive multitasking system,
the context-switch overhead would either effectively paralyze the
process, (Well, okay, says the scheduler, but if you really mean to
yield the cpu that many times, it'll take weeks to get the number of
time slices you're going to suddenly end...) or the scheduler would just
ignore its request to be interrupted and run it generating side channels
as usual (Yeah, says the scheduler, I can schedule a process now.  So
I'll just run a process to move things forward, and oh look, that
process is you.  Again.  Until I can get you out of my face.  Okay?).
Of course the problem with cooperative timesharing is the
non-cooperative process and the more-or-less complete lack of memory
barriers between processes.  You could control machine-level side
channels but you'd become more vulnerable to malware and memory-level
side channels than you've been since 1985.

@_date: 2017-07-14 19:36:45
@_author: Ray Dillinger 
@_subject: [Cryptography] Defeating timing attacks 
Oddly enough, I have just finished putting a dedicated system inside a
Faraday cage, for a completely different reason.
My CNC mill generates a lot of radio-frequency noise.  The controller
box runs fine in the noisy environment (it uses a shielded parallel
cable) but the house wireless network would really rather not be
exposed to the radio noise of the dust extraction vacuum and spindle motor.
And it is already inside a box for dust and (audio) noise containment
purposes, so adding electronic-spectrum shielding amounted to a minor
mod of the box.
But forget the Faraday cage. The real secret of security on the CNC
controller is this:  Not connected to the Internet.  At all.  Ever.
So I'm reasonably confident of its security.
I suppose if I want to do some really serious everything-depends-on-it
secrecy crypto, doing it on that machine, while the mill and dust
extraction vacuum are running, and moving it to and from anywhere else
via (non-USB) sneakernet, would probably be the way to do it.

@_date: 2017-07-16 15:12:20
@_author: Ray Dillinger 
@_subject: [Cryptography] Checkoin: physical crypto-cash 
If the anti-duplicating measures work, why are we assuming that they are
better than those already applied to cash?
Cash already has the anti-counterfeiting measures built into banknotes,
already including holograms and fine-detail-random images in some
countries, probably to include RFIDs and cryptographic
challenge/response if and when it becomes cost-effective.
Cash already has the way banks detect duplication by tracking serial
numbers on higher-denomination currency.
Cash has the "tattletail" code in printers that puts identifying marks
on pages so documents including counterfeits can be linked to the
printer that made them.
Cash has the code in scanners and printers that detects attempts to
print currency and smudges the copy (and may send encrypted packets to
interesting places if connected).
Cash has existing police forces all over the world who recognize it, who
understand what counterfeiting is and why it is bad.
Cash has entire populations of people who already know exactly how it
looks and feels and what weight and texture it has.  The kid at the
fast-food place can routinely make a test with her eyes and fingertips
in a fraction of a second that a counterfeiter needs many weeks of work
to reliably overcome.  You will never get a more reliable test that fast
and easy.
You're looking for physical tokens of value to circulate with
anti-duplication measures? The ratio of general effectiveness to very
low cost is set by cash.  It's really very effective and very cheap to
produce and use, and will likely continue to take up new technology as
it becomes cost-effective.  It will be hard to convince anyone you have
a better solution.  It will be hard to build a better solution.

@_date: 2017-06-07 12:34:06
@_author: Ray Dillinger 
@_subject: [Cryptography] stego mechanism used in real life (presumably), 
Text documents (plain text) have very little 'extra' bandwidth for
identifying stego.  But considering it, there's a level where content
and stego become indistinguishable, and you can 'watermark' a text
document in ways that will even survive retyping on a manual typewriter.
It would be easy (and possibly useful, for agencies trying to control
leaks) to have programs that automatically edit documents to
occasionally create minor variations in sentence structure without
changing the factual content.  Seed the program with a serial number,
process a document to produce a 'copy' when saving, and a sister program
could then recover the serial number based on selected sentence
structures in the document.
You'd want it to avoid editing any direct quotes, but otherwise there
are a dozen things you could do to most sentences in English to change
the word order or clausal structure that wouldn't affect the information
content at all.  Two or three per thousand characters, and leaked text
would become easily traceable.
The downside is that such minor sentence-structure clues are sometimes
useful in author attribution, and you'd be losing some part of that
information.  Not usually an issue, but sometimes, especially if you
were being sneaky and trying to embed it in the 'cp' command or
something so it might be used on documents where you really needed that
information, it could bite you in the shorts.
On a related note, there was a pseudo-stego that mapmakers used to use,
to catch each other if somebody tried to sell copies of their maps
instead of doing the cartography themselves.  They had mapped subtly
different, alternate versions of America.
If your local geography happened to match the idea of "out of the way
and on no path from any real place to anyplace else" you'd notice them
occasionally and wonder what the mapmakers had been smoking.
Where I grew up out between the boondocks and the sky, there was a
fictitous town where a dirt road dead-ended in the next county.  The
rancher who owned the land put up a city limit sign there with
"Population: 0". He kept a little box out there containing an accurate
map, a compass, a cache of a couple of K-rations, and directions to his
house (an 8-mile walk from there) just in case somebody had car trouble
and had gone there by mistake.  I was with him once when he rotated the
cache contents, while fixing fences.
There was also a fictitious quarry (marked as a long-closed railroad
gravel pit) in a pasture less than six miles from my hometown, which we
saw on maps from a different company.  I figured it was bullshit, but
I'd never actually seen the middle of that particular pasture and I was
a curious kid, so I still walked out there once just to have a look.

@_date: 2017-06-08 13:28:26
@_author: Ray Dillinger 
@_subject: [Cryptography] stego mechanism used in real life (presumably), 
I doubt that even half of the IMSI catchers in use are actually being
used by any organization even remotely considered to be one of society's
enforcement mechanisms.
Especially here in Silly Valley.  Seriously, people read CERT reports
around here like race forms, sports programs, and scorecards.
International and corporate championships are mostly separate, but
there's an annual playoff.  Police is just the local minor league team
in the international division.
Everybody who's assuming that IMSI catcher == some brand of society's
enforcement mechanisms isn't paying attention to the history of security
Name a well-known, long-standing, easy-to-exploit vulnerability that
absolutely anybody can use with no real fear of being caught, and ask
yourself who uses it.  Is law enforcement even in the top ten?
This is the real issue when companies like Apple push back against
creating vulnerabilities.  Court ordered or not, a new vulnerability is
just a new event on the program, and a new championship that will
thereafter be held, in both divisions and in the playoffs, every year.
Not everybody can afford to field a team for every event, and nobody
gets the option to drop out of the league.  The only option is a forfeit.

@_date: 2017-06-08 15:41:57
@_author: Ray Dillinger 
@_subject: [Cryptography] stego mechanism used in real life (presumably), 
It would not be easy to implement it so that would be possible without
creating some easily identifiable and reversible patterns.  There is a
tension between the ability to recover the "seed number" that's been
given as an argument to the program and the ability to recover the
verbatim original text.
In theory it's possible, but it would inflate the amount of text you'd
need to identify the serial number considerably, and also mean that the
size of the text would always be strictly increased by the process,
making it easy to identify which version of the document is closer to
the root of the tree.

@_date: 2017-06-08 18:11:17
@_author: Ray Dillinger 
@_subject: [Cryptography] stego mechanism used in real life (presumably), 
Oh, wait.  I think maybe you asked a simpler question than the one
my first message answered.
Given the same IV (serial number) and the same source document, the
same output would be reliably produced.  Given a different IV and the
same source document, there'd be several differences in sentence
structure per page in the output.
But in the most useful and reliable (IMO) direct implementation,  you
should get the same output text given the same IV regardless of whether
the input is the original or the result of processing the original with
some other serial number.
So the savvy leaker who knows the system is in use and gets their hands
on the program could obscure the source of the leak by reprocessing it
with a different IV.
Like all stego, it only works reliably if the opponent doesn't know it's

@_date: 2017-06-11 12:39:12
@_author: Ray Dillinger 
@_subject: [Cryptography] Crypto Books, 2017 
Yes.  Applied Cryptography, 1996, is probably still the best for
protocol design.  The later books, Practical Cryptography,2003,
and Cryptography Engineering, 2010, are less about protocol
design and the primitives available, and more about specific
implementations of protocols that answer specific requirements.
It seems sort of like the difference between a book on architecture
and a book that gives the architectural details and some design
insights of a dozen buildings already constructed.
That said, I can think of eight or nine cryptographic techniques with
particular properties (like Bloom Filters, digital timelocks, properties
of ring signatures, delegated signatures, construction
of cryptographic palimpsets, derived keys, homomorphic encryption, etc)
that ought to get exposed in a modern book on protocol design, and were
not treated there.  Along with a strong dose of what a post-quantum
universe looks like, where we have theories but still don't really know
if there are any really good public-key systems whose keys are shorter
than several megabytes.  I was honestly waiting to hear some answers to
your question in hopes of learning about more resources.
I hadn't thought of it that way, but this is true.  A background in
digital logic is enough to understand most modern cipher algorithms,
but if someone wants to design and/or break ciphers, a general
background in Discrete Mathematics and Statistics is probably more
use than anything written specifically about cipher implementation.
That said, it's still important IMO to study classical cryptography (up
through at least rotor machines and the VIC/Nihilist hand Ciphers) as
well - all those old codes and ciphers and the ways in which they get
broken are very instructive in terms of how the mathematical tools are
applied by cryptanalysts, and the sequence in which various ciphers
appeared is very much in the spirit of a lesson sequence increasing in
sophistication by decade and century of history.

@_date: 2017-06-11 13:02:27
@_author: Ray Dillinger 
@_subject: [Cryptography] Crypto Books, 2017 
It occurs to me that I haven't actually seen a detailed treatment of
cryptographic palimpsets in any book.
Does any existing application or protocol use them, or do they remain
just a theory at this point?
I see a source-forge / Google code project named "Palimpsest" which
seems to be implementing .... uh, as near as I can tell an insecure
and easily broken use of a one-time pad for a combination of several
messages.  D'oh!  Somebody misinterpreted a description and rolled
his own algorithm.....

@_date: 2017-06-14 11:02:08
@_author: Ray Dillinger 
@_subject: [Cryptography] Crypto Books, 2017 
I need to correct that statement.
I got that impression from my expectation that something named
"Palimpsest" would in fact be implementing a cryptographic
palimpsest, and reading a description of a stream-cipher. (yes,
that description used the words "one time pad" adding to the
confusion - but that happens with a lot of stream ciphers).
On a closer inspection however, what this program implements is
neither a cryptographic palimpsest nor a one-time pad.  It is a
stream cipher disk encryption utility.  It appears to be okay;
at least Ubuntu has deployed it, using LUKS for key management.
There was also a "Palimpsest" intended for network communications,
which may or may not be what eventually grew into the disk utility
I mentioned above; but it never had a working secure key exchange.
Like the above, it's a stream cipher based on PGP in counter mode
and uses the words "one time pad" to describe itself.  The non-
working key exchange code looks like it wanted to use Rijndael (in
some variant other than AES) to encrypt keys.  I can't tell whether
development moved on to the disk encryption app, or just stopped.
And, confusingly again, there was a DIFFERENT utility also named
Palimpsest, which also did not create cryptographic palimpsests.
It was presented in a 2003 paper by Timothy Roscoe and Steven Hand.
Instead, it implements ephemeral distributed storage with a
built-in bandwidth-charging system and encrypts fragments using
AES (in ECB mode!) for security.  It's highly experimental and
as far as I can see there are no significant deployments.

@_date: 2017-06-16 13:58:44
@_author: Ray Dillinger 
@_subject: [Cryptography] Brainstorming for encrypted text messaging 
It would work, but you'd want something less unwieldy than a one-time
pad.  I think you would want some kind of purely mechanical device, so
that users could verify for themselves that it is unhacked and
unhackable. All parts visible.
A copy of the M-209 cipher device could be manufactured on a desktop
mill. The cost for one or two units would be an undue burden, but in
small runs of a score or so the cost could be much more reasonable.
Especially if it's distributed in kit form and the purchaser assembles
it themselves - which could be a good idea anyway, as a way for the
purchaser to knowthe device is unhacked.
However, the cipher it generates can be broken by computer given a few
kilobytes of ciphertext. It would only be secure for short messages or
for very few messages per key.
I don't really know of any pocket-size devices which are more secure
than that, unfortunately.
I know a couple of Pen & Paper ciphers that are more secure than that
(never successfully cryptanalyzed) but they're REALLY annoying to use
(fractionation, double interrupted transposition, defractionation with
different table).  You could print out instructions on a sheet of paper,
but nobody would routinely use it because it would take ten minutes to
encrypt/decrypt a tweet.
And neither of these methods gets around the difficulty of typing random
characters into a smartphone.

@_date: 2017-06-18 13:08:52
@_author: Ray Dillinger 
@_subject: [Cryptography] Brainstorming for encrypted text messaging 
So....  This might actually be mechanically possible, if complex. I
imagine a device with a set of message wheels (20 alphabetic rotors?)
under a narrow window so you could read one row. Visible on the opposite
side through a different window but probably on the same axle, would be
a set of key rotors (another 20 alphabetic rotors?).  Each rotor would
be settable with a thumb wheel, and there'd be a winder and a counter on
one end.
Ideally, it is made so you can take it apart and permute all the wheels
on the axle for a longer-term shared key, but for individual messages,
it would use a 20-character session key and a 20-character IV.
Alice sets the session key on the key rotors on the back, sets the
message wheels to some random value(IV) in front, takes a pic and
transmits the pic to Bob.
Then she repeats:  wind forward until the counter has changed twice,
set a line of plaintext, wind backward until the counter changes,
take a pic of the ciphertext now on the message rotors, and send to Bob.
On the far end, Bob sets the session key and the IV value he
sees in the first pic, winds forward until the counter changes, sets the
ciphertext he sees in the second pic, winds forward again, and a line of
plaintext appears.  He then sets the ciphertext he sees in the third
pic, winds forward again until the counter changes, and the next line of
plaintext appears.  And so on.
If Alice & Bob have the patience and are paranoids, they could do a
forward-5/backward-3 encryption with the counter instead, or whatever
else.  But if that's necessary to the security of the cipher, the maker
should just make the device so the counter changes less often instead.
The device itself would implement an autokeying block cipher on
40-character blocks, with 20 characters of the previous block's
ciphertext used as a sort-of-CBC-mode. Both the key wheels and the
message wheels would change chaotically while winding, each move of the
key wheels depending on the previous state of the key wheels only, and
each move of the message wheels depending both on both the previous
state of the key wheels and the previous state of the message wheels.
The device would need to have a bidirectional property in that whatever
happened winding it one direction could be reversed by winding in the
other direction, so the state transitions are limited to a one-to-one
I can sort of imagine an implementation of it with a bunch of
interlocking slidey bits that would remind a lot of people of a sort of
cylindrical rubik's cube.  But it would have, conservatively, at least
200 moving parts.  Making it cheap enough to sell or small enough to be
convenient would not be at all easy, and somebody who knows more about
group theory than I do would probably spot a flaw in the cipher.

@_date: 2017-06-19 09:20:53
@_author: Ray Dillinger 
@_subject: [Cryptography] Brainstorming for encrypted text messaging 
Considering this again and getting back to the simplicity of the
one-time pad.... There is a classic construction for a simple
mechanical device to assist the use of a one-time pad. Tabletop versions
were in use in WWII, but a refined and convenient version could easily
be made shirt-pocket size, with a 20-character photo-friendly display.
It consists of 20 sliding strips, each having the alphabet printed
vertically (in type about the size of watch numbers for a shirt-pocket
device) with a hole drilled next to each letter.  They fit together into
a rack where each strip slides vertically behind a slot where the
letters aren't visible but a stylus can be inserted through the slot
into the holes.  Each slot also has a window in the center where one
letter is visible.  Finally each vertical slot is marked with the
alphabet twice: once above and once below the windows. So each letter in
a strip will occur either under a letter on the face of the device, or
under the window.
Alice takes the device and uses a stylus to shove all the strips to the
top end (this sets the display to all blanks).  Then, strip by strip,
she inserts the stylus into the hole next to the plaintext letter above
the window (the letter now is both the marked location on the slot and
the value of the underlying letter on the strip), and slides it down to
the window. This sets the display to the plaintext.  Strip by strip, she
then inserts the stylus into the hole next to each key letter (the
marked location on the slot, which now does not match the underlying
strip letter) and slides them either up, or down, to the windows.  This
sets the display to the ciphertext.  She snaps a pic of the device,
sends it to Bob, and goes on to the next 20-character line.
At the far end, Bob sets his device to the displayed ciphertext, then
inserts his stylus into the strip at each ciphertext window and slides
it up or down to each key letter to reveal the corresponding ciphertext
letter in the window.
This is easy and cheap to make, mechanically obvious enough to reassure
the paranoid, has a display that can be photographed, doesn't leak key
information, and doesn't contain anything with a battery, much less
anything that can run invisible software.  It implements a one-time pad
mechanically and with instructions simple enough for people to
understand. It can be made entirely out of wood or plastic, so as to
pass metal detectors and some kinds of x-rays unremarked.
But it is still a one-time pad.  And in practice, people will use short
repeating pads (a 'keyword cipher' broken via its index of coincidence)
or non-random one-time pads (a 'book cipher', broken by covariant
frequency analysis) or use one-time pads more than once (a 'two-time
pad', broken by simple subtraction). So while it's easy, simple,
convenient, and at least potentially secure, people will, very
predictably, fail to distribute the required keying material and will
use it in insecure ways.
If someone wants to make it, it should probably be packaged with some
mechanical entropy device (30-sided dice 26 sides of which are marked
with letters, or whatever) to help generate one-time pads that are
actually random.
PS:  Did you know that the classic "30-sided dice shape" is technically
called a rhombic triacontahedron?

@_date: 2017-06-20 08:24:03
@_author: Ray Dillinger 
@_subject: [Cryptography] Can we get some damn security defaults?! 
Haven't seen this story reported elsewhere yet...
Looks like consultants for the Republican party managed to fumble an
extensive database on about 200 million people.  62% of the US
population, by that count.
It was publicly hosted on an Amazon cloud server with no security

@_date: 2017-06-20 17:54:24
@_author: Ray Dillinger 
@_subject: [Cryptography] Trustworthiness 
What is needed is for people to understand that "trust" here is a
pejorative.  You trust your car to get you to work, meaning you know
that you can't get to work if it breaks down.
In every case, trusting anything means acknowledging that it is a thing
that must either work as expected or cause you to fail.
If you lived close enough to a bus line that you didn't have to trust
your car to get you to work in the morning, that would be a good thing,
because not having to trust something means being less likely to fail.
The Trusted Platform Module, for example, is named correctly.  "Trusted"
means simply that it introduces an additional risk of failure.

@_date: 2017-06-20 23:29:09
@_author: Ray Dillinger 
@_subject: [Cryptography] [ANNOUNCE] HashCash Digital Cash 
You, uh, might want to rename that.  "HashCash" already means
something else.  It's a denial-of-service countermeasure by
Adam Back.

@_date: 2017-06-21 08:49:15
@_author: Ray Dillinger 
@_subject: [Cryptography] Trustworthiness 
And that is what elevates the pejorative from mere annoyance to outright
obscenity.  You could just as well substitute a whole string of  more
widely recognized obscenities without diminishing the vitriol contained
in that one word "trusted".

@_date: 2017-06-21 09:06:57
@_author: Ray Dillinger 
@_subject: [Cryptography] Brainstorming for encrypted text messaging 
That is almost EXACTLY what I had in mind.  The little "hook" at the top
of each column to do an addition carry is a nice refinement for an
addition/subtraction calculator but obviously unwanted for the OTP
device.  I was actually thinking of the M-138-A (a very clunky tabletop
thing which was used with interchangeable strips that bore mixed
alphabets) and trying to imagine a way for it to be made pocket-size,
photo-friendly, and convenient.  Seeing that something so very much like
what I imagined, has already been done for the sake of a mechanical
calculator, is amazing.

@_date: 2017-06-23 09:08:51
@_author: Ray Dillinger 
@_subject: [Cryptography] Brainstorming for encrypted text messaging 
in each line of the ciphertext side reveals the plaintext letter.  I
propose to overcome this as follows. Instead of ordered alphabets on the
 front side, have randomly scrambled alphabets.
I don't understand why to not just use the original format for the Orion
sheets. One places the carbon paper directly on the desk, carbon side
up.  Then the Orion sheet, with the ordered alphabets down, upon the
carbon.  Then you go down (or across) the sheet, circling the letters of
the plaintext in the permuted alphabets.
Turn the sheet over and what is now visible is the ordered alphabets,
with apparently-random letters circled in carbon impression.  If the
plaintext side is in fact not visible through the paper, then a pic of
this sheet can be sent directly.  If the carbon paper is dusty on the
non-carbon side, or your stylus leaves an impression on the wood and a
"side channel" is made on the surface of your desk, the side channel
only reveals the ciphertext again.
Ultimately the encryption effort is no easier (Alice still has to find
the letters in a randomly-permuted alphabet) but there seems to be no
cryptographic logic requiring permuted alphabets on both sides.  The
location of the ciphertext letter in a non-permuted alphabet reveals
nothing about the plaintext letter.
As to trusting that the printing on the plaintext side is not in fact
visible through the paper, one *could* worry about the paper and the
optics and the light and the cameras.  I would prefer to unfold the
paper at the center crease, tear along the perforated line, and use it
to make the water in a blender just a bit less clear than it was before.

@_date: 2017-06-27 20:40:06
@_author: Ray Dillinger 
@_subject: [Cryptography] OpenSSL CSPRNG work 
There is no environment in current use where using /dev/urandom more
than a minute after bootup will inconvenience any other process in the
slightest.  Refusing to use it in order to avoid a hypothetical problem
that probably won't even occur on those systems is like a garage
refusing to use an air wrench to put on wheels because using one
carelessly might break the wooden spokes of a model-T. Well, that is,
one of the pre-1934 model-T's that had wooden spokes instead of metal.

@_date: 2017-03-01 13:20:59
@_author: Ray Dillinger 
@_subject: [Cryptography] On New York's new "Cybersecurity Requirements 
This is progress, but....
I feel like I'm pointing out the obvious here but most new attacks work
by doing things that security designers didn't think of.
Making active requirements to do specific things, no matter how helpful,
does not motivate people to think of new attacks and ways to block them.
They simply check off a tick-box, and often technically fulfill a
requirement even though they misunderstand or actively subvert the its
So I feel that a set of positively-expressed requirements actively
address only a fraction of the problem; the attacks we've already seen,
the methods for mitigating those attacks which we've already thought of.
 Further, it addresses those things only under the assumption that the
requirements are understood and implemented in a way that does not
subvert their effectiveness.
Motivating people to think of new attacks, to understand new attacks
when they start happening, to interpret requirements in ways that result
in effective security against known attacks, and to think of new ways to
effectively block known attacks, requires legal consequences that
penalizes the RESULTS of falling victim to those attacks.
IE, a set of requirements and best practices like this needs to be
accompanied by liability requirements that apply to the consequences of
attacks - regardless of whether the requirements are technically
followed. They don't need to be punitively large, but they need to be
sufficient to focus attention on at least getting the implementation
right.  They need to be sufficient to focus attention on promptly
understanding and blocking previously unknown attacks.
Consider a company that fulfills a requirement to encrypt the content of
its packets, and a requirement to NOT embed a decryption key in its
client so that it can be stripped out of the client software with a
debugger.  But does not understand the intent of requirement, or do not
face consequences for subverting its intent, so they then use a header
field to transmit the key with the packet, subject to masking using XOR
against a constant that can be stripped out of the client software with
a debugger.  It sounds ridiculous, but I've seen things equally idiotic....
"A common mistake that people make in trying to design something
completely foolproof is underestimating the ingenuity of complete
fools." --Douglas Adams

@_date: 2017-03-01 13:39:49
@_author: Ray Dillinger 
@_subject: [Cryptography] cryptography Digest, Vol 47, Issue 1 
Hm.  This even forbids recursion that has a statically provable bound.
For example, I often use constructions like:
int chomp(int *bigset, int *smallset, int bigsize, int smallsize){
   if (bigsize < smallsize)
       return(chomp(smallset, bigset, smallsize, bigsize));
   ....
Technically recursive, but the recursion provably bottoms after zero or
one stack frames and before anything else is called.  In fact it's even
a tail recursion that will on most compilers not use an extra stack
frame at all.
Still, forbidding recursion entirely is probably easier than allowing
cases that are provably subject to statically known space&time bounds,
and attempting to forbid anything whose definition is even a little bit
subtle is so subject to misunderstanding.
Eh, it's probably a good thing.  If I need to meet this standard, I'll
"Recursion:  P 342.  See also: Recursion." -- Index of Algol textbook.

@_date: 2017-03-01 21:22:06
@_author: Ray Dillinger 
@_subject: [Cryptography] Bizarre "latent entropy" kernel patch 
It appears to be an attempt to solve or at least workaround the problem
of memory-layout randomization prior to actually loading the filesystem
and getting access to /dev/random.
I think that it will be a bit better for that purpose than
security-by-obscurity; many of the branches it co-opts to side effect
itself will depend on genuine sources of entropy such as timing,
temperature, etc.  But most are deterministic.
It doesn't gather as many bits of "real" randomness as we'd like.  The
search space is probably smaller than 2^32 of the 64-bit entries, though
I haven't done an extensive analysis.
They might have done better by linking some of the already-debugged
entropy collection code that's in the /dev/random drivers.  But (I
think) most of that code depends on the filesystem and runtime already
being mounted, so using that would be hard.
It will not be as good as /dev/random, but it will add a work factor to
schemes that depend on exploiting memory layout.  It's better than many
of the possible alternatives, I think, but not so good as to justify
ceasing to look for a better solution.
And now the kernel has its own tiny little limited-use substandard
entropy pool....  which probably shouldn't get used for anything else
once the kernel and services are running.

@_date: 2017-03-08 12:01:56
@_author: Ray Dillinger 
@_subject: [Cryptography] encrypting bcrypt hashes 
Okay, to make sure I understand:
You have four-decimal pins hashed with sixteen-bit salt, and you're
worried about someone stealing the database.
We can start by talking about the size of the salt.  I mention this
because, since most businesses have more than 65536 customers (or at
least aspire to) it is likely that increasing the size of the salt so
that you can ensure each customer has _unique_ salt, will improve security.
The other reason to talk about the size of the salt is because, instead
of using a traditional symmetric encryption algorithm on a database of
hashes, it is better to simply extend the entities being hashed.
IOW, instead of encrypt(hash(PIN:salt),key), it is simpler to use
This extends the work factor for the attacker as effectively as
encryption; finding a hash preimage for 256 bits of PIN+salt+key
requires the same number of attempts as finding a 256-bit key.  And it
has the advantage of not bringing more or different crypto code into
your application.  Every line of dedicated crypto code in an application
is an opportunity to make a security-critical mistake, so it's best to
But, the dirty dirty truth is that with public implementations of good
crypto algorithms available, key management is by far the hardest part
to get right.  Avoiding the need for key management is why hashing
instead of encryption is preferred for PINs and passwords in the first
place.   If it's out of scope for this project, get someone to sign a
document that says so, because that key has to be present on everything
that can check PINs. Key theft is therefore the most likely way for the
whole thing to fail.

@_date: 2017-03-10 14:40:41
@_author: Ray Dillinger 
@_subject: [Cryptography] encrypting bcrypt hashes 
If it's down to chance whether two different customers get the same
salt, this is exactly true. This is the 'birthday paradox' in action,
and a number of widely-used methods for generating salts (hashing the
customer's name and/or account number for example) provoke exactly this
But making sure that customers have unique salts is in principle no
harder than assigning them unique account numbers.  There's nothing
secret about a salt, it doesn't need to be hard to guess.

@_date: 2017-03-16 16:35:10
@_author: Ray Dillinger 
@_subject: [Cryptography] Crypto best practices 
As long as we have a discussion of crypto best practices going on I'd
like to propose one.
This may be controversial because it is practically the definition of
'stream cipher,' so I'm about to make a blanket condemnation of an
accepted 'standard' in security design primitives, but....
I would like to go on the record as stating that, in my opinion, any
stream cipher based on combining plaintext with keystream using XOR (or
modular addition, or similarly easily-reversed primitive), is not
appropriate for  security purposes.  I do not recommend using that
construction in any design if there is any applicable alternative. And
it is my opinion that there are always applicable alternatives because
whatever virtues these ciphers have are present in other ciphers that
also have other virtues.
Stream ciphers of this type are something we inherited from pre-computer
pen-and-paper codes and ciphers, and making them work at the scale and
complexity of modern protocols, with the required  distribution of
secret IVs, salts, and protection of data integrity, and in spite of
formatted data having related and identical plaintext sequences at known
offsets, has required effort and risk, and caused failures, out of
proportion to the benefits compared with other classes of cipher.
Almost any design made with any other type of cipher in mind, is
automatically broken when implemented with a conventional stream cipher,
and design with a conventional stream cipher in mind is excessively
(dangerously) complex and fragile when compared with any other type of
There are too many attacks (not just the obvious bit-flipping mutation
attacks, but IV-recovery attacks, keystream side channel attacks, parity
attacks, etc) that this class of ciphers is subject to compared with
other classes of cipher.  There are too many things irrelevant to other
ciphers that can be made into attacks w/r/t these ciphers.
It's just plain time to stop using them in new designs.  In fact it has
been past time to stop using them since WWII, but they've just been such
an accepted part of the world that nobody's ever seriously reconsidered
whether they are in fact worthwhile.
Other encryption primitives have gotten stronger.  Resistance to other
kinds of cryptographic attack has gotten stronger as a result.  But
resistance to the specific types of attack that stream ciphers are
subject to have not kept pace. The complexity required to *defend* them
from those attacks on the other hand has been ramping up, for decades,
in proportion to the increasing complexity of protocols and the
proliferation of "related plaintexts" in the form of formatted data and
protocol messages.
Stream Ciphers simply aren't worth the level of complexity risk required
to design with them any more, and have not been for a long time.
This is just my opinion.  But you can use it if you like; I don't mind.
"Engineers like to solve problems.  If there are no problems handily
available, they will create their own."  -- Scott Adams

@_date: 2017-03-16 16:43:39
@_author: Ray Dillinger 
@_subject: [Cryptography] Using SGX to defeat SGX. 
Here is an interesting thing.
Briefly:  The researchers demonstrate using the SGX mechanism (intended
to conceal cache contents from co-resident programs under a hypervisor)
to execute concealed cache attacks....
"Quis Custodiet Ipsos Custodes?" -- Juvenal

@_date: 2017-03-17 14:03:01
@_author: Ray Dillinger 
@_subject: [Cryptography] Crypto best practices 
Wanna use RC4 and XOR in a secure way without IVs or cipher modes?  Use
RC4 to generate a different AES key for every block of your data.
That's actually a decent, secure cipher, and repeated plaintext blocks
won't appear as repeated ciphertext blocks.
But wait, it's four or five times as much compute power as you need to
encrypt securely by some other means, and the additional operations
present additional attack surface for side channel attacks on power
consumption, Tempest emissions, etc. So why bother?

@_date: 2017-03-21 15:20:52
@_author: Ray Dillinger 
@_subject: [Cryptography] Spice Mix Mode. 
Here is a very simple mode where you don't need any IV, where repeated
blocks within a message (or in many messages sent using the same key)
don't appear related, and you don't need any other block to decrypt the
current block.  It's even easy to implement correctly.
All you have to concede is a certain amount of bandwidth overhead for
the spice mix, and a certain amount of CPU overhead to generate the
spice mix.
If Alice is willing to pay, say 12.5% of her bandwidth and CPU, and
she's using a cipher with 512-bit blocks, she transmits her message in
blocks that consist of 448 message bits plus 64 bits from /dev/urandom.
Bob requires no IV, and can decrypt blocks one at a time in any order.
He doesn't need to anticipate the random bits in any way, and doesn't
need to wait to the end of the message; he can simply decrypt,
discarding the random bits as they arrive.  Eve doesn't get a plaintext
IV to help her do a key search, and Mallory loses the ability to make
any change smaller than scrambling a whole block.  Identical blocks in
the plaintext don't appear related, unless the random field happens to
be the same.  And if Alice & Bob are worried about that they can go to a
2048 bit block length and use 256-bit random fields, or allow a larger
overhead cost for the spice mix.
There is the possibility for malware on Alice's machine to use the
"extra" bits to exfiltrate data or form a side channel.  But if malware
has the ability to compromise her system's RNG she's already screwed, so
IMO this additional malware capability doesn't cost her anything she
hasn't already lost.

@_date: 2017-03-21 15:54:30
@_author: Ray Dillinger 
@_subject: [Cryptography] [FORGED] Re: Crypto best practices 
It's a reasonable try, but no...  As described you have the weakness
of ECB within each message.  for any IV and plaintext P
Encrypt(P XOR IV) = Encrypt(P XOR IV).  So unless IV changes between
blocks, identical plaintext blocks produce identical ciphertexts.
If you use a reversible function F(IV,N) where N is the block number,
you could have Encrypt(F(IV,N) XOR P) and it would work reasonably well.
 This is the basis of counter mode, where F is +N and F` is -N.

@_date: 2017-05-01 19:40:45
@_author: Ray Dillinger 
@_subject: [Cryptography] Big ugly security problem in post-2008 Intel 
I have been saying this for years, but kept receiving assurances from
everyone "official" at the manufacturers and OEMs whom I brought it up
to, all claiming that it wasn't true.
Any machine with a hardware BIOS that allows network bootup, data
recovery, and OS installation regardless of the condition or even
presence of an OS installed locally on the machine has got to be broken.
This is self-evident.  To claim otherwise is clearly lying, or
childishly believing in something which is logically impossible for no
reason better than wishing it to be true.
The advertising therefore clearly contained a logical contradiction, and
I'd been assuming that it was the security they were wrong (or lying)
about rather than the remote management capabilities.  There is flatly
and literally no way that the advertised capabilities of "remote
management" on these machines can be provided without the existence of a
hardware security problem that someone can drive a tank through.
Well, guess what.  It's Effing broken, and somebody's evidently been
driving tanks through it.  I'm not the guy who proved it; I'm just the
guy who's been assuming, and sometimes arguing, for years, that it must
clearly be broken in order for those capabilities to exist.

@_date: 2017-05-02 16:06:09
@_author: Ray Dillinger 
@_subject: [Cryptography] Big ugly security problem in post-2008 Intel 
Published details do in fact remain vague.  But here's an ars technica
If I'm reading this correctly the vulnerability doesn't even require AMT
to be activated in your machine, or even enabled in your BIOS, to root
the machine.  Intel chipsets have AMT on the dies, whether or not the
particular machine they're installed in has it activated or reports it
during POST.

@_date: 2017-05-14 00:37:24
@_author: Ray Dillinger 
@_subject: [Cryptography] Bizarre behavior of a non-smart mobile phone 
This does not seem like a thing that could be done by manipulation
of the SIM card alone.  Among other things, the SIM card is not a
device that stores the phone numbers of your contact list.
The behavior you describe would happen only as a result of bad
firmware or drivers in the device, or as a result of a bad device.
We can wonder why the firmware, drivers, or device is bad, but
I can't think of any reason why anyone benefits from this behavior
so I doubt that it's a deliberate act unless the motive is pure
malice.  Malice does not usually suffice to motivate something as
complicated as that behavior, when it would be far easier to just
deactivate the SIM card and leave you with a bricked phone.  And
I don't know your situation, but I hope that you haven't got
anyone harboring that degree of malice against you.
"Any sufficiently advanced incompetence is indistinguishable from
malice." - Jim Gray

@_date: 2017-05-15 13:46:48
@_author: Ray Dillinger 
@_subject: [Cryptography] Bizarre behavior of a non-smart mobile phone 
Hm.  Okay, I wasn't aware of that.  Apologies to you and the list, if
I wrote something misleading.
Aaaaand, your friends getting content-free calls from your phone at
random intervals isn't a very favorable goal of the hacker either.
The only common thread between them is that they might be adopted as
goals by someone who has absolutely no goal whatsoever beyond malice
directed specifically and personally against you.  Someone who gets
absolutely no other benefit from it.
But, if someone wanted to deactivate the SIM, they'd have a bunch of
known, widely-available ways to do it, whereas the more complicated
behavior you describe would require an additional effort of researching
the phone's vulnerabilities, figuring out how to do it, and programming
some original code.

@_date: 2017-05-15 15:22:31
@_author: Ray Dillinger 
@_subject: [Cryptography] CFB/OFB/CTR mode with HMAC for key stream 
You may be thinking of the "Chaffing and Winnowing" paper by Ron
Rivest, from 1998.  If someone wants to implement confidentiality
as well as authenticity using a MAC, it's probably the best-known
It even has its own Wikipedia article.

@_date: 2017-05-17 12:56:18
@_author: Ray Dillinger 
@_subject: [Cryptography] Bizarre behavior of a non-smart mobile phone 
Oh yeah, that's always been true and everybody knows it.  Phone security
bites rocks and every effort to fix it has always been gutted before it
actually hit the ground.   Don't put anything you want to keep private
on a phone.  I don't even keep a contact list on mine.
For a while a comically long stream of technical mistakes and
inadvertently stupid standards was a plausible theory, but now?  Nobody
believes that any more.  The stream of "mistakes and stupidity" now
stretches back for DECADES and it's pretty obvious by this time that
it's being done on purpose.  Mobile phone privacy is actively and
deliberately sabotaged, and always has been.
Any sufficiently advanced incompetence may be indistinguishable from
malice, but conversely any successfully covert malice is
indistinguishable from incompetence.
The question with your phone was never about whether somebody *COULD*
be getting into into it; monitoring any mobile phone is easy, and there
are well-known programs on darknet sites to install unwanted software on
most of them, either via wireless or through a fake tower. It was about
whether anyone would want to produce that specific behavior and how
they'd benefit from it.
To a lesser extent, it might be about whether your "dumb" phone actually
has an OS capable of running any software that isn't loaded into the
firmware at manufacture, but most of them do.  Or whether anyone can get
unwanted software onto it, but with most of them someone can.
Dozens of different kinds of people put up fake cell towers.
Intelligence services, police with warrants, advertisers, corporate
espionage, mobsters, foreign spies, malware distributors, spammers,
script kiddies, whatever: it's a free-for-all.
They're not usually physical towers that occupy traceable real-estate;
instead they are often in moving vehicles or similarly untraceable.
Some of them are just a guy with a laptop at a local cafe, or somebody
who runs it alongside the "free wireless service" in their business.
They monitor and sometimes modify the messages that go through. Some of
the ones that aren't criminal, advertisers, or foreign-espionage
acknowledge that they do this and some don't.  Of course none of the
advertisers, foreign spies, and crooks talk about doing it.
Nobody wants to make the practice illegal because they are either doing
it themselves, or they assume without any proof that most of the fake
towers they themselves aren't able to account for (typically all of them
that they don't actually run) are operated by police or intelligence
agencies who will stop them from making it illegal, instead of by crooks
exploiting it for illegal purposes.
And they're probably right that the police-and-intelligence people
will stop them from making it illegal - although even that might
possibly be reinforced by mobster influence within *those* agencies -
mobsters have a big vested interest in it too.

@_date: 2017-05-17 22:46:14
@_author: Ray Dillinger 
@_subject: [Cryptography] Repeated Salts? 
Is there software still in use that is likely to repeat salts across
separate installations?
For example, does anything hash the username in order to get a salt,
whereupon ALL of the people who select a common username get their
passwords hashed with the same salt, across many installations?
If everybody who uses a common username (or all the people who have user
number 27, or whatever) gets the same salt, then the logic behind salt,
of not giving the attacker the opportunity to reuse password guesses
across multiple accounts, applies only to attackers who are
attacking single installations.
If common usernames or user numbers or account names can lead to
identical salts, then an opponent attacking installations wholesale gets
to build separate rainbow tables (or whatever) that apply to each of a
few million "common" salts, and gets the benefit of reusing computation

@_date: 2017-05-18 09:56:55
@_author: Ray Dillinger 
@_subject: [Cryptography] Repeated Salts? 
It's enshrined in Tanenbaum's operating systems book (Minix) too.
Which, history being history, is still fine for expository purposes
but oughtn't be used today.  Somebody ought to do a second edition
with stuff that's (now) obviously wrong for security, fixed.
Anyway, considering the "wholesale trawling" attack model it's clearly
worth it to make salts big enough that they can be expected not to be
reused EVER instead of just big enough that they can be expected not to
be reused in the same password file.
And considering it, I think my minimum recommendation for salt just
jumped from 64 bits to 128.

@_date: 2017-05-19 13:15:15
@_author: Ray Dillinger 
@_subject: [Cryptography] "WannaCry" ransomware has any payment resulted 
A chuckle about it that happens to match a theory...
I think maybe the people just wanted to disrupt things, cause fear, and
get the vulnerability out there.  One week of giant headlines to
distract people from something else, One more tool in the arsenal of
hackers who will create further distractions for you.
They could also be "burning" a zero-day exploit after finding out that
some adversary of theirs has it, in order to get it fixed and pre-empt
having it deployed against them or theirs - but if that were all they
were doing they could have just gone through bug reporting.
Then again, maybe the "no way to pay up" business was just a programming
error, or something that escaped before they were finished
building it correctly.

@_date: 2017-11-18 13:04:03
@_author: Ray Dillinger 
@_subject: [Cryptography] Is ASN.1 still the thing? 
~clip~
Why, indeed, should anyone be using a serialized representation of
IEEE754 floats that doesn't roundtrip?  You can express them in hex with
a hex point if you like (some) human readability, but the serialization
is now exact and specifies an unambiguous meaning.
Of course, nobody can propose that to the ASN.1 standardization
committee now, and if one could they'd probably have rejected it.

@_date: 2017-10-20 14:20:15
@_author: Ray Dillinger 
@_subject: [Cryptography] Severe flaw in all generality : key or nonce 
How about - and this may be a radical notion here - but how about
Stream ciphers are letters of flame writ forty feet high on a basalt
cliffside which say,
"The designer of this protocol had no idea what the F**K he was doing!"
Seriously, give me any modern block cipher in any chaining mode, and
reuse of keys and IVs exposes far less than any stream cipher.
True.  In CBC mode, reusing key and IV means an analyst can tell how big
a prefix in blocks two messages have in common.  That failure is
*DRASTICALLY* better than the way a stream cipher fails when key and IV
is reused.
I'm pretty sure nobody was talking about that.  The issue is about what
happens when people get it wrong, not about what we will consider right.

@_date: 2017-09-03 11:10:53
@_author: Ray Dillinger 
@_subject: [Cryptography] early archives of cypherpunks? 
I'm trying to trace the origins of some of the ideas that developed into
our current variety of cryptocurrency protocols.
I believe that the earliest description of a cryptocurrency or digital
cash protocol that used block chains (in some form) was published on
cypherpunks in the years between 1993 and 1995.  But I don't have an
archive of the messages; I only have my memory of what was in those
If anybody has an archive of cpunx for those 3 years, or can do searches
in such an archive, or knows of a public-facing archive, I'd very much
like to be able to find a copy of the thing that I'm pretty sure I
If nobody has, I suppose I can probably do the FOIA dance, but it's
bound to be annoying.
If this is the radical right, then who on earth are the radical wrong?

@_date: 2017-09-04 00:47:25
@_author: Ray Dillinger 
@_subject: [Cryptography] early archives of cypherpunks? 
Much appreciated.  At first blush though, I haven't been able to find
the protocol description I'm pretty sure I remember.  At least not
there.  So now I'm wondering if I just need better search-fu, or if it
was actually on a different list entirely.
Did the Metzdowd list exist in the 1993-1995 timeframe?  'cos if so and
if it really wasn't on cpunx, then by process of elimination it was
probably here.  I chased the Metzdowd archives back to March of 2001
and the first message there says the list is coming back after a
four-week hiatus ....

@_date: 2017-09-06 16:34:58
@_author: Ray Dillinger 
@_subject: [Cryptography] Finding Nemo's random seed 
They deserve *some* sympathy.  Everybody has learning experiences, and
learning experiences are often painful.  But if they have the same
learning experience again, they deserve no sympathy whatsoever.
There's almost always a library call that specifies a pseudo-random
stream - that the same sequence will be generated from the same input.
It is almost never guaranteed stable.  They don't specify what
generator, they don't give the parameters, and they don't give test
vectors.  In subsequent versions, in different environments, with
different sets of dll's or shared objects, they can fulfill what the
library documentation promises by giving you a *different* sequence
that's repeatable in *that* environment.
If you need stable repeatable sequences, eg, for documents that may be
read elsewhere or later or by a different version, or even by the same
version as compiled in a different build configuration, it's just plain
dumb to rely on a library call unless they specifically promise
stability. Otherwise version-stable PRNG sequences are
application-specific, and the PRNG has to be part of the application
source code.

@_date: 2017-09-12 11:45:52
@_author: Ray Dillinger 
@_subject: [Cryptography] Chrome & Firefox protecting users against Symantec 
Both of the major browsers apparently have plans to stop trusting
essentially everything issued by Symantec, which is long overdue.
(Side question:  Why the heck did Symantec think it needed so many
different names?  When I see other companies playing shell games like
that my first thought is money laundering.)
Plans are to upgrade Chrome security against certificates issued by the
Symantec root key (including all the additional brand names) over the
next year.
Natch, corresponding security upgrades for Firefox users are underway at
There are a couple of other browsers people care about, but as minor
players they don't have much latitude to make their own decisions
anymore. They used to be more independent, but these days, they just
copy whatever Chrome and Firefox do.
PKI is still broken, but at least in some of the most egregious cases,
and with heroic effort and a year-plus rollout plan, a key revocation
can in fact take effect!

@_date: 2017-09-12 19:19:17
@_author: Ray Dillinger 
@_subject: [Cryptography] Chrome & Firefox protecting users against 
Ah.  That  was obvious, really.  I didn't think of the M&A history.

@_date: 2017-09-26 14:10:52
@_author: Ray Dillinger 
@_subject: [Cryptography] Crypto basic income 
This seems to presume that everyone agrees on the purpose of a
decentralized immutable public ledger system, or even that everyone
views pseudonymity as part of the purpose rather than merely a side
effect of distributed implementation.
Pseudonymity in particular is one of the properties that effort is often
spent specifically to eliminate (introducing Trusted key servers and
certificates) in block chains designed for specialized purposes that I'm
seeing now.
For example there's a startup in Virginia whose CEO I was talking to
earlier today who's using a block chain to manage special-purpose coins
in a sort of distributed, online mall - think Amazon with all goods
shipped from the vendor, and nobody except the vendor gets that vendor's
sales data.
While it's distributed, it's not Trustless - there are privileged
servers that do things like serve keys and match IDs.
As currently mooted, the design has no pseudonymous users allowed
period, but 'mix' transactions in every block to preserve privacy by
preventing anyone from knowing which users spent how much to which
vendors. Data blocks within each 'mix' transaction are encrypted to
individual vendors revealing information about individual orders.
The point is different people have different ideas about what the
"purpose" of block chains and cryptocurrency are.  No single vision has
a monopoly on deciding what it means or what its purpose is.
Before the year is out, we're even going to have some national
governments start issuing block chain currencies.

@_date: 2018-04-01 09:40:16
@_author: Ray Dillinger 
@_subject: [Cryptography] Password entry protocols 
Well, that was the basic remit of the "system request" key - which is
standard on all PC keyboards.  It was provided along with standard BIOS
key input routines that completely ignore it (to prevent any software
from capturing it) supposedly restricting it to privileged use.
That function is largely forgotten today though.
'Print Screen' used to be an alternate function of the System Request
key, invoking a BIOS routine which read all screen memory bypassing any
program control so it could be applied even after a crash, and then
wrote it to the printer port.  Very low-level stuff, intended to be used
for debugging.  But that put a legend 'Print Screen' on the keyboard and
everybody wanted a printing utility that could be used with their GUIs
and under program control and do special things with various software
formats that the pixels on their screen represented and on and on and on.
So there was NO WAY to have a legend on a key labeled 'Print Screen'
that just did that basic BIOS level task, and nothing else, and avoid
having consumers yell about it and demand additional functionality that
could only be provided by making the keystroke visible to all
(virtualized, user-space, etc) software.  So the 'pure' System Request
key had to go, and the key input routines had to become aware of that
key - at least to become aware of the keystroke used for 'Print Screen',
so it could be hijacked (err, excuse me, used) by other utilities.  This
is about the time that 'Print screen' stopped being useful for debugging
after crashes in console-mode programs.  Whatever though, that was a
small loss.
The key is still there, and most BIOSes do support at least one
keystroke isolated from userland software to be used for 'System
Request.' But the 'System Request' legend is not printed on it on most
modern keyboards, and on the very few systems where it still means
anything at all, the 'System Request' keystroke can only be typed using
it in combination with the 'alt' key.

@_date: 2018-04-08 01:03:42
@_author: Ray Dillinger 
@_subject: [Cryptography] Proof of Work is the worst way to do a BlockChain 
I'm listening....
Okay, you assume that this is still a chain where somebody is allowed to
form one block every ten minutes, but the block can be bigger or smaller
depending on demand for bandwidth.  And miners are still competing for a
chance to form the block because they get paid that way.
So the proof-of-work for a bigger block becomes harder - at some ratio
that makes bigger blocks more efficient than more blocks, for processing
the same number of transactions.  IOW, if someone can earn an extra 10%
(in subsidy plus transaction fees) for a block twice as big, we don't
want that twice-as-big block to be more than 10% harder to form, or else
it's to their advantage to work on small blocks rather than handle more
transactions per block.
So it sounds like we should be directly keying the difficulty premium
off tx fees instead of block size.  A percentage "premium" is added to
the difficulty based on the proportion of tx fees to block subsidy.
Spiking a block with a bogus transaction having extra tx fees would make
the block more difficult to solve, for no advantage.  Conversely, if
someone makes a horrible mistake and sends 1000 BTC in transaction fees,
it is significantly difficult to put that into a block and actually make
the block.
It makes decent sense if you still accept the ten minute block as the
basis of transaction processing. So it's still predicated on making the
bandwidth cost an inordinate amount per transaction compared to any
other form of transaction processing.  Which is certainly no worse than
Bitcoin is doing now, so strictly an improvement.
I don't think this is probably viable.  Without being able to see the
actual transactions, nobody other than the people actually making a
transaction can see whether it's valid because they won't know the
provenance of the coins.  Of course those spending the coins got and
added to the provenance when they got them, so it is possible for them
to do.  The issue is that it's not obviously possible for the full nodes
in the network to check.  Unable to tell the difference between a block
containing only valid transactions and a block containing one or more
invalid transactions, it is impossible to tell which of several proposed
extensions to the chain are valid.
And if we do it without creating a situation where people can make
deliberately bogus transactions in order to make subsequent bogus
transactions look legit, then proving the validity of every transaction
means disclosing the exhaustive transaction trail through which every
coin has come, all the way from their creation to the current spend.
The entire history of transactions still has to exist - it's just that
now it's part of the wallet rather than part of the block chain.  And it
has to be disseminated right along with every block, in order that
people can tell that the block is valid.
So while a Merkle root instead of the transactions would technically
shrink the block chain dramatically, it wouldn't shrink the bandwidth
required to form or check the chain by even a little bit.
I don't see how LN differs from reserve banking.  Payment channels just
create the near-necessity of consolidating the payment channels into
hubs around a set of entities whose main business that is.  This will
happen for convenience but more importantly because otherwise the
capital bound up in payment channels would be inefficiently deployed.
You want it at a hub so you can pay anybody, not tied up in some stupid
channel where you can only pay a particular person.  So these hub
entities will become economically dominant in the network. And aren't
those entities then taking the role of reserve banks?
So I don't really see much to talk about - they'll just be regulated as
reserve banks.

@_date: 2018-08-05 09:46:55
@_author: Ray Dillinger 
@_subject: [Cryptography] what application creates single-use coded email 
I've encountered some email addresses that are apparently base64 encoded
usernames prefixed with a four-character nonce.
For an example with a fictitious username, email addresses AX2bY2xhcmti,
gg68Y2xhcmti, cUn4Y2xhcmti, etc, all have the form
{four alphanumeric characters}{'clarkb' encoded in base64}
I was going to write 'encrypted' email addresses in the subject line,
but prefixing a 24-bit 'magic cookie' to base64 of the plaintext is not
encryption. These are encoded, not encrypted.
I am interested to know what application creates these addresses.  They
look to me like a good idea, but I can't find any information about it.
I can think of a dozen uses for such email mangling, first on the list
being the ability to selectively reject mail sent to versions of the
address that have made their way into the hands of spammers without
rejecting versions of the address that are still in the hands of actual
people you give a crap about communicating with.
I was thinking about how it would be done...
It's easy to imagine a filter in a firewall that discards four
characters and then base64 decodes the local part of the 'to' field in
email coming from outside the organization, then reverts the operation
if that results in an invalid email address containing a backspace or
something, and then appends the "as received" form of the address as a
username string.
Somebody can do that in one afternoon, leaving local email service
working normally.  So according to that theory, clarkb would get his
email at some local address like
clarkb at domain.org <'gg68Y2xhcmti at domain.org'>
if somebody mailed him using an 'encoded' email address, or at some
local address like
clarkb at domain.org <'clarkb at domain.org'>
if somebody mailed him using his unencoded address.
Tho I haven't seen any evidence for the second form in these logfiles I
assume it must exist.... people worship at the altar of backward
compatibility, which leaves all email everywhere permanently backward.
The local mail user agents would be correspondingly configured to use
the username string form of the message they're replying to in the
'from' address, and the outgoing firewall configured to strip the local
email address and then the quoting brackets and apostrophes, leaving the
mangle as the 'from' address.  So now we're up to three days or so of IT
effort to set this all up.
But it's hard to imagine asking clarkb to actually manage the nonces or
create new mangles to use for new outside correspondents, unless his MUA
is aware of this mangle and does it automatically. And that gets into
the same issues as key management, which is  non-trivial.
That last part is also necessary for a complete working system, so on
consideration of that, I don't think this is some local config hack done
by a local IT department anymore.
Somebody somewhere has created an MUA that manages these mangles for
people.  And I'm interested to know what it is.
Does anybody know what MUA creates these addresses?

@_date: 2018-08-09 12:35:25
@_author: Ray Dillinger 
@_subject: [Cryptography] XKCD 2030 
In case anyone missed it:
Apropos given the demonstrated threat of election tampering.

@_date: 2018-08-09 12:33:46
@_author: Ray Dillinger 
@_subject: [Cryptography] Krugman blockchain currency skepticism 
I did the design way the hell back in 1995 for a digital cash protocol.
I was combining David Chaum's blinded certificates with proof chains
(now called block chains) to enable the 'coins' to make more than one
offline hop.  Each 'coin' got larger and larger, dragging its proof
chain behind it with an additional link for every transaction, until it
made its way back to the issuer and got redeemed.  If anybody along the
way spent it more than once, then multiple versions of the coin with
inconsistent history chains would come back to the issuer and comparing
the transaction records at the chain fork would reveal the ID of the
I never even attempted to launch it; the only people I knew of who had
ever actually launched digital cash systems were crooks.  I was
interested in working out a protocol puzzle, but not interested in being
a crook and had nowhere near the assets to do it legally and properly.
Being a natural-born pessimist, I had already figured out that the only
people willing to loan me such assets would inevitably turn out to be
crooks.  And paying the interest on a loan would have forced me to make
it more expensive than a checking account - so nobody would want it anyway.
It required a Trusted node to create certificates matching IDs in the
system (IDs that could be revealed by double spends) to entities that
could be held legally/financially accountable for double spending.  So,
since that was the power the Trusted node could abuse, of course the
heavy boot on the door in the middle of the night would have been about
denying someone the use of the system.
I left the question of issuance completely open.  The presumption was
that someone buys a 'coin' from the issuer for a fungible asset to be
held in escrow until the coin is redeemed, but I was then and am now
well aware that the usual outcome of anything like this is that in a few
months to a few years, people notice the assets are gone and some
scammer faces prosecution or jail time.
We keep thinking that "real" bankers who are accustomed to dealing with
fiduciary duty should be able to do better than this. IIRC the only
"real" banker who ever tried Chaum's e-cash - the Mark Twain Bank - did
in fact manage to avoid that variety of scamming, but still went out of
Honestly, the boot on the door, in some manifestation or other,  is
inevitable.  Any payment system requires mutual consent between payer
and payee.  And therefore any entity capable of creating a strong
motivation to refuse consent, for either side, can kill the system.  If
accepting payments via Bitcoin becomes illegal, and enforcement actually
starts charging the owners of any server seen running a node with a
crime and levying fines, then overnight the value of Bitcoin crashes and
it ceases to be useful for making payments.  There is no way to
simultaneously protect all nodes and keep the system publicly available.
 If the people using it know where to send packets, then the people
levying fines know where to shut down servers. Even heroic attempts like
Tor can be and have been penetrated, given the will to do so.  This is
what Tim May called "squishability" years ago.
See, this just isn't true.  What happens when you get caught cheating?
The issuer sees that you have cheated and revokes your key.  How do the
other users of the system know your key has not yet been revoked?  They
access a key server that gets updates and knows when the issuer has
revoked your key.  So how does someone kick you off the system?  They go
to the issuer's key server and remove your key.  If the other users of
the system can't find current proof that your key remains unrevoked,
then mutual consent fails and you cannot use the system.
If the protocol allows a valid revocation to be formed without a
recorded proof of cheating, they'll do that too.  But even if proof of
cheating is required to create a valid revocation, about 99% of the
effect of a revocation can be had just by removing the key from the
issuer's server.
Blinded Tokens always require a Trusted role to act as issuer.  This is
fine, assuming the Trusted role is held by a Trustworthy entity.  But so
far history has taught us, emphatically, that this may not be assumed.

@_date: 2018-08-10 14:50:27
@_author: Ray Dillinger 
@_subject: [Cryptography] PGP -- Can someone help me understand something? 
A start on understanding what the basic shape of the "trick" is:
The short answer is that there's a lot of math that's easy to do but
very very hard to undo.
The classic example is that multiplication is easy and factoring is
hard.  If I give you a number that's the product of two very very large
primes, it's really difficult to find the lowest one of those two
primes.  It's unambiguous; It's the only unknown; It's just very very
hard to find it.
You can take the brute force approach.  It looks like this:
LET FirstFactor = SquareRoot(Target)
LET SecondFactor = SquareRoot(Target)
If IsNotPrime(FirstFactor)
    LET FirstFactor = NextPrimeLower(FirstFactor)
While FirstFactor * SecondFactor != Target AND SecondFactor < Target
   If FirstFactor * SecondFactor < Target
      LET SecondFactor = NextPrimeHigher(Target/FirstFactor)
   Else
      LET FirstFactor = NextPrimeLower(Target/SecondFactor)
If FirstFactor = SecondFactor
   Print "$Target is the square of $FirstFactor (which is prime)."
If SecondFactor = Target
   Print "$Target is not the product of any two prime numbers."
If FirstFactor * SecondFactor = Target
   Print "$Target is the product of $FirstFactor and $SecondFactor."
But you may notice the brute force approach will take a lot longer than
just multiplying FirstFactor by SecondFactor.  Think for a minute about
how many numbers there *ARE* when you're talking about thousand-digit
numbers.  Compare it to the number of microseconds in the lifetime of
the universe times the number of atoms in the visible universe if you
want a yardstick in some terms that might be relevant.
It turns out that there are a couple of tricky methods of factoring that
do better than brute force, but none that are even close to being as
easy as multiplying.  The tricky factoring methods require multiplying
bigger numbers than you'd need otherwise, but it's still possible to do
a multiplication in an instant that you can't factor with more than
infinitesimal probability during the lifetime of the universe.
Anyway, all public-key cryptography is built on one-way functions like

@_date: 2018-08-17 13:53:50
@_author: Ray Dillinger 
@_subject: [Cryptography] Rescuing Encrypt-then-Sig 
It seems to me that the incessant sign/encrypt vs encrypt/sign debate
happens because there are a couple of different purposes being served
here, and that the correct answer might be to use cryptographic
operations to explicitly perform both of them.
Is there a fundamental problem that's a GOOD reason why everybody isn't
encrypt(privacy of message) /
sign (authentication of encrypted message) /
encrypt(privacy of encrypted signature and message)
So when Bob sends a message to Alice, It allows Alice (and nobody else)
to check the signature and decide, eg, that this is a message she does
not want to decrypt on the present machine, at the present time, or in
the present environment.
This way, Alice can, eg, search on "verified sender or signer" without
exposing the signature to Carol the mailman, nor exposing the plaintext
to the system where the mail is stored.

@_date: 2018-08-17 19:24:50
@_author: Ray Dillinger 
@_subject: [Cryptography] God Mode backdoors 
If you really want to do crypto without backdoors, I think you have very
few options, and even fewer practical options.
It is easy to construct a backdoor that is infeasible to ever detect.
The chip may enter the backdoored mode, for example, only when a certain
256-bit value is written to each of three different registers.  It would
be no easier to find that 'magic cookie' than the key of any secure
cipher, and could be a function of the CPUID in order to be unique to
each chip.
1) trust the vendors of these chips to not install backdoors
(increasingly suspect, subject to different pressures for chip fabs
inside and outside the US).  This is the only thing I think most people
are motivated enough to do.
2) play the torturous and failure-prone game of trying to game
probably-insecure hardware into doing probably-secure crypto - there are
ways to do it where the hardware that the chip designer/saboteur expects
to see the plaintext, never actually sees the plaintext, so it can be in
backdoor mode without having the crypto instructions actually betray
secrets.  And you could even obfuscate the XOR by doing it on a
different processor, like your sound card or hard drive controller.  You
might get people to use this application, but only a very few hardcore
people.  And that would put parts all over and be complicated, so you'd
probably screw up implementing it.
3) make or procure an electromechanical machine like a SIGABA which
can't handle public-key crypto at all and absolutely will not handle
large files because it would explode if you tried to run it at those
speeds.  I have a soft spot for those old machines, but NOBODY would do
4) fab your own relatively primitive circuit board using basic (non-CPU)
components and circuit traces anybody can check by eyeball.  The crypto
you can build this way is very limited.  Probably about the same set of
nobody would do this.

@_date: 2018-08-18 11:57:27
@_author: Ray Dillinger 
@_subject: [Cryptography] God Mode backdoors 
The real problem is that we have now seen that the stack of things
you can't trust goes all the way to the bottom of the pile, and
therefore have cause to question whether working on computer security
even has any point.
There was a time you could inspect the source code and think that you
were sure of no deliberate backdoors.  But now you can only inspect the
source code and say that you don't think the backdoors are in the code.
The problem is the code is what we can work with.  We don't get access
to the chip fab process.  Even if the manufacturers let people examine
some chip masks there is no way for us to be sure that the chips they
are actually selling were made from that version of the mask.
Modern chip fabs cost in the $Billions and that means they are few and
the people in charge of them very much under the thumb of whatever
authorities are in charge in their area.  Chip fabs are assets which can
be seized if their owners don't kowtow, and most people don't want to
lose something worth $Billions.
And so every router from China has a chip-level backdoor that allows
Chinese government access.  And every switch from where they do the
board assembly in Taiwan has a BIOS-level backdoor that allows Taiwanese
government access.  And every computer from Intel is made according to
masks made in the USA and has backdoors for US government access.  And
so on.
And now that we understand this, do we throw up our hands and leave in
disgust, despairing of ever getting hardware rid of all the saboteurs so
that it would be worthwhile even to *TRY* to write secure software?
Because, as Bill points out, sooner or later we want the plaintext, and
if it's going to be routinely secure, we want it on an
internet-connected device.

@_date: 2018-08-19 23:12:18
@_author: Ray Dillinger 
@_subject: [Cryptography] A new method to (partly) factorize RSA 250 and 
What order you try factors in doesn't actually matter.  You can start
from just under the squareroot and work your way down, or you can start
from zero and work your way up.  And assuming that the factor has the
same number of digits as the squareroot, or one digit less, you won't
be checking any fewer factors, on average.
Using Euler's method to check for common factors with a very large
number made by multiplying many factors together is one of those ideas
that keeps coming up because it's clever, but doesn't help much.  And,
to the extent that it does help at all, there is already an equivalent
to it which is part of the General Number Field Sieve algorithm.
The very large composites you want to check for common factors against,
must be of a size that's hard to communicate relative to the number
you're trying to factor, and the sheer scale of the operations required
to do Euler's method, even on the first step, makes the operation as
hard as known methods of factoring.
I apologize for using a tricky phrase like "orders of magnitude more
orders of magnitude more digits" if English is not your first language.
Even most native English speakers would gloss over it as mathematical
jargon or, if they understood it, dismiss it as impossible hyperbole.
But please work through that phrase, because it is literally true and
you need to understand what it means to see what is wrong with this
idea.  That is the only "simple" phrase I can think of to that
accurately states the scale of the constructed composite you'd be
checking against for common factors, relative to the composite you're
trying to factor.

@_date: 2018-12-07 14:21:23
@_author: Ray Dillinger 
@_subject: [Cryptography] What if Responsible Encryption Back-Doors Were 
For what it's worth I believe the only "responsible" backdoor suitable
for government use - particularly if it is something that can be
automated - is one that does not allow even one single use of that
backdoor to remain indefinitely secret.
We need a system for accessing the systems of a few dangerous criminals
that allows the government to PROVE at any point that it has not been
invasively scooping absolutely everything that belongs to absolutely
In Broad Fuzzy Outlines leaving a million hard details and a hundred
refinements to be designed....
In order to get their backdoor key, perhaps, they have to interact with
a public block chain creating a blinded transaction.  And no valid block
can be created without unblinding the access transactions of blocks that
are turning more than (say) 180 days old.
Because I get the idea that sometimes you have to have covert access for
the sake of a specific investigation, of a specific person for evidence
of a specific crime.  But if that access never becomes known to the
public, or if people CAN use it in the belief that their use of it will
never be known, then it is guaranteed to be abused.
Every morning, political and business reporters (and inevitably gossip
and tabloid reporters) and courtroom attorneys (and inevitably mobsters)
should be waking up, having a healthy breakfast, and consulting the
chain to see the backdoor accesses unmasked this morning.
Every morning, paranoids and tinfoil-hats who irrationally fear the
government (and inevitably mobsters and shysters and spies who have good
cause to fear the government) should be able to reassure themselves by
checking the chain to see whether it reveals today that a while ago
their own data was accessed.
Unmasking shouldn't be required for accesses less than 6 months old, so
that a legitimate law enforcement purpose can be served before the
targets become aware.  But unmasking must become absolutely certain
after some time so that people cannot be deceived about the extent or
nature of the access.
Unmasking a backdoor access should reveal whose data was accessed, who
accessed it, when it was accessed, what specifically they were looking
for, why they had probable cause to believe it was there, and what judge
signed the search warrant.
Even the masked transactions on the chain must be known to exist; Nobody
should ever be able to authorize some program that harvests 330 million
people's data one morning without it becoming known, on the same
morning, that 330 million accesses were made.

@_date: 2018-12-10 20:23:27
@_author: Ray Dillinger 
@_subject: [Cryptography] What if Responsible Encryption Back-Doors Were 
Okay.  Consider the idea for allowing it but not allowing it to remain
secret withdrawn.  It sort of works when there is only one government
you're talking about and they are answerable to the public or at least
capable of shame...  But that, is not the world in which we live.  There
are too many entities who just plain wouldn't be at all restrained by
that fact.
Your point being that even given the inability to use it *in secret*, as
has been the preference of the US with its mass-harvesting programs,
there are plenty of people who just plain don't give enough of a crap
whether anybody knows they're mass-harvesting everybody's data, and
they'd just do it and probably covertly sell it to everybody else who
didn't want to be on record as having mass-harvested everybody's data.
That ... is probably true.  China, for example, really and truly doesn't
give a crap whether its snooping on everybody it can remains secret or
not.  Having a snooping-but-no-secret-snooping mechanism would just
result in them methodically harvesting everything.
And once that started, it would be harder for people to care about
someone else doing it.  And someone else, and someone else....

@_date: 2018-12-30 12:51:50
@_author: Ray Dillinger 
@_subject: [Cryptography] blake2b 160 
Pretty hard.  I don't think you have anything to worry about, although
I'd probably have reflexively picked 256-bit.  Is a post-quantum
future part of your design requirements?  I think quantum gives a
marginal speedup on Blake, but not by more than a fairly small constant
I worry about hashes because the vast majority, including BLAKE and
variants, are subject to length-extension attacks.  That is, if you can
find a string that matches the hash of any prefix of the message, then
the remainder of the message can be appended to it resulting in a
collision on the whole message.  So there is an attack surface at every
block (at the end of every prefix), not just at the final block.
Although not ideal this is generally an accepted design point for
hashes.  People want to be able to hash a stream of data linearly,
without knowing in advance the length of the message, without having to
access anything out of sequence, and with a tightly bounded number of
bits of state.
That said, producing a collision on any prefix is just as hard as
producing any collision.  So even with the attack surface multiplied by
the number of blocks in the message, the prefix collision isn't easy to
exploit.  I worry about "reduced" security because I'm a worrywart by
nature but in practice it's not much of a reduction.
SHA256 reduces the length-extension attack space by appending the
message length to the message before hashing.  That means any prefix
collision has to be on a prefix exactly the same size as the string that
collides with it.  SHA256D effectively eliminates it by hashing twice;
that way the state of the hash function depends on every bit of the
message (from the first time through) WHILE hashing (on the second time
through). The prefix string that collides with something (against
astronomical odds) starting from the default state won't likely collide
again (astronomical odds squared) starting from a different state.
I got no such algorithm.  Offhand I don't see how it could exist.  While
the length extension attack is a thing, marginally, for finding total
collisions, Blake2B has good diffusion in each block so no partial
collision could be extended.

@_date: 2018-02-01 17:55:46
@_author: Ray Dillinger 
@_subject: [Cryptography] canonicalizing unicode strings. 
Well, the real issue is that the minute they get into a habit - ANY
habit - the people you want to be secure against will try to take
advantage of THAT habit.
But there is another problem that you can just sum up this way:
"You can't get people to read the instructions, by putting 'read the
instructions' into the instructions they're not reading."
I think the quote is from Howard Tayler.

@_date: 2018-02-06 15:36:55
@_author: Ray Dillinger 
@_subject: [Cryptography] Proof of Work is the worst way to do a BlockChain 
I think I agree with Mr. Baker that proof-of-work is problematic.
Aside from the issues he points out, Bitcoin in particular has an
additional problem in that its difficulty adjustment takes place in big
chunks and lags by two weeks of estimated time as measured in blocks.
In the worst case, if a high price has been sustained for two weeks,
the difficulty adjustment has been adapted for it, and then a price
crash of this magnitude cuts the price to 1/3 of its former level right
after a difficulty adjustment....
Then its already-inadequate transaction rate is cut from about 3 per
second down to about 1 per second, and stays there for six weeks.
I think his idea of a Notary Digest chain is not a bad one, and can be
used to resolve the short-term vulnerability of some scheme such as
TaPoS which might be more acceptable to the partisans. TaPoS is
completely Trustless and in the long run secure, but it works on a
cumulative total that can be manipulated in the short run, so it
otherwise doesn't work fast enough to pick between the very most recent

@_date: 2018-02-07 19:18:15
@_author: Ray Dillinger 
@_subject: [Cryptography] Proof of Work is the worst way to do a BlockChain 
Notaries, Bankers, Lawyers, and Accountants, are very literally
professionals who people hire because they don't trust each other.  They
are in the trust business.
If they are widely enough untrusted, their businesses fail. So I don't
really have much of a problem with having them act in their usual
professional role here.  They have a sound motive not to fail.
Speaking of trust, there is the idea of a bond, and it's applicable too.
A thing allowed by the *idea* of proof-of-stake is that the miner
forming a block could use their stake to act as guarantor of the
transactions in the block.  This means the miner accepts some risk, and
gets paid for it, and it allows formation of blocks with limited
And that means you could have parallel formation of blocks rather than
just serial formation of blocks - a block mesh instead of a block chain.
 Conflicting transactions might be entered.  When detected, the 'stake'
put up as guarantee would be used to make good immediately, and further
transactions making fiscal corrections then entered in later blocks.
Which is a whole lot messier than the nice clean elegant world of "that
was inconsistent so it didn't really happen" of classical block chains,
but allows forming blocks in parallel because now the position at which
the next block 'attaches' to the mesh doesn't have to hash the immediate
previous block.  And with parallel block formation you can go quite a
ways toward alleviating the block bandwidth problem.

@_date: 2018-02-12 12:16:15
@_author: Ray Dillinger 
@_subject: [Cryptography] Useless side channels 
It's true.  I have a metal box with a tight lid, which is just a bit
bigger than a standard smart phone.  I don't use it any more, but I
still have it.  And the little mylar "liner" thing (which looks just
like the anti-static bag your video card probably came in) that you're
supposed to stick the phone inside, fold over (the phone occupies about
the bottom half of it) and stick inside the box.
I think it was "both" because the mylar baggie tended to wear out
without the protection of the box, and the box's friction-fit lid may
not be reliably tight enough for a Faraday cage.  But anyway, it's both.

@_date: 2018-02-12 12:25:12
@_author: Ray Dillinger 
@_subject: [Cryptography] Proof of Work is the worst way to do a BlockChain 
The problem with this, Erskin, is that I have read your entire message
and have no idea how to actually implement your protocol.
In fact, at this point I still don't know, from a technical and
engineering standpoint, what your "proof of prophecy" is, how it works,
or against what specific threat model we think it secure and why.
Censorship causes blandness!

@_date: 2018-02-16 12:54:39
@_author: Ray Dillinger 
@_subject: [Cryptography] Quantum computers will never overcome noise 
We could get a few more kicks out of Moore's law, without needing to go
all the way to quantum, if we figure out a way to build chips in a fully
three-dimensional way - with nanoscale assembly as opposed to etching
If and when we are able to reliably assemble strands of high-temp
superconductors they will enable huge speed advances two ways:  First,
by direct use in circuits as wires that don't generate heat, and second
by use as thermal conductors to move all the heat generated by gates and
everything else away from the middle of the chip.
Honestly though, I think we should be looking at protocols more
seriously.  Everybody's trying to harden public-key crypto against
quantum, and maybe we should be trying instead to harden protocols
against needing public-key crypto.
We've got several Public-key systems that we don't know how to attack
with QC.  But I'm not clear about whether that's because they're
actually impenetrable to QC, or just because we don't know how yet.

@_date: 2018-02-22 07:49:19
@_author: Ray Dillinger 
@_subject: [Cryptography] Proof of Work is the worst way to do a BlockChain 
I heard that.  I'm going through the same thing, although I'm moving
out of security code as a profession.
All right.  So, for the very first sentence of a technical paper, let us
begin with a specific statement of technical goals.  Since this is a
cryptography list, I'm going to start with the three questions every
piece of cryptographic software that exists must have answers for.
What specific information do you need cryptography to control or
protect, what specific task do you want your application to be able to
do with that information in spite of its being controlled or protected,
and whom or what are you protecting the information from?
PS.  Sorry about getting your name wrong in my earlier reply.  I have
     actually known someone named Erskin and mistook your name for being
     the same as his.  I should have looked more closely.

@_date: 2018-02-23 20:02:25
@_author: Ray Dillinger 
@_subject: [Cryptography] Proof of Work is the worst way to do a BlockChain 
If we want scalability, we need to have the miners able to form blocks
rapidly when that's needed.  Let's say there's no one-per-ten-minutes
regulation, and anybody can make a block at any time, whenever they can
fill one up.  In that case we have to motivate them somehow to NOT make
blocks for no reason or with no transactions or with meaningless
transactions. So clearly it couldn't pay a regular block subsidy.
It is really hard to come up with some means of coin creation that isn't
easily cheated with meaningless transactions by miners who can form
blocks whenever they want.  If anybody has ideas on that front, let's
hear 'em.
Moving on to something I CAN solve though:
I have a solid design for a type of block chain with the following
desirable property:
You can check the validity of any proposed transaction by downloading a
subset of the block chain rather than the whole thing.  You will need
something on the order of the fourth root of all blocks, per block that
a txIn being used in the transaction came from, to make sure that it
hasn't been spent since it was created.  And if the tx that created it
had multiple inputs, you will need something on the order of the fourth
root of all blocks *younger* than the block the tx is in, to find each
of those.  Etc.  Until you reach a known block at which a known txOut
set existed. (But building that known block/known txout set still
requires you to traverse the whole block chain up to that point. )
If you know the validity of the previous block, you can easily check the
validity of the current block, as with Bitcoin.  This structure would
have the additional property that in theory you can prove the validity
of the block without starting out knowing the validity of the previous
block and without needing to download the whole block chain.  In
practice, this depends on proving the validity of every transaction in
the block, probably recursively, so you might need to download most of
the chain if the sets of blocks you need to download for each
transaction don't overlap much.
I could even modify it such that miners could work on any of the next
few blocks simultaneously or post them out of sequence, further
splitting up the mining job for the sake of scalability, and it would
still work.  Hmm.  On reflection, the maximum number of blocks this
could be done with would be the fifth root of the current block chain
size. So, when the block height is 100,000, you could start working on
10 blocks at a time, and when the block height is 161,051, you could
start working on 11 blocks at a time.  This would extend the
confirmation depth, but if you really need to form blocks that fast it
wouldn't matter.

@_date: 2018-02-25 23:29:24
@_author: Ray Dillinger 
@_subject: [Cryptography] Proof of Work is the worst way to do a BlockChain 
It bloats the blocks a bit with additional hashes and lists of
invalidated txIns, but I think it's a net win.
Okay....  we are actually building successively expanding hypercube
Directed Acyclic Graphs, but for the sake of having a vocabulary to talk
about, we can say that we are building "books."  Except, in these books,
each "word" is a block of the block chain.
And the books get steadily larger, so that even a fairly large block
chain ought to fit in a fairly small number of books.
So, in book N, each line has N words, each column has N lines, each page
has N columns, and the book has N pages.  Book N+1 is larger, in each
dimension.  This could be done with more or fewer dimensions but I think
this ought to work fine.
The blocks are multiply linked to each other.  Each block contains the
hashes of the blocks:
immediately previous (previous to it in the same line),
at its same place in the previous line,
at its same place and line in the previous column,
at the same place, line, and column on the previous page,
at the same place, line, column, and page, in the previous book.
Spends cause invalidations of txIns.  Each invalidation is recorded at
the following blocks in the block chain:
In the block where the tx spending it occurs.
In the first block following the spend that has the same place in line
as the tx that created the txIn spent.
In the first block following the spend that has the same position in its
column as the tx that created the txIn spent.
In the first block following the spend that has the same position on its
page as the tx that created the txIn spent.
In the first block following the spend that has the same position in its
book as the tx that created the txIn spent.
Now, an example of how the structure is used:  Alice wants to make a
payment to Bob in the current block, block B.  She has a TxIn that was
created in block A.  In order to know that this payment is valid, Bob
wants to prove that Alice has not already used this txIn in a payment to
Carol in block C.  But Bob has no idea where in the block chain block C
is.   It's okay because he doesn't need to know.
Bob follows links back word-by-word along the current line (which wraps
around if he gets to the beginning) looking for block C until he gets to
the same place in line where the txIn was created.   If he hasn't found
it yet, he starts following links up line-by-line (wraps around to the
bottom, etc) looking for notice of invalidation until he gets to the
same line where the txIn was created.  If he still hasn't found it, he
starts following links back column-by-column until he gets to the same
column where the txIn was created.  If he still hasn't found it, he goes
back page-by-page, and if he reaches the same page in the current book
where the txIn was created, he can start linking back book by book,
checking one word in each book.  Along this path, he's going to hit
block A, where the txIn was created.  He probably won't actually hit
block C.  But if block C exists, he'll find notice that the txIn was
invalidated along this path.
And that's the "vanilla" structure.  There is a likely enhancement that
could be very useful, and an unlikely enhancement that might be worth it
if desperate.
Obviously there are rows of blocks that "point" at things where, in the
previous volume, there are no things.  All the stuff in book 11, column
11 that points at "the same place in book 10" has nothing to point to.
In the first place this is no problem because if there is nothing to
point to there is also no way you need to point there.  In the second
place these hashes are at known locations so they could be used for
other purposes (like an ancillary block containing a database of known
valid TxIns after each page, with a diff after each column - for example).
If you want parallel block formation, you're going to give up the direct
prior-to-successor linkage of word-to-word at the line level, instead
having linkages that skip over a few blocks going back along the chain.
Under parallel block formation, however, you would need a lot of
ancillary rules to keep transactions compatible with only blocks that
are otherwise doomed to conflict (published with the same block number
for example) so that they don't cause train wrecks by getting played
into other blocks.  You would also need to keep your validation code
aware that two blocks are not necessarily part of the same branch until
there is a successor with hash paths that lead through both back into
the rest of the structure. The stuff you have to keep track of to
accommodate parallel block formation is complex, and I wouldn't
recommend it unless it becomes necessary.

@_date: 2018-01-06 15:28:42
@_author: Ray Dillinger 
@_subject: [Cryptography] Speculation considered harmful? 
If the real impact of this class of attack is as it seems, "we need to
fundamentally redesign our CPUs", then the obvious question is "what is
the best way to achieve the desired result within the now
better-understood design space?"
This has me thinking about different basic architectures with different
approaches to getting a lot of computing done in a short time.
One of them is VLIW, or "Very Long Instruction Word," which exploits
deliberately explicit instruction level parallelism rather than implicit
(speculative) instruction parallelism.  It amounts to having a wide pipe
to instruction memory and loading/executing parallel segments of code
_all_belonging_to_the_same_process_ as though they were single
instructions, explicitly in parallel. Context switching occurs between
VLIW segments, with all cache and register switching done at that time.
VLIW architectures had explicit preloads, which went into a VLIW several
cycles before a cycle that made use of the preloaded data, and
instructions that modern CPUs achieve by means of speculation -
simultaneously updating dozens of weak-ish cores with different
sub-instructions for dozens of VLIW cycles, and then either integrating
the results or "realizing" one by having it write its results to a
shared variable.
It didn't get much tread in the middle '80s and early '90s on account of
the machine code being pretty baffling to humans, and our compilers not
being very efficient at the parallelization task at the time. But to a
first approximation humans no longer look at machine code at all, and
our compiler tech is a hell of a lot better now.  Finally, offloading
the work of figuring out parallelization from the CPU's real-time tasks
means you could run the things while generating significantly less heat
and consuming significantly less power.
So....  I've been thinking, and that's one of my thoughts.

@_date: 2018-01-12 10:31:31
@_author: Ray Dillinger 
@_subject: [Cryptography] Caches considered harmful 
I'm with you on that, but there is a caveat.
Autopilots in particular, if too predictable, are open to attacks
against herds of cars resulting in massive traffic jams etc, which are
non-issues if they are individually somewhat probabilistic.
More generally, in dealing with chaotic systems, dogged predictability
is no advantage.
Cryptography, mortgage balances, and door locks have simple input and
simple requirements.  Dogged predictability is exactly correct.
MRI machines and autopilots interact with chaotic systems.  In the
absence of any possibility of predictable input, there is no advantage
that can be shown for predictable vs. probabilistic behavior - both are
approximations. Predictable can't be made to fail less often, nor made
reliable more easily, than unpredictable.

@_date: 2018-01-14 20:46:34
@_author: Ray Dillinger 
@_subject: [Cryptography] canonicalizing unicode strings. 
Yes there is.  This file summarizes known unicode homoglyphs and
Here is a utility for generating lookalike strings, based on the
information in that file.
It's pretty much a given that strings which are in mixed alphabets AND
contain characters in one of the alphabets that are homoglyphs for any
character in another alphabet used in the same string, should never be
allowed as URLs, identifiers, usernames, titles, product names,
certificate identifiers, etc.
It's also pretty much a given that VERY few CAs, chat boards, social
media platforms, e-commerce sites, etc, check for and enforce any such rule.
The results are predictable.  Lots of fraud and impersonation happens
with homoglyph attacks.
Unicode homoglyphs are a security nightmare.  You can search "homoglyph
attack generator" anywhere for pen testing tools and script kiddie hacks.

@_date: 2018-01-23 16:37:08
@_author: Ray Dillinger 
@_subject: [Cryptography] Cicada 3301: Into The Unknown 
Why did someone tweet a bootup instruction sequence in binary, and
what does Cicada 3301 have to do with it?

@_date: 2018-01-25 16:10:50
@_author: Ray Dillinger 
@_subject: [Cryptography] Perth Mint to back crypto-currency with gold 
A ledger recording disposition of assets is probably the most obvious
use for it, but those transactions can be anything.  Or, to think of it
differently, maybe you're exactly right about what it's for but you have
to think of "ownership of assets" in a much broader sense than real
property or money or liquid assets.
Having a document that you can prove is unaltered is good, but being
able to prove that you have the complete set of such documents (that
none have been withheld from your knowledge), day after day as elements
are added to the set, no matter who adds one or where, is better.
One of the first uses of it after bitcoin was namecoin, where the
purpose of the chain was to record cryptographic certificates for use in
the "deep net" .bit domain.  There were also coins on that chain, but
they were entirely tangential.
Another obvious application would be an inalterable record of votes
taken on various topics, or of diplomatic proceedings that it is very
very important that nobody should later be allowed to lie about.
Heck, maybe one day The Sororal Order Of Martian Clowns will use one to
record who has passed which of their membership levels and trials, and
then the membership will rest assured that nobody back at Candor Chasma
(or wherever they have their HQ) will be altering the records or
backdating them or striking someone who annoys or opposes them from the
rolls, or sneaking in their sister-in-law even though she's sane and
therefore unqualified.  And new members just joining will be able to
check it and see that nobody back at Olympus Mons ever had been allowed
to do so, because they can know that the record they receive, even
though they weren't there while it was being done, is the complete record.
Absolutely.  You can't have distributed production of coins unless you
have (provable!) distributed production of gold to back the coins and
(provable!) delivery of that gold whenever someone wants to redeem their
coins and take possession.  So this is going to be a block chain that
isn't completely decentralized in the way Bitcoin is. Anybody anywhere
can create a transaction, or see the record of all transactions. But
only Trent could create transactions that create or destroy coins on the
chain.   That's because it's Trent who oversees the holding and
redemption of the backing goods, and has to prove it, with audits and
inventories and affidavits and banking paperwork and KYC/AML compliance
inspections and all that entails....
Now, if there's a good reason to separate the creation and destruction
of coins from the creation of blocks, "mining" for the creation of
blocks could still be distributed.  But I don't know if there is such a

@_date: 2018-01-28 11:23:05
@_author: Ray Dillinger 
@_subject: [Cryptography] DAG vs Blockchain 
The proof-carrying Directed Acyclic Graph (the rough parallel to a
block chain) needs some additional magic to provide proof of
completeness.  IOW, you have to find a way for someone to guarantee
that they have all the information relevant to what they want to do.
This is a problem that probably has a good crypto solution, but so far I
haven't seen it solved in a way that would allow a secure cryptocurrency
application to scale better than a block chain.
In a cryptocurrency application, let's say Alice has a TxOut she got in
an earlier transaction (from Zebulon) and wants to make a payment to
Bob.  Alice can use either a block chain or a DAG to show Bob that the
transaction where she got the payment is valid. She can show the
block containing that transaction, and then show previous blocks,
with hashes that verify, in a path that leads all the way back to the
genesis block.  Nice enough.
But that isn't enough for Bob to know that the payment is valid.  He
must also know that Alice hasn't already used the tx output to pay
Carol, in transaction after she got it.  With a block chain, any such
transaction from Alice to Carol must be in one of the blocks on the
singular path from Zebulon's payment to Alice, to the present.  Bob
knows that if he follows the block chain back and doesn't see it, then
it doesn't exist.
But With a DAG, the payment from Alice to Carol (if it exists) may be on
some branch Bob hasn't seen.  So unless Bob has every block of every
branch from the moment Alice got that payment, that transaction
invalidating Alice's proposed transaction to him could be somewhere he
hasn't seen.
Unless there is some property of a transaction that makes it possible
for Bob to prove that he has already looked at every block in the DAG
where that transaction might be, and the set of blocks he needs to
download in order to prove it is also much smaller than the total number
of blocks that have been created since the txOut was created, then there
isn't a scalability advantage over a block chain.  There probably is a
good cryptographic way to achieve this but I haven't seen it yet.  I'd
call it a six-banana problem, meaning probably in about three days of
thinking hard and ignoring everything else, I could either come up with
a practical protocol (which may work without being a solution for the
most general case) or become satisfied that there isn't a practical
Now, everything I've seen so far has Bob either downloading every block
of every branch hunting for that transaction, or blindly (without proof)
accepting the assertion of other nodes that no such transaction exists.
Bearing in mind that those peer nodes have the same problem as Bob in
terms of not having complete information.
I don't believe blindly trusting peer nodes also in possession of
incomplete information, without proof, is an adequate guarantee for a
cryptocurrency application. And if Bob has to have every block of every
branch down to the current moment, then what's the scalability advantage
over a block chain?

@_date: 2018-01-29 11:33:06
@_author: Ray Dillinger 
@_subject: [Cryptography] US contemplating its p0wn Great FireWall for 5G 
We are not on twitter.  You can say clearly what you think twitter
is censoring here.  I myself have never twat, so I'm curious.
But this was perfectly obvious and reminding you of it makes me wonder
if perhaps you intended to communicate something other than what you said.

@_date: 2018-01-29 11:39:37
@_author: Ray Dillinger 
@_subject: [Cryptography] US contemplating its p0wn Great FireWall for 5G 
And now, clear all your cookies, go to Bing, and try the same search.
*gasp!*  *shock!*  DuckDuckGo is just repackaging what Microsoft wants
people to know!!!
DuckDuckGo is just a frontend/repackaging/licensing deal/whatever for
Bing content, made accessible without cookies and overt tracking. You're
still getting spun; it's just the spin from Redmond instead of the spin
from Mountain View.
I don't trust Google in particular.  But I have no reason to trust
Microsoft either.  Why should I be trusting that search results from
Bing are impartial or unbiased?

@_date: 2018-01-30 10:21:20
@_author: Ray Dillinger 
@_subject: [Cryptography] DAG vs Blockchain 
My thought had been more in the line of simple topology and duplication.
Each block would be part of a 'tree' with root at the genesis/root node,
and at each layer of the tree the blocks are linked together.
Each block would be required to re-record any transactions that had
occurred in the most recent "fringe", the leading N digits of any of
whose txIn IDs matched those of its own block ID (where N is the current
tree level).  Each block would also be permitted to record any new
The idea being that in order to check the validity of a txIn, you
can jump links sideways to the branch where it would be re-recorded,
then rootward to the creating transaction's layer, then sideways again
to the block containing the creating transaction itself. If you haven't
found a payment spending that txIn, then it doesn't exist.
The completist will continue rootward from there to the genesis block.
This could result in some transactions being recorded many times.  But
it would guarantee a log2(blocks) path to check the validity of any
txIn, without creating any 'bottleneck' blocks or any blocks that give
anyone special motives to manipulate or control.

@_date: 2018-03-08 14:21:03
@_author: Ray Dillinger 
@_subject: [Cryptography] How fast can a blockchain go ? like Zilliqa? 
The particular constraints of contract checking being a sub-operation
of block chain checking drove that decision; if something is not visible
from data in the block chain, then it is not visible for purposes of the
contract.  So you can't have a contract that says, eg, that someone will
build a house, because somebody's computer in Singapore checking the
block chain can't see whether somebody's house in Nairobi was built or not.
Further, the choice to make a trustless system with no privileged nodes
in Bitcoin forced pseudonymity on the users;  The computer has no idea
who in the real world a contract refers to, because there is no trusted
node that can check identities and confirm that, yes, the signatory is
in fact a human being corresponding to this legal identity.  In order to
check ID's, you have to introduce Trusted nodes - ie, you'd have to give
someone the power to cause the system to malfunction by acting in bad
faith the way Certificate Authorities do for the www.
Between those two design choices it really is nearly impossible for
Bitcoin's smart contracts to refer to much of anything except Bitcoins.
You can make different design choices with a different block chain, of
course, to make smart contracts that apply to a much wider variety of
things and even refer to particular people.
One main approach would be enriching the block chain with information
about the things the contracts are about -- for example if you track
legal identity documents, and stock certificates or other financial
instruments, then you can have a block chain with contracts about
particular people trading in stock certificates and other financial
instruments. But you need to get those ID documents onto the chain in
some way, and that means you probably need Trusted nodes such as
Certificate Authorities. Sorry, there's just no getting around it.  If
you want legal ID's, you have to have a Trusted node somewhere.
Brevity is the soul of wit in Bitcoin contracts, because space in blocks
is at a premium.  This is also a problem, but it is one of the forces
that keeps the contracts simple.
The other force that keeps the contracts in Bitcoin simple is something
that I'm giving Hal Finney credit for.  When reviewing Satoshi
Nakamoto's code, Hal diked several instructions out of the FORTH-ish
language used for Bitcoin's not-excessively-smart contracts for
security's sake, leaving them as just-barely-smart-enough contracts.
The language is simplified by the removal of backward-branching control
instructions, meaning whatever you're going to do you have to do it by
stepping exclusively forward through the contract code.  You can skip
forward over some instructions, like skipping the alternate branch with
an IF, but you can't skip backwards, as with a FOR or a LOOP
instruction, and you can't call subroutines.  This keeps contracts
simple to analyze and keeps bugs easy to spot, but restricts complexity
and expressiveness.
Vitalik Buterin decided that he wanted to run a block chain with a full
Turing-capable contract language, meaning backward branches and
everything left in, plus the ability to refer to other contracts as
subroutines. Ethereum smart contracts are very very expressive, but it's
not always clear what they do,  scammers have been known to present
deliberately misleading contracts and tell bare-faced lies about what
they do, and unbelievable amounts of Ether have been stolen or just
plain destroyed due to bugs in those contracts.

@_date: 2018-03-08 14:50:58
@_author: Ray Dillinger 
@_subject: [Cryptography] Some possible contracting work, or PT/FT employment, 
Speaking of integrating data collection into block chain applications -
an acquaintance of mine is actively searching for contractors/employees
with crypto experience, block chain familiarity, and a bunch of
integration skills to hook value-adds, controls, and data feeds into the
back ends of existing platforms and services.   He says they are using
mainly Python, C, and C++, and if they can get the work done they really
don't care what city person doing the work is sitting in at the time.
It is not primarily a cryptocurrency or ICO endeavor.  There are tokens
on the chain but they're used mainly for back-end purposes and probably
won't wind up being visible to humans except as evidence when
specialists are analyzing the chain history to figure out how and why
automated interactions worked out the way they did. They are unlikely to
be bought or sold on exchanges.
Aaaanyway.  I don't want to hijack a discussion list with job talk. Send
me email offlist if you're interested, and if it doesn't look like
complete mistake, I'll make introductions.  It's not clear how many devs
they need but certainly more than two.

@_date: 2018-03-09 14:50:07
@_author: Ray Dillinger 
@_subject: [Cryptography] On those spoofed domain names... 
We've beaten up on the Unicode committee so often, on this list, for the
lookalike characters that mislead humans, and the alternate encodings
that break hashes, and the alternate codepoint sequences for the same
character that screw with any search for substrings, and .... it just
goes on and on.
We don't have to beat up on them again.  We really shouldn't.  And yet,
I just can't look at krebs' article without comment.
An this, in my estimation, is a big design failure on the part of the
Unicode committee.  The perfectly reasonable impulse that drove it was
an inevitable interpretation of their mission, but "design by
accumulation" is not design.  It produces piles, not structures.  And
Unicode is a pile.
Data gathering is the first part of good design, but, impatient for
results, they made the mistake of doing the data gathering only and
slapping the term 'standard' on page after page of mappings that should
have been considered to only be the notes outlining their scope.
They'll never finish a standard.  They don't even want to anymore.
They'll still be working on that thing a hundred years from now, and
they're even now promoting a view of characters and language that
justifies continuing to work on it forever, instead of finishing it,
using it for the several centuries it'll take until significant reasons
develop why it's not working, and making a new standard then.
Adults are wasting their time cataloging new poo emoji that someone
invents every week and forgets a year later, because "language is
unbelievably complex...."  And they'll do it forever.
Unicode now contains characters that no one, ever, will need to write
except to document Unicode.  And every one of them is a security risk to
the extent that it can be confused with any of the others.  That's
stupid design.

@_date: 2018-03-21 14:14:37
@_author: Ray Dillinger 
@_subject: [Cryptography] Avoiding PGP 
People bag hard on the way private mail plugins work.
The problem isn't that the programmers aren't able to make the plugins
behave the way they should; the problem is that a thousand people who
assume with no evidence or consideration whatsoever that they know
exactly how the plugins "should" behave - that of course they are right
and everybody else is wrong - have never even considered what the hell
their assumptions even MEAN in terms of security.
You don't get privacy in a building if you don't allow builders to
install doors that close.  You don't get security if you don't let them
put locks on some of the doors, and then actually lock them sometimes.
And they're not locked if nobody has to do anything out of the ordinary
to get them open.
I'd be perfectly happy if there were never any confusion of encrypted
mail with unencrypted mail - in fact I'd prefer to be using entirely
different applications for private and non-private communications, and
rest assured that the mail program not designed for security or privacy
simply had no access whatsoever to addresses or contacts or header
information pertaining to messages other than the plain-vanilla SMTP
stuff it knows how to handle.
Failing that?  I don't want something to decrypt just because somebody
using my machine looks at it.   Not unless whoever is sitting behind the
keyboard can actually unpack the key for that message, at any rate.  I
don't want a message received encrypted to be stored in plaintext. I
don't want any part of it to ever be quoted in plaintext in another
If I delete a key, it means that I *INTEND* for messages stored
encrypted to that key to become unreadable.  If they fail to become
unreadable, or if there is any way they can be "recovered", then the
application has done the wrong thing.
Nothing that can be configured to run an executable attachment, or which
loads assets over the network when someone reads messages (like graphics
in "html mail"), or which by default launches a browser for clickable
URLs, should also be handling messages, addresses, or keys intended to
remain private.
This is sort of like saying you don't run around naked in public if you
don't want naked pictures of yourself published.  Pretty straightforward
But it's the first thing people point at when they say they want the
plugins to be "Better."  Thing is, they don't mean "Better" in the same
way I do.  Mostly they want what I would call "Worse."

@_date: 2018-03-23 16:19:26
@_author: Ray Dillinger 
@_subject: [Cryptography] Does RISC V solve Spectre ? 
Shouldn't all of that stuff be in the compilers, where it can be done
ahead of time and without occupying silicon nanoacres, and without
converting watts of electricity into heat during runtime?
Speculative caching is strictly less efficient than a compiler-generated
instruction that says "precache xxxxx to yyyyy now" getting executed
well before needing the data.
Speculative execution is strictly less efficient than an instruction
stream that explicitly says to do both things before the branch and then
reuses one set of registers/information or the other for some different
purpose after the branch.
Instructions that are already in an efficient order, getting executed in
order is strictly more efficient than out of order execution.
Fixing this won't create a performance penalty in the long run; it will
fix one instead.
People wanted to change the fundamental chip architectures without
changing the instruction set, so that the stuff compiled for earlier
chips would continue to work.  So they spent silicon and security on
building very fast but buggy interpreters of earlier or simplified
versions of their instruction sets, instead of using an updated and
restructured instruction set that would require recompiling everything
to get the goodies with their new chip architectures.
But it's never been the best answer for efficiency or speed; only the
one with the lowest barrier to entry for deployment.
Definitely deploy RISC.  Deploy each new generation of RISC with a
clean-slate design that uses explicit instructions to just say exactly
what to do and compilers that know how to tune for that architecture
using those instructions.  Don't do contortions trying to emulate some
earlier instruction set, and this problem goes away.

@_date: 2018-03-23 19:40:56
@_author: Ray Dillinger 
@_subject: [Cryptography] Does RISC V solve Spectre ? 
You're being facetious of course. Compilers don't need to "remember"
what they were doing and why they asked for data to be precached; like
every other instruction, pre-caching instructions are generated in the
first place from the actions and reasons.
But obviously you know that.  So you must be making a joke but I don't
know what you're making a joke about.
Determining when to undertake these actions is not too hard for the
logic overhead expressed in silicon which presently runs during the code
execution. Moving that overhead to the compiler doesn't make it more
complex than it already is.

@_date: 2018-03-27 12:26:16
@_author: Ray Dillinger 
@_subject: [Cryptography] Justice Dept. Revives Push to Mandate a Way to 
Nope.  The "third degree of the law" is a threat that gets responses,
regardless of whether its application would be legal, regardless of
whether the people making the threats have specific knowledge of what
their victim did and how, and regardless of whether the person
threatened actually did anything wrong.  That's one of the reasons why
it's considered unconstitutional.
The rubber hose is an attack on the motivation to keep secrets, not an
attack on any technical part of the system.
The only move that beats an opponent motivated to kill you for your
secrets is the motivation to die to keep them.

@_date: 2018-03-28 14:26:50
@_author: Ray Dillinger 
@_subject: [Cryptography] Specialized crypto processor architectures ? 
Probably two distinct buses with different size words.  One instruction
bus (32 bits? 64? depends on instr and addr structure) And one data bus
(probably 1024 bit blocks, if we project "ordinary" block ciphers into a
post-quantum world by doubling block sizes).  Multiple internal buffers
for both, totalling megabytes long in both cases.  Data instructions
mainly operate on or between buffers, doing operations in parallel or
serially at intervals of one block (for internally defineable block sizes).
One special instruction is "flush all" - it returns every bit on the
crypto coprocessor to a known state, including bits that cannot be
directly read.  I think "flush all" is essential because it's not a
Trusted system, and that therefore a key enclave should be a separate
component from the crypto subprocessor.
This is not a general purpose computer.  A "wall" exists between
instruction bits and data bits.  This starts with separate pins on the
chip die to carry the addresses and separate pins to provide channels
for incoming/outgoing bits, specifically to support special treatment of
these lines in motherboard design and visual/electronic tracing of these
lines to verify designs. Internally, Nothing read on the data bus can
ever find its way into an instruction buffer and nothing can be written
on the external instruction bus.
An Internal MMU is required to ensure that the external address ranges
mapped by the instruction bus never overlap any address ranges which
have been mapped onto the data bus since the last "flush all."
Additionally the "data" bus uses three offsets, also protected by the
MMU to prevent unintentional overlap: Plaintext, Ciphertext, and
KeyMaterial (which includes keys, RNG states, IVs, etc) have different
semantics supported by specialized instructions.
(which ranges have been used as data are by definition forgotten when a
"flush all" is executed and therefore cannot have semantics).
The instruction set is one hundred percent explicit with no speculative
anything.  There are no semantics which are both dynamic and implicit.
To achieve the information bandwidth required to remain performant while
not raising the bit bandwidth to the point where it interferes with
performance, use an explicitly compressed instruction stream.
For explicit compression think "dynamic management of microcode tables",
not "gzip baked into silicon."  Essentially I mean the instruction set
includes "compressed instructions" like "store the subsequent N
decompressed instructions in compression buffer  and "execute
compression buffer  etc.  "Decompressed instructions" are defined as
those which do not have any semantics that depend on the compression
buffers.  Compressed instructions can not be written into the
compression buffers nor executed from the compression buffers.  On-chip
memory is pretty cheap; you can probably have a lot of fairly long
compression buffers.
supported by reasoning.
"It's okay, it's not permanent.  It'll only do that until it explodes."

@_date: 2018-05-04 19:37:11
@_author: Ray Dillinger 
@_subject: [Cryptography] Security weakness in iCloud keychain 
A whole lot of stuff is becoming unusable UNLESS you let something store
your passwords digitally.  Like Kent, I do not want passwords stored
anywhere on any computer.  Yes, I prefer to type them.  EVERY time.
I acknowledge that other people have other opinions, but if I can't tell
a so-called "password manager" that it is supposed to put up a damn
dialog box literally EVERY time ANYTHING wants a password, and NEVER
store them in any non-volatile storage, then I do not want that password
manager and I do not want any software that depends on it.
But even that's not enough.  A lot of "Helpful" software doesn't even
ask the "password manager:"  it simply stores passwords by default, or
even when I specifically have a setting that says DON'T store them.  On
investigation there's always some weaselwording that the setting
"doesn't apply to this other function, only to THAT function..." or that
"this means we don't put it on our cloud, but it still goes around so it
could get stolen by anyone who got his hands on one of your other
machines...." or something.
But the thing which is wrong is that passwords ARE BEING STORED, period!
The whole point of a password is that it is something which is NOT
STORED ANYWHERE ON ANY COMPUTER.  If it is ever available, it is
supposed to be because you personally are present and have just entered
it, authorizing that machine (and no other) to do something at that time
(and no other). That is why passwords are accepted as a confirmation
that something expresses your actual and current will and intent.
I don't want a password I entered six hours ago taken as permission by
software to do anything now.  I don't want a password I entered on my
work computer to be usable by somebody who manages to steal my damn
phone. I don't want work VPNs or mailboxes to be openable on my personal
box. My work machine should never inadvertently gain access to private
mail or my porn stash.  And if I'm getting a new phone, one of the
reasons will be specifically because I DON'T want any part of the
previous configuration or old secrets present on it ever.
If any password is ever available to any program when I am not currently
using that program, or at some time long after I have entered it, or on
some machine other than the one where I entered it, something is WRONG.
Privacy is the ability to manage access to my information.  That
includes giving me the choice to NOT have access to it on the wrong
machine, at the wrong time, or from the wrong location, or in the wrong
We need the power to keep separate parts of our lives separate, and the
power to minimize the damage that could be done via any single stolen
machine.  The people who create password managers are largely ignoring
that need.

@_date: 2018-05-07 12:05:56
@_author: Ray Dillinger 
@_subject: [Cryptography] Security weakness in iCloud keychain 
Okay, point:  high-entropy passwords are hard to remember.  I cheat with
both hands here.
I have a lockbox next to my desk in case I forget one of them, but
mostly I remember some relatively simple passwords (long-ish, but easy
phrases to remember).  One of them is special, and is not written down
even in the lockbox.
I wrote a password mangler (as opposed to a password manager) which does
not access the disk, at all.  I type the "special" password into it
(serves as an encryption key) then one of my "simple" passwords. It
coughs out a string of gobbledegook.  The string of gobbledegook is the
actual password as known to the site.
If it's not a very sensitive one, I cut/paste it into the password
dialog.  But cut/paste on a lot of desktop managers gets recorded, so if
it is sensitive, I retype it.
I can repeat the encryption process on a sheet of notebook paper if
necessary; it's just a pen-and-paper cipher.  But sometimes the ability
to get a mangled password is important when I don't have access to a
computer that I control.  Once in a great while, I want one of those on
my phone, or on someone else's wifi, even if I have to burn and replace
it afterwards.
But I can have that program on every computer I use.  It stores nothing
and doesn't stick a password into anything or transmit it anywhere
without my explicit and current permission.  I am quite happy with its
I need a command-line argument for different cipher alphabets because
some sites don't allow some punctuation and others require it. But if I
ever forget which contexts require which command line arguments, the
error messages from the site remind me.

@_date: 2018-05-14 09:21:07
@_author: Ray Dillinger 
@_subject: [Cryptography] Vulnerability found in PGP. 
Details not announced yet, but evidently a protocol vulnerability
and evidently quite bad. EFF recommends disabling anything you've
got that automatically opens PGP-encrypted mail.
It's unusual for something that's been this widely studied for this
long to have something that's of such concern; I'll be watching
with interest to see what it is.

@_date: 2018-05-14 12:12:58
@_author: Ray Dillinger 
@_subject: [Cryptography]  
=?utf-8?q?_encrypted_emails=E2=80=94_=3Funinstall_now=3F?=
Speculation currently is that there is some information leakage
to an HTTP server when an HTML webpage is rendered from a URL
given in an encrypted email.
This is speculation, not certainty.

@_date: 2018-05-14 12:26:48
@_author: Ray Dillinger 
@_subject: [Cryptography] Vulnerability found in PGP. 
Okay, yeah, it's dumb. Attacker takes your old mail that he wants you to
decrypt for him; manipulates the message using S/MIME to slice & dice it
in parts creating a "fake" message where the message he wants decrypted
is part of a URL inside an encrypted message; sends it to you; and when
your system decrypts it, it goes to auto-fetch that phony URL, which
gives him the plaintext of the original message.
It has always been obvious that the auto-fetch of HTML content is an
unsound security practice.  S/MIME's capability to cut and join messages
at arbitrary boundaries after applying processing (including decryption)
to selected substrings makes it worse.  This subverts most standard
methods of document and protocol design for email use.

@_date: 2018-05-19 11:07:44
@_author: Ray Dillinger 
@_subject: [Cryptography] Durable HSM with fingerprint reader? 
I'm looking for long-lasting, tamper-resistant secure hardware that does
a cryptographically simple job.  It stores a bunch of unique keys, gets
to sit on people's desktops, and in principle never needs to be moved
(except that someone might want to store it to keep it safe - it is
potentially a high-value target for thieves or saboteurs).
It needs to have a fingerprint reader - a pretty good one in terms of
being hard to spoof. Other than that a red LED and a green LED are the
only UI necessary.
It needs to get time from a reasonably reliable external source such as
a GPS signal and/or the atomic clock broadcast. Correctness of time will
also be enforced by limited time windows during which particular keys
are authorized for answering challenges.
It's a device for the user to periodically check-in and assert that "I
am alive and free, today."
And it needs to be reliable enough to last a very long time.
Any candidates?

@_date: 2018-05-20 15:54:09
@_author: Ray Dillinger 
@_subject: [Cryptography] Durable HSM with fingerprint reader? 
It would need to be FIPS-3 for a regulated industry (finance/insurance)
and if we managed to sell a few people on deploying their plan in that
particular form, it would be a market in the tens of thousands.
But it looks like we're going to be focusing on trying to sell them on a
different form, at this point; in the first place we can't find a device
sufficiently robust and long-lived with absolutely minimum attack surface.
In the second place it's unrealistic to expect that no (or very few)
devices would be just plain lost over such a long period of time, no
matter how motivated clients are to keep track of them. In turn that
means critical keys used to create secrets to be later revealed only
when the device reveals them, can't be permanently resident in the device.
So the clients can't really be expected to keep absolute control of
their sensitive data by keeping it in an irreplaceable secure enclave
which they can just lose.  The risk of possibly losing the device,
multiplied by large financial stakes, is just too dire for people to be
expected to accept.  They'll prefer to trust an authority with their
data who can, at least, re-key or replace them.  And with the authority
knowing who they are, they can occasionally contact people to make sure
they're really okay, so we don't wind up totally reliant on the devices
to avoid long-running fraud.
That makes the particular device, far less critically important.  We
may still deploy a device, but it would be a convenience, not a
linchpin, and probably used for additional purposes.
So now we're looking at building a cryptographic chinese wall elsewhere,
between people with different pieces of information that would need to
be used together to bring about the "moral hazard" or to speak more
plainly "criminal opportunity" we're trying to prevent.  It brings about
a collusion risk, but it would be a collusion between people subject to
regular reviews and audits, who don't normally have any direct contact.

@_date: 2018-10-31 21:16:23
@_author: Ray Dillinger 
@_subject: [Cryptography] Ten years ago today 
I can't speak for Hal, of course, but TBH, I expected it to have sharply
limited scope.  Like, for a small community or a campus or so, and
probably for a limited length of time like a semester or two.  Large
enough and long enough for a research paper or case study.  Bigger than
that, I thought, and the bandwidth requirements would cut it off at the
knees.  I was surprised when, a week or so before it happened, I learned
that Satoshi intended an actual launch to the general public.
I did some work on it, because I have a particular personal interest in
digital-cash protocols and because it was interesting.  Not because I
thought there'd ever be a chance of a nonzero valuation more than, say,
a year or so after launch.  Digital cash protocols always fail.  Or at
least, they always had.  We ALL knew that. But it was at least as
interesting as David Chaum's protocol, or several other incremental
improvements on it, and drastically reduced the bandwidth and time
complexity of the Byzantine Generals problem (given the assumption of
distribution of compute power, which has not proven to be reliable).
If I'd thought there'd be a nonzero valuation, I'd have actually mined
it.  I didn't.  Sigh.  Twenty-twenty hindsight.
I'm barely even a blip on the list of suspects, but the occasional
rumors about *me* having some ungodly number of coins stashed somewhere
are, for the record, categorically false. Whenever people even speculate
about that, whenever anybody even thinks it's possible, it gives me the
willies.  "Satoshi" rumors frankly scare the crap out of me because of
what somebody might do to some innocent person (or to me personally) in
an attempt to get at Satoshi's coins.
Bitcoin was different from previous digital-cash schemes because,
lacking Trusted roles, it seemed to be a non-scam.  Or at least, there
wasn't any scam built directly into the protocol.  It was the first
non-scam digital cash system I'd seen in YEARS.
Satoshi didn't make himself into a Trusted gatekeeper who had special
access to coins, the ability to steal from users, or any special ability
to track or exclude anybody.  He wasn't doing the "give me your actual
valuable dollars (or gold or whatever) and I'll give you these
probably-eventually-worthless coupons I printed."  If he had been, I
would have concluded he was scamming and wouldn't have given him the
time of day.
That, you'll recall, was the standard modus operandi of a huge number of
the digital-cash schemes that had been attempted before  (where the
people in Trusted roles usually wound up going to jail). Not doing it,
as far as I was concerned, was the most important NON-technical
distinguishing feature of Bitcoin, and of Satoshi.
Most of the block chain copycats of course did create Trusted roles.
With at least special access to coins (which they call a "premine"), and
frequently some of those other scammy properties too.  And lately
they've taken to "ICOs", which is apparently the new name for the "give
me your actual valuable dollars (or gold or whatever) and I'll give you
these probably-worthless coupons" scams.

@_date: 2018-10-31 14:10:06
@_author: Ray Dillinger 
@_subject: [Cryptography] hash size 
"Just barely possible?"  Unless it's a hash that's specifically made to
be inefficient to compute, then searching 2^64 examples is something
that can be done in a couple of days on a single server rack.
The point of a hash is to be efficient to compute and check but hard to
reverse.  Inefficient computing, obviously, doesn't get you both things.
 It only gets you "hard to reverse."  An efficient hash - one that
doesn't impose a burden on the servers or checkers - needs output bits,
not inefficiency.

@_date: 2018-09-03 10:45:06
@_author: Ray Dillinger 
@_subject: [Cryptography] WireGuard 
While there are some ciphers that had good security reasons for expiring
(Remember BassOmatic and its tremendous variety of classes of weak keys?
DES and its laughable key length?)  I don't recall very many outright
sudden failures where people woke up one day and discovered that a
cipher everybody'd believed secure in the previous week could now be
broken in realtime.
A more typical scenario, as far as we've learned, is that people can see
cipher failures coming, years away.  An attack that reduces work by a
percent or two is found; eight months or so later someone figures out
how to extend it resulting in a 50% work reduction; a year after that
people take note and downrate the cipher from 256-bit to 250-bit
effective security (even though the work factor with currently known
attacks is still 255 bits) and some alarmists say "It's likely to be
broken fairly soon, we should stop using it."  Then a year and a half
after that, someone finds a way to extend the attack recursively, but
each recursion increases the computer power burden by a factor of ten.
With the breakeven point on this new attack, the work factor comes down
to about 253-bit security.
About that time, the alarmists' opinion goes mainstream and folks start
phasing that cipher out ... but this is YEARS after the original attack
was found, and nobody can yet actually read any messages encrypted in it
without converting an entire star into energy and using it all to power
nanoscale computronium that nobody can build yet. And that cipher falls
out of use and is considered broken, and gets replaced by something else.
Meanwhile, all those developers who have multiple ciphers supported, are
in their software up to their elbows, tinkering with key management,
tinkering with protocols, tinkering with algorithm negotiation, and
every line they write is an opportunity to make a catastrophic mistake.
Crippling vulnerabilities in protocols, algorithm negotiation, and key
handling, are routinely discovered between Friday Night and Monday
Morning leaving entire vast tracts of communications infrastructure
insecure with no warning whatsoever, with an implementation in the hands
of Black-hat attackers arriving before the following Thursday.
If I were designing something to have a five-year upgrade cycle, it
would be the right choice to have a cipher currently believed secure,
with key at least twice as long as "justified" for the task at hand
given known attacks, absolutely minimal protocol, and no algorithm
negotiation.  There's no way around key handling, but using a single
algorithm and a single key length can keep it simple.  So you can at
least reduce or eliminate the biggest known design security problems.
If it should become necessary to change the algorithm, I'd expect four
or five years to get a new version out before any possibility of an
actual cipher algorithm break enabling real-world attacks.  I'd have no
such confidence about an excessively complicated protocol or negotiation

@_date: 2018-09-19 19:32:49
@_author: Ray Dillinger 
@_subject: [Cryptography] Previously unknown (I think) Malware 
I have recently become aware of some previously unknown malware, and
need to move it toward analysis/publication/eventual CERT advisory.
It is tentatively named "Gaslight" because for a while the discoverer
thought he was going crazy.
It infects bluetooth devices and definitely does jump between paired
devices.  It was first observed on an Android/Pixel smartphone. I do not
know enough about bluetooth hardware to attempt to extract and analyze it.
I do not know the full extent of what it does.  I know some of what it
does and it's damned scary (turns almost any bluetooth device into an
audio bug - even devices without speakers or microphones).
I know some of the other things it does and there's no comprehensible
motive for a crook to make malware that does that. So there are some
serious unsolved puzzles.  I consider it very likely that it also does
other things.
I am looking for someone to whom I can forward infected hardware, who is
willing and able to extract and analyze, or at least fact check and
verify and publish about, this malware.
If this is you, and you want the credit for figuring it out, or if you
know who I ought to talk to in this vein, please send me email.
All the *known* affected devices have been contained and the environment
where they were installed seems clear of it now.
But it's never just the one place....  this thing is out there.
The situation is complicated by the fact that the parties whose hardware
was infected wish to keep their identities private and the infected
devices may contain identifying, confidential, and/or proprietary data
belonging to them.
Some of the affected devices cause other devices to make characteristic
noises.  You can detect it by hearing if you are in a very quiet
environment, paying close attention, and have sharp ears.  We have
recordings of some of the noises, but they are poor quality; it's just
at the lower edge of audible.  We are working to get higher quality
recordings to check for sound modulation as a possible inter-device or
data-exfiltration channel.

@_date: 2018-09-20 22:44:17
@_author: Ray Dillinger 
@_subject: [Cryptography] Previously unknown (I think) Malware 
I'm getting somebody from the Android Anti-Malware team out there next
week, as soon as we get scheduling worked out.

@_date: 2018-09-24 22:22:42
@_author: Ray Dillinger 
@_subject: [Cryptography] Elixxir 
Keep in mind that Chaum may be walking east while you're pointing out
that he's making no northward progress.
Just because something is a cryptocurrency does not imply that the
threat model it is designed to protect against is government intervention.
The threat model he wants to defend against may in fact be money
launderers, fraudsters, and tax cheats, and making a cryptocurrency that
protects privacy in the routine case while being useless for hiding
those crimes from the FBI may in fact be a primary goal.
Let him explain what threat model he intends to defend against, before
you decide that he's failing to defend against it.

@_date: 2019-12-13 14:33:21
@_author: Ray Dillinger 
@_subject: [Cryptography] FBI: Don't trust IoT devices 
Like almost everything in this business it misses a fundamental point:
You can protect against bad software with hardware, but you can't
protect against bad hardware with software.  Depending on what the little IoTargets devices are, they have the option of damn well ignoring your commands and configuration, and
depending on who manufactured your routers they may either fail to stop them or actively cooperate with them against your wishes.
And if it can't get your routers to cooperate, it can, as you point out, airsnort the neighbor's wireless across the street.
There are a whole lot of people who profit by making it damn near impossible for you to keep your personal information inside your home
or your proprietary information inside your business.  IoTargets
devices are a frequent channel for that data to be extracted, in part
because most of them are easy to subvert and install malware on, and in
part because a bunch of them come from the factory already subverted
with malware burned into the ROM.

@_date: 2019-12-13 17:10:07
@_author: Ray Dillinger 
@_subject: [Cryptography] "[CVE-2019-14899] Inferring and hijacking 
"Constant Traffic" networks can be made resistant to analysis, but in
doing so you will also make them resistant to high bandwidth or low
latency.  Appropriate for correspondence, but not appropriate for
downloading movies.  That's acceptable for me (what's important for me
to encrypt is *SMALL* - usually text files under a half-megabyte) but not acceptable for general use.
If you want to make something hard to analyze, give people a proxy
client that hides its data traffic in bittorrent packets.  Bittorrent
is perfect for cover traffic:  always going at a dull roar.  Bittorrent
packets that fail their checksum - which can only be tested after
decryption on the receiving end, so an eavesdropper can't tell - is
dropped on the floor according to the Bittorrent protocol.  It could be
dropped into a proxy server instead, for further decryption with an
additional key, into IP packets. It would require making only the
slightest modifications of bittorrent code.
Then you have something that looks reasonably like ordinary bittorrent
traffic - going on at a dull roar, and nobody except the sender and
receiver know what's in it - and you can be doing legit downloads on
bittorrent at the same time, or hosting the latest software-update
diffs for your favorite linux distro, or whatever - give back to the
community, right?
It would probably still be somewhat slow and somewhat high-latency, but
most likely far better privacy than the current crop of things that
don't embed themselves in some kind of existing cover traffic.

@_date: 2019-12-19 09:55:53
@_author: Ray Dillinger 
@_subject: [Cryptography] Very best practice for RSA key generation 
Oh fer cryin out loud.  I'm in text-based AI. You can get those
anywhere, they're standard. But for the record, and for the archives
since this is a cryptography list, here they are: First, some standard, published c# code to count Levenshtein distance.
This is "edit distance", or the number of character changes (position
swaps, additions, subtractions) needed to change one word into another.
Second, the most frequent 3000 English Words, as counted by Education
This is the easy part.  Now quit making excuses.  :-)
-----------code-word list---------------
a abandon ability able abortion about above abroad absence
absolute absolutely absorb abuse academic accept access accident
accompany accomplish according account accurate accuse achieve
achievement acid acknowledge acquire across act action active
activist activity actor actress actual actually ad adapt add
addition additional address adequate adjust adjustment
administration administrator admire admission admit adolescent
adopt adult advance advanced advantage adventure advertising
advice advise adviser advocate affair affect afford afraid
African African-American after afternoon again against age agency
agenda agent aggressive ago agree agreement agricultural ah ahead
aid aide AIDS aim air aircraft airline airport album alcohol
alive all alliance allow ally almost alone along already also
alter alternative although always AM amazing American among
amount analysis analyst analyze ancient and anger angle angry
animal anniversary announce annual another answer anticipate
anxiety any anybody anymore anyone anything anyway anywhere apart
apartment apparent apparently appeal appear appearance apple
application apply appoint appointment appreciate approach
appropriate approval approve approximately Arab architect area
argue argument arise arm armed army around arrange arrangement
arrest arrival arrive art article artist artistic as Asian aside
ask asleep aspect assault assert assess assessment asset assign
assignment assist assistance assistant associate association
assume assumption assure at athlete athletic atmosphere attach
attack attempt attend attention attitude attorney attract
attractive attribute audience author authority auto available
average avoid award aware awareness away awful baby back
background bad badly bag bake balance ball ban band bank bar
barely barrel barrier base baseball basic basically basis basket
basketball bathroom battery battle be beach bean bear beat
beautiful beauty because become bed bedroom beer before begin
beginning behavior behind being belief believe bell belong below
belt bench bend beneath benefit beside besides best bet better
between beyond Bible big bike bill billion bind biological bird
birth birthday bit bite black blade blame blanket blind block
blood blow blue board boat body bomb bombing bond bone book boom
boot border born borrow boss both bother bottle bottom boundary
bowl box boy boyfriend brain branch brand bread break breakfast
breast breath breathe brick bridge brief briefly bright brilliant
bring British broad broken brother brown brush buck budget build
building bullet bunch burden burn bury bus business busy but
butter button buy buyer by cabin cabinet cable cake calculate
call camera camp campaign campus can Canadian cancer candidate
cap capability capable capacity capital captain capture car
carbon card care career careful carefully carrier carry case cash
cast cat catch category Catholic cause ceiling celebrate
celebration celebrity cell center central century CEO ceremony
certain certainly chain chair chairman challenge chamber champion
championship chance change changing channel chapter character
characteristic characterize charge charity chart chase cheap
check cheek cheese chef chemical chest chicken chief child
childhood Chinese chip chocolate choice cholesterol choose
Christian Christmas church cigarette circle circumstance cite
citizen city civil civilian claim class classic classroom clean
clear clearly client climate climb clinic clinical clock close
closely closer clothes clothing cloud club clue cluster coach
coal coalition coast coat code coffee cognitive cold collapse
colleague collect collection collective college colonial color
column combination combine come comedy comfort comfortable
command commander comment commercial commission commit commitment
committee common communicate communication community company
compare comparison compete competition competitive competitor
complain complaint complete completely complex complicated
component compose composition comprehensive computer concentrate
concentration concept concern concerned concert conclude
conclusion concrete condition conduct conference confidence
confident confirm conflict confront confusion Congress
congressional connect connection consciousness consensus
consequence conservative consider considerable consideration
consist consistent constant constantly constitute constitutional
construct construction consultant consume consumer consumption
contact contain container contemporary content contest context
continue continued contract contrast contribute contribution
control controversial controversy convention conventional
conversation convert conviction convince cook cookie cooking cool
cooperation cop cope copy core corn corner corporate corporation
correct correspondent cost cotton couch could council counselor
count counter country county couple courage course court cousin
cover coverage cow crack craft crash crazy cream create creation
creative creature credit crew crime criminal crisis criteria
critic critical criticism criticize crop cross crowd crucial cry
cultural culture cup curious current currently curriculum custom
customer cut cycle dad daily damage dance danger dangerous dare
dark darkness data date daughter day dead deal dealer dear death
debate debt decade decide decision deck declare decline decrease
deep deeply deer defeat defend defendant defense defensive
deficit define definitely definition degree delay deliver
delivery demand democracy Democrat democratic demonstrate
demonstration deny department depend dependent depending depict
depression depth deputy derive describe description desert
deserve design designer desire desk desperate despite destroy
destruction detail detailed detect determine develop developing
development device devote dialogue die diet differ difference
different differently difficult difficulty dig digital dimension
dining dinner direct direction directly director dirt dirty
disability disagree disappear disaster discipline discourse
discover discovery discrimination discuss discussion disease dish
dismiss disorder display dispute distance distant distinct
distinction distinguish distribute distribution district diverse
diversity divide division divorce DNA do doctor document dog
domestic dominant dominate door double doubt down downtown dozen
draft drag drama dramatic dramatically draw drawing dream dress
drink drive driver drop drug dry due during dust duty each eager
ear early earn earnings earth ease easily east eastern easy eat
economic economics economist economy edge edition editor educate
education educational educator effect effective effectively
efficiency efficient effort egg eight either elderly elect
election electric electricity electronic element elementary
eliminate elite else elsewhere e-mail embrace emerge emergency
emission emotion emotional emphasis emphasize employ employee
employer employment empty enable encounter encourage end enemy
energy enforcement engage engine engineer engineering English
enhance enjoy enormous enough ensure enter enterprise
entertainment entire entirely entrance entry environment
environmental episode equal equally equipment era error escape
especially essay essential essentially establish establishment
estate estimate etc ethics ethnic European evaluate evaluation
even evening event eventually ever every everybody everyday
everyone everything everywhere evidence evolution evolve exact
exactly examination examine example exceed excellent except
exception exchange exciting executive exercise exhibit exhibition
exist existence existing expand expansion expect expectation
expense expensive experience experiment expert explain
explanation explode explore explosion expose exposure express
expression extend extension extensive extent external extra
extraordinary extreme extremely eye fabric face facility fact
factor factory faculty fade fail failure fair fairly faith fall
false familiar family famous fan fantasy far farm farmer fashion
fast fat fate father fault favor favorite fear feature federal
fee feed feel feeling fellow female fence few fewer fiber fiction
field fifteen fifth fifty fight fighter fighting figure file fill
film final finally finance financial find finding fine finger
finish fire firm first fish fishing fit fitness five fix flag
flame flat flavor flee flesh flight float floor flow flower fly
focus folk follow following food foot football for force foreign
forest forever forget form formal formation former formula forth
fortune forward found foundation founder four fourth frame
framework free freedom freeze French frequency frequent
frequently fresh friend friendly friendship from front fruit
frustration fuel full fully fun function fund fundamental funding
funeral funny furniture furthermore future gain galaxy gallery
game gang gap garage garden garlic gas gate gather gay gaze gear
gender gene general generally generate generation genetic
gentleman gently German gesture get ghost giant gift gifted girl
girlfriend give given glad glance glass global glove go goal God
gold golden golf good government governor grab grade gradually
graduate grain grand grandfather grandmother grant grass grave
gray great greatest green grocery ground group grow growing
growth guarantee guard guess guest guide guideline guilty gun guy
habit habitat hair half hall hand handful handle hang happen
happy hard hardly hat hate have he head headline headquarters
health healthy hear hearing heart heat heaven heavily heavy heel
height helicopter hell hello help helpful her here heritage hero
herself hey hi hide high highlight highly highway hill him
himself hip hire his historian historic historical history hit
hold hole holiday holy home homeless honest honey honor hope
horizon horror horse hospital host hot hotel hour house household
housing how however huge human humor hundred hungry hunter
hunting hurt husband hypothesis I ice idea ideal identification
identify identity ie if ignore ill illegal illness illustrate
image imagination imagine immediate immediately immigrant
immigration impact implement implication imply importance
important impose impossible impress impression impressive improve
improvement in incentive incident include including income
incorporate increase increased increasing increasingly incredible
indeed independence independent index Indian indicate indication
individual industrial industry infant infection inflation
influence inform information ingredient initial initially
initiative injury inner innocent inquiry inside insight insist
inspire install instance instead institution institutional
instruction instructor instrument insurance intellectual
intelligence intend intense intensity intention interaction
interest interested interesting internal international Internet
interpret interpretation intervention interview into introduce
introduction invasion invest investigate investigation
investigator investment investor invite involve involved
involvement Iraqi Irish iron Islamic island Israeli issue it
Italian item its itself jacket jail Japanese jet Jew Jewish job
join joint joke journal journalist journey joy judge judgment
juice jump junior jury just justice justify keep key kick kid
kill killer killing kind king kiss kitchen knee knife knock know
knowledge lab label labor laboratory lack lady lake land
landscape language lap large largely last late later Latin latter
laugh launch law lawn lawsuit lawyer lay layer lead leader
leadership leading leaf league lean learn learning least leather
leave left leg legacy legal legend legislation legitimate lemon
length less lesson let letter level liberal library license lie
life lifestyle lifetime lift light like likely limit limitation
limited line link lip list listen literally literary literature
little live living load loan local locate location lock long
long-term look loose lose loss lost lot lots loud love lovely
lover low lower luck lucky lunch lung machine mad magazine mail
main mainly maintain maintenance major majority make maker makeup
male mall man manage management manager manner manufacturer
manufacturing many map margin mark market marketing marriage
married marry mask mass massive master match material math matter
may maybe mayor me meal mean meaning meanwhile measure
measurement meat mechanism media medical medication medicine
medium meet meeting member membership memory mental mention menu
mere merely mess message metal meter method Mexican middle might
military milk million mind mine minister minor minority minute
miracle mirror miss missile mission mistake mix mixture mm-hmm
mode model moderate modern modest mom moment money monitor month
mood moon moral more moreover morning mortgage most mostly mother
motion motivation motor mount mountain mouse mouth move movement
movie Mr Mrs Ms much multiple murder muscle museum music musical
musician Muslim must mutual my myself mystery myth naked name
narrative narrow nation national native natural naturally nature
near nearby nearly necessarily necessary neck need negative
negotiate negotiation neighbor neighborhood neither nerve nervous
net network never nevertheless new newly news newspaper next nice
night nine no nobody nod noise nomination none nonetheless nor
normal normally north northern nose not note nothing notice
notion novel now nowhere n't nuclear number numerous nurse nut
object objective obligation observation observe observer obtain
obvious obviously occasion occasionally occupation occupy occur
ocean odd odds of off offense offensive offer office officer
official often oh oil ok okay old Olympic on once one ongoing
onion online only onto open opening operate operating operation
operator opinion opponent opportunity oppose opposite opposition
option or orange order ordinary organic organization organize
orientation origin original originally other others otherwise
ought our ourselves out outcome outside oven over overall
overcome overlook owe own owner pace pack package page pain
painful paint painter painting pair pale Palestinian palm pan
panel pant paper parent park parking part participant participate
participation particular particularly partly partner partnership
party pass passage passenger passion past patch path patient
pattern pause pay payment PC peace peak peer penalty people
pepper per perceive percentage perception perfect perfectly
perform performance perhaps period permanent permission permit
person personal personality personally personnel perspective
persuade pet phase phenomenon philosophy phone photo photograph
photographer phrase physical physically physician piano pick
picture pie piece pile pilot pine pink pipe pitch place plan
plane planet planning plant plastic plate platform play player
please pleasure plenty plot plus PM pocket poem poet poetry point
pole police policy political politically politician politics poll
pollution pool poor pop popular population porch port portion
portrait portray pose position positive possess possibility
possible possibly post pot potato potential potentially pound
pour poverty powder power powerful practical practice pray prayer
precisely predict prefer preference pregnancy pregnant
preparation prepare prescription presence present presentation
preserve president presidential press pressure pretend pretty
prevent previous previously price pride priest primarily primary
prime principal principle print prior priority prison prisoner
privacy private probably problem procedure proceed process
produce producer product production profession professional
professor profile profit program progress project prominent
promise promote prompt proof proper properly property proportion
proposal propose proposed prosecutor prospect protect protection
protein protest proud prove provide provider province provision
psychological psychologist psychology public publication publicly
publish publisher pull punishment purchase pure purpose pursue
push put qualify quality quarter quarterback question quick
quickly quiet quietly quit quite quote race racial radical radio
rail rain raise range rank rapid rapidly rare rarely rate rather
rating ratio raw reach react reaction read reader reading ready
real reality realize really reason reasonable recall receive
recent recently recipe recognition recognize recommend
recommendation record recording recover recovery recruit red
reduce reduction refer reference reflect reflection reform
refugee refuse regard regarding regardless regime region regional
register regular regularly regulate regulation reinforce reject
relate relation relationship relative relatively relax release
relevant relief religion religious rely remain remaining
remarkable remember remind remote remove repeat repeatedly
replace reply report reporter represent representation
representative Republican reputation request require requirement
research researcher resemble reservation resident resist
resistance resolution resolve resort resource respect respond
respondent response responsibility responsible rest restaurant
restore restriction result retain retire retirement return reveal
revenue review revolution rhythm rice rich rid ride rifle right
ring rise risk river road rock role roll romantic roof room root
rope rose rough roughly round route routine row rub rule run
running rural rush Russian sacred sad safe safety sake salad
salary sale sales salt same sample sanction sand satellite
satisfaction satisfy sauce save saving say scale scandal scared
scenario scene schedule scheme scholar scholarship school science
scientific scientist scope score scream screen script sea search
season seat second secret secretary section sector secure
security see seed seek seem segment seize select selection self
sell Senate senator send senior sense sensitive sentence separate
sequence series serious seriously serve service session set
setting settle settlement seven several severe sex sexual shade
shadow shake shall shape share sharp she sheet shelf shell
shelter shift shine ship shirt shit shock shoe shoot shooting
shop shopping shore short shortly shot should shoulder shout show
shower shrug shut sick side sigh sight sign signal significance
significant significantly silence silent silver similar similarly
simple simply sin since sing singer single sink sir sister sit
site situation six size ski skill skin sky slave sleep slice
slide slight slightly slip slow slowly small smart smell smile
smoke smooth snap snow so so-called soccer social society soft
software soil solar soldier solid solution solve some somebody
somehow someone something sometimes somewhat somewhere son song
soon sophisticated sorry sort soul sound soup source south
southern Soviet space Spanish speak speaker special specialist
species specific specifically speech speed spend spending spin
spirit spiritual split spokesman sport spot spread spring square
squeeze stability stable staff stage stair stake stand standard
standing star stare start state statement station statistics
status stay steady steal steel step stick still stir stock
stomach stone stop storage store storm story straight strange
stranger strategic strategy stream street strength strengthen
stress stretch strike string strip stroke strong strongly
structure struggle student studio study stuff stupid style
subject submit subsequent substance substantial succeed success
successful successfully such sudden suddenly sue suffer
sufficient sugar suggest suggestion suicide suit summer summit
sun super supply support supporter suppose supposed Supreme sure
surely surface surgery surprise surprised surprising surprisingly
surround survey survival survive survivor suspect sustain swear
sweep sweet swim swing switch symbol symptom system table
tablespoon tactic tail take tale talent talk tall tank tap tape
target task taste tax taxpayer tea teach teacher teaching team
tear teaspoon technical technique technology teen teenager
telephone telescope television tell temperature temporary ten
tend tendency tennis tension tent term terms terrible territory
terror terrorism terrorist test testify testimony testing text
than thank thanks that the theater their them theme themselves
then theory therapy there therefore these they thick thin thing
think thinking third thirty this those though thought thousand
threat threaten three throat through throughout throw thus ticket
tie tight time tiny tip tire tired tissue title to tobacco today
toe together tomato tomorrow tone tongue tonight too tool tooth
top topic toss total totally touch tough tour tourist tournament
toward towards tower town toy trace track trade tradition
traditional traffic tragedy trail train training transfer
transform transformation transition translate transportation
travel treat treatment treaty tree tremendous trend trial tribe
trick trip troop trouble truck true truly trust truth try tube
tunnel turn TV twelve twenty twice twin two type typical
typically ugly ultimate ultimately unable uncle under undergo
understand understanding unfortunately uniform union unique unit
United universal universe university unknown unless unlike
unlikely until unusual up upon upper urban urge us use used
useful user usual usually utility vacation valley valuable value
variable variation variety various vary vast vegetable vehicle
venture version versus very vessel veteran via victim victory
video view viewer village violate violation violence violent
virtually virtue virus visible vision visit visitor visual vital
voice volume volunteer vote voter vs vulnerable wage wait wake
walk wall wander want war warm warn warning wash waste watch
water wave way we weak wealth wealthy weapon wear weather wedding
week weekend weekly weigh weight welcome welfare well west
western wet what whatever wheel when whenever where whereas
whether which while whisper white who whole whom whose why wide
widely widespread wife wild will willing win wind window wine
wing winner winter wipe wire wisdom wise wish with withdraw
within without witness woman wonder wonderful wood wooden word
work worker working works workshop world worried worry worth
would wound wrap write writer writing wrong yard yeah year yell
yellow yes yesterday yet yield you young your yours yourself
youth zone

@_date: 2019-12-23 11:38:30
@_author: Ray Dillinger 
@_subject: [Cryptography] OpenSSL: rsa_builtin_keygen: key size too small 
Further, I doubt anyone there will be interested in helping you create
a version that doesn't throw that error message.
People have been badly burned several times by downgrade attacks. The
openssl maintaners REALLY don't want any versions out there in the wild
that fail to shut such attacks down cold.  Especially if they look
enough like "real" openssl to fool anybody.
If you do get an openssl working that can produce a trivial RSA key,
I'm pretty sure no other openssl in the world will consent to talk to
it using that key.  So generating a version that can produce that key
is only half the job. The other half is generating a version that will
respond to it.

@_date: 2019-02-21 20:33:07
@_author: Ray Dillinger 
@_subject: [Cryptography] The Voynich Manuscript as a product of a mental 
That is an interesting theory, but ...  having read the first two of
these papers, it does not seem believable to me. It would require a very
strange grammar for the language and a very strange person as author.
She would have to be a polyglot familiar with terms from a dozen
different languages, constructing sentences according to an absolutely
opaque set of grammatical rules.
Or, as it seems, by no grammatical rules at all. I cannot see any
definite structure relating the words in the sentences translated in
these papers.  The language proposed is not consistently SOV, SVO, or
VSO.  In fact a fair number of the translated sentences appear to lack
verbs entirely.  It does not seem consistently head first or head final.
 It seems to have no articles, prepositions, postpositions, or
declensions to consistently mark the use of nouns, and no conjugations
or particles to consistently mark the use of verbs. Adjectives and
adverbs appear sometimes but none seem structurally bound to any
particular precedent or antecedent noun or verb.
So I'm having a lot of trouble believing in this language.  I can't
believe in a grammar according to which there seem to be no sequences of
words which are errors.  If nothing is a non-sentence, then finding the
specific meaning of a genuine sentence is intractable.
There is another thing that would make your theory deeply troubling.
This theory would require someone to write an herbal, identifying plants
&c for medicinal use or warning of them as poisons - while consistently
drawing incorrect renditions of those plants.  This is the kind of error
that cannot be attributed to any person of good intentions. Herbals were
the closest thing people of that time had to pharmacies and getting
those illustrations right was literally a matter of life and death.
Producing an herbal while lacking that knowledge would be fraudulent,
recklessly endangering any who believed it, and criminally arrogant.
Deliberately getting it wrong would be an act of sheer malice.
Otherwise I'm not a subject matter expert; it might be the case that
there is some set of consistent grammatical rules which would indeed
verify a specific meaning for each of these sentences and also identify
most purely random sequences of words as incorrect or malformed.  It
might be the case that there were in 1440's Italy a number of people
familiar with applying this mysterious grammar to that kind of polyglot
vocabulary.  It might even be possible that there existed a nun with the
sheer arrogance, disregard for life, or even malice, to create an herbal
while knowingly drawing the plants incorrectly.  But this theoretical
grammar and this supposed author both seem too strange and too horrible
for me to accept this theory readily.
If your theory is valid, then I look forward to confirmation of this
interpretation by other scholars.  But until that time comes, I'm going
to regard it as a theory only, not particularly more believable than
many others.

@_date: 2019-01-21 13:16:13
@_author: Ray Dillinger 
@_subject: [Cryptography] The Voynich Manuscript as a product of a mental 
Hm.  That's a theory with some merit, and I don't remember anyone
considering it previously.
Last I heard about it, some folks had shown that its written content is
consistent with a method of producing a stream of gibberish (the
content-free part of a stream  cipher message) shaped into a fairly
language-consistent form using a Cardan Grille. The theory developed in
that paper was that someone (ISTR they had a particular suspect in mind
though I don't remember who) had produced this work of literal non-sense
as part of a fraud, using that method.
Of course, asemic writing is a known, if unusual, symptom seen in
psychiatric patients. Producing many pages of asemic gibberish using a
Cardan-grille method would be an extreme form of that, but could also be
interpreted as a manifestation of Obsessive-Compulsive Disorder. The
"suspected of fraud" scholar may have genuinely discovered an
indecipherable work, as claimed, unsuspecting that the work was produced
by someone whose thought process was disordered.

@_date: 2019-06-01 21:12:23
@_author: Ray Dillinger 
@_subject: [Cryptography] Craig Steven Wright's Bitcoin Copyright 5-20-2019 
This is transparently a stunt.
CSW and SN are very dissimilar.  The fact that SN's coins remain
unspent is another strong indication that they are not the same person.

@_date: 2019-05-16 16:45:56
@_author: Ray Dillinger 
@_subject: [Cryptography] Voynich Solution Claimed 
I don't even know, at this point, whether the world is better or worse
because that manuscript exists.
On the plus side, I like mystery.  I like the feeling that we don't
know enough to interpret, or that there may still be things to
discover, or that this may be an account, or a manual, or something,
that brings a voice otherwise unheard to light.  I like the way it
occasionally stirs interest in classical cryptography, in medievalism,
in linguistics, in the study of orthography and ancient alphabets and
languages.  I like that there are mysteries people try to solve, and
worthy bodies of knowledge that are, in part, preserved by those
efforts. On the minus side, I'm pretty much convinced that the markings in the
Voynich ms. do not correspond to language.  They are not a writing
system and those are not sentences.  The very premise of this mystery
is false.  If I had my best guess, I'd say it's about an 80/19/1 split
favoring 'medieval fraud' over 'asemic writing' and both of those, by a
wide margin, over 'actual language and writing system.'  If it were a
specimen of something real, then it would not be the only specimen.  If
we find something else in the same 'language' we could start to think
about it, but as long as it's a sample of one, we have to admit
possibilities like fraud which can plausibly produce a sample of one.
And given that opinion, I find the world a worse place because it
raises false hopes and inspires people to waste time.

@_date: 2019-09-13 15:56:11
@_author: Ray Dillinger 
@_subject: [Cryptography] TRNGs as open source design semiconductors 
People who deeply care, are already pointing a camera at a fish tank
with a bubbling air filter and registering a stream of hashes of the
resulting video stream as an additional source of entropy with their
opensource software RNG.  Or something. You're not going to convince those people that any chip they can't
literally decap and trace can be trusted by itself, whether or not they
see claims or promises or assurances indicating that someone else believes that it conforms to an opensource standard design.  Any or all
of those people could be lying, or may have been decieved by some
combination of the others.
That said, they'll happily consider adding its output to the mix, on the grounds that that source would be expected to be unpredictable to most other attackers, and at least one of the other sources they're
using is expected to be unpredictable to you.  The combination results
in an RNG which they hope is unpredictable to everybody.
In other words?  No new product is going to revolutionize the market,
because the market is about trust, not products.  They are evaluating
whether they can trust you (and you need to overcome a default negative
assumption), before they even consider the possibility that they might
be able to trust your product by itself.

@_date: 2019-09-13 16:25:15
@_author: Ray Dillinger 
@_subject: [Cryptography] TRNGs as open source design semiconductors 
I still talk about "random" numbers when talking to people who are
depending on the standard nomenclature for comprehension, but these
days I explain that what I really mean is "unpredictable" numbers.
Randomness is philosophical, and sort of defies a proper definition.  But when we ask whether any attackers can predict the sequence, That
puts it better into the context of security thinking, because what we
immediately have there is a relatively normal security question.  It
has an attacker or set of attackers, and it is evaluating the
capabilities of that set of attackers with respect to a stream of
numbers.  And I still talk about "entropy" when talking to people who are
depending on the standard nomenclature for comprehension, but these
days I explain that what I mean is how many bits do we have stored,
that we have good reasons to believe no potential attacker can predict.
Again, what that does is reduce it to a relatively standard security
question; is there any possible attacker who might be able to guess any
of these bits with any likelihood greater than 50%? That's a qestion
that can be evaluated, with a definite answer, with respect to a
specific set of attackers and a specific set of bits.
The concepts of "randomness" and "entropy" as qualities of numbers,
independent of any attacker, are in fact problematic, and contribute to
some faint and fuzzy thinking. Anyway, this is why standard design combines many sources, in such a
way that for every set of attackers who might possibly be able to
predict one source assuming whatever skulduggery and conspiracy would
be needed to subvert it, there exists at least one source which that
attacker can't predict.

@_date: 2020-12-28 13:08:38
@_author: Ray Dillinger 
@_subject: [Cryptography] Am I missing something about CBDC ? 
The real issue here is the cost of clearing transactions.  If it's a
block chain with limited space available per day, like bitcoin, then
the cost of clearing a transaction is essentially an auction for that
space - which can be an enormous cost relative to the small
transaction.  Nobody will use Bitcoin to pay for a cup of coffee when
processing the payment costs ten times as much as the coffee.
And if the transaction goes through the credit card avenue here in the
US, it's much less expensive but not free; last time I knew what it
cost, it cost about a bit (1/8 dollar) to process.  Stores usually just
pay it. They take the hit and make that cost "invisible" to the user
because making it easy for customers to pay lets the stores sell more
But if you're out there doing true peer-to-peer with a cost comparable
to the credit card system, it's going to be visible.  Someone's going
to have to come up with that extra bit, and so the transactions won't
be frictionless.  And below a certain threshold it won't be practical;
nobody's going to "take the hit" on that form of payment for non-
aggregated parking-meter payments for example.  If they can aggregate
and make one transaction a month on your account, that's going to be a
few dollars and worth it.  If they can't, the processing costs are
going to eat a large fraction of their revenue.
The other drawback of credit card payments for anything is that you
then get junk mail from every purveyor of any related or comparable
thing in the world, forever.  It's not clear yet whether the same
problem applies to these new digital tokens.

@_date: 2020-12-28 17:07:03
@_author: Ray Dillinger 
@_subject: [Cryptography] Bitcoin is a disaster. 
Okay, this may be just my depressive side talking, or it may be the
stress of the last year just boiling over.  But I'm inclined to think
it's not and it isn't. It is my opinion that Bitcoin is a failure.  Worse than that, it's a
disaster.  The pseudonymity of coins being owned by the bearer of some
cryptographic key is a failure;  People have been eavesdropping and
aggressively analyzing the block chain from day 1.  And the block chain
will always be there, it will always be public, and it will always be
subject to further analysis.  And we are learning that analysis of that
record is sufficient to destroy any pretense of anonymity or
pseudonymity.  The scarcity of block chain space has led people to re-invent every
last feature of the banks they thought they were going to be escaping.
Including debt brokering (lightning network) and fractional-reserve
banking, starting with the case of Mt.Gox and continuing to ventures
today by "responsible" businesspeople who just don't get, or don't
care, or both, that the entire reason the system existed, as far as the
early adopters were concerned, was to get away from exactly that.  They
have made Bitcoin into a debt-based system like any other; as long as
the "exchange" holds your keys for you, there is no obligation for them
to maintain assets equal to the deposits.  You can't prove that they
are, or aren't, maintaining sufficient assets until after those assets
are spent and the evidence appears in the block chain.
And it's useless for small transactions.  Had it been deployed to a
market the size of, say, a college campus it could bear the load and
the bidding for block space wouldn't exceed the value of most
transactions.  But had it been deployed to a market the size of a
college campus, the small pool of miners available would make mining bursty and unstable, and the block chain therefore not well protected
from tampering.  Same could have happened to Bitcoin early on, which is
why Satoshi was mining like crazy and jumping on when needed to prop up
the block rate and back off again when the blocks were coming too fast.
And that brings us to mining.  Bitcoin mining has encouraged corruption
(Because it's often done using electricity which is effectively stolen
from taxpayers with the help of government officials), wasted enormous
resources of energy, fostered botnets, centralized mining activity in a
country where centralization means it's effectively owned by exactly
the kind of government most people thought they *DIDN'T* want looking
up their butts and where the people who that government allows to "own"
this whole business work together as a cartel. ?
There's a pretense of monitoring the network to guard against a 51%
attack, but to me it seems pretty clear that what they're guarding
against is merely the mistake of the cartel failing to give the latest
warehouse full of miners a distinct network identity.  The whole idea
of proof-of-work mining is broken the instant hardware comes out which
is specialized for mining and useless for general computation because
at that point the need to have compute power for other purposes is
absolutely irrelevant in having any effect on mining, and there ceases
to be any force that causes mining to be distributed around the world.
It becomes a "race to the bottom" to find where people can get the
cheapest electricity, and then mining anywhere else - anywhere the
government tries to make sure ordinary people actually get the benefit
from electricity bought for tax money, for example - becomes first
pointless, then a net loss. Mining is f***ng broken, and ASICs make it actively work against a
significant number of its design goals.
So, Bitcoin was a good effort, it deployed some new ideas and
technology, and showed that at some scale the "block chain" idea
worked, but ultimately, although a successful proof of concept, failed
to deliver.  It doesn't scale, except by becoming the very thing it was
supposed to replace.
The more scalable the network becomes, the more centralized it becomes,
until ultimately a "scalable" cryptocurrency would be doing things
exactly the same way as a credit card processor.

@_date: 2020-12-31 12:17:42
@_author: Ray Dillinger 
@_subject: [Cryptography] Bitcoin is a disaster. 
He did predict centralization into specialized businesses.  He did not
predict that a "centralization" entailed a race to find the single most
corrupt available country in the world, where electricity could be
cheap because it was stolen. ?
Emphasis on "single country."  The disaster is not centralization
itself; the disaster is that mining is centralized in one country. Satoshi was expecting mining to be done by specialized businesses, but
businesses located all over the planet, probably as a job that
datacenters ran whenever their servers were otherwise idle, until a new
customer with a new job came along.  Because datacenters are all over
the world, no single political regime would control the entire block
As matters stand, if President Xi tells (or pays) the miners to shut
down, or cuts the flow of stolen power suddenly, or seizes all of the
warehouses full of miners, the Bitcoin network will NEVER, EVER, in a
million years, get a single additional block. Keep in mind, it's also a very inefficient theft.  Even on stolen power
they're running on small margins. They make their profit, a few people
get their palms greased to claim they're a "qualifying subsidized
business"?which they don't employ nearly enough people or interact with
nearly enough of the rest of China's economy to be, and the rest of the
value goes up in literal smoke at a nearby powerplant. ?
If the Chinese government goes on another corruption purge, those guys
getting their palms greased could be eliminated, and then somebody has
to ask, with this power plant going full tilt, why are these local
towns not getting the subsidized electricity it's supposed to provide?
Why aren't these enterprise zones developing as, given these subsidies,
they should?  Oh look, they're subsidizing business that don't
qualify."  And throws the switch. ?
And at that point the miners have to either recruit a new generation of
corrupt officials, or Bitcoin's network is gone forever.
Yes, the plan was to raise the size of the blocks. ?And yes, I think it
should have been done.  The 1MB limit was considered temporary. We got
the current limit just to prevent people from filling space with dumb
stuff but thought, of course they'll make it bigger when people
actually need the space for legit transactions.  But now they can't
make it bigger, because that was a classic case of nerds making a
design mistake by failing to note that we were leaving a decision in
the hands of people with perverse incentives. The reason that issue became the screaming sewer knife fight it became
is because of toll trolls.?Miners have a financial stake in the blocks
continuing to be small, because bidding goes higher for a smaller
amount of block space.  So they hired a bunch of trolls to sabotage all
discussion about raising the limit.
All they have to do to kill a raised limit is refuse to mine on larger-
size blocks.  But that would mean abandoning all pretense of not being
a cartel.  The trolls and screaming you've seen are just symptoms of
their need to keep the user base conflicted and uncertain.  That way
they don't have to alienate the community by being tremendously obvious
in a highly visible 51% attack when they participate with each other in
refusing a protocol with bigger blocks.
The current 1MB limit was added to the code at the last minute, because
Hal was concerned about the size of the block chain (and time/cost of
downloading it to set up a new node) growing faster than the Internet
communications bandwidth.  Satoshi was confident that it wouldn't grow
beyond the power of a $mumble-priced hard drive to contain as hard
drives got bigger/cheaper, but that's a different thing. Hal saw a
scalability problem w/r/t internet bandwidth because while drive space
was on an exponential curve, internet bandwidth wasn't.  And isn't.
My input to the bandwidth discussion didn't go much beyond "well, of
course the bandwidth would be a crippling issue at scale, but you don't
need to worry about this for a small experiment .... wait, are you
serious?"

@_date: 2020-02-11 19:03:50
@_author: Ray Dillinger 
@_subject: [Cryptography] Crypto AG and CIA project exposed 
The electronic devices with outright jiggered hardware (Deliberate RF
leaks and a few other things) replaced (or in some cases served
alongside) old-fashioned rotor machines with jiggered manuals and
training materials. These were machines that could be used securely, or not, and our NSA
worked with Crypto AG to ensure that certain nations got documentation
and training materials that trained them to use the machines
insecurely.  The machines passed audit after audit, being
electromechanically identical to machines in secure use elsewhere.  But
the people auditing the machines weren't looking at the training
materials. There were things like key creation methods that gave low-entropy keys
(some amount of actual randomness juxtaposed with predictable
information like a date code that was there "to prevent key reuse", or
a short random key used to look up a longer key in a table, etc.) or a
"data integrity code" that was sent unencrypted and leaked information
about the IV, etc. I recall reading a chunk of one of the jiggered manuals, spotting a
stinker and thinking, "oh, wait, that's wrong." And thinking I'd have
seen the problem with it even if I hadn't known to be looking for one,
but ... there'd probably be a couple more than just the ones I saw,
A greater risk of failure on my part would be that on finding a clear
flaw in official training materials I might have said nothing assuming
that official training materials must have been written by people who
knew better than I.  Crypto AG, at the time, had a reputation a yard
thick!  Such people, well known to be highly competent, would obviously
have spotted it already if it were as bad as I thought, I'd have said
to myself, so I must be wrong about this somehow.  And then I'd have
spent days or weeks trying to work out why it wasn't a real problem
before bringing it up.
Backdoored crypto is not the kind of accusation I would have wanted to
bring against the most reputable supplier in the world, unless
absolutely sure. Even when bringing it up I'd have been treading as
lightly as possible, asking a why-is-this-okay question instead of
jumping straight to I-think-our-partner-is-a-crook.

@_date: 2020-01-01 14:31:17
@_author: Ray Dillinger 
@_subject: [Cryptography] how to detect breakage -- lures etc.?? 
If we're going to bring up rotor machines though, I've got a thought. Remember the flat rotors that were made for the bombes?  They had all
their contacts on one side, just because that made it convenient to set
up the machine.  You could pop them in and out independently, without
tearing down a stack and the machine around it, and setting it up
again.  Why weren't the rotor machines made that way in the first place?
meaning, rotors making contact with contacts on the body of the
machines instead of with each other?
In the first place it allows more flexibility with wiring - outputs
from one wheel could be split between inputs of multiple different
other wheels, or light-board lights illuminated on signals drawn from
more than one wheel, or "extra" outputs of larger rotors beyond the
number needed for the lightboard could be connected back to the "extra"
inputs of the same rotors that take the input from the keyboard on
their other terminals, etc....  As long as each rotor has a definite
input-to-output direction and the wiring respects that parity, it's
guaranteed that all the inputs wind up at the outputs eventually
because there's no place else to go, no way for two signals going the
same input-to-output direction to get connected with each other, and no
way for a signal to get into an infinite loop, because if there's no
way out of a loop then ipso facto there's no way in.  The kind of wiring you could use with "open face" rotors could be far,
far more difficult to analyze than the "stacked rotor" variety of
machines ever made them. Especially because.... In the second place it allows rotors of different sizes, so if you use
six or eight rotors of different sizes with a very large least common
multiple, you don't ever have to NOT MOVE any of them.  They will cycle
into all their permutations, with uniform distributions among those
permutations, just by not having the same period. And you'll have a
very long key stream, and your opponent won't ever get a chance to see
what any proper subset of those rotors do.  And mechanically, it's
brutally simple to just move every rotor one space, every time. That
can easily be made robust, reliable, compact, and lightweight, rather
than intricate, fiddly, and prone to failure. So....  why did the stacked-rotor variety ever dominate over the flat-
rotor variety?

@_date: 2020-01-04 16:19:23
@_author: Ray Dillinger 
@_subject: [Cryptography] how to detect breakage -- lures etc.?? 
It's that "sometimes per letter" thing that is the crucial part.
"sometimes" isn't often enough.  This is about the difference between
uniform movement and movement complex enough that you hope the enemy
never figures it out. Uniform movement, natch, the enemy has already
figured out.  But it doesn't present opportunities to use partial
information the way complex movement does, so having figured it out
doesn't do the enemy any good.  IOW, you don't need movement complexity
to hide behind if there is nothing to be gained from knowing the
movement pattern.
The fact that with same-size rotors some of them must move only
"sometimes" in order to get a decent period, means the opponent sees
the results of different sets of the rotors moving.  From the
differences in effect, the opponent can isolate the effects of one
rotor or one subset of rotors, then use that connection matrix to
subtract its effect and isolate another, etc.... But if the opponent never sees anything but the effects of "all the
rotors moved simultaneously" there is no opportunity to isolate the
effects of a single rotor or subset.  There's no mathematical
distinction between what happened at any two different steps that can
be detected or exploited - no contrasts to decompose.
And then cyclometry doesn't work, Index of Coincidence attacks don't
work (at least not on any index smaller than than the length of the
entire cycle), and there's not really anything that can be used to
decompose the rotor stack and reconstruct the effects of any single
rotor.  So I don't agree with you that it "doesn't do much."
FWIW, I had envisioned a machine in which part of the wiring between
rotors would be determined by the key, for the reason of making the
sequence of cipher alphabets for the whole cycle vary by key.  This
would prevent "depths" from appearing when a sequence of rotor
positions appearing halfway through one message becomes the "message
key" of a different message.  This kind of rerouting would cause a lot more mathematical chaos when
it means that signals actually go to a different next-rotor instead of
just going to a different point on the same rotor.  Instead of treating
it as a composition of matrices (a composition of *known* matrices if
you have one or more of the rotors' wirings known) you now have to
figure out which parts of what you see are the result of composing
*which* matrices in what order, which is hard to do even if the
matrices are known.
Even if you did figure out the wiring of a rotor (or one of your spies
managed to steal one) it would have a profoundly different effect on
the output under one key than it would under another.  And, again,
because nothing ever moves at a moment when something else does not,
there's still no way to pick out the effect of your known rotor from
the effect of the other rotors. Static parts of the key could be: orientation and direction of each
rotor (which side up, and +1 or -1 position at every letter) plus order
and orientation of a half-dozen 'permutation blocks' that complete
internal wiring paths.  Any change to the static key and the entire
cycle of cipher alphabets is changed.  The dynamic part of the key
would be the rotor positions, which should not be relied on for

@_date: 2020-01-07 18:43:19
@_author: Ray Dillinger 
@_subject: [Cryptography] how to detect breakage -- lures etc.?? 
Lorenz is probably a poor example, as each rotor's effect was applied
independently to one bit of a 5-bit signal.  The effects were separable
because the signal itself was separable. If it had simultaneously
stepped through the possible permutations of the 5 input bits, would
the pattern have ever been detected at all? Point.  I hadn't considered some of the "bad idea" configurations this makes possible. As you point out there's some amazing ones.  There are
obvious mitigating measures like a designated "entry rotor" to handle all inputs uniformly and a designated "output rotor" to handle all
outputs uniformly, interval-wired rotors to insure that every input to
a rotor gets connected to every possible output as it turns, etc.  But
non-uniform processing, with some periodicities, is part of the basic
design concept. So the question is whether it's detectable and exploitable more or less
easily than the non-uniform movement in conventional rotor machines. Thanks for pointing that out - that's an important question. My thought had been to make the key affect the signal routing between
wheels, but that obviously requires attention to prevent classes of
weak keys from emerging - ie, the key "plugin blocks" should control
only permutations that can't shortcut or significantly bias the signal
routes. I think I'll code up a simulator for machines of a few different
complexities built along these lines ("rational" designs with interval-
wired rotors, some precautions like no routes from input to output ever
going via less than 3 rotors, etc) and then try out some "standard"
rotor machine attacks like hill-climbing,  cycle counting,
compressibility checks, etc.

@_date: 2020-01-07 19:02:29
@_author: Ray Dillinger 
@_subject: [Cryptography] how to detect breakage -- lures etc.?? 
Yes.  Thank you for stating it more clearly and succintly than I think
I was capable of. Right.  The problem with rotor machines in general isn't that they're
insecure.  Although many of the early ones were insecure, that's not
endemic to rotor machines as a class.  In fact, as they run on systems
that are known with absolute certainty to NOT be running any kind of
malware or spyware (because they can't), they can arguably be even more
secure than modern ciphers.
The *real* problem with rotor machines is that if you try to operate
them fast enough to manage an internet data link, they explode. Random
explosions, sadly, aren't really a feature consistent with the
harmonious operation of modern office environments. For starters you
have to explain to OSHA. I happen to have a soft spot for rotor machines in my heart, but I
don't mistake them for a viable alternative to modern ciphers for data
communication.  Written correspondence is pretty much the limit of
their applicable domain. I will have a look at the shift-register machines.

@_date: 2020-01-08 20:29:32
@_author: Ray Dillinger 
@_subject: [Cryptography] Variable length rotor machines: was: how to 
All of this is true.  With a machine using reflecting rotors, it is
simple to have two independent wirings in a rotor, and just turn it
over for a second option.  Or if you don't care so much about
"independent" you can just wire the contacts on both sides together and
turn it over for a mirror-inversion of the wiring. But, yeah, you're
never going to use a size-27 rotor in the size-25 or size-29 slot, so
you do miss out on the way rotor sequence can add a permutation to the
key space.
A thing that has some implications though is that a reflecting rotor of, say, size 27, must have 54 contacts on its face, because both ends of the 27 connections have to be there.  Thus, the size-27 rotor
can be in any of 54 different positions.  The odd-to-even mappings
around the rotor turn into even-to-odd mappings every time the rotor
advances one position, then back into odd-to-even mappings when the
rotor advances again. This doubles the cryptoperiod of the system by adding a (shared) factor
of '2' to the least-common-multiple.  But it does much more than double
the key space (assuming rotor positions are part of the key).  The keys
would be divided into 2^(n-1) sets for n=number of rotors, each set
having the same number of keys as the cryptoperiod, corresponding to
positions on a different sequence of cipher alphabets.

@_date: 2020-07-06 10:56:50
@_author: Ray Dillinger 
@_subject: [Cryptography] Statement from Attorney General William P. Barr 
Is it actually the case that this guy (sorry, I don't know the
instance) actually went on shooting people after taking some otherwise-
incapacitating bullets on his armor?
My expectation of kevlar vests against rounds of deer-rifle power or
higher is that they make a huge difference but it's the difference
between  dead and "probably won't die but will likely be knocked
down/winded/stunned, may need hospitalization, and will feel afterward
like he's been kicked by a mule."
In rather the same way that cryptography actually protects very little
of your privacy now that we live in a surveillance economy where every
large company is analyzing everything - where you shop, what kind of
dogfood you buy, where you get gas for your car, every word you type on
social media, where your cell phone goes every minute of every day, and
everything else they can get - and constantly cross-referencing it
against public records etc.  Maybe defensive crypto makes a difference,
but really, how much difference does it make?

@_date: 2020-07-06 11:10:24
@_author: Ray Dillinger 
@_subject: [Cryptography] Statement from Attorney General William P., 
Sure, I can see it.  Use Morse-code for the line-level protocol and
hundreds of hams will hate you forever.

@_date: 2020-07-07 10:31:17
@_author: Ray Dillinger 
@_subject: [Cryptography] IPsec DH parameters, other flaws (fwd) 
Summary:  MUST NOT isn't unreasonable.  Extended commentary below the fold.
So.  If I am understanding this correctly, someone relatively
unfamiliar with the RFC process obtained a set of parameters from NIST,
which we later discovered got them from the NSA.  These parameters have
nothing to recommend them; they are horribly inefficient.  If they have
been selected for any criteria, that criteria is neither efficiency (or
they would be more tractable) nor security (we know that they *CANNOT*
be more secure than other keys of the same length.  We are left asking,
what criteria have they been selected for?  The resulting document specifies the use of these parameters rather
than generation of parameters. Security considerations clear to most of
the people on this list indicate that there is better security if
people are generating their own parameters.  Instead this one set of
parameters has seen use so broad that, *IF* there is a back door,
*THEN* these parameters have become a "golden key" to a substantial
fraction of the entire traffic.  That seems like a thing the NSA might
have an interest in selecting for, if it were deliberately weakening
rather than protecting domestic security.
And then there was the Dual_EC_DRBG debacle, in which another NIST
standard (SP 800-90A for anyone who doesn't remember) certified a
pseudorandom generator that has absolutely nothing to recommend it.  It
is slow, inefficient, and hard to implement in a way that doesn't leak.
But like the current construction it admits of jiggered parameters that
could contain a "golden key", and is again certified WITH PARTICULAR
PARAMETERS instead of recommending that people generate their own which
would be better security.  Again we found that these parameters came
from the NSA.  Again people had to ask, what criteria were those
parameters selected for?  We all know the answer, but I'll flog the
horse a little even though said horse isn't breathing any more.  The
Snowden files confirmed the worst suspicions, and now we KNOW what the
parameters were selected for.  That PRNG is no longer certified in the
current version of the standard.
And then there were the Snowden Files and the Bradley Manning leaks,
(yes I know but that was her name at the time) wherein it was
conclusively shown that the NSA is, indeed, in the business of
deliberately and covertly weakening the security of the people whose
security its mission statement says it is supposed to protect, AND
deliberately intercepts, at the time without even a secret warrant, the
same private, proprietary, and domestic traffic it is supposed to be
And a bunch of other things I'm gonna skip. They only make two points
relevant to this discussion.  First, the NSA has lousey security by
comparison to non-backdoored crypto, or these things wouldn't have
leaked in the first place.  Second, the NSA has demonstrated an
eagerness to weaken American data security and also demonstrates
various other kinds of bad faith.
So "MUST NOT" isn't an unreasonable thing to say given the
circumstances. At the very least those parameters snould be decertified.  In the same
way and for the same reasons that DUAL_EC_DRBG was decertified.  Until
then people need to be warned against using them.
The security evaluation of those parameters should reflect the NSA's
history, not the mathematical expectations due a secure cipher.
Because their security is not the unbiased security of mathematics.
Their security is, at best, the security of the NSA.  Which is
demonstrably lousey compared to the security of secure cryptography.
The NSA has had MANY major leaks in the organization's history that
became public, a steady stream of "minor" leaks, and I have no idea how
many more where some publisher somewhere got something just as dire and
decided not to go with it.  All in less than one ten-thousandth of the
time a solid random key would have taken to break once by using
mathematics.  And if the standard is a way to facilitate intercepting domestic
traffic, then it's failing in its purpose.  Just as badly as an
organization created to defend the information security of the United
States is failing in its purpose when they're found deliberately
weakening it.
P.S. They're still intercepting domestic traffic BTW.  They say they
"haven't intercepted" or "don't intercept" things even when the
complete content of those things are stored on their systems and used
daily. They define "intercept" as wet human eyeballs looking at a
printout of it. But in a big-data operation there is *NO POINT* at
which having wet human eyeballs looking at a printout of it would serve
any purpose.  If the system is even marginally competent they can go to
all the way to trial (or all the way to ops planning) with the evidence
in a sealed envelope, still "unintercepted", on the basis of summaries,
searches, filters, cross-references, automated evaluations, etc that
tell them what the evidence is. So when the NSA says they "don't
intercept domestic traffic", they're using words in a way that has
nothing to do with English, to tell you they don't do something
completely irrelevant.

@_date: 2020-06-02 13:18:35
@_author: Ray Dillinger 
@_subject: [Cryptography] Zoom publishes draft cryptographic design for 
First of all, Paul is right - completely different problem set, trying to secure a completely different set of properties.  You're going to need some hashing for integrity checks, but the main problem you're looking at for your intended app is key management.  The hashing here is a relative detail.
Short version: secp256k1 was a very peculiar selection driven by paranoia about potential backdoor'd keys. It was little studied and not noticeably tested in practice.  But at this point,
because of its use in Bitcoin, it has become extensively tested and rigorously studied.  There's no problem any more with moving toward standardizing it.
(Long version follows)
About secp256k1, Keep in mind that Hal & Satoshi (I persist in thinking they were not the same person) were very paranoid about
backdoors in well-known curves and unlikely to use one that had been heavily promoted, or one frequently used by large organizations or projects whose selection of it might, in the paranoid view, have been because of undue influence.  The thinking was that "The crowd is probably wrong" leading to semi-deliberate selection of something odd.
The same kind of thinking leads to a lot of insecure "homebrew crypto."  Homebrew Crypto almost always fails, so reasonable people tend to advise against it.  If you're not *absolutely* *sure* what you're doing and why, stepping off the beaten path in crypto implementation almost always lands your foot (and often more tender parts) in a trap.  IIRC secp256k1 was initially selected by Satoshi, and had a few peculiar properties that convinced both Hal and himself it was desirable and safe. I didn't have the insight into properties of different keys to offer any useful feedback on it. The important property was either because its constants had been selected in a way more constrained than the heavily used curves (hence less likely to contain a backdoor), or because it's defined over a Galois field instead of a modular integer ring.  Or both.
But virtually nothing used it, and it hadn't got any very extensive study at that time.  Secp256k1 was at that time undeniably a very peculiar beast, and while it offered some reassurance against a backdoor risk, its relatively unstudied status was different kind of insecure-key risk.  Both risks were very small, but it would be hard to place odds on which was At this point however, largely BECAUSE of its use in Bitcoin, I can't consider it very peculiar anymore. On the contrary, it has become one of the best-studied and most extensively tested hashes in the world. Given the vast amount of completely futile effort that's been spent on finding any weaknesses, backdoors, or shortcuts in it, I'm convinced that any such things, if they exist at all, are demonstrably damned hard to find. I don't know of any other curve that has been so extensively tested or rigorously studied. There really isn't a problem with moving forward to making it a standard.

@_date: 2020-06-29 15:38:02
@_author: Ray Dillinger 
@_subject: [Cryptography] Side channel nomenclature 
I have always used 'Data Exfiltration' to mean sneaking stolen data out
of a secured environment, regardless of how it was stolen.  You're
extending it AFAIK when you apply it to hardware - at that point it's
necessarily using a mechanism put there by the designer.  But I've
always used it (and heard it used) to refer to data getting stolen from
a human organization like a business or government.  What gets read off your hard drive (or out of your photocopier, or your
print buffer, or your always-listening Alexa/Whatever, or out of your
employee's fitbits (don't laugh, there's tracking information in
there....) by malware that someone has sneaked into your organization
has to get back to the data thief somehow, and needs to go through the
outbound firewall, without setting off network monitors, and without
alerting the people who read the logs (if there are people who read
logs).  So it gets transmitted through 'ping' packets that have a bunch of data
attached, or dumped into bogus DNS requests that are never really
monitored, or encoded as HTTP requests, which can get out if your
employees are allowed to use the WWW, or mixed into the general "dull
roar" of a file-sharing protocol as "error" packets, or ... hell, if
the hacker can get at your electrical system, it might even leave the
building through a window as tiny transient power fluctuations that
cause a lightbulb to 'flicker', in such small time increments as to be
imperceptible to humans, and the channel can be read from across the
God, whole ENCYCLOPEDIA VOLUMES could be written about how hackers
exfiltrate data.  But I had only ever considered the word to apply to
human-scale organizations, never to individual pieces of hardware.

@_date: 2020-10-09 16:54:04
@_author: Ray Dillinger 
@_subject: [Cryptography] Exotic Operations in Primitive Construction 
Heck, I don't mind.
  binary   operations is unfamiliar. */
uint32_t reverse32(const uint32_t input){
