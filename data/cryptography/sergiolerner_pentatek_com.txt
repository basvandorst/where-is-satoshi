
@_date: 2010-03-21 18:13:02
@_author: Sergio Lerner 
@_subject: Question regarding common modulus on elliptic curve cryptosystems 
I looking for a public-key cryptosystem that allows commutation of the operations of encription/decryption for different users keys
( Ek(Es(m)) =  Es(Ek(m)) ).
I haven't found a simple cryptosystem in Zp or Z/nZ.
I think the solution may be something like the RSA analogs in elliptic curves. Maybe a scheme that allows the use of a common modulus for all users (RSA does not).
I've read on some factoring-based cryptosystem (like Meyer-Muller or Koyama-Maurer-Okamoto-Vantone) but the cryptosystem authors say nothing about the possibility of using a common modulus, neither for good nor for bad.
Anyone has a deeper knowledge on this crypto to help me?
Best regards,
  Sergio Lerner.

@_date: 2010-03-22 10:22:35
@_author: Sergio Lerner 
@_subject: Question regarding common modulus on elliptic curve cryptosystems 
[Moderator's Note: please don't top post.... --Perry]
Commutativity is a beautiful and powerful property. See "On the power of Commutativity in Cryptography" by Adi Shamir.
Semantic security is great and has given a new provable sense of security, but commutative building blocks can be combined to build the strangest protocols without going into deep mathematics, are better suited for teaching crypto and for high-level protocol design. They are like the "Lego" blocks of cryptography!
Now I'm working on an new untraceable e-cash protocol which has some additional properties. And I'm searching for a secure  commutable signing primitive.
Best regards,
  Sergio Lerner.

@_date: 2010-03-22 10:23:38
@_author: Sergio Lerner 
@_subject: Question regarding common modulus on elliptic curve cryptosystems 
As far as I understand, Elliptic Curve Pohlig-Hellman is not public-key. It's a private key cipher.
  Sergio.

@_date: 2010-03-22 12:09:16
@_author: Sergio Lerner 
@_subject: Question regarding common modulus on elliptic curve cryptosystems 
I've read some papers, not that much. But I don't mind reinventing the wheel, as long as the new protocol is simpler to explain.
Reading the literature, I couldn't  find a e-cash protocol which :
- Hides the destination / source of payments.
- Hides the amount of money transferred.
- Hides the account balance of each person from the bank.
- Allows off-line payments.
- Avoids giving the same "bill" to two different people by design. This means that the protocol does not need to detect the use of cloned "bills".
- Gives each person a cryptographic proof of owning the money they have in case of dispute.
I someone points me out a protocol that manages to fulfill this requirements, I'd be delighted.
I think I can do it with a commutative signing primitive, and a special zero-proof of knowledge.
  Sergio Lerner.

@_date: 2013-12-16 16:13:25
@_author: Sergio Lerner 
@_subject: [Cryptography] A new digital signature scheme based on the RSA 
This is my first message to the group, and I hope it doesn't bore you.
Playing with RSA digital signatures I realized that the same system can
be used a bit differently and achieve the same security level (as far as
I see). I haven't read about this method before and it's near impossible
to google for a math formula. So this may be a very old broken digital
signature method, or it may be a brand new shinny candidate. If you find
any previous reference, let me know. The main idea is to use the hash of
the message as the public exponent, and everything else derives
naturally from that idea.
*The RSAL Digital signature Scheme*
*KeySetup* ? /n/ , /(p,q)/
 1. Choose two distinct prime numbers
     /p/ and /q /at random,
    of similar bit-length.
 2. Compute /n/ = /pq/. /n/ is used as the modulus
     used as the public
    key.
 3. Compute ?(/n/) = ?(/p/)?(/q/) = (/p/ - 1)(/q/ - 1), where ? is
    Euler's totient function
    .
*Sign*(/M/) ? s
 1.
    Let /M/ be the message to sign.
 2.
    Compute /z/ :=Hash(/M/).
 3. Compute /m/ :=ConvertToInteger(/z/). /m/ must satisfy 0 ? /m/ <
    ?(/n/)//and gcd
    (/m/, ?(/n/))
    = 1. If /p/ and /q/ are safe primes, ConvertToInteger() can be
    implemented simply by shifting /z/ one bit to the left and making
    the resulting number odd.
 4.
    Compute /v/ := Hash(/z/)
 5. Compute /g/ := /m/^/-1/ ( mod ?(/n/) ). /g/ is the multiplicative
    inverse
     of z
    (modulo ?(/n/)).
 6.
    Compute /s/ := /v/^/g / (mod /n/)
 7.
    The signature is /s/
 1.
    Compute /z/ :=Hash(/M/).
 2.
    Compute /v/ := Hash(/z/)
 3.
    Compute /m/ :=Integer(/z/)
 4.
    Compute y := /s/^/m / (mod /n/)
 5.
    Accept the signature if y=v.
If the signature is authentic then we have: y = /s/^/m / = /v/^/g*m / /= v/
This signature scheme security relies on the difficulty of factoring
large integers and the RSA problem (as the RSA cryptosystem).
Suppose that the hash digest is 256 bits. Then for each signature, the
"public exponent" size is generally 257 bits. The ConvertToInteger may
add a 1-bit can prefix the hash to force the "public exponent" to be
always 258 bits.
The "private exponent" will generally have the same size of /n/, so no
small exponent attack is possible.
The cryptosystem has almost no advantage over RSA, only the public key
is just a little shorter.
The disadvantages are that signing requires a modular inversion and an
exponentiation, while RSA requires only an exponentiation. Also
signature verification in RSAL is slower than in RSA signatures. The
only advantage I can think of is that this scheme may be naturally
better protected against side channel attacks during signature
generation. This is because the only secret operation RSAL performs is
modular inversion, and modular inversion (performed with the Extended
Euclidean algorithm) may be harder to attack than modular exponentiation
used in RSA. Also the scheme may be provable secure in the R.O.M., while
RSA requires padding to be provable secure.
Is RSAL broken?
Best regards,
 Sergio Demian Lerner.

@_date: 2013-12-17 10:45:10
@_author: Sergio Lerner 
@_subject: [Cryptography] Fwd: Re: A new digital signature scheme based on the 
Thank you Jonathan. Interesting previous reference. Nevertheless, the
scheme is not the same and there is a very important difference.
The Gannaro,Halevi,Rabin scheme relies on the fact that the hash digests
of H(x) must be divisor intractable, and following the formula on the
paper the H hash digest must be at least 16 384 bits for 128 bit
security (the probability is 2^-sqrt(k), lemma 1), so the modulus should
also be at least 16 384 bits. They "practically" (without proof) reduce
this to 512, noting that it would need 2^k/8 signatures to forge a
signature. So in fact they are claiming 64-bit security, which is not
enough nowadays. They would need a 1024 bit hash function for 128 bit
security on practice.
In my scheme I don't rely on divisor intractability property (in fact
one can freely have H(y)=H(x)*t), and that's the reason to make v
dependent on the message. And my scheme only relies on the RSA
assumption, not the strong RSA assumption, since once the v value is
fixed, the e value is also fixed, so the attacker cannot choose both e
and r simultaneously such that r^e =v, he can only choose r, after
having chosen e and v.
I'm I correct?
Best regards,
    Sergio.

@_date: 2013-12-17 13:31:26
@_author: Sergio Lerner 
@_subject: [Cryptography] A new digital signature scheme based on the RSA 
I think the Gennaro,Halevi,Rabin scheme is completely broken. I cannot
see how they prevent that an attacker forge a signature for H(y).
Suppose H(x)=k*H(y), k can generally be computed in Zn by inverting H(y)
using the Extented Ecleudian algorithm in Zn and computing
k=H(x)*h(y)^-1 (mod n). It's unimportant if H(x) divides H(y) in Z or
not. Inverting H(y) will not be possible if it has no inverse, but it
must be the case that gcd(h(y),n)=1, if not then H(y) could be used to
factor n, so we can assume h(y) is invertible.
Suppose w is the signature for x, then w^H(x) = s
We can forge a signature z for H(y) as z = (w^k).
This is because z^H(y) = w^k^H(y) = w^(k*H(y)) = w^H(x)  = s
What's the scheme security? It seems to me that none.
What did I do wrong?
Best regards, Sergio.
16/12/2013 05:26 p.m., Jonathan Katz escribi?:

@_date: 2013-12-31 17:35:23
@_author: Sergio Lerner 
@_subject: [Cryptography] Strict memory hard hash functions 
I've been playing with a property I named Strict memory hard hash
functions.  Strict memory hard functions are an extension of memory hard
functions such that a slight reduction in the memory available for
computation, compared to a predefined optimal memory size, makes the
function evaluation exponentially slower or infeasible. The main
application of strict memory hard functions is to prove a certain amount
of memory is used during a certain time interval or in a certain
computation. This in turn can be used to attest that areas of memory of
devices do not contain hidden data. Other applications are password
hashing and proof of work. I wrote a preliminary paper of  SeqMemoHash
and TreeMemoHash, two strict sequential memory hard functions under the
random oracle model.
It can be downloaded here:
Please send me comments regarding the security/usefulness of these
Best regards and happy new year,
 Sergio.

@_date: 2014-08-06 12:39:54
@_author: Sergio Lerner 
@_subject: [Cryptography] "The Visual Microphone: Passive Recovery of 
Also you can recover the audio by filming an object with a camera, and
then recording the high-frequency sound the camera makes while filming
the object with a microphone, and then detecting the EM radiation of the
microphone with a radio, and finally detecting the temperature
fluctuations of the radio with a thermostat.  :)
Seriously, very interesting work.

@_date: 2014-01-02 13:56:08
@_author: Sergio Lerner 
@_subject: [Cryptography] Strict memory hard hash functions 
Yes, but I'm unsure if SeqMemoHash can compete with scrypt in password
hashing: password hashing does not require a hard (theoretical) limit in
the memory required, just an economic incentive not to use less memory.
So SeqMemoHash will be slower than any "weakly mem limited" competitor.
Nevertheless, I've not done performance comparisons side-by-side, and I
No, you don't actually need N^2 time. As far I can see, since the amount
of temporary memory increases by each "backtrack", the amount of
"backtracks" each round has to do increases at each depth. So with only
35 rounds (for any value of N>35) , the first round (last round in the
backtrack) will be evaluated 35! times, which is equivalent to 128-bit
security. So for N>35 the amount of time grows linearly with the amount
of memory, since no more than 35 rounds are needed.
As I said previously, the compression function internal state size does
not need to grow (in SeqMemoHash). In TreeMemoHash I think it's true,
but it could be improved.
Maybe the Keccak sponge design can be securely used for any desired
state memory size (the parameter /b/, in the Keccak design).
Best regards, Sergio.

@_date: 2014-01-19 10:37:17
@_author: Sergio Lerner 
@_subject: [Cryptography] Pre-image security of SHA-256 reduced to 16 rounds 
I'm working in a password hashing construction (RandMemoHash, see
I need the fastest possible crypto "hash" function, even if breaking
pre-image resistance requires about 2^32 operations. Collision
resistance is unimportant. This is because the algorithm will repeatedly
apply the reduced round hash function, so at the end, enough rounds will
be applied.
My first choice is SHA-256 with 16 rounds (out of 64). I want to find
the best pre-image attack  that requires little memory.
I searched for information on papers but all I found is attacks against
36 and more rounds.
Any idea?
 Sergio.

@_date: 2014-01-31 23:24:02
@_author: Sergio Lerner 
@_subject: [Cryptography] Pre-image security of SHA-256 reduced to 16 
Thank you John for taking the time to answer my question. Here are my
additional comments...
El 31/01/2014 09:26 p.m., John Kelsey escribi?:
As I see the first two are the same, depending if the function F is a
one-way-permutation or a pseduo-random function.
I don't care about second preimage resistance, and I don't care if it's
a permutation or a pseudo-random function
No, actually the state could be as low as 64-bits, if finding a preimage
costs as much as 2^32 operations.
Nevertheless it must be the case that the cost of "hashing" a fixed
length message must be lower. E.g. for a 80 byte message, applying 10
times a 8-byte input/digest function F should cost less than using a
single 80-byte  i/d function.
Yes, but then I need a function with a very fast (or inexistent)
key-schedule. Maybe XTEA.
Where is this Keccak variant defined?
