
@_date: 2007-12-10 15:11:54
@_author: Francois Grieu 
@_subject: PlayStation 3 predicts next US president 
In this context, the only thing that guards agains an attack by
"some person" is the faint hope that she can't predict the Tn
that the notary will use for a Dp that she submits.
That's because if Tn is known (including chosen) to "some person",
then (due to the weakness in MD5 we are talking about), she can
generate Dp and Dp' such that
  S( MD5(Tn || Dp || Cp || Cn) ) = S( MD5(Tn || Dp' || Cp || Cn) )
whatever Cp, Cn and S() are.
If Tn was hashed after Dp rather than before, poof goes security.
  Francois Grieu

@_date: 2009-05-10 15:29:50
@_author: Francois Grieu 
@_subject: Significance of Schnorr's "Factoring Integers in Polynomial Time"? 
At the rump session of Eurocrypt 2009,
Claus P. Schnorr reportedly presented slides titled "Average Time Fast
SVP and CVP Algorithms: Factoring Integers in Polynomial Time"
I hardly understand 1/4 of the mathematical notation used, and can't
even be sure that the thing is not a (very well done) prank.
Anyone one the list dare make a comment / risk an opinion?
  Francois Grieu

@_date: 2010-07-11 12:50:30
@_author: Francois Grieu 
@_subject: [cryptography] What's the state of the art in factorization? 
There is RSA or Rabin using a signature scheme with message recovery.
With a public modulus of n bits, and a hash of h bits, signing a message
adds only h bits, as long as
- the message to sign is at least (n-h) bits and
- you do not care about spending a few modular multiplication to recover
some (n-h) bits of the message [where few is 17, 2 or 1 for popular
public exponents e of 65537, 3, 2]
This is standardized by ISO/IEC 9796-2 (which add a few bits of overhead
to h, like 16 when n is a multiple of 8).
It is used (with a deprecated and not-quite-perfect option set of
ISO/IEC 9796-2) in many applications where size matters, in particular
EMV Smart Cards, and the European Digital Tachograph.
With e=2 and the newer (randomized) schemes of ISO/IEC 9796-2, you get
security provably related to factoring or breaking the hash.
  Fran?ois Grieu
[I suddenly got a batch of old messages, and wonder what is the
appropriate list address]

@_date: 2010-07-13 08:49:45
@_author: Francois Grieu 
@_subject: Intel to also add RNG 
The Smart Card industry uses True RNG a lot. There, a common line of
thought is to use:
- a hardware RNG, which raw output (perhaps biased) is directly
accessible for testing purposes (only), so that the software can check
it in depth at startup and from time to time to ascertain that it is at
least generating a fair amount of entropy
- followed by appropriate post-processing in hardware (so as to gather
entropy at all time), acting as a mixer/debiaser:; e.g. something LFSR-based
- followed by a crude software test (e.g. no bit stuck)
- optionally followed by software postprocessing (the subject is
debated; this software has to be proven to not include weakness, and the
hardware + crude software test is certified to eliminate such weakness,
so why bother, some say)
There is a standard, known as AIS31, on evaluating True RNG, which
de-facto enforces the first three steps
which references
For German-reading audience, the page linking to that is
Google does good work when fed with AIS31.
  Fran?ois Grieu

@_date: 2010-09-01 08:16:54
@_author: Francois Grieu 
@_subject: RSA question 
I disagree on that last line. There are good digital
signature schemes with no injection of entropy. An example
is ISO/IEC 9796-2:2002 Digital signature scheme 1,
a deterministic digital signature scheme.
Such schemes are very useful because they do not allow a
subliminal channel that the signer could use for
nefarious purposes. Two examples:
- You want to know that this signing black box you purchased,
  accepting (private key, message) and producing a signature
  using an ASIC, does not leak the private key in the
  signature (you also need to guard against other leaks,
  e.g. timing).
- You want to know that this anonymous timestamping web
  service does not embed your IP in the timestamp (although
  admitedly, you can't rule out that it keeps and secretly
  sells a log of the IP associated with each timestamp
  produced).
For RSA-based digital signature schemes, it is possible to
turn a good scheme with injection of entropy into a good
deterministic scheme: replace the entropy by a pseudo random
function of the message, and have that added information
checked by the verifier.
PKCS (which recommands RSASSA-PSS, a probabilistic signature
scheme) acknowledges that:
  RSASSA-PSS is different from other RSA-based signature schemes
  in that it is probabilistic rather than deterministic,
  incorporating a randomly generated salt value. The salt value
  enhances the security of the scheme by affording a "tighter"
  security proof than deterministic alternatives such as Full
  Domain Hashing (..)
  However, the randomness is not critical to security. In
  situations where random generation is not possible, a fixed
  value or a sequence number could be employed instead, with
  the resulting provable security similar to that of FDH (..)
  Francois Grieu

@_date: 2010-09-05 12:12:15
@_author: Francois Grieu 
@_subject: RSA question 
On 04/09/2010 15:07, Victor.Duchovni at morganstanley.com apparently wrote :
No. As far is known, the effort is approximately proportional to
    exp( (n*(64/9+o(1)))^(1/3) * log(n)^(2/3) )
Both the 64/9 and the o(1) term change the effort estimate way more than
a constant factor.
This is wrong well beyond the omission of the 64/9+o(1) term. By
straight application of the above formula with n=16384 and n=1024, and
expressing the ratio as the nearest power of 2, we get that the cost of
factoring 16 kbits RSA numbers would be approximately 2^219 times that
of factoring a 1 kbits RSA number if we neglect the o(1) term [and still
2^114 rather than 3.15 times neglecting the 64/9+o(1) term].
is comparable to 80*3.15 or 252 bits.
One can't multiply key bit size by ratio of effort; we must instead
*add* the *logarithm* in base 2 of the ratio of effort.
We get that 16k bits RSA would be comparable to 80+219 = 299 bits
symmetric key if we neglect the o(1) term [80+114 = 194 bits neglecting
the 64/9+o(1) term].
  Francois Grieu
[Wondering if the crisis of financial institutions may have to do with
how they do math]

@_date: 2010-09-05 17:13:17
@_author: Francois Grieu 
@_subject: RSA question 
[reposted with a correction, the log(2) factor]
No. According to sources such as
the effort to factor a number n bits is approximately proportional to
    exp( (n*(log(2)*64/9+o(1)))^(1/3) * log(n*log(2))^(2/3) )
Any of the log(2), 64/9 and o(1) term change the effort estimate
way more than by a constant factor.
This is wrong well beyond the omission of the log(2)*64/9+o(1) term.
By application of the above formula with n=16384 and n=1024, and
expressing the ratio as a power of 2, I (now) get that the cost of
factoring 16 kbits RSA numbers would be approximately 2^190 times that
of factoring a 1 kbits RSA number if we neglect the o(1) term [and still
2^99 times, rather than 3.15 times, when omitting the (necessary)
log(2)*64/9+o(1) term].
One can't multiply key bit size by ratio of effort; we must instead
*add* the *logarithm* in base 2 of the ratio of effort.
We get that 16k bits RSA would be comparable to 80+219 = 270 bits
symmetric key if we neglect the o(1) term [80+114 = 179 bits when
omitting the log(2)*64/9+o(1) term].
  Francois Grieu
[Wondering if the financial and engineering crisis may have to do with
how *we* do math]
