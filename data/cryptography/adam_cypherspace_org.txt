
@_date: 2001-08-02 20:28:59
@_author: Adam Back 
@_subject: GESG Identity-Based Public Key Cryptography (ID-PKC) 
ID based PKC systems have another application which is very useful:
non-interactive forward secrecy.  Ross Anderson has a nice write up of the
equivalence here:
particulary section 1.3.  An ID based PKC system can be used to build a
non-interactive forward secure communications scheme.

@_date: 2001-10-15 23:43:25
@_author: Adam Back 
@_subject: First Steganographic Image in the Wild 
If you read the web page it was just a demo created by ABC news --
that doesn't count as found in the wild.  Not that it would be that
far out to find the odd image in the wild created as a novelty by
someone tinkering with stego software, or perhaps even individuals
playing with stego.
Stego isn't a horseman, and the press drumming up scare stories around
stego is ludicrous.  We don't need any more stupid cryptography or
internet related laws.  More stupid laws will not make anyone safer.

@_date: 2001-10-17 02:20:28
@_author: Adam Back 
@_subject: limits of watermarking (Re: First Steganographic Image in the Wild) 
So I presume your discussion on the applicability of stego techniques
to the detection of unauthorised copying refers to the framework where
content is personalised by having something identifying the purchaser
encoded in it at time of delivery to the purchaser.
Steganography means hiding the existance of a message -- making it
hard to distinguish content without a stegotext from content with a
stegotext embedded in it.
Copymarks are about making it hard for the user to remove the message
without massively degrading the quality (*).  This means you want some
or all of the purchaser identifying information to be hard to locate

@_date: 2001-10-17 14:54:17
@_author: Adam Back 
@_subject: limits of watermarking (Re: First Steganographic Image in the Wild) 
I think though that that weakness is more workablee -- for example
playstations can be "chipped" to work from copies of CDs, however
probably the proportion of the market willing to make hardware
modifications is sufficiently low that the copying rate is not a
significant financial loss to the distributor (especially after
adjusting for people who wouldn't have bought the work anyway, which
is the group most likely to make the modification (students with low
budgets etc)).
Things which can be defeated in software or firmware upgrades only are
for more fragile, and subject to changing user demographics, more
internet aware and connected users, increasing scale of file-sharing
networks; whereas devices needing hardware modifications have non-zero
reproduction costs, and risk of damaging expensive equipment in the
It may be more to do with attempts to qualify under legal provisions
of DMCA to construct something which is (legally) arguable qualifying
as a system intended to prevent copying, so they can sue people who
by-pass it.
Another argument I've heard for making dumb proprietary schemes is
that they ened them to be proprietary so they can make onerous
conditions part of the licensing agreement, and sue anyone who makes
devices or software without licensing their broken technology from
them.  In effect that it's utterly broken doesn't matter -- that it's
claimable as an "original" work under patent law matters.
That too is doubtless part of the problem.  IBM's cryptolopes lending
credibility by brand recognition to related technologically broken
efforts such as InterTrust and other watermark related business plan
startups "digi-boxes" and the like.  SDMI was another broken attempt.
It could be that the only thing keeping the InterTrust types in
business is the patentability and DMCA qualifying legal arguments
above.  Technologically they are all systemically broken.
There may be an element of technological naivete on the part of MPAA
RIAA too though, perhaps decision makers were genuinely confused to
start with, and crypto-box outfits will have incentives to exaggerage
the technological properties of their systems to their customers, the
RIAA, DMCA etc.

@_date: 2001-10-20 00:07:03
@_author: Adam Back 
@_subject: limits of watermarking (Re: First Steganographic Image in theWild) 
But no such schemes exist, and as I was arguing earlier, I don't think
they will be found either because there are fundamental problems with
the framework before one even gets to implementation details.
Well Kerchoff's principle (strength lies only in the key, assuming
open specifications) is a very good thing, but I don't think in the
case of copy protection schemes, abiding by it would raise the bar
significantly.  It would tend to remove the stupid things like the
broken proprietary algorithms, simply because someone would look at
the specs and guffaw before they'd shipped it.  But schemes meeting
the RIAA and MPAA's objectives are not buildable whether one uses good
crypto or broken proprietary crypto, and whether one publishes what
one designs or not.
For example Microsoft's DRM v2 was cracked recently [1], and if you
read the technical description, there is some sound crypto (SHA1, DES
(small keys, but sound), ECC key exchanges) in the design as well as
one proprietary block cipher used to build a MAC, but the attacker
didn't even have to try to break the proprietary MAC, because the DRM
v2 system, and _all such schemes generically_ are systemically flawed.
(In this case the attacker simply read the keys from memory, and in
fact with far less effort than anticipated by the implementors simply
side-stepped their not that thorough attempts at obfuscation.)
You can't "hide" things in the open in software on a PC.  You can't
even hide things in hardware if the attackers are determined.  And as
DeCSS shows a few million linux users and hackers counts as a very
determined and incredibly technically able group of people.
[1]

@_date: 2001-09-21 00:04:10
@_author: Adam Back 
@_subject: <nettime> "Pirate Utopia," FEED, February 20, 2001 
Also it's interesting to note that it appears from Niels Provos and
Peter Honeymans paper that none of the currently available stego
encoding programs are secure.  They have broken them all (at least I
recognise the main stego programs available in their list of systems
their tools can attack), and it appears that all of the stego encoders
are naive attempts.
So either the FBI and NSA are unaware of and lagging behind Provos
work and the media reports are unsubstantiated hype ("images could
have contained stego content") designed to further alternative agendas
(nasty privacy software outlawing agendas, or perhaps pure media
originated hype).
Or, they found existing stego software and evidence of it's use on
seized equipment or even some 2nd generation, non-publicly available
stego software on seized equipment.
I rather doubt this second possibility as we've also seen reports that
the perpetrators didn't even use crypto.

@_date: 2001-09-21 18:19:43
@_author: Adam Back 
@_subject: <nettime> "Pirate Utopia," FEED, February 20, 2001 
My point was higher level.  These systems are either already broken or
fragile and very lightly peer reviewed.  There aren't many people
building and breaking them.
I did read the papers; my summary is the above, and from that I
surmise it would not be wise for a terrorist to use current generation
steganography systems.
Probably more likely would be the other posters comment that they
would use pre-arranged manually obscured meaning in inoccuous email,
which if done with low enough bandwidth is probably pretty damn
robust and secure.
However unlike the other poster, I don't consider this stego in the
sense of the news report being discussed -- they are talking up the
idea of banning anonymity and steganography software -- where-as in
reality the software is not being used, doesn't make sense to use due
to the current state of the art.  The lobbying by the signals
intelligence community is mis-characterizing the technical reality to
further their own special interest which is easy to do as both the
public and the media are easy to manipulate as they have even less
understanding of anonymity and steganography than they do of

@_date: 2001-09-22 16:11:08
@_author: Adam Back 
@_subject: <nettime> "Pirate Utopia," FEED, February 20, 2001 
To elaborate on this slightly.  There are inherent reasons why
steganography is harder than encryption: the arms race of hiding data
in noise is based on which side (the hider vs the detecter) has the
best understanding of the characteristics of the host signal.  The
problem is the host signal is not something with clear definition,
what is known is primarily empirical statistical analysis.
Manipulating signals with noise in them to replace noise with the
stego text is not so hard, but knowing and modeling the signal and the
source noise is not a solvable problem.
There will be a never-ended stream of more refined and accurate models
of the signal itself, and biases in the equipment that collects the
signal.  So there will be always a risk that the detecter gets the
edge by marginally more accurately modeling the bias, or finding a
some new bias not modelled by the hider.
There have subsequently been news reports claiming the terrorists had
non-publicly available stego software written by their own expert.
This still conflicts with numerous other reports, so it's not clear
what's going on.
But either way none of this would help the signals intelligence
special interest groups arguments to ban steganography, anonymity or
encryption as if anything it would be proof by example of the argument
that terrorists won't have difficulty obtaining software as they can
in the worst case write it from scratch.

@_date: 2002-04-19 14:51:59
@_author: Adam Back 
@_subject: objectivity and factoring analysis 
I'd just like to make a few comments about the apparently unnoticed or
unstated conflicts of interest and bias in the analysis surrounding
Bernstein's proposal.
The following is not intended to trample on anyone's ego -- but I
think deserves saying.
- I'm not sure any of the respondents so far except Bernstein have
truly understood the math -- there are probably few who do, factoring
being such a narrow research area.
- Dan Bernstein stated that it is not easy to estimate the constants
involved to know whether the asymptotic result affects currently used
key sizes; he stated that the conclusion should be considered unknown
until experimental evidence is gained.
- Nicko van Someren -- the person credited with originally making the
exaggerated, or at least highly worst case interpretation at the FC02
panel -- has a conflict interest -- hardware accelerator gear that
ncipher sell will be more markedly needed if people switch to 2048 or
larger keys.  Nicko has made no public comments in the resulting
- Ian Goldberg also on the panel quickly distanced himself from van
Someren's claim, as Lucky's earlier mail could have been read to imply
Goldberg had also agreed with van Someren's claim.
- RSA's FAQ down playing the result seems relatively balanced though
they have an incentive to downplay the potential of Bernstein's
approach.  They have a history of producing biased FAQs: for example
previously the ECC FAQ where they compared ECC unfavorably to RSA.
The FAQ was removed after they licensed tech from certicom and
included ECC in BSAFE.
- Bob Silverman, former RSA factoring expert, observes on sci.crypt,
- Bruce Schneier's somewhat downplaying comments, as far as I know
Bruce isn't an expert on factoring and he doesn't credit anyone who is
in his report.  Bruce's comments lately seem to have lost much of
their earlier objectivity -- many of his security newsletters lately
seem to contain healthy doses of adverts for counterpane's managed
security offering, and calls for lobbying and laws requiring companies
to use such products for insurance eligibility.
- Lucky on the other hand suggested a practical security engineering
approach to start to plan for possibility of migrating to larger key
sizes.  Already one SSH implementation added a configuration option to
select a minimum key size accepted by servers as a result.  This seems
like a positive outcome.  Generally the suggestion to move to 2048 bit
keys seems like a good idea to me.  Somewhat like MD5 -> SHA1, MD5
isn't broken for most applications but it is potentially tainted by a
partial result.  Similarly I would concur with Lucky that it's prudent
security engineering to use 2048 bit keys in new systems.
Historically for example PGP has had similar migrations from minimum
listed key sizes for casual use from 512 -> 768 -> 1024 over the
years.  The progression to 2048 is probably not a bad idea given
current entry level computer speeds and possibility of Bernstein's
approach yeilding an improvement in factoring.
The mocking tone of recent posts about Lucky's call seems quite
misplaced given the checkered bias and questionable authority of the
above conflicting claims we've seen quoted.

@_date: 2002-04-23 00:37:50
@_author: Adam Back 
@_subject: objectivity and factoring analysis 
For people who aren't following as closely I think it would be useful
to remind that this is an estimate of the building cost and running
time of one aspect of computation in the overal proposal.  (You give a
rough running-time estimate which some might misunderstand).
The _overall_ running time of the algorithm and whether it is even any
faster or more economical than existing algorithms remains _unknown_
due to the asymptotic result which the experiment is intended to
If the hardware were to be built it might for example turn out that
the asymptotic result may only start to offer speedups at far larger
key sizes and if this were the case, depending on the results it could
be that the approach turns out to offer no practical speed-ups for the
for-seeable future.
Or it might turn out that it does offer some incremental improvement,
and even that key sizes should be increased.
But right now no one knows.
btw. As disclaimed in the original post no insult was intended --
merely more accurate information given the somewhat wild speculations.

@_date: 2002-08-05 06:00:31
@_author: Adam Back 
@_subject: dangers of TCPA/palladium 
Like anonymous, I've been reading some of the palladium and TCPA docs.
I think some of the current disagreements and not very strongly
technology grounded responses to anonymous are due to the lack of any
concise and informative papers describing TCPA and palladium.
Not everyone has the energy to reverse engineer a detailed 300-odd
pages of TCPA spec [1] back into high-level design considerations; the
more manageably short business level TCPA FAQs [2], [3] are too
heavily PR spun and biased to extract much useful information from.
So so far I've read Ross Anderson's initial expose of the problem [4];
plus Ross's FAQ [5].  (And more, reading list continues below...).
The relationship between TCPA, and Palladium is:
- TCPA is the hardware and firmware (Compaq, Intel, IBM, HP, and
Microsoft, plus 135+ other companies)
- Palladium is a proposed OS feature-set based on the TCPA hardware
The main 4 features proposed in the TCPA/palladium scheme are:
1. secure bootstrap -- checksums of BIOS, firmware, privileged OS code
are used to ensure the machine knows whether it is running certified
software or not.  This is rooted in hardware, so you can't by pass it
by using virtualization, only by hardware hacking (*).
2. software attestation -- the hardware supports attesting to a third
party whether a call comes from a certified software component as
assured by the hardware described in feature 1. 3. hardware assisted compartmentalization -- CPU can run privileged
software, and RAM can contain information that you can not examine,
and can not modify.  (Optionally the software source can be published,
but that is not necessary, and if it's not you won't be able to
reverse-engineer it as it can be encrypted for the CPU).
4. sealing -- applications can store data that can only be read by
that application.  This works based on more hardware -- the software
state checksums developed in feature 1 are used by hardware to
generate encryption keys.  The hardware will refuse to generate the
key unless the same software state is running.
One good paper to understand the secure bootstrap is an academic paper
"A Secure and Reliable Bootstrap architecture" [6].
It's interesting to see that one of the author's of [6] has said that
TCPA as curently formed is a bad thing and is trying to influence TCPA
to make it more open, to exhibit stronger privacy properties read his
comments at [7].
There are a lot of potential negative implications of this technology,
it represents a major shift in the balance of power comparable in
magnitude to the clipper chip:
1. Potentially cedes control of the platform -- while the palladium
docs talk about being able to boot the hardware with TCPA turned off,
there exists possibility that with minor configuration change the
hardware / firmware ensemble that forms palladium/TCPA could be
configured to allow only certified OSes to boot, period.  It's
intereseting to note, if I read correctly, that the X-box (based on
celeron processor and TCPA / TCPA-like features) does employ this
feature.  See for example: [8].  The documents talk about there being no barrier to certifying TCPA
aware extensions to open-source OSes.  However I'm having trouble
figuring out how this would work.  Perhaps IBM with it's linux support
would build a TCPA extension for linux.  Think about it -- the
extension runs in privileged mode, and presumably won't be certified
unless it passes some audit enforcing TCPA policies.  (Such as keeping
the owner of the machine from reading sealed documents, or reading the
contents of DRM policy controlled documents without meeting the
requirements for the DRM policy.)
2. DSS over-again -- a big aspect of the DSS reverse-engineering was
to allow DVDs to be played in software on linux.  The TCPA platform
seems to have the primary goal of making a framework within which it
is possible to build extensions to implement hardware tamper resistant
DRM.  (The DRM implementation would run in a hardware assisted code
compartment as described in feature 3 above).  So now where does that
put open source platforms?  Will they be able to read such DRM
protected content?  It seems likely that in the longer term the DRM
platform will include video cards without access to video memory,
perhaps encryption of the video signal out to the monitor, and of
audio out to the speakers.  (There are other existing schemes to do
these things which dovetail into the likely TCPA DRM framework.)  With the secure boot strap described in feature 1, the video card and
so on are also part of the boot strap process, so the DRM system would
have ready support from the platform for robustly refusing to play
except on certain types of hardware.  Similarly the application
software which plays these DRM policy protected files and talks to the
DRM policy module in the hardware assisted code compartment will
itself be an application which uses the security boot-strapping
features.  So it won't be possible to write an application on for
example linux to play these files without an audit and license etc
from various content, DRM and OS cartels.  This will lead to exactly
the kind of thing Richard Stallman talked about in his prescient paper
on the coming platform and right to develop competing software control
wars [9].
3. Privacy support is broken -- the "privacy" features while clearly
attempts to defuse a re-run at the pentium serial number debacle, have
not really fixed it's problems.  You have to trust the "Trusted Third
Party" privacy CA not to track you and not to collude with other CAs
and software vendors.  There are known solutions to this particular
sub-problem, for example Stefan Brands digital credentials [10], which
can be used to build a cryptographically assured privacy preserving
PKI avoiding the linking problems arising from identity based and
attribute certificates.
4. Strong enforcement for DMCA DRM excesses -- the types of DRM system
which the platform enables stand a fair chance of providing high
levels of enforcement for things which though strictly legally
mandated (copyright licensing restrictions, limited number of plays of
CDs / DVDs other disadvantageous schemes; inflexible and usurious
software licensing), if enforced strictly would have deleterious
effects on society and freedom.  Copyright violation is widely
practiced to a greater or less extent by just about all individuals.
It is widely viewed as acceptable behavior.  These social realities
and personal freedoms are not taken into account or represented in the
lobbying schemes which lead to the media cartels obtaining legal
support for the erosion of users rights and expansionist power grabs
in DMCA, WIPO etc.
Some of these issues might be not so bad except for the track records,
and obvious monopolistic tendencies and economic pressures on the
entities who will have the root keys to the worlds computers.  There
will be no effect choice or competition due to existing near
monopolies, or cartelisation in the hardware, operating system, and
content distribution conglomerates.
5. Strong enforcement for the software renting model -- the types of
software licensing policy enforcement that can be built with the
platform will also start to strongly enable the software and object
rental ideas.  Again potentially these models have some merit except
that they will be sabotaged by API lock out, where the root key owners
will be able to charge monopoly rents for access to APIs.
6. Audits and certification become vastly more prevalent.  Having had
some involvement with software certification (FIPS 140-1 / CC) I can
attest that this can be expensive exercises.  It is unlikely that the
open source community will be able to get software certified due to
cost (the software is free, there is no business entity to claim
ownership of the certification rights, and so no way to recuperate the
costs).  While certification where competition is able to function is
a good thing, providing users with a transparency and needed
assurance, the danger with tying audits to TCPA is that it will be
another barrier to entry for small businesses, and for open source
7. Untrusted, unauditable software will be able to run without
scrutiny inside the hardware assisted code compartments.  Some of the
documentation talks about open sourcing some aspects.  While this may
come to pass, but that sounded like the TOR (Trusted Operating Root);
other extension modules also running in unauditable compartments will
not be so published.
8. Gives away root control of your machine -- providing potentially
universal remote control of users machines to any government agencies
with access to the TCPA certification master keys, or policies
allowing them to demand certifications on hostile code on demand.
Central authorities are likely to be the only, or the default
controllers of the firmware/software upgrade mechanism which comes as
part of the secure bootstrap feature.
9. Provides a dangerously tempting target for government power-grabs

@_date: 2002-08-05 21:46:43
@_author: Adam Back 
@_subject: dangers of TCPA/palladium 
Some comments on selected parts of anonymous post:
1) about claimed "complexity" of cryptographically assured privacy,
rather than the current "trust me" privacy via the privacy CA "TTP":
To address privacy with for example Brands digital credentials, the
underlying cryptography may be harder to understand, or at least less
familiar, but I don't think using a toolkit based on Brands digital
credentials would be significantly harder than using an identity or
attribute based PKI toolkit.  Similar for Chaum's credentials or other
approach.  Also I notice you imply patents are a problem.  However, the TCPA
itself has patents and will of course charge for the hardware.
Patents it doesn't seem would present a problem for this application,
where there is non-zero reproduction cost hardware involved.
2) about the "root key" / potential for malicious remote control claim
that I make (and Ross Anderson I think also makes):
The "root key" to your computing environment is the private key of the
CA that signs the software updates.  You'll recall that in the secure
boot-strapping process if you choose to boot in the TCPA mode, if
there are deviations or updates these are fetched and are only
accepted if certified by the layer owner.  (I presume different layers
would have updates and certification managed by different vendors
Eg. hardware vendor / TPM vendor for firmware, OS manufactturer for
OS, application manufacturer for application software etc, and that
the secure bootstrap process would accordingly transfer control to the
respective next layers certification keys in case of need for software
The closer to the hardware a software update is the more pervasive the
control a malicious update could exert.  For example there are
apparently plans for TPM mediated direct path to input devices
(esp. keyboard), a malicious update close enough to the hardware could
subvert this protection.
and more on the "root key" problem:
The root key is not the endorsement master keys -- that one just
allows the TPM vendor to extract rent from the hardware manufacturers

@_date: 2002-08-05 22:26:28
@_author: Adam Back 
@_subject: more TCPA stuff (Re: "trust me" pseudonyms in TCPA) 
Note there is one key that is endorsed, so per machine there is one
key, singular.
On the other interpretation of your question: do we trust that the
manufacturer didn't take a copy of the key while certifying it?
Good quesion.  The scenario is analogous to the pre-generated private key on a smart
card.  Do you trust what the hardware vendor did with it?  Did they
generate the private key it off chip and keep a copy?  Did they
generate the private key on chip but export it at the time of
certifying the public key?
Except in this case the smart card is attached to your motherboard,
mediates control of the platform and is called the "TPM" Trusted
Platform Module.
While there are approaches to having third party audits of the
process, publishing the source code, etc; it's still typically not a
very transparent affair as it's in tamper resistant hardware, plus
vulnerable to plausibly deniable snafus, and undetectable backdooring
even if it is generated on TPM.
Effectively I think the best succinct description of the platforms
motivation and function is that:
"TCPA/Palladium is an extensible, general purpose programmable dongle
soldered to your mother board with centralised points belonging to
It seems to me there is both strong possibility for it becoming a
focus for future government attempts at policy malware and legislated
technology implementation, and a focus RIAA/MPAA/WIPO polices imposing
futher expansionist and monopoly propping legislation and legislated
technology implementation to enforce the worst excesses of DMCA.
The technology components are very interesting.  The implications of
what can be done with sealing, secure boot-strapping and remote
attestation are a departure from what people were thinking was
possible with general purpose computing.  As anonymous points out it
makes possible all kinds of applications and changes the nature of
what can be cryptographically assured.
With current non-TCPA platforms the limit of what can be
cryptographically assured is for example what can be encrypted with
password, or other cryptographic mechanism.
Cryptographic assurance is also known as "data separation" -- the
concept that the crytography is able to completely cover the
applications policy restrictions without leaving "trusted" software
components necessary to enforce policies too complex to implement with
encryption / data separation.
With TCPA you can build general purpose policy code which does not
exhibit cryptographic assurance, and yet due to the TCPA platform
assures similar levels of security assurance.  That's a huge change in
world view in the domain of security applications.
In slightly more detail, you can either build applications rooted in
the remote attestation, sealing and secure boot-strapping functions I
described in an earlier post.  Or you can add your own custom policy
and even applications inside a hardware assured code compartment which
the user can not access or tamper with.
One aspect of the implications is the implementation and security
possibilities it lends to DRM applications.  Personally I don't find
this aspect a good thing because I think current copyright law has
reached a state of being a net negative for society and freedom, and
that it's time to rescind them and start-over.
I think we should try analyse as William Arbaugh suggested in [7] what
is desirable, what is safe to implement, and ways to change the
platform to remove the negative aspects.
control of this platform.  If it were completely open, and possible to
fix it's potential dangers, it would bring about a new framework of
secured computing and could be a net positive.  In it's current form
with centralised control and other problems it could be a big net
[7] "The TCPA; What's wrong; What's right and what to do about",
William Arbaugh, 20 Jul 2002

@_date: 2002-08-07 00:57:43
@_author: Adam Back 
@_subject: USENIX Security TCPA/Palladium Panel Wednesday 
Anonymous: clearly Lucky and Ross have been talking about two aspects
of the TCPA and Palladium platforms:
1) the implications of platform APIs planned for first phase
implementation based on the new platform hardware support;
2) the implications of the fact that the owner of the machine is
locked out from the new ring-0;
For 2) one obviously has to go beyond discussing the implications of
the APIs discussed in the documents, so the discussion has included
other APIs that could be built securely with their security rooted in
the new third-party controlled ring-0.
In my initial two messages looking at implications I did try to
clearly distinguish between documented planned APIs and new APIs that
become possible to build with third-party controlled ring-0s.  Other areas where analysis is naturally deviating from the aspects
covered by the available documentation (such as it is) are:
- discussion of likelihood that a given potential API will be built
- looking at history of involved parties:
  - Intel: pentium serial number
  - Microsoft: litany of anti-competetive and unethical business
    practices,   - governments: history of trying to push key-escrow, censorship,
    thought-crime and technologies and laws attempting to enforce
    these infringements of personal freedom
  - RIAA/MPAA: history of lobbying for legislation such as DMCA,
    eroding consumer rights
  - industry/government collaboration: Key Recovery Alliance
    ( which shows an interesting intersection of
    big-companies who are currently and historically were signed on to
    assist the government in deploying key-escrow
- suspicion that the TCPA/Microsoft are putting their own spin and
practicing standard PR techniques: like selective disclosure,
misleading statements, disclaiming planned applications and hence not
taking everything at face value.  TCPA/Microsoft have economic
pressures to spin TCPA/Palladium positively. - analysis is greatly hampered by the lack of definitive, concise,
clearly organized technical documentation.  Some of the main
informative documents even microsoft is pointing at are like personal
blog entries and copies of personal email exchanges.
a number of your responses have been of the form "hey that's not a
fair argument, what section number in the TCPA/Palladium documents
gives the specification for that API".
I suspect some arguing about the dangers of TCPA/palladium feel no
particular obligation to point out this distinction the fact that an
API is not planned in phase 1, or not publicly announced yet offers
absolutely no safe-guard against it's later deployment.

@_date: 2002-08-07 07:20:16
@_author: Adam Back 
@_subject: dangers of TCPA/palladium 
Thanks for the clarifications of the differences between TCPA and
Palladium.  The lack of Palladium docs and fact that TCPA docs
describe OS level features led to the inference that Palladium unless
otherwise stated did what TCPA proposed.
* Documentation
I'm feeling frustrated in being unable to properly analyse Palladium
due to lack of documentation.  Surely microsoft must have _some_
internal documentation that could be released.  Two second hand blog
articles by third parties doesn't quite cut it!
The documents claim privacy advocacy group consultation?  Could the
information shared with these groups be published?
It's quite difficult to reason about implications and limits of a
novel new architecture with incomplete information -- you need
grounded facts down to the technical details to work out what the
implications are from first principles and compare and evaluate the
proponents claims.
* privacy CA
Insufficient data to comment on the degree of openness provided by
palladium for 1 (allow owner to load trusted root cert), and 2 (allow
TPM to be completely disabled).
For 3, the TCPA version of the "privacy CA" is broken (implemented
using "trust me" by a server the user does not and should not have to
trust).  Does Palladium do something different?
later in same message you said:
It sounds as if Palladium suffers from the same broken privacy problem
as TCPA then.  Saying you have a choice about whether to use a service
doesn't alter the fact that you are linkable and identifiable to some
extent -- the extent depending on the exact permutation of attribute,
identity and endoresment certificates you have used.
This vulnerability is unnecessary.  You can certifying things without
having to trust anyone.
* architecture functionality
main features of Palladium together with a hardware collection called
the SCP (= ?) can do the following things:
1. no secure-bootstrapping -- unlike TCPA this is not implemented 2. software-attestation -- Palladium uses SCP able to hash perhaps
   the TOR, Trusted Agents, and application software, and then uses
   TCPA-like endorsed hardware keys in the SCP to remotely attest to
   these hashes.  3. hardware assisted compartmentalization -- CPU can run another layer
   of privileged software with ability to prevent supervisor mode
   (and user mode) reading chosen user mode process memory areas.  The
   ubermode code is called the TOR.  The supervisor mode can install
   any TOR into the ubermode, but the TOR can be remotely attested(?)
4. sealing -- TCPA-style sealing
on 2, softeware-attestation from information so far, it's unclear what
is hashed and what is attested.
on 3, hardware assisted compartmentalization
- Presumably Palladium enabled applications would refuse to run
  unless a Palladium/Microsoft certified TOR is running?
  - Limits the meaningfulness of claims of openness in loading your
    own TOR.  More "you can turn x off but nothing will work if you
    do".
   - or perhaps it is up to the individual Palladium application
     which TOR it trusts?
     - but wouldn't this lead to a break:
       - If a microsoft written Palladium enabled application would
         talk to any TOR, user can load a TOR which is under user
         control to by-pass the compartmentalized memory
         restrictions, regaining root.  But if he can do this, he
         can break Palladium enforced DRM.
also about hardware assisted compartmentalization, earlier I said:
you responded:
If I understand the architecture makes it possible to write a TOR
which supports encrypted applications encrypted for the machines key
which is stored in the SCP.
I thought this scheme would be in the current design because as far as
I can see this would in fact be necessary for strong copy protection
for software (software licensed only for a given machine), which I
presumed microsoft has an interest in.
It would be a bit like a "sealed" application.
* on claim "palladium doesn't prevent anything":
Well as you can still run the same software that is a tautology.
The correct question is: can Palladium enabled software prevent you
from doing things you could do with non-Palladium enabled software.
Palladium is in fact designed explicitly to prevent the user doing
The are whole classes of things Palladium enabled software using
Trusted Agents, a default TOR and SCP features can prevent the user
from doing:
- it prevents the owner modifying application code running on his
  machine which uses the remote-attestation functions to talk to
  remote servers
- it could be used to robustly prevent the user auditing what
  information flowing into and out of his machine (the user can't
  obtain the keys negotiated by the SCP and a remote site, and can't
  grab the keys from the application because it's a trusted agent
  running in a TOR mediated code compartment.)
- it could be used to make file formats which are impossible for third
  parties to be compatible with (Ross Anderson came up with this
  example in [4])
- it could be used to securely hide undocumented APIs
- it could be used to securely implement software copy-protection
  using encrypt for endorsed SCP stored machine keys
- it could be used to prevent reverse-engineering applications
- it could be used for DRM to enforce play-once, to revoke fair use
  rights or any other arbitrary policies (on formats only available to
  Palladium enabled applications)
- it could be used to implement key-escrow of SCP stored keys to put
  government or corporate backdoors in sealed data, and comms with
  remote servers encrypted using SCP negotiated keys; wasn't there a
  statement somewhere that CAPI uses would more immediately get
  benefit from Palladium SCP functions?  Not sure how the mix of
  non-palladium using CAPI applications and palladium enabled SCP
  and/or CAP using applications work out for the key-escrow
  implementing TOR scenario.
now as I mentioned in an earlier post you could claim, oh that's ok
because the user has choice, he can still boot with his own custom
TOR.  In the short term that argument would work.  In the long term we
run the risk that:
a) many users will be so baffled by technology they won't know when
they are at risk and when they are not.
b) new non-backwards compatible file formats and feature creep
together with potential "palladium only format" enforcement could
start to make it very inconvenient to use non-standard TORs or non
palladium applications.
On top of that there are next gen issues, and on-going legal issues.
For example what happens when this scenario plays out:
1. recent release digital content is published early (same time as
cinema for the right price say).
2. hardware hackers do a break-once run anywhere by ripping all the
content on their hacked machine and start distributing it on kazaa
(2.5 Peta-bytes of ripped content and growing weekly)
3. RIAA/MPAA goes back and lobbys for further controls, DMCA
4. new legal, DMCA and RIAA/MPAA, and competition from Sony pressures
are placed on microsoft
difficult to see that far out what is going to happen, but blase
presumptions that it's unrealistic to expect key-escrow, or certified
TOR only and to assume that Lucky Green's Document Revocation Lists
don't get rolled out with DMCA style laws against interfering with --
that ignores a lot of history.
Also mentioned in previous post: just because it's law doesn't mean it
should be enforced.  People are currently afforded the ability to
ignore the masses of extreme and ridiculous IP law as individuals.
When a large chunk of it gets implemented into DRM, and narcware the
once free-wheeling internet information exchanges will become
marginalized, or underground only affairs -- the world will become
stifled by their own computers acting as the policeman inside -- your
own computer deputized by the US government, DMCA et al.
I think you must have misinterpreted what I said: I wasn't talking
about Palladium APIs.
I was talking about the fact that microsoft has historically used
undocumented APIs as a business tactic, and I presume this is still an
ongoing strategy, and the Palladium hardware architecture could be
used to build a TOR which forced competitors attempting to discover
undocumented APIs in order to fairly compete to resort to hardware
hacking.  Unless and until we get the software copyright analog of
DMCA reverse-engineering restrictions which makes that illegal.
I'm not sure what you mean by "de-centralized trust".  Perhaps that
the TOR is publicly audited?  Perhaps that there are multiple vendors
endorsing SCP implementations?
I think Palladium clearly has possibilities to magnify centralized
It is in microsoft's economic interests, and historically their
modus-operandi to aggresively try to create and exploit control points
to extract monopolistic rents, and/or suppress competition.  And of
course other companies also have used the same strategies, but the
current limits placed on such practices by the ability to reverse
engineer for compatibility may come to be eroded by TCPA/Palladium.
It's nothing personal, it's just that intent is open to evolutionary
change, legislative attack, pressures outside of your personal, or
microsoft's control -- economic and legal incentives will arise which
make the platform deviate from current stated intent.  2002 stated
intent is zip guarantee.
Someone sent me this hugely appropriate quote in email relating to
this point:
except here we are not talking about legislation directly but a
hardware platform with the tools to create different control points
and the legal and business pressures that act upon the use and misuse
of that platform and those created control points.
I can only think that your definitions of user control probably are in
terms of abstract "security" rather than a distrusting viewpoint that
wants to be able to audit and modify application behavior.  I presume you mean "freedom" in the sense of freedom from the ills
that it is claimed Palladium enabled applications could improve.
I think of freedom as the ability for self-determination, to
completely control and audit all aspects of software running on my
system.  Without these freedoms applications and vendors can conspire
against the machine owner.  I suspect the majority of people feel
With current information it seems that the Palladium platform is
damaging to freedom and indivdual liberty, and a future risk to free
society.  I am of course completely open to being proven wrong, and
look forward to seeing more detailed Palladium specs so that this can
be tested, and to see the community given the opportunity to point out
potential more open and less control point prone modifications.
[4] "Security in Open versus Closed Systems (The Dance of Boltzmann,
Coase and Moore)", Ross Anderson,
(Sections 4 and 5 only, rest is unrelated)

@_date: 2002-08-07 18:54:31
@_author: Adam Back 
@_subject: Palladium: technical limits and implications (Re: more TCPA stuff (Re: "trust me" pseudonyms in TCPA)) 
I have one gap in the picture: In a previous message in this Peter Biddle said:
But what feature of Palladium stops someone taking a Palladium enabled
application and running it in a virtualized environment.  ie They
write software to emulate the SCP, sealing and attestation APIs.
Any API calls in the code to verify non-virtualization can be broken
by putting a different endoresment root CA public key in the
virtualized SCP.
The Palladium application (without the remote attestation feature) has
no way to determine that it is not virtualized.  If the Palladium
application contains the endoresement root CA key that can be changed.
If the application relies on the SCP to contain the endoresmenet key
but doesn't verify it that can be virtualized with a replacement fake
endoresment root CA public key using the existing SCP APIs.
Then Palladiumized application could be debugged and it's features
used without the Palladium non-virtualization guarantee.
Am I free to do this as the owner of palladium compatible hardware
running a version of windows with the proposed palladium feature set?
If so how does this reconcile with your earlier statement that:
Then we also have the remote attestation feature which can't be so
fooled, but for local attestation does the above break work, or is
there some other function preventing that?
Does that imply that the BORA protections only apply to:
- applications which call home to use remote attestation before
  - and even here the remote attestation has to be enforced to data
    separation levels -- ie it has to be that the server provides a
    required and central part of the application -- like providing the
    content, or keys to the content -- otherwise the remote
    attestation call can also be nopped out with no ill-effect (much
    as calls to dongles with no other effect than to check for
    existance of a dongle could be nopped)
- applications which are encrypted to a machine key which is buried in
  the SCP and endorsed by the hardware manufacturer
  - however you said in your previous mail that this is not planned
* now about my attempts to provide a concise, representative and
  readily understandable summary of what the essence of palladium is:
my previous attempt which you point out some flaws in:
OK that is true.  I presume you focussed on SCP because you took the
dongle to mean literally the SCP component alone.
Let's me try to construct an improved succint Palladium motivation and
function description.  We need to make clear the distinction that the
programmability comes from the hardware/firmware/software ensble
provided by:
hardware: the SCP, new ring0 and code compartmentatlization
(btw what are the Palladium terms for the new ring0 that the TOR runs
in, and what is the palladium term for the code compartment that
Trusted Agents run in -- I'd like to use consistent terminology)
super-kernel: TOR operating in new ring0
software: palladium enabled applications using the features such as
software attestation, and sealing with control rooted in hardware and
the TOR
So would it be fair to characterize the negative aspects of Palladium
as follows:
"Palladium provides an extensible, general purpose programmable
dongle-like functionality implemented by an ensemble of hardware and
software which provides functionality which can, and likely will be
used to expand centralised control points by OS vendors, Content
Distrbuters and Governments."
So I think that statement though obviously less possitively spun than
Microsoft would like perhaps addresses your technical objections with
the previous characterization.
btw I readily concede of course that Palladium platform offers the
security compartmentalization and software assurance aspects anonymous
and Peter Biddle have described; I am just mostly examining the
flip-side of the new boundary of applications buildable to data
separation standards of security with this platform.  One could
perhaps construct a more balanced characterization covering both
positive features and negative risks, but I'll let Palladium
proponents work on the former.
So to discuss your objections to the previous version of my attempted
Palladium characterization:
- as you say the hardware platform does not itself provide control
  points (I agree)
- as you say, the TOR being publicly auditable does not itself provide
  control points
however the platform can, and surely you can admit the risk, and even
the likelihood that it will in fact be used to continue and further
the existing business strategies of software companies, the content
industry and governments.
The dongle soldered to your motherboard can conspire with the CPU and
memory management unit to lock the user out of selected processes
running on their own machine.  If you focus on the subset of buildable applications which operate in
the users interests you can call this a good thing.  If you look at
the risks of buildable applications which can be used to act against
the users interests it can equally be a very dangerous thing.  Also if
you look at historic business practices, legal trends, and the wishes
of the RIAA/MPAA there are risks that these bad practices and trends
will be futher accelerated, exarcerbated and more strongly enforced.
I'd be interested to see you face and comment on these negative
aspects rather than keep your comments solely on beneficial user
functions, claim neutrality of low level features, and disclaim
negative aspects by claiming at a low level user choice.  The low
level user choice may in the long run prove impractical or almost
impossible for novice users, or even advanced users without hardware
hacking tools, to technically exercise.  (I elaborated on the
possiblities for robust format compatibility prevention, and related
possibilities a previous mail.)
I mean it is programmable in the sense that software attestation,
certification and the endoresed new privileged ring0 code, together
with the hardware enforced code compartments allow the protections of
the firmware and hardware running in the SCP to be extended up to
complete applications -- the Trusted Agents running in code
That makes it general purpse because it is programmable.  It could be
used to build many classes of application across a spectrum of good,
debatable and evil:
- more flexibile and secure anonymity systems (clear user
  self-interest as anonymous suggested)
- DRM (mixed positive and negative, debatable depends on your point of
  view)
- and for example key-escrow of SCP support crypto functions
  implemented in the TOR accessed with say CAPI (very negative)
You're focussing on the low level platform specifics, not on the
bigger implications of the overall hardware, TOR, OS and software
ensemble which I was talking about.  Yes you can run different TORS.
But using a specific (and audited published) TOR which the user may
find himself choosing to run in order to run Palladium-enabled
applications, all the applications, file formats, services and content
which are tied to doing that -- control points could and likely will
be built.
It is my opinion that the directions and business pressures are such
that meaningful distributed control is unlikely to be the long term
result of the things that will be built using the Palladium
hardware/software ensemble.
This seems a fairly clearly consistent and predictable outcome, unless
the software, IP law, RIAA/DMCA and legal systems all make an
_astounding_ complete U-turn in policy and tacitcs coincident with the
deployment of Palladium.  Can you defend the arguement that Palladium and TCPA don't change the
balance of control against the user and against personal control in
our current balance between technical feasibility of building software
systems enforcing third party control and law?
(btw I'll stop nagging about documentation now, previous mail crossed
with your reply on the topic).
"You do not examine legislation in the light of the benefits it will
convey if properly administered, but in the light of the wrongs it
would do and the harms it would cause if improperly administered."

@_date: 2002-08-07 21:38:55
@_author: Adam Back 
@_subject: Palladium: hardware layering model (Re: Palladium: technical limits and implications) 
some definitions:
hw layer -- SCP which I think provides crypto key store, crypto
 	    co-processor for sealing, remote attesation
ring 0 -- new layer which controls memory management unit and secured
code compartments
supervisor mode -- normal supervisor mode, which can now only read
user space, but not trusted agents running in code compartments
user mode -- legacy user level apps under complete control of
supervisor mode
and some ASCII art:
each layer below can decide policy and information disclosure through
APIs to the layer above.  The implications of which are:
- the SCP can implement sealing with data separation against ring-0
(ring-0 can't bypass sealing data separation)
- ring-0 can read all superviser, user, and trusted agent space, but
- ring-0 and MMU can compartmentalize trusted agents so they can't
tamper with each other, and
- ring-0 and MMU can exclude supervisor mode from trusted agent space
and ring-0 space; supervisor mode is itself just another
compartmentalized trusted-agent level space.  Therefore ring-0 can
restrict what supervisor mode (where the normal OS is located) can do.
whereas the normal protected CPU architecture is just:
- from these assumptions it appears an OS could be implemented so that
all OS calls pass through ring-0 APIs and mediation to get to
supervisor mode OS.  In this case the OS could observe system calls
the trusted agent makes, but not in general read, debug, modify
virtualize or modify trusted-agent code.  The non-virtualization
presumes encrypted trusted-agent code, which Peter said is not done,
so this can't be how it works.
I would be interested to hear what model takes for Palladium mapping
the interactions and restrictions between Trusted Agents, user space,
OS kernel, TOR to the hardware.  We need this kind of detail to reason
about limits of the Palladium and make distinctions between what is
possible with Palladium implementation choices vs what other types of
OSes could be built from the hardware features.
One idea I think would be interest is as follows:
- the TOR (which lives in ring-0) _could_ be used together with the OS
to force all trusted-agent in-flows and out-flows (network traffic) to
go through code under supervisor mode control.  I don't think this is likely in the current design; but this change
would be an improvement: - it would at least allow user audit and control of in-flows and
- the user could block suspicious phone-home information out-flows,
- the user could read out-flows and demand un-encrypted documented
formats, or if encrypted, encrypted with keys the supervisor mode gets
copies of.
- similarly in-flow control is interesting, because with no in-flows a
trusted agent could be more liberally allowed to make out-flows (if it
has no input knowledge, and is in a code compartment, and the user
gave it no sensitive it doesn't know anything to leak.)
(Even with encrypted code, or public code which could not otherwise be
audited actively in the sense of debugging it's actual operation to
see what it does in practice in your machine given your data and
circumstances rather than looking at static code and third party
certifications to try to deduce that.  Not all apps may be unencrypted
(a TOR and SCP could clearly be built to support this feature).
So on anonymous comments about OS control:
I'm not sure if anonymous is just generalising when he says the app
can't in any circumstances know anything if the OS is hostile, but I
think it could potentially know things if the OS is hostile.  As I
described with the control and layer I think the palladium hardware
uses.  It seems possible to build some of separations and exclude the
OS from certain types of application.  It depends what you include in
the OS; if the OS includes the TOR, then no.  But it was stated that
the TOR is somewhat independent from the OS.  You could mix and match
and use an MS Palladium TOR with linux potentially (though perhaps not
in practice, it would have to be designed to allow it).  It also
depends on how the OS, trusted agents and supervisor mode is mapped to
the hardware.
Peter seemed to claim these kinds of assurance.  Sealing doesn't
prevent application virtualization, it just prevents the sealed data
being shared between non-virtualized instances of the apps and
virtualized instances.
So I was wondering how Peter could simultaneously claim that
encryption was not used and that "SW can known that it is running on a
given platform."
Remote attestation, which is not itself general -- just a remote
dongle thing -- if not tied to remote dongle controlled sealing which
is necessary for the main application function could be nopped out.
So in the general case it seems that remote attestation is also
effectively virtualizable, modifiable and debuggable by first nopping
out remote attestation checks.  (This is not strictly virtualizable as
the remote dongle call nopping modification makes it no longer the
same application, but as I said unless this is necessary for the
application it doesn't otherwise change it's behavior, so it's
effectively virtualizable).

@_date: 2002-08-07 22:21:42
@_author: Adam Back 
@_subject: Challenge to TCPA/Palladium detractors 
The TCPA/Palladium folks have been working on this for apparently
around 5 years.  We don't yet have a complete definition of what
Palladium is, but anyway...
It may be that some interesting hardware, TOR and OS design changes
could be added which could change the balance.  Other aspects are as
John Denker said more to do with who will control keys and policies
and how much effective user control and choice remains over these
My initial thoughts were around hardware and TOR enforced in-flow and
out-flow control to trusted agents.
This idea was seeded by the smart-card setting of Stefan Brands
digital credentials.  (Read [1] if you are interested, it's a very
clever idea, related to observers in cryptogaphic protocols in
hardware settings).
Briefly the observer in Brands protocol (and observers have been
proposed in other cryptogaphic literature also) tackles an analogous
problem with cryptographic assurance in the special purpose case of
privacy preserving credentials, e-cash and other applications that can
be built from those techniques.
You have a tamper-resistant smart card.  However the user can't
reasonably audit the behavior of the smart-card processor because it
intentionally hides it's keys from the user.  Even if the source is
published, audited, and claims and endoresments about the hardware
made, the user still can't easily audit or reasonablly trust what is
actually in his smart-card.
The tamper-resistant smart card is somewhat related to the crypto
functions of the SCP in Palladium or the TPM in TCPA, but the observer
approach may offer lessons for TCPA/Palladium in general at higher
The tamper-resistant smart card is considered untrusted and hostile to
user privacy.  The tamper-resistant smart card processor and software
is acting in the interests of the credential issuer / ecash issuer to
prevent the user double-spending coins (*) / using credentials more
times than allowed.
The user has a general purpose computer running software he can
completely audit, control observe running and modify.  The smart-card
has to make all communications with ecash acccepting merchants,
certificate verifiers etc via the general purpose computer the
smart-card is connected to.  The general purpose computer implements
the observer protocols.
The smart-card setting variant of Brands protocol cryptographically
assures 2 things:
- the ecash issuer / credential CA can be assured that the user can
not double spend (or in general violate other properties mediated by
the tamper-resistant smart card)
- the user is cryptographically assured that the smart-card can not
invade his privacy.  This works because the in-flows and out-flows to the smart card are
hardware assured to pass via the general purpose computer, auditable,
use published formats and are cryptographically blinded, to the extent
of optimally frustrating even subliminal channels, via steganography
and the like.
In the same way that TCPA/Palladium are a generalisation of the dongle
concept, this would be a generalisation of the cryptographic concept
of observers.
So for your convenience here's a cut and paste of that initial thought
on applying the observer principle to general purpose TCPA/Palladium
platform from the previous message with subject "Palladium: hardware
layering model":
I wrote in that message:
this is not a fully fleshed out idea as I only thought of it
yesterday, and can't fully analyse it's implications because we don't
yet know proper details of what Palladium hardware is, nor how
microsofts proposed Palladium enhanced windows would be implemented on
that hardware.
(*) Actually he will still be caught and identified with Brands ecash
protocols when the coins are deposited if he does double-spend coins
after breaking hardware tamper-resistance, but that is a level of
detailed not central to this discussion.
[1] "A Technical Overview of Digital Credentials", Stefan Brands, Feb
2002, to appear in International Journal on Information Security.
See Section 8.

@_date: 2002-08-08 06:34:15
@_author: Adam Back 
@_subject: Palladium: hardware layering model 
No I think the above diagram is closer than what you propose.  Peter
also pointed us at Seth Schoen's blog [1] which is a write up of a
briefing Microsoft gave to EFF.  It contains the statement:
Looks consistent with my picture to me.
Your other objection:
I think would just be covered by the details of how the machine
switches from this picture:
to the one above.
For example imagine a default stub nub/TOR that leaves the new MMU
features wide open.  (Supervisor mode can access everything, no
Trusted Agent code compartments running).  The would be some API to
allow the supervisor mode code to load a TOR and switch the TOR code
to ring-0 while leaving the OS running in supervisor mode.
Or alternatively and with equivalent effect: with the boot state, the
OS runs in full ring-0 mode, but just isn't written to make use of any
of the extra ring-0 features.  When it switches to loading a nub/TOR
the OS is relagated to supervisor mode, some MMU permission bits are
juggled around and the TOR occupies ring-0, and the TOR is just an OS
micro-kernel which happens to be written to use the new hardware
features (code compartmentalization, new MMU features, sealing etc)
Clarification on this:
Not what I meant.  Say that you have some code that looks like this:
if ( ! remote_attest( /* ... */ ) ) { exit 0; }
then the remote attest is not doing anything apart from acting as a
remote dongle, so all I have to do to virtualize this code, or break
the licensing scheme based on the remote dongle is nop out the remote
attest verification, then the code can be run as a user application
rather than a trusted agent application and so can be run in a
debugger, have it's state examined etc.
If on the other hand the code says:
download_sealed_content( /* ... */ );
key = remote_attest_and_key_negotiate( /* ... */ );
decrypt_sealed_content( key, /* ... */ );
then nopping out the remote_attest will have a deleterious effect on
the applications function, and so virtualizing it with the remote
attests nopped out will not be useful in bypassing it's policies.

@_date: 2002-08-09 17:08:49
@_author: Adam Back 
@_subject: md5 for bootstrap checksum of md5 implementations? (Re: [ANNOUNCE] OpenSSL 0.9.6f released) 
John Allen's md5-in-perl?
 -iH9T4C`>_-JXF8NMS^$
 N4C24,unpack u,$^I; abs 2**32*sin$_}1..64;sub L{($x=pop)
<<($n=pop)|2**$n-1&$x>>32-$n}sub M{($x=pop)-($m=1+~0)*int$x/$m}do{$l+=$r=read
STDIN,$_,64;$r++,$_.="\x80"if$r<64&&!$p++; V16,$_."\0"x7;$W[14]=$l*8
 unpack H32,pack V4, at A # RSA's MD5
You could include the code in the signed release announcement for
More generally you could also type it in or visually compare it to a
printed version or something as your boot strap of trust, and keep
hash of standard linux statically of known good md5sum with the code
also.  (It's quite a bit slower than md5sum, though it only takes a
couple of seconds to md5 a typical kernel with it -- eg
(See also sha1:

@_date: 2002-08-09 22:13:56
@_author: Adam Back 
@_subject: TCPA/Palladium -- likely future implications (Re: dangers of TCPA/palladium) 
The picture is related but has some extra wrinkles with the
TCPA/Palladium attestable donglization of CPUs.
- It is always the case that targetted people can have hardware
attacks perpetrated against them.  (Keyboard sniffers placed during
court authorised break-in as FBI has used in mob case of PGP using
Mafiosa [1]).
- In the clipper case people didn't need to worry much if the clipper
chip had malicious deviations from spec, because Clipper had an openly
stated explicit purpose to implement a government backdoor -- there's
no need for NSA to backdoor the explicit backdoor.
But in the TCPA/Palladium case however the hardware tampering risk you
identify is as you say relevant:
- It's difficult for the user to verify hardware.  - Also: it wouldn't be that hard to manufacture plausibly deniable
implementation "mistakes" that could equate to a backdoor -- eg the
random number generators used to generate the TPM/SCP private device
However, beyond that there is an even softer target for would-be
- the TCPA/Palladium's hardware manufacturers endoresment CA keys.
these are the keys to the virtual kingdom formed -- the virtual
kingdom by the closed space within which attested applications and
software agents run.
So specifically let's look at the questions arising:
1. What could a hostile entity(*) do with a copy of a selection of
hardware manufacturer endorsement CA private keys?
( (*) where the hostile entity candidates would be for example be
secret service agencies, law enforcement or "homeland security"
agencies in western countries, RIAA/MPAA in pursuit of their quest to
exercise their desire to jam and DoS peer-to-peer file sharing
networks, the Chinese government, Taiwanese government (they may lots
of equipment right) and so on).
a. Who needs to worry -- who will be targetted?
Who needs to worry about this depends on how overt third-party
ownership of these keys is, and hence the pool of people who would
likely be targetted.  If it's very covert, it would only be used plausibly deniably and only
for Nat Sec / Homeland Security purposes.  It if becomse overt over
time -- a publicly acknowledged, but supposedly court controlled
affair like Clipper, or even more widely desired by a wide-range of
entities for example: keys made available to RIAA / MPAA so they can
do the hacking they have been pushing for -- well then we all need to
To analyse the answer to question 1, we first need to think about
question 2:
2. What kinds of TCPA/Palladium integrity depending "trusted"
applications are likely to be built?
Given the powerful (though balance of control changing) new remotely
attestable security features provided by TCPA/Palladium, all kinds of
remote services become possible, for example (though all to the extent
of hardware tamper-resistance and belief that your attacker doesn't
have access to a hardware endorsement CA private key):
- general Application Service Providers (ASPs) that you don't have to
trust to read your data
- less traceable peer-to-peer applications
- DRM applications that make a general purpose computer secure against
BORA (Break Once Run Anywhere), though of course not secure against
ROCA (Rip Once Copy Everywhere) -- which will surely continue to
happen with ripping shifting to hardware hackers.
- general purpose unreadable sandboxes to run general purpose
CPU-for-rent computing farms for hire, where the sender knows you
can't read his code, you can't read his input data, or his output
data, or tamper with the computation.
- file-sharing while robustly hiding knowledge and traceability of
content even to the node serving it -- previously research question,
now easy coding problem with efficient
- anonymous remailers where you have more assurance that a given node
is not logging and analysing the traffic being mixed by it
But of course all of these distributed applications, positive and
negative (depending on your view point), are limited in their
assurance of their non-cryptographically assured aspects:
- to the tamper resistance of the device
- to the extent of the users confidence that an entity hostile to them
doesn't have the endorsement CA's private key for the respective
remote servers implementing the network application they are relying
and a follow-on question to question 2:
3. Will any software companies still aim for cryptographic assurance?
(cryptographic assurance means you don't need to trust someone not to
reverse engineer the application -- ie you can't read the data because
it is encrypted with a key derived from a password that is only stored
in the users head).
The extended platform allows you to build new classes of applications
which aren't currently buildable to cryptographic levels of assurance.
eg. It allows general purpose policies to be built just by writing
policy code that sits in a Trusted Agent code compartment, without
having to figure out how to do split-trust (a la mixmaster chaining),
or forward-secrecy or secret-sharing or any of the other funky stuff;
you can just implement some policy code and it becomes so.
The danger is people will use it to build applications with squishy
interiors, with no cryptographic assurance.  Forward-secrecy
implemented only by a policy in a Trusted Agent that sets a time-limit
on access.  Anonymity but only in the sense that you trust the
hardware isn't tampered with, etc etc.
It will be really tempting because:
- it's much easier: network distributed crypto protocols are
relatively complex
- you can build things you can't otherwise build, the are currently
unsolved problems with distributed crypto protocols
- even where good crypto protocols exist, people will defend not using
them by claims to paranoia: "What you think the NSA has tampered with
your CPU?", or just laziness, cost of implementation etc
So in short probably mostly the answer will be "No", people won't
still aim for cryptographic assurance.
And so a big networked world of distributed applications with a very
squishy and insecure interior inside the closed world will be built.
The new application spaces squishy interior -- like a corporate
firewall with poor to no internal security -- could be ok if you could
be sure the firewall is 100% guaranteed reliable.  TCPA/Palladium
proponents are effectively claiming it be an air-gap grade firewall
guarding the distributed closed world application spaces squishy
interior.  But there is a problem: there are master keys by-passing
all that -- the endorsement CA's private keys.
4. What coming political battles will result?
a. If TCPA/Palladium systems get built -- and it may be politically
unstoppable given the power of the distributed security paradigm it
opens up -- then the battle of the coming decades will center around
control of access to that squishy interior.  The keys that control
access to the closed world are the endoresment CA private keys.
b. You will see many clipper like attempts by governments attempting
to make policies surrounding conditional access to that closed world:
   - law enforcement access to the endorsement private CA keys
     controlling access, so they can setup sting operations,
     demand that ISPs and ASPs collaborate with virtualized versions
     of network services so they can trace things
   - NSA designed protocols to allow such things, black box mediated,
     court order approved, split database access to hardware
     manufacturers private keys
c. As b. progresses RIAA/MPAA will chime in protesting that:
   - Kazaa2 is distributing 10 exabytes a day of ripped recent release
     content not based on BORA (which is now somewhat harder), but on
     ROCA (Rip Once Copy Anywhere) as the content rippers move into
     hardware hacking territory
   - the RIAA/MPAA can't hack, spoof or jam kazaa2 with bogus content
     because cypherpunks have fixed the protocols using WoT, certified
     content, and other crypto-fu so they can't even observe who's
     downloading what or who's serving what
   - and therefore they also demand access to the closed world so they
     can exert their recent legal rights to hack and DDoS the file
     sharing networks
d. Unauthorised access to the closed environment (by hacking your own
hardware) will become illegal with DMCA like restrictions (if it
wouldn't already be where some relation to copyright could be drawn).
e. Software companies, and OS vendors will follow Microsoft's current
lead into an unholy battle with highlights such as:
   - undocumented APIs to gain advantage over competitors, not only
     hardware hacking required to discover APIs, but attestation to
     ensure only those companies who have licensed the right to use
     the API can use it
   - incompatible file formats to lock out competition with
     hardware tamper resistance levels of assurance, even file formats
     that must have certified documents for applications to open them,
     so even if you had the spec you couldn't be compatible
   - copyright protection with software encrypted for the CPU, so you
     can't even audit the static code
   - software renting models again enforced by hardware
   - whole collection of 2nd generation IP "innovations" which will be
     built on top of such things
     - charge per person you share file in a given document format;
     - charge per format conversion
f. Lucky's Documentat Revocation lists to allow governments, companies
etc to to some extent after the fact control distribution of data
g. Increasingly minute enforcement of repressive levels of IP
tracking, and arbitrarily user hostile, fair-use eroding document
viewing and use policies
5. What could be done to protect the user?
a. implement cryptographic assurance inside the closed space where
possible -- that way if you are targetted by someone able to get
inside it you still have the same protection as now.
b. use web-of-trust techniques to provide an overlay of trust on the
endorsement trust.  ie users endorse their own machines to say "this
is my machine" this implies that either:
- it's not tampered with (presuming the user himself was not a target
of some attack or investigation) - or the only tamperer is the certifying user
web-of-trust overlaid on the hardware endorsement helps as:
  - This makes the endorsement keys less useful in a
    covertly obtained endorsement CA private key scenario
  - Even if there are court authorized law enforcement access to
    endorsement CA private keys, or RIAA/MPAA access to endorsement CA
    private keys you can to the extent of your connectivity in the web
    of trust, better avoid using the services of rogue
    agents inside the closed space.
c. Demand ability to audit information in-flows into trusted agents
where there are unauditable out-flows; demand that this is implemented
in a way which allows code under user control to audit
d. Demand the ability to audit information out-flows, where there are
unauditable in-flows or sensitive user data processed by the
application; similarly demand that this is implemented in a way which
allows code under user control to audit
e. Demand cryptographically assured anonymity protection so that there
are no "trusted third parties" who can link your network usage and
identify you.
e. Other ideas?  (Other than to lobby to prevent the building or use
this model).
[1] "FBI Bugs Keyboard of PGP-Using Alleged Mafioso", 6 Dec 2000,

@_date: 2002-08-10 05:37:30
@_author: Adam Back 
@_subject: p2p DoS resistance and network stability (Re: Thanks, Lucky, for helping to kill gnutella) 
The point that a number of people made is that what is said in the
article is not workable: clearly you can't ultimately exclude chosen
clients on open computers due to reverse-engineering.
(With TCPA/Palladium remote attestation you probably could so exclude
competing clients, but this wasn't what was being talked about).
The client exclusion plan is also particularly unworkable for gnutella
because some of the clients are open-source, and the protocol is (now
since original reverse engineering from nullsoft client) also open.
With closed-source implementations there is some obfuscation barrier
that can be made: Kazaa/Morpheus did succeed in frustrating competing
clients due to it's closed protocols and unpublished encryption
algorithm.  At one point an open source group reverse-engineered the
encryption algorithm, and from there the contained kazaa protocols,
and built an interoperable open-source client giFT
 but then FastTrack promptly changed the
unpublished encryption algorithm to another one and then used remote
code upgrade ability to "upgrade" all of the clients.
Now the open-source group could counter-strike if they had
particularly felt motivated to.  For example they could (1)
reverse-engineer the new unpublished encryption algorithm, and (2) the
remote code upgrade, and then (3) do their own forced upgrade to an
open encryption algorithm and (4) disable further forced upgrades.
(giFT instead after the "ugrade" attack from FastTrack decided to
implement their own open protocol "openFT" instead and compete.  It
also includes a general bridge between different file-sharing
networks, in a somewhat gaim like way, if you are familiar with
I grant you that making simultaneously DoS resistant, scalable and
anonymous peer-to-peer networks is a Hard Problem.  Even removing the
anonymous part it's still a Hard Problem.
Note both Freenet and Mojo try to tackle the harder of those two
problems and have aspects of publisher and reader anonymity, so that
they are doing less well than Kazaa, gnutella and others is partly
because they are more ambitious and tackling a harder problem.  Also
the anonymity aspect possibly makes abuse more likely -- ie the
attacker is provided as part of the system tools to obscure his own
identity in attacking the system.  DoSers of Kazaa or gnutella would
likely be more easily identified which is some deterrence.
I also agree that the TCPA/Palladium attested closed world computing
model could likely more simply address some of these problems.
(Lucky slide critique in another post).

@_date: 2002-08-12 17:14:42
@_author: Adam Back 
@_subject: Palladium: technical limits and implications 
Here's a slightly updated version of the diagram I posted earlier:
+---------------+------------+  +----------------------------+  +----------------------------+  Integrity Metrics in a given level are computed by the level below.
The TOR starts Trusted Agents, the Trusted Agents are outside the OS
control.  Therefore a remote application based on remote attestation
can know about the integrity of the trusted-agent, and TOR.
ring -1/TOR is computed by SCP/hardware; Trusted Agent is computed by
The parallel stack to the right: OS is computed by TOR; Application is
computed OS.
So for general applications you still have to trust the OS, but the OS
could itself have it's integrity measured by the TOR.  Of course given
the rate of OS exploits especially in Microsoft products, it seems
likley that the aspect of the OS that checks integrity of loaded
applications could itself be tampered with using a remote exploit.
Probably the latter problem is the reason Microsoft introduced ring -1
in palladium (it seems to be missing in TCPA).

@_date: 2002-08-12 19:30:00
@_author: Adam Back 
@_subject: Palladium: technical limits and implications 
Peter Biddle, Brian LaMacchia or other Microsoft employees could
short-cut this guessing game at any point by coughing up some details.
Feel free guys... enciphering minds want to know how it works.
(Tim Dierks: read the earlier posts about ring -1 to find the answer
to your question about feasibility in the case of Palladium; in the
case of TCPA your conclusions are right I think).
I thought we went over this before?  My hypothesis is: I presumed
there would be a stub TOR loaded bvy the hardware.  The hardware would
allow you to load a new TOR (presumably somewhat like loading a new
BIOS -- the TOR and hardware has local trusted path to some IO
I don't know what leads you to this conclusion.
How would the OS or user mode apps communicate with trusted agents
with this model?  The TOR I think would be the mediator of these
communications (and of potential communications between trusted
agents).  Before loading a real TOR, the stub TOR would not implement
talking to trusted agents.
I think this is also more symmstric and therefore more likely.  The
trusted agent space is the same as supervisor mode that the OS runs
in.  It's like virtualization in OS360: there are now multiple "OSes"
operating under a micro-kernel (the TOR in ring -1): the real OS and
the multiple trusted agents.  The TOR is supposed to be special
purpose, simple and small enough to be audited as secure and stand a
chance of being so.
The trusted agents are the secure parts of applications (dealing with
sealing, remote attestation, DRM, authenticated path to DRM
implementing graphics cards, monitors, sound cards etc; that kind of
thing).  Trusted agents should also be small, simple special purpose
to avoid them also suffering from remote compromise.  There's limited
point putting a trusted agent in a code compartment if it becomes a
full blown complex application like MS word, because then the trusted
agent would be nearly as likely to be remotely exploited as normal
trusted-agents will also need to use OS services, the way you have it
they can't.
I don't think it's a big problem to replace a stub TOR with a given
TOR sometime after OS boot.  It's analogous to modifying kernel code
with a kernel module, only a special purpose micro-kernel in ring -1
instead of ring 0.  No big deal.
In TCPA which does not have a ring -1, this is all the TPM does
(compute metrics on the OS, and then have the OS compute metrics on
While Trusted Agent space is separate and better protected as there
are fewer lines of code that a remote exploit has to be found in to
compromise one of them, I hardly think Palladium would discard the
existing windows driver signing, code signing scheme.  It also seems
likely therefore that even though it offers lower assurance the code
signing would be extended to include metrics and attestation for the
OS, drivers and even applications.
I take this to mean that as stated somewhere in the available docs the
OS can not observe or even know how many trusted agents are running.
So he's stating that they've made OS design decisions such that the OS
could not refuse to run some code on the basis that a given Trusted
Agent is running.
This functionality however could be implemented if so desired in the

@_date: 2002-08-12 21:07:59
@_author: Adam Back 
@_subject: trade-offs of secure programming with Palladium (Re: Palladium: technical limits and implications) 
I think you are making incorrect presumptions about how you would use
Palladium hardware to implement a secure DRM system.  If used as you
suggest it would indeed suffer the vulnerabilities you describe.
The difference between an insecure DRM application such as you
describe and a secure DRM application correctly using the hardware
security features is somewhat analogous to the current difference
between an application that relies on not being reverse engineered for
it's security vs one that encrypts data with a key derived from a user
In a Palladium DRM application done right everything which sees keys
and plaintext content would reside inside Trusted Agent space, inside
DRM enabled graphics cards which retrict access to video RAM, and
later DRM enabled monitors with encrypted digital signal to the
monitor, and DRM enabled soundcards, encrypted content to speakers.
(The encrypted contentt to media related output peripherals is like
HDCP, only done right with non-broken crypto).
Now all that will be in application space that you can reverse
engineer and hack on will be UI elements and application logic that
drives the trusted agent, remote attesation, content delivery and
hardware.  At no time will keys or content reside in space that you
can virtualize or debug.
In the short term it may be that some of these will be not fully
implemented so that content does pass through OS or application space,
or into non DRM video cards and non DRM monitors, but the above is the
end-goal as I understand it.
As you can see there is still the limit of the non-remote
exploitability of the trusted agent code, but this is within the
control of the DRM vendor.  If he does a good job of making a simple
software architecture and avoiding potential for buffer overflows he
stands a much better chance of having a secure DRM platofrm than if as
you describe exploited OS code or rogue driver code can subvert his
There is also I suppose possibility to push content decryption on to
the DRM video card so the TOR does little apart from channel key
exchange messages from the SCP to the video card, and channel remote
attestation and key exchanges between the DRM license server and the
SCP.  The rest would be streaming encrypted video formats such as CSS
VOB blocks (only with good crypto) from the network or disk to the
video card.
Similar kinds of arguments about the correct break down between
application logic and placement of security policy enforcing code in
Trusted Agent space apply to general applications.  For example you
could imagine a file sharing application which hid the data the users
machine was serving from the user.  If you did it correctly, this
would be secure to the extent of the hardware tamper resistance (and
the implementers ability to keep the security policy enforcing code
line-count down and audit it well).
At some level there has to be a trade-off between what you put in
trusted agent space and what becomes application code.  If you put the
whole application in trusted agent space, while then all it's
application logic is fully protected, the danger will be that you have
added too much code to reasonably audit, so people will be able to
gain access to that trusted agent via buffer overflow.
So therein lies the crux of secure software design in the Palladium
style secure application space: choosing a good break-down between
security policy enforcement, and application code.  There must be a
balance, and what makes sense and is appropriate depends on the
application and the limits of the ingenuity of the protocol designer
in coming up with clever designs that cover to hardware tamper
resistant levels the the applications desired policy enforcement while
providing a workably small and pracitcally auditable associated
trusted agent module.
So there are practical limits stemming from realities to do with code
complexity being inversely proportional to auditability and security,
but the extra ring -1, remote attestation, sealing and integrity
metrics really do offer some security advantages over the current

@_date: 2002-08-12 22:13:58
@_author: Adam Back 
@_subject: trade-offs of secure programming with Palladium (Re: Palladium: technical limits and implications) 
At this point we largely agree, security is improved, but the limit
remains assuring security of over-complex software.  To sum up:
The limit of what is securely buildable now becomes what is securely
auditable.  Before, without the Palladium the limit was the security
of the OS, so this makes a big difference.
Yes some people may design over complex trusted agents, with sloppy
APIs and so forth, but the nice thing about trusted agents are they
are compartmentalized:
If the MPAA and Microsoft shoot themselves in the foot with a badly
designed over complex DRM trusted agent component for MS Media Player,
it has no bearing on my ability to implement a secure file-sharing or
secure e-cash system in a compartment with rigorously analysed APIs,
and well audited code.  The leaky compromised DRM app can't compromise
the security policies of my app.
Also it's unclear from the limited information available but it may be
that trusted agents, like other ring-0 code (eg like the OS itself)
can delegate tasks to user mode code running in trusted agent space,
which can't examine other user level space, nor the space of the
trusted agent which stated them, and also can't be examined by the OS.
In this way for example remote exploits could be better contained in
the sub-division of trusted agent code.  eg. The crypto could be done
by the trusted-agent proper, the mpeg decoding by a user-mode
component; compromise the mpeg-decoder, and you just get plaintext not
keys.  Various divisions could be envisaged.
Given that most current applications don't even get the simplest of
applications of encryption right (store key and password in the
encrypted file, check if the password is right by string comparison is
suprisingly common), the prospects are not good for general
applications.  However it becomes more feasible to build secure
applications in the environment where it matters, or the consumer
cares sufficiently to pay for the difference in development cost.
Of course all this assumes microsoft manages to securely implement a
TOR and SCP interface.  And whether they manage to succesfully use
trusted IO paths to prevent the OS and applications from tricking the
user into bypassing intended trusted agent functionality (another
interesting sub-problem).  CC EAL3 on the SCP is a good start, but
they have pressures to make the TOR and Trusted Agent APIs flexible,
so we'll see how that works out.

@_date: 2002-08-14 16:09:24
@_author: Adam Back 
@_subject: MS on Palladium, DRM and copy-protection (via job ad) 
It seems from this article that perhaps MS already had worked out how
to do copy protection with Palladium, or at least thinks it possible
contrary to what was said at USENIX security:
"control what others can do with [...] software.  [...] develop DRM
[...] and Software Licensing products".
Also again shows that Palladium is quite centrally a DRM platform,
which is kind of obvious from the design, and anyway from the naming
of the associated patent "DRM-OS".
MS recruits for Palladium microkernel and/or DRM platform
By John Lettice
Posted: 13/08/2002 at 10:23 GMT
Microsoft's efforts to disassociate Palladium from DRM seem to have hit
their first speed bump. Some voices within the company (and we currently
believe these voices to be right and sensible) hold the view that Palladium
has to be about users' security if it's to stand any chance of winning
hearts and minds, and that associating it with protecting the music
business' IP will be the kiss of death. So they'll probably not be best
pleased by the Microsoft job ad that seeks a group program manager
"interested in being part of Microsoft's effort to build the Digital Rights
Management (DRM) and trusted platforms of the future (Palladium)."

@_date: 2002-08-14 20:02:21
@_author: Adam Back 
@_subject: TCPA/Palladium user interst vs third party interest (Re: responding to claims about TCPA) 
The remote attesation is the feature which is in the interests of
third parties.
I think if this feature were removed the worst of the issues the
complaints are around would go away because the remaining features
would be under the control of the user, and there would be no way for
third parties to discriminate against users who did not use them, or
configured them in given ways.
The remaining features of note being sealing, and integrity metric
based security boot-strapping.
However the remote attestation is clearly the feature that TCPA, and
Microsoft place most value on (it being the main feature allowing DRM,
and allowing remote influence and control to be exerted on users
configuration and software choices).
So the remote attesation feature is useful for _servers_ that want to
convince clients of their trust-worthiness (that they won't look at
content, tamper with the algorithm, or anonymity or privacy properties
etc).  So you could imagine that feature being a part of server
machines, but not part of client machines -- there already exists some
distinctions between client and server platforms -- for example high
end Intel chips with larger cache etc intended for server market by
their pricing.  You could imagine the TCPA/Palladium support being
available at extra cost for this market.
But the remaining problem is that the remote attesation is kind of
dual-use (of utility to both user desktop machines and servers).  This
is because with peer-to-peer applications, user desktop machines are
also servers.
So the issue has become entangled.
It would be useful for individual liberties for remote-attestation
features to be widely deployed on desktop class machines to build
peer-to-peer systems and anonymity and privacy enhancing systems.
However the remote-attestation feature is also against the users
interests because it's wide-spread deployment is the main DRM enabling
feature and general tool for remote control descrimination against
user software and configuration choices.
I don't see any way to have the benefits without the negatives, unless
anyone has any bright ideas.  The remaining questions are:
- do the negatives out-weigh the positives (lose ability to
reverse-engineer and virtualize applications, and trade
software-hacking based BORA for hardware-hacking based ROCA);
- are there ways to make remote-attestation not useful for DRM,
eg. limited deployment, other;
- would the user-positive aspects of remote-attestation still be
largely available with only limited-deployment -- eg could interesting
peer-to-peer and privacy systems be built with a mixture of
remote-attestation able and non-remote-attestation able nodes.

@_date: 2002-08-15 07:06:04
@_author: Adam Back 
@_subject: TCPA not virtualizable during ownership change (Re: Overcoming the potential downside of TCPA) 
Phew... the document is certainly tortuous, and has a large number of
similarly and confusingly named credentials, certificates and keys,
however from what I can tell this is what is going on:
Summary: I think the endorsement key and it's hardware manufacturers
certificate is generated at manufacture and is not allowed to be
changed.  Changing ownership only means (typically) deleting old
identities and creating new ones.
The longer version...
- endorsement key generation and certification - There is one
endorsement key per TPM which is created and certified during
manufacture.  The creation and certification process is 1) create
endorsement key pair, 2) export public key endorsement key, 3)
hardware manufacturer signs endorsement public key to create an
endorsement certificate (to certify that that endorsement public key
belongs to this TPM), 4) the certificate is stored in the TPM (for
later use in communications with the privacy CA.)
- ownership - Then there is the concept of ownership.  The spec says
the TPM MUST ship with no Owner installed.  The owner when he wishes
to claim ownership choose a authentication token which is sent into
the TPM encrypted with the endorsement key.  (They give the example of
the authentication token being the hash of a password).  Physical
presence tests apply to claiming ownership (eg think BIOS POST with no
networking enabled, or physical pin on motherboard like BIOS flash
enable).  The authentication token and ownership can be changed.  The
TPM can be reset back to a state with no current owner.  BUT _at no
point_ does the TPM endorsement private key leave the TPM.  The
TPM_CreateEndorsementKeyPair function is allowed to be called once
(during manufacture) and is thereafter disabled.
- identity keys - Then there is the concept of identity keys.  The
current owner can create and delete identities, which can be anonymous
or pseudonymous.  Presumably the owner would delete all identity keys
before giving the TPM to a new owner.  The identity public key is
certified by the privacy CA.
- privacy ca - The privacy CA accepts identity key certification
requests which contain a) identity public key b) a proof of possession
(PoP) of identity private key (signature on challenge), c) the
hardware manufacturers endorsement certificate containing the TPM's
endorsement public key.  The privacy CA checks whether the endorsement
certificate is signed by a hardware manufacturer it trusts.  The
privacy CA sends in response an identity certificate encrypted with
the TPM's endorsement public key.  The TPM decrypts the encrypted
identity certifate with the endorsement private key.
- remote attestation - The owner uses the identity keys in the remote
attestation functions.  Note that the identity private keys are also
generated on the TPM, the private key also never leaves the TPM.  The
identity private key is certified by the privacy CA as having been
requested by a certified endorsement key.
The last two paragraphs imply something else interesting: the privacy
CA can collude with anyone to create a virtualized environment.  (This
is because the TPM endorsement key is never directly used in remote
attestation for privacy reasons.)  All that is required to virtualize
a TPM is an attestation from the privacy CA in creating an identity
So there are in fact three avenues for FBI et al to go about obtaining
covert access to the closed space formed by TCPA applications: (A) get one of the hardware manufacturers to sign an endorsement key
generated outside a TPM (or get the endorsement CA's private key), or
(B) get a widely used and accepted privacy CA to overlook it's policy
of demanding a hardware manufacturer CA endorsed endorsement public
key and sign an identity public key created outside of a TPM (or get
the privacy CA's private key).
(C) create their own privacy CA and persuade an internet server they
wish to investigate the users of to accept it.  Create themselves a
virtualized client using their own privacy CA, look inside.
I think to combat problem C) as a user of a service you'd want the
remote attestation of software state to auditably include it's
accepted privacy CA database to see if there are any strange "Privacy
CAs" on there.
I think you could set up and use your own privacy CA, but you can be
sure the RIAA/MPAA will never trust your CA.  A bit like self-signing
SSL site keys.  If you and your friends add your CA to their trusted
root CA database it'll work.  In this case however people have to
trust your home-brew privacy CA not to issue identity certificates
without having seen a valid hardware-endorsement key if they care
about preventing virtualization for the privacy or security of some
network application.
Also, they seem to take explicit steps to prevent you getting multiple
privacy CA certificates on the same identity key.  (I'm not sure why.)
It seems like a bad thing as it forces you to trust just one CA, it
prevents web of trust which could reduce your chances of getting
caught in attack scenarios B) and C) by demanding multiple
This section from the spec on trusting the privacy CA is interesting
section 8.3.1 p 195
(not sure what the maintenance policy of the TPM is or what it has to
do with trusting privacy CAs -- it is not otherwise discussed).
also the text on p268 relates to trusting the privacy CA.
Below is some text from the spec which tends to confirm the above.
(Anonymous may have some comments as he seemed to have read the TCPA
spec in more detail than I have.)
Here is an indicative quote from the spec:
informative comment:
normative text:
Anyway Occam's razor suggests that the intent is:
1. the TPM endorsement private key never leaves the TPM
2. the identity private keys never leave the TPM
3. the privacy CA will never issue identity private keys unless the
request is made in relation to a manufacturer certified endorsement
public key.
Note: The endorsement key has key usage restrictions and is marked as
encrypt only, so the assurance the privacy CA gets is not that it
receives a identity certificate request signed by the endorsement
private key, but rather that the issued certificate is encrypted with
the endorsement public key and so could only be decrypted by the TPM
which contains the corresponding private endorsement key.  (I suppose
the motivation might have been that then the privacy CA couldn't prove
to third parties that your endorsement key and identity key are bound

@_date: 2002-08-16 01:44:58
@_author: Adam Back 
@_subject: TCPA not virtualizable during ownership change (Re: Overcoming the potential downside of TCPA) 
I think a number of the apparent conflicts go away if you carefully
track endorsement key pair vs endorsement certificate (signature on
endorsement key by hw manufacturer).  For example where it is said
that the endorsement _certificate_ could be inserted after ownership
has been established (not the endorsement key), so that apparent
conflict goes away.  (I originally thought this particular one was a
conflict also, until I noticed that.)  I see anonymous found the same
But anyway this extract from the CC PP makes clear the intention and
an ST based on this PP is what a given TPM will be evaluated based on:
p 20:
(if only they could have managed to say that in the spec).

@_date: 2002-08-16 02:23:05
@_author: Adam Back 
@_subject: employment market for applied cryptographers? 
On the employment situation... it seems that a lot of applied
cryptographers are currently unemployed (Tim Dierks, Joseph, a few
ex-colleagues, and friends who asked if I had any leads, the spate of
recent "security consultant" .sigs, plus I heard that a straw poll of
attenders at the codecon conference earlier this year showed close to
50% out of work).
Are there any more definitive security industry stats?  Are applied
crypto people suffering higher rates of unemployment than general
application programmers?  (From my statistically too small sample of
acquaintances it might appear so.)
If this is so, why is it?
- you might think the physical security push following the world
political instability worries following Sep 11th would be accompanied
by a corresponding information security push -- jittery companies
improving their disaster recovery and to a lesser extent info sec
- governments are still harping on the info-war hype, national
information infrastructure protection, and the US Information Security
Czar Clarke making grandiose pronouncements about how industry ought
to do various things (that the USG spent the last 10 years doing it's
best to frustrate industry from doing with it's dumb export laws)
- even Microsoft has decided to make a play of cleaning up it's
security act (you'd wonder if this was in fact a cover for Palladium
which I think is likely a big play for them in terms of future control
points and (anti-)competitive strategy -- as well as obviously a play
for the home entertainment system space with DRM)
However these reasons are perhaps more than cancelled by:
- dot-com bubble (though I saw some news reports earlier that though
there is lots of churn in programmers in general, that long term
unemployment rates were not that elevated in general)
- perhaps security infrastructure and software upgrades are the first
things to be canned when cash runs short?  - software security related contract employees laid off ahead of
full-timers?  Certainly contracting seems to be flat in general, and
especially in crypto software contracts look few and far between.  At
least in the UK some security people are employed in that way (not
familiar with north america).
- PKI seems to have fizzled compared to earlier exaggerated
expectations, presumably lots of applied crypto jobs went at PKI
companies downsizing.  (If you ask me over use of ASN.1 and adoption
of broken over complex and ill-defined ITU standards X.500, X.509
delayed deployment schedules by order of magnitude over what was
strictly necessary and contributed to interoperability problems and I
think significantly to the flop of PKI -- if it's that hard because of
the broken tech, people will just do something else.)
- custom crypto and security related software development is perhaps
weighted towards dot-coms that just crashed.
- big one probably: lack of measurability of security -- developers
with no to limited crypto know-how are probably doing (and bodging)
most of the crypto development that gets done in general, certainly
contributing to the crappy state of crypto in software.  So probably
failure to realise this issue or perhaps just not caring, or lack of
financial incentives to care on the part of software developers.
Microsoft is really good at this one.  The number of times they
re-used RC4 keys in different protocols is amazing!
Other explanations?  Statistics?  Sample-of-one stories?
yes, still employed in sofware security industry; and in addition have
been doing crypto consulting since 97 ( if
you have any interesting applied crypto projects; reference
commissions paid.

@_date: 2002-08-18 16:58:56
@_author: Adam Back 
@_subject: Cryptographic privacy protection in TCPA 
With Brands digital credentials (or Chaums credentials) another
approach is to make the endorsement key pair and certificate the
anonymous credential.  That way you can use the endorsement key and
certificate directly rather than having to obtain (blinded) identity
certificates from a privacy CA and trust the privacy CA not to issue
identity certificates without seeing a corresponding endorsement
However the idea with the identity certificates is that you can use
them once only and keep fetching new ones to get unlinkable anonymity,
or you can re-use them a bit to get pseudonymity where you might use a
different psuedonym for a different service where you are anyway
otherwise linkable to a given service.
With Brands credentials the smart card setting allows you to have more
compact and computationally cheap control of the credential from
within a smart card which you could apply to the TPM/SCP.  So you can
fit more (unnamed) pseudonym credentials on the TPM to start with.
You could perhaps more simply rely on Brands credential lending
discouraging feature (ability to encode arbitrary values in the
credential private key) to prevent break once virtualize anywhere.
For discarding pseudonyms and when you want to use lots of pseudonyms
(one-use unlinkable) you need to refresh the certificates you could
use the refresh protocol which allows you to exchange a credential for
a new one without trusting the privacy CA for your privacy.
Unfortunately I think you again are forced to trust the privacy CA not
to create fresh virtualized credentials.  Perhaps there would be
someway to have the privacy CA be a different CA to the endorsement CA
and for the privacy CA to only be able to refresh existing credentials
issued by the endorsement CA, but not to create fresh ones.
Or perhaps some restriction could be placed on what the privacy CA
could do of the form if the privacy CA issued new certificates it
would reveal it's private key.
"Also relevant is An Efficient System for Non-transferable Anonymous
Credentials with Optional Anonymity Revocation", Jan Camenisch and
Anna Lysyanskaya, Eurocrypt 01
These credentials allow the user to do unlinkable multi-show without
involving a CA.  They are somewhat less efficient than Chaum or Brands
credentials though.  But for this application does this removes the
need to trusting a CA, or even have a CA: the endorsement key and
credential can be inserted by the manufacturer, can be used
indefinitely many times, and are not linkable.
As you point out unlinkable anonymity tends to complicate revocation.
I think Camenisch's optional anonymity revocation has similar
properties in allowing a designated entity to link credentials.
Another less "TTP-based" approach to unlinkable but revocable
credentials is Stubblebine's, Syverson and Goldschlag, "Unlinkable
Serial Transactions", ACM Trans on Info Systems, 1999:
(It's quite simple you just have to present and relinquish a previous
pseudonym credential to get a new credential; if the credential is due
to be revoked you will not get a fresh credential.)
I think I would define away the problem of local breaks.  I mean the
end-user does own their own hardware, and if they do break it you
can't detect it anyway.  If it's anything like playstation mod-chips
some proportion of the population would in fact would do this.  May be
1-5% or whatever.  I think it makes sense to just live with this, and
of course not make it illegal.  Credentials which are shared are
easier to revoke -- knowledge of the private keys typically will
render most schemes linkable and revocable.  This leaves only online
lending which is anyway harder to prevent.

@_date: 2002-08-21 03:24:21
@_author: Adam Back 
@_subject: Cryptographic privacy protection in TCPA 
There was some off-list discussion about possibility for sharing these
credentials once a given credential is extracted from it's TPM by a
user who broke the tamper resistance of his TPM.
I also said:
Because Camenisch credentials are unlinkable multi-show it makes it
harder to recognize sharing, so the user could undetectably share
credentials with a small group that he trusts.  (By comparison with linkable pseudonymous credentials and a privacy CA
the issuer and/or verifier would see unusually high activity from a
given pseudonym or TPM endorsement key if the corresponding credential
were shared too widely.)
However if the Camenisch (unlinkable multi-show) credential were
shared too widely the issuer may also learn the secret key and hence
be able to link and so revoke the overly-shared credentials.  This
combats sharing though to a limited extent.
Another idea to improve upon this inherent risk of sharing too widely
may be to use a protocol which it is not safe to do parallel shows
with.  (Some ZKPs are not secure when you engage in multiple show
protocols in parallel.  Usually this is considered a bad thing, and
steps are taken to allow safe parallel show.)  For this application a show protocol which it is not safe to engage in
parallel shows may frustrate sharing: someone who shared the
credential too widely would have difficulty coordinating amongst the
sharees not to show the same credential in parallel.  I notice
Camenisch et al mention steps to avoid parallel showing problem, so
perhaps that feature could be reintroduced.
In contrast, the TPM can easily ensure that the credential is not used
in parallel shows.

@_date: 2002-08-22 16:54:51
@_author: Adam Back 
@_subject: the underground software vulnerability marketplace and its hazards (fwd) 
Right.  And I fail to see how any of this is dangerous.
Clearly people are free to sell information they create to anyone they
choose under any terms they choose.  (For example the iDEFENSE promise
of the author to not otherwise reveal for 2 weeks to give iDEFENSE
some value.)
This commercialisation seems like a _good thing_ as it may lead to
more breaks being discovered, and hence more secure software.
(It won't remain secret for very long -- given the existance of
anonymous remailers etc., but the time-delay in release allows the
information intermediary -- such as iDEFENSE -- to sell the
information to parties who would like it early, businesses for example
people with affected systems.
Criminal crackers who can exploit the information just assist in
setting a fair price and forcing vendors and businesses to recognise
the true value of the information.  Bear in mind the seller can not
know or distinguish between a subscriber who wants the information for
their own defense (eg a bank or e-commerce site, managed security
service provider), and a cracker who intends to exploit the
information (criminal organisation, crackers for amusement or
discovery of further inforamtion, private investigators, government
agencies doing offensive information warfare domesticaly or
I don't see any particular moral obligation for people who put their
own effort into finding a flaw to release it to everyone at the same
time.  Surely they can release it earlier to people who pay them to
conduct their research, and by extension to people who act as
intermediaries for the purpose of negotiating better terms or being
able to package the stream of ongoing breaks into more comprehensive
subscription service.  I think HP were wrong, and find their actions in trying to use legal
scare tactics reprehensible: they should either negotiate a price, or
wait for the information to become generally available.

@_date: 2002-08-24 17:11:42
@_author: Adam Back 
@_subject: Cryptographic privacy protection in TCPA 
Since writing this I realised that there is a problem revoking
unlinkable multi-show credentials:
- I was presuming that revealing the credential and it's secret key is
sufficient to allow someone to link shows of that credential.
- but to link you'd have to try each revoked credential in turn.
Therefore the verifier would have to perform work linear in the number
of revoked credentials at each show, for the duration of the epoch.
Anonymous suggests one way out is to just define that the issuing CA
and the refreshing CA to be the same entity.  Then you already have to
trust the hardware manufacturer not to issue certs whose secrets are
outside of a TPM.  In this case Brands or Chaum credentials work.
The remaining desiderata are:
- it is not ideal from a risk management perspective to have to have
the hardware manufacturers endorsement private key online to refresh
certificates (or in general for there to be any private key online
that allows issuing of credentials whose private keys lie outside a
- not ideal to have to have an online protocol with an otherwise
non-existant third party (credential refresh CA) in order to avoid
Other ideas I gave in an earlier post towards fixing these remaining
issues now that it seems unlinkable multi-show credentials won't work:

@_date: 2002-02-11 09:33:32
@_author: Adam Back 
@_subject: why there is no WoT in S/MIME (Re: PGP & GPG compatibility) 
The fact that S/MIME doesn't work well with WoT, and that there are
two classes of users: end users, and CAs is a design criteria burnt
into the spec and most of the software.  It's a business issue, the CA
players were involved in writing the standards, and they have a vested
interest to force users into paying them money to use the software.
It's also a factor why most of the statistic of users with S/MIME MUAs
aren't using it.  I think S/MIME would be more widely used by
individuals if the end-user software worked without CA certificates
and preferably supported some form of WoT.

@_date: 2002-02-11 09:48:56
@_author: Adam Back 
@_subject: whats bad about RFIDs in currency (Re: Where's the smart money?) 
Isn't it already the case that all notes have serial numbers.  I
thought that with some currencies this is printed in magnetic ink with
letter shape designed for easy electronic reading (blobby numbers with
blobs in different places to make it easy for the reader to
distinguish each of the numbers).  Anyone know if current cash point
dispensers are reading those numbers?
So an electronic serial number (an RFID) is no less or more than a
more expensive, marginally harder to copy (if RFIDs are < 30c each I
can't see them being that hard to reproduce) electronic serial number.
The relatively anonymity of cash comes not from the notes being
indistinguishable (they already are not so), but from the fact that
they are exchanged, potentially many times, in a peer-to-peer fashion
without going via a bank account.  This would still be the case for
When it would start to get scary would be if the financial tracking
special interest groups introduced online equipment for merchants to
verify validty of RFID bank notes as an anti-fraud measure (with the
real objective for the special interest not being the anti-fraud, but
the tracking).  There is a precedent for currency fraud detection at
merchant point of sale, though not an online one with UV lights,
special pens etc.  There is also a precedent with online fraud
detection with credit cards.
Here the RFID would be bad because it would I suspect make the
equipment to off-line or on-line audit and track notes much cheaper
and more robust.  The alternative of scanner reading the serial
numbers sounds clunky and low tech enough that it wouldn't fly, and
wouldn't be able to masquerade effectively as an anti-fraud measure
which is a tracking measure in disguise.
I think the latter (online anti-fraud detection of RFID) should be
strongly resisted to preserve payment privacy, and therefore the
former (RFIDs in currency) also should be resisted for the reasons
above that it moves us along a path towards online anti-fraud
equipment at merchant point of sale.

@_date: 2002-02-12 23:16:17
@_author: Adam Back 
@_subject: CFS vs. loopback encryption (was Re: [open-source] File encryption) 
It's quite hard to guarantee that no one has unattended access to your
machine at any time.  A paranoid user could checksum his binaries, and keep the checksum and
minimal boot image on a flash USB fob, or boot off a CDROM, and keep
the checksum on flash.  Then the user could again consider the machine
part of the TCB modulo hardware keyboard sniffers etc.
So it might be nice for an encrypted file system to have native
support for authentication based on the password for this reason also.
Of course said paranoid users could always do their own authentication
using hmac or whatever of the entire file system contents, but
built-in support could more efficiently do this for larger file
systems by MACing parts (sectors?).

@_date: 2002-01-21 20:50:22
@_author: Adam Back 
@_subject: PGP & GPG compatibility 
If you ask me GPG has as much to answer for in the
non-interoperability problems with it's rejection of shipping IDEA
with the default GPG as PRZ et al for deciding to not ship RSA.
I tried arguing with PGP that if they wanted to phase out RSA use, the
best way would be to support it: then more people would have upgraded
to pgp5.x and started using new key types.  Instead people continued
to use PGP2.x in defense as it was the only thing which reliabily
interoperated.  It's understandable that PGP would have wanted to phase out RSA due to
the trouble RSADSI caused with licensing of the RSA patent, but still
the approach taken had predicatbly the opposite effect to that which
they hoped to achieve.
GPG on the other hand is simply wilfully damaging interoperability by
putting their anti-patent stance over the benefit of PGP users.  I
know there are modules to add IDEA support but they're not shipped by
default so most people don't use them.
It seems that the result of GPG and PGP intentionally induced
incompabilities has greatly reduced PGP use.  I used to use PGP a lot,
these days I use it a lot less, most uses induce all kinds of problems
to the extent that most people resort to using plaintext.
If the -pgp2 option implies that GPG will then ship with IDEA and that
there is a way to request PGP2 compability that is a good step.
However it should be possible to automatically select that option
based on the public key parameters of the person you're sending to,
which was if I recall the reason for the introduction of the new
openPGP RSA format, so that a PGP2 generated RSA keys could be
distinguished from openPGP keys, and compability could be maintained.

@_date: 2002-06-21 16:26:38
@_author: Adam Back 
@_subject: Shortcut digital signature verification failure 
Doesn't a standard digital signature plus hashcash / client puzzles
achieve this effect?
The hashcash could be used to make the client to consume more cpu than
the server.  The hashcash collision wouldn't particularly have to be
related to the signature, as the collision would just act as a
proof-of-work entitling the client to have the server verify the
accompanying signature.

@_date: 2002-06-21 17:01:02
@_author: Adam Back 
@_subject: Shortcut digital signature verification failure 
Client puzzles are interactive, but hashcash is non-interactive being
intended to support store-and-forward scenarios -- the client
effectively chooses his own challenge.  There's a reference library
for client and server here:

@_date: 2002-06-26 20:37:12
@_author: Adam Back 
@_subject: Ross's TCPA paper 
Hear, hear!  First post on this long thread that got it right.
Not sure what the rest of the usually clueful posters were thinking!
DRM systems are the enemy of privacy.  Think about it... strong DRM
requires enforcement as DRM is not strongly possible (all bit streams
can be re-encoded from one digital form (CD->MP3, DVD->DIVX),
encrypted content streams out to the monitor / speakers subjected to
scrutiny by hardware hackers to get digital content, or A->D
reconverted back to digital in high fidelity.
So I agree with Bear, and re-iterate the prediction I make
periodically that the ultimate conclusion of the direction DRM laws
being persued by the media cartels will be to attempt to get
legislation directly attacking privacy.
This is because strong privacy (cryptographically protected privacy)
allows people to exchange bit-strings with limited chance of being
identified.  As the arms race between the media cartels and DRM
cohorts continues, file sharing will start to offer privacy as a form
of protection for end-users (eg. freenet has some privacy related
features, serveral others involve encryption already).
There is lots of technical difference.  When was the last time you saw
your doctor use cryptlopes, watermarks etc to remind himself of his
obligations of privacy.
The point is that with privacy there is an explicit or implied
agreement between the parties about the handling of information.  The
agreement can not be technically *enforced* to any stringent degree.
However privacy policy aware applications can help the company avoid
unintentionally breaching it's own agreed policy.  Clearly if the
company is hostile they can write the information down off the screen
at absolute minimum.  Information fidelity is hardly a criteria with
private information such as health care records, so watermarks, copy
protect marks and the rest of the DRM schtick are hardly likely to
Privacy applications can be successful to the in helping companies
avoid accidental privacy policy breaches.  But DRM can not succeed
because they are inherently insecure.  You give the data and the keys
to millions of people some large proportion of whom are hostile to the
controls the keys are supposedly restricting.  Given the volume of
people, and lack of social stigma attached to wide-spread flouting of
copy protection restrictions, there are ample supply of people to
break any scheme hardware or software that has been developed so far,
and is likely to be developed or is constructible.
I think content providors can still make lots of money where the
convenience, and /or enhanced fidelity of obtaining bought copies
means that people would rather do that than obtain content on the net.
But I don't think DRM is significantly helping them and that they ware
wasting their money on it.  All current DRM systems aren't even a
speed bump on the way to unauthorised Net re-distribution of content.
Where the media cartels are being somewhat effective, and where we're
already starting to see evidence of the prediction I mentioned above
about DRM leading to a clash with privacy is in the area of
criminalization of reverse engineering, with Skylarov case, Ed
Felten's case etc.  Already a number of interesting breaks of DRM
systems are starting to be released anonymously.  As things heat up we
may start to see incentives for the users of file-sharing for
unauthorised re-distribution to also _use_ the software anonymsouly.
Really I think copyright protections as being exploited by media
cartels need to be substantially modified to reduce or remove the
existing protections rather than further restrictions and powers
awareded to the media cartels.

@_date: 2002-06-26 23:03:08
@_author: Adam Back 
@_subject: DRMs vs internet privacy (Re: Ross's TCPA paper) 
I don't mean that you would necessarily have to correlate your viewing
habits with your TrueName for DRM systems.  Though that is mostly
(exclusively?) the case for current deployed (or at least implemented
with a view of attempting commercial deployment) copy-mark
(fingerprint) systems, there are a number of approaches which have
been suggested, or could be used to have viewing privacy.
Brands credentials are one example of a technology that allows
trap-door privacy (privacy until you reveal more copies than you are
allowed to -- eg more than once for ecash).  Conceivably this could be
used with a somewhat online, or in combination with a tamper-resistant
observer chip in lieu of online copy-protection system to limit
someone for example to a limited number of viewings.
Another is the "public key fingerprinting" (public key copy-marking)
schemes by Birgit Pfitzmann and others.  This addresses the issue of
proof, such that the user of the marked-object and the verifier (eg a
court) of a claim of unauthorised copying can be assured that the
copy-marker did not frame the user.
Perhaps schemes which combine both aspects (viewer privacy and
avoidance of need to trust at face value claims of the copy-marker)
can be built and deployed.
(With the caveat that though they can be built, they are largely
irrelevant as they will no doubt also be easily removable, and anyway
do not prevent the copying of the marked object under the real or
feigned claim of theft from the user whose identity is marked in the
But anyway, my predictions about the impending collision between
privacy and the DRM and copy protection legislation power-grabs stems
from the relationship of privacy to the later redistrubtion
observation that:
1) clearly copy protection doesn't and can't a-priori prevent copying
and conversion into non-DRM formats (eg into MP3, DIVX)
2) once 1) happens, the media cartels have an interest to track
general file trading on the internet;
3) _but_ strong encryption and cryptographically enforced privacy mean
that the media cartels will ultimately be unsuccessful in this
4) _therefore_ they will try to outlaw privacy and impose escrow
identity and internet passports etc. and try to get cryptographically
assured privacy outlawed.  (Similar to the previous escrow on
encryption for media cartel interests instead of signals intelligence
special interests; but the media cartels are also a powerful
Also I note an slip in my earlier post [of Bear's post]:
Ross Anderson's comments were also right on the money (as always).

@_date: 2002-03-22 00:06:15
@_author: Adam Back 
@_subject: Secure peripheral cards 
I'm not sure NCipher gear is the  for acceleration, I think they're
probably more focussed and used for secure key management.  For
example they quote [1] an nForce can do up to 400 new SSL connections
per second.  So that's CRT RSA, not sure if 1024 bit or 512 bit (it
does say "up to").  openSSL on a PIII-633Mhz can do 265 512 bit CRT
RSA per second, or 50 1024 bit CRT RSA per second.  So wether it will
even speed up current entry-level systems depends on the correct
interpretation of the product sheet.  And the economics of course depends on how expensive they are relative
to general purpose CPUs, plus the added complexity of using embedded
hardware and drivers and getting to play with your web server.
General purpose CPUs are _really_ fast and cheap right now.
But for the application at hand -- secure key-management, perhaps an
NCipher card is ok -- I haven't compared feature sets so can't really
[1]

@_date: 2002-03-22 12:24:25
@_author: Adam Back 
@_subject: fast SSL accelerators (Re: Secure peripheral cards) 
While that is true, the issue is the economics; depending on the
figures it may be cheaper and much simpler to buy a faster pentium or
better yet an even faster and better value for money Athlon.  Even buy
a dual processor machine.
Cryptoapps seem to make a 2000 key per second clearly stated as 1024
bit (CRT) RSA for $1400 [1].  That might be harder to compete with
with Athlons as one of those PCI cards is around 13x faster than the
fastest i86 compatible processor you can buy right now.
Of course this is now straying off the original discussion of secure
hardware to and focussing on the fastest most economical way to do
lots of SSL connections per second rather than the most secure way to
store keys in hardware, so I changed the subject line.
[1]

@_date: 2002-03-25 12:14:49
@_author: Adam Back 
@_subject: RSA on general-purpose CPU's [was:RE: Secure peripheral cards] 
Hmm probably the question is how did whoever that compiled that binary
(not me) manage to compile it without bignum assembler.
Here's another binary, try this instead (also P3-633Mhz):
OpenSSL 0.9.5a 1 Apr 2000
built on: Wed Aug 30 14:46:28 EDT 2000
options:bn(32,32) rc4(ptr,int) des(ptr,risc1,16,long) blowfish(idx) compiler: gcc -DTHREADS -D_REENTRANT -fPIC -ggdb -DNO_IDEA -DNO_MDC2 -DNO_RC5 -DNO_MD2 -DL_ENDIAN -DTERMIO -O2 -march=i386 -mcpu=i686 -Wall -DSHA1_ASM -DMD5_ASM -DRMD160_ASM
                  sign    verify    sign/s verify/s
rsa  512 bits   0.0019s   0.0002s    523.8   5095.9
(It is -O2 and has debug)
So I think more in-line with your figures, and I suppose making the
point even more strongly than current processors are amazingly fast
and cheap; plus Lucky's figures on the IA64 show the trend continuing.

@_date: 2002-03-26 18:15:22
@_author: Adam Back 
@_subject: ciphersaber-2 human memorable test vectors 
A while ago I wrote some code to search for human readable test
vectors for Arnold Reinhold's ciphersaber-2
Ciphersaber-2 is designed to be simple enough to be implemented from
memory, to avoid the risk of being caught with crypto software on your
computer for use in regimes which have outlawed encryption.  But the
only available test vectors are a big string of hex digits which
probably the implementor will find difficult to remember, and it is
quite easy to make implementation mistakes implementing ciphers -- and
the risks of accidentally using a incorrectly implemented and weak
variant of ciphersaber-2 are significant.  It would be useful
therefore for the stated purpose of ciphersaber-2 to have human
readable and memorable test vectors.
The software for exploring for human readable test vector phrases and
information on using it is here:
        I have not myself spent much time looking for satisfyingly witty,
topical and memorable phrases. I'll appoint Arnold as the judge and
let you the reader see what you can come up with using the
software. The winning test-vector phrases will go on that
page. Perhaps Arnold will make some honorary 2nd level Cipher
Knighthood and certificate for producing the coolest phrase which is
also a ciphersaber-2 test vector.
By way of example the following is a ciphertext plaintext pair:
% csvec -c2 db 3 4 3 5 3 spook.lines
selected 155 words of length [3-5]
k="AMME",iv="spy fraud ": bce DES
which is interpreted as:
ciphertext = "spy fraud DES"
plaintext = "bce"
key = "AMME"
(the iv is prepended to the ciphertext)
and can be checked as: % echo -n "spy fraud DES" | cs2.pl -d -k="AMME"
and so on.
Anton Stiglic and I were also thinking that there would be other
ciphers which you could make human memorable test vectors for.  For
example DES and other 64-bit block ciphers seem plausible though the
searching would be slower as the plaintext would have to be 8 chars
which are slower due to the rate of the English language.
Anton had a number of ideas about how you could make test vectors for
other ciphers like SHA using a partial hash collision as the validity
test as computing a full collision would be impossibly expensive.
In general purely human readable test vectors are not ideal as they
are 7 bit, and there have been cases where implementation errors or
related to the 7th bit (for example one blowfish implementation had
problems with signd / unsigned chars), but it is kind of an
interesting though experiment.
Also if done by the cipher designer, he may choose any magic constants
for the cipher / hash function specifically to allow an human
memorable test vector, though this may weaken the usual kosherized
random number or generators and use of constants such as PI to assure
the reader that there is no ulterior motives for the vectors.
I suppose the general approach of trying lots of human readable
strings for one which has a desired property also calls into slight
doubt the use of purely text phrases as a kosherizing method (eg magic
constants are repeated SHA1 hash of some phrase) -- if in fact the
algorithm designers could try lots of phrases to arrive at a subtly
weakened set of magic numbers.

@_date: 2002-03-29 23:02:44
@_author: Adam Back 
@_subject: ciphersaber-2 human memorable test vectors 
That would work.
No guarantees about what the rest of the test vector would look like
if you choose an easily rememberable high order bit set as the key
(which was where the blowfish problem arose if I recall).  Or perhaps
you were suggesting ORing them into the otherwise 7 bit test vector
and having the additional constraint that the other outputs had the
same or equally memorable high bit pattern.  That would be plausible.
Any takers on ciphersaber-2 test vectors which are also topical and
amusing english phrases?

@_date: 2002-03-30 19:50:12
@_author: Adam Back 
@_subject: ciphersaber-2 human memorable test vectors 
The code on the web page makes that optimization.
Here's what it does: - from the word sets you feed it equal length word pairs are first
XORed and stored for fast lookup with the lookup key being the xor of
the word pair, and the value stored being a list of word pairs (you
get quite often multiple word pairs that xor to the same value)
- brute force by human readable key and iv meeting constraints given
by user
- first test if key output is 7 bit clean (xor of two 7 bit clean
values is 7 bit clean).
- if so lookup successive word lengths from the set of word lengths
the user requested in the pre-computed word-pair database
I use Dan Bernstein's amazingly fast and compact CDB (Constant
DataBase) to store the xor pairs in -- if you have enough RAM, or a
small word set the lookups will anyway be cached, but the CPU to
lookup ratio is such that it's fast enough.  (I don't try to keep the
CPU busy while waiting for disk, the disk isn't exactly buzzing even
with fairly short plaintext / ciphertext words -- if you cared about
that small improvement you could start a few clients in parallel or
fix the code).
Those seemed like the obvious speedups, perhaps there are others.  But
the current approach may be "fast enough", the frequency with which it
finds words goes down as you request longer plaintext - ciphertext
words due to the rate of English, but I presume will become more CPU
bound as a higher proportion of RC4 PRNG outputs will not be 7-bit
clean and so will be rejeced without before getting to the database
lookup for.

@_date: 2002-03-31 18:08:51
@_author: Adam Back 
@_subject: on the state of PGP compatibility (2nd try) 
[This is actually slightly more accurate and even worse than my first
mail which bounced to some of the lists as I had a typo, _and_
separately encountered a mail hub outage at cyberpass.net -- apologies
to those who get duplicates].
So I was trying to decrypt this stored mail sent to me by a GPG user,
and lo pgp6.x failed to decrypt it.
So I try an older gpg I had installed, and it fails because it doesn't
support RSA or IDEA, and this GPG user it seems installed the RSA and
IDEA patches.
So I go fetch GPG from  but it still doesn't contain
IDEA by default due to the anti-patent religion thing gone-to-far over
at GNU land, so I try to download the idea.c plugin -- except they
seem to have removed it (accidentally? hmm -- I fire off a mildly
brusque email to webmaster at gnupg.org -- and hit google.com but all
the first few hits are pointing at the gnupg.org faq with labyrinth of
links ending eventually in the same dud link.)
Either way _even_ if idea.c were still where they claimed it would be
it seems intentionally very well hidden, which I think is
unconscionable for a security product: to _intentionally_ frustrate
users attempts to interoperate with secure previous versions and
alternate implementations.
So then I try pgp5.x but the binary is using dynamic libraries that
are no longer on my shiny new redhat7.1 installation, so I try to
compile it but it no longer compiles.  Tinker briefly fixing up
things, but the errors are multiple, and I haven't got time for this.
So my last hope is pgp2.x, but some buggy pgp variant has left my
pgp2.x key ring empty, and you can't use it directly on pgp6.x
keyrings as it will barf on the new key formats.  So then I ponder
exporting my private key out of pgp6.5.8 which isn't going to do it
compatibly without some serious thought -- openPGP's salted key
stretching maybe being used -- do I want to export it without a
password (not really), so figure out how do I turn the salted key
stretching off, will pgp6.5.8 even let you export private keys, or is
it easier to just extract it with a binary editor?  Fortunately I
finally find a pgp2.x keyring secring.bak file (thanks PRZ), and move
it back and lo pgp2 can't decrypt it because of some unsupported
packetry.  I take a look at it with Mark Shoulsen's pgpacket and it
seems that _even_ pgpacket thinks there is some unsupported packets at
the end, so dig out another packet analysis program -- pgpdump by Kazu
Yamamoto and it doesn't seem to realise it's ascii armored or is
expecting to find an external program to de-armor which is missing --
I don't care, so use emacs and mmencode -u to produce a binary
version, and then it plays.  And there lies the problem: gpg encrypted
it with IDEA using the new openPGP streaming options to encode the
message in chunks despite it being encrypted with idea (presumably the
sender forget to invoke --rfc1991 not realising my potential future
predicament).  Thus sprach Kazu's analyzer:
New: Symmetrically Encrypted Data Packet(tag 9)(512 bytes) partial start
New:	(201 bytes) partial end
So, for now, give up.  I guess it's cheaper to just send the original
author an email ask him if he remembers that idea he sent me 4 months
ago and have him send me it in clear text to be sure!
What a nightmare!  Try that sequence on a novice user and they give up
before they get past the first GPG faq with rant about algorithm
We've really got to do something about the compatibility problems.

@_date: 2002-04-01 01:34:35
@_author: Adam Back 
@_subject: what is GPG's #1 objective: security or anti-patent stance ( Re: on the state of PGP compatibility (2nd try)) 
I've trimmed the Cc line a bit as this is now focussing more on GPG
and not adding any thing new technically for the excluded set.
I don't see how this is a useful distinction.  They are self-evidently
not close enough for practical purposes as evidenced by the fragmented
user base and ongoing problems you experience if you try using PGP.
Back in the days of pgp2.x I used to receive and send a fair
proportion of mail encrypted with pgp; these days it is a much lower
proportion, and a rather high proportion of those fail.  It's not like
I'm using old software or failing to try what is reasonable to get
messages to work.  Even with my fairly complete collection of PGP
versions you saw the results.  Imagine how much worse it will be
between people who do not upgrade frequently or take such defensive
strategies.  So you say upgrade already.  However as I think I have
demonstrated, I follow this strategy myself and as you can see it
doesn't work either.
I can't speak of PGP 7's behavior in this discussion as it is not
available for the operating system I primarily use (linux) as far as I
am aware.
I would characterise the situation as intentionally frustrating
attempts to use IDEA.  The whole point of the little exercise of
stripping out the idea.c, making it a separate dynamically loadable
module, tucking it away in a FAQ where you are pointed to lectures
about how software and algorithm patents are bad is _specifically, and
explicitly_ to discourage use of patented algorithms (and in this case
of the idea.c implementation) and to encourage people to do lobby
about the patent madness.
Campaigning against patent madness is a good cause in itself but not
when it gets in the way of privacy to the point that people are
sending messages in plaintext.  After all what is GPG's primary
purpose: is it an email security software package focussing on
security, or a platform for promulgating political views.  I view the
exclusion of idea.c from GPG as basically a security bug of higher
severity than for example PGP2.x's manipulable fingerprint, or
pgp5.something's (before it got fixed) unsigned ADK bug packet, or the
potential buffer overflow in ZLIB.  This bug is worse because it
reproducibly and frequently causes people to send mail in clear text.
The other bugs are by comparison less dangerous, yet they (the two
more recent ones) were fixed by NAI, and GPG and other PGP software
maintainers with rushed over-night hot fixes.
I would suggest this bug would be best fixed in GPG by:
a) including IDEA as a default option in GPG -- the ASCOM patent
license is really very liberal for non-commercial use, and b) if that goes against the GNU philosophy to the extent that it is
worth causing hinderance to hundreds of thousands of users who
practically are _going_ to want it they could at least make it a
configuration file option and ship it as other crypto libraries such
as openSSL do.  (I'm not sure how it hurts the anti-patent stance to
do this -- gnupg.org is after all _already_ distributing idea.c, just

@_date: 2002-10-17 19:15:38
@_author: Adam Back 
@_subject: palladium presentation - anyone going? 
Would someone at MIT / in Boston area like to go to this and send a
report to the list?  Might help clear up some of the currently
unexplained aspects about Palladium, such as:
- why they think it couldn't be used to protect software copyright (as
the subject of Lucky's patent)
- are there plans to move SCP functions into processor?  any relation
to Intel Lagrange
- isn't it quite weak as someone could send different information to
the SCP and processor, thereby being able to forge remote attestation
without having to tamper with the SCP; and hence being able to run
different TOR, observe trusted agents etc.
I notice at the bottom of the talk invite it says but in this case how does it meet the BORA prevention.  Is it BORA
prevention _presuming_ the local user is not interested to reconfigure
his own hardware?
Will it really make any significant difference to DRM enforcement
rates?  Wouldn't the subset of the file sharing community who produce
DVD rips still produce Pd DRM rips if the only protection is the
assumption that the user won't make simple hardware modifications.
-------- Original Message --------
Open to the Public
Time:     10:30 a.m.- 12:00 noon Place:    NOTE: NE43-518, 200 Tech Square Title:    Palladium
Speaker:  Brian LaMacchia, Microsoft Corp.
Hosts:    Ron Rivest and Hal Abelson
Abstract: This talk will present a technical overview of the Microsoft
"Palladium" Initiative.  The "Palladium" code name refers to a set of
hardware and software security features currently under development
for a future version of the Windows operating system.  "Palladium"
adds four categories of security services to today's PCs:
  a. Curtained memory. The ability to wall off and hide pages of main
memory so that each "Palladium" application can be assured that it is
not modified or observed by any other application or even the
operating system.
  b. Attestation. The ability for a piece of code to digitally sign
or otherwise attest to a piece of data and further assure the
signature recipient that the data was constructed by an unforgeable,
cryptographically identified software stack.
  c. Sealed storage. The ability to securely store information so
that a "Palladium" application or module can mandate that the
information be accessible only to itself or to a set of other trusted
components that can be identified in a cryptographically secure
  d. Secure input and output. A secure path from the keyboard and
mouse to "Palladium" applications, and a secure path from "Palladium"
applications to an identifiable region of the screen.
Together, these features provide a parallel execution environment to
the "traditional" kernel- and user-mode stacks.  The goal of
"Palladium" is to help protect software from software; that is, to
provide a set of features and services that a software application can
use to defend against malicious software also running on the machine
(viruses running in the main operating system, keyboard sniffers,
frame grabbers, etc).  "Palladium" is not designed to provide defenses
against hardware-based attacks that originate from someone in control
of the local machine.

@_date: 2002-10-21 22:52:20
@_author: Adam Back 
@_subject: palladium presentation - anyone going? 
It doesn't sound breakable in pure software for the user, so this
forces the user to use some hardware hacking.
They disclaimed explicitly in the talk announce that:
However I was interested to know exactly how easy it would be to
defeat with simple hardware modifications or reconfiguration.
You might ask why if there is no intent for Palladium to be secure
against the local user, then why would the design it so that the local
user has to use (simple) hardware attacks.  Could they not, instead of
just make these functions available with a user present test in the
same way that the TOR and SCP functions can be configured by the user
(but not by hostile software).
For example why not a local user present function to lie about TOR
hash to allow debugging (for example).
A "trusted bit" in the segment register doesn't make it particularly
hard to break if you have access to the hardware.
For example you could:
- replace your RAM with dual-ported video RAM (which can be read using
alternate equipment on the 2nd port).
- just keep RAM powered-up through a reboot so that you load a new TOR
which lets you read the RAM.
But how will the SCP know that the hash it reads comes from the
processor (as opposed to being forged by the user)?  Is there any
authenticated communication between the processor and the SCP?

@_date: 2002-10-22 06:09:32
@_author: Adam Back 
@_subject: Why is RMAC resistant to birthday attacks? 
I think they are presuming there will be no encryption, so Eve can
verify collisions by observing the MAC values.  Eve just records
messages and their MACs that Alice sends Bob.  They are also presuming
exceedingly long lived MAC keys.  (If you changed keys the collection
of messages would have to start over).  The optional salt ensures that
K3 (the key used to do the final encryption of the CBC-MAC computed
using K1) is different even if the same MAC keys are used
indefinately.  (K3 = K2 xor salt).
Note also in A.3 they are talking about a full collision rather than
just an equal MAC.  If the MAC is truncated (m So Eve wants to convince Bob that a message really is from

@_date: 2002-10-22 16:52:16
@_author: Adam Back 
@_subject: Palladium -- trivially weak in hw but "secure in software"?? (Re: palladium presentation - anyone going?) 
Remote attestation does indeed require Palladium to be secure against
the local user.  However my point is while they seem to have done a good job of
providing software security for the remote attestation function, it
seems at this point that hardware security is laughable.
So they disclaim in the talk announce that Palladium is not intended
to be secure against hardware attacks:
so one can't criticise the implementation of their threat model -- it
indeed isn't secure against hardware based attacks.
But I'm questioning the validity of the threat model as a realistic
and sensible balance of practical security defenses.
Providing almost no hardware defenses while going to extra-ordinary
efforts to provide top notch software defenses doesn't make sense if
the machine owner is a threat.
The remote attestation function clearly is defined from the view that
the owner is a threat.
Without specifics and some knowledge of hardware hacking we can't
quantify, but I suspect that hacking it would be pretty easy.  Perhaps
no soldering, $50 equipment and simple instructions anyone could
more inline below...
I think standard memory could be used.  I can think of simple
processor modifications that could fix this problem with hardware
tamper resistance assurance to the level of having to tamper with .13
micron processor.  The processor is something that could be epoxyied
inside a cartridge for example (with the cartridge design processor +
L2 cache housings as used by some Intel pentium class processors),
though probably having to tamper with a modern processor is plenty
hard enough to match software security given software complexity

@_date: 2002-10-22 22:53:33
@_author: Adam Back 
@_subject: comparing RMAC to AES+CBC-MAC or XCBC (Re: Why is RMAC resistant to birthday attacks?) 
But the salt doesn't increase the MAC length.  It just frustrates
attempts to collect message+MAC pairs to find a collision.  Think of
it like a salt on unix passwords.  You can still brute force one
particular target in the same time, but the salt reduces your scope
for pre-computation.
There is still probability 1/2^m of finding a collision given two
random messages, whether the salt has size 0 or 64.
Note that the salt is optional.  They list parameter sets I through V.
Parameter sets II through V are considered safe for general use.
Parameter set II has salt size 0.
Parameter set I is considered only safe for applications where only a
limited number of messages could be sent.  This is more a function of
the small MAC size (32 bits) I think than the fact that the salt size
is 0 for parameter set I.
I would have thought that unless the keys can essentially never change
(for example burnt into hardware) the salt option is of limited
practical use.
The choice of parameter sets is a bit odd.  For example there are no 0
size salts for MAC outputs over 64 bits, while there is for the
smaller MAC outputs, and yet you would think the smaller MAC outputs
are more in need of the salt as finding a collision is more
realistically achievable.  Collecting 2^64 messages (for parameter set
V) seems already quite theoretical for many applications without
adding a 128 bit salt.  Yet collecting 2^32 messages (parameter set
II) seems much more plausible and yet there is no salt defined at that
parameter set.  Given the definition of the parameter sets I suspect
people will interpret the standard as that they must use one of the
listed parameter sets and can't use their own.  At least most
implementations will tend to do that.  Would it not be simpler to just
do away with the salt and parameter sets and describe the collision
problem and note that minimally K2 should be changed (however the
application may decide to arrange this) frequently enough to avoid a
non-neglible risk of collisions being obtainable to attacker.
If the salt is removed / ignored, RMAC is essentially the same as
CBC-MAC but just defined for use with AES (rather than just DES), so
providing more security due to larger block size (and key size).
The one difference which is an incremental improvement over raw
CBC-MAC is that the final CBC-MAC a-like output is encrypted with the
2nd key K3.  (K3 defined as K2 xor salt, K2 an independent key).
However for example Rogaway and Black's XCBC is simpler, more
efficient (not requiring a key schedule for each salt-change) and
equally deals with variable length messages).
The protection against collisions is of limited practical value, and I
think better left out of the standard.

@_date: 2002-10-23 20:49:24
@_author: Adam Back 
@_subject: comparing RMAC to AES+CBC-MAC or XCBC (Re: Why is RMAC resistant to birthday attacks?) 
The problem with this one-size fits all approach is that for most
applications given the key size of AES, the extension forgery is
impractical.  It would be more flexible to specify RMAC as having an
optional salt, with the size determined by the implementer as
appropriate for their scenario.
So mostly no salt as the number of messages required under the same
key to stand a non-negligible chance of finding a collision would be
greater than that possibly exchanged in the life-time of the MAC key.
For longer lived key scenarios, the size of the salt would be chosen
to address the problem.
See for example Rogaway's arguments about limited value of defending
against extension forgery attacks in XCBC:
"No Added Resistance to Key-Search Attacks. While other CBC MAC
variants use additional keys to improve resistance to key-search
attacks, what is presented here does not. One can perform an
exhaustive key-search on the MAC presented just as efficiently as on
the underlying AES primitive. But this concern, quite appropriate for
DES, would seem to be moot for AES."
Given that RMAC's salt should be _optional_ on all MAC output sizes
(contrary to the parameter sets given in the RMAC draft), and the
choice of salt size should be up to the developer -- for example sizes
ranging from 0 to 128 bits in increments of 8 bits, so they can match
the defense to that which makes sense in the context they are
deploying it.

@_date: 2002-10-24 21:00:19
@_author: Adam Back 
@_subject: comparing RMAC to AES+CBC-MAC or XCBC (Re: Why is RMAC resistant to birthday attacks?) 
The pre-conditions you give are a little over restrictive, but yes
there are limitations due to the structure of XCBC.  However provided
the pre-conditions are met, and they don't seem that implausible to
occur, the extension forgery attacks are possible so I wouldn't say
RMAC is inherently resistant to extension forgery.
Yes.  Perhaps I/someone should submit my comment to them before the
deadline.  If RMAC parameter sets were interpreted strictly they would
be quite incovenient and inflexible for the protocol designer.

@_date: 2002-10-31 00:28:56
@_author: Adam Back 
@_subject: patent free(?) anonymous credential system pre-print 
Some comments on this paper comparing efficiency, and functionality
with Camenisch, Chaum, Brands.
- efficiency
The non-interactive cut and choose protocol results in quite big
messages in the issuing and showing protcols to attain good security.
The user who wishes to cheat must create n/2 false attributes, and n/2
true attributes.  (True attributes being the ones he will try to
convince the CA are encoded in all the attributes).  The user can in
an offline fashion keep trying different combinations of false and
true attributes until he finds one where the attributes selected for
disclosure during issuing are the n/2 true attributes.  Then in the
showing protocol he can show the n/2 false attributes.
But C(n,n/2) grows sub-exponentially and so the user has to for
example encode 132 blinded hashed attributes to provide assurance of
work factor of 2^128 to the CA.  (C(132,66) ~ 2^128).  Without looking
in detail at what must be sent I presume each the issuing message for
a single credential would be order of 10KB.  Similar for the showing
Computational efficiency is probably still better than Camenisch
credentials despite the number of attribute copies which must be
blinded and unblinded, but of course less efficient than Brands.
- functionality
The credentials have a relatively inefficient cut-and-choose based
issuing and showing protocol.  Brands has efficient issuing protocols
which support offline showing.  Chaum's basic offline credentials are
based on interactive cut-and-choose, but there is an efficient
variant [1].
As with Brands and Chaum's certificates if they are shown multiple
times they are linkable.  (Camenisch offers unlinkable multi-show but
they are quite inefficient).
The credentials can be replayed (as there is no credential private
key, a trace of a credential show offers no defense against replay).
Brands credentials have a private key so they can defend against this.
(Chaum's credentials have the same problem).
The credentials unavoidably leave the verifier with a transferable
signed trace of the transaction.  Brands credentials offer a
zero-knowledge option where the verifier can not transfer any
information about what he was shown.
The credentials support selective disclosure of attributes, but only
in a restricted sense.  Attributes can be disclosed with AND
connectives.  However other connectives (OR, +, -, negation, and
formulae) are not directly possible.  Brands supports all of these.
The credentials do not support lending deterence (there is no option
to have a secret associated with a credential that must necessarily be
revealed to lend the credential as with Brands).
The credentials are not suitable for offline use because they offer no
possibility for a secret (such as user identity, account number etc)
to be revealed if the user spends more times than allowed.
Most of these short-falls stem from the analogous short-falls in the
Wagner blinding method they are based on.  Of course (and the point of
the paper) the credentials do offer over the base Wagner credentials
(a restrictive) form of selective disclosure which the base
credentials do not.
On citations:
Brands discusses the salted hash form of selective disclosure in his
book [2], you might want to cite that.  He includes some related
earlier reference also.  I reinvented the same technique before being
aware of the Brands reference also -- it seems like an obvious
construction for a limited hashing based form of selective disclosure.
[1] Niels Ferguson, "Single Term Off-Line Coins", eurocrypt 93.
[2] Stefan Brands, "Rethinking Public Key Infrastructures and Digital
Certificates; Building in Privacy", MIT Press, Aug 2000
viz p27: "Another attempt to protect privacy is for the CA to
digitally sign (salted) oneway hashes of attributes, instead of (the
concatenation of) the attributes themselves. When transacting or
communicating with a verifier, the certificate holder can selectively
disclose only those attributes needed.  Lamport [244] proposed this
hashing construct in the context of one-time signatures."

@_date: 2002-09-17 22:05:36
@_author: Adam Back 
@_subject: but _is_ the pentium securely virtualizable? (Re: Cryptogram: Palladium Only for DRM) 
The OS can stop user processes inspecting each others address space.
Therefor a remote exploit in one piece of application software should
not result in a compromise of another piece of software.  (So an IE
bug should not allow the banking application to be broken.)  (Note
also that in practice with must current OSes converting gaining root
once given access to local processes is not that well guaranteed).
However the OS itself is a complex piece of software, and frequently
remote exploits are found in it and/or the device drivers it runs.  OS
exploits can freely ignore the protection between user applications,
reading your banking keys.
Even if a relatively secure OS is run (like some of the BSD variants),
the protection is not _that_ secure.  Vulnerabilities are found
periodically (albeit mostly by the OS developers rather than
externally -- as far as we know).  Plus also the user may be tricked
into running trojaned device drivers.
So one approach to improve this situation (protect the user from the
risks of trojaned device drivers and too large and complex to
realistically assure security of OSes) one could run the OS itself in
ring0 and a key store and TOR in ring-1 (the palladium approach). Some seem to be arguing that you don't need a ring-1.  But if you read
the paper Peter provided a reference for, they conclude that the
pentium architecture is not (efficiently) securely virtualizable.  The
problem area is the existance of sensitive but unprivileged
The fact that VMWare works just means they used some tricks to make it
practically virtualize some common OSes, not that it is no longer
possible to write malicious software to run as user or privileged
level inside the guest OS and have it escape the virtualization.
(It is potentially inefficently securely virtualizable using complete
software emulation, but this is highly inefficient).
(Anonymous can continue on cypherpunks if Perry chooses to censor his
further comments.)

@_date: 2003-04-18 22:12:30
@_author: Adam Back 
@_subject: the futility of DRM (Re: DMCA Crypto Software) 
So the problem I think higher level: the traitor tracing model is not
practically usable.  ie. To have traitor tracing, you have to
personalize the content by embeding identity in the watermark.  (The
idea is that the watermark should be hard to remove, even if the
content is readily obtainable in digital form.)
But the security of identity capture for such a low value service (a
few $ for a movie rental) limit what can economically be spent on
assuring identity.  Therefor to break the system, you don't even
bother renting enough different copies to overcome the traitor tracing
system; you simply obtain a movie in a fake identity, or pluasibly
deniably have someone "steal" your copy, or remotely compromise your
machine that is playing it.  And on a slightly different aspect of the picture, if you did consider
that the digital copy would be hard to obtain (tamper resistant
player, such as the DVD player model (without the software player
option)), then you don't need watermarks, all you need is a signed
identity of the movie renter's identity, and to make players have a
policy of not playing unsigned content.
But overall I think DRM is economically stupid, and that we are stuck
in a bad local optima for content distribution industry, which is both
bad for them, and bad for freedoms, and bad for the computer hardware
industry.  DRM generically _can not_ stop copying, because
watermarking doesn't work technically (traitor tracing past some low
threshold), and doesn't work economically (because you can't afford
good enough identity assurances to avoid plausibly deniable still
watermarked copies, or copies obtained with forged identity).  Also digital content encrypted out to the monitor, video card,
speakers encrypted to that output device with keys negotiated with the
content provider is also stupid.  It places a silly burden on
hardware, and won't stop copying.  High quality output devices,
together with high quality personal capture devices, plus the
existance of digital content inside the output devices mean that
content will be captured digitally and re-encoded, or simply undergo a
high quality D->A->D path.
On the economic side of economic models which make sense where one
includes the presumption that copying does exist, and can't be
stopped: we already have that model.  ie. The content distribution
industry can sell digital copies and compete with pirate distribution
channels (such as kazaa, etc) because:
- convenience -- if the price is reasonably low, and the rental model
is convenient, it is simply not worth people's time to find an
download copies
- quality -- original digital copies tend to be higher quality because
compared to pirate downloads because the downloads are typically
re-encode at lower bit rates to conserve bandwidth; - reliability -- also current generation file trading does not
generally deal with spoofing, where you end up with something other
than what you wanted, or you end up with a file full of 0s.  I'd think
this is not a limiting factor and will likely be fixed.
- branding, visibility -- if the content distributors work on better
placing in search engines, more visible brands, content available from
official sites etc., they compete again on convenience
I think the content industry could make more money if they lowered
prices, and improved convenience to compete on value for money and
convenience with the free quasi-illegal services.
Basically the only licensed, legitimate content distribution industry
move that I saw that tried to do this was movie88, which promptly got
attacked by the MPAA and lost their internet connectivity.  (Movie88
was a streaming movie rental business which rented one-view of
streamed video for $1-$2; at that price I think they would have
competed very well with kazaa et al on overall

@_date: 2003-04-23 21:40:07
@_author: Adam Back 
@_subject: DRM technology and policy 
I agree with Carsten's comments.
I consider there is no moral obligation to pay the author or
distributor.  There is essentially zero copying cost once content is
The fact that we have the current copyright and patent systems is just
a societal convention codified into current laws.  This is largely
historical accident, and many of the original justifications for
creating these systems have been lost, or changed such that it is no
longer clear these systems are in the public good.
In fact most of the recent expansions of copyright have been basically
laws to profit large corporations which they have succeeded in
obtaining not because of any moral consensus, but because they have
spent lots of money lobbying.
When you face mass civil disobedience and violation of a victimless
crime (considering the scale of software and mp3 trading, movies,
taping of shows etc., etc) it is clear that the end user feels no
moral obligation.  This suggests to me that the world is now a
different place with different schelling points (or natural sensible
defaults) surrounding information exchange, and that current copyright
systems should be phased out.
So the fact that this may change the economics of being a movie
company or record company is not an argument to fixing this problem.
I'm presuming that as long as people are interested in consuming those
things with whatever the legal framework those things will be
produced.  Current content distribution and production companies will
either adapt or lose to other companies.  Maybe they'll lose some of
their ability to charge huge margins over physical reproduction costs,
but that is a net good.
The main remaining and bad effect of copyright law is a government
subsidy of media conglomorates -- the government implements and
enforces laws that benefit large content distribution companies at the
expense of individual freedoms.

@_date: 2003-04-24 18:56:21
@_author: Adam Back 
@_subject: sw and films after copyright? (Re: DRM technology and policy) 
So I realise this is a somewhat controversial view.
And what Peter is discussing here I think is the balance between two
opposing public goods.  One the one hand (1) we have the current
copyright powergrab by the content distributors where individual
freedoms are losing (most seem to agree on this); on the other hand
(2) we have the issue Peter articulates that if the copyright scheme
were completely removed, perhaps some works people would like to view
copies of would no longer be made.
I acknowledge that this conflict exists.
I'd also invite others who agree that (1) is a problem to discuss
other changes to copyright law which might address that problem.
I think that movies, and music would still be created, because there
are existing business models which account for I think some
significant proportion of their revenue which still work without
- live performances of music
- cinema viewing of films before DVD release
(the latter is as easily enforced by trade secret, and private
contract as copyright law; I'm not sure what they currently rely on
but they seem fairly successful avoiding digital copies leaking prior
to DVD release.)
Software is another interesting case.  (Yes Derek, I work in the
crypto software industry, and I realise the irony that my fat salary
is arguably propped up by the current systems).
So would software production stop?  I'd argue not because companies
need software to work.  At minimum software written for a single
company under contract would still work.  Software supported by
support contracts etc., such as cygnus model, redhat etc would still
work.  Such models may end up having a more level playing field in
competing with proprietary software, as the government subsidy effect
I discussed would be gone, and I think this helps the proprietary
software industry more than the open source.
So while it's difficult to predict the overall picture in software and
content production and distribution after such a change, my view is
that in net it would be an improvement.  Freedoms would no longer be
eroded, a government subsidy would have been removed, and software has
enough complexity problems without propping up the proprietary model
with the government subsidy.

@_date: 2003-04-24 19:04:17
@_author: Adam Back 
@_subject: what moral obligation? (Re: DRM technology and policy) 
Why?  That sounds like charity at the point of a gun, not a moral
obligation.  Before copyright your view would have appeared strange.
Currently it seems on the face of it reasonable, but I argue only
because that's the current system and we're used to it.  It has no
moral grounding, and significant amoral effects in it's enforcement:
erosion of individual freedoms, students and so on being made an
example of, the Skylarov case.
If it's your favorite artist that is your problem.  You can sponsor
him, you can pay to attend his live performances, you can pay for
physical media with his content perferring to buy versions where he
gets a royalty vs content distributors content.
Crypto software; yes I'd probably continue doing it absent being paid
because it's what I enjoy; I've written a number of open source
However I think the argument is less black and white.  If it were
truly the case that the software industry would collapse, no more
movies would be producted, no more music, then that would be an
argument that the current copyright laws (or perhaps some less
draconian prior version of them) is a necessary evil.  However as I
described in response to Peter, I don't think these industries would
cease; they may look somewhat different, get different proportions of
their revenue from different business models; but I think the net
result would be positive.
As you agree that the current copyright expansion is bad, also I'd be
interested to hear other ideas for how to reform copyright.

@_date: 2003-04-29 23:36:21
@_author: Adam Back 
@_subject: [Lucrative-L] double spends, identity agnosticism, and Lucrative 
There are also existantial forgeries.
Ie choose random x, compute y = x^e mod n, now x looks like a
signature on y because y^d = x mod n; and when he verifies the
verifier will just do x^e and see that it is equal to y.
These may also look like valid coins to this code!
It's missing a step: the coin should have some structure.  So it can't
be a hash of a message chosen by the user but hashed by the signer
(the normal practical RSA signature) because the server can't see that
it or it would be linkable.
What digicash did I think is something like c = [x||h(x)].  Then you
can reject existential forgeries and unblinded coins because they
won't have the right form.
(If you look back to the post where I gave a summary of the math,
you'll see I included that step.)

@_date: 2003-08-27 22:05:07
@_author: Adam Back 
@_subject: traffix analysis 
I agree with anonymous summary of the state of the art wrt
cryptographic anonymity of interactive communications.
Ulf Moeller, Anton Stiglic, and I give some more details on the
attacks anonymous describes in this IH 2001 [1] paper:
which explores this in the context of ZKS Freedom Network, and Pipenet
presenting attacks on the Freedom Network, Onion Network, Crowds and
Pipenet which affect privacy and availability.
"Traffic Analysis Attacks and Trade-Offs in Anonymity Providing
Systems", IH 2001, Adam Back, Ulf Moeller, and Anton Stiglic.

@_date: 2003-08-28 17:02:35
@_author: Adam Back 
@_subject: traffic analysis 
I agree it doesn't prove anything directly.  However if your proposed
scheme falls to one or more of the traffic attacks we detail then that
conversely demonstrates that your scheme is also not ideally secure.
With reference to your previous post (which I had not read until now),
it's unclear on the datahaven.  You posit that it exists and is
trustworthy, but you seem to be working to a weaker threat-model than
we explored, namely your propose a user trust a single trusted entity.
We explored the more interesting case where the user can choose to
trust some set of nodes operated by different entities and the
objective is to design a system such that you still get good anonymity
as long as some k of n of the nodes are not rogue and hostile to your
Some of the attacks we examined discuss traffic analysis attacks
inside the anonymous network.
But some consider the anonymous network as a black box with perfect
properties (this model seems to be similar to yours.)  Of those the
attack where the user disrupts an input and observes disruption in the
output appear to work.  ie. say there are two users A and B browsing
the web via this idealised system; if I disrupt (DoS / crash etc) user
A's network connection and one of the browsing streams abruptly stops,
I have some statistical information suggesting that browsing stream
belonged to real user A.
Now this is not really a criticism of the anonymous network as such,
but a problem particular to browsing -- the system requires observable
events to happen on the internet as the information is coming from
computers outside of the anonymity system.
Ideas about how to combat these kinds of problems are:
- mimic functions - to have some agent continue the browsing when the
  user's connection is disrupted.  However the limitation here is that
  good user browsing mimic functions are likely hard.
- another is cacheing (ZKS Freedom did this) and this tends to help
  because some of the content is coming from the cache and so only
  observable to a rogue node that happens to be the exit (and
  cacheing) node.
- another is moving the content inside the anonymous network; ie
  trying to host the content in a p2p network that also provides
  anonymity.  For example freenet tries to do this kind of thing.
but overall I have not seen any anonymous system design to date that
comes close to providing interactive anonymity against a threat-model
of retaining security with k of n honest nodes with k < n (!)  (and
where n != 1) Even a single compromised node (eg the exit node) plus ability to
observe or remotely influence network behavior of target users seems
to break most systems.
I restrict that comment to system where the content is outside of the
anonymous network; systems like freenet where the content is inside
the system probably require a different threat model, because there
are a number of new threats still I think would be vulnerable to
similar attacks from hostile insiders (and here anyone can usually be
an insider as it is a p2p system).
New threats in a p2p context include:
1. attacker's ability to discover what content a given node is serving
2. attacker's ability to discover all nodes serving a given file
3. attacker's ability to damage file integrity 4. attacker's ability to flood the network with files (pure volume DoS)
5. attacker's ability to flood the network with bogus files and trick
   downloaders and p2p nodes into downloading and sharing the bogus
   files in place of genuine content
6. search term privacy
7. attacker's ability to flood the search mechanism
attack 1 particularly seems hard to defend against.
about the padding scheme:
this is vulnerable to insider attack because the padding is not
end-to-end if I read your description correctly.  Wei Dai has an
attack on that scheme which we describe in the paper and uses it to
argue for end-to-end padding.  (Note Pipenet is about internal
traffic, it does not propose external traffic, though presumably this
could be added at the cost of the discussed loss of security).
But in fact if I understand you are talking about a single
anonmity-providing node so you have to trust that node to terminate
the padding.
So I think the case is more that what you proposed could be secure
(modulo the problem of black-box correlation of disrupted input links
and distrupted output streams), but is a "trust-me" system, or at
least a "trust a single but chosen 3rd party" system whereas others
are probably thinking of a k of n trust target.
[1]

@_date: 2003-12-26 21:37:18
@_author: Adam Back 
@_subject: Microsoft publicly announces Penny Black PoW postage project 
I did work at Microsoft for about a year after leaving ZKS, but I quit
a month or so ago (working for another startup again).
But for accuracy while I was at Microsoft I was not part of the
microsoft research/academic team that worked on penny black, though I
did exchange a few emails related to that project and hashcash etc
with the researchers.
I thought the memory-bound approaches discussed on CAMRAM before were
along the lines of hash functions which chewed off artificially large
code foot-print as a way to impose the need for memory.  Arnold Reinhold's HEKS [1] (Hash Extended Key Stretcher) key stretching
algorithm is related also.  HEKS aims to make hardware attacks on key
stretching more costly: both by increasing the memory footprint
required to efficiently compute it, and by requiring operations that
are more expensive in silicon (32 bit multiplies, floating point is
another suggestion he makes).
The relationship to hashcash is you could simply use HEKS in place of
SHA1 to get the desired complexity and hence silicon cost increase.
"The main design goal of this algorithm is to make massively parallel
key search machines it as expensive as possible by requiring many
32-bit multiplies and large amounts of memory."
I think I also recall discussing with Peter Gutmann the idea of using
more complex hash functions (composed of existing hash functions for
security) to increase the cost of hardware attacks.
The innovation in the papers referred to by the Penny Black project
was the notion of building a cost function that was limited by memory
bandwidth rather CPU speed.  In otherwords unlike hashcash (which is
CPU bound and has minimal working memory or code footprint) or a
notional hashcash built on HEKS or other similar system (which is
supposed to take memory and generaly expensive operations to build in
silicon), the two candidate memory-bound functions are designed to be
computationally cheap but require a lot of random access memroy
utilization in a way which frustrates time-space trade-offs (to reduce
space consumption by using a faster CPU).  They then argue that this
is desirable because there is less discrepency in memory latency
between high end systems and low end systems than there is discrepency
in CPU power.
The 2nd memory [3] bound paper (by Dwork, Goldber and Naor) finds a
flaw in in the first memory-bound function paper (by Adabi, Burrows,
Manasse, and Wobber) which admits a time-space trade-off, proposes an
improved memory-bound function and also in the conclusion suggests
that memory bound functions may be more vulnerable to hardware attack
than computationally bound functions.  Their argument on that latter
point is that the hardware attack is an economic attack and it may be
that memory-bound functions are more vulnerable to hardware attack
because you could in their view build cheaper hardware more
effectively as the most basic 8-bit CPU with slow clock rate could
marshall enough fast memory to under-cut the cost of general purpose
CPUs by a larger margin than a custom hardware optimized
hashcash/computationally bound function.
I'm not sure if their conclusion is right, but I'm not really
qualified -- it's a complex silicon optimization / hardware
acceleration type question.
[1] [2] Abadi, Burrows, Manasse and Wobber "Moderately Hard, Memory-bound
Functions", Proceedings of the 10th Annual Network and Distributed
System Security Symposium, February 2003
[3] Dwork, Goldberg, and Naor, "On Memory-Bound Functions for Fighting
Spam", Proceedings of the 23rd Annual International Cryptology
Conference (CRYPTO 2003), August 2003.

@_date: 2003-12-28 13:29:27
@_author: Adam Back 
@_subject: Microsoft publicly announces Penny Black PoW postage project 
Oh yes forgot one comment:
One down-side of memory bound is that it is memory bound.  That is to
say it will be allocated some amount of memory, and this would be
chosen to be enough memory to that a high end machine should not have
that much cache so think multiple MB, maybe 8MB, 16MB or whatever.
(Not sure what is the max L2 cache on high end servers).
And what the algorithm will do is make random accesses to that memory
as fast as it can.
So effectively it will play badly with other applications -- tend to
increase likelihood of swapping, decrease memory available for other
applications etc.  You could think of the performance implications as
a bit like pulling 8MB of ram or whatever the chosen value is.
hashcash / computationally bound functions on the other hand have a
tiny footprint and CPU consumption by hashcash can be throttled to
avoid noticeable impact on other applications.

@_date: 2003-02-04 22:16:50
@_author: Adam Back 
@_subject: DRM with remote attestation (Re: A talk on Intellectual Property and National Defense) 
No that's not the way it would work.
There would be a secure remote attestation certified by the
endoresment key which is signed by the hw manufacturer and never
leaves the device.  Bound to this attestation would be a key exchange
which results the device negotiating a shared key with the music
server.  The music server keys would be sealed with keys derived from
your current software state (OS, BIOS etc).
Then you can boot anyway you like, online or offline, just if you ever
boot without the right state the TPM can't recompute the sealing keys
and so you can't access data sealed under that state.
(Personal comments only)

@_date: 2003-02-07 01:07:16
@_author: Adam Back 
@_subject: password based key-wrap (Re: The Crypto Gardening Guide and Planting Tips) 
Peter lists applied crypto problem in his "Crypto Gardening Guide" at:
One of the problems from the "Problems that Need Solving" section is:
I may not be fully understanding the problem spec: you want to encrypt
(wrap) a randomly generated key (a per message session key for
example) with a key derived from a password.
What would be wrong with using PBKDF2 (from PKCS  / RFC2898) as the
key derivation function to give you defense against dictionary attack.
(Allows choice of number of iterations to "stretch" the password,
allows a salt to frustrate precomputation.)
Why do you care about non-malleability of the key-wrap function?
If you do want non-malleability of th ekey-wrap function, isn't
encrypt and MAC a standard way to do this?
Then you would need two keys, and I presume it would make sense to
derive them (using KDF2 from IEEE P1363a) a start key:
sk = KDF2( password, salt, iterations )
ek = KDF( sk, specialization1 )
mk = KDF( sk, specialization2 )
and then AES in CBC mode with random IV encrypting with ek, with
appended HMAC with key mk.
That leaves the comment:
but in this case the attacker could take his pick with no significant
advantage of either method:
- brute force passwords to get sk, derive ek from sk, decrypt the
wrapped key and use some knowledge about the plaintext encrypted with
the wrapped key to tell if the write password was chosen; or
- brute force passwords to get sk, derive mk from sk, and see if the
MAC is valid MAC of the ciphertext (presuming encrypt and then MAC)
Or is the problem that the above ensemble is ad-hoc (though using
standardised constructs).  Or just that the ensemble is ad-hoc and so
everyone will be forced to re-invent minor variations of it, with
varying degrees of security.

@_date: 2003-01-10 00:51:13
@_author: Adam Back 
@_subject: DeCSS, crypto, law, and economics 
I hear that as new-zealand made it illegal to sell region restricted
DVD players there (as John mentioned in his mail), that units (any
brand) bought for the .nz market are all region players.  Shouldn't be
hard to get one mail order of your choice.  (Or so a .nz person told
me -- he's on the list).

@_date: 2003-01-21 17:07:58
@_author: Adam Back 
@_subject: deadbeef attack was choose low order RSA bits (Re: Key Pair Agreement?) 
One cheap way the low order 64 bits can be set is to set the low order
bits of p to the target bitset and the low order bits of q to ...00001
(63 0s and one 1 in binary), and then to increase the stride of
candidate values in the prime sieve to be eg 2^64.
This was the method used in the deadbeef attack on pgp 2.x keyids.
pgp 2.x uses the least significant 64 bits as a keyid, and being able
to easily generate keys with the same keyids can be a nuisance for
keyservers and databases and could possibly confuse a user who
incorrectly though they unique.
(The intended unique id value is the pgp fingerprint, but in 2.x this
had problems, see:

@_date: 2003-01-23 18:01:52
@_author: Adam Back 
@_subject: deadbeef attack was choose low order RSA bits (Re: Key Pair Agreement?) 
An alternate method which doesn't leave such an obvious pattern in the
private key would be to find a factorization of x the target string
other than using ...0001 and x, to use p' and q' being equal length
factors of x = p'.q'.  Or if there aren't any then equal length
factorizations of r||x where r is some number of random bits.

@_date: 2003-07-16 00:33:08
@_author: Adam Back 
@_subject: httpsy, SSH and eternal resource locator/WAX (Re: Announcing httpsy://, a YURL scheme) 
I'm not that familiar with SFS, but httpsy sounds quite related to
Anderson, Matyas and Peticolas' "eternal resource locator" [1], and
the WAX system they describe in that paper.  This scheme allows a
referer to embody in a URL they refer to authentciation information
about the contents of the text in the body of the page referred to
(either by SHA1 document hash, or by reference to a signing key the
publisher of the referred page may use to sign and update that page's
contents).  (WAX was also implemented in browsers if I remember from
earlier reading of that paper).
Their approach is more directly worried about the risks in pointing at
random stuff on the web, and have it change under you.  For example I
had a pointer to a python implementation of hashcash, and the domain
of the author's ISP got sold and now it's a porn site, so where people
were expecting some python library code they would have got bounced to
some porn site.
They were also worried about referring to specific vetted instances of
a _version_ of a web page.  (The application was refereed web pages
with medical reference information).
httpsy seems to content itself doing something similar but based
solely on the identity (akin to the signature only variant in WAX)
where there is no guarantee about the content of the referred page.
there is a use case where you get redirected from a book purchase site
(eg amazon.com) to a book review site and then back from the reviewer
to the book site.  And the claimed weakness with SSL is that a rogue
book reviewer site could redirect you to a different though also
certified site.  Additionally the use-case supposes that the attacker
had gone to the trouble of getting a cert for a similar domain name
(eg amaz0n.com (zero instead of o).
httpsy seems to claim that instead of showing you the hash of the
sites auth information (which Perry referred to), it will instead give
you the option to provide a pet name for that site.  (eg you put "BOOK
SITE" or "AMAZON" or whatever is mnemonic for you as an individual),
then if you get bounced to the wrong place you'll be suprised that
amaz0n.com is not listed by your pet name but is instead prompting you
to check the hash and supply your pet name.  (In a similar way to the
way SSH warns you if the host key changes for a site your connecting
to again, or if you accidentally connect to a similarly named but SSH
supporting site with a host key not already in SSH's known hosts file).
So the httpsy proposal is really quite similar to SSH, but with pet
Also I'm not sure what is special about pet names or introducers.  All
that will happen to my mind is that people will set up informative but
bogus meta-rating sites.  (Best bookseller "amaz0n.com" plus the rogue
amaz0n.com's auth data hash.)  And then again the user will end up
giving their credit card to the rogue site.  Different to the SSL
attack but I'm not sure it's overally cleanly solved this kind of
human semantic gap attack, or even necessarily improved the situation
over SSL.
One thing it does do, which is perhaps good, is avoid the central
trusted point.  (Imagine if SSH used verisign as a CA.  If you became
the target of some investigation and verisign (or one of the other 50
odd CA vendors) complied with the LEAs, they could trick someone into
SSHing into a honey pot instead of the real host they indended to
reach).  By using potentially out-of-band (emailed PGP signed host-key
or user-key) but sticky (via the known-hosts mechanism) SSH avoids
that particular central trust issue and also (which contributes
greatly to SSH's success if you ask me) this simplifies setup as you
don't need to pay money to verisign et al to setup a host for SSH
[1] "The eternal resource locator: an alternative means of
establishing trust on the World Wide Web", Ross Anderson, Vaclav
Matyas, Fabien Petitcolas, 3rd USENIX workshop on electronic commerce
Augst 1998

@_date: 2003-06-14 23:04:12
@_author: Adam Back 
@_subject: Session Fixation Vulnerability in Web Based Apps 
Well particularly the issue is your login URL should not accept an
existing session identifier supplied by the browser (what the author
of the session fixing paper calls "session adoption").  I would
presume that most people would naturally stomp an existing session
Another related issue I'd think would be some login URLs manualy
written or using programing environments may just skip do a redirect
to some application page without prompting the user to login there is
already a still valid session.  In this case the user is logged in as
the attacker, so the attacker doesn't learn the users account info.
Of course it may be that if the user then starts to use the attackers
session, the user may enter something that is private and the attacker
would get that.

@_date: 2003-06-16 03:29:30
@_author: Adam Back 
@_subject: Session Fixation Vulnerability in Web Based Apps 
I think he means higher level frameworks, web programming libraries,
toolkits, and web page builder stuff; not hooks into SSL sessions.
Not to say that a hook into an SSL session is not a good place to get
an application sessions identifier from -- it would be, presuming that
you can't trick a browser into adopting someone else's SSL session.
I wouldn't know one way or the other if these higher level frameworks
fall victim to the session adoption problem as I haven't used them;
but it seems plausible that there might exist some that do.  If this
were the case it would be quite bad as there would presumably be many
users of them who had relied on the security of the high-level
framework.  But I would be suprised if most or many of them did for
similar reasons to the reason people are expressing doubt that many
hand coded login pages would be affected: it seems like generally a
mistake natural login and session managing web programming idioms
would not lend themselves to.

@_date: 2003-03-03 02:04:43
@_author: Adam Back 
@_subject: NSA being used to influence UN votes on Iraq 
Why is US secret service eavesdropping and dirty tricks against UN
votes on Iraq news worthy?
Because it's an attempt to pervert the political process, and sabotage
the political representation of other UN member countries.
I'm sure it is a little more than delegations bothering to protect
their comms; there is plenty of room in physical bugs, black bag jobs,
political bribery, and even potentially individual blackmail whatever
crypto the delegates may be using.

@_date: 2003-05-04 19:08:18
@_author: Adam Back 
@_subject: pgp-in-50-lines-of-perl? (Re: The Pure Crypto Project's Hash Function) 
Actually I did at one point implement a PGP compatible signature
verifier using MD5 in 5 lines of perl, and RSA in perl/dc.
There is also a RSA key generator (including prime generation) by
Steve Reid in 13 lines.  I guess you could probably get a basic PGP
compatible implementation of encryptions, signature and keygen in one
page of perl/dc etc. ie 60 lines perhaps less.  I mean we have RSA in
2 lines, md5 in 8 lines, key gen in 13 lines; you'd need IDEA which no
one has implemented in compacted perl, but we have
2+8+13+IDEA+formatting IDEA has no s-boxes or magic values or
anything, so could probably be quite compact and perl is good at
Putting it together is an exercise for the reader with entirely too
much time on his hands :-)

@_date: 2003-05-06 04:06:30
@_author: Adam Back 
@_subject: pgp-in-50-lines-of-perl? (Re: The Pure Crypto Project's Hash Function) 
Another good source of parts (though not yet optimized for size) is
pgpacket which can parse pretty much any openPGP packets and tell you
about them: It also includes base64 encoded which is another required component.
I'm not sure I still have the PGP2.x compatible signature in n-lines
of code; I was talking with Dan Farmer who was thinking of literally
using it (maybe for satan signing I forget).
But I did start working on this once, and here are the parts (last
file modification Oct 2001, but probably older as they all have same
stamp, so probably correspond to transfer from old machine).  So this
is a bunch of not previously released small stuff:
Actually I forgot I had written an IDEA in perl -- I guess I never put
it on the web because it's PGP CFB style IDEA, and I never got around
to making a pure IDEA version.
This is PGP compatible IDEA in 7 lines (ie CFB mode including the 2
byte checkdigits and zero IV and 8 byte random plaintext prefix).
 -s0777
$n=($m=4**8)+1;sub M{$_[0]%=$m}sub N{$_[0]=(($z=($K[$o++]||$m)*($_[0]||$m))-$n*
int$z/$n)%$m}sub A{N$A;M$B+=$K[$o++];M$C+=$K[$o++];N$D}$_=unpack("B*",pack H32,
$k)x9; n52,pack"B*"x7,/(.{128}).{25}/g;sub E{($A,$B,$C,$D,$o)=unpack
;$C^=$b}1..8;A$B^=$C^=$B^=$C;pack n4,$A,$B,$C,$D}$_=<>;$d?(s/..(.{8})//,$i=$1):
% echo hello world > msg
% pgpidea -e -i=0123456701234567 -k=0123456789abcdef0123456789abcdef < msg > msg.idea
% pgpidea -d -k=0123456789abcdef0123456789abcdef < msg.idea
hello world
(the i is the random plaintext prefix mnemonic i for IV as it is
effectively an encrypted IV).
(Actually I'm finding if you specify the -i flag it doesn't decrypt --
must've introduced a bug when I was shortening last -- no time to
figure that out now).
And here is some relatively compacted PGP keyring database code which
reads a PGP2.x compatible keyring and outputs the public key and
private key:
 -s0777
sub o{open P,"$ENV{'PGPPATH'}/ pub;$h=o sec;$u=lc$u;$w=(
$u=~s/^0x//);sub p{$_=$_[0];s/.//;$t=(ord$&&60)>>2;$l=2**(ord$&&3)&7;s/.{$l}//
s;$l=unpack N,"\0"x(4-$l).$&;s/.{$l}//s;$_[0]=$_;$&}sub r{$_[0]=~s/..//s;$l=
int(.9+unpack(n,$&)/8);$_[0]=~s/.{$l}//s;unpack"H*",$&}sub k{$f=$_[0];while($f
){$_=p$f;if($t==6||$t==5){s/.{8}//s;$n=r$_;$pe=r$_;$r=$_;$_=p$f;$_=p$f while$t
==12;$/=substr$n,-8;if($v){print"0x$/"}}if($t==13){$v?print"\t$_\n":return if(
print "sec 0x$/ $_\n";
print "seckey: ",unpack("H*",$r),"\n";
print "pub 0x$/ $_\n";
the stuff at the end just demonstrates what the function achieves.
It can decrypt private keys, prompting for the password which it gives
to the (external) md5 in perl, and then decrypts the private key
parameters using the (external) pgpidea script above :-)
# lookup by keyID
pgpkey -u=0x01234567
# lookup by userID
pgpkey -u=adam at cypherspace.org
# display keys in the database
pgpkey -v
(actually I'm not sure if this was fully debugged, I don't recall; but
it can't be far off as the external components pgpidea and md5 are
there and working.)
I did once have some compacted base64 code based on uuencode mode of
perl's pack function, think I lost it but it should be easy to
recreate.  (Uuencode is the same as base64 but with a different
alphabet; the trick is to use pack("u*"). and then y/// to transform
the differences).
btw If anyone is interested to work on this I have more commented
versions which I could share or get nudged into putting on the web.
btw Ralf recently sent me some improvements to the rc4 in perl, and
then I got tinkering and found another 32 bytes so it's now quite a
lot smaller, if that motivates anyone to find further improvements (I
ran out of ideas for now):
 -0777
 hex,pop=~/../g;
255,$w]for at s=0..255;
255],print chr($_^$s[$s[$x]+$s[$y]&255])for unpack"C*",<>
the previous version was:
 -0777
&Sprint chr($_^=$s[($s[$x]+$s[$y])%256])}sub S{

@_date: 2003-05-09 03:40:24
@_author: Adam Back 
@_subject: A Trial Balloon to Ban Email? 
Yes, there is some discussion of it on slashdot, including several
other people who have commented similarly to anonymous that it is a
pretty big privacy invasion and centralised control point problem.
The claim that you can optionally be anonymous and not use a cert, or
get an anonymous cert is plainly practically bogus.  You'd stand about
as much chance of having your mail read as if you shared mail hub with
spamford wallace -- ie 90+% of internet mail infrastructure would drop
your mail on the floor on the presumption it was spam.
Plus a point I made in that thread is that it is often not in the
internet user's interests to non-repudiably sign every message they
send just to be able to send mail because that lends amunition to
hostile recipients who from time-to-time target internet users for
bullshit libel and unauthorised investment advice etc. Companies also are I would expect somewhat sensitive to not signing
everything for similar reasons as those behind their retention
policies where they have policies of deleteing emails, files and
shredding paper files after some period.
In addition PKIs because of the infrastructure requirements have
probem complex to setup and administer.  So now we've taken one hard
problem (stopping spam) and added another hard problem (hierarchical
PKI deployment) and somehow this is supposed to be effective at
stopping spam.
In addition unless there is significant financial cost for
certificates and/or signifcant and enforceable financial penalty and
good identification and registration procedures enforced by the CAs it
wouldn't even slow spammers who would just get a cert, spam, get
revoked, get another cert and repeat.
Certificate revocation is already a weak point of PKI technology, and
to reasonably stop spam before the spammer manages to send too many
millions of spams with a cert, you have to revoke the cert PDQ!
And finally it all ends up being no more than an expensive
implementation of blacklists (or I suppose more properly whitelists),
because the CAs are maintaining lists of people who have not yet been
revoked as spammers.  Some click through agreement isn't going to stop
spammers.  Legislation or legal or financial threat is going to stop
spammers either because any level of registration time identity
verification that is plausibly going to be accepted by users, and this
is also limited by the cost -- higher assurance is more cost which
users also won't be willing to accept -- will be too easy for the
spammers to fake.  And email is international and laws are not.
It is pretty much an "internet drivers license" for email.
I also think that fully distributed systems such as hashcash are more
suitable for a global internet service.  My preferred method for
deploying hashcash is as a token exempting it's sender from bayesian
filtering, and any other content based or sender based filtering.
That way as an email user you have an incentive to install a hashcash
plugin  because it will ensure
your mail does not get deleted by ever-more aggressive filtering and
scattergun blackhole systems.  The camram system
 is a variant of this.
It also more directly addresses the problem: it makes it more
expensive for spammers to send the volumes of mail they need to to
break even.

@_date: 2003-05-12 21:45:57
@_author: Adam Back 
@_subject: economics of spam (Re: A Trial Balloon to Ban Email?) 
Bear discussed using hashcash-alike tokens as a challenge response
from the filtering MTA back to the sender giving the sender a chance
to compute a hashcash token.
This approach has the problem you identify -- namely that email is
store and forward; email can and often does go through multiple MTAs
on it's path to delivery, and the MTA doing the filtering may be
multiple hops from the sender.  Indeed sometimes the filterer is the
end-user who is also intermittently connected.
It's more convenient and fits better in the store-and-forward setting
if all email already includes the token at time of sending.  If it
turns out to be needed, then there is no interactive challenge-response needed.
Then the question is whether computing the token at sending time would
be incovenient for the normal sender.  This depends on what parameters
you choose.  A few seconds probably wouldn't be noticed, especially as
with deep MUA integration the token can be computed on each recipient
address as soon as it is selected for receipt.  Depending on MUA usage
therefore the token could be computed while the sender is composing
the message.
In addition it is expected that there would be a mechanism whereby
regular correspondents would white list each other.  (Probably
automatically via their mail clients).
Whether you think a few seconds is sufficient depends on your views of
the economics of spamming.  Ie how close to losing break-even the
spammers are, and whether a few seconds of CPU per message is enough
to significantly increase the cost.  This article for example
discusses the economics of spam:
they give an example of a spam campaign with a 0.0023% response rate,
and a yeild of $19 per response.  They estimate the cost of sending
the spam was less than 0.01c per message.  I've seen significantly
lower estimates for the sending costs.  To deter a given spam campaign
we just have to increase the cost to the point of making it
unprofitable given the response rate and profit per responder.  The
other side of this equation is what a second of CPU costs in monetary
terms to a spammer.  (To an end user it is essentially free because
his CPU is mostly idle anyway; the limiting factor for the user is his
preference for fast mail delivery (and in the dialup case an
unwillingness to sit waiting for tokens to be calcluated before his
mail can be sent).

@_date: 2003-05-13 20:08:54
@_author: Adam Back 
@_subject: Payments as an answer to spam 
In the case of micromint I agree, there still has to be a centralised
mint, so one can reasonably make a direct economic comparison with
blind signature based coins or whatever and do the cost benefit
analysis and decide which scheme to use (micromint may be cheaper per
coin after the large initial investment has been recovered); however
with hashcash it's major advantage is that it doesn't require any
Having a centralised e-cash bank issuing coins (or a group of banks
with inter-bank clearing) is a highly non-trivial task when you're
talking about micropayments that are expected to be attached to every
email.  The volume alone is staggering.  And it's not clear what the
minimum transaction cost for the system would be where the bank could
remain profitable after paying infrastructure costs, staff costs to
maintain security of their bank private keys -- this minimum
translates into a real monetary cost to the sender.  With hashcash,
there is effectively no monetary cost to the normal sender; for the
spammer there is, but I think economists might agree that having
spammers sitting in the corner burning dollar bills to be able to spam
is a net good if it is more efficient overall in saved human
resources, or perhaps feasible where real ecash may not be in the
sense of the profitability of it.  (Hashcash doesn't need a business
model where a bank has to turn a profit because there is no bank, nor
private keys to secure etc).

@_date: 2003-05-13 20:50:17
@_author: Adam Back 
@_subject: economics of spam (Re: A Trial Balloon to Ban Email?) 
To respond on the comments on costs of spamming and costs of CPU, the
figures one can draw from various papers and articles are highly
variable, one suspects they are variously including operator time,
electricity, spam software purchase, and email address list purchase.
To bring it back to just the raw computational costs (equipment
amortized plus electricity) lets do some rough estimates for this.
To take Tim's estimate $500 machine amortized over 2 years seems
entirely reasonable, say this machine has a 1Ghz CPU.  I'll add ADSL
line $500/year for a 1Mbit uplink, and say $200/year in electricity
for a total of $950/year.  For spamming without hashcash let's say
that it can send customized mail messages of size 1KB each, and by
pipelining it manages to max the link and send 64 messages/second.
(Divide by 2 to account for unreachable addresses, etc).
I make that 0.00005c / message.  Presuming the same machine is mostly
unloaded, and the spammer wants to send the same number of mails he
needs a bank of 63 additional CPUs each at a cost of 450/year
(amortized cost+electricity) for a total of $29300/year, so now his
spamming costs 0.0015 / message, and the purely computational costs
have increased by a factor of 30.  On could imagine this would reduce
the amount of untargetted spam a lot.  Clearly you will still receive
spam, just less of it, or more targetted to be likely to interest you
Other issues include that perhaps the spammer can get bandwidth
cheaper per Mbit if he needs more than 1Mbit, which would tend to
reduce purely computational cost of spamming (without tokens).
A 1 second CPU cost on a 1Ghz machine should be negligible and
acceptable to an email user even if the computation happens while he
waits after he clicks the send button.  If he is on a DSL or similar
it could be backgrounded.  On dialup delivery is slow anyway and a
second probably wouldn't be noticed.  Dialup users also often batch
their mail sending (deliver later from a local MUA maintained queue).
An additional cost for spammers is acquiring the email lists.  However
this cost can be amortized across multiple spamming campaigns on
behalf of different spam clients, and mostly seems to consist of
emails gathered from a web spider if one takes the claims of the CDT
spam report, so is itself just a bandwidth cost.
We could probably as was previously noted get away with a marginally
larger delay if tokens are only required to recipients who have never
replied to us in the past.
If one accepts these figures, at 1 second CPU per sent mail for new
recipients, perhaps it may even be economical for ISPs to do the
computation as part of mail service.
If we could think of a distributed way to precompute the token and yet
still have distributed verification without infrastructure, we could
increase the cost to 5 mins without normal users noticing.  It is not
obvious how one would do this however as unless the entire computation
is tailored to the recipient, parts of the computation could be
re-used across multiple recipients.  As Tim notes' Moore's law requires that we increase the collision cost
over time.  (But this is not so hard to do -- I can think of a simple
fully scalable mechanisms to achieve this slowly increasing
distribution of a minimum bit collision).
The possibility for accelerator hardware is definitely a limiting
factor.  Counter-measures to this which have been suggested include
(a) changing the algorithm over time with an authenticated code update
mechanism; (b) defining a cost function which makes use of features of
general purpose computers -- eg. IEEE floating point hardware, memory,
cache, larger code footprint algorithm etc.  This could in theory mean
that absent sufficient market general purpose CPUs remain the most
cost effective approach; (c) memory bound functions such as [1] which
are limited by memory latency rather than CPU speed.  Memory bound
functions have their own economic arguments (see conclusions section
of the paper): perhaps accelerator hardware is also a problem because
all you need is a memory chip, plus a really cheap CPU; they mean the
most cost effective hardware to buy is the cheapest CPU and so perhaps
2 or 3 times cheaper than best Mhz/$; plus they intentionally consume
memory data footprint which can interfere with applications.
Another possibility with accelerator hardware; if ISPs were the
primary deployers, then they are better positioned to buy accelerator
hardware to compete head on with spammers.
[1] C. Dwork, A. Goldberg, and M. Naor, "On Memory-Bound Functions for
Fighting Spam", Proceedings of CRYPTO 2003, to appear.

@_date: 2003-05-15 06:41:32
@_author: Adam Back 
@_subject: deterring coin re-use with offline coins (Re: A Trial Balloon to Ban Email?) 
[...spammer sends 170k mails all with same micropayment coin...]
So I'm not sure if you'd want to do it, and it has other issues
discussed on cpunks recently, but there are some other options here
with ecash that can avoid the bank having to say "already spent"
169,999 times for each valid but already spent coin.  (I concur with
Sunder that if the bank had to fit such usage patterns into their
business model, it would increase ther costs significantly which would
make running the bank even harder to do and still turn a profit,
especially as we are talking very high volume, and exceedingly low
value tokens.)
One assumption I'm making is presumably the micropayment system
provides the option for payer and payee anonymity, or email privacy
just got removed once and for all.  (Trace the payments at the bank
and you know who emailed who in a convenient central database - a
definite privacy no-no).
So with the offline brands protocol of which there was some discussion
recently, the MTA could verify the coin locally.  It would be assured
that if the coin was locally verified as valid, he either gets the
money later when he deposits, or the bank gets the spammers identity
and prosecutes them for payment fraud.
So (and this is why I said I don't know if you would want to do
this...) this payment choice where identity is revealed iff you
double-spend has the recently discussed issue:
A) you have to provide your identity in the first place, and if having
it revealed is any deterrent, you'd better be identified robustly
(doing this identification for every email user on the planet seems a
somewhat daunting task)
B) the spammer will have an incentive to find a way to provide fake
identity to the bank, or of buying someone else's identification
(eg. someone with no credit rating, or of stealing someone else's
tokens, or stealing someone else's mail services which automatically
add a payment (identifying them) on event of double spend
But aside from those issues (plus the showstopper issue of building a
payment infrastructure to support this volume in the first place which
was discussed earlier in this thread) this now gives the MTA the
ability to reject double-spends locally -- modulo the amount of
deterrent to double-spending anyway and being identified ends up
providing after the spammers have finished attacking issue B).
A remaining technical issue would be the MTA could have it's CPU
overloaded as verifying such tokens is while relatively cheap (I think
around DSA signature verification cost) still much more expensive than
it is for the DoS spammer to send you plausibly formatted random
numbers to burn off your CPU.  But we have a separate solution to
that: you make the spammer provide a hashcash token of comparable cost
to that verification and this can be verified an one order of
magnitude or more efficiently and increases the would-be DoSers costs
to be comparable to the signature verification.  (Or more if you wish

@_date: 2003-05-15 09:56:17
@_author: Adam Back 
@_subject: using PoW + filters to avoid false positives (Re: Re: A Trial Balloon to Ban Email?) 
The short term usefulness of a hashcash / PoW filter when used with
bayesian filters (which I think is what Joseph is saying below) is
that you are less likely to accidentally lose mail due to Bayesian
filters.  Ideally blackhole lists should also be exempted if there is
hashcash (they are another big source of loss of email, I've been hit
by that a number of times).
I suspect increasngly more email will be lost to filters and blackhole
lists because the anti-spam people are becoming increasingly gung-ho
and sweeping in their blackholing and filtering because the problem is
accelerating out of control, so the short term function of hashcash to
improve email reliability could be a useful extra function.
(Estimates vary but at ASRG kick off at IETF there were some very high
per month growth figures (10% and higher per month) for spam which
were far in excess of (non-spam) email growth).
Similarly your incentive to send hashcash in the short term is to
avoid your own mail similarly being swallowed by blackholes and
Bayesian filtering false positives.
The limitation with blackholes is it depends on the blackhole
implementation, some are simply refusing the TCP connection at
firewall level; others are accepting but giving you a 500 (or whatever
it is) response code explaining why -- but that is already too early
for them to have read the X-Hashcash headder.  One way around that is
to include hashcash as an ESMTP address parameter which I understand
allows you to say things after the RCPT TO, but even that may be too
late (if they already said go away after the HELO).
Another approach but only longer term and it is debatably too
aggressive/draconian, and in the short term has same problem as TCP
rejection of blackholed IPs would be integration of hashcash into TCP
like syncookie (see section 4.2 hashcash cookies of [1]) so that the
mailer can reject port 25 connections which don't have hashcash
tokens.  Or perhaps (less aggressively) to use a getsocketops or ioctl
to read from the socket whether the sender is using hashcash or not.
One problem with this approach is the PoW received by the MTA may not
be convincing to the recipient, so there remains risk that the
recipient could be spammed by a colluding or host compromised MTA at
their ISP.  (You could add envelope recipient emails to the puzzle,
but that's sufficiently SMTP related you'd just as well send it in
SMTP).  Another integration point could be IPSEC.
On the interactive connection DoS hardening side, there was a paper
about using Juel's and Brainard's Client Puzzles [2] (which is a known
solution puzzle where the server has to issue the challenge
interactively) for SSL DoS hardening [3].
More recently, though I haven't obtained a copy yet, Xiaofeng Wang and
Michael Reiter have a paper about an implementation hardening the
linux kernel TCP stack against DoS using puzzles [4], I'm presuming
this is similar to the hashcash-cookie approach from the abstract,
though I'm not sure which puzzle they used.  (Not sure what the puzzle
auction mechanism is).
[1] Aug 02 - "Hashcash - A Denial of Service Counter Measure" (5 years
on), Tech Report, Adam Back
[2] Ari Juels and John Brainard. Client puzzles: A cryptographic
countermeasure against connection depletion attacks. In Network and
Distributed System Security Symposium, 1999. Also available as
[3] Drew Dean and Adam Stubblefield. Using cleint puzzles to protect
tls. In Proceedings of the 10th USENIX Security Symposium, Aug
2001. Also available as [4] XiaoFeng Wang and Michael Reiter, "Defending Against
Denial-of-Service Attacks with Puzzle Auctions", IEEE Symposium on
Security and Privacy 2003

@_date: 2003-05-16 18:33:49
@_author: Adam Back 
@_subject: cryptographic whitelist tokens (Re: economics of spam) 
CAMRAM ( uses signatures to indicate white
listing.  (Each mail sent with hashcash is also sent with a signature
public key, and if the recipient chooses to respond (I think) then
they accept the signature public key and will accept future messages
for that user signed with that key.)
So while signing mails is generally bad in my view (for privacy and
unintended non-repudiation reasons), you don't have to sign the whole
mail only the sender, recipient pair.
I also suggested to CAMRAM using a MAC instead of a signatures, where
users introduce themselves to each other using hashcash, and include a
whitelisting MAC on the sender recipient pair, if the recipient
chooses to reply he need only include that token.
One advantage of signatures is that they can be safey verified by 3rd
parties (such as MTAs) just by storing the signature keys; where as
with MACs the MTA would have to store the whole MAC as it passes (or
the user has to give his MAC key to the MTA, and risk that the MTA can
forge white list tokens for him).
The MAC-based whitelist could also be implemented with a key held by
the MTA purely at the MTA based on users exchanging emails.
I would argue MACs are simple, much faster to verify, and safely
implemented at the verification point by having the verification point
own the key (verification point being either MTA or recipient).
So you comment So on the deployment side: bear in mind that what ever cryptographic
token you use to implement white lists in this scheme (CAMRAM, or
other), it has the same deployment vector as hashcash, and so has
exactly the same chance of deployment that it does.

@_date: 2003-11-12 15:57:42
@_author: Adam Back 
@_subject: Protection against offline dictionary attack on static files 
Yes this is a good idea, and some people thought of it before also.  Look for paper "secure applications of low entropy keys" or something
like that by Schnieir, Wagner et al.  (on counterpane labs page I
Also the PBKDF2 function defined in PKCS used to convert the
password into a key for unwrapping PKCS uses the same idea.  The
general approach is called "key-stretching".
The approach usually involves some form of iterative hashing so
similar to what you proposed.

@_date: 2003-10-01 14:21:31
@_author: Adam Back 
@_subject: how simple is SSL? (Re: Monoculture) 
eh well _now_ we can say that negotiation isn't a problem, but I don't
think we can say it doesn't add complexity: but in the process of
getting to SSLv3 we had un-MACed and hence MITM tamperable
ciphersuites preferences (v1), and then version roll-back attack (v2).
Maybe but X.509 certificates, ASN.1 and X.500 naming, ASN.1 string
types ambiguities inherited from PKIX specs are hardly what one could
reasonably calls simple.  There was no reason SSL couldn't have used
for example SSH key formats or something that is simple.  If one reads
the SSL rfcs it's relatively clear what the formats are the state
stuff is a little funky, but ok, and then there's a big call out to a
for-pay ITU standard which references half a dozen other for-pay ITU
standards.  Hardly compatible with IETF doctrines on open standards
you would think (though this is a side-track).
I don't think I agree with this assertion.  It may be relatively
simple if you want X.509 compatibility, and if you want ability to
negotiate ciphers.

@_date: 2003-09-06 18:11:20
@_author: Adam Back 
@_subject: cryptographic ergodic sequence generators? 
You might also look at RC5-16.  RC5 is defined on 64, 32, 16 and 8 bit
words with respectively 128, 64, 32 and 16 bit block sizes.
Using counter-mode as suggested by someone earlier in the thread would
be the obvious way to get a sequence with a period of 2^n.
The Yarrow RNG uses counter-mode as a PRNG.  However in the paper they
describe some effects you may want to avoid by re-keying depending on
your application as the stream becomes distinguishable from random

@_date: 2003-09-24 15:33:56
@_author: Adam Back 
@_subject: why are CAs charging so much for certs anyway? (Re: End of the line for Ireland's dotcom star) 
Saw this on theregister.co.uk: geotrust is undercutting veri$ign at
$159/cert vs $350/cert by verisign and $199 by Thawte (which as you
note is just a verisign brand at this point).
You'd have thought there would be plenty of scope for certs to be sold
for a couple of $ / year.  Eg. by one of the registrars bundling a
cert with your domain registration.  I mean if someone can provide DNS
service for $10 or less / year (and lower for some tlds) which
requires servers to answer queries etc., surely they can send a you a
few more bits (all they have to do is make sure they send the cert to
the person who they register the domain for).
browser DBs for free just for the asking by being in the right place
at the right time.  So once you have that charging > $100 for a few
seconds of CPU time to sign a cert is a license to print money.
With all the .com crashes you'd think the price of a root cert ought
to be pretty low by now.

@_date: 2003-09-24 19:18:06
@_author: Adam Back 
@_subject: why are CAs charging so much for certs anyway? (Re: End of the line for Ireland's dotcom star) 
Uh?  The "why" argument you give is basically "price gouging"?
That was my point and why I said I don't see any reason cert prices
with reasonable competition couldn't fall to a few dollars/year.
(Ian: recurring billing is because they expire).

@_date: 2003-09-26 15:28:13
@_author: Adam Back 
@_subject: efficiency?? vs security with symmetric crypto? (Re: Tinc's response to "Linux's answer to MS-PPTP") 
What conceivable trade-offs could you have to make to get acceptable
performance out of symmetric crypto encrypted+authenticated tunnel?
All ciphers you should be using are like 50MB/sec on a 1Ghz machine!!
If you look at eg cebolla (more anonymity than VPN, but it's a nested
forward-secret VPN related thing) it's even possible to do pretty
immediate forward secrecy every second or something at minimal CPU
cost.  (I'll read the writeup but that trade-off argument sounds very

@_date: 2004-08-18 16:33:12
@_author: Adam Back 
@_subject: hash attacks and hashcash (SHA1 partial preimage of 0^160) 
(This discussion from hashcash list is Cc'd to cryptography and
Hashcash uses SHA1 and computes a partial pre-image of the all 0bit
string (0^160).
Following is a discussion of what the recent results from Joux, Wang
et al, and Biham et al on SHA0, MD5, SHA1 etc might imply for hashcash
SHA1 (and for hypothetical hashcash SHA0, MD5 etc by way of seeing
what it will mean if SHA1 eventually suffers similar fate to SHA0).
(All as far as I understand so far).
Hashcash stresses the SHA1 function in a different direction than
sigantures and MACs -- in assuming partial pre-images are hard (ie an
k-bit partial pre-image should take about 2^k operations).  (Partial
2nd pre-images are also "interesting" against hashcash -- see below).
(As a security argument if partial pre-images say up to m To be clear:

@_date: 2004-08-21 01:39:12
@_author: Adam Back 
@_subject: RPOW - Reusable Proofs of Work 
It's like an online ecash system.  Each recipient sends the RPOW back
to the mint that issued it to ask if it has been double spent before
accepting it as valid.  If it's valid (not double spent) the RPOW
server sends back a new RPOW for the receiving server to reuse.
Very like Chaum's online ecash protocol, but with no blinding (for
patent reasons) and using hashcash as way to "buy" coins.  The other
wrinkle is he can prove the mint can not issue coins without
exchanging them for hashcash or previous issued coins (up to the
limits of the effectiveness of the IBM tamper resistant processor
card, and of course up to the limits of your trust in IBM not to sign
"hardware code signing keys" that are not generated on board one of
these cards).  This is the same as the "remote attestation" feature
used in "Trustworthy" Computing for opposite effect -- restricting
what users can do with their computers; Hal is instead using this to
have a verifiable server where the user can effectively audit and
check what code it is running.

@_date: 2004-08-26 21:05:10
@_author: Adam Back 
@_subject: finding key pairs with colliding fingerprints (Re: How thorough are the hash breaks, anyway?) 
You would have to either:
- search for candidate collisions amongst public keys you know the
private key for (bit more expensive)
- factorize the public key after you found a collision
the 2nd one isn't as hard as it sounds because the public key would be
essentially random and have non-negligible chance of finding trivial
factors.  (Not a secure backdoor, but still create a pretty good mess
in DoS terms if such a key pair were published).
The latter approach is what I used to create a sample dead-fingerprint
attack on a PGP 2.x fingerprints.
(No need to find hash collision even tho' md5 because it has another
bug: the serialization has multiple candidate inputs).  So I just
searched through the candidate inputs for one I can factor in a few

@_date: 2004-12-14 18:09:39
@_author: Adam Back 
@_subject: The Pointlessness of the MD5 "attacks" 
I thought the usual attack posited when one can find a collision on a
source checksum is to make the desired change to source, then tinker
with something less obvious and more malleable like lsbits of a UI
image file until you find your collision on two input source packages.

@_date: 2004-12-14 18:47:27
@_author: Adam Back 
@_subject: The Pointlessness of the MD5 "attacks" 
Well the people doing the checking (a subset of the power users) may
say "I checked the source and it has this checksum", and another user
may download that checksum and be subject to MITM and not know it.
Or I could mail you the source and you would check it with checksum
and compare checksum to web site.
Or somone could just go ahead and change the source without changing
the checksum or any of the changlog / cvs change notification stuff
and people would not think there is a change to review.
Some of this scenarios will likely work some of the time against

@_date: 2004-12-15 06:24:49
@_author: Adam Back 
@_subject: The Pointlessness of the MD5 "attacks" 
Is this the case?  Can't we instead start with code C and malicious C'
and try to find a collision on H(C||B) == H(C'||B') after trying 2^64
B values we'll find such a collision by the birthday principle.
Now we can have people review and attest to the correctness of code C,
and then we can MITM and change surrepticiously with C'.

@_date: 2004-12-16 05:50:22
@_author: Adam Back 
@_subject: pgp "global directory" bugged instructions 
So PGP are now running a pgp key server which attempts to consilidate
the inforamtion from the existing key servers, but screen it by
ability to receive email at the address.
So they send you an email with a link in it and you go there and it
displays your key userid, keyid, fingerprint and email address.
Then it says:
So here's the problem: it does not mention anything about checking
that this is your fingerprint.  If it's not your fingerprint but it is
your email address you could end up DoSing yourself, or at least
perpetuating a imposter key into the new supposedly email validated
keyserver db.
(For example on some key servers there are keys with my name and email
that are nothing to do with me -- they are pure forgeries).
Suggest they add something to say in red letters check the fingerprint
AND keyid matches your key.

@_date: 2004-12-22 16:47:55
@_author: Adam Back 
@_subject: Cryptography Research wants piracy speed bump on HD DVDs 
tolerate up to some threshold of colluding players.  However if you go
over that threshold (and it's not too large) you can remove the mark.
I would think the simplest canonical counter-attack would be to make a
p2p app that compares diffs in the binary output (efficiently rsync
style) accumulates enough bits to strip the disk watermark, p2p rips
and publishes.  QED.
DRM is a misguided endeavor.  You can not simultaneously give people
digital content and expect to stop them digitally copying it.  The
"attacker" owns and paid for the player.

@_date: 2004-07-28 14:09:03
@_author: Adam Back 
@_subject: should you trust CAs? (Re: dual-use digital signature vulnerability) 
The difference is if the CA does not generate private keys, there
should be only one certificate per email address, so if two are
discovered in the wild the user has a transferable proof that the CA
is up-to-no-good.  Ie the difference is it is detectable and provable.
If the CA in normal operation generates and keeps (or claims to
delete) the user private key, then CA misbehavior is _undetectable_.
Anyway if you take the WoT view, anyone who may have a conflict of
interest with the CA, or if the CA or it's employees or CPS is of
dubious quality; or who may be a target of CA cooperation with law
enforcement, secrete service etc would be crazy to rely on a CA.  WoT
is the answer so that the trust maps directly to the real world trust.
(Outsourcing trust management seems like a dubious practice, which in
my view is for example why banks do their own security,
thank-you-very-much, and don't use 3rd party CA services).
In this view you use the CA as another link in the WoT but if you have
high security requirements you do not rely much on the CA link.

@_date: 2004-07-30 17:54:56
@_author: Adam Back 
@_subject: should you trust CAs? (Re: dual-use digital signature vulnerability) 
Yes but if you compare this with the CA having the private key, you
are going to notice that you revoked and issued a new key; also the CA
will have your revocation log to use in their defense.
At minimum it is detectable by savy users who may notice that eg the
fingerprint for the key they have doesn't match with what someone else
had thought was their key.
Its a big deal to let the CA generate your key pair.  Key pairs should
be generated by the user.

@_date: 2004-05-09 05:08:09
@_author: Adam Back 
@_subject: Brands' private credentials 
It was implemented at least twice: once by ECAFE ESPRIT project years
ago, more recently by ZKS before they stopped licensing the patents.
I looked at Camenisch protocol briefly a couple of years ago and it is
not based Brands.  It is less efficient computationally, and more
rounds of communication are required if I recall.
But one feature that it does have that Brands doesn't have directly is
self-reblindability.  In their protocol it is the credential holder
who does the blinding, rather than the issuer / holder, and the issuer
can also re-blind to get a 2nd unlinkable show.  The way you do this
with Brands is to have the CA issue you a new credential in a
re-issuing protocol; Brands re-issuing protocol has the property that
you do not even have to reveal to the CA what attributes are in the
re-issued cert.
On re-showable/re-blindable approach, as with Ernie Brikell's
re-showable credential proposal for Palladium the converse side of
unlinkable re-showing is that there is no efficient way to revoke
credentials.  (If eg the private key is compromised, or the credential
owner violates some associated policy in the Palladium/DRM case).
(Caveat of course I think DRM is an unenforceable idea and the
schelling point ought to be not to even pretend to do it in software
or hardware, rip-once copy-everywhere *always* wins).
Is this the same as described in  with
interactive cut-and-choose and large credenitals?  There was some
discussion of that protocol in:
 Not read the new paper you cite yet.
The problem with the Yacobi's scheme (which is based on a composite
modulus variant of DH where you choose n=p.q such that p and q are
relatively smooth so you can do discrete log to setup the public key
for an identity) is that to get desirable security parameters for n
(eg 1024 bits) you have to expend huge amounts of resources per
identity public key.  So I would say it is not really practical.  It
is the only other semi-practical IBE scheme that I am aware of which
is why Boneh and Franklins IBE based on weil pairing was considered
such a break through.

@_date: 2004-05-09 05:15:59
@_author: Adam Back 
@_subject: chaum's patent expiry? (Re: Brands' private credentials) 
Oh yes, my other comment I forgot to mention was that if non-patent
status were a consideration, aside from Wagner's approach, another
approach for which the patent will presently expire is Chaum's
original approach combined with Niels Ferguson's single term offline
coins.  (Don't have citation handy but google will find you both).
Anyone have to hand the expiry date on Chaum's patent?  (Think it is
in patent section of AC for example; perhaps HAC also).
Having an expired patent might be a clearer route to non-patented
status than the putative this is a blind MAC not a blind signature
approach of Wagner's protocol.  But I obviously am not a patent
lawyer, and have avoided reading and participating in the writing of

@_date: 2004-05-09 06:04:31
@_author: Adam Back 
@_subject: Brands' private credentials 
[copied to cpunks as cryptography seems to have a multi-week lag these
OK, now having read:
and seeing that it is a completely different proposal essentially
being an application of IBE, and extension of the idea that one has
multiple "identities" encoding attributes.  (The usual attribute this
approach is used for is time-period of receipt .. eg month of receipt
so the sender knows which key to encrypt with).
so here is one major problem with using IBE: everyone in the system
has to trust the IBE server!
One claim is that the system should hide sensitive attributes from
disclosure during a showing protocol.  So the example given an AIDs
patient could authenticate to an AIDS db server without revealing to
an outside observer whether he is an AIDs patient or an authorised
However can't one achieve the same thing with encryption: eg an SSL
connection and conventional authentication?  Outside of this, the usual approach to this is to authenticate the
server first, then authenticate the client so the client's privacy is
Further more there seems to be no blinding at issue time.  So to
obtain a credential you would have to identify yourself to the CA /
IBE identity server, show paper credentials, typically involving True
Name credentials, and come away with a private key.  So it is proposed
in the paper the credential would be issued with a pseudonym.  However
the CA can maintain a mapping between True Name and pseudonym.
However whenever you show the credential the event is traceable back
to you by collision with the CA.
I would not say your Hidden Credential system _is_ an anonymous
credential system.  There is no blinding in the system period.  All is
gated via a "trust-me" CA that in this case happens to be an IBE
server, so providing the communication pattern advantages of an IBE
What it enables is essentially an offline server assisted oblivious
encryption where you can send someone a message they can only decrypt
if they happen to have an attribute.  You could call this a credential
system kind of where the showing protcool is the verifier sends you a
challenge, and the shower decrypts the challenge and sends the result
In particular I don't see any way to implement an anonymous epayment
system using Hidden Credentials.  As I understand it is simply not
possible as the system has no inherent cryptographic anonymity?

@_date: 2004-05-10 05:35:28
@_author: Adam Back 
@_subject: Brands' private credentials 
Well SSL was just to convince you that you were talking to the right
server ("you have reached the AIDs db server").
After that I was presuming you use a signature to convince the server
that you are authorised.  Your comment however was that this would
necessarily leak to the server whether you were a doctor or an AIDs
However from what I understood from your paper so does your scheme,
from section 5.1:
P = (P1 or P2) is encoded HC_E(R,p) = {HC_E(R,P1),HC_E(R,P2)} With Hidden Credentials, the messages are in the other direction: the
server would send something encrypted for your pseudonym with P1 =
AIDs patient, and P2 = Doctor attributes.  However the server could
mark the encrypted values by encoding different challenge response
values in each of them, right?
(Think you would need something like Bert Jaap-Koops Binding
cryptography where you can verify externally to encryption that the
contained encrypted value is the same to prevent that; or some other
proof that they are the same.)
Another approach to hiding membership is one of the techniques
proposed for non-transferable signatures, where you use construct:
RSA-sig_A(x),RSA-sig_B(y) and verification is x xor y = hash(message).
Where the sender is proving he is one of A and B without revealing
which one.  (One of the values is an existential forgery, where you
choose a z value first, raise it to the power e, and claim z is a
signature on x= z^e mod n; then you use private key for B (or A) to
compute the real signature on the xor of that and the hash of the
message).  You can extend it to moer than two potential signers if
OK so the fact that the server is the AIDs db server is itself secret.
Probably better example is dissident's server or something where there
is some incentive to keep the identity of the server secret.  So you
want bi-directional anonymity.  It's true that the usual protocols can
not provide both at once; SSL provides neither, the anonymous IP v2
protocol I designed at ZKS had client anonymity (don't reveal
pseudonym until authenticate server, and yet want to authenticate
channel with pseudonym).  This type of bi-directional anonymity pretty
much is going to need something like the attribute based encryption
model you're using.
However it would be nice/interesting if one could do that end-2-end
secure without needing to trust a CA server.
this one is a feature auth based systems aren't likely to be able to
fullfil, you can say this because the server doesn't know if you're
able to decrypt or not
I think it would be fair to call it anonymity system, just that the
trust model includes a trusted server.  There are lots of things
possible with a trusted server, even with symmetric crypto (KDCs).

@_date: 2004-05-10 06:02:51
@_author: Adam Back 
@_subject: blinding & BF IBE CA assisted credential system (Re: chaum's patent expiry?) 
I think you mean so that the CA/IBE server even though he learns
pseudonyms private key, does not learn the linkage between true name
and pseudonym.  (At any time during a show protocol whether the
private key issuing protocol is blinded or not the IBE server can
compute the pseudonyms private key).
Seems like an incremental improvement yes.
Note PFS does not make end-2-end secure against an adversary who can
compute the correspondents private keys, as vulnerable to MITM.  Could
say invulnerable to passive eavesdropper.  However you might have an
opening here for a new security model combining features of Hidden
Credentials with a kind of MITM resistance via anonymity.  What I mean
is HC allows 2 parties to communicate, and they know who they are
communicating with.  The CA colluding MITM however we'll say does not
apriori, so he has to brute force try all psuedonym, attribute
combinations until he gets the right one.  Well still not desirable
security margin, but some extra difficulty for the MITM.

@_date: 2004-05-10 17:54:59
@_author: Adam Back 
@_subject: more hiddencredentials comments (Re: Brands' private credentials) 
OK that sounds like it should work.  Another approach that occurs is
you could just take the plaintext, and encrypt it for the other
attributes (which you don't have)?  It's usually not too challenging
to make stuff deterministic and retain security.  Eg. any nonces,
randomizing values can be taken from PRMG seeded with seed also sent
in the msg.  Particularly that is much less constraining on the crypto
system than what Bert-Jaap Koops had to do to get binding crypto to
work with elgamal variant.
The above approach should fix that also right?
dissident computing I think Ross Anderson calls it.  People trying to
operate pseudonymously and perhaps hiding the function of their
servers in a cover service.
Unless it's signifcantly less efficient, I'd say use it all the time.
Yes.  But you could explore public key based without IBE.  You may
have to use IBE as a sub-protocol, but I think ideally want to avoid
the IBE server being able to decrypt stuff.  Sacrificing the IBE
communication pattern wouldn't seem like a big deal.
Hmm well IBE is has a useful side-effect in pseudonymity systems
because it also has the side-effect of saving the privacy problems in
first obtaining the other parties key.  Other way to counteract that
is to always include the psuedonym public key with the pseudonym name
(which works for mailto: style URLs or whatever that are
electronically distributed, but not for offline distributed).
Btw one other positive side-effect of IBE is the server can't
impersonate by issuing another certificate in a pseudonyms name
because there is definitionally only one certificate.
I was thinking particularly if you super-encrypt with the psuedonym's
(standard CA) public key as well as the IBE public key you get the
best of both feature sets.
btw You could probably come up with a way to prevent a standard (non
IBE) CA from issuing multiple certs.  eg. if he does that and someone
puts two certs together they learn CA private key, ala Brands
credential kind of offline double spending protection.
Kind of a cryptographically enforced version of the policy enforced
uniqueness of serial numbers in X.509 certs.  And we change the policy
to one cert per pseudonym (kind of sudden death if you lose the
private key, but hey just don't do that; we'd have no other way to
authenticate you to get a new cert in the same psuedonyms name anyway,
so you may just as well backup your pseudonym private key).

@_date: 2004-05-10 18:24:31
@_author: Adam Back 
@_subject: blinding & BF IBE CA assisted credential system (Re: chaum's patent expiry?) 
But if I understand that is only half of the picture.  The recipient's
IBE CA will still be able to decrypt, tho the sender's IBE CA may not
as he does not have ability to compute pseudonym private keys for the
other IBE CA.
If you make it PFS, then that changes to the recipient's IBE CA can
get away with active MITM rather than passive eavesdropping.
An aside is that PKI for Psuedonym's is an interesting question.  The
pseudonym can't exactly go and be certified by someone else as an
introducer without revealing generally identifying things about the
network of trust.  But ignoring this presuming that the identities
were not subject to MITM from day one, and could build up a kind of
WoT despite lack of out-of-band way to check info to base WoT
signatures on.  It would still be interesting to defend the pseudonym
against MITM colluding with IBE CA that at some point after the
pseudonym has transferred keys without insertion of a MITM from.
This problem of addressing the who goes first problem for pseudonymous
communicants appears somewhat related to Public Key Steganography
where there is a similar scenario and threat model.  (Anderson and
Petitcolas"On The Limits of Steganography"
They also cite a "Prisoners' problem" which might be something you
could extend involving a warden who is eavesdropping and prisoners who
will be penalized if he can detect and identify communicants.
My earlier comment:
may not be that useful a distinction as the IBE CA server also gets
your private key, so he doesn't _need_ to generate a certificate
impersonating you as a conventional rogue CA would.
But if we could make the binding from pseudonym to the pseudonym's
non-IBE public key strictly first come first served, so that the IBE
CA's attemt to claim his later released non-IBE public key is the
correct one would be detectable.  Either secure time-stamping,
extending the psuedonym name to include fingerprint as
self-authenticator would allow this.

@_date: 2004-05-18 23:49:11
@_author: Adam Back 
@_subject: 3. Proof-of-work analysis 
Here's a forward of parts of an email I sent to Richard with comments on
his and Ben's paper (sent me a pre-print off-list a couple of weeks ago):
One obvious comment is that the calculations do not take account of
the CAMRAM approach of charging for introductions only.  You mention
this in the final para of conclusions as another possible.
My presumption tho don't have hard stats to measure the effect is that
much of email is to-and-fro between existing correspondents.  So if I
were only to incur the cost of creating a stamp at time of sending to
a new recipient, I could bear a higher cost without running into
However the types of levels of cost envisaged are aesthetically
unpleasing; I'd say 15 seconds is not very noticeable 15 mins is
noticeable and 1.5 hrs is definately noticeable.
Of course your other point that we don't know how spammers will adapt
is valid.  My presumption is that spam would continue apace, the best
you could hope for would be that it is more targetted, that there are
financial incentives in place to make it worth while buying
demographics data.  (After all when you consider the cost of sending
junk paper mail is way higher, printing plus postage, and yet we still
receive plenty of that).
Also as you observe if the cost of spamming goes up, perhaps they'll
just charge more.  We don't know how elastic the demand curve is.
Profitability, success rates etc are one part of it.  There is an
interplay also: if quantity goes down, perhaps the success rate on the
remaining goes up.  Another theory is that a sizeable chunk of spam is
just a ponzi scheme: the person paying does not make money, but a lot
of dummy's keep paying for it anyway.
Another potential problem with proof-of-work on introductions only, is
that if the introduction is fully automated without recipient opt-in,
spammers could also benefit from this amortized cost.  So I would say
something like the sender sent a proof-of-work, and the recipient took
some positive action, like replying, filing otherwise than junk or
such should be the minimum to get white-listed.
On the ebiz web site problem, I think these guys present a problem for
the whole approach.  An ebiz site will want to send lots of mail to
apparent new recipients (no introductions only saving), a popular ebiz
site may need to send lots of mail.
Well it is ebiz so perhaps they just pass the cost on to the consumer
and buy some more servers.
Another possibility is the user has to opt-in by pre-white-listing
them, however the integration to achieve this is currently missing and
would seem a difficult piece of automation to retrofit.
One of the distinguishing characteristics of a spammer is the
imbalance between mail sent and mail received.  Unfortunately I do not
see a convenient way to penalize people who fall into this category.
Also because of network effect concerns my current hashcash deployment
is to use it as a way to reduce false positives, rather than directly
requiring hashcash.  Well over time this could come to the same thing,
but it gives it a gentle start, so we'll see how long it is before the
1st genuine spam with hashcash attached.
CAMRAM's approach is distinct and is literally going straight for the
objective of bouncing mail without some kind of proof (hashcash or
reverse-turing, or short term ability to reply to email

@_date: 2004-05-25 16:10:21
@_author: Adam Back 
@_subject: Reusable hashcash for spam prevention 
FYI Richard amended the figures in the paper which makes things 10x
more favorable for hashcash in terms of being an ecomonic defense
against spammers.
Richard wrote on asrg:

@_date: 2004-10-05 17:18:30
@_author: Adam Back 
@_subject: Brands credential book online (pdf) 
Stefan Brands book on his credential / ecash technology is now
downloadable in pdf format from credentica's web site:
        (previously it was only available in hardcopy, and only parts of the
content was described in academic papers).
Also the credentica web site has gone live, lots of content.
Credentica is Stefan's company around digital credentials ecash /
anonymity news watchers may have seen some discussion of the
credentica startup company earlier this year.

@_date: 2004-09-07 16:13:13
@_author: Adam Back 
@_subject: will spammers early adopt hashcash? (Re: Spam Spotlight on Reputation) 
It is just market-led research into showing that the bayesian
anti-spam systems are fatally flawed, someone needs to do it :-).
Only when both sides are fully optimized will we see the true picture
of how spam can be stopped.
(Eg. this is why hashcash has been optimized to death in processor
specific assembly code ... spammers will do it so we must not kid
The higher than average SPF adoption rate by spammers confirms they
are early adopters.  I'd like to see them try to early-adopt hashcash
though ;-)
(There are similar incentives to SPF, maybe higher incentives as of
spamassassin 3.0.)
Well we'll see.  If they have lots of CPU from zombies and can get and
maintain more with limited effort maybe even they can, and CAMRAM's
higher cost stamp on introductions only will prevail as the preferred

@_date: 2004-09-08 17:30:51
@_author: Adam Back 
@_subject: Seth Schoen's Hard to Verify Signatures 
I proposed a related algorithm based on time-lock puzzles as a step
towards non-parallelizable, fixed-minting-cost stamps in section 6.1
of [1], also Dingledine et al observe the same in [2].
The non-parallelizable minting function is in fact the reverse: sender
encrypts (expensively) and the verifier encrypts again (but more
cheaply) and compares, but I think the relationship is quite analogous
to the symmetry between RSA encryption and RSA signatures.
I think maybe you have observed an additional simplification.  In my
case I use sender chooses x randomly (actually hash output of random
value and resource string), and computes y = x^{x^w} mod n as the work
function (expensive operation); and z = x^w mod phi(n), y =?  x^z mod
n as the cheap operation (verification).
I think your approach could be applied on the encryption side too
resulting in simpler, faster verification.  Instead it would be:
x is random, compute y = x^{2^t+1} mod n; verify x =? y^d mod n
I'll add a note about that when I get around to updating it next.
[1] Hashcash - Amortizable Publicly Auditable Cost-Functions
[2] Andy Oram, editor.  Peer-to-Peer: Harnessing the Power of
Disruptive Technologies. O'Reilly and Associates, 2001.  Chapter 16
also available as

@_date: 2004-09-11 13:49:23
@_author: Adam Back 
@_subject: anonymous IP terminology (Re: [anonsec] Re: potential new IETF WG on anonymous IPSec (fwd from hal@finney.org)) 
I respectfully request that you call this something other than
"anonymous".  It is quite confusing and misleading.  Some people have spent quite a bit of time and effort in fact working
on anonymous IP and anonymous/pseudonymous transports.
For example at ZKS we worked on an anonymous/pseudonymous IP product
(which means cryptographically hiding the souce IP address from the
There are some new open source anonymous IP projects.
Your proposal, which may indeed have some merit in simplifying key
management, has _nothing_ to do with anonymous IP.  Your overloading
of the established term will dilute the correct meaning.
Zooko provided the correct term and provided references:
"opportunistic encryption".  It sounds to have similar objectives to
what John had called opportunistic encryption and tried to do with
freeSWAN.  Lowever level terms may be unauthenticated as Hal
suggested.  Or non-certified key management (as the SSH cacheing of
previously before seen IP <-> key bindings and warnings when they
The access is _not_ anonymous.  The originator's IP, ISP call traces,
phone access records will be all over it and associated audit logs.
The distinguishing feature of anonymous is that not only is your name
not associated with the connection but there is no PII (personally
identifiable information) associated with it or obtainable from logs
And to be clear also anonymous means unlinkable anonymous across
multiple connections (which SSH type of authentication would not be)
and linkable anonymous means some observable linkage exists between
sessions which come from the same source (though no PII), and
pseudonymous means same as linkable anonymous plus association to a
persistent pseudonym.
Again there are actually cryptographic protcols for_ having anonymous
authentication: ZKPs, multi-show unlinkable credentials, and
refreshable (and so unlinkable) single-show credentials.

@_date: 2004-09-11 15:09:54
@_author: Adam Back 
@_subject: anonymous IP terminology (Re: [anonsec] Re: potential new IETF WG on anonymous IPSec (fwd from hal@finney.org)) 
I think you are confusing a weak potential for a technical ambiguity
of identity under attack conditions with anonymity.  (The technical
ambiguity would likely disappear in most practical settings).
Anonymity implies positives steps to avoid linking with PII.  With
anonymity you want not just technical ambiguity, but genuinely
pluasible deniability from an anonymity set -- preferably a large set
of users who could equally plausibly have established a given
connection, participated in an authentication protocol etc.
We don't after all call TCP anonymous, and your system is cleary
_less_ "anonymous" than TCP as there are security mechanisms involved
with various keys and authentication protocols which will only reduce
Practically, knowing the IP address conveys a lot.  Many ISPs have
logs, some associated with DSL subscriber and phone records, for
billing, bandwidth caps, abuse complaints, spam cleanup etc etc.
The IP may be used for many different logged activities and some of
those activites may involve directly identified authentication.
People go to lengths to hide their IP precisely because it does
typically convey all too much.
If one wants this to be true in practice it has to propogate up the
stack.  (Not the problem of ANONSEC, a problem for the higher level
But even at the authentication protocol level one has to be quite
careful.  There are many gotchas if you really do want it to be
unlinkable.  (eg. pseudo random sequences occur in many settings at
different protocol levels which are in fact quite linkable).  I'll
give you one high level example.  At ZKS we had software to remail
MIME mail to provide a pseudonymous email.  But one gotcha is that
mail clients include MIME boundary lines which are pseudo-random
(purely to avoid string collision).  If these random lines are
generated with a non-cryptographic RNG it is quite likely that so
called unlinkable mail would in fact be linkable because of this
higher level protocol.  (We cared about unlinkability even tho' I said
pseudonymous because the user had multiple pseudonyms which were
supposed to be unlinkable across).
I would say if your interest in fixing such pseudo random sequeneces
is not present you should not be calling this anonymous.
But if it is part of your threat model, then you may in fact be using
anonymous authentication and that would be interesting to me at least
to participate.

@_date: 2004-09-13 16:43:57
@_author: Adam Back 
@_subject: will spammers early adopt hashcash? (Re: Spam Spotlight on Reputation) 
Ben and Richard CLayton's paper makes several assumptions and we'll
see how those pan out in the field as time goes on.
We don't really know what the true cost of maintaining ownership of
many machines.  No doubt much lower than it should be because of poor
security on microsoft OSes.  But even so there must be some turn over
as the user instals AV, firewalls, gets cut off by ISP, gets IP
blacklisted etc.
The general argument is in the FAQ quoted below.
Essentially whatever resources spammers do have, hashcash is going to
slow them down because the balance of CPU power vs bandwidth is such
that 20-bit hashcahs with current hardware is likely to slow down the
output of a typical consumer destkop+DSL line down by afact or 10-100x
less spam.  (Depnds on CPU power, DSL uplink, and number of Bcc
recipients per message).  Hashcash costs equal cpu per Bcc recipient.
Without hashcash Bcc recipients to the same domain or to a hub cost a
tiny bit of bandwidth -- the size of the email address (+"RCPT TO
Will it be enough -- we don't know yet, but if widely deployed it
would make spammers adapt.  We just don't yet know how they will
The other question Ben & Richards paper doesn't explore is the CAMRAM
way of using hashcash.  In this model you only pay hashcash for
_introductions_.  After parties have replied to a mail, the mail is
whitelisted (short term by address only (risky no auth, joe-job
hazard) medium term with CAMRAM email header signatures).  If simple
hashcash per mail turns out not to be enough, CAMRAM can increase the
work factor, as people do not reply to spammers; and many emails are
to-and-fro vs first introduction emails.  (So the sender can afford to
pay more on average).  Eric sent a spreadsheet with some of this type
of calculation.
There may also be some mileage in Hal Finney's RPOW
 where the legitimate user can re-use stamps he
receives.  (The scaling issues of the RPOW servers would need to be
engineered carefully, there are servers, they can be per eg domain ,
but still compared to hashash this is more infrastructure as hashcash
is pure end-to-end).
 2c and 2d

@_date: 2005-08-08 10:55:49
@_author: Adam Back 
@_subject: locking door when window is open? (Re: solving the wrong problem) 
"Single picket fence" -- doesn't work without a lot of explaining.
The one I usually have usually heard is the obvious and intuitive
"locking the door when the window is open".
(ie fixating on quality of dead-bolt, etc on the front door when the
window beside it is _open_!)

@_date: 2005-08-17 09:51:52
@_author: Adam Back 
@_subject: How many wrongs do you need to make a right? 
Not to defend PKI, but what about delta-CRLs?
Maybe not available at time of the Navy deployment?  But certainly
meaning that people can download just changes since last update.
Well presumably you mean a Merkle hash tree or something?  (A single
hash of all the revoked certs doesn't help you as you don't know which
are revoked and so have insufficient data to go into the hash function
verify if a given cert is on the list.)

@_date: 2005-08-26 04:24:32
@_author: Adam Back 
@_subject: Another entry in the internet security hall of shame.... 
Thats broken, just like the "WAP GAP" ... for security you want
end2end security, not a secure channel to an UTP (untrusted third

@_date: 2005-08-26 13:59:47
@_author: Adam Back 
@_subject: e2e all the way (Re: Another entry in the internet security hall of shame....) 
Well I think security in IM, as in all comms security, means security
such that only my intended recipients can read the traffic.  (aka e2e
I don't think the fact that you personally don't care about the
confidentiality of your IM messages should argue for not doing it.
Fair enough you don't need it personally but it is still the correct
security model.  Some people and businesses do need e2e security.  (It
wasn't quite clear, you mention you advised jabber; if you advised
jabber to skip e2e security because its "too hard"... bad call I'd
I don't think it is that hard to do e2e security.  Skype does it.
Fully transparently.
Another option: I would prefer ssh style cached keys and warnings if
keys later change ("opportunistic encryption") to a secure channel to
the UTP (MITM as part of the protocol!).
Ssh-style is definitely not hard.  I mean nothing is easier no doubt
than slapping an SSL tunnel over the server mediated IM protocol, but
if the security experts are arguing for the easy way out, what hope is
there.  I'm more used to having to argue with application
functionality focussed people that they need to do it properly -- not
with crypto people.
I do think we have a duty in the crypto community to be advocates for
truly secure systems.  We are building piecemeal the defacto privacy
landscape of the future; as everything moves to the internet.  Take
another example... the dismal state of VOIP security.  I saw similar
arguments on the p2p-hackers list a few days ago about security of p2p
voip: "who cares about voice privacy" etc.

@_date: 2005-08-27 03:05:03
@_author: Adam Back 
@_subject: e2e & security by default (Re: e2e all the way) 
OK summing up: I think e2e secure, and secure by default.
I think user-to-server "security" is not secure by in my view the most
important comms security metric -- e2e security.  So if one engineers
for this as the default your system becomes not secure by default.
Don't forget that peoples security model can change radically without
warning.  (end users) typically give no prior thought to security
until something goes wrong.  At which point they are screwed if the
default is "secure comm to the UTP".
People complain at microsoft for example, if their software is not
secure by default.  The BSD OS goes to some pains to be secure by
If you're saying yes it _could_ be e2e secure if users jump through
x,y,z hoops (like run your own IM server!) ... well you know that even
power users, who do want security, may give up at the inconvenience of
I draw the line for IM security:
- private keys generated on the client
- public keys maybe by default certified by central entity
  - but advanced user has choice to use other ceritfication, including
    out of band, WoT etc
(central entity one can easily automate, which is what skype does)
now a-kind of active MITM with rogue CA attack is possible with
collusion of the central CA (by issuing a second certficiate for the
wire-tapping party), however the advanced user can detect and come
away with evidence of this.  The fact that the advanced user retains
this ability I think adds value for even non-technical users; the CA
run risk of ruining their reputation, violating the CPS etc. and of
their being evidence.
btw I think there is signifciant additional value in _forceing_ the
attacker to sniff the traffic *and* do an active MITM with rogue CA
attack.  With your by default route through UTP, the attacker has a
natural and convenient place to subpeona, OS penetrate etc. and
undetectably snoop on traffic.
I used key roaming in this scenario when I had this problem.  (Without
giving the central server cleartext private key).
I disagree.  My metrics for secure IM protocol design are:
- private keys are generated on client machine
- private keys do not leave client machine in unencrypted form
- end2end security where possible
- immediate forward secrecy where possible
And you can do auth-key roaming in this.
(Note for IM security you are better off certifying auth keys and
using the auth keys to authenticate EDH; if the user forgets the
password etc., you can just issue new auth certs).
If you've looked at IM security, there are a number of other
interesting challenges in IM security also btw, joining and leaving
security, and the fact that comms group can be > 2 endpoints.  Joining
and leaving security argue for backward secure and forward secure
re-keying respectively.

@_date: 2005-07-15 17:20:24
@_author: Adam Back 
@_subject: mother's maiden names... 
I think in the UK check signatures are not verified below ?30,000
(about US $53,000).  I presume it is just economics ... cost of
infrastructure to verify vs value of verifying given the fraud rate.

@_date: 2005-06-14 04:21:39
@_author: Adam Back 
@_subject: use KDF2 / IEEE1363a (Re: expanding a password into many keys) 
The non-banking version of this is the KDF2 function in IEEE1363a.
Same deal:  void KDF2( const void* Z, int, const void* P, int, void* K, int );
Z = master-key, P = permuter, K = derived key
each is variable sized.  (Sorry I implemented the source for someone
who has the copyright or you could have that).  It's very simple to
implement however:
key = SHA1( Z || 0 || P ) || SHA1( Z || 1 || P ) ...
for as many bytes as you need.  So I would eg use P = "AES" and P =
"HMACS" to derive two different key.  Looks like KDF2 has the same
problem John mentioned, so don't do that (let attacker chose P).

@_date: 2005-06-14 15:42:45
@_author: Adam Back 
@_subject: use KDF2 / IEEE1363a (Re: expanding a password into many keys) 
I suppose I should also have note that the master key going into KDF2
would be derived with PBKDF2 from a password if this is a password
derived set of keys, to get the extra features of a salt and iterator
to slow down brute force.

@_date: 2005-03-24 18:51:25
@_author: Adam Back 
@_subject: and constrained subordinate CA costs? (Re: SSL Cert prices ($10 to $1500, you choose!)) 
The URL John forwarded gives survey of prices for regular certs and
subdomain wildcard certs/super certs (ie *.mydomain.com all considered
valid with respect to a single cert).
Does anyone have info on the cost of sub-ordinate CA cert with a name
space constraint (limited to issue certs on domains which are
sub-domains of a your choice... ie only valid to issue certs on
sub-domains of foo.com).
Maybe the answer is "a lot of money"... CA operators probably view
users of this kind of tech to be corporations with big infrastructure
to secure.  It sounds like the  offers this
kind of service.  However it sounds like its web based so they have
really just bundled a more streamlined way to create lots of certs.
(Thawte spki means "starter PKI" (not simple pki)).

@_date: 2005-03-25 17:06:17
@_author: Adam Back 
@_subject: and constrained subordinate CA costs? 
Well I would say downright dangerous -- if its not flagged critical
and not understood, right?
Implication would be an intended constrained subordinate CA would be
able to function as an unconstrained subordinate CA in the eyes of
many clients -- free ability to forge any domain in the global SSL

@_date: 2005-05-29 17:09:05
@_author: Adam Back 
@_subject: Microsoft info-cards to use blind signatures? 
Yes but the other context from the related group of blog postings, is
Kim Cameron's (microsoft) "laws of identity" [1] that this comment is
made in the context of.
It is relatively hard to see how one could implement an identity
system meeting the stated laws without involving blind signatures of
some form...
[1]

@_date: 2005-09-20 18:39:10
@_author: Adam Back 
@_subject: Defending users of unprotected login pages with TrustBar 0.4.9.93 
I would think it would be safer to block the site, or provide a
warning dialog.  (This is what I was expecting when I started reading
the head post; I was bit surprised at the interventionism to actually
go ahead and "fix" the site, maybe that would be a better default
btw Regarding unadvertised SSL equivalents, I have noticed if you
login to gmail, you get SSL for login, but then http for web mailer.
However if you edit the URL after login to https, it appears to work
ok over SSL also.

@_date: 2006-04-02 13:29:26
@_author: Adam Back 
@_subject: Unforgeable Blinded Credentials 
Well I thought of this a few years ago also.  However I suspect you'd
find the same idea earlier as a footnote in Stefan Brands book.  (Its
amazing how much stuff is in there, I thought I found something else
interesting -- offline transferable cash, turns out that also was a
footnote referring to someone's MSc thesis.)
This is harder, I thought about this a bit also.
I was thinking a way to do this would be to have a self-reblindable
signature.  Ie you can re-blind the certificate signature such that
the signature remains, but it is unlinkable.  I didn't so far find a
way to do this with any of the schemes.
So it would for example be related to the more recent publicly
re-encryptable Elgamal based signatures.  (Third party can re-encrypt
the already encrypted message with out themselves being able to
decrypt the message).
Brands also has a mechanism to simplify the use each cert once method.
He can have the CA reissue you a new cert without having to go through
the attribute verification phase.  Ie you present an old cert and get
it reblinded, and the CA does not even if I recall see what attributes
you have.  So you just periodically get yourself another batch.
Mostly does what you want just with some assistance from the CA.

@_date: 2006-04-04 14:24:38
@_author: Adam Back 
@_subject: Unforgeable Blinded Credentials 
I think they shows are linkable, but if you show more than allowed
times, all of the attributes are leaked, including the credential
secret key and potentially some identifying information like your
credit card number, your address etc.
The main use I think is to have 1-show, where if you show more than 1
time your identity is leaked -- for offline electronic cash with fraud
tracing.  But as you say the mechanism generalizes to multiple show.
Well the other kind of disincentive was a credit card number.  My
suggestion was to use a large denomination ecash coin to have
anonymous disincentives :) ie you get fined, but you are not

@_date: 2006-04-08 16:16:03
@_author: Adam Back 
@_subject: Unforgeable Blinded Credentials 
No they are definitely mutually linkable (pseudonymous), tho obviously
not linkable to the real identity at the issuer.
No.  It arises because the credential public key is necessarily shown
during a show.  (The credential public key is blinded during
credential issue so its not linkable to issue).  So you can link
across shows simply by comparing the credential public key.
Its hard to blind the public key also.  I thought thats what you were
talking about in a previous mail where you were saying about what
could be done to make things unlinkable.  (Or maybe trying to find the
same property you thought Brands had ie unlinkable multi-show, for
Chaums credentials.)
Note with Brands credentials you can choose: unlimited show, 1-show or
n-show.  To do 1-show or n-show you make some formula for initial
witness that is fair and verifiable by the verifier, so there are only
n allowed IWs, and consequently if you reuse one it leaks two shows
with the same IW which allows the credential private key to be
recovered.  ie its just a trick to define a limited number of allowed
(and verifier verified) IWs -- IW is a sort of commitment by the
credential owner in the show protocol.
So there is something compact that the verifier can send
somewhere and it can then collate them and notice when a show is > n
shows (presuming there are multiple verifiers and you want to impose n
shows across all of them).
No I mean put the same high value ecash coin in all of your offline
limited show credentials / offline ecash coins.
eg say you can choose to hand over $100 and retain your anonymity even
in event of double-spending offline ecash coins, or over-using
limited-show credentials.
I was curious about the Chameleon credential as they claim to work
with Brands credentials, I wrote to one of the authors to see if I
could get an electronic copy, but no reply so far.
Note also about your earlier comments on lending deterrence,
ultimately I think you can always do online lending.

@_date: 2006-04-19 15:23:36
@_author: Adam Back 
@_subject: Unforgeable Blinded Credentials 
Agreed, its very nice if we could do this.  However all of the
practical schemes are show-linkable.
I looked at the paper that was referenced earlier in the thread about
the Chameleon [1] credentials which are an attempt to add unlinkable
multi-show to Brands credentials.
So aside from the fact that it uses a non-standard assumption that it
is hard to find e^v = a^x + c mod n (for RSA e,n).  Apparently
Camenisch's other assumption that it is hard to find e^v = a^x +1 was
broken... so thats not very comforting to start.  (They offer no proof
of this assumption).
Then they use an interactive ZKP in the show which I think will
require say 80 rounds for reasonable security, each round involving
some non-trivial computation.
So its not that practical compared to Chaum, Brands etc -- its not
very efficient in time nor communication required for the showing of
the chameleon certs.
[1] "An Anonymous Credential System and a Privacy-Aware PKI" by Pino
Persiano and Ivan Visconti
I put a copy online here temporarily:

@_date: 2006-08-14 17:31:39
@_author: Adam Back 
@_subject: Hamiltonian path as protection against DOS. 
If you're using a hashcash token which takes 20 seconds of your CPU,
it'll slow the spammer down if they owned node has broadband.
(Think about 5k message size, multiple Bcc recipients etc; the spammer
of an owned botnet node can send multple many per second if hashcash
reduces the number of messages that can be sent by a factor of 100,
thats a good thing.)
Whether its enough of a slow down is an open question -- but I think
its difficult to imagine a security protocol that prevent spam with
the attacker owning some big proportion of nodes.

@_date: 2006-08-22 15:58:17
@_author: Dr Adam Back 
@_subject: A security bug in PGP products? 
What they're saying is if you change the password, create some new
data in the encrypted folder, then someone who knew the old password,
can decrypt your new data.
Why?  Well because when you change the password they dont change the
symmetric key used to encrypt the data.  The password is used to
create a KEK (key encryption key) and this in-turn is used to encrypt
the folder key (which is used to do the actual data encryption.)  Now
in common with a lot of other systems, changing the password does not
entail re-encrypting the actual data.
(To do so would require waiting for it to re-encrypt.  There are
systems that do this, but it is a tradeoff, espeically in a
single-user scenario)
Personally my preferred security property (in a multi-user storage
system where users can be added and removed) is that people who had
access can still decrypt the stuff they had access to, but can't
decrypt new data encrypted since then.  I think its a good balance
because that person had the data anyway, and could remember it, have
backups of it etc.
Another thing that can be done is to utilize an online server, which
has an additional key such that it cant decrypt, but can hand it over
on successful auth and can delete that key on request.  Obviously the
key would be combined in a one-way fashion so the server does not have
to be trusted other than to delete keys on request.
However the article also talks about forensics, and I think they maybe
confusing something there because most encrypted content is not
authenticated anyway (you can merrily switch around ciphertext blocks
without triggering any integrity warnings at the crypto level).  And
anyway if the forensic investigator has the password, he can change
anything! -- symmetric encryption keys known to others are not

@_date: 2006-12-26 15:58:44
@_author: Adam Back 
@_subject: secure CRNGs and FIPS (Re: How important is FIPS 140-2 Level 1 cert?) 
You can make a secure CRNG that you can obtain FIPS 140 certification
on using the FIPS 186-2 appendix 3.1 (one of my clients got FIPS 140
on an implementation of the FIPS 186-2 RNG that I implemented for
general key generation and such crypto use.)
You should apply change notice 1 under the section "general purpose
random number generation", or you will be doing needless modulo q
bignum operations for general RNG use (the default, non-change-note
modified RNG is otherwise hard code for DSA k value generation and
related things 186-2 being the FIPS DSA standard doc).
Also about continuously adding seeding this is also provided with
186-2 rng via the XSEED parameter, which allows the system to add
extra entropy at any time.
About the criticisms of Common Critera evaluation in general, I think
why people complain it is a documentation exercise is because pretty
much all it does ensure that it does what it says it does.  So
basically you have to enumerates threats, state what threats the
system is designed to protect against, and which are out of scope.
Then the rest of the documentation is just saying that in increasing
detail, that you have not made mistakes in the design and
specification and to some extent implementation.
So as someone else said in the thread, as a user you need to read the
security target document section on security objectives and
assumptions, and check if they protect against attacks that are
relevant to you.
Another aspect of security targets is protection profiles.  A
protection profile is basically a sort of set of requirements for
security targets for a given type of system.  So you might get eg a
protection profile for hard disk encryption.  The protection profile
will be standardized on and so it makes it a bit easier for the
consumer as its less likely the protection profile will be massaged.
(I mean the consortium or standardization body creating the protection
profile will want some security quality bar).

@_date: 2006-02-09 09:43:20
@_author: Adam Back 
@_subject: conservative choice: encrypt then MAC (Re: general defensive crypto coding principles) 
Don't forget Bleichenbacher's error channel attack on SSL
implementations, which focussed on the mac then encrypt design of
SSL... web servers gave different error for malformed padding vs
plaintext MAC failure.  The lesson I drew from that is the
conservative choice is encrypt then MAC.
I dont think encrypt then MAC presents a timing attack because your
chances of getting past the MAC are 1/2^80 in the first place.
And obviously you include the whole ciphertext (including the IV!) in
the MAC.  In fact anything which affects decryption process should be
in the mac.

@_date: 2006-01-03 17:51:08
@_author: Adam Back 
@_subject: OpenSSL BIGNUM vs. GMP 
Does openSSL defend against cache related attacks?

@_date: 2006-01-11 09:22:30
@_author: Adam Back 
@_subject: long-term GPG signing key 
There are a number of differences in key management priorities between
(communication) signature and encryption keys.
For encryption keys:
- you want short lived keys
- you should wipe the keys after at first opportunity
- for archiving you should re-encrypt with storage keys
- you can't detect or prove an encryption key is compromised as the
attacker will just be decrypting documents
For signature keys:
- you want longer lived keys (or two tier keys, one for ceritfying
that is kept offline, and one for signing communications that is
offline) - in fact many applications dont even want signatures they
want authentication (convince the recipient of author and integrity,
but be non-transferable)
- with signature keys if they are compromised and the compromised key
used, there is risk (to the attacker) that the recipient or others can
detect and prove this.
I do agree tho that the relative value of encryption vs signature
depends on teh application.

@_date: 2006-03-30 02:22:58
@_author: Adam Back 
@_subject: Your secrets are safe with quasar encryption 
How many suitable quasars are there?  You'd be damn lucky if its a
cryptograhic strength number.
Now you might think there are limits to how many signals you can
listen to and that would be some protection, however you still have
brute force guess a signal, and probability of guessing the right key
would be rather high compared to eg 2^-256 per guess with AES.
Also they offer the strange comment "The method does not require a
large radio antenna or that the communicating parties be located in
the same hemisphere, as radio signals can be broadcast over the
internet at high speed."  So if we are talking only about enough
signals such that they can be continuosly monitored or a trusted
server which monitors your subset for you... well then how do you
secure the stream (ie if you send it over the internet AES encrypted,
you'd just as well AES encrypt your data).
Sounds more than a bit dubious overall.

@_date: 2006-05-05 04:17:36
@_author: Adam Back 
@_subject: encrypted filesystem integrity threat-model (Re: Linux RNG paper) 
I think an encrypted file system with builtin integrity is somewhat
interesting however the threat model is a bit broken if you are going
to boot off a potentially tampered with disk.
I mean the attacker doesnt have to tamper with the proposed
encrypted+MACed data, he just tampers with the boot sector/OS boot,
gets your password and modifies your data at will using the MAC keys.
I think you'd be better off building a boot USB key using DSL or some
other small link distro, checksumming your encrypted data (and the
rest of the disk) at boot; and having feature to store the
keyed-checksum of the disk on shutdown some place MACed such that the
USB key can verify it.  Then boot the real OS if that succeeds.
(Or better yet buy yourself one of those 32GB usb keys for $3,000 and
remove the hard disk, and just keep your encrypted disk on your
keyring :)
Of course an encrypted network filesystem has other problems... if
you're trying to actively use an encrypted filesystem backed in an
unsecured network file system then you're going to need MACs and
replay protection and other things normal encrypted file system modes
dont provide.

@_date: 2006-10-09 11:45:53
@_author: Adam Back 
@_subject: TPM & disk crypto 
So the part about being able to detect viruses, trojans and attest
them between client-server apps that the client and server have a
mutual interest to secure is fine and good.
The bad part is that the user is not given control to modify the hash
and attest as if it were the original so that he can insert his own
code, debug, modify etc.
(All that is needed is a debug option in the BIOS to do this that only
the user can change, via BIOS setup.)

@_date: 2006-10-10 12:35:16
@_author: Adam Back 
@_subject: TPM & disk crypto 
I was suspecting that as DRM at least appears to one of the main
motivators (along side trojan/malware protection) for trustworthy
computing that probably you will not be able to put the TPM into debug
mode (ie manipulate code without affecting the hash attested in debug
mode).  Ability to do so breaks DRM.
Also bear in mind the vista model where it has been described that
inserting an unsigned device driver into the kernel will disable some
media playback (requiring DRM).  And also the secure (encrypted) path
between trusted agent and video/audio card, and between video/audio
card and monitor/speakers.  The HDMI spec has these features, and you
can already buy HDMI cards and monitors (though I dont know if they
have the encryption features implemented/enabled).
I think generally full user control model will not be viewed
compatible.  Ie there will be a direct conflict between user ability
to debug attested apps and DRM.
So then enters the possibility to debug all apps except special ones
flagged as DRM, but if that technical ability is there, you wont have
wait long for it to be used for all things: file formats locked to
editors, per processor encrypted binaries, rented by the hour software
you cant debug or inspect memory space of etc.
I think the current CPUs / memory managers do not have the ring -1 /
curtained memory features, but already a year ago or more Intel and
AMD were talking about these features.  So its possible the for
example hypervisor extra virtualization functionality in recent
processors ties with those features, and is already delivered?  Anyone
The device driver signing thing is clearly bypassable without a TPM,
and we know TPMs are not widely available at present.  ("All" that is
required is to disable or nop out the driver signature verification in
the OS; or replace the CA or cert it is verified against with your own
and sign your own drivers).  How long until that OS binary patch is

@_date: 2006-09-09 10:01:31
@_author: Adam Back 
@_subject: IGE mode is broken (Re: IGE mode in OpenSSL) 
Hi Ben, Travis
IGE if this description summarized by Travis is correct, appears to be
a re-invention of Anton Stiglic and my proposed FREE-MAC mode.
However the FREE-MAC mode (below described as IGE) was broken back in
Mar 2000 or maybe earlier by Gligor, Donescu and Iorga.  I recommend
you do not use it.  There are simple attacks which allow you to
manipulate ciphertext blocks with XOR of a few blocks and get error
recovery a few blocks later; and of course with free-mac error
recovery means the MAC is broken, because the last block is
There is some more detail here:

@_date: 2006-09-09 17:21:51
@_author: Adam Back 
@_subject: IGE mode is broken (Re: IGE mode in OpenSSL) 
Well looking at the paper by Gligor in their mode submission to NIST
on IGE, it appears rather that our FREE-MAC was a re-invention of IGE!
Apparently according to Gligor IGE was proposed by Carl Campbell in
Feb 1977, about the same time as CBC mode was proposed.  Gligor et al
wrote the mode-submission for IGE in Nov 2000.
I am not sure about the proofs in the IGE-spec paper, but at least the
proofs about IGE at least must be flawed somehow because the sci.crypt
post shows a a class of known plaintext modifications that exhibits
error recovery.  I worked through it on paper at the time, and as far
as I can see it trivially breaks IGE/FREE-MAC.  No doubt there are
other variations so there are lots of permutations you can do in
rearranging the ciphertext such that the "integrity check" still

@_date: 2007-02-14 18:35:53
@_author: Adam Back 
@_subject: see also credentica announcement about U-prove (Re: IBM donates new privacy tool to open-source) 
Related to this announcement, credentica.com (Stefan Brands' company)
has released "U-Prove", their toolkit & SDK for doing limited-show,
selective disclosure and other aspects of the Brands credentials.
(Also on Stefans blog I believe Brands credentials are considerably more computationally
efficient and more general/flexible than Camenisch credentials.
(Re Hal's comment on the patent status of Camenisch credentials, as
far as I know patents apply to both systems).
Looks like you can obtain an evaluation copy of U-prove also.

@_date: 2007-02-16 11:14:39
@_author: Adam Back 
@_subject: announce: credlib library with brands and chaum credentials (Re: see also credentica announcement about U-prove) 
I implemented Chaumian and Brands credentials in a credential library
(C code, using openSSL).  I implemented some of the pre-computation
steps.  Have not made any attempt so far to benchmark it.  But thought
I could take this opportunity to make it public.  I did not try to
optimize so far.  One optimization opportunity at algorithm level, is
you dont need witness indistinguishability on a single attribute
credential, which saves some of the computations.
Ben, if you have a partial implementation of Camenisch credentials,
you could maybe do some comparisons of that against this C
(I previous shared a copy with a few list participants).
The Brands credential paper I used as reference (simpler precis than
the thesis as a source):
A Technical Overview of Digital Credentials, Technical Report, February 2002.
could be useful as a source of quick reference of whats modexp, modinv
steps would be involved in issuing, showing etc, for comparison with
About flexibility and generality I mean Brands has a huge list of
features, like a very efficient observer setting, with cheap
operations suitable for an 8 bit smartcard, limited multi-show (though
linkable, there is an online credential refresh phase if unlinkable is
desired), single show, ability to show formulae, ability to show and
bombine formulae across credentials from different issuers etc.  And
also prove negatives involving attributes, and related technique for
testing a black list of revoked credentials blindly.  I am a bit rusty
about Camenisch, as its been a few years, but from my recollection it
doesnt do most of these things.  Also Brands in the ecash setting
there is a neat technique for making offline respendable coins with
double-spend protection.  (I thought I discovered it, but I asked
Stefan, and its a foot note in the thesis book that I missed, and
turns out it was topic of someone's MSc thesis).
The credlib library so far does unlimited show linkable credentials
(issuing, showing etc) for 0 or more attributes.
The u-prove library does a lot more things, I think, but its java and
I'm more of a C person, though java is interesting in some java device
and j2ee server settings, and for app portability.  I guess I just
like C efficiency.

@_date: 2007-02-20 19:40:29
@_author: Adam Back 
@_subject: private credential/ecash thread on slashdot (Re: announce: credlib library with brands and chaum credentials) 
Credentica (Stefan Brands ecash/credentials) U-prove library and
open source credlib library implementing the same are on slashdot:
Maybe some list readers would like to inject some crypto knowledge
into the discussion.  There is quite some underinformed speculation as critique on the
thread...  Its interesting to see people who probably understand SSL,
SMIME and stuff at least at a power user if not programmer level, try
to make logical leaps about what must be wrong or limited about
unlinkable credential schemes.  Shows the challenges faced in
deploying this stuff.  Cant deploy what people dont understand!

@_date: 2007-02-24 08:58:03
@_author: Adam Back 
@_subject: New digital bearer cash site launched 
I read some of the docs and ecache appears to be based on HMAC
tickets, plus mixes.  The problem I see is that you have to trust the
mix.  Now the documentation does mention that they anticipate 3rd
party mixes, but still you have to trust those mixes also.
And as we know from mixmaster etc., there are attacks on mixes such as
So it seems to me they would achieve much stronger anonymity, using a
blinding based ecash system such as Chaum (patent expired) or Brands.
In this way the anonymity set would be with all of the coins issued
since coin-epoch start, rather than with the mixes used.  And there
would be no trust concerns as the blinding protocols dont require
trust in any servers (even the bank and merchant in collusion cant
identify a coin with its withdrawer).

@_date: 2007-07-03 08:21:57
@_author: Adam Back 
@_subject: remote-attestation is not required (Re: The bank fraud blame game) 
I do not believe the mentioned conflict exists.  The aim of these
calculator-like devices is to make sure that no malware, virus etc can
create unauthorized transactions.  The user should still be able to
debug, and inspect the software in the calculator-like device, or
virtual software compartment, just that installation of software or
upgrades into that area should be under direct explicit user control.
(eg with BIOS jumper required to even make any software change!)
The ring -1 and loss-of-control aspects of TPM are different, they are
saying that you are not really root on your own machine anymore!  In
the sense that if you do load under a debugger the remote party can
tell this and refuse to talk with you.
This remote attestation feature is simply not required for
user-centric, user-controlled security.

@_date: 2007-07-04 03:45:40
@_author: Adam Back 
@_subject: remote-attestation is not required (Re: The bank fraud blame game) 
I think you misread what I said about "BIOS jumper required install".
Ie this is not a one click install from email.  It is something one
user in 10,000 would even install at all!  It would be more like
people who program and install custom BIOSes or something, people who
reverse-engineer security products.  Point is to allow audit of
running code by a few paranoid people to keep things honest.
The whole point of the separate program space is that it DOES NOT get
infested with viruses like windows does.  The software running in it
will be very very simple, have minimal UI, minimal code etc.
Obviously there would be no software connection between anything
received in email and changing the software in the physical or virtual
software compartment.

@_date: 2007-11-05 17:41:57
@_author: Adam Back 
@_subject: forward-secrecy for email? (Re: Hushmail in U.S. v. Tyler Stumbo) 
What about deleting the private key periodically?
Like issue one pgp sub-key per month, make sure it has expiry date etc
appropriately, and the sending client will be smart enough to not use
expired keys.
Need support for that kind of thing in the PGP clients.
And hope your months key expires before the lawyers get to it.
Companies have document retention policies for stuff like
this... dictating that data with no current use be deleted within some
time-period to avoid subpoenas reaching back too far.

@_date: 2007-10-12 14:40:20
@_author: Adam Back 
@_subject: Password hashing 
I would have thought PBKDF2 would be the obvious, standardized (PKCS
 / RFC 2898) and designed for purpose method to derive a key from a
password.  PBKDF2 would typically be based on HMAC-SHA1.
Should be straight-forward to use PBKDF2 with HMAC-SHA-256 instead for
larger key sizes, or for avoidance of SHA1 since the partial attacks
on it.

@_date: 2007-09-18 03:20:14
@_author: Adam Back 
@_subject: open source digital cash packages 
credlib provides Brands' and Chaum credentials, both of which can be
used for ecash.

@_date: 2010-03-24 00:23:10
@_author: Adam Back 
@_subject: "Against Rekeying" 
In anon-ip (a zero-knowledge systems internal project) and cebolla [1]
we provided forward-secrecy (aka backward security) using symmetric
re-keying (key replaced by hash of previous key).  (Backward and
forward security as defined by Ross Anderson in [2]).
But we did not try to do forward security in the sense of trying to
recover security in the event someone temporarily gained keys.  If
someone has compromised your system badly enough that they can read
keys, they can install a backdoor.
Another angle on this is timing attacks or iterative adaptive attacks
like bleichenbacher's attack on SSL encryption padding.  If re-keying
happens before the attack can complete, perhaps the risk of a
successful so far unnoticed adaptive or side-channel attack can be
reduced.  So maybe there is some use.
Simplicity of design can be good too.
Also patching SSL now that fixes are available might be an idea.  (In
my survey of bank sites most of them still have not patched and are
quite possibly practically vulnerable).
[1] [2] On Tue, Mar 23, 2010 at 8:51 PM, Nicolas Williams

@_date: 2010-03-24 09:35:12
@_author: Adam Back 
@_subject: "Against Rekeying" 
Seems people like bottom post around here.
On Tue, Mar 23, 2010 at 8:51 PM, Nicolas Williams
In anon-ip (a zero-knowledge systems internal project) and cebolla [1]
we provided forward-secrecy (aka backward security) using symmetric
re-keying (key replaced by hash of previous key).  (Backward and
forward security as defined by Ross Anderson in [2]).
But we did not try to do forward security in the sense of trying to
recover security in the event someone temporarily gained keys.  If
someone has compromised your system badly enough that they can read
keys, they can install a backdoor.
Another angle on this is timing attacks or iterative adaptive attacks
like bleichenbacher's attack on SSL encryption padding.  If re-keying
happens before the attack can complete, perhaps the risk of a
successful so far unnoticed adaptive or side-channel attack can be
reduced.  So maybe there is some use.
Simplicity of design can be good too.
Also patching SSL now that fixes are available might be an idea.  (In
my survey of bank sites most of them still have not patched and are
quite possibly practically vulnerable).
[1] [2]

@_date: 2013-11-12 10:10:13
@_author: Adam Back 
@_subject: [Cryptography] NIST Randomness Beacon 
(Top posted, so sue me, my text explains itself without the history).
Thats a big cc list.  I think you could create a beacon with bitcoin hash
chain by having miners reveal a preimage for 6 old, consecutive blocks where
the newest of the 6 old blocks is itself 6-blocks confirmed.  (ie reveal
preimage on blocks 7-12.  The xor of those preimages defines a rolling
beacon (new output every block, just with reference to blocks 7-12 relative
to the current block depth).
The security against insider foreknowledge is not fantastic, as its relating
to the trustworthiness of the 6 random miners (which have probabilty of
winning relating to hashpower, which doesnt always relate to

@_date: 2013-10-01 12:08:22
@_author: Adam Back 
@_subject: [Cryptography] are ECDSA curves provably not cooked? (Re: RSA 
That is itself a problem, the curves are in fact, not fully veriably fairly
chosen.  Our current inability to design a plausible mechanism by which this
could have been done is not proof that it was not done.  Also bear in mind
unlike the NSA the crypto community has focused more on good faith (how to
make thing secure) and less on bad faith (how to make things trapdoor
insecure while providing somewhat plausible evidence that no sabotage took
place).  Ie we didnt spend as much effort examining that problem.  Now that
we have a reason to examine it, maybe such methods can be found. Kleptography is a for the open community a less explored field of study.
Conversely it would have been easy to prove that the curve parameters WERE
fairly chosen as Greg Maxwell described his surprise that the seed was big
and random looking:
So the question is rather why on earth if they claim good faith, did they
not do that?  Another plausible explanation that Greg mentions also, is that
perhaps it was more about protecting the then secrecy of knowledge.  eg weak
curves and avoiding them without admitting the rules for which curves the
knew were weak.  Clearly its easier to weaken a system in symmetric way that depends only on
analysis (ie when someone else figures out the class of weak curves they
gain the advantage also, if its public then everyone suffers), vs a true
trapdoor weakening, as in the EC DRBG fiasco.
So if that is their excuse, that the utility of NSA input one can get due to
institutional mentality of secrecy, is hardening but with undisclosed
rationale, I think we'd sooner forgoe their input and have fully open
verifiable reasoning.  Eg maybe they could still prove good faith if they
chose to disclose their logic (which may now be public information anyway)
and the actual seed and the algorithm that rejected all iterations below the
used value.  However that depends on the real algorithm - maybe there is no
way to prove it, if the real seed was itself random.
But I do think it is a very interesting and pressing research question as to
whether there are ways to plausibly deniably symmetrically weaken or even
trapdoor weaken DL curve parameters, when the seeds are allowed to look
random as the DSA FIPS 186-3 ones do.

@_date: 2013-10-01 18:51:59
@_author: Adam Back 
@_subject: [Cryptography] are ECDSA curves provably not cooked? (Re: RSA 
Right but weak parameter arguments are very dangerous - the US national
infrastructure they're supposed to be protecting could be weakened when
someone else finds the weakness.  Algorithmic weaknesses cant be hidden with
confidence, how do they know the other countries defense research agencies
arent also sitting on the same weakness even before they found it.  Thats a
strong disincentive.  Though if its a well defined partial weakening they
might go with it - eg historically they explicitly had a go at in public
requiring use of eg "differential cryptography" where some of the key bits
of lotus notes were encrypted to the NSA public key (which I have as a
reverse-engineering trophy here[1]).  Like for examle they dont really want
foreign infrastructure to have more than 80 bits or something close to the
edge of strength and they're willing to tolerate that on US infratructure
also.  Somewhat plausible.
But the more interesting question I was referring to is a trapdoor weakness
with a weak proof of fairness (ie a fairness that looks like the one in FIPS
186-3/ECDSA where we dont know how much grinding if any went into the magic
seed values).  For illustration though not applicable to ECDSA and probably
outright defective eg can they start with some large number of candidate G
values where G=xH (ie knowing the EC discrete log of some value H they pass
off as a random fairly chosen point) and then do a birthday collision
between the selection of G values and diffrent seed values to a PRNG to find
a G value that they have both a discrete log of wrt H and a PRNG seed. Bearing in mind they may be willing to throw custom ASIC or FPGA
supercomputer hardware and $1bil budgt at the problem as a one off cost.
[1]

@_date: 2013-10-10 15:30:20
@_author: Adam Back 
@_subject: [Cryptography] was this FIPS 186-1 (first DSA) an attemped NSA 
Some may remember Bleichenbacher found a random number generator bias in the
original DSA spec, that could leak the key after soem number of signatures
depending the circumstances.
Its described in this summary of DSA issues by Vaudenay "Evaluation Report
on DSA"
Bleichenbacher's attack is described in section 5.
The conclusion is "Bleichenbacher estimates that the attack would be
practical for a non-negligible fraction of qs with a time complexity of
2^63, a space complexity of 2^40, and a collection of 2^22 signatures.  We
believe the attack can still be made more efficient."
NIST reacted by issuing special publication SP 800-xx to address and I
presume that was folded into fips 186-3.  Of course NIST is down due to the
USG political level stupidity (why take the extra work to switch off the web
server on the way out I dont know).
That means 186-1 and 186-2 were vulnerable.
An even older NSA sabotage spotted by Bleichenbacher?
Anyway it highlights the significant design fragility in DSA/ECDSA not just
in the entropy of the secret key, but in the generation of each and every k
value, which leads to the better (but non-NIST recommended) idea adopted by
various libraries and applied crypto people to use k=H(m,d) so that the
signture is determinstic in fact, and the same k value will only be used
with the same message (which is harmless as thts just reissuing the bitwise
same signature).  What happens if a VM is rolled back including the RNG and it outputs the
same k value to a different network dependeng m value?  etc.  Its just
unnecessarily fragile in its NIST/NSA mandated form.

@_date: 2013-10-14 16:51:43
@_author: Adam Back 
@_subject: [Cryptography] please dont weaken pre-image resistance of SHA3 (Re: 
I think hash functions clearly should try to offer full (256-bit) preimage
security, not dumb it down to match 128-bit birthday collision resistance.
All other common hash functions have tried to do full preimage security so
it will lead to design confusion, to vary an otherwise standard assumption. It will probably have bad-interactions with many existing KDF, MAC,
merkle-tree designs and combined cipher+integrity modes, hashcash (partial
preimage as used in bitcoin as a proof of work) that use are designed in a
generic way to a hash as a building block that assume the hash has full
length pre-image protection.  Maybe some of those generic designs survive
because they compose multiple iterations, eg HMAC, but why create the work
and risk to go analyse them all, remove from implementations, or mark as
safe for all hashes except SHA3 as an exception.
If MD5 had 64-bit preimage, we'd be looking at preimages right now being
expensive but computable.  Bitcoin is pushing 60bit hashcash-sha256 preimage
every 10mins (1.7petaHash/sec network hashrate).
Now obviously 128-bits is another scale, but MD5 is old, broken, and there
maybe partial weakenings along the way.  eg say design aim of 128 slips
towards 80 (in another couple of decades of computing progress).  Why design
in a problem for the future when we KNOW and just spent a huge thread on
this list discussing that its very hard to remove upgrade algorithms from
deployment.  Even MD5 is still in the field.
Is there a clear work-around proposed for when you do need 256?  (Some
composition mode or parameter tweak part of the spec?) And generally where
does one go to add ones vote to the protest for not weakening the
2nd-preimage propoerty?

@_date: 2013-10-15 20:22:50
@_author: Adam Back 
@_subject: [Cryptography] please dont weaken pre-image resistance of SHA3 
Are you including truncation in that?  (The question was would SHA3-512
STILL have 256-bit preimage security if it was truncated to 256-bit ie
motivated by a workaround to get a 256-bit output with conventional 256-bit
preimage resistance).

@_date: 2013-10-15 23:59:32
@_author: Adam Back 
@_subject: [Cryptography] please dont weaken pre-image resistance of SHA3 
I think what you just said is an attack of work less than 2^128 is harmless
on both a weakened SHA3 preimage and SHA2.  But that is not an argument for
reducing the preimage strength to 2^128.  Actually I dont understand the
argument for weakening it.  Is there a pointer to a rationale? - so far it
makes no sense - unless its micro-optimization to the massive detriment of
preimage security if you care about that.

@_date: 2013-10-17 14:32:57
@_author: Adam Back 
@_subject: [Cryptography] /dev/random is not robust 
I think the more worrying case is a freshly imaged rack mount server,
immediately generating keys or outputting random numbers to the network or
in response to network queries.
The initial entropy is known (ie 0) and if what little entropy there is is
added in brute-forceable chunks, then an attaker able to observe or get
responses including RNG outputs over the network can keep in step with the
RNG state.  (Eg say 20-bits of entropy at a time, and needing 10-bits of
guessing to account for only seeing one in 1000 of the RNG outputs, then you
can for small cost of 2^32 per interval keep up.)
A similar issue could arise with a VM rollback (to a previous un-initialized
state, or repeating the same random outputs to different messages - eg
breaking DSA without the deterministic k=H(d,m) defense.
Yarrow, and the replacement Fortuna try to address this problem by
accumulating entropy and adding it in bigger lumps..

@_date: 2013-10-19 18:36:33
@_author: Adam Back 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
I know its obvious and you mentioned the risks, but this is in principle a
band-aid or worse; it gives the illusion of entropy in the face of actually
no entropy to an attacker who can readily obtain the serial numbers in
question (eg because the MAC is broadcast on the LAN) or simply brute forced
because the guid is while large, highly structured and sparse.
It would seem safer to fail/stop and depand user action.  I know thats not a
popular decision in a distro/package/boot sequence, but churning out
0-entropy keys disguised as having entropy being E_0( mac ) and such analogs
is a bad outcome and wont be observable via identical P, Q key searches.
People are seemingly in a hurry, or dont care much, dont understan when it
come to packaging and thinking about security.

@_date: 2013-10-21 16:49:27
@_author: Adam Back 
@_subject: [Cryptography] Mail Lists In the Post-Snowden Era 
(Oh yeah, I top-posted by habit, better copy some text above to preclude an
excuse for censorship, there done!)
In the context of crypto lists I prefer open, unmoderated/uncensored.
For example the paranoid might note that its desirable for the forces of
darkness to control the medium by which the open community communicates,
delete the odd message with plausible deniability, use moderation as a
platform to squelch traffic with a little hidden bias, who's going to know. Viz this crypto list went dark for a year or so (ostensibly because -
actually we're not sure - anyway no traffic flowed; and finally the list was
reopened - temporarily, when randombit opened up as an unmoderated list, and
threatened to take over as a continuously flowing open medium.) Then again
another long hiatus on this list followed by only reopening when the world
was exploding with Snowden revelations and recriminations.
Paranoid or not?  If Snowden's episode showed one thing its that people were
too niave, and not paranoid enough.  Its easy pickings to step up for admin
positions in organizations, because momentum and laziness dictates that
others will not, and then some regime of sabotage, discussion shaping,
control can ensue.  Anyone who's done any standardization work, will have
found defense research people holding unlikely chair positions - medical
health care message security - UK defense research agency.  Really?  Why? Probably to make avoid use of forward-secrecy or such like soft-sabotage. People should re-read the declassified old sabotage manual and dwell on what
could be done with $250m/year against open discussion forum, protocols, open
source software, chairman/organizational positions etc.  Because, its
probaly be actively done right now.  Accident that android is using crap
ciphersuites - or plausibly deniable sabotage.
(copied to the unmoderated list)

@_date: 2013-09-14 20:38:37
@_author: Adam Back 
@_subject: [Cryptography] RSA equivalent key length/strength 
100-11 = 89 bits.  Bitcoin is pushing 75 bits/year right
now with GPUs and 65nm ASICs (not sure what balance).  Does that place ~2000
bit modulus around the safety margin of 56-bit DES when that was being
argued about (the previous generation NSA key-strength sabotage)?
Anyone have some projections for the cost of a TWIRL to crack 2048 bit RSA? Projecting 2048 out to a 2030 doesnt seem like a hugely conservative
estimate.  Bear in mind NSA would probably be willing to drop $1b one-off to
be able to crack public key crypto for the next decade.  There have been
cost and performance, power, density improvements since TWIRL was proposed. Maybe the single largest employer of mathematicians can squeeze a few
incremetal optimizations of the TWIRL algorithm or implementation strategy.
Tin foil or not: maybe its time for 3072 RSA/DH and 384/512 ECC?

@_date: 2013-09-15 13:47:13
@_author: Adam Back 
@_subject: [Cryptography] prism proof email, namespaces, and anonymity 
Well you could certainly allow people to opt-in to receiving anonymous
email, send them a notification mail saying an anonymous email is waiting
for them (and whatever warning that it could be a nastygram, as easily as
the next thing).
People have to bear in mind that email itself is not authenticated - SMTP
forgeries still work - but there are still a large number of newbies some of
whom have sufficiently thin skin to go ballistic when they realize they
received something anonymous and not internalized the implication of digital
At ZKS we had a pseudonymous email system.  Users had to pay for nyms (a
pack of 5 paid per year) so they wouldnt throw them away on nuisance pranks
too lightly.  They could be blocked if credible abuse complaint were
Another design permutation I was thinking could be rather interesting is
unobservable mail.  That is to say the participants know who they are
talking to (signed, non-pseudonymous) but passive observers do not.  It
seems to me that in that circumstance you have more design leverage to
increase the security margin using PIR like tricks than you can with
pseudonymous/anonymous - if the "contract" is that the system remains very
secure so long as both parties to a communication channel want it to remain
that way.
There were also a few protocols for to facilitate anonymous abuse resistant
emails - user gets some kind of anonymously refreshable egress capability
token.  If they abuse they are not identified but lose the capability.  eg
Finally there can be different types of costs for nyms and posts - creating
nyms or individual posts can cost real money (hard to retain pseudonymity),
bitcoin, or hashcash, as well lost reputation if a used nym is canceled.

@_date: 2013-09-25 14:25:06
@_author: Adam Back 
@_subject: [Cryptography] forward-secrecy >=2048-bit in legacy 
This is all ugly stuff, and probably < 3072 bit RSA/DH keys should be
deprecated in any new standard, but for the legacy work-around senario to
try to improve things while that is happening:
Is there a possibility with RSA-RSA ciphersuite to have a certified RSA
signing key, but that key is used to sign an RS key negotiation?
At least that was how the export ciphersuites worked (1024+ bit RSA auth,
512-bit export-grade key negotation).  And that could even be weakly forward
secret in that the 512bit RSA key could be per session.  I imagine that
ciphersuite is widely disabled at this point.
But wasnt there also a step-up certificate that allowed stronger keys if the
right certificate bits were set (for approved export use like banking.)
Would setting that bit in all certificates allow some legacy server/browsers
to get forward secrecy via large, temporary key negotiation only RSA keys? (You have to wonder if the 1024-bit max DH standard and code limits was bit
of earlier sabotage in itself.)

@_date: 2013-09-30 10:02:09
@_author: Adam Back 
@_subject: [Cryptography] TLS2 
If we're going to do that I vote no ASN.1, and no X.509.  Just BNF format
like the base SSL protocol; encrypt and then MAC only, no non-forward secret
ciphersuites, no baked in key length limits.  I think I'd also vote for a
lot less modes and ciphers.  And probably non-NIST curves while we're at it. And support soft-hosting by sending the server domain in the client-hello. Add TOFO for self-signed keys.  Maybe base on PGP so you get web of trust,
thogh it started to get moderately complicated to even handle PGP

@_date: 2013-09-30 11:47:37
@_author: Adam Back 
@_subject: [Cryptography] TLS2 
I think lack of soft-hosting support in TLS was a mistake - its another
reason not to turn on SSL (IPv4 addresses are scarce and can only host one
SSL domain per IP that means it costs more, or a small hosting company can
only host a limited number of domains, and so has to charge more for SSL):
and I dont see why its a cost worth avoiding to include the domain in the
client hello.  There's an RFC for how to retrofit softhost support via
client-hello into TLS but its not deployed AFAIK.
The other approach is to bump up security - ie start with HTTP, then switch
to TLS, however that is generally a bad direction as it invites attacks on
the unauthenticated destination redirected to.  I know there is also another
direction to indicate via certification that a domain should be TLS only,
but as a friend of mine was saying 10 years ago, its past time to deprecate
HTTP in favor of TLS.
Well clearly passwords are bad and near the end of their life-time with GPU
advances, and even amplified password authenticated key exchanges like EKE
have a (so far) unavoidable design requirement to have the server store
something offline grindable, which could be key stretched, but thats it. PBKDF2 + current GPU or ASIC farms = game over for passwords.
However whether its password based or challenge response based, I think we
ought to address the phish problem for which actually EKE was after all
designed for (in 1992 (EKE) and 1993 (password augmented EKE)).  Maybe as
its been 20 years we might actually do it.  (Seems to be the general rule of
thumb for must-use crypto inventions that it takes 20 years until the
security software industry even tries).  Of course patents ony slow it down. And coincidentally the original AKE patent expired last month.  (And I
somehow doubt Lucent, the holder, got any licensing revenue worth speaking
about between 1993 and now).
By pinning the EKE or AKE to the domain, I mean that there should be no MITM
that can repurpose a challenge based on phish at telecon.com to telecom.com,
because the browser enforces that EKE/AKE challenge reponse includes the
domain connected to is combined in a non-malleable way into the response. (EKE/AKE are anyway immune to offline grinding of the exchanged messags.)
Clearly you want to tie that also back to the domains TLS auth key,
otherwise you just invite DNS exploits which are trivial across ARP
poisoning, DNS cache-poisoning, TCP/UDP session hijack etc depending on the
network scenario.
And the browser vendors need in the case of passwords/AKE to include a
secure UI that can not be indistinguishably pasted over by carefully aligned
javascript popups.
(The other defense with securid and their clones can help prop up
While certs are a complexity it would be nice to avoid, I think that
reference to something external and bloated can be a problem, as then like
now you pollute an otherwise clean standard (nice simple BNF definition)
with something monstrous like ASN.1 and X.500 naming via X.509.  Maybe you
could profile something like openPGP though (it has its own crappy legacy
they're onto v5 key formats by now, and some of the earlier vs have their
own problems, eg fingerprint ambiguity arising from ambiguous encoding and
other issues, including too many variants, extra mandatory/optional
extensions.) Of course the issue with rejecting formats below a certain
level is the WoT is shrunk, and anyway the WoT is also not that widely used
outside of operational security/crypto industry circes.  That second
argument may push more towards SSH format keys which are by comparison
extremely simple, and are recently talking about introducing simple
certification as I recall.

@_date: 2013-09-30 12:27:43
@_author: Adam Back 
@_subject: [Cryptography] three crypto lists - why and which 
I am not sure if everyone is aware that there is also an unmoderated crypto
list, because I see old familiar names posting on the moderated crypto list
that I do not see posting on the unmoderated list.  The unmoderated list has
been running continuously (new posts in every day with no gaps) since mar
2010, with an interesting relatively low noise, and not firehose volume.
The actual reason for the creation of that list was Perry's list went
through a hiatus when Perry stopped approving/forward posts eg
 at metzdowd.com/
originally Nov 2009 - Mar 2010 (I presume the mar 2010 restart was motivated
by the creation of randombit list starting in the same month) but more
recently sep 2010 to may 2013 gap (minus traffic in aug 2011).
I have no desire to pry into Perry's personal circumstances as to why this
huge gap happened, and he should be thanked for the significant moderation
effort he has put into create this low noise environment, but despite that
it is bad for cryptography if people's means of technical interaction
spuriously stops.  Perry mentioned recently that he has now backup
moderators, OK so good.
There is now also the cypherpunks list which has picked up, and covers a
wider mix of topics, censorship resistant technology ideas, forays into
ideology etc.  Moderation is even lower than randombit but no spam, noise
slightly higher but quite reasonable so far.  And there is now a domain name
that is not al-quaeda.net (seriously?  is that even funny?): cpunks.org.  At least I enjoy it and see some familiar names posting last seen decade+
Anyway my reason for posting was threefold: a) make people aware of
randombit crypto list, b) rebooted cypherpunks list (*), but c) about how to
use randombit (unmoderated) and metzdowd.  For my tastes sometimes Perry will cut off a discussion that I thought was
just warming up because I wanted to get into the detail, so I tend more
prefer the unmoderated list.  But its kind of a weird situaton because there
are people I want views and comments from who are on the metzdowd list who
as far as I know are not on the crypto list, and there's no convenient way
to migrate a conversation other than everyone subscribing to both.  Cc to
both perhaps works somewhat, I do that sometimes though as a general
principle it can be annoying when people Cc to too many lists.
Anyway thanks for your attention, back to the unmoderated (or moderated)

@_date: 2014-01-07 04:41:08
@_author: Adam Back 
@_subject: [Cryptography] NSA co-chair claimed sabotage on CFRG list/group 
I am not sure people are aware and I suppose I am going to stick my neck out
and make it my problem to draw the lists attention to it, but the co-chair
of IRTF CFRG (where Dan Bernstein forwarded the above quote from) is an NSA
employee, and there was a call to remove him from that role on the basis
that the NSA is now openly known to be sabotaging internet security
standards.  And also on the basis of several other specific complaints of
claimed likely sabotage looking back with this new information (with
implications like the above observation by DJB, but in relation to proposing
insecure changes, misrepresenting the groups opinion etc).  The claims are
all spelled out if you want to read below.
Lars who is the person who through IRTF process was to review the question,
and concluded he would leave things as they are with various justifications
quoted below by Trevor.
I support whole-heartedly what Trevor said in response (below) and I
encourge people to read it.  A bit of sunlight might help if the IAB gets
involved perhaps.  Whethere or not there is anything provable is not the
The comments on this relatively long thread on CFRG got a little weird and
hard to follow motives for participants comments in places to my reading. Maybe several parties with different slants and motives countervailing the
public interest.  Or just rude "pragmatists" (an exceedingly dangerous
species of engineer in crypto or privacy areas in my experience).
Hi Lars,
Thanks for considering this request.
Of course, I'm disappointed with the response.
I brought to your attention Kevin's record of technical mistakes and
mismanagement over a two year period, on the major issue he has
handled as CFRG co-chair.  You counted this as a single "occurrence",
and considered only the narrow question whether it is "of a severity
that would warrant an immediate dismissal".
I appreciate your desire to be fair to Kevin and give him the benefit
of the doubt.  But it would be better to consider what's best for
CFRG.  CFRG needs a competent and diligent chair who could lead review
of something like Dragonfly to a successful outcome, instead of the
debacle it has become.
I also raised a conflict-of-interest concern regarding Kevin's NSA
employment.  You considered this from the perspectives of:
  (A) Kevin's ability to subvert the group's work, and
  (B) the impact on RG participation.
Regarding (A), you assessed that IRTF chairs "are little more than
group secretaries" who "do not wield more power over the content of
the ongoing work than other research group participants".
That's a noble ideal, but in practice it's untrue.  Chairs are
responsible for creating agendas, running meetings, deciding when and
how to call for consensus, interpreting the consensus, and liaising
with other parties.  All this gives them a great deal of power in
steering a group's work.
You also assessed that the IETF/IRTF's "open processes" are an
adequate safeguard against NSA subversion, even by a group chair.  I'm
not sure of that.  I worry about soft forms of sabotage like making
Internet crypto hard to implement securely, and hard to deploy widely;
or tipping groups towards dysfunction and ineffectiveness.  Since
these are common failure modes for IETF/IRTF crypto activities, I'm
not convinced IETF/IRTF process would adequately detect this.
Regarding (B), you judged this a "tradeoff" between those who would
not participate in an NSA-chaired CFRG (like myself), and those
"affiliated with NSA" whom you presume we would "eliminate" from
Of course, that's a bogeyman.  No-one wants to prevent anyone else
from participating.
But the chair role is not a right given to every participant, it's a
responsibility given to those we trust.  The IETF/IRTF should not
support a chair for any activity X that has a strong interest in
sabotaging X.  This isn't a "slippery slope", it's common sense.
Finally, I think Kevin's NSA affiliation, and the recent revelations
of NSA sabotage of a crypto standard, raises issues you did not
You did not consider the cloud of distrust which will hang over an
NSA-chaired CFRG, and over the ideas it endorses.
You also did not consider that as the premier Internet standards
organization, the IETF/IRTF's actions here will make an unavoidable
statement regarding the acceptability of such sabotage.
We have the opportunity to send a message that sabotaging crypto
standards is unacceptable and destroys public trust in those
organizations in a way that has real consequences.  Or we send a
message that it's no big deal.
This is a political consideration rather than a technical one, but it
needs to be considered.  We're sending a message either way.
I understand there's no formal appeal process, but these issues are of
great importance to the IRTF and IETF, and would benefit from the
perspective IAB possesses.
I would appreciate if the IAB would consider reviewing this issue and
expressing its judgement.
(a couple comments below)
Dragonfly discussions started in December 2011.  David's timeline
begins in October 2012, skipping:
  * The early critical feedback which Kevin ignored [1]
  * Kevin's "nitpicking detail" which breaks the protocol's security [2]
  * Kevin's cheerleading for a protocol whose use cases and
alternatives he made no effort to understand [3]
The chair is far more than a "group secretary".  As RFC 2014 section 5.3
The Research Group Chair is concerned with making forward progress in
the areas under investigation, and has wide discretion in the conduct
of Research Group business.  [...] The Chair has ultimate responsibility
for ensuring that a Research Group achieves forward progress.
Cfrg mailing list
Cfrg at irtf.org

@_date: 2014-01-12 11:07:55
@_author: Adam Back 
@_subject: [Cryptography] Advances in homomorphic encryption 
So cryptdb is mostly using encrypted search (think deterministic or
convergent encryption, encrypted search terms, encrypted indexes) like
Wagner et al's original paper, but with various relaxations and extensions.. Cryptdb model for selective security relacation is there are different
encryptions, and if the querier needs to do some more advanced query he
releases keys allowing a more flexible but less secure search.  Their main
result is that surprisingly you can execute most of SQL by doing that. Homomorphic addition is one of the modes, though the rest are all symmetric
You maybe familiar with OPE order-preserving (symmetric) encryption (for
comparisons).  Its definitely short of semantic security, that you
mentioned, its significantly weak if used in a context with ability to use
it adapativly against an oracle (the database).
Its just what you can do now, practically; if/and until someone manages to
make FHE practically efficient.  I think the original poster of this thread
said "order of magintude slower" I think that is a gross understatement, I
believe its more like "7 or 8 orders of magnitude slower".  But then I didnt
keep up with the latest benchmark and optimization papers.  They could
really do with a standarized benchmark like time to compute one block of
SHA256, on a common desktop/server CPU.  I mean I'd take practical latency
on a 1000 GPU cluster as an interesting step, i think we're still many
orders of magnitude out from that.  Be interested in corrections if I am
over stating that.
I am not nay saying FHE, I think it has paradigm changing possibilities.  It
allows many of the things that the trustworthy computing system can do up to
hardware tamper assurance and manufacturer certification trust, but with end
to end security for the person with the keys.  Just trying to point out the
distance from here and practicality.

@_date: 2016-05-09 00:54:54
@_author: Adam Back 
@_subject: [Cryptography] Proof-of-Satoshi fails Proof-of-Proof. 
Ed25519 (which I believe denotes EdDSA with Edwards 25519 curve) is
actually Elliptic Curve Schnorr and not DSA at all.
It does however use the analog of RFC6979 though much simpler.
Neither is protected against signer malleability because the use of
the deterministic nonce is not detectable to the verifier in either
I consider Segregated Witness quite elegant and the robust solution to
malleability (which extends beyond signatures).  The best way to avoid
malleability is to omit the Script Signature
from the hash which forms the transaction ID - that is what Segregated
Witness does.  The other changes it introduces are architecturally
quite useful.
