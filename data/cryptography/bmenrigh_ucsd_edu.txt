
@_date: 2008-01-01 02:02:24
@_author: Brandon Enright 
@_subject: Storm, Nugache lead dangerous new botnet barrage 
============================== START ==============================
On Fri, 28 Dec 2007 09:06:44 -0800 or thereabouts "' =JeffH '"
Storm made a pretty significant comeback this week:
Note that those graphs are *only* from the peers that speak encrypted
Overnet.  If you include all the legacy Storm bots out there that still
speak the unencrypted variant Storm is getting back up to its heyday

@_date: 2007-10-18 19:22:12
@_author: Brandon Enright 
@_subject: fyi: Storm Worm botnet numbers, via Microsoft 
Detailed analysis of the Storm network, how it works, its size, etc is
being activly worked on by several research groups.  Storm is nowhere near
50 million nodes and never was.
I will be presenting /some/ of this work at Toorcon in San Diego this
The presentation is not academic paper quality and takes more of a
code-monkey approach to the network.  Real (sane and substantiated)
numbers, stats, and graphs will be presented.  To the best of my knowledge,
it will be the first publicly released estimates of the size of the network
with actual supporting data and evidence.

@_date: 2007-10-23 21:41:53
@_author: Brandon Enright 
@_subject: fyi: Storm Worm botnet numbers, via Microsoft 
Things went pretty smooth.  Storm is a complicated and evolving beast so a
50 minute talk can't really go into the depth that is needed to really
understand how it works.  There weren't any other presentations at Toorcon
but it's a pretty hot topic so there should be more talks and papers coming
out from various researchers in the coming weeks and months.
It seems like whenever anyone says anything about Storm, the story gets
picked up by some news service and makes its way to Slashdot.
They are:
The link to the historical trends of the network is here:
It can be very hard to track the size of a botnet, even in the case of
Storm where I'm crawling the network.  Technologies like NAT can
significantly complicate things.
for a discussion on tracking the size of botnets.
My slides should provide adequate detail for someone to understand how to
interpret the graphs and data.  For specific questions, feel free to email
me directly.

@_date: 2007-09-02 21:02:19
@_author: Brandon Enright 
@_subject: World's most powerful supercomputer goes online 
Actually the stormworm network illustrates this example perfectly.  As with
most DHT based P2P networks, stormworm suffers from latent/stale node data
still in the memory of other nodes.  Asside from the overnet peer bootstrap
files for each stormworm node, the list of nodes in the network is
distributed in memory across all the nodes.
Stormworm is especially bad because the authors didn't take the latent
data problem into account.  There is no built-in mechanism for a botted
host to remove dead peers from their list in memory.  With tens of
thousands of nodes, IPs of machines that were infected and cleaned weeks
ago still occasionally show up.  I suspect this behavior is the primary
source of the ridiculously high (and inaccurate) estimates for the size of
the stormworm botnet.

@_date: 2009-07-04 10:12:02
@_author: Brandon Enright 
@_subject: MD6 withdrawn from SHA-3 competition 
It wasn't entirely clear to me if it really was withdrawn.  Ron Rivest
posted on behalf of the MD6 team some thoughts on MD6 performance and
specifically suggested/requested that NIST ask for submitted algorithms
to be "provably resistant to differential attacks".
The logic was that MD6 is slow because the high number of rounds is
needed in their proof.  They won't tweak/submit a version that doesn't
meet this requirement of theirs and based on the current contest
requirements, they can't be competitive speed-wise without losing their
proof of resistance to differential attacks.  Unless the contest
changes to require such a proof, there is no point in moving MD6

@_date: 2009-05-08 02:28:06
@_author: Brandon Enright 
@_subject: 80-bit security? (Was: Re: SHA-1 collisions now at 2^{52}?) 
On Wed, 6 May 2009 20:54:34 -0400
On breaking DES the paper says:
"As explained above, 40-bit encryption provides inadequate
protection against even the most casual of intruders, content to
scavenge time on idle machines or to spend a few hundred dollars.
Against such opponents, using DES with a 56-bit key will provide a
substantial measure of security. At present, it would take a year
and a half for someone using $10,000 worth of FPGA technology to
search out a DES key. In ten years time an investment of this size
would allow one to a DES key in less than a week."
This is surprising accurate.  As Sandy Harris pointed out,
 is selling about $10k worth of FPGA
technology to crack DES in about 6.4 days:
"With further optimization of our implementation, we could achieve a
clock frequency of 136MHz for the brute fore attack with COPACOBANA.
Now, the average search time for a single DES key is less than a week,
precisely 6.4 days. The worst case for the search has been reduced to
12.8 days now."
Now, even assuming 64 bits is within reach of modern computing power, I
still think it is naive to assume that computing power will continue to
grow to 80 or more bits any time soon.  The energy requirements for
cycling a 80 bit counter are significant.  We are likely to get to a
point where the question is not "how parallel a machine can you afford
to build?" but rather "how much heat can you afford to dissipate?".

@_date: 2010-07-09 21:33:23
@_author: Brandon Enright 
@_subject: [TIME_WARP] 1280-Bit RSA 
I looked at the GNFS runtime and plugged a few numbers in.  It seems
RSA Security is using a more conservative constant of about 1.8 rather
than the suggested 1.92299...
So using 1.8, a 1024 bit RSA key is roughly equivalent to a 81 bit
symmetric key.  Plugging in 1280 yields 89 bits.
I'm of the opinion that if you take action to improve security, you
should get more than 8 additional bits for your efforts.  For example,
1536 shouldn't be that much slower but gives 96 bits of security.
For posterity, here is a table using 1.8 for the GNFS constant:
RSA    Symmetric
256      43.7
512      59.8
768      71.6
1024     81.2
1280     89.5
1536     96.8
2048     109.4
3072     129.9
4096     146.5
8192     195.1

@_date: 2010-07-11 00:11:15
@_author: Brandon Enright 
@_subject: 1280-Bit RSA 
This is quite interesting.  The post doesn't say but I suspect at the
factoring effort was based on using Quadratic Sieve rather than GNFS.
The difference in speed for QS versus GNFS starts to really diverge with
larger composites.  Here's another table:
RSA       GNFS      QS
256      43.68     43.73
384      52.58     55.62
512      59.84     65.86
664      67.17     76.64
768      71.62     83.40
1024     81.22     98.48
1280     89.46    111.96
1536     96.76    124.28
2048     109.41   146.44
3072     129.86   184.29
4096     146.49   216.76
8192     195.14   319.63
16384    258.83   469.80
32768    342.05   688.62
Clearly starting at key sizes of 1024 and greater GNFS starts to really
improve over QS.  If the 1993 estimate for RSA 1024 was assuming QS
then that was roughly equivalent to RSA 1536 today.  Even improving the
GNFS constant from 1.8 to 1.6 cuts off the equivalent of about 256 bits
from the modulus.
The only certainty in factoring techniques is that they won't get worse
than what we have today.
