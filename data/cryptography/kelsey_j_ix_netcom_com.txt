
@_date: 2001-04-02 22:08:45
@_author: John Kelsey 
@_subject: secure hash modes for rijndael  
The really weird part is, finding collisions in hash functions lets you do
much more interesting attacks than just reading someone's mail, but those
are attacks that you'd do to frame someone or steal money from them, not to
gather intelligence.  By default, I guess NSA is more-or-less trusted with
the ability to steal lots of money, but not with the ability to eavesdrop
on everyone....

@_date: 2001-04-03 20:53:16
@_author: John Kelsey 
@_subject: secure hash modes for rijndael 
Keep in mind, though, that it's much easier to attack a hash function than
a block cipher, because there's *nothing* unknown to the attacker in a hash
function.  And attacks that require, say, 2^{60} adaptive chosen inputs
against a hash function are more-or-less practical; similar attacks against
a block cipher are ridiculously academic.
--John Kelsey, kelsey.j at ix.netcom.com

@_date: 2001-04-25 09:39:06
@_author: John Kelsey 
@_subject: Requesting feedback on patched RC4-variant 
One interesting point which I haven't seen discussed much:
the way you have to use stream ciphers to avoid reuse of
keystream allows you to really restrict the attacks possible
on your MAC.  For example, you have to build your protocol
so that the IV or position in the keystream or whatever
*never* repeats.  But that means that you can ensure that
your MAC's starting state never repeats, either.  Similarly,
the recipient must only accept a transmitted message with a
given IV (or whatever) once, which also works nicely with a
stream cipher.
 --John Kelsey
   k.e.l.s.e.y.(dot).j.(at).i.x.(dot).n.e.t.c.o.m.(dot).c.o.m
        PGP: 5D91 6F57 2646 83F9  6D7F 9C87 886D 88AF
  ``Slavery's most important legacy may be a painful insight
  into human nature and into the terrible consequences of
  unbridled power.'' --Thomas Sowell, _Race and Culture_

@_date: 2001-06-06 04:03:10
@_author: John Kelsey 
@_subject: Starium (was Re: article: german secure phone) 
I think you can get away from the network effects by
providing a service along with your hardware.  You have a
call center, and (let's say) a unique public key in each
secure cellphone, known by the call center.  All phone calls
to a secure cellphone use some number that gets routed to
your call center; the call center then calls the secure
cellphone, and establishes a secure connection.  Similarly,
calls from the cellphone first go to the call center, and
then out to the recipient.  This ensures that:
a.  The revenue model for this system is a lot more sane
than just selling phones.
b.  The secure cellphones get first-class over-the-air
privacy without having to fix the rest of the world's
communications standards.
c.  The whole thing is interoperable with the rest of the
world's phones, and at least the part of the conversation
going over the air from the call center to the secure
cellphone is always encrypted.  This gets rid of the really
huge vulnerability of people silently receiving and
recording your cellphone calls, and thus gives you about
90% of what you really need from a secure cellphone system
right away.
d.  It's easy to establish end-to-end secure communications
between two such phones.  But you still get the main
security benefit even when people call you on regular
e.  It seems to me like the regulatory hurdles for doing
this would be minimal.  And you can totally comply with any
court-ordered wiretaps with a smile.  Of course, with
competent protocol design, such wiretaps require a
detectable man-in-the-middle attack if both parties are
using secure cellphones.
Is there some reason why this is an unreasonable thing to
do?  For years now, this has seemed to me like the obvious
way to get secure cellphone service going in the real world.
Crypto asides:
a.  You can do the whole system with only symmetric crypto,
if you don't mind being totally dependent on the call
center/KDC being honest.  By recording the shared keys for
commonly-called numbers with secure phones on the other end,
you can even get most of the man-in-the-middle protection
you need.  This may get the cost down somewhat.
b.  The main protection this scheme offers is protection
from people without wiretap orders.  But I suspect that
quite a bit of the eavesdropping on cellphones is by exactly
this group of people, whether they happen to wear badges or
 --John Kelsey
   k.e.l.s.e.y.(dot).j.(at).i.x.(dot).n.e.t.c.o.m.(dot).c.o.m
        PGP: 5D91 6F57 2646 83F9  6D7F 9C87 886D 88AF
  ``Slavery's most important legacy may be a painful insight
  into human nature and into the terrible consequences of
  unbridled power.'' --Thomas Sowell, _Race and Culture_

@_date: 2001-06-26 12:26:45
@_author: John Kelsey 
@_subject: crypto flaw in secure mail standards 
Basically, Don's attack amounts to showing that you can't safely use
PGP-signed or SMIME-signed messages by themselves to make a secure protocol
for contract signing.  It's a good thing to point out, but the basic
problem is hardly new--everyone knows that you can't, in general, make a
protocol between two machines secure when each signs their messages, but
neither includes anything to prevent cut-and-paste or replay attacks.  Now,
the specifics of how and whether the attack works depend on the details of
the messages--if each party quotes the whole of the previous message in
each new message, the attack will fail.  If each party sticks a timestamp
into the body of the message, some but not all attacks will fail.  (We
still fall prey to the interleaving attack, where Alice runs the
contract-signing by e-mail protocol with Bob and Charlie at the same time,
and Bob gets a signature on a statement "It's a deal!", and then gives that
to Charlie to claim that Alice said the same thing to him.
Also, note that anyone who understands what assurances the crypto can and
cannot provide here will not end up convinced that Alice intended to sign a
contract, they will end up convinced that they haven't enough evidence to
decide whether Alice intended to sign a contract.  (Unfortunately, there's
not much reason to expect a judge to know a lot about cryptography, so who
knows how this would really play out in court?)
It's easy enough to fix in various ways, by making the sequence of messages
between Alice and Bob part of a cryptographic protocol--make up a unique
session ID for this conversation, make sure Alice and Bob agree on it, and
then include that plus a message sequence number in each successive message
in the conversation.  Or whenever an e-mail is replying to an earlier
e-mail, include the SHA1 hash of the replied-to e-mail in this e-mail's
signature.  But none of that fixes the underlying problem, which is that
secure and signed e-mail is fundamentally a different thing that a
contract-signing protocol.
--John Kelsey, kelsey.j at ix.netcom.com

@_date: 2001-03-31 21:41:10
@_author: John Kelsey 
@_subject: secure hash modes for rijndael 
============================== START ==============================
-----BEGIN PGP SIGNED MESSAGE-----
It's easy to cause collisions, if you do this, because you can use
the padding scheme to cause two unequal messages to collide.  That
is, M is some message that will need padding P.  (For example, M is 127
bits long, and P is a single one bit.)
M' = M||P  (For example, M' is 128 bits long.)
M' can't end in a one bit, and we can choose M not to, either. M <> M', but after the padding step, you'll get the same actual input
to the hash function, and as a result, you'll get a collision.  This
is why padding rules for hash functions always specify that padding
must *always* be done.
You also need the length included in the hash calculation somewhere.
So, if I'm understanding this, it's:
A = CBC-MAC_{K0}(message+padding)
B = CBC-MAC_{K1}(message+padding)
X = E_A(B)
Y = E_{B xor 00..00ff}(A)
hash output is (X,Y)
Assuming we fix the padding scheme so that there's always at least
one bit of padding, what can we do with this?  We can get a collision
either in the final step of going from (A,B) to (X,Y), or by getting
two messages with the same (A,B) values.  So, here's my solution for getting a collision in this scheme:
a.  Generate a set of 2^{64} messages, M_{0,1,...,2^{64}-1} which all
have the same ending value for A.  I'll show how to do this below,
but it's *really* easy, because we know the key and we can do
decryption.  b.  Of this set, we expect a pair of messages to collide in B, by the
birthday paradox.  That pair will collide for the ending value of
(A,B).  Since (X,Y) is just a function of (A,B), that means that the
whole hash collides.
Making a large set of messages with the same ending A value is
simple.  Suppose we have a desired ending A value, which I'll just
call A.  We generate a set of messages, all of the same length, all
with a full padding block P at the end.  (That padding block is
To generate a message such that CBC-MAC_{K0}(message)==A, we do:
a.  Make a message M_i = R_i, R_j, u, P
where R_i,R_j are random AES blocks, u is still to be determined, and
P is the padding block.
b.  Compute A' = the CBC-MAC before the padding block is processed,
which is c.  Compute A'' = the CBC-MAC after processing M_i as far as R_j.  d.  We now need to solve the equation:
for u, which will tell us what the last block of the message needs to
be.  We can do that easily enough, by computing So, at least that technique is broken.  (There may be more efficient
I've played around with schemes somewhat like this one.  The one I
liked, but haven't been able to convince myself is secure, is:
W,X,Y,Z are the state, K0 is some fixed AES key.  W,X,Y,Z are
initialized to (0,1,2,3).
Pad the message intelligently, including the length.  For each message block of 128 bits, M, do:
At the end, do some complicated munging of these blocks to get an
output, such as:
Let (a,b,c,d) = (W,X,Y,Z)
Process (a,b,c,d) as last four blocks of message.
Output Y,Z
I wasn't able to find an attack on this, but I haven't spent much
time on it, and I've also never been able to convince myself there
wasn't an attack on it.  But it does accomplish the obvious
requirement of making an attacker control many things at once to get
a collision.  I suspect the right way to attack this is to do something clever to
force the states into short cycles or something.  Remember, an
attacker of a 256-bit hash function arguably has close to 2^{128}
work and memory available to mount an attack; he might examine most
of E_{K0} for short cycles or some such.
Anyway, designing hash functions is *hard*.  It seems to me that it's
much harder than designing block or stream ciphers, because there's
nothing unknown to the attacker.  The easiest case is MACs, where you
can design the system to reveal almost nothing to the attacker, and
all your internal state except a 32-bit tag at the end of each
message is totally kept secret.  I worry a bit about this, because AES/Rijndael has such a simple key
schedule.  It's really hard to convince myself you wouldn't be able
to come up with *some* weird property of AES that wouldn't let you
recover a message, but might, say, let you find (key,plaintext) pairs
whose ciphertexts collided relatively quickly.

@_date: 2001-05-31 14:16:56
@_author: John Kelsey 
@_subject: compression & nulls in cryptosystems  
Actually, using CBC more-or-less ruins chosen plaintext attacks.  Each long
message gives you, at most, one block whose value going into the block
cipher you can control (assuming you know the IV ahead of time).  After
that, the blocks going in are XORed with the previous ciphertext, and you
presumably don't know what that will be until you've gotten back that first
ciphertext.  I gave an example in a note I sent a few minutes ago, where keysearch
machines would have little trouble with your scheme: wait for a packet of
some really short length, and build the keysearch machine to decrypt the
whole short packet and run your decompression scheme on it.  These will
trivially distinguish the known packet value from incorrect values.  For differential cryptanalysis, you more-or-less need chosen plaintexts, so
anything that prevents you getting chosen plaintexts is a problem.  For
linear cryptanalysis, you more-or-less need known plaintexts, so anything
that prevents you getting them is a problem.  But if I know whole packets,
and the compression+nulls scheme is public, then I have known plaintext.
So I don't think your scheme helps much against linear cryptanalysis.  DC
is impeded both by your scheme, and by using CBC mode encryption.  Switching from single-DES to triple-DES, or even to DESX (DES with a fixed
64-bit key XORed in before and after encryption), demonstrably increases
security against keysearch attack.  Switching over to DESX costs almost
nothing, and yet has a much bigger impact than compressing your input does,
at least for those short packets.  But CBC-mode does the same thing much more cheaply.

@_date: 2001-05-31 14:00:22
@_author: John Kelsey 
@_subject: The role of compression in cryptosystems 
This is true, but almost certainly irrelevant.  If you're using triple-DES
or AES, nobody who's talking knows how to attack those ciphers with
available resources, even with adaptive chosen plaintext attacks.  (That
is, attacks where you get to send in a plaintext, get it encrypted and sent
back, and then decide what plaintext to request next.)  The difference in
security between LZW-ish compressor output as plaintext or a steady stream
of binary zeros as plaintext is probably really small.  Even in the case of DES, a keysearch machine won't be stopped by
compressing the plaintext.  The compressor output has to be decompressable,
and the decompression routine is public.  In the worst case, you can just
try each key for the first few blocks in the message, see which ones are
consistent with compressed ASCII english text (or whatever you know about
the plaintext), and keep checking later blocks only with the subset of keys
that worked on the first blocks.  This will make the keysearch machine a
little more expensive to build, but not all that much more expensive.  At a
guess, you're increasing the cost of the keysearch machine by a factor of
ten or so, which just isn't very relevant.  (This assumes you've done away
with fixed compression headers; otherwise, the keysearch machine is made a
little easier to build.)  Actually, most block ciphers are designed to be efficient, and to resist
various attacks based on statistical properties that propogate too far
through the cipher.  For example, if you take about 2^{43} 64-bit
plaintexts and feed them into DES under a fixed key, you will be able to
detect a very small bias: the parity of a certain subset of input bits can
be used to predict, with a probability just barely greater than 1/2, the
parity of a certain subset of output bits.  (You have to examine about
2^{43} texts because the bias is so small, it takes this many trials to
detect it with high probability.)  This kind of security doesn't have much
to do with complexity theory, and in fact, I can't recall the last time I
saw a block cipher whose security was claimed on complexity theoretic
grounds.  So, I think you're spending a lot of resources here that are much better
spent somewhere else.  If your system is currently using single-DES for
encryption, you're far smarter to change over to triple-DES (which will
give you real security) than to try all these games with compression and
adding randomness and such.    I'd say that the space of (b) is pretty limited.  The problem is, the
receiving end of the normal communications needs to be able to understand
them pretty quickly, and all that is based on public algorithms.  So no
matter what scheme you have for compressing and adding randomness, I can
run publically available algorithms to decompress it and start getting back
plaintext, and for the overwhelming majority of keys in a keysearch
machine, this will give me garbage of one kind or another.  Indeed, IP
needs to be able to handle fairly short packets, so there will be cases
where 20-30 blocks are the whole message; the attacker can wait for such a
packet, and then feed that to his 30-block-at-a-time keysearch machine.  If that's not true, and your compressor and decompressor are doing
non-public things, then they're part of the cipher, and you've really just
rolled your own new cipher, and are encrypting things twice.  Which is fair
enough, but it's not clear why we ought to do this with your new
compression + nulls cipher, rather than switching from DES to triple-DES,
or doing both triple-DES and AES on the plaintext, or whatever.
--John Kelsey, kelsey.j at ix.netcom.com

@_date: 2001-09-08 22:41:32
@_author: John Kelsey 
@_subject: Field slide attacks and how to avoid them. 
[ To: Perry's Crypto List  Date: 09/08/01 07:35 pm   Subject: Field slide attacks and how to avoid them. ]
I've been noticing a lot of ways you can mess up a cryptographic
protocol due to the "sliding around" of fields within a signed or MACed
message.  The classic example of this is the old attack on PGP
fingerprints, which let you use some odd keysize, and thus get two
different keys (with different keysizes) with the same hash, without
breaking the hash function.  (The raw bits of the two keys are the same,
but the fields are broken up differently.)
The natural way to resist this is to ensure that all information used to
parse a hashed/MACed/signed message is included in the signature.  But I
was curious whether anyone knows of other standard, simple ways to deal
with this problem?
I came up with a couple of very simple ones, thinking about it for a few
a.  Require all fields to include a prepended four-byte length.
b.  Hash each field independently, and instead of using
        hash(field1+field2+field3+...+fieldN)
        hash(hash(field1)+hash(field2)+...+hash(fieldN))
c.  Prepend a "map" of the fields as the first field in a signed or
hashed message.  Thus we authenticate
        hash(fieldMap + field1 + field2 + ... + fieldN)
As long as fieldMap has an unambiguous length, this is guaranteed to
prevent field sliding.  I imagine fieldMap as something like a sequence
of 32-bit integers, the first being a count of fields, the rest being
the offsets of each field.
d.  Encode the fields first, in such a way that there is a single
unambigous field separator between fields.  For example, use the simple
encoding rule that anytime three bytes of successive 0x00s are encoded,
we always insert a 0x01 byte next.  Use four successive 0x00 bytes as
the field separator.   The decoding rules work just the opposite:
Whenever we run into 0x00,0x00,0x00, if the next byte is 0x00, we've hit
a field separator; if it's a 0x01, we discard the 0x01 and continue
Methods (b) and (d) are usable even when the fields are being fed into
the message on the fly.  It wouldn't be hard to do something a little
smarter with (d) to improve its efficiency, but this would only be very
important with very specific and weird messages.  (We might have to
worry about buffer-overrun attacks based on sending an all-zero message,
though.)  It might be useful to specify which field is which, as well,
though obviously there ought not to be any ambiguity in the protocol
about that.  (But it's the sort of thing that might slip in, especially
in a design with many optional fields....)  Both the fieldMap idea and
the encoding of fields idea can be adapted to deal with this in pretty
straightforward ways.
This is the kind of thing that someone must have already dealt with in
detail.  Any references?  Are any of these ideas useful?  There is
nothing very difficult about any of them, and it would be trivial to
come up with dozens of similar fixes.

@_date: 2001-09-08 22:45:14
@_author: John Kelsey 
@_subject: Compression side channel  
[ To: Perry's Crypto List  Date: 09/08/01 07:52 pm   Subject: Compression side channel ]
At Crypto this year, I gave a rump session talk on some research I've
been doing (in my copious spare time) on how compression and encryption
might interact to reduce security.  I've been working on a paper (which
is far from ready to be sent out yet), and I wanted to bounce the basic
ideas around the list, for two reasons:
a.  I think this stuff is actually interesting, and maybe relevant for
some real-world systems.  (Though most of the attacks I've been
considering are basically out in academic-land.)
b.  I'm hoping to find out if anyone else has seen similar work
anywhere.  I've not been able to find any references to this kind of
attack, though once you've had the idea to try it, it's really pretty
straightforward.  (And I know there are a couple of occasional posters
on this list who know a heck of a lot more about compression algorithms
than I do.  Peter, are you listening?)
The basic result: Lossless compression algorithms leak data about their
input in the size of their output.  This is really obvious; if some
inputs didn't compress better than others, we'd either have no
compression, or lossy compression.  However, compressors like Zip
deflate and Unix compress maintain state, which is changed as new bytes
of text are processed.  This state basically is used to make future
texts compress better if they're more like recent texts.
This leads to some really powerful attacks, at least in the
chosen-plaintext model.  We have something like
        E(X) = encrypt(compress(X))
where the encryption preserves length (e.g., RC4 encryption).  Suppose
someone is sending a secret S in these messages, and the attacker gets
to choose some prefix or suffix to send, e.g.
X[0] = S+suffix[0]
X[1] = S+suffix[1]
Well, if the attacker can watch how these compress, he can learn a *lot*
about these secret strings.  Using a slightly more complicated model for
building the X[i], I've experimentally extracted 16- and 32-digit PINs
from messages, using only a few thousand chosen texts and a pretty small
amount of processing power.  (The code to run the attack is written in
Python, so it really doesn't have a lot of speed.)  The gist of the
attack is that I request a bunch of possible 4-digit subsequences in my
messages, and use how well subsequence+S compresses as a way to place
those subsequences in order of likelihood that they appear in S.  I then
try to piece together high-probability candidate sequences for S, based
on making the subsequences fit together.  I end up with an ordered list
of likely S guesses, and can get the right answer in my top 20
candidates about 60-70% of the time with random PINs for S.
There are a bunch of other attacks along these lines.  For example, I
believe a variant of this attack is possible using known plaintexts
only.  And with a much weaker kind of chosen plaintext access, we can do
"string testing," e.g., if we think we know a long subsequence in S, we
can request E(S+guess), and see what the compression ratio looks like.
And we can watch a compressed and encrypted stream, and use the
bandwidth to determine how much redundancy is in the plaintext.  (At
least, we can determine how much redundancy of a kind the compression
algorithm is designed to detect and use is present in the plaintext.)

@_date: 2001-09-08 22:41:32
@_author: John Kelsey 
@_subject: Field slide attacks and how to avoid them. 
[ To: Perry's Crypto List  Date: 09/08/01 07:35 pm   Subject: Field slide attacks and how to avoid them. ]
I've been noticing a lot of ways you can mess up a cryptographic
protocol due to the "sliding around" of fields within a signed or MACed
message.  The classic example of this is the old attack on PGP
fingerprints, which let you use some odd keysize, and thus get two
different keys (with different keysizes) with the same hash, without
breaking the hash function.  (The raw bits of the two keys are the same,
but the fields are broken up differently.)
The natural way to resist this is to ensure that all information used to
parse a hashed/MACed/signed message is included in the signature.  But I
was curious whether anyone knows of other standard, simple ways to deal
with this problem?
I came up with a couple of very simple ones, thinking about it for a few
a.  Require all fields to include a prepended four-byte length.
b.  Hash each field independently, and instead of using
        hash(field1+field2+field3+...+fieldN)
        hash(hash(field1)+hash(field2)+...+hash(fieldN))
c.  Prepend a "map" of the fields as the first field in a signed or
hashed message.  Thus we authenticate
        hash(fieldMap + field1 + field2 + ... + fieldN)
As long as fieldMap has an unambiguous length, this is guaranteed to
prevent field sliding.  I imagine fieldMap as something like a sequence
of 32-bit integers, the first being a count of fields, the rest being
the offsets of each field.
d.  Encode the fields first, in such a way that there is a single
unambigous field separator between fields.  For example, use the simple
encoding rule that anytime three bytes of successive 0x00s are encoded,
we always insert a 0x01 byte next.  Use four successive 0x00 bytes as
the field separator.   The decoding rules work just the opposite:
Whenever we run into 0x00,0x00,0x00, if the next byte is 0x00, we've hit
a field separator; if it's a 0x01, we discard the 0x01 and continue
Methods (b) and (d) are usable even when the fields are being fed into
the message on the fly.  It wouldn't be hard to do something a little
smarter with (d) to improve its efficiency, but this would only be very
important with very specific and weird messages.  (We might have to
worry about buffer-overrun attacks based on sending an all-zero message,
though.)  It might be useful to specify which field is which, as well,
though obviously there ought not to be any ambiguity in the protocol
about that.  (But it's the sort of thing that might slip in, especially
in a design with many optional fields....)  Both the fieldMap idea and
the encoding of fields idea can be adapted to deal with this in pretty
straightforward ways.
This is the kind of thing that someone must have already dealt with in
detail.  Any references?  Are any of these ideas useful?  There is
nothing very difficult about any of them, and it would be trivial to
come up with dozens of similar fixes.

@_date: 2001-09-08 22:45:14
@_author: John Kelsey 
@_subject: Compression side channel  
[ To: Perry's Crypto List  Date: 09/08/01 07:52 pm   Subject: Compression side channel ]
At Crypto this year, I gave a rump session talk on some research I've
been doing (in my copious spare time) on how compression and encryption
might interact to reduce security.  I've been working on a paper (which
is far from ready to be sent out yet), and I wanted to bounce the basic
ideas around the list, for two reasons:
a.  I think this stuff is actually interesting, and maybe relevant for
some real-world systems.  (Though most of the attacks I've been
considering are basically out in academic-land.)
b.  I'm hoping to find out if anyone else has seen similar work
anywhere.  I've not been able to find any references to this kind of
attack, though once you've had the idea to try it, it's really pretty
straightforward.  (And I know there are a couple of occasional posters
on this list who know a heck of a lot more about compression algorithms
than I do.  Peter, are you listening?)
The basic result: Lossless compression algorithms leak data about their
input in the size of their output.  This is really obvious; if some
inputs didn't compress better than others, we'd either have no
compression, or lossy compression.  However, compressors like Zip
deflate and Unix compress maintain state, which is changed as new bytes
of text are processed.  This state basically is used to make future
texts compress better if they're more like recent texts.
This leads to some really powerful attacks, at least in the
chosen-plaintext model.  We have something like
        E(X) = encrypt(compress(X))
where the encryption preserves length (e.g., RC4 encryption).  Suppose
someone is sending a secret S in these messages, and the attacker gets
to choose some prefix or suffix to send, e.g.
X[0] = S+suffix[0]
X[1] = S+suffix[1]
Well, if the attacker can watch how these compress, he can learn a *lot*
about these secret strings.  Using a slightly more complicated model for
building the X[i], I've experimentally extracted 16- and 32-digit PINs
from messages, using only a few thousand chosen texts and a pretty small
amount of processing power.  (The code to run the attack is written in
Python, so it really doesn't have a lot of speed.)  The gist of the
attack is that I request a bunch of possible 4-digit subsequences in my
messages, and use how well subsequence+S compresses as a way to place
those subsequences in order of likelihood that they appear in S.  I then
try to piece together high-probability candidate sequences for S, based
on making the subsequences fit together.  I end up with an ordered list
of likely S guesses, and can get the right answer in my top 20
candidates about 60-70% of the time with random PINs for S.
There are a bunch of other attacks along these lines.  For example, I
believe a variant of this attack is possible using known plaintexts
only.  And with a much weaker kind of chosen plaintext access, we can do
"string testing," e.g., if we think we know a long subsequence in S, we
can request E(S+guess), and see what the compression ratio looks like.
And we can watch a compressed and encrypted stream, and use the
bandwidth to determine how much redundancy is in the plaintext.  (At
least, we can determine how much redundancy of a kind the compression
algorithm is designed to detect and use is present in the plaintext.)

@_date: 2002-08-05 12:58:35
@_author: John Kelsey 
@_subject: Extracting unifrom randomness from noisy source 
The seed idea makes the proof easier, but doesn't really add much, does it?  We have a large set of input strings, whose precise input distribution is
unknown, but whose approximate distribution we know well enough to
reasonably claim some amount of entropy.  Each time we generate an entropy
output, we're sampling one string from our distribution, and applying a
function to it to reduce it to some short, hopefully-random-looking
bitstring.  We're hoping that the function output is going to be so close
to uniformly distributed that a computationally unbounded attacker given a
sequence of these outputs as long as we will get from this RNG during its
whole life can't reliably distinguish between this RNG's outputs and a real
uniformly-distributed set of outputs.  In choosing this function, we're hampered by our ignorance of that input
distribution.  If we knew it precisely, we could design a function that
would be guaranteed to be uniform and independent and all that.  Instead,
what we can reasonably do is select a function so that, for the
overwhelming majority of input distributions consistent with our knowledge
of this input distribution, we will get an approximately uniform output
distribution from our function.  When we choose this function, and whether
we choose it via this short seed or by just picking a convenient function
off the shelf is irrelevant here.  If an opponent were choosing our input
distribution, then it would be important to make him commit to his choice
before we chose our function, but the opponent here is nature or bad luck
or something.  Given our knowledge, choosing a CRC with a random
irreducible polynomial as our function is exactly as reasonable as choosing
one of the standard, off-the-shelf CRC polynomials, for example.  In fact, no sane engineer will choose the function in a way that's truly
independent of the input distribution; he will test the function with the
input distribution as much as possible, and discard it and choose another
if he finds some problem with it.  Surely this doesn't *decrease* security....
There are two points worth considering, here:
a.  If noise is exactly 128 bits wide, this has no effect on the amount of
entropy in the result, of course.  We're just relabeling the symbols,
without altering their distribution.  b.  When noise is much longer, we have to use the encryption mechanism
intelligently, or we'll shoot ourselves in the foot.  For example,
counter-mode has a proof of security that's as nice as the one for
CBC-mode, but we'd better not try to use counter mode to distill entropy
from a long input string.  What we can reasonably do is use CBC-MAC or
XOR-MAC or some such thing, so long as we're not expecting more than 128
bits of entropy from the result.  (In fact, for CBC-MAC, we're losing about
one bit of entropy to internal collisions for many strings that are
plausibly going to be very common in our input distribution.  XOR-MAC looks
good to me, but this is all based on back-of-the-envelope analysis, not
anything very serious yet.)  I claim that the above constructions will be exactly as secure if we choose
the all-zero AES key as if we choose an AES key today uniformly and at
random.  There is no reason to expect any mutual information between my
choice of the all-zero AES key and our specific input distribution.  Can you see a reason why just using a CRC on your input strings wouldn't be
okay?  Certainly, an attacker could choose an input distribution that would
ruin this, but there's not an attacker choosing the input distribution in
practice, and for acceptably long strings, the CRC with any good polynomial
will have a uniform distribution.  The real question would then reduce to
whether or not the input strings were generated by some process that was
going to always have the same CRC value for this polynomial, and it's very
hard to see how that would happen in practice.  To put it into a security-proof framework, imaging choosing the CRC
polynomial at random from the set of irreducible polynomials of the
required size, after the input distribution had been committed to.  --John Kelsey, kelsey.j at ix.netcom.com

@_date: 2002-08-05 23:16:49
@_author: John Kelsey 
@_subject: Extracting unifrom randomness from noisy source 
Of course, we expect to lose the essentially same amount of entropy in
either of these.  It's just a question of where we lose the entropy.
--John "think before you hit the send button" Kelsey

@_date: 2002-08-06 23:10:45
@_author: John Kelsey 
@_subject: Extracting uniform randomness from noisy source 
At 0500 AM 8/6/02 +0000, David Wagner wrote
The thing that struck me, thinking about this, is that the kind of security
proof available for any of these primitives is a poor fit for the actual
problem.  That's pretty-much a recipe for getting proposals that don't work
in practice.
Would it make sense to differentiate the two different things we're trying
to do, here?  a.  Map the entropy from one string sampled from a large set with only
approximately-known distribution to a short string, in such a way that we
have a strong expectation that the short string will have a uniform
b.  Output the result in a way that doesn't leak partial information about
its inputs, and that (if full entropy was provided in the input) isn't
distinguishable from a random value in practice. Also, in a practical system, we're likely to want:
c.  Output the result in a way that is expected to be unconditionally
secure (indistinguishable from random to unbounded attackers) if there was
sufficient entropy in the input, and computationally secure (reducing to
the strength of AES, HMAC-SHA1, etc.) if not.
Er, how much entropy are you assuming in this input?  I see two possibilities:
a.  The first input block is a random 128-bit value, and the rest of the
inputs are known.  Decrypting this with the known key doesn't let you
distinguish the output from a random block, since you can always decrypt
any random 128-bit block in this way and get a random 128-bit block.
b.  The first input block is not a random 128-bit value, and can reliably
be distinguished from one.  In this case, the input just doesn't have full
entropy, and any known function you apply to it with a 128-bit output is
distinguishable from a random output.  A one-way function just makes it
harder to distinguish these outputs, for a computationally-bounded
attacker.  But how important this is depends on our assumptions about the
attacker's abilities; if we assume the attacker can do 110-bit searches,
then he can generally distinguish the output of *any* known function with
only 110 bits of entropy with reasonable probability.  Same comment, though.  If there's a single 128-bit random block in the
middle, with everything else fixed, and it's really random, then the
128-bit output from CBC-MAC with a fixed key won't be distinguishable from
a random 128-bit value.  It's only when you're generating outputs with less
that 128 bits of input entropy that this works.  Ah, this is a nice attack!  But I don't think it scales all the way up as
you're discussing.  Suppose we have 160 bits of entropy spread out as you
describe.  Then, when we do our meet-in-the-middle attack, we get 2^{80}
intermediate values, each of 128 bits, in each list.  With 2^{160} pairs of
intermediate values, each with a probability of 2^{-128} of colliding, we
expect false 2^{32} matches, and one correct one.  Carrying out the same
attack on any random 128-bit value, I'll expect 2^{32} false matches.  Is
there something I'm missing here?  It looks to me like once we get much past 128 bits of entropy, this attack
stops working as a distinguisher.  It's still useful as a way of learning
possibly-secret internal state of the generating machine, but it doesn't
break the RNG.  Right?  This is the really important point, IMO.  We don't yet have a security
proof for doing this job using hash functions or block ciphers or MACs, and
trying to reuse these existing security proofs is not too useful.  (Imagine
trying to use the pseudorandomness proof of AES in counter-mode to solve
this problem!)  The only obvious problems with SHA1 for this job are that it's rather slow
(if we're hashing in lots of samples), and that its actual output
distribution is a little hard to reliably describe.  (For example, is there
any proof that all output values are even possible from the SHA1
compression function, for a given input chaining value?)  If we care about
computationally unbounded attackers, we'd probably like to have some
assurance that they're not going to find trivial flaws in our mechanisms.  When I looked at this problem a couple years ago, I broke it down into the
distilling entropy task and the securely outputing the result task.  This
made things a lot simpler.  Though your conclusions about what function
works here are really strongly determined by your assumptions about
attackers.  My assumptions were something like:
a.  If my input samples have enough entropy to make my outputs random, then
I need to resist computationally unbounded attackers.  (Otherwise, why
bother with distilling entropy; just use a PRNG.)
b.  If my input samples are within some much more forgiving bound, then I
need to resist computationally-bounded attackers.  c.  Attackers with control over some part of my inputs mustn't be able to
cancel out or block entropy in the other parts of the inputs.  --John Kelsey, kelsey.j at ix.netcom.com

@_date: 2002-08-07 09:50:45
@_author: John Kelsey 
@_subject: Extracting uniform randomness from noisy source 
So, the properties we want here are something like
a.  If the input had at least N+epsilon_0 bits of entropy, the N bit output
is indistinguishable from random even to a computationally unbounded opponent.
b.  If the input had at least R+epsilon_1 bits of entropy, the N-bit output
is indistinguishable from random to an attacker who is limited to less than
2^R trial hashes.
Does this look right to you guys?  Are there other subtleties I'm missing?
Are there weaker assumptions that would give us what we need here?  Because
unless I'm missing something big, here, (a) is unattainable from existing
hash functions.  Think about the structure of SHA1 (and all other widely used hash
functions):  we're basically doing an encryption operation using a 512-bit
message block as key, and a chaining variable as IV.  Just this much detail
is enough to prove that we can't ever get the first requirement for some
kinds of entropy input.  Specifically, if we're trying to collect 160 bits
of entropy, and our input strings are padded with at least 512 known bits
at the end, then we will get outputs that are distinguishable from random,
at least if we see any reasonable number of them in a row.  This is true
because of the one-wayness property of the compression function.
Ignore the internals of the SHA1 compression function, and just think of a
block cipher with a 160-bit block and a 512-bit key.  If the block cipher
is E_K(X) for encrypting block X with key K, then our hash compression
function looks like this:
H(X,M) = E_M(X) + X
where + is word-wise addition, or XOR (bitwise addition), or something
similar.  Suppose that X takes on each possible value with equal probability.  For a
single fixed M, H(X,M) loses about half its possible values to collisions.
That is, only about 2^{159} of the 2^{160} possible output values can come
about for a fixed M, regardless of the value in X.  Now, imagine that we have a distribution on our input strings that gives us
full entropy plus a little, but the last 512 bits of each string is known
to the attacker somehow.  Then each output we generate has only about 159
bits of entropy.  A computationally unbounded attacker can try all 2^{160}
possible input values for a given last 512 bits of input to the compression
function, and see which values are impossible.  Each time he observes an
output, he does this.  If he's observing outputs from a real random source,
he has a 1/2 probability of becoming certain that he's NOT observing hash
outputs for each output, as the output has about a 1/2 probability of
taking on some value our hash function couldn't have generated in this
situation.  After 32 outputs, he basically has no doubts about whether he's
watching our hash outputs or outputs from a true random source.
So, at least that first criterion is unattainable with a standard hash
function.  (MD5, SHA1, SHA-256, SHA-512)  A similar argument holds for
RIPE-MD and RIPE-MD160.
A natural thing to try here is HMAC.  HMAC looks like this:
HMAC_K(X) = hash( K xor pad_0 || hash( K xor pad_1 || X ) )
Suppose we have way more entropy than we need, maybe 2^{200} possibilities
for X.  The result of hash(K xor pad_1 || X) is (at best) a
uniformly-distributed 160-bit random number--just what we're hoping for.
But now, we apply that second hash function.  For a known K, only 2^{159}
outputs will be possible, because of the one-way property of the SHA1
compression function with respect to message input blocks.  Suppose the
attacker doesn't start out knowing K, but is computationally unbounded.  In
that case, he will rule out about half the possible keys on each output,
and so after he's seen about 160 outputs (the maximum effective
contribution from that outside instance of K), he will have narrowed the
range down to about one possible key.  He expects to need a few more
outputs to be certain he knows whether he's seeing outputs from our HMAC
function or real random outputs.  (Note that once again, trying to use
another primitive with a different kind of security proof doesn't work.)  The problem is related to using a non-invertible function here, but it's
possible to come up with non-invertible functions that don't have this
problem.  For example, imagine
X = hash(P0 || input string)
Y = hash(P1 || input string)
output = hash(X||Y)
where P0 and P1 are fixed padding strings of 512 bits (for SHA1; for
SHA-512 they'd be 1024 bits wide).    The SHA1 compression function generating the output now gets a 320-bit
string, pads it out to one full block in a fixed way, and uses that block
to encrypt the SHA1 initial chaining value; it then feeds that initial
value forward.  Now, I don't know whether the SHA1 compression function
will give a uniform distribution on the output here, assuming X and Y each
range over about 2^{159} possible values.  But there's at least nothing
inherent in the basic structure of SHA1 that prevents a uniform output
distribution here.  The "key" for the last compression function can range
over about 2^{318} values, and there's no obvious reason to expect
nonuniform results from SHA1 in that case, even given a fixed input
chaining variable.    Basically, the issue involves allowing all the entropy in the input string
to pass through a non-invertible "narrow pipe."  AES-CBC-MAC doesn't have this particular property.  In fact, if the input
has a reasonable amount over full entropy, I believe it's going to be all
but impossible for an attacker to distinguish the outputs from random even
if he's computationally unbounded.  But in the computationally-bounded case
where we get less entropy than we needed for unconditional security,
AES-CBC-MAC doesn't work too well for us--that's where David's clever
meet-in-the-middle attack comes in.  Another possibility is to consider key K to be unknown to the attacker.
(Maybe this key is generated using your first entropy outputs from a fixed
key.)  The attacker is then basically having to break CBC-MAC in order to
distinguish these values from random values, right?  This seems like
something that you can directly use an existing security proof on, without
bending that proof's assumptions all out of shape.  How does SHA1 do on computational security?  Again, let's imagine the case
where the attacker knows the last 512 bits of the input string
corresponding to each output string.  He gets a bunch of SHA1 outputs, and
he's apparently stuck.  Since he can't enumerate the possible inputs, he
can't easily test the outputs to see if they're consistent with having come
out of SHA1.  If he's limited to, say, 2^{120} work, I don't think he can get a
nonnegligible advantage distinguishing these outputs from random outputs.  Again, I can't see a specific property of SHA1 that guarantees that such
computational security will always exist for these outputs.  But I also can't see an attack that ruins that computational security.  --John Kelsey

@_date: 2002-08-07 23:13:39
@_author: John Kelsey 
@_subject: Extracting uniform randomness from noisy source 
Yeah, sorry.  I jumped in without realizing my starting assumptions were
different than yours.  --John Kelsey, kelsey.j at ix.netcom.com

@_date: 2002-08-11 16:45:52
@_author: John Kelsey 
@_subject: Extracting uniform randomness from noisy source 
Hmmm.  I don't see that they're really unattainable, though they may not be
possible to prove secure without unreasonable assumptions.  Consider using SHA1 to hash each input string, but then outputting only 80
bits of the output.
Now, since we're only outputting half the output bits, we don't have to
worry about the fixed-suffix attack I pointed out earlier.  In fact, we
have a nice, 160-bit wide pipe all the way to the end, so any internal
collision problems basically just go away.  I think this meets all three
criteria.  (But it ought not to ever be output or used directly in a way an
attacker can see; it should be shielded by some longer-term cryptographic
secret so that it has computational security even if occasional input
strings are totally known to the attacker.)  a.  With far more than 2^{80} equally likely inputs, we expect to get very
nearly uniform output distribution from this scheme.  b.  With exactly 2^{80} equally likely inputs, we expect to get a single
collision, and so lose almost no entropy.
c.  With less than 2^{80} equally likely inputs, we expect to get no
collisions, lose no entropy, but we may be susceptible to brute-force
search of possible input strings to distinguish our output from a random
output of 80 bits.  I mostly agree with this.  It's hard to convince yourself you have enough
entropy against the most powerful possible attackers, anyway.  But if
someone is claiming to provide you random noise from your system or
something, it sure seems like its strength ought not to be based on the
cryptographic strength of SHA1.  Otherwise, why not just use SHA1 in one of
the obvious ways to generate PRNG outputs once you've gotten a
hopefully-secure seed?  Or AES, for that matter?  Your really big assumptions about SHA1 (or AES-CBC-MAC with an all-zero
key, or a 32-bit CRC, or whatever else may be used here) involve how well
they distill entropy.  This basically requires that you assume that the set
of input strings that occurs isn't pathologically bad with respect to
causing collisions in SHA1, or the 80 bits of SHA1 you actually output, or
whatever.  (It's not possible to choose any function that never has this
happen, but it's pretty easy to choose functions that almost never have it
happen, assuming randomly-selected input strings.  But, of course, the
input strings we'll get in a real-world system aren't randomly selected,
they're just selected in a way that's independent of the structure of the
entropy distillation function.  For example, for the overwhelming majority
of sets of 2^{128} random 1024-bit strings, simply XORing each 128 bits
together will successfully distill the entropy from the strings; however,
just a flat XOR folding would be a bad idea with the kinds of input we
expect in real-world systems.)  Right.  But it's important to specify which you're trying to accomplish.
Distilling entropy and outputting it in some form is supposed to be
information-theoretically secure, right?  You want it to fail gracefully
into computationally secure if your input entropy estimates are messed up,
but that's not the original goal.  There are really different assumptions
and requirements for the two different goals.
For example, in a system whose only goal is to be computationally-secure,
we can distill entropy by computing a 128-bit CRC over our sample string,
using that result as an AES key, and running AES in counter mode.  We can
output trillions of bits per string, even if we have only a set of 2^{80}
equally-likely input strings.  This is computationally secure given a set
of assumptions about the interaction of the CRC with our input string set,
which is identical to the assumptions we need to make about the interaction
of SHA1 with our input string set to justify using the hash function as an
entropy distillation function.  But it wouldn't make much sense to use this
to output distilled entropy (uniform randomness from a noisy source), e.g.,
for one-time pads, or even for generating 256-bit keys.  --John Kelsey

@_date: 2002-08-12 13:54:53
@_author: John Kelsey 
@_subject: Extracting uniform randomness from noisy source 
Hmmm.  It looks to me like there are two big assumptions in this model that
may get in the way of using it for real-world applications:  a.  You have assumed your entropy input is in the form of random bit
strings.  (By "unbounded," do you mean of arbitrary length, or of infinite
length?).  In real-world systems, the input strings are not generally going
to be random in this sense.  (Did you mean something else by "random" here?)
b.  You have assumed the attacker never sees your output, but instead only
gets to try to guess it and see if he's right.  In real-world systems,
you're likely to see some outputs directly.  (In the world of
computationally secure algorithms, hashing your output before using it puts
the attacker in just the position of your model--he can verify correct
guesses, but nothing else.  In the world of computationally-unbounded
attackers, you can't necessarily depend on this property from your hash
function anymore, though you can do simple things to get it.)  Why is the entropy of the output (did you mean input) relevant here?  If
the attacker has a probability of a correct guess of less than 2^{-128},
why would I care about the precise entropy?  In fact, input entropy when the input strings aren't chosen randomly can be
pretty tricky.  Not only is it possible in principle to choose a set of
input strings with large entropy, which just happen to always give outputs
of zero for your function (the reason for using the key, I guess), it's
also possible to have inputs with large entropy, but which will guarantee
an unacceptably high p.  The obvious example is an input distribution that
gives a 0 1/2 of the time, but the other half of the time gives an output
whose high-order bit is always one, and whose remaining 159 bits are
uniformly distributed random bits.  This has entropy of but it guarantees p >= 1/2 for any MUNGE function, independent of key.  --John Kelsey, kelsey.j at ix.netcom.com

@_date: 2002-08-17 03:29:07
@_author: John Kelsey 
@_subject: employment market for applied cryptographers? 
This is my experience, too.  A huge number of the people I know around here
(RTP area, mid-North Carolina) are out of work, or are worried that they
soon will be.  This set of people includes only one cryptographer (and he's
got a job).  Also that regions and industries can vary enormously in how their economy
is going.  Areas where a lot of jobs are in the computer or travel
industries, for example, are going to have a lot of unemployment, as this
area does.  And also, it's important to note that most of us in this field
might move to a different field (e.g., more general software development,
teaching, etc.) rather than live without paychecks for a long time.  Or
might decide that now is the time to go back to school.  Unemployment stats
measure (if I'm remembering it right) only people who are not working, but
are actively looking for work.  (I don't know what definition is used to
decide if you're really looking or not.)  I feel very fortunate to still have a job, given all that's going on in
this industry.
--John Kelsey, kelsey.j at ix.netcom.com // jkelsey at certicom.com

@_date: 2002-08-17 03:37:19
@_author: John Kelsey 
@_subject: employment market for applied cryptographers? 
Also, designing new crypto protocols, or analyzing old ones used in odd
ways, is mostly useful for companies that are offering some new service on
the net, or doing some wildly new thing.  Many of the obvious new things
have been done, for better or worse, and few companies are able to get
funding for whatever cool new ideas they may have for the net, good or bad.
 And without funding, people are a lot more likely to either decide to do
the security themselves, apply openSSL and a lot of duct tape and hope for
the best, or just ignore security.  Sure, it may cost a lot later, but
they're going broke *now*.
--John Kelsey, kelsey.j at ix.netcom.com // jkelsey at certicom.com

@_date: 2002-10-03 09:11:13
@_author: John Kelsey 
@_subject: Gaelic Code Talkers 
There's a possible reference in _Cryptonomicon_, but I honestly thought it
was a joke....
--John  --John Kelsey, kelsey.j at ix.netcom.com // jkelsey at certicom.com

@_date: 2002-09-25 14:32:22
@_author: John Kelsey 
@_subject: unforgeable optical tokens?  
Wasn't there another idea along these lines proposed for currency
counterfeit resistance?  Something about embedding optical fibers into the
paper in some somewhat random way, and digitally encoding a signature on
the resulting pattern somehow?  --John Kelsey, kelsey.j at ix.netcom.com // jkelsey at certicom.com
 --John Kelsey, kelsey.j at ix.netcom.com // jkelsey at certicom.com

@_date: 2002-09-25 14:50:21
@_author: John Kelsey 
@_subject: unforgeable optical tokens? 
Well, you can get a nice (provable) level of security from a big memory
device like this, if the entries are random, and if there is a strict limit
on how quickly you can read information out of it.  Bruce Schneier and I
did a paper on this several years ago.  (Though I'm sure a bunch of other
people had used the same idea in their own systems before....)  Let's
see...."Authenticating Secure Tokens Using Slow Memory Access," at the
USENIX workshop on smartcard technology in 1999.  The big question is under what conditions it's possible to read out a
significant fraction of the data.  If you have a secure token that refuses
to respond to a memory query in less than a second, then the answer is
pretty simple.  For this device, it's not so clear.  It might be that the
device can't be read out by a compromised terminal (assuming there are one
day terminals for these devices), but it may still be readable by someone
who steals the device and takes it apart in a lab or something.  --John Kelsey, kelsey.j at ix.netcom.com // jkelsey at certicom.com  --John Kelsey, kelsey.j at ix.netcom.com // jkelsey at certicom.com

@_date: 2003-04-04 09:00:07
@_author: John Kelsey 
@_subject: TPM coming to Canada 
*Snork!*  That's perfect.
"...as computer security professionals, we feel that it is our duty to advise the legislature of the critical importance of requiring the use of a PKI for this system, preferably with multiple root CAs and online certificate revocation.  In order to ensure that the CAs function according to the will of the legislature, we further recommend that the legislature draft the certificate practice statement as part of this law...."
--John Kelsey, kelsey.j at ix.netcom.com

@_date: 2003-04-08 11:50:46
@_author: John Kelsey 
@_subject: aural cryptography 
I was re-reading the original visual cryptography paper last night, and had an odd thought: Why couldn't we do something similar with sounds?  The human ear/brain is pretty good at pulling patterns out of noise; would it be possible to randomly embed half of a low-quality voice channel in each of two sound channels, so that they didn't sound obviously bad apart, but when played at the same time, would allow the listener to hear a spoken message pretty clearly?
It seems like you could even do some pretty weird things with this, like embedding the signal in four or five sound channels, or embedding them in such a way that the speakers on the different channels had to be a certain distance apart for the embedding to work.
So my questions are:
a.  Is this really possible?  Or am I missing something?
b.  Has this been done in the open literature?  (It seems like the sort of thing that would have been really useful for, say, radio broadcasts that were intended to be received by spies.)
--John Kelsey, kelsey.j at ix.netcom.com

@_date: 2003-04-09 23:57:07
@_author: John Kelsey 
@_subject: Via puts RNGs on new processors 
It depends on what you're worried about, right?  RNG failures can be pretty subtle, and may be impossible to detect in software.  If the RNG fails, it might be nice to still get reasonable security.
Though it's not like it's easy to have unlimited faith in software-based entropy collection processes, either....
More generally, malevolently altered CPUs make a different set of attacks possible; they're more likely to either be interactive attacks, or to be observable in the CPU's behavior.  Like, if your CPU notices whenever a 3DES encryption is being done, and only does single-DES instead, it will be easy to catch.  If the CPU has some backdoor to get it into supervisor mode whenever a certain 64-bit value appears on the memory bus, that's likely to be useful for some attacks, but not for others.  (It won't help you decrypt a stored, encrypted file somewhere.)
--John Kelsey, kelsey.j at ix.netcom.com

@_date: 2003-12-31 18:54:08
@_author: John Kelsey 
@_subject: Non-repudiation (was RE: The PAIN mnemonic) 
Surely a better government-related TLA for this would be derived from Non-changeability, Secrecy, and Authentication....  :)
--John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2003-12-31 22:31:09
@_author: John Kelsey 
@_subject: I don't know PAIN... 
============================== START ==============================
This is the same for discrete log schemes, in general.  (Maybe there are some for which it's not the case.)  Your private key is x, your public key is g^x mod p.  Also for one-time signature schemes and their hash-tree based extensions, which use nothing but a hash function, and for all the variants of the Merkle puzzle schemes I can think of.  (Which are public key, but just barely.)
--John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2003-02-12 14:06:18
@_author: John Kelsey 
@_subject: Stupid security measures, a contest 
I can't imagine this is the stupidest, but there's a state office building in Missouri where (no doubt due to some Directive From On High), they've put up a wooden shack in front of the main entrance, where anyone going in or out has to pass through a metal detector.  The wooden shack isn't directly in front of the entrance, however--probably, that would make life too hard on the smokers, who now have to go outside to smoke.  It's more like about 50' in front of it, completely unconnected to the building.
The really entertaining bit is that, since most people going into the building are basically law abiding (state employees), most people seem to go through the shack and get checked for weapons, rather than around the shack to save time.
--John Kelsey, kelsey.j at ix.netcom.com

@_date: 2003-02-23 23:10:27
@_author: John Kelsey 
@_subject: [Bodo Moeller <bodo@openssl.org>] OpenSSL Security 
This works as long as the data the MAC is computed over includes everything needed to decrypt the message.  If there's context that's not included in the MAC, you can end up accepting a different plaintext than the one that was sent.  (That should be obvious, but I've seen it messed up once or I think this is a good general principle, for the same reason.  If you MAC the ciphertext, then the designer of the protocol has some extra work to do, proving that there's no way to accept the MAC but get a different plaintext than was sent.  If you MAC the plaintext, then the implementors have extra work to do, which won't be nearly as well reviewed or understood as the protocol.
--John Kelsey, kelsey.j at ix.netcom.com

@_date: 2003-02-26 01:37:02
@_author: John Kelsey 
@_subject: [Bodo Moeller <bodo@openssl.org>] OpenSSL Security 
Right.  But it's not hard to set things up so that you can prove that there is no way for the decrypted, unpadded plaintext to change without the padded ciphertext having changed.  (Basically, you just have to include the IV in the MAC along with the padded ciphertext, and use a padding rule that's unambiguous.)   In theory, there's no security advantage doing it this way, since you can get the same proofs doing it both of the other obvious ways.  In practice, though, this way the implementation has only one thing to get right--verify the MAC before you do anything else.  Otherwise, the implementation may have to get all sorts of other stuff right--checking the padding correctly, dealing with the effects of altered decrypted plaintext on compression schemes, etc.
The spec that says how to do the encryption/padding/MACing will probably be reviewed by other cryptographers, who will generally know about reaction attacks.  It's much less likely that any implementation will be reviewed by someone who understands these attacks.
--John Kelsey, kelsey.j at ix.netcom.com

@_date: 2003-02-28 17:31:58
@_author: John Kelsey 
@_subject: [Bodo Moeller <bodo@openssl.org>] OpenSSL Security 
============================== START ==============================
 mics.com>
Yep.  The thing that I found fun about this attack was that it so completely sidesteps the protections of the crypto.  If you think about it, the whole concept of the attack is to force the recipient's execution path to react without the benefit of a cryptographic check on its actions.
This attack made me think of the attack on SSH-encrypted passwords using timing / keystroke analysis.  Again, a clever way to do an end-run around the crypto.
If you think about compression before encryption in this context, you could imagine someone actually causing a software crash (or even a buffer overrun, though not one they could control very well) by altering the ciphertext, and thus giving the decompressor routines a bunch of random bits to deal with.  (I mentioned the possibility of using this sort of thing in an attack in my compression side channel paper at FSE last year, but I certainly didn't have this kind of clever attack in mind!)
--John Kelsey, kelsey.j at ix.netcom.com

@_date: 2003-06-01 10:51:14
@_author: John Kelsey 
@_subject: Nullsoft's WASTE communication system 
AES has gotten a lot of attention, and right now, it's the high-prestige target.  (Among other things, it was clearly a front-runner in the AES process from the beginning, and all of us who'd designed other algorithms spent a lot of time trying to beat up on it.)  Blowfish has been around longer, but has probably had fewer people spend lots of time trying to break it.  The still-unresolved question is whether those equation-solving attacks can really be used against AES, and there doesn't seem to be anyone who's completely confident of the answer to that question.
--John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2003-06-03 11:25:16
@_author: John Kelsey 
@_subject: "PGP Encryption Proves Powerful" 
Yeah, I suspect you're right.  And the big problem with these threshhold schemes is that non-cryptographers end up unable to figure out what the heck is going on with them.  Once you get past 2/n schemes, most peoples' eyes glaze over.
--John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2003-06-03 10:42:01
@_author: John Kelsey 
@_subject: Maybe It's Snake Oil All the Way Down 
I think phones that encrypt the landline part of the call are pretty low-priority for most of us, since it costs something to eavesdrop on these calls.  But anything that goes over the air, whether cellphone or cordless phone, ought to be properly encrypted, and it isn't now.  This is a big vulnerability in a lot of places, and once you've built the intercept and decrypting hardware, it's easy to eavesdrop on huge numbers of people.  You can imagine either rogue cops and spies doing this, or private criminals.
I keep wondering how hard it would be to build a cordless phone system on top of 802.11b with some kind of decent encryption being used.  I'd really like to be able to move from a digital spread spectrum cordless phone (which probably has a 16-bit key for the spreading sequence or some such depressing thing) to a phone that can't be eavesdropped on without tapping the wire.
And for cellphones, I keep thinking we need a way to sell a secure cellphone service that doesn't involve trying to make huge changes to the infrastructure, which probably means a call center that handles all contact with the cellphone itself, always encrypted.  Something like this would allow me to buy a phone and sign a contract, and quickly get real security on all my digital calls going over the air.  End-to-end encryption isn't nearly as important.  There's no reason it couldn't be supported, of course, when both endpoints had the right kind of phone, but it's a small additional value.  The big win is to stop spewing private conversations over the radio in the clear.
--John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2003-06-03 18:17:12
@_author: John Kelsey 
@_subject: Maybe It's Snake Oil All the Way Down 
I agree end-to-end encryption is worthwhile if it's available, but even when someone's calling my cellphone from a normal landline phone, I'd like it if at least the over-the-air part of the call was encrypted.  That's a much bigger vulnerability than someone tapping the call at the base station or at the phone company.  Otherwise, encrypted phone calls with the secure cellphone start looking a lot like encrypted e-mail with PGP--I have PGP, so do a few other people, but most people I want to talk to don't have it installed, and so most of my calls remain in the clear.  This includes phone calls to my doctor, mother, priest, shrink, sister, lawyer, best friend, wife, bank, accountant, etc., e.g., all the calls I probably really wanted secured, and which will basically never be secured end-to-end if this requires each of those people to buy a special new phone, or do some tinkering with configuring secure phone software for their PDA.  "Hmmm, which key size do I need?  Is 1024 bits long enough?  Why do I have to move the mouse around, again, anyway?"  For essentially all of these, just getting to where I can use a cordless or cell phone on these calls without feeling like I'm broadcasting my private conversations in the clear would be great.  Securing the other end is even better, but I'd like to do the part I can do now, not when the world finally realizes that unencrypted wireless stuff is a gaping privacy hole.
Yep.  I have this mental picture of downloading some software for my PDA/cellphone, and buying a $200 box for my home, and getting a secure cordless phone when I'm in range, and a secure cellphone when I'm not, maybe with a secure voicemail system thrown in for good measure.  It seems like most of this is off-the-shelf technology (wireless networking, a box connected to two landlines, some minimal encryption and key management software, etc.).
When you ask for a secure call, your cellphone calls the box in your house (over an encrypted link), and it makes the rest of the call.  Similarly, when someone calls your secure phone line number, it rings at the box, and then gets forwarded over the encrypted link to your cellphone.  If two boxes like this call each other, they do end-to-end encryption.  But the over-the-air stuff always gets encrypted.  It sure seems like this would be worth putting up with a little delay in the call setup.  (But maybe there's some reason this won't work.)
--John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2003-06-04 19:15:13
@_author: John Kelsey 
@_subject: Maybe It's Snake Oil All the Way Down 
Big brother has a limited budget, just like the rest of us.  If he has to produce a warrant or tap a wire somewhere to listen in on me, he probably won't bother.
The only thing protecting my cellphone calls right now is trivially-broken encryption, the need for some moderately expensive equipment, and some laws prohibiting cellphone eavesdropping.  That means that some bad guys may be eavesdropping now, and there's no telling how many bad guys will be doing so tomorrow.  Nobody here knows how much eavesdropping is being done, because communications intercepts can be done without leaving any record anywhere.  Do the police in some cities troll for interesting cellphone calls?  Does the NSA do that in the US, quietly?  Do Russian or French intelligence agencies?  How would we know?
So, what can I do about it, as an individual?  Make the cellphone companies build good crypto into their systems?  Any ideas how to do that?
The only way I can see getting decent security on my cellphone is to do something that doesn't require the rest of the world's permission or assistance.  The simplest version of that is to have a box at my house that's connected to two phone lines, and have all calls to and from my cellphone go through that box.  Calls to other secure cellphones can be encrypted end-to-end.  Calls to everyone else get encrypted between my phone and my box at home.  I spend a little extra for extra security, nobody else has to pay anything, and I can call friends on my cellphone without being susceptible to trivial eavesdropping.
Can the bad guys defeat this?  Sure, they can tap my landline, or bug my car, or do all sorts of other things.  But none of those are cheap enough to do to everyone, and probably none are cheap enough to do to me.  Tapping my landline either means interacting with the phone company, or paying someone to go install a tap, each of which implies a risk of getting caught, practical limits on how often it can be done, etc.
This also bypasses the "network effect" of encrypting phones, where you get approximately zero benefit from having one until they're widespread.  I have an old Comsec 3DES phone at home.  It's nice technology.  I think I've used it twice.  If you're not a cryptographer or a cocaine smuggler, you probably don't know anyone who owns an encrypting phone or would particularly want to.  Even if you'd like to improve your own privacy, you can't buy an end-to-end encrypting phone and improve it much.  That's what I'd like to see change.
--John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2003-06-13 11:47:03
@_author: John Kelsey 
@_subject: Keyservers and Spam 
It's not quite worthless, as it raises the cost of the attack quite a bit.  It's a lot more expensive to keep someone around 24/7 ready to spoof a key fingerprint reading on an intercepted phone call than it is to silently put the wrong key on a key server and automatically intercept and replace e-mails.  If you can't make your system impossible to break (alas, you usually can't), you may as well at least make it an expensive and unpleasant target.
It would be easy enough to specify a key server that only responded to queries on precise e-mail addresses, which would make some sense (it's reasonable to expect that you already know my e-mail address before we start an encrypted conversation).  I think that's much easier and cleaner than monkeying around with the certificate information (e.g., by putting "random_user (at) random_host (dot) org" or something into your certificates.)  As you stated, that ends up undermining one of the assumptions of certificates and the web of trust.  Also, it's nice to let e-mail software have some hope of figuring out which key in the keyring goes with which public key.
--John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2003-06-13 11:56:49
@_author: John Kelsey 
@_subject: Keyservers and Spam 
The thing that strikes me is that the PGP web of trust idea is appropriate for very close-knit communities, where reputations matter and people mostly know one another.  A key signed by Carl Ellison or Jon Callas actually means something to me, because I know those people.  But transitive trust is just always a slippery and unsatisfactory sort of thing--the fact that Jon Callas trusts Fred Smith trusts John Jones to sign a key doesn' t really tell me whether or not I should trust him--by the time we're about three hops away, you'd have to be God to know enough to have your signature mean anything.
A bigger issue is that there's usually no practical way to deal with revoking a root key in a PKI, even if there are technical mechanisms to do so.  "And then you go out of business" is almost as unsatisfactory a protocol step as "And then you go to jail."
--John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2003-03-06 16:53:24
@_author: John Kelsey 
@_subject: Scientists question electronic voting 
The big theoretical question is whether you could tell whether the vote-seller was faking it.  A design goal ought to be to make plausible fake proofs of how you voted easy to generate, IMO.  Why only sell your vote to one side, when you can sell it to both sides multiple times?
In practice, if it's more trouble to generate fakes than to just vote and bring the proof to sell, then the individual vote seller will probably just vote as he's told.  After all, most people eligible to vote don't bother most of the time; presumably, they just don't care that much who wins the next election.  I assume most people who sell their votes aren't committed ideologues who are selling out their cause, but rather people who didn't much care either way.  (But surely someone, somewhere has real data on this.)
--John Kelsey, kelsey.j at ix.netcom.com

@_date: 2003-03-07 00:52:24
@_author: John Kelsey 
@_subject: multiple system - Re: Scientists question electronic voting 
I think one benefit of using paper ballots as the backup is that there are already pretty well-understood ways to deal with paper ballots.  I like the idea of the election observers having at least one piece of the technology they really understand.
Is the relevant question here about probabilistic failures, or about conspiracies?  Clearly, the size and cost of the conspiracy gets much bigger if there's a check value on the election results that is handled completely outside the voting machine.
--John Kelsey, kelsey.j at ix.netcom.com

@_date: 2003-03-07 01:00:25
@_author: John Kelsey 
@_subject: Scientists question electronic voting 
I think the real defense against vote-buying or vote-extortion schemes is external--detecting any such scheme that has much of an impact because it necessarily involves hundreds or thousands of people.  This assumes that the authorities and media aren't totally corrupted, but so does any voting technology.  With a lot of the more elaborate technological attacks, though, it's hard to see an attacker with current technology being able to afford them.
--John Kelsey, kelsey.j at ix.netcom.com

@_date: 2003-03-13 13:13:23
@_author: John Kelsey 
@_subject: Encryption of data in smart cards 
With any kind of reasonable PIN length, though, this isn't all that helpful, because of the small set of possible PINs.  And smartcards don't generally have a lot of processing power, so making the PIN->key mapping expensive doesn't help much, either.
--John Kelsey, kelsey.j at ix.netcom.com

@_date: 2003-03-14 01:13:28
@_author: John Kelsey 
@_subject: Encryption of data in smart cards 
Right.  Which is good for the PIN-guessing-to-get-access attack, but not much help for the decrypting the extracted data using the PIN-generated key --John Kelsey, kelsey.j at ix.netcom.com

@_date: 2003-05-05 12:20:13
@_author: John Kelsey 
@_subject: The Pure Crypto Project's Hash Function 
Actually, SHA1 isn't known to be good, it's just strongly suspected to be good.  Other than information-theoretic stuff (e.g., one-time pads are really known to be good), most stuff in cryptography is presumed good because nobody knows how to break it, or even how to realistically come close to breaking it.)
Of course, that doesn't mean that rolling your own hash function is a good idea.  Or that it makes any sense at all to build all your own primitives in order to design some kind of secure system.  It's like deciding you want to design a better word processor than Word, and so starting by trying to design your own microprocessor architecture.
--John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2003-05-05 12:30:59
@_author: John Kelsey 
@_subject: The Pure Crypto Project's Hash Function 
Suppose you're about to take a job as a policeman or security guard or something, and believe there's a serious chance you'll be shot at.  You're trying to decide on which bulletproof vest to buy.  Several vendors demonstrate both safety arguments involving the tensile strength of Kevlar, the way impacts are distributed across a large area, etc, and extensive tests where various kinds of guns and knives are tried against the vest, without penetrating it.  Another vendor says "well, I decided to invent my own bulletproof vest.  I shot at it a couple times with my .22, and punched it once, and it seems to hold up very well.  Besides, it's conceptually simpler than my competitors' vests, and I spent several days thinking over the design without finding any weaknesses.  Trust me."  Which one do you want to trust?
If you want to design a hash function, that's cool.  In fact, designing crypto primitives is one of the most fun things you can do.  But doing it right involves actually understanding the existing known attacks on the primitives, and being capable of applying those attacks to a new design.  It also involves getting a lot of public comment--meaning writing it up for submission to a good conference (FSE is great for new primitives), and making your writeup so clear that you encourage lots of people to look at it.  And it still may be that people don't jump at the chance to use your primitive, either for performance reasons, or because they have a satisfactory alternative they trust more.
How much trust people have in some primitive is dependent on the reputation of the designers, the amount of review it's seen, and even how well you imagine the problem to be understood by the community.  (Even very sharp people designing block ciphers in 1985 were going to have a hard time getting it right, because the public state of the art in cryptanalysis wasn't all that great.)
--John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2003-05-05 12:38:00
@_author: John Kelsey 
@_subject: The Pure Crypto Project's Hash Function 
You really might want to make sure it's seen some review in that oddball mode of operation, too.  If you want a Rijndael-related hash function, Whirlpool is probably your best bet.
I think you could arrange this for some algorithms, but not so easily for SHA1.  I know MARS (the IBM AES submission) had a complicated key schedule for which there were many fairly low-probability events.  I believe that just trying 10-15 keys wouldn't have been enough to fully test that key schedule.  (The key schedule generated values to be multiplied, and required no runs of more than 10 zeros or ones in the multiplying value.)
--John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2003-05-05 12:51:02
@_author: John Kelsey 
@_subject: my take on "PCP" 
Yes, it will.  Most of the people who are likely to try cryptanalyzing a new crypto primitive have good reasons to want a better publication list--like they want to get tenure, or they want to get better job offers, or they want to get into a good graduate program.  It's much easier getting an attack published on a published design, even if it's not widely used.  An attack on Helix is pretty likely to get into FSE, if you have one.  An attack on your design is much less likely to get into FSE, or any similar conference.    For prospective attackers with limited time, and a real need to get some peer-reviewed publications, this suggests a good strategy for getting a lot of review for your new primitive....
--John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2003-05-05 23:28:50
@_author: John Kelsey 
@_subject: DRM technology and policy 
Yep.  The SPP can be used as a way to enforce promises to release the work when the payments are all made, but not as a way to solve the free-rider problem.  That basically has to be driven by people wanting to pay the artist, just like it is for real street performers.
This kind of model does work for some things--public television and radio stations get a lot of their money from voluntary contributions, street musicians and musicians in coffee shops and such places often get tips, etc.  But it's clear that many kinds of work won't be produced when there's no way to withhold the content until the payment is sent.  It's not clear to me how this affects the ultimate availability of music; easier distribution (without a small number of companies sitting astride the channels and demanding a cut) makes a lot more music available, but no way to collect copyright payments makes a lot more musicians keep their day jobs, instead of making music full time.
Also, as a sideline comment, I've heard the comment several times that Eric Hughes independently came up with a bunch of the ideas in the SPP paper, years before, to give credit where it's due.  (And much of our purpose in writing the paper was actually to point out the problems with copyright enforcement schemes, more than to propose fixes.)
--John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2003-05-12 18:39:13
@_author: John Kelsey 
@_subject: A Trial Balloon to Ban Email?  
The realistic benefit is that you can use something like hashcash as one of your spam filtering rules.  Anyone who is spending 1/2 sec on a reasonable machine per e-mail sent isn't likely to be spamming you, because that won't scale up very well for sending out thousands of e-mails at a time.  The problem is that until it is widely adopted, it's not a very useful additional filter.
There are actually dozens of similar ways to stop nearly all spam, if you can deploy them all over the net at once.  But deploying anything all over the net at once isn't practical, so instead, each user or ISP tries to find some workable solution for the problem, typically involving changing his filtering rules  every few months and spending a minute or two a day going through his spam folder, making sure he's not throwing away something --John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2003-05-31 12:28:59
@_author: John Kelsey 
@_subject: "PGP Encryption Proves Powerful" 
Two comments:
a.  It sure seems like it would be a pain to enter a long passphrase on one of these things, so that seems like the most plausible attack.  But I agree that it would be nice to know more about actual fielded attacks.  (The problem is that if you're actually using them to gather information, you won't want to disclose your methods.)
b.  A nasty (likely to backfire) trick would be to generate a long random password, use it to encrypt a bunch of data, and then forget the password.  Something as simple as the MD5 of the results of typing into a buffer for a couple minutes would do fine.  No attacker will ever guess it.  Of course, the judge may not believe you when you explain why you don't know those passwords, and the cops may try to beat the answers out of you if they're convinced enough that you're a bad guy....
--John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2003-05-31 12:39:58
@_author: John Kelsey 
@_subject: "PGP Encryption Proves Powerful" 
============================== START ==============================
One thought:  How hard would it be to write a Palm app to use the interaction between several devices to derive a key or password, using the IR ports?  The whole thing could easily be encrypted under a common key.  Require the attacker to get a device from each member of the cell (or 3/5 or some such)
before recovering the actual encrypted secrets.  I wouldn't be surprised if technologically sophisticated terrorists and spies were doing stuff like that.  (You could easily do this with pen and paper, too, for simple control structures.  Each member of the cell holds some parts of the password written down, and 4/5 of them have to get togther to reconstruct the full password.)
--John Kelsey, kelsey.j at ix.netcom.com
PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2004-04-15 01:07:31
@_author: John Kelsey 
@_subject: voting 
I think the VoteHere scheme and David Chaum's scheme both claim to solve this problem.  The voting machine gives you a receipt that convinces you (based on other information you get) that your vote was counted as cast, but which doesn't leak any information at all about who you voted for to anyone else.  Anyone can take that receipt, and prove to themselves that your vote was counted (if it was) or was not counted (if it wasn't).  (This is based on attending a presentation of David's scheme at George Washington a few months ago, a conversation I had with a VoteHere guy, and some conversations and documents given to me by each.  I haven't tried to verify the protocols or proofs, but I'm convinced that all this is possible, modulo various assumptions.  There may be a dozen other people doing similar things, that I've simply not heard of.)
The way I understood these schemes, you can see the initial encrypted ballots (they're published), and then there are several rounds of publically verifiable shuffling and decryption by different TTPs.  After the last round of shuffling and decryption, you have raw votes.  So anyone can verify the count, assuming the set of initial encrypted ballots are legitimate.  And anyone can produce a receipt that can be shown to be one of those encrypted ballots, if it was counted.  That doesn't keep someone from stuffing the ballot box, but it does mean that anyone who throws away unfavorable votes is going to leave behind evidence, which can potentially call the whole vote into question.  The way I saw these schemes described, there was no recount capability, but the count was done in a completely public way.
It seems to me that this kind of scheme has a lot of potential for disruption attacks, since one compromised voting machine can be used to call any election into question.  But I could be missing something, as this is really not something I've spent a lot of time on....
I see your point, but there's an awful lot of any voting system that isn't being closely observed by the voters, or that isn't really well-understood by most of them.  It's not so clear to me that the average voter is going to walk away convinced that a voter-verified paper ballot, or a mark-sense ballot, or whatever other thing isn't going to somehow be subject to attack.  Or that if they do walk away convinced, that this has much to do with whether they *should* walk away convinced.
Yep, this is a big issue.  Which is why I think everyone with any sense agrees that we need some kind of independent audit trail, regardless of whether we're doing voting with computers, or with pens for punching out holes.  There are a bunch of ways to do this, one obvious and pretty easy-to-field choice being voter-verified paper ballots.
Huh?  Do you think the same is true of payment systems?  Those also ultimately require some humans to play by the rules, but it sure seems like a well-designed payment system can remove a lot of the ambiguity about who has violated the rules, and can outright prevent other kinds of rule violations.  And it seems to me that this is very similar to the situation with voting.
Touch screen voting (with the audio extensions) has at least one huge advantage over pen-and-paper schemes, because blind people can vote with them.  The VoteHere and Chaum schemes provide other benefits (a lot of kinds of misbehavior by the authorities are prevented by the design, though of course, not *all* possible misbehavior), at various costs in system complexity, dependence on lots of interacting systems that might not be all that reliable, ability to recover from some low level of fraud, etc.  Paper ballots printed behind glass provide a different set of tradeoffs.  And you could design twenty other sets of tradeoffs.  I'm not at all convinced that the way we optimize for best security is to minimize technology.
I agree that it's easy to get carried away by the elegance of your mathematics, or by the really spiffy blinking lights on the computer, and forget the essentials.  But technology and math aren't somehow inherently bad things to introduce to voting systems.  It just has to be done in a way that makes sense, right?
Less subject to vote tampering than the old machines with mechanical counters and levers?  That's not too hard.  Less subject to vote tampering than paper ballots marked by hand, that may be a little more of a challenge.  I think it's more fair to say that the attacks and threats will be different, and that the risk of a class break (work out the details of the attack once, then change votes all over the country) is seriously scary.  But it's sure not clear to me that adding computers to the mix must decrease security, or even must leave it unchanged.
--John Kelsey, kelsey.j at ix.netcom.com, who is definitely speaking only for PGP: FA48 3237 9AD5 30AC EEDD  BBC8 2A80 6948 4CAA F259

@_date: 2004-08-10 08:16:32
@_author: John Kelsey 
@_subject: Cryptography and the Open Source Security Debate 
So, how many people on this list have actually looked at the PGP key generation code in any depth?  Open source makes it possible for people to look for security holes, but it sure doesn't guarantee that anyone will do so, especially anyone who's at all good at it.

@_date: 2004-08-25 11:08:26
@_author: John Kelsey 
@_subject: On hash breaks, was Re: First quantum crypto bank transfer 
[[Note: I've tried to sort out who wrote what, but something odd was
going on in the quoting of the messages, so I may have it all
How about this: When someone finds any collision at all in your hash
compression function, even a pseudocollision or a free-start
collision, it's time to change hash functions.  This is true, even
when the alternatives are slower, and the existing attacks don't yet
turn into a full attack.  Also, when your collision resistance is
known to be vulnerable to brute-force collision attacks, you really
need to stop using it.  Even when the alternatives are slower, and you
think you can maybe get away with using MD5 here if the stars all line
up properly.
Now, for fielded hardware and (to some extent) software, you can try
to phase out the use of the broken primitive, if the attack isn't yet
leading to a practical fast collision-finding algorithm.  If MD5 had
started being phased out when the pseudocollision attack was found, or
even when the Dobbertin attack was found, it seems like we'd be in
better shape now.  True.  But was anyone surprised at another attack on MD5, which had
already had two high-profile attacks on its compression function?  Was
anyone surprised at an attack on HAVAL?  You would have lost the bet.  Where's the fundamental flaw in SHA1,
SHA256, SHA512, or RIPE-MD160?  Where's the fundamental flaw in
Whirlpool?  There may *be* such flaws in any or all of these hashes,
but they haven't been shown yet.  (Phil Hawkes' results on SHA256 look
interesting; it will be interesting to see if they lead anywhere, but
it sure doesn't look trivial to control those corrective patterns with
choices of message block differences.)  Remember that we had the algebraic attacks, which claimed the ability
to break the whole AES, though the attacks apparently don't work as
claimed because of a miscounting of variables.  (It's certainly
possible that someone will find an algebraic attack on AES.)  I don't know.  If you had to build something today to be secure, it
wouldn't be crazy to use SHA1, IMO.  But you just can't ever rule out
cryptanalytic advances of this kind.  I think the difference between
block ciphers and hash functions is that there's a much better
developed theory of block cipher design and analysis in the public
world than for hash function design and analysis.  This may be
changing, though.  And new attacks (algebraic attacks, the integral
attack that is so effective against reduced-round Rijndael versions)
are always coming up, even so.  I think seriously trying to beat up on our algorithms, publishing
intermedaite results, etc., is the best we can do at our current state
of knowledge.  --John Kelsey

@_date: 2004-08-26 11:09:14
@_author: John Kelsey 
@_subject: HMAC? 
The big question is what the probability is of getting a successful
colliding message pair when you have complete control over the
message, but don't know the IV.  For repeated queries, you can know
it's always the *same* IV, if that helps, just not what it is.  I
don't think we can know that until we've seen the full explanation in
the Wang, et. al. paper, which hasn't been released yet.
--John Kelsey

@_date: 2004-12-08 09:24:41
@_author: John Kelsey 
@_subject: MD5 To Be Considered Harmful Someday 
The pseudocollision on MD5 paper was published in 1994, and Doebbertin's full collisions for MD5's compression function were published in 1996, so there was plenty of reason by 1997 to want to move to a different hash function.  People who stuck with MD5 for collision resistance after that were demonstrating seriously bad judgement, since the only argument left for MD5's security was "well, but nobody's published a way to exploit the attack on full messages yet."  --John Kelsey

@_date: 2004-12-13 12:00:30
@_author: John Kelsey 
@_subject: Blinky Rides Again: RCMP suspect al-Qaida messages 
They're going to have the same problems as the rest of us using strong cryptography--configuration and usability problems, key management hassles, incompatibilities between versions and programs, etc.  They have to do this with no central authority, no single support line or person who can reliably start things up and help them, in a basically decentralized way.  The cypherpunkish idea of a decentralized conspiracy using strong crypto only works if either the tools are a lot easier to use, or if the conspiracy is made up of cryptographically sophisticated people.  AQ is presumably made up of people who know a lot about the Koran, and probably a lot about day-to-day operational security against the Pakistani or Indonesian secret police, but there's not much reason to think they are very sophisticated about cryptography.  If you can't get most computer-literate people you know to use PGP to send you e-mail, how well is it going to work to do with a bunch of random jihadis?

@_date: 2004-12-15 10:06:10
@_author: John Kelsey 
@_subject: The Pointlessness of the MD5 "attacks" 
So, are you sure there can never be a program which allows such an exploit?  I've seen programs that had embedded components (state machines in particular) which were not easily human-readable, and had themselves been generated by computer.  And even large graphics, sound, or video sequences can really change the meaning of a program's actions in some ways; those might be susceptible to the requirements of the attack.  I agree it's hard to see how to exploit the existing MD5 collision attacks in programs that would look innocent, but I don't see what makes it *impossible*.  Then you have data files, as Adam Back mentioned, which are often not human readable, but you'd still like to know if the signature on them is valid, or if they've been changed surreptitiously since the last time they were checked over.  Finally, I'm very skeptical that the attacks that have been found recently are the best or only ones that can be done.
Do we have any special reason to think that there will never be a way to adapt the attack to be able to slip something plausible looking into a C program?  Once your hash function starts allowing collisions, it really just becomes a lot less valuable.

@_date: 2004-12-22 12:38:10
@_author: John Kelsey 
@_subject: Cryptography Research wants piracy speed bump on HD DVDs 
Think about the effect on P2P systems, if having one extracted movie from your player available for sharing meant that your player would stop working for all new content....  I'm not saying I think this (or any other technical solution I've seen) will work.  I'm saying that it's a pretty reasonable attempt to undermine participation in P2P systems.

@_date: 2004-12-22 13:08:32
@_author: John Kelsey 
@_subject: The Pointlessness of the MD5 "attacks" 
Hmm.  So maybe I'm missing something.  Here's my scenario:
a.  Alice decides to use GOST for encryption.  She finds an implementation in C from Eve, which includes the S-boxes hard-coded in.  Note that the specific S-boxes used for GOST are potentially pretty important for their security.  b.  She also finds a review from some well-known cryptanalyst, Bob, discussing the requirements on the S-boxes, and verifying that the above implementation uses good S-boxes, which includes the md5 hash of the C source code.
c.   Alice downloads the C source code, and checks the md5 hash.  Since the hash is correct, she compiles the code, and starts using her known-secure version of GOST to encrypt sensitive data.
d.   Unknown to her, though, Eve has slipped in a changed version of the C source code, with the S-boxes changed in a way that makes the encryption much weaker.  What prevents this attack from working?  Alice counts on a review done by someone competent and a hash of the source code, but the weakness of the hash function means that she's vulnerable to an attack.  The only thing that might keep it from working is if it happens to be impossible to choose a pair of sets of S-boxes so that one is weak, the other is strong, and the pair allows an md5 collision.  I don't know whether this is possible or not, but there's no inherent reason to think it's impossible--just making some of the 4-bit wide S-boxes in GOST non-bijective has pretty big security implications.  (Though 32 rounds covers a lot of sins in S-box selection, in terms of practical attacks rather than academic ones.)
I think you can use them in ways that may fool people who *are* looking.  All you need is a legitimate reason to have a more-or-less arbitrarily chosen block of bits in a part of your program, and then the person reviewing the code says "okay, that's random-looking, but reasonable enough."  As an alternative example, consider embedding a large prime number in your code, to be used as the modulus when you're doing Diffie-Hellman.  Someone reviews the code, and verifies that the number is prime and has all the other required properties.  Then, you swap in a different bitstring of equal length, but which is composite and yields a reasonably easy attack on Diffie-Hellman.  What prevents this?

@_date: 2004-06-08 11:55:23
@_author: John Kelsey 
@_subject: Chalabi Reportedly Told Iran That U.S. Had Code 
Did the Iranians actually think they could get technical information about a computer product from a *salesman*?  --John Kelsey

@_date: 2004-06-25 15:06:48
@_author: John Kelsey 
@_subject: cryptograph(y|er) jokes? 
From: bear (bear at sonic.net)
  Sent: Jun 22, 2004 3:46 PM
  Bob and Alice routinely discuss bombs, terrorism, tax cheating, sexual
  infidelity, and deviant sex over the internet.  They conspire to commit
  crimes, share banned texts and suppressed news, or topple tyrannical
  governments whose agents eavesdrop on their every communication.  They
  do all this with utmost secrecy and unbreakable codes.
  However, Alice and Bob do not even trust each other.
  A protocol designer then, is someone who does not think that Alice
  and Bob are insane.
Who was it that said that protocol analysis is essentially formalized paranoia?

@_date: 2004-05-27 12:35:00
@_author: John Kelsey 
@_subject: Satellite eavesdropping of 802.11b traffic 
Does anyone know whether the low-power nature of wireless LANs protects them from eavesdropping by satellite?  Is there some simple reference that would easily let me figure out whether transmitters at a given power are in danger of eavesdropping by satellite?  --John

@_date: 2004-11-17 14:03:53
@_author: John Kelsey 
@_subject: A new academic hash result on the preprint server 
Bruce and I have a new result on hash function security, which uses Joux' multicollision trick in a neat way to allow long-message second preimage attacks.  We've posted it to the e-print server.
The basic result is that for any n-bit hash function built along the lines of SHA1 or Whirlpool (e.g., using an n-bit compression function and Damgard-Merkle strengthening), we can mount a second preimage attack on long messages for a lot less than 2^n work.  For a 2^k message-block message, we do about 2^{n-k+1} work (when k<n/2) to get a second preimage.  We also have a little cheaper way to find a kind of goofy set of multicollisions than Joux gives.  None of this result leads to a practical attack on anything, as far as I can see.  The messages that are vulnerable are impractically long, and there's never an attack cheaper than offline collision finding.  But I think this result raises some kind-of interesting questions about the security of hash functions between the 2^{n/2} bound for collision finding and the 2^n bound for first and second preimage finding.  Comments appreciated,

@_date: 2004-11-19 10:40:40
@_author: John Kelsey 
@_subject: Gov't Orders Air Passenger Data for Test 
News story quoted by RAH:
The interesting thing here is that they can't really test how effective the system is until they have another terrorist event on an airline.  Otherwise, they can assess the false positive rate of their list (people who were on the no-fly-list, shouldn't have flown according to the rules, but did without trying to hijack the plane), and the false positive and false negative rate of their search for names in the list (e.g., when it becomes obvious that Benjamin Ladon from Peoria, IL would have matched, but wasn't the guy they were hoping to nab, or when it becomes obvious that a suspected terrorist was in the data, did fly, but wasn't caught by the software).  Presumably a lot of the goal here is to stop hassling everyone with a last name that starts with al or bin, stop hassling Teddy Kennedy getting on a plane, etc., while still catching most of the people on their watchlists who fly under their real name.  This is a goofy number.  If there were 100,000 likely terrorists walking the streets, we'd have buildings and planes and bus stops and restaurants blowing up every day of the week.  I'll bet you're risking your career if you ever take someone off the watchlist who isn't a congressman or a member of the Saudi royal family, but that it costs you nothing to add someone to the list.  In fact, I'll bet there are people whose performance evaluations note how many people they added to the watchlist.  This is what often seems to make watchlists useless--eventually, your list of threats has expanded to include Elvis Presley and John Lennon, and at that point, you're spending almost all your time keeping an eye on (or harassing) random harmless bozos.

@_date: 2004-10-05 09:37:11
@_author: John Kelsey 
@_subject: Linux-based wireless mesh suite adds crypto engine support 
I think the need for interoperability constrains the ability for a crypto module to implement some weak algorithm in place of AES or 3DES.  Unless the designer can know which encrypted messages have to be handled by someone else's non-hacked module, he can't safely do this.  I'll note that this is supported two separate ways in the (in progress) X9.82 standard.  a.  A standard way to produce a random bit generator with a guaranteed fallback to computational security is to XOR  the outputs of some good hardware generator with the outputs of a crypto PRNG (aka DRBG in X9.82-ese).  b.  Any approved random bit generator can always be combined with an unapproved generator by XORing.  The only security requirement here is that the unapproved generator be independent of the approved one.
All that said, though, it's far from clear how you monitor this in the standard crypto environment, since you usually take great pains to make it hard for anyone to get key material out of the tamper-resistant modules.  You provide the random value to XOR into the RNG output, and the module says "Thanks, I XORed it in.  Trust me."  Or, you demand the random value from its RNG, XOR in your own, but now, you've exposed the key outside the tamper-resistant module, which introduces a whole different set of problems.  I'm sure there are some clever crypto protocol ways to address this (basically, do a zero-knowledge proof of the value of the random number you used in deriving the key), but I have a hard time thinking this is at all practical....
--John Kelsey

@_date: 2004-10-06 09:23:46
@_author: John Kelsey 
@_subject: IBM's original S-Boxes for DES? 
I believe people have since come up with S-boxes that resist both linear and differential cryptanalysis.  But we don't know whether there were still other attacks or constraints they were trying to address.  However, it makes no sense to assume that they left linear attacks in as a backdoor, for two reasons:
a.  They already left a 56-bit key, which was a practical backdoor for people with experience and expertise in building keysearch machines.  (Think of all the expertise in parallel and distributed keysearch that has come out in the public world in the last fifteen years; surely, that was an area NSA had worked on at great depth years earlier!  Things like time-memory tradeoffs, parallel collision search and meet-in-the-middle search, clever optimization tricks for getting the keysearch to run efficiently, etc., along with a large hardware budget, must have made a 56-bit key look much worse from inside the agency than from outside.  (Though there were plenty of people who saw the problems from outside, as well, thus leading to our current understanding of keysearch techniques.)  b.  Linear attacks on DES, at least the ones we know about, are spectacularly impractical, requiring more plaintexts than you could ever hope to get from an innocent party using the speeds of hardware available when DES was designed and standardized.

@_date: 2004-10-12 09:57:15
@_author: John Kelsey 
@_subject: AES Modes 
I think CCM is just about perfect for this goal.  The MAC isn't free, but it's integrated into the chaining mode.  There are also some patented modes that provide a MAC for almost no extra computation(OCB, IACBC), and some proposed modes that combine an efficient, parallelizeable MAC with encryption in a secure way (CWC,GCM), though none of these are standards yet.

@_date: 2004-10-13 15:51:22
@_author: John Kelsey 
@_subject: Financial identity is *dangerous*? (was re: Fake companies, 
Okay, but there's a problem:  If you want to mug me personally, you have to show up where I am, catch me unaware, take some personal risk that I'll fight back or shoot you or something, or that a cop will happen by at an inopportune moment, or that there's some surveilance camera you don't know about catching the whole thing on tape.  At the end of that, you've done one mugging, and made maybe $100 or so.  This is why mugging, armed robbery, etc., is basically a crime for people who don't think too far ahead.   If you want to steal anonymous bearer assets from networked computers, you're going to contrive to do a whole lot of it at once, and you're going to have enormous incentives to develop new attacks to do so.  I have to care about attackers everywhere on Earth, and about the most capable getting past my defenses.  It's not like trying to keep random bored teenagers from breaking into your house by putting a proper lock on a properly installed door, it's like trying to keep a team of ex-SEALs, safecrackers, locksmiths, and demolition experts from breaking into your house.  Today, most of what I'm trying to defend myself from online is done as either a kind of hobby (most viruses), or as fairly low-end scams that probably net the criminals reasonable amounts of money, but probably don't make them rich.  Imagine a world where there are a few hundred million dollars in untraceable assets waiting to be stolen, but only on Windows XP boxes with the latest patches, firewalls and scanners installed, and reasonable security settings.  IMO, that's a world where every day is day zero.  All bugs are shallow, given enough qualified eyeballs, and with that kind of money on the table, there would be plenty of eyeballs looking.  And once it's done, several thousand early adopters are out thousands of dollars each.  This isn't much of an advertisement for the payment system.  It's anonymous and based on bearer instruments, so there's no way to run the fraudulent transactions back.  The money's gone, and the attackers are richer, and the next, more demanding round of attacks has been capitalized.  They also have to be able to do something about it.  What would you tell a reasonably bright computer programmer with no particular expertise in security about how to keep a bearer asset as valuable as his car stored securely on a networked computer?  If you can't give him an answer that will really work in a world where these bearer assets are  common, you're just not going to get a widespread bearer payment system working, for the same reason that there's probably nobody jogging with an iPod through random the streets of Sadr City, no matter how careful they're being.
--John Kelsey

@_date: 2004-09-01 10:37:10
@_author: John Kelsey 
@_subject: ?splints for broken hash functions 
I believe this falls to a generalization of the Joux attack, as well.  (Someone may have already noticed this.)  a.  I build a 2^{80} multicollision on h(m) using Joux' attack, requiring 80*2^{80} work.  b.  I now have 2^{80} different messages which are being hashed with the same IV.  I expect one pair of them to give me a collision.

@_date: 2004-09-03 09:36:49
@_author: John Kelsey 
@_subject: ?splints for broken hash functions 
Ah, good point.  I honestly was just trying to see if this
construction gave inherent resistance to the Joux attack, and it
doesn't.  That means you still have to do 2^{80} work to get your
first collision, but you can get multicollisions a lot more cheaply
than you'd expect.  Let's consider a slightly simpler version of the Practical
Cryptography scheme, and I'll show how to get K-collisions on it for
about lg(K)^2 2^{80} work, as opposed to lg(K) 2^{80} work for a
standard hash function.  I'll call this variant the "two pass hash:"
hash'(x) = hash( x || x )
a.  I produce a message with 80 * 80 = 6400 blocks, in which each
message block is a collision pair.  I thus do about 2^{93} work, in
order to use Joux' attack to find a 2^{6400}-collision on hash(x).
b.  I now break the message into superblocks of 80 blocks, which I'll
call B[0],B[1],...,B[79].  Note that for each of the 2^{80} possible
values for a superblock, the result of hash(x) is identical.  (In
practice, I'll probably need a few extra blocks in each superblock to
deal with the collision search taking longer than expected sometimes.)  c.  For each superblock, I try as many of the 2^{80} possible
sequences of message blocks as I need (which all collide for hash(x)),
until I find one that causes a collision in the second pass, as well.
The total work is dominated by the 6400*2^{80} search at the
beginning, and so we get a set of 2^{80} messages with the same result
from the two-pass hash using this variant of Joux' attack, for about
2^{93} work total.  Is there a better way to mount this kind of attack
on this scheme?  Unless I'm missing something, we can iterate this
attack for more passes in a pretty straightforward way: for P passes
of the hash over the message, when we want a K-collision, we need
lg(K) distinct pieces of the message which are 2^{n/2} collisions for
the P-1 pass hash. Now, applying this to the Practical Cryptography scheme is complicated
a bit by the fact that the message handled by the second pass doesn't
break into blocks in quite the same way (it's offset by the number of
bits in the hash output).  However, this doesn't prevent the attack,
it just restricts which bits of the message blocks you can play with
when finding collisions.  For concreteness, think of SHA1, with 5-word hash output blocks and
16-word message blocks.  Now, each of our superblocks from the first
pass of hashing are offset five words into the next superblock.  That
means that the last colliding message block in each superblock needs
to have no changed bits in its last 5 words.
So, what this means is that you can still find multicollisions on
these variant ways of hashing (the two-pass and Practical Cryptography
constructions) much more quickly than you'd expect from an ideal hash
function.  You can extend the result to convince yourself that you
can't get much more than 80 bits of collision resistance from
constructions like:
hash(X) = SHA1(X) || RIPE-MD160(SHA1(X)||X)
Comments?  --John Kelsey

@_date: 2004-09-03 09:51:06
@_author: John Kelsey 
@_subject: Implementation choices in light of recent attacks? 
Yes, but it does depend a little on what you're trying to defend against, right?  I mean, if you're worried about not having a strong hash function with a 128-bit output anymore, then it seems like you should always be able to truncate a stronger hash.  If I had a way to force the low 128 bits of SHA1 to collide much faster than 2^{64}, while randomizing the remaining 32 bits, I could use it to find full SHA1 collisions faster than 2^{80}work.  This doesn't work if my trick for getting collisions in 128 bits requires that the remaining 32 bits not collide, however.  (This can happen if you have a truncated differential through the whole hash function whose output value is (x,0,0,0,0), e.g., it forces a nonzero difference into the first word of output.  I believe this came up in Biham and Chen's SHA0 near collisions, requiring running the differential across two or more compression functions.  The same basic problem came up in Biham and Shamir's N-HASH results, many years ago.  So you can't get a simple reduction proof here, but maybe someone better at proofs can do a more complicated one....
If you're worried about cryptanalysis of existing MD5-like functions, then there's probably some benefit to looking at alternative designs that look radically different, like Whirlpool or Tiger.  But I'm not sure how much analysis either has seen, so I'd be reluctant to feel like I really understood their security yet.  It's clear from recent events that "designed by smart people" isn't enough by itself to give you lots of confidence in hash functions, any more than in block ciphers.  Finally, if you want to use truncation on hashes, make sure it's never possible to get the two sides confused about which hash is to be used.  There are a lot of places, such as KDFs, where you can get some really nasty attacks if you can get Alice to use SHA256 and Bob to use SHA256 with the output truncated to 224 bits.  (Yes, this is the reason SHA224 has a different starting IV than SHA256.) --John Kelsey

@_date: 2004-09-14 09:32:59
@_author: John Kelsey 
@_subject: will spammers early adopt hashcash? (Re: Spam Spotlight on 
It sure seems like one other impact of this is going to be that zombie machines can't do much spamming in the background, while letting the user of the machine think he still is in control of it.  I don't know whether they do that now, though.

@_date: 2004-09-20 10:03:57
@_author: John Kelsey 
@_subject: Academics locked out by tight visa controls 
I guess I've been surprised this issue hasn't seen a lot more discussion.  It takes nothing more than to look at the names of the people doing PhDs and postdocs in any technical field to figure out that a lot of them are at least of Chinese, Indian, Arab, Iranian, Russian, etc., ancestry.  And only a little more time to find out that a lot of them are not citizens, and have a lot of hassles with respect to living and working here.  What do you suppose happens to the US lead in high-tech, when we *stop* drawing in some large fraction of the smartest, hardest-working thousandth of a percent of mankind?

@_date: 2004-09-22 10:08:42
@_author: John Kelsey 
@_subject: Time for new hash standard 
Note that in the open world, there are very nice security proofs for existing MACs based on combining universal hashing with strong crypto components (such as block ciphers).  I gather that the classified world isn't as enamored of security proofs as we are, but it's pretty easy to see that it's harder to find a colliding pair of messages when you don't know the internal state, for almost any nontrivial function.  Even if you're doing a differential attack on the function, you can choose message blocks to make sure that your differential clears some rounds with probability one, and you get to do all your trial hashes offline, on your own equipment, rather than online on your intended victim's equipment.

@_date: 2005-08-06 16:03:45
@_author: John Kelsey 
@_subject: draft paper: "Deploying a New Hash Algorithm" 
.Subject: Re: draft paper: "Deploying a New Hash Algorithm" Yes!  I've noticed that it's really common for me to work on
a project for a very short time (like an hour or two), and
start noticing all kinds of security holes, including a lot
of stuff with nothing to do with cryptography.  I'll still
be asking very basic questions of the other people on the
project about how things are *supposed* to work, but be
pointing out attacks they never thought of at the same time.
I think this is just a different way of thinking.  Attackers
and security people do this all the time.  Most normal
people never do--it's like once they've got the rules in
their heads, that's what's possible, and they don't even
think about it.  How many times, working on security for some system, have
you pointed out an attack, only to hear some variation on
"but who would think of that?"  And you can see the same
thing happening in discussions of homeland security and
counterterrorism stuff.  It's like most people look at the
national guardsmen in the airport, and say "whew, I feel
safer," rather than "what the heck are those guys supposed
to do to stop hijacked planes crashing into buildings?" I like your starting points, but I think the real approach
to thinking about this is a bit broader.  It has to do with
understanding the rules, and trying to ask, for each one,
"and what makes me obey that rule?" or "what would happen if
I didn't do such and so?"  --John Kelsey

@_date: 2005-08-06 16:30:15
@_author: John Kelsey 
@_subject: solving the wrong problem 
A couple of these guys gave a talk at NIST recently.  The
thing is, I can think of a bunch of uses for the thing
they're doing.  This looks genuinely useful as a tool.
Whether they've worked out how to use the tool to best
effect is a different question.
The passport idea doesn't add much, as you pointed out.  The
reason is that the thing you care about there is that the
information on the passport hasn't been tampered with and
originated from the right source.  An identical copy of my
passport is no worse than the original.  On the other hand, think about the uses of this technology
for paper bearer instruments.  Design travelers' checks that
include a 2D barcode with a BLS signature, bound to the
piece of paper, and you can print the damned thing on
regular paper if the readers are cheap enough.  Similar
things apply to stamps, tickets, etc.  If you can get
readers into peoples' homes, you can even allow home
printing of tickets, travelers' checks, etc., each bound to
a specific piece of paper.  Add a reader to your favorite
DVD player platform (I think it's the same basic hardware as
is used in a DVD player), and you can uniquely sign content
on a disc, and use the player's hardware to enforce only
playing content when the disc's biometric matches the signed
content.  You could use the technique to scan small bits of
flat surfaces of all your stuff (the basic technique works
on paper, plastic, and metal, at least; I'm not sure if it
works on wood or glass), record the biometrics and locations
of the scans, and provide this to the police when your house
gets burgled.  There are some wonderful potential uses for
this technology in making paper-based voting systems *much*
more secure.  And on and on.  If I were in the business of
producing tamper-resistant paper, I'd be scared to death.
Yes.  As I said, sometimes this stuff looks almost useless
(like quantum cryptography), other times it looks like it
may provide powerful tools, despite the fact that its
designers don't know much about how to use those tools yet.
The same is often true in cryptography, where we have some
very theoretical work which sometimes ends up having
enormous practical consequences.  In my consulting days, I used to use the term "padlocking
the screen door" for the related phenomenon of piling
security on one part of the system while ignoring the bigger
vulnerabilities.  But this is a bit different....
--John Kelsey

@_date: 2005-08-08 10:21:00
@_author: John Kelsey 
@_subject: Possible non-extension property for hash functions 
[Talking about the length-extension property.]
There are actually a couple ways to do this.  Either:
a.  Process the message sequentially, but do a final
operation on the intermediate result before putting out an
output hash, which introduces new collisions.  Any truncated
hash like SHA384 or SHA224 does this.  As an added benefit,
once you truncate enough bits (SHA384 truncates 128 bits),
the length extension attack on prefix MAC goes away, and the
Joux multicollisions and long message second preimage
attacks Bruce and I came up with become much more
difficult.  (On the other hand, it doesn't seem easy to do
any nice reduction proof from the strength of the SHA256
compression function to the strength of the SHA384 hash
b.  Process the message multiple times, or give yourself
random access, or whatever.  Just processing through the
message twice sequentially does eliminate the simple
length-extension property, but there are variations on it
that can still be used--that's why Joux multicollisions can
be found even when you process the message twice
sequentially. Are there other ways I'm not seeing to do this?   --John Kelsey

@_date: 2005-08-11 19:07:20
@_author: John Kelsey 
@_subject: How much for a DoD X.509 certificate? 
Ah, so this was more of an attribute certificate, then.  And
that the certificate was issued based partly on a
nonstandard proof of possession protocol.  (More
specifically, "proof of possession with intent to

@_date: 2005-08-22 16:43:29
@_author: John Kelsey 
@_subject: herding attack paper submitted to ePrint archive 
Yoshi and I have submitted a draft of the Herding Hash Functions
paper up on the IACR ePrint server, and assuming there are
no problems, it should be up reasonably soon.  The core of
the result is that when I can find lots of collisions for a
hash function by brute force (or maybe analytically, though
that gets more complicated), I can also break most systems
that use a hash function to prove prior knowledge.  I gave a
rump session talk on this a few days ago at Crypto.
--John Kelsey, NIST, August 2005

@_date: 2005-08-23 21:42:02
@_author: John Kelsey 
@_subject: Another entry in the internet security hall of shame.... 
Recently, Earthlink's webmail server certificate started showing up as expired.  (It obviously expired a long time ago; I suspect someone must have screwed up in changing keys over or something, because the problem wasn't happening up until recently.)  So, I contacted Earthlink's technical support, and I got this really encouraging reply....
Dear John Kelsey,
Thank you for contacting us.
I understand that you are having problems viewing Webmail and that it send out an
error on SSL certificate.
I suggest that you try lowering the security settings of your Internet Explorer.
Please follow the steps below on how to lower the security settings on your Internet
1. Open Internet Explorer.
2. On the Task panel click on Internet Options.
3. Click on the Advance Tab.
4. Scroll down and uncheck [Warn about invalid site certificates].
5. Remember to click on Apply.
6. Click on OK.
You have successfully lower your Internet Explorer settings.
Should you have any other concerns, please get back to us. You will receive a prompt
Therese B. 3613
EarthLink Electronic Customer Support
EarthLink, Inc.
Case ID 69080634
Looking for easy access to news, stocks, sports, and your favorite links?
With the EarthLink Personal Start Page you can customize everything from
the background colors to your local weather. For more information please
visit Resolve your customer service questions on-line at our Account maintenance
web site. To add email mailboxes, change passwords, or update your credit
card information, go to:
You can also trade real-time messages with one of our friendly Live Chat
Or email us and get a response that day:
Original Message Follows:
*	Presented Article: 142454
*	Name: John Kelsey
*	Email: kelsey.j at ix.netcom.com
*	Account Type: EarthLink Experience
*	Issue: Spam/Internet Fraud Problem
*	Detailed Issue: Report an Issue
*	Article Title: Protecting Yourself Against Email/Internet Fraud
*	Message Body: The SSL certificate for webmail.earthlink.net is
expired. The webmail.atl.earthlink.net certificate is fine, it's just
the webmail.earthlink.net certificate.

@_date: 2005-02-03 09:55:15
@_author: John Kelsey 
@_subject: Is 3DES Broken? 
Yep.  In fact, there's a birthday paradox problem for all the standard chaining modes at around 2^{n/2}.  For CBC and CFB, this ends up leaking information about the XOR of a couple plaintext blocks at a time; for OFB and counter mode, it ends up making the keystream distinguishable from random.  Also, most of the security proofs for block cipher constructions (like the secure CBC-MAC schemes) limit the number of blocks to some constant factor times 2^{n/2}.  --John Kelsey

@_date: 2005-02-16 12:48:32
@_author: John Kelsey 
@_subject: SHA-1 cracked 
Well, there *weren't* any a week ago....
--John Kelsey

@_date: 2005-02-17 09:57:16
@_author: John Kelsey 
@_subject: SHA-1 cracked 
Yep.  The thing that's interesting here is that the more-or-less obvious fallbacks for SHA1 are RIPE-MD160 and SHA256/512.  But given the pile of bodies in front of Wang's door already (MD4,MD5, Haval, RIPE-MD, SHA0, SHA1), it's hard to have any confidence at all that RIPE-MD160 will survive long.  All the remaining SHA functions are the same, modulo some constants and the wordsize used--SHA512 is just SHA256 using 64-bit words, different constants, and a few more rounds.  So there's really only one SHA function left.  It's different enough from SHA1 that it's plausible Wang's attacks won't work, but I can't see any really strong reason to trust in that.  Whirlpool looks like the best bet for a fallback right now,  but it really hasn't seen anything like the amount of analysis I'd like.   This is what it looks like when someone develops a new class of attack that breaks a whole bunch of your available cryptographic primitives in a big hurry.  --John Kelsey

@_date: 2005-02-17 10:20:07
@_author: John Kelsey 
@_subject: SHA-1 cracked 
Anyone know where we could find the paper?  It'd be kind-of convenient when trying to assess the impact of the attack if we knew at least a few details....
If it's really the case that the attack requires colliding messages of different sizes (that's what this comment implies), then maybe the attack won't be applicable in the real world, but it's hard to be sure of that.  Suppose I can find collisions of the form (X,X*) where X is three blocks long, and X* is four blocks long.  Now, that won't work as a full collision,  because the length padding at the end will change for X and X*.  But I can find two such collisions, and still get a working attack by concatenating them.  --John Kelsey

@_date: 2005-02-17 10:20:07
@_author: John Kelsey 
@_subject: SHA-1 cracked 
Anyone know where we could find the paper?  It'd be kind-of convenient when trying to assess the impact of the attack if we knew at least a few details....
If it's really the case that the attack requires colliding messages of different sizes (that's what this comment implies), then maybe the attack won't be applicable in the real world, but it's hard to be sure of that.  Suppose I can find collisions of the form (X,X*) where X is three blocks long, and X* is four blocks long.  Now, that won't work as a full collision,  because the length padding at the end will change for X and X*.  But I can find two such collisions, and still get a working attack by concatenating them.  --John Kelsey

@_date: 2005-01-07 12:40:39
@_author: John Kelsey 
@_subject: entropy depletion (was: SSL/TLS passive sniffing) 
Right.  The critical question is whether the PRNG part gets to a secure state, which basically means a state the attacker can't guess in the amount of work he's able to do.   If the PRNG gets to a secure state before generating any output, then assuming the PRNG algorithm is secure, the outputs are indistinguishable from random.  The discussion of how much fresh entropy is coming in is sometimes a bit misleading.  If you shove 64 bits of entropy in, then generate a 128-bit output, then shove another 64 bits of entropy in, you don't end up in a secure state, because an attacker can guess your first 64 bits of entropy from your first output.  What matters is how much entropy is shoved in between the time when the PRNG is in a known state, and the time when it's used to generate an output.  --John Kelsey

@_date: 2005-01-10 13:07:32
@_author: John Kelsey 
@_subject: Entropy and PRNGs 
... Well, the broader problem isn't the context, it's the model.  If your attacker (who lives sometime in the future, and may have a large budget besides) comes up with a better model to describe the process you're using as a source of noise, you could be out of luck.   The thing that matters is H(X| all information available to the attacker), which is based on P(X | all information available to the attacker), which includes a model that may be better than yours.
But I think it's of practical value to consider the different attackers whose information might not include some information you use for seeding a PRNG.  Some sources of entropy, such as packet arrival times, are not worth much for attackers on your local network who are attacking you in real time, but are quite valuable against attackers who attack you later.  Other sources of entropy, such as the hash of the contents of your Windows registry, or a full directory tree from your hard drive, are worthwhile against real-time attackers without access to your machine, but worthless against  attackers with your machine in their hands.  Using cheaply-available sources of each kind in seeding a PRNG decreases the set of attackers that will be able to attack you,  while not preventing you from also using some source of entropy you believe to be good against all attackers.  Differentiate between measures of entropy.  Collision entropy (Renyi entropy of order two) is very useful in determining how many samples you can take before expecting a collision, and it's not conditioned on an attacker's information.  And collision probabilities do matter, in both obvious and subtle ways, for PRNG security.

@_date: 2005-01-27 09:46:50
@_author: John Kelsey 
@_subject: entropy depletion 
This is a good point.  In the ANSI X9.82 work we've been doing (working on a standard for random number generation for cryptography), we kind-of make a continuum:
PRNGs seeded once --> PRNGs with live entropy sources --> full entropy PRNGs The idea here is that you can use a PRNG algorithm in a mode where it's seeded once at the factory and runs forever, or where it has access to an entropy source but has to produce output bits faster than the entropy source can, or where it produces outputs that include as many bits of entropy as bits of output.  Any good PRNG algorithm can be run in all three of these modes, with a bit of thought.  (We have our own terminology for all this in X9.82; we call a PRNG a "DRBG" and a random bit generator producing full entropy an "NRBG".)  We also distinguish among full-entropy RNGs that include a strong PRNG and those that are pure hardware based.  When you're running the PRNG in a full-entropy mode (we give constructions for this) you get a guaranteed fallback to a secure PRNG even if your entropy source fails.  If you're using a pure hardware-based RNG and the hardware fails, you're out of luck.  If there's really no entropy ever entered, then no PRNG algorithm can help you.  If we ever get to an unguessable state, then Yarrow should (barring some clever cryptanalysis) stay in a secure state for as long as we need to use it.  The tricky bits seem to happen in the middle--when the entropy trickles in at a slower rate than expected.  That's what Yarrow's two pool reseeding strategy is for, and what Niels Ferguson's Fortuna design does in a pretty-close-to-optimal way.  I think these strategies are interesting, but as I've worked on X9.82, I have become a lot more concerned with getting the PRNG to a secure starting point than with recovering later.  Recovering is important, too, but a lot of real-world systems use their first PRNG state to generate their high-value signing key, or the session key used to communicate their high-value secrets to some server, or whatever.    --John Kelsey

@_date: 2005-01-27 10:06:04
@_author: John Kelsey 
@_subject: entropy depletion 
So, the big issue here is that  we're counting on a cryptographic algorithm to both provide full entropy outputs and to mask the different outputs from one another.  There's no guarantee that it can do either.  That is, even if another 160 bits of entropy have been put into the pool, there's no guarantee that there will be no relationship between the next 80 bit output and the last one.  That depends on your beliefs about SHA1, and about unproven properties of it. (It's been a long time since I've looked at the algorithm used by /dev/random, but I think there are some narrow pipe issues there which might limit the total entropy that can affect a sequence of outputs from a sequence of inputs.)  --John Kelsey

@_date: 2005-01-30 11:12:05
@_author: John Kelsey 
@_subject: Simson Garfinkel analyses Skype - Open Society Institute 
One thing most people seem to miss about this, though, is that cellphones and cordless phones are *great* for privacy from other humans who live in your house or work in your office.  When you don't want your children to hear a conversation, you can go take the call in the bathroom or in the car while you're driving alone.  Everybody seems to miss this--cellphones and cordless phones don't diminish privacy, they just move it around.  Sophisticated eavesdroppers can violate more of your privacy, but nosy family members, roommates, and office mates can violate a lot less.  I thnk most people correctly evaluate which of these groups is more likely to do something unpleasant with what they learn by eavesdropping.  It seems to me that VOIP pushes this in a somewhat different direction, because it's probably easy for your high-speed internet access (maybe a wireless hop to a router that talks to a cable modem) to be eavesdropped by moderately technically savvy nosy neighbors, and because there are a lot of criminals who are using more technology, and will surely target VOIP if they think they can make any money off it.  --John Kelsey

@_date: 2005-01-31 10:31:09
@_author: John Kelsey 
@_subject: Simson Garfinkel analyses Skype - Open Society Institute 
I think there are two parts to this.  First of all, this may be a case of simply not understanding the implications of the loss of privacy to sophisticated eavesdroppers.  I tend to think this is the case with a lot of privacy issues (like grocery store bonus cards), but not here.  Second, this may be a correct evaluation of the relative risks.  Until the set of eavesdroppers who listen in on digital cellphone traffic becomes pretty large, most people aren't very interesting targets for eavesdropping, at least not in terms of making any profit from it.  Most people don't have a lot of money or power to reward blackmailers, aren't in a position to leak confidential, high-value data, aren't likely to end up in some powerful elective or appointed office, and aren't discussing information that would let an eavesdropper make a profit from it directly.  On the other hand, keeping secrets from your parents about who you're dating and what you're doing with them is really common among teenagers.  Embarassing personal revelations that you would be humiliated to disclose to your coworkers or roommates are a lot less embarassing if they're heard by some FBI agent who listens in all day--that guy is going to be as hard to shock as a priest or a doctor, and he'll presumably never show up at work and start a rumor about you.  High tech criminals trolling for blackmail material might be interested in your affair with your best friend's wife, but not if you don't have any substantial assets lying around waiting to be sold off for hush money.  Probably most people haven't thought through this at great depth, but I think most people who think of the FBI listening in on their calls aren't all that concerned about the consequences to themselves, and I think they're correct.
This isn't an endoresment for cordless/cell/VOIP phones without crypto, just a comment about why it's hard to get people to pay extra for adding crypto to those phones.

@_date: 2005-07-05 10:42:12
@_author: John Kelsey 
@_subject: /dev/random is probably not 
I think you're landing on the genuinely hard problem here.
Designing a PRNG intelligently is an exercise in design and
cryptanalysis, so long as you get to assume that you know
how much entropy you're getting.  But actually getting
reliably entropy estimates is:
a.  A data analysis problem, where you never really get a
final answer, you just get the best model you knew how to
test and don't find strong evidence to discard it, and b.  Enormously sensitive to implementation details that are
hard to deal with in software.  In a hardware RNG design,
you can at least analyze test-mode raw outputs of the ring
oscillator or whatever, build a statistical model, and know
that the same basic model will also describe the devices in
the field.  They may vary because of manufacturing defects,
changes in the field (like heating/cooling or component
failure), etc., but you at least know what kind of thing
you've got.  With software sources, there's pretty much no
limit to what changes the designer of the hardware is
allowed to make to your devices, so long as he keeps the
interface the same.  You do lots of analysis on a machine
with a spinning disk drive, and end up on one with one
networked drive and one flash drive, or some such horrible
thing.  Additionally, building a probability model for stuff you
observe on a general purpose computer is *hard*, because
there's so much complicated deterministic stuff going on.
Even if you're using the same machine and setup to collect
entropy in production as you did to build your probability
model for entropy estimation, it's hard to have enormous
confidence in the correctness of your estimates.  How much
of that apparently random behavior you were getting when you
sampled the cycle counter in a tight loop was because of
genuine unpredictability, and how much was because of the
very patterned but complicated stuff going on behind the
scenes on your machine?

@_date: 2005-07-10 02:50:01
@_author: John Kelsey 
@_subject: halloween hash bash reminder--July 15 deadline 
This is just a reminder that the NIST hash workshop (Oct
31-Nov 1 of this year) is still taking submitted talks,
abstracts, etc., until July 15.  There are no proceedings,
so there should not be any problem publishing things that
you discuss at this workshop.  A major goal of doing this is
to get people to discuss interesting ongoing work so we can
understand it now, rather than after we've made decisions
about how to react to all the excitement in the hash
function world.  (For what it's worth, I plan on presenting
some new hash function results with Yoshi Kohno that we
intend to publish somewhere else.  I expect we'll post these
on the ECRYPT server before that.)
This workshop is going to have a big impact on decisions
like whether we should do some AES-like process to get a new
hash function standard, whether we should try to standardize
on some additional algorithms (like Whirlpool or the Russian
standard GOST hash function), etc.  Taking part is a great
opportunity to influence those decisions.  If you have
something new to say about hash functions, or something old
that should be repeated, send us some slides, or at least an
extended abstract, and we'll see whether we can fit you onto
the agenda for some discussion time.  --John Kelsey, NIST, July 2005

@_date: 2005-07-14 11:58:34
@_author: John Kelsey 
@_subject: ID "theft" -- so what? 
One nontrivial reason is that many organizations have spent
a lot of time and money building up elaborate rules for
using PKI, after long negotiations between legal and
technical people, many hours of writing and revising,
gazillions of dollars in consultants' time, etc.  So,
anytime you start doing anything involving public key
cryptography, all this machinery gets invoked, for
bureaucratic reasons.  That is, you've now trespassed on PKI
turf, and you'll have to comply with this enormous set of
I know of a couple cases where this led to really irritating
results.  In one, a friend of mine was using a digital
signature to verify some fairly trivial thing, but was told
it was against policy to use a digital signature without the
whole PKI.  (I believe he changed the documentation, so that
he was using a "modular arithmetic checksum" instead of a
signature verification.) As a consultant, I designed and evaluated a lot of systems
that used public key cryptography.  None of the successful
ones tried to use the whole X.509 + CRL + CPS + everything
else overhead--typically, they just used a one-deep
hierarchy, where the keypair was put into the device by the
manufacturer along with a copy of the top-level public key
used to sign all device public keys.  This works, because it
doesn't try to incorporate the output of 20 years of
make-work programs in cryptography (they weren't intended
that way, but that's largely how they turned out), and it
doesn't try to incorporate every idea that might be useful
anywhere in the world into some very limited and simple
--John Kelsey

@_date: 2005-07-27 13:35:23
@_author: John Kelsey 
@_subject: Possibly new result on truncating hashes 
I have what seems like a new and interesting result, which I
haven't seen before, but which may or may not be new.  The high order bit is that you can't generally guarantee
that truncating your hash (chopping off some bits) won't
weaken it.  That is, if you chop SHA256 off to 160 bits as a
replacement for SHA1 (something I'm working on with Niels
Ferguson for X9 right now), it's possible that there's no
attack on SHA256, but there is an attack on SHA160.  How could this work?  Suppose we have an algorithm like the
Wang attacks on MD5, SHA0, or SHA1 for finding a single
collision pair.  The algorithm returns a single collision
pair on the first 160 bits of SHA256 for (say) 2^{64} work.
(Remember that this is just an example--I don't have any
such algorithm!)  Each time the algorithm is run, it gives a
new, unrelated collision pair, and the remaining 96 bits are
completely randomized by the collision pair.  Now, this is an attack on SHA256 truncated to 160 bits.
Does it lead to an attack on SHA256 as a whole?  If it does,
then we can make a reduction proof that says that the
truncated hash is strong if the original hash is strong.
Unfortunately, we can't make this argument, because this
postulated collision algorithm can't be used to find a
collision in the whole SHA256 more efficiently than brute
Let's do the counting argument:  Each time we call the
160-bit collision algorithm, we get a new pair which has the
same first 160 bits of SHA256 output, and random unrelated
last 96 bits of SHA256 output.  Each pair has a probability
of 2^{-96} of colliding in the remaining bits.  So, to get a
collision on the whole SHA256 using this 160-bit collision
algorithm, we expect to have to try about 2^{96} collision
pairs, each found at a cost of 2^{64}.  The resulting work
is 2^{64} * 2^{96} = 2^{160}, more than a straight
brute-force collision search on SHA256.  What does this mean?  It means that just because you have a
good 256-bit hash, you can't necessarily make a good 160 bit
hash from it.  You might be able to--it seems like you
usually will be able to--but there's no guarantee.  Comments?  Is this some well-known result that I'm

@_date: 2005-06-06 09:49:01
@_author: John Kelsey 
@_subject: Papers about "Algorithm hiding" ? 
I think you need one more step here to get the protective coloration
effect you'd like, where encrypted files aren't automatic evidence of
wrongdoing: During installation, generate 50 or so random passwords
with too much entropy to feasibly guess (easy to do when no user need
ever remember them), and encrypt some reasonable-length files full of
binary zeros with them.  The number of randomly-generated files needs
to be randomized, naturally, and probably should follow some kind of
distribution with a big tail to the right, so that it's not that
uncommon for a random install to put several hundred encrypted files
on the drive.  The value of this is that an attacker now sees
encrypted files on every machine, most of which nobody on Earth can
decrypt.  If this is normal, then it's not evidence.  (There are
probably a bunch of issues here with putting plausible tracks in the
logs, datestamps on the files, etc.  But it seems like something like
this could work....)
Huh?  There have been effective tools for protecting data from
disclosure for a long time, though it's not clear what good they'd do
for a company whose whole business was just selling access to that
data for a fee.  I'll bet the Choicepoints of the world are pretty
careful protecting, say, their payroll and HR records from disclosure.
It's just *your* data they don't mind giving out to random criminals.
No amount of crypto could have helped this.
--John Kelsey

@_date: 2005-06-07 09:52:30
@_author: John Kelsey 
@_subject: Papers about "Algorithm hiding" ? 
[My comment was that better crypto would never have prevented the
Choicepoint data leakage. --JMK]
So, this argument might make sense for some small business, but
Citigroup uses a *lot* of advanced technology in lots of areas, right?
I agree crypto programs could be made simpler, but this is really not
rocket science.  Here's my guess: encrypting the data would have
required that someone make a policy decision that the data be
encrypted, and would have required some coordination with the credit
agency that was receiving the tapes.  After that, there would have
been some implementation costs, but not all *that* many costs.
Someone has to think through key management for the tapes, and
that's potentially a pain, but it's not intractible.  Is this really
more complicated than, say, maintaining security on their publically
accessible servers, or on their internal network?  ... Well, Choicepoint is a bit different, right?  I mean, as I understand
it the big disclosure happened because they sold peoples' data to
criminals, but they were in the business of selling peoples' data.
They just intended to sell it only to people of good intention, as far
as I can tell.  (Perhaps they should have demanded X.509 certificates
from the businesses buying the data and checked the "evil" bit.)  I
just can't see how cryptography could have helped prevent that attack,
other than by making the data that Choicepoint depends on harder to
get in the first place.
But this does no good whatsoever if there's not some reason for the
people holding the data to use those tools.  Everyone with a network
presence and any kind of high profile does, in fact, use moderately
complicated computer security tools like routers, firewalls, VPNs,
virus scanners, and spyware detectors.  Everyone has to deal with
keeping their boxes up to date on patches.  However imperfectly, it
seems like Citigroup and Choicepoint and the rest can actually do
those things.  So when you excuse their failures to secure customer
data with "the tools aren't there," this sounds absolutely implausible
to me.  I'm not crazy about a HIPAA-style mandate for encryption and shredders
either, but we have this basic problem:
a.  It's basically easy to buy or find some amount of data about many
b.  It's basically easy to use that amount of data to get credit in
their name.
I suspect a better solution than trying to regulate data brokers is to
make it more expensive to give credit to Alice under Bob's name.  The
thing that imposes the cost on me isn't when someone finds my SSN,
it's when someone takes out a bunch of loans which I'm then expected
to pay back.  Then it becomes my problem to resolve the disputes
created by the lender's desire to extend credit at minimal cost.  (The
lender also loses money, of course.  But much of the cost is shifted
to the identity theft victim.)  --John Kelsey

@_date: 2005-06-13 16:34:11
@_author: John Kelsey 
@_subject: expanding a password into many keys 
There's a length extension property with what you're doing, so if I
get to choose your key names, I can do something unpleasant to you.
Suppose I know the length of pass, and get to choose two key names,
K1_name and K2_name.  You give me K1 = sha1( pass||K1_name), then I
need to guess K2_name.  I can choose K2_name to be K1_name,
appropriately padded to the full block size exactly as it will be in
the SHA1 computation that produces K1.  Then, I can compute K2 on my
own, because the only effect of the secret value "pass" on K2 is going
through K1.  This doesn't look like an especially realistic attack model, but I'm
not sure what you're doing with this....
--John Kelsey

@_date: 2005-06-14 09:07:00
@_author: John Kelsey 
@_subject: Collisions for hash functions: how to exlain them to your boss 
Hang on a minute.  The issue isn't whether your data format is being
executed (in some sense almost any nontrivial data format can be seen
as a scripting language interpreted by the viewer).  The issue is that
I can make two different documents, one of which displays exactly what
you tell me you want it to display, the other of which displays
anything I like, with the same MD5 (MD4, RIPEMD, SHA0, SHA1) hash
output.  You can view the document on a viewer you trust, without any
security vulnerabilities in the viewer/data format, but you still
get fooled.  Saying "inspect the code/data/whatever" as an answer to this problem
isn't too useful, since an inspection intended to turn up security
problems may not turn up the fact that the executable code has some
region of data in it which could be varied in just the right way for
the MD5 or SHA1 attacks to work, without making it an invalid
program.  (I've been thinking about how these attacks apply to
validated software in voting and gaming machines....)
Fundamentally, it's just really hard to safely use a hash function for
which collisions can be found cheaply.  It requires every crypto
engineer to become an expert on both how the collision attack works
(and all the possible variants!) and also an expert on what ambiguity
may exist in each data format he may ever want to hash.  There's an obvious solution to this in principle, though the huge
installed bases of MD5 and SHA1 make it painful....
-John Kelsey

@_date: 2005-06-15 09:18:50
@_author: John Kelsey 
@_subject: NIST Public Workshop on Cryptographic Hashes 
[Discussing the NIST public hash function workshop.]
Informally, we're calling this the halloween hash bash.  Come dressed
as your favorite hash function!  If you want to have some impact on
where we go with hash functions, this is a good thing to attend....
--John Kelsey, NIST

@_date: 2005-06-15 09:27:17
@_author: John Kelsey 
@_subject: expanding a password into many keys 
Actually, there are quite a few attacks on these schemes that amount
to messing up that property--getting the message to span multiple
blocks (as with the length extension attacks).  The other thing that
any direct use of a crypto primitive can't help with is "sliding" data
between fields.  If you don't encode your fields in some parseable way
before feeding them into the hash/MAC/block cipher/whatever, you get
these weird attacks where I either request key "XXX" with optional
additional string "Y", or I request key "XX" with optional additional
string "XY", and I get the same result.  Usually this doesn't lead to
a practical attack, but it puts an extra burden on the engineer using
the KDF, since he or she had better understand this attack and make
sure it doesn't apply.  Adams, Kramer, Mister, and Zucherato have a
recent paper pointing out a bunch of these problems with widely used
KDFs.  I think of these as analogous to the length-extension property
of hash functions or the complementation property of DES--it doesn't
keep the crypto mechanism from being used securely, but it does make
the job of an engineer trying to use it needlessly more complicated.  --John Kelsey

@_date: 2005-06-15 10:04:24
@_author: John Kelsey 
@_subject: Collisions for hash functions: how to exlain them to your boss 
[Discussing the MD5 attacks and their practicality, especially the
recent postscript demonstration.]
I think our disagreement here has to do with what we're
seeing from the attack.  You're seeing a specific attack
vector--use conditional execution/display + the ability to
find specific collisions of a particular form to yield these
nice attacks where we have two messages that amount to
X ||M0||M1
where when the first part of the message is X, some kind of
conditional execution displays M0, while X* leads to the
display of M1.  And I think you're right to say that in many
cases, once you're viewing the result of blindly executing
programs that I send you, you're vulnerable to other attacks
that are about as damaging.  Now, it's certainly possible
imagine cases where this kind of conditional execution
wouldn't be allowed to access anything outside the file, but
once you've decided to put in a full featured scripting
language, it's not that much of a stretch to think you'll
let me read the system time.
I'm seeing a more general pattern of attacks, in which X and
X* amount to context for the display of whatever follows
them.  That seems to me to encompass a lot more than macros
and script files with conditional execution.  And even when
I don't have a specific attack in mind, it worries me that
if I'm trying to help someone safely use MD5, I've got to
think through whether there is any way at all to make this
kind of attack pattern work.  It's a heck of a lot easier to
say "don't use MD5."
--John Kelsey

@_date: 2005-03-23 09:33:09
@_author: John Kelsey 
@_subject: NSA warned Bush it needed to monitor networks 
I think a bigger issue here is a sort of rational (to the bureaucrat) risk aversity: if he declassifies something and it turns out he's leaked something valuable (in the eyes of his boss), he's in trouble.  As long as there's no cost to stamping "secret" or "FOUO" on every document his office produces, this is safer for him than any other course of action.   Along with this, going through a document to make sure there's nothing secret in there is a lot more work than just classifying it.  The same logic works in the private world--how much of the stuff you've seen under NDA was genuinely going to cause a problem to the company that produced it, if someone just posted it to their website?
I suspect something very similar happens with the watchlists.  I wonder how many different layers of watchlist there are by now....
--John Kelsey

@_date: 2005-11-17 11:28:46
@_author: John Kelsey 
@_subject: timing attack countermeasures (nonrandom but unpredictable 
Let's assume d(k,x) is a random function of k||x, uniformly
distributed between 0 and T where T is the average time of the
encryption.  I choose a set of inputs to the cipher x[0,1,2,...,n-1]
so that if my guess of some part of k is right, I expect their total
timing to be much lower than the average case.  I get back Sum(f(k,x[i])+d(k,x[i])+r[i]).  Suppose my guess is wrong.  Now what we expect is:
a.  Sum(f(k,x[i]) = average value b.  Sum(d(k,x[i]) = average value
c.  Sum(r[i])     = average value
Suppose my guess is right.  Now what we expect is:
a.  Sum(f(k,x[i]) = unusually low value b.  Sum(d(k,x[i]) = average value
c.  Sum(r[i])     = average value
So, right guesses still give me unusually low values, and wrong
guesses still give me average-looking values.  That means the timing
channel is still there--d(k,x) only adds random noise.
The only way to avoid this is to make d(k,x) somehow related to
f(k,x).  That's the idea behind things like having software or
hardware go through both the 0 and 1 case for each bit processed in an
exponent.  In that case, we get d(k,x) being fast when f(k,x) is slow,
and vice versa, and we close the timing channel.  As long as d(k,x) is independent of f(k,x), I can still test guesses
of parts of k or parts of x.  --John Kelsey

@_date: 2005-11-17 12:10:53
@_author: John Kelsey 
@_subject: the effects of a spy 
Nipick: The system was Clipper, the algorithm was Skipjack.  Three comments here:
a.  Maybe they really do have a good generic differential-probability
limiting algorithm.  There are algorithms like this in the public
world (they can be really computationally expensive, and they only
tell you upper bounds on a subset of possible attacks), and you'd
expect NSA to be interested, since they design a lot of algorithms.
It's not so intuitive to me that this would have applied to impossible
differentials unless they designed it to, though.  In that case,
you're looking at differentials that can't appear instead of
differentials that appear too often.
b.  Maybe they don't care that much about attacks that require some
huge number of plaintexts.  The academic world has defined the game in
terms of total work being the critical parameter in the attack, and
we're seeing a push over time to move that to total attack cost.
(That is, it's not so interesting if you have a 2^{100} attack on a
128-bit block cipher, if the attack is impossible to parallelize.)  If
someone publishes an attack on Twofish tomorrow which requires 2^{96}
plaintexts to break it faster than brute-force, we'll all agree that's
an attack.  But there's no reason NSA has to think that way--maybe
they have some other parameter like 2^{n/2} texts for n-bit block
ciphers, after which they don't care about attacks because they're not
practical.  c.  Maybe they just got it wrong.  SHA0 and SHA1 demonstrate that this
is all too possible.  (It's quite plausible to me that they have very
good tools for analyzing block ciphers, but that they aren't or
weren't sure how to best apply them to hash functions.)  Reducing Skipjack to 31 rounds wouldn't make a practical trapdoor
appear!  You're still talking about 2^{34} chosen plaintexts!
--John Kelsey

@_date: 2005-11-30 13:54:10
@_author: John Kelsey 
@_subject: Encryption using password-derived keys 
I think this is sensible for convenience reasons:
a.  You can now change passwords without decrypting and re-encrypting
all your data.  And similarly, if you ever had a reason to change
keys, you could do that without changing passwords.  b.  You can now check the correctness of the entered password when you
decrypt the data encryption key (using authenticated encryption!),
rather than needing to process the whole data.  (You could also just
generate a few extra check bits from PBKDF2.)  You almost certainly need to do encryption and authentication both on
your bulk data and your encrypted key.  So why not do some mode that
does both (CCM being the obvious choice) for both the key encryption
and the bulk data encryption?
salt = PRNG_output(128)
iteration_count = 1000000
nonce = current nonce
DEK = PRNG_output(128)
KEK = PBKDF2(password,salt,iteration_count,128)
KeyBlob = CCM(KEK,0,DEK)
BulkData = CCM(DEK,nonce,plaintext)
I'll toss two other random ideas out there to see if they're useful to
a.  You may be worried about having a properly seeded PRNG available
to do your data encryption key generation.  I think a sensible way
around this is to use both a PRNG output and some extra bits from
PBKDF2 to derive the first data encryption key.  Like you could do:
X = PBKDF2(password,salt,iteration_count,256)
KEK = left 128 bits of X
S = right 128 bits of X
DEK = S xor PRNG_output(128)  b.  You can use a clever trick by Abadi, Lomas and Needham to save
yourself most of the work you do on iterating the password hash during
the creation of the KEK, but not when rederiving it.  Basically, what
you do is instead of setting an iteration count of 2^{21}, you
generate a big random salt, and omit 20 bits of it from the salt
that's stored with the encrypted file.  This forces anyone trying to
re-derive the KEK to do about 2^{20} work on average, but it makes
generating the original encrypted file almost free.  I'm always
surprised that this isn't used more often, because it's such a clever
--John Kelsey

@_date: 2005-10-24 15:57:32
@_author: John Kelsey 
@_subject: [fc-discuss] Financial Cryptography Update: On Digital 
Sent: Oct 24, 2005 2:14 PM
More to the point, an irreversible payment system raises big practical
problems in a world full of very hard-to-secure PCs running the
relevant software.  One exploitable software bug, properly used, can
steal an enormous amount of money in an irreversible way.  And if your
goal is to sow chaos, you don't even need to put most of the stolen
money in your own account--just randomly move it around in
irreversible, untraceable ways, making sure that your accounts are
among the ones that benefit from the random generosity of the attack.
The payment system operators will surely be sued for this, because
they're the only ones who will be reachable.  They will go broke, and
the users will be out their money, and nobody will be silly enough to
make their mistake again.

@_date: 2005-10-25 10:20:05
@_author: John Kelsey 
@_subject: [fc-discuss] Financial Cryptography Update: On Digital 
Well, one difference is that those transactions can often be undone,
if imperfectly at times.  The whole set of transactions is logged in
many different places, and if there's an attack, there's some
reasonable hope of getting the money back.  And that said, there have
been reports of spyware stealing passwords for online banking systems,
and of course, there are tons of phishing and pharming schemes to get
the account passwords in a more straightforward way.   The point is,
if you're ripped off in this way, there's a reasonable chance you can
get your money back, because the bank has a complete record of the
transactions that were done.  There's no chance of this happening when
there's no record of the transaction anywhere.  I don't think so.  Suppose there's a widespread attack that steals
money from tens of thousands of users of this payment technology.
There seem to be two choices:
a.  The payment system somehow makes good on their losses.
b.  Everyone who isn't dead or insane pulls every dime left in that
system out, knowing that they could be next.  It's not even clear that these are mutually exclusive, but if (a)
doesn't happen, (b) surely will.  Nobody wants their money stolen, and
I don't think many people are so confident of their computer security
that they're willing to bet huge amounts of money on it.  If you have
to be that confident in your computer security to use the payment
system, it's not going to have many clients.

@_date: 2005-10-26 02:13:18
@_author: John Kelsey 
@_subject: On the orthogonality of anonymity to current market demand 
The prerequisite for all this is that when the asset changes hands,
it's very nearly certain that this was the intention of the asset's
previous owner.  My point isn't to express my love for book-entry
payment systems.  There's plenty to hate about them.  But if the
alternative is an anonymous, irreversible payment system whose control
lies in software running alongside three pieces of spyware on my
Windows box, they probably still win for most people.  Even bad
payment systems are better than ones that let you have everything in
your wallet stolen by a single attack.  What's with the heat-death nonsense?  Physical bearer instruments
imply stout locks and vaults and alarm systems and armed guards and
all the rest, all the way down to infrastructure like police forces
and armies (private or public) to avoid having the biggest gang end up
owning all the gold.  Electronic bearer instruments imply the same
kinds of things, and the infrastructure for that isn't in place.  It's
like telling people to store their net worth in their homes, in gold.
That can work, but you probably can't leave the cheapest lock sold at
Home Depot on your front door and stick the gold coins in the same
drawer where you used to keep your checkbook.
Why do you say internet bearer transactions are more secure?  I can
see more efficient, but why more secure?  It looks to me like both
kinds of payment system are susceptible to the same broad classes of
attacks (bank misbehavior (for a short time), someone finding a
software bug, someone breaking a crypto algorithm or protocol).  What
makes one more secure than the other?  --John Kelsey

@_date: 2005-10-29 06:20:50
@_author: John Kelsey 
@_subject: On Digital Cash-like Payment Systems 
Note that there are crypto schemes that use huge keys, and it's
possible to produce simple variants of existing schemes that use
multiple keys.  That would mean that the whole 8GB string was
necessary to do whatever crypto thing you wanted to do.  A simple
example is to redefine CBC-mode encryption as
C[i] = E_K(C[i-1] xor P[i] xor S[C[i-1] mod 2^{29}])
where S is the huge shared string, and we're using AES.  Without
access to the shared string, you could neither encrypt nor decrypt.

@_date: 2005-09-14 14:03:50
@_author: John Kelsey 
@_subject: multiple keys to 1 
The straightforward symmetric crypto approach to this (it's
not pretty or elegant, but it works) is to have each
encrypting key be shared with the owner of the recipient
key.  Thus, we might have:
Alice has key K_a
Bob has key K_b
Carol has key K_c
Dave, the recipient, knows all three keys.
Now, to encrypt a message to Dave, anyone can just do
something like
Dave can try all possible keys, or can require that the
sender prepend some kind of unique identifier, like
Alice, Encrypt(K_a,Message)
If you're looking for more elegant solutions, you end up
needing to look at public key cryptosystems as Perry pointed
out.  Or look at multicast encryption and signature schemes,
which have some neat stuff right at the boundary between
symmetric and public key crypto.

@_date: 2006-04-01 12:28:07
@_author: John Kelsey 
@_subject: [Cfrg] HMAC-MD5 
The question is, can these still be controlled when the attacker
doesn't know the internal state of the chaining variables?  If not, we
may end up with second preimage attacks (which would finish off MD5
for most hashing applications!), but still not know how to attack
HMAC.  The attack model is really different!  For what it's worth, though, I agree that we need to get rid of MD5
anywhere it's still in place, since the only thing we know about its
security is that it's a lot less than anyone expected it to be even a
year ago.  In fact, we should have started this when Dobbertin had his
free-start collision result.  If we had, we'd be able to regard the
devastating MD5 collisions we're seeing now in the same way we regard
devastating attacks on FEAL.  (If someone extends the best attack on
FEAL to 64 rounds, that will be cool, but nobody will be scrambling to
replace FEAL in their products and protocols.)
--John Kelsey, NIST

@_date: 2006-02-24 17:59:01
@_author: John Kelsey 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
I'm certain that only a small percentage of e-mail will ever be
signed, so long as the tools to do that are so hard to use, and the
value added so small.  I find it useful to use encryption all the time
on my private data, but virtually never use it for communications,
because even among cryptographers the setup hassles are too great, and
the value added too small.  What we ultimately need is encryption and
authentication that are:
a.  Automatic and transparent.
b.  Add some value or are bundled with something that does.
c.  Don't try to tie into the whole horrible set of PKI standards in
terms of uniquely identifying each human and bit in the universe, and
getting them to sign legally binding messages whose full
interpretation requires reading and understanding a 30-page CPS.  If email encryption became as transparent as SSL, most e-mail would be
encrypted.  This would still leave various phishing issues, etc., but
eavesdropping and a lot of impersonation and spam and phishing would
get much harder.  --John Kelsey

@_date: 2006-01-03 14:14:41
@_author: John Kelsey 
@_subject: [coderman@gmail.com: Re: [dave@farber.net: [IP] more on AP 
I'm coming late to this discussion, but if you're already trusting
AES256 for security, why not just exchange a single long-term AES256
key between mutually-trusting sites?  Then, you can generate today's
piece of the one-time-pad using a shared counter or a timestamp or
something.  Further, this lets you store the secret that derives your
keys inside a tamper-resistant crypto module.  --John Kelsey, NIST

@_date: 2006-01-03 15:08:12
@_author: John Kelsey 
@_subject: RNG quality verification 
It's helpful if you try to determine what you're testing here.  What
is the ideal situation which you hope to approximate by your RNG?  For
example, RSA moduli will always be odd, since they're the product of
two odd primes.  So you're going to get some statistical flaws when
you look at the low bit.  Similarly, the high few bits will be
somewhat skewed by our choices of p and q (you usually set the high
bits so that you get the desired size of modulus).  And you may be
choosing p and q according to some other criteria--I don't know what
OpenSSL does exactly.  Further, you're looking at the result of using the PRNG outputs in a
really complicated way.  There might be all kinds of nonrandom
behavior in the outputs that would not be apparent at all in the
result of generating RSA keys.  Think about what's happening--you're
generating a random starting point of 512 bits with a certain form
(high bit set), and then sieving it, and then running a primality test
for a bunch of rounds.  Then you're doing the same thing a second
time.  Then you're multiplying the numbers together. To assess a cryptographic PRNG, you need to know two things:
a.  If it had a starting point or seed which was impossible to guess,
would you be able to find any problems with its outputs?
b.  Does it get a starting point or seed which is impossible to guess?
Assessing (a) is about cryptanalysis; statsitics can help there, but
mostly, you're looking at the output from some cryptographic function
like SHA1 or AES or 3DES.  Assessing (b) is about data
analysis--you're going to look at the sources for seed material, and
try to determine what makes them ultimately unpredictable, and to
model them somehow.  You can't assess how much entropy some variable
has without some kind of probability model for it.  It's really hard
to get a satisfactory model for most of the OS/software sources,
unfortunately.  If you can't get a simple model for it, you can at
least attempt to determine some kind of bound on its entropy.  See
Peter Gutmann's wonderful paper from USENIX a few years ago on his
Cryptlib's PRNG for some flavor of what this kind of analysis looks
like.  The critical thing to understand here is where your statistical tools
are and are not useful.  There's no harm in running some big
statistical tests on the outputs from a cryptographic mechanism, and
you may in principle even find something this way, but you probably
won't.  To the extent that you have a probability model for the source
of seed material or entropy you're using, you can use statistical
tests to check the plausibility of your model.  (That is, if your
model says that successive samples of some variable are independent,
it's really a good idea to run some statistical tests to check
that--Chi Square tests of independence, autocorrelation, whatever
makes sense with the kind of data you have.)  Well, the analysis of your entropy source is ultimately a data
analysis problem.  You go back and forth between your best
understanding of the underlying process that generates entropy, your
data, and what kind of models you can work with, refining your
understanding of your entropy source until your money or time run
out.  If some attacker has a much better model than yours, he may be
able to predict your seeds and thus break your PRNG.  (This is a good
reason to seed the PRNG with more estimated entropy than the minimum
you can get away with--there are other reasons to do this, too.)  Suppose I'm using AES128 with a key of 0 in counter mode, starting
with a counter of 0.  This will pass all the statistical tests you run
against it.  But with a single 128-bit output, I am nearly certain
of identifying this source.     If you don't know the key I'm using for AES, then distinguishing
counter mode outputs is as hard as breaking AES, for some very
precisely defined meaning of the work "breaking."  But it has no more
than 128 bits of entropy, because if you ever guessed the right 128
bit AES key, you could predict all future outputs from my PRNG
forever.  That means it would be silly to use, say, AES128 to generate
256-bit AES keys, since the attacker would only need to guess the
128-bit AES key to learn all the 256-bit AES keys you were generating.
For what it's worth, we're working on a standard for cryptographic
random number generation in X9.  There's a draft document (SP800-90)
up on the NIST website
discussing some hopefully good crypto PRNGs, and some guidelines for
their use.  This doesn't discuss much about analyzing entropy sources
(we're still hashing that out).  If you want to understand the
security of crypto PRNG algorithms, you can look at some papers I
did on this with Bruce Schneier, Niels Ferguson, David Wagner, and
Chris Hall:
 Also, Peter's PRNG paper is at his website:
And Lisa Yin did a paper with Desai and Hevia for Eurocrypt 2002
trying to do reduction proofs for various PRNGs--basically showing
that if certain properties of the hash function or block cipher used
hold, then the PRNG is secure.  I don't know if there's an online
version available.  If you want to understand how to do the data analysis for a hardware
entropy source, I recommend looking at the analysis of the Intel and
VIA C3 hardware RNGs, both on Cryptography Research's site.
The big thing to understand here is how much nicer life is when you
design the source of entropy from the beginning to follow some kind of
reasonable probability model, rather than looking for some way to
adapt a mathematically useful model to some complicated process like
OS loading or something. There's a reasonably nice suite of statistical tests available from
NIST, though I've heard some complaints about the portability of the
code.  More broadly, there are a gazillion statistical packages out
there.  But if you don't understand your model, you'll just shoot
yourself in the foot by throwing sophisticated statistical tools at
the problem.  --John Kelsey, NIST

@_date: 2006-01-27 09:27:20
@_author: John Kelsey 
@_subject: thoughts on one time pads 
I think that's because you missed the point.  You're confusing manual
key distribution (which makes sense in some cases, but is unworkable
in others) with using a one-time pad (a specific method of encrypting
information that uses up key material very fast but has a security
proof).  Manual key distribution means that I carry the key material to you by
hand.  This can be on a DVD or CD or tape or USB drive, or for that
matter on a piece of paper or punched card or cryptographic token.  A one-time pad means that I take my key material, which must be
perfectly random for the proof to work, and XOR it with plaintext to
get ciphertext.  That can't possibly be cryptanalyzed, because there's
no information about the plaintext in the ciphertext, so long as the
key is unknown and random.  (Any plaintext could lead to any
ciphertext with equal probability.)   You're talking about manual key distribution here.  This works the
same for both OTPs and conventional encryption.  The difference is
that managing the keys in a secure way is *much* easier when you're
doing conventional encryption.  The only advantage using a one-time
pad gives here is that you don't have to worry about cryptanalysis.
And one-time pad encryption can't be used with anything but manual key
distribution, or other methods that are at least as awkward (like
quantum key distribution).  You can't hand me a business card with
your PGP fingerprint on it and establish secure communications with me
using a one-time pad, but you can using PGP and conventional crypto.  But then you're not using an OTP anymore.  And there's no need for a
station wagon full of DVDs, you can use a piece of paper with a
32-digit hex string on it to exchange the AES key, ugly though that
is to type in.  In fact, there are some procedures people have worked
out to do this.  But it doesn't scale well.  Yep.  You've got to store the key material safely in transit and at
the endpoints either way, though, and that's much easier for 256 bit
AES keys (which can be put inside an off-the-shelf tamper-resistant
token), and easier still for hashes of public keys (which only have to
arrive unchanged--it doesn't matter if the bad guys learn the
hashes).  Not to put too fine a point on it, it's because he's right and you're
wrong.  You have to worry about securing the key material from cradle to
grave, and operationally makign sure you use the right key material
with the right person and never reuse it.  OTPs are terribly sensitive
to the randomness of your key material (if you screw up and use bits
biased 60/40 for your 128-bit AES key, you'll drop to 90+ bits of
security; if you do that for your OTP, you'll reveal quite a bit of
the normal ASCII text you send, because there's not very much entropy
per bit.).  The issue of preventing reuse is enormous.  If you dig around, Rivest wrote a paper at Crypto 82 or 83 about
randomizing encryption which is still pretty nice.  To a first
approximation, superencryption (encrypting the OTP-encrypted text with
AES-CBC or some such thing) is always okay.
There are provably secure authentication schemes that use much less
key material per message.  Google for universal hashing and IBC Hash,
and for provably secure authentication schemes.  I seem to recall that
Stinson has a really nice survey of this either webbed or in his
book.  (Anyone else remember?)  Yep, you can flip bits, so long as you know the CRC polynomial.  If
the CRC polynomial is randomly chosen for each message, and used
correctly, you get one of those provably-secure authentication
schemes.  Read The Friendly Literature.
--John Kelsey

@_date: 2006-07-13 10:57:20
@_author: John Kelsey 
@_subject: Interesting bit of a quote 
It's interesting to me that this same kind of issue comes up in voting
security, where computerized counting of hand-marked paper ballots (or
punched cards) has been and is being replaced with much more
user-friendly DREs, where paper poll books are being replaced with
electronic ones, etc.  It's easy to have all your procedures built
around the idea that records X and Y come from independent sources,
and then have technology undermine that assumption.  The obvious
example of this is rules for recounts and paper record retention which
are applied to DREs; the procedures make lots of sense for paper
ballots, but no sense at all for DREs.  I wonder how many other areas
of computer and more general security have this same kind of issue.   --John Kelsey, NIST

@_date: 2006-07-15 03:47:25
@_author: John Kelsey 
@_subject: Interesting bit of a quote 
I think this is going to resolve to chain-of-custody rules of some
kind.  One problem is that so long as the company making the records
is storing them onsite, it's hard for an outside auditor to be sure
they aren't being tampered with.  (Can the CEO really not work out a
way to get one of his guys access to the tape storage vault?) You could do the whole digital timestamping thing here.  You could
also just submit hashes of this week's backup tape to your auditor and
the SEC or something.  Another solution is to use cryptographic audit logs.  Bruce Schneier
and I did some work on this several years ago, using a MAC to
authenticate the current record as it's written, and a one-way
function to derive the next key.  (This idea was apparently developed
by at least two other people independently.)  Jason Holt has extended
this idea to use digital signatures, which makes them far more
practical.  One caveat is that cryptographic audit logs only work if
the logging machine is honest when the logs are written.

@_date: 2006-06-23 11:02:06
@_author: John Kelsey 
@_subject: A weird macro virus story 
Some of my co-workers here at NIST got an email macro virus which
appeared to be targeted to cryptographers.  It appeared to be
addressed to Moti Yung, and come from Lawrie Brown and Henri Gilbert
(though that name was misspelled, maybe a transcription error from an
alternate character set).  Did any of you notice something like this?
The email appeared to be addressed to several submission addresses for
various crypto conferences.  Wow, we've really made it as an influential group of people.  We're
important enough to be *targets* now....
--John

@_date: 2006-03-21 14:22:49
@_author: John Kelsey 
@_subject: pipad, was Re: bounded storage model - why is R organized as 
When you build this scheme, you have to compare it to all other ways
of generating random-looking keystream for a stream cipher.  That
means comparing it with generators which are guaranteed to be as hard
to predict output bits for as a large integer is hard to factor, for
example.  Beyond the coolness factor of pi, it's hard to work out what
additional guarantee we're getting from using it here.  I don't know what the performance of the algorithm for generating the
next n bits of pi looks like, but I'm guessing that I can do a fair
number of AES/Serpent/Twofish/MARS/RC6/Blowfish/CAST/3DES/IDEA/RC5
calculations for the same cost.  And we know how to build a stream
cipher out of all those ciphers (running in OFB or counter mode) which
is guaranteed to be as strong as the strongest of them.  It's all about tradeoffs between performance, security, and what
strong statements you can make about your security when you're done.
In some applications, I am willing to give up a lot of performance for
a solid proof of security; in others, I am willing to give up any hope
of a proof of security to get really good performance.    --John Kelsey

@_date: 2006-03-23 11:05:45
@_author: John Kelsey 
@_subject: Entropy Definition (was Re: passphrases with more than 160 bits 
As an aside, this whole discussion is confused by the fact that there
are a bunch of different domains in which entropy is defined.  The
algorithmic information theory sense of entropy (how long is the
shortest program that produces this sequence?) is miles away from the
information theory sense of entropy, and even that has a bunch of
different flavors.  For the information theory meanings of entropy, what you're really
talking about is a property of a probability distribution.  You can do
that in terms of Shannon entropy if you want to know about bounds on
average bandwidth requirements for transmitting symbols from that
distribution.  You can look at guessing entropy if you want to know
the -lg(expected work to guess the next symbol).  You can look at
min-entropy if you want a hard bound on how many symbols you need to
sample to derive a 128-bit key that won't ever expose you to
weaknesses based on how you selected it.  And so on.  Shannon entropy is the one most people know, but it's all wrong for
deciding how many samples you need to derive a key.  The kind of
classic illustration of this is the probability distirbution:
0 occurs with probability 1/2
each other number from 1 to 2^{160}+1 happens with probability
The Shannon entropy on this distribution is 81.5 bits.  But if you
tried to sample it once to generate an 80-bit Skipjack key, half the
time, I'd guess your key on my first try.  Right.  If the probabilities are independent, you can add the
entropies.  That's why they're defined in log terms.

@_date: 2006-03-23 17:53:52
@_author: John Kelsey 
@_subject: Entropy Definition (was Re: passphrases with more than 160 bits 
No, this isn't right for the algorithmic information theory meaning at
all.  For that measure, we can intelligently discuss the entropy of a
specific random string, without reference to a probability model.
Indeed, what's the probability distribution of the sequence of bits
defined by Chaitin's Omega?  You can certainly complain that they should have used a different term
than entropy here, but you're not going to get these to be the same.  Well, you need to decide which of the probability distribution kinds
of entropy measures to use, and that differs in different
applications.  If you use min-entropy or guessing entropy to estimate
the limits on how well some sequence of symbols will compress, you'll
get a pretty lousy answer.  The same goes for using Shannon entropy to
determine whether you have collected enough entropy in your pool to
generate a 128-bit AES key.  Hmmm.  I've seen plenty of people get this wrong.  If you use Shannon
entropy as your measure, and then say "when I have collected 128 bits
of Shannon entropy in my pool, I can generate a 128-bit AES key," you
will generate keys that aren't as secure as you think they are.  Now,
most TRNGs seem to evade this and many other problems by designing the
hardware to give a relatively simple probability model.  If your
probability model is close to uniform, then all these probability
distribution based entropy measurements converge (with a constant
difference).   In the case above, we had better specify N.  If you sample it 16
times, then you have a 2^{-16} chance of still being in a weak state.
Once you get enough samples that the probability of being in the
pathological worst case is negligible (whatever that means in your
application), then you can start generating output bits.  That's
probably somewhere between N=32 and N=64.

@_date: 2006-03-23 18:09:34
@_author: John Kelsey 
@_subject: Linux RNG paper 
Starting clarification:  Min-entropy of a probability distribution is -lg ( P[max] ), minus the base-two log of the maximum probability.  The nice thing about min-entropy in the PRNG world is that it leads to
a really clean relationship between how many bits of entropy we need
to seed the PRNG, and how many bits of security (in terms of
resistance to brute force guessing attack) we can get.
Suppose I have a string S with 128 bits of min-entropy.  That means
that the highest probablity guess of S has probability 2^{-128}.  I
somehow hash S to derive a 128-bit key.  The question to ask is, could
you guess S more cheaply than you guess the key?  When the min-entropy
is 128, it can't be any cheaper to guess S than to guess the key.
That's true whether we're making one guess, two guesses, ten guesses,
or 2^{127} guesses.   To see why this is so, consider the best case for an attacker: S is a
128-bit uniform random string.  Now, all possible values have the same
probability, and guessing S is exactly the same problem as guessing
the key.  (I'm ignoring any bad things that happen when we hash down
to a key, but those can be important to think about in a different
context.)  Now, why is this the best case for an attacker?  Because it gives the
highest probability of guessing right on the nth guess.  If the
min-entropy is 128, then the highest probability symbol must have
prob. 2^{-128}.  If the next highest probability symbol has lower than
2^{-128} probability, then his second guess has lower probability.
And then the next highest probability symbol is constrained in the
same way.   This makes it really easy, once you're working in min-entropy terms,
to answer questions like "do I have enough entropy in this string to
initialize a PRNG based on running AES-128 in counter mode?"    --John Kelsey, NIST

@_date: 2006-03-25 12:08:23
@_author: John Kelsey 
@_subject: Entropy Definition (was Re: passphrases with more than 160 bits 
There are a bunch of different entropy measures.  Shannon entropy is
the right one for compression ratios and channel capacity, but not for
difficulty of guessing the string.
The best discussion I've seen of this is in a couple papers by John
Pliam, about something he calls the work function, W(n).  This is just
the probability of having guessed some secret value after n operations
(typically n guesses). You can generalize this to the number of
operations you have to carry out to have a given probability of
violating any security property (repeating a nonce, getting a block
collision in a block cipher chaining mode, etc).  It's a very general
way of thinking about limiting parameters of crypto algorithms.
You're basically heading toward this idea in what you said above.
Let's think of this for an ideal case: a 128-bit key.  When you have
done 2^k guesses of the key, you have a 2^{n-k} chance of having
guessed the key correctly.  So if you graphed the work vs probability
of success on a log/log chart, you'd get a straight line for the ideal
case.  W(n) = 2^{-128} n, as you said above.  Now, consider the case where you are generating a 128-bit key from a
bunch of sampled events on your computer that have been concatenated
together in a string S.  Imagine making a list of all the possible
values of that string, and sorting them in descending order of
probability.  You now have the best possible sequence of guesses.
W(n) is the sum of the first n probabilities.  If W(n) > 2^{-128} n for any n, then the attacker has some point where
it is to his advantage to guess S instead of guessing your key.
So, this is where min-entropy comes in.  Min-entropy is just
-lg(P[max]), the base 2 log of the maximum probability in the
distribution.  You can convince yourself than if the min-entropy of S
is at least 128, then it's never easier to guess S than to guess K.
This is because each possible input string must have probability lower
than 2^{-128}, so the sum of the first n, W(n) < n 2^{-128}.  This doesn't solve the much harder engineering/data analysis problem
of getting some reasonable approximation for the probabilities of S,
unfortunately.  The easy way to solve that is to design a hardware RNG
in such a way that you pretty-much know the probabilty model to use
and can work out how to sample it to get a good estimate of the
probabilities.  However, it is kind of nice to recognize that you only
have to estimate the largest probability to compute the min-entropy.
(It ought to be the easiest probability to measure and bound!)  --John Kelsey, NIST

@_date: 2006-03-25 12:57:33
@_author: John Kelsey 
@_subject: Entropy Definition (was Re: passphrases with more than 160 bits 
The example is a contrived, pathological case.  And there are a bunch
of easy solutions once you know this is the distribution.  The point
is to demonstrate that Shannon entropy gives you the wrong answer when
you try to use it here.
Now, you might ask if this is a problem in a real-world setting.  So
let's imagine a very simple distribution--sequences of flips of a
biased coin.  There are nice ways to remove the bias, but let's
imagine we're not doing that--instead, we're going to take a sequence
of coin flips, turn it into a 0/1 sequence, and then hash it down to
get a key.  Suppose the coin is biased so that heads comes up 0.9 of the time, and
that we generate 16-bit sequences at a time.  The Shannon entropy of a
16-bit sequence is about 7.5, but the most likely symbol (all heads)
comes up about 18% of the time.  So if we tried to generate a 7-bit
"key", we'd lose on the attacker's first guess 18% of the time. So, let's go further with this.  We want to generate a DES key, with
56 bits of entropy.  A 64-bit sequence produced with this biased coin
has Shannon entropy of about 60 bits, but an attacker has about a
1/1000 chance of guessing the DES key we derive from it on his first
try, which is unacceptable for just about any crypto application.
(The min-entropy is about ten bits.)
So yes, I used a contrived example to demonstrate the problem, but no,
the problem isn't just an ivory-tower concern.
The intuition here is that Shannon entropy is concerned with
communications channels, where we assume we have to send every
symbol.  So when you have lots of low-probability symbols, and one of
those low-probability symbols is chosen 999 times out of a 1000, the
amount of bandwidth you need to transmit those symbols easily becomes
dominated by them.  Most of the time, you're sending one of a large
set of symbols, and they all have to be distinguished from one
another.  The situation for the attacker is a lot more like having a channel
where it's acceptable to only send a small fraction of those symbols,
and just drop the rest.  That is, it's okay for the attacker's model
to just ignore the huge set of low-probability symbols that occur
999/1000 of the time, and instead just transmit the highest
probability symbol 1/1000 of the time.  Instead of transmitting it,
he's just guessing it.  When he gets it right, he learns your DES
key.  Yes!  The "independent" part is much harder to deal with than the
per-symbol distribution, in many real-world applications.  The worst
of these are operating system events (used in Windows and various
Unixes to get random bits).  Peter Gutmann did some really nice work
on this on a bunch of operating systems in a Usenix paper, and he
updated that and has a version of it on his website.  If you're doing
OS-event sampling to generate random bits, you really ought to look at
his stuff.  (But if you can get some hardware support, either directly
or via sampling the microphone like the Turbid design does, you're
probably on much firmer ground.)  --John Kelsey, NIST
