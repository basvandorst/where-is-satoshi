
@_date: 2009-08-10 11:20:05
@_author: Jason Resch 
@_subject: [tahoe-dev] cleversafe says: 3 Reasons Why Encryption isOverrated 
With a secret sharing scheme such as Shamir's you have information theoretic security.  With the All-or-Nothing Transform and dispersal the distinction is there is only computational security.  The practical difference is that though 2^-256 is very close to 0, it is not 0, so the possibility remains that with sufficient computational power useful data could be obtained with less than a threshold number of slices.  The difficulty of this is as hard as breaking the symmetric cipher used in the transformation.
Is there any data storage system which does not require some protection against attackers, resiliency to media failure, and trusted administrators?  Even in a systems where one encrypts the data and focuses all energy on keeping the key safe, the encrypted copies must still be protected for availability and reliability reasons.
The security provided by this approach is only the icing on the cake to the other benefits of dispersal.  Dispersal provides extremely high fault tolerance and reliability without the large storage requirements of making copies.  See this paper "Erasure Coding vs. Replication: A Quantitative Comparison" by the creators of OceanStore for a primer on some of the advantages: If a particular "vault" (Our term for a logical grouping of data on which access controls may be applied) had data stored on on a threshold number of compromised drives, then data in that vault would be considered compromised.  Our systems tracks which vaults have data on which machines through a global set of configuration information we call the Registry.
When drives or machines are known to be lost or compromised one may perform a read and overwrite of the peer-slices.  This makes obsolete any slices attackers may have accumulated up until that point.  This is due to the fact that the AONT is a random transformation, and newly generated slices cannot be used with old ones to re-create data.  Therefore this protocol protects against slow accumulation of a threshold number of slices over time.
Please let me know if you have any additional questions regarding our technology.
Best Regards,
Jason Resch

@_date: 2009-08-10 12:14:00
@_author: Jason Resch 
@_subject: cleversafe says: 3 Reasons Why Encryption is Overrated 
I would define that combination as a threshold secret sharing scheme.  Noting of course what you said below in that it is a computationally-secure as opposed to Shamir's information theoretically secure scheme.
Recalling what the original poster said:
"Surely this is fundamental to threshold secret sharing - until you reach the threshold, you have not reduced the cost of an attack?"
Cleversafe's method does have this property, the difficulty in breaking the random transformation key does not decrease with the number of slices an attacker gets.  Though the difficulty is not infinite, (as is the case with an information theoretically secure scheme) it does remain fixed until a threshold is reached.

@_date: 2009-08-10 12:56:54
@_author: Jason Resch 
@_subject: [tahoe-dev] cleversafe says: 3 Reasons Why Encryption isOverrated 
You have stated how Cleversafe manages the key but not provided any details regarding how Tahoe-LAFS manages the decryption key?  In your documentation it was stated that many of your users choose to store the capability (containing the key) for their root file on your data storage servers.  I would think that this results in less security than Cleversafe's approach because our servers enforce authentication and access controls.
I agree.  I should also note that the use of AES-256 or any cipher is a configuration parameter for our generalized transformation algorithm, which also can support stream ciphers.
Symmetric ciphers frequently break in small pieces at a time, reducing the number of bits of protection below what would be expected for the given key length.  If an asymmetric algorithm were to break (due to finding solutions to factoring or discrete logarithms) those algorithms would fail utterly, no length of a key could be considered secure.  This of course has not happened yet, but it remains a possibility unless it is someday proven that there is no efficient solution.  Even if math does not provide a path to breaking asymmetric ciphers, physics does by way of quantum computing.
Hundreds of symmetric ciphers have been devised and as weaknesses are found in currently used symmetric ciphers it is easy to migrate to other well-vetted algorithms.  Asymmetric ciphers are in short supply, and depend on discover trap door functions in math, so a break in them would offer fewer exit strategies.
We evaluated that approach shorlty before discovering the AONT and I believe it has similar security properties.  The AONT was much simpler to integrate, however, as it is a single pre-processing step before the IDA.
It is fundamentally safer in that even if the transformation key were brute forced, the attacker only gains data from the slice, which in general will have 1/threshold the data.  Also, it avoids the threats to asymmetric cryptography that I mentioned above.
Many modern cryptosystems use 1024-bit RSA keys, (my bank's website for example).  The security of a 1024-bit RSA key is about equivalent to an 80-bit symmetric key.  Assuming a conservative estimate for Moore's law of doubling processing power every 2 years, a bot-net owner with 1,000,000 computers could reach that point in 28 years.  Existing trends show doubling ever 18 months, which reduces this time to about 20 years.
I think your points were clearly written and professional.
Best Regards,
Jason Resch

@_date: 2009-08-10 16:40:30
@_author: Jason Resch 
@_subject: FW: cleversafe says: 3 Reasons Why Encryption is Overrated 
I would define that combination as a threshold secret sharing scheme.  Noting of course what you said below in that it is a computationally-secure as opposed to Shamir's information theoretically secure scheme.
Recalling what the original poster said:
"Surely this is fundamental to threshold secret sharing - until you reach the threshold, you have not reduced the cost of an attack?"
Cleversafe's method does have this property, the difficulty in breaking the random transformation key does not decrease with the number of slices an attacker gets.  Though the difficulty is not infinite, (as is the case with an information theoretically secure scheme) it does remain fixed until a threshold is reached.

@_date: 2009-08-12 18:03:40
@_author: Jason Resch 
@_subject: strong claims about encryption safety Re: [tahoe-dev] cleversafe says: 3 Reasons Why Encryption isOverrated 
You failed to quote the other reason I offered:
It is not dependent on asymmetric cryptography, which depends on:
1. No one ever figuring out a fast way to factor primes, an area in which there has been substantial progress.
2. No one ever building a quantum computer with more than twice as many qubits as your key length.
In other posts on the subject I have offered even more reasons, including:
1. It is not dependent on Password Based Encryption, which struggles to walk the tightrope of reliability vs. confidentiality, use a long password and you are likely to forget it, use a short one and it is easy to brute force.  Write down a long password and you've made it easier for someone to find.
2. In our system there are no master keys.  Therefore a compromise of the client computer which reads data only leads to loss of confidentiality for the data that was read during the compromise, not the loss of confidentiality for all data encrypted by a master key as is the case with most systems.
3. It offers an elegant solution for reliably and confidentially storing keys.  Making copies of keys is a trade-off between confidentiality and reliability, secret sharing schemes, such as this can achieve both simultaneously.
See above, this is just one and perhaps the most trivial and "meaningless" of the security advantages.  The biggest advantage in my mind is the way it addresses the problem of key management, which in my opinion is the elephant in the room for most cryptosystems.  I look forward to your response on this subject, as practically speaking it is the most relevant issue for such systems, not which ciphers or key lengths are used.
Assuming the hash function is a random oracle, the way the key is masked is equivalent to One-Time-Pad encryption.  There would need to be an extremely serious flaw in the hash function (such as having output bits highly skewed towards 1 or 0, or for earlier input to have very little impact on the hash result) for it to compromise the security of the masked key.  Recall that the hash is calculated over random-seeming encrypted data, so even if the input was highly or specially formed to cause trouble for a hash function, the fact that it is encrypted before hashed eliminates this type of chosen-plaintext attack.
You can point out cryptographic primitives used in AONT and say if they aren't secure then your system isn't secure, but one could do the same for any system.  When designing systems, one should work with the assumption that the hash algorithms and ciphers do what they are meant to, but always plan for forward support of new algorithms if/when a critical flaw is discovered.  We have done this, implementing support for different ciphers, key lengths and hash functions, and I believe this is about the best anyone can do when designing a system.
I agree that statement you quoted is misleading.  It should have come with an asterisk that to be protected against advances in processing power, one must use a random transformation key long enough that it cannot be cracked by any foreseeable technology within the life of the universe.  Whether or not a 256-bit key meets this requirement is up to debate, but the AONT+Dispersal protocol is not limited to any particular key length.  Note that this contrasts with systems dependent on asymmetric algorithms, for which there is a foreseeable technology that could break keys of any length.
AONT+Dispersal is a secret sharing scheme, just not information theoretic like the Shamir Scheme.  Not all secret sharing schemes have the property of being information theoretically secure.  We abandon it seeing more value in the storage and I/O efficiency than the practical benefits of information theoretic security vs. the difficulty of breaking a 256-bit key.  It is the same reason hardly anyone uses OTP encryption, that level of security is too costly for its benefits.
How is that misleading or unjustified?  One could use a massive key so long that it couldn't possibly be cracked.  For example a 448-bit Blowfish key.  One quickly reaches a point where the cost/difficulty of stealing a threshold number of storage nodes will be << the cost/difficulty of cracking the transformation key, and at that point, what does it matter if it is information theoretic or just 2^448 in complexity?
Thank you.
I too would prefer to avoid discussions on topics of law.  As far as I know, however, the bill that would have eliminated the encryption safe harbor has not been passed and most likely was abandoned after being moved around a few committees.  Nevertheless, on the topic of disclosure itself, dispersal offers some strong protection against accidental and ill-intentioned exposures.
Interesting reference, I will check into it.
I too believe it to be the most important issue, as the confidentiality, reliability, and availability of an encrypted storage system is necessarily bounded by the confidentiality, reliability and availability of the key storage system.  I eagerly await what you have to say on this topic.

@_date: 2014-10-02 17:53:36
@_author: Jason Resch 
@_subject: [Cryptography] Creating a Parallelizeable Cryptographic Hash 
Assuming there was a secure cryptographic function H() with an output of L bits, what attacks or weaknesses would exist in a protocol that did the following:
Digest = H(B_0 || C_0) ^ H(B_1 || C_1) ^ H(B_2 || C_2) ^ ... ^ H(B_N || C_N) ^ H(N)
Where B_0 through B_N are the blocks (of size L) constituting the message and C_0 through C_N are L-bit counters.
One problem seems to be that if any collision can be found for a given H(X || C_i) and H(Y || C_i), it leads to an essentially infinite number of collisions (any message that contains X as a block can have that block replaced with Y), but what other vulnerabilities does this construction have that would make it unsuitable as a general purpose cryptographic hash function?
Thanks for your expertise.

@_date: 2014-10-03 12:18:12
@_author: Jason Resch 
@_subject: [Cryptography] Creating a Parallelizeable Cryptographic Hash 
But here you are not using counter values in the digest calculation. Is there a way to determine any kind of homomorphism when no collisions can be found in H()?
Interesting, thanks for pointing this out. If I interpret the improvement of the GBA correctly, does that mean the time complexity to find a collision is N^(L/2) / L vs. N^(L/2)?

@_date: 2014-10-03 12:23:16
@_author: Jason Resch 
@_subject: [Cryptography] Creating a Parallelizeable Cryptographic Hash 
David, Sandy,
Thanks for these resources on tree hashing.
I was considering a case where a small changes between very large messages, M and M' could be computed efficiently to produce an updated hash value. I am correct that tree hashing doesn't support this without using a lot of extra memory to store the intermediate hash values?

@_date: 2014-10-03 13:16:43
@_author: Jason Resch 
@_subject: [Cryptography] Creating a Parallelizeable Cryptographic Hash 
Okay I see how that works now. That is an interesting property, but can it be used to undermine the security of any typical applications of hash

@_date: 2014-10-03 17:30:33
@_author: Jason Resch 
@_subject: [Cryptography] Creating a Parallelizeable Cryptographic Hash 
Very clever. I see now that this is clearly vulnerable to a length extension attack.  However, it isn't clear to me why throwing the final result through H() as a final post-processing step wouldn't serve to address it.
