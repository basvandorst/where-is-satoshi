
@_date: 2013-10-08 19:23:33
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Elliptic curve question 
The very general answer: If it's not a big problem, it's always better
to separate encryption and signing keys - because you never know if
there are yet unknown interactions if you use the same key material in
different use cases.
You can even say this more general: It's always better to use one key
for one usage case. It doesn't hurt and it may prevent security issues.

@_date: 2013-10-30 19:02:42
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Standard exponents in RSA 
NIST SP 800-56B says so:
(or to be precise, it says minimum size 65537 - so most people seem to
choose the minimum, which is also fast in computation)
There have been some attacks in the past that only work with very small
exponents (like 3 or 4). An example is the Bleichenbacher attack on RSA
signatures, it only works with e=3, see here:
65537 seems a reasonable choice, because it allows still fast
computation. See Wikipedia:
"due to its low Hamming weight (number of 1 bits) can be computed
extremely quickly on binary computers, which often support shift and
increment instructions"

@_date: 2013-09-09 23:14:31
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] What TLS ciphersuites are still OK? 
I don't really see from the document why the authors discourage
ECDHE-suites and AES-256. Both should be okay and we end up with four
Also, DHE should only be considered secure with a large enough modulus
(>=2048 bit). Apache hard-fixes this to 1024 bit and it's not
configurable. So there even can be made an argument that ECDHE is more
secure - it doesn't have a widely deployed webserver using it in an
insecure way.

@_date: 2013-09-30 16:14:06
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] TLS2 
On Mon, 30 Sep 2013 11:47:37 +0200
It's called SNI and it is widely deployed. All browsers and all
relevant web servers support it.
However, it has one drawback: It doesn't work with SSLv3, which means
it breaks every time browsers do a fallback on SSLv3. And they do quite
often, because they retry SSLv3 connects if TLS connections fail. Which
is also a security problem and allows downgrade attacks, but mainly it
means with weak internet connections you often get downgraded

@_date: 2014-06-15 23:15:55
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Dual EC backdoor was patented by Certicom? 
This is all certainly quite interesting, but it is hardly new. This
fact has been known since Dec 2013:
Or am I missing something here? From what I can see, the project
bullrun webpage just lists already known facts (however I think it was
originally Tanja Lange who made this issue public, so credit goes to
the right people - it's just not new).

@_date: 2014-06-16 12:19:08
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Dual EC backdoor was patented by Certicom? 
Okay, thanks for clarification, then I was obviously wrong. I just had
a quick lock on the webpage and didn't read it in detail, I thought
that it's just a writeup of already known facts.

@_date: 2014-03-05 19:50:36
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Silly Diffie-Hellman question using XOR 
Your protocol breaks already here. Attacker knows A2, B2 and A3.
So he can calculate A3 ^ B2 ^ A2. And get's A1. Wow!
Same here. B3 ^ B2 ^ A2 gives attacker B1.
Honestly, if you didn't see this, you shouldn't even dare to invent any
crypto yourself.
And rule of thumb: If you make up your own algorithm, it's broken.
Exceptions only if you are super-intelligent and have studied number
theory for years.

@_date: 2014-03-08 23:40:22
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] RC4 again (actual security, 
Simple question: Why do you want RC4?
Beside all the details the situation is:
a) it may be possible to implement RC4 in a way that avoids all the
known attacks, BUT:
b) lots of cryptographers think RC4 is crap and attacks have a real
potential to get better. Rumors about realtime RC4 attacks are there.
c) we have alternatives.
From what I hear a lot of people have a very high opinion on salsa20 or
its successor chacha20. I have done some tests with openssh recently
which supports now both rc4 and chacha20. I am not sure if this is a
fair test as I don't know implementation details (maybe one is more
optimized than the other), but chacha20 is faster. Yes, it requires a
few more lines of code, but not that much.
So: Why? I mean even if you can use RC4 in a way navigating around all
the known issues. You may do it wrong. You may learn tomorrow that
there's been a new attack on RC4. There already may be an attack you
don't know about.
Why not stay on the safe side and use a stream cipher everybody out
there thinks provides very high security?

@_date: 2014-03-09 12:33:44
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] RC4 again (actual security, 
On Sun, 9 Mar 2014 07:20:26 -0400
Okay, my wording was a bit - let's say informal.
So let me rephrase my reasoning:
* Matthew Green thinks salsa20 is the way to go [1]. chacha20 is the
  successor of salsa20 with very few changes.
* Adam Langley tries to improve SSL and thinks chacha20 is the way to
  go [2]
* Dan Bernstein wrote chacha20 and was author of the latest rc4 attack
  - he probably also thinks his own invention is the way to go.
* OpenSSH authors think chacha20 is the way to go and added it to
  openssh 6.5.
* There's an ongoing debate in the TLS WG about chacha20. There are
  heated discussions about implementation details but I haven't yet
  read that anyone objects the idea in general to have chacha20.
I dare to say: I am not qualified to judge if a stream cipher is any
good. So the best thing I can do is look out there what people who I
know they know a lot about crypto say. There are a number of people who
think chacha20 is good. There is no famous cryptographer I'm aware of
that thinks it's really bad.
Totally agreed. "new shiny" is not a reasonable category. But salsa20
has been out there for over 10 years. chacha20 is only a small variant
and it basically was improved based on the (few, highly theoretical)
attacks on salsa20.
I'm totally with "stand by the good proven old stuff if reasoanble".
I'd prefer RSA with long keys and PSS over any elliptic curve cipher
(even if it's done by DJB).
[2]

@_date: 2014-03-09 22:54:21
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] RC4 again (actual security, 
On Sun, 9 Mar 2014 17:11:49 -0400
We were discussing rc4 vs. chacha20, not AES vs. chacha20. Or in other
words: I think chacha20 is the stream cipher of choice these days.
Block ciphers are a different question (and we have plenty of them that
I'd feel comfortable with).
If people use AES I think that's pretty fine. No argument with that.

@_date: 2014-03-16 19:51:11
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Client certificates as a defense against MITM 
Yes. We had this technology for ages and nobody is using it.
The problem is: Users don't use a single browser. And transferring
certs from one browser to another is hard in a user-friendly and secure
way. Just think of internet cafes or people using random foreign
computers to log into their webmail. (that's a security nightmare on
its own without any relation to tls, but that's the way it is)

@_date: 2014-03-16 20:08:44
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
There tends to be a consensus that ECDSA is NOT the way to go with
elliptic curves due to its property that it fails with bad random
ed25519 has some fans and is the thing openssh recently adopted. GnuPG
also is working on it.
There are plenty of choices that are probably reasonably secure and
different from AES:
* The AES competitors. People tend to think that Serpent was the most
  secure choice and Twofish was the best compromise between security
  and speed and many consider it as the "real winner".
* Use SHA-3-winner keccak for encryption. The keccak authors have
  defined an authenticated encryption mode for keccak [1].
* estream-competition [2] was a crypto algo competition for stream
  ciphers. Salsa20[2] and its successor chacha20 seem to have quite
  some fans (openssh adopted it, tls will probably do soon).
* There are plenty of old and still good algos like blowfish or
  camellia.
Amongst them, chacha20 is probably the most likely choice. It's a
stream cipher, AES is a block cipher, so there are situations when you
can't switch them, but basically in many areas you can use both block
or stream ciphers.
I really don't see that we don't have alternatives for symmetric
encryption algos. I'm much more worried that we have very little
alternatives once the public key algos break, because they are all very
similar (and we pretty much already know that it will happen if quantum
computers become usable).
[1] [2]

@_date: 2014-03-30 23:51:21
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve numbers 
I doubt that this is the best idea, as they are certainly not
pseudo random. They have a pretty defined structure. It is
probably unlikely but not entirely impossibe that the
gemoetric properties of Pi somehow turn into an attack surface.
My idea would be: If a normal block or stream cipher behaves like a
good cipher, it's output should behave like a random number generator.
So why not do something like: Use the most simple key you can think of
(which is 0) and encrypt the most simple thing you can think of (which
is - probably a line of zeros) with a believed-to-be-secure encryption
like chacha20+poly1394 or aes-cbc or aes-gcm? (if algo needs an IV it
should obviously also not be an arbitrary number but something simple -
like zero).

@_date: 2014-05-07 23:47:08
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] cryptography Digest, Vol 13, Issue 6 
It is a good topic I think. At the moment it's kind of corner science
with very few people caring, but I'm almost certain it is a topic that
will gain much more attention in the upcoming years.
Starting points: There's an irregular conference on post quantum
Also, there's a basic introduction by DJB linked there:
The bottom line is: If a quantum computer would appear soon, we're in
trouble, because basically we have nothing usable today. There are a
couple of algorithms that are believed to be quantumcomputer-safe, but
they mostly have two problems:
a) often impractical to use due to very large keys
b) not that much research done to investigate their security compared
to well-studied algs like RSA or ECC-based cryptosystems
The only thing that comes near a usable algorithm is ntru.
Unfortunately it's patented and therefore can't be used widely.
What you should be aware of is that quantum computing has almost
nothing to do with what is called quantum cryptography or quantum key
exchange. These are cryptosystems that rely on physical properties
instead of math. I mostly share DJBs opinion on quantum cryptography:
It's probably not really usable in practise and mostly a marketing gag.

@_date: 2014-05-07 23:48:19
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] cryptography Digest, Vol 13, Issue 6 
And I  forgot: If you really want to dig into the topic, there's
a lecture series on youtube:

@_date: 2014-05-08 11:53:51
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Cryptography topic for Research Paper 
There are really two things you should understand before proceeding any
* Quantum computers and quantum cryptography are really two things that
  don't have much to do with each other, except for the fact that both
  contain the word "quantum" in it.
* There are no quantum computers anywhere in the world (at least as far
  as we know it). They are a theoretical idea and if we're optimistic
  about the research we'll see the first quantum computers in a few
  decades. We also may never see them, because building them is really
  challenging.
  (To be 100% precise there are some experimental quantum computers
  with very low bit lengths - I think 15 is a record - but they're not
  very useful except as a theoretical physicis experiment)
Judging from your words I think you're really missing some basic
information here. I'd really advise you to read a bit more about the
topic or watch some youtube lectures before proceeding any further.

@_date: 2014-05-17 18:46:33
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Are there other anonymous key exchange 
When you're asking about usable (== well tested and deployed in
real-worl protocols) key exchanges there really is only DH either in
prime fields or elliptic curves.
There exists NTRU-KE [1], but ntru is patented and it should probably
be considere highly experimental.
[1]

@_date: 2014-05-20 22:12:12
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Facebook on the state of STARTTLS 
If you configure your mail server to not deliver mails to servers with
an untrusted cert many of your mails won't be delivered at all (or you
deliver them without TLS).
Nobody would want to do that, because everyone wants email to stay
usable. Nobody will use an email service that can't send mail to 80% of
the rest of the internet.
So basically everyone just accepts every cert. The only way out would
be either some kind of certificate pinning or some other way to enforce
certificat checking like DANE. But as it stands now: A self-signed cert
is as good as every other cert.

@_date: 2014-05-22 11:07:32
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] New attacks on discrete logs? 
This paper has nothing to do with elliptic curves at all.

@_date: 2014-05-22 11:14:08
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] New attacks on discrete logs? 
Okay, to clear this up:
There's been an algorithm improvement on discrete logs in so-called
finite fields with small characteristics. However, it's not that "new",
it's from early 2013, it just has been presented at the eurocrypt
conference recently.
What does that mean for crypto? Almost nothing at all.
Crypto algorithms that are based on discrete logarithms usually either
use finite fields with large characteristics (usually prime fields) or
elliptic curves.
In theory one could build a cryptosystem in finite fields with small
characteristics. Such systems have been proposed in the past. However,
nobody uses them.
DSA, ElGamal, Diffie Hellmann, ECC-based crypto etc. are all still safe.
So there are two things to learn from this research:
a) If you invent a new cryptosystem, don't rely on discrete log
hardness in finite fields of small characteristics.
b) maybe (but very very unlikely) these results can be extended to
discrete logs in general. Then lots of crypto is screwed. But most
people who know this stuff don't think there's any chance this can be
extended to normal discrete logs.
Oh, and actually there is a third thing you can learn:
Press departments of research organizations tend to present research in
a misleading and exaggerated way. And some journalists tend to just
write that down without critically investigating the claims made.
(and to make that clear: I don't blame the researchers. This is
important stuff. I just blame PR and press people for making this sound
like something it just is not)

@_date: 2014-05-22 21:06:34
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] The proper way to hash password files 
And now how do you guarantee that the key is never disclosed? I mean if
you can do that you can also just make sure the password database gets
never disclosed.
This is the whole point I find annoying about the whole password
hashing / saving debate: Basically, when your database gets stolen
that's one of the worst things that can happen. And no amount of
intelligent storage of passwords will change that.
You can mitigate it a little bit by making password cracking attemps
harder. But you can't change the fact that it's one of the worst things
that could happen. Better try to invest your time in protecting your
database than debating whether scrypt or bcrypt or [insert whatever] is

@_date: 2014-05-23 23:17:58
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] New attacks on discrete logs? 
On Fri, 23 May 2014 14:03:01 -0700
This involves some math, I hope I got everything correct and no
mathematician will beat me for this:
Finite fields are mathematical structures where you can do certain
calculations with certain rules involving addition and multiplication.
Finite fields exist with a number of elements if that number is
[a prime]^[some number]
So you can e.g. have a finite field with 5 elements (because 5 is
prime) or with 2^4=16 elements (because 2 is a prime).
Finite fields with large characteristics means finite fields with a
large prime as the number of elements.
Finite fields with small characteristics means finite fields with a
small prime to the power of a large number of elements.
E.g. a diffie hellman key exchange works like this:
Choose a large prime p and a small number g
A calculates g^a mod p and sends it to B
B calculates g^b mod p and sends it to A
A calculates g^b^a mod p, B calculates g^a^b, both have a shared secret.
Our finite field here is the finite field with p elements, which is
everything calculated mod p. As p is a large prime it's a finite field
with large characteristics.
You could also use not a prime but e.g. 2^[large number] or 3^[large
number]. But that's no good idea, because that'd be insecure according
to this new algorithm.

@_date: 2014-11-18 23:39:43
@_author: Hanno =?ISO-8859-1?B?QvZjaw==?= 
@_subject: [Cryptography] New free TLS CA coming 
This all looks quite nice, except one sentence that puzzles me:
" The automated issuance and renewal protocol will be an open standard
and as much of the software as possible will be open source."
What the hell does that mean? "As much as possible" sounds to me like
"not everything". "Not everything open source" sounds not good.
Why should it be not possible to publish the software that runs this CA?

@_date: 2014-11-19 00:47:53
@_author: Hanno =?ISO-8859-1?B?QvZjaw==?= 
@_subject: [Cryptography] New free TLS CA coming 
Am Tue, 18 Nov 2014 15:35:21 -0800
schrieb Peter Bowen :
I made it a habit to trust people more that make their tech transparent
and less if they present me some certification as an argument for
This is probably a clash of worldviews, but past experiences don't give
me the feeling these kinds of certifications have achieved much in
terms of security.
Is there any ruleset that requires such hw for CAs to be certified in a
way that excludes open source? That'd be very strange indeed...

@_date: 2014-11-19 13:51:03
@_author: Hanno =?ISO-8859-1?B?QvZjaw==?= 
@_subject: [Cryptography] New free TLS CA coming 
Am Tue, 18 Nov 2014 21:56:37 -0500
schrieb "Salz, Rich" :
Sorry, my initial mail probably sounded more rude than it was meant.
That explanation sounds reasonable, it was just some "hu, what do they
mean with that?" moment when I read the webpage. Probably some
explanation on the webpage would be a good idea.
However as I already implied in the other part of this thread: I'd
strongly suggest if the choice is between a "open, verifiable
source code controllable HSM" and "has some fancy certification with
questionable security value" to choose the open solution.
(every time I hear FIPS and common criteria certified I have to think
about this story:

@_date: 2014-11-19 16:31:19
@_author: Hanno =?ISO-8859-1?B?QvZjaw==?= 
@_subject: [Cryptography] Where should I start with cryptography? 
Am Tue, 18 Nov 2014 20:50:16 -0400
schrieb Juan Francisco Verhook Greco :
A very good start is this online course by Dan Boneh:

@_date: 2014-10-04 21:50:51
@_author: Hanno =?ISO-8859-1?B?QvZjaw==?= 
@_subject: [Cryptography] 1023 nails in the coffin of 1024 RSA... 
Am Sat, 04 Oct 2014 12:08:42 -0700
schrieb ianG :
I saw this earlier and got curious, but this doesn't make sense from
start to end.
I personally tried if openssl will for whatever reason round 1025/2
down to 513 by inserting a printf at that point for bitsp and bitsq. It
Even if it would it is not clear how N as a product of a 511 and a 513
bit prime should pose any significant risk.
That said: There are good reasons to get rid of 1024 bit rsa. This is
not one of them. It's a very vague rumor with an implausible story.
However it certainly doesn't hurt if a few people look at the supposed
source code and see if there's anything suspicious.

@_date: 2014-10-07 23:30:41
@_author: Hanno =?ISO-8859-1?B?QvZjaw==?= 
@_subject: [Cryptography] SPHINCS: practical hash-based digital signatures 
I like it that the whole area of post-quantum-crypto is getting more
attention lately.
However what immediately catched my attention: The webpage says
"Signatures are 41 KB, public keys are 1 KB, and private keys are 1 KB"
The signature size is a problem. It makes the claim that it's a
"drop-in replacement" for current signature schemes somewhat
41 kb may not seem much, but consider a normal TLS handshake. It
usually already contains three signatures (2 for the certificate chain
and one for the handshake itself). That already makes 120 kb.
It may not seem that much, but it definitely is an obstacle because this
would significantly impact your loading time.

@_date: 2014-10-14 09:35:05
@_author: Hanno =?ISO-8859-1?B?QvZjaw==?= 
@_subject: [Cryptography] factoring small(ish) numbers 
Am Mon, 13 Oct 2014 19:23:24 -0400
schrieb Jonathan Katz :
The best algorithm for factoring is the general number field sieve.
There's some code available [1], however it's not "ready-to-use" and
requires some manual steps.
I think factoring 512 bit is what's "doable". People have been doing
this on their home PCs for quite a while [2].
768 bit is still challenging and probably not something you do at home.
[1] [2]

@_date: 2014-10-22 11:28:40
@_author: Hanno =?ISO-8859-1?B?QvZjaw==?= 
@_subject: [Cryptography] Simon, Speck and ISO 
Am Tue, 21 Oct 2014 22:16:13 -0000
schrieb dj at deadhat.com:
That sounds interesting, can you give some more background on this?
I'm probably not the only one who has never heard of JTC1/SC27 before.
Wikipedia tells me this is located at the DIN in germany.
What's the role of these approved ciphers? Is anyone bound to
support / use them?

@_date: 2014-10-23 13:30:20
@_author: Hanno =?ISO-8859-1?B?QvZjaw==?= 
@_subject: [Cryptography] In search of random numbers 
Am Thu, 23 Oct 2014 04:43:05 +1100 (EST)
schrieb Dave Horsfall :
You don't really have a problem with getting enough entropy once you
have a system running with mail and an anti-spam-filter. At that point
you already have network timings and disk access.
The tough part is "early-boot-time-entropy" - where do you get your
entropy if you don't have any filesystems and network access
initialized yet?
Please remember: Once you have a single source of reliable entropy for
a few bytes you don't really have a problem any more if your PRNG isn't
completely crap.

@_date: 2014-10-24 10:49:08
@_author: Hanno =?ISO-8859-1?B?QvZjaw==?= 
@_subject: [Cryptography] In search of random numbers 
Am Thu, 23 Oct 2014 17:09:54 -0700
schrieb Tom Mitchell :
Networking, Stack Canaries of first processes etc.
Recently saw a talk on Blackhat EU about it, this seems to be the
background paper:
Interesting stuff.
The other issue you'll have is "first time boot". Then you don't have
any entropy from previous boots. See the RSA key issue Nadia
Heninger and others found a couple of years ago:
It's not just IoT. The RSA attack shows that there are very real
problems with embedded devices on the market today.

@_date: 2014-10-25 00:40:20
@_author: Hanno =?ISO-8859-1?B?QvZjaw==?= 
@_subject: [Cryptography] In search of random numbers 
Am Fri, 24 Oct 2014 11:02:51 -0700
schrieb Bear :
Do you have a smart alternative? What should these devices do? Pre-load
them with a key? (I don't particularly like that idea) Tell users they
need to generate a key on their Desktop for their new Internet of Things
light switch?
Basically most exploit-mitigation techniques (aslr, stack canaries)
these days require some kind of randomness. Sequence numbers should be
random. There are a number of reasons in-kernel and early boot
processes need good randomness.

@_date: 2015-04-16 12:39:19
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] fighting designs in habituation since 1883 
While the result has some plausibility, yeah, I have some comments:
This raises all red flags for a poor study.
* Small number of participants
* MRI scans
* Probably not replicated and not pre-registered (high risk of
  publication bias)
These "we scanned their brains and we found something" studies are done
all the time - and very often they are just not meaningful in any way.
Read this:
And also this one, about MRI scans on dead fish:

@_date: 2015-08-27 21:52:59
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] RC4 and SHA2 
You need to be more precise with the question: What do you mean by
There is no technical reason that would prevent that. Right now as
far as I know RC4 is specified with SHA1 and MD5 MACs, you could replace
that with sha2, would increase your MAC blocks, but it's certainly
But of course it would suffer from all the known attacks on RC4. And
there is an RFC "forbidding" RC4.
Also there is pretty much agreement that future TLS ciphers should have
AEAD modes. So even if RC4 wasn't broken your new construction probably
wouldn't be welcomed.

@_date: 2015-08-28 09:37:35
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] AES Broken? 
Rule of thumb: If anyone finds a real attack on AES, RSA or any other
major cipher you will read about it on the frontpage of the new york
times and everyone will run around trying to shutdown IT
Try to actually read the paper. You'll find things like this.
"In this note we show that
a key component of AES in fact contains a backdoor the allows the
Belgian Government and The Catholic Church (the forces behind
Rijndael / AES design, who obviously hid the backdoor in the cipher) to
secretly eavesdrop on all AES communications."
This is clearly a joke paper.

@_date: 2015-12-07 00:16:02
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Montgomery multiplication bug in OpenSSL? 
I'm the one who discovered this bug. Here's a writeup:
It is still an open question whether this is really exploitable. The DH
case seems to be the most plausible exploit scenario.
Also I have reason to believe this is not that unusual. We already had
a bug in BN_sqr earlier this year. I think testing bignum libraries is
something that needs to be done more thoroughly.

@_date: 2015-12-07 23:50:12
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Opinions on signatures algorithms for 
There are stateful and stateless hash-based schemes, I think XMSS and
SPHINCS are what's state of the art right now.
Stateful schemes are very problematic in many real-world usecases.
SPHINCS is stateless, but it has rather large sigs (42 kb afair).
That's feasible for some apps (think of gpg-like systems), but not for
others (https, where you easily have to transmit 3-4 sigs just for the
It's the standard dilemma of postquantum today: You can choose between
probably secure, but impractical (in this case sphincs, xmss), and
practical, but security is highly uncertain (lattice-based stuff). And
if you really want to use the stuff widely, patents come in as another
complicating factor.

@_date: 2015-12-13 22:43:33
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
In short: Yes, you can do it, yes, you can also buy such devices.
(I'm not sure if they use exactly this method, but there are devices
using quantum effects to generate random numbers.)
But then let me ask: Why would you want to do this?
Essentially what you're doing is you create a complex physical device
for a non-problem. Use a good PRNG and you're fine. There are very few
problems with random numbers and complicated physics devices don't solve
any of them.
It's far more likely that your device has a failure and will thus bias
your random numbers than any average PRNG with a proper cryptographic
algorithm in the background failing.

@_date: 2015-12-20 23:16:38
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Questions about crypto that lay people want to 
Because Voting is a much more complicated problem than banking.
The crypto in onlinebanking is essentially just "You communicate with
your bank and your bank needs to know that it's you and not someone
else." That's relatively easy solvable with existing crypto methods.
With vothing you require:
a) You communicate to another person
b) That other person needs to know that it's authentic, i.e. comes from
a legit voter
c) But that it's not a duplicate. (You're supposed to vote only once.)
c) But - here's the really tricky part - that other person must not
know that the communication (the vote) came from you.
You can have secure online voting, but only if it's not anonymous. The
anonymity of voting is the part that makes electronic voting a hard
(probably impossible) problem.

@_date: 2015-05-01 20:29:28
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] "Trust in digital certificate ecosystem eroding" 
Writing about CA problems and not mentioning HPKP and CT is kinda
I mean I wholeheartedly agree that there are many problems with CAs. I
just would like to ask people who write on that to recognize that
people have been trying to improve things. And some of these
improvements are available and usable.
If you're worried about CA problems use HPKP. It makes CA failures not
impossible, but much less likely.

@_date: 2015-05-07 00:16:01
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Is there a good algorithm providing both 
That almost certainly would result in an insecure construction. While
an attacker can not regenerate the full secret text he'll learn a lot
about it, e.g. things like "this starts with a pattern that is then
repeated 3 times at certain locations in the text". That doesn't
fulfill any reasonable definition of encryption security.
It also hardly makes sense. Performance of symmetric encryption is
largely a non-issue these days. Compression performance is a trade-off.
There are very fast algos, but obviously they aren't the best
compressing ones.
I don't see any reasonable performance gain in a combined algorithm.
BUT and here comes the big BUT: It is tricky to combine compression and
encryption at all. Numerous attacks in the past have shown that this
combination is super-dangerous, e.g. the CRIME and BREACH attack.
I think for HTTP/2 there was some work done on creating a compression
system that is immune to these kinds of attacks (hpack), but I am not
familiar with the details.

@_date: 2015-05-08 00:06:25
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Is there a good algorithm providing both 
The output of an encryption is usually indistinguishable from random
garbage. Random garbage doesn't compress.
The whole point of compression is to exploit structure in the data. The
point of encryption is to hide structure in data. So that cannot work
by definition.
The problem of compression and encryption is a hard one and as far as
I'm aware nobody came up with a good answer to that (except "just don't
compress", but it's an answer we might not like in many situations).

@_date: 2015-10-26 17:31:07
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] composing EC & RSA encryption? 
I don't see any reasonable evidence that would favor either RSA or ECC
in terms of security, therefore I doubt that it would be reasonable.
There's little that the NSA announcement changes about this.
However if you want to design a crypto system with long term high
security goals it might be reasonable to combine a postquantum system
with a "classic" (meaning RSA or ECC) system.
One of the big worries with postquantum systems is that if you replace
RSA/ECC now with a postquantum alg you may end up less secure because
they're less well tested.
There already has been one attempt to combine a key exchange using both
ecdh and rlwe, however as far as I'm aware that rlwe exchange is
patented and the cryptographers I spoke to have a lot of doubts about
the security of rlwe.
One decision you'd have to make is whether you'd want to choose one of
the highly experimental postquantum systems with nice behaviors or
something conservative which is likely secure, but has extraordinary
big keys or signatures.

@_date: 2016-04-09 22:08:43
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] What standards are there for post-quantum 
Generally the answer is just we aren't there yet. The research in
pqcrypto is currently at the algorithm level. There's no agreement
what algorithms will prevail and in what direction to go.
Certificates are a very highlevel use case of crypto. You're asking for
step 3 while we're just starting with step 1.
(There is a more than a decade old draft for NTRU which also includes
some info on how to use it in X.509:
It never made it past draft status. It's controversial whether ntru is
a useful algorithm for the postquantum future.)

@_date: 2016-12-07 19:13:43
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Anyone else seeing an uptick in infected IoT 
Several people have already pointed out mirai, but to give a bit more
context: There is a SOAP script injection vuln that affects a lot of
routers/modems, this one:
(The description is for one specific brand, but it seems it affects a
variety of devices)
And there's currently a mirai variant that uses this bug to infect
You can observe this botnet pretty easily: Just let netcat listen on
port 7547 and you'll see an attack payload pretty quickly.
It was first thought that the telekom outage was because the routers
were part of that botnet. However it turned out they were not, but they
had another bug that let their network stack malfunction if lots of
connections arrive on port 7547. So the telekom outage was kinda a
sideeffect of that botnet activity.

@_date: 2016-02-11 20:27:09
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] 
=?utf-8?q?of_Suite_B?=
On Thu, 11 Feb 2016 07:38:06 -0800
You miss two major reasons why people don't like DSA:
1. It's extremely fragile when it comes to bad random numbers. Use it
once with a bad RNG: Your key is compromised.
2. DSA was limited to 1024 bit for a long time, a 2048 bit option was
only added later. For many implementations this means either use it
with 1024 bit or not at all.
Given that I find it reasonable to drop support (and I have strongly
argued for the removal from TLS 1.3).

@_date: 2016-03-04 23:30:59
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Side channel attack on OpenSSL ECDSA on iOS and 
This is a different issue called cachebleed. Their webpage and paper is
It suffered a bit from lack of attention because it was published at
the same time as DROWN. It's quite interesting research (although
honestly I don't understand large parts of it).

@_date: 2016-05-01 13:16:03
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] WhatsApp, Curve25519 workspace etc. 
On Sun, 1 May 2016 10:58:59 +0300
Not sure where you're getting with this. 56 bit security is broken, 128
is not (and most likely never will be).
Maybe you're line of thinking is that 128 is "only" a bit more than
twice the size of 56. But that's not the case. You're counting bits
here that exponentially increase the complexity. 128 bit is not (a bit
more than) twice the security of 56, it's another universe of security.
Ok, I must say I was surprised that Whatsapp uses CBC (I had expected
either gcm or chacha20-poly1305), but there is no risk here either.
All the weaknesses of CBC don't affect the mode itself, but a bad
combination of cbc+hmac. Quickly skimming into the whatsapp whitepaper
they use cbc+hmac with encrypt-then-mac. That's safe. What's unsafe is
using the other way round or some wacky encrypt-and-mac constructions.
It seems to me that what you classify as "so many risks" are just two
misunderstandings. Neither the 128 bit security of curve25519 nor cbc
in encrypt-then-mac mode are a risk.

@_date: 2016-05-23 08:27:59
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Entropy Needed for SSH Keys? 
This is a real problem, Nadia Heninger and others found countless
devices producing breakable keys due to this:
~128 bits of entropy are enough for everything with a reasonable
safety margin. (As long as you can be sure that your 128 bits are really
random. If you are not add some more.)
Here you have a fundamental misunderstanding (albeit a common one).
Entropy bits don't get used up (although Linux's /dev/random manpage
tries to tell you so). Once your rng is properly initialized with enough
entropy you can use it practically forever.

@_date: 2016-10-12 11:50:51
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] 
=?utf-8?q?rapdoors=E2=80=9D_in_millions_of_crypto_keys=22?=
On Wed, 12 Oct 2016 09:19:49 +0200
Brainpool is a particularly bad example of a NUMS mechanism.
They claim that this is a problem with the NIST curves, yet their own
mechanism looks almost as suspicious. They have a repeated pattern in
their curve parameters [1] and via brute forcing it's easily possible to
generate a 1 out of 16 million parameter set, as has been shown in the
bada55 research [2].
To clarify: I don't believe there's a backdoor in Brainpool. But in
terms of NUMS mechanisms it's more an example of how not to do things.
[1] [2]

@_date: 2016-10-14 19:34:00
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] How the NSA broke trillions of encrypted 
"How the NSA broke trillions of encrypted connections"
or more accurately:
"How journalists completely failed to understand a scientific paper
about cryptography and wrote a stupid article about it"

@_date: 2016-09-17 09:47:39
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Ada vs Rust vs safer C 
On Fri, 16 Sep 2016 16:31:04 -0400
People have tried to make safer C. The problem is, if you want memory
safety you'll have a big overhead.
E.g. let's simply say you define that all invalid memory reads and
writes (buffer overflow, use after free, ...) lead to a termination of
the application. That would prevent exploiting any of them, although at
the cost of potential DoS.
There is Softbound+CETS, which implements a strong concept of memory
safety. It has more than 100% overhead and it never got really
completed, so you can't run real world code with it.
The closest thing to a safer C that works with real code is Address
Sanitizer, which has a weaker concept of memory safety and is not
designed for production. In its current form it'll fight some security
issues and introduce new ones. But that could potentially be fixed.
You still have a runtime overhead of 50% and probably more
significant a huge memory overhead.

@_date: 2017-04-02 13:14:05
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] RSA Crypto is officially insecure due to NIST 
They're not, Signal and OMEMO use ecc-based crypto.
Yet the issue you're referring to seems to be threats from quantum
computers. ECC and RSA are equally affected.
The transition to post quantum cryptography is a topic of very active
research, yet the community hasn't reached a conclusion which
algorithms are practical and trustworthy.
Goldbug looks like a snakeoil messenger.
Ntru has been patented and only very recently its patent holder
announced that it intends to make it freely available. This has
hindered any adoption in the past.
Mceliece is only secure in variants that have keys in the megabyte size
range. This is impractical for most use cases.
Alternative encryption schemes being investigated are based on ring
learning with errors. New Hope and NewHope-Simple [1] are recent
variants. Other fields of postquantum research are supersingular
I agree that messengers should start investigating postquantum crypto,
but the field is very much in flux and early adopters should be
prepared to overthrow whatever they've been implementing. For now
implementing hybrid schemes is a good option.
If you read the NIST document you linked then you'll notice that NIST
is only in the phase of starting a standardization effort for pqcrypto.
Unfortunately we don't have "standard options" for postquantum
encryption schemes yet that we can easily adopt.
[1]

@_date: 2017-03-08 00:34:44
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Crypto best practices 
That's already bad advice. Use an AEAD, always. From the mentioned ones
only CTR is an AEAD.
If you read through the whole document it's long and contains a lot of
strange advice, including recommendations for RC4 + countermeasures
that we know don't properly work. It's full of recommendations that I'd
name outdated.
There's also some good advice in there, but none of it is surprising.

@_date: 2017-03-08 00:35:25
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Crypto best practices 
argh, sorry. Of course this should read "Only GCM is an AEAD"

@_date: 2017-03-10 15:49:39
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Crypto best practices 
This is horrible advice nobody should follow. Using both HMAC and
asymmetric cryptography has led to a pletora of vulnerabilities in the
past. It can be done right, but it is full of pitfalls. With HMAC you
have to consider the order of encryption and MAC-ing, with signatures
there are very subtle bugs that can and do happen easily (see XML
signature wrapping attacks or the recent imessage vulnerability).
Just use authenticated encryption with an AEAD. Don't try to do anything
that you think is like an AEAD. It most likely is not.

@_date: 2017-03-14 12:47:35
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Crypto best practices 
GCM isn't perfect. I should know, I've written a paper about attacks on
GCM. We should move to more resilient AEADs. A lot is happening in that
space right now.
But I'd always say using GCM is a better solution that not using AEADs
at all and handcraft your own solution with CBC+HMAC or - worse -not
using any authentication (which is surprisingly common).
I'm not sure I fully understand your remark about RC4. I guess it's
something that RC4 is a stream cipher and GCM is based on counter mode
and thus also "like a stream cipher". One can argue that this is a more
fragile thing than some other constructions.
But the main problem with RC4 was that the keystream is biased. I'm not
aware of any similar issue with GCM.

@_date: 2017-03-16 00:02:32
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Crypto best practices 
It does, but GCM fails more catastrophically. Peter has a point there.
There's certainly room for less bad AEADs.
TLS 1.2 basically says "implementor can choose the IV however he
likes, making sure that it doesn't repeat is his business". That's a
terrible way of doing things. And obviously some people got it wrong:

@_date: 2017-05-07 23:45:50
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] Blockchained code signing. 
This idea has already been discussed under the term "binary
transparency", although not a whole lot has happened implementing it,
but mozilla made some experiments:
However I'm not really happy with this limited form of transparency.
Ideally I'd like not only to sign and log the software binary, but also
the corresponding source and a build instruction how the source became
the binary.
Wanted to write down some thoughts on that for a while...

@_date: 2017-10-27 00:46:42
@_author: Hanno =?UTF-8?B?QsO2Y2s=?= 
@_subject: [Cryptography] [FORGED] Response to weak RNGs in Taiwanese and 
Good luck getting an answer.
I covered this for a german news article [1] and asked both BSI (german
office for it security, they were responsible for the CC certification)
and NIST about a statement. I'm still waiting for an answer.
It seems the gov agencies responsible for certification aren't willing
to talk about this incident.
