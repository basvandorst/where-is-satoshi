
@_date: 2001-08-02 18:11:41
@_author: Arnold G. Reinhold 
@_subject: moving Crypto? 
I don't think it is an overreaction. Would you hold a political science conference in China right now?  Moving to Canada would be an important statement.  I'd prefer Toronto because it's a fairly central location, but Vancouver is cool too.

@_date: 2001-08-16 10:45:00
@_author: Arnold G. Reinhold 
@_subject: If we had key escrow, Scarfo wouldn't be a problem 
I don't think anyone in the Scarfo case is denying the need for probable cause. The issue, as I understand it, is whether an ordinary search warrant, which the FBI got, is enough or whether a more-difficult-to-obtain electronic surveillance order was required.
Arnold Reinhold

@_date: 2001-08-31 09:26:18
@_author: Arnold G. Reinhold 
@_subject: CIA funds anonymous web surfing 
Today's Boston Globe and New York Times report that the CIA is funding Sefeweb technology that lets users surf the Web anonymously. The parent agency of the Voice of America is negotiating a license for the technology to use in bypassing China's 'Net censorship.
"The US-funded network of proxy servers running Triangle Boy would provide a ''dynamic, constantly changing set of access points through which Internet surfers in China can connect to VOA and to other Web sites that are typically blocked,' Hsu [Safeweb CEO] said."
 So if a U.S. citizen working as a programmer on this project visits China to deliver a paper at a conference and is arrested for conspiracy to violate China's laws, will the U.S. consider the arrest and subsequent prosecution a legitimate expression of Chinese Arnold Reinhold

@_date: 2001-12-07 14:18:59
@_author: Arnold G. Reinhold 
@_subject: I-P: Papers Illuminate Pearl Harbor Attack 
This story smells of revisionism.  The events leading up to Pearl harbor are throughly chronicled in the first chapter of David Kahn's classic, The Codebreakers. In particular:
o The Tojo government, regarded as militarist, came into power in October 1941 (Togo was Tojo's foreign minister)
o The order to attack Pearl Harbor was promulgated on November 4
o The last ship of the Pearl Harbor strike force, the aircraft carrier Kuikaku, reached the fleet assembly point  in the Kuriles on November 19
o Japan presented its ultimatum to the U.S. on November 20, which would have required the U.S. to acquiesce to Japan's conquests in Asia and supply her with oil
o On November 25, the the Pearl Harbor strike force was ordered to leave on it mission at 6 am the next day
o The reply U.S. that the delivered to Japan on November 26 came "after a week of frantic ... consultations ..." (Kahn). These may have led to the Chinese cables reported in the story and created a flicker of hope among whatever doves remained in the Japanese government. It was not a close situation, however. Had the U.S. been willing to accede to Japan's terms, the strike could have been called off, but the die had been cast and Japan's war preparations were well under way. (Remember that Pearl Harbor was but one of many places that Japan attacked 60 years ago today).
As for the notion that "Japan may have tried to warn the United States about the attack," this is no doubt the famous 14-part telegram breaking off negotiations that was to be delivered to the Secretary of State Cordell Hull at 1 pm Washington time (7:30 am Honolulu time), 25 minutes before the first bomb fell.  The U.S. had intercepted and decoded the first 13 parts that telegram early on December 7. The last part and the portentous instruction to deliver it at 1 pm were recovered later that morning.  Secretary Hull was concerned about looking properly upset when he was handed the official copy by the Japanese ambassador, lest the ambassador suspect that Hull had already seen it. Kahn describes how Army attempts to deliver a final warning to Pear Harbor were bollixed up by communications problems.
Arnold Reinhold

@_date: 2001-12-10 12:20:56
@_author: Arnold G. Reinhold 
@_subject: Japan Broke U.S. Code Before Pearl Harbor, Researcher Finds 
I'd add "The Broken Seal," by Ladislas Farago, which discusses the cryptography as well.
The fact that Japan was breaking U.S. codes (not ciphers) is nothing new, though it would be interesting to know which systems the new researchers claim were broken. Kahn discusses the weakness of U.S. diplomatic codes at length in "The Codebreakers," 1967. See Chapter 15, especially p. 495. The most secure encryption system the U.S. State department used at the time was the M-138 strip cipher, which is cryptographically identical to the Jefferson wheel. However, according to Kahn, the Japanese did not break that one. Other U.S. diplomatic codes were notoriously weak. Kahn describes one incident in which a retiring U.S. diplomat delivered his farewell speech in GRAY code.  Kahen even says "The nonsecrecy of the State Department codes inhibited negotiations with the moderates in Japan," referring to the summer and fall of 1941.
U.S. foreign policy had opposed foreign colonization of China since the Opium wars of the 19th century. Far from being surprised by the U.S. response to Japan's November 20 cable, there is plenty of evidence that the Tojo government was counting on it to get the Emperor's final approval for their elaborate war plans, which were already in motion.
As for the "lesser-known command, "Climb Mt. Tsukuba," which meant return [for the Pearl Harbor attack force]",  its primary use would have been in case the Japanese got an indication that the U.S. knew of the impending attack. The Japanese were very worried that the attack force, which was the heart of their Navy, might be discovered and pounced on by the American Navy, thousands of miles from Japan. This, of course, is exactly what happened six months later at the Battle of Midway.
Arnold Reinhold

@_date: 2001-12-11 09:29:19
@_author: Arnold G. Reinhold 
@_subject: FreeSWAN & US export controls 
In the most recent ruling, Universal v. Remerdez/Eric Corley 2600.com (00-9185),  , the US Court of Appeals for the Second Circuit declined to overturn an injunction against the posting of DeCSS on the Internet. The Court held that software was speech, but did not enjoy the level of First Amendment protection accorded to pure speech because it is functional with little human intervention. This is a very disturbing precedent which I hope will be reversed on appeal, but given the post-9/11 mood and the limited technological understanding of most judges, I wouldn't count on it. Also I believe the U.S. Supreme Court has upheld export controls in the past, the First Amendment notwithstanding.
Having a body of open source crypto software that is not entangled by any U.S. input is not a foolish idea.  Surely there are good programers outside the U.S. who understand the importance of making FreeSWAN work seamlessly with Linux.
Arnold Reinhold

@_date: 2001-12-25 23:46:30
@_author: Arnold G. Reinhold 
@_subject: Stegdetect 0.4 released and results from USENET search 
This is an nice piece of work, but I have a couple of comments:
1. The paper asserts "Even if the majority of passwords used to hide content were strong, there would be a small percentage of weak passwords ... and we should have been able to find them."  That might be true if there are a large number of stego users independently selecting passwords, but it's not a compelling argument if stego is being employed by a few sophisticated terrorist  organizations, as suggested by the April 1991 Newsday article,  . It is quite likely that such organizations  train users to select strong passwords or passphrases. Indeed, since the stego systems use symmetric keys, field cells would have to be assigned passwords prior to deployment. In all likelihood this would be done by a central communications group, with good crypto skills.
Even if some cells did use weak passwords, they are likely to derive them from languages and religious quotes  that I suspect are not well represented in your dictionary. There is also the possibility that the terrorist organizations modified published stego programs or built their own from scratch, perhaps to incorporate public key methods. In that case, a dictionary attack is hopeless.
I don't think you can conclude much from the failure of your dictionary attack to decrypt any messages.
2. The signature graphs you presented for several of the stego methods seemed very strong. I wonder if there is more pattern recognition possible to determine highly likely candidates. I would be interested in seeing what the graphs look like for the putative false alarms you found. It also might be interesting to run the detection program on a corpus of JPEGs known NOT to contain stego, such as a clip art CD.
3. If you did succeed in decrypting one of Osama Bin Laden's missives, wouldn't he have a case against you under DMCA?
Arnold Reinhold

@_date: 2001-12-27 17:52:30
@_author: Arnold G. Reinhold 
@_subject: CFP: PKI research workshop 
It seems to me that a very similar argument can be made regarding the need (or lack there of) for a national identity card.  Organizations that require biometric identity can simply record that information in their own databases. The business most widely cited as needing national ID cards, the airlines, already maintain elaborate customer databases for their frequent flyer programs. Adding biometrics, with mileage points or faster check-in as incentive, would be easy enough. Your frequent flyer application would authorize the airline to compare your  identifying data with security databases at other airlines, credit bureaus, the government, etc.
If photo matching software is unable to validate two photos of you from different databases, both would be displayed next time you check in. If the clerk decides they match, that would be recorded. If the discrepancy is major, you would be taken aside and the matter investigated. Over time, the confidence in any individual's record would grow as more use is made of it. There is still the problem of protecting the database from alteration, but that applies to whatever database would be used to issue national ID cards as well.
Even if high speed data lines are not available at all gates, reservations are normally made well in advance, so a passenger list with biometrics could be prepared overnight and delivered to each gate in printed form (for photos) or on a CD-RW.  Last minute reservations could be handled by slower data links or the maker would simply be subject to a higher level of scrutiny. Passport numbers could be requested at the time of an international reservation and checked with the issuing government well before flight. Government's that don't cooperate would have their citizens subject to additional scrutiny. During times of heightened alert, additional cross checking can be implemented.
None of this is particularly hard and all the issues of of forged, revoked, stolen or cursorily examined ID cards go away. So do the issues of abuse where petty officials confiscate your ID card leaving you helpless.
Arnold Reinhold

@_date: 2001-12-28 13:41:36
@_author: Arnold G. Reinhold 
@_subject: Stegdetect 0.4 released and results from USENET search 
I certainly can't imagine any group or activity that would generate the hundreds of thousands of stego messages a 0.6 percent rate In general you are of course correct. But this particular case may be an exception. I am not a stego maven, and before reading your paper, it never occurred to me that some stego software would be designed to place message bits in the first n available slots. Spreading them pseudo-randomly seems so easy and so obvious a win.  However, since much software out there does use first n slot message placement, detection of such messages may be possible with a very high signal to noise ratio. The graphs in your papers, with very flat tops and bottoms and steep skirts suggest that to me.  They are very different from the false-positive graphs in the slides above. It may possible to distinguish them with high enough confidence to be able to assert the presence of stego messages even if they cannot be decrypted.
Arnold Reinhold

@_date: 2001-12-30 10:23:26
@_author: Arnold G. Reinhold 
@_subject: Stegdetect 0.4 released and results from USENET search   
A background stream of ordinary, unencrypted voice and e-mail to family and friends, plus some pre-established code phrases, is all one needs for the occasional "Attack at Dawn" message. From press reports, that appears to be what the September 11 cell used.
I would assume cybercafes are prime targets for signal intelligence organizations and all e-mail traffic they generate is recorded. More generally, imagine you are a consultant to some nefarious organization and think about what it would take to convince them that the method you propose is safe, capable of being taught to their covert agents, and tolerant of the inevitable slip ups in the field (and remember their attitude toward warrantee disclaimers).
All this is fun speculation, but avoids the original question in the thread: is it possible to reliably detect stego use, given certain weakness in many widely available methods?
Arnold Reinhold

@_date: 2001-12-31 12:32:44
@_author: Arnold G. Reinhold 
@_subject: Steganography & covert communications - Between Silk and 
What makes this book so excellent is that Marks was not just the chief cryptographer at SOE, he was the *only* cryptographer. He got to do everything. A young amateur crypto enthusiast, he didn't make the cut for Bletchley after basic cryptanalysis training due to a bad case of smart ass and was sent down to SOE. He almost failed to get that job when it took him all day to decipher a test message. He didn't realize he had been supplied the key.
More important, it sharply increased their risk of being caught by German radio direction finders. Agents had been captured or shot in the middle of re-transmissions.
That's not quite fair to Mr. Marks. General de Gaulle used a double transposition cipher similar to the one the OSE had been using since before Marks got there, though Marks had to discover this on his own. Marks' codemaking efforts were directed toward improving that cipher and replacing it with a one-time pad. One advantage of the one time pad was that messages could be short, reducing the DF risk. Double transposition cipher required a minimum length, 200 letters, lest they simply be anagramed.
I have a review of the book at Arnold Reinhold

@_date: 2001-07-15 21:59:46
@_author: Arnold G. Reinhold 
@_subject: Crypto hardware 
The CA's secret key is not the only weak link. There is also the the software that submits certs to be signed to the "tamper proof" smart card. If I can gain control of that software, it is a simple matter to have your smart card sign any cert I want. And if I get root on your off-the-shelf PC, such an attack would not be hard to mount.
At the very least, one needs some audit trail maintained inside the tamper proof module and a tamper proof means to display that audit Arnold Reinhold

@_date: 2001-07-27 11:33:08
@_author: Arnold G. Reinhold 
@_subject: Attention CipherSaber Users!! 
A draft paper by Scott Fluhrer, Itsik Mantin and Adi Shamir was released on July 25, 2001 and announces new attacks on the RC4 cipher that is the basis for CipherSaber-1. Some of these attacks specifically involve the use of an IV with a secret key, the very scheme used in CipherSaber.  Prof. Shamir states in an e-mail accompanying the release:
"Attached you will find a new paper which describes a truly practical direct attack on WEP's cryptography. It is an
extremely powerful attack which can be applied even when WEP's RC4 stream cipher uses a 2048 bit secret key (its maximal size) and 128 bit IV modifiers (as proposed in WEP2). The attacker can be a completely passive eavesdropper (i.e., he does not have to inject packets, monitor responses, or use accomplices) and thus his existence is essentially undetectable. It is a pure known-ciphertext attack (i.e., the attacker need not know or choose their corresponding plaintexts). After scanning several hundred thousand packets, the attacker can completely recover the secret key and thus decrypt all the ciphertexts. The running time of the attack grows linearly instead of exponentially with the key size, and thus it is negligible even for 2048 bit keys."
The paper itself, titled "Weaknesses in the Key Scheduling Algorithm of RC4," has been posted at  (in PDF format) and  at  (in Postscript).
WEP is an encryption system used with 802.11 wireless Ethernet that employs RC4, but the attack affects CipherSaber as well.  Note that "several hundred thousand" separate CipherSaber messages encrypted with the same key would have to be collected for this attack to succeed.  None the less, from a cryptographic standpoint, this is too close for comfort.
Accordingly I recommend that CipherSaber users switch to CipherSaber-2 with a parameter N=20 or larger. The RC4 state vector will thus be mixed 20 times instead of once. This large a value for N is probably overkill, but until there is time to fully digest the implications of this paper, it is better to err on the safe side.  If this is impractical for any reason, I recommend changing keys on a regular basis to limit the amount of traffic encrypted with any one CipherSaber key (even though the IVs differ).
If and when a consensus develops on the best way to fix RC4, I will announce a corresponding version of CipherSaber. Visit the CipherSaber page  periodically for updated information.
Arnold Reinhold

@_date: 2001-07-27 18:36:53
@_author: Arnold G. Reinhold 
@_subject: Criminalizing crypto criticism 
If you read the language carefully, you will see that 1201g only permits *circumvention* as part of cryptographic research (and then only under limited circumstances). There is nothing in the law that allows publication of results.
Even the recent Shamir, et. al. paper on RC4 and WEP could arguably violate DMCA. WEP could be considered a TPM since it protects copyrighted works (e.g. e-mail). More importantly RC4 could be used in some other copy protection system that we don't know about -- it's use might even be a trade secret.  There is simply no way to guarantee that a given cryptoanalytic result doesn't compromise some TPM. Even software that breaks Ceaser ciphers could be actionable. DCMA is *that* bad.
Arnold Reinhold

@_date: 2001-07-29 15:23:21
@_author: Arnold G. Reinhold 
@_subject: Effective and ineffective technological measures 
The law attempts to define it:
'1201(a)(3)(B) a technological measure ''effectively controls access to a
               work'' if the measure, in the ordinary course of its operation,
               requires the application of information, or a process or a
               treatment, with the authority of the copyright owner, to gain
               access to the work.'
I suspect most judges would interpret "the ordinary course of its operation" the latter way.  Clearly Judge Kaplan was not impressed by the fact that CSS was broken by a high school kid.  There is also the argument that if a measure is really effective in plain English meaning, you don't *need* an anti-circumvention law.
Whether the anti=circumvention provision is constitutional, since it eliminates fair use, is another question. There is an excellent "Twiki" site at Harvard Law School that has many of these arguments and also allows others to contribute: Arnold Reinhold

@_date: 2001-07-30 09:14:54
@_author: reinhold@world.std.com 
@_subject: NYTimes.com Article: Organized Crime Case Raises Privacy Issues  
This article from NYTimes.com has been sent to you by reinhold at world.std.com.
This is apparently the first case to come to trial where the FBI used a "keyboard logger" to capture a PGP key. The evidence is being challenged on the question of whether the FBI needed a wiretap order or just a search warrent to place the logger.
Arnold Reinhold
Let NYTimes.com Come to You
Sign up for one of our weekly e-mails and the news will come directly to you. YOUR MONEY brings you a wealth of analysis
and information about personal investing.  CIRCUITS plugs you into the latest on personal technology. TRAVEL DISPATCH offers you a jump on special travel deals and news.  For information on advertising in e-mail newsletters or other creative advertising opportunities with The New York Times on the Web, please contact Alyson Racer at alyson at nytimes.com or visit our online media kit at For general information about NYTimes.com, write to help at nytimes.com.  Copyright 2001 The New York Times Company

@_date: 2001-06-04 11:31:38
@_author: Arnold G. Reinhold 
@_subject: McNealy -- Get over it, Part Two (was Re: BNA's Internet Law 
I agree that consumers are all too willing to sell the privacy for cheap, but my point is that the bargain is being misrepresented. Scott, who I suspect is motivated by a desire to sell servers, has been saying that protecting the privacy of consumer records is impractical and anyway there are some benefits to relinquishing privacy. When the  example benefits do not, in fact,  require privacy to be compromised, that is a deception.
As to the first point, that privacy laws are impractical, I've created a Web page with an actual US privacy law that covers one narrow type of computer record.  Identifying information as to the type of record has been deleted. You have to guess what it is. Take the quiz at Arnold Reinhold

@_date: 2001-06-12 20:32:24
@_author: Arnold G. Reinhold 
@_subject: Thermal Imaging Decision Applicable to TEMPEST? 
This decision(Kyllo v. US) is important and very welcome, but I am not sure you are right about the prior status of TEMPEST. There was an earlier decision (Katz v. US, 1967), cited in the Kyllo decision, that "involved eavesdropping by means of an electronic listening device placed on the outside of a phone booth." The court held back then that doing this without a warrant violated the Fourth Amendment. I can't see how this would fail to apply to TEMPEST.
TEMPEST is not shut down by any means. This decision applies to homes and places where there is an reasonable expectation of privacy (like a phone booth). The status of computers in offices, cars, and public places is less clear. Your data stored on someone else's computer outside you home is apparently not protected (they got Kyllo's electric bills legally without a warrant). In any event, the NSA can still use TEMPEST against foreign nationals and overseas, the FBI can use it against US nationals with a warrant, and the government can, de facto, use it secretly, as many people believe they now use wiretapping, to develop information that leads to other evidence that is admissible.
The other interesting thing about Kyllo is that the Court clearly needed the help of a good physicist.  If you read the oral arguments,  508.pdf you'll see that no one in the court had a basic understanding of the science. The case involved a bust for growing marijuana. The police had obtained Kyllo's electric bills (no warrant required) and found he used a lot of power.  Since power usage varies a lot among houses, this was not considered sufficient to get a search warrant. They then used the thermal imager. The government claimed they only used the imager to verify that a lot of heat was being produced in the house. No one pointed out that, except for highly unlikely circumstances (e.g. someone running a lighthouse or charging a LOT of batteries in the basement), essentially all the electricity consumed by a house is converted to heat.  Discovering that the house radiated a lot of heat added no new information to what the utility bills said. The defense claimed it was the presence of specific hot spots in the image that made the warrant issuable and that these revealed what was happening inside the house.
There is also some physically unrealistic stuff in the dissenting opinion. Justice Stevens suggests that "the rare homeowner who wishes to engage in uncommon activities that produce a large amount of heat [can] make sure that the surrounding area is well insulated." Unless the homeowner is planning to set her house on fire, that won't work. The heat has to escape somewhere. A system that spread the heat so evenly that a thermal imager couldn't detect the source is far beyond the abilities of a homeowner to construct.
This is a great science and law case.
Arnold Reinhold

@_date: 2001-06-20 17:12:00
@_author: Arnold G. Reinhold 
@_subject: septillion operations per second 
One septillion =  10**24 or about 2**80. If you assume 1000 operations to test a key, a septillion ops per second machine tests about 2**70 keys per second. For a 128 bit key, that means you need about 2**57 seconds on average to find a key, or about 4.6 billion years, the age of the Earth.  A million of them (not likely) would do the job in only 4600 years.
Arnold Reinhold

@_date: 2001-05-31 08:38:09
@_author: Arnold G. Reinhold 
@_subject: McNealy -- Get over it, Part Two (was Re: BNA's Internet Law 
"I have agreed to let my car company, for instance, track my every move through GPS satellites. Some people might consider that an invasion of privacy, but I find it comforting to know that, should my air bag deploy, they know where I am and can send help."
Why is it necessary to track a car's exact location at all times just to know it's position when the air bag deploys? The radio that send the message "Air-bag deployment on car US CA 9XYZ123"  can simply include a position report. Yeah, it's a few more bits, but the radio had better be pretty survivable if I'm going to rely on it for crash reporting and, anyway, the cellular system that receives the message can do a location check as well.
Another benefit Scott suggests to compensate me for letting myself be tracked is the ability to be told of nearby restaurants and other services, based on my preferences.  But again, why not just let me push a button that says "I'm hungry" and send out a position report then? Why do I have to be tracked 7/24?
Arnold Reinhold

@_date: 2001-05-31 15:15:09
@_author: Arnold G. Reinhold 
@_subject: McNealy -- Get over it, Part Two (was Re: BNA's Internet 
As I understand the new FCC E911 rules  we soon won't have a choice, at least in the U.S. Mobile phones will be required to include tracking capability, supposedly to allow faster response to 911 calls.  Again, there is no reason microprocessor in the phone couldn't detect a 911 call and send out a position report only then, but we will probably be tracked whenever the phone is on. At least there is a law limiting what the phone companies can do with this data (47USC222), but I don't expect law enforcement, at least, to have any trouble accessing it.
Second, I'm not sure I buy the choice argument. What happens, for example, when auto insurers demand that you have car tracking on at all times? A little database listing speed limits on all roads, the location of stop sign, etc. would make it easy to spot unsafe drivers. Add in the location of bar parking lots, adult book stores, neighborhoods where drugs are dealt, etc. and the insurance company can reduce its risks even further. You say switch to an insurer that doesn't make those demands? Try finding health insurance that doesn't make you sign away your rights to control access to your medical If anything Scott is at more risk. Think kidnappers. Think terrorists. Think insider trading ("Scott has visited ZorchCorp twice in the last week, time to buy!"). And he better not have a mistress. Of course the real reason Scott is saying "Get over it" is that he estimated how many servers will be needed to collect, store and data-mine all this tracking info.
Arnold Reinhold

@_date: 2001-11-07 10:06:42
@_author: Arnold G. Reinhold 
@_subject: More on Drivers' Licenses 
Noah Silva recently brought this interesting 1994 article on DMV data exchange by Simson Garfinkel to the attention of the dvd-discuss at eon.law.harvard.edu list:
The article discusses the  AAMVAnet system and the extent to which the threat of revocation of driver's license is already being used as a tool for social control.  It's also clear that the state DMVs are in a unique position to provide identity information for a future PKI.
I did some poking around on Google to see what has been happening in this  area since then. I found the American Association of Motor Vehicle Administrators web site which announces:
"On October 24, 2001, AAMVA's Executive Committee passed a resolution creating a  Special Task Force on Identification Security to develop a strategy on enhancing the issuance of secure identification credentials for driver licensing and  photo ID purposes, and to develop short- and long-term priorities and actions."
They already have a standard for Driver IDs that is available on-line
 (full text)
It is a very through and detailed document that builds on a raft of existing international standards (smart cards, bar codes, JPEG, etc.) and US DMV and LE practices (data dictionaries, encodings, fingerprint and signature storage, etc.).  It does not prescribe any card technology, but sets standards to be used if a technology is What is strikingly to me about the document is the complete lack of cryptographic standards. The document specifically discourages encryption of machine readable data unless required by law. In a very interesting Appendix H on physical security measures, digital signatures are mentioned only in passing under Machine Readable Data:
"Common techniques to ensure data integrity include:
   ? Check digits and data encryption (presumably with public key encryption)
    ? For IC cards, tamper detection and chip disabling; and digital signatures for all data written to the chip."
That's it! There is a set of proposed revisions to the standard, but they are only accessible to AAMVA? members.  I don't know if the revisions  address crypto issues, but from the quote above,  I suspect they have a long way to go.
Arnold Reinhold

@_date: 2001-10-01 18:20:08
@_author: Arnold G. Reinhold 
@_subject: Best practices/HOWTO for key storage in small office/home   
Here are a few suggestions:
o Use mini-CD-R's for key storage. There is even a rectangular, credit-card sized format available. (Note that mini-CDs are not compatible with slot loading CD drives.)
o Perform all encryption, signing, etc. on a lap top or palm top that is kept in a safe or on your person when not in use and is never connected to a network.  Transfer files via floppy or memory cards. Standard PC's that are physically accessible to strangers or connected to networks are not safe for storage of unencrypted keys, even for a moment.
o Use strong, randomly selected passphrases, see e.g. diceware.com
o Use Macs, particularly pre-OS-X. The OS has fewer hooks for viruses and worms and there are fewer virus templates out there. That doesn't mean these machines can't be broken into, only that it takes more o Safes should be backed up by a good alarm company. Even the best safes afford protection against attack for a matter of hours.
I have some links to information on safes at Arnold Reinhold

@_date: 2001-10-04 13:00:04
@_author: Arnold G. Reinhold 
@_subject: NSA upgrade plans 
There is an interesting article in Federal Computer Week  that says NSA planning a major effort to modernize the nation's cryptoystems "which are rapidly growing obsolete and vulnerable." They quote Michael Jacobs, head of NSA's information Assurance Directorate as saying the the underlying encryption algorithms are nearing the end of their life expectancy.
There were hints in the past that NSA used 90-bit keys for some ciphers. I wonder if that is the issue or if they see the quantum computing handwriting on the wall and plan to go to 256-bit (or larger) keys.
Arnold Reinhold

@_date: 2001-10-04 13:00:30
@_author: Arnold G. Reinhold 
@_subject: Historical PKI resources 
You might also look for information on the NSA's STU-III secure telephone system which I believe uses a form of PKI.  There is a fair amount of information about it available on the web.
Arnold Reinhold

@_date: 2001-10-04 18:41:44
@_author: Arnold G. Reinhold 
@_subject: AGAINST ID CARDS 
I too am very nervous about the prospect of national ID cards.  I have an idea for a possible compromise, but I have not made up my mind on it. I'm interested in hearing other people's opinions.
The idea is a federal standard for secure  drivers' licenses. These would be cards containing a chip that stores an electronically signed and time stamped data file consisting of the driver's name, date of birth, height, address, photo, and scanned signature, as well as endorsements such as truck, school bus, motorcycle and hazmat operator licenses. All this information is contained in existing drivers' licenses, but in a way that is too easy to forge.
The licenses would still be issued by the states so there would be no new bureaucracy.  People who don't drive could get "proof of age" cards using the same technology. Many states now issue such cards in conventional formats for liquor purchase. There would be pressure to expand the use of these licenses to other uses. That has already happened for conventional DLs with liquor purchase and airline boarding. Some new uses might be acceptable, e.g. using the cards to contain  pilot or boating licenses. Limitations on new uses could be included in the enabling legislation.
The security model of the card would be privacy oriented, i.e. limiting who could access the cards to authorized users and the owner. The integrity of the information would come from the electronic signatures.  As I understand it, much of the forgery of DLs that now takes place involves unauthorized use of the equipment that produces legitimate cards. The secure DL would cut down on this because the information on the card would be signed by by the operator of the equipment, making the forgery more traceable. The data would also be signed using a key that is only available at a central location and a copy of the signed info would be retained in the driver database (this information is already collected anyway). This would make it more difficult to change just the photo on the license, for example.
The main difference between a secure driver's license and a national ID is that there would be no new requirement to obtain or carry the card.  One can look at it as the nose in the camel's tent or as a way to deflect pressure for more Draconian solutions.
Arnold Reinhold

@_date: 2001-10-16 09:14:42
@_author: Arnold G. Reinhold 
@_subject: Scarfo "keylogger", PGP 
Reading between the lines, I think the FBI is taking the position that e-mail stored on your computer, either before or after you send it, is a business record and not an electronic communication. Thus they would also claim the right to key-log a mail client when it was off line under the authority of just a search warrant, without a wire tap order. In effect, they seem to be claiming that only instant messaging is protected under anti-wiretapping laws.
Press reports said PGP was used to encrypt gambling records. The defense challenged the keylogging on the grounds that it must have intercepted electronic communications as well, and therefore went beyond the authority of the FBI'ssearch warrant.
It also seems that the FBI used two separate tools on Scarfo's computer:
1. an only-when-the-modem's-off key logger
2. a tool to capture the passphrase when it was entered into the PGP dialog box.
One way to create the latter tool is to simply use the PGP source code to make a doctored version of PGP that saves the passphrase in a hidden file or even e-mails it and the secret key to a special address. This possibility suggests that it is a mistake to include the full PGP version number in plaintext, as is done in the present PGP message format. Doing so allows any attacker to prepare a doctored program that matches the target's version in advance, reducing the number of surreptitious entries needed. This may not matter much to the FBI (which apparently made five entries is this case) but could be significant to an attacker with fewer resources, e.g. a terrorist cell.
Transmitting the software version enclar may also help in creating a capture tool that knows where keying information is stored in memory. If there is a need to alert the receiving program as to the format of the encrypted message, a message format code should be used, not the software version number.
Arnold Reinhold
(who is not a lawyer)

@_date: 2001-10-23 08:58:39
@_author: Arnold G. Reinhold 
@_subject: RC4 [was: RE: Passport Passwords Stored in Plaintext] 
An important advantage of RC4 is that it is easy to reproduce from memory. If efforts to suppress cryptography ever intensify enough, it may be the only cipher that is widely available.
There was a news report on NPR this morning that the U.S. Nuclear Regulatory Commission  has taken down its Web site after a request by the Department of Defense to remove material that might be helpful to terrorists. The site now says:
"In support of our mission to protect public health and safety, the NRC is performing a review of all material  on our site. In the interim, only select content will be available. We appreciate your patience and understanding during these difficult times."
The same could happen at NIST some day.  Your tag line is particularly apt here.
Arnold Reinhold

@_date: 2001-10-26 11:54:51
@_author: Arnold G. Reinhold 
@_subject: DOD goes to Smart Card ID's 
"The U.S. defense department has ordered chip-based ID cards for 4.3 million military
  personnel over the next 18 months to tighten security on access to buildings, including the
  Pentagon (news - web sites), and to computer networks, including access to encrypted
  e-mail and online transactions."
``This is extremely important, not only to us, but to the whole smart card industry. It's the
  biggest Java-based smart card order yet,'' ActivCard Senior Vice President Tom Arthur told
  Reuters at annual chip card congress Cartes 2001.'

@_date: 2001-10-30 09:04:45
@_author: Arnold G. Reinhold 
@_subject: NYT article on steganography 
[More alarmist than I would expect from Ms. Kolata. Many sources quoted who claim to have seen lots of stego, but won't give details.

@_date: 2001-09-09 10:37:20
@_author: Arnold G. Reinhold 
@_subject: Sen. Hollings plans to introduce DMCA sequel: The SSSCA 
I think the key point is that it doesn't outlaw private ownership of home computers. It merely requires all home computers to include provisions that allow the State to control what it is used for and to trace any information it produces. In this regard it is exactly the same as the laws in the old Soviet Union that forbid private ownership of a typewriter unless it was registered with the local police department, with a typing sample provided.
At least the enemy has finally removed his mask.
Arnold Reinhold
"1984 wasn't a novel, it was just another high-tech product plan with an unrealistic ship date."

@_date: 2001-09-10 12:39:38
@_author: Arnold G. Reinhold 
@_subject: Rijndael in Assembler for x86? 
There are a number of implementations listed on the Rijndael home page   including a GPL'd 80186 version by Rafael R. Sevilla . It says
;; Note that the only 80186 instructions here are shr/shl instructions
;; with multibit counts, and these only appear in the key expansion
;; function.
so it might not be that hard to adapt to x86.
Arnold Reinhold

@_date: 2001-09-09 10:37:20
@_author: Arnold G. Reinhold 
@_subject: Sen. Hollings plans to introduce DMCA sequel: The SSSCA 
I think the key point is that it doesn't outlaw private ownership of home computers. It merely requires all home computers to include provisions that allow the State to control what it is used for and to trace any information it produces. In this regard it is exactly the same as the laws in the old Soviet Union that forbid private ownership of a typewriter unless it was registered with the local police department, with a typing sample provided.
At least the enemy has finally removed his mask.
Arnold Reinhold
"1984 wasn't a novel, it was just another high-tech product plan with an unrealistic ship date."

@_date: 2001-09-10 12:39:38
@_author: Arnold G. Reinhold 
@_subject: Rijndael in Assembler for x86? 
There are a number of implementations listed on the Rijndael home page   including a GPL'd 80186 version by Rafael R. Sevilla . It says
;; Note that the only 80186 instructions here are shr/shl instructions
;; with multibit counts, and these only appear in the key expansion
;; function.
so it might not be that hard to adapt to x86.
Arnold Reinhold

@_date: 2001-09-13 11:59:09
@_author: Arnold G. Reinhold 
@_subject: The tragedy in NYC 
I would go one step further: the U.S. Government's misguided effort to suppress crypto is a root cause of the massive vulnerability of the United States information infrastructure.  Manufacturers of commercial operating systems and application software have sharply limited the security features they include out of fear that their products will be subject to export controls.  If security isn't built into foundation products, it can't be bolted on later.
Some say the reason security is lacking is that no one wants to pay for it, but the software we use is bloated with features most people don't need or want.  Absent export controls I believe free markets would have produced good security solutions because companies need any competitive edge they can find.
In addition, many of the anti-crypto measures the government has suggested in the past, such as key escrow, only create new vulnerabilities. In time the security at escrow storage sites would have degenerated to the joke level we saw at our airports.
The Pandora's box of strong crypto was opened long ago. The bad guys already have it. The question is when will the good guys start using it for real?
Arnold Reinhold

@_date: 2001-09-14 09:08:09
@_author: Arnold G. Reinhold 
@_subject: Congress mulls crypto restrictions in response to attacks 
The big argument for a crypto ban is "the need for intelligence." Yet Jane Garvey, the head of the FAA, was quoted on the radio (WBUR) this morning as saying the FAA's security measures were not designed to stop someone who was willing to die in an attack.  If the steady stream of suicide bombings in the middle east in the past year was not considered a credible indication of a new threat, what would be needed? An intercepted status report on the plan from Bin Ladin himself?  When a stolen nuclear weapon is set off on a yacht in the East River, we'll no doubt be told they never thought about the boat The "intelligence" that is needed is the brain power kind, not the information kind.
Arnold Reinhold
Note to WTC copycat terrorists: don't try this on El Al.

@_date: 2001-09-21 18:22:17
@_author: Arnold G. Reinhold 
@_subject: New encryption technology closes WLAN security loopholes 
This sounds a lot like a proposal I made to improve 802.11 WEP security after the first round of attacks in February.   I've been working on updating the proposal in light of the Shamir, et al, paper. One difficulty is getting a good upper bound on the number of packets transmitted per second. None the less, it's clear that at least with the 128-bit versions of 802.11b, you can get reasonable security by frequent key changes. With 40-bit it's hard to avoid at least one byte being compromised, which would reduce the problem to attacking a 32-bit encryption every few seconds.  On the other hand, the original 40-bit WEP encryption could be brute forced with an office full of desktop PCs.
As I understand things, and please correct me if I am misinformed, IPSec is still quite complex to install and setup. Many 802.11b users are individuals or small offices. Until IPSec is user friendly enough for them, a solution that restores WEP to a reasonable level of privacy is worthwhile.
While we are on the topic, it seems to me that the other implication of 802.11 is that the Ethernet backbone in most offices can no longer be considered secure. It is too easy for someone to install a 802.11 base station without permission inside the corporate firewall. It may be that the only way to maintain corporate security is for every computer in an organization to use IPSec, with keys authorizing connection to the network transmitted out-of-band, (e.g. by hand).
Arnold Reinhold

@_date: 2002-12-04 01:38:40
@_author: Arnold G. Reinhold 
@_subject: DOS attack on WPA 802.11? 
Is this 802.11i or something that will be available sooner?

@_date: 2002-12-05 12:40:18
@_author: Arnold G. Reinhold 
@_subject: DOS attack on WPA 802.11? 
Cryptographic standards should be judged on their merits, not on the bureaucratic difficulties in changing them. Specs have been amended before. Even NSA was willing to revise its original secure hash standard. That's why we have SHA1.  If I am right and WPA needlessly introduces a significant denial of service vulnerability, then it should be fixed. If I am wrong, no change is needed of course.
Check out the President's message for September 202 at the Association of Old Crows web site ("Serving the Electronic Warfare and Information Operations Community"): Arnold Reinhold

@_date: 2002-02-05 09:37:45
@_author: Arnold G. Reinhold 
@_subject: Welome to the Internet, here's your private key 
I'd argue that the RSA and DSA situations can be made equivalent if the card has some persistent memory. Some high quality randomness is needed at RSA key generation.  For the DSA case, use 256 bits of randomness at initialization to seed a PRNG using AES, say. Output from the PRNG could be then used to provide the nonces for DSA.  For extra credit, PRNG seed could be xor'd periodically with whatever randomness is available on chip.
The resulting DSA system requires about the same randomness at initialization as RSA. The additional vulnerability introduced requires breaking AES to exploit, even if no further randomness is available.  All things considered, I'd trust an AES PRNG more than a smart card RNG whose long term quality I cannot assess. Better to use both, of course.
Arnold Reinhold

@_date: 2002-02-06 09:37:06
@_author: Arnold G. Reinhold 
@_subject: Welome to the Internet, here's your private key 
One criteria for a cryptographic system that is rarely mentioned is auditability. To the maximum extent possible users should be able to verify every component of the system that affects security. We have gotten too used to systems so bloated that they no one can know what's in them. There are historic reasons for this but that is no excuse. Finding out how to simplify systems is far more important today than designing the next great cipher.  A great virtue of doing all crypto on a smart card is that they can be verified, at least with some effort.
And creates a potential legal liability  for the smart card manufacturer. This gets to the original question of this thread. I wonder why the CA's lawyers let them generate private keys themselves. If it ever came out that private keys were misused by CA employees or even someone who penetrated their security, they would be legally defenseless, all the gobbledygook in their practice statements not withstanding. There is no good business reason for a CA to generate private keys and very powerful business reasons for them not to.
Arnold Reinhold

@_date: 2002-02-08 16:11:37
@_author: Arnold G. Reinhold 
@_subject: Welome to the Internet, here's your private key 
If the CA has nothing to do with key generation in the first place, I'm not sure how weak keys would affect the CA's reputation. "We had nothing to do with making that key, we just signed it" is a concept even the general public can understand. And the risk of weak keys seems small compared to the myriad ways a user's private key can be compromised.  If the CA has any access to private keys, any compromise can be blamed on the CA and diminish their reputation.
It's hard to see how to establish a secure protocol between the user's machine and the CA without a good source of randomness on the user's machine in the first place.  You can't presume there's a shared secret.
Simply providing an applet or plug-in to generate keys would seem sufficient.  The CA could maintain a list of approved smart cards based on inspecting their source code.  They might even let approved smart card vendors embed a signing key in the smart card to let the CA know that the user key had been generated by an approved device. Such a system could be defeated but it's not clear why anyone would have the motivation to do so. If someone wants to create a compromised key incident, they merely have to leak a key.
Arnold Reinhold

@_date: 2002-02-22 13:34:04
@_author: Arnold G. Reinhold 
@_subject: Report on a James Bamford Talk at Berkeley 
Nonsense! The NSA's existence and purpose hasn't been much of a secret since the early 60's, long before The Puzzle Palace was published in 1982.  Kahn has a chapter on NSA in The Codebreakers, which came out in 1967, and that chapter wasn't much of a revelation even back then. No wonder Bradley was not pleased. He'd been The helical scan system, later commercialized as the video tape recorder, was invented for NSA so they could record whole swaths of the radio spectrum for later analysis. Instead of monitoring each station, they could go back and replay the tapes once they knew what to look for. I believe they attempted to record the entire HF spectrum continuously from multiple locations. 1.6 million tapes sounds low if anything.
Other signals as well, such as missile telemetry, satellite control, unintended emissions and all types of radar. (I believe one of the arms control treaties prohibits the U.S. and Russian from encrypting missile test telemetry.)
I think Bamford got it close to right on this one.  Aviation Week has reported on NSA geosynchronous satellite launches from time to time. See also  and There are several problems with low earth orbit satellites as listening systems. First, they only able to monitor any given spot for a short time. Target countries can shut off systems of interest while the satellite is overhead.  The second is that low earth orbit satellites can be attacked more easily and quickly in time of war. The U.S. at one time had an air launched missile that could do this. Geosynchronous orbit takes more energy and a longer time to get to.
Finally, a directional antenna on a low earth orbit satellite has to be steered very rapidly as the satellite moves over its target. That is very hard to do mechanically, and electronically steered antennae have narrow bandwidths, not what NSA wants for monitoring.  A geosynchronous monitoring satellite can have a huge, light weight parabolic mesh pointed at Earth. It only needs to steer very slowly, if at all. Remember that while signal strength drops as the square of the distance, a parabolic antenna's gain grows as the square of its diameter.  Geosynchronous orbit is about 50 times higher than typical low earth orbits used by NSA, so a 50 times wider antenna gets you to beak-even on signal strength.
NSA also uses satellites in 12-hour semi-synchronous elliptical orbits for the same reason that the Soviets put their "Molniya" communications satellites in similar orbits: the northern portions of Russia are not visible from synchronous orbit.
Umm, what about Venona? The NSA has lots of info on that break on its home page.   In that case, the Soviets were sloppy in manufacturing onetime pads. Soviets undoubtedly used mechanical crypto systems in the 50's and maybe 60's whose key length is short by modern standards.  Some of these must have been broken by now. The lack of any reports about such breaks suggests that the NSA is able to keep such info secret.
This raises an interesting question. How far back does NSA go in recovering communications that are only of historic interest? Would they release ciphertext of 1950's messages so amateurs could try?
There is no way anyone is going to intercept DC phone calls from a station in Cuba. On the other hand, a PC with a UHF receiver card can do a great job of monitoring cell phones from the Russian embassy in DC. NSA was very upset when the State Department let the USSR build its new embassy on a hilltop. Thanks to the none-to-poor encryption used by cell phones, any foreign government can afford to monitor these calls from their embassies, consular offices or even staff About 12 hours.
The message (sent in 12 parts) instructed the Japanese Ambassador to deliver a note breaking off negotiations at exactly 1 pm  Washington time. That was just after dawn in Hawaii. The Emperor wanted a legally colorable declaration of war to be delivered just before the attack. The Navy got the significance of the timing and specifically wanted to warn Pearl Harbor. An earlier message (Nov. 27, 1941) warned all bases that war could break out at any time.
The Navy wouldn't use an Army circuit that was up.  The message arrived at Western Union Honolulu in time but was sent to the base by bicycle. A teletype link between Pearl and the Honolulu WU office was being installed but wasn't up yet. Note that even an hour or two's warning would have been enough to get planes and anti-aircraft batteries on the ready to put up enough resistance to reduce the attacks effectiveness.  Here's a photo showing the clear field the Japanese enjoyed that morning: The parallels between September 11, 2001 and December 7, 1945 are striking:
    o  Both attacks were extremely well planned to preserve surprise.
    o  We had general knowledge that an attack was likely but no specific details.
    o  There were bits and pieces of intelligence that were overlooked.
    o  Low tech protection measures were available that could have reduced the damage, but were not deployed (torpedo nets in 1941, strong cockpit doors in 2001).
    o  There was no command follow up to the general warning to verify that preparations were being made.
    o  In both attacks our enemy maintained radio silence. (Admiral Yamamoto's orders were delivered by courier to the fleet in Edo harbor. Bin Laden team leader in the U.S.  apparently traveled to Europe for briefings).
    o  The attacks were far more audacious than anything we expected.
    o  Inter-organizational barriers and communication failures diminished our effectiveness.
After both attacks, attempts were made to put the blame on intelligence. The failure of military leaders to act on the general warnings that intelligence did provide was down played. Admiral Kimmel, in charge of Pearl harbor,  explained the presence of nearly the entire fleet in port by claiming he wanted the troops to have as much rest as possible prior to war. We have yet to hear the excuses for 9/11, but there are lots of questions:
    o  What preparations had the Air Force made for dealing with suicide aircraft hijackers? (We had been attacked by car bombs, truck bombs and a boat bomb.  A small plane had already crashed into the White House during the Clinton years. How hard was this to foresee?
    oo  The FAA was initially uncertain as to whether the first flight was diverting because it was hijacked or had merely suffered a radio failure. Why didn't the FAA have better procedures for dealing with loss of radio communication? All it takes is publishing a phone number for pilots to call on their cell phones. Do they have one now?
    oo  Why didn't the FAA contact the Air Force immediately when the first flight diverted from its flight plan? We were on alert for a possible major terrorist action and having a fighter jet along side could be helpful to a pilot in an ordinary emergency.
    oo  Why wasn't the first flight of F-15's, scrambled to NYC from Otis AFB in Massachusetts, ordered to fly supersonic? They knew at that point an attack was in progress. There was enough time to intercept the second jet to hit the WTC if they had.
    oo  Why didn't the Air Force scramble fighters to cover Washington after the second plane hit the World Trade Center (or sooner). There would have been enough time stop the Pentagon attack at least if they had. Instead they waited until they a report came in that a third jet was missing. By then it was too late.
A big issue in WW II cryptanalysis was haw much the fruits of the intercepts could be used in battle, with the attendant risk of the enemy realizing their codes were broken. According to Kahn, the then rule was that results could be used to win a major battle. One exception was the assassination of Adm. Yamamoto. His skills were so respected that his death was considered as important as winning a major battle. The same considerations apply here. The U.S. military thought they could use the signals intelligence to deliver a devastating blow against Bin Laden. They didn't.
There are many steps in using signal intelligence to thwart a surprise attack:
1. The attacker must send messages that reveal his intentions is some way.
2. Those communications must be intercepted and deemed important enough for further processing.
3. The message must be decrypted and translated, if necessary.
4. The message must be interpreted correctly in the context of other 6. A effective action plan must be formulated and ordered. This includes weighing the risk of not acting with the risk of revealing 7. The actions must be communicated to the field.
8. The actions must be executed in time.
Note that cryptography only plays a role in steps 3 and 2 (stego). Yamamoto and Bin Laden short circuited this process at step 1. Relying on signals intelligence, or any intelligence, as the sole defence against terrorism is folly. A layered approach is needed.
The failures at Pearl harbor were thoroughly investigated and the U.S. learned a lot. I hope the failures on and before September 11 will be looked at unblinkingly, and not covered over to spare the feelings and careers of those who missed opportunities.
Arnold Reinhold

@_date: 2002-02-26 09:12:01
@_author: Arnold G. Reinhold 
@_subject: Cringely Gives KnowNow Some Unbelievable Free Press... (fwd) 
Highly-unexpected?   All of public key cryptography is build on unproven mathematical assumptions. Why should this be the last breakthrough? If you plot the curve of what key length was considered long enough as a function of time, it doesn't look very good.
Perhaps it is time to stop claiming "secure forever" altogether until solid mathematical proofs of security are available.
I'm not completely comfortable with Elliptic-Curve systems. The mathematics is relatively young and has seen a lot of progress. Yet typical EC key length recommendations are based on the assumption that there is no way to calculate discrete logs in EC groups that is any faster than the general algorithm that applies to all finite groups. That sounds pretty aggressive to me.
If we are going to have to upgrade OpenPGP standards in light of the Bernstein paper, I would suggest a standard that combines RSA, EC and, if possible, a third PK system whose algorithm is based on an apparently independent problem.  The advantage of double or triple encryption is that a breakthrough in one problem area does not immediately compromise all your previously encrypted data. And you can upgrade the component key in question and distribute it signed with the old key, without have to start from scratch in establishing trust. Most personal computers are capable of this level of security. Why settle for less?
Arnold Reinhold

@_date: 2002-02-27 21:22:47
@_author: Arnold G. Reinhold 
@_subject: theory: unconditional security 
I don't think that's quite fair. Pretty much any organization that wishes to protect sensitive information needs to be able to segregate it from other data that is not protected or enjoys a lower level of protection. Most PGP users only encrypt critical data.  And given the best security system imaginable, there will be COMSEC failures due to human error (q.v. the John Deutch case).
Generating enough random information to fill a CD should take a few hours using a sound card and a hardware noise generator, running FIPS-140 tests along the way and whitening so as to assume only 3 or 4 bits of entropy per digitization. That CD would be enough to protect all the text e-mail I'lI ever want to exchange with another person.  An unconditionally secure text link between two people could be considered useful.
I suspect a video input device connected to a TV set tuned to an empty channel would be copious enough for most uses, but some real world testing should be done. Unconditional security is sounding a lot better in the post-Bernstein era.
Arnold Reinhold

@_date: 2002-01-03 11:26:46
@_author: Arnold G. Reinhold 
@_subject: PAIIN crypto taxonomy (was Re: CFP: PKI research workshop) 
The PAIIN model (privacy, authentication, identification, integrity, non-repudiation) is inadequate to represent the uses of cryptography. Besides the distinction between privacy and confidentiality, I'd like to point out some additional uses of cryptography which either don't fit at all or are poorly represented in this model:
    Anonymity - the ability to communicate without messages being attributed to the sender (e.g. remailers).
    Confidential verification -- the ability to verify information without disclosing it (e.g. zero knowledge proofs).
    Fragmentation -- dividing control over information among several parties.
    Invisibility -- the ability to communicate or store information without being detected. This includes stegonography, low probability of observation communication techniques such as low power spread spectrum, and measures against traffic analysis such as link     Proof of trespass -- The ability to demonstrate that anyone having access to data knew they were doing so without authorization, (e.g. for trade secret and criminal evidence law).
    Remote randomization -- the ability for separated parties to create fair and trusted random quantities.
    Resource taxing -- techniques to prove a minimum expenditure of computing resources  e.g. hash-cash.
    Time delay -- making information available but not immediately.
    Transmission assurance -- anti-jam and anti censorship technology.
    Use control -- the whole digital rights management scene.
I'm not suggesting this is a complete list or the best breakdown, but I hope is shows that the cryptographic imagination goes beyond PAIIN.
Arnold Reinhold

@_date: 2002-01-14 17:58:56
@_author: Arnold G. Reinhold 
@_subject: CFP: PKI research workshop 
The point is that the risks are not the same. A CA can lower the cost of insurance it sells by taking additional precautions to reduce risk.  The CA is also in a better position to estimate the true premium. A third party has to charge a very high premium since it is in a poorer position to make an accurate assessment of the risk.
There would be a way for third parties to reduce their risk if some simple mechanism existed for independent verification of certificates. I once proposed that all PGP users display a small card containing their key fingerprint in a window near their front door. The corporate equivalent would be for organizations to display a hash of a master signing key in their main and branch lobbies. Anyone could then verify this key if they wanted to. There might be a bounty for discovering any irregularity. A network of certificate insurers might develop who would go from office to office recording fingerprints  and then selling lists by subscription along with a guarantee of reimbursement for damages up to a certain amount if any of their data were incorrect.
Arnold Reinhold

@_date: 2002-01-15 17:52:01
@_author: Arnold G. Reinhold 
@_subject: Linux-style kernel PRNGs and the FIPS140-2 test 
This result would seem to raise questions about SHA1 and MD5 as much as about the quality of /dev/random and /dev/urandom.  Naively, it should be difficult to create input to these hash functions that cause their output to fail any statistical test.
Arnold Reinhold

@_date: 2002-01-16 11:49:35
@_author: Arnold G. Reinhold 
@_subject: Linux-style kernel PRNGs and the FIPS140-2 test 
Quite the opposite. The only thing you should be able to determine from the output of a good hash is whether two input strings are identical.  You pretty much acknowledge that in your first paragraph. You shouldn't be able to tell the difference between a random string and the sequence n || n+1 || n+2 || ... . Even a mediocre hash should make it impossible to  distinguish between a good random input string and a not-so-good one.  That is one of the criticisms of the Pentium RNG: the whitening hardware prevents one from analyzing the underlying randomness of the generator hardware.  Any statistical irregularities in the output of a hash like SHA1 or MD5 are far more like to be an artifact of the hash algorithm rather than some regularity in the input.
Arnold Reinhold

@_date: 2002-01-17 11:23:49
@_author: Arnold G. Reinhold 
@_subject: password-cracking by journalists... 
Outside of the good possibility that they might be quotations from Islamic religious texts, why would you think Arabic passwords are any easier to guess?
Another interesting question is whether the reporters and the Wall Street Journal have violated the DCMA's criminal provisions. The al Qaeda data was copyrighted (assuming Afghanistan signed one of the copyright conventions--they may not have), the encryption is arguably a "technological protection measure" and the breaking was done for financial gain.
"17 USC 1204 (a) In General. - Any person who violates section 1201 or 1202 willfully and for purposes of commercial advantage or private financial gain -(1) shall be fined not more than $500,000 or imprisoned for not more than 5 years, or both, for the first BTW: The 2600 Magazine defense team has filed an appeal for en banc review of the 2nd Circuit's DMCA opinion:
Brief: Press Release: Arnold Reinhold

@_date: 2002-01-18 10:43:50
@_author: Arnold G. Reinhold 
@_subject: password-cracking by journalists... 
This law has LOTS of unintended consequences.  That is why many people find it so disturbing.  For example, as I read it, and I am *not* a lawyer, someone who offered file decryption services for hire to people who have a right to the data, e.g. the owner lost the password, or a disgruntled employee left with the password, or a parent wants to see what was stored on their child's hard drive, could still be charged with committing a felony.
As for the legal situation before the DMCA,  the Supreme Court issued a ruling last year in a case, Barniki v. Volper,  of a journalist who broadcast a tape he received of an illegally intercepted cell phone conversation between two labor organizers.  The court ruled that the broadcast was permissible.  So the stolen property argument you give might not hold. The change wrought by the DMCA is that it makes trafficking in the tools needed to get at encrypted data, regardless whether one has a right to (there is an exemption for law enforcement) unlawful.
Arnold Reinhold

@_date: 2002-01-20 09:27:13
@_author: Arnold G. Reinhold 
@_subject: password-cracking by journalists... 
You can presumably write your own programs to decrypt your own files. But if you provide that service to someone else you could run afoul of the law as I read it. The DMCA prohibits trafficking in technology that can be used to circumvent technological protection measures. There is no language requiring proof than anyone's copyright was violated.  Traffic for hire and it's a felony.
Now a prosecutor probably wouldn't pursue the case of a cryptographer who decoded messages on behalf of parents of some kid involved in drugs or sex abuse. But what if the cryptographer was told that and the data turned out to be someone else's? Or if the kid was e-mailing a counselor about abuse by his parents? Or the government really didn't like the cryptographer because of his political views?
There is also the argument that Congress only intended to cover tools for breaking content protections schemes like CSS and never intended to cover general cryptanalysis.   You might win with that argument in court (I think you should), but expect a 7 digit legal bill.  And if you lose, we'll put up a "Free Will" web site.
Correct. The Barniki opinion pointed out that the journalists were not responsible for the interception.  But journalists receive purloined data from whistle-blowers all the time. Suppose in the future it was one of those e-mail messages with a cryptographically enforced expiration date? A journalist who broke that system might be sued under DMCA.  That possibility might not frighten the WSJ, but what about smaller news organizations?
I've read the statute very carefully and I never found such language. (You can read my analysis at  It's certainly possible that I overlooked something. Perhaps you could cite the language you are referring to?
Arnold Reinhold

@_date: 2002-01-20 21:58:17
@_author: Arnold G. Reinhold 
@_subject: password-cracking by journalists... 
Arabic Unicode is based on ISO 8859/6 so this was presumably the case before Unicode as well.
I'm not sure why someone would only write the root. I don't think it's any more natural for speaker of those languages than writing Latin roots would be for English speakers.
A few more factors:
1. Neither Hebrew nor Arabic have capitalization the way Latin does. This reduces opportunities for variation. The Hebrew final forms make up for that to a small degree.  They are treated as different code points in all encodings*, by the way.
2. Almost all Hebrew encodings* include the Latin letters as well. In 7-bit ASCII Hebrew, the Hebrew alphabet replaces the lowercase Latin letters. In IBM-PC and ISO 8859/8  encodings, the Hebrew alphabet is in the upper 128 characters, with the lower 128 printable characters being standard ASCII. So a Hebrew user could mix Latin and Hebrew characters if they wished.  I suspect most Arabic computer users have easy access to Latin characters too.
3. Arabic and Hebrew users might be counseled to selectively use vowels or diacritical marks in their passwords.
4. People outside the U.S. are less likely to be mono-lingual. Someone from Israel for example might be expected to know several languages among Hebrew, Arabic, Aramaic, English, Russian, Yiddish and Ladino.
5. Unicode includes an extended Arabic-encoding with 96 additional letter/diacritic forms used in non-Arabic languages that use Arabic alphabet, including 9 for Pashto. I don't know if these are available in consumer PC's yet.
6. Finally users of these or other non-Latin alphabet languages might well choose to transliterate their password into Latin characters to make them easy to enter on any computer.
I think the analysis depends on the type of password system employed. In a properly designed system that places no restriction on password length and applies a cryptographic hash to the password input + ample salt, the existence of constant bits per character in some encodings has no effect. The entropy of the password is determined by the symbol space the user is employing, not the internal encoding.
Systems like these are probably best attacked by trying long lists of likely passwords, preferably guided by whatever personal information is known about the password creator.
If the password bit length is limited to a low number, e.g. the Unix 56-bit limit,  switching to 16-bit or 32-bit per character encoding would be disastrous. As far as I know, no one does this. I don't know if any implementations attempt to accept UTF-8 encoding. There are clearly some pitfalls there.
On the other hand, the Unix password system, particularly those where the hashed password can be obtained by an attacker, is so broken that any natural language password is going to be weak.  Random 8 character passwords from a 26 letter alphabet, will only have 38 bits of entropy.  A dictionary attack is quite feasible at that size. A random password with 6 letters, one digit and one special character (typical of what users are counseled to choose) has 42 bits.  A random password using the full 96 printable ASCII character set only gets you to 53 bits of entropy. Stamping out the 8 character Unix password limit would be a good use of Homeland Defense money.
Arnold Reinhold
*At least all those listed in Narshon and Rosenschein, "The Many Faces of Hebrew," Kivun Ltd. (a developer of multilingual software), Jerusalem, 1989.

@_date: 2002-01-21 10:30:02
@_author: Arnold G. Reinhold 
@_subject: password-cracking by journalists... 
Circumvention is defined in 17 USC 1201 (a) (3):
"As used in this subsection - (A) to ''circumvent a technological measure'' means to descramble a scrambled work, to decrypt an encrypted work, or otherwise to avoid, bypass, remove, deactivate, or impair a technological measure, without the authority of the copyright owner; ...
I'd read that as implying that the law is talking about a copyrighted work; otherwise if someone encrypts text in the public domain, no one would be allowed to decrypt it. But an aggressive prosecutor might adopt your interpretation. It's a very poorly written law with great potential for abuse.
Arnold Reinhold
Who is not a lawyer and is not offering legal advice

@_date: 2002-01-22 10:27:21
@_author: Arnold G. Reinhold 
@_subject: password-cracking by journalists... (long, sorry) 
No, DMCA bans trafficking in devices whose primary purpose is *circumvention.*   I'm not trying to nit pick, it's an important point. DMCA creates a whole new class of proscribed activity, circumvention, that does not require proof of infringement.
As for the phrase "primary purpose," I can easily see a judge accepting the argument that the primary purpose of a tool that breaks encryption is circumvention as defined in this act. In the 2600 case, the defense argued that DeCSS was also useful for playing purchased DVDs on Linux machines and for fair use. The courts dismissed this Right, but just about everything written today is copyrighted from the moment of creation. You have to go out of your way (or work for the U.S. government) to place new works in the public domain.
I've heard that story as well. I don't know if he saw the final language, how long he had to study it or what he based that opinion on.  Maybe there is some statement in the legislative history, which is only what the legislators said about the bill, that might be helpful in court. Absent that, we have to rely on what the law actually says. Bruce's opinion of what the law means would carry no weight in court.
I see nothing in the law that exempts you from liability if you didn't know you acted without authorization of the copyright holder. There is a provision, 1203(c)(5), that lets a court reduce reducing civil damages if you didn't know.  That presumably does not apply to the criminal provisions and prosecutors are notorious for doing whatever it takes if they want to get someone.  See, for example Again, there is this new offence called circumvention.  You don't need to prove infringement or trade secrets.  There are statutory damages (1203(c)(3)(A)), $200 to $2500 per act of circumvention "as the court considers just,"  plus you can be assessed the legal expenses of the other side. But the real kicker is that circumvention for hire is a felony.
Ulterior motives or no, it's not in the law. Judge Kaplan and the Court of Appeals for the 2nd Circuit flatly rejected fair use arguments in the 2600 case.  The 2nd Circuit wrote "Fair use has never been held to be a guarantee of access to copyrighted material in order to copy by the fair user's preferred technique or in the format of the original."
Again just about any work produced today is copyrighted and therefore protected under this title.
I agree that you can get at your own work.  I said you might be over the line if you help someone else get at their stuff, especially if you get paid for it.  In drafting this reply, I found a footnote (14) in the Second Circuit's 2600 opinion that suggests such assistance *is* permissible:
"When read together with the anti-trafficking provisions, subsection 1201(a)(3)(A) frees an individual to traffic in encryption technology designed or marketed to circumvent an encryption measure if the owner of the material protected by the encryption measure authorizes that I am not a lawyer, but I think this might be considered "dicta," statements in a court opinion that are not necessary to the decision, and lack binding precedential value. There is also the question of what "owner" means.  Still, it is encouraging.
As I pointed out above, other uses arguments have not gotten anywhere in court to date with respect to DMCA.
DMCA is much more broadly written than 18 USC 1030, which deals with breaking into others' machines.
I would say less. See my comments below and my amicus brief  which the Second Circuit ignored.
This applies to research, not other uses of cryptoanalytic technology.
Save that receipt.
The judge is looking over your shoulder.
You have to ask permission and expose your self to possible legal action. Has anyone here actually tried to get permission from a copyright owner to attempt to break encryption?
The phrase "factors to be considered" means each situation requires a separate, time consuming and expensive determination by a court.
If you publish too many details, you may lose your research exemption.
I trust everyone's credentials are in order.
You have to give them another opportunity to sue before you publish.
Not that there is nothing in the above two paragraphs that permits one to *publish* the results of the research.
Felton was threatened for attempting to publish his work, not for doing the research. Again, there is no language in the law that authorizes publication. I don't know what the RIAA was thinking, but they were on shaky First Amendment grounds and probably did not want to lose an early test of the law. If the law is upheld elsewhere, they may get bolder.
I agree with you that the law ought not to apply to ordinary cryptographic activity and that it should be unconstitutional if it does. But the law can be read the other way and it has survived unscathed so far. Add to that the post Sept 11 attitude of accepting greater restrictions on personal liberty and the likelihood of further incidents of alleged crypto use by terrorists, drug dealers and pornographers, and I think there is a real danger that it may be used against cryptographers.
The Second Circuit's 2600 ruling is particularly troublesome in this regard since it allows software to be proscribed based on the functional effect it can have on computer systems, not withstanding the fact that it is speech. If that ruling is upheld, we might see the enemies of open cryptography become more aggressive.
I'm not suggesting that anyone panic or stop their research and publication. But people should be aware of the risk, get competent legal advice and at least take care to document in writing situations where they believe they are breaking encryption systems with the owner's permission.
Arnold Reinhold

@_date: 2002-01-25 14:32:53
@_author: Arnold G. Reinhold 
@_subject: Diceware for picking Unix passwords 
Prodded by comments about password cracking in another thread, I've added a table to my Diceware FAQ  for selecting random characters out of the ninety five printable symbols in 7-bit Ascii. The intent is to provide a practical and secure way to choose passwords as strong as Unix allows.
Below is what I've added. It's best viewed in a monospace font like courier. Comments are welcome.
Arnold Reinhold
How do I use dice to create random character strings?
To create passwords of maximum strength for a given number of characters, you must use all available symbols. This is especially important for most Unix systems where passwords are limited to eight characters from the 7-bit ASCII printable character set. In particular, Unix "root" passwords should always be constructed in this way! The following set of three tables allows you to create such a password.
Roll a die three times (or roll three dice) for each character and then select one of the following three tables, based on what the first die says:
If first roll=1 or 2             3 or 4             5 or 6
            Second Roll        Second Roll        Second Roll
          1  2  3  4  5  6   1  2  3  4  5  6   1  2  3  4  5  6
T  1     A  B  C  D  E  F   a  b  c  d  e  f   !  @  #  $  %  ^
h  2     G  H  I  J  K  L   g  h  i  j  k  l   &  *  (  )  -  =
i  3     M  N  O  P  Q  R   m  n  o  p  q  r   +  [  ]  {  }  \
r  4     S  T  U  V  W  X   s  t  u  v  w  x   |  ~  ;  :  '  "
d  5     Y  Z  0  1  2  3   y  z  ~  _  sp     <  >  /  ?  .  ,
    6     4  5  6  7  8  9
Note: Roll all three dice again whenever a blank appears in the table. The table entry "sp" means a space character. If you do not want spaces in you password, roll all three dice again.
Repeat this procedure eight times to get a maximal strength Unix password. Each random character adds 6.55 bits of entropy. Eight characters provides 52.4 bits of entropy.
224 T
131 C
553 }
215 Y
465 ,
334 u
326 roll again
535 /
364 x
The password is then:
       TC}Y,u/x
Easy to remember? Hardly, but it is the only type of password that provides full security on Unix systems. Only such passwords should be used for root and administrative accounts or high security user accounts. If security is less of a concern for user accounts, then eight characters from the first table can be used.

@_date: 2002-01-27 12:07:21
@_author: Arnold G. Reinhold 
@_subject: A risk with using MD5 for software package fingerprinting 
The cryptographic hash function MD5 is often used to authenticate software packages, particularly in the Unix community. The MD5 hash of the entire package is calculated and its value is transmitted separately. A user who downloads the package computes the hash of the copy received and matches the value against the original.
Putting aside the question of how the the hash value can be safely transmitted separately, there is a potential attack on this method due to the 128 bit length of the MD5 hash output.
If all the individuals having input to the creation of the original software package are trustworthy, then 128 bits would appear to provide adequate security. A man-in-the-middle attacker would have to solve a 128 bit problem to create a Trojan horse infected package that passed the hash verification. That is considered computationally infeasible, at least until the advent of quantum cryptography.
One might think the above argument proves MD5 is sufficient, since if an attacker had an agent working inside the organization that produced the package, the agent could simply insert the Trojan software patch in the original package. However such an insertion is very risky. A sophisticated software company would likely have code reviews that would make introduction of the Trojan code difficult. In an open source model, anyone could detect the insertion. The insertion would then be foiled, the agent would be uncovered and the technical means that the Trojan employed would be compromised.
A safer attack would be for the agent to insert an apparently innocent modification to the package selected so that the MD5 hash of the package with the Trojan code matches the hash of the original package. Since the attacker controls the Trojan code, calculating the value of this modification is subject to the birthday paradox and presents presents a 64-bit problem. Solving such a problem is within the means of a well-funded attacker today.
The modification could be designed to get past code reviews in a number of ways. For example, 64 low order bits in a JPEG icon might be altered. The agent would have to be in a position to make the last modification to the software package prior to release and to send a final pre-release version of the package to the attacker, but those are hardly insurmountable hurdles.  In the open source model, where new releases can be frequent, it may suffice to carry out this attack only occasionally, say to recover private keys.
The obvious solution to this problem is to use a wider hash. For example, SHA-256 would present an group using this attack with a 128-bit problem. Even SHA1 would be preferable, making such an attack an 80  bit problem.  The cost of using a wider hash in this situation is trivial. It would seem the prudent thing to do.
Arnold Reinhold

@_date: 2002-01-28 20:23:29
@_author: Arnold G. Reinhold 
@_subject: Fingerprints (was: Re: biometrics) 
There is some interesting information at  They make the point that finger scanning differs from finger printing in that what is stored is a set of recognition parameters much smaller than a complete fingerprint image.  So there is no need for a lengthily process to acquire an initial image. Presumably this also makes finger scan data proprietary, since each vendor will use a different recognition algorithm.
Finger Scan also has a page on accuracy where they debunk other vendors' claims of 0.01% false reject/ 0.001% false accept, but tell you to e-mail them for the real numbers.
Arnold Reinhold

@_date: 2002-01-29 17:55:37
@_author: Arnold G. Reinhold 
@_subject: Cringely Gives KnowNow Some Unbelievable Free Press... (fwd) 
I think there are significant advantages to a passphrase-derived public key system. It allows total portability and the encryption hardware can be totally zeroized between uses.  One of the biggest threats to modern cryptosystems is their large electronic footprint that leaves too much room to hide things.
Passphrase-derived public keys also allow very long term storage of keys (e.g. on acid free paper in a vault) without worries about deterioration of media or inability to read old formats.
Method 2 is totally impossible in systems that use long salt (48 bits or more) or probably unique salt e.g an e-mail address or complete phone number.
Here are three very practical techniques to protect against Method 1:
The first is aggressive key stretching that burns up on the order of 1 second of processing time and utilizes silicon-consuming resources like memory and 32-bit multiplies.
The second is for the system itself to suggest strong passphrases. Users could ignore the suggestion but nothing can protect a user who is not willing to follow recommended precautions. With good key stretching even a 5 word diceware passphrase (64-bit entropy) would provide strong protection.
The third would be to combine the password and salt with a secret stored in the encryption device. This makes the key dependent on the device, but requires the attacker to capture both the device and the Arnold Reinhold

@_date: 2002-07-21 17:50:25
@_author: Arnold G. Reinhold 
@_subject: It's Time to Abandon Insecure Languages 
Language wars have been with us since the earliest days of computing and we are obviously not going to resolve them here.  It seems to me though, that cryptographic tools could be use to make to improve the reliability and security of C++ by providing ways to manage risky I have in mind a modified development environment that detects dangerous programming instances like pointer arithmetic,  assignments in "if" statements, C (as opposed to C++) strings, char array declarations, maloc's etc.  Methods where such usage is necessary would be signed by the author and one or more reviewers, with the signature embedded inside a special comment statement.  The development environment would then check whether only approved usages are present and, if so, sign the executable file. Final versions of code would be built on trusted servers whose compilers could not be tampered with and whose private key is not accessible to the Implementing such an environment should not be difficult. No real language changes would be involved, beyond reserving a standardized comment prefix for signatures. Most programmers would only be able to employ safe objects and constructs.  The few instances where dangerous usages were really needed would be limited, visible and require authorization.
Arnold Reinhold

@_date: 2002-07-23 09:13:46
@_author: Arnold G. Reinhold 
@_subject: building a true RNG (was: Quantum Computing ...) 
At 8:21 PM -0400 7/22/02, John S. Denker replied:
You don't have to put yourself inside the cage, just the FM radio. several layers of aluminum foil should work. The radio can run on batteries. Getting the audio out without allowing FM signal in is a bit tricky. The bast answer is to use fiber optics to carry the audio, but a good low-pass filter should work. Instead of detuning the receiver, tune it to the strongest station in your area. You'll know the shielding is effective when the signal is no longer detectable.  Of course if an attacker gets a high power transmitter close to you, all bets are off, but simply listening to another receiver nearby tuned to the same station would make such an attack The same technique with a portable TV set and a video digitizer should be a good source of high bandwidth noise. In both cases you are just using the receivers as high gain amplifiers of the thermal noise at the antenna terminals.
Arnold Reinhold

@_date: 2002-07-29 17:37:46
@_author: Arnold G. Reinhold 
@_subject: building a true RNG 
It's been discussed here some time back as well. If you believe your crypto primitives are infeasible to break, a crypto-based PRNG with a long enough random seed should be indistinguishable from a true, perfect RNG. If you are only confident that your crypto primitives are expensive to break, then using a true RNG for keys and nonces, rather than deriving them all from one PRNG, adds security.
This suggest a continuum of solutions: Construct a crypto PRNG and periodically (once enough has accumulated) stir your entropy source into it's state in some safe way. If you extract entropy slower than you put it in you can expect the equivalent of of a true RNG. If you extract entropy faster than you put it in, the system degrades gracefully in the sense that someone who expends the effort to break the number generation scheme only gets to read messages since the last entropy update.
The reason for batching entropy input is to prevent someone who has broken your system once from discovering each small entropy input by exhaustive search.  (There was a nice paper pointing this out in. If someone has the reference...)
Arnold Reinhold

@_date: 2002-06-05 21:15:13
@_author: Arnold G. Reinhold 
@_subject: Secnet11 an 802.11b clone with Type 1 crypto 
Harris Corporation is about to release its wireless LAN PCcard card based on IEEE-802.11b (WiFi) technology, but using NSA approved Type 1 (ok for classified data) encryption. Harris has a web site for the product at  with a spec sheet and a briefing. Some highlights:
o The card sticks out of the computer with two antennae poking up.
o It uses an NSA encryption algorithm called BATON (from various stuff on the Web, I get the impression that BATTON is a 64-bit block cipher with 128-bit keys that is designed for very fast operation)
o the message address is encrypted to prevent traffic analysis (this is a big selling point vs VPN technology, tho I suspect an adversary could learn quite a bit about a wireless net using a sniffer and one or more directional antennae to count how many packets come from each o Each packet has an 80-bit IV (it's rare to learn even that much about a Type 1 encryption system)
o Cards cost $2770 each. That's 30X commercial WiFi card, but cheaper than traditional NSA encryption data products which seem to run around $5K per node.
o "Red keys" are loaded via a special cable that connects to a data transfer device such as the CYZ-10.
o Harris is taking orders for August delivery.
It would seem that a commercial product using AES instead of BATON (and at a more reasonable price) would be of some value.
Arnold Reinhold

@_date: 2002-06-05 21:15:41
@_author: Arnold G. Reinhold 
@_subject: Commercial quantum crypto product - news article 
I think that is a very good question. All quantum crypto claims to do is insure that someone who accesses the fiber optic cable between the end points can't recover your secret data. You still have to verify that the quantum transceivers are doing quantum crypto properly and don't contain any malware that records and leaks keys.
According to the article, the Swiss vendor claims to be able to send 1000 bits of quantum secured info per second over short distances. By contrast, a pair of ordinary CD-Rs filled with random bits can supply 1000 unique bits/sec for over 20 years. The problem of getting the duplicate disk to the other end point without being compromised is no harder than the problem of getting the quantum transceivers to the endpoints without being tampered with.
The quantum approach does offer forward security (assuming the hardware can be trusted). This can also be done with CD-Rs by shipping a box of them and destroying them as used. A one year supply of weekly CDs (or 5 year supply of monthlies) fits in a shoe box. Another approach is to xor the CD-R derived key with a nonce exchanged using public key techniques, e.g. D-H. Then you are only depending on PKC technology to provide forward security for a week or month. One other advantage of the CD-R approach is that it is immune to an obvious denial of service attack against the quantum method: cutting the fiber optic cable.
Not only do random CD-Rs cost far less than laying a fiber optic line, the process of creating them can be understood and implemented by the organization that wishes secrecy using off the shelf hardware, without reliance on outside vendors. In cryptography, complexity only multiplies risk.
Arnold Reinhold

@_date: 2002-06-06 09:31:49
@_author: Arnold G. Reinhold 
@_subject: Commercial quantum crypto product - news article CORRECTED 
[Moderator: Please replace my earlier posting on this topic with this one. My concept was ok but my calculations were way off. Shouldn't hit send late at night! -- agr]
I think that is a very good question. All quantum crypto claims to do is insure that someone who accesses the fiber optic cable between the end points can't recover your secret data. You still have to verify that the quantum transceivers are doing quantum crypto properly and don't contain any malware that records and leaks keys.
According to the article, the Swiss vendor claims to be able to send 60 bits of quantum secured info per second over a 67 km path.  By contrast, an ordinary 100 GB hard disk filled with random bits can supply 60 unique bits/sec for over 50 years. The problem of getting duplicate disks to the end points without being compromised is no harder than the problem of getting the quantum transceivers to the endpoints without being tampered with.
The quantum approach does offer forward security (assuming the hardware can be trusted). But this can also be achieved by shipping a box of CD-Rs and destroying them as used. One CD-R will hold four months of data at 60 bits/sec. At the higher speed of 1000 bits/sec for short paths, mentioned in the article, one CD-R will hold a week's worth.  A one year supply of weekly CDs (or a 5 year supply of monthlies) fits in a shoe box.  If you combine the disk derived key with a nonce exchanged using public key techniques, then you are only depending on PKC technology to provide forward security for a week or You can ship random data disks periodically using a variety of means (courier, FedEx,  business travelers...) and combine them with earlier disks (xor or, better, addition) so the an attacker has to intercept all the disks to keep up.  Of course there is no limit on how far you can send the random data disks and the disk approach is immune to an obvious denial of service attack against the quantum method: cutting the fiber optic cable.
Not only do random data disks cost far less than laying a fiber optic line, the process of creating them can be understood and implemented by the organization that wishes secrecy, using off the shelf hardware, and without reliance on outside vendors. In cryptography, complexity only multiplies risk.
Arnold Reinhold

@_date: 2002-03-13 01:06:21
@_author: Arnold G. Reinhold 
@_subject: Optical Time-Domain Eavesdropping Risks of CRT Displays 
An historical note: In the early 1970's I did some contract programming work at the Air Force Cambridge Research Lab at Hanscom Field in Bedford, Mass.  Their main computer was a CDC 6600, a super computer in its day (60-bit words, 10 MHz clock). The wall separating the computer room from the hallway had a large picture window facing the console, with its two (vector-scanned) CRTs.  There were curtains on the window that were to be  drawn whenever classified programs were run. In addition, I was told, there was a large board that the operators were supposed to place over the console.  I don't know if these precautions were based on analyses similar to Kuhn's excellent paper or general paranoia, but there is a lot to be said for the later.
Arnold Reinhold

@_date: 2002-03-21 19:44:51
@_author: Arnold G. Reinhold 
@_subject: crypto question 
It's not clear to me what having the human present accomplishes. While the power was out, the node computer could have been tampered with, e.g. a key logger attached.
Who said you were allowed to lose power and stay secure? Laptops are pretty cheap and come with multi-hour batteries.  There should be enough physical security around the node to prevent someone from "tripping" power.
One approach might be to surround a remote node with enough sensors so that it can detect an unauthorized attempt to physically approach it. Web cams are pretty cheap. Several cameras and/or mirrors would be required to get 4Pi coverage.  Software could detect frame to frame changes that indicated an intrusion. The machine would be kept in a secure closet or cabinet. The the machine would be set up in what ever location by a trusted person or team and would remain "conscious" from then on. Entry would be authorized via an authenticated link. Any unauthorized entry would result in the node destroying it's secrets. It would then have to be replaced.
The approach I outlined offers very high availability.
Arnold Reinhold

@_date: 2002-03-22 14:21:01
@_author: Arnold G. Reinhold 
@_subject: crypto question 
There are groups with lots of money and dedicated, trained agents who are willing to die that would dearly like to steal a nuclear weapon. So far, they have not succeeded (if they do, I fear we will know about it quickly).  So someone has been able to do physical security The problem is doing it in a way that is affordable and doesn't require an army. Designing computers that can detect an attack seems worth exploring. FIPS-140 envisions such an approach when it talks about wrapping security modules in a mesh of insulated wire whose penetration tells the module to zeroize.
I'm not sure what changes in your argument if you delete the word "physical."  Perhaps we should all just give up with this security Arnold reinhold

@_date: 2002-03-27 15:51:49
@_author: Arnold G. Reinhold 
@_subject: crypto question 
or just security proportional to risk ...
While a valid engineering truism, I have a number of issues with that dictum:
1.  It is too often used as an excuse for inaction by people who are poorly equipped to judge either risk or cost.  We've all encountered the "experts on tap, not on top" attitude of many managements.  There was a good reason the U.S. centralized all crypto in the NSA after WW II. Managers in organizations like the State Department simply ignored known security compromises.  Communications security never had a high priority with functional managers, so it was taken away from them.
2. Costs are often overstated or quoted out of context. A $1000 coprocessor that can verify 100 keys per second ends up costing under a millicent per verification, even allowing a large factor for peak demand.  The added cost to store long keys is tiny. Good engineering (often the biggest cost) can be spread over many applications. Cost of keeping up with security patches is likely modest compared to 24/7 watchman security for a physical location.
3. The nature of risk is very different in cyberspace. Many cryptographic techniques introduce single points of failure.  Bonnie and Clide can't rob all the banks at once, but the wily hacker might. It may be cheaper to employ bullet-proof solutions than to really understand the risks in "good enough" approaches.
4. There is also the question of risk to whom. Many businesses seem to assume the the government will pick up the tab for a major cyber terrorism incident.  If business execs can say with a straight face that basic accounting principals are too difficult for them to grasp, imagine what they will say about a massive crypto failure. So in a sense taxpayers and  consumers are being asked to insure some of these risks.  I suspect they would gladly pay the added costs (pennies) to apply the best available technology.
5. There is a failure to distinguish between components and systems. It may be true that any real world system has holes, but that is no reason to give up on perfecting the tools used to build these systems. Incorporating known weaknesses into new designs is not justifiable, absent a compelling, fact-based, cost/security analysis.
Arnold Reinhold

@_date: 2002-03-31 12:46:18
@_author: Arnold G. Reinhold 
@_subject: ciphersaber-2 human memorable test vectors 
Human memorable test vectors are a great idea and very much in the spirit of Ciphersaber, which is to enable oral transmission of strong cryptography.  Test vectors are vital, particularly for a string cipher, because even an erroneous implementation will decrypt the ciphertext it produces.
The good news is that Ciphersaber/RC4 implementations are so sensitive to signed vs unsigned chars errors that they should be detected even by a test vector with all high order bits zeroed. The S-array will be totally screwed up. It's a very common problem.
A more subtle problem is detecting attempts to use C strings to store keys.  That will work fine unless there is a zero byte in the IV. I got tripped up by that indirectly because I was using Chipmunk Basic, which implements Basic strings as C strings. I only found out when I got complaints that one of the test vectors I published was bad.
I think screwing around with compression is more trouble than it's worth. Another approach might be to look for human memorable hex as the ciphertext or maybe just the IV, e.g. "00112233445566778899"  or "badbad00badbad00" But that would be for extra credit.  Adam's vectors accomplish most of what is needed.
Arnold Reinhold
Test vector trivia: anyone remember what "RYRYRYRYRY" was for and why?

@_date: 2002-11-06 20:25:47
@_author: Arnold G. Reinhold 
@_subject: New Protection for 802.11 
See the following two Intel links with detailed discussions of TKIP and Michael which i found via Google:

@_date: 2002-11-07 16:17:48
@_author: Arnold G. Reinhold 
@_subject: DOS attack on WPA 802.11? 
The new Wi-Fi Protected Access scheme (WPA), designed to replace the discredited WEP encryption for 802.11b wireless networks, is a  major and welcome improvement. However it seems to have a significant vulnerability to denial of service attacks. This vulnerability results from the proposed remedy for the self-admitted weakness of the Michael message integrity check (MIC) algorithm.
To be backward compatible with the millions of 802.11b units already in service,  any MIC algorithm must operate within a very small computing budget. The algorithm chosen, called Michael,  is spec'd as offering only 20 bits of effective security.
According to an article by Jesse Walker of Intel  :
"This level of protection is much too weak to afford much benefit by itself, so TKIP complements Michael with counter-measures. The design goal of the counter-measures is to throttle the utility of forgery attempts, limiting knowledge the attacker gains about the MIC key. If a TKIP implementation detects two failed forgeries in a second, the design assumes it is under active attack. In this case, the station deletes its keys, disassociates, waits a minute, and then reassociates. While this disrupts communications, it is necessary to thwart active attack. The countermeasures thus limits the expected number of undetected forgeries such an adversary might generate to about one per year per station."
Unfortunately the countermeasures cure may invite a different disease. It would appear easy to mount a denial of service attack by simply submitting two packets with bad MIC tags in quick succession. The access point then shuts down for a minute or more. When it comes back up, one repeats the attack.  All the attacker needs is a laptop or hand held computer with an 802.11b card and a little software. Physically locating the attacker is made much more difficult than for an ordinary RF jammer by the fact that only a couple of packets per minute need be transmitted. Also the equipment required has innocent uses, unlike a jammer, so prosecuting an apprehended suspect would be more difficult.
The ability to deny service might be very useful to miscreants in some circumstances. For example, an 802.11b network might be used to coordinate surveillance systems at some facility or event.  With 802.11b exploding in popularity, it is impossible to foresee all the mission critical uses it might be put to.
Here are a couple of suggestions to improve things, one easier, the other harder.
The easier approach is to make the WPA response to detected forgeries more configurable.  The amount of time WPA stays down after two forgeries might be a parameter, for example.  It should be possible to turn the countermeasures off completely. Some users might find the consequences of forgeries less than that of lost service. For a firm offering for-fee public access, a successful forgery attack might merely allow free riding by the attacker, while denied service could cost much more in lost revenue and reputation.
Another way to make WPA's response more configurable would be for the access point to send a standard message to a configurable IP address on the wire side when ever it detects an attack. This could alert security personal to scan the parking lot or switch the access point to be outside the corporate firewall. The message also might quote the forged packets, allowing them to be logged.  Knowing the time and content of forged packets could also be useful to automatic radio frequency direction finding equipment. As long as some basic hooks are in place, other responses to forgery attack could be developed without changing the standard.
The harder approach is to replace Michael with a suitable but stronger algorithm (Michelle?).  I am willing to assume that Michael's designer, Niels Ferguson, did a fine job within the constraints he faced. But absent a proof that what he created is absolutely optimal, improving on it seems a juicy cryptographic problem. How many bits of protection can you get on a tight budget? What if you relaxed the budget a little, so it ran on say 80% of installed access points? A public contest might be in order.
Clearly, WPA is needed now and can't wait for investigation and vetting of a new MIC. But if a significantly improved MIC were available in a year or so, it could be included as an addendum or as as part of the 802.11i specification.  Some might say that 802.11i's native security will be much better, so why bother? My answer is that 802.11i will not help much unless WPA compatibility is shut off.  And with so many millions of 802.11 cards in circulation that are not ".11i" ready, that won't happen in most places for a long time. On the other hand, an upgraded MIC could  be adopted by an organization that wished improved security with modest effort. Backward compatibility could be maintained, with a countermeasure that simply turned off access by Michael-based cards when a forgery was detected.
Arnold Reinhold

@_date: 2002-11-07 17:23:02
@_author: Arnold G. Reinhold 
@_subject: Windows 2000 declared secure 
A couple of comments:
I realize that this is a very preliminary draft. Please don't take this as criticism of your protection profile. It is a very useful start. I am not familiar with this stuff, so please accept these comments as coming from a naif.
I think these profiles should start with criteria (functionality, requirements, assumptions, etc.) that directly address non-technical users.  Ideally they should be quotable in a white paper describing the benefits of the target of evaluation or, even better, in the product warrantee.  More technical criteria added later in the document should be tied to these or explicitly justified in some other manner. The NSA CAPP does this to some extent in sections 3 and So, for example, requirements on IPL and power fail behavior might derive from a general specifications that the system not be subject to compromise during abnormal situations.  A list of such conditions would include IPL and power failures, along with hardware malfunction, periodic maintenance, terrorist attack and so forth. Conditions where security is not protected, say hardware maintenance, would then be made explicit.
I also think there is an opportunity to componentize protection profiles.  The designer of a new profile should not have to reinvent stuff like authentication or entropy generation. Profiles for these components would be included by reference, perhaps with parameters for components with options. This has the additional advantage of allowing the components to be updated independently.  (Any particular certification would specify the revision level  of all components used.) Here are some candidate components, not all of which involve software but are none the less important in secure systems:
o Entropy generation -- there is lots of art that can be captured here, e.g. batching entropy input, that might not be obvious to profile writers
o Login authentication -- again there are many approaches that should be captured, multi-sue password, PKI credentials,  multi-factor,  no lone access.
o Cryptographic algorithms with key and salt length recommendations-- The widely accepted algorithms and modes of operation might simply be listed, with provision for "Type 1" supplied by some government owner. Home grown algorithms should be banned. By the way your Quantum assumption is too narrow. None of the popular cryptographic algorithms have been mathematically proven secure. This risk should be explicitly stated.
o Secure networking--safe use of TCP/IP stacks, VPNs, services to be avoided, ...
o Multi-site systems --Security solutions employing several machines at individual locations, each backing up the other's data and cross checking proper operation.
o Event logging (e.g. what to log, being careful not to log passwords typed in the wrong box, sending logs to remote sites, dealing with lack of space)
o Configuration management -- validating that the software in use is the software that was certified; secure patch distribution and installation; verification that all patches are installed
o Forensic requirements -- what kind of evidence of misuse must be collected and how must it be handled to have legal standing in various jurisdictions.
o Data port protection -- preventing attacker from breaching security by gaining access to built in ports (RS232, USB, Firewire, SCSI, etc.) This could involve special drives or physical covers.
o Attack detection
o A common threat vocabulary--levels of attacker sophistication (nosy user, malicious insider, script kiddies, teams of hackers, well funded organizations, large national security services) and attack geography (intercepted packets enroute, attacks via publicly accessible ports, war dialing/driving,  inside job,  physical capture and  exploitation of the hardware.
o Detected attack response (this can vary of course. A secure system might zeroize all keys and seeds.  A long term  archive might publish keys before all data is lost.)
o Secure sensor modules -- GPS receivers, cameras (still and movie), intrusion detectors,  sound recorders, biometrics, etc that include a tamper resistant capability to sign the data they produce.
o Power availability assurance (loss of power is a denial of service as much as any flooding attack)
o Physical security, FIPS 140, for example. There may be useful stuff in the DOD Industrial Security Manual and insurance industry guidelines. There are potential tie-ins to software. A system might want to know whether an individual is still in the building before granting access through an inside terminal
o Training requirements and awareness maintenance for users, operators and administrators, including frequency and specific topics to be covered
o Legal forms -- security notices, employee agreements, acceptable use policies, etc.
I can envision this stuff evolving into something like the fire protection regulations that every architect has to either follow or request a waver.
Arnold Reinhold

@_date: 2002-11-11 12:03:31
@_author: Arnold G. Reinhold 
@_subject: DOS attack on WPA 802.11? 
I appreciate Niels Ferguson responding to my concerns in such detail. I don't want to give the impression that I object to WPA on the whole. That is why I said "major and welcome improvement" in my opening sentence. I am particularly mollified by Niels' statement that "most existing cards will be useable with 802.11i by putting a lot of the cryptographic processing onto the laptop."  If AES based solutions are available in a year or two that do not require selling all our old hardware on eBay, then WPA is indeed good news.
Still, I feel additional discussion is in order.  One of the tenets of cryptography is that new security systems deserve to be beaten on mercilessly without deference to their creator.  And I would argue that the Michael countermeasure is no ordinary design tradeoff. It is rather like a doctor prescribing a drug with severe side effects on the theory that it is the only way to save the patient's life, something that should be done only with the greatest caution:
o First, the doctor should be sure that the side effects aren't as bad as the disease.
There is a community of "wardrivers," people who look for 802.11b networks they can access. Even assuming most of them are ethical hacker types, who will good naturedly find something else to do when WPA starts to spread, there might be a few who are less sporting about it.  All they have to do is write some code that sends a couple of bad packets every minute or so to any network it finds.  This won't even be noticed by 802.11 nets that aren't using WPA, but those that are will be severely disrupted. Guess what will happen? The network administrators attacked will turn WPA off.  As word spreads, other net admins won't even bother turning it on.  They are overburdened anyway and installing WPA won't be a picnic.
Here is a story from today's Security Wire Digest:
I would argue that the Michael countermeasure DOS attack breaks WPA security as effectively as a cryptographic attack. It's simple, it's practical, it's specific to WPA, and could even be spread by virus. And if such an attack occurs, it will generate as much bad press as a cryptographic attack. How will the WiFi Alliance respond? Issue a press release pointing out that other DOS possibilities exist in ordinary 802.11? And how much credibility will be left when 802.11i is finally ready?
o Second, the doctor should be certain of the diagnosis.
Is the patient's life really in danger? In this case that means asking how easy it really is to break Michael. Normally, cryptographers should be extremely conservative in assessing the strength of an algorithm.  But when the response to perceived weakness is to add a different vulnerability,  I would argue that the test should be what is realistic, not the ultra conservative worst case.  The Intel article said the best known attack is a 29-bit differential cryptanalysis. How practical is that? Does it require vast amounts of chosen plain text?
If there is no practical Michael busting attack on the horizon, than the objection to allowing users to turn the countermeasure off, perhaps with a warning that doing so risks security, seems harder to o Third, the doctor should be certain that no other treatments are available.
The question of whether a significantly stronger MIC can be created within the limited computational budget available is still an interesting one. I hope more details about the algorithm and the constraints, both in time and space for object code, will be available very soon, if they are not already.  If something markedly better were developed in the next few months, perhaps the WiFi Alliance could be persuaded to drop it in before release.  At worst, work in this area could be a useful backup in case AES-based solutions prove too cumbersome to retrofit.  I have some preliminary ideas based on what I read in the Intel paper, but I will put them in a separate message.
o Then there is the notion (which is never supposed to cross a doctor's mind) that the patient's job isn't vital so why worry?
I take issue with is the proposition that users can be expected to avoid 802.11 for mission critical applications.  One of the main reasons for the explosive growth of this technology is that it enables non-technically trained people to build networks in a  simple plug-and-play way. These people expect stuff they buy to work and will use this systems in ways we never imagine.
And why shouldn't they? The marketing for WiFi is very aggressive. The WPA press release uses the word "robust" three times in two paragraphs. I could find nothing on the WiFi Alliance page  that cautions users against mission critical applications. Yes, there is that little FCC Part 15.19 notice on the box that says you are subject to interference, but every product comes festooned with warning labels these days.
The economics of WiFi mass adoption mean that other solutions will become too expensive, if any are available at all. Even if a system designer wants to avoid the risks of using 802.11, his boss may axe the extra cost. Then there is the question of the third world, where often no hard wired infrastructure exists. In many impoverished regions, wireless solutions are providing the first and only Internet connectivity. You can be sure mission critical applications will use o Some doctors might justify a risky drug because the patient has several other diseases that could be fatal.?
The argument that wireless solutions don't have to worry about DOS attacks because there are so many of them smacks of this. WiFi is a huge success and with that success comes a responsibility to keep improving the product and eliminate known risks.
Take the packet cancelling attack Niels described.  There may well be defenses that could be developed against packet cancelling. The higher level attacks he described could be dealt with by encapsulating over-the-air TCP/IP packets in encrypted envelopes, perhaps padded to standard lengths. Even the low level packet canceling technique itself might be defeated if the receiver cards can be persuaded to report all bad packets.  If we are using military-strength crypto, why not use military strength antijam? There is a lot of AJ technology developed for military use that could be employed. Indeed the spread spectrum underpinnings for 802.11 come from that world.  In my opinion, this attack ought to be on the agenda for 801.11i. And in any case, the packet cancelling attack is a lot more complex than the Michael countermeasure attack I posited.
The legal obstacles to pursuing DOS attackers also are a poor excuse. I am not a lawyer, but as I understand things, the problem arises in the U.S. because WiFi is authorized under FCC Part 15 rules, and those rules state that users of Part 15 devices have to accept interference from other users.  Still, if the interference is intentional, there may be bases for actions under a variety of federal laws.  For example, 47 USC 333 :
"No person shall willfully or maliciously interfere with or cause interference to any radio communications of any station licensed or authorized by or under this chapter or operated by the United States Government." (1 year in jail per 47 USC 501). If the network is used by a US Government site or someone doing defense work, 18 USC 1362 would kick in, with 10 year sentences.
Active attacks, such as the Michael countermeasure DOS attack or packet canceling, would seem to come under the anti-hacking law 18 USC 1030a5A:  "knowingly causes the transmission of a program, information, code, or command, and as a result of such conduct, intentionally causes damage without authorization, to a protected computer"  (5 years). The recent anti-terrorism law broadened the definition of "damage."
The law in other countries is probably less finicky.  And the U.S. Congress seems generally willing to expand the anti-hacking laws to cover new problems.  The notion that a large part of the national data communication infrastructure will enjoy no protection from malicious attack is simple untenable long term. What is going to happen when hospitals start buying computers with Bluetooth o I'm aware of the old adage "the best is the enemy of the good." WPA is good and reflects a lot of hard work but the Michael countermeasure makes me uncomfortable. I suspect there are ways to fix it, even in the short time available.
Arnold Reinhold

@_date: 2002-11-11 12:06:45
@_author: Arnold G. Reinhold 
@_subject: Possible fixes for 802.11 WPA message authentication 
Here are some thoughts that occur to me for improving the security of 802.11 WPA message authentication (MIC), based on what I read in Jesse Walker's paper One approach is to second guess Niels Ferguson and try to find a different combination of operations that will produce greater security than his Michael algorithm. That is a worthy research idea and might even be automated, since there are relatively few possibilities given the tight computation time budget.  My guess is that Niels has done a good job and, in any case, revisiting the Michael design not likely to produce anything that can be implemented before WPA is introduced. So this doesn't seem the most productive place to look right now.
A different approach might be to select an MIC algorithm that is much stronger but breaks the bank on computing time for older access points, yet still works on existing cards. One could then have two variants, WPA and WPA-XS (extra strength). Sites that wanted the best security would have to junk older access points. XS could also be required for 802.11a, the new, faster standard for the 5 GHz band, which will presumably require beefier access points anyway.
A third approach for the short term would be to leverage Michael, i.e. use Michael as is and add stuff that makes the WPA MIC harder to break. Then all the cryptoanalytical work done to date on Michael remains valid. Here are several approaches I have come up with. For this discussion call the Michael key produced under WPA as it exists K. I am not proposing any change in the way K is generated or 1. Shuffle the order of the message words stirred into Michael.  For example, divide the message payload into four blocks. Let L be the length of the payload in words (after padding). Compute M = L/4 (a shift).  Then the blocks are [0 to M-1]. [M to 2M-1], [2M to 3M-1] and [3M to L]. At the time a new K is created, compute a randomized permutation of 4 elements and four randomized "order-determining" bits, all derived securely from K.  Then for each packet, compute the Michael hash of the blocks in the order of the permutation, with the additional wrinkle that each block is hashed in either ascending order or descending order, based on the value of the corresponding bit. Note that each word is hashed exactly once and the added overhead is modest and outside the Michael inner loop.
A 4 element permutation has 4.5 bits of entropy, with the four order-determining bits, that adds at total 8.5 bits to Michael's strength. The same concept with 8 blocks would add 23 bits. The source and destination addresses are also hashed. They can simply be considered part of the payload, or they can be hashed separately, before any of the blocks or at the end, again determined by K, to add additional variability.
Since the MIC generated here is exactly the same as the original Michael MIC of the permuted message, there is no reduction in Michael security.  This method breaks down for very short packets, however computation time is presumably less of an issue for short packets, so we should be able to come up with something in these cases. Perhaps we could apply the permutation to data word bytes and use the order determining bits to specify a shift.
2. Refresh the Michael key frequently. This proposal rests on WPA's need to keep packet order in sync for the IV counter.  I propose generating a sequence of 64-bit sub-keys derived from K using a reasonably secure algorithm and using them instead of K to key Michael.  Since each sub-key gets very little exposure, breaking Michael become much more difficult.
2a. Here is one way to generate the sub-key sequence: Create an instance of RC4 in software and initialize it using K as the RC4 key. Then generate 8 cipher bytes each time a new sub-key is needed.  One could do this for every MIC that is generated. This would require eight RC4 cypherbyte generations per packet.   A 258-byte RC4 state {i, j, S} will be required for each active K and the RC4 key setup will need to be performed each time K is changed. For extra credit, one can discard the first 256 cipherbytes, though I think that is overkill here.
2b. If that is too much overhead, one could generate one cipherbyte for each packet and change keys every time eight had been accumulated.  Each Michael key only gets used eight times.  This computation and storage load does not seem like a lot to me, but If it is too much here is a yet another approach:
2c. This one makes me a bit nervous, but it is worth putting on the table.  A new RC4 key is generated for every packet fragment sent. Borrow one bit from each such key. The bit number used might be derived from K. Accumulate the bits in a series of 32 bit word, say 8 of them.  When you have accumulated them, use them to compute a new sub-key, either by adding them pair-wise, or, better, using Michael keyed by K or the previous sub-key.  This takes very little overhead and limits Michael to 256 uses on any one sub-key. What makes me nervous is that the per-packet keys are related and that this could introduce a new weakness into the packet encryption.
3.  Do MIC chaining.  Xor (or add) the MIC output block from the previous packet to K (or to the previous sub-key) to form the Michael sub-key for the current packet. This costs very little and makes it much more difficult to figure out K without breaking the WPA Obviously there may be reasons of which I am not aware why each idea above is unsuitable. They are presented for what they are worth and in the hope that they stimulate additional thinking.
Arnold Reinhold

@_date: 2002-11-12 23:54:25
@_author: Arnold G. Reinhold 
@_subject: DOS attack on WPA 802.11? 
I hope you won't mind another round then.
Tell me if I understand this attack correctly. Bob intercepts a packet he knows contains a certain message, even though it is WPA encrypted, say "Transfer one hundred dollars from Alice's account to Bob's account. Have a nice day."  (Maybe he know what time it was sent, or the length, whatever.) Because WPA uses a stream cipher, Bob can create a message that will decrypt with the same key to "Transfer one million dollars from Alice's account to Bob's account. Have a nice day."  This was one of the problems with WEP.
WPA is designed to prevent this kind of forgery by adding a 64-bit MIC. Even so, I could send lots of packets containing the million dollars message but with random stuff in the MIC field (or in the "Have a nice day" part that Bob knows nobody reads) and if I do this enough times I will accidently create a packet with a valid MIC.  If MIC were really strong, this would take about 2**64 tries, a big enough number not to worry about.  But because Michael is puny, you were able to find some clever tricks for picking the randomizing data so that only about 2**29 (aka half a billion) tries are needed. Furthermore, you are worried that there might be a way that requires only 2**20 (about a million) tries.  And because we are trying MIC codes at random, the MIC key in use at the moment doesn't matter. Eventually Bob gets lucky and the packet goes through.
The logic behind your countermeasure is that forgery attempts are very easy to detect and by shutting down for a minute after 2 forgery attempts within one second, Bob needs an average of half a million minutes to get his packet through, or about one year. And that's an acceptable risk.
If I got this right, here are a couple of observations. Assume for a moment WPA as is, but with your time out countermeasure turned off.
1. Bob only gets that one packet through.  If he wants another packet he has to start all over with another million or more attempts. So that packet had better be worth the effort.
2. This forgery only affects the 802.11 layer. If the "Transfer one million dollars" message has an electronic signature or another layer of protection, this attack does nothing to defeat that.
3. The network will get and detect hundreds of thousands of copies of the forged message before a valid one gets through. If Bob is tampering with the MIC code, they will all be identical. If Bob is munging an unimportant section of the message, they will still be highly correlated. So we will have hours, maybe days of warning that someone is attacking our system and exactly what Bob is trying to do. Even if we were asleep and he succeeds, we would know about the attack and what message he was trying to send.
4. Bob has to do a lot of transmitting and we will have hours or days of warning to track him down with direction finding equipment.
This is not a very attractive attack from Bob's point of view.  He must find a single packet so valuable it is worth all risk and time involved in mounting this attack. He telegraphs his scheme well in advance of its success. He risks being caught in the act and he leaves a trail of evidence that can be used to catch him, say when he cleans out that bank account. It sounds like a Woody Allen movie scenario. ("What does this note mean 'I have a bun'?" "It says 'gun'" "Hey Charlie does this look like a 'b' or a 'g' to you?")
Furthermore, if I got this right, a filter could be turned on that simply blocked the packet Bob is attempting to send when it finally gets a valid MIC. For extra credit, you could do the following: automatically detect forgery attempts and devise a filter for them (say, look for the constant region of the forgeries). When a valid packet comes through that matches the filter, reject it and force a key change.  The transport layer will request a retry. If, by chance, the packet was legit, the station that sent it can send it again and the Internet goes on. Bob on the other hand, needs another million tries, after which the same thing will happen.
Any security hole is a matter for concern, but if my understanding is correct, I am more convinced that a valid alternative to your time out countermeasure is for WPA to tell us we are under attack and let us log the forgery attempts verbatim, which I suggested in my first Regardless of whether my understanding of the differential attack is correct, I think the nub of our disagreement rests in three areas. First, you don't seem to think the Michael countermeasure DOS attack is worth worrying about. Second, you object to configuration options that would allow alternatives to the time out countermeasure, or stronger MICs for those who can use it. Third you seem to believe that there are few legal consequences to attacking 802.11, so forensic countermeasures have no value.
As to the first, you say:
There are three important differences between the Michael countermeasure DOS attack and the packet canceling attack you described earlier. First, the Michael attack is much easier to program, hence more likely to happen. Second, since it is new and specific to the touted WPA, it will be especially attractive to hackers, while at the same time more damaging to WPA's reputation.
Third, the countermeasure attack is inherently very hard to detect while I believe there are defenses against the packet cancelling attack that force the attacker to make lots of transmissions. As I mentioned, TCP/IP packets can be encapsulated in a layer above 802.11. Also two stations on the same wireless network that also had a wired link could collude to force the attacker into transmitting more.  These aren't great defenses, but they could be developed fairly quickly if packet cancelling attacks became a problem.
Absent the ability to alter the time out, there is no defense against the Michael countermeasure DOS attack nor any way to make it less stealthy. That makes it unique among the attacks I have heard about so far.
Then why not have two levels of strength, one what is now proposed and the second with a stronger MIC, perhaps Michael with more rounds as you suggest, and let the user choose?  And why not insist that 802.11a use the stronger mode? Because it is just coming out, 802.11a has no installed base and there is less crud on its 5 GHz band. It is also much faster so it will require more powerful processors anyway and any forgery attack will take much less time.
I sense a shift in argument here from "We had to retrofit existing systems and did the best we could," which I can buy for 802.11b but not in the 802.11a case, to "We don't care about DOS attacks, so we won't increase hardware cost a dime to defeat them."
As for configurability,
I think the lesson is that the majority of networks use the default settings.  Giving the site administrator an option, with suitable warnings, to choose to disable the Michael time out countermeasure and/or to log forged packet attempts does not make it likely that systems will be poorly configured. Admins without a reason to do so won't change the settings.  But it does give flexibility to deal with DOS attacks should they become prevalent and allow for third parties to develop other protections.
The two extremes in designing a software system are having a bunch of security options,initially turned off, that the user is supposed to select correctly and having no options at all on the assumption that all the tradeoffs were figured out correctly. In my opinion, both extremes are unwise.
I think a spec that says "Probability of undetected forged packet less than 10**-6. Forgery attempts are optionally logged. Mean time for successful forged packet with default-enabled time out is greater than one year."  would meet expectations.  And at least apply the mantra to 802.11a. Why launch that product with a weak MIC?
Finally, the legal stuff:
I don't think that logic will work in court.  An active DOS attack (not an RF jammer) involves sending carefully crafted and timed signals, e.g. false ACK packets. I believe that is well covered under this language.
Here is the new definition:
     "the term `loss' means any reasonable cost to any
      victim, including the cost of responding to an offense,
      conducting a damage assessment, and restoring the data,
      program, system, or information to its condition prior to the
      offense, and any revenue lost, cost incurred, or other
      consequential damages incurred because of interruption of
      service;
left to the lawyers.
agreed, but my concern is that misconceptions about the extent to which active attacks on an 802.11 can be prosecuted may be distorting the engineering tradeoffs. Obviously my opinion on what the laws means isn't worth much, but I do think these questions are important. The wardriving community was able to get a letter from the FBI indicating what they thought might be prosecutable. I don't see any reason why the WiFi Alliance cannot do likewise.  Just having it would reduce the hacker threat somewhat, especially if the notion that you can do anything you want to an 802.11 net is commonplace. And if the FBI won't agree that this sort of thing is illegal, then that is something the WiFi people can take to Congress and try to get There are also other countries in the world and I suspect most would be able to deal with active attacks on computer through their legal Arnold Reinhold

@_date: 2002-11-15 18:15:18
@_author: Arnold G Reinhold 
@_subject: DOS attack on WPA 802.11? 
I agree that we have covered most of the issues. One area whre you have
not responded is the use of WPa in 802.11a. I see no justification for
intoducing a crippled authentication there.
Also here is one more idea for possibly improving Michael.
Scramble the output of Michael in a way that depends on the MIC key, K.
This could be as simple as rotating each output word a number of bits
derived from K. Or you could generate a 8 by 8 permutation from K and
apply it to the bytes in the Michael output. you might even be able to use the
small cipher that is used to generate the individual packed encryption
keys in WPA.
This would break up an attack that depends on messing with the bits of the
MIC in the message. It does nothing for attacks on parts of the message
body. Any additional integrety check on the message would catch that,
On  the other hand it is very cheap and might interfere with future more
sophisticated attacks.
Arnold Reinhold

@_date: 2002-11-18 21:58:25
@_author: Arnold G Reinhold 
@_subject: DOS attack on WPA 802.11? 
[please ignore previous mesage, sent by mistake -- agr]
Modularization is a poor excuse for shipping a cryptographically weak
product. Second in this case the PHY layer does affect a MAC layer
feature. 802.11a is much faster than 11b. That makes Michael
even more vulnerable to attack.  If Michael is subject to one forged
packet per year on 11b, it is vulnerable to one every 10 weeks or so in
11a. Third, a stronger variant of WPA designed for 11a could also run on
11b hardware if  there is enough processing power, so modularization is
not broken.
As for shipped hardware, does anyone know that it couldnot run with a
stronger version of Michael? And a few shipped units, is far less
justification than the 10's of millions of 802.11b units out there.
A marginal improvement on a marginal algorithm can be worthwhile. It does
break up one attack mode at negligable cost. It might prevent other
attacks that have not been envisioned.
If the rotation constants are derived from the MIC key using a strong hash
(e.g. SHA1) there is little risk of recovering key bits. Since this only
needs to be done when the MIC key changes, the computation time should be
There is a risk that an attacker who is doing an exhaustive key search
could use knowledge of the rotation bits to rule out most trial keys with
just a hash computation. But even if they could completely test all MIC
key candidates with just the hash, that would require 2**63 SHA1 trials to
recover the MIC key on average. That is a reasonable level of security
compaired to WPA, and with 10 rotation bits we are very far from even that
Another cheap varient would be to derive the rotation constants from the
hash of the last two MIC keys. This eliminates even this minute risk.
 >
I wasn't suggesting they be done by 802.11, but by  higher layers.
With greetings form Las Vegas,
Arnold Reinhold

@_date: 2002-11-29 13:53:41
@_author: Arnold G. Reinhold 
@_subject: DOS attack on WPA 802.11? 
I'm not sure that is true for all existing 802.11b hardware. And vendors of new 802.11b hardware could certainly elect to support the stronger variant of WPA.
That is what I am suggesting. If a stronger version of Michael is too expensive to develop, there is still the option of using a standard message authentication function, say an HMAC based on MD5 or an AES solution. I spoke to several 802.11a/g chip-set vendors at Comdex and they seem to be allowing extra processing power to support 11i. Intersel said they were using 20% of available MIPS.
[regarding my suggestion to rotate the Michael output words in a key dependant way:]
I have responses to your concerns about using SHA and the issue of re-keying, but you point out:
That would be fine. You only need ten additional keying bits for arbitrary rotation of the two output words. Maybe an additional bit to optionally swap the words. This only adds a few instructions per

@_date: 2002-10-11 10:33:23
@_author: Arnold G. Reinhold 
@_subject: Microsoft marries RSA Security to Windows 
I can see a number of problems with using mobile phones as a second channel for authentication:
1. It begs the question of tamper resistant hardware. Unless the phone contains a tamper resistant serial number or key, it is relatively easy to clone. And cell phones are merging with PDAs. If you have secure storage, why not implement a local solution on the PDA side?
2. Even if the phone is tamperproof, SMS messages can be intercepted. I can imagine a man-in-the-middle attack where the attacker cuts the user off after getting the SMS message, before the user has a chance to enter their code.
3. Cell phones don't work everywhere. Geographic coverage is limited. Most U.S. phones don't work overseas. Reception can fail inside buildings and cell phone use is prohibited on commercial airplanes in-flight (the airlines are planning to offer Internet access in the near future). And what happens if I choose to TEMPEST shield my 4. The cell phone network can get clogged in times of high stress, e.g. a snow storm at rush hour, a natural disaster or a terrorist incident. Presumably some people who use two factor authentication have important work to do. Do you want them to be locked out of their computers at such critical times?
5. Cell phones are vulnerable to denial of service attacks. A simple RF jammer could prevent an individual or an entire building from accessing their computers.
6. People are generally cavalier about their cell phones. They wear them on belt pouches, leave them in cars and gym lockers, let strangers borrow them. I left mine in a coat pocket that I checked at a restaurant and ended up with a $40 long distance bill. Habits like that are hard to change. On the other hand, a token that goes on a key chain or is worn as jewelry taps into more security conscious cultural behavior.  Human factors are usually the weak link in security, so such considerations are important.
7. It's a tax on logins. SMS messages aren't free.
8. If I lose my token, I can use my cell phone to report it promptly. If I lose my cell phone...
9. Improved technology should make authentication tokens even more attractive. For one thing they can be made very small and waterproof. Connection modes like USB and Bluetooth can eliminate the need to type in a code, or allow the PIN to be entered directly into the token (my preference).
10. There is room for more innovative tokens. Imagine a finger ring that detects body heat and pulse and  knows if it has removed. It could then refuse to work, emit a distress code when next used or simply require an additional authentication step to be reactivated. Even implants are feasible.
Arnold Reinhold

@_date: 2002-10-15 09:54:45
@_author: Arnold G. Reinhold 
@_subject: Microsoft marries RSA Security to Windows 
If we're looking at high security applications, an analysis of a two-factor system has to assume that one factor is compromised (as you point out at the end of your response). I concede that there are large classes of low security applications where using a cell phone may be good enough, particularly where the user may not be cooperative. This includes situations where users have an economic incentive to share their login/password, e.g. subscriptions, and in privacy applications ("Our logs show you accessed Mr. Celebrity's medical records, yet he was never your patient." "Someone must have guessed my password." "How did they get your cell phone too?") Here the issue is preventing the user from cloning his account or denying its unauthorized use, not authentication.
What percentage are enabled for downloadable games? A security program would be simpler than most games.  It might be feasible to upload a new "game" periodically for added security.
You may be right here, though assuming SSL lets one solve a lot of security problems associated with traditional password login.
Security tokens work everywhere I can think of.  I'm not sure the cell companies are spending much to push into rural areas given the current economy.  Might be a new market for Iridium, but that doesn't work well inside buildings.
The WTC collapse took out a major portion of lower Manhattan's landline capacity. Cell phones were better than nothing, but many people experienced difficulty placing calls.  It is simply too expensive to design a switched system to handle all the calls people want to make in a major crisis. Military systems include priority tags to deal with this.
This does raise an interesting possibility: giving SMS messages priority over voice could be very useful in an emergency. SMS messages take much less bandwidth than voice and the entry mechanism on most cell phones is very slow. So existing cell infrastructure might be able to handle all the SMS traffic generated a crisis. Anyone know if cell phone companies are doing this?
Reduced privileges won't do in a crisis. That's when people need maximum privileges: to authorize emergency expenditures, reroute circuits, reboot servers or reprogram firewalls defeat cyber-attacks. The Internet is based on technology designed to survive nuclear war. Making its most valuable applications dependant for security on other, less robust communication systems seems a bad idea.
I'd like to see a personal token that is used for multiple applications. Particularly if it were on your key chain you'd notice its loss soon enough.  It would be handy if it told the time of day and also included an LED flashlight (on a separate battery).
Maybe not in YOUR cell phone, but I'll bet a lot of people store passwords in their cell phone address books. With the combined cellphone/PDAs of the future it will be common to do so. And for a high value attack, you have to assume the password is already The price of an SMS message is actually twice that -- both the sender and receiver pay. If I am authorizing access to a page of content for which I am only charging a buck or so, that dime is a big expense.
For someone who needs access privileges many times a day, $0.10 per login can add up to several hundred dollars a year.  The latest MacMall catalog lists a 16MB USB pen drive for $29.99. A security token should cost less than that, patents aside.
I also see a problem in agreement labor (union) situations. There might be demands that the company pay for the workers' cellphones if they are required to have one.
Tried to find a pay phone lately? But I admit this is a quibble.
That "pesky interface at the other end" seems less of an issue in the context of a deal between RSA and Microsoft. This is an opportunity to do the right thing and build it into the operating system once and for all.  Support for USB and Bluetooth tokens that reveal a public key and can sign a nonce should also be included in the OS. There is no need for a PKI here. Token binding would be done by the application owner when access is initially arranged.
If a personal (as opposed to special purpose, for bank-vice-presidents-only) tokens become more common, their possession won't mean that much.  In a cellphone-based security world, traffic analysis of SMS messages could reveal a lot about who does important work.
A second channel is very helpful in establishing trust, but for most applications, that trust can be stored in something the user carries. Often it is more important that the second channel be truly secure than that it operate contemporaneously with access.  I would hope that any Microsoft/RSA standard solution allows for both models.
Arnold Reinhold

@_date: 2002-10-20 22:38:35
@_author: Arnold G. Reinhold 
@_subject: palladium presentation - anyone going? 
I went. It was a good talk. The room was jam packed. Brian is very forthright and sincere. After he finished speaking, Richard Stallman gave an uninvited rebuttal speech,  saying Palladium was very dangerous and ought to be banned.  His concerns are legitimate, but the net effect, I think, was to make the Q&A session that followed less hostile.
Palladium sets up a separate trusted virtual computer inside the PC processor, with its own OS, called Nexus, and it own applications, called agents. The trusted computer communicates with a security co-processor on the mother board,  and has a secure channel to your keyboard and mouse and to a selected window on your CRT screen.
How to prevent the secure channel to the on-screen window from being spoofed is still an open problem. Brian suggested a secure mode LED that lights when that window has focus or having the secure window display a mother's-maden-name type code word that you only tell Nexus.  Of course this doesn't matter for DRM since *your* trusting the window is not the issue.
All disk and network I/O is done thru the untrusted Windows OS on the theory that the trusted machine will encrypt anything it wants to keep private. Windows even takes care of Nexus scheduling.
A major design goal is that all existing software must run without change. Users are not required to boot Palladium at all, and are to be able to boot it long after Windows has booted.
The specific question never came up. As Brain did say, Palladium is just a platform. People can built whatever they want on top of it. It seemed clear to me that the primary goal is DRM, but as someone else in the audience said (approximate quote) "We always hear that you can't do this or that without trusted hardware. Well, this is trusted hardware."  I don't see why anyone would think protecting software copyright could not be done.
No. The SCP is based on a smart card core and is to be a "light weight, low pin count chip" with a target cost of $1 in volume.  I presume future deals between MS and Intel are always possible.
The SCP will support several algorithms, including 2048-bit RSA, 128-bit AES, SHA1, an HMAC. They may include another cipher and another hash. There will also be a FIPS140-2 Random Number Generator and several monotonic counters, but no time of day clock. Each chip will have a unique RSA key pair, an AES key and a HMAC key. The only key that the SCP will reveal to the outside is the RSA public key and it will only do that once per power up cycle.
There is also a change to the PC memory management to support a trusted bit for memory segments. Programs not in trusted mode can't access trusted memory. Also there will be three additional x86 instructions (in microcode) to support secure boot of the trusted kernel and present a SHA1 hash of the kernel code in a read only register.  There may be a hole somewhere, but Microsoft is trying hard to get it right and Brian seemed quite competent.
Near as I can see, the real trust comes from the RSA key pair stored in the SCP and a cert on that key from the SCP manufacturer.  There is no command to obtain the private key from the SCP.  Presumably they leverage smart card technology plus what ever tricks they think of to make it hard to get that key.   Differential power analysis or HNO3 might do the trick. We'll have to wait and see.
The real question from Microsoft's stand point is will the entertainment industry be satisfied with Palladium's level of security and release content that can play on Palladium equipped PCs? DVDs aren't Hollywood's main problem.  Movies are becoming available online long before the DVD is released.  Hollywood probably wants something that monitors ALL content for watermarks. Palladium as presented doesn't do this.  But again it is a platform. Once it exists, a later version of Windows might require it to be up and would then verify all content displayed.  If Hollywood doesn't convince Microsoft to do this, Sen. Hollings will be more than glad to introduce the necessary legislation. To paraphrase Stallman's rant, in the Palladium context Alice and Bob are corporations and Mallory is the PC owner.
Arnold Reinhold

@_date: 2002-10-21 21:36:09
@_author: Arnold G. Reinhold 
@_subject: palladium presentation - anyone going? 
One of the services that Palladium offers, according to the talk announcement, is:
It seems to me such a service requires that Palladium be secure against the local user. I think that is the main goal of the product.
Brian mentioned that the system will not be secure against someone who can access the memory bus.  But I can see steps being taken in the future to make that mechanically difficult. The history of the Scanner laws is instructive. Originally one had the right to listen to any radio communication as long as you did not make use of the information  received. Then Congress banned the sale of scanners that can receive cell phone frequencies. Subsequently the laws were tightened to require scanners be designed so that their frequency range cannot be modified.  In practice this means the control chip must be potted in epoxy.  I can see similar steps being taken with Palladium PCs. Memory expansion could be dealt with by finding a way to give Palladium preferred access to the first block of physical memory that is soldered on the mother board.
Brian also mentioned that there would be changes to the Southbridge LCP bus, which I gather is a local I/O bus in PCs.  SCP will sit on that and presumably the changes are to insure that the SCP can only be accessed in secure mode.
There are two cases here. One is a buffer overflow in one of the trusted "agents" running in Palladium. Presumably an attack here will only be able to damage vaults associated with the product that contains that agent.  The vendor that supplies the agent will have a strong incentive to avoid overflow opportunities.
The more dangerous case is  buffer overflow in Nexus. Brian admitted that this would be disastrous.  Obviously QA will be intense. They plan to publish Nexus source code. Brian was even asked if they would publish source for their C compiler. He said they had thought of that, didn't think they could get the VisualC compiler published but are considering coming up with a stripped down C compiler they can They realize that the whole back up/upgrade issue is a big concern. Brian briefly presented some very complex schemes for doing this which I didn't grasp.
Presumably an intact Nexus can trash any trusted app.  And I don't see how any data in the vault could prevent you from loading a clean nexus, say from CD-ROM, as long as the SCP isn't altered and there is supposed to be no way to do that from software..
Arnold Reinhold

@_date: 2002-10-22 15:29:26
@_author: Arnold G. Reinhold 
@_subject: Palladium -- trivially weak in hw but "secure in software"?? 
I think the most important phrase above is "at this point." Palladium is still being designed.  I'd argue that the software/firmware portion is the trickiest to get right. It seems rational for Microsoft to let that design mature, then analyze the remaining hardware threats and turn the hardware engineers loose to try to plug Palladium has to be viewed in the larger context of a negotiation between Microsoft and Hollywood (I include here all the content owners: movie studios, recording industry, book publishers, etc. ). Hollywood would prefer a completely closed PC architecture, where consumers' use of the computer could be tightly monitored and controlled.  They perceive general purpose computing as we know and love it to be a mortal threat to their continued existence. Keeping the content of DVDs and future media locked up is not enough in their eyes. They want all material displayed to be checked for watermarks and blocked or degraded if the PC owner hasn't paid for the content.
Microsoft wants to preserve general purpose computing because it realizes that in a closed architecture, the OS would become a mere commodity component and the consumer electronics giants would eventually displace Microsoft. On the other hand, Microsoft needs Hollywood provide the kind of content that will drive PC sales and upgrades. The base line PC platform of today or even two years ago is powerful enough for most consumers and businesses. People are keeping their PCs longer and not upgrading them as often. Most everyone who wants a PC (at least in North America) already has one. Microsoft needs something new to drive sales.
I expect Microsoft and Hollywood to haggle over the final specs for Palladium PCs and no doubt additional hardware protection measures will be included.  The actual spec may well be kept secret, with NDA access only. Hollywood will hold two strong card at the table: its content and the threat of legislation.  I'm sure Senator Hollings is watching developments closely.
The big question in my mind is how to get PC consumers a place at the bargaining table. It seems to me that PC consumers have three tools: votes, wallets and technology. The Internet is well suited to political organizing. Remember the amount of mail generated by the modem tax hoax? Consumer boycotts are another powerful threat, given how powerful and upgradable existing computer already are. Technology can provide an alternative way to gain the benefits that will be touted for controlled computing.  Anti-virus and anti-DDS techniques come to mind. Also, since I expect an eventual push to ban non-Palladium computers from the Internet, alternative networking technology will be important.
The Palladium story is just beginning.
Arnold Reinhold

@_date: 2002-10-29 08:25:22
@_author: Arnold G. Reinhold 
@_subject: M-209 for sale on EBay 
I'd have to agree with Jim.  I have some WW II military radios in my basement and they look pretty pristine on the inside.  Military equipment is built for long shelf life.  Even stuff that's seen a lot of service often cleans up nicely. Also the (unmet) minimum bid for the M-209 on Ebay was $3000. It's hard to see how someone could make a small quantity of something that mechanically complex and make much money selling them for $3000 each.
Arnold Reinhold

@_date: 2002-09-24 22:47:52
@_author: Arnold G. Reinhold 
@_subject: unforgeable optical tokens? 
It might be possible to get the same effect using a conventional silicon chip. I have in mind a large analog circuit, something like a multi-stage neural network. Random defects would be induced, either in the crystal growing process or by exposing the wafer at one or more stages with a spray of pellets or chemicals. The effect would be to cut wires and alter component values such as resistances,  zener diode break down voltages, transistor gains.
Critical parts of the circuit would be protected by a passivation layer or would  simply designed with  larger geometries to make them less sensitive. Multiple inputs would be driven by D/A converters, either in parallel or through a charge coupled analog shift register. There would be enough "stuff' in the middle to make it impractical to characterize the entire circuit from the inputs. One could use very small geometries for the network and still get high circuit yield since defects are something we want.
The advantage of this approach over a optical system is that it would be very easy to interface with existing technology -- smart cards, RF ID, dongles, etc.
Arnold Reinhold

@_date: 2003-04-03 11:54:47
@_author: Arnold G. Reinhold 
@_subject: Russia Intercepts US Military Communications? 
The Army actually has a training course (from 1990) on-line that describes such a system in detail. The cipher system, called DRYAD is covered in

@_date: 2003-04-09 12:36:35
@_author: Arnold G. Reinhold 
@_subject: Via puts RNGs on new processors 
The FIPS-140 tests failed if they found excessive deviations from perfect randomness. That's overkill for detecting most hardware failures, say all output stuck on, and fails to address the real danger: someone substituting a PRNG seeded from a small set of values known to the attacker. A substitution could be effected through a trap door in the CPU micro code, the operating system or by a worm. Designed properly, such a PRNG would pass the FIPS-140 statistical tests with flying colors.
One nice feature of the VIA TRNG is that hardware whitening can be disabled. This facilitates testing for deviations from randomness that would be expected from the underlying design, particularly deviations that can be correlated with physical properties like chip temperature and supply voltage.  Output that appeared perfect would be suspect.
Of course any behavior can be faked with enough resources, so at best a CPU TRNG should be used as one more input to a randomness generator.
Arnold Reinhold

@_date: 2003-04-24 23:20:00
@_author: Arnold G. Reinhold 
@_subject: DRM technology and policy 
And what makes you think the DRM'd news media of the future will let you keep a plaintext personal archive?  Your subscription lets your view a story for a week. For a small additional charge you'll be able to download a one-week copy of the same story whenever you want.  But plaintext versions? Never! That would permit unfettered copying, you Arnold Reinhold
"1984" -- just another high-tech product plan with an over-optimistic ship date. But it's finally in beta.

@_date: 2003-12-21 11:47:25
@_author: Arnold G. Reinhold 
@_subject: PKI root signing ceremony, etc. 
One approach to securing infrequent signing or working keys from a corporate master certificate is to store the certificate in a bank safe deposit box. The certificate generation software (say on a self booting CD or perhaps an entire laptop) could be stored in the safe deposit box as well. The certificate signing would take place at the bank, either in one of the small rooms they provide or in a borrowed conference room.
This approach buys a large amount of physical security and an audit trail for the process at very minimal cost. It  also addresses another thorny problem: how to  match the control of a corporate master certificate to corporate governance mechanisms.  Board members of most corporations are poor potential custodians  of cryptographic material. Any password sharing system runs the risk of what to do if the secret holders are all fired.  Banks, on the other hand, are used to dealing with situations like changing access controls after a major management shakeup.
Arnold Reinhold

@_date: 2003-12-31 12:22:12
@_author: Arnold G. Reinhold 
@_subject: why "penny black" etc. are not very useful 
There is something else one can do that might help. The hashcash stamp algorithm can be designed to provide a strong, constant signature to virus detectors. For example, in my HEKS-1 algorithm, I populate a large array with pseudo random words. It would be easy enough to have some fraction (say 1/8th or 1/16th) of those words be a special constant (or one of a few special constants).  There would be no way for the spammer to avoid exhibiting the same constants while generating stamps without incurring a severe computational penalty. So any stamp generation activity would be easy to detect. Since the signature would never change, the detection software could be built into the operating system (or even the CPU itself).
Legitimate stamp generation would have to be distinguished, perhaps by code signing or some Touring test.  A sufficiently clever virus writer with root access might be able commandeer the legitimate stamp generator. If this happens, periodic required updates of the hashcash software can be issued that thwart viruses in the field. Also a large number of countermeasure variants can be generated, making it hard for the virus to recognize them all. This reverses the tactical advantage normally enjoyed by virus writers. Illegitimate stamp generators are forced to present a fixed target while legitimate programs and counter measures can continuously morpf.
Arnold Reinhold

@_date: 2003-02-10 22:51:36
@_author: Arnold G. Reinhold 
@_subject: Columbia crypto box 
It's worth remembering that the original WEP used 40 bit keys. For some time, RC4 with 40 bit keys was the only crypto system that could be exported without a license.  It's hard for me to believe that export concerns were not the primary factor in the initial choice of Arnold Reinhold

@_date: 2003-02-12 00:08:58
@_author: Arnold G. Reinhold 
@_subject: Columbia crypto box 
I might add that using RC4 with a key composed of a 40-bit secret and an IV transmitted in the clear would not necessarily qualify automatically under that 1992 agreement. It is quite possible that the foolishly short 24-bit IV in WEP was the result of real or anticipated pressure from the export control folks.
(It feels weird to be citing Schneier as a historical document).
Indeed, but it is important to remember just how thickheaded the anti-crypto effort of the '80s and '90s was and how much damage it Arnold Reinhold

@_date: 2003-02-18 16:19:23
@_author: Arnold G. Reinhold 
@_subject: AES-128 keys unique for fixed plaintext/ciphertext pair? 
Here is another way to look at this question. Each 128-bit block cipher is a 1-1 function from the set S = {0,1,...,(2**128-1)] on to itself, i.e. a bijection. Suppose we have two such functions f and g that are randomly selected from the set of all possible bijections S-->S (not necessarily ones specified by AES). We can ask what is the probability of a collision between f and g, i.e. that there exists some value, x, in S such that f(x) = g(x)?  For each possible x in S, the probability that f(x) = g(x) is 2**-128. But there are 2**128 members of S, so we should expect an average of one collision for each pair of bijections.
If the ciphers specified by AES behave like randomly selected bijections, we should expect one collision for each pair of AES keys or 2**256 collisions.  Just one collision violates Mr. Weinmann's hypothesis.  So it would be remarkable indeed if there were none. Still it would be very interesting to exhibit one.
For ciphers with smaller block sizes (perhaps a 32-bit model of Rijndael), counting collisions and matching them against the expected distribution might be a useful way to test whether the bijections specified by the cipher are randomly distributed among all possible Arnold Reinhold

@_date: 2003-02-18 19:42:18
@_author: Arnold G. Reinhold 
@_subject: AES-128 keys unique for fixed plaintext/ciphertext pair? 
In general, if G has n randomly chosen members of F, isn't the answer just 1/e**(n**2)?  There are n**2 pairs of functions in G (ok n*(n-1)) and the probability of no collision for each pair is 1/e as you point out above.
Just plain minuscule:  |G| = 2**128,  |F| = (2**128)!  ~= 2**(2**135)
Even if |G| << |F| that is true.  If G contains 5 functions, there are 20 pairs and 1/e**20 ~= 2.06E-9.
Arnold Reinhold

@_date: 2003-02-21 07:56:39
@_author: Arnold G. Reinhold 
@_subject: AES-128 keys unique for fixed plaintext/ciphertext pair? 
Maybe I'm missing something here, but the unicity rule as I understand it is a probabilistic result.  The likelihood of two keys producing different natural language plaintexts from the same cipher text falls exponentially as the message length exceeds the unicity distance, but it never goes to zero. So unicity can't be used to answer the original question* definitively.
I'd also point out that modern ciphers are expected to be secure against know plaintext attacks, which is generally a harsher condition than knowing the plaintext is in natural language. Furthermore they are usually subject to chosen plaintext attack which is always harsher.
Arnold Reinhold
* Here is the original question. It seems clear to me that he is asking about all possible plaintext bit patterns:

@_date: 2003-01-24 13:42:16
@_author: Arnold G. Reinhold 
@_subject: [IP] Master Key Copying Revealed (Matt Blaze of ATT Labs) 
I knew how master keys worked. I had one when I was at MIT and I've picked a few locks myself. I know a little crypto too, but I didn't think of this attack.  Lots of things are obvious once you've read I'm sure there is street argot for most of these terms, but Matt's paper is great tutorial on what they mean in a practical, physical setting.  Anyway, it got his picture in New York Times: If all the master cuts are higher than the change cuts, I believe you can carry out Len's procedure with a single blank. You start with the master key and file it down one pin position at a time until it becomes the change key.
The apparently common restrictions on where the master cuts can be relative to the change cuts would seem to severely limit the number of possible master keys for any given lock style.  It might well be possible to construct a priori a set of all possible master keys for a given lock style. This would make such systems vulnerable to someone who lacks even a change key. A careful lock picker could also deduce a lot of information on where the master cuts are.
Arnold Reinhold

@_date: 2003-01-29 05:20:02
@_author: Arnold G. Reinhold 
@_subject: [IP] Master Key Copying Revealed (Matt Blaze of ATT Labs) 
I took a look at the "MIT Guide to Lock Picking"  August 1991 revision at
It says:
"9.10 Master Keys
Many applications require keys that open only a single lock and keys that open a group of locks. The keys that open a single lock are called change keys and the keys that open multiple locks are called master keys. To allow both the change key and the master key to open the same lock, a locksmith adds an extra pin called a spacer to some of the pin columns. See Figure 9.8. The effect of the spacer is to create two gaps in the pin column that could be lined up with the sheer line. Usually the change key aligns the top of the spacer with the sheer line, and the master key aligns the bottom of the spacer with the sheer line (the idea is to prevent people from filing down a change key to get a master key). In either case the plug is free to The parenthetical comment suggests awareness of the general vulnerability Matt exploited, but I suspect that had the authors known the multiple partial copy trick Matt described, they would have published it.
Arnold Reinhold

@_date: 2003-03-06 10:26:41
@_author: Arnold G. Reinhold 
@_subject: Wiretap Act Does Not Cover Message 'in Storage' For Short  
I2ANAL, but I don't think that's clear at all, unless your are talking about specific paragraphs within the Wiretap Act and the Stored Communications Act.
The Konop decision specifically talks about government intercepts. See section B7, for example. They even discuss the post 9/11 situation in B6.
Add the Railway Labor Act in this case.
There are some who would argue that the simple password protection scheme Knopp used would be a "technological protection" covered under DMCA.  However, the penalty for access to protected material, as opposed to trafficking in technology, is a $2000 fine, which may not seem draconian to an airline.
Arnold Reinhold

@_date: 2003-03-09 00:14:45
@_author: Arnold G. Reinhold 
@_subject: Active Countermeasures Against Tempest Attacks 
Part 15 is pretty complex, but reading a summary at  suggests a number of problems. First there are dozens of bands where intentional radiators are not permitted to operate (15.205). Designing a noise source that avoided all these band might be difficult.
Second, the permitted signal levels associated with intentional radiators (15.209) are very similar to those permitted for unintentional radiators (15.109), including most consumer grade CRT monitors (Class B). Commercial monitors (Class A) are permitted higher levels of radiation, but I suspect most monitors made today are Class B.
Now the radiation from a monitor is mostly sweep signals and the like, which carry no information. The signals that drive the CRT guns are much weaker. But I suspect you will need the noise to be much more powerful to obliterate the signal carrying data. The situation is even worse if the attacker suspects what the data may contain. He can then use correlation techniques to find the data well below the noise level.
I'd also point out that the noise source has be be co-located with the data signal. Otherwise, the attacker can use a directional antenna to capture the noise signal without the data signal, allowing it to be subtracted from the data+noise signal.  Similarly, it will be vital to change the noise pattern whenever the content of the CRT changes, otherwise the attacker who had reason to suspect when the screen changed can subtract data1+noise from data2+noise to get data2-data1, which is likely to leak a lot of information.
I suspect it would be cheaper to shield the CRT or operate in a Faraday cage.
Arnold Reinhold

@_date: 2003-03-10 09:14:59
@_author: Arnold G. Reinhold 
@_subject: Active Countermeasures Against Tempest Attacks 
The dot clock on a megapixel display is around 70 MHz, or 14 nanoseconds per pixel. Syncing that over some distance is not trivial. Remember the speed of light is 1 nanosecond/foot. On the other hand, I think syncing the sweep signals would be enough to implement your idea and that should not be hard to do, possibly even in software since they are created on the video card.
Effectiveness is another matter. The attacker could use a directional antenna to separate out monitors. Even if his equipment was outside the building, the windows would act like an antenna whose radiation pattern would be different for the different monitors in the room. The attacker might be able to discriminate between different monitors just by driving his van around outside.
Even if he can't distinguish between different monitors, he still gets a signal that is the sum of the content on each monitor.  That is analogous to a book code and likely just as secure, i.e. not very.
Modifying existing monitors to shield the video signal wouldn't cost that much either. As I understand it the big expense in Tempest rated equipment is the testing  and the tight manufacturing control needed to insure that the monitors produced are the same as the ones tested.
Simply buying some class A monitors for the dummy data might do what you want, but I'm not sure 10-20 db of reduced signal to background buys you much.  I've heard numbers of 100 db or more required for effective Tempest shielding, with Class B shielding (the higher grade FCC requirement) buying you 40-50 db. See for example The signal is still serialized in digital form at some point on a pixel by pixel basis.  Because flat panels do not have the high-power sweep signals of CRT monitors, the overall shielding needed to meet Class B may be less.  That might make life easier for attackers.
This does suggest one simple approach that might be useful for flat panels displaying sensitive text: chose foreground and back ground colors that have the same number of on and off bits in each color byte pair, e.g. foreground red and background red each have three bits on, both blues have four bits on, both greens have five bits on. That might make background and foreground more difficult to distinguish via RF radiation in an all digital system.
On the other hand, remember that the earliest Tempest systems were built using vacuum tubes. An attacker today can carry vast amounts of signal processing power in a briefcase.
All in all I would not put much faith in ad hoc Tempest protection. Without access to the secret specifications and test procedures, I would prefer to see highly critical operations done using battery powered laptops operating in a Faraday cage, with no wires crossing the boundary (no power, no phone, no Ethernet, nada).  In that situation, one can calculate shielding effectiveness from first principles.  suggests US government requirements for a shielded enclosure are 60 db minimum.
Arnold Reinhold

@_date: 2003-03-11 11:04:17
@_author: Arnold G. Reinhold 
@_subject: Active Countermeasures Against Tempest Attacks 
One of my pet ideas is to used older, 1990's vintage, laptops for secure processing, e.g. reading PGP mail, generating key pairs, signing submaster keys, etc.  They are cheap enough to dedicate to the task, they'd be off most of the time thereby reducing vulnerability, older operating systems and firmware have fewer opportunities for mischief and most viruses won't run on the old software.  Easier shielding due to lower clock rate is an advantage I hadn't thought of before.
You could say that about strong crypto in general. Anyone with valuable information stored on a computer has lots to worry about.
Arnold Reinhold

@_date: 2003-03-31 17:46:44
@_author: Arnold G. Reinhold 
@_subject: Russia Intercepts US Military Communications? 
There is a lot of material on SINCGARS available on line via Google. This is a low-VHF system used primarily by U.S. ground forces and those who want to talk to them.  It offers both frequency hopping and Type-1 encryption (at least the newer models) and can also be used in single channel, unsecured mode to talk to older VHF-FM radios. According to one source, about 164,000 SINCGARS radios have been fielded and all older VRC-12 radios should have been replaced by 2001.
The key management systems (nightmare may be a better term) are described in considerable detail in  . It's from 1996 and makes very interesting reading. For example, radios have to have their time set to within 0.4 sec of GMT. It's easy to believe that units switch to un-encrypted modes under the stress of battle.
Even tho the radios seem quite versatile, the usage is extremely hierarchical.  News reports have stated that one advance in this war is that the daily "tasking order" can now be distributed electronically.  This probably includes all the material needed to set up the SINCGARS (frequency hop list, frequency hopping keys, communications security keys, call sign lists, network IDs, etc.). That may make things a little better than in 1996.
I went to a lecture at MIT by someone for the US Army talking about the "soldier of the future," an integrated body armor/backpack/electronics system. I asked about encryption and he said it was Army doctrine not to use it at the intra-squad level. Key management is one of the issues. That is consistent with the number of SINCGARs radios produced. So there should be plenty of open voice traffic to analyze.
Arnold Reinhold

@_date: 2003-03-31 17:50:35
@_author: Arnold G. Reinhold 
@_subject: Kashmir crypto 
While Googling for material on SINCGARS, I found an article about crypto in the India/Pakistan conflict. Old style cryptanalysis isn't dead yet:
Arnold Reinhold

@_date: 2003-05-14 13:19:56
@_author: Arnold G. Reinhold 
@_subject: Payments as an answer to spam 
In the paper world, it is quite common to do apparently wasteful things in the hope of getting your message read. Examples include fancy stationary, hand addressed envelopes, using FedEx where a 37 cent stamp would suffice, and paying Ed McMahon for the use of his picture on envelopes. I think hashcash should be seen in that light.
If hashcash proof of work is combined with an easy to use whitelist, the hash stamp is only needed when communicating with strangers. For most people that happens infrequently, so quite high work levels are reasonable, even one minute per message  or more.  If a bidding war develops, individual users with gigaHerz machines will be able to outbid the spammers and I think users will accept the delays necessary. I could see a service that sampled email and suggested what hashcash level is required to stand out above most spammers.
Handheld devices would be at a disadvantage, but wireless email providers could offer a stamping service. Also there is more acceptance of a per message charge in the wireless world (e.g. SMS) so an alternative would be for the wireless carrier to sign messages attesting to the amount that the sender has paid. The PKI and private-key security issues in doing this are quite manageable, unlike in schemes requiring all individuals to sign messages.  This could easily be combined with a proof of payment to charity approach.
E-mail clients that can support a variety of approaches, whitelists, hashcash, proof of payment, rule-based filters, etc., may be the best answer to spam.
Arnold  Reinhold

@_date: 2003-05-20 14:03:39
@_author: Arnold G. Reinhold 
@_subject: Taking aim at denial-of-service attacks 
One interesting aspect of using proof of work (POW) to protect against denial of service attacks is that it can be implemented and demonstrated without the need for widespread adoption. The basic idea (as I see it) is that the servers that handle end-user PCs have the ability to demand proof of work from the end users before accepting packets and give priority to the delivery of packets where required work has been demonstrated. Higher level servers then give priority to packets where POW has been demonstrated.
To establish an initial system, large user, such as the Federal Government, a large corporation or a consortium of universities, only has to insure that there is chain of POW-aware servers between several of its sites. The selected sites should then enjoy protection from DOS attacks for inter-site communications and this would be evident when such attacks occur. Additional sites could be added incrementally and, as long as proper standards are created and observed, different networks that adopt POW antiDOS can be linked merely by establishing a POW aware path between the nets. Since POWawareness would likely be just a software upgrade the technology should spread quite rapidly.
Arnold Reinhold

@_date: 2003-05-30 12:21:16
@_author: Arnold G. Reinhold 
@_subject: "PGP Encryption Proves Powerful" 
There is a lot of material from the World War II era (e.g Silk and Cyanide by Leo Marks) and the early cold war (e.g. Government cryptographic successes are usually highly classified and kept that way for decades. There was one recent story about the FBI's apparent use of a keyboard logger to get a accused organized criminal's password. The latest U.S. Government wiretap report  (they are now required to report on encryption incidents) says: "Encryption was reported to have been encountered in 16 wiretaps terminated in 2002 and in 18 wiretaps terminated in calendar year 2001 or earlier but reported for the first time in 2002; however in none of these case was encryption reported to have prevented law enforcement officials from obtaining the plain text of the communications intercepted." By comparison they reported 1358 intercepts authorized in 2002.
Arnold Reinhold

@_date: 2003-11-14 08:47:54
@_author: Arnold G. Reinhold 
@_subject: Protection against offline dictionary attack on static files 
Jill's approach to key stretching is not quite the same as the traditional iterated hash.  It imposes no cost at encryption time, you only have to work at decryption. This might be valuable when you want to save your files as the Gestapo is breaking down your door.
I've been working on a similar method for use as an anti-censorship tool. Files would be encrypted with a random key and posted on the Internet. The key size would be selected to require a long time to crack: hours, days or even weeks. People in countries behind national Internet filtering could download these files and crack them, possibly telling friends the recovered key. Censors would have to expend a lot of effort trying to learn the files that contained forbidden ideas. It would be inexpensive to create many different encryptions of the same file and mirror them in multiple locations or to flood them on Usenet. The URLs of good stuff could be spread by word of mouth.
Arnold Reinhold

@_date: 2003-10-03 14:21:44
@_author: Arnold G. Reinhold 
@_subject: anonymous DH & MITM 
I think that is an excellent summation of the history-based approach to threat modeling. There is another approach, however, capability-based threat modeling. What attacks will adversaries whom I reasonably expect to encounter mount once the system I am developing is deployed? Military planners call this the "responsive threat."  There are many famous failures of history-based threat modeling: tanks vs. cavalry, bombers vs. battleships, vacuum tubes vs. electromechanical cipher machines, box cutters vs skyscrapers, In the world of the Internet the time available to put in place counteract new threats once they are publicized appears to be shrinking rapidly. And we are only seeing one class of adversaries: the informal network of hackers. For the most part, they have not tried to maximize the damage they cause. There is another class, hostile governments and terrorists, who have so far not shown their hands but are presumably following developments closely.  I don't think we can restrict ourselves to threats already proven in the wild.
Then there is the matter of costs and who pays them. Industry is often willing to absorb small costs, or, better, fob them off onto consumers. Moderate costs can be insured against or written off as "extraordinary expenses." Stockholders are shielded from the full impact of catastrophic costs by the bankruptcy laws and can sometimes even get governments to subsidize such losses.
Perhaps guilds are the right model for cryptography. At their best, guilds preserve knowledge and uphold standards that would otherwise be ignored by market forces. Anyone out there willing to have open heart surgery performed by someone other than a member of the surgeon's guild?
Arnold Reinhold

@_date: 2003-09-13 23:36:48
@_author: Arnold G. Reinhold 
@_subject: quantum hype 
I think there is another problem with quantum cryptography. Putting aside the question of the physical channel, there is the black box at either end that does all this magical quantum stuff. One has to trust that black box.
- Its design has to thoroughly audited  and the integrity of each unit verified
- It has to be shipped securely from some factory or depot to each end point
- It has to be continuously protected from tampering.
It seems to me one could just as well ship a 160 GB hard drive filled with random keying material to each endpoint. The disk drive would receive the same  level of physical security as the quantum black boxes. At one AES256 key per second, a 160GB hard drive holds 150 years of keying material.  For forward security one can erase used keys.  (If you don't trust disk erasing, ship a carton of CD-Rs or DVD-Rs and burn them as they are used up).
The 160 GB hard drive has a couple of advantages over quantum key exchange:
- No special assumptions about the channel are needed. One can use the existing  Internet, telephone, satellite and even shortwave - The hard drives and the PCs to use with them can be purchased off the shelf from a random computer store. No one is alerted that you are engaging in secret communications so  no one is likely to tamper with your equipment before you get it.
- The necessary software is easy to write and audit
- I expect a quantum crypto box to cost far more than a160 GB disk drive, not to mention the cost of the dedicated fiber channel.
What am I missing?
Arnold Reinhold

@_date: 2003-09-21 06:54:49
@_author: Arnold G. Reinhold 
@_subject: quantum hype 
It seems to me that because key-exchange methods such as DH only depend on exchanging bits (as opposed to specifying a physical layer), they can rely on a wide variety of techniques to combat DoS. If Bob and Alice can safeguard their local connections to the Internet, its multi-routing properties provide significant DoS protection. Other options available to them include the switched telephone network, wireless, LEO satellites, cybercafes, steganography,  HF radio, and even postal mail. In addition, DH users have no need to call attention to themselves by leasing a fiber-optic Arnold Reinhold

@_date: 2004-04-05 10:46:31
@_author: Arnold G. Reinhold 
@_subject: [Mac_crypto] Apple should use SHA! (or stronger) to  
Dobbertin's 1996 collision demonstration is another good reason not to use md5, but is obviously hasn't gotten the open source community or Apple to stop.  Whether my attack will be any more successful in effecting change remains to be seen. Publishing SHA1 hashes in parallel with md5 seems like such an inexpensive thing to do, but one should never underestimate cryptographic inertia. For the record, I first published my attack on Perry Metzger's cryptography list in February, 2002.
Arnold Reinhold

@_date: 2004-04-05 18:43:07
@_author: Arnold G. Reinhold 
@_subject: [Mac_crypto] Apple should use SHA! (or stronger) to 
The file ordering may be deterministic, but someone who is well versed in the configuration control and release engineering process might well be able to have a chosen file placed at the end of of the package. The method for getting stuff at the end needn't be perfect. The attacker can keep trying until he succeeds.
Having a tail 2 MB or longer may make the processing time comparable to finding an SHA1 collision, but it is still a 64-bit problem and thus requires far less memory than finding an SHA1 collision.
I am not saying that my attack is easy, but that it is feasible for a large organization and very dangerous and stealthy if it succeeds. History has shown, over and over and over again, the folly of ignoring cryptographic attacks that are theoretically possible but seem too hard to implement. On the other hand, defending against my attack certainly is easy, just publish an SHA1 (or stronger) hash alongside the MD5 hash.
Arnold Reinhold

@_date: 2004-04-08 13:35:57
@_author: Arnold G. Reinhold 
@_subject: voting 
I can see one potential problem with having the machine produce the receipts. Let's say the system is well designed and completely fair. There will be a certain percentage of voters who will complain that the receipt recorded the wrong vote because they in fact inadvertently pressed the wrong button.  Over time, that percentage and its variance will become well known.  Call that rate "r.' A party with the ability to make surreptitious changes to the voting software can then have it occasionally record a vote and print a receipt contrary to what the voter chose as long as the number of such bogus votes is small enough relative r and its variance to escape notice. They can then determine what fraction, f, of voters who get wrong receipts  report them. They can then increase the fraction of bogus votes by 1/f.  Over the course of several elections they can slowly grow the fraction of bogus votes, claiming that voters are getting sloppy. Since major elections are often decided by less than one percent of the vote, this attack can be significant.
We have a system now in Cambridge, Massachusetts where we are given a paper mark sense ballot and fill in little ovals, like those on standardized tests. We then carry our ballot to a machine that sucks it in and reads it. The totals are reported after the polls close, but the mark sense ballots are saved inside the machine (which I assume is inspected before the voting starts and then locked) can easily be recounted at any time. This system seems ideal to me.
A important thing to remember is that these poll watchers, along with the workers running the voting for the election authorities are often retired people who have very little computer skills. It is much easier for them to understand and safeguard systems based on paper and mechanical locks.
Arnold Reinhold

@_date: 2004-04-14 17:15:29
@_author: Arnold G. Reinhold 
@_subject: Definitions of "Security"? 
Here is one of mine that is biology inspired:
"A division of a set of actors into 'self' and 'other' with a mechanism that allows self actors  to perform certain activities that are effectively denied to others."
Arnold Reinhold

@_date: 2004-04-14 18:31:21
@_author: Arnold G. Reinhold 
@_subject: AES suitable for protecting Top Secret information 
I was the one who updated the Wikipedia entry . It was shortly before the cryptography list came back up.  I found the June 2003 CNSS fact sheet while looking for other information on NIST's standards program. The first reference that I found that suggested AES could be used for classified was in a slide presentation at a Dec. 4, 2002 NIST Wireless Security workshop   by Timothy Havighurst of NSA on DOD Wireless Policy One slide reads:
" SECRET and TOP  SECRET data must be approved with a Type I algorithm
(I believe the BATON algorithm itself is still classified.)
This is a major milestone in cryptography. I believe it is the first time in modern history that the public knowingly has access to a cipher that the U.S. Government currently considers strong enough for Top Secret information.
Note that the CNSS fact sheet goes on to say:
"The  implementation of AES in products intended to protect national security systems and/or  information must be reviewed and certified by NSA prior to their acquisition and use."
Another interesting  presentation at the same NIST workshop was by Bill Burr on NIST's Cryptographic Standards Program.  It has a nice chart comparing the strengths of various crypto primitives based on their key length (page 7).  Anther slide (page 13) contains the following interesting statement:
"Proposed 80-bit crypto end of use date: 2015"
Based on the page 7 chart, this presumably includes SHA1, Skipjack, 1024-bit RSA/DSA and 160-bit ECC.
Arnold Reinhold

@_date: 2004-04-29 10:49:00
@_author: Arnold G. Reinhold 
@_subject: Can Skype be wiretapped by the authorities? 
From the Skype FAQ "Is the source code for Skype available? Can I have a copy?
No. Skype is proprietary and closed-source software."
In a closed source system it is certainly possible for the authors to provide "backdoors" that would allow wiretapping.  There are many ways to do this. Perhaps the simplest way is to constrain the random number generator to select values from a limited, searchable set of possibilities. The constraint might be turned on by receipt of a special message.
The backdoor could be included in all copies of the program  or just selected copies, particularly if there are provisions for automatic updates. A backdoor could also be delivered as a virus or worm.
If the authorities can gain one-time physical access to one of the computers in the Skype network, all encrypted communication to and from that computer as an end point can be compromised regardless of how well Skype has designed its system (this does not include messages relayed by that computer if Skype has done things right).
This is not to suggest that Skype is a bad product or that all open-source encryption solutions are safe, but a closed-source system is only as trustworthy as its authors.
Arnold Reinhold

@_date: 2004-01-09 11:42:10
@_author: Arnold G. Reinhold 
@_subject: [Fwd: Re: Non-repudiation (was RE: The PAIN mnemonic)] 
I did a Google search on "irrebuttable presumption" and found a lot of interesting material. One research report on the State of Connecticut web site
says: "The Connecticut Supreme Court and the U. S. Supreme Court have held that irrebuttable presumptions are unconstitutional when they are not necessarily or universally true and the state has reasonable alternative means of making the determination."
The comment appears to apply to statutes and regulations (as opposed to contracts).  Still the two tests mentioned seem very appropriate to a discussion of non-repudiation as used in cryptography. In deciding whether the existence of a verified signature should automatically lead to some real world action, we should consider both the adequacy of the technology and the nature of the application.
So, for example, the military might adopt an irrebuttable presumption that a cryptographically signed order comes from the registered owner of a cryptographic key, because it has vetted all the technology employed, it can't tolerate delay, and  is willing to impose a duty on a key holders to protect their key or suffer the consequences.
On the other end of the scale, anti-spam software might accept a signature validated by a public key that is included in a user's white list as conclusive proof that the message should be transmitted to that user because the consequences of doing so with a forged message are so minute.
In the case of ordinary consumer transactions, an irrebuttable presumption for public key signatures would not seem to pass muster. There are too many problems with the technology (its not just a question of protecting the private key, but also of insuring the the document actually signed is the one the user thought he was signing) and there are usually other forms of evidence (e.g. delivery records) to substantiate the transaction.
This is apparently a very complex area of law. Another paper
includes these quotes:
"Every writer of sufficient intelligence to appreciate the difficulties of the subject matter has approached the topic of presumptions with a sense of hopelessness and left it with a feeling of despair."5  Commenting on the law of presumptions, Judge Learned Hand has commented: "Judges have mixed it up until nobody can tell what on earth it means."6
It sounds like the legal profession long ago recognized the difficulties the cryptographic community is now grappling with regard to "non-repudiation."  We should be very wary of assuming mathematical constructs naturally transform into the legal arena.
Arnold Reinhold
(who is not a lawyer)
  5  Edmund M. Morgan, "Presumptions," 12 Wash. L. Rev. 255, 255 (1937).
  6  L. Hand, 18 ALI Proceedings 217-18 (1941).

@_date: 2004-06-16 00:14:37
@_author: Arnold G. Reinhold 
@_subject: Is finding security holes a good idea? 
"The Mythical Man-Month" is a great book, but it's almost 30 years old. Brooks considered OS/360 to be hopelessly bloated. My favorite quote (from Chapter 5, The Second System Effect, p. 56):
"For example, OS/360 devotes 26 bytes of the permanently resident date-turnover routine to the proper handling of December 31 on leap years (when it is Day 366). That might have been left to the Modern operating system are 2 to 3 orders of magnitude larger than OS/360.. They are far more reliable than OS/360 was in its early days and do not presume the availability of an on-site team of operators and system programmers.  For the most part they are still maintained one bug at a time The bug fixing process has not reached Brook's predicted crisis.
My other concern with the thesis that finding security holes is a bad idea is that it treats the Black Hats as a monolithic group. I would divide them into three categories: ego hackers, petty criminals, and high-threat attackers (terrorists, organized criminals and evil governments).  The high-threat attackers are  likely accumulating vulnerabilities for later use. With the spread of programming knowledge to places where labor is cheap, one can imagine very dangerous systematic efforts to find security holes.  In this context the mere ego hackers might be thought of as beta testers for IT security.  We'd better keep fixing the bugs.
Arnold Reinhold

@_date: 2004-06-24 08:31:55
@_author: Arnold G. Reinhold 
@_subject: cryptograph(y|er) jokes? 
Q: How many cryptographers does it take to change a light bulb?
A: XIGHCBS
Also see numbers 2.3 and 2.4 in the humorous list of proof techniques at Arnold Reinhold

@_date: 2004-05-13 13:28:33
@_author: Arnold G. Reinhold 
@_subject: The future of security 
Here are my thoughts on the future of cryptography:
A major use of crypto will be in efforts to restrict the dissemination of information to the public (corporate security, digital rights management, state censorship)
Human factors will be regarded as equal in importance with algorithms and protocols.
Servers and workstations will incorporate video and other sensors to provide self protection against physical intrusions.
As cellphones and PDAs merge there will be a new generation of privacy applications for text messaging and/or  voice that use light weight protocols and, perhaps symmetric keys.
Cellphone cameras will be used for stenographic communication.
Cellphones and PDAs will be used as security tokens for desktop/laptop access, perhaps using Bluetoth
Self-booting, open source CDs will become available that turn any PC into a secure messaging system with private keys and messages stored on an encrypted disk image on a memory stick.
4096-bit RSA keys will become the standard (RSA is already recommending 1024-bit keys be phased out by 2010.)
Key stretching techniques will be enhanced and standardized to allow password-based security to remain viable.
Password entry will be done using mouse and display screen, rather than keyboards because of all the risks keyboards represent (software and hardware loggers, video cameras, acoustic analysis, etc.)
Desktop systems with no hard drive and no I/O ports will become required for processing confidential information.
One or more secure networks will emerge that parallel the existing Internet. They will use IPv6 and have mandatory encryption and Cameras and audio recorders will be equipped with GPS, digital signing and secure time stamping technologies to restore confidence in  recorded evidence.
Stored value smart-cards will finally become popular in the U.S. through use in public transportation systems.
Hashcash will be used to bring spam under control and to protect networks against zombie attacks.
Anti-spam white listing will be the killer app that finally creates a universal public key infrastructure.
Patent concerns will be a major barrier to progress.
Arnold Reinhold

@_date: 2004-05-23 08:24:09
@_author: Arnold G. Reinhold 
@_subject: No encryption in federal wiretaps in 2003 
The 2003 wiretap report from the US Court system's Administrative Office is out:
 This annual report is mandated by Congress and since 2002 has been required to include information on encryption.  It states:
"In 2003, no instances were reported of encryption's being encountered on federal wiretaps. One state jurisdiction reported that encryption was encountered in a wiretap terminated in 2003; however, the encryption was reported to have not prevented law enforcement officials from obtaining the plain text of communications intercepted. "
According to the 2002 report :
"Encryption was reported to have been encountered in 16 wiretaps terminated in 2002 and in 18 wiretaps terminated in calendar year 2001 or earlier but reported for the first time in 2002; however, in none of these cases was encryption reported to have prevented law enforcement officials from obtaining the plain text of communications intercepted. "
The 2003 report goes on to state that:
"After decreasing 9 percent in 2002, the number of wiretaps reported increased 6 percent in 2003. A total of 1,442 applications were authorized in 2003, including 578 submitted to federal judges and 864 to state judges. Judges approved all applications. Compared to the number approved during 2002, the number of applications approved by federal judges in 2003 increased 16 percent, and the number of applications approved by state judges remained stable (up 0.3 "... 77 percent of all applications for intercepts (1,104 wiretaps) authorized in 2003 cited drug offenses as the most serious offense under investigation."
Arnold Reinhold

@_date: 2004-05-28 07:56:30
@_author: Arnold G. Reinhold 
@_subject: Satellite eavesdropping of 802.11b traffic 
I would say quite practical. A huge advantage for the attacker is that 802.11b/g is in a fixed frequency band. A half-wave dipole is 6.25 cm long. A large phased array could be assembled out of printed circuit board tiles, each with many antennas.
The outdoor range for 802.11 is up to 100 m.  Low earth orbit is about 150 km.  That is a factor of 1500. Power attenuation is the square of that, which works out to a 64 db loss.  Throw in another 10 db for slant range, building attenuation, etc. The loss has to be made up by a combination of antenna gain, improved receiver performance and better signal processing. That doesn't sound undoable.
A single LEO satellite would only have a few minutes of visibility per day over any one location on Earth. That suggests an active attack, where the satellite looks for files or even changes data. The satellite's ability to transmit at much higher power levels is an A third option is spot jamming. Here high power means one can get away with a smaller antenna, perhaps wrapped around a cheaper spin stabilized satellite.  Such a system could be used to briefly disable 802.11-based security systems, perhaps allowing a spy to gain access to a building.
Other interesting possibilities include long endurance remotely-piloted aircraft, balloons and small receiving stations that could be planted by spies or even parachuted into position. I'm sure 802.11 has given the SIGINT community much joy.
Arnold Reinhold
