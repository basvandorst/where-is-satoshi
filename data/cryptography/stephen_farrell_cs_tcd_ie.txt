
@_date: 2013-11-15 23:52:26
@_author: Stephen Farrell 
@_subject: [Cryptography] Moving forward on improving HTTP's security 
Maybe in the medium/long term, perhaps. As of now, there is
I think exactly one CT log operational. Putting all eggs in
Google's basket doesn't sound like a good plan to me.
Having said that, I like CT, and I know the Google people
would like more logs to exist, and maybe there're some being
stood up already for all I know, but CT is not yet ready
for that level of prime-time. I'd say it is by far the most
credible big-DB-of-public-keys candidate at present though
so it might get there sometime.

@_date: 2013-10-10 20:29:33
@_author: Stephen Farrell 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
Suggest it on the tls wg list as a feature of 1.3?

@_date: 2013-10-12 13:00:14
@_author: Stephen Farrell 
@_subject: [Cryptography] PGP Key Signing parties 
If someone wants to try organise a pgp key signing party at
the Vancouver IETF next month let me know and I can organise a
room/time. That's tended not to happen since Ted and Jeff
don't come along but we could re-start 'em if there's interest.

@_date: 2013-09-09 22:49:13
@_author: Stephen Farrell 
@_subject: [Cryptography] What TLS ciphersuites are still OK? 
Hi Ben,
I don't agree the draft says that at all. It recommends using
the above ciphersuite. (Which seems like a good recommendation
to me.) It does not say anything much, good or bad, about any
other ciphersuite.
Claiming that all the rest are no good also seems overblown, if
that's what you meant.

@_date: 2013-09-10 14:10:40
@_author: Stephen Farrell 
@_subject: [Cryptography] What TLS ciphersuites are still OK? 
There are issues, sure. And way too many ciphersuites certainly.
Since they're talking about it now on the TLS wg list, I'll
leave that them (and to folks who're qualified to figure if
the NIST, brainpool etc curves are ok, which doesn't include
me :-)
What I was pointing out is that there's a bit of a gap between
"no good" and "not what we'd recommend today." Since getting
rid of deployment of old stuff takes years, I think its
better that we don't overstate the issues that do exist. But
I very much welcome Yaron's draft and hope it shoots along

@_date: 2013-09-22 19:00:28
@_author: Stephen Farrell 
@_subject: [Cryptography] RSA equivalent key length/strength 
That's a mischaracterisation I think. Some folks (incl. me)
have said that 1024 DHE is arguably better that no PFS and
if current deployments mean we can't ubiquitously do better,
then we should recommend that as an option, while at the same
time recognising that 1024 is relatively short.

@_date: 2013-09-28 18:07:48
@_author: Stephen Farrell 
@_subject: [Cryptography] Gilmore response to NSA mathematician's "make 
I think there probably are some fair criticisms that we were a
bit complacent after the clipper and export stuff seemed to be
sorted out and the whole NIST/NSA thing with the AES and SHA-3
competitions seemed to be ticking over nicely.
That's more than a bit silly though IMO.
The sensible approach here is to a) see what's the best we can
do now with deployed code given that we know it takes years to
get anything near everything updated, but also b) figure out what
do we want to do, knowing that it'll take years for deployment
to happen no matter how small a change we make.
a) is Yaron's BCP draft
b) is TLS1.3 (hopefully) and maybe some extensions for earlier
   versions of TLS as well
Arguing for (b) only, and that we ignore (a) would be dumb.
For (a), we are entirely constrained in what we can do, basically,
the only thing we can do is say how to better configure already
deployed code.

@_date: 2013-09-30 08:13:29
@_author: Stephen Farrell 
@_subject: [Cryptography] TLS2 
Sounds like a suggestion to make on the tls wg list. It might get some support, though I'd guess not everyone would want to do that

@_date: 2014-08-09 00:34:41
@_author: Stephen Farrell 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
Two wee comments...
Well yes and no, but mostly yes. Purely personally I figure that's
becuase NIST did such a good job with AES that we figured that
suite B would be fine. The "no" part is that unfortunately once
we'd gone there, and NIST do turn out to be a .gov, that did make
it impractical to easily say no to other .gov.ccTLD types. Somewhat
absurd though, no question.
Sigh. Ian - you have participated on a few IETF mailing lists
recently. Are you a part of this cabal of committees now? I'm
guessing you would reasonably say no. And the reason is not that
you're special, but rather that there is no cabal of committees.
That's just not how it works and continuing to describe the IETF
thusly seems to me to lack... style. Well accuracy at least.
That might seem a bit precious, but the fact is that this mailing
list is as likely to conclude something crazy as any other,
whether hosted by Perry or the IETF. Using the pejorative
"committee" is just basically careless. The IETF is made up of
mailing lists with participants who are people (well, almost
all;-) with all their foibles and imperfections, same as here.

@_date: 2014-08-11 10:47:40
@_author: Stephen Farrell 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
No. OTOH, I have been through the process a number of times.
Coming to things fresh and having experience both have their
pros and cons.
Well, feel free to propose a better way to do things at the
scale at which the IETF works. Off this list is probably
better for that if its IETF specific. I really would be
interested in ways to improve things.
I've seen your point about things designed by one person
(or small teams) and it has some validity but is by no means
a panacea.

@_date: 2014-08-25 13:43:25
@_author: Stephen Farrell 
@_subject: [Cryptography] Encryption opinion 
With s/HTTPS/TLS/ in the above, I'd fully agree.
What the httpbis WG and afaik browser implementers are working
on is to try keep the MitM-resistance properties of "https" URLs
(i.e. all the same PKI stuff being enforced by the TLS engine)
but to define a way to transport "http" URLs over TLS invisibly
to the user. In the latter case, they'd not barf on expired or
self-signed certs presumably. (The work in progress for that
is [1], and as I think Rich Salz mentioned here before its not
clear which browser implementers will adopt that.)
PS: Note, I'm not saying I agree with all the above, I'm just
reporting what I think is the current state of play.
[1]

@_date: 2014-08-26 23:51:47
@_author: Stephen Farrell 
@_subject: [Cryptography] Fwd: [Endymail] Off we go... 
FYI. We just started a new mailing list. [1] The list description
is below followed by a kickoff mail. I think there are some folks
on here working in this space and it'd be great to get their good
[1] There is significant interest in improving the
privacy-related properties of Internet mail. One focus of
current efforts is on the per-hop (connection-based)
protections provided by TLS. However a wide range of other
work has a focus on end-to-end protection, at the Internet
scale of billions of end users and perhaps millions of
operators. Such work typically involves new forms of mail
header or body protection, new public key management
(compared to S/MIME or PGP), and security mechanisms more
appropriate for mobile/web user-agents. Other
security-relevant approaches may be discussed if needed.
Various proposals and development efforts on this topic are
underway outside the IETF. This mailing list provides an
IETF venue for discussion of elements that might be commonly
needed by such efforts and to identify work that the IETF
could do to aid in achieving better end-to-end security
deployed for Internet email.
-------- Forwarded Message --------
Hi all,
We've got 88 people subscribed now, which is pretty quick for a list
like this. Seems like there really is interest in the topic, which is good.
What we'd suggest is to start off with some sharing about what folks
are/have-been doing in this space. We know there are a range of projects
and proposals and it'd be good to get some information/pointers on those
we all know about.
Its probably worth stating now that this list is *not* intended to pick
a winner amongst those, nor to anoint one as the "official" IETF thing,
so luckily we don't need to have a bunfight about which is the shiniest
proposal of them all. :-)
Rather a goal of this list is to identify bits of work that the IETF
could do to help such projects/proposals so they could achieve
significant deployment. So if there are common bits to some
projects/proposals that'd be interesting and especially if there'd be
value in having those bits standardized. Or if there are other ways in
which we could help things along that's fair game for discussion too.
And I suppose its inevitable that we'll discuss requirements and the
real-world constraints on solutions too. And even get new and possibly
radical proposals for how to do better in this space. And those are also
OK and interesting for this list.
Success here will be when/if we identify some bit(s) of work that the
IETF could credibly do that'd improve the real-world end-to-end security
and privacy of email. And note that "credible" there requires stuff to
be both technically sane and to have a sufficient set of capable folks
interested and willing to do work.
When/if we do identify such, we'd probably want to start a new WG/list
to actually do the work identified at which point this list could
languish or continue on with more discussion of the next good thing(s)
to do.
So off we go... What projects are folks working on, and what should the
IETF be doing in this space?
Pete & Stephen
Endymail mailing list
Endymail at ietf.org

@_date: 2014-08-29 18:16:07
@_author: Stephen Farrell 
@_subject: [Cryptography] Phishing and other abuse issues [Was: Re: 
FWIW, I'm not clear on what scope extension (or whatever)
is being proposed. I do get that the IETF not considering
UIs has in the past caused problems, in particular with
HTTPS, but I do not get how one might sensibly go about
improving that. If someone has a concrete proposal, I'd be
happy to chat about what to do about that.
I'm not sure that's really in scope for this list though,
so maybe better to handle that offlist unless the moderators
say its ok.

@_date: 2014-02-18 02:40:45
@_author: Stephen Farrell 
@_subject: [Cryptography] BitCoin bug reported 
I do agree that ASN.1 does encourage specification writing rather
than real implementation, testing and deployment. And that is a
real downside. Turning that into a diatribe was quite amusing
around about the time I figure I cured myself of the above
affliction. But that's going on for a decade and a half ago at
least. IOW, for me, that particular joke was funny once, with
the emphasis on the past tense. Maybe others differ though.
But you ignore a bunch of boring detail, for example that the
X.500 Name data structure was intended to be part of a hugely bigger
picture (X.500) that did not work out in the real world. There
really did appear to be valid reasons to reasonable folks back
then for all the gnarliness of X.400 addresses for example, and for
including those as an option in X.500 names. Ignoring such detail
IMO risks repeating the errors, maybe in JSON or whatever is next,
and is a bad plan. I think the error of the X.500 Name is a
fairly straightforward one (but hard to recognise as you do it) of
being over general via allowing options and is constantly repeated.
To give another example, the notAfter field in X.509 was quite
reasonable for DAP authentication within an enterprise or telco
environment which was its original purpose. For the web PKI, and
even more for embedded devices, notAfter is just a total PITA, but
knowing why it was there in the first place is better than being
ignorant, which seems to be the case for some folks these days.
(I.e. it was not there to generate money for CAs.) The error of
notAfter is nothing to do with ASN.1, but is rather to do with
not getting the right balance between current and future use-cases.
Bottom line: I fear that *only* lampooning 1980's data structure
design risks repeating rather than learning from the errors made.
And it seems a bit odd to be so fixated on essentially the 1988
blue-book, from which most of the rest of the design followed.
Surely there have been some more significant errors made since?
(Snowdonia for example, to pick my own recent fav;-)

@_date: 2014-01-14 09:21:55
@_author: Stephen Farrell 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
TLS has always included anon-dh ciphersuites, going back to 1996
at least. [1] It could be interesting to think about why fewer
protocols wanted to use that, and why its not been implemented
and deployed widely, but its in the protocol and always has been.
I have no idea what that's supposed to mean, other than
being a pejorative. And 'committees' is plain wrong, IETF
WGs are mailing lists to which anyone can contribute, quite
like this one, but with some more structure because they
exist to produce output. But the IETF is far from perfect
of course.
Anyway, if you want to change the IETF then you can do that
simply by being involved. If you want to just tell the IETF
how to change from the sidelines, then you can even do that,
you've a day and a bit left to submit a position paper. [2]
(I do think it'd be good to have some position papers from
outside the usual consensus so I do mean that.)
[1] [2]

@_date: 2014-01-15 12:26:33
@_author: Stephen Farrell 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
Fair enough.
Yep, a good question. And one I can't answer. I don't
recall if the SSL surveys count servers that can do
anon-dh ciphersuites but that'd be a start.
There we disagree again. I figure most people didn't
want anon-dh because they wanted at least web server
authentication and we also wasted a lot of time on
trying to push TLS client auth, which was probably
more of a disconnect between security folks and
(web) application developers. Anyway, I don't think
there is one single reason for pretty much anything
as complex as what does or does not get widely
[...maybe-realistic pessimism elided;-) ...]
Perhaps you're right, and maybe I'm naive, but I think
we should try nonetheless. I guess we'll see how it
comes out in a few years. If I'm right I hope things
will be variously better. If I'm wrong it'll be more
or less the status quo or worse, but I think that last
is the inevitable outcome if we take your approach
and don't engage.
All that said, the IETF is just one bit of the whole
thing, so working outside that context is just as

@_date: 2014-07-20 17:54:01
@_author: Stephen Farrell 
@_subject: [Cryptography] The role of the IETF in security of the 
the net?
You may end up in the rough in that discussion, but I would point
out that a) you (and anyone able to talk sense:-) are free to take
part and b) the discussion is not over - I think its not unlikely
that Russ' draft will change as a result to at least note the
downsides of how algorithm agility has been handled in the past
(e.g. see [1] and follow ups).
On your more general point, yes, people with money (*) can work
within even open processes like the IETF's more easily than those
without. I don't know how you tackle that. Best seems to be
as open as you can be, which the IETF does and tries all the
time to do better, but the issue remains.
(*) meaning money and the time, expertise etc. that all buys
[1]

@_date: 2014-07-22 20:46:26
@_author: Stephen Farrell 
@_subject: [Cryptography] The role of the IETF in security of the 
the net?
That's rhetorical rubbish frankly. But you're as free to
participate as to not and we'll see how (un)successful
TCPinc ends up. In my experience its pretty hard to predict
the future anyway.
And FWIW I also disagree with your assertions about TLS etc.
but its not really worth a blow-by-blow disagreement. There
is clearly an element of truth in the assertion that things
like the IETF are less good at inventing stuff, and are
better at improving and maintaining already-good things (when
stuff goes well, which it doesn't always). Its fine that we
disagree about that but you're exaggerating IMO.

@_date: 2014-06-15 14:37:53
@_author: Stephen Farrell 
@_subject: [Cryptography] basing conclusions on facts (was: Re: [cryptography] 
I've no public opinion on Certicom's patent practices. And the
behaviour of the signals intelligence agencies has been IMO
deplorable. So I sympathise with some of what you are saying.
However, building your case on bogus claims that are not facts
as you are pearly doing is a really bad idea. In particular...
That is not correct as far as I can see. In my local archives,
I see one email from him to the TLS list in 2011 and none in
2012. For the security area list (saag), I see a smattering
of mails in 2011 and 2012 and none in 2013. For the IRTF's
CFRG, I see a few in 2010, none in 2011 and some in 2012 and
2013. I do see increased participation over the last year on
the the DUAL-EC topic.
None of the above is anywhere near "highly active" which is
therefore simply false.
And I don't believe you yourself are sufficiently active to
judge whether or not someone else is "highly active" in the
IETF to be honest. Nor do you seem to have gone through the
mail list archives to check.
You are both of course welcome to become highly active if you
do want to participate, same as anyone else.
And that supposed conclusion, based only on an incorrect claim,
is utter nonsense. I would have expected better logic and closer
adherence to the facts.
Yes, the IETF security area needs to do better, and quite a few
folks are working on that. Yes, its almost certain the someone
was paid by BULLRUN to muck up IETF work. Nonetheless unfounded
misstatements such as the above don't help and are wrong. And
the correct reaction is to do better work and not to fall for
the same guily-by-association fallacy that the leads the spooks
to think that pervasive monitoring is a good plan.

@_date: 2014-06-15 17:48:19
@_author: Stephen Farrell 
@_subject: [Cryptography] [cryptography] basing conclusions on facts 
You are completely wrong. But you'll believe that or not.
I am mainly pointing out that guilt-by-association is just as
wrong if done by the Internet community as if done by/for NSA/GCHQ.
And that by not bothering to pay attention to the evidence in
this case (which is all visible in many mail archives), iang
is doing his case, and IMO the good of the Internet, a disservice.
I have no interest in defending Certicom's IPR practices.

@_date: 2014-06-15 19:29:08
@_author: Stephen Farrell 
@_subject: [Cryptography] basing conclusions on facts 
Thanks for that refreshing approach!
I appreciate it,

@_date: 2014-06-25 16:06:45
@_author: Stephen Farrell 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
Just two clarifications...
s/Algorithms/protocols/ in the above I think.
There's no claim to date about consensus on the tcpcrypt list.
There is a general consensus in the IETF that algorithm agility
in protocols is desirable. And s/algorithms/protocols/ again:-)

@_date: 2014-06-25 23:53:14
@_author: Stephen Farrell 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
Absolutist? Hrmpf. (I didn't type Hrmpf at first;-)
I'd also be interested if this discussion landed somewhere else
sensible, but I'd be surprised.
Please do note though that:
- there's the backup algs argument that the protocol has to have
codepoints defined and interop working now for the next one we
may need to deploy in some years e.g. its taking some work to get
the fine detail of how to do 25519 (and ed25519) key representation
in TLS now, and that has to happen well before being needed for a
backup alg; you just cannot do that and get interop in the time it
takes to upgrade a single implementation and assuming all future
alg breaks will be slow burners is not considered good engineering
by many
- there are national algs in the world, and its not easy to say
no to all of those in all cases for all implementers and deployments
(I wish it were)
- sometimes you have platform issues, e.g. lots of layer 2 h/w does
AES CCM, whereas we generally prefer AES GCM in many cases. Both
are reasonable decisions made in different places (IEEE and IETF
for example) so protocols on the boundaries (e.g. CoAP) will need
to support one or both of those in different deployments; the same
thing can happen for timing reasons e.g. if a highly popular dev
environment just doesn't support the perfect ciphersuite but has
one nearly as good and no sign of an update coming
I do however agree that once you start down that road you quickly
end up in 300+ ciphersuite hell, so yes alg agility is a pain, but
maybe an unavoidable one.
I could however see an argument that only 1 bit be allocated for
algorithm/ciphersuite identifiers though that doesn't work so well
for the platform issue, it might be good enough for the other two.

@_date: 2014-03-10 23:06:33
@_author: Stephen Farrell 
@_subject: [Cryptography] RC4 again (actual security, 
I wish that were clearly true; s/all of/some of/ is more correct
as I understand it. A few browser folk are actually arguing to aim
for much more https and less http, which agrees with Ian's post,
but without afaik any evidence that content authors will move at
all, which'd seem necessary to me. That position just puzzles me.
Personally, I'd love to see more http URIs accessed via TLS and
am arguing for that. If some of you here agree with that and have
ways to get that message to your fav. browser folks doing so
might be useful. (A set of folks swamping an IETF list never
having contributed before is not a good way to do that btw.)
Separately, there's discussion in UTA about a bunch of ways of
doing opportunistic-foo and also of how to best deprecate RC4.
Any btw, PKIX is deceased.

@_date: 2014-03-15 22:34:34
@_author: Stephen Farrell 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
Too many questions rolled into one for me.
(Being v. active in the IETF) I do trust that its making real
attempts to do as well as possible, imperfect and all as the
outcomes always are. Any process that produces so much would
be equally imperfect according to pretty much any process I
figure, but we don't need to go there.
I do not personally trust that Governments will ever stop
pervasive monitoring so long as they have the capability. That
holds for me no matter what they say, or the various relevant
legislatures say. I do nonetheless think its worth (other:-)
folks time trying to campaign for legal changes etc.
But for me I conclude that trying to make as much of the
technology that gets used have as much resistance against
this attack is the approach to try, while not aiming for
perfection, and without claiming that other approaches are
that wrong.
I figure folkk

@_date: 2014-03-24 23:46:30
@_author: Stephen Farrell 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
I agree with the above.
It'd also be a fine thing if such a history tracked the length
of time it takes to get rid of algorithms from whenever they
are considered dubious or not-good-enough-for-new-stuff. But
that's just me asking for someone else to do more work so I
don't expect it to happen.

@_date: 2014-03-25 23:32:03
@_author: Stephen Farrell 
@_subject: [Cryptography] The role of the IETF in security of the 
the net?
Its a shame that people do not (or will not?) take in
that anyone can write an internet-draft about anything
and post their draft.
The one to which you refer has no status whatsoever and
is just one set of authors ideas, as has been pointed
out before on this list I believe. Insinuations to the
contrary are afaik baseless.

@_date: 2014-03-25 23:55:14
@_author: Stephen Farrell 
@_subject: [Cryptography] Fwd: New Non-WG Mailing List: Tcpcrypt -- Discussion 
FYI, while folks are on about IETF and crypto at many
layers. Jump in if interested, discussion on the list
hasn't even started yet.
-------- Original Message --------
Reply-To: ietf at ietf.org
A new IETF non-working group email list has been created.
List address: tcpcrypt at ietf.org
Archive: To subscribe: The goal of this mailing list is to discuss encryption of TCP
sessions, without necessarily requiring endpoint authentication
in all cases (but while also making endpoint authentication
possible). The initial purpose of the mailing list is to discuss the
scope and a potential charter of a WG that would work
on the definition of TCP extensions to support such encryption,
or to find an existing WG to perform the work.
For additional information, please contact the list administrators.

@_date: 2014-03-26 11:45:58
@_author: Stephen Farrell 
@_subject: [Cryptography] The role of the IETF in security of the 
the net?
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Go for it then. While you might be tilting at a windmill,
that's ok too sometimes I figure, and I've done that myself
for what's probably v. similar to one of the moving parts
of your proposal. [1]
The IETF doesn't vote and doesn't have members. People participate
mainly via email same as here, but with a load of bureaucracy that's
unfortunately needed to get to and establish a rough consensus that's
written down in an RFC when there're a few thousand voices that can
in principle be involved in anything.
WRT barons, kings, henchmen and fear: if you're able to make
sound technical arguments and figure out how to get stuff done
the IETF is a place where you can get stuff done. It does take
work though, and not everyone wants to devote their time to
that kind of thing, which is just fine. (ISTM there are some
folks who never manage to figure out how to get stuff done
despite trying, whilst others grok that really quickly. People
are just different I guess.)
And have the times changed? Yes, in the IETF its looking like
they may have, but we won't know for sure for some time. As one
example, in London this month, we had a productive discussion
about adding some form of confidentiality for DNS queries. [2]
There's a mailing list [3] for that, and that list might well
figure out something useful in that space that could be deployed.
Two years ago that wouldn't even have gotten onto an agenda I'd
say, as privacy or confidentiality were considered non-goals
for DNS in general. There are more examples like that too.
While some skepticism is always warranted of course, I do
think things are clearly moving the right direction at the
moment. (But then I would, wouldn't I:-)
[1] [2] [3]

@_date: 2014-05-31 14:58:34
@_author: Stephen Farrell 
@_subject: [Cryptography] FW: RFC 7253 on The OCB Authenticated-Encryption 
This RFC is an IRTF stream RFC. The IRTF follows the IETF's IPR
policies, which are set out in various RFCs/BCPs listed at [1].
The basic idea though is that if you personally know of e.g. a
patent (filing), then you need to declare and then the relevant
IETF WG or whatever can decide what to do. While there is a
preference for things that are not known to be encumbered, in this
world you can't be sure about that. If interested, or you want
to comment on this, there's a list [2] which is discussing
clarifications/twesks to these policies. And if you are doing
stuff in the IETF/IRTF you need to go read the actual RFCs/BCPs
yourself, i.e. the above is not the policy but a pointer to the
policy (yes, a lawyer said that to me once;-)
[1] [2]

@_date: 2014-11-18 10:06:26
@_author: Stephen Farrell 
@_subject: [Cryptography] IAB Statement on Internet Confidentiality 
Both HTTP/2 and TLS1.3 are looking at including traffic
padding mechanisms. I've not checked the latest drafts
for those but I think they should allow implementations
in future to do better at this issue.

@_date: 2014-10-16 19:32:58
@_author: Stephen Farrell 
@_subject: [Cryptography] DSAuth: public key authentication for the web 
Be interested in how that compares to [1] which
is (cross fingers) almost done processing in the
IETF. See also [2] for an implementation.
[1] [2]

@_date: 2014-10-22 14:28:15
@_author: Stephen Farrell 
@_subject: [Cryptography] Simon, Speck and ISO 
Hash: SHA1
Not that I know of. Sometimes people do come to e.g. the IETF
and say "but ISO standardised our alg, why won't you?" That
isn't treated as very meaningful though as anything relatively
credible can afaik get through the ISO process with not that
much effort, if backed by a nation-state that participates
in SC27. (That said, I think SC27 has some capable folks
involved, but a pretty small number of 'em probably.)
It may well be the case that some other national, or nation-state
oriented, standards bodies prefer algorithms that SC27 have ok'd,
or even that some of those standards might not be voluntary in
some places for some things. That'd be a crappy idea really but
could happen I guess. The crappiness there though would be as much
to do with mandatory vs. voluntary standards as it is with
potential lack of broad review of crypto.
Personally, I'd weigh the "was it published at crypto more than
5 years ago with a history of papers since" smell-test as being
a more important factor than that something was standardised by
SC27. (But with neither by itself being sufficient.)

@_date: 2014-10-23 11:45:52
@_author: Stephen Farrell 
@_subject: [Cryptography] Simon, Speck and ISO 
As of now, it looks like IETF protocols will be adopting chacha20
with poly1305 as per [1]. I believe that is being implemented in
a number of prominent code bases. (I didn't go check, but you can
already see such ciphersuites popping up in TLS stats even before
there's an RFC.) If ISO want to do something, that'd seem like a
better plan to me.
But please also ask 'em not to futz around and end up with
something "almost" interoperable;-) Actually, it'd be better that
they did nothing at all if that outcome were likely.
FWIW, I've heard of no equivalent implementer interest in the new
NSA algs. Not even a squeak. (But the US govt market is probably
big enough that they may get fairly widely implemented I suppose.)
[1]

@_date: 2014-09-01 12:54:59
@_author: Stephen Farrell 
@_subject: [Cryptography] Encryption opinion 
That would be considered out of scope as "browser chrome" is not
a protocol concept but is an implementation issue.
I do get how that is annoying, but I don't see how to re-scope
the IETF (or HTTP) in a useful way that'd put that problem in
As an aside, when I've asked browser implementers this question
they have a bunch of reasons why they think authenticating the
client or user in HTTP isn't a good plan. I don't myself agree
with all of those (e.g. [1] seems reasonable to me:-) but some
of their reasons are good ones, for example that sites prefer
to control the login UI (e.g. for I18N and workflow reasons)
and and not have that determined by the browser chrome.
[1]

@_date: 2014-09-21 14:25:51
@_author: Stephen Farrell 
@_subject: [Cryptography] new wiretap resistance in iOS 8? 
That seems like the logical next step beyond what device vendors
and network operators are reportedly doing now. Presumably they're
doing what they've done so far as they conclude they'll make more
money if their customers are more confident in using their devices
and services.
So I guess a question is whether or not additionally opening up
their protocols and implementations would seem worth it to such
a commercial entity. TBH, while I do think such openness is the
right thing, I'm not sure how the cost/benefit analysis would
come out there, from the POV of a commercial entity.
I wonder if someone has already done work on such an analysis
for crypto-related security/privacy functions taking into account
snowdonia? I don't recall seeing such, but wouldn't be surprised
if it existed.

@_date: 2014-09-24 12:31:51
@_author: Stephen Farrell 
@_subject: [Cryptography] NSA versus DES etc.... was: iOS 8 
Hi Rich,
I get that you're paraphrasing there, but I don't believe that
argument stands up. I do get that it has an (IMO emotional) appeal
to those who've been directly or indirectly affected by the kind
of event you mentioned, but I think we should counter such
arguments, esp. when they're catchy and quotable.
Prevention very much depends on the job. I can (sufficiently well)
prevent my kids from accessing what I consider undesirable content
from my home in various non-intrusive ways, for example threatening
various bad consequences if they're found to do the wrong thing. You
don't need to fully agree with me there and can substitute any
example you like that doesn't require pervasive monitoring. So,
there are plenty of specific bad things we can prevent (sufficiently
well) without pervasive monitoring.
So, ISTM that only if you define the "something" that needs to be
prevented nebulously enough (e.g. "all bad things", or "all spam"
or "all malware" or "all abuse") do you approach an argument that
you need to intrusively watch lots of things. And of course the
problem there is that preventing all bad things is a ridiculous
goal so the argument falls on that basis.
So a) there are plenty of bad things one can prevent without
pervasive monitoring and b) there are many things that cannot
be prevented and pervasive monitoring is about as useless as
other mitigations for those, and c) I don't think I've seen any
"something to prevent" that is well defined, really could be
prevented and that demonstrably requires pervasive monitoring.
Someone might respond that "we have the evidence, but we can't
tell you, just trust us" and that could be true or false (and
there's no way to know) but definitely does not justify the
pervasive monitoring attack for a few reasons, not least of
which is that for all possible sets of "we" there are many who
simply cannot and probably ought not trust that set of folks.
Someone else might finally say, yes, we know pervasive
monitoring isn't really needed or effective, but (my) society
makes the unreasonable demand that we prevent all bad things
so what can we do? Well, if one accepts that argument then
one should also get rid of all speed limits on roads, as
drivers everywhere demand to go faster than speed limits. It
ought be clear that pandering to such unreasonable wishes,
no matter how popular, is not good government.
So no, I at least don't agree with Geer's argument, no matter
that it was well written.

@_date: 2014-09-24 22:48:05
@_author: Stephen Farrell 
@_subject: [Cryptography] NSA versus DES etc.... was: iOS 8 
No, I don't believe that, but I do think we should question/contradict
arguments that damage crypto or the Internet and that depend on the
fallacy you mention above.

@_date: 2015-08-02 12:33:23
@_author: Stephen Farrell 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
Also trying to keep away from specifics of any one protocol.
In general you assume that the attacker (who I agree exists) is active
as part of the process. There's no way to know the probability of
that. I do know that people have the ability and propensity to disagree
with one another for all sorts of reasons that are nothing to do with
the posited attacker. Perhaps especially the kind of people who
currently dominate discussions about new Internet protocols. And even
more especially in fully open environments where anyone can try to
participate. And since the new work represents change, and for some
folks, significant change, it's entirely likely that genuine
differences of opinion will exist even without any action from the
There is also the fact that any rough consensus process has to be
run by fallible humans. Not everyone is good at herding cats so that
the cats agree they have arrived at rough consensus. So in addition
to genuine technical disagreement one also has to take into account
the chances of accidental mis-management. IMO, that probability is
also quite high - not every engineer ends up being good at cat
herding sadly;-)
Lastly, given there are a whole bunch of proposals and bits of work
being done in parallel, it's entirely to be expected that at least
one of those gets stuck because of some process-stupidity.
All of the above argues that we need be very realistic and quite
well informed to arrive at a realistic evaluation of whether or not
they might be an active attack being attempted against a specific
To move to slightly more specifics, you mention rough consensus so
I assume you're talking mainly about the IETF, since the IETF is
afaik the only set of folks that use "rough consensus" as a term of
In the case of the many bits of good work that are being done to improve
security and privacy in the IETF, I do think it's quite likely
that some but not all people working for signals intelligence agencies,
and/or companies who work with them, do disagree with some of that
work. Some of that disagreement is openly expressed I'm sure and
that's just fine - we can handle openly expressed technical
disagreement fairly easily, if not perfectly.
Since there are only a tiny number of direct employees of signals
intelligence agencies who participate in the IETF, and those folks
are generally not trying to game the system in obvious ways, (I
think I would notice if they were, 'cause yes I look out for it:-),
I think we can ignore them here. Three are however a lot of esp.
large companies who work with/for signals intelligence agencies
and who do participate in the IETF, so I'll focus on those since
any sensible attack would be done via a player like that.
In any such case, my experience is that perceived commercial advantage
(which may be long term) is what causes such participants to try
to game the system. And indeed working with signals intelligence
agencies is presumably profitable, so there is the potential for
this attack. (One can argue that individuals within such enterprises
may be used in an attack by leveraging their inflated egos etc,
and that's true, but is indistinguishable from other personality
related reasons to disagree so is covered above I think.)
The remaining question then is whether or not people from commercial
enterprises are, in addition to openly participating as expected,
attempting to manipulate the open process to their own commercial
advantage. And the answer is yes, of course they are, as always. But
is that only because of the signals intelligence agencies? No it is
not. For any of the relevant players, which includes basically all
large companies, they have many more interests in play and it's not
possible to disentangle those from the outside. (Or even from inside
sometimes I bet:-)
So it's impossible to tell what has motivated any particular bit of
process gaming, and it's mostly silly to bother asking. That's just a
part of operating in the big bad world, once you get beyond the playing-
with-friends stage of any project you can't worry about the motivations
of all participants. (You can decide if specific folks are worth
worrying more about and pay more attention to technically examining
their inputs, that's IMO fine, and I do that, but not based on current
employer, rather based on a pattern of contributions.)
Basically, we can describe what we consider good behaviour but we need
to recognise that clever people will figure out ways to try to game any
system for reasons we can't know, so worrying about all motivations is
counter-productive, we need to examine visible actions and not worry
about the unknowable.
IMO that would not be a defensive move. That represents surrender.
And not only of the rough consensus approach. To have any effect you
would also have to surrender openness and decide who to allow into
your secret cabal. That kind of cabal doesn't scale IMO. (The outputs
of such cabals can be and often are useful inputs to the open rough
consensus process that the IETF uses, so secret cabals are not all
With your surrender proposal, the putative attacker wins as there
would no longer be an organised way to produce new protocols that are
likely to fairly quickly see widespread deployment. In that case
instead of heading towards 99% deployment in some reasonable timeframe,
it's much more likely that 2% is a potential high-water mark. There may
be exceptions but those will be exceptions. (And yes, we're both
pulling numbers from the air.)
I do agree that over time things like the IETF will evolve or perish
and the concepts of rough consensus and openness ought be part of
that evolution. (For example, the IETF IMO needs to figure out how
to consider the views of users who are not engineers, but I don't
know how to do that today.)
So one can sensibly work towards a world where things like the IETF
evolve more to one's liking, or one can sensibly work to try create
something else to fill the niche currently filled by e.g. the IETF.
But simply suggesting dropping rough consensus is nonsense, any
credible attempt to avoid the posited attack would need to also
say how you'd effectively replace the whole of the IETF if you're
not proposing some feasible evolution within the IETF.
Yes. Where you think it sufficiently important you should participate
in the rough consensus process with sound technical argument about the
technical proposals made. That is IMO far more effective as a way to
counter the attacker, compared to surrender.
And if you get really exercised about all this and are a masochist
you can participate in trying to evolve things like IETF processes.
That's not for everyone by any means, and is a recipe for learning to
deal with frustration, but is the kind of worthy-but-tedious stuff
that does actually need to be done to improve the rough consensus
open approach to improving the Internet.
And for things where you can't participate (time available being
finite), to the extent you can, refuse to use the outputs of the
process that you don't like or doubt, and tell other folks (the
technically sounds reasons) why and recommend they do likewise. (I.e.
sure, go ahead and make noise about the actual bad features of outputs
from  the process.) That can produce change in those who do participate
in the parts of the work that you can't get to helping with or that
you don't care that much about.
Your argument is ill-informed and incomplete and your conclusion is
erroneous. (That's my thought anyway:-)

@_date: 2015-08-02 20:09:20
@_author: Stephen Farrell 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
Sigh. You are wrong to think of IETF working groups as "committees."
There are similarities but there are huge differences. I realise that
using that term serves your rhetoric as it conjures up images of
closed rooms full of staid 19th century gentlemen  but that is just
not a relevant way to think about an IETF working group.
Many of the O(100) IETF working group lists have hundreds of
subscribers, and dozens of active mailing list participants. And all
people (with an email address) are welcome to participate at any
time - the main requirement being to that one's contributions need
to be technically sound or they will be ignored. Those working groups
have no real membership and we no real voting (there being no
enumerable electorate) so many of the concepts associated with
committees (including by you below when you say "elected") are not
Yes, the reality is not perfect but the real imperfect dynamics are
just not those described by the (here pejorative) term committee.
And before one argues to discard a significant part of such a process,
especially on the basis of an invisible hand on the scales, I do think
one has a duty to at least accurately describe what one is arguing to
discard. And you have not done that.
That is another part of why I think your argument here is ill-informed.
So let me see, you argue that there's an attack that can always be
invisible, and that therefore we should surrender to that attacker.
I don't find that at all convincing.
(Separately, I never said economic "progress" - I said interests which
is just not the same:-)

@_date: 2015-08-02 21:58:15
@_author: Stephen Farrell 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
If you're implying Ian or I wanted to obfuscate something that's
nonsense. He chose to try to generalise, and I'm fine with that.
The alternative would be to repeat arguments that are currently
already being (pointlessly IMO) repeated on the relevant IETF list.
S/spent/wasted/ but yes. (And I don't mean wasted in terms of
did/didn't get what they want, I mean in terms of it being a
really really stupid way to mis-use money.)
I'd be surprised if some of that money wasn't mis-spent on trying
to muck up IETF work. And I ack'd that already.
My point is that Ian's supposed defence is surrender. I am not
trying to deny that there may be an attack. The point is though
that we will never know if any specific action is part of such
an attack and we therefore have to react via our normal processes
that aim to counter that and other kinds of gaming. We do have to
be more alert/vigilant for some aspects of what is proposed but
mostly we just need to run the processes well. (There are I'm sure
some improvements that can be suggested too, but that's again
not at all the same as Ian's surrender proposal.)
I don't agree with the above characterisation.
While some of the history of TLS hasn't been great, I doubt that's
down to this kind of attack.
I've no idea what you mean (that could be relevant).
Bad question and a wrong answer anyway. The question is bad because
it's people (or maybe programs written by people) who discover
vulnerabilities. Whether they chose to feed that information into the
IETF process in a usefully timely manner is a different question. As
is how well or badly the IETF process handles such input. And even
though it's a bad question, I think the example of DKG figuring
out issues with 0-RTT in the TLS1.3 proposals is a case that comes
close to providing a "yes, stuff works sometimes" answer to your bad
question. (Not that I'm yet very happy with the results so far on
that score:-)
If you think I said I was, you mis-read what I wrote or I wrote
My main point is that we ought treat this as another kind of gaming
the system and ensure that we handle it well, just as we have to
with other more purely commercially motivated attempts to game the
I doubt it. X.509 was part of X.500, all of which was similarly baroque,
as was X.400 at the time. And that all started back in the mid-1980's
too when using strong crypto was hard to impossible in most
applications. Seems pretty unlikely to me that X.509-complexity was
part of any such attack.
The dual-ec fiasco isn't a good model for a similar attack on a piece
of IETF work IMO. The setup there was much more vulnerable to capture
by just a few parties for many reasons. That problem affects the IETF
much less - it's still an issue but far less of an issue so long as
we have enough capable folks participating. I do agree that other
standards development organisations can be very vulnerable to that kind
of capture though. As are industry consortia and small-team projects.
The scale of the IETF is a PITA in many ways, but for this aspect it
My argument doesn't require one and I even acknowledged that starting
from the output of a small team is often a good way to end up with
better IETF output. The IETF isn't great at starting from a blank
sheet of paper, but is often good at improving various aspects of
small-team output.
Wrt "committee" see my earlier mail to the list.

@_date: 2015-08-02 22:36:15
@_author: Stephen Farrell 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
Strongly agree.
Also agree.
I disagree there. I think the attacker is probably more interested in
there being protocols for which turning on any security is hard. That
could be attempted by making some specific security protocol hard to
deploy, (*) but equally by making e.g. a protocol that requires that
every node have the ability to add/subtract/change PDUs. That way it's
hard to add any e2e security features, no matter how well designed
those are.
So I think it'd be as likely that lots of non-security-area WGs
would be targets. The latter might also be easier to influence, as
many participants could be commercially motivated to not want better
security and privacy as that has a cost.
I'm afraid you did suggest just that. The rough consensus thing and the
open-ness thing are inextricably intertwined and necessary for the IETF.
Take away one or both and you're no longer dealing with the IETF.
So while I'm interested in feasible ways to improve IETF process, I'm
not interested in surrender, but I said that already I guess:-)
(*) I think I'm on record as saying that the IETF has in the past
failed in developing security protocols that were too hard to deploy.
It could be that this attack was a part of the cause of that. But
my take is that perfectionism and inexperience with scale on the
part of security folks was a bigger factor. In any case I think
we're improving in that respect, but have a ways to go.

@_date: 2015-08-04 00:41:07
@_author: Stephen Farrell 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
That could work in some place but not in the IETF. (Although there are
timers and cutoffs involved in the nominal IETF process.)
In the IETF we have a theory, which is actually fairly well reflected
by practice, that any decision can be overturned by a sufficiently
compelling new fact. That has not infrequently resulted in work being
sent back to working groups at IETF last call time when a different
set of folks not involved in the working group get to describe their
views of the downsides of some thing or other.
I think overall the benefit of being fact-based regardless of how much
it buggers up progress is more significant than the potential for fixed
timings such as you've suggested to mitigate an action taken as part of
an invisible bullrun attack. (Once again, I assert that we need to not
try consider bullrun in isolation, but we need to try our best to
counter all methods of gaming the system without worrying about the
unkonwable details as to why someone may be gaming the system.)
That said, I do agree that there's usually a giant debate about what
are in fact the facts in most such situations, so YMMV in terms of
what reasonable folks can conclude on this point.

@_date: 2015-08-15 20:42:50
@_author: Stephen Farrell 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
Isn't that out of date? I think 2048 RSA is now more common
than 1024 bit RSA and ECDH is or has become more common than
RSA key transport. We had presentations on various measurements
at the saag session at the last IETF. [1,2,3]
Slide 4 of [3] says that 96% of TLS certs (seen in use I assume)
are "2KRSA" and that 70% of servers (web servers I think) now use
ECDH with p256.
[1] [2] [3]

@_date: 2015-01-03 20:09:27
@_author: Stephen Farrell 
@_subject: [Cryptography] New Encryption Standard of the Russian 
With how many folks did you make contact? I did see some traffic
here and one or two other places, but am not sure how broad a net
you cast.
I don't know how that'd pan out with the IETF's rough consensus
process (surveys being vulnerable to various effects) but thinking
about ways of reaching out is worthwhile yes. I'll feed that back.

@_date: 2015-01-27 23:34:43
@_author: Stephen Farrell 
@_subject: [Cryptography] traffic analysis -> let's write an RFC? 
If folks wanted to work on that RFC angle, I'd be delighted
to help out as I can.
I think that traffic analysis mitigation is the next big area
we need to start trying hard to work. So far, we've (IETF)
gotten general padding capabilities added to protocols (HTTP/2.0
for example, still in discussion for TLS1.3) on a case by
case basis, but we've not yet done anything systematic. I'd
love to see a WG chartered to try figure out how to most
effectively counter traffic analysis and then go write
protocol and/or BCPs as needed. (Speaking personally of
course, it'd require IETF consensus for that to happen
Pragmatically, it's very late for a BoF to happen at the
March IETF in Dallas. Deadline is Feb 6 for requests which
are mostly far more developed than one single email:-) But
if someone wanted to speak to the topic, we still have
space/time available in the agenda for the security area
meeting in Dallas. Ping me if you're interested and willing
and able to work on that.

@_date: 2015-05-25 21:55:25
@_author: Stephen Farrell 
@_subject: [Cryptography] open questions in secure protocol design? 
The former simply does not work as an engineering concept.
Performance counts. Systems with AES h/w come in two practical
flavours: CCM and GCM and those do not interoperate. There are
also systems without crypto h/w instructions that need to use
something in s/w such as chacha.
Those facts are IMO entirely sufficient to demonstrate that
the 1TCS concept doesn't work, except in an alternate universe
where you control all platforms.
The performance and security characteristics of platforms will
also vary over time, say if in a few years OCB ended up being
added to systems that now do either GCM or CCM and provided a
way to get interop between many more systems whilst using AES
h/w. That would be a good reason to start wanting to see OCB
I would really like to see us (IETF and more broadly) make
progress on limiting pointless ciphersuite proliferation but
the 1TCS concept is just a distraction that makes real progress
harder to achieve IMO.
We ought be concentrating on how to do the agility thing but
*much* better than we have to date (so including ways to get
rid of useless/unused cruft) but we should not be pursuing an
apparent ideal that is really a broken idea.

@_date: 2015-05-26 14:44:18
@_author: Stephen Farrell 
@_subject: [Cryptography] open questions in secure protocol design? 
Well yes and no. Yes, CT handles this differently from e.g. TLS and
that's fine. No, in that 1TCS is just a broken concept and hence is
not IMO part of any rational design space in the real world. 1TCS is
part of the rhetorical landscape but not a real design choice.

@_date: 2015-05-26 21:21:27
@_author: Stephen Farrell 
@_subject: [Cryptography] open questions in secure protocol design? 
For me the term "one true cipher suite" carries with it an
obvious implication that it's proponents consider every other
approach is wrong.
I am very much in favour of aiming to minimise the number of
ciphersuites needed in any given situation but the very concept
that there is one true and hence only one true way to do this is
If you want to re-factor your rhetoric to describe what you're
talking about as one design pattern amongst other equally valid
ones then yes I would agree it has some places it might work, but
that it definitely needs a better name as the current 1TCS term
is extremely misleading. Changing that badly chosen term would not
be bikeshedding and could be worthwhile. (Deciding what to replace
it with would of course involve bikeshedding so I won't go there:-)

@_date: 2015-05-28 19:41:44
@_author: Stephen Farrell 
@_subject: [Cryptography] open questions in secure protocol design? 
Design-by-committee is a soundbite that doesn't always apply.
It clearly does apply in some cases, but in others I think
the situation is just complex with a lot of interested folks
with slightly different needs. Life is just way simpler if
you only have to bother with what you want yourself;-)
TLS is more the latter than the former I think. Esp given
that the basic design was in fact the output of a new
Netscape folks if I recall correctly.
If you write that up in an Internet-draft I'd be willing to
try see if we could get IETF consensus for it. I'm not sure
we would, but I'd be willing to try.

@_date: 2015-06-01 01:19:17
@_author: Stephen Farrell 
@_subject: [Cryptography] open questions in secure protocol design? 
I wonder if an "at most two" protocol restriction could work.
Probably not though, even if we designed protocols with a 1
or 2 bit algorithm id field I bet implementers on the receiver
side would still try out old stuff to see if it works even
after we'd all agreed to deprecate the last-but-one old/crap

@_date: 2015-09-06 22:32:04
@_author: Stephen Farrell 
@_subject: [Cryptography] NSA looking for quantum-computing resistant 
That action doesn't require there to be any real issue with the NIST
curves though. If I were to construct a conspiratorial reason for NSA
to encourage a move to some yet-to-be-developed post-quantum crypto,
it'd be more along the lines that they whatever subset of them are in
the ascendency now conclude that they benefit if they discredit current
However, as we've all said, we've no real information. And what NSA or
any capable government merely assert can't be considered as trustworthy
information, which is entirely their own fault.
So, my conclusion is very similar to Peter's - post-quantum crypto seems
like a good research topic for now, no more.

@_date: 2016-04-12 01:16:55
@_author: Stephen Farrell 
@_subject: [Cryptography] At what point should people not use TLS? 
Yes, that's being done by various folks. Here's a way to help...
Part of the work is to analyse the impact of 0rtt replayable data
on various protocols. DKG did a fine job of that for DNS/TLS
(DPRIVE) [1]. I'd love to see additional inputs of that nature
so that our decisions about how to most safely describe what is
the (sadly;-) inevitable (foot-gun that is;-) 0rtt replayable
data in TLS1.3. As there are so many protocols that run over TLS,
it will really be valuable to see as many such analyses as we
can in the next few months. So please do consider helping out
in that way, esp. if you can contribute some analysis that is
less likely to be reproduced by others.
So if you have the time and ability, please pick something and
just go do the work and send a mail somewhere visible. In the
worst case, just ping me with a pointer to your analysis and
I'll ensure all of those that are worthwhile are brought to
relevant folks' attention.
PS: Yes, this mail is an invitation to take part in the process
of making imperfect things less imperfect. I don't apologise for
that, it is one of the many things that need doing, and not the
least important as TLS1.3 has many other features/aspects that are
quite a good bit better than using TLS1.2 or earlier. The fact
that we're faced with a tricky set of trade-offs between sticking
with earlier versions vs. getting the latest, with it's dangerous
implement included, is perhaps a sign that we're involved in an
effort that is maturing.

@_date: 2016-02-19 22:56:41
@_author: Stephen Farrell 
@_subject: [Cryptography] On the false choice between privacy and security 
Well said.
I'm not sure I'd 100% agree with "deranged" though, I think a part of it
is that law enforcement folks are scared to revisit the traditional set
of requirements that they've posed for interception since they know
that a rational analysis of those in today's network, done in the open,
would likely lead to very different conclusions as to what's ok and
what's not. Personally, I think such a requirements analysis could be
very interesting, but seems unlikely.

@_date: 2016-02-26 13:36:07
@_author: Stephen Farrell 
@_subject: [Cryptography] Response to "I don't have anything to hide" 
This comes up generally in relation to pervasive monitoring and
isn't at all specific to the apple case.
The response I use to "I've nothing to hide" is that everyone who
abides by their local laws will have something to hide from someone,
sometime. Most people seem to accept that, but it's not clear to me
if they really find it convincing.
For older IT literate folks, I've also found that this really does
work: If a large set of your emails from decades ago was leaked today,
would all of the language you then used be considered acceptable now?

@_date: 2016-02-27 02:18:01
@_author: Stephen Farrell 
@_subject: [Cryptography] From Nicaragua to Snowden - why no national 
Ignoring the pejorative "pander," I'd love if the IETF could ignore
the sets of national algorithms that exist, but we can't. There are
people who are forced to implement them. Fighting for better crypto
by trying to deny the allocation of code points for such algorithms
isn't a good plan IMO as it'd be a losing plan and very expensive in
terms of effort, whilst losing the battle.
I think likely the best we can do is to annotate cipher code points
(e.g. in IANA registries) as being "desirable" or "other" and to
discourage everyone from implementing "other." If we can come up
with an acceptable but disparaging term for "other" that'd be great
("crap" has been suggested but might not be effective).

@_date: 2016-02-27 17:04:47
@_author: Stephen Farrell 
@_subject: [Cryptography] From Nicaragua to Snowden - why no national 
There's that. But "national" is also no good as a label, as
AES is national and some national algs are less well vetted/known
than others.
And there's also the set of algs that we think are ok, but are
not yet needed, e.g. codepoints associated with Curve448 might
not be needed right now, but might move to "good" status later.
Deprecated also doesn't quite seem right for e.g. integer DH
where we might want to prefer ECDH even though integer DH is
still fine with sufficient length primes.
And then (for TLS) there's stuff like SRP where almost nobody
cares and we might not want that code in general libraries so
maybe that's also better not in the "good" category too.
Deprecated does match stuff like RC4 quite well of course.
Anyway, in the IETF the TLS WG have agreed to try to figure this
out for TLS ciphersuites. They've not started that work yet, as
they're busy with TLS1.3 but the bikeshedding will happen there
in the not too distant future.

@_date: 2016-02-28 23:49:46
@_author: Stephen Farrell 
@_subject: [Cryptography] From Nicaragua to Snowden - why no national 
Well most important about AES is that it's an algorithm in
which we still seem to have confidence and that's widely
deployed and available for use.
However, it still is a national standard and hence a national
No, that's at best an interesting phrase, but pretty meaningless.
Conflating conspiracy and ineptitude and openness like that is
not IMO a useful way to consider issues. (Even when there are
real conspiracies like BULLRUN.)
That one isn't even an interesting phrase, sorry:-)
My experience is that there are equivalently capable and highly
motivated folks involved in non-security, non-crypto controversies.
It is true that the spooks are funded with deception as a goal
though, which I agree makes a difference. However, the closest, and
actually very close, analogy here is patents - our best defences
against crap and sneaky patents are the same as our best defences
against BULLRUN, and all involve being more open.
That's not something that'd work at Internet-scale.
The "benevolent dictator" open-source project is a smaller beast where
diktat can work fine, but it breaks down once a large enough set of
folks get seriously engaged.
No experiment is needed for SRP. We know the properties of PAKEs
in general and TLS-SRP BTW is documented in RFC5054 from 2007 so
any experiments needed have happened I'd guess. The result is that
almost nobody cares.
That sentence seems backwards. I guess you mean that by running an
open process we handed NSA two more years by arsing around not really
picking between tcpcrypt and the folks who'd like to re-use TLS for
tcpinc. That's a fair criticism. OTOH, I think that cost (being open)
is one we are better off paying in the end regardless.
I really like tcpinc but the above oversells it. It'll be a fantastic
thing for folks running TCP applications that can't use TLS and where
your kernel supports tcpinc and both sides enable that. (Later, if all
that works, it might end up as a really good new default, but that'll
be down the road some.)
Also fair. The nearly 400 TLS ciphersuites thing we've ended up with
is a joke. Adding an "is-crap" (or whatever) attribute to almost all
of those is an imperfect improvement that is well worth making.

@_date: 2016-06-09 19:58:40
@_author: Stephen Farrell 
@_subject: [Cryptography] Blue Coat has been issued a MITM encryption 
In that case, yay for CT and for Rob! Whether the transparency is
due to CT or something else isn't such a major deal so long as we
get the efects. But thanks for clarifying the record.

@_date: 2016-03-06 23:07:44
@_author: Stephen Farrell 
@_subject: [Cryptography] DROWN attack on SSLv2 enabled servers 
Nonsense. DROWN is a contra-indicator for your favourite approach of
protocol versioning.
In fact, none of algorithm agility nor protocol versioning nor upgrade
nor the lack of upgrade are what caused DROWN. DROWN was caused by two
things: primarily 1) government mandated weak crypto, but also 2) not
removing old code because of inappropriately assigning priority to
interoperability above everything else.
Just because you constantly profess your dislike for that particular
hammer does not mean that there are no nails that it can whack well:-)
That is very true.
Even with s/w update, the various folks involved (which includes us on
here) have to decide to remove old code. And we have to fight against
stupid government/law-enforcement attempts to weaken security and
privacy via crap crypto and other bad ideas. Within the technical
community we have pretty good agreement about the latter. But sadly
we are far from on the same page as to the former.

@_date: 2016-03-24 18:36:01
@_author: Stephen Farrell 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
I agree with a lot of your analysis but not your title.
Monoculture is wrong. I really don't see AES-GCM going away while
there's h/w support. And nor will RSA until a lot of CAs have made
a lot of changes, or we figure out how to do better than X.509 in
the real world.

@_date: 2016-03-25 20:08:39
@_author: Stephen Farrell 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
The IETF is not doing that.
Peter's essay is misleading.
We are not heading for a monoculture.
AES, HMAC, RSA, ECDH and ECDSA are not going away for
It is true that the set of new algorithms being considered
recently is DJB-dominated. (Passwrord hashing is not his.)

@_date: 2016-05-29 13:55:04
@_author: Stephen Farrell 
@_subject: [Cryptography] Blue Coat has been issued a MITM encryption 
Yeah, two things strike me:
1 - yay for certificate transparency - CAs behaving oddly being spotted
    and outed is good
2 - what kind of "testing" would require symantec to issue a CA
    cert with path-len 0 and for symanetec to hold the private key? I
    can't figure anything that makes sense unless symantec were thinking
    of actively helping blue coat spoof web sites better, maybe at
    run-time, or on a case-by-case basis  - or am I missing something?

@_date: 2016-10-02 19:55:18
@_author: Stephen Farrell 
@_subject: [Cryptography] distrusted root CA: WoSign 
While I agree that Peter's rhetoric is a bit OTT, there is a real
issue reflected in the above - the lack of any voice for users of
browsers, web server developers and content authors is IMO a real
reason to be somewhat wary of CAB forum. I don't know that there
are any moves to improve that situation, though of course there may
What Viktor said.
That is a fine question. I've not seen any good answers myself in
the last 20 years which is a shame. I have seen many proposals for
things that are a little better than X.509-based PKI, but none of
them that were sufficiently better to displace the current, wildly
imperfect, X.509-based PKI.
I do think CT is an improvement though, and in the longer term may
point to other solutions involving large databases of public keys.
But I've yet to see one of those that might really take hold.

@_date: 2016-10-03 08:31:35
@_author: Stephen Farrell 
@_subject: [Cryptography] distrusted root CA: WoSign 
So yes, Mozilla have a public list and a process.
That's very far from covering the points Peter and I raised
about who has a voice inside CAB forum.
They could and that'd be an improvement. It'd still not be
a "fix" for the CAB forum though.
I guess Google could do similarly too. (I wonder why you
didn't mention Google - do they do something different or
follow the Mozilla process?)
He's still correct though:-) There's no need to diss DANE in this.
DANE's another attempt to improve things which may find a niche
where it does help. (SMTP/TLS in particular, but who knows maybe
back in the web later if something like [1] gets traction.)
   [1] Yeah, cert transparency, I'm guessing you're familiar with it:-)
My point is that CT is a "large DB of keys" improvement to X.509
based PKI. I suspect that may point to the possibility of future
solutions where relying parties each carry around a large DB of
public keys. But that's just me speculating, no more.

@_date: 2016-10-04 00:43:37
@_author: Stephen Farrell 
@_subject: [Cryptography] distrusted root CA: WoSign 
I'm biased, but though EFF do fantastic work, I don't think
they're that open in the sense most relevant here. That said,
I think either your or Rich's postulated futures would be
better than the status quo - which of those would be the
"betterest" is probably moot however.

@_date: 2016-10-04 01:58:24
@_author: Stephen Farrell 
@_subject: [Cryptography] distrusted root CA: WoSign 
Well, fwiw, I disagree. Vested interests, when they exist and have
money, will find a way to vest their interests. Being open to all
comers and accountable is a good (maybe best?) defence against that,
at the exact same time as that openness makes one very clearly
vulnerable to that exact same threat. (Less obviously vulnerable
is not in itself desirable I think.)
But that kind of openness doesn't require such venerable (though
still IMO fine) things as an IETF. A well setup open-source effort
can do just as well if there aren't too many interested parties.
(Once there are, it'd evolve to become an IETF-like thing anyway
given some time and the typical personality-types involved in
this kind of thing;-)
Yuk. I remember how the ABA and various electronic signature
laws made PKI even worse. I really don't think we want to add
more non-repudiation bits or similar. Adding lawyers for fun is
not any part of an answer here. Subtracting some may be a start.
But... we may be wandering off-topic for this list maybe.

@_date: 2016-10-11 20:59:06
@_author: Stephen Farrell 
@_subject: [Cryptography] 
=?utf-8?q?rapdoors=E2=80=9D_in_millions_of_crypto_keys=22?=
No. I believe this affects integer DH only. And not
2048-bit DH, the paper is about 1024 and progress
beyond that gets harder.
And nothing to do with RSA or elliptic curves at all.

@_date: 2016-09-12 00:27:31
@_author: Stephen Farrell 
@_subject: [Cryptography] Secure erasure 
Really? That seems awfully inaccurate to me. Single DES
does not require a nation state and passive attacks are
far less risky than active. I think you've let rhetoric
overly affect your words there, and in a way that could
cause harm.
I think it's a bad idea to encourage use of weak crypto.
If there are situations where we need to live with weak
crypto for some years, that's one thing, but claiming it
as a good is just a bad plan IMO not supported by risk

@_date: 2016-09-12 08:33:07
@_author: Stephen Farrell 
@_subject: [Cryptography] Secure erasure 
So I do think your message encouraged weaker crypto but the more
interesting part is perhaps why the weakest point, as in the simplest
attack to demonstrate, is not the only relevant argument.
Yes, a rational attacker will choose the weakest point, in the
sense of the most effective attack. For most non-nation-states
that will mean the most cost-effective attack. (Nation states
have a less cost-driven definition of effective;-) But some attackers
will consider the cost of a detected attack as high and so may
find a more passive attack more attractive. For example, a hoster
or ISP might either collude with someone else or be coerced into
doing so, and that can then become the most cost-effective attack.
At that point a des-cracker would be entirely usable and would not
(in contrast to your statement) require a nation-state. And that
argument generalises to weaker crypto and not just single-DES.
For many attack vectors, this doesn't matter, but it does matter
when considering weaker crypto, such as single-DES. So your point
about weakest points is relevant but not the only relevant argument.
I don't curate such lists, but perhaps deployments where mschap
is still usable and where such packets traverse networks that
allow sniffing packets? I'd be shocked but happy if that was now
a small number of deployments. I think we all know that it takes
many many years to get rid of crypto that used be, but is no longer,
considered ok, which is all the more reason to really be careful
to not encourage continued use of such, once there's an alternative
that's feasible. (Where there's no feasible alternative, that's
another discussion.)
All that said, I do fully agree that adding better crypto is not
a way to make systems magically secure. But encouraging weaker
crypto, when that's not the only option, is a way to make systems
less secure, which is what I objected to in your message.

@_date: 2017-12-04 21:10:29
@_author: Stephen Farrell 
@_subject: [Cryptography] So long to Startcom 
Personally, I'm a little bothered by that SPOF. I'd love
to see a 2nd LE-like entity that didn't charge for DV
certs, and that's operating for some definition of the
public-good, even if it were one that only operated at a
small proportion of the scale of LE.
With 90-day validities (a good idea in general) and with
automation combined with ignorance of the internals of
ACME or PKI in many cases (also good things), I think the
scramble to a paid operator could be damaging if LE had
some significant issues even if those were temporary.
And who knows, a bit of competition might be a good
thing for LE too in the long run.

@_date: 2017-02-02 02:21:27
@_author: Stephen Farrell 
@_subject: [Cryptography] Why is a short HMAC key zero-padded instead of 
I'll ask on cfrg and dispose of that accordingly. I'm not
sure the suggested fix is the right thing, nor what is done
in deployed code.
Thanks for bringing that up

@_date: 2017-02-03 18:30:34
@_author: Stephen Farrell 
@_subject: [Cryptography] Why is a short HMAC key zero-padded instead of 
I did that and concluded that "hold for document update" was
the right outcome so I've done that too.
[1]

@_date: 2017-01-05 11:53:49
@_author: Stephen Farrell 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
That's a really unfair characterisation and there is
no way in which you can somehow know what all of the
hundreds of people who read or participate on the TLS
list actually care about.
Your criticism that TLS and HTTP evolution may be too
influenced by the web has some merit, but you destroy
your own argument by including the nonsense above.
There are cases when your exaggeration and hyperbole
work just fine but in the above (and in your recent
"it's all djb crypto" postings), you're IMO wandering
too far from describing reality, and in ways that could
be damaging (hence this mail).
There are non-trivial issues to consider in terms of the
device and network capabilities that are needed to make
use of TLS and what to do when one cannot (e.g. see the
ongoing group-key discussion in the IETF's ACE working
group). The way to influence the handling of those is
to participate (as you do Peter) and not to discourage
participation via the kind of silly slur quoted above.

@_date: 2017-09-13 22:03:38
@_author: Stephen Farrell 
@_subject: [Cryptography] letsencrypt.org 
I also appreciate the variety of clients, so could use
the acme.sh one for openwwrt more easily than the main
certbot client.

@_date: 2018-04-14 20:46:09
@_author: Stephen Farrell 
@_subject: [Cryptography] Will We Ever Learn? 
That not a useful description in at least two ways: 1) CERTs were
started in response, and 2) almost nobody cared - the Internet
wasn't important to anywhere near "everyone" in 1988.
While I agree it'd be better if more people cared more, we should
argue based on accurate information.

@_date: 2018-12-02 00:23:10
@_author: Stephen Farrell 
@_subject: [Cryptography] [TLS] ETSI releases standards for enterprise 
Yes, it is a shame that ETSI's role in transport security
appears to be to stick their noses in the trough of cast-off
proposals that didn't garner IETF consensus due to insecurity.
I hope that that properly (i.e., negatively), influences
people's opinions of ETSI, and of any government or industry
body so easily open to capture. Put another way, the IETF's
imperfect but terrifically open process considered this for
more than a year, (once there were people who raised the
topic, no matter how ineptly) and concluded there was no
consensus to even start such work, whereas ETSI appear to
have picked up those droppings and started and finished their
"standardisation" "process" (ironic quoting intended:-) in
roughly the same amount of time an open process requires to
conclude that such proposals are rubbish.
I also hope the IETF aren't shy about enforcing copyright
on the name TLS. (Not that I understand copyright;-)

@_date: 2018-12-08 19:09:32
@_author: Stephen Farrell 
@_subject: [Cryptography] What if Responsible Encryption Back-Doors Were 
It's probably obvious but seems to get forgotten a lot
by all sides in this discussion, so...
There are O(200) governments in the world, yet all depend
on the same mathematics (no matter what they think in Oz:-)
and we all use the same Internet. Those are important reasons
why there is no such thing as a responsible back-door. Were
one to exist it'd have to work for all the entities involved
at once, (or be "irresponsible") and given that some of those
countries are adversaries...
In fact, my estimate is that there are O(10^5) different
LEA-like entities who'd perhaps like to use a back door if one
considers the number of separate fiefdoms within different
countries. So it's a lot worse than just countries.

@_date: 2018-01-06 03:34:37
@_author: Stephen Farrell 
@_subject: [Cryptography] Speculation re Intel HW cockup; 
I think this is the main point - ISTM that performance has been
seen as a way more important goal than security or privacy, and
the same is (IMO sadly) still true. (See TLS1.3 and 0rtt, albeit
that TLS1.3 is still an overall good thing.)
Given that difference in prioritisation, and the fact that many
designs were done before cache based side-channel attacks were
known, I don't think these attacks are much of a surprise really.
(They are of course excellent work though.)
More interesting for this list (or for me anyway:-) is whether
or not there are any implementation countermeasures open-source
s/w can take to better protect secret/private values if running
on a guest-OS alongside other potentially bad guests. Other than
obfuscating I'm not seeing many worthwhile avenues to explore,
but would love to hear if there are.

@_date: 2018-05-17 09:52:44
@_author: Stephen Farrell 
@_subject: [Cryptography] Attacks on PGP (and allegedly S/MIME) 
I have two reasons to disagree with the above.
1) I don't believe anyone can realistically process things they
need to be careful with differently unless they're forced to do
that (e.g. via regulation, or some technical feature of the thing
being dealt-with). I reckon everyone's drowning in so much mail
that that's not really feasible as "important, must encrypt" is
highly unlikely to be something a user would realise for a mail
just before sending.
2) I do get a lot of mail, don't render HTML etc. and can work
just fine. That's a teeny pain every now and then when I have to
save and use munpack, but that's <1/month I figure. I'm very
sure I ignore a lot of HTML-only encoded attachment crap, but
I don't seem to feel any sadness resulting:-)
So I guess we disagree - in my experience "what's important" isn't
easy to treat specially but "what's broken that needs handling" is
more tractable. If I'm right then encrypting doesn't make this
situation worse for the reasons stated. (Encrypting by default,
more does clearly increase the risk related to other aspects of
these bugs.)

@_date: 2019-08-15 00:03:21
@_author: Stephen Farrell 
@_subject: [Cryptography] Well, that only took ten years 
I concur that EV was muck, but...
Not quite a deckchair re-arrangement, but I'd love to
see an 2nd independent instance of what LE do, capable
of handling about 10% of the same volume, and of ramping
up, just in case.
Hundreds of CAs was wrong. Approximately one sensible
CA also seems wrong to me.
If we did have an LE-alternate that had no effect (at
the moment) that'd be just fine:-)

@_date: 2019-02-12 11:52:05
@_author: Stephen Farrell 
@_subject: [Cryptography] [openpgp] AS2+OpenPGP protocol extension review 
I just had a quick peek (will try read more later) but wondered
why PGP is a better primitive here than MLS? [1] (Or one of the
IM security schemes that motivated MLS, if dealing with a work-
in-progress like MLS is problematic.)
[1]

@_date: 2020-02-06 20:18:27
@_author: Stephen Farrell 
@_subject: [Cryptography] SSL Certificates are expiring... 
Well, it kinda is how it was supposed to be though:-)
[1] says:
"To indicate that a certificate has no well-defined
expiration date, the notAfter SHOULD be assigned the
GeneralizedTime value of 99991231235959Z."
We added that for just this kind of reason.
But I also agree with you that X.509 based PKI doesn't
match the needs of lots of applications, but nonetheless
gets used by them, due to the lack of an alternative.
ISTM there's a chance to fix that with the upcoming
transition-to/integration-of PQ algs. Sadly though, a
whole bunch of people seem to still want to keep using
x.509 even then;-(
[1]

@_date: 2020-02-12 22:55:28
@_author: Stephen Farrell 
@_subject: [Cryptography]  
=?utf-8?b?ZSBjZW50dXJ54oCZIg==?=
Ahem. Cryptech [1] is an open source hardware crypto
project started in large part as a result of some of
the Snowden documents.
There may be a name collision but I can personally
assure you that [1] is not Swiss, not a company, and
not AFAIK borked. Happy to dig out pointers to any
relevant bit of code for whatever feature may be of
interest. (Would have to ask as I'm not one of the
developers, I just help 'em raise funds when I can.)
[1]
