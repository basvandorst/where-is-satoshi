
@_date: 2002-04-21 19:37:32
@_author: Nicko van Someren 
@_subject: objectivity and factoring analysis 
I'm inclined to agree with you there
Since you mention it, I will make a comment for the purpose of
clearing up a number of misunderstandings about what I said
and the context in which the comments were made.
At the Financial Cryptography 2002 conference a small and
impromptu panel was convened to discuss the Bernstein
proposal.  Since I'm in the business of building hardware I
was asked to comment on the cost of building some of the
hardware described therein.  I limited myself to comments
about the design for the engine that could be used to take
the results of the sieve process and compute values leading
to a pair of roots, and furthermore prefaced and qualified
my comments with strong statements about any numbers being
very rough back of an envelope calculations.  The estimate
of the cost of construction I gave was "some hundreds of
millions of dollars", a figure by which I still stand.
I was then asked how fast this machine would run and I tried
to do the calculation on the spot without a copy of the
proposal to hand, and came up with a figure on the order
of a second based on very conservative hardware design.
This figure is *wildly* erroneous as a result of both not
having the paper to hand and also not even having an
envelope on the back of which I could scratch notes.  The
number was based on a miscalculation of the number of
clock ticks the circuit would need by a factor of 10^11,
which is a vast error that is only slightly moderated by
the fact that on further analysis I concluded that the
hardware could be made to operate on a clock that ran
between 10^3 and 10^4 times faster since the inter-circuit
communication did not need to be as slow as I had originally
thought.  Thus I think that with care a matrix reduction
machine of the sort described could be built to run in a few
weeks or months for 1024 bit keys.
Despite all the qualifying of these statements I felt then,
and still feel now, that if you have keys that you think
rich governments might genuinely be interested in then you
should use ones that are longer than 1024 bits.  If you have
personal information that you want to keep secret from rich
governments for many years to come then you should probably
use longer keys anyway.  After all we can expect on past form
that the security agencies are some years ahead of the
academic state of the art in this field anyway.  On the
other hand if you are moving general commercial data around
on an everyday basis I don't think that there is much wrong
with 1024 bits keys and I would not, and have not, suggested
that there is anything insecure about such key lengths for,
say, electronic banking or e-commerce using SSL.
I have to say that Adam's suggestion that there was some
sort of ulterior motive for my comments is both disingenuous
and somewhat insulting.  In the context of short and impromptu
discussion on a topic which people felt was a live one, I made
a back of the envelope calculation in which I used the "time"
figure as the "cost" figure and came up with totally the wrong
number.  I wasn't expecting that this was going to then be
used as the basis for anything other than maybe an excuse for
looking into the problem a little more deeply.  The critical
problem here seems to be that Lucky then quoted this number on
a mailing list before I'd had a chance to look more closely.
I don't think that there was any conflict of interest involved
at all given both the nature of the discussion and the fact
that these days cryptographic acceleration is pretty peripheral
to the nature of my business.
I agree entirely.  The field of factoring is still moving forward
and with luck Bernstien's proposal will lead to him getting funding,
which will lead to new advances, and this my yet necessitate longer
keys in the future.  The biggest cause for concern with regards to
factoring at the moment is that 17% of the SSL servers on the net
are still using 512 bit keys and in countries like France (which have
in the past had domestic usage controls) the figure is more like 40%.
My research student last winter showed that 512 bit keys can be
factored in a matter of weeks using only the hardware found in a
busy 70 person office.  People tend not to change over existing
keys for longer ones as time goes by and now that 512 bit keys are
low hanging fruit people's inertia and resistance to altering working
server configurations is in danger of becoming a real problem.
I think your suggestion of bias is itself misplaced.  Most of
the comments on this that I have seen have been pretty measured.
Even if your cynicism is well founded in some cases I think that
the real problem here has been people being too swift to quote
and too slow to check with the source.

@_date: 2002-04-22 09:50:32
@_author: Nicko van Someren 
@_subject: objectivity and factoring analysis 
You'd need a large abacus and a vary large stack of paper...
If you had read the Bernstein proposal in detail you would
understand that (among other things) it details the conceptual
design of a machine for computing kernels in a large, sparse
matrix.  The design talks of the number of functional units and
the nature of the communication between these units.  What I
set out to do was look at how complex those units are and how
hard it would be to connect them and to place a cost on this.
I'm sorry, but I don't think at infinite speed.  I started
this process after lunch and the panel session started at
1:30pm.  I did say that this was an impromptu session.
See my comments above.  The costing was based on transistor count
and engineering costs.  The design suggested in the Bernstein
proposal does not have a simple size/time trade-off since the size
of the system is proscribed by the algorithm.
I used the number 10^9 for the factor base size (compared to about
6*10^6 for the break of the 512 bit challenge) and 10^11 for the
weight of the matrix (compared to about 4*10^8 for RSA512).  Again
these were guesses and they certainly could be out by an order of
OK, here are the numbers I used.  Again I preface this all with
it being order of magnitude estimates, not engineering results.
It's based on a proposal, not a results paper, and there are
doubtless numerous engineering details that will make the whole
thing more interesting.
The matrix reduction cells are pretty simple and my guess was
that we could build the cells plus inter-cell communication
in about 1000 transistors.  I felt that, for a first order guess,
we could ignore the transistors in the edge drivers since for a
chip with N cells there are only order N^(1/2) edge drivers.
Thus I guessed 10^14 transistors which might fit onto about 10^7
chips which in volume (if you own the fabrication facility) cost
about $10 each, or about $10^8 for the chips.  Based on past work
in estimating the cost of large systems I then multiplied this
by three or four to get a build cost.
As far at the speed goes, this machine can compute a dot product
in about 10^6 cycles.  Initially I thought that the board to
board communication would be slow and we might only have a 1MHz
clock for the long haul communication, but I messed up the total
time and got that out as a 1 second matrix reduction.  In fact to
compute a kernel takes about 10^11 times longer.  Fortunately it
turns out that you can drive from board to board probably at a
few GHz or better (using GMII type interfaces from back planes
of network switches).  If we can get this up to 10GHz (we do have
lots to spend on R&D here) we should be able to find a kernel in
somewhere around 10^7 seconds, which is 16 weeks or 4 months.
There are, of course, a number of other engineering issues here.
One would want to try to work out how to build this machine to
be tolerant of hardware faults since getting 10^7 chips to all
run faultlessly for months at a time is tough to say the least.
Secondly, we are going to need some neat power reduction
techniques in these chips to dissipate the huge power that it
needs, which will likely run to 10^8 watts or so so the facility
will look a bit like a steel foundry.
Lastly, I want to reiterate that these are just estimates.  I
give them here because you ask.  I don't expect them to be used
for the design of any real machines; much more research is
needed before that.  I do however think that they are rather
more accurate than my first estimates.
I hope this helps.

@_date: 2002-04-24 09:42:58
@_author: Nicko van Someren 
@_subject: Any info on this maybe improved matrix algebra for GNFS? 
there are some neat tricks in this implementation.  Ben Handley
from the University of Otago in New Zealand worked for us last
winter and spent some time on a fast implementation of the
block Lanczos algorithm with the speed critical sections done
in Itanium machine code.  This was one step towards the goal of
showing that 512 bit keys can be factored without having to use
a supercomputer but can in fact be factored using just what we
had in the office.  The technology is little different from the
stuff done to solve the last puzzle in The Codebook other than
the fact that few offices have quad-processor Compaq Alpha
servers but an increasingly large number have Win2K servers
with Itaniums in them.
and code as part of his university dissertation.  I don't know
if he reads this list so I will forward this message to him in
case he wants to add more.

@_date: 2002-12-09 15:10:05
@_author: Nicko van Someren 
@_subject: PGPfreeware 8.0: Not so good news for crypto newcomers 
I can also vouch for GPG on OSX and the GPGMail plug-in for the Apple Mail application works fairly well for GPG use, though not for key generation or administrative activities.  I moved over to PGP 8.0 when the beta came out and while I'm no novice with these things I greatly appreciate the slick user interface (and the disc encryption that's a few times faster than Apple's) so I am now running the PGP 8.0 Personal edition rather than GPGmail.
I think this comes down to a classic time/money tradeoff.  PGP 8.0 Personal edition is currently priced at $39.  Even as an experienced Unix and PGP user I think that the GUI on PGP 8.0 will save me an hour of effort over the lifetime of the product, which means it saves me money in the long run.

@_date: 2002-01-18 16:42:12
@_author: Nicko van Someren 
@_subject: Financial Cryptography 2002 
Financial Cryptography 2002
March 11-14, 2002
Southhampton, Bermuda
Call for Participation
Financial Cryptography is the only international conference dedicated
to the understanding cryptography and its relevance to all aspects of
the world of finance. The conference aims to bring together
cryptographers, technologists, businesses, bankers, lawyers and policy
makers to further the understanding of what can be done with
cryptography and what needs to be done for the world of finance.
Topics for the conference range from Anonymity to Authentication, from
Digital Cash to Digital Rights Management, from Legal and Regulatory
Issues to Loyalty Mechanisms and from Payments Systems to Privacy
issues.  The program is a combination of peer reviewed papers, panel
discussions and invited talks and the proceedings will be published in
the Springer-Verlag Lecture Notes in Computer Science series.
Registration for Financial Cryptography 2002 is now open; details and
online registration can be found at  along with
information about discounted hotel accommodation.
Financial Cryptography is organized by the International Financial
Cryptography Association (IFCA), a non-profit company dedicated to
the same goals as the conference.  More information can be had
from the IFCA web site@ or contacting the
conference general chair, Nicko van Someren, at fc02 at nicko.org or
by phone on +44 1223 723600.

@_date: 2002-01-30 17:34:58
@_author: Nicko van Someren 
@_subject: Financial Cryptography 2002: Discount rate ends 1st February 
**** Please note ****
The discount registration rate is only available until the
1st of February 2002.  After this date registration will be
changed at the full rate.
Financial Cryptography 2002
March 11-14, 2002
Southhampton, Bermuda
Call for Participation
Financial Cryptography is the only international conference dedicated
to the understanding cryptography and its relevance to all aspects of
the world of finance. The conference aims to bring together
cryptographers, technologists, businesses, bankers, lawyers and policy
makers to further the understanding of what can be done with
cryptography and what needs to be done for the world of finance.
Topics for the conference range from Anonymity to Authentication, from
Digital Cash to Digital Rights Management, from Legal and Regulatory
Issues to Loyalty Mechanisms and from Payments Systems to Privacy
issues.  The program is a combination of peer reviewed papers, panel
discussions and invited talks and the proceedings will be published in
the Springer-Verlag Lecture Notes in Computer Science series.
Registration for Financial Cryptography 2002 is now open; details and
online registration can be found at  along with
information about discounted hotel accommodation.
Financial Cryptography is organized by the International Financial
Cryptography Association (IFCA), a non-profit company dedicated to
the same goals as the conference.  More information can be had
from the IFCA web site@ or contacting the
conference general chair, Nicko van Someren, at fc02 at nicko.org or
by phone on +44 1223 723600.

@_date: 2002-10-17 19:55:51
@_author: Nicko van Someren 
@_subject: QuizID? 
[Note: I have an interest, since QuizID use nCipher hardware]
Their device has a neat way of synchronizing the sequence number to the server which both avoids the clock drift problems that trouble RSA SecurID and mean that you'd have to get the user to pass you a large number of codes before you got them out of sync with the server.  It also helps them avoid some of RSA's later patents which deal with their troublesome clock sync problems.

@_date: 2002-10-25 16:42:28
@_author: Nicko van Someren 
@_subject: more snake oil? [WAS: New uncrackable(?) encryption technique] 
I appreciate that as cryptographers we should be rightly skeptical of anyone claiming to have a new, unbreakable encryption scheme.
That said, given the tone of the message from Multiplex Photonics perhaps launching attacks on, or laughing at, those who are ill informed about cryptography is not the best use of our energies.  If this system is so eminently breakable then surely we should be applying our skills to solve what scientists in another field currently believe to be a hard problem, thereby advancing the sum of human knowledge, rather than just sitting around sniggering at them.

@_date: 2003-10-07 14:51:58
@_author: Nicko van Someren 
@_subject: nCipher netHSM 
"cryptographic boundary" as validated by FIPS compared to the most recent release of our PCI cards; all necessary changes to the HSM were put in before the last re-validation of the cards.  The UI components themselves are outside the cryptographic boundary.  That said, communication with the HSM thought the screen and input devices on the front panel does NOT pass through the computer inside the case but instead goes through a micro-controller and into the serial port on the PCI card HSM.  This is analogous to the way things have always been with out smart card readers plugged into the HSM which themselves were not FIPS certified.

@_date: 2003-09-24 03:29:23
@_author: Nicko van Someren 
@_subject: [Mac_crypto] Re: Peppercoin Raises $4 Million in First Round Funding, Appoints CEO 
I think you'll find that when the transaction value is small the merchant rates from all the credit card companies are substantially higher than that.  It's common for the marginal merchant rate to be on the order of 2-4% for online transactions but there is usually a base charge of 25 cents or more as well as the marginal rate.  On a $2 charge for a song the merchant could well be paying more than 30 cents in fees, which is more than double the rate for Peppercoin.
This is a severe limitation if you're trying to sell to people on this mailing list but it covers a large fraction of the market by value.  I would expect to see a Mac client too soon, thereby covering more than 99% by value of their target market.  I doubt you'll see open source interface software and I doubt that their market will care.
To push a technology like this into even 6 merchants before you've got proper funding for your company is not bad.  That said, I have no doubt that they will ever scare Visa and I have no doubt they they never intended to.

@_date: 2004-04-06 10:07:53
@_author: Nicko van Someren 
@_subject: [Mac_crypto] Apple should use SHA! (or stronger) to authenticate software releases 
Just because SHA-1 is O(2^80) and this problem is O(2^64) does NOT necessarily mean that this problem is easier in practice.  Complexity orders are helpful for comparing like with like, and helpful when working out what's best in the limit as the sizes go up, but they give no immediate information about the absolute cost.
Incidentally, there are attacks on hash functions that do not require masses of memory.  The amount of memory only determines how much effort you have to apply to find the exact solution once you've found where to look.  In particular, changing the amount of memory does not change the O() order of the complexity!
Upon reflection, if the attacker can arrange the specific case in which the original and the bogus files are exactly the same length, and that all the information in the file after the part that the "tweakable" section is the the same in both files, then you can make use of the limited memory of the hash functions and look for collisions right after the tweakable part, which means you get to ignore the length of the tail of the file.  In these circumstances the attack you describe is probably practical, at least for governments if not for Microsoft.
