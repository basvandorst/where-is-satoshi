
@_date: 2013-08-27 20:34:47
@_author: ianG 
@_subject: [Cryptography] Email and IM are ideal candidates for 
Right.  One of the problems with email (as pointed out in OP's original post) is that it is free to send *and* it can be sent to everyone.  The combination of these two assumptions/requirements is essential for spam.
Chat systems have pretty much killed spam by making it non-possible to send to everyone.  You need an introduction/invite/process/barrier, first.
This has worked pretty well.  Maybe the writing is on the wall?
Maybe we just need to let email die?
We can move email over to the 'IM technology' layer.  We can retain the email metaphor by simply adding it to chat clients, and by adding IM technology to existing email clients.  Both clients can allow us to write emails and send them, over their known IM channels to known contacts.
Why do we need the 1980s assumption of being able to send freely to everyone, anyway?

@_date: 2013-08-28 12:33:50
@_author: ianG 
@_subject: [Cryptography] Petnames & Zooko's triangle -- theory v. practice 
Perhaps in a sense of that, I can confirm that we may have an elegant theory but practice still eludes us.  I'm working with a design that was based on pure petnames & ZT, and it does not deliver as yet.
One part of the problem is that there are too many things demanding names, which leads to addressbook explosion.  I have many payment vehicles, many instruments, and in a fuller system, many identities. Each demanding at least one petname.
And so do my many counterparties.  A second part of the problem is that petnames are those I give myself to some thing, but in some definitional sense, I never export my petnames (which is from which they derive their security).  Meanwhile, the owner of another thing also has a name for it which she prefers to communicate about, so it transpires that there is a clash between her petname and my petname.  To resolve this I am exploring the use of nicknames, which are owner-distributed names, in contrast to petnames which are private names.
Which of course challenges the user even more as she now has two namespaces of subtle distinction to manage.  Kerckhoffs rolls six times in his grave.
Then rises the notion of secured nicknames, as, if Alice can label her favourite payment receptacle "Alice's shop" then so can Mallory.  Doh! Introduction can resolve that in theory, but in practice we're right back to the world of identity trickery and phishing.  So we need a way to securely accept nicknames, deal with clashes, and then preserve that security context for the time when someone wishes to pay the real Alice.   Otherwise we're back to that pre-security world known as secure browsing.
Then, map the privacy question over the above mesh, and we're in a traffic analyst's wetdream.  One minor advantage here is that, presswise, we only need to do a little better than Bitcoin, which is no high barrier ;)
In sum, I think ZT has inspired us.  It asks wonderfully elegant questions, and provides a model to think about the issues.  Petnames and related things like capabilities answer a portion of those questions, but many remain.  Implementation challenges!
Yes.  I was never scared of the NSA.  But the NSA and the FBI and the DEA and every local police force ... that's terrifying.  That's a purer essence of terror, far worse than terrorism.  We need a new word.

@_date: 2013-08-29 11:27:59
@_author: ianG 
@_subject: [Cryptography] Separating concerns 
<521CE337.6030706
Hi Phill,
I think that was acceptable in general up until recently.  But, I believe the threat scenario has changed, and for the worse.
The firewall between national intelligence and all-of-government has been breached.  It is way beyond leaks, it is now a documented firehose with pipelines so well laid that the downstream departments have promulgated their deception plans.
And, they told us so.  In the comments made by the NSA, they have very clearly stated that if there is evidence of a crime, they will keep the data.  The statement they made is a seismic shift;  the NSA is now a domestic & criminal intelligence agency.  I suspect the penny has not dropped on this shift as yet, but they have said it is so.
In threat & risk terms, it is now reasonable to consider that the USA government will provide national intelligence to back up a criminal investigation against a large company.  And, it is not unreasonable to assume that they will launch a criminal investigation in order to force some other result, nor is it unreasonable for a competitor to USA commercial interests to be facing a USA supplier backed by leaks.
E.g., Airbus or Huawei or Samsung ...  Or any company that is engaged in a lawsuit against the US government.  Or any wall street bank being investigated by the DoJ for mortgage fraud, or any international bank with ops in the USA.  Or any company in Iran, Iraq, Syria, Afghanistan, Pakistan, India, Palestine, ....  or gambling companies in the Caribbean, Gibraltar, Australia, Britain.  Or any arms deal or energy deal.
(Yes, that makes the task harder.)

@_date: 2013-08-31 16:35:34
@_author: ianG 
@_subject: [Cryptography] Functional specification for email client? 
Some comments, only.
An email user may have one or more identities... (confusion between email addresses, keys, chat handles, etc).
This requirement is troubling, and it has bedevilled many systems because it has artificially locked them into perfect traffic analysis, low key agility, poor economics, and messy identity semantics.
It typically is an assumption of the email providers that an email address must have a certificate, and this allows the certificate to be 'checked' against the email address.  But it is not necessary nor particularly effective.
A better requirement might be worded:
When a user receives an email, she is sure that it comes from the stated identity as found in the address book.  The stated identity may not be related to the "from" line.
all identities (email addresses and/or keys)...
This needs to nailed down, otherwise the system falls into the abyss of digital signatures.  What this means (at a lower level) is that every mail is digitally signed by the sender.  It needs to be stated that the signature of the sender's key means that the message came from the sender's key, and not anything else.  Especially, it is not a signed contract, not a non-repudiable bla bla, and is not even a proof that the person sent the message (without significant other support).
That is, the digsig is a low-level protocol tool, not a legal digital Also, to bed in a complete understanding, a separate requirement 6.b should be added for a second signing process using separate "signing keys" following the notions expressed in (eg) EU digital signature directive (eg) OpenPGP cleartext signing.  However, this should be clearly stated as optional, as such digital signatures are fraught, and if not optional, the system will fail to be implemented and be accepted.

@_date: 2013-08-31 16:37:03
@_author: ianG 
@_subject: [Cryptography] NSA and cryptanalysis 
It's all speculation of course, but that is what it feels like to me. An interesting clue from the earlier report is that they aren't there yet, they're building towards a capability.  They've figured out some way to crack in theoretically, and with a big investment they'll get there.
Which suggests a combination of massive crunch power, keys on the margin *and* cribs from side-channel attacks.  The bright shiny new 3rd division of the NSA is responsible for the side-channel attack.  And it was very expensive...  Coincidence?
Or, it could all be fluff, designed to suck money from cow in w.DC. Many a conman has made rich by claiming some secret invention;  the investors are the muggins for putting their money in without doing the due diligence.

@_date: 2013-12-04 11:56:24
@_author: ianG 
@_subject: [Cryptography] Email is securable within a coterie [was: Email 
No, you have to get the support for those tools into popular GUI clients.  E.g., Thunderbird.  If you want ordinary people to be protected, the GUI clients have to be delivered with the solution already installed and enabled.
If you talk to say Mozilla, they say that they follow standards, they typically don't do any new / original security work.  It's in the manifesto!  (Microsoft will probably say, show us the money!  Google will probably say, show us the data ;) )
So you have to then go to IETF and get them to implement a standard. E.g., OpenPGP followed this route....
Then you have to go back to Mozilla and convince them to implement, or let you implement.  Then you're get mumble mumble plugin mumble.
We're facing realpolitik & economics, not a technical challenge.  Good luck!

@_date: 2013-12-11 14:27:29
@_author: ianG 
@_subject: [Cryptography] Kindle as crypto hardware 
Yes, designed but never demanded.  This is the old /design a better smartcard moustrap and they'll beat a pathway to your door/.  And yes, the iButton was basically a variant on the smartcard.
People want their iToy to do everything.  When contemplating a smartcard or an iButton, the thought was either that (A) you plug your thing into the network (ATM, POS, etc) and it would work, or (B) you would plug your thing into your own personal device (laptop, iPhone, Android or Palm Pilot for those who remember).
Case A succeeded in a thing called Chip&PIN but the lead time was decades and the rollout cost is in the billions.  So it only happens in a totally controlled market.  I'm not even sure it has reached US of A yet.
Case B never succeeded because a system that did it all on the personal device always wins in the marketplace over a system that requires two

@_date: 2013-12-11 16:17:38
@_author: ianG 
@_subject: [Cryptography] Kindle as crypto hardware 
I think that point bears repeating, especially by those of us who were paranoid ravers :)
I agree that the intervention likely didn't start as more than an incremental tweak to programmes already in existence for other purposes.   Step by baby step.
But now it is policy.  The DUAL_EC_DRBG is just the one we have the more or less complete picture on.  A reasonable observer should be able to conclude that the SSL/PKI debacle is in the NSA's best interests, and this puts all of the PKIX and TLS and HTTPS-everywhere efforts under a cloud [2].  As is cloud :)
Hardware encryption is regularly targeted.  Commercial software crypto is compromised.  We have no "evidence" that they interfered in each case, but we've long suspected it and the expectation should now switch to a probable.
As in, probable cause, good enough for an arrest, if we could identify the crime.  Yesterdays news:  they targeted game communities, without any evidence!  While it is interesting to figure out how it happened, that's really the topic of history.  It happened.
Security must concentrate on the here and now -- how do we defend?  Do we?  Can we?  How much to pay?

@_date: 2013-12-13 18:55:36
@_author: ianG 
@_subject: [Cryptography] Size of the PGP userbase? 
I suspect you might be better off counting messages encrypted rather than users.  Anyone got an angle into a message tracking service?

@_date: 2013-12-14 09:49:12
@_author: ianG 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
That would be to reinvent Yarrow?
If that were known as Linux's approach, and RDRAND where spiked, it would be a simple matter to spike the RDRAND in microcode again (a known/suspected capability).
Perhaps to unXOR the contents of the previous instruction and XOR in the secret stream...

@_date: 2013-12-14 09:56:12
@_author: ianG 
@_subject: [Cryptography] Size of the PGP userbase? 
Aha!  So you could measure the ratio of PGP to S/MIME usage by message from that.  You could also count the number of distinct users for each As a proxy for PGP population, if you have the S/MIME population estimate from somewhere, multiple that with the ratio, and you have an estimate.  Standard marketing calculations...

@_date: 2013-12-14 16:54:05
@_author: ianG 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
I'm sure there are multiple ways of doing it.  I'm trying to spot the simplest, one that could be slipped into microcode without upsetting things -- minimal use of space/time/op changes.
The RDRAND instruction I'd say is the low hanging fruit, because it is called rarely and for precise purposes.  Inside that instruction, check whether there is a XOR coming up, of the output of RDRAND and some other X.  Likely, that value X is already calculated, sitting in a register somewhere.  Do some pre-XOR magic with that X, and the RDRAND output, and the secret sauce.
This of course all depends on some high value target (breadth or depth) using a simple RDRAND XOR yaPRNG approach.
Of course, the answer is to go back to the core design:  put RDRAND as just another collector feeding into the mixer, so there is little sensitivity especially to that one, and no easy way to determine (in microcode) how much extra processing comes after the RDRAND point. Treat RDRAND like every other entropy source -- borkable.

@_date: 2013-12-16 12:44:09
@_author: ianG 
@_subject: [Cryptography] What is or isn't Bitcoin 
I wrote similar musings in
I don't disagree, but the devil is in the details.  As I mentioned in my recent talk at Afrikoin, the economics never lies.  The problem is that we might apply the wrong economics, and it will only reveal itself to be right or wrong well after the fact.
Your analysis gets somewhat caught up in the definition of decentralised as being some sort of binary property, when in reality (de)centralisation is more of a comparison between different solutions along an axis of more or less (de)centralisation.  Proving something as decentralised per se isn't going to work, showing that Bitcoin is more decentralised than say USD is more likely.
Also, the argument fails to account for stickiness and transaction costs.  The group is sticky, in part because the only ones interested are the ones holding the value.  The same argument can be made for USD, those holding the value are more interested in preserving the group.
Yes, the group can move.  A good movie is the "The Counterfeiters" a german movie about concentration camp inmates being used to 'enter' the group that subscribed to the value of the British Pound.  But because of the various transaction costs, the group cannot easily move so fast that the value just collapses without redemption.
(The counterpoint to this observation is that Bitcoin is frequently seen as a Ponzi or bubble.  Those operations migrate their groups as they feed on their value, and will burst at some point when the group has migrated too far, and consumed too much value.  Right now, the group is migrating to China...)
[1] Also good to see hints about the Sabotage Manual in action...

@_date: 2013-12-17 10:52:41
@_author: ianG 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
Means?  The NSA has the means (& budget) to do opcodes, microcode, Intel hacking, make no mistake.
Opportunity?  Only very few persons have the opportunity to manipulate the microcode, which is almost certainly what would be needed to mount this attack.  So it would seem to be out of reach, a pipe dream, a fantasy, at first blush.
But: on knowledge or belief (to use the legal term) the NSA is one of those very few persons who can manipulate the microcode of Intel's CPUs (and this has been true for at least 2 decades, to my belief or knowledge).
Motive?  On the question of malevolence, I think Snowden revelations have taken us there.  They want this, if it can be made to work, and they'll try it if they don't know.  That's what they said in the goals revelations, and I believe them.
If it works on Linux, that's an unimaginably rich prize.
That's probably half the server space out there!  Probably 90% of their SSH and SSL keys could be attacked if there was a secret stream embedded into their 'safe' /dev/random feeds.
If it works on Cisco/Juniper routers, ditto.  Microsoft, same.
Think about how to make it work on more platforms -- it's already out there, staring us in the face :)
What I am assuming is that Linux devs are a trusting bunch, and don't believe that this could be done.  As Nemo posted earlier:
Lays out the story better than I can.  They didn't believe it could be done.  And they've created the incentive to try.  Bad Linux!
Which is the problem we are facing in unravelling what they can do.
Intel is not audited or otherwise publically verifiable as being clean, and has a very strong, long standing relationship with the NSA.  As a matter of historical fact, they added a population count opcode (others will know more).  The only one who needs that is the NSA.  This isn't the end of their involvement, just the bits we know about.
A question is indeed, assuming attacks are happening at the CPU level, what is the easiest way security could be perverted?
It has to be a really simple thing -- something that can be spotted in very few instructions, as anything else would be non-robust in the face of compiler optimisations, updates, etc, and very hard to analyse in the limited space within.  As you say.
As we're talking about the RNG, then the current yarrow-post-XOR is that very simple thing.  If the CPU can do pipelining, then it can spot the XOR after the RDRAND.  And find the other value... and pre-process it for the perfect RDRAND output.
This is a beautiful attack!  Impossible to verify, secret, and unbelievable.  There are no time implications, RDRAND is already a complicated instruction.
The solution is simple -- RDRAND should be as trusted and untrusted as every other source.  It should be collected before the mixer, like the others.  There should be no post-RNG special XOR phase.  Never ever.
Linux should not only dump that bad idea for its own security, but also so as to not encourage anyone to try that attack.  Bad Linux!

@_date: 2013-12-18 10:46:33
@_author: ianG 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
Yep.  I think mine was grad level but I did it as undergrad.  And, yes, we built a computer, and wrote a microcoded instruction set.  A lot of fun.  I'll admit my knowledge is way out of date tho, this was back in '83, the sophistication of what is now done in microcode has way eclipsed my understanding.
I think it is clearly necessary to gimmick RDRAND, and probably this would be a first step along the journey.  As PHB recently posted, each subversion is likely a process of small steps, starting out with one or two very reasonable requests.
      "we want to put your RDRAND mode into a deterministic mode so we can test it.  As you know, FIPS requires deterministic RNG testing, and we (the NSA) consider that to be an article of faith.  What can we do here?  By the way, one of our engineers knocked up this idea..."
As to whether the secret can be held, consider the story of DUAL_EC. That was a secret that Snowden knew, a contractor.  I draw from that, that a lot of people knew about the project.  I also think that a certain amount of hubris affected the secrets sharing of the NSA over the last decade, they have done things that they promised would never come to light, and have been found out.  E.g., somewhere it was reported that they got authorisation from Obama for Stuxnet on the promise that the secret would never come out.
Consider also Olympic Games.  That secret must have been shared by many hundreds, perhaps thousands, across multiple agencies & countries.  Yet, the only way we found out was when the darn furriners found the samples and decided to ask around what they were.
I'm positing that this is normal:  pipelining does more or less the same thing for a different goal.
Yep.  This would be the fast version of the infamous lunch time attack posited by Adi and Nicko.
Yes they could do that.  I don't like that attack because it leaks information by means of packets.  That's something that would be reserved for very hard and rare targets, and it would need to be precisely tailored.  How many 128 bit random strings are there to scan?
No such is necessary.  Just don't elevate RDRAND to a superior position as a post-RNG XOR phase.  Be happy with it as a collector.
Just get rid of these lines:
         for (i = 0; i < LONGS(EXTRACT_SIZE); i++) {
                 unsigned long v;
         // arch_get_random is RDRAND.
                 if (!arch_get_random_long(&v))
                         break;
                 hash.l[i] ^= v;
         }
(I'm just reading this guy:   and I could be

@_date: 2013-12-18 10:46:56
@_author: ianG 
@_subject: [Cryptography] [IP] 'We cannot trust' Intel and Via's 
I think we're in consensus at later stages in your post.  I would imagine the return from RDRAND would be some time-dependent AES generated stream, XOR'd with the other random from the RNG, as found in the sneek-peak.
Oh, well, I assume the guys writing this are much better than that. Time, serial number and some secret would be the inputs to the stream Note that Flame was an intelligence gathering virus.  I am guessing it's a simply matter to report back things like CPU serial number and time bases.
Yes, a difficulty here is whether the mode is spikeable into the ON position, and how.  I don't have a view on that, I'm simply not up on those thoughts.  But they are.  I'll bet they already know the answer to that one.
Yeah.  But, by reduction, this is simply to re-invent the RNG.  Why not use the RNG construction that is already there?  Feed the RDRAND in as a collector, and not do any fancy XORing at the end.
ps, offtopic;  I've been toying with a name for that pattern -- how does the trident pattern sound as a name?
     <-----\
            \
     <------XXX-----0101010
            /
     <-----/
Collectors -> Mix -> expansion/whiten

@_date: 2013-12-19 20:14:34
@_author: ianG 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
Nice!  If I read it right, this seems to be a good compromise between those that insist there be no special mixing and those that insist there be special mixing.  In order for the RDRAND opcode to breach this, it would effectively have to break SHA1.  This might be done, but unlikely in the picoseconds available.

@_date: 2013-12-20 11:48:15
@_author: ianG 
@_subject: [Cryptography] What do we know?  (Was 'We cannot trust' ...) 
What do we know?
The most solid crypto fact I have seen is this:
     The CCP expects this Project to accomplish the following in FY 2013:
        ...
         (TS//SI//NF) Shape the worldwide commercial cryptography marketplace to make it more tractable to advanced cryptanalytic capabilities being developed by NSA/CSS. [CCP_00090]
more here: Yes, that, and...
this.  The combination of complexity (too much for audit) and secrecy makes for the perfect environment for intervention.  We don't know and we can't tell.
Do we know the intervened?  No.
We don't 'know' it ... but what does it mean to 'know' anything ???
There is the court definition of knowing something -- a witness can state (her) knowledge, and a court can declare its fact in a ruling. But plenty of people have gone to death row and been found exonerated on DNA evidence, so that standard of knowledge, whilst totally certain legally, is not what us scientists would accept as the truth.  Nor does the court handle 'knowledge' well when the state secrets card is played.
There is the scientific sense of knowing something -- easily repeatable experiments confirm a hypothesis which then becomes a 'law of science.'   But scientific method is a rather weakened tool in the face of byzantine behaviour.
There is the libel sense of knowledge.  And integrity.  When criticising someone, we are taught to stick to the truth, because that is a position of integrity.  Do unto others how you would have them do unto you, etc.
Nice, win-win, biblical even, but a person lacking integrity can use our own integrity against us, by forcing us to stick to our own standards.
The NSA has long left our standards -- it/they lied to everyone from congress down, under oath.
As they have done that -- left the standards of integrity that we would hold for ourselves -- then they have also defied our 'court' or fought our jurisdiction.  They can no longer be reliably held to our standards.   It's not a civilian disagreement any more, but a criminal prosecution, and we expect the perp to lie.
The upshot of this is not that we know more, but we have to develop new ways of determining "what we know."
Ironically, our best source of this is the intelligence community itself

@_date: 2013-12-21 09:37:41
@_author: ianG 
@_subject: [Cryptography] What do we know?  (Was 'We cannot trust' ...) 
Just on that last point, new data came out yesterday.
Two snippets:
    "Undisclosed until now was that RSA received $10 million in a deal that set the NSA formula as the preferred, or default, method for number generation in the BSafe software, according to two sources familiar with the contract."
    "RSA adopted the algorithm even before NIST approved it. The NSA then cited the early use of Dual Elliptic Curve inside the government to argue successfully for NIST approval, according to an official familiar with the proceedings.
    RSA's contract made Dual Elliptic Curve the default option for producing random numbers in the RSA toolkit.  ..."
(I haven't seen the original documents, John, have you?)
What's interesting in this process is that it lays out *one path* for subversion in quite good detail.  Another snippet:
    "... No alarms were raised, former employees said, because the deal was handled by business leaders rather than pure technologists.
    "The labs group had played a very intricate role at BSafe, and they were basically gone," said labs veteran Michael Wenocur, who left in 1999. "
Companies that have been under attack should take note of these ways: google, facebook, microsoft, etc, because it is beyond reasonable doubt that these methods have been tried on them.  There is another which I'm writing up in the background.

@_date: 2013-12-21 09:52:58
@_author: ianG 
@_subject: [Cryptography] Decentralized, global, irreversible, 
Google on 'colored coins' ... the general idea is to layer a protocol over the top of Bitcoin transactions using the inbuilt smart contracts feature, and in particular to use the transaction database to handle other currencies and to extend its capability to do more of the heavy lifting in the formation of contracts, especially atomic trades.
The idea is basically plausible because Bitcoin implements smart contracts already, this was how zerocoin was implemented to add blinding to Bitcoin.  The smart contracts idea was pioneered by Nick Szabo in the 1990s, and was not credited in the original paper.  It's now credited by Mike Hearn in a wiki post:
It's early work so far, they are concentrating on the technicalities of getting the information flowing.  Once that happens, there is still a vista of work to do.  E.g., the idea runs slap-bang up against Gresham's Law, and little work has been done on the semantics of each new unit (shameless plug for Ricardian Contracts).

@_date: 2013-12-22 09:13:50
@_author: ianG 
@_subject: [Cryptography] Why  don't we protect passwords properly? 
Some answers on a bright Sunday morning:
1.  Your attacker of choice has to have custom hardware, this knocks out 2.  The best benefit is gained when the bar is only lifted some way up, as most attackers are 'economic' and they will move on if there is difficulties experienced.  Even targetted attacks by the NSA go through an 'economic' analysis, and if active hard attacks are required then the burden of need gets higher for them, because the one sin they cannot commit is sunlight (ok, your attack above isn't active).
3.  The amount of stuff to learn to defeat the aggressive knowledgeable attacker is seriously scary.  One guy could possibly do it after 10 years or so, but it really requires a team of diverse strengths.  E.g., This week there was news of acoustic analysis, which perversely seems to be reverse correlated with other side-channel analysis techniques.  Oh dear.  A month ago there was a scare story about jumping airgaps.
4.  Critics think every thing should be fixed, and give the developers no credit.  So criticism is loud, but it more follows the crowd than is actually useful.
5.  K6 is the killer.  Most of the work should be in the UI. Cryptographers bemoan and wail about some weakness or other, but it is easy to show that by far the biggest weakness is that the user chooses not to use the tool.

@_date: 2013-12-22 09:28:59
@_author: ianG 
@_subject: [Cryptography] Why don't we protect passwords properly? 
It's money.  They are serious.  Bitcoin is the cutting edge for what really matters in crypto -- money.
Why do you think they should know any better?  Just curious...
This is like the old von Mises fallacy of government regulation.  He asked why it is that people think that the government knows more about the market than those in the market?  When you analyse what happens in the real world, all the signs point to the opposite:  if people knew more about the market than the players, then they would be in the market making money.  The reason they join the government is more likely that they know too little to be in the market.
What's that old saw about teachers?
Knowledge is more like a pyramid than a set of wings.  We need a big and broad base in order to build towards the sun.

@_date: 2013-12-22 09:36:44
@_author: ianG 
@_subject: [Cryptography] [IP] 'We cannot trust' Intel and Via's 
This is a good point.  Using RISC chips would be a substantial defence against the attack that has been outlined (leaving aside the obviously contentious debate as to whether the risk is serious).
What RISC CPUs are there these days in widespread deployment in off-the-shelf general purpose computers?
If there was a way to reveal a signature of the PALcode, then it could be checked against known good sigs.  Just musing...

@_date: 2013-12-22 10:17:31
@_author: ianG 
@_subject: [Cryptography] RSA is dead. 
I doubt that too, although there are reports that this sort of thing happened elsewhere.
What is clear is that the team did their research and figured out who would be open to such things.  RSA was vulnerable, we know the channel and motives:  money, business decisions, weakened core crypto team.
I heard it as "and we especially want the DUAL_EC to be the default RNG".  That is, it was an actual request, and it was part of the contract discussions.
Now, the thing to realise is that this is benign from RSA's pov, but only seemingly benign from NSA's pov.  This is how it is done -- an unauditable, unverifiable, benign, totally reasonable shift.
Absolutely, we need to separate the people from the problem.  Old Dutch expression:  go soft on the people, go hard on the problem.
Nobody needs to accuse the RSA folk of being evil.  Nor should we accuse the NSA of being stupid, and to say they wouldn't do such things is simple ignorance.
The NSA are very smart.  They know how to figure out the openings, what is possible.  They know how to convince someone who wants to be convinced.  $10m makes someone want to be convinced.
As I seem to be saying a lot, *it is their job* !  The NSA are spies, after all, and they're very good at it.  If this doesn't make any sense, read more spy novels -- there is a common thread, *the asset always loses*.
So what about RSA?  One could say that RSA were naive, or innocent, or tricked.  It can happen to all of us.
But, RSA didn't make one small mistake, they made two huge mistakes.
What was RSA's job?  Their job was to serve their customers with secure crypto.  They didn't, instead, they allowed an interested party to get between them and the customers, which was an abrogation of their self-claimed standard:
   "Unlike alternatives such as open source, our technology is backed by highly regarded cryptographic experts."
This mistake is not like (say) an airline being tricked into revealing their customer list or a phone company being tricked into letting someone tap their fibres.  An airline flies people in planes, a phone company delivers calls, they aren't in the privacy business.
This is like an airline dropping maintenance, and putting planes into mountains.  RSA was in the crypto business -- it shipped dodgy crypto. They made the one mistake that is impossible to argue away:  Negligence in the core business.
It's still just one mistake.  Where RSA made their second mistake [0], and crossed into gross negligence was when all the warnings came out (2007, Microsoft), and
       *RSA did nothing* .
It's all over.  For the sake of the entire crypto business, RSA must be blacklisted.  Every provider must be taught that breaking trust in core business with customers is unacceptable.
And, don't blame me for this rationale.  The NSA must be taught that if they wish to pervert a supplier, the responsibility for its failure must come back to the NSA.  The NSA brought RSA down.
Yup.  And no doubt RSA sales are down a long way.  On this dire thread, this is a termination event;  if I was boss at EMC I'd be looking at breaking up the division, selling it.  At a minimum, re-branding it and cleaning out the staff.
All this at NSA's door.  Who think it is fine to destroy their own country's industry to get a leg-up on a bunch of net cowboys and towelheads.  And they still aren't taking it seriously, still saying they are doing god's work, protecting Americans from idiots with firecrackers, to paraphrase that Wall Streeter.
Strange bunch of people.
[0]

@_date: 2013-12-22 11:38:58
@_author: ianG 
@_subject: [Cryptography] Bitcoin is the crypto bleeding edge 
right, which leads me to think of
6.  Show me the money!  Today's revelation for me is that
    * bitcoin is cryptography's bleeding edge *
This is because of two factors:  it is where the money is *and* they are stealing it.  These two factors together mean that this is the one live experiment where we get to see crypto in the real live economic experiment.
Which leads to the next answer:  if bitcoin is being lost (and it is being lost in bucket loads) through password crunching, then they have that problem and they will fix it.
So, do what they do.
If in the alternate bitcoin is not being lost for the cause of password crunching, then they've solved it.  Economically.
Then, do what they do :)
ps; more on this "show me the money" here:

@_date: 2013-12-23 10:06:08
@_author: ianG 
@_subject: [Cryptography] BitCoin Question - This may not be the best 
That is the collision strategy.  Consider this:  in the old days we used to use MD5 which was 128 bits long, so a collision could be engineered in 2^64 bits space.  That's now achievable.
So in or around 1996 we mostly (should have) shifted to SHA1 which is 160 bits.  That is now scary, and has been scary since 2005 when the Shandong team of Xiaoyun Wang, Yiqun Lisa Yin, Hongbo Yu found weaknesses.
So people started switching to SHA2 which has 256 bits to 512 bits, and NIST started a SHA3 competition which is now revealed.
   1991   1996    2001       2012
   MD5 -> SHA1 -> SHA2    -> Keccak/SHA3
   128 -> 160  -> 256-512 -> ...
The collision resolution strategy is (1) use a big enough hash to start with and (2) have some means of changing it if the cryptanalysis starts to get dodgy.
That's standard in crypto work.  It works.  There are even proofs in the market place that it works -- Verisign used MD5 too long in a CA of theirs and got hacked.  In 2011 or so, various fabricated certs based on MD5 started appearing.
What Bitcoin's strategy for (2) is I don't know.  That's a bit murky because they haven't got a clear roll-over path built in.
ps; which might become the ultimate test of the concept of One True Cipher Suite ... also scary!

@_date: 2013-12-23 10:17:54
@_author: ianG 
@_subject: [Cryptography] Why don't we protect passwords properly? 
Hi Jerry,
fun debate,
You are conflating regulation with government.  There is such a thing as self-regulation;  and it works well in its context:  The referees are appointed by the football associations which includes the players' as well.  In legal terms, such things typically refer their entire disputes to own dispute resolution, something called Arbitration or Alternative Dispute Resolution.
Which itself is typically enacted in law in the Arbitration Act (various names) in many countries, but its tradition & custom predates most all governments in existence today.
He did what he was best at, and acted to maximise that?  His fallacy was pointed at those who believe what they are told without analysis, it was also an observation of governments on the field of the market;  it wasn't aimed at one man's personal choices.
Indeed.  Although, your argument assumes a peculiar definition of 'best' which isn't as yet surfaced.
What von Mises was assuming is that 'best' was as defined by market popularity (in some sense, call it numbers or revenue or profits).  If we look at the 'best' in encryption ciphers and hashes, what do we see?
    DES -> IDEA/Blowfish... -> AES
    MD5 -> SHA0 -> SHA1 -> SHA2 -> Keccak
The shift is clearly towards the market;  NIST has discovered that it is not really capable of reliably doing better than the market.
Coming back to the IETF committees, they are a fixed target.  They are like a government of the net.  Of course they will be subject to the same forces that makes government bad.
And, DJB is challenging for the prize for best cryptographer, because (today's outrageous claim) he is first and foremost a software engineer.
How many IETF committees is he on?  Adi?
Indeed, why isn't there a committee for cryptography?
Flame away :)

@_date: 2013-12-23 10:30:17
@_author: ianG 
@_subject: [Cryptography] RSA is dead. 
I don't think so, but I agree it would be nice if it was so.  If you look at all the failures in cryptosystems, there might be a bias one way or the other but it isn't a slam dunk.
Open Source as a guarantee of security is really just the marketing of the open source folk.  It certainly helps but collecting those smart eyeballs isn't as easy as saying it.

@_date: 2013-12-23 13:58:24
@_author: ianG 
@_subject: [Cryptography] RSA is dead. 
There is an evolving sense that we can do more to help.
1.  toolboxes are moving up the stack.  We aren't interested in encryption algorithms any more, we're interested in authenticated encryption algorithms.
2.  Competitions are delivering our best results, not committees or government fiat.  e.g., above, there is a competition called CAESAR for AE modes.
3.  If you look at DJB's design for curve25519xsalsa20poly1305 you will see further movement up the stack -- one way to do the whole thing.
4.  In Object oriented coding it gets even easier.  I use a concept I call a Cryptor which combines everything together and does both ends. Popular cryptors would be nice.
5.  We do need more basic cryptoplumbers.  So one of the things we can do is unwind the pogrom against ordinary coders doing crypto.  Knock yourself out, you can only hurt yourself and your customers, and the concept of a false sense of security has not been shown to be any more than another false myth amongst hundreds.
6.  Many more counterculture hints here:
7.  Learn some risk analysis.  This is how life is;  take some risks. Risk analysis gives you a framework for deciding how much effort to put into things, and also points out that security is wider than tech or crypto or yet another software feature.

@_date: 2013-12-24 10:43:34
@_author: ianG 
@_subject: [Cryptography] [IP] 'We cannot trust' Intel and Via's 
Aha.  So, are there any case studies of this actually happening?  This might shed light on the RDRAND question.  If we had a documented case of (say) the Chinese slipping spiked chips in to one of the hot USAF toys, then we'd have some sense of how likely this is.
Papers, conferences, budgets, hype, FUD, gosh.

@_date: 2013-12-24 12:43:08
@_author: ianG 
@_subject: [Cryptography] Passwords are dying - get over it 
Hi Joe,
(thanks for the reminder, I needed to post on blog about MITB dual channel defences being broken.)
 > Kent commented:
Oops!   sorry 'bout dat!  Overtaken again:
    "    Zeus and other MITB trojans have used social engineering to bypass this process. When a user on an infected PC authenticates to a banking site using SMS authentication, the user is greeted by a webinject, similar to Figure 1. The webinject requires the installation of new software on the user?s mobile device; this software is in fact      ZitMo malware intercepts SMS TANs from the bank. Once greeted by the webinject on a Zeus-infected PC, the user enrolls by entering a phone number. A ?security update? link is sent to the phone, and ZitMo installs when the link is clicked. Any bank SMS messages are redirected to a cyber criminal?s phone (all other SMS messages will be delivered as My commentary:
New Report:
Original 2006 MITB paper from Philipp G?hring:
Nice page.  Perhaps that could be expanded to include precursors and attacks :)

@_date: 2013-12-24 12:58:30
@_author: ianG 
@_subject: [Cryptography] Old tech rulez! 
Yes.  The problem is that the number of risks is huge, and you have to accept some.  Insider threats are mitigated, but you cannot ever get over the fact that insiders are human and don't bend well to risk analysis or tech or rules.
Outsiders can never understand that.  You've gotta take some losses on the chin, elsewise you're in fantasy land.
There is historical precedent on switching to old tech [0].  The Battle of the Bulge was a surprise attack because Adolf Hitler -- himself only, and not his generals -- did not trust the crypto and comms anymore.  He got suspicious about how many battles were going the enemy's way.
In his last roll of the dice, Hitler sent all the orders by motorcycle riders [1].  Total surprise.
[1] Sorry, I don't have a reference.  Ob crypto:  the allies were reading the Enigma traffic.  Those that don't read history are doomed to repeat it ...
[0] as well as Hollywood precedent:  the line "Old tech rulez!" comes from _Mercury Rising_ a well recommended entertaining film about crypto, albeit based on a slightly unrealistic premise.

@_date: 2013-12-25 01:05:14
@_author: ianG 
@_subject: [Cryptography] RSA is dead. 
90 mins + copious slurps of xmas cheer + rusty(C),,,, and I found it. effing macros, this bird is worth two in the ...
I think I somewhat agree with James.  I could have rewritten that code in less time than it took to fund the flaw.  But that concept only works when (a) I know there is a flaw ... which the comp kinda reveals and (b) i can rewrite myself or somehow magik the blubbercoder to do a better job.
I also agree with Jerry.  If I could find the Jameses of the world and employ them for cheap, I'd be happy as Santa.
So, assuming I sober up by the morn, and SO doesn't notice, where's Ping's code?
iang (hic!) merry xmas all, and may your cheer not infect your macros

@_date: 2013-12-25 09:48:10
@_author: ianG 
@_subject: [Cryptography] Can we move this list to an online forum please? 
Not all respects.  I never use forums.  I enjoy push to my email box. If it isn't pushed to me, I'm not interested to follow yet another forum (there are thousands...), I haven't the time to validate someone else's marketing campaign.

@_date: 2013-12-25 11:12:18
@_author: ianG 
@_subject: [Cryptography] Serious paranoia... 
appeal for calm, season's greetings and all that...
In my experience, it is definately the latter.  Dorks might be a bad term, but the world is full of people who achieve a certain level of expertise, but not mastery.  Sometimes this is for good reasons, such as, they're simply too busy on other responsibilities.
You see it in big teams.  Most all are middle ranking programmers. Getting those stars to lead them is the hard part.  If it was easy, we'd all be that star, right?
It's just humanity;  look up the Star effect or the Hollywood effect. People range in skills according to some form of pyramid.
And when it comes to seriously hard things like security, which has a huge list of considerations, it's just seriously hard to figure out what matters and what you can leave out.
Rest easy, you're not surrounded by shills :) have a merry xmas!
ps; on the question of the shills, what should be reminded frequently, and is 2013's hot topic, is that in such an environment that we live in, when the enemy wants to send in a shill, it is remarkably easy to do. But calling people shills off the cuff isn't going to help, it takes a fair amount of work and time to out a real shill, and getting it wrong can be really destructive.

@_date: 2013-12-26 10:35:40
@_author: ianG 
@_subject: [Cryptography] how reliably do audits spot backdoors? 
Which is the problem.  People *talk about open source being safer* but they have no mechanism to really make it safer, other than (windmill) "you can make it safer if you just contribute..."
Bug bounties have been tried, but they seem to be inherently blunt tools.
We need some sort of flow of value that rewards the hard slow effort of code review.  Something like a bitcoin mining algorithm, where the proof of work is the review.  Bugcoin?

@_date: 2013-12-26 10:37:09
@_author: ianG 
@_subject: [Cryptography] how reliably do audits spot backdoors? 
It's meat & drink to cryptoplumbers.  Sometimes it is known as overflow, sometimes as modulo math.  % is your friend, bits are your chisel.
When I was in the business of employing C programmers a long time ago, I asked two questions.  The first was "how big is an int" ... which pretty much separated the sheep from the wolves.
(Apropos, the second q was to opine on whether to use a macro or a function for a simple task like max(a,b) and why.  Again, baa v. howl with a high confidence level.)
Yeah, you beat me.  I just read it and did it all in my mind, not serious.
The choice of language is heavily influenced by personal and business factors.  There is no one real theory or way here, which is perhaps why there are so many languages.  Emotionally, once a coder has committed to a language and written lots of code, that coder is incentivised to promote their choice, in no small part in order to self-validate their investment, both in kloc and mental energy.
I do it all in Java.  Once, when I did a port from Java to various languages, it took 5 times longer to get it into C as opposed to various OO languages (PHP, Perl).  I imagine C++ is more that latter case, OO speed.  I prefer Java because it is faster, safer, cheaper in the only metric that matters to me: coder time.
But I note there are many others who also speak with such emotion, but in obsequience to other idols.
If we're talking real security, don't use other people's libraries.  Bad.

@_date: 2013-12-26 11:57:58
@_author: ianG 
@_subject: [Cryptography] Fwd: [IP] RSA Response to Media Claims Regarding 
Yes.  The reported evidence suggests that RSA lost some of its core crypto mojo in the early 2000s, and that this deal was done by business folk not the crypto lab.
For the business folk, this is called cash cow.  Likely, you'll find that EMC purchased the company for its deal flow, and under-invested. They've probably made their money back by now.
Yep.  And aggresively document the method of attack, so that others can learn and defend.
The RSA breach is a beautiful thing;  all the elements are in place and documented.  We have the full case story.  I wish we had it for the other 10-100 methods.

@_date: 2013-12-27 10:03:30
@_author: ianG 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
Yes, indeed, I think this is an important problem, and I've not ever come across a comfortable solution.
Content.  As anyone can join, the content isn't that important.  A mild form of security comes from knowing what the list is called.
This might also be improved by having an invite practice, so even if you know that there is a list "NewWorldConspirers" there is more of a barrier to getting to it.
Another issue with content is having it escrowed.  Does it sit on the laptop mail client forever?  Or can we put a timer in that wipes it?
Metadata.  Who is on the list?  This could be considered valuable information (and it is what the big data organisations treasure).
Who said what?  If all the posters are benign, and one calls for worldwide cryptographic jihad, the attacker wants details on the target...  perhaps to offer her a job, privately.
Which leads to an obvious split in individual protections:  anonymous or psuedonymous?  That is, is each post by Alice recognisably from her, or is each post unlinked?
Moderator.  We should really model the moderator as an attacker.  Call her Trinity as a ttp.  What happens if she starts drifting the conversation towards ... oh, encouraging the IETF to standardise on DUAL_EC?  Her easy attack is to drop posts, so we might want to browse that which was censored.
Trinity might also start mitm'ing, by actively sending messages out to people that don't go to others.  So we might want to know that all messages got to everyone, and no selective conversations are happening.

@_date: 2013-12-27 11:25:14
@_author: ianG 
@_subject: [Cryptography] On Security Architecture, The Panopticon, 
Getting somewhat off-scope here, but Bitcoin is now the bleeding edge of cryptography, so important to understand *the business model* as well as the technical stuff.  You can't do technical security without a business Ponzis, pyramids and bubbles generally offer a better rate of return than is natural.  They feed on this 'great rate of return' by using their customers to bring in more customers, which in the long run creates the great rate of return.
They cannot go for ever up or even continue in operations.  The limit is how many new customers can come in to pump up the value of the insiders, which is something that can be estimated.
Bitcoin matches all these core requirements.  As a symptom, also, there is often unrealistic hype surrounding these things.
Where ponzis / pyramids differ from bubbles is that the former two have a person or few people behind them, early winners, whereas the bubbles tend to have more of a real underlying asset (real estate or stock market).
This also matches bitcoin:  the early miners are stockpiling their early buckets, and they are worth a fortune at today's price.  Perhaps we don't know who they are;  but they exist, and the blockchain tells us how much the early insiders have.
When the price crashes, people are not left with any asset at all. Whereas with a real estate bubble, people are left with the land, regardless of the price (leaving aside mortgage issues...).
So, if you want to model Bitcoin as a pyramid or a ponzi, it's possible.   To a great extent the differences are just semantic arguments or legal bla bla.
Whether this makes you upset is really your own emotional issues.  It is to be noted (and frequently noted by Bitcoin enthusiasts) that the governments run their own pyramid schemes called pension schemes, and also the recent wall street bubbles have insiders as well:  wall street itself.  And the USG runs a system called the USD which has been artificially pumped up by its use as the world trade currency;  a game that stopped in or around 2000, and has been lsoing a percent or so of dominance every year since then.
And and and...  Be careful to separate economics from excuses & emotions :)
Yes, I agree.  Such systems have a sustainability problem.  The trust model is very important for long term sustainability.  Unfortunately, trust models are often too complex to understand easily or too simplistic to work, a good trust model is hard work.  Examples of both available on request, but I need you to trust me with some hard currency As to the rest of the combined posters' comments -- as the business model that you are trying to solve is unclear, it is difficult to ground a solution...

@_date: 2013-12-27 21:54:50
@_author: ianG 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
Hey, dude!  It's a hypothetical.  Jerry asked the rhetorical question:
 > I actually addresses this issue a couple of weeks back as a
 > hypothetical.  So let's think about it:  Just what *would* a "more
 > secure" version of this discussion (ignoring the actual technology)
 > look like?  Keep in mind that, by design, anyone can join by sending
 > a simple request to the moderator.  They'll promptly receive copies
 > of all messages.  Given this, what's your threat model?
WYTM?  Then the next step is we list out *all the threats we can think of* ... without prejudice.
Later on we do some risk analysis and decide which are serious or not.
Actually that's already begun; Trinity has promised to me in private email that she hasn't access to the mailing list software so she cannot possibly MITM us and change the messages as they wand their way across the ether.
So there's no risk there.
Hmmm... I have to think about this some more.  If you don't see this message, stick your hand up.
iang, more seasonal liquidity required, methinks...

@_date: 2013-12-28 20:53:47
@_author: ianG 
@_subject: [Cryptography] deniable symmetric ciphers? 
Leaving aside the rather delicious trap of deniability in cryptography... this part here:
On the other hand, if we all hide all our data as indistinguishable from random, it becomes easier.

@_date: 2013-12-30 00:07:22
@_author: ianG 
@_subject: [Cryptography] how reliably do audits spot backdoors? 
OK, knew that one...
Blech.  This thread has been the bane of my last 36 hours.  On reading that above revelation, I scanned my code for close() calls ... and found pathetic results.  Hmmm...
Worse, it's not worth fixing it all until Java 7 is available across all platforms.  Sigh.
Does C# have this problem?
And, what actually happens if the close() isn't called?
And, why didn't I know this before?  What other secrets?  And and and...
iang, blurrr...

@_date: 2013-12-30 07:28:00
@_author: ianG 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
Is that really the case?  Synchronous comms can be recorded on or at the node, it is only "on the wire" that it is somewhat easy to do things like expunging ephemeral keys, but we've known for a long time that what we can on the wire does not mean that this is the security job done.
In the alternate, asynchronous comms can have the same characteristic, if the defence is deleting the keys.
Maybe it is just that with sync comms, we typically negotiate the ephemeral key because we can in a handshake.  There is a handshake, so we tap in the ephemeral keys and use that.
Whereas with async comms, there is an assumption of fire-and-forget, no handshake.  So can we add the handshake back in?
If it was the old PGP days, no way.  But if the mail client is even slightly aware, why not?  If a client can receive and process a mail, why can't it talk to the other client and set up?
For those who've used OTR over gmail chat, one sees this in the live, the OTR aware client sense the handshake message over the chat, and if there isn't an OTR aware client on the other end (e.g., ordinary gmail chat) then the user says "what's that?"

@_date: 2013-12-30 07:46:12
@_author: ianG 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
Seems to be:
Donn Parker,
"Making the Case for Replacing Risk-Based Security"
ISSA Journal, May 2006.
But I am unable to find a copy, anyone got a link to hand?
What I read of that article is that it is talking about (against) risk-based security rather than best practices.  Maybe I've got the wrong one?
Oh, it's worse than that.  Best practices as a concept only applies when you have a large group of people who need a 'practice'.  As you point out, the game mechanics of such is that the worse one typically survives because it is by definition the only one that everyone can implement, once they've decided they need consensus.
So best practices is actually a misnomer of perfect (!) proportions, it is by consensus the worst practices, or more politely, the least they can all agree on.  The race to the bottom.
But even worse is that even the very notion of people coming together to document their 'best practices' means that every participant is unsure what they are about.  They lack the confidence to say, "my practices are good enough, I don't need to achieve consensus."  They need the insurance policy of saying "we're doing what everyone else is doing, therefore we can't be wrong."
So, by the same game mechanics, best practices are those that are shared by people who aren't confident enough to do it.  It's turtles all the way down, from there.
Inconvenient changes to threats need not apply, we're perfect!
To be fair, this speaks right back to another problem that 'pure' cryptology has.  Without an understanding of the business model, there is no way to differentiate 'perfect' from any other standard.
Without that input of value, only 'perfect' withstands scrutiny.  My algorithm must be perfect -- squillion bit security over a lifetime of packets against a pentagon of crunching.
Sometimes this makes things easy -- such as block encryption algorithms and message digests.  Just as often or more often, the drive for perfection makes for a complete mess, like endless discussions about PK algorithm keylengths & params, overweight HMACs, key negotiations, who signs what, etc.  Perfect then becomes a millstone around our necks.

@_date: 2013-12-30 07:56:39
@_author: ianG 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
I forgot to mention that content should be encrypted in flight, at least point to point :)
Mailing lists typically strip attachments, which is what S/MIME uses. Because of malware.  Which some think x509 is...
OpenPGP uses cleartext sigs to get around that and they work nicely. But something went wrong on the way to deployment with S/MIME, they never got around to re-engineering to fix it, in oh so many ways.
Well, if it was that easy remailers would be trivial.  The thing is, if I don't want to be iang, today, it's harder.  And if I want to be 'anon' it's harder still.
But, the hards are also at the list level.  Each post of the list includes various metadata which gives the fingerprint.  The From is a real give away, but other things too:
Received: from tormenta.local (skaro.afraid.org [212.169.1.61])
Asserts I'm on the other end of an ADSL in Britain.  Personally, I found that terrible... not only because if I want to be Alice tomorrow, you'll pretty quickly figure out that Alice is a very close friend.
Why does being part of a conversation mean that I'm allowing anyone in the world to track my whereabouts?  This is privacy like Bitcoin, where doing a transaction means everyone gets to share it, and they only barrier is a little traffic analysis...
Well, if everyone is on their guard.  Remember the poor dear users, who are not used to this sort of thing.  The attack can be even as benign as google's attack on your daughter's gmail account, by noticing her search for pregnancy tests and spiking the adds with baby products (c.f., the infamous Target case).
The best attacks are the ones we can say afterwards "but hey, you knew we were doing that ..."
More shades of Bitcoin mechanics.  I'll bet there are a few groups in that world trying to hijack the blockchain for sending idle chat :) They're trying everything over there...
One sometimes sees odd posts with gaps in conversations here, my attempt to deal with this is adding the CCs, so there are duplicates floating right.  You sometimes see that effect in chat clients, where chat messages overtake.

@_date: 2013-12-30 08:52:43
@_author: ianG 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
Threat modelling is separated because the threat is the domain of the attacker, whereas the risk is the domain of the defender.
To the extent that you can think like an attacker at the same time as thinking like a defender, they can be combined.  We all do a little of that.
The danger is that we impose our wisdom of defence on the attacker's mind, and we design a system that defends against what we know how to defend, and justify that attack model post-facto.
C.f. SSL [0] hence Adi's "cryptography is typically bypassed." [1]  Or being inside your own OODA loop.
Initially at least, listing of threats should be considered as a whiteboarding session;  there are no rules, get it all up there.  Then:
Right, and that is why every listed threat has to be filtered.  In risk analysis, they do teach you to do a preliminary pass and drop stuff that is on the face of it unrealistic (at least in my class they did).
I've got a mighty fine LavaLamp driver for you :)  Seriously though, this is just historical.  We have every expert under the sun saying that the RNG in the OS is a prime risk, post-Snowden, we have a documented case study of a breach of an RNG, and we have various other illuminating events:  FreeBSD, Android, Debian.
There is going to a lot of attention on this point for RNGs, and for Linux.
Personally I'd take it as *an opportunity*.  Use the energy to assemble a better understanding, and build towards it.  You won't get this sort of attention in a year's time.
Indeed.  We presume he demanded access to raw stuff, what can you say/do to the vice commander in chief?
Now, later, we all know what was happening;  Cheney intervened so as to pervert the process to an already laid-out game plan;  he wasn't paranoid or crazy, he was only pretending that so he had plausible Another threat that we should consider in our list:  what happens if there is an insider in *our process* that has interests that are incompatible with ours, and pushes us to weaken our process (and improve C.f., the IETF's embattled crypto group.  We should use x.509 and CA-signed keys only :)
Indeed.  So we have a quandary.  Do it one way, fall in one trap.  Do it another way, fall in another trap.  Is there a way to avoid all traps?
We know what doesn't work:  committees, broad-based low-level crypto tool analyses, government standards, consultancies.
What that leaves is, I think:  the business must appoint one person to take responsibility.  That person must make the decision to drop the unrealistic threats, once they've had their day in the sun.
The job and the person takes on the success as well as the failures.
[0] [1]

@_date: 2013-12-31 10:09:28
@_author: ianG 
@_subject: [Cryptography] TAO, NSA crypto backdoor program 
They are supposed to be chosen at random, probably 16 bits of entropy in each one.  For our lowsec civilian purposes, they're almost good enough to feed them into our RNGs.
Only issue is the supply is a bit slow, but at least we claw back some of those taxpayer dollars...

@_date: 2013-12-31 10:53:14
@_author: ianG 
@_subject: [Cryptography] On Security Architecture, The Panopticon, 
As I've mentioned in the past, Bitcoin is the bleeding edge of cryptography.  That's because it is successfully launching crypto on a wide scale (like skype or ssl) and it is related strongly to money (so the theft / arbitrage activity gives us a strong indicator of strength).
It's therefore one of the small handful of important crypto projects to study -- at all levels.  So we're likely to drift from crypto to some extent...  but it is important to understand what the business does to crypto, in order to understand what crypto can do for a business.
Demand is almost entirely speculative as far as I can see.  It may move across to retail trading, but I suspect that's in the future.  We're still at the point where one retailer == news == hype, which means there is almost zero retail possibility.
The next signal to watch for is whether a retailer's stock price jumps on the news...  Does anyone remember the mid 90s, when a company's price would bounce when they announced a website?  If that starts to happen, we have a mass movement, but I think we're a long way from that.
The volatility is a function of how there are a few traders with large hordes and how there are few newcomers buying tiny amounts.  When that smooths out -- either by disbursement of the large hordes so the price pushing disappears, or by larger numbers of small traders -- then the volatility will lower.
The transaction costs of Bitcoin are actually surprisingly high, around the $50 mark.  This is mostly (99%) picked up in the miners' rewards at the moment, and direct transaction fees are low or zero.  But as the miners' rewards for blockchain calculation slow down (halving every period) there will be more incentive for higher fees.  The higher fees will then start to feel like direct transaction costs.
 From the history of e-gold, transaction fees being somewhat lower than say mainstream payment systems, gave about a 10 times 'velocity' advantage.  Cash at bank is typically measured at around a 50 days velocity, the period over which transactions equal the entire stock of money, whereas e-gold was doing 4-8 days, sometimes peaking at 2.
But e-gold's fees were a lot lower, 0.5% capped at 50c.
Inflation / deflation at the moment will be swamped by newcomers into the market.  This is the reason why nobody notices the transaction costs, newcomers are driving the price up.  In a state-based economy, newcomers typically are limited to immigration, birth/death increases which is 0-3% in most cases.  Tiny, not noticed.  And, GDP growth of 1-3% is typically snaffled by government.
In contrast, Bitcoin is all over the map, there isn't a lot of point in trying out inflation theories :)

@_date: 2013-12-31 11:17:16
@_author: ianG 
@_subject: [Cryptography] TAO, NSA crypto backdoor program 
The problem with this is that the NSA does what it is told -- by their political masters.  We were all around in the early 2000s, we all saw what the pressures were.
So, while we are probably all agreed that the terrorism was a false cassus belli, and still rolls on as a sort of Orwellian war chant, there isn't a lot the NSA can do about it until they are told to change their Yes, 9/11 was without a doubt the cassus belli, without a doubt.  But also the seeds were sown earlier in the late 1990s.
I suspect they were told there were no limits.  Remember TIA, Poindexter, the Ideas market?
The leaders in the USA are typically driven by one thing only -- campaign contributions.  These are driven by corporates.  And corporates in the Eisenhower's military-industrial complex are very powerful.
If you want to change the NSA, you have to battle the corporates that are clustered around.  Which is the above group.  Over the last half decade or so, they all added heavy weight cyber-warfare divisions, some have quipped it is now the military-industrial-cyberwar complex.
Which need to win contracts.  Which contracts will feed the politicians.
I agree, the media is useless, pretty much wiped out as an independent force since 'embedding.'
But I would ask, what can a popular movement do against the above cycle of funds?  That's a rhetorical, I don't claim to know the answer...
That's where we can step in.  As far as we're all likely agreed, governments are likely all tarred with the same brush:  doing stuff outside the norms, unlimited budgets [0], probably illegal [1] and definitely out of control [2].
If so, they are a threat to us and to our users.  That's us, being the open and competitive community around the use of crypto & security tools to secure out users.
In the past, national security was a threat we typically declared as not one to mitigate.
I think that's changed.  Snowden has changed that.  I think we have to mitigate.  I think we have to fight.
The crypto wars are on again.
[0] By unlimited budgets, I mean, huge & way in excess of ours.  We have to counter their huge budgets with our cunning and careful hacks.
[1]  The illegal part comes with the nature of spying and espionage; what they do to foreigners is illegal in the foreigner's country, and likely most others.  The home government gives a pass to the spy in her own country, but that only lasts inside the borders.  Outside the country, the spy is a spy, and can be punished with lifetime sentences or death in many cases.  C.f., the Italian rendition case.
 From our perspective, it is simple to unroll the politeness of espionage -- it's illegal in most contexts -- and to reject the sophistry of the TLAs if they are also out of control.  Therefore, we can consider it to be a valid threat to us and our people.  If you want to be polite to your own countryfolk, you can simply say, we're fighting every other TLA, not our own...
[2]  They crossed the line -- they set up secret and dangerous channels to feed national security intel to a wide range of agencies.  Once that happens, the national security resources of a nation are available to any political bureaucrat with a domestic enemy.  It's all in secret, it's all denied in court even under oath, so it's going to happen.  It probably already has.  Systemic corruption is inevitable.

@_date: 2013-12-31 12:37:00
@_author: ianG 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
NO, the point is, when in the domain of the attacker, we have to think like an attacker.  When in the defence domain, we think like a defender.
It is hard to think like both at the same time.  Hence, they are separated steps.  You can combine them if you can multithread, but that's your choice.
It's an open-to-join list.  It's like a committee :)  There are a range of capabilities and opinions...  an act of patience is required.
Right, I only skimmed that.  I couldn't see the threat.
It's a mailing list :)
How you do your modelling is up to you.  Which is why for example, PHB only occasionally posts questions on his exercise.  He isn't asking the list to do the exercise, he's just getting feedback on what he's done.
lol... if one thing has been cleared up in the Snowden revelations it is that no solution is a panacea.  DUAL_EC was an open proposal with open source and open peer review.  We could argue that other "open" things are also subject to influences, we just haven't seen the smoking gun yet.
If you're still not convinced, put yourself in the NSA's shoes.  The DD-ops says that the Linux stuff has to be stopped.  How are you going to do it?
If you haven't figured out half a dozen channels into that open project by the time you've finished the next cup of coffee, you're not thinking ... *like an attacker*.
Remember.  They have the budget, the orders, the incentive, the capabilities and the complete lack of respect for you and your high-minded thoughts.
Indeed.  Read the OSS sabotage manual, cerca 1940s.  That is a documented attack.
Business in the above is a metaphor for the overall economic purpose. The business might be your employer.  It might be the community of payment & speculative trading partners in the case of Bitcoin.  In the case of Linux, the business is that of users and corporates using a base OS to do software.
Exactly.  It only applies loosely.  The business of OpenSSL is ... what?   Undefined?  Protecting credit cards?  Online banking?  Are the users phishing victims or shy bits & bytes or SSL experts haunting open source projects and committees and PCI audit rooms?
The point is, at some level, with some tools like OpenSSL and DES and reliability.  These are security tools, not security projects in and of themselves.  Tools cannot deliver security, because security can only be defined from within the context of a business.
Therefore, security modelling is at a loss.  No business means no threats.
So the problem that these groups have is that they generally copy a threat model from somewhere, and assume it, without regard to its closeness to reality (infamously, SSL's obsession with MITM), or they come up with a 'perfect' security model that may or may not meet the needs of the next layer up.
E.g., the block cipher ... which does not meet the needs of the developers, because it is simply too hard to put into a protocol. Answer?  redefine the problem scope and develop a new API.  In particular AE modes and CAESAR.
E.g., the RNG, which we now have a model for (trident-like collectors->mixer->whitener/expander courtesy of John & Bruce & Neils, etc) but it's still a tool, with a model that strives to be 'perfect' .. that may or may not play a part in a security design to meet the needs of a business.
That's the point.  They chose someone.  If he makes you feel uncomfortable, then move your business.
Wait ... can you depend on anyone in a committee?  Is that zero or many you're assuming there?
Lol... Sure, maybe.  It is in some respects far easier to corrupt a committee than a single person, cf documented attack above.  Again, it's no panacea:  the business might live or die by those decisions.
Sooo... we're ready to start an open committee to decide what happens to the /dev/random on Linux in an open process?
Where do we sign up :)
Jokes aside, you're it, right?  You're in the hot seat.  What happens carries forward with your name into the future.
Why do you think the Linux kernel has some respect?  Because one person decides what happens in the kernel.  In app land, it's the wild west :)   Why do you think OpenBSD has a lot more respect in the security context?  One guy, and he's an obsessive fruitcake when it comes to security.  FreeBSD?  One guy, the security officer.
Microsoft?  No clue.  Apple?  They say nothing.  Google?  That many headed hydra has no single mind of its own, and wants to own your mind, so no joy there.  Mozilla? ...  These groups are so diverse they can't, don't and won't do it.
To paraphrase, there is only one CSO, and he's responsible.

@_date: 2013-12-31 13:09:48
@_author: ianG 
@_subject: [Cryptography] On Security Architecture, The Panopticon, 
Probably best put as 'price rises above its long term ability to deliver returns to the owners, ...'  because...
There is no such thing as 'true value' in any measurable or definable sense.  There is only price on a market.  This might indicate today's 'value' but it changes momentarily, and it's better to talk about today's price if one wants to test economic theories.
 From that perspective, you have an expectation of future prices going up.
Hold that thought close.
Wrong.  You can't call it a bubble unless the people inside have an expectation that the price will rise, where that expectation is based more on people's expectations than the delivery of productivity and real Probably you're confusing 'tangible' value with measurable metrics in some physical sense such as commodities.
Payments are an 'information' good, sometimes called a virtual good. They no less valuable than say movies or web pages, all of which are virtual and intangible in terms of not being haptic or touchable.
A payment's value can be proxy-measured by transaction costs.  So, a wire has value, with costs from $10 to $100 with costs rising with the bank's incapabilities.  Paypal has fees of around 4.2%, plus the incompetence loading.
That's a value that people will pay to get that good.  BTC has clear benefits there that are definitely measurable.
One of the *symptoms* of a bubble is that insiders frequently talk it up   :)  That's a simple incentives result, there's no reason to believe that insiders have the lock on economic analysis just because they hold BTC.
Before we go further, why is it that Bitcoin community don't want to label their thing as a bubble?  Or a ponzi or pyramid?
Is it just because these are bad words, of poor respect?

@_date: 2013-11-01 11:48:19
@_author: ianG 
@_subject: [Cryptography] PGP Key Signing parties (Trust Link Grid) 
Sounds good, quickly answering, apologies as am busy.
Let me talk a bit about CAcert and perhaps other CAs as it presents an illuminating counterpoint.  CAcert provides that as a network of Assurers.  There are thousands of them across the planet, with about a third concentrated in the Germanic belt of countries.
CAcert does their assurance using paper evidence and electronic distribution.  The whole system is backed up by Arbitration so that any particular claim can be tested by independent individuals.
Sounds about right.  But don't treat it as a statistically-even graph across the country, treat is a social network issue.  That's the way to propogate it.
People don't do that.  What they do is travel to the nearest 'meet' of everyone.  Forget geography and distances, concentrate on community.
Au contraire!  What you describe *is the globally agreed-on standard* :
"Each person shall sign a message stating that they have met the person in first-hand and established various keys, etc, which are listed herein, and various other facts (enumerated) to ensure reliable distribution of the key fact."
That has to be written.  And agreed.  And promulgated.  It has to be taught and trained and tested...  Have a look at the CAP form for CAcert, its contents are controlled by a policy.
This is worthless.  Reputation systems never work, because any mistakes made are easily dismissed -- "I made a mistake, let me fix that..."  And any nefarious attacker knows how to mix mistakes and innocence with dirty deeds.  Remember, the attacker is thinking completely different assumptions to you, and they are expert in their job.
Hence the need for globally agreed documentation :)
In terms of risk, you have to ask yourself what happens when something goes wrong?  You don't have much of an answer for this, other than 'reputation' which is a sort of hand-wavy new-age 1990s thing that never worked out.
If something goes wrong with a CA-issued certificate, you are SOL.  This is as it is writ.  So the CA system survives because the statement is pretty clear, albeit buried and we agree as insiders not to market it.
For CAcert, if something goes wrong, the Arbitrator can issue a ruling that is backed by the courts of most countries, and can issue monetary damages up to 1000 EUR as well as other 'remedies' .
Both of these systems give a defined value for damages, which allows an individual to calculate how much weight the system can support.  A CA-issued cert:  zero.  A CAcert-issued cert is 1000.
If you want the system to be robust, and have a risk equation, ask those       What happens when something goes wrong?
      How much money can I lean on this system?
Apologies for rushed reply.

@_date: 2013-11-01 12:03:33
@_author: ianG 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
Good point.  The only RNG attack I can think of off-hand for which we have reasonable evidence is the Android Bitcoin theft [0].  Very recent.   Any others?
It would seem that attacking the RNG is rather esoteric.
We don't even have evidence that the NSA has ever used their Dual_EC pre-positioned attack vector, assuming we all agree that they did that.   What we have is supposition that if this is an attack, it's plausibly convenient for them, 32 bytes being enough:
[0] for me, I always exclude demos, academic papers, etc.  Attacks must be done by bad guys, coz that is the only way we know what the economics of the attack are.  And attacks must succeed, they must steal money or

@_date: 2013-11-06 08:35:17
@_author: ianG 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
Claiming it is to protect you is an old trick.  Don't give it a moment's thought, save the analysis for important problems.
Clues...  and might put to bed some other conspiracy theories of hardware level manipulation.
iang, proudly wearing the tin foil hat today

@_date: 2013-11-09 14:14:04
@_author: ianG 
@_subject: [Cryptography] randomness +- entropy 
I would recommend you not call the methods in the Crypto subsystem, and not negotiate with the developers at all.  Simply copy the AES code across (from anywhere) and duplicate it so that you have complete independence and complete control.  Interdependencies between security modules are a sin, and code reuse while nice is not a good enough reason to complicate the net of dependencies underneath the security surface.
Also, your use of the AES algorithm is entirely distinct to theirs.  You only go one way, like a hash, theirs is two way, encrypt and decrypt, reversibly.  You may be able to happily strip out parts of AES in order to get a better efficiency, they cannot.  E.g., it may be possible to use less of the code and more of the AES instructions directly to get all you need (I don't know, I'm just speculating here...).

@_date: 2013-11-10 12:57:01
@_author: ianG 
@_subject: [Cryptography] randomness +- entropy 
Ah, point.  You pointed out earlier that AES was attractive because of the AES-NI instruction ... which indicates a desire for speed.
I'm not sure about the need for speed here.  I can't see the motivation for it at all in pure RN project terms, it seems to just be the normal obsession of programmers?
(In amusing contrast, many were sold on DRBG_EC, because it was slower, in part...)
Also, it seems the preference to many is to block while not receiving quality RNs;  is speed really an issue?
Presumably, if your need is only for whitening the output, you could also look at SHA3.  The core algorithm comes from the same team (or close) and it has a lot of flexibility in its sponge construction that might help it play a bigger part.  It should be faster than AES.

@_date: 2013-11-10 16:12:58
@_author: ianG 
@_subject: [Cryptography] Which encryption chips are compromised? 
In the recently revealed [0] NSA Consolidated Cryptography Program (CPP) goals for FY13, there was one as yet still redacted word [1]:
    * (TS//SI//REL TO USA, FVEY) Complete enabling for [XXXXXX] encryption chips used in Virtual Private Network and Web encryption devices. [CCP_00009].
What's the XXXXXX ?  The full length of the redaction can be seen in the graphic, about 15 letters.
[0] A related question is where were these slides posted on the Guardian and NYT sites?  Which did which redaction?

@_date: 2013-11-11 16:29:06
@_author: ianG 
@_subject: [Cryptography] SP800-90A B & C 
Is guaranteed on paper, but this only works if we assume there is no manipulation.  Which is the topic de jeur.
As always, it is a trade-off which is informed by your risk analysis. Do the risks from the threat of manipulation exceed the risks due to

@_date: 2013-11-11 16:31:07
@_author: ianG 
@_subject: [Cryptography] NIST should publish Suite A 
Good question ... I guess there are likely 1000s of contractors that know what the real Suite A consists of because they make kit that includes it.  Creating a 'deception suite' would have a low chance of success, and failure to keep the secret will achieve worse results.

@_date: 2013-11-12 19:44:41
@_author: ianG 
@_subject: [Cryptography] NIST should publish Suite A 
Hi Peter,
thanks for this collation!
That's exactly the starting point we would want.
So pretty clearly (from that single data point) EC is the future, RSA is the legacy.
So, same framework, means likely same techniques.  Which leaves some credibility to the ECDSA approach.  Lengths are 512 - 1024, I guess?
Question then is, is there enough information about generating the params to tell us whether the NIST/standards are amusingly different or curiously similar?
Super!  This is very encouraging.  If we agree with this albeit limited data point, then it means that we don't have to steer clear of EC nor do we have to go back to RSA.
(leaving out the MEDLEY stuff as my specific interest was whether EC was good or bad...)

@_date: 2013-11-13 10:21:16
@_author: ianG 
@_subject: [Cryptography] HTTP should be deprecated. 
There is a fallacy that web stuff need not be encrypted then it should not be.  This is wrong, but it takes a bit of logic and circumstances and experiences to find out why, below.
(This is old boring stuff, people who've read my rants on it can ignore it.)
In short, HTTPS is vulnerable to MITM.  Not good.
In longer terms:  Web stuff is used for secure online ecommerce, or online banking.  In this mode, the browser and the server have to authenticate each other.  If HTTPS is some part of this authentication, then we face a problem:
An attacker can set up a false website that is under HTTP.  He can then redirect across to the real site.  This is an MITM against an online Now, the browser and the server have to authenticate each other so that the above MITM doesn't work.  Guess what?  The server doesn't authenticate the client using HTTPS because that requires client certs and they are not used.  (Server uses passwords instead, see where this is going...).
Likewise, the client doesn't authenticate the server at all if it isn't using HTTP -- which is exactly the above MITM.  Instead, there is a sort of handwavy "user is supposed to spot the switch from HTTPS to HTTP" argument which makes no sense because (a) the user is trained to accept the entire authentication done by the browser if HTTPS is used (consider the case of one CA issuing another CA's cert) and (b) the chrome refuses to show enough details to make the users aware of what is happening.
IOW, the browser knows what it is doing, and it doesn't.
In the cryptographic literature this is known as a downgrade attack, but we prefer to label phishing as an MITM.  In military terms it is "attack at the juncture of the maps" and nobody will see it (coz it's on the other map you don't have).
The *only practical/business approach* to the MITM weakness in secure browsing is to make everything HTTPS.  This is strategic: only then will there be sufficient concentration on in-HTTPS MITMs to force the browsers to change their thinking about how they authenticate the servers.
Everything other work is futile while the downgrade attack exists.
Hence the project for HTTPS everywhere.  Coming to your browser since 2005 [0].
Ya.  Broken.  Eggs 'n omelets.
Google wields a two-edged sword -- it is both the server (services) and the customer (webmail clients, etc) and the  browser supplier (therefore also the uber-CA) and hooked in deeply to the  server supplier.  It's practically every side of the HTTPS box that they find themselves in.  No-one else inhabits the HTTPS box like they do.
Those outside that box know were this is going...
There are many, e.g.:
Browsers should be enabling opportunistic upgrades, but they are not. If I was a CA I'd be terrified of where this is going.  I'd sell.
But in terms of overall benefit for users, if one just looks at the world of ecommerce and online banking, there is only one direction: HTTPS everywhere.
[0] IIRC, we identified this logic back in 2005 and started the process to eliminate SSL v2 so as to use TLS to distro TLS/SNI so as to make it plausible to use mass SSL...

@_date: 2013-11-15 10:44:06
@_author: ianG 
@_subject: [Cryptography] Moving forward on improving HTTP's security 
There is a complicated choice here:  Get HTTPS everywhere, get CAs everywhere, or some combination in-between.  The problem here is that the combination of these two axes (and a few more) is likely an unsolvable equation, but we can see where some of the extremes are:
   i. Get all-TLS & get all-CAs:  fail.  All CAs will fall to the state.
(This of course can be seen as a tinfoil claim, and it is easy to dismiss because people simply don't know the reality.  FWIW, been there, got the t-shirt:  CAs are a legitimate, popular and priority target of the TLAs.)
   ii. Get TLS (or HTTPS) as an option:  fail.  This is the current situation, and results in the downgrade attack.  SSL then provides loose, maybe, sometimes security, which cannot be relied upon *and* it is expensive because of all the load that other systems place on people.   That's an unacceptable compromise.
The path from endpoint (ii) is rocky, and may or may not lead to endpoint (i).
For my money, I assume that everyone can see that if we TLS-everything, then we cannot accept CAs everywhere, and we must add easy opportunistic I might be wrong;  there is a lot of vested interest that can only see their own paycheck, and they are making good money claiming that HTTPS+CAs is a complete security package for now and the future, we just need to PKI-'em harder!

@_date: 2013-11-22 11:58:48
@_author: ianG 
@_subject: [Cryptography] Moving forward on improving HTTP's security 
(apologies for late reply)
Right.  There are of course many documented threats & mitigations.  I would suggest that CAs remain breachable against today's attacker, but it doesn't seem to be germane to the actual problems we face.
Yes, more or less.  No work has been done on this from an academic pov, so we only have some historical data.
Phishing is the outlier -- thousands or millions of attacks, and millions or billions of damages.  The claim that TLS+CAs provided MITM protection wasn't true, it only pushed the MITM around a bit, and in the end not enough to justify the cost at the application level.
If we leave out phishing, the numbers of known CA breaches are in the low double-digits [0], and the numbers of other beaches on the direct TLS protocol side are about the same.  Likewise, damages are low or To a large extent is all comes back to WYTM? or what's your threat model?
I'm assuming here, today, we are adding the NSA's mass surveillance into the equation as a valid and important threat model.  That's an assumption that can be challenged...
If NSA mass surveillance is our threat model (addition), then the solution to that is probably mass opportunistic encryption, to force them to go active.
Is that right or wrong?
Obviously, we can change our conclusion by flipping our assumptions [1]   What are they and what do we agree on?
[0] what I know as breaches relevant to CA risk modelling are here:
[1] e.g., if we assume that only google/yahoo/microsoft are valid targets, we could mitigate with HTTPS-everywhere.  Might be enough?

@_date: 2013-11-23 14:44:20
@_author: ianG 
@_subject: [Cryptography] Dark Mail Alliance specs? 
Shrink, Steal, Compete.  Don't try and form a committee ;-)
But, the format is locked into the MUA.  So it has no bearing to anyone, as we can't change the design locked into N*MUAs.
I'm obviously missing something here -- what can be done with S/MIME that unlocks the formats within the MUAs?
Yes.  This gets into the fallacy of signing -- what is the statement being signed?  For PGP it is "I met this person, maybe."  For the CAs, it is "the person had some identity docs, maybe."  These are rather vacuous statements, of about the same value as the TOFU statement "this key is the same nym as it was last time."
I'm not sure what the answer is, but if one supports all statements then competition might allow a better economics to emerge.
3a. root-list supplied CAs
3b. peer- or user-endorsed CAs
Practically speaking, people will look for endorsements over CAs.  At the moment, there is no choice here, you get imposed by the ubur-CA called the mail vendor, and it is hidden.
But in terms of the marketplace and the way users think, what they want is some sort of more localised trust list that they can choose.  E.g., just go with Iang's list or EFF's list or the Politburo's list. Obviously, people in Iran aren't that happy with the USA-dominated list of CAs, and people in the Pentagon aren't that keen on Mozilla's list including an Iranian CA.
One list does not work.
This mechanism can then be seen as separating the list from the application, or as you might put it:  decouple the problem of trust management from the application.
In this way, we would get the compromise of both worlds.  Mozilla's list might be chosen by many, but there are also a lot who won't want the worldwide grabbag of CAs which adds risks to those who care.
In practice, this is very important.  To make this product work, you are going to have to succeed without the support of the vendors, which means you'll have to develop support heavily towards 1) and 2).
The process will need to be slick;  Skype is the standard.
I would leave any blocking until last.  Yes, we as security folk are horrified when an email slips out unprotected.  But, they the people are horrified when the email doesn't go, regardless of anything else.
Communication is the primary objective of the people, security is a secondary one.  K6 says they turn the product off and then you lose.
(OK, I suspect you're probably thinking the same thoughts anyway.)
I wouldn't suggest using that old stuff.  Pick something well portable/ported and entirely distinct with a trivially small footprint, so that the code can be delivered in the app without any needs for any external library support.
You've got a chance to dump the legacy, take it!
Protocol Buffers from google is the closest thing I've seen of late, but it might already be too complex and require external libraries and developers to learn YetAnotherFormatLanguage.  All that is needed for just about everything is an integer and a byte array, everything else is composable in objects over streams.  Once that is done, you can do precise checking in each object over basic primitives, so semantics and syntax are covered in one.
Death is to involve another language.  At this point, weaknesses start to erupt at the crack between the languages, and security fails.  Costs mount as developers need to be adept in more tools than they can comfortably master, and still have time to write app code.
Oh, yes, critical assumption:  it's all OO these days, and the non-OO people should not be catered to.
Another extremely important problem is metadata or as we now know it, fodder for mass surveillance.  Unless there is something in there to handle that issue, I suspect the result will be a lot of work for limited utility.
Even a single hop would be enough to break the metadata chain...  It doesn't need to be sophisticated, to begin with.

@_date: 2013-11-24 13:38:52
@_author: ianG 
@_subject: [Cryptography] Moving forward on improving HTTP's security 
I agree.  There might still be some debate about how we get there.
Going HTTPS with the current (PKI v. MITM) arrangement is not going to work, IMHO, because of the economics.
Look at the OODA cycle for changes in SSL, it's minimum 3.5 years [0] more likely a decade (SNI, MD5).  Now apply an OODA prediction across to the HTTP world.  It will be longer for a dramatic, non-compatible, costly change.
The only economic way this is going to happen is if the change is cost-free, plus-benefit and is viral.  Turning on opportunistic encryption is one way that meets those goals, give or take.  Like STARTTLS, if I recall correctly.
( And, for those who are upset at the NSA and their "golden age of SIGINT" [1] opportunistic encryption has an added bonus of stopping the easy flow of economic intel across to the various agencies of interest.   That alone is worth the price -- cryptography advances in employment have always been pushed by the perception of danger, not by the real dangers. )
[0] [1] Thank you John Young and Edward Snowden:
"For decades, Signals Intelligence has sustained deep and persistent access to all manner of adversaries to inform and guide the actions and decisions of Presidents, military commanders, policy makers and clandestine service officers. As the world has changed, and global interdependence and the advent of the information age have transformed the nature of our target space, we have adapted in innovative and creative ways that have led some to describe the current day as ?the golden age of SIGINT.? "

@_date: 2013-11-25 09:01:31
@_author: ianG 
@_subject: [Cryptography] Email is unsecurable 
It's an interesting question, and one worth studying for pedagogical motives.  From my experiences from both sides, it is clear that both sides failed.  But for different reasons.
S/MIME failed because it is an atrocious key management design. Everything about it is designed to rely on certs, and nobody wanted to buy certs, and when you bought them, they didn't work well enough.  It's a CA's perfect protocol because it places the cert at the apex of the mission, and a user's nightmare because certs fail too frequently in the aggregate to avoid the curse of K6 -- turn it off, dump it.  In practical import (from actual experience), if you had a group of say 12 people with one year certificates, every month some person was failing to communicate because her cert had expired.... Do the math.
PGP failed because it never succeeded in conquering the GUI clients. That was in part because of what PHB calls the Betamax-VHS war.  The providers of the major clients were already in the certificate camp, so they locked out the PGP side.  It was beyond the resources of the PGP group to crack that barrier.
If you look at the other big comparison, SSL, it won its early battles against the alternatives in part because one company held the reins, Netscape.  They were able to force through their decisions.
But, there are other reasons.  If you look at the overall picture, there are many other difficulties.
For example, consider traffic analysis or metadata or mass surveillance

@_date: 2013-11-26 13:23:57
@_author: ianG 
@_subject: [Cryptography] Email is securable within a coterie [was: Email 
I entirely agree that if you put all that in place, it will work.  Mail is theoretically securable.
What I would question is whether we can agree on how to get to that place (IETF committees, PHB v. Dark Alliance, S/MIME v. PGP, etc etc), and whether it is cost-effective, given alternatives.  E.g., Coteries in Skype or OTR/Jabber, etc.

@_date: 2013-11-26 17:49:03
@_author: ianG 
@_subject: [Cryptography] Moving forward on improving HTTP's security 
Indeed.  As with all security, the game is to push the attacker to a place he doesn't want to go.  So he heads off for easier pickings. Information released in l'affaire Snowden confirms that the NSA is indeed economic about its operations.
It's also worth pointing out to the economically-challenged amongst the stakeholders (CAs and browsers) that if you can get the entire browsing activity over to HTTPS in opportunistic mode, then the step up to authenticated mode is of much lower cost.
It will likely increase the size of CA sales by an order of magnitude.
Unfortunately, in the minds of the stakeholders, an opportunistic encryption is a lost sale, rather than an opportunity, and must be attacked at all costs.  Sad failure of marketing, that one.

@_date: 2013-11-28 07:33:08
@_author: ianG 
@_subject: [Cryptography] Email is unsecurable 
Is this what the NSA called the home field advantage?  It seems that there are a number of factors which align strongly in NSA's favour: they are the ones with more money, so can outspend.  Their contractors love them for it, so congress approves too.  Hardware designs are harder to crunch at cheap costs because specialised hardware is indicated. Hardware is oh so much easier to control (read: stop) at the border. Hardware is oh so much easier to control (read: pervert) at the fab.
Yup.  I suspect we are at a watershed for national standards.  Following them may no longer make any sense.  Even before the Snowden revelations, it was widely recognised that the FIPS standard process created unnecessary bloat and expense, with no perceivable security benefit over simpler open engineering.
If TLS moves forward with the open curve suite, this will be a big signal.
I don't think we ever knew as much as the NSA.  They employ thousands to our 1s and 10s.  However, we can also do economics, and we can also do things that make sense in smaller teams.  And in software.  One thing we do know is that good crypto still works.

@_date: 2013-10-01 11:48:07
@_author: ianG 
@_subject: [Cryptography] NIST about to weaken SHA3? 
This could be the uninformed opinion over unexpected changes.  It could also be the truth.  How then to differentiate?
Do we need to adjust the competition process for a "tweak" phase?
Let's whiteboard.  Once The One is chosen, have a single round + conference where each of the final contestants propose their optimised version.  They then vote on the choice.
(OK, we can imagine many ways to do this ... point being that if NIST are going to tweak the SHA3 then we need to create a way for them to do this, and have that tweaking be under the control of the submitters, not NIST itself.  In order to maintain the faith of the result.)

@_date: 2013-10-01 12:02:36
@_author: ianG 
@_subject: [Cryptography] RSA equivalent key length/strength 
Just to clarify my original poser -- which *public key methods* are suggested in Suite A?
RSA?  EC?  diversified keys?  Something new?
The answer will probably illuminate what the NSA really thinks about EC.
(As well as get us all put in jail for thought-crime.)

@_date: 2013-10-02 08:03:32
@_author: ianG 
@_subject: [Cryptography] TLS2 
Right.  I see the encoding choice as both integral to any proposal, and a very strong design decision.
I would fail any proposal that used some form of external library like ASN.1, XML, JSON, YAML, pb, Thrift, etc, that was clearly not suited for purpose of security.  I would give a thumbs-up to any proposal that created its own tight custom definition.
This is why I like not using a decoder.  My requirement is that I read exactly what I expect, check it for both syntax & semantics, and move on.  There should be no intervening "lazy compilation" steps to stop the coder seeing the entire picture.
Another problem with decoders is that you need a language.  So that makes two languages - the primary one and the layout.  Oops.  Have you noticed how these languages start off simple and get more and more complicated, as they try and do what the primary could already do?
The end result is no savings in coding, split sanity & semantics checking, added complexity and less security.  For every element you need to read, you need a line of code either way you do it, so it may as well be in the primary language, and then you get the security and the full checking capability, for free.
Right.  To solve this, we would generally know what is to come, and we would signal that the exact expected thing is coming.
Following-data-identification is the one problem I've not seen an elegant solution to.  Tagging is something that lends itself to some form of hierarchical or centralised solution.  I use a centralised file with numbers and classes, but there are many possibilities.
If I was to do it, for TLS2, I'd have a single table containing the mapping of all things.  It would be like (off the top of my head):
1  compactInt
2  byteArray
3  bigInt
4  booleans
20 secret key packet
21 hash
22 private key packet
23 public key packet
24 hmac
40 Hello
41 Hiya
42 Go4It
43 DataPacket
44 Teardown
I don't like it, but I've never come across a better solution.

@_date: 2013-10-02 13:17:48
@_author: ianG 
@_subject: [Cryptography] TLS2 
I would see that as optional.  If a designer thinks it can be done, go for it.  Let's see what the marketplace
Believe me, that way is a disaster.
The first thing that happens is someone says, let's get together and we'll fix this.  Guys, we can do this!
The second thing that happens is they form a committee.  Then the companies insist that only their agenda be respected.
End of (good) story, start of rort.
I'm sorry, this is totally embarrased by history.  The CAs have *all* the say, the vendors are told what to say by the CAs.  The banks have *none* of the say.  We can see this from the history of CABForum, which started out as I suggested above.
(The users were totally excluded from CABForum.  Then about 2 years back, after they had laid out the foundation and screwed the users totally, they invented some sort of faux figurehead user representation.   I never followed it after they announced their intent to do a facade.)
Patent free or free licences provided, yes.
Fewest dependencies.
3 too many!
I agree with that.  You'll find a lot of people don't agree with the key size being fixed, and people like NIST looooove yanking the chain by insisting on upping the numbers to some schedule.
But that resistance is somewhat of an RSA hangover; if the one cryptosuite is based on EC then there is more chance of it being fixed to one size.
And that opens pandora's box.  It requires a WG.  I have a vanity need.   Trouble begins...
I like it, but DJB raises a good point:  if EC is fast enough, there may be scope to eliminate some of the phases.
I see this as difficult.  A lot of the problems in the last lot happened because the institutions imposed x.509 over everything.  I see the same problem with the anti-solution which is passwords.
How the past is rectified and future auth needs are handled will be part of what makes a winning solution the winner.
That won't get very far.  We need client auth for just about everything.
The business about privacy is totally dead;  sophisticated websites are slopping up the id info regardless of the auth.  Privacy isn't a good reason to drop client-side auth.
(Which isn't to say privacy isn't a requirement.)
Well, they want a complete solution.  Not the crapola they have to deal with now, where they have to figure out where CIA stops and where their problems start.
So, it seems that there is no consensus on the nature of client auth. Therefore I'd suggest we throw the whole question open:  How much auth and which auth will be a key telling point.

@_date: 2013-10-02 13:28:21
@_author: ianG 
@_subject: [Cryptography] RSA equivalent key length/strength 
Hi Peter,
Indeed it can.  So how do we differentiate?  Here are two oft-forgotten Firstly, when systems fail, typically it is the system around the crypto that fails, not the crypto itself.  This tells us that (a) the job of the crypto is to help the rest of the system to not fail, and (b) near enough is often good enough, because the metric of importance is to push all likely attacks elsewhere (into the rest of the system).
An alternative treatment is Adi Shamir's 3 laws of security:
Secondly, when talking about security options, we have to show where the security fails.  With history, with evidence -- so we can inform our speculations with facts.  If we don't do that, then our speculations become received wisdom, and we end up fielding systems that not only are making things worse, but are also blocking superior systems from emerging.
OK, so TLS.  Let's see the failures in TLS?  SSL was running export grade for lots and lots of years, and those numbers were chosen to be crackable.  Let's see a list of damages, breaches, losses?
Guess what?  Practically none!  There is no recorded history of breaches in TLS crypto (and I've been asking for a decade, others longer).
So, either there are NO FAILURES from export grade or other weaker systems, *or* everyone is covering them up.  Because of some logic (like how much traffic and use), I'm going to plumb for NO FAILURES as a reasonable best guess, and hope that someone can prove me wrong.
Therefore, I conclude that perfect security is a crock, and there plenty of slack to open up and ease up.  If we can find a valid reason in the whole system (beyond TLS) to open up or ease up, then we should do it.
It is a significant question.  Who are we protecting?  If we are talking about online banking, and credit cards, and the like, we are *not* protecting against the NSA.
(Coz they already breached all the banks, ages ago, and they get it all in real time.)
On the other hand, if we are talking about CAs or privacy system operators or jihadist websites, then we are concerned about NSA-level Either way, we need to make a decision.  Otherwise all the other pronouncements are futile.
All good questions.  What you have to do is decide your threat model, and protect against that.  And not flip across to some hypothetical received wisdom like "MITM is the devil" without a clear knowledge about why you care about that particular devil.
Indeed, so many unknowables.  Which is why a risk management approach is to decide what you are protecting against and more importantly what you are not protecting against.
That results in, sharing the responsibility with another layer, another person.  E.g., if you're not in the sharing business, you're not in the security business.
In that lies the trap.  Because we can make a block cipher that is unbreakable, we *think* we can make a system that is unbreakable.  No such applies.  Because we think we can make a system that is unbreakable, we talk like we can protect the user unbreakably.  A joke.   Out of this sort of fallacious extension has come non-repudiation, off-the-record, authenticated HTTPS, and other myths.
Indeed, we can't even agree on a stream cipher that is unbreakable, let alone SSL or HTTPS or SSH or Skype or those higher level services.
And, the trap springs.  In order to make our constructions unbreakable, we do a bit of handwaving, and hand the user a crock full of implausible smelly brown stuff.
Sure.  Just be sure you don't hobble real world systems with perfect security models.
Systems are used by users.  Not by cryptographers.  It is the security people who have the bias from reality;  users not.  Users are the reality, what they do with your system is the reality.
P_i's and I_i's.
Indeed.  Not only does it not work well to model, it provides little basis for assumption at all.  Which is why risk management typically doesn't use it.
Yep, busted.  But TLS is not designed to protect Manning and Snowden.
Nope, and it famously didn't.  Manning was caught with chats over OTR to a trusted accomplice who recorded them and handed them over.  As is entirely predictable.
This is why I say, the threat is *always* on the node.  From your perspective, we can make the wire protocol so (cryptographically) strong that we can ignore any residual weaknesses, and simply model the threat on the node.  That's what we should protect for, and that's what went wrong for Manning (as well as most other examples you can find).
No problem!  Hey, I'm unsure about your references, but it's like a 10 hour flight from that location to where I am ;)

@_date: 2013-10-03 11:01:11
@_author: ianG 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
I know others have already knocked this one down, but we are now in an area where conspiracy theories are real, so for avoidance of doubt...
This might relate to the related-key discoveries in 2009.  Here's an explanation from Dani Nagy that might reach the non-cryptographer:
I don't think they did.  Our Java code was submitted as part of the competition, and it only got renamed after the competition.  No crypto changes that I recall.

@_date: 2013-10-03 12:17:45
@_author: ianG 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
This has been a favourite topic of mine, ever since I discovered that the entire foundation of SSL was built on theory, never confirmed in practice.  But my views are informal, never published nor systematic. Here's a history I started for risk management of CAs, informally:
But I don't know of any general history of internet protocol breaches.
Most attacks go around the protocol, or as Adi to eloquently put it. Then, of the rest, most go against the software engineering outer layers.  Attacks become less and less frequent as we peel the onion to get to the crypto core.  However, it would be good to see an empirical survey of these failures, in order to know if my picture is accurate.
Yes.  Software engineers are especially biased by this issue.  Although, it rarely causes a breach, it more often distracts attention from what really matters.
Frankly, I see this as a waste.  The problem with rounds and analysis of same is that it isn't just one algorithm, it's many.  Which means you are overdesigning for many algorithms, which means ... what?
It is far better to select a target such as 128 bit security, and then design each component to meet this target.  If you want "overdesign" then up the target to 160 bits, etc.  And make all the components achieve this.
The papers and numbers shown on keylength.com provide the basis for this.  It's also been frequently commented that the NSA's design of Skipjack was balanced this way, and that's how they like it.
Also note that the black-box effect in crypto protocols is very important.  Once we have the black box (achieved par excellence by block ciphers and MDs) we can then concentrate on the protocol using those boxes.  Which is to say that, because crypto can be black-boxed, then security protocols are far more of a software engineering problem than they are a crypto problem.  (As you know, the race is now on to develop an AE stream black box.)
Typically then we model the failure of an entire black box, as if it is totally transparent, rather than if it becomes weak.  For example, in my payments work, I say "what happens if my AES128 fails?  Well, because all payments are signed by RSA204 then the attacker can simply read the payments, cannot make or inject payments."  And the converse.
This software engineering approach dominates questions such as AES at 128 level or 96 level, as it covers more attack surface area than the bit strength question.
As above.  Perhaps the reason why I like a balanced approach is that, by the time that some of the components have started to show their age (and overdesign is starting to look attractive in hindsight) we have moved on *for everything*.
Which is to say, it's time to replace the whole darn lot, and no overdesign would have saved us.  E.g., look at SSL's failures.  All (most?) of them were design flaws from complexity, none of them could be saved by overdesign in terms of rounds or params.
So, overdesign can be seen as a sort of end-of-lifecycle bias of hindsight.
Proofs are ... good for cryptographers :)  As I'm not, I can't comment further (nor do I design to them).

@_date: 2013-10-03 12:19:47
@_author: ianG 
@_subject: [Cryptography] encoding formats should not be committee'ised 
SQL, too, had that goal.  4GLs (remember them?).  XML.  Has it ever worked?

@_date: 2013-10-05 11:09:05
@_author: ianG 
@_subject: [Cryptography] encoding formats should not be committee'ized 
Au contraire!  I think what we have shown is that the elements in dispute must be found in the competition.  Not specified beforehand.
Every proposal must include its own encoding, its own crypto suite(s), its own identity-hiding, and dollops and dollops of simplicity.
Let the games begin!

@_date: 2013-10-05 11:41:03
@_author: ianG 
@_subject: [Cryptography] encoding formats should not be committee'ized 
I do similar.  I prohibit reflection and serialization in java.  In C I used to prohibit malloc().
protobufs I would see as just like any external dependency -- trouble, and not good for security.  Like say an external logger or IPC or crypto library.  It would be really nice to eliminate these things but often enough one can't.
On the other hand, if you are not so fussed about security, then it is probably far better to use protobufs to stop the relearning cycle and reduce the incompatibility bugs across a large group of developers.
Right.  Real world is that an org has to call on the talents of a variety of programmers, high-end *and* aspirational, both.  So one tends to prohibit things that complicate the code for the bulk, and one tends to encourage tools that assist the majority.
I'd probably encourage things like protobufs for google.  They have a lot of programmers, and that tends to drive the equation more than other I often wish I had some form of static multiple inheritance in Java...

@_date: 2013-10-11 08:48:35
@_author: ianG 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
What's your goal?  I would say you could do this if the goal was ultimate security.  But for most purposes this is overkill (and I'd include online banking, etc, in that).
Right now we've got a TCP startup, and a TLS startup.  It's pretty messy.  Adding another startup inside isn't likely to gain popularity.
(Which was one thing that suggests a redesign of TLS -- to integrate back into IP layer and replace/augment TCP directly.  Back in those days we -- they -- didn't know enough to do an integrated security protocol.   But these days we do, I'd suggest, or we know enough to give it a try.)

@_date: 2013-10-11 09:24:17
@_author: ianG 
@_subject: [Cryptography] PGP Key Signing parties 
Where is this writ?
+1  I grew up in the "sign-on-first-meet" doctrine.
Good question.
Right.  A signature has to mean something.  What is that something?  The CA world is mumble mumble over semantics, whereas the PGP world openly offers incompatible conventions.  Which is better or worse is beyond me.

@_date: 2013-10-11 15:26:31
@_author: ianG 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
Look at the produce life cycle for known crypto products.  We have some experience of this now.  Skype, SSL v2/3 -> TLS 0/1/2, SSH 1 -> 2, PGP 2 -> 5+.
As a starting point, I would suggest 10 years.

@_date: 2013-10-11 15:28:33
@_author: ianG 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
That same regulator that asked for that capability is somewhat prominent in the current debacle.
Feature or bug?
A shortage of bread has been the inspiration for a few revolutions :)

@_date: 2013-10-14 23:17:42
@_author: ianG 
@_subject: [Cryptography] please dont weaken pre-image resistance of SHA3 
I tend to look at it differently.  There are ephemeral uses and there are long term uses.  For ephemeral uses (like HMACs) then 128 bit protection is fine.
For long term uses, one should not sign (hash) what the other side presents (put in a nonce) and one should always keep what is signed around (or otherwise neuter a hash failure).  Etc.  Either way, one wants here a bit longer protection for the long term hash.
That 'time' axis is how I look at it.  Simplistic or simple?
Alternatively, there is the hash cryptographer's outlook, which tends to differentiate collisions, preimages, 2nd preimages and lookbacks.
 From my perspective the "simpler" statement of SHA3-256 having 128 bit protection across the board is interesting, perhaps it is OK?
I might be able to differentiate the preimage / collision / 2nd pi stuff here if I thought about if for a long time ... but even if I could, I would have no confidence that I'd got it right.  Or, more importantly, my design gets it right in the future.
And as we're dealing with money, I'd *want to get it right*.  I'd actually be somewhat happier if the hash had a clear number of 128.
Um.  Seems like this argument only works if people drop in SHA3 without being aware of the subtle switch in preimage protection, *and* they designed for it earlier on.  For my money, let 'em hang.
Use SHA3-512 or SHA3-384?
What is the preimage protection of SHA3-512 when truncated to 256?  It seems that SHA3-384 still gets 256.
  And generally where
For now, refer to Congress of the USA, it's in Washington DC. Hopefully, it'll be closed soon too...

@_date: 2013-10-15 09:56:45
@_author: ianG 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
Hmm, ok, that is what you said the first time :)
No reply needed, it is unfair to pound the table when you're shutdown!
I see two ways of looking at this.  If the choice is between
     (a) top-bottom layers with complementary strengths, and
     (b) inserting ciphersuites or protocol versions as time goes on
I would definitely go for (a) because it is bounded and we can at least control the effects into the future.
Alternatively, it is making things more complex.  How do we differentiate between that and the simpler case of a single layer?
     (c) single simple layer, done best we can.
I'm unconvinced by the argument that if we can't make things work with one layer, we should use two layers.  It seems to have an element of hope in it, which I don't like.  If in some sense it is more than hope, if it is logical and scientific, why can't we figure that out in one layer?
At a reduction level, we often hope that we can improve a cipher by xoring another cipher with it.  But this amounts to "inventing" a new cipher, so it seems to reduce to a sort of absurdium argument that an xoring programmer can outdo a cryptographer.
I'd also go further.  I would say that the problems that have caused us not to have confidence in protocols have been born out of complexity, and not our ability to do things right the first time, one time.  SSL, etc is a trap of complexity, and the result is a failure in upgrade path.
Also, it seems somewhat hubristic of us -- we continually pretend to predict the future failure.  If we could really do that, we'd fix it now.  Instead, we need to recognise that knowledge advances -- attacks always get better -- and we need a more accepting model to deal with a future where our world has been turned upside down.
Well, is this a glass half full or half empty?
I don't think the record is so bad.  Few attackers have really put a dent in SSL v3 and beyond.  I personally don't see the recent attacks (in their singular form) as being more than annoyances and embarrassments.  To quote Adam Langley [al] "On the whole, the crypto is doing great in comparison to everything else!"  SSH survived nicely. Skype did rather well, and had to be bought out from eBay before it could be perverted.
Empirically, where's the beef?  The one case I can think of where we have clear claims of damages from a protocol layer attack is Bitcoin's Java RNG embarrasment.  Some number of coins were stolen.  How many?  I don't recall, but I doubt the value was in excess of the time-value spent here in the group discussing how to do it better...  Adam reports that Disneyland Tokyo suffered loss of business, probably far more damaging than the
(OTOH, I can imagine the NSA arguing that we're exaggerating the Dual_EC case as no damages have been shown ;-)
If we look at where the problems are occurring, it's (a) outside the crypto, which reminds me:  the best comment I've seen so far is tlscrypt's thoughtful approach on authentication, and (b) in the failure of an upgrade model when problems are identified.
People would use it if it were the only one mode, and it were secure.
If there was a choice, then I think it is very difficult to predict how the usage equation would fall out.  Which means the protection would necessarily be equally impossible to predict, and easy to manipulate.
C.f., the current nonsense with Android and Java SSL suites [android]. Unbelievable!  More evidence that defaults & choice are the friend of your enemy.
[al] [android]

@_date: 2013-10-18 10:47:46
@_author: ianG 
@_subject: [Cryptography] funding Tor development 
Did the lawyers present a rationale for that advice?  I'm interested because some other projects were asking the same question recently.

@_date: 2013-10-19 16:25:38
@_author: ianG 
@_subject: [Cryptography] NIST should publish Suite A 
We now have a crisis of confidence in the cryptographic industry. Agreed?  The Snowden revelations have thrown the deck in the air, and while we have not seen all the cards land as yet, we can draw some points of agreement.
One point of agreement is that public key and Elliptic Curve Cryptography now has a cloud over it.  Just as one example, seen on OpenPGP list (archived therefore open for reposting) is discussion about using 1024 bit curves:
The point here is not that the above argumentation is valid or otherwise, but that *the suspicion runs deep*.  How deep does the EC rabbithole go?
The best I've seen so far is as found on this site  which seems to say (my reading only) that the prior standards work on curves is suspect, but we can do a good job ourselves if we recalculate to best of ability (us meaning not me).
But we really don't know.  Meanwhile, as a side pointer as to how far the 'defaults' trap has taken us, here's another pointer [0]:
Android is using the combination of horribly broken RC4 and MD5 as the first default cipher on all SSL connections . This impacts all apps that did not care enough to change the list of enabled ciphers (i.e. almost all existing apps). This post investigates why RC4-MD5 is the default cipher, and why it replaced better ciphers which were in use prior to the Android 2.3 release in December 2010.
If you're into Java or Android, and you love the JCE, this will leave a sinking pit in your stomach.  A herd of rabbits were stampeded deep down that hole...
I would suggest -- point of agreement? -- that we now have *a crisis of confidence in standards and crypto* .
If I was a standards organisation, or a player who was invested deeply in industry in some sense or other, I'd be also thinking about how to increase confidence.
There is one possibility to increase confidence dramatically:
      what's in Suite A?
If we knew what Suite A used for PK work, we would then be able to triangulate.  Although this is a claim based on absence of evidence, I predict that we'll be able to triangulate the question of ECC and settle the question of confidence.
Treason or revelation?  You pick. This revelation may even be so useful to industry (billion dollar losses?) that it might be a dominating interest over the normal unquestioning patriotic duty of following the say-so of those previously wiser heads in Fort Meade.
It might be cost-effective.  It might even be a 'fair cop'.

@_date: 2013-10-20 12:28:32
@_author: ianG 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
Good example.  I'm going to get off the fence and say that the RNG should never block.
The security/availability tradeoff is *always* a business decision.  In the above example, it is a decision of Postfix.
The problem for the tool above, TLS, is that it doesn't know the business, nor the business decisions.  So it tries to make its product as universal as it can.  Which means, in the eyes of the tool-maker, it should be as secure as it can be.
But not more.  The trap is sprung when in order to make the product that little bit more secure, and defeat the bogeyman threats, an impractical requirement is loaded onto the business.
Examples abound:
   * In the above, Gnu TLS exits.  Bad.
   * With RNGs, they block.  Not good because programmers can't deal with it, and blocked startup sequences can cause deadlocks, and and and... (I recall that FreeBSD 'fixed' this by making /dev/random never block, and equivalent to urandom.  Highly controversial at the time.)
   * With SSL, to solve the bogeyman MITM, they imposed certs and turned off opportunistic opportunities -- and consequently split the security for the web, and made it always vulnerable to an easy downgrade attack.   A billion or so of damages for that 'perfect' decision.
A product should be as secure as it can be *at no cost to the user*, but no more.  Beyond that, the user has to make the decisions, and if the user really cares, the user can read the doco and understand the If they don't do their research, and they need to, then they are negligent.  It is not up to the tool to predict how negligent or needy the caller is.  There is no such thing as perfect.

@_date: 2013-10-22 12:38:28
@_author: ianG 
@_subject: [Cryptography] "Death Note" elimination for hashes 
There's a name I was scared of a long time ago :)
Actually, consumers want a business.  Paypal provided an element in a business being settling the eBay transactions.
Yes, everything else is convenience, and security is part of convenience, but Paypal didn't win just because of convenience.  They won because they found themselves in a business, a subtlety that new payments providers continue to miss today.
Kerckhoffs' 6th principle is the most important and the most forgotten:
"6. Finally, it is necessary, given the circumstances that command its application, that the system be easy to use, requiring neither mental strain nor the knowledge of a long series of rules to observe."
The penalty for breaching K6 is oblivion.
Back on topic -- yes, I agree with the criticism.  While a very interesting thought experiment, the idea of a death notice brings in a lot of complexity, which will make the system less robust.  Including at the business level as pointed out.  The idea that you can simply 'turn off' a part of a protocol and expect the resultant protocol to repair itself is a tough call.  Also, I'm not sure how well it will work when sending two megabybe messages that collide...
But it's definately a fun thought experiment to have over beers.

@_date: 2013-10-30 15:29:12
@_author: ianG 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
Bingo.  Authenticated!
Do we see a multi-phase approach here?
1.  Limit the sources to FIPS-authenticated inputs.
2.  Limit the number of sources that can be used.
3.  Do a deal with all major suppliers of FIPS-authenticated inputs.
4.  Profit.
This is looking like the same multi-pronged strategy that sunk DRBG_EC.

@_date: 2013-10-31 12:15:04
@_author: ianG 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
The assumption that a FIPS-approved entropy source is a good source is now challenged.
If it is a misunderstanding, it's had larger than normal ramifications.   There have been many reports of dropping all external sources as a need to get approval.  It's the process?
The above should be read:  "do a deal with the vendors of FIPS-approved entropy sources."  (If there is any FIPS authentication, I assume that this is a commercial product that is sold for $$$ as otherwise there is little chance of covering the cost of FIPS.  Challenge?  Peter?  Ben?)
The process of the NSA, as apparently expounded in various press articles, is to go to these suppliers and ask in a sort of joking fashion, "hey, are you going to do a back door for me?"  If that conversation works out, isn't rejected, then the approach proceeds.  If the conversation is rejected, they back off.
(Insert normal business approaches here about priorities, government contracts, influence.)
I don't think anyone has published a definitive writeup on this process.   There are multiple confirmations of the above, in some sense or other.   Bruce Schneier mentioned he is collecting stories, he mentioned he already had several so I guess we can await a blog post on the stories.
I understand that it is really tough to sort the tin-foil hatters from the serious folk.  E.g., I frequently got slapped down when I suggested to serious security and crypto people that Skype post-USA-sale could no longer be trusted.  Now we know it is fact;  the protocol was changed and opened up to server manipulation in or around 2008/2009.  USG influence started the moment Skype entered jurisdiction (although maybe not in the way that interests this group).
More tin-foil hattism revealed:
It's really really hard when you work at a job where everyone's entire reputation is dependent on this question.
Fertile ground for interventions, that is precisely the point.
The question probably comes down to:  do you believe that the NSA intervenes in the process?  If not, then you will tend to err on the side of incompetence, security being too hard for people, avoid committees, etc etc.
If you do believe that the NSA has and is interfering in the process, then all bets are off.  We have to establish a theory as to how, when, why, etc.
What's that theory?
Remember, intelligence community are the experts in interfering with processes;  it is their job to do it in other countries.  They've been doing it as a profession since the 1930s in USA, far longer in other We are the amateurs.  We are children compared to their capabilities. Interventions will always be explainable, and adults always know better.
Back to the topic:  my conclusion is to not accept the FIPS standardisation requirement that only FIPS sources be used, and to insist that an independent carefully designed source be added into the mix.  If that costs our FIPS compliance, then so be it.  Call it FIPS compatible and let the marketing dudes worry about it.

@_date: 2013-10-31 12:20:43
@_author: ianG 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
Every intervention will be reasonable in isolation.  Only in full will the real goal become apparent, and chances are we will never know the full story.  They are after all the experts at secrets.  We are not.  We will never ever be brought in to the real story, we can only triangulate from their mistakes.
Absolutely.  In a benign statistical world, this is an important aphorism, and our experiences reinforce this.  The wiser we get, the more we battle with incompetence.
But in a machiavellian world, I turn to things like 48LoP or the sabotage manual.
This latter is so useful that I present it to any board I work with;  it tells them they are the enemy, pogo-like.  The business about intervention doesn't have to be true, and this is still a useful reminder of how they can stuff it up all by themselves.
ps;   Or spy novels;  the famous authors all get their material from ...

@_date: 2013-09-04 09:20:03
@_author: ianG 
@_subject: [Cryptography] Popular curves (was: NSA and cryptanalysis) 
(Not answering your direct question.)  Personally, I was happy to plan on using DJB's Curve25519.  He's done the research and says it is good.   Comments?

@_date: 2013-09-06 11:32:09
@_author: ianG 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
It isn't the whiners that are the NSA plants, but the people behind them, egging them on, while also mounting attacks on the competent honest ones to confuse and bewilder them.
The general process is first to push the group into crap, and then to influence it with competence.  In order to influence, the group's own competence must be neutralised first.
And, controlling processes is just what the NSA does.
The process of an inside takeover is well known in *certain* circles. It only takes one or two very smart competent people to take down an entire organisation.  The mechanisms might well be described as crapification then exploitation.
This is not to say that the IETF WG chairs are NSA plants, nor that all or any particular IETF committee is sunk.  Rather, it is to say that it is very difficult to stop a committee being hopeless, and it's rather easy to tip a good committee into it.
In contrast, it is not well known how to repair the damage once done. The normal method is to abandon ship, swim away, build another ship with 1 or 2 others.

@_date: 2013-09-06 11:52:37
@_author: ianG 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Think bigger picture, think about the intervention possibilities.
E.g., when the NSA goes to a major commercial supplier who is about to ship some product that is SP 800-90, they can agree to indeed do that, but switch around to the Dual EC DBRG.  And still maintain their standards compliance.  As it is likely a closed source, hush-hush area, it can even be done without the adversary (who was once called the customer) knowing.
Right.  If you don't care, just use what the OS provides.  /dev/urandom or CAPI or whatever.  If you do care, you should implement a collector-mixer-DRBG design yourself.

@_date: 2013-09-06 13:13:40
@_author: ianG 
@_subject: [Cryptography] NSA and cryptanalysis 
It looks like it is "all of the above."  These are the specific interventions I have seen mention of so far:
* weakened algorithms/protocols for big players (e.g., GSM, Cisco)
* weakening of RNGs
* inside access by 'covert agents' to hand over secrets (e.g., big 4)
* corruption of the standards process (NIST 2006?)
* corruption of certification process (CSC)
* crunching of poor passwords
* black ops to steal keys
* black ops to pervert systems
Which makes sense.  Why would the biggest player just do "one thing" ? No, they are going to do everything within their power.  They'll try all the tricks.  Why not, they've got the money...
What is perhaps more interesting is how these tricks interplay with each other.  That's something that we'll have trouble seeing and imagining.

@_date: 2013-09-06 22:33:00
@_author: ianG 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Oops, for those unfamiliar with CAcert's peculiar use of secure browsing, drop the 's' in the above URL.  Then it will securely load.
(thanks Joe!)

@_date: 2013-09-06 22:46:14
@_author: ianG 
@_subject: [Cryptography] People should turn on PFS in TLS (was Re: Fwd: 
So was SSL ;-)  Sorry, couldn't resist...
That's what the crims do to, they ask for all the numbers, they don't bother much with SSL.

@_date: 2013-09-07 10:19:10
@_author: ianG 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
In principle, it doesn't -- which is why SOPs are saboteur's tools of preference.  They are used against you, as the lesser experienced people can't see the acts behind [1]
The point is one of degree.  SOPs are there to resolve real disputes. They can also be used to cause disputes, and to turn any innocent thing into a fight.  So do that, and keep doing that!  Pretty soon the org becomes a farce.
In contrast, strong leadership (the chair) knows when to put the lid on such trivialities and move on.  So, part of the overall strategy is to neutralise the strong chair [2].  As John just reported:
   *  NSA employees participted throughout, and occupied leadership roles
      in the committee and among the editors of the documents
Slam dunk.  If the NSA had wanted it, they would have designed it themselves.  The only conclusion for their presence that is rational is to sabotage it [3].
[0]   SOPs is standard operating procedures.
[1]   This is the flaw in "don't attribute to malice what can be explained by incompetence."  Explaining by incompetence does not eliminate that malice inspired incompetence.  Remember, we are all innoculated against malice, so we prefer to see benign causes.
[2]  this is not to say that committees are ill-intentioned or people are bad, but that it only takes a few with malicious intent and expertise to bring the whole game to a halt.  Cartels such as IETF WGs are fundamentally and inescapably fragile.
[3]  as a sort of summer-flu-shot, I present that document to each new board as their SOPs.

@_date: 2013-09-07 10:30:20
@_author: ianG 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
I'm not as yet seeing that a block cipher with a backdoor is a public key system, but I really like the mental picture this is trying to create.
In order to encrypt to that system, one needs the (either) key.  If everyone has it (either) the system is ruined.
A public key system is an artiface where one can distribute the public key, and not have to worry about the system being ruined;  it's still perfectly usable.  Whereas with a symmetric system with two keys, either key being distributed ruins the system.
One could argue that the adversary would prefer the cleaner, more complete semantics of the public key system -- maybe that is what the theorem assumes?  But if I was the NSA I'd be happy with the compromise.   I'm good at keeping *my key secret* at least.

@_date: 2013-09-07 10:35:12
@_author: ianG 
@_subject: [Cryptography] People should turn on PFS in TLS 
The sentiment I agree with.  But the record of such transitions is not good.
E.g., Back in September 2009 Ray & Dispensa discovered a serious bug with renegotiation in SSL.  According to SSL Pulse, it took until around April of this year [0] before 80% of the SSL hosts were upgraded to cover the bug.
Which gives us an OODA response loop of around 3-4 years.
And, that was the best it got -- the SSL community actually cared about that bug.  It gets far worse in stuff that they consider not to be a bug, such as HTTPS Everywhere, TLS/SNI, MD5, browser security fixes for phishing, HTTP-better-than-self-signed, HTTPS starting up with its own self-signed cert, etc, etc.
[0] it depends on how you measure the 80% mark, though.
PS: More here on OODA loops

@_date: 2013-09-07 10:57:07
@_author: ianG 
@_subject: [Cryptography] Why prefer symmetric crypto over public key 
It's a big picture thing.  At the end of the day, symmetric crypto is something that good software engineers can master, and relatively well, in a black box sense.  Public key crypto not so easily, that requires real learning.  I for one am terrified of it.
Therefore, what Bruce is saying is that the architecture should recognise this disparity, and try and reduce the part played by public key crypto.  Wherever & whenever you can get part of the design over to symmetric crypto, do it.  Wherever & whenever you can use the natural business relationships to reduce the need for public key crypto, do that ps;

@_date: 2013-09-07 12:19:24
@_author: ianG 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
If so, then the domain owner can deliver a public key with authenticity using the DNS.  This strikes a deathblow to the CA industry.  This threat is enough for CAs to spend a significant amount of money slowing down its development [0].
How much more obvious does it get [1] ?
[0] If one is a finance geek, one can even calculate how much money the opponents are willing to spend.
[1] As an aside, NSA/DoD have invested significant capital in the PKI as well.  Sufficient that they will be well aligned with the CA mission, and sufficient that they will approve of any effort to keep the CAs in business.  But this part is far less obvious.

@_date: 2013-09-08 13:50:55
@_author: ianG 
@_subject: [Cryptography] Does NSA break in to endpoints (was Re: Bruce 
The eye-opener for me was that they were investing and trying in every known attack.  They are acting like true economic attackers, try everything, and select the one that generates the best ROI.  Just like the industrialised phishing/hacking gangs that emerged in the 2000s...

@_date: 2013-09-08 20:31:13
@_author: ianG 
@_subject: [Cryptography] Trapdoor symmetric key 
Thanks.  This far better explains the conundrum.  There is a big difference between a conceptual public key algorithm, and one that is actually good enough to compete with the ones we typically use.

@_date: 2013-09-09 10:36:03
@_author: ianG 
@_subject: [Cryptography] Techniques for malevolent crypto hardware 
When I audited the process for root key ceremony for CAcert, I worried a fair bit about randomness.  I decided the entropy was untestable (therefore unauditable).
So I wrote a process such that several people would bring their own entropy source.  E.g., in the one event, 3 sources were used, by independent people on independent machines:
   * I used a sha-stream of laptop camera on dark paper [0]
   * Teus used sound card driver [1]
   * OpenSSL's RNG.
The logic was that as long as one person was honest and had a good source, and as long as our mixing was verifiable, the result would be good.
Then, I wrote a small C program to mix it [2];  as small as possible so a room full of techies could spend no more than 10 minutes checking it on the day [3].
The output of this was then fed into the OpenSSL script to do the root key.  (I'm interested if anyone can spot a flaw in this concept.)
[0] This idea from Jon Callas from memory, the idea is that the lack of light and lack of discrimination between pixels drives the photocells into a quantam uncertainty state.
[1] John Denker's sound card driver.
[2] As an amusing sidenote, I accidentally used | to mix the bytes not ^.  My eyeball tests passed at 2 sources but at 3 sources it was starting to look decidedly wonky.
[3] It was discussed on the group at this time, it was advised that the output of the mix should be sha'd, which I eventually agreed with, but I don't think I did in the event.

@_date: 2013-09-09 10:48:08
@_author: ianG 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Hi Jeffery,
Granted!  I do not want to say that the IETF people are in a conspiracy with someone or each other, or that they are not hard workers [0].
But, I do want to say that, when it comes to security, we now have enough history and experience to suggest:
     the committee may be part of the problem [1],
     it is not clear that it can ever be part of the solution.
Insultingly;  those who've spent a decade or so devoting themselves to this process will not take to that notion kindly.  It's sad and frustrating -- I also spent a lot of time & money pushing OpenPGP code

@_date: 2013-09-09 10:58:11
@_author: ianG 
@_subject: [Cryptography] The One True Cipher Suite 
Something I wrote a bunch of years ago seems apropos, perhaps minimally as a thought experiment:
Hypothesis  -- The One True Cipher Suite
In cryptoplumbing, the gravest choices are apparently on the nature of the cipher suite. To include latest fad algo or not? Instead, I offer you a simple solution. Don't.
     There is one cipher suite, and it is numbered Number 1.
Cypersuite  is always negotiated as Number 1 in the very first message. It is your choice, your ultimate choice, and your destiny. Pick If your users are nice to you, promise them Number 2 in two years. If they are not, don't. Either way, do not deliver any more cipher suites for at least 7 years, one for each hypothesis.
            And then it all went to pot...
We see this with PGP. Version 2 was quite simple and therefore stable -- there was RSA, IDEA, MD5, and some weird padding scheme. That was it. Compatibility arguments were few and far between. Grumbles were limited to the padding scheme and a few other quirks.
Then came Versions 3-8, and it could be said that the explosion of options and features and variants caused more incompatibility than any standards committee could have done on its own.
            Avoid the Champagne Hangover
Do your homework up front.
Pick a good suite of ciphers, ones that are Pareto-Secure, and do your best to make the combination strong [1]. Document the short falls and do not worry about them after that. Cut off any idle fingers that can't keep from tweaking. Do not permit people to sell you on the marginal merits of some crazy public key variant or some experimental MAC thing that a cryptographer knocked up over a weekend or some minor foible that allows an attacker to learn your aunty's birth date after asking a million times.
Resist the temptation. Stick with The One.

@_date: 2013-09-09 11:29:19
@_author: ianG 
@_subject: [Cryptography] Why are some protocols hard to deploy? (was Re: 
It's also worth remembering that one reason the Internet succeeded was that it did not need the permission of the local telcos and the purchase of expensive ISO/OSI stuff from the IT companies in order to get up and This lesson is repeated over and over again.  Eliminate permission, and win.  Insert multiple permission steps and lose.

@_date: 2013-09-09 13:50:31
@_author: ianG 
@_subject: [Cryptography] Market demands for security (was Re: Opening 
Part of the problem is that we are trained to label ideas that we find uncomfortable as conspiracy theories.  And, when they are shown to be true, we aren't ready to apologise for our slapping down of those wretches so labelled.
It's far better to talk in risk terms.  Yes, the NSA could have hacked the RNG in intel's chips.  But what is the likelhood?  Low?  Medium? High?  Everyone can choose, and now the desire to slap down is detuned.
Same with NSA infiltrating the IETF.  Yes, we can agree it is a risk.
But what steps has the IETF taken to mitigate it?  It is an open forum, we can check the bona fides of all players, we can read their comments forever, etc etc.  We can therefore all (personally) decide on whether the risk is adequately mitigated.  And whether to do more mitigation at the individual level.
I think we can just about comfortably put our own professional difficulties into risk analysis, and agree to differing levels of risk.   But once we get into non-security issues such as racism, politics, etc, our ability to be objective rapidly diminishes.
(I don't disagree with what is said above, I just agree we can't talk productively at that level...)

@_date: 2013-09-10 09:30:14
@_author: ianG 
@_subject: [Cryptography] Random number generation influenced, HW RNG 
The question of whether one could simulate a raw physical source is tantalising.  I see diverse opinions as to whether it is plausible, and thinking about it, I'm on the fence.
I'd say it might be an unstudied problem -- for us.  It's sounding like an interesting EE/CS project, masters or PhD level?
If anyone has studied it, I'd bet fair money that the NSA has.

@_date: 2013-09-11 05:57:58
@_author: ianG 
@_subject: [Cryptography] Availability of plaintext/ciphertext pairs (was 
In a protocol I wrote with Zooko's help, we generate a random IV0 which is shared in the key exchange.
Then, we also move the padding from the end to the beginning, fill it with a non-repeating length-determined value, and expand it to a size of 16-31 bytes.  This creates what is in effect an IV1 or second transmitted IV.

@_date: 2013-09-15 12:41:57
@_author: ianG 
@_subject: [Cryptography] real random numbers 
He's not tossing it out, he's saying that it is no basis for measurement.
Think of the cryptography worldview -- suppliers of black boxes (MDs, encryptions, etc) to the software world are obsessed about the properties of the black box, and suppliers want them to be reliable and damn near perfect.  No come back, no liability.
Meanwhile, in the software world, we think very differently.  We want stuff that is "good enough" not perfect.  That's because we know that systems are so darn complex that the problems are going to occur elsewhere -- either other systems that don't have the cryptographic obsession, our own mistakes or user issues.
E.g., SHA1 is close to perfect for almost all software needs, but for the cryptographers, it isn't good enough any more!  "We must have SHA2, SHA3, etc."  The difference for most real software is pretty much like how many bit angels can dance on a pinhead.
As John is on the supplier side, he needs a measurement that is totally reliable and totally accurate.  Squish must therefore be dropped from that measurement.
Once the adversary has done that, all bets are off.  The adversary can now probably count the keys bits in use, and is probably at the point where they can interfere at the bit level.
Typically, we don't build designs to that threat model, that way lies TPMs and other madness.  In risk terms, we accept that risk, the user loses, and we move on.
It is a problem.  Those on the supplier side of the divide cannot deliver the water unless it is pure enough.  Those on the builder side don't need pure water when everything else is so much sewage.  But oh well, life goes on.
Right.  The more the merrier.

@_date: 2013-09-16 14:33:34
@_author: ianG 
@_subject: [Cryptography] djb's McBits (with Tung Chaou and Peter Schwabe) 
McBits: fast constant-time code-based cryptography
This paper presents extremely fast algorithms for code-based
public-key cryptography, including full protection against timing attacks. For example, at a 2^128 security level, this paper achieves a reciprocal decryption throughput of just 60493 cycles (plus cipher cost etc.) on a single Ivy Bridge core. These algorithms rely on an additive FFT for fast root computation, a transposed additive FFT for fast syndrome computation, and a sorting network to avoid cache-timing attacks.
CHES 2013 was late August, already.  Was anyone there?  Any comments on (skimming paper reveals one gotcha -- huge keys, 64k for 2^80 security.   I'm guessing that FFT==fast fourier transform.)

@_date: 2013-09-17 12:48:03
@_author: ianG 
@_subject: [Cryptography] The paranoid approach to crypto-plumbing 
Hi Bill,
The problem with adding multiple algorithms is that you are also adding complexity.  While you are perhaps ensuring against the failure of one algorithm, you are also adding a cost of failure in the complexity of E.g., as an example, look at the current SSL search for a secure ciphersuite (and try explaining it to the sysadms).  As soon as you add an extra algorithm, others are tempted to add their vanity suites, the result is not better but worse.
And, as we know, the algorithms rarely fail.  The NSA specifically targets the cryptosystem, not the algorithms.  It also doesn't like well-constructed and well-implemented systems.  (So before getting too exotic with the internals, perhaps we should get the basics right.)
In contrast to the component duplication approach, I personally prefer the layering duplication approach (so does the NSA apparently).  That is, have a low-level cryptosystem that provides the base encryption and authentication properties, and over that, layer an authorisation layer that adds any additional properties if desired (such as superencryption).
One could then choose complementary algorithms at each layer.  Having said all that, any duplication is expensive.  Do you really have the evidence that such extra effort is required?  Remember, while you're building this extra capability, customers aren't being protected at all, and are less likely to be so in the future.

@_date: 2013-09-17 12:49:46
@_author: ianG 
@_subject: [Cryptography] The paranoid approach to crypto-plumbing 
A good question.  In my work, I've generally modelled it such that the entire system still works if one algorithm fails totally.  But I don't have a name for that approach.

@_date: 2013-09-18 10:16:30
@_author: ianG 
@_subject: [Cryptography] An NSA mathematician shares his 
Speaking as a non-American, you guys have big problems concerning the nexus of cryptography and politics.
I really, really doubt that.  I don't really wish to attack the author, but the style and phraseology is pure PR.  Ordinary people do not write PR.  Nor do they lay out political strategies and refer to their commander-in-chief as the supreme leader.  Nor indeed are employees of military and intelligence *permitted to talk to the press* unless sanctioned at high level.
illegally or surreptitiously targeting or tracking the communications of other Americans?
Of course, Americans talking to Americans might be one debate.  But then there are Americans talking to the world, and people talking to people.
It should be remembered that espionage is illegal, and the activities of the NSA are more or less illegal *outside their borders*.  I give them no permission to monitor me or mine, and nor does any of the laws of my The fact that we cannot stop them doesn't make it any less legal.  The fact that there is a gentleman's agreement between countries to look the other way doesn't make it any less palatable to us non-gentlepersons excluded from the corridors of powers.
And all that doesn't make NSA mathematicians any less a partner to the activity.  Any intelligence agent is typically controlled and often banned from overseas travel, because of the ramifications of this activity.
In two paras above, and the next two paras below, this 'mathematician' lays the political trap for Americans.  The collection by the federal government of data is almost certainly unconstitutional.  Yet, everyone acts as if that's ok because ... we live in the new world of Big Data?
Unless one subscribes to the plain wording of your (American) ditto, repeat.
Although, to be honest, we-the-world don't care about it;  the USG's temptation to rewrite the constitution in the minds of its subjects is strictly a domestic political affair.  For most other countries, the Big Data genie is truly out of the bottle, and there's precious little we can do about it.
Good luck!
I seriously doubt that.

@_date: 2013-09-18 11:05:46
@_author: ianG 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
Yes, that's the logical consequence & approach to managing risks. Mitigate the attack, to push attention to easier and less costly attacks, and then start working on those.
There is a mindset in cryptography circles that we eliminate entirely the attacks we can, and ignore the rest.  This is unfortunately not how the real world works.  Most of risk management outside cryptography is about reducing risks not eliminating them, and managing the interplay between those reduced risks.  Most unfortunate, because it leads cryptographers to strange recommendations.
If other attacks are more costly to defender and easyish for the attacker, then perhaps it is bad.  But it isn't really a common approach in our security world to leave open the easiest attack, as the best alternative.  Granted, this approach is used elsewhere (in warfare for example, minefields and wire will be laid to channel the attack).
If we can push an attacker from mass passive surveillance to targetted direct attacks, that is a huge win.  The former scales, the latter does not.
If I understand it correctly, PRISM is or has become the byword for the NSA's vacuuming of all traffic for mass passive surveillance.  In which case, this is the first attack of all, and the most damaging, because it is undetectable, connects you to all your contacts, and stores all your open documents.
 From the position of a systems provider, mass surveillance is possibly the most important attack to mitigate.  This is because:  we know it is done to everyone, and therefore it is done to our users, and it informs every other attack.  For all the other targetted and active attacks, we have far less certainty about the targetting (user) and the vulnerability (platform, etc).  And they are very costly, by several orders of magnitude more than mass surveillance.

@_date: 2013-09-19 13:12:42
@_author: ianG 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
Hi John,
(I think we are in agreement here, there was just one point below where I didn't make myself clear.)
Right.  So the amount of effort we should put in should not be dictated (solely) by received wisdom about perfect security, but (also) by how quickly we can push the bulk of the attackers elsewhere.  Thus releasing our costly resources for 'elsewhere'.
I wrote about this tradeoff many moons ago.  I called the preferred target Pareto-secure as a counterpoint to the expected 100% secure, which I defined as a point where there is no Pareto-improvement that can be made, because the attacker is already pushed elsewhere.
The other side of the coin is to have a gentler attitude to breaches.
When a breach is announced, we also need to consider whether anyone has actually lost anything, and whether the ones that weren't attacked have got good service.  A protocol is rarely broken for the user, even if the cryptographic world uses the word 'broken' for a few bits.  E.g., if one looks at the TLS changes of the last 5 years due to a series of attacks, there isn't much of a record of actual hacks to users.
To some extent, mass passive surveillance is entirely possible because SSL/TLS is so poorly employed.  I haven't looked for a while, but it was always about 1% of web traffic.
This is the motive behind HTTPS Everywhere - All The Time.  Let's make SSL the norm not the exception.  Then we've got some security against passive surveillance, then we force the attacker to other attacks, which are typically much more expensive.
Just to point out, in the above I meant 'systems provider' not as an end-user-facing services supplier, but as a cryptographic protocol/tool provider.  E.g., OpenSSL is the latter, whereas gmail.com is the former.
Right.  So this issue has become substantially complicated for (a) very large suppliers such as google/apple/microsoft because they control every part of the supply chain and we are reduced to 2-eyes verification, and (b) closed source suppliers like skype because they can slide in their non-contractual sharing without anyone noticing.

@_date: 2013-09-23 11:20:53
@_author: ianG 
@_subject: [Cryptography] RSA recommends against use of its own products. 
Etc.  Yes, we expect the company to declare itself near white, and the press to declare it blacker than the ace of spaces.
Meanwhile, this list is about those who know how to analyse this sort of stuff, independently.  So...
I don't see a lot of distance between choosing Dual_EC as default, and the conclusion that BSAFE & user-systems are insecure.
The question that remains is, was it an innocent mistake, or were they influenced by NSA?
We don't have much solid evidence on that.  But we can draw the dots, and a reasonable judgement can fill the missing pieces in.

@_date: 2013-09-23 15:15:13
@_author: ianG 
@_subject: [Cryptography] RSA equivalent key length/strength 
1024 bits is pretty good, and there's some science that says it's about right.  E.g., risk management says there is little point in making a steel door inside a wicker frame.
The problem is more to do with distraction than anything else.  It is a problem that people will argue about the numbers, because they can compare numbers, far more than they will argue about the essentials. There is a psychological bias to beat ones chest about how tough one is on the numbers, and thus prove one is better at this game than the enemy.
Unfortunately, in cryptography, almost always, other factors matter more.
So, while you're all arguing about 1024 versus 4096, what you're not doing is delivering a good system.  That delay feeds in to the customer equation, and the result is less security.  Even when you finally compromise on 1964.13 bits, the result is still less security, because of other issues like delays.
Yeah, they are getting confused (compatibility failures) from too much choice.  Never a good idea.  Take out the choice.  One number.  Get back to work.

@_date: 2013-09-24 11:52:46
@_author: ianG 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
I think, if we are about redesigning and avoiding the failures of the past, we have to unravel the false assumptions of the past...
Reasonable people may disagree with that claim.
PKI for the web was designed to secure *one small part* of the financial process -- sending credit card numbers over the net.  To secure financial transactions without limit, we'd need an end-to-end solution.   E.g., online banking (which comes much later) requires an authentication solution, which offering by WebPKI (the client cert) is infamously not used;  and, as a counterpoint, the biggest hacks occur at the server, being that "large part" of financial transactions that WebPKI explicitly ignored.
Further, "very well" is a gross exaggeration of marketing proportions. In order to say it works "very well" at even its small part of protecting access to servers, we'd have to solve the browser authentication problem that is at the root cause of phishing.  I grant that the phishing bug was addressed at a level of PKI-me-harder, but we still lack a solution...
Oh, they broke it.  Criminals send an unauthenticated URL and the user goes to that URL.  The browser doesn't notice, the user doesn't notice, and the implementors conspire not to notice.  WebPKI is totally broken.   The fact that the criminals didn't follow the cutesy rules laid out in the WebPKI security model is not a circumvention but a breach and an excuse -- the rules weren't applicable to the real world.
And, regardless of whether we decide that it is circumvention or breach, nothing positive was ever done about it.  So we're left arguing about the point of something that is too easy to circumvent and doesn't get fixed.  WebPKI is either an historical oddity or an economic drag on real security.
(Quite where reasonable people might have a reasonable disagreement is where the breach/circumvention is;  that's an argument that will (and did) roll on for a decade, which is perhaps why it never gets fixed... insert long thread.)
Yes.  Challenge is to get that into the supply chain.
It's either that, or bypass completely.  I agree email looks difficult, and the economics suggest bypass not rebuild.
Curiously (digression), Paypal bought Skype for a secure end-to-end solution to many of these problems.  They never capitalised on it.  Did they ever say why?

@_date: 2013-09-25 19:17:27
@_author: ianG 
@_subject: [Cryptography] RSA equivalent key length/strength 
The trap of a false sense of security is far outweighed by the benefit of a "good enough" security delivered to more people.
We're talking multiple orders of magnitude here.  The math that counts is:
    Security = Users * Protection.

@_date: 2013-09-25 19:31:50
@_author: ianG 
@_subject: [Cryptography] RSA recommends against use of its own products. 
Hi Jerry,
I appreciate the devil's advocate approach here, it has helped to get my thoughts in order!  Thanks!
My conclusion is:  avoid all USA, Inc, providers of cryptographic products.  Argumentation follows...
Well, defaults being defaults, we can assume most people have left it in default mode.  I suppose we could ask for research on this question, but I'm going to guess:  most.  Therefore we could say that BSAFE is "mostly" unsafe, but as we don't know who is using it in default mode, I'm sure most cryptography people would agree that means "unsafe, period."
Firstly, this is to suggest that quality of implementation is the issue.   It isn't, the issue is whether the overall result is safe -- to end-users.  In this case, it could be fantastic code, but if the RNG is spiked, then the fantastic code is approx. worthless.
Reminds me of what the IRA said after nearly knocking off Maggie Thatcher:
     "Today we were unlucky, but remember we only have to be lucky once.
     You will have to be lucky always."
Secondly, or more widely, if the NSA has targetted RSA, then what can we conclude about quality of the rest of the implementation?  We can only make arguments about the rest of the system if we assume this was a one-off.  That would be a surprising thing to assume, given what else we * knowing it was an innocent mistake:  well, everyone makes them, even Debian.  So perhaps these products aren't so bad?
* knowing it was an influenced result:   USA corporations are to be avoided as cryptographic suppliers.  E.g., JCE, CAPI, etc.
Supporting assumptions:
1. assume the NSA is your threat model.  Once upon a time those threatened were a small group of neerdowellers in far flung wild countries with exotic names.  Unfortunately, this now applies to most people -- inside the USA, anyone who's facing a potential criminal investigation by any of the USA agencies, due to the DEA trick.  So most of Wall Street, etc, and anyone who's got assets attachable for ML, in post-WoD world, etc.  Outside the USA, anyone who's 2 handshakes from any neerdowellers.
2. We don't as yet have any such evidence from non-USA corps, do we? (But I ain't putting my money down on that...)
3. Where goes RSA, also follows Java's JCE (recall Android) and CAPI. How far behind are the rest?
4. Actually, we locals on this list already knew this to a reasonable suspicion.  But now we have a chain of events that allows a reasonable person outside the paranoiac security world to conclude that the NSA has corrupted the cryptography delivery from a USA corp.
True, 2005 or thereabouts, such a "story" could be and was told, and we can accept for the sake of argument it might not have been a "mistake" given what they knew.
That ended 2007.  RSA was no doubt informed of the results as they happened, because they are professionals, now conveniently listed out by Mathew Greene:
At that point the story unravels.  At that point, many would have concluded to change default, or even dropped the collector entirely.  If they were doing their job!  At that point, RSA did not change the default, and we move from "innocent mistake" to "negligence".
We know, to as great an extent as we ever can.  The evidence problem has to be seen in context.  Here is what we know so far:
1. the NSA will never ever give us the evidence, and it will even lie to a court about it.  They will go to the grave rather than admit it.
2. the NSA has perverted the standards process.
3. the NSA has perverted a commercial supplier of cryptography.
4. the NSA has a pattern and practice of repeated attempts at the above.
5. they are the spooks -- they wrote the book on deception.
6. a perverted supplier will always put a positive spin on it.  It's their livelihood, after all.  Can't blame them for trying to survive.
I suggest the above are facts, but anyone is free to establish their own.
In such a context, resting on lack of evidence of the "smoking gun" variety, and applying the legal theory of "guilty until proven innocent" is inappropriate.  I feel we could get away with that if the job is marketing, but inappropriate and negligent if the job is security.
Well, obviously this was a 2 phase story.  Either phase can fail, thus rendering the result fail.  But to conclude that RSA's part as meaningless is somewhat weird, that's the essence of finger-pointing. They had a choice, and they had a duty of care.  Pointing at NIST doesn't change that.
According to the information seen, and what I've been told informally in answer to my questions, that is exactly what happened:  Under influence of a large government contract, RSA was directly influenced ("encouraged") to make Dual_EC into the default.  "For the benefit of us Of course, there is no smoking gun, and Lucky's characterisation of the NSA paying $10m to spike the RNG could be said to be inaccurate and journalistic.  But not a completely wrong picture.
The NSA isn't that stupid.  Should we be?
 From the above facts, can we conclude either that
    (i) we have found the *one and only one* flaw,
or, are we wiser to conclude that
    (ii) we found /one of the/ flaws?
As to your questions:
(a) we cannot conclude BSAFE is safe, or not safe.  We never could, nor can we now, for this or any other product.  What we can ask is how much safer or less safe it is, now that we know what we know.
Which is to say, there are no absolutes, and the framing of the question as an absolute is a trap.
We can only analyse BSAFE in terms of options -- would it be (for example) now safer to switch to another provider?   Building on the theme of alternates, according to NIST records, these use Dual_EC:
RSA, Thales, Catbird, McAfee, Cummings, OpenSSL, ARX, Certicom, RIM/Blackberry, Mocana, Microsoft, Cisco, Juniper, Blackberry, OpenPeak, Samsung, Symantec, Riverbed, CoCo, Kony, Lancope, SafeNet, SafeLogic, Panzura, GE Healthcare.
(b) If we assume one and only one flaw, then, turning off the default Dual_EC is "safe".
(c) If we assume one and only one flaw, sure.
(d) Well, and there we have it.  If we already knew not to use Dual_EC, what to make of the current discussion?  One rule for RSA, one for others?
I think the conclusion is reasonable:  Avoid all USA cryptographic providers.  I guess that will cause some to be upset, but if they are upset, I'd ask how we can establish some reasonable judgement over the above questions, and specifically, why we can't conclude that all sizable targets within USA, Inc are targetted for perversion by the NSA?

@_date: 2013-09-26 08:21:39
@_author: ianG 
@_subject: [Cryptography] RSA recommends against use of its own products. 
Ah well, that is the sticky question.  If we accept the conclusion, I see these options:
1.  shift to something more open.
2.  use foreign providers.
3.  start writing.
4.  get out of the security game.
Right, scratch the Brits and the French.  Maybe AU, NZ?  I don't know. Maybe the Germans / Dutch / Austrians.
If you are referring to testing a provider's product for leaks, I think that's darn near impossible.
(If referring to the platform and things like leakage, that is an additional/new scope.)
As I have said, if you care, you write your own collector/mix/DRBG.  If not, then you're happy reading /dev/random.
(for the rest, all agreed.)

@_date: 2013-09-26 09:20:37
@_author: ianG 
@_subject: [Cryptography] RSA recommends against use of its own products. 
Nice.  Or, as I heard somewhere, there is only one mode, and it is secure.
Today?s internet presumes that individuals are capable of configuring software to address issues such as spam, security, indecent content, and privacy. This assump- tion is worrying ? common sense and empirical evidence state that not everyone is so interested or so skilled. When regulatory decisions are left to individuals, for the unskilled the default settings are the law. This article relies on evidence from the deployment of wireless routers and finds that defaults act as de facto regu- lation for the poor and poorly educated. This paper presents a large sample beha- vioral study of how people modify their 802.11 (?Wi-Fi?) wireless access points from two distinct sources. The first is a secondary analysis of WifiMaps.com, one of the largest online databases of wireless router information. The second is an original wireless survey of portions of three census tracts in Chicago, selected as a diversity sample for contrast in education and income. By constructing lists of known default settings for specific brands and models, we were then able to ident- ify how people changed their default settings. Our results show that the default settings for wireless access points are powerful. Media reports and instruction manuals have increasingly urged users to change defaults ? especially passwords, network names, and encryption settings. Despite this, only half of all users change any defaults at all on the most popular brand of router. Moreover, we find that when a manufacturer sets a default 96?99 percent of users follow the suggested behavior, while only 28?57 percent of users acted to change these same default settings when exhorted to do so by expert sources. Finally, there is also a suggestion that those living in areas with lower incomes and levels of education are less likely to change defaults, although these data are not conclusive. These results show how the authority of software trumps that of advice. Consequently, policy-makers must acknowledge and address the power of software to act as de facto regulation.

@_date: 2013-09-26 09:52:39
@_author: ianG 
@_subject: [Cryptography] RSA equivalent key length/strength 
I'm sorry, I don't deal in omniscience.  Typically we as suppliers of some security product have only the faintest idea what our users are up to.  (Some consider this a good thing, it's a privacy quirk.)
With that assumption, the various i's you list become some sort of average.  This is why the security model that is provided is typically one-size-fits-all, and the most successful products are typically the ones with zero configuration and the best fit for the widest market.
Right, and you know that, how?
(how valuable each person's info is, I mean.)
2nd order effects from the claim of security, granted.  Which effects they are, is again subject to the law of averages.
Ah, and therein lies the rub.  Maybe.  This doesn't mean it will.
Typically, the fallacy of false sense of security relies on an extremely unusual or difficult attack (aka acceptable risk).  And then ramps up that rarity to a bogeyman status.  So that everyone is scared of it. And we must, we simply must protect people against it!
Get back to science.  How risky are these things?
(You'll see this play out in phishing.  Banks are the number one target for attacks on secure browsing.)
You are resting on taught wisdom about TLS, which is oriented to a different purpose than security.
In practice, a direct attack against TLS is very rare, a direct attack against your browser connection to your bank is very rare, and a direct attack against your person is also very rare.
This is why for example we walk the streets without body armour, even in Nairobi (this week) or the Beltway (11 years ago).  This is why there are few if any (open question?) reported breaches of banks due to the BEAST and other menagerie attacks against TLS.
We can look at this many ways, but one way is this:  the margin of fat in TLS is obscene.  If it were sentient, it would be beyond obese, it would be a circus act.  We can do some dieting.
Well, right.  So, as TLS is supposed to be primarily (these days) focussed on protecting your bank account access, and as its auth model fails dismally when it comes to phishing, why do we care about something so exotic as the NSA?
Get back to basics.  Let's fix the TLS so it actually does the client - webserver auth problem first.
1024 is good enough for that, for now, but in the meantime prepare for something longer.  (We now have evidence of some espionage spear phishing that bothered to crunch 512.  Oh happy day, some real evidence!)
As for the NSA, actually, 1024 works fine for that too, for now.  As long as we move them from easy decryption to actually having to use a lot of big fat expensive machines, we win.  They then have to focus, rather than harvest.  Presumably they have not forgotten how to do that.
In ten years, the NISTs, the committees, the vendors, and the CAs will still not have addressed the number one threat to users -- that's the i's in your equation.  See also Lynn Wheeler's post about the persistence of ignoring the customers' risk.
But I'm pretty confident that in ten years, they will have addressed the 1024 limit that annoys the NSA.  That's the power of the bogeyman.
I don't disagree.  Hopefully, a few more of these stunts will finally convince them that they can't go on like this.  In military thinking, we call it preparing for the last war.
In the meantime, if they are going for a stunt, don't we want it to be the most convenient, cost-effective stunt?  So they can get back to serious business?  1024 sounds fine...
PS: btw, was this whole debate about a BCP?  If so, it is a woftam.  Any time spent on best practices is lost to society.  You'll never get those seconds back, guys.

@_date: 2013-09-28 22:06:24
@_author: ianG 
@_subject: [Cryptography] RSA equivalent key length/strength 
What's in Suite A?  Will probably illuminate that question...

@_date: 2013-09-29 10:51:26
@_author: ianG 
@_subject: [Cryptography] TLS2 
SSL/TLS is a history of fiddling around at the edges.  If there is to be any hope, start again.  Remember, we know so much more now.  Call it TLS2 if you want.
Start with a completely radical set of requirements.  Then make it so. There are a dozen people here who could do it.
Why not do the requirements, then ask for competing proposals?  Choose 1.  It worked for NIST, and committees didn't work for anyone.
A competition for TLS2 would bring out the best and leave the bureaurats fuming and powerless.

@_date: 2013-09-30 11:16:27
@_author: ianG 
@_subject: [Cryptography] check-summed keys in secret ciphers? 
I'm not really understanding the need for checksums on keys.  I can sort of see the battlefield requirement that comms equipment that is stolen can't then be utilized in either a direct sense (listening in) or re-sold to some other theater.
But it still doesn't quite work.  It seems antithetical to NSA's obsession with security at Suite A levels, if they are worried about the gear being snatched, they shouldn't have secret algorithms in them at all.
Using checksums also doesn't make sense, as once the checksum algorithm is recovered, the protection is dead.  I would have thought a HMAC approach would be better, but this then brings in the need for a centralised key distro approach.  Ok, so that is typically how battlefield codes work -- one set for everyone -- but I would have thought they'd have moved on from the delivery SPOF by now.
It also seems a little overdone to do that in the algorithm.  Why not implement a kill switch with a separate parallel system?  If one is designing the hardware, then one has control over these things.
I guess then I really don't understand the threat they are trying to address here.
Any comments from the wider audience?

@_date: 2013-09-30 11:41:26
@_author: ianG 
@_subject: [Cryptography] encoding formats should not be committee'ized 
Experience suggests that asking a standards committee to do the encoding format is a disaster.
I just looked at my code, which does something we call Wire, and it's 700 loc.  Testing code is about a kloc I suppose.  Writing reference implementations is a piece of cake.
Why can't we just designate some big player to do it, and follow suit? Why argue in committee?

@_date: 2013-09-30 11:49:49
@_author: ianG 
@_subject: [Cryptography] TLS2 
Exactly.  By setting the *high-level* requirements, we can show how real software engineering is done.  In small teams.
Personally, I'd do it over UDP (and swing for an IP allocation).  So it incorporates the modes of TLS and UDP, both.  Network packets orderable but not ordered, responses have to identify their requests.
One cipher/mode == one AE.  One curve, if the users are polite they might get another in v2.1.
Both client and server must have a PP key pair.  Both, used every time to start the session, both sides authenticating each other at the key level.  Any question of certificates is kicked out to a higher application layer with key-based identities established.

@_date: 2014-04-01 23:57:22
@_author: ianG 
@_subject: [Cryptography] TLS/DTLS Use Cases 
Taking the contrarian view, as always.
Connecting to ones console, aka SSH.  It seems to be the perfect and
perhaps the major use case that actually matches the protocol.  It is
the only one that uses reliable stream, because SSH emulates ones tty,
which going back to the original Unix days, was a concept of one
character being sent per key push all the way to the server (we called
them computers in those days) and then being 'echoed' back to the tty,
and then another character, and then another echo, ad infinitum.
In-order character delivery and echo back again is perfect for a
bi-directional connection.
In contrast:  web aka HTTP is a request-response protocol, being sliced
into a request datagram and a response datagram.  Efforts abound to
layer more RR pairs or datagrams over the top of connections with
keepalives and so forth which may answer why TLS was confused about its
use cases, or may not....  Not a pretty site.
Sending messages aka Email sending is a one datagram going out and a
confirmation coming back.  Order not important, non time-critical.  Etc etc.
Sending messages aka Chat/IM is many datagrams going out and coming
back.  Order here is important, and latency is an issue of sorts, but
the resolution of this is an application decision, so a mistake to bring
it into the lower layers, which also pushes order up to application
(imagine two TLS connections, oops).
Indeed, most of these datagram protocols also aren't fussed about
in-order delivery, and that becomes an app-level decision.  In fact they
often think of it as a bug.
So it is highly debatable as to whether any of the list above are valid
use cases or not, as opposed to vestigial, accidental.  If they are, the
generic use case is probably "layer a request-response datagram protocol
over the top of a (secured) connection."
Ug.  From memory, DTLS was a sort of halfway house to apply TLS back to
datagrams. Which imported all the errors in TLS back to datagrams.  The
result was not going to please.
As per the current thread, its use cases were ... that, it seems,
literally "export TLS back to datagrams."  VoIP for example is lossy and
time critical where as DTLS was not lossy and time non-critical, so
rather a miss.
(Ok, this is from memory, I only read it the once a decade back, maybe
they've fixed that...)
DNS needs to operate below the "named" layer, so it should really be
talking strictly IP# to IP  This rather defeats the heavyweight PKI
that rides on the back of the camel.  Also it has happily worked so far
with datagrams so why go backwards?
Maybe these weren't the drones you were looking for...

@_date: 2014-04-02 12:14:52
@_author: ianG 
@_subject: [Cryptography] Fwd: [messaging] Announcing the EFF Crypto 
Fantastic news!
If the t-shirt needs a logo, I'd suggest K6:
     "Finally, it is necessary, given the circumstances that command its
application, that the system be easy to use, requiring neither mental
strain nor the knowledge of a long series of rules to observe."
I find it is very nice to remind that K6 has been around since 1883.
In terms of awarding a prize, there needs to be a consensus of different
forces coming together.  There are many considerations.  I can see
these, so far.
1.  the commercial/user base success.  By this we mean hard numbers:
how many people signed up?  How many events per day
(messages/calls/...)?  How many people is each person bringing in, the
2.  Objective external metrics.  Things like average time for new user
to install on their platform, for different contexts.  Average time to
set up a new session.  Reliability of delivery.
How many clicks to find needed functions?  How many clicks to set the
thing up into full secure mode?  How long does it take average users to
turn it on to full secure?
3.  business purpose.  It is very hard to be objective about this, and
maybe this should just be a committee vote.  E.g., comparing (say)
snapchat's disappearing pictures to (say) anon uploading of strife
videos is a judgement call, only humans can make it.
How important is the traffic -- social, economic, democratic, life,
peace, etc.
Bring together successful / prominent people who most typify what it is
you are trying to make happen, and get make them a committee.
4. the security model.  this is difficult to assess from outside, and
probably needs to come last in consideration because only with the
relevance of the user base and purpose is there any (need for) security
model.  It will also need a committee voting approach, I suspect, as
although these things can be objectified, we haven't got a good theory
on that, better to stick with the judgement call of people who build and
ship these things.
So, if those are the forces, I would simply put 25% in each of those,
and award on the sum of the points.
I'd also suggest they be quite fierce and rank the contenders from 1 to
N.  It's useless to ask them to allocate a 1-5 range because most will
end up with 4.  You have to force the judges to make a call.

@_date: 2014-04-07 00:55:56
@_author: ianG 
@_subject: [Cryptography] OpenPGP and trust 
One question:  Over both or either can we sent OpenPGP messages?  For
example, this email, with an OpenPGP plaintext signature affixed?
(What I thought you were asking for was two hams to auth each other over
radio.  Which would have meant being able to send a short word, get one
back, and do that in both directions.  This is called a
challenge-response.  It is entirely possibly, but OpenPGP does not
specify such a technique, so it would have to be hacked in.  At least I
don't think it does...)
Do you mean,
Do you mean, someone might just copy the call-sign?
By this you mean, individuals can request client certificates?
No, you should not implement digital signatures yourself.
(and I'm the guy that generally advises programmers to give it a go ;)
*any* of them ... ok.
OK, as a select group, you want to maintain a file that lists these keys.
Well, not really.  It isn't written that way in OpenPGP.  In practice,
the tech does "the key X signed key Y" and the people imply stuff over
the top of that.
According to custom, there are two popular groups of signers.  There are
those who will sign a key on meeting a person.  These people will trade
fingerprints, look them up when back home, sign those keys and upload
the sigs or something.  In this former case, the names don't matter,
what matters is you met a person who says "this is my FP."  For example,
you might end up signing a key saying "Mad Max" or "Mickey Mouse."
Then there is another group of people that check the identity docs of
the person and see that it matches the name in the public key.  These
people will check that the name is held by that person, but they don't
do a thorough check.
Without knowing what the situation is, you can't rely either way.
OK, so your key needs to name you as "Stuart Longland - VK4MSL" or
similar.  Or "Mad Max - VK4MSL" ... so far.
So, the missing link here is that you sign Bob's key with an implied
"can access computer" statement.  Or, you take a record of that key and
install it manually into the file (so far I'm seeing this latter idea).
Ah.  So, how far does the accepting go?  Do you accept Alice's sister
Malice, as Alice turns out to sign anyone's key any time she likes?
Assuming that we're only talking about two layers here, you then the Bob
layer then the Alice layer, what you have is this:
   your sig says "accept this key to grant access to others"
   access grantor's sig says "accept this key to access machine"
OR, completely different, the signature only makes this statement:
   "this key has a radio licence as per the name."
Hmmm... viral 'trust'.  Nice in concept, but I don't know that you want
to go too far without a statement of some form.
lots :)
Two problems - the group name and the statement of membership.
You can't really use any so-called trust signature for any purpose
without some form of written or agreed custom.
In PKI this is sometimes included in a CPS or Certification Practice
Statement.  This is potentially relevant if you're just able to survive
on the default CPS statement which is something like "CA checked some ID
docs of this person and decided they matched the client cert's name."
But this is rather unlikely because you're after something else:  some
mishmash of the following:
     has a radio licence,
     we know the radio licence number,
     has a call sign,
     we know the call sign,
     is a person,
     we know the name of the person,
     has been granted permission to use the machine,
     has met a certain person,
     is going to be a responsible member of the community,
     ...
(I'm only speculating here, but) the point is that you need to be
somewhat sure that you have this statement down, and you communicate it
to the grantors (Bob's?).
At this stage I'd suggest *you write down the statement you want*.
Then you need others to sign off on it -- that is, agree to it in
consensus, over beers, at your meeting.  Then, ideally you would need to
identify that statement with the name below.
Now, this gets tricky because OpenPGP doesn't let you WoT-sign a key
with a statement.  What you would need to do is add an additional UID (I
think) with distinct words in them.  For example, instead of your normal
name, you'd add something like an additional UID with words like
"AMATEUR RADIO KEY" in it.  (and fix your code to search for those words
in the sig checking.)
So all of that would end up with a UID like:
      "Stuart Longland - VK4MSL - ARK1.0"
Does that make any sense?  Now, ARK1.0 would then be by custom your
above Amateur Radio Key Statement, v 1.0, in which you would state what
your signature over the other person really signified.
Then, everyone who uses this document, and has been signed by someone
upstream, has agreed to the document and acts according to it.
In summary:
1.  Write the statement you want to have signers make.
2.  Add a UID into OpenPGP keys that signifies that statement.
3.  Teach everyone to follow the statement, when signing.
4.  Get your code to find those key/sigs.
(Some not unimportant details ommitted as we're still whiteboarding)
Also, you should probably have a look at CAcert, which does something
similar.  They run a PKI/x.509 style CA, with a web of trust, and with
(6000) assurers being those who are approved to check people's identity.
 These assurers could also be tasked to do various other tasks, such as
"check the radio licence" and then report back to others that "this
person has a radio licence", which statement can be affixed with CARS
meaning CAcert Assurer Reliable Statement, which you can then rely on as
a member of the community.  If it turns out to be false, you can
arbitrate against the Assurer.  The point being here they can do careful
checking, make the statements needed and send them on to you.  All in
theory.  They also do OpenPGP signing, which means you can get a general
sense of the person's name being correct, and also that you can find
them through the CAcert community.

@_date: 2014-04-07 23:57:16
@_author: ianG 
@_subject: [Cryptography] Announcing Mozilla::PKIX, 
Reply-To: mozilla's crypto code discussion list
We have been working on a new certificate verification library for
Gecko, and would greatly appreciate it if you will test this new library
and review the new code.
NSS currently has two code paths for doing certificate verification.
"Classic" verification has been used for verification of non-EV
certificates, and libPKIX has been used for verification of EV
As many of you are aware, the NSS team has wanted to replace the
"classic" verification with libPKIX for a long time. However, the
current libPKIX code was auto-translated from Java to C, and has proven
to be very difficult to maintain and use. Therefore, Mozilla has created
a new certificate verification library called mozilla::pkix.
Request for Testing
Replacing the certificate verification library can only be done after
gaining sufficient confidence in the new code by having as many people
and organizations test it as possible.
We ask that all of you help us test this new library as described here:
Testing Window: The mozilla::pkix certificate verification library is
available for testing now in Nightly Firefox builds. We ask that you
test as soon as possible, and that you complete your testing before
Firefox 31 exits the Aurora branch in June.
(See Request for Code Review
The more people who code review the new code, the better. So we ask all
of you C++ programmers out there to review the code and let us know if
you see any potential issues.
We look forward to your help in testing and reviewing this new
certificate verification library.
Mozilla Security Engineering Team
dev-security mailing list
dev-security at lists.mozilla.org

@_date: 2014-04-08 11:46:49
@_author: ianG 
@_subject: [Cryptography] The Heartbleed Bug is a serious vulnerability in 
We have here a rare case of a broad break in a security protocol leading
to compromise of keys.
While everyone's madly rushing around to fix their bits&bobs, I'd
encouraged you all to be alert to any evidence of *damages* either
anecdotally or more firm.  By damages, I mean (a) rework needed to
secure, and (b) actual breach into sites and theft of secrets, etc,
leading to (c) theft of property/money/value etc.
In risk analysis, we lean very heavily on firm indications of actual,
tangible damages, because risk analysis is an uncertain tool and the
security industry is a FUD-driven sector.  Where we have actual
experiences of lost money, time, destruction of property or whatever,
this puts us in a much better position to predict what is worth spending
money to protect.
E.g., if we cannot show any damages from this breach, it isn't worth
spending a penny on it to fix!  Yes, that's outrageous and will be
widely ignored ... but it is economically and scientifically sound, at
some level.
I maintain a risk history here:  for
the CA field, so if anyone can find any real damages effecting the CA
world, let me know!

@_date: 2014-04-08 20:42:10
@_author: ianG 
@_subject: [Cryptography] The Heartbleed Bug is a serious vulnerability in 
Well, be blind if you like.  But 40 million stolen credit cards are
measurable, are damages, and are directly relatable by statistical
models to theft damages.
My advice is when you have a number like 40m in front of you, then you
should DO SOMETHING.  Spend a penny, dude!

@_date: 2014-04-08 21:10:33
@_author: ianG 
@_subject: [Cryptography] [cryptography] The Heartbleed Bug is a serious 
Right, exactly.  Thought experiment.
Precisely, that is the question.  What happens if we wait a year and
nothing .. happens?
What happened with the Debian random plonk?  Nothing, that I ever saw in
terms of measurable damages.  The BEAST thing?  Twitter, was it?
What happened with PKI?  We (I) watched and watched and watched ... and
it wasn't until about 2011 that something finally popped up that was a
measurable incident of damages, 512bit RSA keys being crunched from memory.
That's 16 years!  Does that mean (a) PKI was so good that it clobbered
all attacks, or (b) PKI was so unnecessary because there was nobody
interested in attacks?
Dan Geer once said on this list [0]:
    "The design goal for any security system is that the number of
failures is small but non-zero, i.e., N>0. If the number of failures is
zero, there is no way to disambiguate good luck from spending too much.
Calibration requires differing outcomes."
We now have what amounts to a *fantastic* opportunity to clarify delta.  We've got a system wide breach, huge statistics, and
it's identifiable in terms of which servers are vulnerable.
Hypothesize:  Let the number of attacked servers be 1% of population of
vulnerable servers.  Let our detection rate be 1%.  Multiply.  That
means 1 in 10,000 attacked servers.  Let's say we have 1m vulnerable
We should detect 100 attacks over the next period.
We should detect something!
(Well, right.  I doubt we can actually tell anyone to wait.)
[0]

@_date: 2014-04-08 21:17:59
@_author: ianG 
@_subject: [Cryptography] The Heartbleed Bug is a serious vulnerability in 
Not me, you might be thinking of the other iang?
the shoe turns, the knife fits...
Nothing, nix.  I wish.  Please!?
At this stage it is customary to post a bitcoin address but I don't even
have one of them....

@_date: 2014-04-11 09:57:54
@_author: ianG 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
That problem - exactly, does anyone know a solution in Java to cleansing
(I write the password, etc code in byte[] but sometimes one has to have
a String, such as asking the user for some input, .. like a password.)

@_date: 2014-04-11 17:02:26
@_author: ianG 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
Hi Twan,
No, the problem is precisely *not choice* in that in order to get a
SWING element, one has to use the SWING methods available.  In this
context, the JTextField.getText() returns a String.
OK, I suppose I could further investigate SWING and see if there is a
class that gives better byte[] access, etc.

@_date: 2014-04-11 17:09:42
@_author: ianG 
@_subject: [Cryptography] Preliminary review of the other 
This question:
I really doubt the latter choice, I'd be pretty sure we get the former
choice, and even that is suspect.
Althought EV was started because of this question, the result was not to
answer it.  I do not recall anything from EV documents that spoke to
actually taking on liabilities.  CAs will not at any time disavow your
misinterpretation, nor will they at any time take on any liability that
isn't easily dumped to another party.  It's just not the business they
are in.

@_date: 2014-04-12 11:05:24
@_author: ianG 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
What he's describing is a general defence against the presence of the
broad class of attack, not a specific blocker.  If we assume that the
attacker can at random times grab lumps of the memory, but isn't
otherwise in control, how do we make the app more robust?
The answer in general is crypto secret cleanliness.  In principle this
means minimising the number of copies and minimising the time spans
during which raw secrets are revealed.
Each piece of code that touches a crypto secret should keep the data for
the minimum amount of time, and scrub it on termination.
However this is fairly tough to do, you have to rewire a lot of thinking
in coding environments that copy stuff at random, and do lazy garbage
Hence, Jerry's solution posits objects that clearly identify their use,
and hold the actual secret tightly inside them, thus at least making it
easier to force the coder to respond when the objects come into play.
It's an interesting solution because if symbolises the problem and
forces the coder to respond to the symbols.

@_date: 2014-04-12 11:49:48
@_author: ianG 
@_subject: [Cryptography] cryptography Digest, Vol 12, Issue 9 
Excellent article.  Somebody at EFF is actually thinking about what we
can learn.  Tracking exploits leads us to hints of damage.
I don't recall writing any such facetious suggestion, but hey, I'd love
to be precise about an apology!
OK, so you raise a good point -- in the byzantine security world,
knowledge of an exploit changes the equation, where as in the
statistical reliability/safety world, knowledge of risks should not
presumably change the likelihood of risks.
(leaving aside insurance, FUD and homeland security for now).
However I'd question your claim of near-certainty.  Seriously, and
non-facetiously.  Near-certainty predicts that there were damages/losses
for the other recent events:
   BEAST,
   the debian random bungle,
   renegotiation,
   512 bit keys,
   apple goto fail,
   gnutls goto confusion,
   (any others?)
Right?  Or have I misunderstood?
I'd really like to see it.  Precisely one of the above has some form of
reported damages of a real attack (*).  Otherwise on
Just because this is shoot the messenger week, I will specify why as
basically as I can:  I collect this evidence -- claims of damages -- so
we can build up some history of damages to ground future thinking in
some plausible imitation of scientific method.
It's either we get serious about the science of security, or we go back
to wondering why they call us Henny Penny all the time.
Fire away.
(*) just to revise, damages are about people having money stolen or
similar attack of real aggression.  They are not "demonstrations" as per
the Twitter renegotiation hack.

@_date: 2014-04-12 11:52:49
@_author: ianG 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
Thanks, I was just too lazy or too busy, will read.
Chars aren't any good because the crypto algorithms need bytes.  Let's
not suggest the alternate to anyone ;)
Hmmm, yes, thanks.  And I even found the switch to make the passwords
not be starred so people can remember them and type them in more
clearly.  Old practices from the terminal lab die hard.

@_date: 2014-04-13 06:03:18
@_author: ianG 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
:) ok...  I was just wondering if it (they) deserve mention under
Sketches, and if not why not?
And while we are on the subject, I'm curious about something:
    "Unfortunately, the modern Internet does not look favorably upon
UDP. Preliminary benchmarks of CurveCP showed nearly complete packet
loss in the face of TCP congestion which is arguably a problem with
CurveCP's decongestion algorithm but much more likely an intractable
problem with trying to do bulk transfers with UDP."
Is this over-obsessing on an edge case, aka congestion?  Most use for
most protocols is bursty therefore presumably uncongested.  As long as
this works most of the time, we've got a protocol, and the occasional
time it gets into congestion-deadly-embrace, we can handwave it off as
dead net;  happens anyway, apps deal with it, or don't.
I noticed the same long discussion in QUIC doco, and the recent post
where someone had shown CurveCP to be bad in certain tests that focused
on the congestion space;  is this a case of solving the edge cases
before proving the fundamentals?
Or is there something fundamental about the congestion issue that stops
UDP being usable under any circumstances?

@_date: 2014-04-13 12:08:42
@_author: ianG 
@_subject: [Cryptography] Preliminary review of the other 
Right, belief is the ideal result of the marketing.  My general tactic
there is to refer believers to the contracts that support that belief
and point out that IMO the contracts expressly won't and don't.
I agree.  In more precise terms, I would say it comes down to the
statement that is claimed by the cert.  Making various assumptions here,
there should be a statement of reliance that can be tied to the cert,
and that statement and reliance is not something that can be tied into
something as simple as an evil bit.  (Or the so-called "trust bits" for
that matter.)
Hmmm.  A highly nuanced answer that sidesteps the foundations of those
beliefs.  But yes, I can imagine this is the polite and politic answer.
Google and Mozilla developers have apparently expressed a desire to only
support HTTP2 if it is TLS only.  This is good news.  This indicates
some willingness to at least move the game forward, and to go against
Long but interesting reply!  I had to read it twice to grasp it.
The answer as I suspect you know is found not so much in the allocation
of the resource (one programmer) but in the employer-mindset.  Who is
paying to do the work will be key, at both an individual level (am I for
it?) and an aggregate level (are we against it?).
For the most part, all the work surrounding this area in the browsers
has been conducted by engineers that have been focussed on preserving
the PKIX solution.  So, that's what they have done;  done their job.
It's hard to fault them for it.  As their interests are in preserving
the franchise they have tended to act against alternates in subtle,
deniable ways.
It was for this basic motive that no alternates found favour until CT
came along.  CT afaics was no better or worse than any other idea, it
just had the one thing that makes a difference, google.
(Whether that latter can be attributed to brand, resource, interest or
other is probably beyond scope... but it's a really interesting question.)
What is the DNSSEC last-mile problem?  It's the week for displaying
ignorance, seemingly.

@_date: 2014-04-14 21:31:20
@_author: ianG 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
Thanks, to all.  Those 3 explanations look much closer to what I expected.

@_date: 2014-04-15 17:56:40
@_author: ianG 
@_subject: [Cryptography] NSA disclosure of vulnerabilities 
I'm curious.  NSA claims that it discloses vulnerabilities.  But I do
not recall any such event.  Does anyone?
Or have we reached Peak Deception?  We disclose, but we can't disclose
that we've disclosed?
monkey's uncle

@_date: 2014-04-15 20:41:47
@_author: ianG 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
For Java, there is a book:
Which is a must-read if you've got $32 idle.  It's  recommendation is:
    1. Limit the lifetime of sensitive data
rather apropos recent thread, I wonder what they say about Strings.
Especially in the light of     75. Do not attempt to help the garbage collector by
        setting local reference variables to null
??  Chapter 2 is downloadable, or some portion thereof.
And for C++ there is:

@_date: 2014-04-16 11:30:52
@_author: ianG 
@_subject: [Cryptography] I  don't get it. 
Well, the goto bug, maybe.  The gnuTls was a coding failure, not
something that is easy to pick up.
The Heartbleed one, not if it is hard coded without a serialization
framework that understands the object being written.  Which requires
either an IDL as has been pointed out, or a type safe language, or OO,
or a home-grown methodology for limiting these errors (which is what I
use, must publish it one day...).
They are diverse causes.
It was written and reviewed by two people.  I know one of them, he's
unlikely to have inserted that bug in on purpose.  From memory, he
worked on TLS/SNI which was one of the few real security improvements
we've seen, because it's a security multiplier, not a diminishing
returner.  Little chance of the agencies helping in that.
Yes, possibly.  But that still leaves 1%.  Now look at how many kloc
we're dealing with.  I'd guess OpenSSL is O(100kloc) so that still
leaves many bugs.
right.  Use an OO, type-safe, memory-safe language, preferably with some
sort of psuedo-code.  If you're serious, use only code you know is good;
 eschew packages.
Well, exactly.  There but for the grace of Papa Legba go I.  We do what
we can, and we architect systems to fail well.
I'm not sure about this sudden explosion of angst that people are
feeling.  The Heartbleed event has been waiting for a long time.  The
code is very complex, the dev team is under-funded and overwhelmed, the
design is atrocious, the user-base is unhelpful, the apps are a lazy
mess, the security model is a vestigial facade and the critics have no
Why hasn't it happened more times, is my question...  This is why I
track real events, because I can't believe they've been so lucky with
such a bad situation.  There has to be other things going on, and to
find them we need real science (is my guess).

@_date: 2014-04-16 12:16:21
@_author: ianG 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
I see you've added commentary on the CurveCP variants, thanks!  Still
shy on the google QUIC offering, which makes me think it is even more
worthwhile to hear ;)
I think it is going to happen.  Without some direction, we know that the
TLS wg will simply do what they always do -- incremental fixes and more
ciphersuites.  Reactive to events, not to the big picture.
People are beginning to realise that a big change is needed.  E.g.,
this:  "One way or another, we need encryption for TCP streams that we
can trust."
No, we don't.  What we need is (a) a reliable request-response protocol
that allows packets to go back and forth between end-points reliably,
and (b) a subscription packet distribution service (chat, flashy web
We don't need streams.  We never needed streams for the original use
case, being protecting credit cards in the web.  The one place we really
absolutely need streams is secure terminals, and SSH doesn't use TLS.
Go figure.
It's all a myth.  What happened was that HTTP used TCP because it was
too lazy to do its own connection & packet reliability layer.  It got
streams not packets coz it didn't know the difference.  We've been
suffering this misconception ever since.
This is why I suggest a reset.
The system is way old, and the use case and design has been roundly
trashed.  The players are locked in deadly embrace.  We need new
original thinking, and we need reset thinking (like CurveCP's suggestion
to do away with KEX).
It has to be done at an individual level because the instant we get into
committee, there will be people blocking any movement forward because
others aren't supporting their vanity suite.
We already have market evolution at the individual level:  things like
CurveCP and QIUC gaining supporters and battle it out in the market
place.  I think they are on the right track (disclosure: I do something
similar myself with UDP-based request-response protocol called SOX) but
I wonder whether one individual team can find enough support, enough
How can we accelerate the process of individual teams,
cross-fertilisation of ideas and of the needs cases, and also the
building of consensus?
A competition.
It's not that far away, we already know how to do competitions, e.g.,

@_date: 2014-04-16 21:38:55
@_author: ianG 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
Another permathread, what's the best layout...
Well, the task is to replace that broadly distributed implementation
with a hypothetical or experimental system.  Comparisons are possibly
unavoidable :)

@_date: 2014-04-16 22:39:29
@_author: ianG 
@_subject: [Cryptography] I  don't get it. 
Oh, wait till you get me started on Bitcoin :)
Good description.  In SSL, we kind of got by in spite of bad practices.
 No such luck in Bitcoin, oh boy.  There, the results are as predicted
by the practices.  Why?  Why is it so?

@_date: 2014-04-17 12:06:12
@_author: ianG 
@_subject: [Cryptography] I  don't get it. 
Just spotted, some more on the statistical nature of things:
Software programmers produce more than 100 billion new lines of code for
commercially available software put into operation each year, according
to a recent article published in Defense Systems. Meanwhile, programming
errors happen at an estimated rate of 15 to 50 errors per 1,000 lines of
code. Even with the advent of automated testing tools, the article
states that ?numerous studies and a substantial amount of research
suggest that approximately one error per every 10,000 lines of
production code still exists after testing. That would equate to
10,000,000 errors in the code produced each year.?
Now, how many lines in OpenSSL?  Plug it in and get an estimate of bugs
The existence of bugs is statistical;  the exploitation is not.
ps; statistical here means "follows the law of big numbers" but it
doesn't read so well.

@_date: 2014-04-18 01:12:06
@_author: ianG 
@_subject: [Cryptography] Cue the blamestorming 
Just on that point, there are also actions to balance words, it seems:
    Changes so far to OpenSSL 1.0.1g since the 11th include:
        Splitting up libcrypto and libssl build directories
        Fixing a use-after-free bug
        Removal of ancient MacOS, Netware, OS/2, VMS and Windows build junk
        Removal of ?bugs? directory, benchmarks, INSTALL files, and
shared library goo for lame platforms
        Removal of most (all?) backend engines, some of which didn?t
even have appropriate licensing
        Ripping out some windows-specific cruft
        Removal of various wrappers for things like sockets, snprintf,
opendir, etc. to actually expose real return values
        KNF of most C files
        Removal of weak entropy additions
        Removal of all heartbeat functionality which resulted in Heartbleed
Yes, that particular misinformation campaign has been revealed.  I'm not
sure who it was aimed at tho...
Yes. This is perhaps happening as we speak in the world of Bitcoin.  The
design, first aired in this forum about 4 years back, eliminated the
single point of failure known as the issuance server (or mint or bank,
etc).  However it ended up with a single point of failure known as the
dev team.
Right now the dev team faces a dual pincer movement.  The volunteers are
too scared to make the radical changes that are needed to keep up with
developments, and the businesses out there are busily strip-mining the
team for developers.  This ensuring no independence and a facade of open
source, as we've seen with other notable corporate-controlled programs.
Why is this happening?  Well, one can poke a lot of factors.  Point
remains that Phillip's comment about systemic weaknesses in the security
projects is now emerging as a big issue.
That's not a sufficient reason.  You'd also have to show that the
government can do a better job, rather than make a bigger mess.  I err
on the latter, so I'm interested to hear claims to the former.
He:) that rabbit hole, too late for me.

@_date: 2014-04-18 21:05:50
@_author: ianG 
@_subject: [Cryptography] I  don't get it. 
Because it doesn't effect them, directly.  It's generally some other 'them'.
With a clear exposed bug, there can be a tight feedback loop -- the
coder or team is embarrassed.  We can see Heartbleed it's a massively
embarrassing bug in the implementation of OpenSSL, it's really easy to
identify who is to blame, and everyone's piled into to fix it.  Even,
there are people now actually reviewing and rewriting OpenSSL.
Slash & burn, go guys!
But, if the bug is not a bug, or is arguably a bug, or if it's a
documented feature, or just a weakness, then it's SEPs -- someone else's
problem.  strcpy and C pointers fall into that.  They aren't bugs, they
are design decisions.  If there's even a shred of doubt as to whose
responsibility this is, the answer is clear:  it's not our responsibility.
(The entire PKI + phishing falls into this, which is why it was never
"fixed" even though there were no shortage of fixes suggested.)
This is called agency theory in economics, which people recognise more
comfortably as interests, aligned or unaligned.
When interests aren't aligned, we get dysfunctional behaviour at the
group level.  Examples abound:  the interests of the CAs are not aligned
with the interests of the users, and the browsers are stuck in the
middle.  The same problem occurs in IETF working groups where the
interests of the corporations are clear, but the interests of the
individuals are again lost in the noise;  although they are permitted to
do so, the individual internet users cannot or do not band together to
(e.g.) pay a salary to represent their interests in the groups against
the professionals who champion the corporate interests.  It's a stacked
game, rigged.  Occasionally we hear professionals on salaries saying
it's open, open for all, you're bad to suggest stuff and not stick
around to make it happen, but they only get to say that because their
comfortably paid, for the most part.  There are very few who can put the
effort into do an ID, credibility is handed out primarily to companies
before people, and power belongs to those with long-term staying power
and the recurring price of an IETF conference entrance and flight.
(This is why DNSSEC key signing is an interesting example -- how did
they do that and avoid corporate takeover?  Idk the answer...)
The same dilemma is seen in the cradle-to-grave behemoths, Apple,
Microsoft, Facebook and Google.  In those, there is a much tighter
feedback between the user and the vendor, so for example we see google
pushing to improve the SSL, almost alone amongst the companies, this is
primarily because they are on the hook for so many of the components:
software supplier, websites, email provider, apps provider, standards
player, browser vendor, CA, etc.  For the first time, almost all the
interests are aligned!  On the other hand, we have the massive
datamining problem where the behemoths have so much information on our
lives that there is no way we can trust them.
I talked earlier about Bitcoin, where the interests of the dev team are
slowly being eaten up by the startup horde, Ted talks about a deal he
struck with google to spend 50% of work on his project.
So the question with bugs.  How do you make such *external* bugs in the
'interest' of the group that controls and does most of the work?  Pick
OpenSSL if you like, the same problem exists.  I was involved in pushing
the TLS/SNI stuff for a long while.  It was really hard, nobody really
cared, pretty much everyone said it was SEPs.
Yet, arguably, this one patch would have led to more security than
fixing all of the pre-Heartbleed bugs put together, because it is a
So we need a way to align the interests of say the victimised phished
users with the developers in the groups.

@_date: 2014-04-18 22:18:42
@_author: ianG 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
I actually thought I knew how to break it up and impose some stability
to the chaos too.  Until people started breaking my model.  Here on this
group a few months ago, under the subject line TLS2, if memory serves.
Then I realised that actually there is too much complexity here, and any
attempt to impose structure is going to limit the finding of a solution.
So my current favourite is:  no rules, no holds barred.  Replace TLS,
take your best shot.  Votes from all, drinks at 7.  Start your state
engines, gentlebodies.

@_date: 2014-04-19 12:39:13
@_author: ianG 
@_subject: [Cryptography] It's all K&R's fault 
How many people here were trained early 1990s and before?  In those
days, quality programming was the standard, no question.
Then the dotcom boom happened and time-to-ship became the standard.  No
question.  (although skeptics will say that M$ showed the way...)
Everything changed.  Everything in programming now is about how to get
half-way decent, half-way survivable code out the door, with people who
aren't up to the standards of the old days.
It's an economics and numbers thing, supply and demand.  When the
industry expands faster than the universities can supply grads, then
other sources are tapped.  The overall body of programmers changes.
That's why we don't recommend C or C++ or Perl or PHP.  Too easy for the
real programmer, too easy when the masses get stuck in.
(As Peter said.  In less words.)

@_date: 2014-04-19 13:01:23
@_author: ianG 
@_subject: [Cryptography] Apple and OpenSSL 
Coz it's a different group of people.  Apple did the right thing, but
they allowed an escape valve so that some small dedicated devs with
openssl code bases or mindsets could do the wrong thing.
Once it is them that muck up, it isn't Apple.  No problem, no mud
sticks, and more importantly, their devs can get back to work and their
users are happy.
It's all about groups and interests.  Alignment or misalignment.

@_date: 2014-04-19 13:20:42
@_author: ianG 
@_subject: [Cryptography] Cue the blamestorming 
Exactly.  According to the evidence of their actions, NSA's mission is
to spy on everyone, and the rest is deception planning;  either they're
deceiving their enemy, or us, or themselves, doesn't really matter.
Which leads straight into the "we will own the net" mindset, aggressive
or otherwise.
And, continuing this thread,...
Governments start cyberwars, then rush to the people and say they want
money for cyber defence.  Because, shock, horror, other governments are
attacking them.
Now, of course, we probably do need government money for cyberdefence.
But they caused it in the first place.
So letting them in the game, encouraging them, and asking for their help
is strictly the wrong reward for the wrong behaviour.
Government-backed cyberdefence?  Just say no.

@_date: 2014-04-20 12:30:46
@_author: ianG 
@_subject: [Cryptography] Are dynamic libs compatible with security? was: 
Yes, it's a point.  But, Apple are in the real world, serving real
people with real product.  OpenSSL are not, they're in the
geek/crypto/IETF/PKIX/x509/1990s world serving ... geeks,
cryptoplumbers, other WGs, PKI committees, vendors, and some 1980s
masters thesis which nobody can remember the name of.
Indeed.  A good point.  But the measurement of static linking point is
not against dynamic linking, it is within the context of the overall
delivery of security to a userbase.  With Apple's context, it's pretty
easy to see that dynamic linking was a better security delivery, because
it enabled them to control more of the overall process, which got more
security delivered for less maintenance cost across to more users.
Yeah, I do that myself.  I replace everything.  I've seen every line of
code in my stuff.
And somewhere I write that you should write your own crypto and protocol
and do it all, then shoulder that burden.  But the reason I say that
isn't because (only) it is more secure.  It is because the overall
effect is better.  That's also including the ability to control and
maintain code.
I personally don't have the resource to manage the 1000 library suck
that the linux people love.  I've had times where I've spent 2 entire
weeks discovering I cannot get old stuff up and going because it depends
on some arcane library, and I've spent another 2 weeks to replace it.
And never looked back, free!
But Apple are different.  They know *their context* and they have the
resources to efficiently deliver a library context.  If they say it is
easier for them to deliver, and securely, with dynamic ABIs, then I
believe them.  Coz they know their customers.
If OpenSSL says they should be using static libraries, then this just
suggests to me, like most open source library projects, that they are
too far from the real world of customers to give as much weight to their
This is all true.  But, again we're missing the context, and this is
that Apple take the whole cradle-to-grave responsibility for their users
in a way that others (library projects) won't understand.  So if they
say they are going to take on that responsibility, it again overrides
theoretical concerns in a static, no-user context.
(OTOH, they seem to have chosen to replace OpenSSL entirely.  That I
approve of.  Hopefully they're also replacing SSL.)
I like Apple.  They take hard decisions.  What I don't like about them
is their secretiveness, like google and Microsoft, it erodes trust and
when they are apparently self-serving and make mistakes, there is
nothing but downwards for trust.  But overall, they seem to make the
best decisions, have the best focus, the best mix and the best process.
Which is to say, what you point at above is coming to the same
conclusion;  there are hard choices and sometimes you have to go against
the herd.  It's hard for us counterculturalists and pioneers, but in the
end, public opinion comes second to results.

@_date: 2014-04-20 16:42:01
@_author: ianG 
@_subject: [Cryptography] Code as if everyone is the thief. 
(This is what financial cryptography is about, h/t to Jeroen):
A novice asked of master Bawan: ?Say something about the Heartbleed Bug.?
Said Bawan: ?Chiuyin, the Governor?s treasurer, is blind as an
earthworm. A thief may give him a coin of tin, claim that it is silver
and receive change. When the treasury is empty, which man is the
villain? Speak right and I will spare you all blows for one week. Speak
wrong and my staff will fly!?
The novice thought: if I say the thief, Bawan will surely strike me, for
it is the treasurer who doles out the coins. But if I say the treasurer
he will also strike me, for it is the thief who takes advantage of the
When the pause grew too long, Bawan raised his staff high. Suddenly
enlightened, the novice cried out: ?The Governor! For who else made this
blind man his treasurer??
Bawan lowered his staff. ?And who is the Governor??
Said the novice: ?All who might have cried out ?this man is blind!? but
failed to notice, or even to examine him.?
Bawan nodded. ?This is the first lesson. Too easily we praise Open
Source, saying smugly to each other, ?under ten thousand eyeballs, every
bug is laid bare?. Yet when the ten thousand avert their gaze, they are
no more useful than the blind man. And now that I have spared you all
blows for one week, stand at ease and tell me: what is the second lesson??
Said the novice: ?Surely, I have no idea.?
Bawan promptly struck the novice?s skull with his staff. The boy fell to
the floor, unconscious.
As he stepped over the prone body, Bawan remarked: ?Code as if everyone
is the thief.?

@_date: 2014-04-21 12:25:21
@_author: ianG 
@_subject: [Cryptography] Apple and OpenSSL 
This makes for a pretty good reason to use objects not structs.
However, if you need to persist your objects (and who doesn't) you're
back to the same problem.  In general, I find that each object always
needs a version number (and also for super classes) and then the object
can unpersist itself, being kinder to older generations.
However, if you're upgrading asymmetrically, such as e.g., clients and
servers, where you control the servers and can upgrade them now but
clents cannot handle new versions until later upgrade, then the trick
with spare fields starts to pay off.
What rules/guidelines do you use to achieve that?

@_date: 2014-04-22 01:47:53
@_author: ianG 
@_subject: [Cryptography] GCC bug 30475 (was Re:  bounded pointers in C) 
This is the nature of OSS and also many commercial projects: limit all
responsibility to zero.  We write the EULAs and similar to remove all
responsibility in all possible ways that we can think of.  (It is taken
to a high art in the PKI industry which layers and structures itself to
ensure complete firewalling of legal responsibilities, indeed the
structure of CAs can only be understood with this in mind.)
In some theoretical generality, there appear to be two solutions with
     nobody's responsible for anything,
     everyone's responsible generally.
In CAcert, we choose the latter.  Everyone's responsible, even the
developers.  They might not be responsible to you for your particular
itch, but they are in general responsible, they have to accept outside
intervention to decide what the responsibilities are in any particular
dispute. (c.f., arbitration).  With some limitations, it's not perfect,
but you can put a lot of weight on it and it will still carry on.
Yes.  I think some of the Linux communities might have adopted
arbitration as a way to deal with disputes but I've not heard of them
taking it to the aggressive level of CAcert.
right.  Many smaller OSS projects are powered by developers who are just
happy doing their thing, in a black box.  This is a good thing if it
gets code written that otherwise would not.  It is often necessary to
spark the original project.
But pretty soon the lack of responsibility leads to deadlock.  You can
see this in projects that reach a small number of core developers and
don't grow beyond there;  the reason is typically that the power is
concentrated and the incumbents don't know how to move beyond that
point, don't even know they are deadlocked at that point.
Once responsibility enters, it needs a different sort of mindset to have
fun in that arrangement.  Resources shift over to a new style, which
might take years.
(Somewhere there is a description of the 6 stages of evolution of a
growing OSS project...)
[ How can this be?  Security without customers?  What is that? :]

@_date: 2014-04-22 01:50:34
@_author: ianG 
@_subject: [Cryptography] It's all K&R's fault 
Couldn't think of a snappy reply to this in less than a few days, so I
guess I'm now old and cranky...
Ya!  iang

@_date: 2014-04-22 01:58:50
@_author: ianG 
@_subject: [Cryptography] Open Source developer employment agreements, 
This is the guild solution.  Although it is appealing, there are always
those who don't like the result.
If it is successful, it tends to move fairly quickly to shutting people
out who are not liked for some reason or other.  The reason is always
couched in professional language such as "not the right qualifications"
but the motives tend to become politicised over time.
In the alternate, if it doesn't work, or it's aimed at too low a level,
commercial efforts tend to race towards the bottom by pushing paid
courses/events/tests down the throats of the members.
You could find some group that is already part way there and just add
the guild-like things you want, over time.  Small steps in time.  Guilds
or orders don't spring into life over-night, they evolve over time.

@_date: 2014-04-23 16:23:08
@_author: ianG 
@_subject: [Cryptography] bounded pointers in C 
Back in the day, I used to ask interviewees what the size of an int was.
 If they spouted a number, I knew they hadn't done much.  If they said
it was dependent on the machine size, I knew they had done enough C to
consider them.
The correct answer at the time was "no longer than a long and no shorter
than a short" although I forget the words from K&R.  Nobody ever got that.
Problem being, if you fail someone for not knowing the spec, you haven't
enough people to employ.
This is why we prefer interpreted, memory-safe OO languages over jedi
coders using heavy metal languages from the 1970s.  It's not idealistic,
it's pragmatic.

@_date: 2014-04-25 22:36:24
@_author: ianG 
@_subject: [Cryptography] [cryptography] OT: Speeding up and strengthening 
Progress for OpenSSL!  Here's hoping they also see the light and drop
every other ciphersuite as fast as they can.
Close!  2 is soooo much closer to 1, it's even O(1).
ps;  obligatary toot:
pps;  Google, take your lead from Guus:
The man!

@_date: 2014-04-27 15:30:05
@_author: ianG 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
Right, you have to check.
Yes, this is where OO wins.
On the first point, it comes down I think to control.  Do you want
control over everything you do, or do you trust some tool to do the job
for you?  I guess that's a personal opinion, and a resources question.
On the second point, there is a way to lock down the errors.  It
involves a bit more work, but generates much more confidence.
This is what I call the Ouroboros method.  You define a couple of extra
equals() which checks two objects for wire equality (similar to OO
equality but more precisely defined).
example() which generates a random object with all fields filled in some
random amount.  E.g.:
    example() {
        int a = rand();
        byte[] b = Example.randombytes();
        X x = new X(a, b);
        return x;
    }
Then you write a method to create a million examples, encode them,
decode them, and compare the output object to the input object.
This catches "positive" failures over time.  To catch negative failures,
you'd need to extend the technique to do fuzzing which is a bit more
I think if one is working in a big team then one might want that.
Successor is a good word.
If one is paranoid then one prefers to do it oneself...  Paranoia lives
forever, succession isn't an issue.

@_date: 2014-04-28 00:46:13
@_author: ianG 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
I just spent the last year in Kenya.  There, they have little clue what
western style privacy is.  But they do have greater needs in security
than the west, very much greater and more pervasive.  In order to secure
themselves, they do what the rich world eschews, they do things like
share their financial information amongst each other ... for security.
It takes a while to unravel the western internalisations;  It's no
surprise that these committees get it wrong, given their starting
positions and their assumptions.
ps; apropos crypto, we give them rsa/sha1hmac/aes over the wire and
chach20/poly1305 for the static data, self-authenticated using their
social groups with CAcert-style assurance and photos.  Look ma, no CA!
Probably over the top for transactions <$100 but it is far easier to do
the job properly once, then move on to something else, than as you say,
start adding fixes in when the real needs start slapping you in the face.

@_date: 2014-04-28 01:04:49
@_author: ianG 
@_subject: [Cryptography] Improving the state of end-to-end crypto 
To paraphrase, work with ... Advisory Board, developer communities,
academics, funders, civil society, private partners, existing contacts
-?? yours and others? -?? developers, designers, academics,
complimentary efforts, security experts, academics, and partners,
auditors, conferences, venues,...
Everyone *but the users* !!  Shake it up, Ben.  You can't improve the
lot of the users unless you actually meet some of them.

@_date: 2014-08-01 12:33:11
@_author: ianG 
@_subject: [Cryptography] [cryptography] Browser JS (client side) crypto 
As James points out, this is essentially an argument about the economics
of passive attacks versus active attacks.
The essence of opportunistic security is to force the attacker to attack
*actively* which then carries a cost.  Passive attacking carries no
cost, or a cost so marginal when spread over pervasive monitoring that
it is approximately zero.
By forcing a PM attacker to target the attacks, this achieves a
filtering effect because the attacker must now justify use of resources,
get permission, analyse the target, risk revealing, etc.  This is
socially beneficial, those that haven't already "come to the attention
of the attackers/authorities" are now not going to be attacked just
because they can be.
What you have to do is make an argument that says that active attacks
are more costly to society than pervasive monitoring.  That's a tough call.
Yes, but that is a flawed argument.  Monkey see, monkey do.  It ignores
the costs of the attack to the attacker, the victim and the defender.
Well, no.  Implementing HTTPS:// is hard.  It is simply out of the cost
range of about 99% of the websites [0].  Otherwise they would.
The fact that *you might be able to reach that high bar* is irrelevant.
 What is relevant is the 2 decades of history that we have that says
clearly, HTTPS is simply too expensive.
Yes, and they can make that argument.  HTTPS and PKI carries with it
some downsides such as vulnerability to CA-based attacks, tracking,
Sure, we can say that HTTPS + JS crypto is clearly stronger than either
alone.  But that is the wrong comparison.  The question is, what is
better than nothing?
JS crypto is BTNS -- better than nothing security.
[0] old figures.  It used to be that around 1% of the websites used
HTTPS, no idea what it is now.

@_date: 2014-08-02 08:46:20
@_author: ianG 
@_subject: [Cryptography] [cryptography] Browser JS (client side) crypto 
What part of "old figures" .. "no idea what it is now" do you not
understand?  Those numbers came from examination of SecuritySpace
figures and that other company.  They're out of date.
The thing that upsets mass rollout of HTTPS is the configuration,
certificate, IP# and associated sysadm costs.  Only big-end merchants
grumble about the CPU costs, but they have always been able to afford it.
Again, you're resting on the false argument of "see active attack, must
defend against active attack."
Citation needed ;)  Actually, you need that, otherwise it seems
nonsense.  I don't know anyone using FireSheep, how can it have made a
difference?  "Massive" ??
A start... 11 years after they should have acted, they are
"contemplating" ...  Impressive.
You introduced a different tack.
You're cherry picking :)  And, of course HTTPS will prevent the things
it is designed to prevent.  But ...
So is 40 bit SSL which would have stopped PM in 1995.  But the world
said it wasn't good enough so we fought the NSA for 128 bit, nothing but
128 bit dammit!!!  Look what we got...
If cypherpunks had had the smarts to shut the fluff up and let 40 bit
pervade the planet, then we'd have upgraded to 64 bit then 80 bit then
128 bit by now and the job would be done.
All-or-nothing means you probably get nothing.

@_date: 2014-08-03 14:13:58
@_author: ianG 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
Having options is good, as a systems designer.  But if you pass those
options on to users, that is bad.  Users have no idea what to do, and
they spin wheels while listening to so-called experts pontificate.
For this motive, now I suggest well supported by history, I propose that
the designer should pick one suite for the whole lot, and take on the
responsibility for everyone that the choice is good.
(PHB's question is slightly more complicated, as I understand it.  His
project has both ephemeral keys and long term secrets.  That complicates
a little, because that suggests can be two different suites depending on
the purpose.  But that choice can still be stripped as an option, the
software choosing what it needs according to purpose.)

@_date: 2014-08-03 14:33:16
@_author: ianG 
@_subject: [Cryptography] Browser JS (client side) crypto FUD 
This is a fundamental feature of the infosec industry.  Customers want
to be told they are doing the right thing.  The thing itself is totally
opaque to both customers and suppliers in general, so the end result is
that the one who markets best wins best.
It also works in the sense that better locks work.  They aren't
unbreakable, but they do move the burglar on to the next house.  Which
is all we need.  This is a self-reinforcinc cycle, slowly everyone
upgrades over time together, as they can afford it.
What it isn't is the old "security must be perfect" nonsense that was
peddled in the early days.  We all know a perfect lock on a flyscreen
door is a stupidity, but apparently we have difficulty seeing what is
wrong with ECC512 bit encryption on a website taking credit cards with
some home written PHP.
Just an inevitable conclusion of the market for silver bullets.  Sad,
yes.  If one is uncomfortable with the structural implications of the
industry, there are plenty of others ;-)

@_date: 2014-08-04 11:31:39
@_author: ianG 
@_subject: [Cryptography] You can't trust any of your hardware 
In CAcert we used the USB memory sticks for sneaker-packets in
key-signing ceremonys, and for later escrow.  We use 2 for each.  They
are to be purchased at a random retail street store on the day.  Those
not escrowed are destroyed afterwards.
We might need to rethink the approach, perhaps with open source designs?

@_date: 2014-08-08 17:28:12
@_author: ianG 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
to play devil's advocate...
Yeah, I for one think that is a bad reason.  Let's see some data?  Some
We've been doing secure protocols long enough, if this was a reasonable
design practice we'd have seen enough benefit by now to justify it.
Why do we pander to these organisations?  People quote Russian and
Chinese ciphers, but I don't see why we should inflict the choice on the
rest of the net just because some organisation thinks they'd like to
push an agenda.
It seems to be a logical absurdity.  NIST has a standards suite that
people think highly of.  So we have to accept NIST.
So, if we accept NIST, we now must let the Russians GOSTs in.  And the
Chinese.  ... We're back then at the same place of vanity ciphers, 'cept
on a national level.  Absurd.
That argument seems to have died now that crypto consumes <1% of one core...
The hardware people do push that they have implemented certain
algorithms and modes in hardware, so therefore we have to support them.
 Personally, I think there are reasons to not support them, as much as
there are reasons to support them.
It is easy for a standards group / committee to add things in.
But they've got no answer as to how to take things out.  It's the sysadm
community that has to take it out, and they aren't doing that sort of
work en masse, their world is more about upgrading in isolation than
re-configuring as a herd.  And, sometimes not even upgrading.
If say IETF committees followed Microsoft and announced end-of-life
dates for ciphersuites, then it might work.  But even that is a stretch,
look how hard it was for NIST to push 2048 bit RSA keys as a minimum

@_date: 2014-08-12 10:43:22
@_author: ianG 
@_subject: [Cryptography] Dumb question -> 3AES? 
The conventional answer to not doing anything is that you are now doing
cryptography.  So you have to explain to yourself why you think you can
do better than the people who spent their lives on this.  Are you that good?
In practice, you are far better off using their work as described in the
manual.  And spending the spare time on writing better software.
Given that the leading software break is still due to buffer overflows,
and nobody's ever cracked a big-name crypto algorithm in living memory,
you're probably better off focussing on the known roadkill not the
zombies in hollywood movies.

@_date: 2014-08-14 12:46:56
@_author: ianG 
@_subject: [Cryptography] Dumb question -> 3AES? 
I don't remember those at all ;)
Not cracked, overtaken.  Someone built a cruncher, recall ;)
Same same.  I'm excluding stupidity in this.  It is engineering, after
all, choose the tool that is appropriate for the job.
What ifs are so seductive, but they don't write a line of code for us.
Seems like we need a Goldman approach:  advocate that everyone employ
standard sizes like 1024 and 2048, but don't do that yourself.

@_date: 2014-08-15 11:37:35
@_author: ianG 
@_subject: [Cryptography] cryptography Digest, Vol 16, Issue 11 
Thanks for the update!  I'm still waiting for someone to report on which
big-name algorithm got broken in living memory.
(Oh, and what the strategy is for initiating a replacement in real
time... oops!  IETF, I'm speaking to you, but nobody's listening ;)
OK, so why doesn't someone propose Skipjack expanded to more bits
security?  Skipjack-X?  3-Skipjack?
If the NSA are still decades ahead of the public sphere, why not use the
Hmmm... I haven't heard of any such embarrassment for the ChaCha family?
OTOH, as far as I can tell, it's just a block cipher internally with a
stream wrapper around it...

@_date: 2014-08-15 13:15:49
@_author: ianG 
@_subject: [Cryptography] Encryption opinion 
What sort of people?  What sort of communications?
Reason for q is:  you said encrypted by which we guess you mean secure.
 But secure is not a binary concept, it is really a defence against an
attacker against your business.  In short, without a context that tells
us how much everyone cares (you, user, attacker) there is no way to
judge how much security is 'good enough'.
a.  Are you using RSA to directly encrypt the messages?  This is
typically frowned upon as RSA is tricky to use and many uses leads to
weaknesses.  The preferred classical method is to use RSA once to share
a secret with your counterparty and then use that to encrypt using a
block or stream cipher (etc).  Latter are less vulnerable.
b.  RSA 1024 is considered a weak length these days.  But it still
depends on "against whom?"  NIST recommends 2048, but the little known
open secret is that NIST has a mandate only to protect USG agencies.  So
2048 is recommended for USG agencies.  As they are currently targetted
by aggressive foreign spies, this makes some sense.  It makes less sense
for anyone else to pay attention to them, absent clear disclosures and
Nobody has any evidence of a 1024 RSA keylength being crunched.  And if
it happens, it won't happen to you, rather someone else more important.
 HOWEVER, public opinion will be against you.  So if you rely on your
claims of security as a big part of your marketing message, it's going
to be tough.
c.  The biggest danger in more ordinary usage of security systems is
your counterparty.  E.g., your spouse who's about to divorce you.  As
these messages are generally in cleartext on your future court
antagonist's phone, the encryption used isn't really a big concern.
EC has much smaller key sizes for the same strength.  Which helps... but
it also prefers to to do a key exchange, see a. above. although there
are some brave efforts to do EC only packet exchanges (read
 ).
You do understand that this is not a hobby, right?  People earn the big
bucks by spending a decade learning this stuff.  They don't just hand it
off to anyone as if it were an open source project, and it's their duty
to society to share their hard work...
You might get lucky and someone will throw you a few pointers, but you
should be prepared to pay for the decade or so of experience that you
need here.  If you don't respect the people who do the work to secure
your customers, how will customers respect you?

@_date: 2014-08-15 20:51:48
@_author: ianG 
@_subject: [Cryptography] cryptography Digest, Vol 16, Issue 11 
I think I mentioned before, but maybe the PR office needs a reminder.
If anyone in the NSA wishes to put to rest arguments about whether they
support the protection of civil infrastructure, they could simply
release Suite A.
It would be nice to feel that they are the good guys, again.

@_date: 2014-08-15 21:01:27
@_author: ianG 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
Yes.  1997?
But they don't count as being big-name algorithms.
Sadly, the GSM family were weakened by intent.  The business -> threat
-> security modelling of the mobile telephone network included journos,
PIs and free-phone spoofers as attackers;  it did not include spies, and
probably could not even if telcos wanted to.

@_date: 2014-08-16 14:16:06
@_author: ianG 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
Yes, these are all deprecated.  Once deprecated, they live in the halls
of fame as an algorithm that served their purpose but are now marked for
not being used.
This is engineering, right?  Once the end of life is reached, we
shouldn't be using them.  Right?
(That would be a protocol, requirement thereof, not an algorithm.
Although there is a sense that things like CAESAR/AE algorithms are
moving up the stack, and the new black boxes are doing more, we aren't
anywhere close to slotting in black boxes for protocols as yet.)
Yes, separate problem.  The people that love this algorithm agility idea
love to *add* algorithms but have no process to take them away.
Just because there are some famous names out there still relying on
EOL/deprecated algorithms, doesn't change the basic situation.
lol...  OK, point.  So what is it about the zombie algorithms?  Why do
they keep popping up?
Do we need NIST or IETF to put the dragonglass blade into them?  An RFC
that lists deprecated algorithms, updated on a yearly basis?
(That's a serious question, btw.  As far as I know, they don't have an
answer to the overall question...)
;-)  The point we should be making is that the one thing we can trust is
the strength of the big-name algorithms.  They've never failed us,
within their design parameters (including EOL).
Everything else has failed us.  But not the basic algorithms.

@_date: 2014-08-17 12:53:14
@_author: ianG 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
What are the national standardisation institutes responsible for?
Not the net:
I think we are all guilty for assuming their interests have an
unchallengeable connection with the interests of the global community.
And, guilty of letting them tell us what to do.  How many people do you
know who stood up and said NIST was wrong about the keylengths for CAs?
We assume too much, we know what the net requires, they do not.  We have
a job to do, why don't we do it?
NIST seems to publish a very short list.  shows 2,3-DES (they call it DEA) and AES.
What they don't say is anything else:  RC4?  MD5?  Camellia?  Skipjack?
 IDEA?  Maybe the sense of this thread is that RC4 is still a big-name
because nobody bigger has told people to stop using it?
Yes.  Back in 2000 or so, Lenstra and Verheul published their now
classic paper which draw a connection between the different types of
algorithms.  This made it tractable to think in bit-numbers, which even
managers could reason with.  128 bits of strength now made sense.
The result of that is a series of national statements on what they think
are the appropriate lengths.  For the rest of us, there is
 .
But that is about key lengths, assuming that the sizes of the keys are
reasonable proxies for the strength of the result.  This is probably a
good starting point if the algorithm is unchallenged, e.g., it is
big-name in the sense I was using the term.
(Yes, we don't have a really good title for that as yet.)
Ah.  Tell me about the SSL process for that?  Or any IETF process?
:)  So, let's say they were doing a responsible design effort to produce
your next smartcard citizen's ID.  The would want to pick some
algorithms that had at around 15 years left to go, after they started
rollout.  That's 10 years for the card to match passport, rolling out
for the next 5 years.  Now add 5 years before that for the rework.
So they need a 20 year timeframe.
But, if they are following national standards, they would likely then
have to add another 10 years to that, because that organisation has its
own lifecycle issues.
Which becomes tougher if you also consider that the smartcard has its
limitations:  you need a public key algorithm that is strong out to 25
years that will run fast on a smartcard?
If the IETF had a consensus about this they would, yes.  They would have
done it ages ago.
Problem is, IETF has a consensus to keep multiple algorithms in play,
probably because of the game theory aspects of securing rough consensus.
 What they do not have is any consensus on retiring old stuff, probably
for the same reason.
Meanwhile, your poor sysadm has to trawl through openssl config strings
because he's not actually represented in the consensus of IETF WGs.
It's his fault of course, he could always join, he knows how to use
email ;-)

@_date: 2014-08-22 21:50:05
@_author: ianG 
@_subject: [Cryptography] On 40-bit encryption 
There was an article printed in I think _Foreign Affairs_ from a long
time ago that was found and referenced by (IIRC) Ross Anderson by some
crypto warriors.
In there was an anecdote about the games they played with the South
Africans.  They stopped crypto exports to them.  But then the South
Africans started developing an indigenous capability.  So the NSA opened
up the crypto exports again in order to squash the local companies.
Take-away:  it's as much economic as a technical war.
ps; if anyone can find that article, please post.  It's a great view of
the inside tactics.

@_date: 2014-08-23 11:10:45
@_author: ianG 
@_subject: [Cryptography] some eyeballs make bugs shallow? 
NSA and GCHQ agents 'leak Tor bugs', alleges developer
By Leo Kelion Technology desk editor
Andrew Lewman: "The fact that we take a completely anonymous bug report
allows them to report to us safely"
British and American intelligence agents attempting to hack the "dark
web" are being deliberately undermined by colleagues, it has been alleged.
Spies from both countries have been working on finding flaws in Tor, a
popular way of anonymously accessing "hidden" sites.
But the team behind Tor says other spies are tipping them off, allowing
them to quickly fix any vulnerabilities.
He said leaks had come from both the UK Government Communications
Headquarters (GCHQ) and the US National Security Agency (NSA).
By fixing these flaws, the project can protect users' anonymity, he said.
"There are plenty of people in both organisations who can anonymously
leak data to us to say - maybe you should look here, maybe you should
look at this to fix this," he said. "And they have."
Mr Lewman said that his organisation received tips from security agency
sources on "probably [a] monthly" basis about bugs and design issues
that potentially could compromise the service.
However, he acknowledged that because of the way the Tor Project
received such information, he could not prove who had sent it.
"It's a hunch," he said. "Obviously we are not going to ask for any details.
"You have to think about the type of people who would be able to do this
and have the expertise and time to read Tor source code from scratch for
hours, for weeks, for months, and find and elucidate these super-subtle
bugs or other things that they probably don't get to see in most
commercial software.
"And the fact that we take a completely anonymous bug report allows them
to report to us safely."

@_date: 2014-08-23 12:19:58
@_author: ianG 
@_subject: [Cryptography] On 40-bit encryption 
Nice story!
Lucky that was all in the 1980s.  He would not have been allowed to do
that in the 1990s ;-)

@_date: 2014-08-24 12:40:26
@_author: ianG 
@_subject: [Cryptography] Encryption opinion 
The bad things that seem to happen at the user level rely on the split
between HTTP and HTTPS, in that users cannot tell the difference and get
Pervasive HTTPS would be one solution, because it then becomes
economically worthwhile for the browsers to work on the problem.
Otherwise, there is no economic solution seen to get the browsers to
work on secure UX.  While the TLS people are still obsessed about how
many bit-angels they can paint on an algorithmic pin, and HTTPS is a bug
to the UX crowd not a feature, there will be only marginal change to the
deployment ratio of HTTPS.
Right.  The crypto's purpose isn't to defeat the worst the NSA can throw
it at, but to assist the user in safely avoiding crooks.  But the NSA
has been ueber-successful in setting our agenda as opposing them, not
the crooks who steal value.  The latter isn't practical, is unreachable,
and the former is ignored;  the users lose both ways.

@_date: 2014-08-25 11:50:44
@_author: ianG 
@_subject: [Cryptography] Encryption opinion 
Indeed.  And if users have credit cards, they'll still use them or lose
them or hand them to waiters at dodgy restaurants.
So what was the mission of HTTPS, again?
HTTPS lists amongst its features protection against MITM.  Indeed, the
primary design influence is protection against MITM, being the big
difference between SSL v1 and SSL v2, and the  thing that the WGs
worry about.
Phishing is an MITM.  We could talk about this MITM versus that MITM,
but do you recall the 1990s when all this was pushed through against
objections? "If you do not stop the MITM you are evil" is the basic
sense of it.
So it turns out that HTTPS protects against a class of MITMs, not all
MITMs.  As the easiest MITM is outside HTTPS, what is the point of
protecting MITMs at all?
Yeah, this is CA-talk, it is a marketing story to get browser vendors
and developers to understand that they must have a certificate.
What it distracts from is:  protecting the user.
Right.  Now the browsers make the stupid decision that somehow a
self-signed certificate is somehow less secure than HTTP.
Basically the browsers don't do security.  They follow some recipe that
they were sold a long time ago.
Try this.  Imagine the browser vendors sat down and said, let's do
security.  Let's protect our users.  How would they be able to have that
conversation without saying "and we must stop phishing?"
And, do you think that if they browsers had said "we must eradicate
phishing!" they would have succeeded?  Of course the would.
I don't understand how they did it, but they managed to forget the user.
 Entirely.  Industry-wide cognitive dissonance allowed everyone from
IETF to vendor to CA to audit to proceed happily without addressing
phishing [0].  How do you explain that?
[0] Rest is history:  phishing in mid 2000s financed the investment
cycle that created the industrial attack machine we now know and loath.

@_date: 2014-08-25 14:32:25
@_author: ianG 
@_subject: [Cryptography] Encryption opinion 
I don't see the distinction.  The phisher redirects Alice's browser to
him.  He then goes to the site and extracts information to perpetuate
the deception.  What's not middle here?
I certainly understand that there is a frustration in taking a term that
is 'captured' by the SSL industry and using it against them.  That is
both the frustration and the point -- wake up and see what it is that is
No, it isn't useless.  But it isn't doing what it was sold to do.  Until
it is understood what it can do and can't do, and the messages are
aligned, the industry will remain in mobius loops of finger pointing.
So what we actually need here is an honest message.  CAs and PKIXs and
so forth need to go to browser vendors and say:  "sorry, we got it
wrong.  We can't stop all the MITMs.  You are responsible for
outside-HTTPS MITMing and you have to fix it."
But getting any industry player to admit weakness is like slicing off
limbs.  Players protect themselves, not the users.
People outside tried and succeeded, there are several plugins which did
the job.  Some of that work is now being appreciated, c.f. 'pinning'.
The original model was quite clear but did not survive to Netscape v1.0
because of what were then known as the real estate wars.  In essence
everyone believes what they have now works, so changing things is as you
say very hard.
Indeed crypto cannot;  but a holistic approach can.  And maybe this is
the issue.  We aren't up to fielding a holistic approach to security.

@_date: 2014-08-25 23:49:46
@_author: ianG 
@_subject: [Cryptography] phishing, was Encryption opinion 
MITM is an abstract term denoting two endpoints and a node in the
middle.  The correct communication goes between the endpoints without
interference.  An MITM interposes a middle node by one means or another
that can see plaintext and pervert intent.
Above, you've met those requirements.
A phish is a teaser mail that includes a URL pretending to be your bank
(eg Bob).  If you (Alice) click on it, you go there instead of your
bank.  You're now talking to the middle, which will then talk to the bank.
What happens after the MITM is successfully launched is outside scope of
the abstractness.  Likewise, how the attacker successfully interposes is
outside the scope of the term;  it could be protocol, it could be UX, it
could be other things.
Yeah, phishing is sending out a lot of URLs with deceptive messages to
try and get people to click on the URLs.
Right.  It's basically up to the websites and individuals to protect
their users from MITMs outside the HTTPS barrier.  It's also up to the
users to protect themselves from other failings...  which gets
exceedingly messy because browser policy and user demand is to provide a
protected or not appearance, no information.

@_date: 2014-08-26 13:02:40
@_author: ianG 
@_subject: [Cryptography] phishing, was Encryption opinion 
So, what then?  The phish then loses the credentials?  It does crossword
puzzles with them?
Clearly, the phish site uses the information found on the bank site,
captures the user's credentials, then hands the credentials over to
another agent (site? human?) who then contacts the bank.
Crystal?  It is to the attacker.
If you mean, these days, they do more, for sure.

@_date: 2014-08-26 13:12:02
@_author: ianG 
@_subject: [Cryptography] Encryption opinion 
Those technical terms were muddied and destroyed way back when.  That is
the flip side of the point;  the terms are used one way in one
conversation and another way in another.
Fair characterisation, albeit an unsporting attempt to point the finger
at the user.
Does not make it not an MITM.
I agree.  Why didn't they?
We could try your definition if you like, but the consequences are
equally ugly, worse even.
If MITM is only 'technical' this means that HTTPS only provides
technical protection.  Which is then going to knock out claims of HTTPS
ensuring you talk to your bank, because it can no longer do that, it's
not a social or human entity.
Which still leaves the browsers in deep-do-do because they have to
comprehensively translate the technical information they receive into
protection information for the user.  Only the browser can ensure you
are talking to your human counterparty, right?
Yet, users & vendors have been told repeatedly that HTTPS/SSL ensures
that they are talking to the agent that the user wanted to talk to.
Which, now that we are drawing precise lines in the sand with neoMITM,
is revealed as a deception, because it cannot make that statement.
Which deception they fell for, perhaps because vendors didn't want to
employ the serious architects to do that, they were happy for engineers
to follow the PKI playbook?  (Speculation as to reason, granted.)
The deception foisted on the browser vendors by the combined technical
community is a killer, it's far more deleterious than misuse of the term
MITM.  Basically the 'technical community' has unprofessionally looked
after its own house (jobs for everyone, more algorithms than can be
counted, heartbleed at a cost of $500m [0], etc) and sold a dangerous
product on without a proper warning manual.
If it were professional, we'd be looking at gross or criminal
negligence?  Deception?  Fraud if anyone participated in the spoils.
Pick which side of the fence you want to be on?  It's turtles all the
way down, the only choice is how fast you slide.
I'd prefer the side of "we made a mistake, we were wrong, we forgot that
an MITM could happen outside the tech, now you have to fix it, sorry."
We can get away with this because when it was all done in the 1990s,
nobody (including me) understood the way risk management works.
Now we do, or we should, if we're professional.
Phishing turns out to be far more important than (technical) MITM.  The
only serious record we have of MITM is phishing.  Active attacks against
small individuals were pretty much non-existent, except for phishing.
Help me, please, by providing evidence against.  Hence, newer risk
analysis approaches to security have struggled to find a believable case
for (technical) MITM on the net [2].  Even now, post-Snowden, there's
little to practically no evidence of a threat [3].
We got sold a hill of beans, to use an Americanism.
It's better to treat the terms for what they are.  A simple, conceptual
description of agents trying to talk to each other.  Stick artificial
limits on it like "only crypto protocols can be MITM'd" and you're going
to get yourself in trouble.
???  If you are trying to communicate with your bank, and the phisher
has taken those comms before they get to the bank, then that's an MITM.
Check, check, check.
[0] [1] or nuisance level like bots and viruses, which were more resource
thefts than money thefts.
[2] I guess we should include corporate middleboxen in this, and GWoC,
but as that's the 'legal' or one of the parties and/or ok... it's not a
threat by definition.  Even cafe wireless attacks seem to be more
mythology than fact.

@_date: 2014-08-27 02:29:11
@_author: ianG 
@_subject: [Cryptography] Encryption opinion 
It's curious that you say that.  In MITM there are the two end nodes and
a node in the middle.  When MITB takes over Alice's node, he isn't in
the middle anymore, he's Alice's node.
The sort of working assumption that was behind the common thinking of
the times, aka ITM or Internet Threat Model, was:
    Designers of Internet security protocols typically share a more or
less common threat model. First, it's assumed that the actual end
systems that the protocol is being executed on are secure. Protecting
against attacks where one of the end systems is under the control of the
attacker is extraordinarily difficult, if not impossible. This
assumption comes with two caveats. First, compromise of any single end
system shouldn't break security for everyone. There should be no single
point of failure. For instance, if an attacker breaks system A, then all
communications between B and C should be safe. If we must have a single
point of failure it must be possible to harden it against attack.
Second, attackers may control systems that attempt to pose as legitimate
end systems. All we're assuming is that users can expect that their own
machines haven't been compromised.
Other than that, we assume that the attacker has more or less complete
control of the communications channel between any two machines. ...
Eric Rescorla, _SSL and TLS -- Designing and Building Secure Systems_
Although I wouldn't swear to it, when Philipp was writing his essay on
MITB in 2007 or so, it got christened with a new title because the
attack wasn't an MITM and it wasn't phishing either.  But in order to
grab attention it was felt that a term closer to MITM was more sexy
whereas something like "New Developments in Phishing" was already too
boring for words.
So, terms matter.  In using the term MITM narrowly or broadly, or new
terms such as MITB, we are framing the conversation.  We're making
statements about whether it is IN or OUT of our bailiwick.  And we're
making statements as to who is responsible for it.
    An obvious corollary of the assumption that the attacker can modify
traffic is that the attacker can shut down all communications between
any pair of machines simply by removing all relevant packets. This is
one form of denial-of-service attack. Another form would be to force you
to use up enormous CPU resources responding to connections.
Conventionally, protocol designers don't worry about denial-of-service
attacks not because these attacks aren't important but because they're
extraordinarily difficult to prevent.
So, back in those days, us protocol designers totally ignored DOS.  And
node compromise, and all sorts of other things.  Now we know better.
Now we know that even if we can't do much about it, the attacks outside
will frame the security result of our protocol.  And, attackers don't
care what we label it.

@_date: 2014-08-27 10:47:41
@_author: ianG 
@_subject: [Cryptography] Encryption opinion 
2 for 1.
Right -- but framing it as inside or outside doesn't change the
importance, only the marketability.  Sick world, huh?
That is a question!
I spent many years over at Mozilla, trying to get them to do something,
anything about phishing.  They refused.
Once, just once, patient long-winded argument got the engineers there to
say "Oh, you have a point.  Right.  Phishing.  Our users.  Shit."
To which they added:  "Now you have to go to IETF and PKIX committee and
get them to tell us what to do."
Boom.  The long and the short of it was that the browser vendors had
outsourced their security architecture to the standards groups.  (Why
they did this is a fascinating study in and of itself.)  So, now that
they had no architecture components for security they are entirely
dependent on the IETF and/or other folks ... *to tell them what to do*.
Yet, the IETF are unified in their consensus that phishing is not their
problem.  Perhaps, a cute social engineering thing that happens to other
people, but decidedly not their purview because it ain't no MITM, dammit.
Both groups aren't wrong, to themselves.  Both groups are right in some
light, by some assumptions akin to SEPs, "someone else's problems."
But neither group are serving the public interest;  they are locked in a
deadly embrace of insecurity, serving a long out-of-date security model.
It would help if the IETF loudly said something like:
    "Use of the HTTPS product will solve these problems:
    in-protocol MITM, confidentiality leakage, reliability...
    It will not solve phishing or ex-protocol MITM, however
    we agree to term it.
    *Vendors must solve these problems in the application*
    else be eternally damned by their users."
But they don't.  And won't.  IETF group members believes that phishing
is not MITM, and it is outside their bailiwick.  They really don't care
that much, they only care about the model-properties from 1990s.
Want proof?  Go check out the new work to put opportunistic security
into TCP.  Half of the people there cannot grok the need for the
encryption before the authentication, and keep coming up with "what
ifs..." as if things like SSH, Skype don't exist.  It's like they are
not actually on the Internet, but rather in some sort of CS protocol
course bubble that teaches CIA and the MITM as the only danger they have
to destroy above all things.
See now why I describe MITM as include phishing?
Tru, dat.  The nearest we got to a widely distributed secure platform
was a macbook.  Controversy starts...
I don't claim to understand the above point, but it's redolent of a few
rants that have been ruled 'in the rough' ;-)
Yup.  It's good to tell people we're teaching them security, but
actually teach them correctness or reliability.

@_date: 2014-08-27 11:02:35
@_author: ianG 
@_subject: [Cryptography] phishing, was Encryption opinion 
I found a couple of definitions on the net last night, and neither of
them closed the argument.  Here's one from Steve Kent:
      Man-in-the-Middle attack (MITM)
      A form of active wiretapping attack
      in which the attacker intercepts and selectively modifies
      communicated data to masquerade as one or more of the entities
      involved in a communication association.  Masquerading enables the
      MITM to violate the confidentiality and/or the integrity of
      communicated data passing through it.
[good notes snipped, yes there are many uses for information!]
Deciding what to do about MITM at whichever level it occurs, however it
occurs, and whatever label we put on it has everything to do with design
and employment of crypto-security protocols.
  it turns out that blocking MITM using crypto is possible,
  but introduces user costs which slow adoption, and
  it turns out that low-level protocol MITM isn't much of a
  threat, and
  it turns out that there is an easy attack at the user level,
  far far easier than any in-protocol-MITM,
  it might be pointless to load up the user with additional
  costs of defending something that ain't going to be a problem,
  and instead switch those resources to what is a problem.
Right.  Because the protocol didn't solve the authentication needs, the
users have to do it themselves.  A mystery!

@_date: 2014-08-29 16:34:43
@_author: ianG 
@_subject: [Cryptography] Encryption opinion 
I would not suggest that phishing was an MITM just for mere politics.
You and I disagree on the facts, but it's the net, that's what we do
here ;-)
Close.  My problem (the net's problem?) is that the IETF doesn't care
what happens outside their mandate.  They are happy to throw protocols
over the wall, and get back to cute protocols problems.
But the world has moved on from the 1980s.  Attacks are far more
sophisticated -- as y'all keep pointing out -- than can be modelled in
committee and a CS textbook.
Meanwhile, inordinate amounts of important groups follow IETF protocols
without thought.  They think they are done.  When there's a gap, oh
dear, no solution.
And of course, it's impossible to blame either group in isolation...
Hence I say that the 4 or so groups that are responsible for phishing
are locked in a deadly embrace of irresponsibility.
:)  Yeah.  So there are any number of these groups out there, as Paul
Ferguson pointed out.  Suffice to say, forming a group or joining a
group is not the problem.
Meanwhile, you are definitely on the money in that the center of the
problem is the user interface.

@_date: 2014-08-29 16:39:33
@_author: ianG 
@_subject: [Cryptography] Phishing and other abuse issues [Was: Re: 
Good pointers.  But no banana sadly.  The crux is whether these groups
have the credibility to provide security leadership to the browser
vendors.  They don't.  IETF holds the credibility in vendors' mindset,
but as many have suspected, that isn't IETF's mandate.
Starting another group isn't the answer, we've already got a crowded
field of them.  As xkcd puts it:
Or, if it is the answer, it should be done like vancouver or montreal or
wherever that first anti-phishing meeting was done.

@_date: 2014-08-30 11:39:56
@_author: ianG 
@_subject: [Cryptography] digital currency is currency - Ecuador 
Sigh, yes.  A term with even less precision than MITM ;)  In the 1990s
we called it eCash (which was a trademark) or digital cash.  These days
the Bitcoiners seem to have coalesced around the term cryptocurrency
which I understand to be the same thing, although Bitcoiners as a
community mostly only understand the blockchain family.
What Ecuador is doing is more properly called mobile money.  It is an
application inserted into the phones[0] by the telco.  It does simple
requests to the telco to do transfers.  The business was pioneered by
Safaricom with mPesa, so whatever you know about that will likely happen
with Ecuador.
Every developing country should have mobile money, that they don't is a
crime against humanity.
The reason for the criticism is threefold:  Banks will kill this project
if they can because it puts money in controlling hands of another, a
telco.  2, the USA will be extremely nervous of any attempt to move away
from the dollar, as its dollar reserve status is something they fight
hard to keep[1].  And 3, the normal latino incapability to put a project
into the field, in that the Ecuadoreans have been talking about projects
like this for at least a decade to my knowledge.
link seems gone.
[0] so works on dumb/feature phones as well which is important for
unbanked market
[1] not that you'd know it from their foreign policy.

@_date: 2014-08-30 12:02:07
@_author: ianG 
@_subject: [Cryptography] heartbleed first blood? 
In the hunt for actual data of thefts not myth, ftr:
As many of you may have already been aware, a breach at Community Health
Systems (CHS) affecting an estimated 4.5 million patients was recently
revealed.  TrustedSec obtained the first details on how the breach
occured and new information relating to this breach. The initial attack
vector was through the infamous OpenSSL ?heartbleed? vulnerability which
led to the compromise of the information.
This confirmation of the initial attack vector was obtained from a
trusted and anonymous source close to the CHS investigation. Attackers
were able to glean user credentials from memory on a CHS Juniper device
via the heartbleed vulnerability (which was vulnerable at the time) and
use them to login via a VPN.
working their way through the network until the estimated 4.5 million
patient records were obtained from a database.  This is no surprise as
when given internal access to any computer network, it is virtually a
100% success rate at breaking into systems and furthering access.  This
is the first confirmed breach of its kind where the heartbleed bug is
the known initial attack vector that was used.  There are sure to be
others out there, however this is the first known of its kind.

@_date: 2014-12-01 13:46:55
@_author: ianG 
@_subject: [Cryptography] "completely unexpected" drop in Cisco's foreign 
Cisco?s disastrous quarter shows how NSA spying could freeze US companies out of a trillion-dollar opportunity
Bellwether Cisco indicates American tech companies are no longer welcome in Russia and other emerging markets.(AP Photo/Lee Jin-man)
Written by
Christopher Mims at mims
November 14, 2013
Cisco announced two important things in today?s earnings report: The first is that the company is aggressively moving into the Internet of Things?the effort to connect just about every object on earth to the internet?by rolling out new technologies. The second is that Cisco has seen a huge drop-off in demand for its hardware in emerging markets, which the company blames on fears about the NSA using American hardware to spy on the rest of the world.
Cisco chief executive John Chambers said on the company?s earnings call that he believes other American technology companies will be similarly affected. Cisco saw orders in Brazil drop 25% and Russia drop 30%. Both Brazil and Russia have expressed official outrage over NSA spying and have announced plans to curb the NSA?s reach.
Analysts had expected Cisco?s business in emerging markets to increase 6%, but instead it dropped 12%, sending shares of Cisco plunging 10% in after-hours trading.
This completely unexpected turn, which Chambers said was the fastest swing he had ever seen in emerging markets, comes just as Cisco is trying to establish itself as a bedrock technology provider for of the internet of things, which industry analysis firm IDC says will be an $8.9 trillion market by 2020. This quarter Cisco unveiled the nPower chip, a super-fast processor designed to funnel the enormous volumes of data that the internet of things will generate. Cisco also announced the Network Convergence System, a handful of routers that will use the nPower chip.
Arguably, the current shift in the underlying infrastructure of the internet makes Cisco and other American companies uniquely vulnerable. The move to cloud services, streaming video and machine to machine communication (i.e., the internet of things) means new standards and new default hardware providers are taking root, and if NSA spying keeps American companies from dominating the market at an early stage, it could mean that in the long run they?ll simply be locked out of these markets while competitors like Huawei and ZTE reap the benefits.

@_date: 2014-12-03 14:39:32
@_author: ianG 
@_subject: [Cryptography] MITM watch - Tor exit nodes patching binaries 
The real MITMs are so rare that protocols that are designed around them fall to the Bayesian impossibility syndrome (*).  In short, false negatives cause the system to be ignored, and when the real negative indicator turns up it is treated as a false.  Ignored.  Fail.
Here's some evidence of that with Tor:
... I tested BDFProxy against a number of binaries and update processes, including Microsoft Windows Automatic updates.  The good news is that if an entity is actively patching Windows PE files for Windows Update, the update verification process detects it, and you will receive error code .... If you Google the error code, the official Microsoft response is If you follow the three steps from the official MS answer, two of those steps result in downloading and executing a MS ?Fixit? solution executable. ... If an adversary is currently patching binaries as you download them, these ?Fixit? executables will also be patched. Since the user, not the automatic update process, is initiating these downloads, these files are not automatically verified before execution as with Windows Update. In addition, these files need administrative privileges to execute, and they will execute the payload that was patched into the binary during download with those elevated privileges.
(*) I'd love to hear a better name than Bayesian impossibility syndrome, which I just made up.  It's pretty important, it explains why the current SSL/PKI/CA MITM protection can never work, relying on Bayesian statistics to explain why infrequent real attacks cannot be defended against when overshadowed by frequent false negatives.

@_date: 2014-12-03 16:36:45
@_author: ianG 
@_subject: [Cryptography] Construction of cryptographic software. 
I've written up my philosophy of RNGs here:
1. Use what your platform provides. Random numbers are hard, which is the first thing you have to remember, and always come back to. Random numbers are so hard, that you have to care a lot before you get involved. A hell of a lot. Which leads us to the following rules of thumb for RNG production.
     a. Use what your platform provides.
     b. Unless you really really care a lot, in which case, you have to write your own RNG.
     c. There isn't a lot of middle ground.
     d. So much so that for almost all purposes, and almost all users, Rule  is this: Use what your platform provides. E.g., for *nix, use urandom [Ptacek].
     e. When deciding to breach Rule  you need a compelling argument that your RNG delivers better results than the platform's [Gutmann1]. Without that compelling argument, your results are likely to be more random than the platform's system in every sense except the quality of the numbers.
If you find yourself disagreeing with Rule  read on...

@_date: 2014-12-05 16:14:06
@_author: ianG 
@_subject: [Cryptography] cost-watch - the cost of the Target breach 
I often point out that our security model thinking is typically informed by "stopping all breaches" rather than "doing less damage."  Here's some indication of damage.
The ruling is one of the first court decisions to clarify the legal confusion between retailers and banks in data breaches. In the past, banks were often left with the financial burden of a hacking and were responsible for replacing stolen cards. The cost of replacing stolen cards from Target?s breach alone is roughly $400 million ? and the Secret Service has estimated that some 1,000 American merchants may have suffered from similar attacks.
The Target ruling makes clear that banks have a right to go after merchants if they can provide evidence that the merchant may have been negligent in securing its systems.
At the time of its breach last year, Target had installed a $1.6 million advanced breach detection technology from the company FireEye.
But according to several people briefed on its internal investigation who spoke on the condition of anonymity, the technology sounded alarms that Target did not heed until hackers had already made off with credit and debit card information for 40 million customers and personal information for 110 million customers.

@_date: 2014-12-07 16:41:35
@_author: ianG 
@_subject: [Cryptography] Sites certified as secure often more vulnerable 
"Seals certifying the security of e-commerce sites and other online destinations have long aroused suspicions that they're not worth the bits they're made of?much less the hundreds or thousands of dollars they cost in yearly fees. Now, computer scientists have presented evidence that not only supports those doubts but also shows how such seals can in many cases make sites more vulnerable to hacks."
These seals sellers are examples of /institutions/, which are ways to move information or resources from one group to another in a trusted fashion, when they can't clear the market for direct trade.
The market for security exhibits a paucity of hard information.  It is very clear that the buyer has no way to know if some product is helpful or not, if she is left to looking directly at the product.
E.g., an anti-virus product does stuff, but the buyer doesn't and can't measure whether it is doing any good.
The buyer needs help.  Where does that help come from?
An institution can help to capture seller information and move it to the buyer.  Brands can use past good behaviour and leverage it to promise future protection.  Closely related, word-of-mouth or reputation can also inform.  Insurance can share the losses across a large group. Warranties can put 'skin in the game' for the vendor.  Audit is another.
But brands [0] and word-of-mouth need to be built on these other things, and can quickly flip.  Warranties or insurance are unheard of in the security business [1].  Audits are impenetrable, unreliable and self-serving.  Which leaves institutions?
What the above research is saying is that these institutions in particular are not doing any good, and are possibly opening new holes and encouraging users to relax.  Which says something bad about institutions, but it is difficult to sort out the correlations from the Are all institutions in security failures?  Is there a successful institution in security?
Is there a tweak that these seal sellers could perform, perhaps at greater cost, to push their institution into positive benefit?  If so, why didn't they?
Is the information base so suspect that an institution cannot be honestly built?
Is there something about the economic incentives that leads all well-meaning ventures into a race to the bottom, and eventual irrelevance on security if not incumbency questions?
[0]   The 10 brands that are involved: Norton Secured, McAfee Secure, Trust-Guard, SecurityMetrics, WebsiteProtection (provided by GoDaddy), BeyondSecurity, Scan Verify, Qualys, HackerProof, and TinfoilSecurity are mostly well known, perhaps we can say this is as good as it gets.
[1]  Perhaps not unheard of but notable through its rarity.  PHB mentions it recently for CAs, and there are occasional people popping up claiming that they are going to build it.

@_date: 2014-12-07 18:16:30
@_author: ianG 
@_subject: [Cryptography] Toxic Combination 
As you yourself show below, asking for references is a setup for a As I'm sure you know, things like certificate pinning were trialled and tested seriously in the mid 2000s as phishing turned up.  Browsers successfully ignored those efforts.  I don't recall whether that excuse was used, but I'd not be surprised, they were on a mission to block all outside influence.
This is all pre-chrome times so perhaps google can look to avoid the mistakes of Mozilla and Microsoft.  But as I'm also sure you found out, it wasn't that easy, the power of standards and compliance is immense.
No, no, let me put words into Peter's mouth ;)
The reason the vendors won't act for user security is twofold.  Firstly, the vendors are doing what they are told by the standards bodies and the upstream vendors.  The browsers don't really have security / architectural capability because they just follow the standards [0]. The vendors have outsourced the security equation, so they are totally going to ignore any input from any alternate source, peter paper or proven, up to and including evidence that they are part of a perpetual and profitable criminal enterprise.
You might (should) ask why.  There is at least one reason why Mozilla and Microsoft refused to enter into the strategic architectural security game:  liability.  If they recognised that there was a security weakness, and they sought to do something about it, they could become theoretically liable for phishing losses from their users.  Given the state of American legal behaviour, they did the obvious thing, and denied their liability for all security losses [1] and therefore sat on their "we follow standards" principles aka "best practices" lie aka get out of jail card.
(The exception to the above dynamic might be google which has been caught on both sides of the fence - as browser vendor and as online merchant.  It has therefore been incentivised by being liable for more parts of the equation to somewhat rock the status quo.  By thinking of alternates, and trying to push them through [2].)
Then, secondly, when there is a new standard, the vendors wait until others have done it.  As nobody wants to leap off without a guarantee of the others doing it too, the natural state is that nothing happens.  As per Peter's suggestion.  This is by way of a natural cartel, and yes, there is such a thing, and it can be deliberately constructed, and it can be manipulated.
Yes, but you confuse the causality and/or intent.  It is a fact that the vendors entered into a cartel called CABForum that prepared its do-nothing set of standards in secret.  But, entry into a cartel only worked because the vendors had a total absence of security capability at the architectural level, as above, which is the underlying causality. The vendors did not themselves think they were trying "to keep the CAs in business" but of course the CAs didn't care what was thought, only what was achieved.
The mission of browsers is pretty much that.  They don't do new security designs.  The do what standards and cartels tell them to do, and try to preserve the web in the process.
Sucks to be a user.
[0] in Mozilla's case, this is written into their manifesto and their bloodstream.  Observance of standards is an inviolable part of their genetic makeup, and I doubt they even understand where it comes from (the browser wars, as it happens).
[1] for fascinating circumstantial evidence of this, look to BR 18.2 from memory.
[2]   FWIW, YMMV, that's just my theory.  Google like all vendors famously never says anything of any substantial fact basis so they are unable to correct that theory if it were to be erroneous.

@_date: 2014-12-07 19:04:43
@_author: ianG 
@_subject: [Cryptography] cost-watch - the cost of the Target breach 
That would be an exaggeration.  Chip&pin is a lot more secure, it's very hard to clone them [0].  Although, implementations can go badly wrong, eg., it has recently been found in the USA where some players elected to ignore the signature checks.
Well.  So, the banks can choose to move the burden of proof across to the customer.  They can also choose other things.  It isn't the technology's fault that the burden of proof moves, but the deliberate choice of the parties involved.
And the party in direct control is the banks, and it is no surprise that the banks in Britain have elected to move the liability directly to the consumer when the consumer has no choice nor capability to deal, because this rests nicely on a claim by banks that their systems are perfect [1].  In contrast, European banks are much more ready to assume liability for things, perhaps because they think more along the lines of overall service and stability than this quarter's bottom line.
It's also the case that the British courts have bungled the handling of liability, including one case where they ignored evidence of the bank's culpability to assign the losses to the victim [2].
Right, it is entirely possible that the American banks stuff up the implementation.  Or it doesn't make economic sense [3].  Or any of 100 other reasons.  It's not really set up for them to do well :)
[0] [1] [2] [3] Steve Bellovin:

@_date: 2014-12-07 20:19:46
@_author: ianG 
@_subject: [Cryptography] MITM watch - Tor exit nodes patching binaries 
That looks very close, thanks, Jerry!  The base rate fallacy presents the logic I'm trying to get to.  If we use the numbers of the 2nd example in wikipedia [0], and paraphrase heavily:
Software looks at certificated SSL connections and displays a false result in 5% of the cases where the connection is not MITM'd.  However, the code never fails to detect a real MITM.  Assume 1/1000 of connections are MITMs.
Let's look at the case where the browser looks at a random SSL connection and does a test on it, and discovers it is an MITM.  How high is the probability that this is an MITM?
Many would answer as high as 0.95, but the correct probability is about To find the correct answer, one should use Bayes' theorem. ...
[real maths snipped]
A more intuitive explanation: in average, for every 1000 connections tested,
     * 1 connection is MITM, and it is 100% certain that for that connection there is a true positive test result, so there is 1 true positive test result
     * 999 connections are not MITM, and among those connections there are 5% false positive test results, so there are 49.95 false positive test results
therefore the probability that one of the connections among the 1 + 49.95 = 50.95 positive test results really is an MITM is about 2%.
If we are saying that the developers of the secure browsing system are relying on a high true positive rate (detects an MITM, is an MITM) to predict that the system will defend against MITMs, then yes, the SSL secure browsing system falls to the base rate fallacy.
But it does this via a few steps, being fallacy then result.  Firstly their logic ignores the false negative rate (hits a clean connection, declares it an MITM), which literally would be the base rate fallacy. Secondly, the prediction ignores the effect that the flood of false negatives has on the users:  they turn it off, aka click-thru syndrome, confusion, etc.
We might call this second part the base rate fallacy response or syndrome.
ps; people will note I routinely screw up the false/true negative/positive matrix...
[]

@_date: 2014-12-08 01:37:50
@_author: ianG 
@_subject: [Cryptography] cost-watch - the cost of the Target breach 
On that point of bad legal systems supporting the banks which dump liability, I've just been alerted to this new book from Stephen Mason who probably knows more about the legal side of this equation than anyone.
When Bank Systems Fail ?19.99
Debit Cards, credit cards, ATMs, mobile and online banking: your rights and what to do when things go wrong
  New Second Edition
Previous edition: Electronic Banking: Your Rights
Paal ?iestad?s credit card was stolen in 2008 in Rome, and over Nok 50,000 taken. The PIN was not written down, because it had been committed to memory. The bank claimed, without any evidence, that he was grossly negligent by keeping the PIN with the card. The Norwegian Complaints Board and the District Court agreed with the bank. Mr ?iestad was labelled a liar. While waiting for the appeal hearing, the bank wrote to him in June 2012, admitting it was wrong.
The aim of this book is to help you deal with your bank if anything like this should happen to you.
The interconnected world of online, mobile and ATMs is a ripe hunting ground for thieves. Protecting your rights if you are a victim can be difficult, especially if the banks suspect you have been involved. Stephen Mason is a renowned expert on electronic evidence. Here for the first time, he has provided the information and the tools with which to protect yourself.

@_date: 2014-12-08 15:48:21
@_author: ianG 
@_subject: [Cryptography] Toxic Combination 
Ah, I had hoped to choose my words carefully, but maybe not.
CABForum may not be a secret cartel *now* but they certainly started as one, and did the important work as one.  And, they prepared the two key documents in total secrecy, away from the public eye, then sprung them on a naive public.
E.g., Baseline Requirements was 2 years in the making, and not a word was breathed about that document in the Mozilla forums before it was complete.  Mozilla itself was working on the document in secret, while talking the open talk on the open mailing list.  It was a complete coup, a pillaging of the Mozilla manifesto by insiders.
After Baseline Requirement was rammed through a false public comment period using Mozilla's "open" list (yes I was there, it was rammed) CABForum started to open up.  Certain large users such as Paypal had publically expressed serious reservations about being part of a secret cartel by then, and the taint of truth was hard to shake.
But by then the damage was done -- the key documents were set in contract and the end-user was totally screwed.
As to whether they are fully open now, I don't know, but I personally doubt.  We'd have to check the voting structure and see if you and I and everyone with users' interest in mind can for example join and get a vote, enough to outweigh the power of the vendors or the CAs.  I'd bet dollars to dust that they've structured it such that the CAs maintain power, we can't for example overturn BR and put in some user-aligned ObCrypto:  just another page in the history of why secure browsing ended up where it is.  Crypto is less a dual-purpose munition, more a multi-bladed swiss army knife that'll make a mess of clumsy fingers.

@_date: 2014-12-09 14:41:50
@_author: ianG 
@_subject: [Cryptography] MITM watch - Tor exit nodes patching binaries 
Good point!  Perhaps we should all start wearing t-shirts with
       ask me to explain
       BASE RATE FALLACY
in huge letters 2nd line as we go through security.

@_date: 2014-12-10 16:41:25
@_author: ianG 
@_subject: [Cryptography] North Korea and Sony 
This is a real development.  Large IT companies (I'm referring to the banks here, who are by majority vote are IT orgs at this stage in their evolution) are unable to secure themselves.  This is a gathering trend.
The number of large groups that find themselves unable to deal with the increasing number of serious attacks is an indication on the security E.g., Did we not predict this?  Did we not prepare?  Did we not know how to prepare?  Was it considered an acceptable risk?
It's probably OK to say, we got the risk wrong, now we'll just do some re-work, add some stuff and get back to business.  That will just cost hard money, no hard thing for banks at least.
But that might not be what is happening.  If these orgs are demanding state representation, that looks awfully like going to the USG and saying we need NSA help.  Google and others have also in the past cut deals with the NSA to get their help, before backing off in realisation that the NSA isn't exactly going to help them.
The point here is if google can make this choice, even for a short time, what hope Sony?
Does the security industry actually know enough to deal with this?
The implications of this question are pretty severe.  If the security industry has the smarts to deal with it, then there are institutions (NSA, companies, etc) that can do the Sony makeover and turn the story into one that will sell.
If not, then not.  What does the world look like when no-one can save Sony?  Or the banks?  Or...
ps; long term readers will know that what is being tested today is the hypothesis that security is a market in lemons, or the alternate, a market in silver bullets.

@_date: 2014-12-10 18:01:22
@_author: ianG 
@_subject: [Cryptography] North Korea and Sony 
It's a widely discussed secret:
My point in highlighting it however is along the lines of "even google cannot avoid the search for institutions, and may come up short."

@_date: 2014-12-12 12:58:36
@_author: ianG 
@_subject: [Cryptography] North Korea and Sony 
(snipping parts on ye olde familiar systemic risk :)
I discuss this effect in market for silver bullets [0].  The interesting thing here is that the regulated / banking market has a dynamic that provides a strong stability to a best-practices set of behaviours that is disconnected from the primary goal.  In other words, it is better to do what everyone else is doing, than to do ones own security.  And what everyone else is doing is too strong a consensus to migrate with changing needs.  Indeed, it's strong enough to even withstand common knowledge that it is a fake.
Breaking out of this dynamic requires competition, by which I mean real competition on security models, not tight regulation.  It is the regulatory model which drives competition away and creates the flock of sheep known as 'best practices'.
In the alternate, if the best practices do rule, then there is obviously a pot of gold for the supplier who can deliver that.  But curiously, this by itself provides little or no evidence that the supplier provides security, only superior listening skills and incumbency.
[0]

@_date: 2014-12-12 13:29:31
@_author: ianG 
@_subject: [Cryptography] North Korea and Sony 
If it is the bank's own state, then commitment means courts means open But, I think it is a different thing when we are talking about say BoA and China, or a Swiss bank and NSA.  Although they may not be able to deny every attack, I think they should be able to put up a pretty good Spooks don't use rocket science, except in the rarest of cases, and much of the stuff they do use routinely is similar that which any criminal gang could specialise in over time if it works. E.g., credit card forgery, sql injection, phishing of employees, perversion of insiders.
I think the biggest reason why a big bank can easily be attacked by a serious attacker is because it doesn't even try to defend itself.
Well, given the SWIFT scandal of around 2008, we have the luxury of knowing more.  At the time (hearsay has it) there were no less than 3 US agencies hacking into SWIFT though various means.
As you cast it, sure, I'd agree that it is quite rational for the bankers to ask for state-level protections.
But look a little deeper.  They aren't asking for FBI protection, which is also "state-level protection" to which they are entitled, one suspects.  They are asking for the NSA, in effect.
Which also in effect says that the FBI -- who have their secret/illegal backchannel to the NSA -- aren't helping either.
They are also asking for a subsidy.  That might say they are just used to asking for subsidies for everything, as they also outsource any tricky problem to someone who can be paid to provide a figleaf.
Or it might be a reflection on their view that the current security suppliers / knowledge is inadequate at their budget levels.  I'm inclined to the former because their budget is huge and they can afford to take the long view -- if they care.
Given all that, it would also be in a sensible regulator's interests to refuse such a request.  I can't see any upside to the request, it all looks like downside to me.  Sadly, I doubt the regulator will even blink at such a request.

@_date: 2014-12-12 14:57:03
@_author: ianG 
@_subject: [Cryptography] Why Alexander Hanff won't be using "Let's 
He is making an assumption that existing CAs don't also have the problems he claims.  I don't know how he would go about proving that assumption, and I'm pretty sure it is dead wrong.  My working assumption would be the reverse.
I'd say that Verisign removed that component themselves from architectural consideration by writing their contracts to eliminate liability to their clients.  To the extent that they did, other CAs followed suit, whether they understood or not.  If there is no liability, insurance is easy to get, and is only marketing.
As an aside, I don't think in my time in CAs (mostly 2005 to 2012) I came across a serious discussion of insurance.  There was one requirement about insurance written into EV but it included such a blatant "Verisign exemption" written into it (wte $bn companies could elect to self-insure), I don't know if anyone took it seriously.
Right.  Also, audit has been part of the game for so long they've ensured that their work is expensive, has little liability blowback, and is impenetrable to others.  It is no surprise that audit adds little, that's no longer its purpose.
I wrote all this logic up in a 7 part essay:
with comics ;)
OK, so to point:  how much can a user get back from such a insurance? Has Verisign or any other CA ever paid out to a user from an insurance contract?  Or even self-insurance, by covering losses from their own pocket?  Has an insurer ever been claimed against?
If the answer to these questions is NONE, NEVER then we have to ask what the point is.  If it shows no evidence of actuarial mechanics, it's there for another purpose.
This is Europe v. North America.  In the former, the regulator exists and is very prominent in their documentation and thoughts.  Considered essential.  In the latter, not.
Yeah.  So, up until the late 2000s, there was no real evidence of much attack against the system.  It was put in place on a theoretical security model pulled from some old text book.
But around about 2011 (I date it) the scene hotted up dramatically.  The problem then was that the industry had pretty much atrophied into its structure and had no capability to migrate.  One could not have said that about the mid 1990s when people were thinking about these threats :)  See fig 6. here:
As an aside, CAcert which is arguably a forerunner to LetsEncrypt, was able to scale up to deliver the certs en mass.  However it wasn't able to crack a revenue model that enabled it to climb the audit barrier (*).
(*) noting of course that, as auditor, I was biased towards the question of increasing remuneration, to the extent that I failed them.

@_date: 2014-12-15 19:18:54
@_author: ianG 
@_subject: [Cryptography] OneRNG kickstarter project looking for donations 
About this project
After Edward Snowden's recent revelations about how compromised our internet security has become some people have worried about whether the hardware we're using is compromised - is it? We honestly don't know, but like a lot of people we're worried about our privacy and security.
What we do know is that the NSA has corrupted some of the random number generators in the OpenSSL software we all use to access the internet, and has paid some large crypto vendors millions of dollars to make their software less secure. Some people say that they also intercept hardware during shipping to install spyware.
We believe it's time we took back ownership of the hardware we use day to day. This project is one small attempt to do that - OneRNG is an entropy generator, it makes long strings of random bits from two independent noise sources that can be used to seed your operating system's random number generator. This information is then used to create the secret keys you use when you access web sites, or use cryptography systems like SSH and PGP.
Openness is important, we're open sourcing our hardware design and our firmware, our board is even designed with a removable RF noise shield (a 'tin foil hat') so that you can check to make sure that the circuits that are inside are exactly the same as the circuits we build and sell. In order to make sure that our boards cannot be compromised during shipping we make sure that the internal firmware load is signed and cannot be spoofed.

@_date: 2014-12-16 16:39:11
@_author: ianG 
@_subject: [Cryptography] OneRNG kickstarter project looking for donations 
Surprisingly, the OneRNG project is already half way to the goal of $10k NZD after only a week.
One reason I really like this project is that it is hopefully totally open.  If we can seed the world with open hardware designs, we can have a chance of leaking this project into all sorts of other things like home routers, IoT things, Bitcoin hardware wallets etc.

@_date: 2014-12-17 15:11:03
@_author: ianG 
@_subject: [Cryptography] Any opinions on keybase.io? 
Transparent on & off encryption?  Yes, that makes it easier for all of us ;-)

@_date: 2014-12-19 02:44:20
@_author: ianG 
@_subject: [Cryptography] OneRNG kickstarter project looking for donations 
With a secure operation I know of [0], procedure for purchasing critical hardware is to buy it from a random street seller, ex-stock, with no Not that I'm saying this was always done.  But it was written and that was the intent.
[0]

@_date: 2014-12-21 16:34:32
@_author: ianG 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
How about dual-sourcing through mutual enemies?  E.g., use a China fab and a fab run by the dalai lama.  Or a Russian one and a Chechen one.
Then, sample the chips, open them up, and test whether the tracks / layout are the same as each other?
(I have no idea if such a technique for reading the chip like that

@_date: 2014-12-21 17:36:45
@_author: ianG 
@_subject: [Cryptography] Certificates and PKI 
Gawd, I wish I had a nice answer to that, but it's turtles all the way down.  This is a bad story, and is a graveyard for your soul.
First the problem.
Unfortunately, the problem with the vendors is that they are locked into an oligarchy or cartel.  This group have also traditionally outsourced the security architecture question to any other place that seems conveniently remote [1] but in reality is nowhere.
Typically with an open project the solution would be to submit a patch.   This is the time-honoured real path to address an itch.  (Of course that isn't available for the others.)
Also, in principle, there should be open discussion eg here on this group or on Mozilla's lists or perhaps in google groups.  People should suggest ideas.  Stuff should be coded up.  Patches submitted.
But this won't work for secure browsing.  In practice there is little or no chance that a browser vendor will accept any patch that unwinds the PKI model [2].
The upshot of this is that we frequently get the browser vendor representatives arguing against proposals by well meaning people on 100 or so bases [3] but there isn't really a dialogue per se because even if the argument is won, the vendor is not really discussing and isn't going to do anything.  In effect what happens is that representatives of vendors are just defending their personal angst at criticism.
The only way to solve this problem is to break the cartel.
One way is to cause one of the vendors to break ranks and do something.   You'll note that all the changes currently being aired are sourced from google -- very interesting!  For the first time in browser history since Netscape threw the SSL model over the IETF wall in about 1994, a browser vendor is "doing something" in user security [4]
Another way to break the cartel might be to fork?  But to do that we'd need a lot of us to start working on it.  Maybe not such a bad idea? I'd guess to do that we would need about 1-4 continuous developers on it to make it work?  If we could get about $200k per annum (kickstarter) for say 4 devs it might be a worthwhile project?
A third way is to just call it what it is;  a cartel that is promotes the PKI industry at the expense of user security, and hope that participants agree to stop being a cartel.  But I've been saying that of CABForum for years and they happily disagree with me [5].
[1] Variously this might be PKIX or CABForum or whoever but you can reliably predict that such a labelled group won't then pick it up.
[2]  IE of course won't, as they traditionally accepted the PKI model *at a legal level* and the business direction isn't strong enough to overturn that let alone understand what I just said.  Firefox traditionally won't accept patches from outside the PKI group as the developers are mostly paid for by PKI interests so their jobs are on the line, in effect.  Chrome, I do not know, it might be interesting for someone to submit a patch that unwound the self-signed approach there.
[3] popular techniques are to drown the discussion in cites & research, suggest participation, throwing random technical criticisms in such as "X needs Y" or "Z won't scale", suggest joining this group or that WG, etc etc.
[4] From a business perspective it's actually a fascinating question as to why google has broken ranks.  But that's out of scope for this list.
[5]   Allegedly, they even begin their meetings declaring they are not a cartel :D which I just have to say is best evidence yet if one has *any* knowledge of the signalling aspects of cartels.

@_date: 2014-12-21 17:53:38
@_author: ianG 
@_subject: [Cryptography] [cryptography] OneRNG kickstarter project 
And, boom.  OneRNG just blasted through its $10k ask.  This project races ahead.  I'd like to think that the depth of support indicates we really do have a need for vibrant cheap open RNGs.  The more the merrier.
Paul tells me over-funding will be used to do a bigger run.  So we can pretty reliably predict that these things will happen sometime after Jan when it closes.
Probably still a good idea to support the project because you get sent a unit anyway, and more funds will almost certainly lead to other benefits.

@_date: 2014-12-22 14:54:45
@_author: ianG 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
Hmmm... you make it sound as though this stuff is going on all the time all around us.
Open question.  To what extent should we treat this as a realistic threat?  How prevalent is this?  Is there any way we can draw boundaries around this?
(Note I'm not asking for cites to prove ;-)
OK, I see that.
When I was young enough to be a uni student there was a lot of research into hardware reliability and the notion of having alternate hardware implementations vote on results.  I though it all a bit of a woftam, but I wonder to what extent this research was encouraged by the knowledge that these sorts of attacks could be practical threats?
Nod, to both.  Thanks!

@_date: 2014-12-24 22:35:30
@_author: ianG 
@_subject: [Cryptography] Certificates and PKI 
Yes, this is what we call PKI's race to the bottom.  The way that the browsers have structured the UI means that all CAs are the same, all certs are the same, and there is no incentive to quality.
EV was a brief attempt to create a step up in marketing terms but it failed to step up more than cost wise because EV also was subject to the same economic flaw.  Although some browsers naturally understood what it would take to make EV work (surprise:  Microsoft [0]) the other browsers declined and the result was again essentially an economics or commodity one:  the race to the bottom.
The end result of this is that we live in a world where DV is approximately the only solution available to us, which also applies by nature to the DNSSEC situation.  Given that DNSSEC has even less likelihood of ever seeing a UI for the users, there is little hope of doing anything else.
Markets are complex and users might be lemmings.  But economic rules do eventually win out -- users won't pay for quality with a commodity product.
[0] I speak of the decision to put or not put the CA on the URL bar as the actor that made the statement as to whether your current site is authenticated.  See the second image for early ruminations by Microsoft:

@_date: 2014-12-29 17:24:34
@_author: ianG 
@_subject: [Cryptography] Certificates and PKI 
It's in the definition of commodity?
Right -- the marketing battle is to reverse the commoditisation.  In order to gain the attention of the consumer, you have to put something in front of their eyeballs.
Pencils:   colour?  brand?  feel?  sharpness?
cables:    shielding
food:      colour, shape, absence, warmth...
DRAM:      speed, reliability
organic:   taste, welfare, fair trade
Monster:   exotic theories about electron movement, bulk, fatness
All of these are things that (purport to) say the above products are not The browser vendors control the certificate display such that they remove or reduce the ability of the CA to de-commoditise their product.   As the CA cannot present any information to the consumer, the CA is commoditised.  No "pull".  Verisign is the same as ... name any other brand, and thus there is no brand.
Therefore the race to the bottom.  Inevitable, by the laws of economics.   In the race to the bottom, contracts must be imposed that dump all liability, all responsibility on parties other than CAs [0], and other actions, so that the marginal cost of each certificate is zero.  As cost goes, so does security.
The race to the bottom can only be unwound by browser vendors making a decision that draws on economics/marketing theory and practice to show the CA's name and brand (logo) on the URL bar or similar.  Until that happens and the CA gets a reason to perform, secure browsing is a bottom (Typically, browser vendors can't see past the "advertising" aspects of CA brands.  "What's in it for me?" or "advertising is evil.")
[0] Originally CAs dumped liability on vendors and users.  See the oft-cited Baseline Requirements for a realignment of liability such that the CA liability dumping on vendors was eliminated.  This results in both CA and vendor the dumping all liability on users.

@_date: 2014-12-29 17:27:28
@_author: ianG 
@_subject: [Cryptography] Certificates and PKI 
That is part of the point of opportunistic encryption:  force the attacker to go active.  Now that we see ISPs are stripping the STARTTLS flag, we can respond.  Now we know what the enemy wants, now we know how far he is willing to go to get it.
Without that, the attacker gets it all for free.

@_date: 2014-12-29 18:07:32
@_author: ianG 
@_subject: [Cryptography] Of web form passwords etc 
Firefox is probably better than you at creating, remembering, filling than you can ever be [0].  In a fair test there would be no comparison.
The reason that Firefox is not used more in this area is probably slowness in developing agent software to manage this whole arrangement, also affected by conservative security thinking that prefers to replace weaknesses in the agent security model with weaknesses in the user's security model.
[0]   Indeed, Firefox should probably be using PKs to log in, which are much better in security terms, but the design wasn't really aligned to what we usefully want so client-cert tech is a bit fallow.

@_date: 2014-02-01 11:04:13
@_author: ianG 
@_subject: [Cryptography] Hard Truths about the Hard Business of finding 
This is to put the lie to the NSA's other role of helping to secure
industry, and their mandate to secure government.  (And all the other
agencies as well.)
    Question:  when have they ever shared a compromise?  They purchase
them by the pallet.
    Question:  when google sought out the help of the NSA about 2-3
years back because of the gmail hacks in the middle east (from memory)
how did the help go?  Does a cold-hearted analysis in sunlight indicate
anything that google couldn't have figured out themselves?  What else
came out of those discussions?
    Question:  when NSA is helping the IETF in their many and friendly
ways, how many of their tips end up being shown to be better and not
worse?  The only one I know where we've proven positive results is the
DES s-boxes.  Which triumph of industry assistance is mitigated by their
cunning plan to reduce the keyspace from 64 bits to 48 bits, thwarted by
IBM's resistance at 56 bits.
    Question:  Has the NSA provided any advice to (say) Linux RNG
authors over time?  The history seems to indicate that some timely
advice might have helped, and the ROI in terms of helping servers
throughout their catchment area would be pretty good.
Indeed, if the NSA spent all that money on protecting the Android, we
would be in a better place.  They'd still be able to do their traffic
analysis, they just wouldn't be able to post as many juicy slides saying
things like "if it's on there, we can get it..."
Right now, I think the NSA has a serious problem justifying their
budget.  Not the level, the entire thing.
Show us the value, I'm not seeing it?
Meanwhile, their own records suggest they are more obsessed in
self-protection than anything else.

@_date: 2014-02-01 20:19:39
@_author: ianG 
@_subject: [Cryptography] Now it's personal -- Belgian cryptographer MITM'd by 
"Belgium is too naive" (google translated)
01/02/2014 | Mark Eeckhaut, Nikolas Vanhecke
"I did not think they'd go that far," said the victim of hacking,
Professor Jean-Jacques Quisquater, however, himself an expert in
computer security.
"For me it was a very unpleasant surprise when the police came to tell
what had happened to me," says UCL professor Quisquater.
According to the professor, the hacking should be fairly recent. "When
someone I do not know sent me a LinkedIn request, which must have been.
known, but I'm still not been attentive enough. "
Hundreds of others
Quisquater does not believe that he is the only individual who hacked
the NSA is.
"We should have no illusions. I may be an expert in cryptography. But
when I got hacked, then there are hundreds of other individuals who are
too. There have also been others in Belgium. The Snowden files have only
uncovered the tip of the iceberg in this area. "
"The NSA would cost what it would get all the information she thinks it
can help her to break into secure communications. That is already in the
fifties its mission. And while she clearly is not limited to the fight
against terrorism. I have the impression that the NSA meanwhile grabs
everything they can get. "
According Quisquater our country is still far too naive. "I always say
there is the earth, the sea, air, space and cyberspace is. But Belgium
has that last almost nothing. "
Ally not an ally
"We are mainly a naive target. Even in our country, European
institutions, international organizations and companies. Nevertheless,
we think that we are not at risk. But the world has changed: even an
ally such as the United Kingdom, does not treat us as an ally. We need
the signals of recent months will urgently take seriously. "
(probably needs chrome, firefox barfs on the translation:)
Using a fake invitation to the social networking site LinkedIn, they
infect the system of professor.
The LinkedIn invitation in the mailbox of Professor Quisquater came from
a certain LB, an existing member of the European Patent Office.
Blank web
When the prof on the icon in the mail clicked that was to bring him to
LinkedIn he came to a blank webpage correctly. With the naked eye, the
visitor will see a site that leads nowhere, and that you do not
unwittingly click away. What the professor did. But under the hood of
the page was a computer system that the professor injected with spyware.
(no confirmation as yet whether this is a downgrade attack on SSL or a
false cert replacement.  Either would be interesting...)

@_date: 2014-02-02 12:55:09
@_author: ianG 
@_subject: [Cryptography] cheap sources of entropy 
I agree in principle.  But what strikes me is as odd is that this is a
similar argument to that used to justify side-channel timing analysis of
server keys.
I cannot fathom how we can see through the complexity of modern server
software and internet connections to analyse public key
signing/encryption to extract out the private key.
Yet it's been done.  Maybe this is just one of those things where we can
extract enough information if we do it in the lab, and control the
environment closely (set up the HTTPD to be the only service running,
nothing else allowed on the network, one hop only...).
I think, in a busy world, there are more important things to think
about.  We have to be careful to optimise our limited time to deal with
threats that are real, and not ones that make our geekly spines tingle.

@_date: 2014-02-02 13:38:59
@_author: ianG 
@_subject: [Cryptography] Now it's personal -- Belgian cryptographer 
Because the Belgians are saying it is, and because the Snowden
revelations pointed at a persistent attack of the indicated parties.
We can play the game of "you don't know that for a fact" forever, but at
the end of the day, they will never enter court and let the court
declare it a fact, so that easy excuse is their game, their rules, their
Old military truism:  the battle is won by the general that imposes his
plan over the other.
I've got no doubt that others are attempting to hack into the telcos and
I think there are several considerations here.
1.  We need models of all players.  We need statistics and likelihoods.
2.  We have an attitude that keeps tripping us up on polite diversions
such as "you don't know that for a fact" or "these are the nice guys,
they wouldn't do it to you" or "the other guys are doing this, give us
We need some way of avoiding our own biases, and that starts from
knowing ourselves.
3.  We need a model that describes the control that these folks have.
Is it no control?  Or is there some way to limit it?  Right now the
evidence suggests that there are no controls that haven't been trashed
by one means or another.
Facts claimed recently:  they routinely lie to congress and court.  The
secret non-court never analysed mass surveillance before Snowden.  They
collect and target citizens.  They hack allies, they spy on sovereigns,
they spy for industry.
4.  And, as a minor consideration for some citizens of some countries
that have a no arbitrary search or seizure clause, we need the facts to
see if they have self-declared themselves the subject of criminal
5.  Ditto for alliances.
6.  You can't stop the Chinese unless you've first stopped the NSA.
Ditto for 5-eyes.  Unless you have principles, you cannot decide when
and how to face up to your external threats, you cannot even
differentiate external from internal.
7.  GCHQ, ASD, the others, they more or less follow the NSA.
If you put it all together, at a first order of approximation, maybe it
is that: about stopping the NSA.
Idk.  I think the others are well-outclassed at this stage.  Back in the
cold war, the Russians did a pretty good job in humint.  They still
couldn't match the satellite & sigint assets tho.  The Chinese, now?  I
have no idea, I have yet to see any real unbiased data (by that I mean
data that has been released for the direct purpose of convincing
congress to fund cyberwar).
What's their budget this year?
I agree on one point -- the Chinese seem to be more focussed on economic
theft than trying to mass surveille the world of angry birds.  I really
would be asking for my money back if I was congress.
Well.  PCI models against hacks and insider attacks.
Then there is mass surveillance.  The model against mass surveillance
has been known for 2 decades:  mass crypto.  Yet, we've never been able
to get that idea through to the NISTs, the IETF, the committees, the
toolmakers, etc.
It would be a mighty fine idea if NIST were to come out and start
pushing opportunistic encryption, but they do not serve the users, they
serve the toolmakers, who use cryptography as a discriminator.  You
can't have a national standard without a national industry to sell tools.
Then there is phishing.  The model against phishing -- which was used in
the belgacom attack -- has been known for 2 decades as well, it was
built into secure web browsing.  But it never worked, and the tool
makers like it that way.
Exactly -- change!  Maybe we need those people who build PCI and FIPS
and whathaveyou to start recognising that the models they built have to
actually work.  Else they should fall on their swords, because they are
incapable of changing.
Pigs might fly.  PCI like all such more likely exists to serve PCI
people.  We definitely don't want such a millstone around the social
network folks.  We want them to change, to face their threats as they

@_date: 2014-02-03 13:13:37
@_author: ianG 
@_subject: [Cryptography] Mac OS 10.7.5 Random Numbers 
Hmmm.  Apple are famously secret so we'll never hear a peep from them
about their views on it.  I think the mitigation here would be that
although the improvement would be well appreciated for security, and for
a sense of evolving security (which is even more appreciated), I'm not
sure I am seeing the direct danger as yet.  However:
Does anyone doubt this?  Seriously?
If so, here's a simple argument.  Attackers have a 1 month OODA loop,
which means they can spin their attack within a month to take benefit of
new attacks.
FIPS has an OODA loop of a decade.  It can't respond.  Its role is to
build the Maginot Line, over and over again.
I suppose we could find evidence for these claims, but who would pay us
for the effort?
The concept of the barrier to entry is well known amongst economic
warriors.  For those seeking cred, it is part of Porter's 5 forces, a
well respected industrial framework.  There is no doubt in my mind that
the NSA is all over this concept.  And indeed we had evidence in that
old FP article of wayback concerning their use of econ weapons against
the South African crypto industry (anyone know where that is?).
The same barrier to entry was used in PKI.  For a long time, browsers
thought an audit was a good idea, not realising that it was a high cost
and easily subvertible.  Then, in the late 2000s, phishing threatened
the browsers, so the vendors banded together in secret to try and find
some way forward.  Smart CAs headed them off at the pass, and absorbed
them into a new association that in the fullness of time multiplied the
barriers by 3-fold or so.  They thus managed to avoid the question of
phishing completely, and to bed the piles of the skyscraper PKI another
100m into the bedrock.
I'm not sure what the current situation is, but last I looked, they
required the old audit, the new base audit and the new EV audit (which
actually required 2 audits), all of it negotiated in secret, with a faux
public presentation afterwards.  NIST is a powerful force there, the
association members don't respond to much in the way of input, but when
NIST decided to get them all to shift up in key size, out of mandate,
they spent way too much focus time on it.  It's basically a big boys
club and the biggest boy in town has more of the power.
It has been claimed by people who would know that the NSA was behind it
all, but I've never seen any evidence of that.  Yet, again, one would
never know;  the association started out in secret, negotiated in secret
and only in the last couple of years added a couple of open enhancements
as figleafs.  Compared to them, IETF working groups are a paragon of
virtue, you can actually see their maillists, join and and get your
disgust on the public record against the 'rough consensus'.
Browser PKI is certainly as close to a setup for NSA control as one
could imagine.

@_date: 2014-02-03 14:10:03
@_author: ianG 
@_subject: [Cryptography] Now it's personal -- Belgian cryptographer 
Don't give up, Jerry!  The first thing is to not panic.  When our
systems fail around us, panic will surely develop the wrong reaction.
When our standards fail us, we need new systems by which to measure what
we see.  Everyone in the scientific world is comfortable with scientific
method.  But sadly, that doesn't consider attackers.  Most people are
also aware of the legal standards of truth and innocence.  Details
aside, the attacker blew them away.
What is left might be styled an adverse witness model or a
counter-intelligence model.  Verify before trust would be a kinder way
to put it.
Information has to be evaluated differently.  Stuff that they want to
hide -- snowden -- is accorded more credibility.  Same with the stuff
that they attack in public, attempt to undermine.  Claims made should be
seen not from the lens of truth-until-proven-lies, but interested,
agenda-laden and manipulative.  Turn the lie.  Reverse the deception
plan.  Follow the money.  Follow the budget.  Follow the jobs.
No, that's their answer.  It does nothing to them, as they don't care
what you think of them, they only care that you slumber unopposed.  By
imposing your standards on you, and reminding you of that, they force
their game on you.
Our problem is to get them back across the line, without as you say,
adopting their lack of standards.  Us giving them the benefit of the
doubt doesn't cut it, they already factored that into their strategy,
and demand it.  Every PR, every senate hearing, every response says this
loud and clear:  you must give us the benefit of the doubt, the legal
presumption of innocence.
No more.
Indeed, these are the standards *we* hold.
And when it is gone?  When we fight those who don't have them?
Recall the old socialist's catchphrase, better red than dead.
Really?  Red XOR dead?
The urge is there to stop with the excuses, sure.  The anger is
everywhere.  /Dangerous and strident calls/ for revolution would be
pathetic and stupid.
But we need something.  We need a response.
Indeed, the abyss would be before us, if society's failure to hold the
NSA to standards resulted in us accepting the wholesale abandonment of
those same standards.
But nobody here is going to do that.  There is a middle ground -- which
is to do what we do and do it more and better:  build secure crypto
systems to protect people from all threats.
Not much of an abyss, more like a crack in a pavement or a line in the sand.
Granted, if we gaze long enough, everything becomes a nation-state
threat.  We've all met those guys.  We could probably gaze for a fair
bit longer before it even notices us, and arguably, that focus is better
than the current wishywashy PCI/DSS/PKI/FIPS Maginot Line, because it
includes the economic attacker (aka thief) as well as the uneconomic
attacker (aka *police).

@_date: 2014-02-03 20:30:31
@_author: ianG 
@_subject: [Cryptography] Mac OS 10.7.5 Random Numbers 
(Yeah, that was my thought.)
Why can't validation say RSA2048 or longer, SHA1 or longer (SHAn) ?
PRNG Has 160 bit state, or more, all else held constant?
Cryptography often relies on proofs from some related property, such as
the factoring difficulty problem.  It would seem something could be done
along similr lines.
Just musing...

@_date: 2014-02-04 08:23:57
@_author: ianG 
@_subject: [Cryptography] Mac OS 10.7.5 Random Numbers 
Well, Jerry, if you wish to torture yourself, please don't let me stop
you :)
Yes, for it to work in this context it has to be simply stated
somewhere, in the same sense that a FIPS verification is ... a
statement.  (The concept of reliance on statements is well understood in
the audit world.)
Yes, you can do all that.  Someone simply needs to state what
substitutions are allowed, and you can rely on it.
In detail the process would be something like this:
NIST (or whoever) publishes a table that states:
     SHA1 < SHA2 <= Salsa20/HashVariant < SHA3/1234
     SHA2 == Chinese variant X not Y
Etc.  Now, you dial into the table and can 'rely' on what they say.  To
some extent, NSA does this with Suite B.  OpenSSL could do it with their
ciphersuites, but that would ruin everyone's fun.
 does it from an academic pov.
Of course, the downside is that this encourages soft cryptographic
numerology, which is a bad thing, and not hard cryptoplumbing.  But with
verification processes, you get what you get.
Speaking of Keccak, did the controversy over the change in emphasis over
the various pre-image resistances ever get resolved?  The sort of change
that had been mooted would have ramifications for this table idea.
Verifications are always broad-brush, what we care about today, etc, and
it is a mistake to place them on a pedestal.  Did all those FIPS things
check for constant time?  Clear explanation of source of defaults?
Right, the problem space is immense, and any thought of a solution tends
to overwhelm.
Seriously, someone at NIST should contract some local econ post-grads to
do some research on the economic effects of the approvals, as against
say baselines of open source or "Apple update" or Microsoft Tuesday.
Who's delivering more value?

@_date: 2014-02-04 10:03:21
@_author: ianG 
@_subject: [Cryptography] Random numbers only once 
There is a historical perspective here, where schools of thought have
Back in the 1990s we all believed in entropy as the important target.
Recall the old PGP instructions to bash on the keyboard...  It was
thought that this was 'the way' to create sufficient unpredictability
for keys.
But this proved too hard.  Collecting lots of entropy is a target that
only gets reached in a smaller number of circumstances.  VMs, phones,
servers, embedded, users, GUIs, etc, have trouble.
Then, some bright spark (not sure who) pointed out that as we are in
security, we only need to provide RNs that are unpredictable *to the
attacker*.  Not to us.
This is a strictly weaker target than entropy.  It takes a while to wrap
ones head around this shifting of the goal posts.
So, given this strictly weaker target, how to do?  Well, it turns out
this is relatively easy:  use a stream cipher, and see it with a small
amount of entropy.
Now the task has become:  collect a /small/ amount of entropy, say 256
bits, and seed a /big/ cipher stream.  (DJB would say salsa/chacha and I
Which has rather dramatic ramifications on the API.  Now back to Linux.
 It views the old school, and says "if you want entropy, use /dev/random
and if not, use /dev/urandom."  The problem here is that because it is
promising an entropy target, it needs to cope with blocking, etc.
In contrast FreeBSD shifted across a decade or so ago and tied them both
together as a PRNG.  You can't get entropy any more, you can only get
the output of the PRNG.  Which is sufficient, and it works far better.
As nobody has been able to explain why we need real entropy in general
[0], this is actually the better choice.
Philosophical Question for Linux is then, why are they still bothering
with the old school entropy thing?  It's too hard.
I say more here:
I think the daemon does that, right?
[0] Until that is RDRAND came along, and the PRNG suddenly wasn't
suitable :)

@_date: 2014-02-05 09:48:49
@_author: ianG 
@_subject: [Cryptography] Random numbers only once 
It was a hard lesson to learn I think.  I recall being quite angry when
FreeBSD tied them together, for years even.
Now that hindsight is possible, one can look at the results.  Did
FreeBSD ever find an application that had a genuine need for entropy
rather than unguessable numbers?
Um, this goes back to the philosophy of Unix system calls, and the
notion that adding system calls for approximately similar things is a
bad idea.  I think calling it a special device / file is actually quite
perfect in that sense, because all one ever does is read and write to it.

@_date: 2014-02-06 10:43:33
@_author: ianG 
@_subject: [Cryptography] who cares about actual randomness? 
John, you seem to have gotten in your head that I somehow am preying to
the false god of no entropy.  Not sure why.  The thing I wrote clearly
states that we need Entropy Collectors.
In contrast, what that essay does not do is prey to the false single god
of Entropy.  Which is the (a?) criticism of the Linux thought process;
Entropy isn't the Answer, the whole Answer nothing but the Answer.
Which we all happened to believe, in times gone past.
The Answer, if one can summarise, is redundant sources of tiny Entropy,
some very careful software engineering to mix it, and a good stream
cipher to turn a little drop of entropy into a firehose.
Entropy, redundancy, software engineering and a cipher.
ps; the goal of pedagogy is to impress good thoughts on the learner, not
to bore them with 'the truth'.  This sometimes (always) requires being

@_date: 2014-02-06 10:49:43
@_author: ianG 
@_subject: [Cryptography] who cares about actual randomness? 
The auditors are easily convinced.  They will take academic input, best
practices, peer pressure as much as the public.  They'll also take the
money, and in better spirit than any mere employee.  No lack of
precedent in this, have a look at recent PKI efforts or the ongoing
repeat saga of the 2007 financial crisis.
The public are convinced by PR and the insider / beneficiaries, as is
always.  Most people in most democracies believe democracy to be fair,
in some sense or other, but only in a few places do they actually check.
 C.f., Switzerland.
For an interesting review of how to corrupt the dropping ball technique,
there is a novel called The Winner by Baldacci.  The attack thesis is
based on a real case, if memory serves.  Anything can be corrupted, but
the more subtle thesis is that the lotteries are already corrupted, just
not by whom one expects.

@_date: 2014-02-14 02:36:34
@_author: ianG 
@_subject: [Cryptography] BitCoin bug reported 
It's partly credible, although I say it from a knowledge of payment
protocols, not Bitcoin.
That's the market place.  We saw this with the first wave, based on
Digicash.  As a personal anecdote, we were doing psuedonymous payments
with a confidential accounting;  but we were consistently hammered
because this was alleged to be a privacy disaster.
In comparison Bitcoin is a psuedonymous payment system with a public
accounting.  Strictly worse from a privacy perspective.
The real thing that is going on (both times) is that the 99% fanbase
cannot actually analyse anything at the level of people in say this
mailgroup, no matter how you try.  They fixate on some token.  In the
earlier wave, it was the blinding formula that offered untraceability
(which wasn't, but that's a sort of insider secret).  In this latter
wave it is the blockchain to mediate without centralised control (which
again has some limitations, but sigh.).
The vast 99% of the people there have no clue except "blockchain".  Of
the remaining, 99% of them haven't the experience of the alternates to
be able to debate or analyse the edge cases.  For the most part, most of
the people involved are new to the payment systems biz.
It's sad.  I see sooooo many repeated mistakes, I really don't care anymore.
Ah. That's a different thing.  We saw the same thing with e-gold.  Where
money is involved, so come the scammers.  The arbitrageurs.  The fast
business types.  The used car salesmen.  The believers.  The
quick-buckers.  The missionaries.
The victims.
The problem is more one of, how do you work your way through these to
get to the sort of people we are more used to dealing with.  And a sort
of mental dissonance we have to deal with, which is, just because
bitcoin is based on crypto, this isn't a crypto community.
Yes, it doesn't scale.  Oh well.
That depends... on your definition.
Yeah, it has built in bubble mechanics.  I would see it more as an
amplification of changes in supply and demand.  With some upper limits
that will hit at times unexpected.
The thing is, crypto doesn't really solve everything.  Once you get
people involved, things change.  The events of the last week are really
about people, not crypto.

@_date: 2014-02-14 10:32:19
@_author: ianG 
@_subject: [Cryptography] RAM memories as one source of entropy 
Indeed, will tomorrow's chips even give us a view of the hardware?  Or
will they all be virtual CPUs arranged through an on-chip hypervisor and
given idealised access to perfect memory?
Disks are already there.  Memory is already swapping stuff inside
itself.  Is the CPU the next frontier?
ps; quick, someone, put a patent in for a VM on a chip

@_date: 2014-02-14 11:57:14
@_author: ianG 
@_subject: [Cryptography] Another Bitcoin issue (maybe) 
It is a problem, if you don't rely on it.  If you need to rely on it,
as per Mt.Gox, it becomes a problem.  An exchange merchant friend of
mine was telling me that his database to manage the blockchain is now
at 40Gb.  As a merchant, he's subject to attack .. so he's keen to
manage the entire process.
Fees will rise, certainly.  On today, the cost of bitcoin transactions
is running at order of $33 over average $1000 transactions, of which
about 0.34% is made from fees.
Last week it was over $60.  So, what is going on is that is there is a
sort of inflation tax transfer from existing owners to miners to pay
for the infrastructure (those 25btc are issued at expense of everyone
who holds any btc, pro rata).
As this disappears, there is a need to change the fee structure (fees
go up or costs go down).  However, as the daily economics are driven
more by exogenous factors (shutdowns, scares, incoming bubble blowers,
new countries) I think it is impractical to predict in advance how
this will pan out, only to predict the forces and possibilities.
An alternate speculative hypothesize is that as the miners lose their
reward income, they have to use their huge purpose built hardware rigs
to make money other ways.  As the only thing it can do is hash
collisions, they might turn to market timing attacks.
Of course, this is just idle speculation at the moment.
Yup, the 51% attack is something that bothers a lot of people.  A
perfectly smooth and competitive market typically shows no one player
getting economies of scale beyond say 15% of the overall, whereas most
markets have leading players up above 25% and often enough above 50%.
The leading miner announced that he would never do that, so that's
good.  I've also got a nice bridge for sale, send me a BTC advance,
and I'll send you photos.

@_date: 2014-02-15 09:40:26
@_author: ianG 
@_subject: [Cryptography] BitCoin bug reported 
It is indeed fascinating that we lived the last several millenia
championing financial privacy, and now we're laying it all out there
for everyone to see.
I can only presume that this either a nod to the cryptographic
elegance of the blockchain, or an bow to the vampire squid:
Old Chinese curse:  Be careful what you wish for!
Most notably, Lawsky indicated that the NYDFS now feels as though
existing money transmission regulation will not be sufficient for
virtual currency firms.
Further, he suggested the NYDFS could move to mandate that all virtual
currencies maintain a public block chain due to its potential to help
track criminal wrongdoing, and that bitcoin businesses that qualify as
money transmitters could be made to comply with certain net worth and
permissible investment requirements.

@_date: 2014-02-15 10:09:07
@_author: ianG 
@_subject: [Cryptography] Another Bitcoin issue (maybe) 
This is a wider problem than just this ML.  What has become apparent
over time is that crypto without application is worthless
pontification.  Yes, crypto libraries, I'm looking at you.
Without a top-down driving need, there is no grounding what we do.
You have to have something to secure.  So some sense of discussing
apps is needed.
Then, Bitcoin.  It is the bleeding edge of crypto work today.  It's
growing fast, it uses a lot of crypto, is is about money, which is the
 app for crypto, and it uses and loses a lot of it.
Without the tight feedback of attacks, crypto doesn't advance.  Even
when there is fraud, the fraud needs to be direct in your face to get
the attention of the developers.  This is the difference between
bitcoin and SSL.  Theft in the one matters, the other is subject to
all sorts of personal vagueries.

@_date: 2014-02-16 01:24:11
@_author: ianG 
@_subject: [Cryptography] BitCoin bug reported 
Hmmm good question.  Let me muse a bit.
Not in that form.  It is hard to write about the business aspects when
most people just want to talk about the elegance of the crypto.  Problem
is, people have paid 95% of attention to the tech whereas 5% would be a
better fit.  It'd be a pretty boring book, if it ever gets written.
I wrote this:  to lift people's attention
away from the crypto and give a framework.  As well as that, it is
useful to read Neal Stephenson's Cryptonomicon because it displays a
broad-based business view, interleaved with dollaps of tech to keep it
Also the Paypal book, although I've not read it.  A history of e-gold
would be the most valuable thing for Bitcoiners because they are on the
same journey.  And in a non-payments sense you can't go past the
Facebook movie for a view on the startup world, it is compulsory viewing
in my team.  There are also manifestos and writings from the cypherpunk
tradition, they are out of date in security terms but they are probably
good in threat terms.
Some of the oldtimers were talking about writing a history of FC up
until Bitcoin, because there are so many influences that seemed to get
less recognition.  But the choice is tough:  write a book or write more

@_date: 2014-02-16 11:24:34
@_author: ianG 
@_subject: [Cryptography] BitCoin bug reported 
This is where words matter.  It has never been possible for more than
one transaction to exist, covering the same inputs.  The definition of
transaction is quite well established in computer science, it is that
which is, that which results.
It is not that which comes before.
And there, Bitcoin terminology departs from computer theory, and all are
confused.  It is not possible to have 'multiple forms of the same
transaction' as this is a contradiction of the term.
Rather, there are multiple instructions to commit a form of payment, but
only one of them can result in a transaction.
Now, this may seem to be just pedantry about wordage, but the issue is
that every user, caller, business, accountant, auditor, competitor,
regulator, and indeed person understands that a transaction is the ONE.
 There is the transaction, full stop.
It is Bitcoin that has misued the term, and in this, Mt.Gox have been a
victim.  OK, they should have recognised that the term was bungled, and,
they should have upgraded their software.  But they are merchants, they
are not programmers.
OK, that's adding more meat to the story!
Hmm, ok.
So, next question:  what's the OODA loop length of these changes?  Here
we have serious, live changes flowing through, and people are suffering.
My question is directed at this meta-question:  What's the best method
of cooperative systems development?
We've got IETF's SSL group, with an OODA loop of about a decade, minimum
3.5 years when they care.  Above, I'll speculate order of a year for
Bitcoin's dev team / userbase.  Then there's QUIC which suggests order 1
What's the best way to improve?

@_date: 2014-02-17 01:30:15
@_author: ianG 
@_subject: [Cryptography] Transactions, was BitCoin bug reported 
We can send a check (with or without a "k") to the bank.  Or many... we
can bounce them around until we fill a castle.  Sometimes called a
SUBSTITUTE CHECK.
Yes, it amounts to one transaction -- 1 or 0.  Many checks, one transaction.
As it happens, the "form" of a check in english banking tradition, as
inherited by the rest of the anglo world, is not set.  The explanatory
metaphor is that you can write a check on the side of a cow, drag it
into the bank and deposit it.
Which is to say, encoding can be quite versatile, before the transaction.
Nobody on the inside of the banking world mistakes a slip of paper or an
image or a cow for a transaction.
Certainly, outside the banking world, people simplify things.  Lingo is
imprecise, life too rushed.  "Check's in the mail!"  That's not
important, as long as everyone inside understands what is going on.
And, that includes merchants.  As long as merchants understand what it
takes to clear a check, all is golden.
Ask a merchant, those that hold risk on their understanding of what it
means to get a transaction;  they will tell you they take the check to
the bank, and pay for fast clearance.  I.e., to settle it, into a

@_date: 2014-02-17 01:41:48
@_author: ianG 
@_subject: [Cryptography] BitCoin bug reported 
As I mentioned.  In my experience, there are two sorts of payments
system.  There are those that understand what a transaction is, and
those that lose money.  Call it pedantry if you like, but it's pretty
clear in history of payment systems that if there isn't a good solid
understanding of transactional integrity, and/or that understanding
isn't pushed through aggressively to all players, then money is lost.
Which is where we are.  Mt.Gox lost money.  I don't like being called a
pedant, but I hate losing money 1000 times more, and if that is what it
takes, slap it on.
Encodings are at the bits & bytes layer, transactions are at a much
higher layer.  They both exist as tools for the programmer, and they
both apply to this problem.  Just because they've spotted the flaw at
the bits&bytes means little for other layers.
The fact that there was looseness in the definition of the encodings is
an issue that is solved ultimately by the blockchain -- that transaction
which is in the blockchain has the encoding that is correct, by
definition.  We are agreed that the transactions in the confirmed
blockchain are transactions, right?  ACID and all that?
Then, any other encoding, whether defined, canonical, displayed or
etched in stone are simply for convenience, they aren't the encoding
that is in the blockchain.  And any pre-transaction identifier is simply
a prediction as to what to look for in the next blockchain.  It helps,
sure.  Just don't rely on it.
Right, so as an assist to the applications, Bitcoin attempted to come up
with an identifier that was consistent before and after the transaction.
 A noble goal.
I fully agree that context matters.
I agree.  Above, you clearly established the context of the term within
the field of accountants.  I would not complain, especially as with
double entry systems, the "accounting transactions" are typically
However, Bitcoin is not accounting, it is a payment system, and by the
nature of money, it is transactional according to the CS/database sense.
 Any other interpretation is a state of sin.  If you do not subscribe to
this, I have a transaction I wish to share with you.
Indeed.  In encodings, there be gremlins.
But lack of transactional integrity is a far greater sin.

@_date: 2014-02-18 09:34:33
@_author: ianG 
@_subject: [Cryptography] The ultimate random source 
At Ars Technica a few years back, some artist presented a random number
generator made of candles and fans.  He had maybe 30 candles, and above
each was a homemade fan that spun from the heat generated by its candle.
 As the fan spun, the candle blinked on and off.  Each candle and fan
combination had a different frequency...
It was very pretty!  If photographed and hashed I imagine it would work
fune, but I didn't have the heart to tell the accompanying art critics
about the predictability of counting the frequencies.

@_date: 2014-02-18 15:54:01
@_author: ianG 
@_subject: [Cryptography] Encodings for crypto 
I think we can do a lot lot better.  I have a document somewhere on
this, but in brief:
There are too many primitives.  I see 11 doing numbers alone!  In
practice, in network protocols, we do not need bignums, we do not need
floats and we do not need negatives.  Then, for different sized numbers,
we should remember that we are about simplification and higher level
concepts.  Which should tell us we need a number.  Not the four
remaining of 8, 16, 32, 64 bits which are hangovers from the hardware
days.  We need one number that simply expands to fill the needs.  This
is done with the 7 bit trick, where the high bit being set says there is
a following extra byte.
Next, we should really be thinking in OO terms.  When we are dealing in
OO, we have a single object that 'knows' its output, and its input
intimately.  Which is to say, it knows whereas your spec does not.  The
object can do semantics such as range checking and small composition
such as conversion of byte arrays to strings.
We do however need a sequence of bytes, and a byte array is constructed
simply with a length (number as above) and a sequence of bytes.  That's
2 primitives so far.
Once we get thinking in OO terms, we can then create anything that is
desired within the context of the class.  Need a version?  Add a number.
 Need a boolean?  Add a flags number.  Need a negative?  Add another flag.
Next.  When we get more complicated, we can simply encapsulate the
complications into a new class/object.  This would be idea for floats
for example.  Which leads us to the second observation:  composition of
objects is a far more natural way to build up protocol elements.
One more thing.  Without a seriously good testing and debugging system,
this whole area breaks down with complexity, which is why people eschew
binary and go for text.  There is a loopback technique I use to solve
this issue, which I call the Ouroboros pattern.  In short it is this:
1.  each object has an example() method which produces a correct object
with with each field constructed randomly.
2.  write that out through the stream process.
3.  read it back in to a new object, using the converse stream process.
4.  compare the two objects with standard equals() method.
Run 2^5 times for each class.  Due to composition and repeated tests
this solves the complexity issue.
It should be noted that another way to do this is to use a high level
meta language like XML or ProtoBufs.  But that drags in all the
complexity of another language / system / library / parser madness.
This technique does not, it's just simple OO programming.
Richard Farrell said:
Probably the key point here is that if you are sill thinking about
protocols along the old bits & Bytes way that Richard highlighted,
you're missing out.  Doing protocols with OO thinking is so much easier
you never ever go back.  Once you do that, all of the formats, ideas,
layouts, MLs start to look a little .. 20th century, steampunk, historical.
ps; I have a paper that describes this in more depth, bug me for a
review preview (yes, feedback :) ) or bug me to release it.

@_date: 2014-02-21 12:23:50
@_author: ianG 
@_subject: [Cryptography] Plenty random for everyone 
If the user is a person who typed the above command, then 'whatever'.
If however the user is an app on a mobile phone where battery is
important then I guess their battery will tell them that using urandom
for that purpose is less than friendly.  But again it seems like an
optimisation best left to user space.
That's a big assumption.
So, you are assuming that all this 'equality' of purpose lies within the
PID process, and firewall should be erected between the PID/processes.
I'd say that was just another assumption taken from the pov of
convenience.  E.g., we can do that, regardless of whether it is helpful.
Two counterpoints: Some apps are spread across multiple processes.  Some
apps have high quality needs and low quality needs.  E.g., the wiping of
data /versus/ construction of high quality keys.
Now they all can, together or individually :)
I don't know how you derive that conclusion...
True in principle, but I personally would rather concentrate on finding
the primary weakness and fixing it.  For all processes, assuming they
are all equally important.  OK, I see the point of this if userland is
used as a sort of sandbox as it is in Android.
This is a point that has been highlighted by FreeBSD's approach.  What I
would like to see is some empirical observations as to whether this
approach has worked better/worse over the long term.  They've been doing
it for about a decade (memory?) so we should have some sense of the
benefit here.
Right, but this is also gained by just turning random to be symlink to
Random numbers are inherently backward compatible :)
Right, why?  Q for FreeBSD.  What was that insistence really about?  I'm
genuinely curious as to where we need real random numbers as opposed to
Well, it is a good time to explore all this.  I would say brave effort,
but you have failed to make your case, at least for me.  This is a good
example of how hard it is to do 'better' than the platform, which
presumably has smart people working for a long time on the problem, with
lots more research and support.
Or, 'use what your platform provides' aka Rule  as epoused here:

@_date: 2014-02-24 10:09:49
@_author: ianG 
@_subject: [Cryptography] NIST asks for comment on its crypto standards 
As part of a review of its cryptographic standards development process,
the National Institute of Standards and Technology (NIST) is requesting
public comment on a new draft document that describes how the agency
develops those standards. NIST Cryptographic Standards and Guidelines
Development Process (NIST IR 7977) outlines the principles, processes
and procedures of NIST's cryptographic standards efforts.
NIST is responsible for developing standards, guidelines, tools and
metrics to protect non-national security federal information systems. To
ensure it provides high-quality, cost-effective security mechanisms,
NIST works closely with a broad stakeholder community to select, define
and promulgate its standards and guidelines.
In November 2013, NIST announced it would review its cryptographic
standards development process after concerns were raised about the
security of a cryptographic algorithm in NIST Special Publication
800-90, which was originally published in 2006 (an updated version,
800-90A, was published in 2007). Based on those concerns, that
publication was re-issued in September 2013 for a new period of public
review and is being revised to address comments received.
With the draft NIST IR 7977, NIST is seeking feedback on how it develops
its documents; engages experts in industry, academia and government; and
communicates with stakeholders. Public comments will be posted on the
NIST website and used to create a revised document. NIST will then
review its existing standards and guidelines to ensure they adhere to
the principles laid out in NIST IR 7977. "If any issues are found," said
NIST's Donna Dodson, who oversees the process, "they will be addressed
as quickly as possible."
The draft version of NIST IR 7977 and questions for reviewers can be
found in the Computer Security Resource Center at Comments may be submitted to crypto-review at nist.gov by April 18, 2014.

@_date: 2014-02-24 12:41:46
@_author: ianG 
@_subject: [Cryptography] Entropy Attacks! 
(I agree that attacking the code in some future programmed sense from
within the CPU is just too cumbersome to be considered a high priority
I would hope so too, but I think the numbers are against us.  There are
O(1m) people in USA with top-secret clearances, and O(10) whistle
blowers.  Do the division.  From my experience with large organisations,
they happily do evil things, and very few people would ever think of
being a whistleblower.
And why would you?  It ruins your life.  I've not been one, but I've
watched the results.  Unemployment, poverty, deaththreats, random arms
of the government searching for anyway to take your head even after
status has been granted.
How many people put their life before some vague moral principle, and
for what?  Because Intel has some nice special government business?
Nothing to see here, move on.
Indeed.  But we also now know that they don't do just one thing.  They do
everything.  Everything they can think of, including hacking into games
machines.  Which is decidedly annoying, we now have to defend against
And we have to get economic about it, which is sort of what they are
doing:  going after everything according to their unlimited budget.
ps; in terms of the permathread on RNGs, there is Dan's google-group if
one can join that.

@_date: 2014-03-01 01:07:10
@_author: ianG 
@_subject: [Cryptography] GOTO Considered Harmful 
It's a terrible point.  This is security code, it tests tricky things to
do with certs.  It's also intermingled with slow message digests, there
is really no excuse here for using "fast code" instead of careful code.
Yes.  That is what return is for.  Always bug out when you can, as soon
as you can.  That makes the code following that point far simpler,
because it eliminates whole paths of choice.  That is what functions are
That.  Do.  Of course.
Sure... If and when the code is changed to be structured optimally for
security, then we can have that discussion...

@_date: 2014-01-02 08:19:29
@_author: ianG 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
Yes, they are almost implementation terms.  At the core is the coordination problem, aka 2 generals.  Where they get mushed up is that the user experience is either synchronous or async (when i'm sending email or chat, I think differently), but underneath it's both or neither, it's just what we do with protocols.
Nice!  Let's capture that term and use it!  Protocol Forward Security, PFS, repeat 3 times while brushing teeth...
Indeed.  The assumption of asynch, not the reality.
This begs the question why email innovators haven't tried this more ... and reminds me that email is as dead as a dodo when it comes to future Which is why certain smart people are saying that their future email will be like email but won't be email as the techies know it.
Hmmm...  So, yes, we could do this.  In essence it is a plausible defence against node compromise, stopping backwards decryption from some point of compromise.  It doesn't stop forwards decryption, afaics.
We are presuming that the messages on the node aren't kept?  As a compromise on the node that can pick up a key can also likely pick up any stored messages.  So we are postulating that a compromise on the node is a snapshot which exfiltrates a single key in a short inexpensive It also requires the protocol to have a pretty good idea of message number, so it can calculate which hash position in the chain to use, in case messages arrive out of order.
Now, if you're going to do that -- hash the key forward and both sides then have a very good idea of each message sequence so the receiver can accurately decrypt based on position in hash chain -- could we also do something else?
Would it be plausible to simple negotiate a new ADH pair for each messsage sequence?
For some reason in my mind I can't shake the notion that this is like a block cipher in CTR mode, where the message sequence number is the counter, and if we know the full secrets, we can restart the suite at that point.
( OTR, there's another unfortunate TLA that cries out for a new phrase to celebrate its successes not its failures.  c.f., the sad tale of Pvt. Manning is now very much on the record. )
 h
ope that the sender didn't realize he was about to be captured and send an "under attack, ignore all after this" message as his last gasp!)
Right, you're assuming the sender is offline, "email batch" mode.  And can stack up the messages encrypted with a hash-chained key.
Now, if the protocol accepted say N forward hash positions, and could also reset to position P,Q ADH key, would it achieve the best of both Where N is a sequence in Alice->Bob comms, and P is Alice->Bob and Q is Right, got it.  Our threat model is one of single state node compromise, such that long term private key can be exfiltrated, either/or short term session keys.
And our job is complete if this only compromises a few time-local messages, +/-.  Which then forces the attacker to abandon the attempt as it's not cost-effective.  The attacker must then go for total node compromise, which is a ratchet upwards in cost & risk.
I think it is valuable, but it is no panacea.  If we look at both the history of private sector attacks (server breaches) and intel sector attacks (thanks Ed & Jake) we see that there is a dominance of node attacks.  Hence, without an accompanying SnapChat like approach, it is pretty much worthless because the node attack is actually a lot more likely, and the goal that seems to be expressed is total node dominance, as that provides the flexibility for the attacker.

@_date: 2014-01-02 10:10:26
@_author: ianG 
@_subject: [Cryptography] TAO, NSA crypto backdoor program 
It's a CSS catalogue, from the division that is responsible for securing all the government agencies.  I am guessing that the products are available for anyone who's got a close relationship.  Definitely CIA and military, especially special forces.  The NSA is supposed to be a non-active entity, it doesn't do the interventions, it more or less has to work in concert with others, but its incidental expertise in high tech makes it by accident the most likely partner in developing the tech.
So this catalogue probably goes to all Homeland agencies as well, think DEA, FBI, USSS, etc, although they would likely not be given the best toys except in high profile cases.
It is interesting to look at the classifications as well.  The GSM copy equipment is Secret whereas most of the other stuff is TS.  Only a few of the toys are 5-eyes, most is REL TO US only.  But I'd say that there is a lot of quiet sharing goes on with GCHQ and maybe others than is indicated by the REL classification...

@_date: 2014-01-02 20:04:35
@_author: ianG 
@_subject: [Cryptography] Dual_EC_DRBG backdoor: a proof of concept 
Tantalising!  I've no time to look (and wouldn't know an eliptic curve if it slapped me in the face).  Comments?
If you still believe Dual_EC_DRBG was not backdoored on purpose, please keep reading.
In 2007 already, Dan Shumow and Niels Ferguson from Microsoft showed that Dual_EC_DRBG algorithm could be backdoored. Twitter also uncovered recently that this algorithm was even patented in 2004 by Dan Brown (Not the Da Vinci guy, the Certicom one) as a ?key escrow mechanism? (government jargon/lingo for trapdoor/backdoor).
I will go a little bit further in explaining how it works and give a proof-of-concept code, based on OpenSSL FIPS. This is in the best of my knowledge the only public proof of concept published today. (correct me if I?m wrong).

@_date: 2014-01-03 10:39:48
@_author: ianG 
@_subject: [Cryptography] Dual_EC_DRBG backdoor: a proof of concept 
Very nice description of Blum-Blum-Shub, elided, thanks!  Just on the general question of whether we can ascribe error or malicious intent...
* Interference was written up in their goals.  In the large.  It said, we are going to take over the crypto industry.
* They know more about RNGs than we do -- it was them that pushed NIST in the direction of a deterministic output whitener/expander, the DRBG.   Until this came along, I don't think it was an understood concept, the open world was still working on the premise that we didn't want determinism.
Open question -- when did we the open guys figure it out?
* They have 70 years of practice at sabotage.  It's their job.  They have the advantage when it comes to thinking like an attacker.
* They are known to attack the RNG.  They attacked Crypto AG's RNG, as Dave pointed out.  Spooks attack RNGs, nobody else does.  So if they didn't know about how to attack the BBS design, it's more than incompetence, it's gross negligence.
* In complete contrast, we just don't get that practice.  I do not know of any real attack where someone has spiked the RNG in our open/commercial domain [0].  We think about defence.  And we typically aim at crims and academic breachers, not TLAs, until recently.  Crooks think about server hacks and social engineering.
So having something placed right in front of us ... might work.  It's be worth a try!  We had to have it pointed out...
* There are insider rumours of what happened at RSA.  This has the halmarks of an 'approach'.
* Everything they do is secret, disinformed, and deniable.  They don't lay tracks.  They don't make it easy for us.  And they fill the airwaves with excuses and pointers to other theories.  They have their shills.
* And, as you have pointed out:  They had $250m to do these attacks. Per year.  If they did this accidentally, I think you guys want your money back, and get them out into productive jobs.
For my money, unless we arrest the perps and they confess, this is as good as it gets.
[0] if memory servers me right, Netscape had a bad RNG, and Dave & Ian attacked it.  But that wasn't deliberate on Netscape's part.  The acccidentally-on-purpose breach was in their key length generation.

@_date: 2014-01-03 11:25:30
@_author: ianG 
@_subject: [Cryptography] Dual_EC_DRBG backdoor: a proof of concept 
This is a seriously good point.  Defaults are meant to be changed, and are offered as a sort of security feature.  Alternatives are offered as if this makes sense in a security context [1].
But can defaults be changed?  The barrier to this is often high, and too high to be realistic or give any security benefit.
Two questions, possibly as research topics:
      1. How often are security defaults changed?  In any given environment such as OpenSSL, etc.
      2.  How hard is it to change the defaults?  What is the mental energy, skill & time required?  How high is this barrier?
The result of defaults seems to be that they are poorly chosen [2], end up being the only choice for 99%, and open up an easy attack, DUAL_EC [3].
[1] [2] [3]

@_date: 2014-01-04 13:22:44
@_author: ianG 
@_subject: [Cryptography] defaults, black boxes, APIs, 
It's an important point.  In software engineering we want two things of our lower-layer cryptoplumbing:
1. to create a black box / tool approach so that we engineers can construct the business knowing that the tool does what we expect, and can be substituted when it breaks.
For RNGs the interface is pretty simple:
    get(byte[] buf)
    {
        // fill buf with cryptographically secure random data
    }
That's it!  Fantastically interoperable, in general.
2. a standard pattern which we can use to construct a new one, so when new particular requirements come up we can throw the task to an intern, and say "build this to this design!"  Hence my comments on what this pattern is, and what name we give it ... for us engineers, we need a codename, just like the NSA and their project names.
3.  oh, and we want a security promise of some sort that is easy to think about in software & design...
Right.  But.  Software engineering as we know it shows that it is an expensive tool that is brittle, can be logically expected to miss the target when it is needed, and does not actually solve the overall problem of upgrade, which needs to be solved anyway.
It's overshadowed completely by other realities.  Have you noticed how the entire world is moving to a much more sophisticated update model, typically dynamically, monthly?  If you can do that, you don't need algorithm agility as a static tool.
We need to rethink the problem and find a better approach.  Any time spent on designing a better algorithm agility is wasted time, but I suppose it is better to think about it than actually deploy it...

@_date: 2014-01-04 13:49:14
@_author: ianG 
@_subject: [Cryptography] Dual_EC_DRBG backdoor: a proof of concept 
Right, we need collectors of entropy, we need a mixer, and we need an output stage, variously called a whitener, expander, PRNG, DRBG.  They all need to work together.
It is the latter, in our space of cryptography (says I).  Entropy was a 1990s viewpoint, I think.
Entropy is unpredictable by anyone, so it's good for the task, theoretically.  But we don't care about us predicting it.  We care about the attacker predicting it.
We can draw a circle, and declare everyone inside is us, and everyone outside is a potential attacker.  Then we can draw a small amount of entropy from inside the circle, and put it into our expansion function.   We know by the properties of hashes and ciphers that we can make an expansion function that is unpredictable, if the seed is unpredictable:   it's the same property as breaking the hash or cipher.  Hence the proofs.
Now, we could use entropy, because it achieves the goal as well.  It dominates, which is to say, it is more better in every way, in terms of its quality.  But, entropy is very hard to collect, it's expensive.  And only comes in small doses.  Not enough to really fill the inputs of say big RSA keys, by itself, without a lot of work.  Which we really can't rely on, attempts just haven't worked.
Hence we end up with the design:
    Entropy collector  ----\
                            \ _____          _________
                             /     \        /         \
    Entropy collector  ---->( mixer )----->( expansion )-----> RNs
                             \_____/        \_________/
                            /
    Entropy collector  ----/
Which I sometimes call a trident design, someone else called it a cascade design, and yet others think of it as Yarrow or Fortuna.  But I'm not sure enough about this all to know whether they've stamped the name, or what.  They've obviously pioneered the way...
Well, at the end of the day, it is an engineering problem.  If you subscribe to the Entropy school, you do not answer the engineer's question of "what happens when it breaks?"
The above design answers it.  We have to care *a fair bit* about the mixer and the expansion function, but each component is limited in its API and demands on quality.  They are simple black boxes.
Then, say BBS.  Talking about it is fine, but it only makes sense when placed in a context.  If it is in the above context, as an expansion, then fine, it might work.  (Or as a collector, even.)
But so might AES.  And engineering has it that if we've cared some about the modules, once we get to that stage of building the structure, we can substitute in different modules and still get the same good result.
As engineers in software, the acid test is whether we can throw it at an intern and say "build it this way."  (Which I just did.  It worked. RNGs are now a solved engineering problem, as far as I can see.  Until someone turns it upside down in a paper...)

@_date: 2014-01-05 10:56:22
@_author: ianG 
@_subject: [Cryptography] defaults, black boxes, APIs, 
Oh, maybe, indeed.  I'm not arguing that this is a good thing or a bad thing, I'm suggesting that software distribution models are changing, and the old assumptions about bugs and quality and so forth aren't reliable:
Jonathon Thornburg writes in same thread:
 > But OSs which make
 > security a very high priority, like (say) OpenBSD, aren't moving
 > that way at all -- they're staying with the old "updates are
 > manually applied by a (human) system administrator" model.
 > The OpenBSD website points out that they've only had two remote
 > holes in the default install in "a heck of a long time" (I
 > think more than a decade).  So perhaps the manual-updates
 > security model remains > viable....
Right, so those who have security as an obsession write once, right once.  I spoke too quickly when I said *entire* world...
But even Linux, the close competitor, has update insanity, it's one reason I don't use it.  Apple is delivering stuff more frequently and more aggressively, both my machines are bugging me to upgrade, when I used to survive on FreeBSD with a rebuild every 2 years.
Ah, so sorry.  This is assuming the software world is driven by bugs. It isn't, it is driven by features.
Right, so we're agreed.  The point then is that if this is the state of the world we are trying to protect -- leaving aside the *BSDs who'll not need our help -- then we do have faster update cycles to help us.
So the notion of putting in extra algorithms up front so we can switch from one to the other on signs of trouble doesn't make as much sense. We can replace the whole lot in the update cycle.  We don't need to ask the sysadm to start fiddling with these strings:
SSLCipherSuite (which never worked as a security practice anyway).  (BTW, this comes from BetterCrypto.org project's draft:  )
Apply SUITE1.  We can just work on SUITE2 in the background and when the failure occurs, roll it out entirely.
OK, so I hear from here that people are shaking their heads and saying, he's crazy, loco, off his rocker.  Granted, this is a *thought experiment* .  Start from the facts we know:
   * the world is moving to frequent dynamic updating *
   * the old algorithm agility suite promiscuity idea failed *
   * we will always need an ability to upgrade bits & pieces *
What else can we do?
We are shouting vociferously in agreement, then!
Let me put my view into that perspective:  by stripping out much of the internal elements of the software (the spare suites, the defaults, the management of options, the FIPS features, etc) I'm making the overall design simpler.  Much simpler.  Write once, right once.
So, it's also more robust, lasts for more time, and locks down more stuff.
However, there is always a need for a fix or a tune or a new feature or a replacement.  So some way to replace things is needed.  What's that? I'd say that should be built in too, but it should be at a much higher layer, and it should be part of the overall major version update process.  Do it very infrequently.

@_date: 2014-01-06 09:15:13
@_author: ianG 
@_subject: [Cryptography] defaults, black boxes, APIs, 
It's very annoying.  All of those bugs labelled as Java are really applets in browsers.  So every time a scare story comes along, we have to read all about it only to discover it is really nothing to do with the language.
Browsers are kitchen sink.  Like OSs.  Do you recall the mid 1990s when the browser delivered by Netscape also had email and web design in it?
That takes security questions to a whole new level...  Considering browsers as secure cannot be done in the normal single app sense, they are really OS or platforms.  And for this reason, they can't ever be any more secure than the stuff loaded up into them.
I generally model this by saying that browsers are good for medium security stuff, not high security stuff.  With that thought in mind one can have a discussion as to whether favourite app (say, online banking) fits in which category.
+1 on the competition approach, all the above.
Why not narrow it down?  PDF however is a huge project.  Pick the one thing that we all seem to revert to in any secure code discussion:
       buffer overflows in C.
Design the mod to current C language/libraries that best addresses the Open competition.  No rules.  Big prize of open endowment for academic/research project...  (Format already known & practiced.)
Anyone got a spare mil?
Right.  Yet, this technique works surprisingly well in languages that handle overflows and have exceptions.
In OO I don't think I can imagine doing it any other way.  When I port stuff into C for example, I port all the OO concepts (including buffer overflow handling) to make it happen, and when the code is up and going, it works and debugs very quickly.
Im my experience, the vast majority of programmers do not know careful.   They are struggling with complexity -- of the language itself, of the designs they have to code, of the things thrown at them.
Unless the language solves these problems for them, they're SOL.  Or their employer is.
My own revelation in this came at your hands a few weeks ago, I had to go through and find the critical Closables in my code and ... close them.  I have no idea why I missed that.

@_date: 2014-01-06 10:10:59
@_author: ianG 
@_subject: [Cryptography] defaults, black boxes, APIs, 
Hi Kevin,
finally, some thinking aimed at the thought target :)
(Those are the typical notions.  There are others.  There is for example, authorisation schemes, digital money, chat (which is data at rest *and* in flight), shared data schemes (goggledox), etc.)
The problem with this scenario is that you are imagining something, and then developing a solution without generalising it to the reality.
With any scenario, we can always have this issue:
Let's suppose we have a single method to store data-at-rest, call it 'MethodA' and then for some reason, we find the need to deprecate it and get people to start using 'MethodB'.
Now, what in the above makes people change their viewpoints just because Methods A,B are crypto algorithms?
Let's take a real live data example from my work.  I have migrated my data at rest formats several times.  Here are the reasons:  language change, software mirroring to alternate disks, adding hash macs for integrity, discovery of a corruption bug...  Upcoming is insertion of stream ciphers, live backup to other databases, replication.
In each case I've had to write code that takes in the old formats, understands them, and then writes out the new formats.  The old formats had to stick around until all old stuff was guaranteed to have gone. Which date could be measured and planned for, with various inducements at the business level.
(in 2014 I got rid of formats invented in 1995...)
Point being, the problem is exactly the same - for crypto or for non-crypto.  But we don't imagine that we should have MethodA in parallel with MethodB, do we?  Just in case one fails?
Who runs MySQL alongside Oracle?  Yet, both can corrupt the database and refuse to deliver...
Yep, basic software engineering.
Indeed.  Or you still might not be able to drop using tape reels (remember those old circular things with the black tape wrapped around them tightly) because your backup process is locked into trucking them Hmmm... ever had to handle a transition from cpio to tar to zip?  It has probably meant that you had to keep around the older programs...
The point that seems to be missing in everyone's viewpoint is that this process has to exist regardless, for every feature / method / layout / Which is to say, the migration problem exists at the wider application level, not only at the algorithm level.  This is the same problem as migrating DOC95 to whatever today's is.  Cpio to tar.  XP to OSX.
And, here's the clanger:  the wider application level succeeds in better migrating things, to the extent it does or doesn't.  Because it is more focussed on business.
So, kick the problem up to the higher layer.  Lock the suite to that year's format.  You won't do worse than you already do, and you might do a lot better because more time can be spent on more important things.
Remember, this is a false fear.  It's never really happened in the terms&conditions that we contracted for in our nightmare.  If we do our work well, and we know how to do that, there is no reason to believe it is remotely possible.
For example, history of TLS.  It's algorithm problems are mostly or all to do with bad/old deprecated algorithms that should have been dropped a decade ago, but for some reason the committees thought nice to keep around.  Even there, the algorithm problems were swamped with complicated protocol cases.  Or, anal policy choices that interfere with business of deployment.
Their problem has never been the failure of a good algorithm.  Notice also that universally, we've found that the last round contestants in the competitions have all survived well.  Indeed this is why some competitions don't even pick a winner, they go with the best 5.
And indeed that's what happened.  TLS had *protocol* problems so everyone was advised to switch to .... RC4!
Which becomes much easier when there is only one :)  Either you support version 2013 or you don't.
Good.  So we know that the upgrade problem is the devil, right?  We know that the upgrade problem is where all the problems lie?  And we know that history-wise, TLS has carried the cross of upgrade forever, with all of the problems it has faced not being solvable without a major point upgrade.  SSL v2 -> v3 -> TLS v1.0 -> 1.1 -> 1.2.
This is a major design flaw, it's a design failure even, in engineering terms.  What TLS never anticipated was how to upgrade those major points.  They seem to think that major point upgrades are unfortunate and hopefully will go away soon.  They don't understand that they are actual real live business life.
Anyone in the long term business knows that upgrade of software is Sure, we all know there are the XP laggards.  But they get what the ask for, longevity of process with security that is no longer reliable. They also face dramatic business risks because if their reliance on old stuff is too tight, then their business can be overtaken by upstarts with new gen software.
(Right, so there is a sense that privacy and security 'authorities' which is a contradiction in terms, but never mind, will turn up and fine you for breaches.  This changes the payoffs.  It works, to the extent that the so-called authorities have a clue...)
(There's a thought... if this concept worked, we should agitate for an 'authority' to run around and fine laggards for use of RC4 and MD5, or for shipping dodgy RNGs.  Perhaps a new direction for NIST :-p )
Absolutely.  The point is that this is business.  Business is the business.
The decision is made at a business level.  Either you can handle it or not.  There is therefore no point in trying to handle it at the microcosm of cryptography algorithms because any attempt will be (a) swamped at the business level and (b) will add complexity that generates problems and never ever sees a return on investment.
The philosophy point is:  integrate into the business more.

@_date: 2014-01-07 11:49:00
@_author: ianG 
@_subject: [Cryptography] [cryptography] NSA co-chair claimed sabotage on 
I think, like James, I see the sacrificial lamb approach.  There is benefit in watching what they are up to.  If a measurable push comes out of the IAB's CFRG, then this is a clear signal to avoid that like the Pushing ECC patents.  Pushing NIST curves.  Clear signals!
Without those signals, where would we get our information? I've always thought that IPSec, DNSSec, and similar were highly suspect because the IETF was there at the start, precisely.  Unlike say SSH which was cut from whole cloth, in original form, or Skype which had to be sold to the borg, before it could be assimilated.
In the wartime OSS Simple Field Sabotage Manual, it suggests things like:
  (4) Bring up irrelevant issues as frequently as possible.
  (6) Refer back to matters decided upon at the last meeting and attempt to reopen the question of the advisability of that decision.
  (2) "Misunderstand" orders. Ask endless questions or engage in long correspondence about such orders. Quibble over them when you can.
  (7) Insist on perfect work in relatively unimportant products; send back for refinishing those which have the least flaw. Approve other defective parts whose flaws are not visible to the naked eye.
  (10) To lower morale and with it, production, be pleasant to inefficient workers; give them undeserved promotions. Discriminate against efficient workers; complain unjustly about their work.
Written from those times.  It would be fascinating to read a current version, one that had been written with the IETF and national standards orgs in mind.  Maybe someone could reverse-engineer these emails to figure it out?

@_date: 2014-01-07 12:28:04
@_author: ianG 
@_subject: [Cryptography] defaults, black boxes, APIs, 
Appealing to authority, on this question of algorithm agility, it occurs to ask whether any NSA designs use algorithmic agility?  I've not seen it, does anyone have a perspective here?

@_date: 2014-01-08 10:10:15
@_author: ianG 
@_subject: [Cryptography] defaults, black boxes, APIs, 
Yes but.  I already implemented a library to replace the strevil, and so did half the people here.  Like those above, they sit on the shelf, much dust and sagging is our only reminder.
We all failed -- is the point.
Why?  probably because we were divided in our solutions, and the committee bendeth to the volume of our consensus.
What better way to achieve the consensus?
Yes, true.  I was more referring to the rules that burden the thoughts not those that shackle the winner.
Well, if it got it into use, it's be worth the prize.  There's less honour in ya10kloc, more in crash-proof apps.
Ah, there's that word, 'good' ... the solution to buffer overflows 'good' enough?  Or is there a better?
Or, if belief is keen, make an offer for CodeCenter.  Make it open source.
We know how to do it, individually.  We as a community or internet have singularly failed to do it.

@_date: 2014-01-08 10:25:21
@_author: ianG 
@_subject: [Cryptography] defaults, black boxes, APIs, 
Thanks to all those replies, I think it is clear that we do know how this can be done.  Onwards...
Au contraire!  That is precisely what makes it a good contest.
Consider AES.  When that contest was launched, did we already know how to do a good block cipher algorithm?  Apparently, yes.  We had Blowfish, IDEA, 3DES, and others whose names escape me.
What was at issue was replacing these with a single choice we could all centralise on and get away from the algorithm spaghetti soup syndrome.
Yes.  Not impractical because we don't know how to do it individually, but impractical because we don't know how to do *consensually*.  E.g., it is proven that the committees and their 'rough consensus and 2 working implementations' has not worked.
Right.  For that to happen, we have to make it big.  Then, push groups to sign up to it.
It meets both criteria, as long as they are framed appropriately ;-)
ps; of course, we're playing devil's advocate here.  I'm not actually wedded to the notion of a "C sans overflow" competition.  I would actually rather do a TLS2 competition ... that would be much more fun, and much more inventive.
But hey.  What's the biggest problem we face?

@_date: 2014-01-08 10:52:16
@_author: ianG 
@_subject: [Cryptography] defaults, black boxes, APIs, 
God, no, that would be sin.
I personally dismiss the sandbox and SecurityManager stuff as likely false sense of security.  OK, nice when working, it probably won't hurt, but, how do you know it is working, and how much effort is required to dance around it?  These are unanswerables.
(I'm not even sure what the use case is, was it just applets?)
Right.  On server-side, the server is the sandbox.  Anything else is another sin brought to you by the marketing borg, and as equally valueless.
So, these are extraordinarily different applications.  People seem to think that because Java is potentially used in both, the results should be about the same.  But the applications are so different, there is no Substitute Java in the above for any other language, including a different one for each, and you've still got exactly the same problem statement.  Afaics.
Looks like wheelspinning for non-core managers with too little to do.
Right.  But if someone can get at my java server-side code, then I've got bigger problems.  Problems that exist regardless of the language...
Sure.  Like browsers, that environment deliberately mixes apps from different places.  Something is needed.
No doubt.  What benefit did you earn for this acrimony?

@_date: 2014-01-08 11:47:41
@_author: ianG 
@_subject: [Cryptography] What is an attack, and what is not an attack? 
Apologies for late reply!  This point was important...
Indeed, so what's a validated attack?  How do we know, really know? Here's my call:
1.  Literature is theory.
2.  Attacks in labs are experiments, not attacks.
3.  Academic exploits and corporate embarrassments aren't real demonstrations of economic risk, they are more reputation-leaching from innocent corps to press-hungry security rock stars.
4.  Absent any evidence, we cannot disamgiguate between myth, fear, marketing, fraud and self-deception (Dan Geer's observation).
So, if we're doing risk analysis, attacks do not include literature, lab demos (like that of Dave and Ian in the Netscape days), academic stuff, journalism, and stuff we make up ourselves in order to sell our product.   These are all interesting, informative, helpful, but they do not add to our knowledge of economic attacks directly.
The only evidence that slices through is *damages*.  How much money was lost?  (Excluding reputation damage and re-work efforts.)  If there are events with damages, if we can measure losses and frequencies, then we can calculate likelihoods and expected losses, etc [0].
So by this definition, the only validated attack on an RNG that I know of was the Bitcoin theft of coins using the sloppy Java PRNG [1].  We know how many coins were lost, roughly.  Notice the economics -- theft of money, and it's the sort of money you might be able to get cleaned before anyone notices, so there is a tight and solid feedback loop to inform us.
If not, then we have to use our judgement.  I use my judgement to say that DUAL_EC was a real attack, but I can't validate it because I cannot calculate the damages.  It goes on the list, because my judgement says so, but I don't *know it happened* as yet.
[0] CA threat history: [1] Java's crypto system should be called Diana because it has a huge cross painted on it...

@_date: 2014-01-10 12:18:06
@_author: ianG 
@_subject: [Cryptography] On threat models and progress 
This is why I say:  the threat is always on the node.
This fallacy that we are protecting the wire, and we do/can not protect the node is at the root of a lot of our woes.  The proof is in the pudding, the last 20 years of security has been the history of, on the one hand, wire security that seemed to wallow around without direction, and on the other hand, attacks on the nodes that powered on apace without much notice from the server-side community (excluding the *BSD community that is) because they were convinced that wire was where the threat was.
10 years ago last October, posted on this very list.  One of the things that I discovered was that the MITM-TM derived from military experience, and we didn't at the time have a good view of an indigenous Internet TM.   Now we more or less have the experience, assuming we actually do the analysis, and now we have sufficient evidence in the post-phishing, post Snowden world to ground that analysis.  But then, it was all That's very scary.  But I'm not sure how real it is.  With a nod to other threads about what an attack is, how much data do we have that crooks are actually using timing channels or side channels to defeat real systems and do real damages?  Open question?

@_date: 2014-01-12 10:54:51
@_author: ianG 
@_subject: [Cryptography] defaults, black boxes, APIs, 
Web browsers are a huge product, which requires a huge company to deliver it.  Which means a huge revenue stream and a lot of paid engineers.  Which means loyalty to the brand and the business.  Which means stagnation of difficult areas, and advancement of customer exploitation ideas.  Web 2.0.
Even open source projects suffer more or less the same syndrome once they reach a certain size;  they are cohorts of many many engineers, of whom many are paid for their product, by businesses shipping it.  When the businesses decide things, that's what gets done.
Guess what they decide?  Generally, in favour of status quo -- ship more product, don't change anything.
Bitcoin is the bleeding edge of cryptography at the moment, because that's where the crypto and the money is.  Curiously, there, you can be your own business and ship your own product.  But I can easily see a time where the development team loses its mojo and succumbs to the big businesses.  Then what?
It's tough to work on big systems.   One can be an honest security geek, a good security geek, or a loyal security geek.  Pick any two.

@_date: 2014-01-12 11:01:43
@_author: ianG 
@_subject: [Cryptography] Dumb idea: open-source hardware USB key for 
Responding to all, it seems that the only constant here is USB, and nobody's particularly wedded to that.
So, we don't know what the best solution is.
Let a thousand flowers bloom.  Get your ideas out and try it.  One thing is clear:  not having anything is generally less secure than having

@_date: 2014-01-13 10:51:28
@_author: ianG 
@_subject: [Cryptography] Dumb idea: open-source hardware USB key for 
Sounds like lots of fun!  For the most part, I would say that all devices can be defeated in the lab by persistent attack, and the primary protection is:  Don't lose your stick!
In the first instance we need to get things going.
Much later on, it might be fun to start attacking the various ideas and see which can resist, which can boost their resistance, etc.
What does Travis recommend for preventing attacks?  Microswitches? Acid bottles?  Plastique?

@_date: 2014-01-13 11:41:27
@_author: ianG 
@_subject: [Cryptography] Dumb idea: open-source hardware USB key for 
That is what good defenders do, they think like attackers.  And that is what good attackers do, they think like defenders.
Just some words to challenge the idea forward ;-)
Revocation was a terrible failure in PKI.  As we saw, and predicted (and yes, the critics predicted revocation would not work), the CAs and their keys had to be revoked at the uber-CA level in software updates.
However, revocation can be done properly if done as part of holistic system design.  Payment systems are often designed like this.  Without specifying more of the details and requirements it is not easy to say more, but typically, it plays its part.
if one is fabricating ones own chip, then one could probably achieve the same effect by layering a heater circuit over the top of the key store.
And then having the microswitch dump the remaining battery life into it, or using a capacitor or somesuch.
The thing is, these are exotic attacks.  Remember, for most people, just having the stick will defend against their weakest point, their desktop or laptop or phone.
If you like, for yourself.  Avoid consensus on this, the list will drag on for ages and nobody will agree.  Any idle question will distract everyone.  "What about bluetooth?"  "We can't use EC!"
Slap your own ideas down and build it.  The builders will win this battle, not the talkers, nor the committees.

@_date: 2014-01-13 12:35:08
@_author: ianG 
@_subject: [Cryptography] Dual_EC_DRBG backdoor: a proof of concept 
(apologies, late reply, had to think about it!)
OK, I stand corrected!  I'll have to see if I can find that reference to NSA saying using a non-deterministic RNG is a state of sin.
If you are going to sabotage a standard, then it pretty much involves being ... at the meetings, and wielding power to direct efforts.  This might not involve much pushing, or it might involve a lot of pushing.
For example, the GSM committees that designed their A3, A5 algorithms were pushed, but only gently.  Spooks were at some meetings, but they said little.  As they were all telcos at heart, and as their threat model was paparazzi snooping and billing theft, and as they were Europeans and traditionally were in bed with the agencies, they really didn't care that much.  Only a small amount of pushing was required to set the key at 40 bits, the famous 8 * 0 bytes plus the checksum-of-deception copied from DES.
An open group preparing standards for open industry has much broader threat models.  GSM had theirs quite clear and narrow, vertically integrated, but when you are building a general tool, you have to more or less consider every threat ... or you have to come up with a stylised perfect design model (like SSL did with their perfect secure connection).
Then, influencing such a group as a NIST committee requires a lot less subtlety and a lot more deception.  And taking much bigger risks.
Right.  You might ask why they took that risk.  What we do know is that they take these risks, in general, they have the mindset, it is here, in black and white:
Why did they take that particular risk?  (assuming they did?)  knowing or should-knowing that the consequences would be disastrous?  For that I guess we have to look at the history of the last decade.
One open question is whether they took that risk particularly with NIST/DUAL_EC, as asked by Jon and you and many others.
That's a judgement call, we'll 'know for certain' in 50 years when they declassify.  Until then ... what matters is that they are taking that risk, in general, everywhere they can.
So we have to act "as if" the NIST standards are under attack.
This is a useful thing, as it also has consequences.  It protects the NSA from blundering again -- if they know we all act "as if" the NSA is going to pervert the NIST standards, then they are much less likely to do it.  OTOH, if we act "as if" this is unlikely, silly, implausible, unsubtle or whatever, then the prize is sitting there .. they are more likely to give it a go.
Yes, interesting counterpoint:  the Android/Java RNG hack to steal bitcoins.

@_date: 2014-01-13 12:51:05
@_author: ianG 
@_subject: [Cryptography] What is an attack, and what is not an attack? 
Rather your comments address it point-wise, my way of thinking is like this:
1.  If you do compliance, do what they tell you and ignore the rest. This is best practices;  you're not actually in the game, you're insured by the herd.  Read no further.
2.  If you have history of attacks [0], then calculate the probability of attacks, the damages to you the victim of those attacks, and multiply it out to give you some sense of budget you should spend to.
3.  If you have no history and no compliance, then you have to estimate all these things.  But these are all judgement calls, made by you.  Your name goes on these calls.
4.a If you are responsible for managing the corporate budget (CFO), err on spending zero, especially for unproven stuff from (3) above.  Your name depends on spending the least and nothing going wrong.
4.b If you are responsible for spending the corporate budget (CSO), err on spending more, especially on unproven stuff in (3) above.  Your name depends on spending the most and nothing going wrong.
ps; I did write a long point-wise reply, but it seemed voluminous and who's got the time?

@_date: 2014-01-14 09:31:56
@_author: ianG 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
What's your suggestion?
Let's try a hypothetical.  Say I'm CEO of some really important security
company.  And I know I'm being punked.  Or, I'm a techie and I think
I've spotted a punking in progress.
What message do you want to give me?  It's ok for RSA?  I'll get away
with it because people need my product?  As long as I don't admit I knew?
This exact same thing happened in the CA industry -- CAs were selling
dodgy MITM sub-certs.  The alleged price for these was around $50k.
Gravy!  While nobody said anything, everyone was sweet.  Then, when we
started finding them ... the CAs didn't get punished.  Oh, there was
some flak in the papers, and sub-CAs were revoked, but no significant CA
lost anything.
So what do we do?  Being honourable doesn't work.
I find this all very curious.  People are really angry.  The NSA is
continuing to deceive or lie through their teeth or be so incompetent or
blind or dissonant that they should not just be fired but sent to the
gulags [0].
When people are angry, they want to punish someone.  We can't punish the
NSA.  People also want results, fixes.  Congress won't fix the NSA.
So how do we go about that, in a general sense?
Deploying more tools isn't it.  Tools get deployed, then weakened, then
forgotten.  We've travelled that path, that's where we are not, that's
the path they breached.
[0] compare this comment
"The officials resisted this characterization. Why, they asked, would
they compromise security of products they use themselves, like Windows,
Cisco routers, or the encryption standards they allegedly compromised?"
With these documents released by Snowden:
"Shape the worldwide commercial cryptography marketplace to make it more
tractable to advanced cryptanalytic capabilities being developed by
NSA/CSS."  etc etc.
[1] Or compare this:
They believe their intelligence gathering is palatable because it?s
controlled by laws, regulations, and internal oversight. Looking at the
world through their eyes, there is no privacy threat in collecting
massive amounts of information ? if access to that information is
rigidly controlled and minimalized.
To this:
?Approval to release to non-Sigint agencies,? a GCHQ document says,
?will depend on there being a proven non-Sigint method of acquiring keys.?
They are releasing SIGINT assets ... if they can get away with it?
That's preservation of their secrets, not protection for our secrets!

@_date: 2014-01-14 10:01:09
@_author: ianG 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
Also, I think a fair proportion of the blame lies with NIST.  They force
their standards on the world (never mind that they don't say that) and
then act surprised when they get turned.  What's worse, they take no or
little account that they are pursuing industrial control policies by
their barriers to entry, the cost of the stuff is huge, for what dividend?
I'd boycott NIST.  Dump all the security FIPS and what have you.  How
much good have they done?
I'd also boycott companies doing business with the NSA.  And USG.  If
their primary purposes is dealing with those agencies, then we know they
are likely vulnerable.  Seek companies with clean records.  Especially,
ask questions:  how much influence?  what options were asked for?  what
The attack on the RSA conference is an attack on the brand of RSA.  This
covers the whole company.  Yes there is collateral damage, but there is
also an easy fix:  change the name, sell the company.  It can even be
If the response is serious, EMC will realise.  OK, that's not a given,
the idiot journalists universally attack the brand of Java whenever an
applet exploit is found, and Oracle sits their sleeping like a sloth.
So maybe some of these companies don't understand what a brand is.
That is an idea.  If one is in the business of sanctions and one is
concerned with collateral damage, it is a competitive market.
I think all boycotts have this problem.  But what other tool do we have?
Is it a battle to win?  CISOs pick the tokens.  They are unlikely to
look past their noses.  The tokens are typically customer-branded.
We would need more that speaks directly against the tokens to spread the
message, hypothetically something like a Snowden revelation that
indicates the NSA has a back door to the tokens.
Perhaps just those questions.  If RSA dropped the security baby on
BSAFE, why not on the tokens?  Did the NSA approve or vet the tokens?
Did they sit in on any of the meetings for government sales?  What
features and options did they request?  Does BSAFE play a part?  Was
DUAL_EC used for the generation of the token secrets?
In the contrary, do we do more damage to companies by tricking them into
dropping perfectly good tokens for some other equally ropey product?
I feel like we should also boycott the IETF.  They have truly not served
us.  We should have had opportunistic SSL covering the planet by now,
and that would have been a fantastic defence against the worldwide
surveillance -- it would have shifted the NSA to an active attack, which
would have been eventually detected.
They're still sitting there doing the work of the companies and not the
work of the people.  What success have the IETF committees brought us,
other than to surface the corporate wars?

@_date: 2014-01-14 20:59:25
@_author: ianG 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
Yes.  And it's soooo easy to do with a WG/committee.  IMHO, if they had
decided to do the opposite (make the products strong) they would have
had a very hard time of it.

@_date: 2014-01-15 14:15:31
@_author: ianG 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
Hi Steve,
I beg to differ!
Yes.  Why is anon-dh there in TLS but not covering the planet?
Opportunistic crypto (more like Self-signed Certs) is there in SSH, and
SSH *is covering the planet* !
It's bad, I grant.  It's a rhetorical question, it's supposed to
indicate that IETF doesn't actually have a good track record here...
Write them down, successes v. failures, left v. right.
A WG achieves the same result as a committee.  Of course they are not
committees, anyone can join.  But we get the same effect, it's a zebra
without stripes, or is it a horse with stripes?
"Getting involved" is the reason why TLS opportunistic encryption is not
covering the planet.
Although widely criticised for its crypto, SSL v1 was opportunistic
until certain financially interested parties (remember their names?
infamous now) forced Netscape to "get involved" and get certified before
it had got enough traction.  Since then, those that are "involved" have
made sure that there is no change.
"The Vancouver IETF plenary concluded that pervasive monitoring
represents an attack on the Internet, and the IETF has begun to carry
out various of the more obvious actions required to try to handle this
attack. "
We've been here so many times.  OK, so here's what's going to happen.
The people at the event will agree to try more stuff, they'll hear more
presentations, and there will be a general move to accept more thinking
about opportunistic crypto.
Once the declaration of consensual outrage is done, the WGs will ignore
it totally.  They'll get back to their fiddling around with MD5 fires
and beastly breaches and what have you.  PKIX will not support
opportunistic.  The vendors won't do a thing, because they can't change
anything without leadership from the WG, who they "involve" to control.
 And if anyone convinces the WG to actually shift and say something like
"all servers must now offer anti-mass surveillance suites," then the
vendors will say, oh, and now you have to convince the vendor's
associations...  get involved!
The IETF cannot be presented with a position paper describing the
problem because the IETF is part of the problem, and nobody wants to
hear it who has already taken the investment to participate.

@_date: 2014-01-15 14:25:25
@_author: ianG 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
But, it is our side show.  All of the above examples are in other
businesses, where they promise little to do with security.  As the telco
CEO said, "our business is to connect phones together."
In our world, and in RSA's world, we promise to deliver security.  Not
phone calls, not oil-for-food, not airbases in Iraq, not 3rd world debt,
not higher returns, etc.
Security is the promise, and that was the promise RSA fell on.  When a
Samurai fails to protect his lord, he falls on his sword.  When he
delivers a lousy bowl of noodles, he cares not.  The lord should employ
a noodle cook.
iang, who's wondering when his own promises are going to bring the sword
too close for comfort...

@_date: 2014-01-15 20:14:25
@_author: ianG 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
Read a website:
"RSA BSAFE Crypto Kernel offers versions of popular cryptographic
algorithms optimized for both small code size and high performance.
Unlike alternatives such as open source, our technology is backed by
highly regarded cryptographic experts."
They make the promise above, and in many other places and times.  They
delivered the DUAL_EC against that promise.
Also, we have the fact that they ignored the warnings that came out
about DUAL_EC, from around 2007 - 2013.
In short, their highly regarded cryptographic experts were not deployed,
not available, not on that job.

@_date: 2014-01-15 22:28:49
@_author: ianG 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
Could have been, but that isn't the case.  There is enough background
info to conclude that the experts were not consulted on the deal.  Not
that it makes much difference, remember the clanger.
There are some things that can be exaggerated ... and some things that
can't be passed off as mere bluster and marketing.
I never said they were evil, but it might be evil to reinterpret words
to defend the indefensible, dunno.
As has been repeatedly mentioned in this list, RSA were tricked.  They
and the people within were not evil nor are they evil.
Rather, *there but for the grace of the crypto gods go we all*.
(You're right about the looking back part for myself, I never even heard
of a DUAL_EC before this blew up.)
As has been mentioned, we are in a different space - the attacker
refuses to play fair with us and appear in court to answer our
prosecution.  No discovery is possible.  He will lie, prevaricate,
deceive, and perjure, ignore orders to reveal.
We cannot therefore rely on the standard of "beyond reasonable doubt"
without committing a willful blindness ourselves.
This won't change.  I therefore choose not to be willfully blind, and
use a weaker standard.  Balance of probabilities is suggested for civil
cases, and that seems to be a good working metric.
Anyone of course can decide to insist on a smoking gun -- beyond
reasonable doubt.  But we're dealing with an attacker that isn't that
Should we be?  If you choose that path, all power to you, but you've
taken yourself and your opinion out of the game.  Sorry about that.

@_date: 2014-01-15 22:57:07
@_author: ianG 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
Hi steve,
We disagree :)  Most people didn't have a clue what they wanted, or what
they meant by security.  They just wanted security, in a 6-pack, and at
the discount price please.
The reason people wanted web server authentication was that they were
told that's what they wanted.
A long long long time ago we worked out that this was a fallacy.  Now we
can show it with even better numbers, with risk analysis.
For your ordinary non-finance site, what's the likelihood of
eavesdropping and what's the likelihood of MITM (for which we want
    Eavesdropping:    100%  (thanks, NSA!)
    MITM:             0%    (to many extra zeros.)
Not wanting anon-dh on all HTTP means either you do not understand the
economics of security or it means you're a marketing droid.
Wait, that's not fair, I never did the economics!  OK here goes:
                      Likelihood             Cost
    Eavesdropping:    100%  (tx, NSA))       Free   (unmeasurable)
    MITM:             0%    (.00000..)       $100 per server
(proper risk analysis would multiply those numbers out, but we can see
where this is going...  Zero is good that way.)
Oh, there is no one simple factor.  But there are driving forces that
push the factors around.
Do you remember what happened in the late 1990s with the IETF
announcements that strong crypto was the only answer?  Servers and
browsers are still shipping 40 bit crypto... Compatibility is a far
bigger force or factor than IETF monthly memes.
Yes, so the real action is in Bitcoin, in p2p, and out there in the
startups.  The security models that matter to ordinary people aren't
found in the IETF WGs, they are found in Skype (dammit), in Facebook
(security? wot security?) in snapchat, in gmail, and the gogglecenters.
We should still try to fix the ones we have:  get HTTP over to
opportunistic encryption of some form, up the ladder, get more sites up
to authenticated as and when they judge it right to pay the price.
We should try, but this is all terrifically old stuff.  Most of the
people on this forum are already nodding off, we are too old for
repeated arguments and lost battles...
Maybe it really is up to a new generation, like the bitcoin borg?  Hmmm,
no, their idea of security is ...

@_date: 2014-01-16 21:30:12
@_author: ianG 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
Well, we could do nothing.  Seems a bit pathetic, how do stand before
customers and say "oh, it's ok, it's just the media again?"
We could choose an alternate approach.  I'm not sure what that would be

@_date: 2014-01-16 21:37:05
@_author: ianG 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
New for CAcert is part 1 of the two part series on the secret cells:
This was inspired by Snowden revelations, and it is the sort of sharing
of information that I think people should really pursue.
I'm particularly interested in comments about the 6 Assumptions.

@_date: 2014-01-17 10:39:04
@_author: ianG 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
So, counterpoint.  Skype.
They weren't tricked, they turned -- it was a conscious decision of the
owners to take the king's shilling somewhere around 2009, before the
sale to Microsoft.
Skype now tracks the content back to HQ, as was reported by Heise,
proven by Adam, and leaked by Snowden.  Undoubtedly, at SKype HQ in
Redmond there are friends from Ft Meade with their special fibre link,
indeed the revelations say as much (can't recall where).
Why is there no boycott on Skype?

@_date: 2014-01-18 11:17:17
@_author: ianG 
@_subject: [Cryptography] cheap sources of entropy 
Jon Callas (I think) a long time ago suggested pointing your cheapo USB
camera at a photographer's grey card in low light.  The theory is that
the cells in a camera seek for information and if they don't see
something that is worth reporting, it drives them a little tipsy.  The
claim is that this effect can drive them into some form of quantum
When we were creating a CAcert root key at one stage, I used this
technique to deliver one of the independent feeds.  I wrote a shell
script to take a photo once a second, and sha it into a log file.
Examination with Mark-I eyeball of the photos and shas didn't reveal any
artifacts, and the results at any one point were certainly chaotic.
(In the same exercise, we XOR mixed in Linux's RNG and John Denker's
audio.  The goal was to have each person bring in an independent stream,
and then examine the XOR program to ensure it was clean.  About 2 pages
worth of simple C.)
Open question:  What do people think of the production of big important
keys using the old compliance method of "must use a HSM" now ?

@_date: 2014-01-18 11:30:08
@_author: ianG 
@_subject: [Cryptography] How to use FST-01? 
Why don't you try all of these things and let us know the results?  I
suspect that would be easier.
When it comes to the combination of two exotic pieces of hardware,
you're not likely to find someone expert in both...
ps; reminds me of what the hardware guys used to say:  suck it and
see.  A sadly lost art...
A cryptographic (hardware) token is designed to carry your GPG (or
similar) high level signing keys on it.  When you plug it in, the hope
is that your email program will then be able to ask for signatures and
get decryptions of incoming mail done.  When you take it away, your
email program won't be able to do that.
So your host machine is now no longer solely responsible for
protecting all your keys.  If your computer is stolen, then as long as
you have your USB token in your pocket you are still 'secure'.
Sticking the keys on an external device gives a small measure of
security.  As we know, most host platforms are subject to all sorts of
malware, so we can expect most users' machines to be easily scanned
for keys.  Unfortunately it isn't much more security, because the
malware can simply sit there and wait until the USB stick is plugged
in and then ask for all the sigs/decrypts it wants.  It maybe can't
get the keys, but it can act as if it is in control of the keys.
( The old rule is that a key controller has to also have a keypad and
a display so that it can show what it is being asked to do to the
human, and it can wait for the OK button to be pressed.  That's called
transaction authorisation, and a variant is typically used by advances
(European) banks with mobile phones to defeat MITB. )

@_date: 2014-01-19 10:50:47
@_author: ianG 
@_subject: [Cryptography] HSM's 
Hmmm... So one could imagine a key generation process that worked like this:
a. each box generates its random contribution Rn.
b. each box commits to its random contribution by sharing the MD of it.
c. each shares their Rn with all.
d. all shares are XOR'd together to get R.
e. RSA key is created deterministically out of that combined R.
So for the API, we'd need to map that.  It would be more a messaging and
broadcast protocol than library API.
For signing, it's a simple compare, if it is deterministic.  For
encryption, that should be self-correcting.
Then, what about the negative properties -- proving the negative?  Aside
from the physical security [0], there is the obvious temptation of
spitting secrets out on the message bus.  So messages can be monitored,
but what would happen if for example one of the Rn were an encrypted
version of the last private key generated?
We could take the random generation out of the HSMs.  Principle!?
There could be N RNGs coupled to N HSMs.  The set of Rn could be created
in a write-only fashion, delivered singularly to the matching HSM-n,
committed, and then after the calculations, the RNG set could do the
attest to say they had generated the Rn set independently.  If the RNGs
cannot see the HSM messages, then they cannot be interfered with.
a. each RNG-n generates its random contribution Rn.
b. each RNG-n commits to its random contribution by sharing the MD of it
on its write-only bus interface.
c. each sends its Rn to the write-only bus.
d. each HSM-n reads the full set of Rn  and XOR's together to get R.
e. Private key is created deterministically out of that combined R.
f. Publish.  Compare.
g. Be happy.  Drinks...
       RNGs    Write             Read
               Only      HSMs    Write   BUS
               Lines             Lines
     /-------\                           ||
 RNG-1 | ----->----->----->------> ||
     \-------/                           ||
                         ~~~~~~~         ||
                        { HSM-1 } <====> ||
                         ~~~~~~~         ||
     /-------\                           ||
 RNG-2 | ----->----->----->------> ||
     \-------/                           ||
                         ~~~~~~~         ||
                        { HSM-2 } <====> ||
                         ~~~~~~~         ||
     /-------\                           ||
 RNG-3 | ----->----->----->------> ||
     \-------/                           ||
                         ~~~~~~~         ||
                        { HSM-3 } <====> ||
                         ~~~~~~~         ||
[0] Which we can outsource to the user for now.  Let's leave out
side-channel stuff as well.

@_date: 2014-01-19 11:42:24
@_author: ianG 
@_subject: [Cryptography] Conferences, committees, compliance 
Today's devil's advocacy post...
To a surprising extent, yes.  Certainly in comparison to the committees,
conferences, and also the compliance processes, yes.
Possibly this is just self bias, but I've been yammering on about things
like single modes and single algorithms and single architects for yonks,
and now these guys are starting to do it.  Have a look at CAESAR -- this
ground shift away from 'perfect' block ciphers has only occurred in the
last decade or so.  Why did that take so long?
My answer is this:  cryptographers and cryptoplumbers have really only
started talking together seriously in the last decade or so.
Conferences, committees, compliance processes didn't help that -- the
interfered with it.
(C3 considered evil?)
If you look back on C3, ask what notable results have come out ... list
them out ... and then look at what the builders have achieved by
themselves.  As just plain engineers.
The list isn't all one way, but there is a surprising amount of stuff
that came from engineers acting alone or in teams of 2.
Skype, Bitcoin, SSH, SSL, were all done initially by engineers.
Then look at all the cryptographer-led ventures:  DigiCash, Peppercoin,
various DRMs.
Yes.  I agree the advocacy, relationship-building and cross
fertilization is needed.
But these processes aren't admitted to much of C3.  Many academic
conferences are captured by their paper-acceptance process, where you
have to be in the acceptance committees, accept crap papers from your
buddies, so they accept your crap papers.  It's a career-building
necessity, if you want academic credibility!  Unfortunately, the more
you win, the more you lose, as these little peer groups isolate
themselves in self-perpetuating crap.
Commercial conferences are captured by the vendors.  That's the main
defence of the RSA conference: "oh, my, where will I go to sell my
stuff?"  Committees are captured by the vendors, who send in their
engineers to make sure they get the least bad deal they can fight for.
Compliance processes are written by the industry leaders to establish
and cost-increase their own position.
These processes are all based on fallacious and expensive assumptions.
Here's one wrong assumption:  cryptographers know how to do
cryptography.  Sounds an odd thing to say, but look at the ones James
names ... they are actually computer scientists.  Programmers.  As much
and even more than they are cryptographers.
Adi said:  "Only the simplest cryptography is used..."  Yet look at what
his peers are working on?
Why is this?  Well, it's like materials science I suppose, the theory is
fine but it only advances to the extent that the practical issues are
sorted out.  Which requires engineers in the field.  And cryptography
was pretty much an ivory tower field for most of its open history, never
came out into the field.
Now, the engineers are learning enough such they don't need them.  Dan
and Jon aren't gods, but they are role models:  learn enough so you can
do it yourself.  It's your job, do it.

@_date: 2014-01-19 11:59:28
@_author: ianG 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
Yes, and when they can't find it in their haystack, they go looking in
others' haystacks.  All of the agencies have been implicated in tracking
democratic protests, under the guise of "breeding terrorists."
I definitely agree with your point.  We should not be seeking to
interpret information that isn't available.  The news I saw last week
was that the NSA's Trillion Dollar Anti-Terrorism Decade has succeeded
in ... stopping one $8500 transfer to an implicated somali group.
That's it!  My first calculator couldn't even display that ratio...
Right, doesn't make sense.  The better way to deal with this is to use
what information we have, and seek to augment it with new information.
Which is what I'm saying above.
No, you're missing the point.  We don't have to prove a negative, we can
assert and agree to a positive.  We can create evidence by means of
disclosure, up front.
Let's say you are going to buy a crypto tool.  Something to replace
BSAFE, whatever, doesn't matter.  You can go to vendors and ask them to
disclose their practices.
"Do you have any sales to security agencies?"
"Do you have a process where security agencies provide you with security
"Do you ship product that is influenced by security agencies?"
"What process do you use to accept influence from security agencies?"
"Can we rely on these answers?"
Basically, a vendor that wishes to stand apart will answer these in one
way.  A vendor that is uncertain will answer them in another way.  And a
vendor that is compromised will answer them in a third way.  You can
interpret the answers, and you can hold the company to those answers;
this is the nature of civil litigation.
Try it.  Ask Silent Circle questions like these.  Ask RSA.  What would
Lavabit have said?  I don't know how they will answer (and I'm not
buying today) but I'm pretty sure anyone will be able to see the
difference in the answers, and any lawyer will too.
I agree.  Entirely.  But observe the distance between 'la la la' and
trying to prove the negative -- Nothing!
One is a benign response and the other is a circular aggressive
response.  Both have the same result.
Look elsewhere for answers.

@_date: 2014-01-20 19:43:19
@_author: ianG 
@_subject: [Cryptography] HSM's 
That stuff ... I could never understand how it would work in a
complicated environment when we have live active inside and outside
Yes, that's more or less what we developed at CAcert for creating new roots:
At CAcert I more or less decided I could not trust the HSMs, as
essentially they were unauditable.  I don't see that has changed, and
what I've heard of other CA practices is that they basically wing it in
this direction.  I guess some Auditors just nod off as soon as they hear
that an approved (?) HSM is used without even checking the circumstances
of the procurement and usage.
So we stuck with the "home grown" HSM concept which was to build a
machine, and lock it down in the secure rack.  This has the risk that
someone can sneak in and steal the root by opening it up.  My call was
that as the CA had covered pretty much all the other risk better, this
was an acceptable risk.  But in the future they should work to reduce
this one as well.
(Formal audit practices are both more stringent and more ropey.
Typically they say "must use a HSM" but no more, so it's a bit of a joke.)
:) A problem is that the only people who buy these are forced to buy
them.  So the only ones that are worth making are the ones that people
are forced to buy.  Which is the ones on a popular list somewhere.
Which of course then becomes a self-perpetuating myth, a barrier to
entry and an industrial support policy, all in one.

@_date: 2014-01-20 21:35:50
@_author: ianG 
@_subject: [Cryptography] cheap sources of entropy 
Indeed.  I should mention that the context was the key creation ceremony
where I could monitor the situation.  I don't see it as a general solution.

@_date: 2014-01-21 09:26:21
@_author: ianG 
@_subject: [Cryptography] HSM's 
I don't think it is necessary for the RNGs to audit their output.  It's
only necessary for the HSMs to audit the results.  The RNGs can be dumb.
I thought about that a little.  My musing at the time was that you could
avoid it by having a time-broadcast.  If the RNGs can deliver say 10k
per second, why not just have them do that?
Each RNG sends out a packet one per cycle.  And for committing, each
packet can include the MD of the next packet.
OK, so this sets up a lot of complications for the HSMs that now have to
agree on the time-set of RNG output.  Maybe not worth it.

@_date: 2014-01-21 10:16:57
@_author: ianG 
@_subject: [Cryptography] HSM's 
(Hmmm, I thought it was there.)  N-of-m is so cool that you only need
1/m of analysis.
Is n-of-m deterministic?  Or can one of the m inject
distinct signature components and still get a good signature?  What
opportunities are there for byzantine manipulation?
It only gives you an advantage when you have distributed HSMs or more
typically distributed active people.  Which you want because you're
trying to avoid the centralised threat of human insider corruption, not
the external supplier corruption.
If we're talking about 3 HSMs in a single box where we are relying on
each HSM to check that the others aren't doing the wrong thing, then
n-of-m doesn't give us any advantage that I can see.
If we're doing (distributed) voting schemes over a transaction, just
doing straight signatures with distinct keys over a transaction is
nice for a voting algorithm, the sigs are the votes.  Using n-of-m in
that circumstance gives us strictly less information because the sig
fails or it doesn't, we don't know why/who withdrew.
Yes, that's OP's goal, I think.

@_date: 2014-01-21 12:06:22
@_author: ianG 
@_subject: [Cryptography] RSA is dead. 
(I think that's precisely what it is:  "Historical revisionism, the
critical re-examination of presumed historical facts and existing
historiography" or, the rewriting of history to better interpret with
new logic and/or new facts.  Always controversial, often painful,
sometimes pejorative, but never wrong.)
Such a ncie story.  It is a textbook spook approach, the story has to be
reasonable, plausible, benign and even insulting to criticise.
Defaults are seen as a way to improve security, never a way to reduce
security.  Yet, by definition, if there is a way to improve, there must
be a way to reduce.  What's that about?
Assuming 99% of the users are not competent at this level, they cannot
be relied upon to choose the setting with increased security.  The
existence and taking of a choice of two options is at least as likely to
lead to decreased security as increased security.  These probabilities
sink into ridicule as the number of options increases...
So the only secure choice for the 99% is to never touch the default,
*and* to rely entirely that the designer has chosen the only one mode,
and it be secure.  As the default.
Yet, the DUAL_EC system is apparently, allegedly only secure if you
change the defaults.
Fort Meade knew this.  This attack on our cognitive dissonance can be
eliminated entirely by removing the default.  Defaults are security sin.
 Oh, my, that was easy.
I certainly agree with the logic.  The comment I saw from some NSA
official indicated that anything other than this approach was another
sin.  I wish I could recall where that was (John recently called me on
my misinterpretation, so I'm stuck!).
But this obviously places a lot of power in the tester's hands, without
a clear way to mitigate it.
"Works for us!"  but is this an *exportable* best practice?

@_date: 2014-01-22 07:19:12
@_author: ianG 
@_subject: [Cryptography] Does PGP use sign-then-encrypt or 
First you need to establish what the sig is.  Is it a MAC, for
protecting the message, or is it a semantically interesting token over
the contents, aka a signature?
One of the architectural flaws of PGP, and competitors like S/MIME, is
that the signature is a cryptographic component with no implied semantic
meaning or purpose.
One needs to go back to the theory of what a signature is, and decide
ones purpose.  In short a signature is a token that is placed over a
document to signify or memoralise an agreement of something like
sighting or contract or written-by or an endearment.
However a digsig in cryptography can sometimes mean that signature
concept above, and it can sometimes claim agreement, or it can mean a
MAC to protect a packet.
Now, typically, in OpenPGP we have preferred to use a cleartext
signature over the contents to do that human signing component, and a
detached digitally preserved signature for message purposes (coz if it
is detached it matters less if it gets lost).  But this isn't written
anywhere!  In practice, then, detached becomes a MAC so it is better off
in technical terms being part of an authenticated encryption suite, and
encrypt-then-MAC is more popular if we got our choices again.  But,
again, because it is a digsig, it also then reveals who is sending the
message, so that's a consideration for those who think mass surveillance
is a real threat.
In short, there is no easy answer, because of these issues.  You can do
whatever you feel comfortable with ...
Well, some think that this makes it easy to avoid a DOS attack.  You
only verify the signature against some expected sender, and if it fails,
you throw it away.  This doesn't make a lot of sense in normal email
mechanics tho.
Also, the email clients have typically (and stupidly) decided that the
signature should be checked against the email address of the sender.
Especially so in S/MIME, you can't use one email address to send a
signed email from another ... because it can check and so it does. Just
another reason why S/MIME fails to deliver and GUI clients have had
trouble.  What they should do is clearly show who the sender of the
inner envelope is, and ignore the sender of the outer envelope.  Then
we'd be a lot happier ...
yep.  Which is why more advanced protocols these days expect to use
ephemeral keys to do the MACs and encrypts, and strive for PFS or
protocol forward security.

@_date: 2014-01-22 08:01:37
@_author: ianG 
@_subject: [Cryptography] Does PGP use sign-then-encrypt or 
Yes, an inspired and deep critique of sigs, but it is fundamentally
flawed in much the same way as the systems because it does not
consider the semantics.
As I mentioned in the earlier post, unless you decide what it is that
a digsig is supposed to represent, you're up the creek without a
paddle.  This is an architectural flaw of the times, as is seen by the
confusion in both S/MIME and PGP.  These days, we more typically just
use digsigs for their cryptographic abilities, hide them away entirely
from the user, and generally steer away from signature ideas.
Others on that committee that does that standard might disagree, but
I'd say "no".
PGP was invented in the early 1990s.  This was pre-web and
pre-net-explosion.  It was done in an age of manual email preparation,
when techies were the only users, and everyone was comfortable
downloading code and talking directly to pop servers and whathaveyou.
 Few then understood the conflicts between digsigs and signatures.
Some still believed that a CPS could solve all ills, if it was big enough.
Things have changed a lot since then.  Now, everything is GUI, phone,
chat, video, sharing.  Everything is dynamic.  We have way better
knowledge of how to do things.  We understand
industry/proprietary/government dynamics better.  The attack models
have now been solidified with hard data.  We know which were the
theoretical minefields (e.g., PKI which nobody copies today) and which
are the casebook studies on how not to do things.
So much so that some (like myself) claim that email is fundamentally
broken, old, deprecated and cannot be secured.  So don't bother.
That's controversial, not least because I still use email, we all here
in this list do obviously, and some even spend significant amounts of
time designing proposals to secure it!  Notably PHB.  I hope he
succeeds because we're all addicted to it and need it.
But, the fact remains that the list is quiet, and no plans to augment
the OpenPGP draft from its current phase have been aired to my
knowledge.  There is a little attention on adding EC, but even that's
quiet and it doesn't change the overall situation, it's just an
upgrade of an algorithm that should be transparent.

@_date: 2014-01-22 09:48:42
@_author: ianG 
@_subject: [Cryptography] Auditing rngs 
It seems to me that all you are proving is that the box is correctly
creating keys.  I suspect that any internal attack is likely to be based
on feeding the special RN sequence into the key creation, so this isn't
really addressing the thing we are most worried about.
Is there an attack that is more plausible that doesn't change the RNs?
I guess what I would view as the best attack would be to change the RNG
to be a cryptographically secured PRNG, seeded with the time, serial
number, nonce and some magic only known to NSA.  Reseeded every second.
 Time and nonce to be put into the certificate as delivered.
(Likely there is self-testing on the device and in the interface
software that covers the same area.  Not that this achieves the same
purpose, but it is also possible that this self-testing could do what
you describe already.)
So the defence seems simple to me:  take the RNG out of the HSM, and
then we can do things like verify if the keys are created nicely.
That is the general concept, the HSM is for public key operations on a
high value key-pair.  But it is harder to implement than it is to sell.
 There are these difficulties that I've come across (and I'm no more
than a skeptical observer):
a.  the interface requires pretty tight software to drive it, and
especially for low-frequency, high-value operations such as root key
creation, there can be a mismatch between the quality of the software
and the importance of the task.
b.  backups!  Once these high value keys are created, there needs to be
a process to recover.  Lost/broken HSMs?  No problems, we'll just buy 3
instead of 1.  Ah, now, how do we get the high value key from HSM 1 to
HSM 2 ... which has to be done before hand....  HSMs have this ability
but it's also fraught as above.
c.  Storing the HSM is a difficulty.  Storing the other 2 as well.
d.  Something goes wrong ... and we don't have the skills to figure it
out.  Only the purchased software can drive the HSM, and that's too hard
to figure out.  The people who set it all up are long gone, the company
who sold the HSM is sold to another and the salesman wants to solve your
problem by selling you another better type.  Problems of this nature are
things like serial numbers changing, variations in the HSMs, batteries
going flat coz they sat on the shelf for 5 years, water damage, fans
gumming up, host hardware needing to change and having incompatible
specs, even the size of the new machine can impact, etc etc.
On the whole, I think I understand the attitude of many that get into
the HSM compliance trap, just about get the thing up and going, and stop
doing anything because of the energy involved.  The solution is more
painful than the problem it is trying to solve.

@_date: 2014-01-22 10:06:07
@_author: ianG 
@_subject: [Cryptography] Auditing rngs 
How about this variant.  Let's have the HSM have its own entropy source.
 But let's expand the scope to multiple HSMs (which are required anyway).
HSM1 is put into key generation mode.  HSM2 is put into RNG/audit mode.
HSM2 collects the entropy, processes it into an RN stream, escrows it,
and passes it to HSM1
HSM1 reads in the RN stream, and creates the key.
HSM1 then passes the key back to HSM2 which then verifies the key and
verifies that it was deterministically.
If the HSMs follow the same protocol, then they can be used to verify
each other.

@_date: 2014-01-22 10:56:53
@_author: ianG 
@_subject: [Cryptography] Does PGP use sign-then-encrypt or 
Yes, an inspired and deep critique of sigs, but it is fundamentally
flawed in much the same way as the systems because it does not
consider the semantics.
As I mentioned in the earlier post, unless you decide what it is that
a digsig is supposed to represent, you're up the creek without a
paddle.  This is an architectural flaw of the times, as is seen by the
confusion in both S/MIME and PGP.  These days, we more typically just
use digsigs for their cryptographic abilities, hide them away entirely
from the user, and generally steer away from signature ideas.
Others on that committee that does that standard might disagree, but
I'd say "no".
PGP was invented in the early 1990s.  This was pre-web and
pre-net-explosion.  It was done in an age of manual email preparation,
when techies were the only users, and everyone was comfortable
downloading code and talking directly to pop servers and whathaveyou.
 Few then understood the conflicts between digsigs and signatures.
Some still believed that a CPS could solve all ills, if it was big enough.
Things have changed a lot since then.  Now, everything is GUI, phone,
chat, video, sharing.  Everything is dynamic.  We have way better
knowledge of how to do things.  We understand
industry/proprietary/government dynamics better.  The attack models
have now been solidified with hard data.  We know which were the
theoretical minefields (e.g., PKI which nobody copies today) and which
are the casebook studies on how not to do things.
So much so that some (like myself) claim that email is fundamentally
broken, old, deprecated and cannot be secured.  So don't bother.
That's controversial, not least because I still use email, we all here
in this list do obviously, and some even spend significant amounts of
time designing proposals to secure it!  Notably PHB.  I hope he
succeeds because we're all addicted to it and need it.
But, the fact remains that the list is quiet, and no plans to augment
the OpenPGP draft from its current phase have been aired to my
knowledge.  There is a little attention on adding EC, but even that's
quiet and it doesn't change the overall situation, it's just an
upgrade of an algorithm that should be transparent.

@_date: 2014-01-22 11:02:21
@_author: ianG 
@_subject: [Cryptography] RSA is dead. 
Good stories.  These are stories that need to be catalogued somewhere.
What he said.  This is a spy agency.  These people are trained to
dissemble, lie, to seduce, to get inside people's defences.  It might be
polite to believe they won't do it to us, because ... what?   we are
citizens?  white?  nice?  they are helpful?  it's against the law?  on
our side?
But when they do it to us, this is our mistake to not expect it, not
theirs.  We need to act /as if/ they are the same as any other agency
run by the Russians, the Chinese or whoever.

@_date: 2014-01-23 11:47:42
@_author: ianG 
@_subject: [Cryptography] Does PGP use sign-then-encrypt or 
Even better, the anon-MAC(encrypt(sign(content))) solution.
The problem is the hammer of digsigs;  auth and auth and ID all look
like nails.

@_date: 2014-01-27 12:18:20
@_author: ianG 
@_subject: [Cryptography] Does PGP use sign-then-encrypt or 
That is a flaw with PGP, it didn't necessarily have the richness to
express what users actually need.  In today's better-known world.
But I would say that the flaw is in the presentation of the security
model.  The most important thing that a security system can do is to
align its security model to what the users need and assume, without
having to ask the users to make a decision.  K6.
Non-repudiation was an invention of cryptographers and does not exist
in the real world.
Right.  Most documents aren't signed, and not expected to be signed.
They're still useful.
Again, we were screwed by the cryptography myth from the PKI world
that tried to sell their certs on the basis that they could be used to
sign a document.  And documents that weren't signed weren't worth
anything.  It didn't work, and we have to unroll that myth every time
we see it.
Nice protocol, the name is a disaster.  I gather there is a review in
the works for this protocol.  If they could get one thing right, it
would be to change the name and drop any reference to being
I understand the mistake in logic was easy to make, but the logic
creates a clear trap of entrapment.  Compare and contrast your above
comments, and there is a clear dichotomy:  records kept of any chat
sessions are records presumed good before the court.
If you seek to deny the records on the basis that OTR is the tool to
forge these results, you've convicted yourself.  Just by using OTR,
you have given your accuser evidence that you intended to forge and lie.
Another problem is that PGP was an email protection system, and it was
commonly thought that the email should be protected at rest as well as
in flight.  As opposed to a communications protection system, which
only protects in flight.
OTR and most chat systems (perhaps not Skype) clearly separated out
the in-flight component and the at-rest component, in ways that PGP
did not.
A further problem is the so-called security model.  Email protection
systems rarely protected you against traffic monitoring, the surveillance target.  Skype on the other hand did, to a fairly good
extent, until they fell from grace.
The big picture, if one can see it is easy (to write):  systems that
were built on 1980s thinking aren't protectable under terms we think
of today.  Abandon email, build chat systems.
Mail has already been abandoned :)  These analogies are now past their
shelf life.  The young generation don't use email, and many probably
don't even know how to send a letter or seal an envelope.

@_date: 2014-01-30 01:14:05
@_author: ianG 
@_subject: [Cryptography] Hard Truths about the Hard Business of finding Hard 
Here's my effort (easier to read on the site):
Hard Truths about the Hard Business of finding Hard Random Numbers
As many have noticed, there is now a permathread (Paul's term) on how to
do random numbers. It's always been warm. Now the arguments are on solid
simmer, raging on half a dozen cryptogroups, all thanks to the NSA and
their infamous breach of NIST, American industry, mom's apple pie and
the privacy of all things from Sunday school to Angry Birds.
Why is the topic of random numbers so bubbling, effervescent,
unsatisfying? In short, because generators of same (RNGs), are *hard*.
They are in practical experience trickier than most of the other modules
we deal with: ciphers, HMACs, public key, protocols, etc.
Yet, we have come a long way. We now have a working theory. When Ada put
together her RNG this last summer, it wasn't that hard. Out of our
experience, herein is a collection of things we figured out; with the
normal caveat that, even as RNs require stirring, the recipe for
'knowing' is also evolving.
1.    *Use what your platform provides*. Random numbers are hard, which
is the first thing you have to remember, and always come back to. Random
numbers are so hard, that you have to care a lot before you get
involved. A hell of a lot. Which leads us to the following rules of
thumb for RNG production.
        a. *Use what your platform provides*.
        b. Unless you really really care a lot, in which case, you have
to write your own RNG.
        c. There isn't a lot of middle ground.
        d. So much so that for almost all purposes, and almost all
users, Rule  is this: *Use what your platform provide*s.
        e. When deciding to breach Rule  you need a compelling
argument that your RNG delivers better results than the platform's.
Without that compelling argument, your results are likely to be more
random than the platform's system in every sense except the quality of
the numbers.
2.    Software is our domain.
        a. Software is unreliable. It can be made reliable under bench
conditions, but out in the field, any software of more than 1 component
(always) has opportunities for failure. In practice, we're usually
talking dozens or hundreds, so failure of another component is a solid
possibility; a real threat.
        b. What about hardware RNGs? Eventually they have to go through
some software, to be of any use. Although there are some narrow
environments where there might be a pure hardware delivery, this is so
exotic, and so alien to the reader here, that there is no point in
considering it. Hardware serves software. Get used to it.
        c. As a practical reliability approach, we typically model every
component as failing, and try and organise our design to carry on.
3.    Security is also our domain, which is to say we have real live
        a. Many of the sciences rest on a statistical model, which they
can do in absence of any attackers. According to Bernoulli's law of big
numbers, models of data will even out over time and quantity. In
essence, we then can use statistics to derive strong predictions. If
random numbers followed the law of big numbers, then measuring 1000 of
them would tell us with near certainty that the machine was good for
another 1000.
        b. In security, we live in a byzantine world, which means we
have real live attackers who will turn our assumptions upside down, out
of spite. When an attacker is trying to aggressively futz with your
business, he will also futz with any assumptions and with any tests or
protections you have that are based on those assumptions. Once attackers
start getting their claws and bits in there, the assumption behind
Bernoulli's law falls apart. In essence this rules out lazy reliance on
4.    No Test. There is no objective test of random numbers, because it
is impossible to test for unpredictability. Which in practical terms
means that you cannot easily write a test for it, nor can any test you
write do the job you want it to do. This is the key unfortunate truth
that separates RNs out from ciphers, etc (which latter are amenable to
test vectors, and with vectors in hand, they become tractable).
5.    Entropy. Everyone talks about entropy so we must too, else your
future RNG will exhibit the wrong sort of unpredictability. Sadly,
entropy is not precisely the answer, enough such that talking about is
likely missing the point. If we could collect it reliably, RNs would be
easy. We can't so it isn't.
        a. Entropy is manifest physical energy, causing events which
cannot be predicted using any known physical processes, by the laws of
science. Here, we're typically talking about quantum energy, such as the
unknown state of electrons, which can collapse either way into some
measurable state, but it can only be known by measurement, and not
predicted earlier. It's worth noting that quantum energy abounds inside
chips and computers, but chips are designed to reduce the noise, not
increase it, so turning chip entropy into RNs is not as easy as talking
about it.
        b. There are objective statements we can make about entropy. The
objective way to approach the collection of entropy is to carefully
analyse the properties of the system and apply science to estimate the
amount of (e.g.) quantum uncertainty one can derive from it. This is
possible and instructive, and for a nice (deep) example of this, see
John Denker's Turbid.
        c. At the level of implementation, objective statements about
entropy fail for 2 reasons. Let's look at those, as understanding these
limitations on objectivity is key to understanding why entropy does not
serve us so willingly.
            i. Entropy can be objectively analysed as long as we do not
have an attacker. An attacker can deliver a faulty device, can change
the device, and can change the way the software deals with the device at
the device driver level. And much more...
            ii. This approach is complete if we have control of our
environment. Of course, it is very easy to say Buy the XYZ RNG and plug
it in. But many environments do not have that capability, often enough
we don't know our environment, and the environment can break or be
changed. Examples: rack servers lacking sound cards; phones; VMs;
routers/firewalls; early startup on embedded hardware.
        d. In conclusion, entropy is too high a target to reach. We can
reach it briefly, in controlled environments, but not enough to make it
work for us. Not enough, given our limitations.
6.    CSRNs. The practical standard to reach therefore is what we call
Cryptographically Secure Random Numbers.
        Cryptographically secure random numbers (or CSRNs) are numbers
that are not predictable /to an attacker/. In contrast to entropy, we
might be able to predict our CSRNs, but our enemies cannot. This is a
strictly broader and easier definition than entropy, which is needed
because collecting entropy is too hard, as above.
        Note our one big assumption here: that we can determine who is
our attacker and keep him out, and determine who is friendly and let
them in. This is a big flaw! But it happens to be a very basic and
ever-present one in security, so while it exists, it is one we can
readily work with.
7.    Design. Many experiments and research seem to have settled on the
following design pattern, which we call a Trident Design Pattern:
       Entropy collector  ----\
                               \ _____          _________
                                /     \        /         \
       Entropy collector  ---->( mixer )----->( expansion )-----> RNs
                                \_____/        \_________/
                               /
       Entropy collector  ----/
    In short, many collectors of entropy feed their small contributions
in to a Mixer, which uses the melded result to seed an Expander. The
high level caller (application) uses this Expander to request her random
8.    Collectors. After all the above bad news, what is left in the
software toolkit is: redundancy .
        a. A redundant approach tells us to draw our RNs from different
places. The component that collects RNs from one place is called a
Collector. Therefore we want many Collectors.
        b. Each of the many places should be uncorrelated with each
other. If one of these were to fail, it would be unlikely that others
also would fail, as they are uncorrelated. Typical studies of
fault-tolerant systems often suggest the number 3 as the target.
        c. Some common collector ideas are:
            * the platform's own RNG, as a Collector into your RNG
            * any CPU RNG such as Intel's RDRAND,
            * measuring the difference between two uncorrelated clocks,
            timings and other measurands from events (e.g., mouse clicks
and locations),
            * available sensors (movement on phones),
            * differences seen in incoming new business packets,
            * a roughly protected external source such as a business feed,
        d. By the analysis that got us past Rule  there are no great
Collectors by definition, as otherwise we'd already be using them, and
this problem would go away.
        e. An attacker is assumed to be able to take a poke at one or
two of these sources, but not all. If the attacker can futz with all our
sources, this implies that he has more or less unlimited control over
our entire machine. In which case, it's his machine, and not ours. We
have bigger problems than RNs.
        f. We tend to want more numbers than fault-tolerant reliability
suggests because we want to make it harder for the attacker. E.g., 6
would be a good target.
        g. Remember, we want maximum uncorrelation. Adding correlated
collectors doesn't improve the numbers.
        h. Because we have redundancy, on a large scale, we are not that
fussed about the quality of each Collector. Better to add another
collector than improve the quality of one of them by 10%. This is an
important benefit of redundancy, we don't have to be paranoid about the
quality of this code.
9.    Mixer. Because we want the best and simplest result delivered to
the caller, we have to take the output of all those above Collectors,
mix them together, and deliver downstream.
        a. The Mixer is the trickiest part of it all. Here, you make or
break. Here, you need to be paranoid. Careful. Seek more review.
        b. The Mixer has to provide some seed numbers of say 128-512
bits to the Expander (see below for rationale). It has to provide this
on demand, quickly, without waiting around.
        c. There appear to be two favourite designs here: Push or Pull.
In Push the collectors send their data directly into Mixer, forcing it
to mix it in as it's pushed in. In contrast, a Pull design will have the
Mixer asking the Collectors to provide what they have right now. This in
short suggests that in a Push design the Mixer has to have a cache,
while in Pull mode, the Collectors might be well served in having caches
within themselves.
        d. Push or Mixer-Cache designs are probably more popular. See
Yarrow and Fortuna as perhaps the best documented efforts.
        e. We wrote our recent Trident effort (AdazPRING) using Pull.
The benefits include: simplified API as it is direct pull all the way
through; no cache or thread in mixer; and as the Collectors better
understand their own flow, so they better understand the need for
caching and threading.
10.    Expander. Out of the Mixer comes some nice RNs, but not a lot.
That's because good collectors are typically not firehoses but rather
dribbles, and the Mixer can't improve on that, as, according to the law
of thermodynamics, it is impossible to create entropy.
        a. The caller often wants a lot of RNs and doesn't want to wait
        b. To solve the mismatch between the Mixer output and the
caller's needs, we create an expansion function or Expander. This
function is pretty simple: (a) it takes a small seed and (b) turns that
into a hugely long stream. It could be called the Firehose...
        c. Recalling our truth above of (c) CSRNs being the goal, not
entropy, we now have a really easy solution to this problem: Use a
cryptographic stream cipher. This black box takes a small seed
(a-check!) and provides a near-infinite series of bytes (b-check!) that
are cryptographically secure (c-check!). We don't care about the
plaintext, but by the security claims behind the cipher, the stream is
cryptographically unpredictable without access to the seed.
        d. Super easy: Any decent, modern, highly secure stream cipher
is probably good for this application. Our current favourite is ChaCha20
but any of the NESSIE set would be fine.
        e. In summary, the Expander is simply this: when the application
asks for a PRNG, we ask the Mixer for a seed, initialise a stream cipher
with the seed, and return it back to the user. The caller sucks on the
output of the stream cipher until she's had her fill!
11.    Subtleties.
        a. When a system first starts up there is often a shortage of
easy entropy to collect. This can lead to catastrophic results if your
app decides that it needs to generate high-value keys as soon as it
starts up. This is a real problem -- scans of keys on the net have found
significant numbers that are the same, which is generally traced to the
restart problem. To solve this, either change the app (hard) ... or
store some entropy for next time. How you do this is beyond scope.
        b. Then, assuming the above, the problem is that your attacker
can do a halt, read off your RNG's state in some fashion, and then use
it for nefarious purposes. This is especially a problem with VMs. We
therefore set the goal that the current state of the RNG cannot be
rolled forward nor backwards to predict prior or future uses. To deal
with this, a good RNG will typically:
            * stir fresh entropy into its cache(s) even if not required
by the callers. This can be done (e.g.) by feeding ones own Expander's
output in, or by setting a timer to poll the Collectors.
            * Use hash whiteners between elements. Typically, a SHA
digest or similar will be used to protect the state of a caching element
as it passes its input to the next stage.
        c. As a technical design argument, the only objective way that
you can show that your design is at least as good as or better than the
platform-provided RNG is the following:
            i. Very careful review and testing of the software and
design, and especially the Mixer; and
            ii. including the platform's RNG as a Collector.
12.    Business Justifications. As you can see, doing RNGs is hard! Rule
 -- use what the platform provides. You shouldn't be doing this. About
the only rationales for doing your own RNG are the following.
        a. Your application has something to do with money or journalism
or anti-government protest or is a CVP. By money, we mean Bitcoin or
other forms of hard digital cash, not online banking. The most common
CVP or centralised vulnerability party (aka TTP or trusted third party)
is the Certification Authority.
        b. Your operating platform is likely to be attacked by a
persistent and aggressive attacker. This might be true if the platform
is one of the following: any big American or government controlled
software, Microsoft Windows, Java (code, not applets), any mobile phone
OS, COTS routers/firewalls, virtual machines (VMs).
        c. You write your own application software, your own libraries
*and* your own crypto!
        d. You can show objectively that you can do a better job.
    Note that it is still a hard test, you want ALL of those to be true
before you start mucking around in this chaotic area.
That all said, good luck! Comments to the normal place, please, and Ed's
note: this will improve in time.

@_date: 2014-01-30 14:44:50
@_author: ianG 
@_subject: [Cryptography] Hard Truths about the Hard Business of finding 
Hi John,
thanks for comments!  I know we're opposed philosophically on this, so
it helps more than normal to have holes poked.
:) indeed.  That is why I had a stab at writing down the things.  I
think we're at a point where we can do that now.
Well, that's just it.  The people who manage the supply chain will
give up far faster than you.  However, our job as programmers still
goes on, and we have to deliver software that eliminates problems, not
comes with instructions like "don't give up!"
Good point, I copied that in almost as is.
Yes, that's an inevitable result of trying to compress our knowledge
into enough of a story and enough of a plotline to get people to the
end, before they get bored and wander away.
As a practical pedagogical technique, we must be bold and state hard
truths.  Subtleties will be lost and should only be introduced where
I agree with what you state, but this is why context is important.  It
is fine when we are in the 'hardware' domain to talk about calibration
and measurement, but in the 'software' domain we don't really have
that luxury.
Most developers in this area are in two worlds:  one is the world of
Ted and John who are developing a tool that is used, as is.  So it is
modelled to a 'perfect' design and delivered to a wide unsuspecting
unwashed audience.  Ted gets to lean on CPU RDRANDs and Johm gets to
lean on FIPS...  (This is not written for them.)
The other world is people like myself who write applications and have
to build a component that does the job.  So the requirement (audience)
is very precise, but the platform is highly variable.
In my field, today, I'm using android and bsd.  In order to do a
calibration I have to look through the app, through Java (and past JCE
of course) to the OS.  Then I have to look past the sandbox / VM.
Then I have to look at the devices to see what I can get from them.
Yet, I can't predict what phone my software will run on.  I can't see
what Oracle/SUN have done with Java.  I can't see what the Android
team has done with their OS (Linux, another black hole of wisdom) or
hook-up to sandboxes and Jave and whathaveyou.  Androids typically
have a sensors API but the sound is a very different proposal -- it's
supposed to be a phone!  The BSDs typically have a soundcard by no
sensors API.
There is no common hardware available, 'cept maybe the CPU and RAM.
Point, I made a note.
Not giving up, clearly :)  Going to an engineering solution.  I can't
buy or specify any form of hardware at all.  So I go redundant.  What
choice do I have?
It's reachable *if we can control the environment*.  Otherwise...
Yes, this is a good point.  I've made a note to stress that.
I think in a sense this is true.  But it is also a fact that people
like myself are too far from the hardware.  It matters not how we
strive to touch it, it's still too far away.
Rather than giving up, what is left then is the old engineer's trick
of redundancy.
Thanks again for comments!

@_date: 2014-01-31 09:34:17
@_author: ianG 
@_subject: [Cryptography] The crypto behind the blackphone 
Safety is a relative term used absolutely.  Secure phones aren't
secure if one enlarges the envelope slightly to include the owner, and
the crook standing next to her with a gun to her head.
Many will say "that's not our problem" but that also means they're not
doing security.  Security is measured by the results not the promises.
That would be a seriously interesting document.  Is there any such
thing around?
There used to be the NSA hardening guide for Mac OSX but they didn't
publish it after 10.4 iirc.  Another example of how they could help
industry and government, thwarted by some agenda issue, no doubt.
Right.  That's an economically efficient service.  Assuming a guide, I
can harden my phone, I suppose, but I haven't the time.  I'd rather
pay the premium and have it done, out of the box.

@_date: 2014-01-31 10:53:47
@_author: ianG 
@_subject: [Cryptography] cheap sources of entropy 
No, it is more like, people do not know if 4rot13 is good nor if Turbid
is good.  They hear noise.  But they are not scientists, and do not have
the time to sort through things.
Think of it like investing on the stock market.  Choosing good
investments is a risk business.  One can either become expert in stock
picking ... or one can invest-the-market.  Those who are trying the
former are fooling themselves that they can do better than the market,
those that are doing the latter are giving up on it.
It's turtles all the way down .. which is why things like the 'perfect
market hypothesis' that says inter alia that all knowledge is already
priced in is treated with both scorn and long term respect.
Your context -- which we might not see as a problem -- is that in this
analogy, you are the company.  You know your stock pick is good because
you have the inside info.
Nobody else has that.  So they are left with a choice -- pretend to be a
stock picker, lean hard on diversification theory, or become an insider...
Well, (a) that's an opinion, and (b) it makes incomparably more sense
until it doesn't.  The problem with the one true source that a
purchasing manager has is that it's one and true until it isn't, and
there is no way to get around that fact.  So when supplier fails, he
does what he typically does, and starts purchasing elsewhere.
Agreed.  But the purchasing manager will give up in seconds.  That's
Not sure I know how to load an appropriate driver on my android.  And
I'm pretty sure as a marketing droid that the old story of downloading
drivers is a dead duck.  In today's world, you get the platform and it
either works or it doesn't.  You don't download drivers, you get another
platform.  Attention cycles are short, and dealing with hard things like
crypto just causes eyeballs to roll.  "Oh no, not that 1990s crypto
nonsense again..."

@_date: 2014-01-31 10:55:57
@_author: ianG 
@_subject: [Cryptography] Hard Truths about the Hard Business of finding 
It's definitely a trade-off, we're all agreed here.  We are in
engineering space.
Yes, but the cost of that is very high.  Moderately well designed
equipment seems to run into o($1000), and top class equipment, add
another zero.  And that's before we add the much higher costs to program
and maintain the gear.  These costs aren't easily split across projects
(prop dev kits typically just push the problem around) whereas at least
costs generation by FLOSS stuff is more easily spread out.
Which is to say, we pay a high price for that supposedly better designed
security, and that justification is often not economic but is
Oh, that we had that problem!  Please!
The market will self-correct itself soon enough, serious security folk
will come out with more serious solutions.
Yes, *but* how many of those attacks are real?  Validated?  A clear and
present danger?  If the market were allowed to operate (which means, get
rid of the compliance millstone which breaks the economic equation),
then we would find out.  Then there would be a market for better gear.
Well, except that a cold-hearted analysis in the hard light of sunlight
has it that the entropy source is likely the low hanging fruit for a
built-in compromise to a high-value HSM.
As it is, the compliance model has killed the market for serious gear,
because the costs are so high that there is no compromise possible,
which forces the cost higher and less economy results.  It's a
self-defeating feedback cycle, so the only recommendation that makes any
sense is to avoid all compliance gear at all, and do the best you can
without it.
I agree with the theory of the compliance model.  I think it is however
evident that it has failed to deliver a workable and economic product to
Which, is shifting the burden.  The on-board testing is also subject to
all the other questions...
( As a minor quibble, the reseed requirement is derived from the goal of
a platform PRNG.  If one moves into app space (as my OP was about) then
there is more flexibility.  If one cares about a freeze&copy-state
attack, then the app can simply request new PRNGs on demand.  E.g., this
is what I do for key generation. )

@_date: 2014-02-01 02:26:28
@_author: ianG 
@_subject: [Cryptography] Unified resource on Random Number Generation 
I think there is interest in getting a complete picture.
Any posts I have written is released, attribution would be nice but hey,
it's the Internet.

@_date: 2014-07-08 13:16:31
@_author: ianG 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
Is your project a target?  Some more on this [0].
There aren't specific restrictions as such with security clearances [1]
but there are conflicts of interest.  If a person has a security
clearance, then they have a master or power.  If they are devoted to
your project, then this means they serve two masters, the best you can
hope for is that the other master is dormant.
That power can be used at will.  There are a range of pressures that can
be put on a person to assist the power.
There is a security reporting requirement.  Any issue of security
relevance has to be reported to the security officer of the
organisation.  This is deliberately vague, it might not be immediately
clear that this is of interest, but consider how bureaucracy and spying
works, and what the interests are in this.  In effect, your contributor
has a duty to report *anything*.
You need to model this from a security pov.  What is the worst this
person can do?  What does the presence of a security clearance do to
likelihood of some threat?  Is the threat there regardless?
Then, what can you do to stop it?  In a typical security project the
code is not committed until reviewed.  So does your review process
provide enough cover?  What happens if 2 or 3 of these people turn up
and start reviewing code together?  Does your review process stop that
breach as well as other breaches?
If the result of this is damaging, then you might want to consider
moving the person out of harm's way.  To him as well as to the project;
 if the person is really vulnerable, it isn't nice to go dangling
carrots before the spooks.  People can get hurt, the spooks will think
it nothing to destroy this person's career in order to get at some keys.
 Do you hate your contributors that much that you'll put them in harms
way?  Odd way to run a project ;)
You can never know that.  And, you can never know what you are exposing
the person to, as they won't be telling you.  All you can know is what
they will do if an opportunity arises.  That, you can get from reading
the better breed of spy novels, they are often based on real events.
[0]  [1]  I think this is true of all countries.

@_date: 2014-07-11 11:41:10
@_author: ianG 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
I would say, unless your FLOSS project is specifically a target, this is
probably true.
Right.  In at least one case I saw, the agent tried to keep the
relationship a secret, citing privacy concerns.  This was intentional.
Yup, it comes down to modifying your existing systems to cope with a
novel attack vector, more or less.  If they don't already cope with the
approximate attack then that's likely because you don't care.

@_date: 2014-07-11 13:20:51
@_author: ianG 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
Well, there are clearances that we do on our people, and the clearances
that our enemy does on his people.  We're talking about the latter, so
following your train of thought, we are dealing with (a) a signal of
something, and (b) people who are already compromised ... by the issuer
of the clearance, aka, the enemy.
Of course, compromise is a relative term, as is conflict of interest.
Yep, but one we can defend against these issues, if we take care.  As
we're in the security business, one would think we could also take care
of this issue.  It's just a variant of any other insider attack.
Sort of, maybe.  Actually, anyone infiltrating your project will set it
up so they don't need to tell you.
Very different thing.  You simply have to respond by making it mandatory
for them to state such things.  It's a common thing to have a policy
requiring conflicts of interest to be disclosed, indeed it is even law
in some circumstances.
Yep.  Precisely.  The agency attack is just a variant.

@_date: 2014-07-13 11:26:43
@_author: ianG 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
Right.  We're all arguing loudly in agreement -- dealing with a person
with a security clearance, who might have loyalty to an attacker --
should be treated as a variation of existing processes.
Yep, we can work with that.  So, the tactic is this:  Monitor the
packets coming in, and ask them if they have their evil bit set.
If the answer is YES (I have a conflict of interest) then you have
established a framework for a shared ethical response to future
problems.  This protects both you and the person.
If the answer is NO (I lie, I'm an agent trying to infiltrate) then you
have established a deception, which you can write down.  If your
processes are good, then you can establish future costs that mean that
an agent faces future risks of being exposed and run out of town
backwards on a donkey.
In CAcert [0] we collect transcripts of interviews that explore all
these issues.  We have processes to check identity.  We have it wrapped
in civil arbitration with the power to punish.  On the basis of the
original deception, the Arbitrator can blow open the case.
Obviously a spy can deceive and get in using a cover story.  But at the
end, we'll have a story to tell, and that spy will be blown.
Imagine his photo on the newspaper [1].  That spy's career is now over,
as a spy.  Think Valerie Plame, by whatever name.  If that spy has been
caught making a deception, then they are completely exposed.
Right, this is not about the FBI.  They are investigating crimes.  This
is about spooks, who are spying.  Very very different.
No, this is an error.  You're assuming that because there are some
attackers who will lie to you, then all attackers will lie to you.  This
is not the case.  Most spying is done by not lying, but by getting the
victim into a state of self-deception.  There are reasons for this.
So the tactic is to first make sure you don't self-deceive.  Block their
favourite approach.  Which means you have to address the possibilities
face on.  Require disclosure of conflict of interest.
If you develop this process, that also makes it harder for the other
attack (with deception) because you are taking records.  Which is an
increased cost to them, therefore a benefit to you;  they won't attack
unless they really really need to.
Right.  And, any attacker may be being deceived by the spook agency.  So
creating the framework for disclosure makes it harder for the spooks to
trick honest people, at all levels.  What they do in defence, FLOSS
projects can do in defence.
E.g., hypothetically, if you are in a project to write (say) NIST crypto
code, and there is a standard to which you have agreed that says "we
operate only to the benefit of our users" (say).
Then, the spooks have to get you to break that commitment.  This reduces
their potential attack surface area, because most people are honest, and
will be intensely conflicted if asked to slip a backdoor in.  Even if
you agree (the Chinese spies have compromising photos...) your chance of
making a mistake goes up immensely, and your time to being caught goes down.
Right, we can't stop them lying.  But we can make it non-cost-effective.
[0] [1] we don't take photos as yet.  We should...

@_date: 2014-07-15 00:03:34
@_author: ianG 
@_subject: [Cryptography] VCAT report on NIST's process review 
Thanks for that.  A quick look at the slides indicates that this is a
good step forward, a way to review what went wrong.  There but for the
grace of the cryptogods go we all...
Is there audio?  I don't usually listen to audio, but this one is really

@_date: 2014-07-16 00:55:41
@_author: ianG 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
In CAcert it is the former.  Every person who provides 'critical' input
must go through a process.
To a large extent this is because CAcert has always been a target, and
the evidence on this has been clear.  On reflection, in no other org
I've ever been involved with has this been an issue.

@_date: 2014-07-17 10:42:37
@_author: ianG 
@_subject: [Cryptography] What has Bitcoin achieved? 
Yep, I'd say so.  Bitcoin has broken the drought of respectability for
these efforts, and VC money is pouring in.
But, if any VCs are reading today, do the opposite of what crinkle did.
 And google:
A problem with financial cryptography is that it is too easy to make a
huge belief play.  And belief plays lead to disaster.  Huge cap can make
huge plays on huge beliefs, and nobody can tell them otherwise.  In this
case, even a NYT journo can spot the mistake google made.

@_date: 2014-07-17 22:33:46
@_author: ianG 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
Right, we get the security clearance signal confused from an offense and
defence pov.
I would adjust that slightly:  *If you are a target*, you have to
respond, or be owned.  The typical method is to to do a background check
Different community.  Here, in this mail list, we write crypto security
code.  Not necessarily FOSS?  Many commercial companies face the same
Indeed.  Security clearances are a signal, only.  What is a signal?  It
is a message that is interpretable and misinterpretable.
That said, it represents a very good foil from which to construct a
conversation, leading to an ethical approach.  You only have to ask.
That's all we do, we ask:  "do you have a security clearance?  Do you
have any relationship to police?  Intelligence?  Law?  So, tell us about
that?  What responsibilities?  Is there a conflict of interest here?"
[0]  Quite what you do, how you do it, what to look for, ... is beyond
scope.  But if anyone's got a real issue here, feel free to ping me.

@_date: 2014-07-20 09:16:23
@_author: ianG 
@_subject: [Cryptography] The role of the IETF in security of the 
the net?
This has never been obvious, at least not to the IETF WGs, or more
broadly I suspect, any committee approach.

@_date: 2014-07-20 10:02:26
@_author: ianG 
@_subject: [Cryptography] attacking algorithms with substitution 
h/t to cryptogram: Cryptology ePrint Archive: Report 2014/438
Security of Symmetric Encryption against Mass Surveillance
Mihir Bellare and Kenneth Paterson and Phillip Rogaway
Abstract: Motivated by revelations concerning population-wide
surveillance of encrypted communications, we formalize and investigate
the resistance of symmetric encryption schemes to mass surveillance. The
focus is on algorithm-substitution attacks (ASAs), where a subverted
encryption algorithm replaces the real one. We assume that the goal of
``big~brother'' is undetectable subversion, meaning that ciphertexts
produced by the subverted encryption algorithm should reveal plaintexts
to big~brother yet be indistinguishable to users from those produced by
the real encryption scheme. We formalize security notions to capture
this goal and then offer both attacks and defenses. In the first
category we show that successful (from the point of view of big brother)
ASAs may be mounted on a large class of common symmetric encryption
schemes. In the second category we show how to design symmetric
encryption schemes that avoid such attacks and meet our notion of
security. The lesson that emerges is the danger of choice: randomized,
stateless schemes are subject to attack while deterministic, stateful
ones are not.
Category / Keywords: secret-key cryptography / Algorithm-substitution
attacks, big brother, kleptography, mass surveillance, symmetric encryption

@_date: 2014-07-20 10:18:06
@_author: ianG 
@_subject: [Cryptography] The role of the IETF in security of the 
the net?
Categorically, there is no single way in which a process like the IETF
could be perverted ;) Concentrating on one particular weakness such as
trusted proxies will allow people to argue it hasn't happened (here)
therefore the IETF is not corrupted (at all).  That's as pointless as
saying the onus is on the accuser to provide the evidence.
Instead, we need to concentrate on the process, and develop
understanding of methods and cases [0].  Which is what Paul tried to do.
 Anyone can submit a draft, anyone can talk to ADs.
That defence is easy to subvert, I claim.  Anyone can do it, but that
anyone must have money.  Not money to pay bribes but money to pay for
people time.  An organisation with a big budget can pay for people to go
to the IETF events, submit a draft, edit, improve, sit on the mail
groups to push the agenda, spend a term as AD, etc.
Hence, we can suggest that IETF WGs are vulnerable to takeover by rich
organisations.  Who would find that a ridiculous claim?  Who's voice is
dominant in the IETF security WGs?  The large American corporations that
pushed PKI always?  Or the many American banking customers who got
phished because it didn't work?  Clearly, the former.  The latter can't
afford it.
E.g., a recent case in point was a discussion on algorithmic agility
which I engaged in at saag and tcpinc.  It was *expensive* ... the
discussion bounced back and forth between groups, with procedure and
claims of 'consensus' being used as weapons by incumbents.  I spent a
lot of hours!  Which I cannot afford!  In the end, the emerging fresh
anti-consensus was more or less slapped down, but it also seems that the
push to encode algorithm agility into RFC got stalled.
The winner will be the one who spends more resources.  The winner won't
be the merits and benefits of either approach.  IETF Security Working
Groups are no different to the blockchain, the security WGs are
vulnerable to a 51% takeover by the NSA mining cartel, only the GRU
mining cartel and the PLA can save us.
I have no idea whether they've done that.  But it's a no-brainer to me
that they would try it.  They've got the budget, they've got the
incentive, they've got the expertise.  What haven't they got?
If I was in the NSA, I'd be thumping the desk.  "Get our people in there
and get our agenda set.  Dammit, 350 algorithms isn't enough!"
IETF process is set up for more benign players.  It assumes commercial
organisations who are roughly honest, won't push so far into deception,
criminality, etc.  Submarine patents would be as far as they go, because
it's in the law, it's their right.  We all want to make money, and money
will eventually flow to the better solution, right?  We can all agree on
a consensus on money, right?
Spooks are a whole other ball game.
[0] Which is why the DUAL_EC case is so important, and hats off to NIST
for conducting a fullsome and documented enquiry.  We need those
developed cases.

@_date: 2014-07-20 10:34:57
@_author: ianG 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
Anyone with a security clearance is vulnerable to being manipulated into
an attacker.  The reason for this is that a security clearance is more a
contract, they are more bound to the organisation;  e.g., it is their
duty to report activity to their security officer, which leads them
almost immediately into a breach of either security rules or your FLOSS
If your org is targetted, then the risk of this person being used goes
right up.    So, the existence of the security contract might place the
person in a position of vulnerability from both sides, which has to be
managed carefully.  Most FLOSS projects care about their people, and
would not want their contributors to be put in harm's way.
So in very general terms, I would say:  if your organisation is
targetted, and the person has a security clearance with an aggressive
agency, then the person should not be put in harm's way.  Disqualified
from working on security critical areas.
In order to avoid privacy being a weapon in this way, we place the
process before arbitration, which is typically 'sealed' that is recorded
but closed off to public view.
Yes, I would say so.
In my experience, a miniscule number of FOSS projects have a clue about
security.  And a miniscule number of those practice hard security.

@_date: 2014-07-22 10:51:48
@_author: ianG 
@_subject: [Cryptography] The role of the IETF in security of the 
the net?
It's curious, isn't it.  To argue perhaps contrarily:
The essence of good security design is to (a) solve what problems we can
in software, completely, and (b) provide tools to the users to solve the
ones we can't do automatically.
In the early days of the net, we were told which algorithms we were
allowed to use.  People started shipping 'export' versions and
'domestic' versions.  Which opened the door to algorithm agility,
today's hobby horse, into which everyone piled in their philosophical
So we (a) solved the protocol parts in software and (b) kicked the
choice of algorithm up to layers 8 & 9.
The result was disaster.  It wasn't just SSL.  IPSec suffered from
over-engineering-by-committee.  OpenPGP failed to move forward;  it
spent 10 years in RFCland, trying to cater for the fashion trade in
algorithms, amongst other things like five ways to say one.
The heart of the matter is the committee design:  it finds itself having
to let in many little devils in order to please everyone, including
people not in the committee.  In order to reach consensus, too much is
accepted;  is rough consensus the solution?  Or the problem?
Let me speculate on TCPinc.  Clearly, TCP that bootstraps a DH key
exchange in the startup phase and then encrypts packets with a
ChaCha20/Poly1305 suite will work to knock out mass surveillance.  It
will be small enough and tight enough to be provable and fit in a kernel
without angst.  Easily codable.  Popular.
It meets all useful criteria *except the committee one*.
Just as clearly, I speculate that the above will not survive the IETF
working group process.  In order to get that rough consensus, everyone
must be pleased and everyone must get their favourite thing in there.
There's already 4 proposals, and at least two of them are "my favourite
kitchen sink..."
We know of two paths to success:
1. the dictatorial, or "trusted" path, being we have to use this one, or
the General told us so [0].
2. the competition, or the no-holds-barred knock-out [1].
The open consensus process may have done its dash, the recent successes
have used the competition process.
The challenge for the IETF especially security Area is to face up to
this dilemma.  Maybe time for IETF to change the rules, or the game.
Shooting from the rough should be part of game, not a way for insiders
to push people out into impossible positions.
[0] John Kelsey pointed out some successes where decisions were made by
a tight team.  Unix by 2 guys, Linux by one, Bitcoin by one, SSH by one,
etc.  TCP wasn't originally written by committee, it was written by a
company under contract to DARPA.  Indeed cryptography has many lessons
in single directors:  DES, SHA1, SHA2.  The ChaCha family.  Mostly a
positive story.
[1] AES is the great example.  SHA3 is another.  CAESAR is coming.

@_date: 2014-07-22 11:11:47
@_author: ianG 
@_subject: [Cryptography] The role of the IETF in security of the 
the net?
Hi Stephen,
I'd say this is a difference of perspective.  Of course, anyone who pays
green fees and wears the right shoes is welcome to take part.  And if
they end up being pushed out into the rough, well, no doubt that can
happen to anyone, the rules are fair, right?
(For others:  Russ' draft promotes algorithm agility.  After some degree
of skepticism that this is always good, a rewrite ensued that ...
promotes algorithm agility.  To some of us, this was a surprising
result, so a new rewrite would be welcome.)
The discussion is not over with Russ' draft, but the discussion is over
with TCPinc.  The charter still says
    "must employ algorithm agility"
even though Russ' draft did not survive that same assertion.
This stinks of club politics.  No willingness to give grounds on the
direct important issue, even though it is clearly in play in the more
philosophical arena of saag/Russ ... seems like pushing the rebels out
into the rough, while the local champion is eased forward.
I personally tackle it in the time-honoured way - theft.  I steal time
from other activities that own my time.  It works until various
supervisors, SOs and other nosy parkers start quizzing me on the
unproductive time spent there, and the time not spent writing code...
So, likely I won't be doing so much more.  The point against algorithm
agility has been made, and I cannot justify more expenditure if others
in IETF are going to outspend me, *especially* if the message is that
politics are being used to get what is desired.
John Kelsey's post on the 'volunteer' nature of the net is well taken,
individual volunteers are easy to push around, and corporates can be
focussed.  Either the IETF WGs are corporates in volunteer's clothing,
or it's too easy for it to slide that way.
How do we address this?  Well, I reckon the only answer for an
organisation like IETF is to look to competitions with winner take all.
 But they have to be open competitions, and the rules have to be open,
not stacked in advance such as is happening with TCPinc.
(The dictator approach solves the design mess, but it doesn't solve the
fight for power within the WG, indeed it probably makes it as bad or worse.)

@_date: 2014-07-22 11:12:13
@_author: ianG 
@_subject: [Cryptography] What has Bitcoin achieved? 
This is both true and false, in measures :)
There are *some people* who want this, and there are *many people* who
don't.  Those who've spent time analysing the market (aka marketing)
will know that the former is the minority and the latter is the
majority.  This is a well-studied feature of most markets, it's the core
of the concept of 'early adopters'.
The situation is that Bitcoin finds itself in the market of early
adopters, those who rebel and try new stuff.  The big question is how to
cross from the early adopters to mainstream.  And deal with the
time-honoured truth that the mainstream do not give more than
titillating lip service to the notion of freedom from central control.
Which is something that marketeers and early adopters alike often
mistake for confirmation of beliefs.
It's not personal.  It's just People, it is repetitive.

@_date: 2014-07-22 15:25:49
@_author: ianG 
@_subject: [Cryptography] The role of the IETF in security of the 
the net?
Hmm.  I see the flaw.  For a competition to work, we need a dictator to
decide the winner.  And a deadline.
Then I suggest -- naively indeed -- that the IETF designate external
non-IETFers as the jury to decide the winner.  Use a conclave method,
lock them in the church until they come out with a winner. Bread & water
only, small pieces of paper to burn...
You're right.  They'll never do that.  Outside it is then.

@_date: 2014-07-23 15:24:03
@_author: ianG 
@_subject: [Cryptography] hard to trust all those root CAs 
This is the sort of argumentation that was used in the mid 1990s :)
Basically:  the threat exists, therefore we must defend against it.
The problem with this approach is that it ignores costs.  If it costs
something to defend against it, and that cost is in the aggregate
greater than the value saved, then this is not a good use of our money.
 We should accept the risk, and pay the cost when it happens.
Back in the 2000s when phishing was debated-not-fought, Pinning was
basically rejected by Browser Vendors, in line with everything else.
It's got a new lease of life because 'one of their own' was hit by
phishing, and is now experimenting with solutions.
My point is, the solution is whatever the 4 majors want to do here.  It
isn't anything to do with security, it's more to do with getting the
gang of 4 to move together and find a lowest common denominator.
By definition, no "perfect" solution is possible because there is no
"perfect" definition of security.  The best we've come up with is a menu
of risks, and you get to choose which ones you pay to slow/stop, and
which you can live with.  Like the insurance game or options trade, it
isn't wise to over-insure.
Unfortunately, much security thinking was bedevilled by the fake
universe approach:  Construct a perfect security model, construct your
threat model to be beaten by your security model, and market the threats
An interesting question is why we should think like this...  but I digress.
It's worth being a little bit more scientific about this whole threat
thing [0].
Before 2003, there wasn't much of a threat, just isolated incidents, and
we could have ignored it ('accepted the risk').  If 2003 means anything,
up until that point HTTPS was indistinguishable from a placebo.
In 2003, phishing started up against HTTPS, but it was ignorable because
it was outside the envelope.  The users carried the costs, no response
In 2011, things warmed up.
My answer:  17 years was spent defending against a threat that didn't
exist or could be ignored.  When it finally turned up, the machinery for
dealing with the threats was so sclerotic that it couldn't respond any more.
(In military affairs this is called the Maginot Line syndrome.)
[0]

@_date: 2014-07-25 10:29:02
@_author: ianG 
@_subject: [Cryptography] hard to trust all those root CAs 
What's the charge here?  Making a false statement?  Failing to
cooperate?  Taking down an untruth?
What's the prosecutor asking the court to order the defendent to do?
Lie for the government?  Stop interfering with an investigation that
didn't exist?
The court will have a hard time of finding a charge of lying on behalf
of the prosecutor, or of ordering the defendent to participate in an
active deception.  Truth is the meat & drink of the court, when the
court plays games with the truth, the court impeaches itself.

@_date: 2014-07-26 20:55:24
@_author: ianG 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
2c worth,
I thought you were protecting email?  What rational can there be for
having two strengths?
Email is primarily hacked on the machine, and 2^256 is so far beyond
reasonable that we won't see it challenged for a long time.  If you
don't like that argument increase to 2^512 but it still doesn't support
having two strengths.
5)  Choose one.  Get back to work...  I would use curve25519 as it's
much more clearly open than Microsoft's stuff, I don't need to go
researching it, and I know there are plenty of open source code snippets
to draw from.

@_date: 2014-07-27 12:05:17
@_author: ianG 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
Ah, well, if you're also doing long term public roots then you probably
want the top end.  And use that everywhere else.

@_date: 2014-07-27 12:23:02
@_author: ianG 
@_subject: [Cryptography] hard to trust all those root CAs 
Ah.  So if you publish you are in breach of your contract (your
problem), if stop publishing you are in breach of the law.  Got it.
And in general, contempt of court, because you deliberately put yourself
in a situation to break the law.

@_date: 2014-07-31 10:00:32
@_author: ianG 
@_subject: [Cryptography] [cryptography] Browser JS (client side) crypto 
No, you're prioritising an active attack as more frequent and more
harmful than a passive attack.
As we now know, passive attack is a certainty and active attacks are rare.
(The question of 'harm' still lacks data...)

@_date: 2014-06-01 14:45:38
@_author: ianG 
@_subject: [Cryptography] What is going on with TrueCrypt? 
Actually, I wanted to compare the original 2.6 as referenced in that
post you linked to.  To declare my colours, I wanted to see if the 'Red
Hat legal people' were as bad as they evidenced themselves to be.  But
without the other license, we are left with only self-evident bloopers
like this:
   " While Fedora certainly has no intent to commit
     copyright infringement, our counsel advises
     *that licenses are promises not to sue*. "
(my emphasis)  Contracts and licences are the roadmap for disputes, they
are *by definition* not promises to not sue but are rather the agreement
under which the suit is held.
(I came across these people with another matter.  The way they advised
their client had to be seen to be believed, their misunderstandings of
contracts in particular and business in general were bald-faced.)
obCrypto:  crypto projects that don't understand the law are doomed to
waste a lot of resource.  C.f., 'Digital signing' is one such historical
trainwreck that misunderstood the law of signing, one famous chat system
has unintended consequence of slam-dunk entrapment, and 'contracts' in
the p2p finance world is an emerging herd of blindfolded lemmings.
Yes, I got that impression too.  I have no beef with that because, last
I checked, I'm a living eating programmer, and I don't like stealing
bread for my table.  I agree they could have said it more clearly, but
then, life is not always kind to the hard-working programmer.
If you don't understand the lawyer, then you've not moved an inch from
that fate.  The advantage of not using a lawyer is that the
responsibility rests with only one fool.
Might be.
iang J.D. U.Grisham.

@_date: 2014-06-01 15:21:38
@_author: ianG 
@_subject: [Cryptography] Is it mathematically provably impossible to 
Hi Stephan,
Thanks for that, that was a perfect walk-through of the logic.
Is this also sufficient to prove the halting problem unsolvable?
Something like:
   /**  TRUE if this program C will halt */
   boolean f(Code C) {
        ...
   }
   void haltNowIf () {
        return ! f(C);
   }
I agree.
Sure, but the recent rash of suspicions concerning open processes in
security & crypto does give one pause for thought.
:)  Perhaps the interesting thing about that is that Ralph Merkle
probably learnt that lesson well.  Yet when a group gets tripped by this
logic, do they learn it?  Does anyone else?
thanks again,

@_date: 2014-06-02 12:48:26
@_author: ianG 
@_subject: [Cryptography] DOJ Wants to Expand Authority to Break Into 
Please help!  The way I read this, it means a warrant would allow them
to hack across borders.  So, tit-for-tat:  when the PLA decides to hack
DoJ's computers, or Target's computers, or IBM's computers, or Lockheed
Martin's computers, it can simply get a warrant from the People's Court
 of Beijing and cite that to the DoJ.
This seems to give them an asymmetric result far out of Doj's favour,
why would they pursue this?
Anyone?  What do I not get here?
Secret vulnerabilities?  NSA whispers, or their own zero-day collection?
 Oh my?!
The way I read this, they have now de facto authorised every other
justice department to start collecting zero-days and use them against
USA corporations.  So next we see the gang of 5 PLA generals in Chinese
court to collect their warrant.
I can see an advantage here that this might defer the tit-for-tat arrest
in 5 NSA generals in their next vacation to visit the great sights of
China ... but other than that, it again seems again like a net loss to DoJ.
And, when the execs of those countries are fronted before court in
another country, what is the defence?
Is the next thing we are going to see arrests of employees (American and
Chinese) in China for hacking.

@_date: 2014-06-02 18:10:14
@_author: ianG 
@_subject: [Cryptography] What has Bitcoin achieved? 
Having been in the biz for a while, I feel your pain.  It is
counter-intuitive.  I've always thought that the job of financial
cryptographers was to steer clear of carnival tricks.
Why did it work?  How did it work?  (Take it from me -- it succeeded,
and it is probably unstoppable at this stage.)
1.  The success of Bitcoin is based in part on the bounty that was
delivered to everyone, weighted to early adopters.  Quite what to make
of this is something harder to say;  detractors call it a ponzi scheme
and proponents call it fair rewards.
It is worth noting that Paypal did something similar.  When it took on
its first investment, it gave out $10 to each account opened.  I know
someone who tried to open a thousand accounts... later on the bounty
dropped down to $5 per new account.
2.  We can definitely achieve the same *tech result* without the waste.
 I've been doing it all along, and so have a few others here (James for
one).  But again, notice how the waste was cunningly turned into rewards
that are paid by new adoptors (see 1).  This is a neat trick.  I always
valued clear and honest transactions;  I never would have credited the
mining rewards and bubble mechanics as a credible proposal;  but the
market speaks.  FWIW, new designs are tending towards "proof of stake"
because there is begrudging recognition of the waste, but only Ripple
has really reduced it down to the levels that we technologists would say
are reasonable.
3.  The marketplace of early adoptors is a suspicious lot;  they want to
be able to do things completely at their own discretion.  Bitcoin
appeals to them, they think others can't control them.  The same
anti-control aspect appealed to the blinded crowd in the 1990s.
4.  Also the marketplace of early adopters wants geewhizbangfireworks.
Back in the good old days, that was the blinding formula.  So crypto, so
sexy, so little undersood!!  Many thousands flocked to its call, but
few, maybe a handful? a dozen? understood that beneath the light and
fireworks, blinding delivered only a vague handwave of untraceability.
It didn't matter, the promise was everything.
We've seen this 'marketing-stretch' in other aspects of successful
crypto systems as well.  Same today with the magic blockchain and PoW.
Now compare your 32 notaries design.  Where's the light?  The bang?
Compared to smartcontracts, blockchains, p2p, silk road .. you've got
These generalisms are repeated in the copy-cat issues that followed
after Bitcoin.  Each repeats most of the above factors in some sense or
other.  Success does not go to the best technical solution, it goes to
the best marketing solution.
For that, we have a problem;  how many of us understand marketing?

@_date: 2014-06-03 09:44:44
@_author: ianG 
@_subject: [Cryptography] Fork of TrueCrypt 
Auto-update is a key feature in keeping the user-base secure.  It's the
only way to get the roll-out of major critical security bug fixes out in
o(month) as opposed to never.
Right.  So this is one of those subtle unsolvable equations.  You can
possibly judge patches as being secure, if you see them.  So for you you
might get better security by sticking to what you know.
But for the masses, they don't look, they don't upgrade.
It might come down to who TrueCrypt is for;  as you asked at the top
c.f., Vision.
Does a higher base security for most justify a lower absolute security
for a few?  This is a question not a few devs have had to wrestle with.

@_date: 2014-06-04 12:11:07
@_author: ianG 
@_subject: [Cryptography] What has Bitcoin achieved? 
[snip, not with disagreement]
I just spent a lunchtime hearing about the new LEIs or legal entity
identifiers that are being standardised.  They are nationally based, you
have to apply and maintain a sort of D&B History, and you can then mark
your documents with new LEIs which allow anyone to look up the pedigree.
In contrast, in my work, we just hash the document.  The document has to
establish its own pedigree.  If you haven't got the document, why not?
Once you've got the document, you can know that the tx relates to it.
OK, it takes a bit of thought to wrap ones head around the idea of
anti-permissive crypto-computer-science.  But once you do, the effect is
liberating.  Meanwhile, the fintech industry is embarking on yet another
global push to build a standard which will consume thousands of
man-years to just interface.
So what do we do?  I say, let them go ahead.  Do the TCP/IP thing, and
build a better system for us, those who want to work with better
systems.  Let them do their ISO7 thing.
People who are trying to push Bitcoin into banks, etc may just be
wasting everyone's time.
I'd say the major thing holding it back is the huge costs they have
imposed on themselves in order to create a legacy industry that knocks
out new entrants.
Right, it is all in your compromises.  Bitcoin disposed of Eve as a
threat model in order to deal with Sybil and Trent.  Maybe there's a
sort of ZT here in that any one system can deal with only two of Eve,
Sybil and Trent, pick your poisonous bedfellow.
My work specifically deals with having Trent as the compromise.  Seen
from the Bitcoin perspective, issuances in my world are 'limited'
because they are intermediated ('accounted') through a server.  As a
partial nod, the number of issuances is unlimited, so the number of
servers are unlimited.  This is decidedly unwasteful.
Now, there is another hidden assumption which I must surface:  there is
an issuer.  In Bitcoin, there is no 'issuer' of record, nobody who
stands behind a contract.  This means that all Bitcoin value (and
copies) is limited to supply & demand games;  this is the "copy the gold
idea" trick.  It was pulled off once or thrice, but it isn't a trick you
can copy more than a few times.
For most of finance, we want actual people and actual promises standing
behind the issue.  Sure, for cash, we might not (or we can handwave
around gold or fiat or BTC).  But for anything else, it has a semantic
component that must be documented, a contract.  And a reputation.
So, once you've got the reputation, you've also got the Trent.  It might
be that you can handwave around the accounting, but if I capture your
Trent then your contract is unmeetable, barring a rewrite of millenia of
(How to take BTC and the Trent-free context and turn that into
waste-free zone is a topic of much discussion.  I think I know how to do
it, but need to be sure.  This is alchemy, the experiments can blow up.)
Right, in my context, I use overlapping cells as well.  Note also that
once you have a cell, the possibility of local Trents is more plausible.

@_date: 2014-06-04 12:20:51
@_author: ianG 
@_subject: [Cryptography] What has Bitcoin achieved? 
If there is one thing that SSL got right (and I'm pointing here to
success comment above) it was that it was a simple upgrade to the TCP
concept/interface.  There was nothing *directly new* to grasp.  All the
implications were submerged.
Cash issued by anyone except the government is beyond most people's
minds, but it works and exists, it has existed for a long time, and you
don't have to go far to find it.
The point I think that you're missing here is that people accept that
which works.  In Kenya they have a thing called mPesa which is mobile
money on the phone.  It works.  It is issued by a telco.  Nobody there
really questions it (except the banks) ... because it works.
If there is any mystification, it surrounds why Kenya has it and few
other countries seem to.  But that doesn't matter, not to them.
I'm hoping it is the other way around, crypto-technologists are slowly
starting to understand that this "user" thing means more than just
scrambling and unscrambling data...
There's some sort of twitter competition on the top 5 retailers they
want for Bitcoin.  They've already lost;  retail is by far the worst
idea and sector for a digital money.

@_date: 2014-06-04 12:51:08
@_author: ianG 
@_subject: [Cryptography] Fork of TrueCrypt 
It's very very hard selling crypto security.  Most or all efforts have
failed.  The ones that succeeded sold something else (eg Skype), or sold
to a particular niche which ain't real people (eg Silent Circle) or some
other A/B provision (eg Bitcoin) or weren't selling what they said at all.
Been there, done that, ate the t-shirts.  You have to give these guys a
*lot* of leeway for them to find a business model that keeps them
eating.  And that means getting off the crypto-privacy-at-any-cost

@_date: 2014-06-04 14:54:56
@_author: ianG 
@_subject: [Cryptography] To what is Anderson referring here? 
Hmmm... good ideas, let's analyse, fwiw?
I would call the RSA comment perverse but not entirely inaccurate.  RSA
patent was a hugely influential force in the choice of SSL/RSA/certs in
the 1994 timeframe.  This model was imposed more from a marketing pov
(RSADSI had a patent to sell..).
The result was phishing.  The perverse part here is that phishing didn't
start up until around 2003, by which time the RSA patent was no longer
an issue.  I think it is a stretch to blame the patent for the phishing
thing, it's a factor, but it isn't a central factor.
FWIW, I never heard of patents being a problem in all my observations of
the phishing wars.  It's best explained by an institutional &
competition framework in deadlock with the CA-WG-CabForum-vendor
primaries, and others around the edges;  as is described frequently any
time someone asks.
The second comment in that paper by Anderson [0]:
    (the other is the network effect created by the
    two-sided market in servers and clients; no merchant
    wants to change its web server if it would lose even
    a few percent of web browsers).
I wouldn't disagree with as an issue, but I would say it doesn't get to
the nub of the problem, because there are things that can be done that
don't effect the two-sided market;  and they weren't done.  And in some
cases they were rolled back (the yellow bar, the independent audit).
I'd say the two-sided observation is more an excuse that was frequently
rolled out by those who were incentivised to not fix it.
Again, I'm unaware of anyone pushing that.  I grant that the patents
might have knocked it out as a solution.  But where there's a will,
there's a way;  if people really want the solution, we've generally
found a way.  So, skepticism.
[0]

@_date: 2014-06-04 15:05:22
@_author: ianG 
@_subject: [Cryptography] It's GnuTLS's turn: "Critical new bug in crypto 
Well, yes and no.  Ted starts off from the premise of the market for
lemons (Akerlof) but he also walks a path on how such a market moves to
more like a Spence market, where the perception of the article becomes
the product and not the article itself.
I call the Spence space a market in silver bullets [0] although I might
arrive it via a different path.
I will note that economics is difficult.  We may laugh and gawk at how
they mucked up recent times, but their work can be deep [1].  The
winning ideas can be picked up from wikipedia easily enough, but if one
is going to build on the ideas then one is wise to read the actual
papers to get a real view of what people like Akerlof said and also
importantly, did not say.
[0] quick read:
long read:
has detailed snippets from the actual relevant primary literature, as
opposed to passing around fruity comments around for lolz and credits.
[1] obCrypto is the Austrian literature which informs the cryptcurrency
world.  Or not, it's frequently and disastrously conflated with
libertarian literature, which isn't sound enough.

@_date: 2014-06-04 23:21:06
@_author: ianG 
@_subject: [Cryptography] What has Bitcoin achieved? 
Depending on ones interpretation of that document, it is the states that
have that power, not the Federal.  But we might segue into another civil
war here...
Maybe that's what you mean by 'ahistorical' :)  Anyway, recent forms of
non-government issued money exist in Ithaca, Las Vegas, e-gold, Liberty
Dollars, those canadian tyre coupons, the tokens at the shops that sell
iTunes records, air miles, these are just from memory.
Right, the 'free banking period'.  There is more recent research that
debunks the notion that this was unstable, there is a fair correlation
between the free banks that failed and those that were required to hold
state bonds which brought them down.
Right, that's what I meant.  Most people assume the government issues money.
Full agreement.  If you give people a working money product, they're
fine with it, generally.

@_date: 2014-06-05 13:07:10
@_author: ianG 
@_subject: [Cryptography] To what is Anderson referring here? 
phase commit.  Chaum's was ok.  But also, Wagner was pretty good too, as
it made up for some shortcomings because it was patent unencumbered.
There were three others on the list at the time, but we concentrated on
BRN -> Wagner -> Chaum -> Brands.  BRN is just BigRandomNumbers which
act the same way so the software can shake out.
Again from memory, this was a deeply researched topic back in 98-01 or
so until we actually started fielding blinded cash and discovered that
... there wasn't so much interest any more.
So my conclusion is this:  the blinding formula (patent) actually played
more of a role as a signal than as a barrier.  99% of the people you
deal with won't see the difference.  Not seeing it as a signal that
could be bypassed meant that 99% of the activity was wasted.  By
bypassing the signal and finding the real ways around it, success was
Today, the blockchain is the signal, but it isn't the business.  You'll
note the same fervour for "must be blockchain" as back then, "must be
Zerocoin again sits at the hilarious intersection of these signals and
is not getting as much attention as either did, purely.
Yes, precisely.  There is always another way.  Back in the day, to avoid
blinding patent, we used psuedonyms that allowed you to create multiple
accounts, which coupled with low entry barriers and distributed issuers,
made for reasonably strong privacy.  That could also be combined with
blinding if one was keen enough (had the patent, or alternative).  These
days you would want to mix blinding with blockchain, as blockchain has
published ledger, which is pathetically traceable.  c.f., Zerocoin.
However, while this cypherpunk focus was fun and interesting back then,
things have changed.  In order to ensure privacy we have to also now
conquer KYC/AML.  Which means we have to use more advanced 2000s designs
to integrate in the requirements, and focus at the institutional level,
not the transactional level.  Once this is in place, the old ideas about
transaction privacy look a little quaint.
Back to patents.  There is always another way.  In cryptography, RSA is
the only patent that ever made a lot of money, as far as I am aware.
Yet, even with that, there was another way:  DSA & ElGamal.
I agree with the general thread.  Someone needs to do a long term study
showing the relationships between patents, value to owner, value to
society and cost to society of crypto patents.  It's probably a PhD
topic in econ.

@_date: 2014-06-05 13:31:12
@_author: ianG 
@_subject: [Cryptography] To what is Anderson referring here? 
Just for clarification, I wasn't trying to imply you were perverse --
but that the link is perverse.
Right.  But the original quote is this:
       "A security-economics example is the
       thicket of conflicting patent claims on
       authentication protocols, one of the two
       main reasons we?ve been unable to improve
       browser security and deal with phishing...."
Tie RSA to phishing?  So, yes, RSA patent was a big issue in IETF WGs,
we all recall the noise.  But not in secure browsing, which relied on
TLS which had RSA and it got DSA/ElGamel patent free suites as well.
According to the noise, RSA+certs *was the solution to authentication*.
Which was what was broken by phishing.
I think the original claim needs evidence.  Yes, there is a link or
links.  But there wasn't any direct noise connecting the two, and there
was a lot of noise to choose from.
Probably what we could do is construct an argument that goes thusly:
Patents allowed RSA to be controlled.  This allowed RSADSI to force
Netscape into adopting certificates for SSLv2.  This paid RSADSI a huge
bounty, and created another huge bounty, Verisign.  This then created an
industrial standards-complex that locked the certificate into the
authentication hole of the puzzle for all time for all secure browsing
for all muggles.
When the cert/PKI/TLS/x509/secure browsing/RSA royalties/architecture
was breached by phishing, the industrial standards-complex that
stretched from WG to vendor to CA to NIST to parts undiscovered was
unable to respond.
As the ISC was built on and required a patent as the keystone, then it
could be said that patents are to blame for phishing.
But it's a longish story.  Most people will not understand it, not want
to believe it, prefer to keep their jobs and talk about other things. So
I called it perverse :)

@_date: 2014-06-05 13:44:10
@_author: ianG 
@_subject: [Cryptography] To what is Anderson referring here? 
RSA as I mentioned.  The RSADSI accounts are probably available, and it
was a history of some interest, showing literally the fortunes of the
patent over time.  Verisign was a public company, and reached into the
many billions in market cap, and its existence was based more or less
entirely on the ability of the patent to force Netscape to add RSA into
Just to quibble, I'd say it is PhD.  You actually need to do quite a lot
of research, gather quite a lot of evidence.  You'll need to construct
various theories and then collect the data to show which of the theories
better explains the various observations.
Also because it is basically competition theory and business results
(micro) I'd suggest it be done at a business school where they are
actually interested in this sort of interaction.  You need the hard
competition econ with a take-no-prisoners attitude.
But I agree with the plea -- this would be of huge interest.  And, we
now have enough data on the topic (time) to give us an empirical base.

@_date: 2014-06-05 14:36:19
@_author: ianG 
@_subject: [Cryptography] To what is Anderson referring here? 
I disagree -- but perhaps we need some cites?
A nice example of an industrial-standards-military-complex franchise.
Right.  But also there is the negotiation cost, which is typically
E.g., as mentioned, I once was doing blinding payments.  In order to
secure a licence for a patented blinding algorithm I once spent 4 trips
requiring intercontinental travel.  I got the agreement in principle.
In order to turn that into an actual contract would have required that
again, as a prediction.  So the costs to me would have been something
like 10-100k depending on your cost-base, just for the negotiation alone.
When I see the "this algorithm is patented" I generally feel like for
that sort of negotiation cost, I can design the work-around...  as a
And that's before we get to the pricing.  I once heard that RSADSI made
50c on each copy of windows that was shipped -- anyone know the truth of
moral:  never negotiate with the industrial-standards-military machine,
you'll just waste your money.  There's always a way around...

@_date: 2014-06-05 14:43:24
@_author: ianG 
@_subject: [Cryptography] What has Bitcoin achieved? 
Welcome!  I've been gnawing at these things for so long I have no limbs
Yes.  iiuc, they are IMHO a hack over the top of Bitcoin.  This might be
OK as long as it works well enough, as long as the various flavours play
well together, and if your biz is not sensitive.  But I'm not sure that
these assumptions are reliable.  I've heard the dev team is not
favouring the alternate use of the blockchain.
Also, the token control is not good as yet.  There are one-way pegs
where one destroys bitcoin and gets granted new value, but the idea of
going backwards is a bit fraught or absent.  There is one venture that
is trying to develop the idea of the two-way peg, but I don't know if it
can (again, the core dev team problem).
This is perhaps the reason why Ripple and Ethereum and others decided to
fork entirely;  I also feel that there are many difficulties, and there
isn't the emotional support you would need from the center in order to
base a serious business on.  I know you understand these things because
you saw how much code was in Digicash;  you can't just write a biz
code-base of that size on the hope that the blockchain will let you play
It also depends massively on what the business is;  it turns out that
large parts of the market can operate "off-blockchain" which basically
means "we're not doing Bitcoin but we're doing Bitcoin."  Did I mention
Yeah, I get that.  I've been chewing on it for only a few months ;)  I
recently lost my notes, got them back again, must pick up.

@_date: 2014-06-06 12:19:45
@_author: ianG 
@_subject: [Cryptography] What has Bitcoin achieved? 
Bitcoin payments are hugely expensive, it's just that the payer doesn't
pay that price.  Instead, it's spread across the buyer's fee and an
inflation tax.  If the system were static, the fees would amount to
maybe $30-60, about the same as banks internationally.  Part of the
problem is that the mining rewards are incentivised upwards, a race to
the most expensive.  When the mining rewards stop there will be
interesting times/effects, because all that hardware has to be paid for
Does that feel somewhat like credit cards, where the buyer doesn't pay
to get the free month's credit?
Irreversible transactions are charge a 100% fee when you make a mistake.
 What is your percentage error rate, and what support is there in the
system to help that?
The debate goes back and forth on reversing transactions.  Cash works
fine and has no forcable reversion.  Mobile money tends to have it;
digital cash tends not to have it.
What my answer would be is that there has to be something to deal with
the dispute.  With e.g., e-gold, there wasn't anything realistic ("we
take court orders..."), and it didn't survive in part because of
specific complaints.  In contrast WebMoney had a very sophisticated
internal dispute system and it did survive.
In USA about 40 years ago that was true.  It's still true in many parts
of the world.  In some places they use cash or mobile money ...
Amen to that!

@_date: 2014-06-06 12:58:28
@_author: ianG 
@_subject: [Cryptography] Is it mathematically provably impossible to 
This is a point worth stressing, although Bitcoin fanboys are going to
be disgusted.  For the Bitcoin design, government is THE threat.  For
everyone else -- including all the users -- it is only one of the threats.
For this reason, Bitcoin has little or no support for user
authentication.  Bugger all in the way of application security (hence
the rash of cold wallet startups).  A joke when it comes to governance
features (e.g., everyone bought into Mt.Gox until it failed and now the
learning begins).  A privacy approach that is a giggle, published
psuedonymous ledger, oh my.
And this is before we get to the exotic systemic threats such as the
Gresham's effect.
Why?  Because THE threat excluded from minds all the other threats that
people might worry about:  theft external and internal, merchant and
exchange fraud, privacy & snooping, etc.
Bitcoin community is full of people who think that THE threat is THE
model.  They're wrong, but learning is only one theft away...
I read somewhere that there have been 30 major Bitcoin thefts involving
values greater than a million bux.  Bitcoin is a community of beginners
losing money to idiots and fraudsters.
So what do we do about it?  The solution I think is to invent a new
generation (c.f. Ethereum, clean piece of paper) that is focused on a
more user-oriented package of security and values.  And in the short
term, use Bitcoin for its working abilities.  But migration is needed in
the medium term, being 3-5 years.

@_date: 2014-06-08 20:54:06
@_author: ianG 
@_subject: [Cryptography] Aggregate signatures 
Look at what your user base has.  There isn't a cryptographic community
out there that has a million people and is ready to sign.  So your first
question at a crypto level is likely to be, how to distribute keys to
everyone.  Which leads to, how to ensure that everyone is anyone.
Hence, I'd say that your comment above, that a list of individual
signatures doesn't work, might be premature.  And, looking for
algorithms would be even more premature.
Tell us more about how and why you want this...

@_date: 2014-06-08 20:56:18
@_author: ianG 
@_subject: [Cryptography] ADMIN (sort of): Opportunistic TLS now turned on 
It's important to eat ones own dog food.
As an aside, I've always been disturbed by maillists that publish all
the headers.  IPs and clients and the like.  I don't see the need for
this in discourse, and if there are disputes, the moderated list solves
How easy is it to filter all that cruft out?  Is there any value in it

@_date: 2014-06-09 11:00:38
@_author: ianG 
@_subject: [Cryptography] one last thought for today on formal methods... 
What in practical managerial terms would be a recipe for achieving that?
 I mean in words like read this book/ do this masters/ download this code...

@_date: 2014-06-09 11:18:37
@_author: ianG 
@_subject: [Cryptography] Back door competition for TrueCrypt fork? 
As an alternate idea, CAcert has a process for addressing the risk of
secret cells, amongst others.  The issue I would see is that it is very hard to write a backdoor.  The
adversary manages to do it because that's all he is interested in, and
he is paid to do it.  You however are interested in writing clean code;
 time spent on a backdoor is time not spent on writing new clean code.
Above you are asking for a backdoor ever week :)  That seems way too
much load.
What devs could do is to write and escrow a disclosure file that could
be empty, or could have a backdoor documented in it.  Then, at some
agreed time such as one month before release, all of these could be
opened up and any undetected backdoors cleaned out.
In the disclosure file there should be a statement words to effect "this
file contains the only back doors known to this developer, the code
written contains the best efforts at user security."
It is useful to get a legally binding statement in place.  This will
block the attempts of most surreptitious plants.  It won't block the
downright criminal, but your own country's spies are generally trained
not to do things that will land them in their courts, especially if
those things might be illegal.

@_date: 2014-06-09 10:52:29
@_author: ianG 
@_subject: [Cryptography] Aggregate signatures 
If you are talking about cryptocurrencies, then I'd guess you're working
with contracts.  At least, that's what I do if talking about 'aggregate
In that case, you don't need any crypto for 2nd and mass parties to sign
the statement, directly.  You just need to quote the hash of the
contract in the business request you are doing, which as a consequence
of the request is signed.  We can then interpolate the acceptance of the
statement (contract) from that.
As you are quoting the contract in your own signed/identified request,
moving value around, you've (most of) the elements of the contract covered.

@_date: 2014-06-09 18:35:49
@_author: ianG 
@_subject: [Cryptography] WG Review: TCP Increased Security (tcpinc) 
A new IETF working group has been proposed in the Transport Area. The
IESG has not made any determination yet. The following draft charter was
submitted, and is provided for informational purposes only. Please send
your comments to the IESG mailing list (iesg at ietf.org) by 2014-06-15.
TCP Increased Security (tcpinc)
Current Status: Proposed WG
Technical advisors:
  Stephen Farrell Assigned Area Director:
  Martin Stiemerling Mailing list
  Address: tcpcrypt at ietf.org
  To Subscribe:   Archive: The TCPINC WG will develop the TCP extensions to provide unauthenticated
encryption and integrity protection of TCP streams. The WG will define
an unauthenticated key exchange mechanism. In addition, the WG will
define the TCP extensions to utilize unauthenticated keys, resulting in
encryption and integrity protection without authentication. This is
better than plain-text because it thwarts passive eavesdropping, but is
weaker than using authenticated keys, because it is vulnerable to man-
in-the-middle attacks during the initial unathenticated key exchange.
This work is part of the IETF effort to evolve the Internet architecture
given the latest events of pervasive monitoring (see BCP 188).
The goal of this WG is to provide an additional security tool that
complements existing protocols at other layers in the stack. The WG will
be looking for the designs that find the right tradeoff spot between
conflicting requirements: to provide reasonable security for the
majority of connections.  This work will deal with unprotected
connections, and therefore will focus more on improvements from a
baseline of no security than on achieving the high standard of security
that is already available to users of authenticated TLS.
Providing unauthenticated encryption and integrity protection at the TCP
layer will provide a set of features that cannot be achieved with
existing tools.
Those features include:
- encryption and integrity protection without modifications to the upper
  layers (no API changes),
- encryption and integrity protection with forward secrecy with a
  per-connection granularity,
- simple NAT and firewall traversal capabilities,
- key rollover without significant impact to the TCP connection,
- lower overhead compared to solutions relying in stacking multiple
  protocols to achieve different features,
- no manual configuration required.
A more detailed description of the motivations for TCP-based solutions
can be found in draft-bellovin-tcpsec-01 and in RFC5925.
The working group will produce documents specifying the required TCP
extensions and additional documents needed.
The high-level requirements for the protocol for providing TCP
unauthenticated encryption and integrity protection are:
- It should work over the vast majority of paths that unmodified TCP
  works over, in particular it must be compatible with NATs (at the very
  minimum with the NATs that comply with BEHAVE requirements as
  documented in RFC4787, RFC5382 and RFC5508).
- The protocol must be usable by unmodified applications.  This effort
  is complementary to other security protocols developed in the IETF
  (such as TLS) as it protects those applications and protocols that are
  difficult to change or may even not be able to be changed in a
  backward compatible way.  It also provides some protection in
  scenarios where application developers are unwilling to change their
  applications (e.g., by configuring encryption) solely for the sake of
  improving security.
- The protocol must provide cryptographic algorithm agility.
- The protocol must gracefully fall-back to TCP if the remote peer does
  not support the proposed extensions.
- When encryption is enabled, it must at least provide protection
  against passive eavesdropping by default,
- Any required TCP option should use a minimum amount of TCP option
  space, especially in SYN segments.
- The protocol must not require any authentication or configuration from
  applications or users.  However, hooks for external authentication
  must be made available.  The WG will not work on new authentication
  mechanisms.
- The protocol must have acceptable performance, including acceptable
  latency and  processing overheads.  For example, the protocol may try
  to re-use existing cryptographic material for future communication
  between the same endpoints to avoid expensive public key operations on
  connection set up.
When encryption is enabled, then the protocol:
- must always provide forward secrecy.
- must always provide integrity protection of the payload data (it is
  open for discussion for the WG if the TCP header should or should not
  be protected).
- must always provide payload encryption.
- must not provide extra linkability: when encryption is enabled, the
  TCP traffic should not give a third party observer any extra way to
  associate those packets with the specific peers beyond information
  that would have been present in a cleartext session.
- must allow the initiator of the connection to avoid fingerprinting:
  some initiators may want to avoid appearing as the same endpoint when
  connecting to a remote peer on subsequent occasions. This should
  either be the default or some mechanism should be available for
  initiators to drop or ignore shared state to avoid being
  fingerprintable any more than would be the case for a cleartext
  session.
Security features at the TCP-level can benefit other TCP extensions.
For example, both Multipath TCP and TCP Fast Open require proof that
some connections are related.  Session resumption and Message
Authentication Codes (MACs) can provide this evidence.  The working
group should identify synergies and design the security protocol in such
a way that other TCP efforts can benefit from it.  Of course, TCP
extensions that break must be identified too, and kept to a minimum.
The working group will produce the following documents:
- A framework for unauthenticated encryption and integrity protection of
TCP connections. This document will describe basic design
considerations, including the motivation and the applicability of the
proposed mechanism, the interaction with other security mechanisms in
different layers of the stack, the interaction with external
authentication mechanisms, the expected protection, privacy
considerations and residual threats.
- Definition of the unauthenticated key exchange mechanism and the
extensions to current TCP to utilize unauthenticated key to provide
encryption and integrity protection. This covers all the protocol
changes required. This will be an experimental document.
- An extended API describing how applications can obtain further
benefits of the proposed extensions. In particular, the hooks for
supporting external authentication will be defined in this document.
This will be an informational document.
Tcpcrypt mailing list
Tcpcrypt at ietf.org

@_date: 2014-06-11 11:01:00
@_author: ianG 
@_subject: [Cryptography] Bitcoin compute power (was Re: Aggregate 
Got spare rig?  Say hello to market timing attacks.
    Bitcoin Verification Latency
    The Achilles Heel for Time Sensitive Transactions
    Griffith & Grigg
    Abstract. Bitcoin has a high latency for verifying transactions, by
design. Averaging around 8 minutes, such high latency does not resonate
with the needs of financial traders for speed, and it opens the door for
time-based arbitrage weaknesses such as market timing attacks. Although
perhaps tractable in some markets such as peer to peer payments, the
Achilles heel of latency makes Bitcoin unsuitable for direct trading of
financial assets, and ventures seeking to exploit the market for
financial assets will need to overcome this burden.

@_date: 2014-06-11 19:06:12
@_author: ianG 
@_subject: [Cryptography] Subject: Re: Swift and cryptography 
It should *not* have a JCE/JCA or any other form of lock-in for
pre-supplied crypto or algorithms or security protocol.
(opinion. discuss.)

@_date: 2014-06-15 14:13:04
@_author: ianG 
@_subject: [Cryptography] Dual EC backdoor was patented by Certicom? 
In what is now a long running saga, we have more news on the DUAL_EC
backdoor injected into the standards processes.  In a rather unusual
twist, it appears that Certicom's Dan Brown and Scott Vanstone attempted
to patent the backdoor in Dual EC in or around January of 2005.  From
Tanja Lange & DJB:
   ... It has therefore been identified by the applicant that this
method potentially possesses a trapdoor, whereby standardizers or
implementers of the algorithm may possess a piece of information with
which they can use a single output and an instantiation of the RNG to
determine all future states and output of the RNG, thereby completely
compromising its security.
The provisional patent application also describes ideas of how to make
random numbers available to "trusted law enforcement agents" or other
"escrow administrators".
This appears to be before ANSI/NIST finished standardising DUAL_EC as a
RNG, that is, during the process.  What is also curious is that Dan
Brown is highly active in the IETF working groups for crypto, adding
weight to the claim that the IETF security area is corrupted.
Obviously one question arises -- is this a conspiracy between Certicom,
NSA and NIST to push out a backdoor?  Or is this just the normal
incompetent-in-hindsight operations of the military-industrial-standards
It's an important if conspiratorial question because we want to document
the modus operandi of a spook intervention into a standards process.
We'll have to wait for more facts;  the participants will simply deny.
One curious fact, the NSA recommended *against* a secrecy order for the
What I'm more curious about today is Certicom's actions.  What is the
benefit to society and their customers in patenting a backdoor?  How can
they benefit in a way that aligns the interests of the Internet with the
interests of their customers?
Or is this impossible to reconcile?  If Certicom is patenting backdoors,
the only plausible way I can think of this is that it intends to wield
backdoors.  Which means spying and hacking.  Certicom is now engaged in
the business of spying on ... customers?  Foreign governments?
In contrast, I would have said that Certicom's responsibility as a
participant in Internet security is to declare and damn an exploit, not
bury it in a submarine patent.
If so, what idiot in Certicom's board put it on the path of becoming the
Crypto AG of the 21st century?
If so, Certicom is now on the international blacklist of shame.  Until
questions are answered, do no business with them.  Certicom have
breached the sacred trust of trade -- to operate in the interests of
their customers.

@_date: 2014-06-15 19:16:59
@_author: ianG 
@_subject: [Cryptography] basing conclusions on facts 
For my part, I had seen his name only with respect to IETF WGs.  However
I admit that I do not follow IETF security WGs closely, so am not
qualified to assert "highly active."  You are right, I am wrong.
I had a long post addressing this issue, but as it takes us further from
the subject at hand, I'll pull my head from out of the rabbit hole.

@_date: 2014-06-15 20:00:39
@_author: ianG 
@_subject: [Cryptography] What has Bitcoin achieved? 
This is not really true, I'd say this is an example of western rich
country bias.
In the western/rich zone, they have courts, police and customs that work
to a fashion.  This is a long standing tradition that dates back
centuries.  So much so that, as a result, the people themselves trade
according to higher principles, which might be summarised as "thou shalt
not steal."  Else, you're nicked, or fear of it.  And we don't believe
it can be any other way.
However, this is not the case in all places or all times, nor is it
especially the case in those places where we assume it.  For example, in
the west, there are plenty of examples in the financial system where the
incumbents are robbing the victims blind, and regulators, police and
courts stand by without lifting a finger.  Cognitive dissonance?
And in the poorer zones or the non-first world, the existence of courts
and police can be a curse.  The system can work differently.  It might
be bakshis all the way to the bank.
So, when the cryptocurrency community looks to facilitate trade on the
Internet, our sometimes adoption of a regulator/police/courts-free
assumption might actually be closer to the reality of more people than
you or I rich western geeks would like.
That is to say, the question is pretty darn relevant, and claiming it is
just a bunch of anarchists who think that way is missing the real picture.
(Leaving aside totally whether the bitcoin answer is worth anything.)

@_date: 2014-06-16 13:34:22
@_author: ianG 
@_subject: [Cryptography] [cryptography] Dual EC backdoor was patented by 
I guess this would be true if one is in the EC world choosing curves.
Patently, a view expressed in the act by DJB and Tanja.
But this is about international standards and an approved way of doing
RNGs.  A rather different kettle of fish.  We in the user community were
supposed to be able to implement a standard like DUAL_EC, perhaps get it
approved, and be done with such crapola.  Or buy an approved product,
and ditto.
One would have thought that NIST, ISO, etc had long since got tired of
the notion of all that good work being done for the public benefit, only
to be snaffled by greedy patent trolls for the price of a filing.
Although it is now historical as the DUAL_EC RNG is withdrawn as a
standard, I think it would be very interesting to hear NIST's views.  It
may not be submarine in some technical lingo, but it rather seems to be
asymmetrical to the standards horizon.
I wonder if NIST knew about the patent?
Indeed.  I'm fascinated to understand Certicom's business thinking.
What is the business model behind patenting backdoors?

@_date: 2014-06-17 21:56:41
@_author: ianG 
@_subject: [Cryptography] Implementing constant-time string comparison 
That's what I do.  My code below, fwiw.
    /**
     * Constant Time Equality of two byte arrays.
     *
     * Constant time is only reliable to the extent that the arrays
     * are of the same length.
     * (It could be organised to be CT if one of them is null, but that
     * seems like broken code to rely on such a thing, and any trickery
     * might not survive the JIT.)
     *
     *  one is a byte array of any length, else null
     *  two ditto
     *  true if the arrays are byte/bit-wise equal,
     *         true if both null, false if one only null,
     *         false if different lengths
     *  webfunds.util.Equals which is more comprehensive and
     *       adds noisy()
     */
    public static final boolean ctEquals(final byte[] one, final byte[] two)
    {
        if (one == null && two == null)                  return true;
        if (one == null || two == null)                  return false;
        if (one.length != two.length)                    return false;
        int xorDiffs = 0;
        /*
         * Run through the entire array(s), and XOR them to highlight
         * any differing bits.  This is the contant time bit.
         */
        for (int i = 0; i < one.length; i++) {
            xorDiffs |= ((int)(one[i] ^ two[i])) & 0xff;
        }
        /*
         *
         * Original NaCl C code turns any zero into -1, any bits into
         * zero flipped those back to zero or -1 respectively,
         * so a definition of 0==equals, -1==differs was returned.
         *      return (1 & (((int)diffBits - 1) >>> 8)) - 1;
         * The problem with this is that we have to turn it into
         * a boolean here, so the CT maths don't gain us anything,
         * and even in C, the caller is likely to do the same thing...
         */
        return 0 == xorDiffs ;
    }

@_date: 2014-06-17 22:04:29
@_author: ianG 
@_subject: [Cryptography] Help please, 
as you're asking for sanity on a crypto list, anything goes ;)
Concur most vigorously.
I'm really not sure how the word simplicity can be honestly used in the
same sentence as X.509.  Nor can I think of any advantage found in using
X.509 except when dealing with the borg that is legacy code.  And that's
no advantage, that's a term of service.
My advice, bite the bullet.  Do your own format now.  It will save you
You intend to do roll your own for the 3,4.  So I suggest you just start
with 3,4 and migrate upwards with the same formats ...
Personally, over beers, I'd say forget JSON.  Roll your own binary, you
will anyway.  But that's really personal preference.
What happens when Alice loses control of her lifelong key?  How does she
encourage others to switch to a new one?  Can she sign on to her own new
lifelong with the old subroot?
And, can an attacker do the same?

@_date: 2014-06-18 11:16:26
@_author: ianG 
@_subject: [Cryptography] Implementing constant-time string comparison 
The problem is (I suspect) the optimiser.  If it can see back from the
boolean comparison, and into the loop, we run the danger of having
wildly different timings.
In C, you can do the neat trick with maths which an optimiser can't be
expected to reduce;  in fully typed languages, no such help applies
because we want a boolean, so from that boolean, optimisation can tease
its way in.

@_date: 2014-06-18 18:55:54
@_author: ianG 
@_subject: [Cryptography] [cryptography] Dual EC backdoor was patented by 
I wouldn't be surprised if this was it.  Or, they claim it is the reason
in PR.  Basically, a company that patents everything in the space they
can think of, without deep analysis or oversight.
  "Hey boss, I just invented a weapon of mass crypto destruction!"
  "Top gun!  Patent it quick, here's a bonus!"
Michael said:
Right.  That would be first order analysis, there's a market, let's
patent into it.  Second order analysis would be to ask ones customers
what they think about such a proposal, and try and and work out how it
fits with everything else.
Third order analysis would be to ask about the correlation between
ex-customers and those who were asked about the backdoor strategy.
The term of art for this sort of sales action is conflict of interest.
The product that Certicom are trying to patent and eventually benefit
from raises a conflict of interest with ... just about all of their
  "Wait, you knew there was a backdoor in our RNG since 2005 and waited
for the patent to clear before telling us?"

@_date: 2014-06-19 12:18:41
@_author: ianG 
@_subject: [Cryptography] [cryptography] Dual EC backdoor was patented by 
Yes, that is what a patent is.  To prevent someone using an invention so
that a royalty can be extracted.
Right.  But that is commercial/competitive hardware, not open software
standards.  In software standards for the Internet, the emphasis is on
making a design freely available to the public.
The consensus is that the specs should have no patent encumbrance, and
can be freely used.
And, exploits are announced, damned, and fixed.  Not hoarded.  5 eyes,
we're looking at you...
And, it doesn't work to try and sue the industrial phishing machine.
Wow.  Funny side story:  the European smart card labs knew about this
stuff well before CR.  They kept it all secret, didn't patent anything.
 And, lost business across to the upstart.
Big mistake ;-)

@_date: 2014-06-20 15:31:23
@_author: ianG 
@_subject: [Cryptography] What has Bitcoin achieved? 
Take a block.  Calculate the amount of money spent on the mining of that
block, which you can use 25BTC as a proxy for.  Divide that by the
number of transactions in the block.
The result varies.  When BTC was in the 1000 range, it was up to 60 per
transaction.  These days it seemed to be around 30.
Shortly after Ken posted the paper showing this, blockchain.info stopped
showing some of the info, ... oh, but it's back now:
  Cost per Transaction:  $41.68. And a graph:

@_date: 2014-06-20 15:37:16
@_author: ianG 
@_subject: [Cryptography] "Is FIPS 140-2 Actively harmful to software?" 
Just for good measure, I say the same thing about the entirety of the
Audit process:
  In seven parts.
Yes, commonly managed by a new industry body with all the same players
in it.  Singing, "meet the new boss, same as the boss!"

@_date: 2014-06-20 17:27:58
@_author: ianG 
@_subject: [Cryptography] What has Bitcoin achieved? 
Fair?  What does that mean?  Just curious, do you mean, calculating a
cost that way seems inapplicable, or do you mean that there is a notion
of what would be 'fair' in a cost allocation sense?
One could see things that way, sure.  I'm just trying to be pragmatic here:
What does it cost to run the entire machine?
How is that cost distributed?  Who pays, who is compensated?
Well, when we establish a cost, someone has to pay it.  Then, we can
then build a marketing myth around why Joe-customer should pay that
cost, as a participant, a consumer.  Idk that I give a lot of attention
to such a myth.  "It's the security you deserve..."
I think I'm more interested myself in comparing the costs of various
machines and seeing where the market can be improved.  We can compare
the cost of say the credit card machine (4%?) as against the Bitcoin
machine (25BTC/block) as against the world wide slow SWIFT machine
(20-60?) ... and so far we're not seeing a lot of difference.
Which means happy days -- there is fantastic opportunity to improve the
matter.  I reckon I can do it for pennies ;)
Right, so the myth is that the money comes from nowhere, sure.  It's
free.  Don't worry your pretty little head over it.
The reality is that there is huge latent pent-up demand for open payment
systems from the general public, and people are prepared to pay for
that.  Currently the exchange rate seems to be say 5% which is pumping
up a lot of exchange business, and the demand is overwhelming other
factors such as the cost of mining.  So mining 'appears' to be free, but
in fact it is only cheap while the system is growing and demand is
pushing up the price so as to overwhelm the cost-transfer accounting.
Well, if the system can be run at scale without the mining reward, e.g,
when it switches over to fees-based only, then we can also hope that the
cost will drop dramatically.
But wait -- if so, why did we need the miners in the first place?  It
was to stop the double spend, right?  Or the false spend, to include a
wider degree of ills.  So as long as the miners are doing their job on
this on a fee-basis then it is fine.
(idk the answer to this, I've basically ignored fees and distribution of
same to this point, what's the story?)
Ah, now you're speaking to my heart!  Developers are taken for a ride on
this one.  There was a sort of proxy reward for the original developers
in that those close to the code could mine most.  But that was so rough
a connection that it is pretty well wiped out within months (I speculate).
Now we have the problem that the dev team is the SPF or CVP for the
whole darn network.  And they're not funded at all, directly.  This is
why ripple and ethereum and others have started foundations before hand
and pumped value into them -- so as to ensure the survival of the dev
team.  Meanwhile every startup is eating out the core of the dev team
and bidding the developers up through the roof.  This does not end well.
 Am I the only one who sees this???
ps; SPF == single point of failure, CVP centralised vulnerability party.

@_date: 2014-06-20 17:34:06
@_author: ianG 
@_subject: [Cryptography] "Is FIPS 140-2 Actively harmful to software?" 
I disagree with your pessimism.  The common factor for FIPS, Mont Blanc
and Dutch dykes-with-roofs is this:  government.
And the whole politician effect.
Back in the market world, we don't need no stinkin' grandstandin'
politician to promise us software security while sliding the 5 fingers
into our back pocket to pay for it.  Vote  if you are American...
The open security market is a market in silver bullets.  FIPS, CC,
audits, etc are just more silver bullets.
What seems to speak in this market is two things:
    * market results:  how many customers have been protected from how
many attack(ers) .. and for that, we can get a reasonable estimate just
by looking at the market (by which I mean, standard marketing
calculations, not cryptothink).
    * name.  And, while we're on the meme of signals, if there is one
reliable signal in a world of silver bullets, it is:  rep.  I trust
certain guys to get it right.  I trust James to get a good payment
system, I trust Jon to do a good comms system, and I'll back PHB on the
mail thing, altho I disagree on the premise.  DJB & Tanja impress on the
ciphersuite.  Zooko on file systems.  Etc.
Those guys will secure the herd.  If any of them start talking FIPS or
CC or ISO or ETSI ... then my faith goes down.  And the herd better
watch out.
ps; and I'm not so keen on IETF WGs either ;)

@_date: 2014-06-21 13:40:12
@_author: ianG 
@_subject: [Cryptography] [cryptography] How big a speedup through storage? 
If you're thinking of an agency that has at least a modicum of economics
sense, then the target of choice is RSA or EC.
Although interesting to cryptographers, and their oxymoronic use of the
term 'broken' nobody spends an ExaByte resource on a single AES key and
gets to keep their job afterwards...

@_date: 2014-06-21 15:02:49
@_author: ianG 
@_subject: [Cryptography] Code Spaces has been under DDOS attacks and... 
DDOS and breach extortion has been going on for the longest time in the
gaming world.  There, the costs can easily be quantified because they're
making real money, hand over fist.
This doesn't apply to the startup world who are on a shoestring and
their value if ever found is in some IPO or buyout.  Which is
untouchable really.
So, what is going on in the attacker's head that made this a good use of
their time?
When I think of cloud I assume there will always be some form of control
panel.  And this will be protected by a password.  Which leaves me
pretty vulnerable to a complete attack.
So I guess it all depends on whether you are in a business where this is
an unacceptable risk?
To answer your top question, it is about securing a control panel.  If
it is a password, then the answer is simple -- don't use passwords.  Use
If it is such an important asset, then it needs to be protected by a
token that the server can't forge or lose (has Amazon been hacked?
sure...) and also that cannot be easily stolen from the owner company.
So you need some form of secured device that has private key to drive an
interface that is otherwise unbreachable.
Take online banking.  Because the PC/browsers aren't secure against
e.g., phishing and viruses and sandbox attacks, the standard advice
would be to use a computer that is used for nothing else, isn't
connected to the network, and has a browser that isn't used for any
other thing.

@_date: 2014-06-21 18:20:28
@_author: ianG 
@_subject: [Cryptography] Spaces in web passwords 
Costs of crypto, yes.
It's not a technical problem but a human/economics problem.  People
don't recall when they typed a space.  Spaces are hard to write down.
Spacekeys are more likely to bounce than others.  Some software decides
to trim spaces.  Or add spaces.  Or change over to that other form of
space, the tab.  Or UTF.
So, because spaces tend to cause password problems, they cause more
headaches.  We can solve headaches at the support desk by ... banning
In some services businesses, the entire cost is ... the number of
support calls per user.  Lost password is typically the leading support call, which is why there are so many automated password
replacement services/ideas.

@_date: 2014-06-24 10:34:48
@_author: ianG 
@_subject: [Cryptography] What has Bitcoin achieved? 
Well, the term 'issue' is generally to imply that one stands behind the
contract, in the sense of 'issuer.'  In this case it is Safaricom that
is the issuer of the unit, and they stand behind their contract (to the
extent they do, never really looked at those aspects).
The fact that the contract is a one-to-one derivative of the KSH doesn't
make it not-an-issue, but rather a derivative issue.  It's still a
separate issue, because or perhaps indicated by the escrow fund they
place with one of the banks.
The question then might arise, who is the issuer of Bitcoin, and what is
the contract?  Well, Bitcoin is like fiat in that there is no contract,
only demand factors.  Who the issuer is, is more like gold; there is
none, although one could claim it was NS and now Bitcoin dev team, who
can adjust the t&c by means of software.  This is an arguable claim
though, mostly specious.
Right, moneygram/WU is a form of transferring bank money.  mPesa however
is a separate issue.  You can transfer mPesa only from phone to phone
(actually, SIM to SIM).  To get bank money you have to convert or redeem
your mPesa.  To get mPesa you have to buy it from one of the many
agents.  Once bought, you can then take it to anywhere and sell it (buy
something) as long as your counterparty has a phone.  You can't do that
with moneygram.
Average Kenyans don't do bank accounts, right.  Last I checked, the
price for a bank account is about the same as for western banks, so no
surprise there, about $5 per month, which is 1.5 days average pay.
Oddly, the banks in Kenya have enjoyed a surge in accounts, essentially
riding the wave that mPesa generated.  They should not complain.
But that suggests why it succeeded in Kenya ... not why it was so hard
to push into other countries.
The reason is the banks.  Once they saw what had happened in Kenya, they
made darn sure it couldn't take off in other countries.
Coming back to Bitcoin, the same resistance will be seen with banks.
Same thing happened with other 'alternative' currencies before they
muscled into becoming mainstream (PayPal, mPesa).  It's a strategy, not
an accident.

@_date: 2014-06-24 11:15:24
@_author: ianG 
@_subject: [Cryptography] Almost decentralized currency 
Precisely.  The end result is that Bitcoin's design has moved the
problem around, it hasn't removed it entirely or at all.
  Pre-BTC cryptocurrency business was all about that.  Don't ask
that question around us old-timers, you'll get odd looks ;-)
Once you have a trust fabric (to use some unfortunate jargon) then
everything changes.
But, changing these environmental factors also changes the requirements.
 It is a truism that you should base your protocol on the existing lines
of trusted communication;  Bitcoin assumes none which informs its
radical design.  Indeed it could be said that the only requirement it
has is to be fully defended against centralised attack, and as a
corollary, no trust at all is assumed.  This is fine in a sense, but it
doesn't mirror the real world.
Yes.  If you are dealing in networks of smaller internally trusted
groups then you maybe don't need the blockchain at all, because they
already trust members internally.  Use that trust, run a small
centralised system, locally.  Then, your challenge is to bring the
groups together.  What do they want?  What does relating to other groups
BTW, a lot of this is not about crypto.  It's far more about
understanding what the groups are and do, and molding the crypto to
assist that functionality.  You can't just assume the groups behave like
some paper on Sybil attacks and then go find that plastic world.

@_date: 2014-06-24 11:54:51
@_author: ianG 
@_subject: [Cryptography] "Is FIPS 140-2 Actively harmful to software?" 
There is no practical or reasonable audit in our space where this is
even remotely true.  I grant it is possible, and the aerospace folks
sometimes claim it, but it isn't a reasonable thing to assume, as you do
Right.  That is the question.  Having worked both sides of this
equation, I'd say (and said [0]) that the answer is No.
As you correctly point out, the damage done by freezing is alone enough
to cause any benefit to disappear.  However there are other things that
need to be taken into account.
Firstly, the only people who know what audit means are the people doing
it, and that doesn't necessarily mean all of them, all of the time.  The
consequence of this is that it is almost certain that the outside
doesn't understand what it means, and it is almost certain that the true
requirements of the beneficiary are not met by the audit.
(To see this, consider a Spencarian argument:  year 0 when everything
was good, but nobody understood.  Then in year 1 there is a delta of
change, and so on for many years.  After a few several cycles,
self-fulfilling feedback loops take over.  Good disappears for the
customer, but costs do not.)
However, there is some value in doing an audit, once.  And then carrying
on without.  In that, if you can pass, it has done something to improve
your capability to achieve something.  Once you've mined that, you
should drop it.
That isn't to say, you should do an audit.  It is to say that given your
particular circumstances, you could kick your team into a higher plane
of existence.  Or maybe not.  It all depends.
Right.  The requirer of the audit is the one that should benefit.  Now,
if they are working to the book, then they get what they asked for,
which is something.
But, is it likely that they are working to the book?  In a fast moving
software world, are the various USG users of (say) OpenSSL still using
the FIPS approved versions?
I don't think so.  So, as another consequence of the uncertainties, even
the USG is mostly upgrading its product as and when releases come along.
 In our space.
Which leads to .. the dev team.  If you are that close to the changes
coming in, and you're responding to each of them, you may as well *be
the dev team*.
In which case, you the dev team is doing self-audit, and the external
audit isn't necessary.  In which case, the only thing you need is for
your own procedures to be fully disclosed, so that an external validator
can come to a reliance decision.
Hope and dreams.  That is not how audit works.  Audit provides no
guarantee that there aren't any horrendous bugs in it even on day 0 with
the approved version.
(To repeat, only those doing the audit know what it means, in general.
Therefore, in general, your idea of what it means is wrong.  In general.)
Yep.  That's one of the self-fulfilling feedback loops.
[0]

@_date: 2014-06-24 12:10:44
@_author: ianG 
@_subject: [Cryptography] Help please, 
Estate planning is first, middle, last a human business .. you can't
really do much with technology except to support the humans that clean
up the mess afterwards.  What do they do?
Same thing with digital cash, we "had it" in the mid 1990s.  Now it is
rebranded as cryptocurrencies and everyone thinks time started in 2009.
Spot on.  What are the risk calculations here?
    Chance of death?  100%
    Chance of a messy divorce?  30% ?
    Chance of a government abusing your secrets?  1%
Which one do you plan for?
Not sure about that comment, but the oft-repeated aphorism about liberty
and freedom is true, but only in the aggregate.  Your contribution to
such a cause would typically be indirect;  your contribution to your
family's future wellbeing would be more direct.

@_date: 2014-06-24 18:43:59
@_author: ianG 
@_subject: [Cryptography] seL4 going open source 
General Dynamics C4 Systems and NICTA are pleased to announce the open
sourcing of seL4, the world's first operating-system kernel with an
end-to-end proof of implementation correctness and security enforcement.
It is still the world's most highly-assured OS.
What's being released?
It will include all of the kernel's source code, all the proofs, plus
other code and proofs useful for building highly trustworthy systems.
All will be under standard open-source licensing terms. More details
will be posted here closer to the release date.
When is it happening?
The release will happen at noon of Tuesday, 29 July 2014 AEST (UTC+10),
in celebration of International Proof Day (the fifth aniversary of the
completion of seL4's functional correctness proof).
 What's special about seL4?
Completely unique about seL4 is its unprecedented degree of assurance,
achieved through formal verification. Specifically, the ARM version of
seL4 is the first (and still only) general-purpose OS kernel with a full
functional correctness proof, meaning a mathematical proof that the
implementation (written in C) adheres to its specification. In short,
the implementation is proved to be bug-free. This implies a number of
other properties, such as freedom from buffer overflows, null pointer
exceptions, use-after-free, etc.
There is a further proof that the binary code that executes on the
hardware is a correct translation of the C code. This means that the
compiler does not have to be trusted, and extends the functional
correctness property to the binary.
Furthermore, there are proofs that seL4's specifcation, if used
properly, will enforce integrity and confidentiality, core security
properties. Combined with the proofs mentioned above, these properties
are guaranteed to be enforced not only by a model of the kernel (the
spec) but the actual binary. Therefore, seL4 is the world's first (and
still only) OS that is proved secure in a very strong sense.
Finally, seL4 is the first (and still only) protected-mode OS kernel
with a sound and complete timeliness analysis. Among others this means
that it has provable upper bounds on interrupt latencies (as well as
latencies of any other kernel operations). It is therefore the only
kernel with memory protection that can give you hard real-time guarantees.

@_date: 2014-06-25 06:57:54
@_author: ianG 
@_subject: [Cryptography] What has Bitcoin achieved? 
You mean, a cheque payment system.  As a matter of monopoly marketing,
banks would like you to believe that only a bank can accept cheques
(checks).  This is really a marketing distinction, and it is not
recognised in all countries all the time.  For example, in Europe there
is a Banking Directive and a Payment Systems Directive.
The point of distinction for banks is deposit taking -- which means
taking your money, placing it into long term assets (mortgages) but
promising it back to you on demand.  A contractual fraud for others,
permitted by the 'charter' for banks.
However, to attract deposits, they offer checks, which they will clear.
 It's just a marketing product.  Many non-banks such as S&Ls or credit
unions will also offer checks, in different countries.
There's no reason why your employer couldn't offer checks against your
salary... except his bank would get upset.
Right.  But the check (cheque) technique is just a way to send an
instruction to the money issuer.  How would you do it in a digital
sense, otherwise?
Deposit insurance is a central banking invention that can be applied to
anyone in the net.  It's not really a hallmark of banking but of central
banking.  As it happens, post-cyprus, every central bank is scurring
around trying to figure out how to drop their insurance.
But, as far as mPesa goes, they put their aggregate float account with a
bank.  That probably doesn't earn deposit insurance via central bank
rules, but likely they have some other form of accomodation.
So, checks.  Tech.  How would you instruct your money issuer to move
money from Alice to Bob?  In Bitcoin?  In some hypothetical 3 party
money system with Alice, Bob and Ivan?
You'd write a check as a digital layout specifying Alice, Bob, the
contract and the amount.  Sign it with your public key.  Send it in a
packet to the issuer.  Who would 'settle' it.
Well, yes.  Pre-blockchain cryptocurrencies, we just called that person
Ivan and wrapped some governance about it.  NS said no, we need to not
have a person, so he wrote a blockchain settlement system, and
eliminated the contract's issuer, and wrote the contract into hard code.
But this shoved the onus onto the dev team.  Which is fine, in some
sense it is just moving the pieces around, the head of engineering at
the the dev team is now the one who can decide to change the formula.
But he can be watched easily, and it turns out that techies are good at
reading code, if not contracts.
Conceptually, one can see who we have to trust, same as before.  In
market results, the difference is radical, and inspiring to a lot.
Your point about recourse is a good one tho, blockchain technologies
don't have much of an answer to that, to their cost.

@_date: 2014-06-25 13:14:04
@_author: ianG 
@_subject: [Cryptography] "Is FIPS 140-2 Actively harmful to software?" 
Huh.  This is interesting!  So they err on the side of "culpable by
security" not "culpable by the book..."
So, a way to proceed would be to contact those people who do SSL
scanning (or grab their database) to measure the SSL versions in use
versus USG.gov sites.  Perhaps publish a USG vulnerability page, and see
if we can correlate that to the FIPS usage.
This would be very interesting info for OODA life cycles, which is
useful for reminding us how to build our protocols and distro them...
Would make for a nice undergrad level nice paper for some student...
Change happens slowly, look to the IETF for precedent ;)
You'd probably need 3 incidents.  1 to warm things up, a second to
really push the message home, spread the fear, and a third so that the
culpable party is really culpable in the eyes of the witchhunters.
If they are anything like the other committees then you're better off
not wasting your time.

@_date: 2014-06-25 16:01:51
@_author: ianG 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
Hi all on the crypto group,
over on TCPcrypt [0] it is being debated whether Algorithms should be
Agile [1] for a future extension of TCP that includes opportunistic
It has been claimed that the consensus is that algorithms MUST be agile.
 And, ditto for TCPcrypt, being a lightweight MITMable protocol.
My question in 2 parts, purely to measure *consensus in this group* as
opposed to the IETF group:
     1.  Do you believe that in general case for the security for the
net, (a) security protocols MUST be agile w.r.t cryptography ciphers ?
OR, in the negative, no, protocols may set one cipher and stick with it.
     2.  Do you believe in the specific case of an opportunistic,
dynamic, transparent upgrade inside TCP to an (e.g.,) anonymous DH
protected secret key protocol, that, the ciphers must be agile?  OR, in
the negative, no, that particular protocol can insist on only one and be
done with it.
[0] [1] agility:  TLS has a config string selecting algorithms at the
server;  SSH has an ability to handle different ones by selecting
command line switches.
    non-agility:  DJB and Tanja Lange have presented protocols in which
you get one choice only:  one EC curve + Poly + Salsa.  DJB calls it a
cryptobox.  Bitcoin has one suite: SecP256K1 ECDSA over double SHA256,
RIPEMD-160 fingerprint.  Etc.
I'm explicitly handwaving over the level issue of cipher v. small suite
v. combined PK large suite.  Which distinction is important in TLS.

@_date: 2014-06-25 21:49:10
@_author: ianG 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
That'd be FIPS version of the One True Transport Protocol, yes ;-)
Your third alternative is exactly spot on.  My intent is to allow any
algorithm choice to occur only at the upgrade from vN to vN+1.
I think it impossible to get away from version upgrading... So it
becomes the upgrade of last resort.
Philosophically, however, I think it is necessary to get people to
accept that algorithm agility is not a MUST within the context of
MyTransportProtocol(2014), in the absolutist sense that IETF sees it
right now;  before moving on to the dynamic consideration of version
I'm also thinking that IETF is locked in a mindset, and those that have
seen the light simply don't go anywhere near the place.  So if we can
show an external picture that is different, it might get some more light
into the dusty dark corridors of net power.
I could be wrong, often am, happy to bat towards a 50% average in this

@_date: 2014-06-26 10:13:52
@_author: ianG 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
Hi Stephen,
That's how it feels from the outside, when a new idea comes along, and
someone on the inside says "oh, they will reject that, no chance."  It
only takes a few people within to believe that view without question and
it becomes self-fulfilling.
Well, here's what I've seen so far (excluding you and me):
  1. NO 3, minimalist variation 2.
  2. NO 2.
Now, any survey can be fudged, and our data points are so low here any
conclusion can be reached.  For example, the respondents can be
self-selecting, those that would vote YES might simply not want bother
with such a silly post.
But one thing I think I can show is that while I grant that the IETF
might have a consensus internally, externally, opinions differ.  For
exactly the same self-selecting reasons, people who disagree with the
IETF won't go there.

@_date: 2014-06-26 11:33:53
@_author: ianG 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
Just to separate the meta-notion of consensus from the precise argument
at hand, here is response in separate cover.  Shooting from the rough,
take cover!
There is the backup algs argument, and then there is the slow
implementation issue.  I'll take them separately.
- firstly, backup algs argument is a postulation that an alg can break.
Debunk 1.  This has never really happened, within reasonable bounds
(pick a proper algorithm to start with, and migrate as it is getting
Debunk 2.  In risk analysis, if something has never happened, that means
we shouldn't mitigate it, unless it is free to mitigate.  But the
mitigation is indeed more costly, as we now know that the presence of
multiple negotiation options is causing pain and loss of security.  In
short, we're making security worse to deal with a mythical break.
Debunk 3.  Indeed, we have so much confidence in our algorithms that the
following argument shows we've never taken the 'alg backup' argument
Take your favourite two block ciphers.  Independent keys, XOR the
output.  This provides the strength of both, together.  In agile terms,
it totally dominates the agile argument because it employs both, fully
at the same time, and it eliminates the negotiation morass.  (Same thing
can be done with other algs like HMACs).
Yet, we rarely do that.  We rarely use superior engineering, we always
fall back to the negotiation approach.  Where I've seen it used is in a
Dutch cipherphone and I think blockchain technologies used 2 for one
hashing.  Rare!
Why?  (a) Because cryptographers teach us that to do this is futile
because if it was a good idea *they'd already have done it*. (b) many
would say that to use two algorithms would be a performance hit (never
mind that they have created a performance uncertainty...) so this shows
that 'backup alg' argument is secondary to performance.  Which kind of
puts it in the shade as far as security is concerned.
In security and software engineering, doubling the cipher totally
dominates the backup alg argument ... conundrum?  Or debunk.
Debunk 4.  Although we've never had an alg break, we have had protocol
breaks.  So why don't we have a backup protocol?  There are plenty of
good protocol ideas out there.  Why doesn't the IETF entertain a TLS
over TCP and a TLS over UDP ?  and if there are any problems, we switch?
(No, I'm not talking about DTLS, more like a QUIK within TLS as a
selectable protocol element.)
The reason is that we're assuming alg breaks *because we feel we can fix
them*, whereas we're not assuming protocol breaks because we don't feel
we can fix them.  This is creating our requirements from our
capabilities, which is to ignore the user public's needs, and
concentrate on our own needs for gratification.
I'm not sure what this argumentation is called, but it ain't engineering.
- then secondly, it takes time to bed a new alg in.  Well, true.  But it
takes time to bed anything new in.  This rests on the above argument
that breaks happen in algs and we need to mitigate;  but this also is a
false separation.  In fact, breaks never happen in algs, but they do
happen in protocols, and fixing protocols takes time.
So, yes, this is actually the opportunity.  The new protocol should be
in the works and well advanced.  And that's the perfect time to change
DUAL_EC, Snowden and AES.
The DUAL_EC story is a case study of how a national algorithm can be
used to force in an insecurity into a cryptosystem.
Therefore, the case for national algs can be seen as an open door for
insecurity.  I'm not sure how much easier it gets, as Snowden has given
us all the answers here.
Finally AES.  The AES algorithm wasn't a national algorithm.  It was a
worldwide competition, and the winner came from some uni somewhere.  As
all the cryptographers involved were competitors, as they all
scrutinised the winner, they gave the inside 5 a clean bill of health,
and they participated in and scrutinised the process, we've got a fairly
good answer to the selection of algorithm question -- at least against
bowing to national prejudices.
They're in the past.  We're planning for the future.  At some stage we
have to cut away the legacy of the past.
The hardware argument is just a variation of the vanity argument, as
expressed by the big corps that send their salesmen into the committees
to press their arguments for more sales.  Remember, for each of those
big corp salesmen, there are groups out there in the world who can't
press their case because the entrance price to IETF is too high, and
they get no security because those same big corp salesmen stuffed up
previous protocols.
And, if they really care, that much, what they can do is install feature
changes.  If the protocol is a better protocol and wins the day, and if
the single cipher suite is better, and it wins, then they'll adjust
their hardware.
Software leads, hardware follows.
Indeed, if the protocol wins, it will win in the software domain.  It
won't get any help from hardware because nobody will create the extra
linkage until it is worth wringing the speed out.  First deployment.
Then speed.
Also, note that inside the open software world, outside the IETF, there
is widespread suspicion of hardware solutions.  E.g., RNGs and AES.  So
much so that if a WG voted for say Salsa *because it was not in
hardware*, you'd make a lot of friends in the software world.  And it is
the software devs who will decide the deployment win, not the hardware
devs...  And of those, it is primarily the open software crowd, who are
most suspicious of hardware.
Same story.  Actually I have a lot of sympathy for this argument as some
libraries are slow to respond.  My pet peeve is Java institutionalised
crypto (aka JCE) which is so limiting, kludgy and ancient that if you
were planning for that, it sometimes feels as though you'd be better
doing plaintext.
But this is not an argument for the protocol.  It's a strawman.  The
algorithms that are in use in a typical tight suite can be coded or
cribbed in less time than it takes to manage the negotiation suite.  You
just need to make sure that the protocol specifies some nice algs within
a tightly designed suite, all with properly documented test numbers.  Do
a reference in some easy language.  And find yourself a cryptoplumber
per other language.
It's security coding.  You get better results by *not using a library*.
 (OK, I'm the only one who believes that...)
The apocryphal story of the Dutch boy and the dyke ... once you let one
choice in, well ... and we all know how that story ends.
I suppose if one were in the process of fixing TLS  then one
could postulate a sort of A/B process.  E.g., in each new version
release, the older of the pair was retired, and the newer of the
previous two took pole position, to be accompanied by a new gen suite as
backup.  If it were a disciplined approach such that for example the
protocol said that the primary suite in use was the A for odds and B for
evens, then it might stick enough for people to handle it.
(But such an argument makes no sense in the context of TCPcrypt.)

@_date: 2014-06-26 23:22:53
@_author: ianG 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
Very good point.  There isn't anything about that.
Good point.  So requiring algorithm agility is actually inadequate.
Another timely reminder that these people require without understanding.
I would go further.  It's not happened in modern times with properly
designed systems.  (By "properly designed" I exclude things like
wireless which seemed crippled by design.)
If you think I'm wrong, by all means, shout!
What we have seen is a steady run of failures in the outer areas.  So it
seems unfair to blame the humble algorithm for protocol failings.
Yes, but to take this to its conclusion, there should be two
mini-protocols inside the protocol.
Right.  Any ideas?  I've none.  Or, at least, all ideas I have seen that
actually work reduce to "re-install".
All good points, I have a rant somewhere with 14 points against.
However, you haven't opined on the original poser:
1.  Should IETF insist on MUST for algorithm agility?  That is, all
protocols should have it, in general ?
2.  Should IETF insist on MUST for algorithm agility in the special case
of the opportunistic extension to TCP, called TCPCrypt ?
Perhaps I should explain some more context.  IETF WG for TCPCrypt has
been formed to create an opportunistic extension to TCP.  Now it has to
write its charter.  Then they will start on the protocol.
In the charter they set down rules.  Right now the rules state that
TCPCrypt MUST have algorithm agility.  I for one think that is daft [0],
but I am told it is IETF consensus.
So what I'm actually trying to show today is ... it may well be IETF
consensus to insist on algorithm agility, but it ain't the consensus
outside the IETF.  Outside, we all look on and shake our heads.  In
wonder.  Can't they see the cross they create for themselves to carry?
And, it might be that the IETF are a somewhat self-reinforcing crowd ...
and we're about to see a reinforcing cycle where they scare away anyone
who thinks agility results in white elephants.
So, what do we think, here, those of us who aren't steeped in IETF?
Algorithm agility?  Thumbs up?  Down?
[0] for a longest time:

@_date: 2014-06-27 16:08:07
@_author: ianG 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
CTR mode.  Feed counter (eg. block number) in as the 'plaintext' into
each cipher.  Take each cipher's output and XOR them together.  Then XOR
that with your real plaintext.
Do exactly the same for decrypt.  Don't ever use the same key/counter
combination for different packets ;-) but that is true of CTR mode anyway.

@_date: 2014-03-01 15:28:24
@_author: ianG 
@_subject: [Cryptography] Btcoin -- bubble or investment opportunity? 
The alternate direction -- debit then credit -- also had its element of
perversity.  At a smartcard builder that you will recall presenting to,
various managers who were not actually techies would delight in trying
to pull the smarcard out at the point between debit and credit, and then
chase up their allegedly lost funds.
This process of chasing lost funds was one of the reasons for the help
desk in what was claimed to be a perfect process.  Out of this came a
need to figure out whether lost funds could actually be traced and
proven given that the cards were out in the field, and the user was on
the phone, not in the office.
So, it transpired that the help desk were given access to the
transaction database that held and tracked all transactions.  Using
this, they could easily see if the transactions had broken in between,
and then refund the money.
But wait, I hear you say, as I said at the time, weren't digital money
products meant to be privacy protecting and anonymous and card-to-card?
Yes, they were claimed to be that way.  So where did this database come
from?  Therein lies another story...
It turns out that the central bank imposed the full tracking on the
company within some discussion with hand-wavy meltdown / governance / ML
agreed to keep the database totally secret, and only two people in the
company could access it under strict controls.  And of course the
central bank under agreed and legitimate needs.
Then, step by baby step, this database enlarged its purposes slowly
until the clear and compelling case was made that the support desk also
had to be able to access it.
In effect, step by baby step, the product was molded until the original
requirements were totally trashed and lost.
They are, MtGox is failed, and others are wobbling.  Here's my take on
it:    It is a
failure of governance, no more no less.
There's a frustration here, a tension.  On the one hand we see multiple
millions, possibly as much as a billion dollars, being thrown at these
bitcoin ventures often on no more substance than the word 'bitcoin'.
And then, we see those bitcoins and millions flow like sand into the
hands of others, with nothing to show.  In each case, it is because
lessons that were learnt by prior generations are not researched,
listened to or re-learnt quickly enough.
Yet, the investors seem to be swayed by the confidence and miracle of
the new generation.  It is the investors decisions more than anything
that contribute to the conclusion that Bitcoin is a bubble.
If one was to look for a good bitcoin investment, what would it be?  I
think something like this:
* they have a regulatory model.  It doesn't need to be right or
sustainable, they just need to understand the word, because whether they
know it or not, the word is coming for them one day.
* they have a governance model.  Ditto.
* they have a Sean Parker.  By this, I mean the person with actual
experience of business, who's there at the critical juncture to go from
2 kids and a fridge full of beer to a business.  See the Facebook movie
if this doesn't make any sense.
* Finally, they have a business model that is agile to the complete
collapse or departure of bitcoin itself.  The point here is not that it
will or won't, but the future is so traumatically unpredictable that it
will consume more naive blockchain believers than we can possibly count.

@_date: 2014-03-01 15:55:55
@_author: ianG 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
Au contraire!  You've fallen to exactly the same trap that you accuse
others.  You see that we are all focussed on cause A: bad code, and we
are missing cause B: lack of process.  By saying it is all about B, you
are missing that A is a contributor.
All of these examples, including Ariane B are characterised by 'accident
chains' being that set of circumstances where multiple checks fail to
block the disaster.  (Aviation disaster investigations are basically
that;  figure out the accident chain that led to the disaster.)
In this case, the code is so woeful, so cause-full, that cause B will
always be suspect.  IMHO you can't actually design a good process of
review and control that deals with a spaghetti mess of gotos for error
Or, in other words, as suspected by a consensus of programmers here
who've lost their dinner, a massive refactoring exercise is *the good
engineering and delivery process that was missing in OpenSSL*.
(This is not a new observation, I first saw admission that OpenSSL is a
dog's breakfast at least 15 years ago.)
This is true, but misses the point by focussing on the either/or binary
The coding style process (and indeed the choice of language as a subset)
is not there to make the code perfect, it is instead there to eliminate
easy errors, to make the code tractable, and to allow the programmer to
think higher layer.  In short, to help the coder concentrate on security
rather than any other aspect.
Of course it is impossible to eliminate errors in the code by code
style, just as it is possible to write secure code in assembler.  The
real question is, what is the likelihood and cost of secure results, in
either approach?
Right.  This is where it does get hard.  I agree with that, and to be
frank, I don't do a lot of it myself.
The problem that I think is going to occur here is that the test cases
become so complicated and impenetrable that they overwhelm the code --
both economically and agility-wise.  So in practice, the only groups
who've been able to maintain such a lifestyle have been those groups
with sum of components equal to 1, or those with infinite budgets
It is always easy to derail any opponent by referring to another
language as a better hammer ;)

@_date: 2014-03-03 11:00:36
@_author: ianG 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
Just in order to clear up any confusion, I'm not begging it, I'm stating
it.  *Bad coding style*.  IMHO, a goto cleanup handler is an atrocious
Just so there's no mistake here...
Let's not confuse 'what people do' with 'good/bad coding style'.  Also,
just because Theo has a rep as a security devil doesn't necessarily mean
his patch is the cleanest.
As far as I know, OpenSSH adopted OpenSSL or parts thereof.  I would not
have advised them to do that, because they would inevitably end up with
things like 'bad coding styles'.  Indeed, I would have advised (and do
advise) them and everyone to do everything exactly different, as a fair
chance at accidentally improving matters.
But hey, they are adults.
As to the yaddayadda about it being open source and everyone's welcome
to give it a go, no, sorry, might be the geek fantasy but it isn't the
reality.  Firstly, the open code is closed.  You can't just wander in
and fix it, they've got repos, barriers, code reviews, reputation, etc
to get through.  It's about as likely as me sending a dev/random fix in
to linus and expecting it to be slotted in.
Second, it's a huge gap between knowing what to do, and doing it.
Programmers have to eat.  And this particular programmer has already got
his extensively huge project.
The reality is that as an open source project, we get to criticise it
from the armchairs, and Openssl/ssh will ignore us totally.
Occasionally a real blooper like this might slip through and they might
respond, but let's not hold our breath on that one.  But for the most
part, open source projects are effectively closed, we have more
influence over the practices of commercial closed-source projects
because we can embarrass them in the media.
Thanks but no thanks.  Recall an influential letter to ACM?  To be
frank, I'm not that keen to go back to the dregs of the literature of
the 1970s or 1980s when this work was likely done, in droves.  So I'll
pass on the academic shut-down.
I would actually agree with that.  Whole-heartedly.  We need to start
again.  Big time, not fiddling around at the edges.
Now we are left with:  why hasn't this been done?  I've indicated some
reasons above, another reasons is shear inertia.  A fourth reason is
hubris, 99% of the people involved think that they are doing a good job
(and they might be right) and therefore criticism is wrong and should be
dismissed out of hand (right or wrong, no change needed here...).
If there is going to be any rewrite of this, it actually needs to start
by first getting the consensus moving that OpenSSL in particular and SSL
in general is utter crap.
Once the mental barrier is torn down, people might then take the brave
step of building a replacement.
Brave steps:  here's a relative newcomer to the scene:
There are others.  DJB has a paper somewhere that is poorly linked and I
can never find it.  Here's my idea on the group:
 at metzdowd.com/msg12958.html
Yup.  However, the fact that this is a difficult task needs to be
separated from the fact that they are running with a code base that
makes their job and our lives harder, and they have no strategy for

@_date: 2014-03-03 19:28:29
@_author: ianG 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
I personally think it is accidental.  In the court of cryptography, it
took about 2 minutes to say this:
$ cat moo.c
  int main(int argv, char **p)
$ cc moo.c
$ ./a.out
hello world
$ cc -Wall moo.c
Look ma, no warnigns!  OK, would have taken less time, but it's a mac,
and rusty-C needed to be guided by warnings, snipped...

@_date: 2014-03-03 22:23:39
@_author: ianG 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
And for those that can't teach, there's always the army :)
Yup, that's the reality.  So we're in loud agreement, that very open
source is part of projects that are very closed if anyone actually has
the temerity to discuss change.
Of course, *in principle* a patch could be accepted, but the reality not
the marketing is ... pigs fly faster;  it takes quite some time before
the average developer can earn his wings and can get up there and soar.
Yup, it's open like American democracy is open to another party.  Repeat
& rinse, civic classes at 3pm.
What probably is going on here is that the open source magic is blinding
insiders to the humdrum of basic economics.  Customers have mass, and
that mass can be gripped.  And closed off ... just like the old monopoly
days of old.
I'm not saying this is a bad thing, just that it's a thing.
As I mentioned, it is possible to write good security code in assembler,
but ... few do.
Why not?  There is another test that cuts through the theoretical /
academic / scientific method / evidence-or-death rhetoric, and that's
the market test.  When the market has moved on, it has spoken.
Oh indeed.  But, one stood the test of time, the other didn't :)
That would be somewhere between a religious myth and unfounded fantasy.
 There is no way possible that some one person can write an SSL
replacement and get anywhere with PKIX, TLS, browsers, vendors, CAs,
government interested parties, standards orgs.  If you think that,
please send me all your bitcoin, I've got a bridge to sell you.
I know that the open source community has their mantra, but please....,
every religion has its blind points.  The IETF is not a place for open
source, it's the place for huge corps to duke it out, without killing
each other.  It's the Internet's answer to universal war.  For that
benefit, we thank it.  But open?  No, do not be fooled by the brochure.
 It's open if you're a household name, otherwise you're roadkill.
Nice knock down, but you missed the point.  The real point (again) is
that the plausible way to replace it is a movement, not an individual.
Indeed.  See why my code isn't presented to the IETF?  Because I am one
person, or one business, and that's a non-starter.
See why google is giving it a go?  Because they are google, and they are
about the only ones who have a shot at this.  They are the only group,
bar none, that can make this happen.
Nobody else.  That's a prediction, by the way, and it's not that
reliable, as I'm not sure they can do it, but I hope they try.
Oh, except that part about "nobody else."  That you can take to the bank.
So if I change myself, does this mean that CABForum will accept my
proposal to fix SSL?  No, I didn't think so, because the only 'changed
self' they will accept is in their own image.
Back to facts.  Back to earth.
My strategy ... as you asked:  I'm building it and deploying it in
developing-world financial scenarios where mainstream institutions
cannot reach.  I'm doing it outside the mainstream western scenario
because of incumbrance and other issues.  Think bitcoin meets Grameen,
but without the imbalance, without the western mindset and definitely
without the helium.
As you're a technologist, I'll describe the tech:
Packets over UDP encrypted with AES/SHA1/HMAC (called SDP1) negotiated
over a request-response model with RSA signing (called SOX).  Once
secure UDP/SDP1 context set up, requests include monetary transactions,
identity info, files, chat and admin.  Values range across the spectrum
with gold, dollars, euros, shares, etc.  Although its tech genesis was
in small fast reliable financial transactions, as of a few months ago,
my intern coded up a large-packets-over-UDP in a month (yeah, that's all
it took, 1 intern-month) so now we can do things like encrypted cloud
backup and photo sharing in one request-response cycle over a secure
link.  In terms of making lives easier, this provides an end-to-end
(person to person) security and governance wrapper for the investors in
a Grameen context.
So, yes, if we had a business reason to create a generic replacement for
SSL or HTTPS, I would do it, I'm can almost do it now.  But we haven't,
so I won't, and my name's not google.
However, if there was *a competition for TLS2* where we could get a fair
hearing according to the rules, such as NIST's most famous AES
competition where even the guy who got knocked out within his own round
1 presentation walked away /with honour/ then we might have a go.  Not
because we'd win (resources, again) but because it would be fun, a great
learning and a chance to share.
It would be the best show the security world had ever seen.
Otherwise, it's left to google, who do have the industrial clout to play
the politics game that the IETFians so love and cherish.  What took me
10 years took them about 2.  Go guys!
Indeed.  And one of the things that demotivates potential contributors
is key personnel in open source projects telling them that their project
is totally open, while proving how closed it is.
All open source projects would gain more integrity and more contributors
if they said:  "This project is CLOSED.  There's the door, go look.  To
open the door, follow these instructions...."
:)  On the Internet, when a dog barks, someone thinks their leg is being
pissed on, their knee jerks and they tell the dog to f*ck off.
Right.  Would be fine, as long as you didn't use the term "open"
anywhere near...
iang, back to crypto code, coz it's almost working, yay!

@_date: 2014-03-04 11:46:48
@_author: ianG 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
Right, now have a read of the code in question.  You will see it wasn't
just the inadvertent C&P that was the problem.  What was at deeper issue
was the error handling flow, which was structured around a goto concept.
 There were several flaws present, one of which was that there was a
default good condition, another of which was that the branch-by-goto did
not carry the error with it.
Exactly.  Follow that thought when reading the code...  The use of the
goto in this case was expressly bad because it created/caused/led to an
unreliable error handling flow.
The calls to replace the gotos are simple ones.  They are for simple
programmers who don't understand why they are dangerous.  Best to use
simple rules there.
The underlying call is to refactor the code *to handle errors*.
Security code and reliability code (same thing really) is all about
errors.  If you aren't programming for errors, you're not doing security
No, it's a problem for coding style.  Quality assurance cannot improve
on a bad style;  QA can make it less errorful, and so can testing, but
neither can make it less error-prone.
Sorry.  Security code.  SHAs, pk sigs.  Forget performance.
Right.  The problem is that the sort of tests that are espoused -- the
negative is negative tests -- are expensive to write.  And they deliver
no obvious or tangible benefit because ... the code already works,
right?  Once people are on deadlines or bug queues, and managers are
grumpy, doing what is 'right' as opposed to what you can get away with
is hard.
It's a thing, it's not a happy thing, but reality is what it is.
Until we can measure and tangibalize (!?) the cost of an apple goto fail
or a debian random delete, we cannot expect it to play much part in the
process.  In essence, we all accept the risk.  We all walk in the shadow
of Papa Legba, the god of bugs.

@_date: 2014-03-04 14:08:01
@_author: ianG 
@_subject: [Cryptography] GOTO Considered Harmful 
Oh, Patrick, your cup runneth over!
We've got overloading of error variables, use of manifest numbers (1 as
success, 0 fail, -1 gotcha), use of the famed goto cleanup mechanism,
leading to more gotos than worms in an apple, ... and now a patch which
adds a separate goto branch!
All kudos to Nikos Mavrogiannopoulos, who took the initiative to trawl
through this code in the wake of the apple bug.

@_date: 2014-03-07 10:23:03
@_author: ianG 
@_subject: [Cryptography] GnuTLS -- time to look at the diff. 
Other than being stuck in C, sure.  Well, even there, there are several
strategies for dealing with it:  pass the variables in to the function
and have the caller handle it.  Do it on the stack, do it in statics,
that's what functions are for, etc.  They weren't doing it.
Why not?  I'd bet it is a combination of fear of bugs, lack of resource,
lack of testing, momentum of the codebase.
I agree with the former, C not having memory control makes it very
expensive to write in.  My informal experiments lead me to believe it is
5 times as costly to write in than an OO language.  So yeah, I'm tired
of those weeds too.
However, the bool issue is a siren, it's much messier.  What occurred in
both those examples was inadequate treatment of the error conditions,
and it manifested itself in several ways:
  * overloading bools with error codes
  * using manifest numbers instead of declared constants
  * overloading of variable usage
  * conversion of error status on the fly
  * transmission of error status along the flow of code
It really is all about the errors.  And the answer to this is style ---
establishing a set of practices that best works with the language you
are stuck with, and best allows the flow of errors.
OK, I byte!  What's the power of ten?
They aren't globally acceptable.  They are locally acceptable in those
two TLS source bases, manifestly.  But Peter posts that he doesn't in
effect accept them as style in his C implementation, and his gnashing at
defeat in one single case proves the point.  So it isn't even acceptable
universally within the SSL community.
What I find curious is the almost mirrored style from OpenSSL to gnuTLS.
 It's as if the latter just copied it all just to get a licence change.
 Idle speculation, but the cultural impact is something to consider here.
Hmm.  Tipping at Nico's complexity argument, I'd say that correctness is
a possibly unachievable target.  I'd prefer to prioritise safety.  Err
on the side of...  Because, I never got the impression from PKI that
there was one precise correct answer to every case.  That's just me,
those that have sold their life into it might say different.
unfortunately I suspect those who are in there and writing the code
probably prioritise to speed & efficiency.  It's the only reason I can
think of to use those goto layouts.

@_date: 2014-03-10 10:25:19
@_author: ianG 
@_subject: [Cryptography] RC4 again (actual security, 
Yup.  We would have been better off if they'd stuck to 40 bit crypto in
1994 and covered 100% of the web.  It's relatively easy to upgrade from
40 bit to 128 bit.  It's a real pig to upgrade from 0 bit to 40 bit.
But you can't tell a standards group or a committee or a cypherpunk or a
vendor or any other cartel things like that.
Imagine going to PKIX and saying "oh, RC4 is fine, but can you make SSL
opportunistic and phase out HTTP in favour of HTTPS, please?  Pretty
ps; I agree with replacing RC4 wherever it is seen.  It's where it is
not seen I get upset about.

@_date: 2014-03-10 19:48:32
@_author: ianG 
@_subject: [Cryptography] RC4 again (actual security, 
Little, seemingly.  As a historical observation, MD5 was long considered
dying, with even demonstrated breaks as far back as 2005.  Yet nobody
much moved, they all grazed on happily.
Until RabbitSSL was sent running off a cliff...
However, when NIST announced that for its government customers, only
2048 bits was considered good enough, they all moved!  Very fast.  Even
the shepherds woke up and started running...  Given that the need was
entirely for one customer for one specious and not-us attack, it is a
surprise that anyone cared .. but *everyone* moved as if this was the
most dire threat to all.
Meanwhile, SHA2 support is still spotty and SHA1 is still in use, even
tho there is a cloud over it.
Herd behaviour.  Call it lemmings if you like.  Observed, and repeat.
What hope is there?

@_date: 2014-03-14 09:31:11
@_author: ianG 
@_subject: [Cryptography] embedding security (was ChaCha) 
True, but it might embed a blameless immortal unupdatable vulnerability
into the host manufacturer.
Assume the cost of the embedded device is some number X.  Installing it
into a largish installation (with many devices) is probably some factor
fX where f is o(10).  Which leads to some interesting economics.
As the owner of device 2 is now likely responsible for replacement in
some foreseeable time, he's going to have to plan on that (f+1)X cost
into the future on that regular basis.  So let's say this is a
slam-dunk, and he switches over to device 1.
Yet, device 1 also has finicky requirements.  Probably, such a system
can only be operated with a remote management console of some form.  Add
in some form of approved network.  And now we're looking not at a device
but at a system.
Which as the manufacturer is now liable for, she's also keen to certify
(maybe) or operate directly.  The more complexity, and the more
liability, the more the tendency to vertical integration.  Factor in
desires to increase margins, desires to acquire additional protection
from a wall-of-standards and fat pentagon contracts and guild
certifications, and the tendency will probably emerge without much
kicking from outside.
So we'll find that there is now a tax t over the additional raw costs
due to oligopoly controls.  In most countries there will be one major
dominant player, the rest will be shut out of the big contracts.  In the
US there will be 2-4 as always.
The end result then will be a facade of control but a patchwork of
weakness, and most of the control will be directed at preserving the
structure.  Typically I'd expect that we would find that the secure
remote consoles themselves become the fast way into pervert the entire
network, and the manufacturer was breached way back when by all the
This notion of a secure embedded device may be worth something, but are
the consequences of an insecure embedded industry worth something as well?

@_date: 2014-03-15 15:31:05
@_author: ianG 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
until we've been personally Feinsteined, yes.
It is for this reason that story telling is so important.  Humans assess
risk on the basis of what happens to their friends.  If enough of them
get hit, then the personal probability assessment rises.  If not enough
of them are hit, then the risk is considered remote.
I suspect that this is essential, because what other objective metric is
there that doesn't get perverted or misused by someone or other?
To make that point, over on the Cryptogram today, there are a couple of
unintended back and forths between Schneier and Doctorow over the "where
are we on the NSA/GCHQ thing?" that lead the latter to this analogy:
But: If I had just stood here and spent an hour telling you about
water-borne parasites; if I had told you about how inadequate
water-treatment would put you and everyone you love at risk of
horrifying illness and terrible, painful death; if I had explained that
our very civilisation was at risk because the intelligence services were
pursuing a strategy of keeping information about pathogens secret so
they can weaponise them, knowing that no one is working on a cure; you
would not ask me ?How can I purify the water coming out of my tap???
Which is to say that we have failed as an industry, and our normal
metrics or objectives lie in tatters, so much so that we allowed a bad
actor to weaken our infra so badly that it is beyond our capability to
roll back the damage.  Now this bad actor has taken on a viral
proportion, and is now the pathogen of strongest intent, such that even
us technically competent people are not capable of defeating or
protecting ourselves, nor is there easy advice for grandma to keep her
house (or more topically today, nor is there easy advice to ones
teenaged daughter to keep her nudity from being hoovered up by GCHQ).
Doctorow's point is that when the individual defence fails in general
society, because of the viral proportions, the solution is "public
health".  Which means nationally administered and imposed protection
because a free choice approach does not account for infection.  (He
doesn't really explore what happens when the pathogens are the national
Back to the personal Feinstein moments of individuals when their
embedded devices fail at the behest of attackers, who we can now
identify as likely either inside the supply chain or outside the supply
chain, a complete set.
If people stop believing in institutions such as standards bodies,
certification bodies, and governments, the question is, what or whom
will they trust?  And what could actually deliver that trust?
It seems that without a good answer to that, there isn't much point in
choosing one technical approach versus another.  To me at least.  It is
possible to impress techies with tech, but ordinary people don't think
that way.

@_date: 2014-03-16 12:01:19
@_author: ianG 
@_subject: [Cryptography] Focus 
:) Which reminds me, in yesterday's Crypto-gram:
The Voynich Manuscript has been partially decoded.  This seems not to be
a hoax.  And the manuscript seems not to be a hoax, either.
or or The description of how it was done (in the highly readable PDF is
better) is an interesting read, and a little bit of a tease to us!

@_date: 2014-03-16 14:49:13
@_author: ianG 
@_subject: [Cryptography] The role of the IETF in security of the Internet: 
Yeah.  The irony is too deep for me, I'm drowning already.  Especially,
couple that above admission of shame by browser players to the comment
seen on the other crypto list about TOR leakage of SSL certs, and the
PKIX's desire to commit institutional Seppuku seems like not an overreach.
It is becoming clearer from evidence that the IETF has totally failed
the Internet community in terms of cryptography and security.  Here's
one more piece of evidence, which many will find intensely disturbing.
Let's assume the following tactics of the NSA in their meta-goal of
owning the net [0]:
   1.  make sure that only very few people used cryptography.
   2.  make it spectacularly complex to use.
   3.  channel all users of crypto through TTPs.
Now, the reason for tactic  is a bit more subtle.  If very few people
use crypto, then those that really care are marking themselves out;
crypto is easy to spot in a harvest of plain straw.  OTOH, if very many
people use crypto, then targets aren't clearly marking themselves out,
and the NSA is forced to attack everyone equally, thus spreading their
precious expensive resources too thinly.
The choice then was whether we would make crypto into our haystack, or
whether the NSA makes crypto be their needle to us?
This is where the IETF has played the NSA game.  Thanks to the IETF's WG
platform of bringing together industry players, the emphasis is on
protecting only *those who use the product*.  Which meant that anyone
not using the product was irrelevant.  According to IETF unwritten
policy, as enforced by industry players, everyone had to pay the price
of admission in order to be considered worthy of protection.
Which, allied to tactic 2, made tactic 1 a winner.  The IETF then became
the compass that spun the needle, rendering our haystack transparent and
Once the NSA separated out the people (tactic 1) who really care from
those who just do what is easiest -- thanks to the IETF and its platform
that elevated industry players to power which lead to WG obsession with
protecting the direct users^H^H^H^H^Hbuyers of its products and thus
total ignorance of everyone outside that circle -- the above slide
became possible.
1.  Collect everything [1].
2.  Pre-proccess to strip out meta-data (because it's there).
3.  Go to the TTPs and extract their cooperation.
4.  Attack the connections.
Look at the bottom right corner.  From "NSA Net" we go out through a Web
Services Gateway through some machine and out to something called CA
Then we get "VPN attack orchestration" [2].  One that is presumably
protected by a CA's certificate.
The term "CA Resources" could refer to an internal unit, but "Web
Services Gateway" suggests it could equally well refer to direct access
to something outside the NSA's network ;-)
Either way, that slide suggests that the CAs are owned.
Which we've known for a long time from security & governance principle;
 the term CVP or centralized vulnerability party was coined at least a
decade ago by Mark Miller, from memory.  And, I can confirm from
personal experience that CAs are considered legitimate targets by the
spooks, with intent to breach [3].
The challenge then for IETF and browser players and all the industry is
not to bring the URLs into the protection of SSL, it's way too late for
that.  The challenge is how to reform their working practices such that
they serve the security of the Internet, rather than the NSA and its
insecurity mission.
[0]  Those tactics are claims, but I hope we can avoid the "oh you can't
prove that, therefore you can't say it" thing these days, now that we
understand that very rejoinder is just another weapon of theirs.
[1]  I'm using English today, not the NSA's Big Brother Redefinition of
the word "collect".  See Schneier in yesterday's crypto-gram or here:
[2]  It turns out their use of the word attack is similar to the English
meaning of the word, which helps a lot.  Schneier again:
[3]  CAcert has a write up on its strategy for dealing with attacks of
this natuere here:

@_date: 2014-03-16 20:44:50
@_author: ianG 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
First question -- when was a backup suite last successfully fielded?
Second question -- what proportion of problems are addressed by a suite
Which is to say, in challenging your assumptions, I'm not sure your
going to get any benefit form the huge amount of work you need to do to
handle a backup suite.
Another indicator is this:  when you come to launch the backup
algorithm, will everything have changed?
Since I last designed a packet suite using AES/SHA1/HMAC, we have
shifted over to a fanboy consensus of ChaCha20/Poly1305;  which as it
happens is a stream-based mechanism, so the code to implement it looks
entirely different.
Further, in some sort of foreseeable future, CAESAR comes out and we now
have a suite of AE algorithms, so even ChaCha/Poly is starting to look
like last year's fashion, not cool.
Then add in the whole RSA => DSA => RSA (again) => EC/NIST => EC/safe
progression, and the public key side is looking as volatile.  What's in
the future?  I've got one recommendation that NTRU is needed within 5
years, if it is, then we're likely back to the drawing board.  Again.
So, in a sense, there is an emerging consensus that competitions are
what we need to re-establish trust in a process.  h/t to Ralf as well.
That isn't going to go away.  We're kind of left with a sense that we
need a competition for every darn problem we have.  So why not do that?
Why not start COMPETE 2014 -- the yearly event for crypto competitions?
Every year, we assemble at  and have an open knock
presentations followed by attack rumps.
Presentations are ideas designed to solve today's problem.  Open rules

@_date: 2014-03-17 10:28:36
@_author: ianG 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Postel was succeeded by ICANN.  Do we take the average?
or there's the market.  I am not sure myself why there needs to be one
*standard* algorithm or suite as an absolute.
People tend to say things such as, "we know it is good because it uses
AES."  But this is just a signal, no more, which means something like
"the author knew something or copied something or something."
As we know, implementation is generally a weaker point than the choice
of alg suite.  A well written RC4 implementation is better than a
schlock AES one.
OTOH, the market is also a defence;  small players rarely get looked at,
and security by obscurity works for them.

@_date: 2014-03-17 12:01:39
@_author: ianG 
@_subject: [Cryptography] The role of the IETF in security of the 
the net?
Absolutely -- I set myself an almost impossible hurdle.  But that's
aside from whether the analysis is correct or not.
Skype.  Let's ask ourselves what would have happened if Skype had gone
the WG ID path?
As did Skype.  And, Skype protected everyone within its reach (until it
didn't, but arguably it is still protecting people better than CAs ever
(Of course, I understand that people need to eat.    I need to eat, and
my crypto is currently begging for revenue... this is fraught;  for an
example of an unfolding revenue model eating its own babies, check out
Bitcoin.  I suppose we could look at the Jabber community for a
contrast, and also the myriad of chat things that made money like Snapchat.)
All granted.  This says it is a good game for the corporates -- which I
agree with.  Indeed this is my point:  the model serves the companies
and not the Internet.
I agree that in order to wean companies off the teat of the IETF WG
business model, and therefore reduce their impact on security according
to the NSA's game plan, we might need a way to incentivise them in
another direction.
It's an option, but I don't think it is the only one.  Copying their
model is literally the wrong thing to do.
Well, not as of the moment.  The innovation space is fantastically
alive, and has found many exciting models (check out Auroracoin for some
revenue & security excitement this coming week!).
However, first things first.  First we need both the understanding of
what's wrong with the old model.  Second, we need the will to find another.
YURLs are yet another innovation that got battered to ignorance on the
rocks of the IETF WG business groups (or their near cousin, the Mozilla
security process).  There are many such innovations.  The process is not
about innovation, and it is not about security.  As you correctly
pointed out, it is about the maintenance of a business franchise for a
group of corporates that have settled into a stable group.
I claim:  stable revenue is the wrong strategy for security.
(Tyler played a big part in unearthing the problem.  When he was pushing
his YURL design for his anti-phishing plugin, he was told that he had to
join the security group in order to be heard.  When he tried, he was
told it was invite only.  Doors closed, slam.  It took another 4 years
or so to find out why tho.)

@_date: 2014-03-18 01:29:06
@_author: ianG 
@_subject: [Cryptography] The role of the IETF in security of the 
the net?
And, we lost.  The Clinton decision was a stroke of strategic genius by
the NSA.  However, reasonable people may disagree on this point.
See below.
That's all received wisdom.  I'm not sure there is any evidence for it
being necessary and the only solution.  Yes, it's done, and it is
mandated by IETF/CABForum/Vendors.  That doesn't make it right.
The old appeal to authority, sure.  Some, not a lot, but enough in that
process and other like processes to understand the structure.
Obviously if my view has some merit, there isn't a lot of point
investing in it, so the notion that "I should just try harder" does
rather fall flat -- to me.
No, no, this is what was said:  Point by Philipp:  The only successful
security model we have seen in wide deployment is the CA model.
Counterpoint by me:  Skype.  It is successful.  It is a security model
(and pretty good at that).  It is in wide deployment.
And, to address your points, it was not standardized, nor had anything
to do with committee-manufacture or similar cartel mechanics.
(To respond to "better" and "more secure" would require definitions, so
I'll skip past for brevity.)
I never said it ;)  But yes, I personally do trust Skype more than any
IETF security protocol.  Mostly because it delivers whereas IETF has
terrible delivery record.  S/MIME is unusable, IPSec was broken by
install, SSL only reaches about 1% of the market, and its flagship
product, HTTPS and secure browsing is ravished by phishing.  Password stuff?
(The only stunning success is SSH.  Trustworthy, usable, free, secure.
The only thing it's missing is wider applicability to other use cases.
OK, I don't know much about Kerberos and GSS, etc.)
Against that what have we got for Skype?  A bit of spam, a proof that
Microsoft are now reading the chats (like google & SSL), some vague
sense that the NSA now like it (attack kits?  special hooks?).  Let's
put that in context:  nobody seems to have lost any money on Skype yet
('cept ebay :) but HTTPS is a pig.  If you get phished or MITB'd or
MITM'd, you're SOL unless your bank is nice, and then it's out of pocket.
So yeah, and in reverse:  I find it hard to take IETF product seriously.
 Why am I so alone?
Right, a flaw!  But even then, the record looks not so bad.
Let's be specific.  The generalisation is this:  IETF brings corporates
together who have a vested interest in the commercialisation of a
product.  As a process it leans inevitably towards those interests --
commercial interests as espoused by Bill.
Is that so far from the truth?  I mean, some people actually champion
this as good!
Yes, I agree with that.  Indeed, IETF stands as an improvement over the
prior effort which could be seen as the ISO national standards efforts
to do OSI.
(Yes I was part of that too, briefly enough, but long enough to spot an
actual protocol error, submit the description, and see it ignored for a
decade or two until some academic spotted it.  Why?  Because the
committee approach was there designed to endorse not change.)
We're in agreement.  So let's get to the nub.  Two points:
   1. In 2 decades of IETF work, they couldn't fix the cert / URL
display in SSL.
   2. It took Snowden to get the vendors to start thinking about fixing
that bug.
We have met the enemy, and he is us?

@_date: 2014-03-18 09:57:14
@_author: ianG 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Sounds good to me.  There are two general efforts that I know of:
google's QUIC and a project by DJB and friends called TP Curve or
something (I can never find it, can anyone?).
It's not impossible, I do one myself that is more purposed at financial
transactions but has migrated over time to being an also-ran contender.
 I reckon a competition would bring out o(10) contenders.
Although with a nod to PHB, he's looking for help on his messaging
project, not on his TCP/TLS replacement ;)

@_date: 2014-03-19 01:19:08
@_author: ianG 
@_subject: [Cryptography] How to build trust in crypto (was:recommending 
you may :) but I will call it risk.  I would say that you look at the
risk concerning all the possibilities in front of you and decide to take
it on.
That's fine, no shame, indeed it is what humans do very well.  They work
very fast with all the info available and go left or right.
Trust however is another thing.  It's like a higher-order integration
over many risk calculations in the past [0], which results in a new
risk:  I take on the risk that I do not have to do any more risk
calculations over a particular context (Bruce is Bruce?  Bob looks after
my children?  I'll get paid on Friday?) and I'll just keep going on as
if he is.
Trust then is optimised risk analysis over time.
So coming back to the PGP context.  PGP's so-called web of trust
provided a framework to help you do a risk calculation, but maybe only a
single input to one.  With repeated reliance over time on risk
calculations over successive events, you might have reached the point of
trust.  Maybe.
But what is rather apropos here is that PGP didn't really give you much
help there.  It gave you a name, which was the thinnest of context, and
even the name wasn't really promised,  "Mickey Mouse [0xabcd01234]"
wasn't considered bad.
The rest you had to do yourself.  And you had to keep doing it.
how do we put (more?) trust into crypto, if PGP is our starting point?
For the next step in evolution, I'd suggest looking closely at CAcert's
Assurance programme.  That programme rewrote the WoT and the CA rulebook.
It didn't (IMO) quite create trust.  It came a bit short of it (I say
this in the sense that it went further than anything else I am aware of
in the space).  But it did lay the foundation for the next evolution.
[0] I'm making this up as I go along;  the real point I am trying to
make is that there is a big difference between risk calculations and
trust.  Think of what one means when one says "I trust my spouse."  Does
one really say "That's Bruce" and put the word 'trust' in there
somewhere?  Or, to really put the point on it, what is meant by "the
trust business" ???

@_date: 2014-03-19 01:41:59
@_author: ianG 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Not quite on the mark but:  Raspberry Pis are based on an ARM.  It turns
out that the ARM is a small part of the silicon, and the rest is a high
powered graphic processor, with something like o(10) of the silicon.
The manufacturer of the chip (Broadcom?) intended the chip to be SoC for
media players, so it was a licensed/prop/undoc'd API situation.  ARM
thrown in for fun.  With some badgering over a fairly long period by Mr
Raspberry, Broadcom have now released the specs for the graphics side.
So, for fun and games at a cheap price, you now get a big fat GPU on the
side of your tiny generic ARM.  The fun to bux ratio is very high.
ps; scuze details, I'm not a rPi guy, just passing on the chinese whispers.

@_date: 2014-03-19 12:26:39
@_author: ianG 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Thanks to Adam!
Well, DJB in curveCP makes a very good point:  Now that EC is fast
enough for the average app to ignore the cost, why not move to a pure
single request-response-PK-encrypted model?  Instead of the older
performance-dominated model of PK-signed-key-setup + RR-key-encrypted?
For fast turnaround this is much easier, but the iceberg below the
waterline here is that the coding of the former is much much easier,
about an order of magnitude.
Personally, I buy the argument because I wrestle with the iceberg, but
there is one counter-argument which holds me short of the conclusion,
which is side-channel attacks.  Many side-channel attacks are conducted
where repeated access to the PK pair is possible, and the latter model
reduces that possibility a lot.
I do the same thing, I've worked to move all my stuff into UDP.  The
reasons are many and various and sometimes not particularly well
founded.  Here's one attempt:
So I strongly empathise with DJB on this, but I can't explain it
quickly.  Note the same desire over at QUIC.
Yeah, basically this is an early-optmisation asumption.
The benefit of using UDP in a performance process is not overall
bandwidth of a packed tube nature (which logically TCP might be better
at, except for the in-order deliver constraint) but fast turnaround,
faster startup and lower overall packet load.  And as he admits in the
paper, he didn't go there: "When used for short connections (i.e.,
transfer of a small amount of data) however, CurveCP uses significantly
less traffic than either HTTPS or SSH. This has not been quantified
yet.[6]"  So to some extent, he might have picked a less interesting cherry.
If CurveCP is interesting, I'd suggest also reading the QUIC discussion
especially where they get into comparing how TCP does it, and the
difficulties this presents.
nice!  On my must-read stack.  Somewhere near mosh :(

@_date: 2014-03-20 13:15:49
@_author: ianG 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
What would be your pick for a non-brittle modern asymmetric cipher?
(Context:  When I had to hack this in -- against my better wishes --
about a year ago, I used the blinded-RSA construct.  I'm not thrilled at
this because the details are way beyond my understanding, but it will do
for now, low levels of value at protection.  I'd love a better way.  My
alternate planned path is to switch to later generation
safecurves.cr.yp.to at some point but that depends on having the
design/intern/paper/reference code to do it, segway to other posts about

@_date: 2014-03-20 16:59:35
@_author: ianG 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Clarity-failure, here's a rewrite of the ideas.
Assume that the client has a public/private key pair and wants to do
regular discussions with a server.  Assume also that the public key of
the server has been acquired by a safe method.
The normal conceptual method of talking to the server is called
request-response model (RR).  It's used in REST, HTTP, and a squillion
other systems.
In order to do secure RR, there are approximately two methods.  The
first method is for the client to pick a symmetric secret key, encrypt
it to the server's PK, then encrypt the request using the secret key,
append these two packets together and send them to the server.  This is
what I meant by "single request-response-PK-encrypted model".
This is good because it is easy to code up.  Yet it also suffers: CPU
performance, brittleness.  e.g., Jerry says,
So in response to this sort of issue (and brittle really is a good word
because there are many issues) the 'wiser' method from cryptography
schools is to do this:
Encrypt a secret key with the PK as before, send that as a
"shared-secret key exchange request" to the server.  The server can then
probably munge that key (concat, hash or DH) and send back a reply from
which both parties derive an agreed single shared secret.
Then, in future exchanges, all requests are encrypted with the shared
secret (derivative thereof) and responses likewise.
Now, this is fast for crypto, and it takes the pressure off PK, etc by
only using it once a day (say).  So it is the wiser choice, from a
crypto standpoint.
But, the iceberg under the waterline is the code complexity of having to
deal with all the states of the session.  The 'session' method is an
order of magnitude more code than the previous 'singleton' method,
IMHO/experience, mostly because the former is state free, or
state-freer.  To see this, you also have to factor in PK changes, net
loss, server loss, timeouts on sessions, IP agility, bugs, user
impatience, etc.
So when I saw curveCP, there was an element in it of reducing the states
and of using PK to do all the encryption.  Thereby eliminating
substantial complexity.
However, on further looking it turns out that curveCP does not do quite
what I imagined:
It replaces the secret key in the session with *an ephemeral EC key*
which we can see as a sacrificial lamb.  Counterpoint being that it
still goes through the handshake, which is still fairly costly in
code/state terms but is certainly simplified over the comparison with
say TCP/TLS.
(When I find the original paper not slides
 I'll be able to confirm that.)
So where are we with the wider picture?  Here's my take.  There is now a
developing understanding that the old TCP/TLS path to universal and
happy security isn't going to serve us well into an aggressive future.
For many reasons, and regardless of criticisms, it may just be that it
is out of date.  We know much more than we did back then.
There are a few brave souls daring the disparagement of the priesthood
and experimenting to replace all of TCP/TLS with ground-up designs.  The
ones I know of that are fairly advanced (code, trials etc) are:  QUIC,
CurveCP and my own stuff.  I only include my own stuff because I'm me.
It is in these experiments that we will discover what works.  It isn't
in theoretical discussions, because the space is too complicated for
theory to predict, beyond a certain complexity limit, theory just
becomes another weapon for the various pundits to bash each other, it
doesn't actually inform us a group.  Only running code and working
successes will do that (yes, the RFC/WG concept does get many things
right, which we can steal).
This is why I support PHB's experiment, even though I disagree with his
assumptions.  This week, he seeks a new encryption algorithm but really
what we are doing (I think) is seeking a new security approach.
Full circle then, it would be pretty fantastic to have a competition to
bring all these ideas together, so we can knock our heads together
gladiator style and find out what we can.

@_date: 2014-03-21 15:22:13
@_author: ianG 
@_subject: [Cryptography] The role of the IETF in security of the 
the net?
This is pretty clear, yes.  The IETF model allows capture by an industry
group.  Some would say that is the model -- to allow the industry to
reach consensus with some semblance of openness.  Fine.
Industry groups have their own interests, which might or might not align
with the Internet community.  And, industry groups are far more easily
aligned to national interests than some wild internet bunch.
Like everyone, PKI employees work from a script.  We aren't any
different, when standard complaint A turns up, we roll out standard
response A' and move on, job well done.
After a while you start to learn the script.  Only once you've learnt
the script can you start to think more deeply, start to figure out what
is behind the script, what are the metrics that really matter, what is
the objective.
With CAs, and the others, it is pretty clear that the employees are in
it for the job.  It's not personal or anything; their primary objective
is to protect their paycheck.
There's nothing wrong with this at the individual level, and it is no
different outside that business (we haven't seen a rush of disclosures
or whistleblowing from Facebook, google, MS, Dropbox, Skype, Intel, etc...).
As someone said recently, in such a situation, we may have to find a
better incentive for them.  (It's an option, of many.)
Sure.  In the military, we always say, "what's the mission?"  Without
that, the military process fails.
In this context, who are we trying to protect?  Against what?  At what
level of cost and risk?
I could have sworn that said "by competition" when I first read it :)
Yep.  Compared to CABForum, IETF is the dream team.

@_date: 2014-03-23 18:52:22
@_author: ianG 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
that's a red flag to me, why not?
Indeed, see DJB's scathing blog post on DSA.
There appear to be two ways to interpret this state of affairs:  either
the private sector cryptographers have caught up with NSA/NSA, or the
thing was set up to fail in the first place.  It's not out of court that
we see some very embarrassing revelations about DSA in the future, on
par with the Dual_EC story.
Do you really care if it's your fault?  Or theirs?  What difference does
it really make, in the long run?
To my mind, people who chose NIST because it's NIST have already done
the damage.  This suggests they haven't done the work themselves, they
haven't understood the risks, and they're not prepared to carry them.
Doesn't speak well of the process.
This is our long running thread.  How do we figure this out in the
future?  NIST?  IETF WG?  Competition?  Conference?  Voting?  National
standards?  And, does it really matter that much?
What I find curious is that this logic is good and common and
defensible, but the focus is on the algorithm, the basic building block
or the black box.
Whereas, there isn't nearly as much attention on the protocol, which
IMHO is far more important.  People will obsess about the choice of hash
algorithm far more than they will over whether the protocol is this or
that, or properly implemented or whatever.
The protocol should tell you how much of a hash you need. E.g., seen in
link above:
   "Use half-size H output. Given that we don't need collision
    resistance (see above), do we really need a 256-bit hash?"
In practice, you could choose any of the finalists and be fine, actually
you'd likely be over-engineered (Pareto-secure I once called it).
OTOH, if you were careful, SHA1 would likely work as well for most
protocol purposes.  And that's what I don't see -- I don't see people
carefully designing the protocol such that each of the components work
to a balanced whole.  E.g., do you have a collision problem?

@_date: 2014-03-24 13:59:41
@_author: ianG 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
(good points snipped)
Ahem - SHA0 was also in there, and lasted about a month?  Brings the
batting average down a bit.
Also, rather short in terms of PKI times, where root keys are desired
out to 30 years.
Signature times for (hopeful) digital signing protocols look out to 30
years, as do evidentiary things I've heard about.  However, signature
schemes can be bolstered by storage/archiving, so the archive becomes
the diviner of disputes.  This is why CAs archive all their certs...
Another consideration is that protocols typically do not rely nearly as
much on their hashes as other components.  Especially if you avoid
collision effects.

@_date: 2014-03-24 15:51:59
@_author: ianG 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
I would speculate it comes down to complexity.  PK is beyond many
people, I include myself, but I also look askance at cryptographers and
cryptoplumbers, where they don't work together.
Block ciphers and hashes are hard to get wrong in ordinary use.  And
it's getting harder, especially when AE lands.
In contrast, PK is hard to get right in ordinary use.  Same with RNGs,
as it happens...
If we look at what they have done with the standards, there is tendency
to make hard things more complex, and to create a bounty of
implementation errors.  They are exploiting the natural tendency for
cryptographers to create complicated and unwieldly designs, and for
implementors to rush through their code without review.
If we look at the successes of say AES comp and NaCl family, we find
that the implementation is delivered as part of the product.  In both
cases, there was little distance between what the crypto said and what
the code said.  Blake(2) also is able to benefit from this.
Whereas, the source for good RSA or DSA info is spread over a variety of
places .. and times and peoples and licences and and and.
This is all speculation tho, based on strategy of intervention and
observation.  I'm not taking it to the bank.

@_date: 2014-03-25 13:36:26
@_author: ianG 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
You should try working in cryptocurrencies !
(James, Zooko and Bill and perhaps others will recall when they were
called ecash...)

@_date: 2014-03-25 13:42:11
@_author: ianG 
@_subject: [Cryptography] The role of the IETF in security of the 
the net?
Analogy may serve, but look to Chinese curse:  be careful what you wish for!
Is the IETF a place for individuals to find their rights?
Or is it a place for the barons to force their rights over the monarch
at the point of a sword?  A reading of the history of Magna Carta may
show a very different view to cozy ideals.
Who are today's barons?  Google, Facebook, IBM, Microsoft?  And if the
fought over a new Great Charter with the monarch, what would they ask for?
And what would be left for the individuals?

@_date: 2014-03-25 22:17:53
@_author: ianG 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
This is no weakness, this is a strength.  You are closer to what matters.
In terms of directly combining two ciphers, the common approach is to do
two stream ciphers and to xor them.
There is academic work to combine ciphers at block level, which in
general shows that it is tricky to do, has a few surprising effects, and
often doesn't really help more than picking a good cipher.  Others will
write about that, I'm sure, and see the comment previously about DES-X.
However, there is a top tip from the NSA:  you should use two systems,
not two ciphers.  That is, you should use an underlying p2p system such
as IPSec or TLS or SSH, and then layer an application security system
over the top of it.
Which is to say;  if you are going to do super-encryption, the best
thing is to separate the layers as much as possible.
my 2c.

@_date: 2014-03-27 01:18:20
@_author: ianG 
@_subject: [Cryptography] Michael Haydon on the NSA spying -- blackberries 
In 2008, when President Obama was elected, he had a BlackBerry. We
thought, oh God, get rid of it. He said, "No, I am going to keep it." So
we did some stuff to it to make it a little more secure. We're telling
the guy who was going to soon be the most powerful man in the most
powerful country on Earth that if in his national capital he uses his
cell phone, his BlackBerry, countless number of foreign intelligence
services are going to listen to his phone calls and read his e-mails.
It's just the way it is.

@_date: 2014-03-29 02:20:04
@_author: ianG 
@_subject: [Cryptography] Dark Mail Alliance specs? 
Take a deep breath.  Sit down and think about it.  Do the threat
analysis, decide what tools you can use, and how.  Do the risk analysis.
Pah.  Libertarian ranting and defeatism.  Crooks use force all the time,
and in their use, they have a local temporal monopoly.  Governments just
have a slightly longer one, with rather less clarity and purpose.
Either way it hurts the same, but somehow we manage to keep crooks at bay.
It's just a security exercise.  It's no different to any other.
Read Sun Tzu.  The art of war teaches us to rely not on the likelihood
of the enemy's not coming, but on our own readiness to receive him; not
on the chance of his not attacking, but rather on the fact that we have
made our position unassailable.
Although, I grant, using a cloud server is a bit like surrendering
before the drums of war have even sounded....but maybe you can use a
1000 cloud servers.

@_date: 2014-03-29 02:30:08
@_author: ianG 
@_subject: [Cryptography] Dark Mail Alliance specs? 
Facebook, Skype, Snapchat, whatsapp and the apple thing (facetime?) says
otherwise, but perhaps the language has changed?
Of course, we can all agree or disagree on what "widely adopted" means.
My view is, whatever definition you come up with, it will be
self-selecting.  The younger generation don't use email, except when
forced to by schools.  Africa doesn't use the web, they use phones.
Email can't be eliminated from spam without fixing one of two things:
free of cost and free of identity.

@_date: 2014-03-30 13:45:55
@_author: ianG 
@_subject: [Cryptography] OpenPGP and trust 
[interesting analogy snipped]
What Peter said.  I see all sorts of needs interspersed with half-baked
crypto notions leading to nice long dark rabbit holes.

@_date: 2014-03-30 13:46:48
@_author: ianG 
@_subject: [Cryptography] OpenPGP and trust 
Um.  I thought the 'global' is shorthand for or implies decentralised,
e.g., my name 1ANG is also allocatable by me, in hope of uniqueness and
But I get your analogy point, it can be seen through the ZT lens.
That be part of the unique aspect, in that there only be one AE6JV.
A one bit checksum over source?

@_date: 2014-03-30 22:06:05
@_author: ianG 
@_subject: [Cryptography] Dark Mail Alliance specs? 
Right.  The last telegram was only sent a few years back.  They are
still delivering letters by hand in some countries.  Windows XP is still
in use.
Alternatively, there is a clear play for someone who can create an email
look-alike that uses a completely separate protocol / means.  This could
be a frontend to Jabber / OTR or Skype or ... or it could be an API into
the various messaging agents that advertise email delivery to a certain
select group from the addressbook ... or it could be a new viral
download that runs as its own service and then gateways out to the old
email world when not talking to "us".
Point being, I think the answer will be found by changing our
internalised understanding of email, not by securing it as it is now
The problem with this is that it breaches some of the laws of security.
 To secure, you must have a definition of what security is (private?
auth or auth? transactional?) and that cannot be done without resort to
humans and an application.
E.g., in some security contexts, unshared messages mean secure messages,
and in other contexts, shared messages mean secure messages.
When you say "small proportion" you mean those that are skype to skype?
well, we "know" that the protocol was changed pre-Microsoft so as to
open certain features up, including listening to chat.  No details of
course.  Snowden "knows" that Microsoft and Skype are partners of the TLAs.
Is there any information that certain governments are tracking and
listening dynamically to phone calls?
(I'm interested in facts, not popular beliefs...)
Ah.  I sense the implication of a threat actor.
Note however the shift from apples to oranges.  These systems that I
suggested deliver more security than alternates, even when they have
some holes in them.  If we are going to compare the holes in one, we
have to compare it to the holes in another.
Doesn't work.  Never has, never will.  Encryption has to be provided,
every time, all the time, and at no additional cost.  It's the only way
we've ever successfully fielded it.  Skype, SSH, GSM, etc.
If it can be turned off, then your attacker will turn it off.

@_date: 2014-03-31 15:40:26
@_author: ianG 
@_subject: [Cryptography] ADMIN Re:  Amateur Radio Authentication 
Yes, it's a nice reflection of what people need and we can't provide.
Or can we?
!    Is that... *everything that is wrong with TLS* in only two sentences?
Right now, entirely believable.  Tomorrow, who knows?

@_date: 2014-04-01 02:06:50
@_author: ianG 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve numbers 
In some sense, the NIST document for Dual_EC was long published... but
not queried on the day it seems.  It is worth reading the new paper on
how they minded your Ps and Qs:
Fascinating how the authors had to reverse-engineer the hardcoded params
in binary libraries in order to change the defaults...
Wait!  That's not foolproof, it won't work for Goldman-Sachs or JP Morgan.
ps; couldn't resist, it is the day for it ;)

@_date: 2014-05-06 22:15:35
@_author: ianG 
@_subject: [Cryptography] crypto software design advice; 
If you trust your crypto then this is a way to deliver more value from
it.  More reliability over all as you just have one crypto-channel to
worry about and improve.
Also, if you do trust your crypto, but wonder how to detect attacks,
then it is likely that the admin interface is a higher value target than
any particular pass-thru traffic, so you'll see more attention to that
key.  Watch it more closely, ignore the rest?  Speculation...
If one is doing some form of high security setup then one would likely
have the admin access come in through another net / interface.  This
might be as simple adding an option to open two sockets and combine them
into one incoming stream internally.  Or as differentiated as only
allowing UNIX style commands in some fashion, so the admin has to SSH in
and run some local command.
Also, if you are subject to DOS considerations then a flooding on the
traffic channel will also impact admin access.
But I agree that having one single channel makes a lot of sense, from a
software engineering perspective.

@_date: 2014-05-15 14:26:54
@_author: ianG 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
I've noticed that we always get long threads when the subject is (a)
well known and (b) there is an identifiable disagreeing party, who might
or might not be here.
On the other hand, when opinions aren't well formed, most posts go with
less attention.  Which seems to indicate we prefer to spend time honing
our existing treasures & beliefs rather than developing new ones.
I personally had a bit of a wakeup call last month when I researched the
origins of Bitcoin for a post/forum I was at (happening every week these
I found that Bitcoin was aired in this very forum, back in 2009/2010.
Hal and James and others tackled it, gave it their best.  I did not.
Why?  Because I have this belief that PoW breaks Gresham's Law and
therefore it'll never work as a money.  Big oops.  I'm trying to be more
accommodating with my anti-beliefs now...

@_date: 2014-05-17 10:16:43
@_author: ianG 
@_subject: [Cryptography] [cryptography] Is it time for a revolution to 
TLS does the job so badly that using a different method is just as
plausible.  People fight to avoid TLS already, they'd rather send stuff
in the clear if they could.  So just solve the problems they have.
Yes, this is the way in.  One problem that occurred with TLS was that
there was an assumption that the job was to secure the reliable stream
connection mechanics of TCP.  False assumption.
Pretty much nobody uses streams by design, they use datagrams.  And they
use them in a particular fashion:  request-response.  Where we went
wrong with TCP was that this was the easiest way to handle the mechanics
of getting the response back to the agent that sent the request.
Without TCP, one had to deal with the raw incoming datagrams and
allocate them to the different sending agents.
A second problem was that the design was too intertwined with commercial
PKI so certs were hung on the side as a millstone for server
authentication and discarded as client side, leaving passwords to fill
that gap.  A mess, which is an opportunity for redesign, frequently
exploited by many designs already.
Right, divide and conquer.  This is one way forward.  What I can't see
is the right way forward.  This is why I see the market for replacements
in some form of competition already, and maybe the way to accelerate
this process is to formalise the competition;  as part of the
competition we also leave the requirements open.
Right.  Compatibility support will happen regardless.  But if you make a
clean break, then people can work to that target -- they can upgrade all
their code to the 2.0 version and one day a time comes to start cutting
out the old.  But to do that we have to have a clean target.

@_date: 2014-05-18 14:01:58
@_author: ianG 
@_subject: [Cryptography] [cryptography] Is it time for a revolution to 
Hi Rich,
Let's go back to basics [0].  What is a datagram and what is a stream?
A datagram has a fixed number of bytes.  It's a packet.  It's a cohesive
record that relates to one thing, which is transport-independent of any
other thing.
A stream is a contiguous series of bytes, with no end in sight.  We
don't know how many are needed, we have to be prepared for more.  We
also have to deliver the bytes in order at the other end, and stop we
can't do more.
So, when a HTTP request goes from client to server, is it a datagram or
a stream?
It's a datagram.  Because we know precisely how long it is.  What comes
back is also datagram, as it is a precise length, and indeed there is an
internal length header so as to make it easier.
Next question is, what happens when we have a request-response model
such as the web where a datagram request elicits a datagram response?
The only requirement here is that the response datagram has to make its
way back to the sender.  So we need a connection, or a session mechanics
of some form [1].
It turns out that we can layer our RR over a stream connection protocol
such as TCP and avoid the requirement to do connection mechanics
ourselves.  OR, we can layer our RR over datagrams, and then we have to
write our own connection mechanics.
It also turns out that we can layer a stream protocol over a datagram
underlying network, OR we can layer a datagram protocol over a stream
network.  And, sometimes it is hard to see the lines between them.
The question is, which is the most efficient to do.  This might depend
on history, good design, and knowing enough of the landscape;  skip to
Jerry's last two paras:
[0] yes, this is a top post ;-)
[1] the clue here is that we need it anyway...

@_date: 2014-05-19 12:53:06
@_author: ianG 
@_subject: [Cryptography] [cryptography] Is it time for a revolution to 
Hi John
Both.  An essential measurand for a revolution to replace TLS will be
how well the contender handles HTTP.
Right ==> Mostly.  If you follow the REST/CRUD philosophical upgrade to
the web, then we can say yes, SHOULD be.
Yes, now Jerry's comment about how we can look back at history and see
how the various additions and layers have accreted.  So, with such a
complicated and evolved history, eventually someone was going to do [2]
... because they could.
Now, what would have happened if HTTP had gone on a different path being
datagrams?  Well, someone would have done [2] in a different fashion.
No big deal.
The existence of these corner cases isn't evidence that the web is
basically a datagram request-response model, it's evidence that people
did what they could with the underlying tech assumptions.
Again, as above.  Probably, once we were using TCP, someone found a
corner case where they could exploit the now-unnecessary length information.
That doesn't change the basic argument.  Go back and analyse those
features using a web-as-datagrams model and you might be pleasantly
surprised -- it's just as easier, often easier.
The one I used to laugh about was the April Fool's RFC of layering of
datagrams over TCP ;)
Right. SPDY however (iirc) isn't enough, what it does is tweak the stack
as much as it can without breaking the stack.  Now I'm guessing that out
of SPDY came the impetus to create QUIC.  That's because if you go
through the exercise as outlined by Lynn Wheeler, you realise that there
is a better way to do it.
But it really is a completely different stack.  Hence, the subject line.
Right.  I do a payments-purposed one myself.  But I have little chance
to get people to adopt it [0].  Hence the subject line:  we need a
revolution before we can reset the security.
[0]  well, Bitcoin has shown one way... ;)

@_date: 2014-05-19 18:04:30
@_author: ianG 
@_subject: [Cryptography] [cryptography] Is it time for a revolution to 
Lol... any business person worth salt can do the numbers to show that
micropayments is a non-starter.  Doesn't work, never has, never will,
but every couple of years there is a fool who thinks they can change the
Yeah.  And Paypal which was going to revolutionise payments and put the
central banks out of business, and and and.
Part of the problem is that the winners get to rewrite history.  From a
professional pov we have to strip out the rewrite to get to the facts.
Which brings us full circle back to SSL... my old rant below ;-)
ps;

@_date: 2014-05-19 18:13:05
@_author: ianG 
@_subject: [Cryptography] [cryptography] Is it time for a revolution to 
Right, these are all examples of edge cases where it can be done that
way, because ... it can be done that way.
Let's tackle dynamic pages.  We don't know how long they are,
apparently.  So we have to sit and wait?
Well, no.  That's just how it does it now.  PHP for example simply
exports the stream IO interface out to the program, so the programmer
can start writing out the page as and when it is calculated.
But this is not what well-written PHP programs do, IMHO, or at least
they do not need to do that.  A well-written program can cache the
entire lot, and write it out in one block.  For efficiency, for error
handling, and for security.  So we can call this PHP practice for what
it is:   bad!  no surprise there.
Bring on other examples!  I'll lay 10 to 1 that most dynamic page models
are really datagram models.

@_date: 2014-05-22 08:06:49
@_author: ianG 
@_subject: [Cryptography] The Trust Problem 
Follows is a long rambling response.  This might mean I don't know, or
it might mean I think it's a hard problem...
We're in trouble already.  Firstly, "secure" and security models are ...
assumed.  Likely, borrowed from some crypto book and rehased for the
ages, so all the top acronyms inserted, I'm guessing.
So we don't actually know as yet what we are protecting the "important
documents" from.  loss?  theft?  spying?  teenage daughter?  spooks?
extortionists?  local council, big state?  mice?  heirs?
Then, trust.  Does one trust on the basis of a website?  No, not really.
 Trust is something you build up over multiple transactions,
measurements, recommendations etc.
Multiple transactions in this sense build up trust that the data is not
lost.  But secure?  If we think about it, can only be seen /in the
attack/.  In the absence of attacks, a secure system works as well as a
non-secure system.  Only in the breach is security going to make a
difference.  And only knowledge of that breach is going to inform us and
therefore build trust in any measurable sense.
Reputation -- who are these people?  As posted recently Jeff, the
Lavabit debacle reveals behaviour under attack -- Ladar Levison shut
down his company.  He then went on to work on that secure email thing.
That speaks volumes.
Another myth.  Is there any scientific evidence for this?  Might make a
suitable grad project ;)
Using OpenSSL is a signal.  The nature of signals is that they are
presumed at face value, but can be confused.  Other signals might have
included NIST standards, FIPS testing, NSA review, verifiable randomness
in seeds, etc.
What people tend to do is look at all the signals on offer and see if
they are aligned in the same direction.  But as you point out, the
signals popular a few decades ago "uses RSA 1024 and DES 64" are not
really sufficient any more.
Information on actual attacks is the real info we want, because it shows
the product doing the job.  Problem is, the existence of a failed attack
is taken as a negative signal by the MSM, whereas it should be seen as a
positive signal;  both evidence of demand and evidence of defence.
Promote your hacked company wherever and whenever you can.
Right.  In the absence of any really useful information on attacks, an
audit or review could help.  The review by Benson on Skype helped a lot.
(What raised eyebrows was that it was never repeated.  What killed
Skype's reputation dead was that they ditched the security model without
cause, without real notification, and left the Benson report up there.)
Basically, disclosure.  Maybe the new signals include "depth of
disclosure" ?  There are two components to this;  how we do things (inc.
OSS) and what we claim.
Claims are also important because it sets a statement that can be tested
over time.  So what is the cost of a broken claim?  Can a company put a
credible claim on the table?
Skype's broken security claim has cost them what?  Embarrassment for
Microsoft and Skype, I guess.  But the former cares little, and the
latter is probably happy with the phone market.
What would happen if there were real fallout?  RSA suffered quite a lot

@_date: 2014-05-22 15:54:45
@_author: ianG 
@_subject: [Cryptography] The Trust Problem 
Just on the "claims" thread:
Snapchat treated claims with fairly callous regard:
What was the cost to them?  Are they being rewarded for lying or punished?
I wonder what it would take to create a reasonably robust measurement of

@_date: 2014-05-25 13:07:44
@_author: ianG 
@_subject: [Cryptography] USG asks for time served (7 months) as Sabu's 
Interesting!  Aside from the human interest aspects of the story, the
FBI calculates some damages (blue page 8, edited to drop non-damages
In the PSR, Probation correctly calculates that the defendant?s base
offense level is 7 pursuant to U.S.S.G. ?2B1.1(a)(1) and correctly
applies a 22-level enhancement in light of a loss amount between $20
million and $50 million [4]; a 6-level enhancement given that the
offense involved more than 250 victims;
4 This loss figure includes damages caused not only by hacks in which
Monsegur personally and directly participated, but also damages from
hacks perpetrated by Monsegur?s co- conspirators in which he did not
directly participate. Monsegur?s actions personally and directly caused
between $1,000,000 and $2,500,000 in damages.
That last number range of $1m to 2.5m is interesting, and can be
contrasted to his 10 direct victims (listed on blue pages 5-6) exploited
over a 1 year period.
One could surmise that this isn't an optimal solution.  E.g.,
hypothetically, if the 10 victims were to pay each a tenth of their
losses, they'd raise a salary of 100-250k and put perp to productive
work, and we'd all be in net profit [0].
Obviously this didn't efficiently solve in society due to information
problems.  LulzEconSec, anyone?
[0] additional comments on the 'profit' side:
blue page 13:  "Although difficult to quantify, it is likely that
Monsegur?s actions prevented at least millions of dollars in loss to
these victims."
blue page 16: "Through Monsegur?s cooperation, the FBI was able to
thwart or mitigate at least 300 separate hacks. The amount of loss
prevented by Monsegur?s actions is difficult to fully quantify, but even
a conservative estimate would yield a loss prevention figure in the
millions of dollars."

@_date: 2014-05-27 12:21:48
@_author: ianG 
@_subject: [Cryptography] client certificates ... as opposed to password 
As an aside, the trick is to bypass Apache config on client certs and do
it in code.  It turns out to be no worse than passwords to code & run,
much easier to manage because change is outsourced, and it lacks the
"oops-breached" moment of terror.
Not that I'm disagreeing with that.

@_date: 2014-05-28 09:38:04
@_author: ianG 
@_subject: [Cryptography] client certificates ... as opposed to password 
Yep, concur.
If you're in your own app, talking to your own server then you might as
well use something better than certs/PKI/x.509.  The heavy lifting will
be less, the customisation will be real, and the experience can avoid
all the minefields.
Like Twitter ... the point of client certs is only when you you're using
the web as per Mozilla-think.

@_date: 2014-05-29 00:07:07
@_author: ianG 
@_subject: [Cryptography] client certificates ... as opposed to password 
Well, it passes the demo but not really practical.
The problem is that within a community of say 10+ there is always
someone who is losing their key for some reason.  E.g., cert expired.
Which then requires a long period for that person to wake up and find
another cert.  During that time, that person's offline.
My call is that S/MIME fails routine use.
The only practical way around this is a keyserver approach, and even
that requires the keys to not expire, practically.  Note that the same
problem occurs with OpenPGP;  when people expire their keys too
frequently, others get out of sync and stop talking to them.  Only the
NSA enjoys this scenario.
Use case happily from the CAcert community.  Short story:  every Assurer
has to have a cert in their browser so the provisioning problem is
solved by some other factor.  With that benefit, client certs work fine
*iff* the client software is up to the job.
For email, yes.  It is only the GUI clients that are not really robust

@_date: 2014-05-29 14:30:53
@_author: ianG 
@_subject: [Cryptography] client certificates / client-side proxy 
Same as it was, 10 years back when this whole security thing started to
become interesting.
So why is good UI design so hard?
Here's my answer, or at least the journey to understanding:  Have you
ever tried to talk to a team and asked them to do something radically
The problem isn't the design.  It's the institution.

@_date: 2014-05-31 16:22:40
@_author: ianG 
@_subject: [Cryptography] Is it mathematically provably impossible to 
Talking about proof of work and so forth, seen on the net:
In Bitcoin, such a fork is useless, since you?re just increasing the
amount of time you would need to catch up. In blockchain-based proof of
work, however, it is a serious problem. The reason is that if you start
a fork straight from the genesis block, then while your mining will be
slow at first, after a few hundred blocks you will be able to fill the
blockchain up with contracts that are very easy for you to mine, but
difficult for everyone else. One example of such a contract is simply:
i = 0
while sha3(i) !=
    i = i + 1
You know that the contract will take exactly one million rounds before
the hash matches up, so you can calculate exactly how many steps and how
much gas it will take to run and what the state will be at the end
immediately, but other people will have no choice but to actually run
through the code. An important property of such a scheme, a necessary
consequence of the halting problem, is that *it is actually impossible*
(as in, mathematically provably impossible, not Hollywood impossible)
*to construct a mechanism for detecting such clever contracts in the
general case without actually running them*. Hence, the
long-range-attacker could fill the blockchain with such contracts,
?mine? them, and convince the network that it is doing a massive amount
of work when it is actually just taking the shortcut. Thus, after a few
days, our attacker will be ?mining? billions of times faster than the
main chain, and thereby quickly overtake it.
The bit half-way through second para.  It seems to be an interesting
property;  can someone work through the logic of it for me?
I'm specifically wondering whether we are limiting this case to all
backdoors, backdoors of the nature of a public-private key pair, or
something in between.
E.g., if it is impossible to detect any backdoor, why do we favour open
source, which might provide a neat mechanism to insert undetectable back
doors?  Just to be controversial...

@_date: 2014-06-01 04:26:50
@_author: ianG 
@_subject: [Cryptography] What is going on with TrueCrypt? 
============================== START ==============================
I have a feeling that this thread has already sunk to a
GPL-versus-the-world crusade.  Before responding, and allowing pause for
reflection, can someone post a current link to the license, so both
sides can be considered?

@_date: 2014-11-07 15:05:39
@_author: ianG 
@_subject: [Cryptography] Wind River Security Features and Cryptography 
Given the strategic obfuscation of the crypto controls, is anything
derived from analysis reliable?
BIS's actions may be misinformed, our views may be misinformed, and it
may be that there is no meaningful information to be found at the source
of this strange turn of events.
We don't know what's going on.  In such a case, the wildest possible
claims are as confirmed as the most sensible explanation.
"We believe this to be the first penalty BIS has ever issued for the
unlicensed export of encryption software that did not also involve
comprehensively sanctioned countries (e.g., Cuba, Iran, North Korea,
Sudan or Syria). This suggests a fundamental change in BIS?s treatment
of violations of the encryption regulations.
Historically, BIS has resolved voluntarily disclosed violations of the
encryption regulations with a warning letter but no material
consequence, and has shown itself unlikely to pursue such violations
that were not disclosed. This fine dramatically increases the compliance
stakes for software companies ? a message that BIS seemed intent upon
making in its announcement."
So, out there in crypto-product land, there has been a sea-weather
change.  BIS, Snowden revelations, open source, and the politics of the
USA administration in its war on everything.  In practical advice, the
above legal guys say:
"Encryption is ubiquitous in software products. Companies making these
products should reexamine their product classifications, export
eligibility, and internal policies and procedures regarding the export
of software that uses or leverages encryption (even open source or
third-party encryption libraries), particularly where a potential
transaction on the horizon ? e.g., an acquisition, financing, or initial
public offering ? will increase the likelihood that violations of these
laws will be identified."
Your average crypto-CEO is advised to ramp up on cost but provided no
guarantees!  This means for American proprietary sellers:  raised costs
across the board.  Uncertainty of exportability, uncertainty of risk
explosion (can an exporter reasonably predict BIS's next actions).  Add
to that, thanks to Snowden uncertainty of interference and *perception*
of interference for branded "made in USA" crypto product.
If I was them, I'd say "it just ain't worth it to sell good stuff overseas."
Just like the Crypto Wars in the 1990s, the new war will push a lot of
cryptographic work out of USA.

@_date: 2014-11-10 12:01:53
@_author: ianG 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
I agree in theory with what you're saying above, but I'll side with
Peter on this one.
I think in practice, the issues of dodgy code and weird bugs still
outweigh by o(100) the likelihood of exotic crypto attacks.  Doing the
sign-then-verify routinely has caught a lot of bugs, whereas attacks on
keys in the wild are rare, and at this level are almost unheard of.
ps; E.g., as a recent case in point:   My sign-then-verify on androids
was failing, every time on one phone.  This was eventually tracked down
to a BigInteger bug in androids <= 2.2.  The Bitcoin people had already
tracked it down, but I don't know if they were doing sign-then-verify.
Or if they were losing money on this bug...

@_date: 2014-11-15 12:46:26
@_author: ianG 
@_subject: [Cryptography] Saving Bletchley Park 
A book about Bletchley Park, birthplace of modern cryptanalysis, seeking crowd funding.  Note that it has already achieved its target, but a few extra contribs would not hurt:
"This is a story about saving Bletchley Park, one of the UK?s most important sites of historical significance. It begins with Alan Turing and the team of codebreakers who worked there during World War II, and it ends with plans to transform it into the world class heritage and education centre it deserves to be. In between is the story of the hundreds of people who have dedicated years of hard work and determination to save it."

@_date: 2014-11-15 16:40:36
@_author: ianG 
@_subject: [Cryptography] FW: IAB Statement on Internet Confidentiality 
For what it is worth, I twittered the below statement last night, and it got 2 orders of magnitude more response than anything I've ever said.  I conclude that the IAB's statement has struck a public nerve; there is clear approval in the public's mind.
ps; I submit that this is a sensible top-post ;)

@_date: 2014-11-17 09:00:36
@_author: ianG 
@_subject: [Cryptography] FW: IAB Statement on Internet Confidentiality 
Curious.  Reddit is read by geeks, I guess, whereas most of the twitter response I saw was less directed.  OK, that's speculative.
Well, the approach is more to retrofit the key ones.  TCP is in play, there is a group called TCPInc which is looking at a few ways to make it opportunistic.  There is already a protocol fielded for that called TCPcrypt, which has some serious lab testing.
The approach is opportunistic.  Eg., for TCP, do a key exchange startup using the optional extensions capability.  If that works, use it for packets, if it doesn't, back off to unencrypted.
Then, as a bonus, the key exchange result is made available to wider protocols through some undefined socket mechanism.  Applications are then capable of using the kex to do wider authentication.  Also, there is scope for saving the key and using it a new startup.  But these things are more for later phases.
Gawd no, I hope not.  They're a committee, they're not competent to do new work that the rest of us have failed to do.
Let's call it beyond purview, but I'd rather say it is missing the point.  The goal is unauthenticated encryption, first and foremost.
Wot?  I encrypt all the time without dealing with legal issues.  No idea what this means for a protocol, what are you going to do, arrest a packet and read it it's rights?
Nah.  You are thinking 1990s full security models, tracing back to military aggressive threat models.  This is not what this is about.
This is opportunistic security, where the attacker is moved from pervasive surveillance passivity to an active decision making.  He must attack!  He must knock down the protocol if he wants to eavesdrop.  No more free lunches, no more rolling over and playing doggy.

@_date: 2014-11-18 00:31:44
@_author: ianG 
@_subject: [Cryptography] IAB Statement on Internet Confidentiality 
Yes, definitely.  Before, we didn't know who the attacker was.  We just handed everything over on a plate.
Now he has to attack.  Now we know who the attacker is.  It's a dramatic step forward.

@_date: 2014-11-20 08:59:50
@_author: ianG 
@_subject: [Cryptography] IAB Statement on Internet Confidentiality 
Right.  So far, if "defined in this way," and we stop there.
Right, so one measure is TCPInc which adds opportunistic security into TCP without the user doing anything.
Right, and no auth, and needing to understand the protocol.  This is not a good direction to go.
Yes.  So the stage is set.
Skirmish 1:  attacker is just doing passive reading ov everything.
Counterskirmish 2:  we switch on OS and bite thumbs at their house.
Strike 3:  attacker strips off the OS layer and reads the traffic.
Counterstrike 4:  we read the logs, and draw swords.
Attack 5:  ...
Counterattack 6: ...
We're setting up an arms race.  The approach here is one of strategy. To get to the point where we can have a full scale war, we have to escalate it through some skirmishing first.
In order to do there, to get the attacker into the field, we have to force the attacker to actually attack.
Yup.  This process has to be seen against the alternate choices we've tried:  Do nothing (we got pillaged), and use TLS (failed to deploy).

@_date: 2014-11-20 09:42:49
@_author: ianG 
@_subject: [Cryptography] New free TLS CA coming 
Answering 3 at once.
For lolz, that was my immediate reaction too :)  But it doesn't really accord with the facts.
What happened to CAcert was more complicated.  They tried to create a CA with governance only on the user side, not very much if anything at all on the systems side.  In order to put some amount of governance into the systems side, they had to write a fair amount of doco, develop quite a few procedures, and roll them out.
It all took a while, like about 3-4 years.  Now they are doing it, and now it is pretty good, and would likely pass the audit of that time. For example of how good their governance is, CAcert have more or less defeated a steady run of intel attacks to insert trusted spooks into the operation, which cannot be said for any other organisation that has been named recently as having been insider-breached, e.g., google, Mozilla, not to mention the happily owned slaves such as Cisco.
But unfortunately, the game changed while CAcert was doing this.  The largest factor of this was the arisal of phishing, which triggered a brief rebellion by the vendors who met in secret in Toronto (?) one day.   This then caused the CAs to get spooked, who were already running around trying to set up a cozy new cartel, so they headed the rebellion off at the pass, brought the vendors into the fold, and then worked in secret for 2 years to craft "Baseline Requirements."
By the time they were done, and CAcert was still puddling along working at its glacial speed with practically no funding, the game had shifted substantially.  They now had to deal with 3 or 4 audits:  BR, WebTrust, pre-EV and EV.  Also, the original browser equation had changed from Firefox + IE to add in Chrome, and Safari on Mac OSX was now interesting again.  So the Firefox-first notion failed as well.
Game over.  You'll notice that only some the problem can be attributed to the anti-competitive behaviour of vendors and CAs;  another portion attaches to the luck of having more browsers to approach, and a fair-sized chunk lands at CAcert's door for being just so darned slow.
Right, so this is the other path.  CAcert could in theory purchase a sub-root access from a bigger CA.  I'm not sure how viable this is, nobody at CAcert really likes that idea, and I've not come across a CA that likes it either ;)
Well, to clarify:  as it was taking so long, and as people were 99% stupidly claiming that this was a push-button operation, and it was Mozilla's fault for not pushing the button, I withdrew the application that had provisionally been put in place.  It had no real effect anyway, it was all going to be in the real application, which last I heard is still not really on the table for lack of an external auditor.

@_date: 2014-11-20 10:58:42
@_author: ianG 
@_subject: [Cryptography] New free TLS CA coming 
Well, those words aren't in there, but you can guarantee that such a product will be resisted.  The industry is set up to give certain parties a pay-off, and the HSM manufacturers need theirs.
They aren't about to give it up.  If there is any possibility of an open source HSM turning up for serious, they'll go to one or other of the cartels and get some words changed to knock it out of consideration.

@_date: 2014-11-25 00:57:01
@_author: ianG 
@_subject: [Cryptography] OneRNG - another USB RNG 
I got one, I'm going to have a go at adding it to our RNG system over xmas.  For those who like photos

@_date: 2014-11-26 17:04:57
@_author: ianG 
@_subject: [Cryptography] Underhanded Crypto 
The Underhanded Crypto contest was inspired by the famous Underhanded C Contest, which is a contest for producing C programs that look correct, yet are flawed in some subtle way that makes them behave inappropriately. This is a great model for demonstrating how hard code review is, and how easy it is to slip in a backdoor even when smart people are paying attention.
We?d like to do the same for cryptography. We want to see if you can design a cryptosystem that looks secure to experts, yet is backdoored or vulnerable in a subtle barely-noticable way. Can you design an encrypted chat protocol that looks secure to everyone who reviews it, but in reality lets anyone who knows some fixed key decrypt the messages?
We?re also interested in clever ways to weaken existing crypto programs. Can you make a change to the OpenSSL library that looks like you?re improving the random number generator, but actually breaks it and makes it produce predictable output?
If either of those things sound interesting, then this is the contest for you.

@_date: 2014-11-27 22:21:26
@_author: ianG 
@_subject: [Cryptography] [cryptography] Underhanded Crypto 
This is a good question.  I suspect the idea here would be, how much code can you reduce it to?  The more elegant of the obfuscated C entries had nice and tight code.
Yeah so my vote on seeing such a thing would be "hohum" and "how much do you wish to pay for the security review?"
Sounds good to me.
OK.  I'd rule out simple byte-wise changes to constants myself, but on the other hand this is a valid attack.  So why not?
Right, but they can point out that it is a weakness.  This is the Dual_EC thing, where the params came from *somewhere unverifiable* and that later turned from benign to smelly.
Or a few more years to shake out the easy ways :)

@_date: 2014-11-28 10:22:04
@_author: ianG 
@_subject: [Cryptography] [cryptography] Underhanded Crypto 
Given that it is signalled in advance, and given that for the most part our job is to stop these things and thinking about how to do it is the flip side of the same coin, I suspect reputation isn't an issue.
Seems like we'll find out tho, as Peter and Bear are willing to give it a shot.

@_date: 2014-10-01 10:33:23
@_author: ianG 
@_subject: [Cryptography] NSA versus DES etc.... 
One point for:  Suite A and friends, which remains a heavily shared secret.
One point against:  In this particular place called cryptography, there
is a frequently repeated aphorism "the enemy knows my algorithm"
recently attributed as Shannon's maxim and historically as Kerckhoffs'
2nd Principle.
I guess the various well-funded enemies have figured out each other's
secret algorithms by now, but out of politeness and common interest they
cartelise the secrets.

@_date: 2014-10-04 12:08:42
@_author: ianG 
@_subject: [Cryptography] 1023 nails in the coffin of 1024 RSA... 
(some skepticism about whether this there is really a break in OpenSSL,
but the rumour mill will no doubt throw mud on the 1024 bit part as well...)
OpenSSL bug allows RSA 1024 key factorization in 20 minutes
So just a few minutes ago has finished a talk at Navaja Negra 2014, the
third? most important security congress in Spain, where the speaker (a
member of the organization) claimed to have found a bug in OpenSSL RSA
key generation, which he is able to exploit to factorize N into p and q
in around 20 minutes (on a laptop). He did a live demo. I wasn't there,
but some friends were.
He claimed:
    The bug originates in this lines of rsa_gen.c:
    117 bitsp=(bits+1)/2;
    118 bitsq=bits-bitsp;
    the main problem being that the rounding of 1025 isn't downwards but
upwards, resulting in bitsp= 513 and bitsq=511, which, supposedly, later
on the code and due to compiler optimizations, causes the bug.
    It affects all versions of OpenSSL.
    He is neither going to report it to the developers, nor publish
I personally think he's full of shit, but the fact that he's a member of
the organization and thus not only his personal prestige but also the
organization's is at stake, makes you wonder. Anyhow, we'll see.
I posted it yesterday to netsec but the mods removed it. Let's discuss
it here!
Edit 1: so my friends talked to him today, and he's serious about it. He
says he's broken 1024 keys on Amazon clusters in 18 seconds.
Edit 2: he claims some guy from Argentina found the same thing 6 years
ago, and has been trying to show it on cons since then, but no con
accepted his talk because they wouldn't believe him.
Edit 3: he also says the attack consists in trying "probable primes",
whose probability is generated by said bug. Might it be some variation
on Fermat's attack?

@_date: 2014-10-09 07:12:32
@_author: ianG 
@_subject: [Cryptography] The world's most secure TRNG 
My guess is that if you don't have an easy defined interface (file? tty)
then it won't work in the marketplace.
In terms of the nasty malware, what would be nice would be a firewall.
A device that has male & female and sits there and watches for naughty
traffic.  If this came with a good RN source as well, I'd reckon it
would be a hit.
Yes, otherwise it will be noisy :)  You don't want it interfering with
random gear.
You could probably get away without in a prototype device and encourage
someone to do some testing...
Oh, no :)  In the crypto world we deal with bit-rated paranoia.  Even
one bit leaked to an attacker will earn the device the BROKEN award.

@_date: 2014-10-11 16:18:45
@_author: ianG 
@_subject: [Cryptography] HP accidentally signs malware, 
indeed, and we need more of them ;-)
That's um amazing.  So a 4 year old expired cert is still a critical
piece of infrastructure, and they are still going to revoke it.  Rather
finishes the argument of whether revocation means anything different
than expiry...  More on Krebs.
Revocation as a system only works if it is reasonable to roll out a new
cert, and this works as long as the scale is small.  It looks like
code-signing can escape that assumption, making one userland cert as
powerful as .. a root cert!
Revocation was always a safety blanket, cute for users but not for
serious applications, so this must be causing some headaches in the risk
ps; HP's comment that they weren't breached is laughable.

@_date: 2014-10-13 11:00:50
@_author: ianG 
@_subject: [Cryptography] HP accidentally signs malware, 
Expiry and revocation were supposed to "mean" the same thing, the cert
could no longer be used.  There was either/or not both, and the
revocation lists typically had scaling problems so had to be kept brief,
'and' was not good.
(Indeed some CAs did revoke on expiry...)
But, there are differences.  It might "mean" the same thing but it can't
mean the same thing, if you get my drift.  Expiry is "can't use" coz you
need to feed the gasmeter to stay warm.  Contractual issue?  Whereas
revocation is "must not use" because there's a gas leak and the house is
about to blow.  Safety issue?
They are completely different in meaning... but not "meaning."
Expiry of course is an optional concept.  If software realised there was
nothing wrong with an expired cert then the game was up.  And, some
software does realise this.  And, expiry can be tricked by changing the
date, so for example the compromised cert (if it is indeed compromise)
can be used to still sign a 3 year old package...  And, it gets very
complicated trying to manage all the corner cases.
So, because of risk analysis not being able to answer the real size of
the problem, HP decided evidently to cover all bases and revoke as well.
The answer to all this is that certs, expiries and especially revocation
simply do not work as advertised.  In short, the only thing that works
is liveness and capabilities, which is the favoured choice for just
about every other system.  But you cannot fix a system like PKI without
staring the architectural myths in the face, and backing off and finding
some honest work to do.  So we're stuck.  HP get tricked that they've
been compromised 4 years ago, and they have to now compromise all their
customers today.  Oops.

@_date: 2014-10-15 01:03:02
@_author: ianG 
@_subject: [Cryptography] SSL bug: This POODLE Bites: Exploiting The SSL 3.0 
SSL 3.0 [RFC6101] is an obsolete and insecure protocol. While for most
practical purposes it has been replaced by its successors TLS 1.0
[RFC2246], TLS 1.1 [RFC4346], and TLS 1.2 [RFC5246], many TLS
implementations remain backwards?compatible with SSL 3.0 to interoperate
with legacy systems in the interest of a smooth user experience. The
protocol handshake provides for authenticated version negotiation, so
normally the latest protocol version common to the client and the server
will be used.
However, even if a client and server both support a version of TLS, the
security level offered by SSL 3.0 is still relevant since many clients
implement a protocol downgrade dance to work around server?side
interoperability bugs. In this Security Advisory, we discuss how
attackers can exploit the downgrade dance and break the cryptographic
security of SSL 3.0. Our POODLE attack (Padding Oracle On Downgraded
Legacy Encryption) will allow them, for example, to steal "secure" HTTP
cookies (or other bearer tokens such as HTTP Authorization header
We then give recommendations for both clients and servers on how to
counter the attack: if disabling SSL 3.0 entirely is not acceptable out
of interoperability concerns, TLS implementations should make use of
CVE?2014?3566 has been allocated for this protocol vulnerability.

@_date: 2014-10-22 09:57:05
@_author: ianG 
@_subject: [Cryptography] CFP by 24 Nov - Usable Security - San Diego 8th Feb 
The Workshop on Usable Security (USEC) will be held in conjunction with
NDSS on February 8, 2015. The deadline for USEC Workshop submissions is
November 24, 2014. ? In previous years, USEC has also been collocated
with FC; for example in Okinawa, Bonaire, and Trinidad and Tobago.
Additional information and paper submission instructions:
The Workshop on Usable Security invites submissions on all aspects of
human factors and usability in the context of security and privacy. USEC
2015 aims to bring together researchers already engaged in this
interdisciplinary effort with other computer science researchers in
areas such as visualization, artificial intelligence and theoretical
computer science as well as researchers from other domains such as
economics or psychology. We particularly encourage collaborative
research from authors in multiple fields.
Topics include, but are not limited to:
* Evaluation of usability issues of existing security and privacy models
or technology
* Design and evaluation of new security and privacy models or technology
* Impact of organizational policy or procurement decisions
* Lessons learned from designing, deploying, managing or evaluating
security and privacy technologies
* Foundations of usable security and privacy
* Methodology for usable security and privacy research
* Ethical, psychological, sociological and economic aspects of security
and privacy technologies
USEC solicits short and full research papers.
Program Committee
Jens Grossklags (The Pennsylvania State University) - Chair
Rebecca Balebako (Carnegie Mellon University)
Zinaida Benenson (University of Erlangen-Nuremberg)
Sonia Chiasson (Carleton University)
Emiliano DeCristofaro (University College London)
Tamara Denning (University of Utah)
Alain Forget (Carnegie Mellon University)
Julien Freudiger (PARC)
Vaibhav Garg (VISA)
Cormac Herley (Microsoft Research)
Mike Just (Glasgow Caledonian University)
Bart Knijnenburg (University of California, Irvine)
Janne Lindqvist (Rutgers University)
Heather Lipford (University of North Carolina at Charlotte)
Debin Liu (Paypal)
Xinru Page (University of California, Irvine)
Adrienne Porter Felt (Google)
Franziska Roesner (University of Washington)
Pamela Wisniewski (The Pennsylvania State University)
Kami Vaniea (Indiana University)
With best regards,
Jens Grossklags
Chair ? USEC 2015

@_date: 2014-10-22 10:20:32
@_author: ianG 
@_subject: [Cryptography] Simon, Speck and ISO 
What pray tell is the ISO JTC1/SC27 and who cares?  Or to put it
cynically, who is the NSA trying to ease into this time?
(Seriously, we can only comment on the threat of Simon & Speck if we
know what the business model the security model needs to defend.)

@_date: 2014-10-26 09:45:50
@_author: ianG 
@_subject: [Cryptography] In search of random numbers 
Actually this is hard.  To do it properly requires the measurement of
entropy.  This is a mess.  It's a hard problem to get right.  Sure there
are some people who think they've got the handle on it, but they aren't
enough for this task.  Hence, some who are pragmatic would say, no,
never block.  Just do the best you can, supply what you've got and take
on the knocks.  Sucks if you want your light switch to have an SSH key...
What might be easier is a simple 1st order approximation such as
counting the number of light switch hits (and using the difference as
entropy).  In the manual you can have a line that says "SSH key will not
be generated until lightswitch hit 10 times..."
That part I agree with, but the challenge is in getting the IoT people
to do it.
You only get product recall when it is likely to kill the user.  Bad as
the randomness issue appears to us, I'm not sure we're there yet.
Huh?  Light switches require an electrician.  Problem part is sub $1,
install is o($100).  A whitegoods replacement part is a callout fee,
even the lightbulbs in a fridge will challenge some people.

@_date: 2014-10-28 13:32:03
@_author: ianG 
@_subject: [Cryptography] Paranoia for a Monday Morning 
As some of us have been saying for many a year, the crypto is the least
of your problems.  Concentrate on the software engineering, and when
you've got that right, you may have cause to demand better crypto.  And
stop this sophistry with algorithm agility and other vanity concepts...
Right.  The experience I had reduced to this, if I can compress it.
Following the Browser wars that destroyed Netscape, Mozilla swore to
implement standards, and clawed back the high ground.  Others sort of
followed suit.
But, standards don't change in ways that improve security for end users.
 Standards have no feedback loop back to endusers because they aren't
represented, and standards groups are stuck in 1990s security model
thinking (e.g., the ITM).
Hence, the secure browsing system might have started out with some
notion X of security back in 1994.  But absent a correcting feedback
loop back to users, it was set up to deviate from X in a negative
direction.  Spencian mechanics apply, market for silver bullets and all
Enemy action is part of it, but what the enemy did was leverage our own
accident-proneness.  The notion of security from PKI was strongly pushed
by the NSA, through many channels, because they knew they could backdoor
the CAs.  This was a big thing at the time, it was written somewhere
that they 'bet the farm' on this strategy.
In contrast, it's not clear to me that they understood the MITM agenda
per se which undermined the entire Internet security.  But, once they
saw how it raised the complexity barrier, they would have been all for
it.  "You must defend against the MITM at all costs!"  Including
security for all, unfortunately, but we've got a great story to tell
about MITMs.  So I'd expect that as shills in standards groups and
browsers are outed over time, their actions will be strongly correlated
with MUST-anti-MITM-ness.
Once the PKI was bedded in with standards, the security failure over
time was a certainty.
I suspect the NSA bungled this strategy.  They probably rationalised
that the USG would be protected by their own strong CAs, but it turned
out that the weakness outside that vector was so endemic that everyone
suffered, equally.

@_date: 2014-10-29 11:20:17
@_author: ianG 
@_subject: [Cryptography] In search of random numbers 
Precisely.  It needs to be so cheap that you'd drop it in if it solved
any problem you had.
The way to approach this problem is strategically:
   1. create enough free designs such that there aren't any barriers to
deployment.  No excuses!
   2. create some demand 'pull' from the market such that users,
customers, journos are asking questions of the builders.
   3. create some supply 'push' where those who claim to use an RNG are
rewarded by attention and recommendations.
In order.  No point in saying anything until 1. is in place.  Muzzle the
journes for now.
Go Bill, go Paul!
I'm shocked that a bit of silicon costs that much!
Of course they'll be backdoored!  But this is the wrong way to look at
We need to seed these things throughout the market place.  Once the
standard is established that "you must use an RNG any RNG" then we can
ratchet up the pressure.  The grad students will do that for us, with a
little nudging.  Whole classes of IoTs will be broken.  Armageddon,
apocalypse, gosh oh my.
Then, things will get better as equipment suppliers get sick of their
name being dragged through the mud.
The reason this works is that it is impossible to just fix a broken
industry.  You have to introduce a framework and understanding at all
levels first.  Seed it from the bottom.  Then, when it's pervaded, start
the ball rolling for continuous improvement.  Love your arms race.
ps, the precise wrong way to do it is to involve NIST, IETF, national
standards bodies.

@_date: 2014-09-01 21:06:22
@_author: ianG 
@_subject: [Cryptography] Enigma's Secret Twin 
(snippets only)
Enigma's Secret Twin
Most of us know the story of Enigma, the German cipher machine
eventually broken by clever people at MI6's country house, Bletchley
Park, following brilliant early work by Polish mathematicians. The
achievement, reckoned to have shortened the war by two years, remained
secret for decades. Also secret, and less well known to this day, was
the fact that Britain had Enigma too. That's why the Germans---no
slouches at code-breaking---couldn't read British signals.
The story began in 1928 when the Government Code and Cypher School
(GC&CS)---forerunner of GCHQ and then part of MI6---acquired two Enigma
machines at the Admiralty's request. The Admiralty inexplicably lost
interest but in 1934 Wing Commander Lywood of Air Ministry Signals asked
the GC&CS if he might borrow one. ...
Typex was deployed by the RAF and the army at senior levels, often with
MI6 operators. MI5 and MI6 used it but the Admiralty did not, until the
readability of many of their early wartime signals became tragically
apparent. Typex remained secure throughout the war and continued in use
"Dancing with Eva"/

@_date: 2014-09-03 09:57:16
@_author: ianG 
@_subject: [Cryptography] stories from the real life MITM book 
Evidence of MITMs is so rare it has to be trumpeted.  Snippets only.
To show what the CryptoPhone can do that less expensive competitors
cannot, he points me to a map that he and his customers have created,
indicating 17 different phony cell towers known as ?interceptors,?
detected by the CryptoPhone 500 around the United States during the
month of July alone. (The map below is from August.)  Interceptors look
to a typical phone like an ordinary tower.  Once the phone connects with
the interceptor, a variety of ?over-the-air? attacks become possible,
from eavesdropping on calls and texts to pushing spyware to the device.
Who is putting up ?interceptor? cell towers? The mystery deepens
The discovery ?appears to confirm real-world use of techniques that have
been highlighted by researchers for years,? said Stephen Ellis, manager
of cyber threat intelligence at security firm iSIGHT Partners. While
noting that his company ?cannot confirm the accuracy of this reporting
without further information,? Ellis told us that iSIGHT is ?highly
confident that we have observed real-world use of this technique in
support of another of its uses ? cyber crime [for] financial gain.?
?We have observed and reported on cases in other parts of the world
where actors are known to have set up fake base stations to send spoofed
SMS messages,? Ellis said, ?possibly to send spam or to direct
unsuspecting victims to malicious websites.?
The Federal Communications Commission (FCC) announced last month that it
is launching an investigation into the use of cell network interceptors
by criminal gangs and foreign intelligence.
We asked Goldsmith if he could be mistaken about the towers. Perhaps
they are just commercial ones that seem unusual?
?We can definitely tell? that they?re non-network towers, he said, by
analysis of the infrastructure. These phony towers, without names as
normal towers have, insist to your phone that they must handle the call
and then trick the phone into turning off its normal encryption.
Such a tower tells you that ?none of your towers are currently
available,? Goldsmith told us. It says, ??I?m your tower.?
?If you wanted to listen to a phone call,? he said, ?this would be the
easy way.?

@_date: 2014-09-05 00:23:33
@_author: ianG 
@_subject: [Cryptography] What is the difference between a code and a 
Code -- think codebook -- is a translation of expected symbols:
Onion:   attack
Banana:  reinforcements
Aunt:    supreme leader
Whereas a cipher is something that takes any message, like a Caesar
cipher, which rotates every letter by 3 forward.

@_date: 2014-09-08 13:56:35
@_author: ianG 
@_subject: [Cryptography] sunsetting SHA-1 in Chrome 
The opposite of crudge is not wisdom :)  There is nothing wrong with
112-bit security as long as that's all you need.  For example, in a
protocol HMAC, 112 bit security would be overkill.
The mistake you (and most others) are making here is that because there
is a known attack in one use of one single 160 bit hash (SHA0) then all
160 bit devices are damned for all purposes.

@_date: 2014-09-08 18:05:26
@_author: ianG 
@_subject: [Cryptography] sunsetting SHA-1 in Chrome 
That's a good post, highly recommended for a review of the whole topic.
 It even gets the criticisms levelled squarely against those who should
be embarrassed.  Well, says I, at least, being a least unbiased
protagonist in this sorry story.

@_date: 2014-09-09 17:09:34
@_author: ianG 
@_subject: [Cryptography] distributing fingerprints etc. via QR codes etc. 
It mostly depends on what you are trying to do.  If your worldview is to
distribute PGP keys from person to person, then this above might be
helpful.  But, questions abound...
Right, which takes us to 'why are you distributing keys?'  Or perhaps,
having done all this, 'what are you going to do with the info?'
Keysigning parties struggle to make meaning of the signature and of the
key.  What does it mean when I sign your key?  In some groups it means
"I saw this person" and in others it means "this person's ID matched
their key ID text fields."  Then in other more sophisticated groups it
means "this person is one of us and is reliable" and there is
documentation to back the semantics of that statement.  Then there is a
variant where the statement is meaningless but imposed, so it also means
whatever you think it to means but you can't test it.
It turns out that this statement is far more key to the question of how
to distribute than the mere tech notions of QR codes or keysigning
parties.  E.g., in my work, I provide 'it' to groups of poor people
working to create investments in Africa.  They know each other in the
group and trust each other already by definition (at some level).
The statement then is something akin to 'we are part of the same group'
therefore we can share.
As there is a server involved (another big assumption/question) then
Alice uploads a packet from her Android (++assumption) containing
'everything' to the server, which is indexed with a short term tag
called an IntroCode: like 'ABCD' which is the first N bits of a hash
over the packet.
She speaks that to Bob, who types it in to his phone.  The phone then
downloads the packet from server citing ABCD, and extracts out the
necessary key info from that packet, and returns a packet via some other
server channel (++assumption) to her newly-found friend.
My assumptions here are: small group, server, android, pre-trust.
Change any of those and my solution might change.
ps; obviously I'd like to do QR and SMS as well, but it's all about
getting the minimum tech that does the maximum delivery in place...

@_date: 2014-09-10 03:21:05
@_author: ianG 
@_subject: [Cryptography] phishing, was Encryption opinion 
Just from an opinionated pov:  If it is done through a click-thru
licence, then it's not really done with the end user's permission, only
with their trust that the service won't screw them.  That's because an
end-user is not expected to nor can read the licences in modern Internet
software.  It would be an argument in court as to whether the above
'adjustment' is reasonable or not, but what is not really sustainable is
that "end user's permission" exists unless the end user has been
explicitly asked the question and there is recorded evidence to that effect.
E.g., this would pass the reasonable test:
This is a key point.  MITM was introduced to the net from various
sources as a key threat.  The one big threat that would tear everything
apart.  So people responded to it.
Things have changed, we're now 20 years on and we're not talking about
single threats, single packets, single connections indeed single
anythings.  A protocol that considers MITM as its prime threat is naive.
What should we be protecting?  It ain't the MITM.  So how do we find a
focus or a goal that allows us to develop systems that better do
something of help to users?  It seems like the classical threat
modelling ideas have run out of grunt.
Yes.  What do we have to do to get back to the original design?
Everything HTTPS?

@_date: 2014-09-10 14:13:51
@_author: ianG 
@_subject: [Cryptography] phishing, was Encryption opinion 
Rewriting of history?  Well, I'd say it was already rewritten several
times and we're into the revisionist phase, trying to find out what the
real forces were that made SSL what it was, leading up to today's
post-phishing and post-Snowden world.
The stated threat was against credit cards, its a bit more precise that
the hand-wavy e-commerce term.  There was no value to an attack that
changes your credit card; the value of the alleged attack was all in
revealing the credit card to someone who could snoop.
Which is why SSL v1 was opportunistic, it dealt with the attack at a
reasonable level.  However this was strongly criticised because SSL v1
could be MITM'd and therefore it should ("must") have certificates in
order to stop the MITM attack revealing the credit card.
Well indeed.  But you will note that no CA will state any of that in
writing.  It is all self-marketing.  So whether you can claim this as a
reasoning for the introduction of certificates is difficult because
we're resting on claims we know to be false but deliberately widespread.
Indeed, what we are talking about is *entirely perceptional* in that
there are a group of people who've built their careers on a classical
threat model.
The disconnect is that the concept does not meet the needs of users.
How do we get back to meeting the needs of users?
Yes.  Most people in the western world get by on the ability to use
various visual signals and domains to create a relationship mapped in
the mind.  Including on the net.
The farcity of the situation is proven when you connect to do all your
shopping using the HTTP service, then click to payout -- if the payout
takes you to a remote site called "secure-payments.com" then users don't
Part of the problem I suspect is that most security people haven't ever
really studied marketing and institutional relationships.  There's a
whole world out there that changes all our assumptions.  E.g., it is
deals like Apple's recent pay thing that make things tick, the security
stuff is just geek-candy.
 n
Right.  But marketing tells us that, if we assume a counterfactual that
emails are only sent to companies, then companies would have been sold
certificates...  It's not the emails that are vulnerable to spying, it's
the companies that are vulnerable to selling.
I think we're all in agreement here :)
The reason to point it out is that it is mentally ingrained into some
security folks minds that the MITM is why we do things.  Meanwhile the
world has moved on.  The question is for those people who structure
their entire designs around the MITM (certificates/PKI/etc) whether
they'll be able to retain any relevance, or the world will bypass them.
Unfortunately some of those people have the lock on certain very
important core technologies -- IETF.  It is 2014, and only this year
have they woken up and started working on opportunistic security.

@_date: 2014-09-12 10:20:31
@_author: ianG 
@_subject: [Cryptography] keys, signatures, trust, identification, badges, 
To put it bluntly, PGP community eschews getting too close to the PKI
concept of a CPS, which is supposed to [0] answer this question.
I think there is some merit in confusing the formal channels of
identification.  If I meet some dude at a hacker event, I'm more
interested in who he is than that he has some document from some
overbearing state that tries to compress his personality into a 9 digit
Yes.  But at least it's benign, unlike some other uses of the term.
Well, the fundamental flaw is the digital signature as human signing
token.  If we could crack doing RSA in the brain then it is possible
we'd be able to make statements here, but all designs end up with
"computer executes code which makes multiplication over hash over
document ..." which is unfortunately too far removed from "user performs
ceremony widely agreed to be local assent."
This is heading in the right direction, but again I wonder about
capturing information in a technical sense without the asserting the
providence or pedigree of it.  A perhaps powerful example of this idiocy
is LinkedIn's endorsements which because they have no foundation end up
being noise.
Right, at this level, mere technical solutions that indicate such
trivial facts can then be assembled into something that might enter in
to a decision to trust.  There is a current wave of startups that are
analysing social media data in order to assess whether you are a good
risk for a loan.  As long as they don't go too far, this kind of works,
but the danger and inevitability is that they will go too far on
reliance on this metric base.
It wouldn't be that hard, but one of the flaws in the sense of meaning
is the absence of any real-world implication of the statement.
Typically we would consider things like contracts for legal clout,
escrow or insurance for backup, and/or reputation for soft issues.
If trust is on the table, there has to be skin in the game, to use the
Americanism.  About the only thing possible in the PGP world is
reputation as skin, which isn't that much, and not enough to support
more than mailing-list or academic contributions.  To go further, or to
do it properly, something like CAcert's arrangement is needed.
So, exchanging QR codes is just an optimisation of the old fingerprint
convention.  It improves the tech, the reliability and speed of the old
method, but it changes nothing in the semantics.
Life has moved on.  What seemed clear and useful in the 1990s is now out
of date.  The gold standard today is how social networks do it.  Which
in short is a photo of the person, plus various shared relationships.
I suppose that we should say that those early efforts to put photos
inside OpenPGP keys were on the right track, although they didn't really
take off in any sense.  The question today is how to get that photo
across efficiently and reliably, which will typically dominate things
like QRs, as we need o(100) times the data and complexity.
[0] But does not typically.

@_date: 2014-09-21 23:06:26
@_author: ianG 
@_subject: [Cryptography] new wiretap resistance in iOS 8? 
That's not a rule, it's a plea for unconstrained spending.  The attacker
does not likely spend more than he gains unless he is stupid.  While
stupid attackers do exist, they tend to go out of business in a while.
Well, again, no.  We actually do have a good picture about attackers.
We have about 20 years of experience now in internet attacks.  We know
what phishing takes, we know how APTs work, sort of, we know what
penetration is, and how likely silly attacks like SQL injection are.  We
now also have lots of Snowden stuff.  And we know that the attacker
works through a smorgasbord of attacks, before getting serious.
Only if he can get what is worth something to him.  A state level
attacker is not interested in the contents of my laptop because there
are no state level secrets on it;  in contrast a phisher might spend up
to $10 on the chance that he can steal $1000 from my bank account.
Certainly, if there are multiple attackers (and there are) then we need
to do a pretty sophisticated risk analysis.  If our attackers range from
state level attacks to economic attacks, then we've got a range of
attacks, and sometimes these aren't easy to compromise on.
But for the most part, I welcome any defence against hard attacks such
as the state level thing, because (1) most people are concerned with
boring theft attacks and (2) anything that defends against the state is
likely good or better at defending against economic attacks.
Here's a thought baloon.  If the NSA were to actually take steps to stop
economic attacks, and do so well, but the cost of that is to let them
poke around all our hard drives ... would that be a fair deal?
Most of us are *always* at risks.  Highways, hospitals, all systems use
risk analysis and they accept that some people will lose.  They all use
modelling that turns every successful attack into a damages model, and
they basically decide to mount defences that cost less than the damages
they stop.  It's all economic.  Most of us win this way, and that's the
only way to rationalise the complicated threat environment.
In that I agree.  There is no excuse for cost in encryption, nor in
Well, they are economic minded, but in the minds of their employers not
their customers.  Most security systems have to be "sold" and therein
lies a few complexities.
Yup, full agreement.
Ah, but do distributed key shares really work?  Any studies?  Or is it
all just shamir key sharing irrefutable mathematics bla bla?

@_date: 2014-09-21 23:37:21
@_author: ianG 
@_subject: [Cryptography] Of writing down passwords 
I've been recommending that people write their passwords down for about
a decade now [0].
How else to handle it when the passwords have to be non-memorable
because of dictionary attacks and re-use attacks?
(I suspect this "don't write passwords down" thing relates to the old
days of terminal labs where there were idle and nasty students  who
would make a habit of surfing over people's shoulders to capture a few
letters...  Those days are long gone.)
[0]   down the end, Top Tips,

@_date: 2014-09-21 23:52:49
@_author: ianG 
@_subject: [Cryptography] sunsetting SHA-1 in Chrome 
Very different.  SHA1 sits in a cert for about 2 years, and (originally
[0]) certs were vulnerable to collision attacks.
Whereas a HMAC in protocols sits there for maybe 2 seconds and isn't
really vulnerable to collision attacks.
Yeah.  The problem is that although TLS espoused a goal of
crypto-agility, the evidence suggests it was more to do with
crypto-vanity as seen in a steady stream of new block ciphers.  The
failure of hashes was unexpected so not actually part of the protocol;
whereas the failure of ciphers was expected but didn't really return
In practice, crypto-agility has never really delivered the returns,
cryptography has been the one solid thing we can lean on for a decade
into the future.
[0]   These days, certs are supposed to include a 20 byte nonce which
kills the collision attack.

@_date: 2014-09-23 09:50:50
@_author: ianG 
@_subject: [Cryptography] new wiretap resistance in iOS 8? 
Big concur.
Nothing to be ashamed of there!
We also know that NSA argued for a 40 bit key, and the compromise was 56
Which suggests that they didn't actually have the confidence that they
could crack 64 bits even with DC.  Maybe DC didn't scale?
(I think it is important to keep mining this 'event' because how the
threat actor acted with DES gives us a window on how they will act in
the future.  Given the spook tendency to slap a secrecy order over
everything up to and including trips to the bathroom, we can only
reasonably construct plausible threat models by reconstructing events
that have entered the public domain with sufficient facts.  Same for

@_date: 2014-09-23 10:09:24
@_author: ianG 
@_subject: [Cryptography] science of security, NSA paper awards 
Nice!  I find it slightly ironic that the second paper was of far more
practical use to the world ;-)

@_date: 2014-09-23 10:52:31
@_author: ianG 
@_subject: [Cryptography] new wiretap resistance in iOS 8? 
There are gremlins in every corner.  If you spend your life insisting on
"never underestimating" every fear, uncertainty or doubt that others can
inject into you, you'll set yourself up as the victim not the defender.
 You'll be too busy to contribute much to humanity.
True, but a different issue.  You also should spend only what you can
that saves you money according to your costs.
I think you'll find that an attacker doesn't spend more than he gains,
it is just a perception difficulty in us knowing how he gains, and
likewise, a perception difficulty in him knowing how he gains.  Added to
this confusion, the unproportionality of our costs to his gains/costs,
as you point out, leads us to assume he is irrational.
So it will statistically even out over a lot of attacks, otherwise he'll
run out of money.
If maintaining capacity is a gain, need to account for that...
I'm not sure of that expression, but pork is valuable :)
This is a sunk costs argument.  So the cost-benefit analysis needs to be
extended over the sunk costs and the marginal costs.  Can't cherry pick
the last effort's marginal costs and call that an analysis.
If they are not presently deployed, how do you know they are dangerous?
 Economic?  You have zip facts about them because there are no facts,
only conjectures, as discussed in the other thread concerning DES
crackers.  Deep Throat was a fact.
Undeployed threats are indistinguishable from FUD.  If that's ones
business -- selling FUD -- then this is good and aligned;  I prefer a
more straightforward approach myself :)
Of course.  Problem is, the possibility of all new and future attacks is
too hard a problem for society to deal with.  Hence we lean on events
and facts to filter our limited resources.  E.g., if we get hit by a
buffer overflow, we tend to fix it, and at the same time fix all buffer
overflows.  (We wish.)
Well, true, there is a steady stream of research and complaints about
the PKI/SSL business in the late 1990s up to 2003 when phishing started
We knew that there were potential attacks.  What we didn't know is which
of these potential attacks were economic.  E.g., when phishing first got
tested against e-gold in 2001, it failed.  Some would then say it was
uneconomic.  Fair bet without hindsight?
In contrast, "always look for plaintext" is simply daft.  We now know
enough from the Internet's use of credit cards over plaintext to say
that there is no reasonable or measurable threat.  Credit card sniffing
over the unencrypted net is not a sufficient threat to make us deploy
encryption such as authenticated SSL.  In contrast we know that attacks
against banks is sufficient for SSL, and hacks into servers that have
databases full of validated and sorted credit card information is severe
and sufficient to deploy hardened websites.
Yeah.  And you're mixing threat actors.  The NSA has one set of threat
actors to deal with.  We have another set.  The intersection of the sets
is typically small in the Internet security trade.  Not zero, but not
the driver by any means.
Following NIST / NSA in security thought is to not do security thought.
Well, post snowden we know that this is not quite the case.  The state
level attacker is interested in everything he can get his hands on as
long as his resources can hold out.  But, he's not interested in *my
laptop* especially, more he's interested in *every laptop*.  Which means
he's not that concerned if he misses out, and he's not actually using
any of the data he finds, if its on my laptop.
(Although, the breach of the intelligence firewall between IC and the
police/civilian agencies is by far the most troubling thing of the
post-snowden era.  It is that breach of democratic faith that makes the
NSA the enemy of humanity.)

@_date: 2014-09-28 19:44:08
@_author: ianG 
@_subject: [Cryptography] The world's most secure TRNG 
The former.  The generally advisable thing to do is to collect o(100)
and use that to seed a PRNG.
You might also want to look at  which has about the
same goals.

@_date: 2014-09-29 21:56:32
@_author: ianG 
@_subject: [Cryptography] The world's most secure TRNG 
This is where it gets messy because there are two answers in opposition.
If we (the buyer/user) are serious enough about using a hardware part
then that means we don't trust other parts.  Which also means we don't
trust your part.  So we have to construct a mixer/PRNG that takes inputs
from a number of collectors.  Your collector being one of them, thanks
muchly, and it should be fully uncorrelated with the others.
Then, because we mix and then plug the result into a PRNG, which
typically is guaranteed to have a whitened output, there is no need to
whiten your collector output.
However, because most devs won't understand the above argument, if you
actually supply an unwhitened RNG then geeks will look at it and decide
that because they see certain biases in it then it must be broken!  And
broken they will call it.  And broken will be your sales.
So from a marketing point of view you should put a whitener on the part.
That said, I'd not go for anything sophisticated.  Keccak is way too
much especially if it breaks your part budget.  What can you do with
your $2 part?

@_date: 2014-09-30 18:49:10
@_author: ianG 
@_subject: [Cryptography] Cryptography for consensual sex in California ? 
I've been working on something similar.  The problem faced here is that
the fact you are resting on "yes means yes" is not a fact that can be
measured by tech for a variety of reasons.  Assuming that, the task is
to capture what facts you can capture and save it for later review by a
In my threat model, we are faced with intimate aggression delivered over
an IM/chat channel.  So we've decided to add a mode that BCC's the
messages encrypted to an arbitrator.  If a person is unsure about the
situation, then she can hit the BCC button and carry on.  If/when a
dispute arises on any question, the transcript can be pulled out,
decrypted and become part of the evidence for fact finding.
That's the sort of design you expect from foundation or grant money...

@_date: 2015-04-01 12:22:05
@_author: ianG 
@_subject: [Cryptography] Cipher death notes 
I can see how to do it for a root key.  Just issue a new subroot and it's private key :) or use the root key to sign its own revocation, might be more polite.
How would you do a 'proof' for a symmetric cipher?  Simplistically, the cipher sponsor could encrypt "this is a secret" and anyone who can crack it could then reveal the cipher.
But many cracks happen under exotic conditions, such as 1 million ciphertexts probed on a key made with a broken RNG while standing on your head...
Creating a 'proof' with a symmetric cipher seems to imply turning it into an asymmetric signature cipher:  something hard to crack but easy to proof?  Or at least a one way function.
Hmmm... ok.  So a death note is a pretty powerful virus.
We can imagine the WGs worrying about the security effects of that.  Can someone craft a virus that turns off *all* ciphers?  If the IoT thing is 20 years old and switches the cooling water on a NY nuclear powerstation, is it clearly more secure by eliminating its 20 year old cipher?  Does the fallback to cleartext make the effect of the last cipher dropping off worse?  Is letting someone hack the cipher worse or better than disabling access?
I agree it's an academic exercise.  But so was asymmetric cryptography when it was first proposed, until D&H and then RSA found their solution.
I see the "death note" as a perfectly good answer to the original question:  if you have decided to adopt multiple ciphers, you've also adopted the responsibility of retiring those.  How?  When?  What delivery mechanism?
If you have multiple ciphers, you *have to have an answer to retirement* else you've only done half the job - the fun and easy part, but also the less responsible part.
It's an *important* exercise.
I don't think it does that.  Detecting the adversary is more the domain of certificate transparency, a trip wire that picks up a breach after the fact and makes the job of doing an MITM that much more uncertain.
I would see the "death note" more of a by-fiat administrative requirement, and more of a signed assertion than a proof of compromise.   Holistically, I'd expect the agency that accepted the cipher into the lists in the first place to be the one to issue the death note.  So we could imagine the WG for TLS deciding to vote on dropping DES, and once done, the Area Director would use the specially secured "death notice key" to sign the statement, and then just post it back to the WG, for

@_date: 2015-04-03 23:40:22
@_author: ianG 
@_subject: [Cryptography] how to put a password in an evidence bag? 
Many here will likely decline to follow the Bridges / Force arrests in USA, but in short, the two federal agents who were chasing the "Silk Road" website for bad stuff were also raiding the pot.
At one point, they arrested an employee of the website and got the admin password from him.  They then proceeded to raid the value that was escrowed within (customer funds) and sell it through an exchange provider - Mt.Gox.  Once they had extracted all the cash out via this exchange provider they proceeded to shut it down for running an unlicensed money transmitter ... you can't make this stuff up.
But the crux of this particular theft was getting the password to the accounts on silk road.  Now, passwords are "obviously" evidence that has been seized.  But how do you protect the chain of custody?  You can't exactly put it in an evidence bag ... or you can but that is not protecting it if one of the agents has copied it.
Does anyone know how the cops secure the password?  Or have they not yet begun to deal at this level.

@_date: 2015-04-04 19:10:01
@_author: ianG 
@_subject: [Cryptography] Fwd:  OPENSSL FREAK 
I think that's worth asking.  As an open internet, we now have 20-23 years of deployment of crypto.  We should now be in command of a substantial body of evidence concerning what works and what doesn't work, over a period of time.
Right.  So do we need a DeathNotice for the Implementation?
And does this DeathNotice also serve to cover the algorithm?
Which by nature of its claim over the implementation, it cannot be a proof-of-breach because if we could prove the breach we'd fix the breach.  Therefore it is likely administrative --> revocation by I suspect we can probably do the latter if we employ root-key like ceremonies to create the challenges and then destroy the keys.
(So, an encrypted message with a text phrase and the key to be used, such that when cracked, you can use the key revealed to confirm the encryption?  "The magic key is SHA1(OstrichContrariness)" .. ok)
No, the discussion isn't pointless, but maybe frustrating or obvious in If the Death Note is the plan for retirement, and we can show that the Death Note doesn't work, then we've shown something:  there is no plan for retirement.
Those who think algorithm agility is a good thing need to show that there is a plan to utilise the benefit of that agility -- a SWITCH -- and a plan to clean up later on when the inevitable expiry decision or event is reached -- the DEATH NOTE or revocation or similar.

@_date: 2015-04-10 19:20:20
@_author: ianG 
@_subject: [Cryptography] Fwd: OPENSSL FREAK 
I'm going to assume that every device has a remote management interface.
Then, channeling all the posts, and not crediting names:
1. the RMI must be capable of defending itself because likely it itself might not be able to be upgraded.  Therefore it has a small and strong 2. The RMI can be used to upgrade the device.
3. A death notice could just be limited to killing the networking?
4. The RMI could be used to kill the device.
5. If the owner just wants the device to keep running because it works for her, even when we know it to be secure, then she can disconnect it from the net / run a private net.  Which she'll probably want to do anyway.
5. If she doesn't firewall it into a private net, then likely over time, the device might get hit by a death notice.
6. Which would be a bit shocking.  So maybe there needs to be a warning?   A grace period?  Some way in which the device alerts the user that she needs to disconnect the fridge.
7. Complexity of this arrangement seems to indicate that all such devices will then be coupled with private networks and private management consoles which manage the devices.
8. Then, the message might be that if you place your target on the open net, it's fair game, in whatever circumstance.  It's up to the installer and/or owner to put in the appropriate protection.  If not, they are being negligent.  With appropriate doco and PR, we could get this established sufficient to survive in court.
9. *We are looking at the wrong part*.  Instead of looking at the device, we should be thinking about the protocol for upgrade, management, tunnelling, virtual death imitation, etc, and the app&box that does that work.
10. Someone will make a lot of money here because that box is likely a lot higher margin than the average target ;-)

@_date: 2015-04-10 19:50:16
@_author: ianG 
@_subject: [Cryptography] upgrade mechanisms and policies 
(Nice neologism!  I wonder if a better spelling of that is outreduction ?)
That is more or less what I argue.
Right.  But we can't get to that stage until the fans of the mechanism realise that without a policy, the outcomes are unbalanced.
:)  And I think the discussion will be solved in this place, because it is a discussion group, with no "agenda" in mind.
Yes please.  And I think that this message will eventually make its way into better practice in writing protocol documents.  That is, if agility is part of the protocol, then a policy section or reference is a MUST.
good stuff snipped.  That's what flag days are...
That is an important corollary of the upgrade mechanism & policy principle.  Once everyone has seen how we need a holistic approach -- if we are to include these upgrade possibilities -- it becomes much clearer that probably we want to have a more planned & conscious replacement of the old cruft before it becomes dangerous.
The "odds & evens" version replacement approach is what I think we'll drift to in the future, for those protocols have decided to dispense with the internal upgrade possibility.

@_date: 2015-04-11 23:19:42
@_author: Ian G 
@_subject: [Cryptography] upgrade mechanisms and policies 
Well, not totally crazy, just maybe tricky.  Case in point, later generations of Skype since about 2009 have decreased security & privacy by sharing with Redmond and Maryland.  But the counter to that is that the sane mass-user policy is still to accept the version upgrades, until the point of abandoning the product.

@_date: 2015-04-12 08:39:08
@_author: Ian G 
@_subject: [Cryptography] upgrade mechanisms and policies 
And of course once we accept the policy that latest is best, the attacker is now incentivised to attack the version provider.  Hence, NIST's recent troubles, and frequent grumbles about NSA people in IETF WGs voting for more complicated versions of protocols.

@_date: 2015-04-12 16:44:03
@_author: Ian G 
@_subject: [Cryptography] upgrade mechanisms and policies 
We assume that the package preparers know more than the users. Fairly safe assumption in the aggregate.  Obviously it breaks down with some people and some times.  But 90% of those discussions are esoteric.  9% reasonable people can disagree, but the package choice is still fine for the most.  1% might well be right, the choice is bad, or less good.
Indeed.  But the users are typically orders of magnitude less capable of figuring it out than the devs.  Certainly in security, and definitely in suite choices.
Oof... that is what this whole thread is about - getting rid of all that choice.  More choice is always more bad for general users.
The question we are trying to answer is whether there is *any general case where any user choice in security* is better than no choice at all.

@_date: 2015-04-13 16:59:07
@_author: ianG 
@_subject: [Cryptography] upgrade mechanisms and policies 
Negotiating the protocol version as N or N+1 means that in N+1 we can fix all the *protocol* bugs found in N.  Algorithmic agility doesn't cover that territory, although once, with the switch to RC4, it was sort of kludged in by going backwards to a deprecated algorithm.
Count up how many protocol bugs we have seen in TLS.  Versus how many algorithm failures we've experienced.
The ratio is about 10:1 - the real problem is in protocols, not in algorithms.  When WGs look at the algorithms, they are looking at the wrong area; worrying about algorithms and trying to preserve agility in algorithms means they're distracted by the sex appeal of beautiful cryptography rather than the ugliness of protocols.
I don't understand how that follows, but my suspicion is that it is based on false assumptions about algorithms being more important than Yep, that's a good analogy.  We could call it the grandfather's axe approach :)

@_date: 2015-04-15 00:24:37
@_author: ianG 
@_subject: [Cryptography] the TOFU lie - or why I want my meat... 
All agreed except "typically".  I've done email exchanges with hundreds of people over PGP in various guises, and I probably checked FPs in about 5% of cases.  A lot of people I know also don't bother to check.
I don't know what the ratio is, but it certainly not clear that "OpenPGP people typically do direct auth" rather I'd suggest it is the other way Well, except for the "trust in CAs" part, sure.
Also, the browser typically places most if not quite all of the checking under the covers.  It is the browser that makes any choices, thus resulting in weird results when the browser says "you do not trust this website" when in fact you do.
Yes, we sometimes say "leap of faith" to deal with these unclear parts of the user's trust.
To the extent that people understood or didn't understand who was to blame for what crime there was, then yes, people didn't perceive that the right parties should make an effort.
It's not that cheap, but money certainly helps.
Simply ... is probably an overstatement, but ok.
Which is probably a good assumption.  Your attacker doesn't typically predict what you're going to do next, especially with a new site, so he has to pretty much attack everything.  Which leaves him open to detection if you have even a few certs cached from somewhere.
Well, people don't "sell" it.  Rather, it's what you get for free. Pretty good privacy, perhaps we should suggest it?
For a tiny little bit more work, users can do things like ADH and compare the results out of band.  This is done by some phone products, in-band, assuming that it is pretty hard to forge the other person's voice in real time.  Again, pretty good privacy, could be on to something here ;)
Listen --> SNEAK is what we want.  Now they can listen to everything for free, literally because we didn't paper the globe in crypto, and again that because we got tricked into doing the whole CA thing.
However the key here is that when we use TOFU, it's still an option.  We can always do the fingerprint thing.  And we will, when the NSA sneaks in and does enough MITMs.
Bring it on!
Ahhhhhhh... so you're aware of persistent attacks on the banking system.   So now we have to ask, why are there persistent attacks on the banking accounts of users, but no persistent attacks on email?
In the past it was because there was no efficient way to monetarise it as a crim.  Now however, it turns out that any ISP worth its salt is MITMing the customers to the point where we can spot the difference on pure performance.  Which is to say they figured out how to sell your data.
So the answer is:  money, or absence of it.
Not so much too expensive but too dangerous.  Even if they do 0.1% we're going to spot it, and then the flag goes up.  It's already the case that people now getting annoyed at cafe wirelesses that MITM the SSL connections, and are starting to find local-password WIFIs in preference as they have no MITMing rather than big service providers' WIFI.
No, so ok, there is something that might not be clear to the world. When dealing with the spooks, there is one and only one rule over everything:  don't get caught.  This is their whole constitution.
The same will apply to cyberhacking.  Recall they actually promised POTUS that the Iranian virus stuff would never get out.
They will only MITM when there is a very high near-certainty of not being caught.
Bring it on!
No, you're assuming that because the attack succeeded, then the system has failed.  It has only failed in your case, but in the aggregate it has succeeded (assumed) as it has defended more people more times than the alternate.
The way to think of this is risk analysis.  You take a risk in all things.  What matters is not whether you get hit by one particular risk but whether the expected value over all of (damage, risks) is kept to a reasonably low level.  It's about aggregation of all your risks.
Now, in the above, you got attacked, successfully.  But in using say TOFU PGP you would also benefit from not being mass surveilled.  Only a successful MITM would hit you then.
And, if you believe that you're at risk of that -- again risk analysis

@_date: 2015-04-15 13:46:46
@_author: Ian G 
@_subject: [Cryptography] fighting designs in habituation since 1883 
That which I once sarcastically referred to as click-thru syndrome is now apparently called habituation.  And it's being measured using MRIs:
MRIs show our brains shutting down when we see security prompts
This is your brain after repeated security warnings. Any questions?
by Dan Goodin  - Mar 20, 2015 2:53 pm UTC
Ever feel your eyes glazing over when you see yet another security warning pop up on your monitor? In a first, scientists have used magnetic resonance imaging to measure a human brain's dramatic drop in attention that results when a computer user is subjected to just two security warnings in a short time.
In a paper scheduled to be presented next month at the Association for Computing Machinery's CHI 2015 conference , researchers will present data that maps regions of the brain responsible for visual processing. The MRI images show a "precipitous drop" in visual processing after even one repeated exposure to a standard security warning and a "large overall drop" after 13 of them. Previously, such warning fatigue has been observed only indirectly, such as one study finding that only 14 percent of participants recognized content changes to confirmation dialog boxes or another that recorded users clicking through one-half of all SSL warnings in less than two Building a better mousetrap
The inattention is the result of a phenomenon known as habituation , or the tendency for organisms' neural systems to show partial or complete cessations of responses to stimuli over repeated exposures. Such repetition suppression, or RS, has long been documented in everything from sea slugs to humans. By directly measuring RS in the brains of people exposed to computer security warnings, the scientists were then able to test more effective ways that software makers can alert people to potential risks. The paper?titled "How Polymorphic Warnings Reduce Habituation in the Brain?Insights from an fMRI Study "?is one of two to be presented at CHI 2015 that studies people's responses to security warnings. A second paper is titled "Improving SSL Warnings: Comprehension and Adherence  From Cryptogram:  New research: "How Polymorphic Warnings Reduce Habituation in the Brain -- Insights from an fMRI Study."
 or

@_date: 2015-04-16 22:59:05
@_author: ianG 
@_subject: [Cryptography] upgrade mechanisms and policies 
I agree with this.  I'm surprised it isn't celebrated more.  In all our time on the net, the crypto has been unfathomably rock solid as far as algorithms go.  The protocols have also been pretty good compared to the rest of it.
I think encryption has become so good and so fast, and will become faster still, that encrypting everything with a good-enough cipher is probably ... good enough.  For everything in general.
You're betraying your CIA bias ;)  I'm guessing you mean here that for most business models, auth is more interesting.
For most traffic on the net, I'd say auth is highly dependent.  For some things we want auth.  But for other things we want the opposite of auth, call it anti-auth or unauth.  This is the notion of sexchat, snapchat, OTR, etc in principle, not in implementation.
These emails should be un-auth, moderated and encrypted.
The fact that they are not encrypted now means we have a podium at which we can tell the NSA that we don't agree, and we didn't start the fire ... but I'd still rather it was encrypted and unattributed and we could talk with complete confidence.

@_date: 2015-04-17 19:15:17
@_author: ianG 
@_subject: [Cryptography] upgrade mechanisms and policies 
See, this is such an overstatement that all your models fall to dust. It is literally not true, and it is so fallacious as to be dangerous.
Confidentiality can be modelled between two people Alice and Bob.  If Mallory interjects and passes the traffic along, we know that Alice, Bob and Mallory know the secrets.
That's 3 and we all know that 3 people can keep secrets if 2 are dead...
But the other viewpoint is that it is 3 people amongst 6 billion.  Now, if the normal threats that face Alice are amongst the other 6 billion then she still has the benefit of confidentiality [0].  Even if the threats are evenly spread across Mallory, Bob and 98 others, she's still batting at 98%.
Ergo, confidentiality still exists and is valuable even if there is an MITM.  What is lacking in meaningful value is your security model, because you stripped out the value of confidentiality for ... other reasons as we'll show.
To put it more in context, consider the model in greater depth.  There is no security system that goes Alice <--> wire <--> Bob, it just doesn't exist.  The hallowed Internet model is simplistically unrealistic because Alice can't do RSA in her head.
In practice, there is at least Alice and Alice's computer - which latter has such a combination of agents inside it that we can't even catalogue them let alone secure them.  So even if you believe that you can't permit Mallory, you can't actually reach that target in any meaninful sense.  Your model is not real world.
So why is there an old adage of confidentiality as 'meaningless' without protection against unauthorised listeners?  It is really reduced to a marketing statement:  you must put authentication ahead of other considerations, and by the way, we happen to be selling a mighty fine authentication system...
Now back to the real world of real protection for real users...
*hold onto that thought*
As covered above, this conclusion is only theoretically plausible if you write the assumptions to be so far away from reality as to be meaningless.
Everything you've said is the classical argument for PKI/CA/TLS/ITM.  It is a reconstructed argument starting from the position that we have a hammer (x.509) and we need to go out and find some nails.
Now, if we went back to actual privacy considerations -- not your constructed but well learnt theory -- and asked what Alice and Bob wanted to do privately:
   1. do you want your messages to be secret?
   2. do you want your contacts to be secret?
   3. do you want your activity to be untracked?
The answer to the above is typically YES, YES, YES [1].  My business is my own.
But of course everything-secure is a hard problem, really challenging. Let's assume we can't answer that right now.
What can we do?  We know how to authenticate people using this telco design.  We know how to use that to bootstrap a secure point to point connection.  So why don't we do that?  Hey presto, the auth-pyramided system we now know so well and love/hate.
Unfortunately these systems however are diabolically bad at certain things.  X.509 directly makes tracking and tracing not only easy but *authenticated*, so points 2,3 above are blown out of the water.  What's the solution?  Education.  We have to go out and tell people "your threat is mallory" and we have to also tell them that "your threat is not tracking."
Go back to those thoughts: "let's say that Alice and Bob are fine with Eve and Mallory knowing _that_ they are communicating with each other."   That's not true.  That's you telling Alice and Bob what they are allowed to do in order to benefit from your system.
Indeed, it is so not true, it's illegal under (eg) European data protection directive.  It is literally not allowed to use and release numbers that relate to people that permit cross-correlation without showing some seriously hard reasons.  In other words, certificates are actually against the data protection directive (the reason nobody much cares about this is because certs only work for companies, who aren't "protected" under the data protection directive).
Indeed, it's so not true that most police forces and most intel agencies and most other such people say directly that if they can pen-trace everything, then the rest is ... not worth much more than they already have.  And most cheating spouses and business partners and whathaveyou have exactly the same problem.
It is NOT OK in their threat model to accept tracking.
So what's going on here?  Only by telling users to ignore certain threats, and telling them and telling them over and over again, can you educate people to accept and love your security model.  But you haven't delivered security -- what you've delivered is sales.
Security only delivers if it starts out from real user needs -- not from reversed seller needs.  Reverse the education you've learnt.  Assume that Alice and Bob want secrecy and don't care about confidentiality. Build it.  Or something.  Until you've unravelled and expunged the CIA/ITM security model, you can't actually have (another) security iang, unravelling false ITM/CIAs since 2003 [2] ;)
[0] This is the insight that historically allowed the entire open source crypto community to wave off ("accept") the threat of the NSA.  Until they started shipping data to the FBI, IRS, egg board and taxi medallion printer, the NSA was benign.  It therefore didn't matter to Alice that NSA could read her traffic.  It didn't matter to practically anyone here - or our customers.  We accepted that risk of a clear MITM danger, back then.  Of course, now, we're screwed.  Now we have to go back and re-do [1]  Typically, of course we argue that point, but this'll do for now.
[2]

@_date: 2015-04-26 15:25:03
@_author: ianG 
@_subject: [Cryptography] Holy Heartbleed Batman - an Internet-scale attack on 
26 April 2015
GoodCrypto Attacked
Published here to resist censorship.
Surveillance system used for censorship in Europe
Censorship attack combines packet injection and Heartbleed
We all know there is censorship online. It happens in China. It happens to "terrorists". But we don't believe it will happen to us.
As Eben Moglen[1] and Kaspersky[2] have pointed out, companies developing crypto are prime targets no matter where they are. So you don't have to be a bad guy for the NSA to attack you. You just have to protect people from the NSA. Even protecting yourself is often enough. NSA prefers their victims to be defenseless.
Detection in the wild
In early 2015 people were still downloading our ISO file for GoodCrypto. But suddenly installations stopped.
After a lot of checking we noticed that the downloads got HTTP 200 result codes, but the lengths were all too short. This isn't supposed to happen. A 200 result means success. These weren't successful downloads, but the web logs said they were. Ordinary log checks didn't show the bug.
Finding the vuln
Downloads from goodcrypto.com to goodcrypto.com worked. Downloads from another site at a different datacenter in the same country worked. A little further away in the network, downloads failed but the server logged a "Success" status code.
The obvious answer was a server misconfiguration. We couldn't find one. A server side packet dump showed the client just dropped the connection in the middle of the download.
We couldn't get a browser to download the whole ISO file. The browser thought it came in fine, but the file was incomplete.
So was it a browser bug? We tried other browsers. They couldn't download The wget program often helps debug downloads. It doesn't have the same malware issues as browsers, because wget doesn't support malware vectors such as javascript, java, and css. It also retries failed downloads, and often tells you why it failed.
When we tried wget, it detected errors, retried, and finally succeeded. It said the error was a bad length field in a TLS packet. That didn't make sense at first because we thought TLS packets were error corrected by TCP.
We searched for other bug reports like this. They were all during session initiation, not in the middle of a long download.
But our searching led to Heartbleed. Modifying SSL/TLS length fields is exactly how it works.
Wasn't Heartbleed fixed in 2014?
In 2014 when we all heard about Heartbleed many servers were vulnerable. But OS providers fixed it fast. Our own servers get regular security How were we seeing it now?
Servers around the world were fixed fast, but clients were vulnerable too. The Heartbleed news coverage was all about servers. Servers got fixed. Many clients didn't. A client side Heartbleed attack is sometimes called Reverse Heartbleed.
Packet evidence shows MITM
Was our server cracked? We're pretty careful, so that doesn't happen often. But we checked, and checked again. Even though we don't usually have any packet logs, we ran download tests with simultaneous packet logs on both the server and client.
The server packet logs showed an ordinary number of bad packets, all error corrected. During the download the client dropped the connection.
The client packet logs were very different. There was a surprising number of bad incoming packets. Almost none of these bad packets showed in the server logs for the same session. These packets appeared to be injected into the packet stream. This is an MITM/MOTS attack, specifically a packet injection attack.
Finally wget reported that a TLS packet with a bad length field got through and caused the TLS connection to break. Now we knew this was a MITM variant of Reverse Heartbleed.
Working around the censorship
We added simple instructions to our Download[3] page:
Somebody (Hi NSA!) is trying to censor GoodCrypto downloads.
But don't worry. The workaround for their super duper advanced network attack is: Just use wget.
If you're on windows you need wget.exe[4].
At a command prompt:
wget --no-check-certificate (URL appeared here)
Because you will verify the file hash, "--no-check-certificate" is ok in this rare case, and Windows needs it.
We also strengthened the encouragement to verify the file, and asked visitors to let us know[5] when the attackers change tactics.
False Flag
The attacker injected packets by forging our site's IP address. If a site visitor notices the packets, they will think that the attack is from us. The attacker didn't just shift blame away from themselves. They framed someone they don't like. Faking the evidence to blame an attack on someone else, especially someone the attacker doesn't like, is a classic False Flag operation.
An MITM attack like this requires impersonation, so the benefits to the attacker of a False Flag are built right in.
One way to censor anyone is to attack their reputation.
Censorship by a nation state
Even though the download broke, the browser didn't complain. It looked like a successful download. The server showed an HTTP 200 result code. Neither the client nor server detected the attack.
Server packet logs didn't show anything unusual except the abrupt client disconnection. The attack didn't show at all during downloads from a client that was close to the server in network space, such as another nearby datacenter. It wasn't an attack on the server or the hosting We set up another server in a different country and the attack continued. It wasn't an attack by the hosting country. We were nowhere near our transfer limits for either server, so it was not traffic shaping by hosting providers.
The clients were all over the world. It wasn't separate attacks launched from close to individual clients.
The attack appears to be from someone sitting on the net pipes and injecting packets. This requires huge resources. The U.S., U.K., Canadian, and Chinese spooks do this. Ordinary criminals don't have the Because the goodcrypto.com servers are in Europe, China is an unlikely suspect. GCHQ and CSE are dependent on the U.S. for their QUANTUM capability. That means it was likely an NSA attack, either directly or by proxy.
But which nation really doesn't matter. There is no known way to protect against specific nations. You have to protect against all or none.
Who is vulnerable
Anyone who publishes on the web is vulnerable to this form of censorship. Even if you just use HTTP, the Chinese censorship method of a simple RST works.
Update: The attack is now intermittent. Exposing them often helps. When it's paused they may be avoiding forensics, or just changing tactics. Let us know.[5]
[1] [2] [3] [4] [5]

@_date: 2015-04-28 14:42:32
@_author: ianG 
@_subject: [Cryptography] upgrade mechanisms and policies 
Indeed.  I was speaking to what I think we want, not what I think we can do easily.
Entirely.  But it is a cryptography list.  If there is anywhere that should be able to do an encryption list it is here.
Actually the encrypted list is something that is often demanded of groupware software, and it is a well-known hard problem.  Normally this is helped a bit by closing off the access to ones group, but this doesn't help the fact that too many holders of the secrets is too brittle.  Forwarding, backups, local hacks and all that.
Indeed.  But take the well-known case of SSL.  The industry bends over backwards to state that authentication of the server is the key benefit.   Yet, they fail to authenticate the server in phishing - a simple bypass attack.  And for most webservers, they couldn't care less about their own authentication, what they care about is the client authentication.  SSL completely muffs client authentication, leaving the users to come up with ad hoc password stuff.
(Yeah, now poeple will chime in and repeat the marketing about how it's hard because of multiple devices and and and ... the point is, every which way you look at the SSL story, it isn't about what the users need, it's about what was easy to convince to be sold.)
Well no.  I said un-auth.  Specifically, my threat here is in an unknown time and place in the future, during hot crypto wars II, my comments on this list are taken as evidence that I'm an international arms dealer. I don't want to be auth'd on this list.
Even simple point-to-point communication is hard, because of that additional rider.  And if we factor out email, which is practically insecurable (handwavy opinion) there isn't another robust Internet p2p communications mechanism that is open and distributed.

@_date: 2015-08-02 05:27:12
@_author: ianG 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the rough 
There's a group working on a new crypto protocol.  I don't need to name them because it's a general issue, but we're talking about one of those "rough consensus and working code" rooms where dedicated engineers do what they most want to do - create new Internet systems.
This new crypto protocol will take a hitherto totally open treasure trove of data and hide it.  Not particularly well but well enough to make the attacker work at it.  The attacker will have to actually do something, instead of just hoovering.
Doing something will be dangerous - because those packets could be spotted - so it will be reserved for those moments and targets where it's worthwhile.  It's not as if the attacker cares that much about being spotted, but embarrassment is best avoided.
So this could be kind of a big deal - we go from 100% open on this huge data set, down to 99% closed, over some time and some deployment curve.
Now, let's assume the attacker is pissed at this.  And takes it's attitudinal inspiration from Hollywood, or other enlightened sources like NYT on how to retaliate in cyberwar (OPM, anyone?) [0].  Which is to say, it decides to fight back.  Game on.
How to fight back seems easy to say:  Stop the group from launching its protocol.  How?
It turns out that there is a really nice attack.  If the group has a protocol in mind, then all the attacker has to do is:
   a) suggest a new alternate protocol.
   b) balance the group so that there is disagreement, roughly evenly balanced between the original and the challenger.
Suggesting an alternate is really easy - as we know there are dozens of prototypes out there, just gotta pick one that's sufficiently different.   In this case I can think of 3 others without trying, and 6 people on this group could design 1 in a month.
Balancing the group is just a matter of phone calls and resources.  Call in favours.  So many people out there who would love to pop in and utter an opinion.  So many friends of friends, willing to strut their stuff.
Because of the rules of rough consensus, if a rough balance is preserved, then it stops all forward movement.  This is a beautiful attack.  If the original side gets disgusted and walks, the attacker can simply come up with a new challenger.  If the original team quietens down, the challenger can quieten down too - it doesn't want to win, it wants to preserve the conflict.
The attack can't even be called, because all contributors are doing is uttering an opinion as they would if asked.  The attack simply uses the time-tested rules which the project is convinced are the only way to do these things.
The only defence I can see is to drop rough consensus.  By offering rough consensus, it's almost a gilt-edged invitation to the attacker. The attacker isn't so stupid as to not use it.
Can anyone suggest a way to get around this?  I think this really puts a marker on the map - you simply can't do a security/crypto protocol under rough consensus in open committee, when there is an attacker out there willing to put in the resources to stop it.
[0] you just can't make this stuff up...

@_date: 2015-08-02 19:16:46
@_author: ianG 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
So, to just add something to the above point about committees being difficult without any help, it is of course possible for a committee to act the same way even in the absence of an attacker.  This is what makes the attack so neat - as long as the attacker just acts as disorganised and catty as a normal engineer, there is no observable difference.  The attack is invisible, and the hand that guides is also invisible, but not the invisible hand of economic progress.
Learning that these two things exist - that we alone can stall the process by being bad at committee, and that others can use this badness against us - is a really tough lesson.  However, I have discovered a rather elegant way that at least gets leads the horse (ass?) to water.
Way back in WWII, the USA's OSS was engaged in the process of sabotaging the German production machine.  To assist its agents it created a manual [0] which was distributed out to the field.  This manual has since been declassified as it was presumably only of historical interest.
As it was a comprehensive look at how to interfere with the enemy, it also exhorted the common factory worker to do his or her part.  And it created a set of tactics to slow everything down.  This is chapter 11 of the manual, which has such gems as "engage in long correspondence" :)
It turns out that Chapters 11 and 12 [1] are a rather poignant reflection of what can go wrong in committee.  So when I found myself as part of such a committee back in late 2000s, I copied the manual in and I euphemistically named it "the manual for our committee" [2].
Then, every time there was a new committee elected, I would pop up and say "and don't forget to read the manual on how you do board meetings" or some such.  New members would then diligently read it, and quietly chuckle and figure out I was having a joke or something.
But the seed is planted.  Not only can we stuff up with histrionics ("Cry and sob hysterically at every occasion") and bad behaviour, this can be used against us by an enemy.
The board of CAcert, a community certification authority that changes its board around every year.

@_date: 2015-08-02 22:17:54
@_author: ianG 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
So, just to forestall any thoughts in a particular direction.
1.  It is fruitless to name a person who might be a shill.  The reason is quite logical - the attacker is better at this game than you are, and will use your attempt to name a shill as a way to create discord, and will (eg) also use the same noise to name YOU as a shill.  Or worse.  In case you're wondering, this is known art, I'm not just talking out my tl;dr don't name a shill, you'll lose.  Attacker is better at it.
2.  Naming a WG is also amusing but distracting.  This is the security area.  The attacker exists.  He spends millions of dollars on this, he has been caught with his finger in the cookie jar before (nod to Watson on these points), and he's said in revealed docs he's going to do it. We all know that.
So, it's a systemic problem.  It might be happening today in a group, but actually it's more likely a honed process across 10 or more groups.   What's the systemic response?
3.  This only applies to security when there is a known attacker who's decided to stop this particular protocol from interfering with his actions.  That's a fairly narrow slice of WGs.  Probably less than 10 I.e., I'm not arguing to dispose of the entirety of the IETF.  Not today at least :)
So, assumptions:
1.  The attacker exists.
2.  The attacker has approximately infinite resources and is prepared to spend them.
3.  The attacker can call on a large network of people, including ones who might not agree with the call, and ones who don't spot the motives.
4.  The attacker cares not to be spotted, but not that much.  You're not going to sue him.
5.  The attacker has decided that deployment of protocol X on wide-spread basis is to be stopped.  (Somehow.)
Then the attack.
As described, attacker eases the WG into rough anti-consensus, a balance between two opposing forces by
   (i) proposing an alternate protocol, and
   (ii) stacking the group so there is roughly enough opposition.
The defence *I proposed* was to drop rough consensus.  I stopped there.
Stephen pointed out that any replacement of rough consensus with a directional method ("one czar" or AD or ...) would then shift the burden of the attack to another place.  I.e., could very will just work in the attacker's favour.  A very good point.
Jerry described the coin toss.  This "addresses" Stephen's dual-attack at some level.  What it does is actually give a 50% chance of the good protocol, and a 50% chance of the challenger.  So now we can refine our attack by saying, the challenger should be also a non-optimal protocol.   We've now got a 50% chance of killing it by putting in a non-working protocol, a familiar scenario to everyone who's been engaged in these efforts, sadly.
Now I'll propose another way, just thought of it:
Split the protocols.  Group A proceeds, so does group B.  Then both are standardised.  Now, the market works both over.  There is now a betamax story to get through as the market gets to have a second call on the rough consensus.
If you believe in rough consensus that much, let the market vote ;-)
Engineers of course will be horrified.  "We can do better!"  But actually, maybe we can't.  Betamax resolved more quickly in the marketplace than many standards groups took to come to rough consensus and produce their standards.
Maybe the question here is, where is the pain?  And perhaps a bit of user pain is the price we pay?
(None of these points are entirely new!)
This is all by way of a thought experiment.  I set some parameters. Everyone's free to knock it down, and/or change the parameters to be more interesting (someone has already proposed an entirely new set of parameters in private email).
Where it gets "interesting" is when we inform a particular situation in reality.  That's of course a crapshoot.
But we don't know how close the thought experiment gets to reality unless we try.
(Right.  In this case, I reckon the interests are directly opposed. Attacker's mission is pretty clear.)

@_date: 2015-08-02 22:20:09
@_author: ianG 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
ftr, I read this post earlier, and it inspired me in my mind to copy it and think I'd thought of it myself...

@_date: 2015-08-03 21:51:36
@_author: ianG 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
It's the latter - generate the deadlock on decision.  That could be done in theory with two essentially equal protocols, then fine, but I expect this Buridan's Ass story to collapse;  it's a dynamic world, and either two essentially equal protocols are not equal tomorrow with more analysis or news, /or/ the engineers know it and go with a coin toss.
I'm expecting the two protocols to be quite different and difficult to compare.  This is in order to preserve the tribe that supports each; the two protocols have to be oriented to their own tribe in ways that they appeal and horrify in equal measure.
Also, the nature of the attack is that the attacker will change the nature of the attack, if it suits...  The essence is the outcome, not the inputs, and this attacker cheats.  So I'd fully expect the attacker to actually improve the underdog if it was losing support.
I think even in real life that's not easy.  Two protocols can score highly on different criteria, thus setting off an argument as to which criteria is more important.

@_date: 2015-08-04 14:01:19
@_author: ianG 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
So in essence, the group forks, and the running code teams pursue informational track rather than standards track.  The consensus battle shifts to the marketplace.
The attacker has presumably concentrated his forces on people who don't write the code.  So that thrust is only valuable in WG if the decision is kept within the WG.  As the running code teams leaves, they achieve a hollow victory.
OK, I get it.  This approach works if the big companies look at things from an engineering pov, and adopt the Informational doc on the merits.   It fails if the big majors hold out and it doesn't achieve scale.
(So if this attacker can also impact the majors, it's harder.  Although, one has to note, that even in the case of the WG coming to a conclusion and publishing the RFC, we still depend on the majors to deploy in order to reach market consensus / critical mass / effective deployment.  The majors are more likely to respond to a formal RFC;  eg at least one major declares only to follow standards, it won't save its users unless someone tells it how to do it by standard.)
Right, so attacker downgrades the running code aspect in WG.  I think I see the tactic.
Yup.  No "democratic group" ever is.  Sadly, a battle of dictators has more success in designing new stuff.  A "democratic group" is only useful when the group decides they are better off agreeing in a room what is the combined way forward, when the alternative is open warfare.
ps;  There's something of hubris in the standards world.  They all seem to believe they can do more than resolving market battles into standards.  E.g., the British Standards Institute just recently came up and said they wish to start standardising cryptocurrencies.  The bind

@_date: 2015-08-04 14:29:41
@_author: ianG 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
I think I'm seeing the fallacy in my thought experiment.
My false assumption here is that the decision will succeed.  If we do a good job, prepare a good document, then profit and happiness will ensue.
Unfortunately that assumption is so far from useful that it is actually raises questions.
The crux of the difficulty comes down to this, I think:  The biggest issue by far is deployment of the protocol - how likely it is that the various erstwhile users of the protocol are going to pick it up, write it, deploy it.
Success in deployment is approximately an unknowable, a priori.  There are so many factors involved that from the group's perspective it is unpredictable.  We seriously are looking at from approximately 0% to approximately 100% without any real scientific tool that helps us further.
In effect, the factors that effect success of the efforts are outside the group's control.  And outside written requirements.  We can't "require" deployment.
But, the unwritten requirement is imposed on us - we still have to evaluate every proposal from the point of view of later deployment. Indeed, if I'm right, it is the most and only important criteria.  Even though it is unstated and cannot be stated.
Hence, this is fertile ground for two groups to do what you state - bifurcate on favourites, and only list the benefits of their choice. Because as soon as the get into the real question - which will deploy better - the useful question is withdrawn because we're crystal ball gazing.

@_date: 2015-08-04 14:39:21
@_author: ianG 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
This is a good point.  Were I to start accusing of a rough consensus attack on some WG, I'd probably be assisting that very same rough consensus attack...
NIH == not invented here?  Yes, I see that.

@_date: 2015-08-06 16:24:18
@_author: ianG 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
It's a little bit difficult to tell because often there is substantial cross-fertilisation, and sometimes successful protocols go from base invention and then into standardisation (some would argue that is the meaning of the word standardisation...).
Eg., In terms of successes, SSL, SSH, Skype, PGP, Bitcoin, OTR, were all invented outside standards bodies.
Most of those went into standards, but arguably their best work [1] was done before hand.
Practically all ciphers are done outside standards bodies, although one could argue that AES was done within a "standards" context.
In terms of failures, IPSec, DNSec, Secure Telnet, were invented inside the standards process.  Wifi 802.11?
S/MIME was inside, as far as I know, and could be called as much a success as PGP at invading the email world, debatable.  GSM was inside a standards process, and was a success, notwithstanding the bugs and interferences found.
So all in all, for my count, the answer is closer to 100% than 0%.
The difference might be in the way we define 'better'.  I define 'better security' as what is delivered and deployed and protected to users, as opposed to what they miss out on.  So SSL is a failure in my definition because it only covers about 1% of browsing [2], and its authentication is too easily bypassed.  Whereas others define 'better security' according to some standard model such as CIA in a lab setting.  In which case they define SSL as a success because it meets that criteria.  Yet others might go further and define 'better' as a loss-rate difference, but we don't have the data to support that as yet, IMHO, except in the case of phishing.
It's certainly a very good question and it should be widely debated.
I'd even go so far as to say it's a topic that should be researched and mined.  Someone needs to do a big table with protocols down the side, and metrics of success across the top.... [3]  A masters project?
[1] by "best" I mean the best bang for buck.
[2] may be higher by now, haven't seen any figures on this lately.
[3] like this:

@_date: 2015-08-06 16:28:54
@_author: ianG 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
Just avoiding x.509 and CA stuff is probably the biggest win in terms of ROI, and is enough to justify bringing in a high-paid resource who can do that.  It took me about 1 month to write a custom equivalent, and another month to roll it through all my code.  Since then, peace on earth.  Replacing both OpenPGP (and x.509) sits right up there on the top investments I've ever made.
Amen to that.  iang

@_date: 2015-08-06 19:31:20
@_author: ianG 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
It would be useful if someone more informed could post the proper URLs for this.  This is what DuckDuckGo MITM'd for me:
but croogleanalysis says it might be this one:
I don't think it is this one, which is not revised to include Keccak although it is advertised as revised:

@_date: 2015-08-09 16:26:09
@_author: ianG 
@_subject: [Cryptography] Threatwatch:  CIN - Corruptor-Injector Network 
There's a long post by "cryptostorm_team" that describes a capture of the activity of a CIN or Corruptor-Injector Network.
The short story appears to be malware injected into the router which then proceeds to present a false view of many things, including google sites and chrome downloads.
That last part again - the CIN appears to be capable of injecting a special download of Chrome which then participates in the false presentation to user.  Given the complexity of modern software I'd say this to be an impossible task except for a very well funded, long term The implied conclusion is nothing good - if this attack is scalable and scaled, the secure web system (HTTPS+CAs, etc) is no longer capable of defending. The implied limitations:  the attack works through a pwned router (no hope there), and it may rely on downloading a new pwned brower (slight hope!?).
It's pretty clear I don't follow the ins & outs, and could be well off base.  But worse than that, the team that wrote the blog post don't have the confidence to say what's really happening.  The story is full of "we don't know what's happening here, but..."
If true -- if this isn't some monumental failure to follow some new google gyratory security system -- then we have the spectre of a very bad situation:  A team that claims to spend their full endeavours on this security stuff is also not able to be certain of what's going on. Even if they're a mediocre bunch of undergrad dropouts with the hubris of gamers, even if you know better, or Kaspersky's got it in the bag, they're still likely more informed than 99% of the corps and 99.99% of the users.
What hope the rest?

@_date: 2015-08-10 11:55:36
@_author: ianG 
@_subject: [Cryptography] Threatwatch:  CIN - Corruptor-Injector Network 
Yep.  Plenty of skepticism there.
Let's step back from the situation and ask what's really happening here?   Let's say the guys are mostly pretty competent.  They've gone down the rabbit hole.  Come out hyperventilating.  Can't see what's what and what's not.
They could have just made a mistake and a better team would have figured it out.  But actually ... likely not.  If you take a random team across the world and try and figure it out, my guess is you would come up with the same situation:  "don't know."
Imagine a Security Certified Engineer's exam.
Or, contrast this to your vehicle, where you drop it into the mechanic for a checkup.  He's supposed to come back and say it's safe and operating fine.  The brakes work and will continue to work.  The engine won't blow up, the tires are safe.  All these things.
Even with the snafus of recent Jeep rides - that's still pretty much true.
Whereas here - if a customer had been mostly infected, we've got a situation where a mostly competent mechanic (an assumption, I grant) cannot figure out what's happening.  Can't even point at the correct path.
Now, granted, everyone knows their favourite lab that can handle this question, but at what cost?
And, here's the clanger - your car mechanic will issue a certificate that it's safe to drive (does so every year for registration) - but those labs aren't going to issue a certificate that the network is clean for any reasonable cost.
Google isn't going to declare formally that "everything is clear, it's good."  Note how they are fixing a few misconceptions about certs, but to go further than that is probably out of reach.  Google is not saying there is no injection.  There isn't a CA in sight that is going to put its nose above the parapet.
I think we've hit and passed the peak of complexity that is tractable for security.
We know that attacks and breaches have been rising rapidly in the last 5 years or so;  complexity has been rising since the web was invented. Have we created a situation where only very large players can muster the ability to defend themselves, large attackers can do what they want, and the rest are sheep for slaughter?

@_date: 2015-08-14 20:15:06
@_author: ianG 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
so picking the smaller convenient s=128, and d of 128 for ephemeral purposes, I get these strengths:
collision   = 64.
1,2preimage = 128.
Good enough for government work.  For longer term general stuff, s=128 and d=256
collision   = 128
1,2preimage = 128.
Taking the s=128 and d=256 above I would then have:
collision   = 85
1,2preimage = 85.
Which is probably enough in the short term for most purposes to create a breathing room for upgrade.
So if we live in a world where Quantum is a threat we need to deal with, we'd like to jump to s=256 and up to say d=512:
collision   = 170
1,2preimage = 170.
Does all that make sense?
One of the things that has emerged in the last N years or so is that it is up to the protocol designer to present a good cipher suite that is balanced.  Letting a user choose different algorithms has proven to be a bad idea -- the user doesn't know more than us, and to a pretty good confidence level knows much less.  So to some extent we've lent on this idea that for a 128 bit strength we need a 256 bit hash, a 128 bit cipher, etc etc as is now popularised by the Suite B list.
But Sponge is challenging us to get a bit more precise about our calculations.  The good thing here is that the tools described above aren't hard to deal with.  The annoying thing might be that the idea of us being able to deliver an algorithmic smorgasbord is receding, but nobody I know has come up with a good reason for preserving that as a feature (as opposed to the more general argument that we need the flexibility to swap out entire suites).
So if people want to go full IoT, can we ask:  what does that mean?  Can we draw the line and say the OpenPGP offering here is CipherSuiteIoT which means x/y/z in numbers and params and no more no less?
 > IOT looks set to create a demand
 > for an absolutely minimal cryptographic
 > suite. One signature algorithm, one
 > exchange algorithm, both on the same
 > curve, one authenticated encryption
 > mode, one digest/pseudorandom function.
Or are we offering full cipher flexibility to those IoT designers, and thus forcing them to implement all the multiples, because they won't know what other designers will choose, etc?
My thinking right now is that (assuming we're doing this) we should put in the draft a recommendation that precisely identifies a minimum most-popular obligatory to implement suite that covers as far down as we can get it.  And leave the rest up to the market?

@_date: 2015-08-14 20:49:26
@_author: ianG 
@_subject: [Cryptography] Threatwatch: CIN - Corruptor-Injector Network 
Basically, yes.  The situation we are looking at isn't verifiable from the outside.
It's like the financial system, without auditing.  (And we all know where that's gone.)  It all works perfectly fine when nobody's doing anything wrong, and the insiders know what they're getting out of it. We get verbal assurances that all is good, go back to sleep.
But as soon as something goes wrong, we get another complicated description, and no assurances of any value - we'll fix it, go back to It used to be that a standard techie - say a university student - could come in, check what the browser and server was up to, and declare it safe and secure.
The user could take on some risk, be part of the process.
Now, we can't even rely on a crypto-security org to come in and verify the situation.  Audit is no longer tractable.  The barriers to entry are written so high that only specialist insiders at every point can check these things.

@_date: 2015-08-15 04:06:54
@_author: ianG 
@_subject: [Cryptography] NSA has just recommended that Quantum is a threat 
IAD recognizes that there will be a move, in the not distant future, to a quantum resistant algorithm suite. Based on experience in deploying Suite B, we have determined to start planning and communicating early about the upcoming transition to quantum resistant algorithms. Our ultimate goal is to provide cost effective security against a potential quantum computer.  We are working with partners across the USG, vendors, and standards bodies to ensure there is a clear plan for getting a new suite of algorithms that are developed in an open and transparent manner that will form the foundation of our next Suite of cryptographic algorithms.
Until this new suite is developed and products are available implementing the quantum resistant suite, we will rely on current algorithms. For those partners and vendors that have not yet made the transition to Suite B algorithms, *we recommend not making a significant expenditure to do so at this point* but instead to prepare for the upcoming quantum resistant algorithm transition.
For those vendors and partners that have already transitioned to Suite B, we recognize that this took a great deal of effort on your part, and we thank you for your efforts. We look forward to your continued support as we work together to improve information security for National Security customers against the threat of a quantum computer being developed. Unfortunately, *the growth of elliptic curve use has bumped up against the fact of continued progress in the research on quantum computing*, necessitating a re-evaluation of our cryptographic strategy.

@_date: 2015-08-15 04:30:12
@_author: ianG 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
Wait - I'm on the wrong bloody list .. this was supposed to be a message to OpenPGP.  Oh well.

@_date: 2015-08-15 04:39:01
@_author: ianG 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
Ever since the Lenstra & Verheul 2001 paper, people have been arguing about how to match up strengths.  To little consistent and methodological effect.
Keccak may be the most significant step since then.  It at least claims an internally consistent methodology.
(I might be wrong.  Maybe they stole if from somewhere else.  But keylength.com is testament to a 15 year history of ad hoc methods.)

@_date: 2015-08-17 10:12:41
@_author: ianG 
@_subject: [Cryptography] Speculation about Baton Block Cipher 
I'm not sure how you get that it is has a variable blockwidth from the wikipedia page?
But yes, I see the hint about the checksum:
"160 bits of the key are checksum material."
Yeah, interesting point.  Although it's not really a "cipher" in the old terms, it's more a cipher suite, and maybe the composition just got lost in the bureaucracy of creating the standard?

@_date: 2015-08-18 16:19:06
@_author: ianG 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
NSA is now pushing the notion that quantum vulnerable algorithms are to be avoided [0] [1].
fwiw, my understanding is in responding to quantum, we prefer large RSA in the medium term (8k?) and switch to NTRU [2] in the longer term.  We avoid ECC.
Right, dial down to 128 level.  Or, we go to second order risk analysis

@_date: 2015-08-29 13:20:36
@_author: ianG 
@_subject: [Cryptography] A thought about backdoors and quantuum-resistant 
As PHB indicates, prevailing thesis is that this means the algorithm is a public key algorithm with the NSA holding the private key.  Or perhaps the weak keys argument.
It's certainly possible.  If it was done using paramaters, that would be one thing.  If it was done using some scientific understanding, that would be another, more risky thing, because others could figure it out.
It's certainly something they would try if they could get away with it.   If one has followed the DUAL_EC story, and recent revelations about Crypto AG and the NSA mission statements that directly seek to pervert commercial cryptography, one can only conclude they would do it if they thought they could get away with it.
I think on the whole it is possible.  It is also likely that they have thought of it.  And they are spending money on that area in a big way.
Whether it happens or not is too many hypotheticals for us to seriously predict at this stage.  Which is to say, what the risk level is and whether to mitigate is too hard to tell.  And normal Occam's razor logic on risk analysis would say that if you can't model it, treat it as if it doesn't exist.

@_date: 2015-12-08 01:32:21
@_author: ianG 
@_subject: [Cryptography] Cryptography is not a science currently 
There is one rule in spying that makes the difference between democracy and dictatorship:  The charter to break the law - which spying completely is because it is against the law in every other country - must never be used on ones own people.
That is the line in the sand that must never be crossed.  NSA crossed it.
(And so did every one of the 19 US agencies that cooperated with the bounty - but the responsibility ultimately lies in the hands of those with the charter to break the law.)

@_date: 2015-12-11 18:53:26
@_author: ianG 
@_subject: [Cryptography] Opinions on signatures algorithms for 
Do you have a URL we could click to see a 2-D rendition of your text rendition of a 2-D matrix?

@_date: 2015-12-12 20:15:14
@_author: ianG 
@_subject: [Cryptography] Talk on encryption to non-crypto audience ? 
Where I would start is the humble message digest or hash.  You can use it for so many things, that if they walk out understanding this exists they'll be better for it.  You can also drop in there that the secret of Bitcoin is nothing more than elegant use of hashes, and they'll be all gooey and happy that they've got an insight.
Perhaps we need a new picture for a persona called Rogaway, a character that always does what he knows is wrong to do ;-)

@_date: 2015-12-14 20:28:55
@_author: ianG 
@_subject: [Cryptography] GCHQ puzzler for xmas 
Britain's most secretive organisation - GCHQ - has added a cryptic twist to Christmas card season by including a baffling brainteaser.
This year spy agency director Robert Hannigan is sending out a complex grid-shading puzzle inside his traditional Christmas cards of the nativity scene.
Successful codebreakers will uncover an image in the grid that leads to a series of tougher challenges.
Those not on the card list can have a go here or on the GCHQ website.
Mr Hannigan is asking players who complete all the stages to submit their answer to GCHQ by the end of January.
Those who enjoyed the challenge are asked to make a donation to the National Society for the Prevention of Cruelty to Children.

@_date: 2015-12-19 23:36:17
@_author: ianG 
@_subject: [Cryptography] GCHQ puzzler for xmas 
Hmmm...  I could send in a solution under the name of "Phillip H-B", hoping that the honorable PHB hasn't already named "iang" and I won't disappear before I get mine into the post.
Santa's Cryptic Xmas Race Condition!

@_date: 2015-12-25 16:44:15
@_author: ianG 
@_subject: [Cryptography] Questions about crypto that lay people want to 
That's marketing.  This is liability:
NEITHER PARTY WILL BE LIABLE UNDER ANY CIRCUMSTANCES WHATSOEVER FOR ANY
CONSEQUENTIAL, INDIRECT, SPECIAL, PUNITIVE, INCIDENTAL OR EXEMPLARY
DAMAGES, INCLUDING WITHOUT LIMITATION LOST PROFITS OR REVENUES, WHETHER
FORESEEABLE OR UNFORESEEABLE, EVEN IF SUCH PARTY HAS BEEN ADVISED OF
THE POSSIBILITY OF SUCH DAMAGES.
It's hard to see, yes.  How about:  the CAs actively stop the browsers from changing the security model to deal with any alternate model that might prevent the spoofing, on the assumption that any better security model won't sell as many certificates.
Showing that in court is a bit more work though :)
Which in theory is stopped by the security model - email that is signed by the real boss looks different to the non-real boss.  Problem is, it doesn't work, in practice.  Fundamental reason it doesn't work is because the CAs can't figure out a way to get everyone to pay for individual certs.  And even if they can't figure it out, they're sure to block any attempts to use another technology.  Deadlock, users are screwed.

@_date: 2015-12-25 16:45:34
@_author: ianG 
@_subject: [Cryptography] Questions about crypto that lay people want to 
Governments have been running "steaming" operations against the post from since the post was invented.  They have been running spying operations against other forms of communications since forever.
Does that effect ordinary citizens?  Typically not.  The reason this is so appears to be pure economics:  a spying operation is typically very costly, so a government focuses very drastically on the biggest threats - other governments.
Now however spying can not only be done at automated cost, it can be done at mass, across vast numbers of people at the same time.  The economic barrier to mass surveillance is removed.
As spying goes from economically rare to mass surveillance, the question arises as to whether citizens should care.  A lot of literature says yes.  A more directed answer would be, is ones government benign and just, or is it liable to do dirty things?  Each citizen would need to answer that for herself or himself.  But if the answer is the latter, government with no limits, then crypto may provide some protection.
The crypto war was lost in the 1990s.  In contrast to the popular story, what happened was that the export restriction for open source software was removed in USA.  But, in exchange, they also doubled down on "filing with the government" and many other behind the scenes situations.  They traded one fig leaf for another, "regulated behaviour /1st amendment for
Now, we're in to something a bit different.  This time, we've discovered we're under mass surveillance, which we weren't in 1990s. This time, we're staring at our navels and wondering why all of our efforts to deploy a lot of protection seem not to be working out (whether it be crypto or something else).
The difference is that randomness is the test for decryption.  It must be indistinguishable from randomness to the attacker. Of course, we know it is not random because it derives from a deterministic encryption, but if it can be shown to be indistinguishable-from-random to the attacker, that test tells us it's very strong.
In cryptography, a code is a number with a meaning.  E.g., 345=ship.  In this sense, an error correcting code is not quite a code, it's more of a digest, but common computer science doesn't necessarily follow cryptography all the time.
It's a metaphor, only.  As a metaphor, it works until it stops working.   It's maybe more accurate to say that the combination lock's "combination" is more of a cryptographic key.
One for John Denker :)
This is a big-O discussion.
1. HTTPS isn't good enough for your bank account, but it would take a very long and serious investigation to show you why.
2. You can't vote using HTTPS because the problem of voting is to do with extortion over your person, and crypto hasn't got a good answer to that (alone).
Slowly, over time.
Nah, it's a marketing thing only. What it "means" is that the crypto uses stronger keys like 256 bit instead of say 40 bits. But the strength of the keys is rarely the issue, the dominating factor is the way it integrates into application and your usage. Weak crypto better deployed beats strong crypto badly deployed every time (unless you're a spy or government). Beyond that, the engineering under the hood is far more important. In a nutshell, it's like comparing engine sizes, it doesn't really tell you much about how fast the car will go, or even the truck, but it's a nice number to boast over beers.
There are various compliance and standards things.  Suffice to say, that isn't necessarily going to result in better protection, but it might result in "engineering to a certain standard" whatever that means.
Nope.  Yous keys will be an extra 2^1024 bits harder to crack, but that doesn't mean you'll be safer because the weak links are all elsewhere in typical systems.  2048 is the current recommended length, but bear in mind that although we've recommended 2048, it is one of 10000 details which we have no recommendations over.... "Trust us, we're doctors"

@_date: 2015-12-25 17:23:30
@_author: ianG 
@_subject: [Cryptography] Some bits from the 1981 NSA COMSEC Guide 
tl;sr - too long, should read - it is all fascinating material.

@_date: 2015-12-27 14:36:14
@_author: ianG 
@_subject: [Cryptography] Some bits from the 1981 NSA COMSEC Guide 
In the United States, it would seem that the majority of extramural cryptographic funding may now come from the military.155 From 2000 to 2010, fewer than 15% of the papers at CRYPTO that acknowledged U.S. extramural funding acknowledged DoD funding.156 In 2011, this rose to 25%. From 2012 to 2015, it rose to 65%.157 Nowadays, many cryptographers put together a large patchwork of grants, the largest of which are usually DoD. The following funding acknowledgment isnt so very atypical:
      This work was supported by NSF, the DARPA PROCEED program,
      an AFOSR MURI award, a grant from ONR, an IARPA project
      provided via DoI/NBC, and by Samsung.158
The military funding of science invariably redirects it159 and creates moral hazards.160 Yet suggesting to someone that they might want to reconsider their taking DoD funding may anger even a placid colleague, for it will be perceived as an assault both on ones character and his ability to succeed.
No matter what people say, our scientific work does change in response to sponsors institutional aims. These aims may not be ones own. For example, the mission of DARPA is "to invest in the breakthrough technologies that can create the next generation of [U.S.] national security capabilities." Having begun in the wake of Sputnik, the agency speaks of avoiding technological surpriseand creating it for Americas enemies.161 In the USA, the NSA advises other DoD agencies on crypto-related grants. At least sometimes, they advise the NSF. Back in 1996, the NSA tried to quash my own NSF CAREER award. I learned this from my former NSF program manager, Dana Latch, who not only refused the NSA request, but, annoyed by it, told me. An internal history of the NSA reports on the mistake of theirs that allowed funding the grant leading to RSA.
NSA had reviewed the Rivest [grant] application, but the wording was so general that the Agency did not spot the threat and passed it back to NSF without comment. Since the technique had been jointly funded by NSF and the Office of Naval Research, NSAs new director, Admiral Bobby Inman, visited the director of ONR to secure a commitment that ONR would get NSAs coordination on all such future grant proposals.162
People are often happy to get funding, regardless of its source. But I would suggest that if a funding agency embraces values inconsistent with your own, then maybe you shouldnt take their money. Institutions have values, no less than men. Perhaps, in the modern era, they even have more.
Large organization have multiple and sometimes conflicting aims. Military organizations with offensive and defensive roles in cybersecurity have COIs built into their design. Individuals are wrong to assume that their work is non-military work errantly funded by the In his farewell address of 1961, President Dwight D. Eisenhower introduced the phrase, and concept, of the military-industrial complex. In an earlier version of that speech, Eisenhower tellingly called it the military-industrial-academic complex.163 If scientists wish to reverse our complicity in this convergence of interests, maybe we need to step away from this trough.
None of this was clear to me when I first joined the university. A few years ago I joined in on a DoD grant proposal (fortunately, unfunded), which I would not do today. It took me a long time to realize what eventually became obvious to me: that the funding we take both impacts our beliefs and reflects on them.
In the end, a major reason that crypto-for-privacy has fared poorly is that funding agencies dont want to see progress in this direction,164 and most People will of course point to Tor as a counterexample; it has received funding from DARPA, ONR, the State Department. I dont think theres much to explain. Even companies dont want progress here, either. Cryptographers have internalized this. Mostly, weve been in the business of helping business and government keep things safe. Governments and companies have become our customers, not some ragtag activists, journalists, or dissidents, and not some abstract notion of the people. Crypto-for-privacy will fare better when cryptographers stop taking DoD funds and, more than that, start thinking of a very different constituency for our output.
 Think twice, and then again, about accepting military funding.165
 Regard ordinary people as those whose needs you ultimately aim to

@_date: 2015-12-27 14:54:31
@_author: ianG 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
The process we developed at CAcert was:
1.  Everyone brings their favourite RNG on their laptop.  I used laptop photos of a white card in lowlight, similar to above, and hashed every photo.  One person used John Denker's audio device.  Another used the OpenSSL RNG ;-)
2.  Each feed was then transferred on USB stick to a single offline 3.  All feeds were then combined (XOR'd together and hashed) by a 1 page C program.  Result was fed into the key generation process.
4.  After successful key generation, the single computer and the USB sticks were destroyed.
5.  All steps were supervised in the open by all.
The goal of the process was to create a good RNG, even if N-1 conspirators were able to slide in their borrowed NSANGs.

@_date: 2015-12-27 15:24:16
@_author: ianG 
@_subject: [Cryptography] Questions about crypto that lay people want to 
I'm not saying it doesn't exist - I'm saying it is there for marketing, not for insurance.
Has it ever paid out?
It is an old trick in the insurance industry - to sell an insurance or warranty protection for something that never or rarely happens or will never pay out for "legal construction" reasons.  Another old trick is to put an excessive requirement for insurance into a standard, knowing that large firms can handle this more easily than small firms.  It's called a "barrier to entry."
Certificate Transparency would be one of those :)  Also Identity based encryption, from an email address.  Skype or SSH opportunistic models. Certificate pinning - proposed in the mid 2000s, rejected then, now enjoying a resurgance (because?).  Full SSL rather than half-baked server-auth where every browser creates client certs on the fly, one per site and automatically authenticates itself to the server.  Gmail authentication based on all of google's capabilities.  Same with Office360.
Think of it like a grad student problem - throw the problem at a couple of grad students and see what they come up with.  Is there any reason to believe that the PKI model that dates back to a 1980s masters thesis and was designed in a 1-telephone-national-telco world is the last word in What we are seeing is that Google is breaking away from the traditional model by re-doing the whole lot.  Started with CT, moved over to QUIC, then to HTTPS2.  The reason (I theorise) that google is doing this is because it's been caught on both sides of the client-server divide, in ways that the the other suppliers have not (not a particularly strong theory, but the only one I've got).
We're also seeing the steady evolution of the browser manufacturer as super-CA or ber-CA.
Does "blame the user" mean that users are inadequate or that they security model is inadequate?

@_date: 2015-02-02 00:25:09
@_author: ianG 
@_subject: [Cryptography] traffic analysis -> let's write an RFC? 
Value to us?  Not a lot.  Value to them:  huge.
It forces them to prepare their documentation knowing that there will be public audit of the process.  Lots of arrows will be loosed at it.  Many wild, but some perceptive.
Sunlight is the best disinfectant.  Openness will lift the game.  They will do a better job, think about wilder scenarios, come up with a more balanced risk approach because they will be less likely to sweep inconvenient risks under the table.  We might spot something they truly And, open disclosure that will give their customers better security.
ps; no idea who the "we" & "they" is in the above, but hopefully the principles are sound and universal.

@_date: 2015-02-02 00:31:34
@_author: ianG 
@_subject: [Cryptography] How the CIA Made Google 
Sadly, these are really good tactics.  They're almost costless, they really hit hard against the auditing public, and they're almost blameless.
I'd love to see evidence of the program, and I don't doubt it exists, it's just too good to pass up on.  We know for example that the USAF was running around stoking up the UFO people so as to give cover for the experimental plane flights.  There is no doubt that the spies are running around stoking up the cypherpunkian elements to provide cover for what they are really doing.  Read a hundred classic spy stories, etc.
Even if we see the evidence, the masses still won't believe it.  But, speaking for myself, knowing that there was compelling verified evidence of actual skulduggery was something that kept me sane.

@_date: 2015-02-02 15:06:35
@_author: ianG 
@_subject: [Cryptography] best practices considered bad term 
Right.  A 'best practices' wants to be a code.  But it ain't, and talking it up as a code won't make it so.
Clearly we would all be better off it was a code, or regulation, or law.   So what's the difference, what's the leap here that the 'best practices' fails to make it to good-reg-ship?
For my money it is this:  We don't know how to do it / write it.
Opinions differ, and that is for good reasons.  Recall threat modelling, In a dynamic OODA security world, is it even reasonable to think that we can so clearly build a security fence that does the job for all the people all the time?  For enough of the people enough of the time? Maybe, some portion of it, says Gunnar:
Electrons don't fight back, so we can build a code over a century of learning to keep them corralled in their proper cables.  No such luck in the security game, I suspect.  Attackers don't lie down and die, suddenly turn into sheep and bleat just because we put CODE On our security model.
Maybe the answer is that attackers won't let us 'know' how to write a 'best practices' guide?
ps; apologies in advance to those who believe the security industry knows how to deliver security...

@_date: 2015-02-02 21:33:29
@_author: ianG 
@_subject: [Cryptography] best practices considered bad term 
(which follows the strength of the code, doesn't explain why the code is inherently strong and sensible.)
Of course, codes and 'best practices' alike, and all similar things, are basically battlegrounds for competing economic interests.  I always chuckle when I recall the Europeans referring to the group that creates the German Electrical Code as the Committee for Siemens.
Right.  Cost of 'code' outlets way exceeds the cost of powerboard outlets .. and we have a stability in the market for outlets :)
So can we come up with a 'best practices' for passwords?  If these things work as a concept or an idea, surely we should be able to write one set of guidelines -- short and sweet so people can grok them -- that solves the issue for most people most of the time.  Right?
Here's my contribution:
      1. Write passwords down.
Man or mouse?  Anyone here says they don't know how to do passwords?
(In a separate document, sure.  Or in a few words attached, so it doesn't weigh the document down past readability.  Right, I see your point.)

@_date: 2015-02-02 23:32:50
@_author: ianG 
@_subject: [Cryptography] How the CIA Made Google 
Thanks for bringing us back on track.
Our minds tend to turn the original dramatic claims into what we want to and can defend against.  Says something, right?
The original article was not about google doing bad security work.  We all know google employs all the best and brightest on the planet, it's technical security work is presumably second to none.
Rather, the original article was about its executive levels being too cozy with the 'Highlands Forum' for want of a better name.
In my experience, the favoured attack of the agencies is not to attack the corporation's data systems, but to place people inside the org.  I say this from 1st hand:  I was taught what the approach would look like, we already had track record in the approach, we developed the systems to mitigate the threat, and I personally had to deal with an approach, which was later confirmed by an independent source.
Circumstantially, that matches what the article is trying to say, but lacking the sort of deep paranoid (!) spy understanding of how these things work, the article is easy to dismiss.  And, as clearly, if your mind is a tech-hammer, you'll relish banging cryptographic nails in to any problem.

@_date: 2015-02-04 01:04:36
@_author: ianG 
@_subject: [Cryptography] crypto standards and principles 
At this stage of the game, they need the complete-body plastic surgery job, never mind the makeup.
I have in the past suggested that NSA might actually improve their image and gain some lost respect from the open community by releasing some info about Suite A.  But so far the packet has been returned unopened.

@_date: 2015-02-04 01:12:41
@_author: ianG 
@_subject: [Cryptography] best practices considered bad term 
I'm not so sure.  If you look at the 2000s, Apple shipped gear that was remarkably free from bugs and attacks.  Their security bug list was in the 3 figures whereas Microsoft was in the 5 figures.  I suspect that is still the case, although I don't track it.
Now, here's the sell:  Over the 2000s, people drained out of the Microsoft world to the Apple Mac OSX world pretty consistently.  At the start, Apple was tiny.  At the end, the biggest.
And -- my hypothesis -- they did that in significant part because the Mac OSX product was more secure.  By this I mean, no requirement to run virus scanners, and until last few years, very little update and change requirement.  Which meant more time and more $$$ in users' pockets.
The fact that this happened slowly, user by user, outside the stores and support channels, meant that the security journos and pundits and experts simply didn't notice.
I'd say, *in the long run*, Apple beat Microsoft on software security. It helped that their hardware was good too, and that they had the sense to aim for the premium price range.  By that, I mean Jobs took the long view, a decade.  Wouldn't fly in other circumstances of course.

@_date: 2015-02-04 01:16:40
@_author: ianG 
@_subject: [Cryptography] best practices considered bad term 
Yes please.  Can we get the IETF to look at this, too?  Their huge & lengthy committees have not as yet realised that they are tiny and inconsequential compared to the vast masses of developers out there that have to actually build their impressively weighty designs, and make them

@_date: 2015-02-06 11:11:29
@_author: ianG 
@_subject: [Cryptography] where is crypto going in the next decade? 
There is now a evolving field in encryption that is tailored for data of special sorts:  database rows and columns, maths through the encryption without decrypting, that sort of thing.  At a recent RWC2015 in London, the local grad students did their poster thing and I'd guess at least half of them were about that area.
(I'm personally skeptical but hey....)
Another big direction is AE.  This is very welcome, there is no reason that cryptographers should concentrate on block ciphers and software engineers should muck around composing modes and macs and whathaveyou. Now that the AE concept has been defined, throw it over the wall and have the cryptographers deal with it.
On that note, at a recent (FOSDEM) talk the Keccak people announced that NIST is now putting out the draft of SHA3 and it will include optional features for a HMAC and an AE cipher (which latter is also submitted to (Corrections welcome if my note-taking proved bad.)

@_date: 2015-02-08 11:36:39
@_author: ianG 
@_subject: [Cryptography] What do we mean by Secure? 
Interesting idea.
In a roundabout way this is similar to something I wanted created at CAcert (still waiting).  In essence there is a Security Policy, and you can do whatever is in there.  Fine.  But you can also breach the rules if you think you have to.  But if you breach those rules, you have to file a dispute to Arbitrator, and explain yourself.  4 eyes and all that.
Now, in the system, to support this process, we wanted a comment field added to every critical action into which you either put your reason or you put the arbitration case number.  Those events with case numbers would be shot across to the Arbitrator to track, and those without would go to CSO who asks why you haven't filed dispute, yet.

@_date: 2015-02-08 11:44:00
@_author: ianG 
@_subject: [Cryptography] What do we mean by Secure? 
Well, they often do, as we see.  The issue isn't so much that the result is nebulous, but that security is *individual*.
In the old days, we used to say, WYTM or what's your threat model?  The problem with this was it captured the above fallacy perfectly -- we were all searching for the one threat model to rule all others.
E.g., the threat model _du jour_ is for the state to shut down your system and therefore we have to now use the blockchain to secure our socks & undies drawer.  The threat model of the 1990s was that everyone would listen/MITM your traffic on the open net so you have to get some CIA.
Security is an individual attribute and is not easily aggregated.  Even with homogonous groupings like "USA middle class white dudes" there is sufficient variation to make any 'security policy' look daft.  E.g, those aforementioned guys care little about their iPhone photo collection, but their girlfriends are paranoid about them.
Something that snapchat made billions on, so they know more about security than us, by some market measure.  Which leads us to some form of aggregation history:  the 'security' products that have made a lot of money are these:  SSH, SSL, Skype, Facebook, Snapchat, Bitcoin.  (How, why and repeatability is an interesting MBA-business-case-study exercise.)
So I guess we need a different way of approaching the question.  Maybe we need to ask two questions not one:
    1. what would make *you* feel secure?
    2. how aggregatable is that over a larger population?
I'm assuming here that we can't for example construct a security-for-the-individual process/technique that scales & works. I.e., now we need to take you through the policy wizard, relax, this won't hurt a bit...
I think ... people need to get a lot better at understanding that (1) security is something you have to do yourself, if you care.  If you don't care, then you're back to the firewalls & SSL & best practices approach;  but the evidence that you don't care is clear.
Which is rather tricky.  As others pointed out in previous threads, if you do care, then we hit limits to scale:  we don't have enough programmers to produce good security code, and we don't have enough CSO types to manage it well for enough companies.  Or, we're a bank, and our competitors have stopped caring and are shipping schlock, we have to respond and stop caring...
So the search is on to (2) turn security into an opportunity not a cost.   This is something that Apple do (I claim) and Microsoft have tried to do.  List above might also claim it as well.

@_date: 2015-02-08 11:57:15
@_author: ianG 
@_subject: [Cryptography] best practices considered bad term 
Yeah, data would be good.  But how to do that?  I'm not sure, the experiment has already been run, and isn't repeatable.
One thing that might be measurable is the switch over from monoculture to duoculture if that's the term.  People have speculated that you need 10% or more to make attacks for a single platform worthwhile.  So a prediction is that we should start seeing more trouble at the user level for Macs.
Oh yes - the business about bug reports not matching actual bugginess or even damages is well known.  I'm just using this as a proxy for "trouble."
What I do know is that throughout the 2000s people on Macs had zero trouble with viruses, re-installs, slowdowns etc.  In comparison to the Microsoft crowd.
Now, you can "explain" this by monoculture, versions, user idiocy, etc.   My point is that wherever it came from, it happened, and that was a significant factor in this:
No, it wasn't security for the users, it was reliability and usability.   And it was only apparent if you took a longer view, like a year or three in comparison.
Sure.  But their development lifecycle failed to kick in.  Remember Vista?  That was the product of the lifecycle.  It had to be replaced by W7.  Which was good, but *then* not as many upgraded, the legacy bit.
It's complicated.  I think in the long run, Microsoft dug themselves into a security hole out of which they had a lot of trouble digging themselves out of.  Remember they'd been digging that hole for 20 years ... unlike the spin-on-a-dime Blackbird thing in 1997, security turned out to be an oil tanker, and Bill Gates' famous memo wasn't enough to turn the thing around.

@_date: 2015-02-10 12:33:27
@_author: ianG 
@_subject: [Cryptography] What do we mean by Secure? 
Right.  So, there is a huge analogue here with the insurance market. Users want to be protected but won't pay for it because it always happens to someone else, and it is far in the future.  When it does happen, it's too late.  So FUD is used to sell these things, and there are scale opportunities such that a nationalised insurance scheme is often efficient at least on paper.
However we can't take this too far and say that we should have purchased security protection things like insurance or a nationalised PC health programme, because this assumes that one or other party knows how to do (C.f., oft-mentioned markets in imperfect information, link at bottom.)
Nope.  Threat models are only completely useless if no business model was in place to inform them.
(I'm simplifying here.  You can sell a completely useless security model and make people feel good about themselves - placebo.  Or you can sell it and cause them to reach compliance, which is good for business.)
Right, so what is happening here is that the organisation understands its business.  So the business model part is done, but the org does not understand how to take the next step.
And, you the security dude do not understand the business model, but you do understand the whole security lifecycle thing.  So there are two tasks going on in parallel:  bring the org forward into the threat discussion, and push the security dude backwards into the business.
So a common thing here for first task is to thrust a series of threats extracted from popular threat models in front of them.  "Are you worried about MITM?  Are you worried about your SO seeing your photos?  Are you ..."
Another simple way which gets more at the question of 'what is your business model' is to ask them "how did you lose money?"  Collect anecdotes.  And then go ask the customers "how did you lose money..." because often the business has internalised or dumped the losses of their customers.  Think retail shopping or browsers.
What I might be saying here is that you cannot ask the population anything in this area because the population only repeats today's memes.   If you started asking today about security threats of the population you'll get global warming, cyber security, terrorism, bitcoin, etc. Completely sodding useless.  You have to go ask people what they are scared of, and do it alone so there is no feedback loops, and work with that data.
Yeah.  I think security is one of those things in life that pops up models that are then self-defeating.
Right.  This is the liability dilemma.  If you actually do secure people more, and they get robbed through some ommission or other, because they were secured more, they are more likely to take it seriously and seek damages recovery.
The market for "best efforts for all" security is quite stable, the market for "better efforts for you" is not.  E.g., browsers do everything they can to provide one security model "and it's secure" and fight any attempts by the users to vary that model.
It works, the market for browser security is stable.  The benefits for the entire body for lesser security are often in excess of the losses to the provider for better security.
Yeah.  The market for programmers is definitely broken, but that's a topic way out of scope for this list ;)
Markets in Imperfect Information - Lemons, Limes and Silver Bullets

@_date: 2015-02-10 12:52:58
@_author: ianG 
@_subject: [Cryptography] Do capabilities work?  Do ACLs work? 
Having watched/worked with capability ideas for a while, I'm of the opinion they don't work as well in practice as the theoretical pundits would have it.
Also, the users continue to demand ACLs.
So my current view is that what is needed is a hybrid.  At a limited sense one can see this with expiries:  a cap with a time limit on it is a cap with a "control" on it.
In a more developed sense, my software has lots of caps running around, but servers that serve those caps also look at who's asking.  E.g., when Bob looks at Alice's photo, the server only grants it if Bob is in Alice's A list.
This certainly makes for more complicated software.  But when the judge asks, it's much easier to say "only Bob could have seen the photo" than anyone with a cap...
ps; a capability in the sense I mean above is implemented by an object which is hashed canonically and stored somewhere on the net.  If you have the hash, you can ask the store to reveal it.

@_date: 2015-02-15 14:45:08
@_author: ianG 
@_subject: [Cryptography] phishing attack again - $300m in losses? 
"In many ways, this hack began like any other. The cybercriminals sent their victims infected emails ? a news clip or message that appeared to come from a colleague ? as bait. When the bank employees clicked on the email, they inadvertently downloaded malicious code. That allowed the hackers to crawl across a bank?s network until they found employees who administered the cash transfer systems or remotely connected A.T.M.s. "
So, it's phishing, again.  Exotically called spear phishing, but still your common or garden phishing attack.
I am fascinated by the industry dynamics that make this attack impossible to deal with.  We first saw serious phishing in about 2003, and here we are a decade later .. with what progress?
It is clearly the fault of the browser [1].  All the finger pointing -- user responsibility for social engineering, mailer, standards -- is just that, finger pointing in ways to move the angst away from the mind of the owners of the browser projects.  The successful strategy for doing nothing has been a mix of "It's not our fault" and "we're working in XYZ The industry has cleverly constructed itself in a deadly embrace between browsers, standards, liability dumping contracts, IETF working groups, CAs, auditors, developers, lawyers who never-admit-nuttin, all of whom collectively share the responsibility and all of whom individually manage to actually take on none of it.
But they take the fees.
And, it isn't as if we can't calculate the value of the take.  Mozilla for example takes something like 85% of its revenues from one source, google, for one purpose, browser advertisement, which serves as a pretty good proxy for the value of Firefox to users.  Audit fees are known numbers, at least to industry insiders.  CA pricing is known, as is the occasional buyout windfall.  Developer salaries are known, and they can be counted and summed.
I'm fascinated because this situation should not by all our understanding of life, the universe and everything about security last.   The user is loosing money and has been for a decade now, and the browser vendors do ... nothing?  Approximately?
How is this deadlock to break?  Will someone actually put real authenticating crypto into the URL?  Will someone invent a new identity concept that works?  Will the browser vendors be sued for their millions in the bank by some group of ... upset banks?  Will the banking regulators finally do something that actually sounds like proactive security not 'best practices'?  Will someone start asking phishing victims which browser they used?  Will someone advertise a hardened browser for a price?  Will a government get hacked and decide to actually ask why the browser isn't doing the job?  Anti-trust?
I don't have any answers, but as I say, it is fascinating that we're looking at an industry structure that has concreted itself into iang, watching phishing since 2003...
[1] of course, the industry does not agree, but Bill Gates did:

@_date: 2015-02-16 17:47:13
@_author: ianG 
@_subject: [Cryptography] Do capabilities work? Do ACLs work? 
One threat outlined above:  when the judge asks...
The threat of being hailed to court might seem esoteric to cryptographers but it is pretty normal IRL.  Real people are threatened by real threats, not 1990s fighting-last-war myths like MITM, eavesdropping, CIA, etc.  Out in the world, the biggest threat is likely to be someone you trusted highly and has now turned against you.  Hence court is a big issue, and as a tactic in threat modelling, we can borrow from the lawyers:  "Imagine you are in court, how would this play out?"
Right - it's mushy.  The way users think (or one version of that) is that they want their data to be secure from threatening people, and available to friendly people.  They can describe friendly, sometimes describe threatening, and we can simplify by saying anyone who is not friendly is not given access.
Of course, this is mushy, as outlined above, the judge.  It changes.  So the above protections aren't really enough, what do you do when the friend becomes the attacker?
Right.  So when we compose two separate rights over two distinct objects into one delivery, what are the ramifications?  It turns out ACLs aren't good at that, whereas caps are much better at least at the technical question of what that means.
However, the fact that we can successfully build these interacting resource constructs with caps more than ACLs doesn't mean the result is pleasing to users.
Yup, in that the ACL system known as Unix users and groups was an artifact of 'departmental computing' more or less rendered dead by the arrival of the PC.  So, 1983 was 32 years ago, why does this keep coming up?
So, is that a context where you're designing a system / OS?
In my context, I'm designing a social network cum mobile money system, and I fully trust the (my) software.  What I don't trust is all the people hovering around near my user.  And sometimes I can't trust the user herself.
Meanwhile, I have to lean pretty heavily on Android to protect my software from all the other software.  Yes, big limits there, so when that breaks, we need defence in depth and ways to stem the damage.  But it turns out that statistically, users near my users are more likely by orders of magnitude to attack so I can pretty much let the defences against other people pick up the intra-android attack.
Someone recently made it very clear (Bill?) when the DoD person said, "no, you've got it wrong - we trust our people *because they've all been security-cleared* it is your software we don't trust."
Which war are we fighting again?  Who's war?
Grass is always greener, of course.  I'll lay good odds they haven't even come close to solving the identification problem.
Indeed.  But, when talking about caps and ACLs it helps to have at least one concrete technology at hand, otherwise the conversation spirals into the theoretical.

@_date: 2015-02-16 21:03:45
@_author: ianG 
@_subject: [Cryptography] phishing attack again - $300m in losses? 
Now, how many here clicked on all John Young's links about the Equation group, downloaded PDFs, went to dodgy links via funny redirects ... even though the mailer was saying "We think John Young is a scammer!!!"
This situation sucks.  Which is why I don't buy that it is a mailer problem.  Firstly as above.  Secondly because of all the other places a link can be sent -- modern people below 30 don't even know what email is these days.
The browser, or whatever we call the agent that handles the URL, has to be able to defend itself.  No ifs, no buts.

@_date: 2015-02-17 11:24:08
@_author: ianG 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
I agree they could do likely it, but I suspect they are constrained by the same economics as us.  How likely is it that their big malware packages are going to be useful in the case of a secret foreign piece of Their suite cost lots of programmers over a lot of time.  We're counting it in man-years.  They have to leverage that cost across a lot of victims unless we're talking about a Manhattan style attack victim.
In contrast their victim is likely moving around across a bunch of standard hardware.  Changing laptops, upgrading to newest hardware, etc.   As it is the sysadm community at relatively high-tech outfits, they're probably seeing rollover at 1 year, which means they likely have to hit all the big disk drive players, and hope nobody goes for arcane stuff.
One question of detail -- are drives bus-masters on modern systems? What does this drive do once it is pwned?  It would seem that just seeing all the data come in isn't enough, it needs to be able to start doing things actively.  If only to bypass the effects of FDE.

@_date: 2015-02-17 11:35:36
@_author: ianG 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
Back in late 2000s, there was a surge in interest in APTs and the industrial-military contractors went on a shopping spree looking for cyber-warriors.  At the time I discounted it as yet another hype thing, but it seems that it happened, and we're now in a cyber-arms race.
I'd rather say it this way:  we have circumstantial evidence that we are at about the same level for all practical purposes and intents.  As far as we are concerned.
There's a bit of a difference.  I'd say they are still way ahead in cryptanalysis, but not in ways that seriously damage AES, KECCAK, etc.
In contrast, I'd say we are somewhat ahead in protocol work.  That is, the push for eg CAESAR, QUIC, sponge construction, is coming from open community not from them.  In the 1990s we infamously blundered by copying their threat model;  now no longer, we have enough of our own knowledge and deep institutional experience to be able to say that's garbage, our customers are different.  And our needs are pushing the envelope out in ways they can't possibly keep up with.
Although, I could be wrong here - Equation team reports from Kaskersky didn't say much about the protocols they were using to exfiltrate, just that they had a fetish for Ron's ciphers.
In sum, I'd say they are ahead in the pure math, but you'd be hard pressed to find an area where it mattered.
E.g., as Peter & Adi and I are infamously on record for saying [0], the crypto isn't what is being attacked here.  It's the software engineering and the crappy security systems.
[0]

@_date: 2015-02-17 12:04:37
@_author: ianG 
@_subject: [Cryptography] self-MITMing my own TLS connection ... 
Interesting case study of where the market for MITMs is going...
How it Works
A user, called the 'auditee', wants to prove to another user, called the 'auditor', a certain fact attested to by an organisation (a bank, a government, a company etc.). This fact could be a monetary balance on an account, the fact of a money transfer, a particular set of identity information such as address, amongst others. The auditor and auditee create an encrypted messaging connection between each other over some neutral communication channel (such as IRC). The auditee connects to the website as normal and logs in, and then browses to the specific page that proves the required information. Then the auditor and auditee use their encrypted connection to negotiate secrets for the SSL/TLS session such that the auditor can find out what is on the page that the auditee loads, without gaining control of the connection or seeing the auditee's login details. The diagram below gives the outline of what happens.
white paper:

@_date: 2015-02-17 16:46:44
@_author: ianG 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
Snowden saying "encryption works."  EquationGroup use of RC4-6, AES, SHAs.  FBI complaining about going dark, we need backdoors - they only ever complain at that level as proxy for NSA, and same complaint is repeated in rapid succession in UK, DE.  Practically all the exploits so far disclosed are about hacking the software, hardware, nothing we've seen comes even close to hacking the ciphers.  Some of the interventions are about hacking the RNGs - which typically take the cryptanalysis to places where we can hack it.  Off-the-record comments I've heard. Analysis of released systems such as Skipjack.
It's all circumstantial.
There is the story about differential cryptanalysis - they released the first 4 volumes, but still haven't mentioned the other 4 ;-)
At one level, this all comes down to your model of science.  Typically we in the science world like to "know" stuff based on evidence from experiments, or similar facts that have been built up over time.  We are very careful to not let our imagination run away with us.
But this doesn't work with the spy business.  They will never let us run the experiment, they will not let us read the literature, and if we ever find enough to put 2+2 together, they'll run a deception campaign to break that logic.  Or lie.  Or they will remind us that "you don't know" or all of the above.
So we have to develop a better approach.  We can probably benefit from thinking of the question as a murder investigation - clues, hypotheses, correlations, etc.  We can't take it to a court of law -- they deny us that as well -- but we can form a view as to whodunnit.
Many won't accept that view, of course.  To them I say, you're dancing to their tune.
Right.  I'm surprised Android sells any phones in USA market.  Although I understand that it is the only way to compete with Apple, it is also the weaker position.  Which comes out in a price insensitive market. OTOH, I'm surprised to see an iPhone in Africa ;)
Maintenance of protocols is really hard, really expensive.  I know, I manage a 100kloc code base with several hard crypto protocols in it, and I'm drowning, perpetually.  Whatever we can do to get that into the open source world, the better.
Yes.  That is the huge mystery.  It's pretty clear the NSA is doing the non-NSA mission huge damage.  Yet no movement on the priorities, just blather about 'sharing' from Obama.  That's a mystery.
Right.  I think that we know, even though they won't release much evidence of it ;)
No, I think that is unfair.
I absolutely agree.  In the day, I also learnt about CIA, and so forth.   Only as time went on did I start digging into the reasons as to why famous systems weren't doing what we had hoped they did, and find that the original threat and security modelling wasn't good, was 'borrowerd' without thought.
 z
ations".  (The military has had to back-port some of these innovations as it, too, has become more knowledge/expertise based.)
Good story!
Yeah.  And their vested interest, following that priority, is to make things better for the offense side.  Which means dodgy software, dodgy security... for everyone including them.  Go figure.
Exactly.  Forget the crypto, look at the security systems.  They are experts at this and they pay huge numbers of people to be expert at this.
What's the guess -- how many cyber warriors are there in employment in USA today?  100,000 ?

@_date: 2015-02-17 21:40:39
@_author: ianG 
@_subject: [Cryptography] What do we mean by ... ??? 
Hi John,
My original complaint is one about the term:  "best practices" is by definition of how the process arises not best, and is in fact close to lowest common denominator.
That is a good suggestion!  And the B matches.  Baseline is a far closer (I'm not so sure about Prudent.  I'm quite happy with Current as it does rather point out it's what we do now, if only because now is not then.)
Yup indeed.  Perhaps we should rename the entire field to Cryptographic

@_date: 2015-02-18 11:58:48
@_author: ianG 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
Snowden had a pretty broad view as to what was going on.  From all the stuff they are doing, how they are acting, you can draw inferences back to what worries them, what doesn't worry them and so forth.
This is what intel is all about:  collect all that you know and stare at it until you see the patterns.  The fact that is very strong here is that the NSA is spending huge amounts on cracking into machines.  Would they do that if they could crack the crypto?  Likely, less.  The NSA is no longer a *passive signals* agency, it is now more an *active digital command and control* agency.
Right, we don't know which parts they can break and which not.  But we do know where they spend their energies.  Which indicates whatever they can do to break any of the crypto, it isn't enough.
They care some - they are using RC6 in places instead of RC4.  What this indicates to me is that they have respect for the outside crypto.  Which as a minimum means that they don't think the Russians or Chinese can break it.
What however is strange is that they used entirely American product. That's a finger pointing right back at the NSA.  Why not mix it in with some GOST?  Blake?  Ripem-D?  What's the thing they have going with Ron?   Is he a poster-boy for the root kit builders?
No, I see this a concerted campaign that was begin by the FBI and then spread to two other countries that I saw -- UK and DE just recently, which means it is being spread through the intel agencies.  This is a response to the American companies being embarrassed and showing a little spine - they need their backdoors in there.  So either they encourage bad software engineering or they get the companies on board to backdoor them.  Or?
I agree the NSA doesn't share this stuff with the FBI.  But they do use the FBI as the proxy for this in political lobbying.  This is the same thing that was happening with Loiuse Freeh in the 1990s.  The FBI doesn't care about crypto, they are on record as saying they came across one serious case where crypto *might* have impacted the result but did not.  In how many decades?
(Naive of me -- I actually grew up thinking it was illegal for government agencies in western democracies and *especially the military and intel* to get involved in politics.)
Right.  One new fact can shift the pattern dramatically.  One deception plan by the enemy can lead us down the garden path for years.  Different analysts can draw opposing conclusions from the same 'facts' and flip This is how intel is.
Oh, sorry, I didn't finish that, got confused and sent the post without The insider theft has always been a huge difficulty.  But the NSA is more a victim of changing circumstances than any huge laxness.  A scratch list:
  * They haven't had a major spy case in years.
  * Statistically they have to have a number of insider cases and afaics it is about 10 over the last decade or so, drawing widely to other agencies.
  * The 911 switch caused massive opening up across all intel agencies.
  * Which meant that if you trust the FBI with stuff, you are going to worry less your sysadms who are security cleared very heavily.
  * Also the 911 switch caused a massive switch away from the normal NSA doctrine of ignoring americania to total information awareness.  This latter being the thing that offended Snowden.
  * the Bush / Obama administration debased the patriotism coin to worthless, something that is not easy to see in the center.
  * A massive upgrade in cyber warrior caused attention and resources to switch away from existing long term and non-cyber-warrior insiders to the incoming flood of people.
If they hadn't been 'lax' they couldn't have coped with the above. Snowden was bullet they had to take to get where they are going.  They can't go very open, very broad and also illegal at the same time without offending some statistical number of insiders who don't buy the patriotism rhetoric.
Right, I mean, count the entire US military-industrial machine, so DoD and contractors as well.  I recall a couple of years ago talking to a major contractor in AU and the indication was lots of slots in new cyber division, and any other slots opening were due to people being transferred into this new growing carbuncle.
Yep.  I can imagine the Seals being told "the reason you are going in today is because someone somewhere past stole some digital data...  So time to pay it forward, guys: *bring back the data*.  More important than the body."

@_date: 2015-02-19 16:18:35
@_author: ianG 
@_subject: [Cryptography] Passwords: Perfect, except for being Flawed 
Odd thing to say ;)  Security means nothing outside the context of a human.
As a meta-comment on passwords:  there is a big shift underway now to start doing dual factor using the person's phone.  It is now clear that everyone has a phone, to some statistical certainty, and we can rely on it.  So every system and his dog has now migrated to using something to couple the phone and the password together.
(In the meantime, while this Phone+password hybrid rolls out, others have gone further.  ApplePay, bitcoin light clients, my stuff, are putting the whole thing on the phone.  So, actually we are exposing the phone to single points of failure/attack modes.  But this direction is still so novel and so far rare that there is no economic case for attack and won't be for a few years...)
Which is to say, micro-re-designs of how passwords work and can be improved might be missing a macro-trend that is going on.

@_date: 2015-02-20 12:33:24
@_author: ianG 
@_subject: [Cryptography] phishing attack again - $300m in losses? 
No of course not -- I gave up years ago trying to support proposals to browser vendors who stuck their fingers in their ears and sung lalalala until people went away.
The only role left for the open security world is to look at the browser vendors and say your security is broken, until they give in.
Nah.  This whole "it can't be done because I can't see it" routine is a waste of time.  Same with the whole "you aren't providing a solution, you're just whinging" copout.
If the browser vendors and the mailer vendors and the others are serious about protecting users, they will sit down and start working on the problem.
That can be done -- but the first step is to acknowledge that *there is a problem*.
Get over the denial.
This can be done, and google for one is doing it.  Mostly internally, they are not doing it so much in the open as to admit that there is a problem at the systemic level.  I suppose we have to blame American lawyers for that, they won't let security people tell the truth, so there is a culture of lying about security in American corps.
Nor, adusting the terms of the attack minutely so it's not addressed by the claimaint, dismissed, next please.
Let's start with something simple.  Do we agree that there is a problem?
Wellll.... of course Gates' quote was a tad out of date.  He made it in Where did the 'install malware' thing come from? the industrial hacking machine.  Where did the industrial hacking machine come from?  The industrial phishing machine.  Where did the industrial phishing machine come from?
The head-in-sand act of the browser vendors.  In 2005.
A sense of history helps :)

@_date: 2015-02-20 12:37:56
@_author: ianG 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
2 heads (up) it seems :)
Yes, precisely my point.  The organisation is so large that this has to be a statistical thing.  And as they have offended their people's constitution and other sensibilities, the statistics lean against them, not for them.
3 years is not a long time to roll out a change of the size needed to stop a Pvt. Manning episode, in an org the size of the NSA.
Dragging this back to security and the use of cryptography to protect the assets of the corporation and its clients.  To channel a recent perceptive comment
    "our people are security cleared, it's your software we don't trust"
it is the case that people are a risk as much as software is a risk, and a balanced approach is needed.  Hot crypto is nothing without people-aligned architecture and behaviour.
The NSA is under attack from within.  It will likely respond.
USA corporations are under attack from without.
Yet, they are less likely to respond.  Have any?  I'm happy to help people work through that, but the first thing to understand is:  you are under attack [0].
Postgrads with physics PhDs know how to build bombs, but it hasn't caused a problem as yet.
What's far more troubling is the friends they keep and the friends they share those secrets with.  And the enemies of the friends and the friends of the enemies and the enemies of the enemies of the friends of the friends...
"No foreign entanglements" makes sense, and it isn't the morals.
[0]

@_date: 2015-02-20 12:44:59
@_author: ianG 
@_subject: [Cryptography] Do capabilities work? Do ACLs work? 
Just on that point, at a recent crypto conference I went to, about half of the posters put up by the grad students were about how to encrypt data in the many and various forms that it was stored remotely (databases, but including cloud).

@_date: 2015-02-21 11:38:45
@_author: ianG 
@_subject: [Cryptography] Passwords: Perfect, except for being Flawed 
Very true.  But the notion that law enforcement or advertising was waiting for this little help-up in their efforts is for the birds.  As far as I know, they've been all over this for the longest time.

@_date: 2015-02-27 22:57:07
@_author: ianG 
@_subject: [Cryptography] DIME // Pending Questions // Seeking Your Input 
Nice write up of questions.  For my money, responses below.  Note that I haven't had time to read the draft.
For my money, binary.  Line-based or JSON are good if you are working with lots of random implementations of low quality, but for security work, it is easier to work in binary.  Be precise about sodding everything.
case insensitive.
I'd be comfortable accepting UTF-8 etc if there were a canonical identifier defined (eg hash) that made visual substitutions implausible.
I think the days of assuming ASCII are over.  Half the world out there wants something impenetrable to us.
Pass?  (Or, steal one of them, wrap it up in a packet with a leading version=1 number.  Is what I'd do.)
I used SHA-512 last time I had this question.  In the future I'd probably use Keccak, as sponge is good.
Don't do it, man!
The notion that an "alternate" is needed is some magical thinking from the golden age of irresponsibility.  If you need an alternate, it means (a) you can predict where the break is going to happen, and (b) didn't fix it!?
Also, if you have an alternate, you must have a rollout plan, and nobody has that, especially at RFC level.  The experience we've got when anything breaks in these protocols is that we're in a mess, and all these spare protocols only helped once, and even then only by walking backwards to a deprecated alg.
My preferred way to handle the need for protocol break it is to define ++version.  As you roll out the current version, start working on the new one.  Have it in advanced readiness and tune it over time.
An alternate everything.  If you can predict what is not changing in the alternate, then you're predicting what is going to break.  If you could do that, you could also fix it.  The only logical conclusion is that everything should change.  Our knowledge has a time component, and we're not yet at the point where we can predict the future.
(Yes, that is good, it is the current fave indie AE suite, and there is a good choice of implementations to crib by now.  You might also want to look at the new Keccak modes coming from NIST this summer, rumour has it that it will include an AE mode.)
Providing a SHA-512 fingerprint is a solution that assumes people talk to each other out of band.  Super.  Using key signing to verify a supplied cert assumes there is a hierarchy and someone at the top tells you what to do.  Can also work.  The answer is more about the business than the crypto.  There is a preference for the former because it is simpler in tech, and a preference for the latter because it is complex and requires lots of jobs.
This is the weirdest question.  A security system should be in total control of its data, so it should never need to even ask whether the CPU is big/little endian.  It should read in the data, byte by byte, and construct the number needed at high level using logic.
(A more practical answer might be to just use network order.  I have no idea which that is...)
This is what I do:  A number is a number of 7-bit encoded bytes. If the high bit is zero, you're on the last one.  If the high bit is set, there is another byte following.
This is simple.  It works.  There is no need for any other number type.
(OK, that's not quite true.  If there is a need for a negative, a float, or a bignum, you need something a bit special.  But that's what objects are for...  In practice, 99% of numbers are low positive numbers.)
Yes (I guess) and yes (absolutely).  The notion that you're leaking metadata is right at the heart of security.

@_date: 2015-01-01 16:23:56
@_author: ianG 
@_subject: [Cryptography] hash/sign material for distro of IoT params 
One of the things that is going on in the bitcoin 2.0 world is that some of the groups are wrestling with how to distribute different instruments.  As we know, Natoshi Sakamoto hit upon an elegant simplification by eliminating the semantics of his issue, by the trick of only having one.  This simplification breaks down as soon as you want more than one issue, more than one chain, more than one semantics.  As soon as you want choice in anything, more or less.
One way to distribute information about something like an issuance of value is what I call the Ricardian Contract [0].  This is a contractual document that has a few smart fields slipped in, and carries its own PKI.  When cleartext signed to fix it, it can then be canonically hashed to form the identifier for the unit of issuance.
Now, if you look at things like blockchains, there is an emerging pattern that many people want to run their own, and different ones.  But the basic pattern is the same, in that the description of any given blockchain remains largely in the same format, with some different parameters [1].  If one imagines a commercial service running a chain for some particular purpose -- call it coffeechain for low value fast retail -- then we could also include some static contractual information.  Something like an open combination of params and legal text could be useful to describe coffeechain.
The same pattern might be observed with IoT devices that can be accessed from anywhere.  We need to access the public key of the device, we need examine the params, and we need to be able to examine the service Why is this interesting?  Because if we are in a world of millions of these things, we also need some strong identifiers, and ways to go from the identifier to the description without fail, and ways to go from the description to the identifier without fail.
We also need a world in which anyone can play.  We don't want a world where in order to run a chain or put up a new device, we have to get the permission of someone else, or get enslaved to some facade security model which incumbents lock up and stop from migrating in OODA time.
Using an open document and taking the message digest of it for service as the identifier for that document/device achieves some of those goals, at least on paper.
Indeed.  And my knowledge of IoT devices is rather popularist.  So, does a device have these characteristics?  Does it have some params that need exploring?  Does it have a service agreement?  A need to publish keys, control info, etc?
Could we benefit from having an architecture that settles on one identifier across the IoT space, and uses (eg) a DHT to find the document for it?
I don't know.  It seems analogous enough to ask, what would a cryptographically secure data infrastructure be for small cheap devices on the net?
[0] [1] for Bitcoin this is hardcoded into the source, there are 4 different fixed blockchains with minor variations.  From eg, altCoins and sidechains, it is pretty clear that this hardcoding isn't going to last.

@_date: 2015-01-01 17:50:52
@_author: ianG 
@_subject: [Cryptography] on brute forcing 3DES to attack SIMs 
?To brute-force DES keys, we use a set of field-programmable gate arrays (FPGA), which became trendy for Bitcoin mining a couple of years ago and got cheaper after the hype was over,? the researchers wrote. ?The speed of our 8 modules *ZTEX 1.15y board with the price tag of 2,000 Euro is 245.760 Mcrypt/sec. It is enough to obtain the key within 3 days.?
That was their fastest brute-force. If they had a partially known 3DES key, they could break it in 10 days.

@_date: 2015-01-01 23:21:52
@_author: ianG 
@_subject: [Cryptography] on brute forcing 3DES to attack SIMs 
What stopped me from parsing it as total nonsense was the *partially known* 3DES key.  Of course if it is partially known, then ... anything is possible.
What the blog post doesn't say is ... how do they know the part of the 3DES key?  Well, then, it is telco, so they probably guessed it as multiple zeroes ;-)
Yes, the "." is probably a thousands separator, see the table here:

@_date: 2015-01-02 18:39:46
@_author: ianG 
@_subject: [Cryptography] New Encryption Standard of the Russian 
*Interesting* and it would be very interesting to hear what the real cryptographers think of the Russian cryptographer's invention!  Good work!
Germane also to current applied cryptography debates.
Let me set the scene.  Over in IETF they run a WG that is responsible for a little thing called SSL, now in its TLS 1.3 guise.  This protocol is markedly successful in the marketplace.
It has however some agreed drawbacks, one of which is its 350 or so (opinions differ) suites or combos for ciphers.  Some people defend that, others say it is too many but choice is important.
In opposition, there are the crazy radicals who call for the one true cipher suite.  One is good enough for all uses, assuming certain things.
One -- just one -- of the arguments defending the 350 suite smorgasbord approach is that *national governments mandate ciphers* which then have to be used in protocols.  That is, SSL must support GOST else SSL is in effect banned for the Russian public sector.  Approximately, in short.
I call foul.  I do not believe that we as an Internet promote the legislative or standardised suites of any nation.  Or should do.
One argument here is that if NIST/NSA were to mandate some algorithm for any communications, we'd not bow down to them.  Only if it is voluntary would we accept their suggestion, and only because of historical circumstances (a fair and open competition) did the net voluntarily swing to AES.
A second argument here is the futility of supporting N suites where N is the number of government mandates.  Nobody cares what the French say. Nor what the North Koreans say.  Why care what the FSB says?
What do people say?  Should GOST be supported in SSL?  Is there any merit in the "national government mandates" argument?
ps; this argument as to the number of cipher suites bounces in and out of the IETF lists from time to time.  But I'd also like to stress that there are people who know stuff that do not participate there.  Their voice can and should be heard.

@_date: 2015-01-03 16:33:12
@_author: ianG 
@_subject: [Cryptography] keybase.io 
Yes.  SO one essential feature should be the ability to form these relationships without broadcasting the info.  But, due to success of open ledger in blockchain, etc, and past successes from social networks, the idea of publishing everything is very much in vogue right now.
The presumption is that knowing someone's identity allows you to trust them.  This is famously leveraged to promote a little thing called SSL and certificates, which "knowing" leads to trust in online commerce leads to wealth & riches for some.
The world at Bitcoin large is also trying to figure out this little thing called online commerce.  In general, the presumption "knowing is trust" is also being bandied about there.
There are two flaws here.  One is "knowing what?"  As you discover above, for the "what" to be intelligent there needs to be a unified system.  This is a challenge for mainstream Bitcoin as it isn't actually set up for the unification;  hence all sorts of add-on startyps that allow one piece of the puzzle to be collected in the hope that others will accept that one piece belongs there.
There are more and better ideas at the Bitcoin 2.0 players:  Ethereum et al have included these ideas inside their scope, so the hope there is that the tool can be seamlessly used for (eg) online commerce and other apps yet to be conceived.
The second great flaw is "what happens when something goes wrong?" Which never got answered in the past.  If you're interested, I can wax on for days on how to that, show you the blueprints, sell you my t-shirt, etc.   But, for most people at most times and for most Bitcoin startups and most online commerce people and most participants in the old "trust business", it suffices to say "it'll never happen to me, you're a loony, pass on please."
I'm sorry.  Would a nerf ball suffice?

@_date: 2015-01-03 16:41:02
@_author: ianG 
@_subject: [Cryptography] New Encryption Standard of the Russian 
I'm assuming you mean the last part:  How can their voice be heard?
Well, about 6m back I took a straw poll on the question of one or many algorithms and posted it somewhere.  What it showed (I claim) is that there is about a 50:50 split between the maximalists and the minimalists.  Which contrasted to an alleged "consensus" in IETF that there should be many algorithms.
Another way is for people to reach out.  Easy to poo-poo and dismiss because IETF WGs have open lists.  But there is a fair percentage of people who decline to participate for other reasons.  An open list might not be enough.  Perhaps a survey to ask them?

@_date: 2015-01-08 22:48:59
@_author: ianG 
@_subject: [Cryptography] lessons learned -- or not learned -- from Enigma 
I think history is fairly clear on that point:  It definitely wasn't designed, in any sense that we understand the word.
Netscape assumed one CA -- themselves.  RSA or their proxies argued this was terrible state of affairs, and rushed forth to create Verisign.
If Netscape was convinced to not be "the root" then of course we can't have just one company being the CA.  Which means it became an open market.  But, that all happened without a thought or plan or intent.
And, from what I've seen of PKI literature, there was no thought at the time of an open market of CAs.  PKI was envisaged as a pure single hierarchy (and had to be because original PKI made deep and meaningful assumptions about contracts that would be rather upset by peer CAs).
Having said all that, I did not see these developments first hand, and would love to be corrected!

@_date: 2015-01-09 21:56:31
@_author: ianG 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
A bit of both.  History of WWII suggests that Hitler overrode and controlled his war effort with more than the normal gusto.  Sometimes he got it right, often he got it wrong.
An example of the former to spectacular effect was the Battle of the Bulge.  By that point, Hitler distrusted the codes and decided to distribute the orders by motorcycle riders.  Result was complete surprise, although there was low level intel suggesting something was up, the Allied generals had seemingly gotten used to a diet of clear and accurate suggestions from on high.
Oh, absolutely.  Think of it this way.  In every routine battle, one side will lose, and the search for reasons for failure will be on, for that side at least.  The other side will have as many reasons, but will be able to sweep them under the carpet due to their "brilliant" victory.
1883, Kherkhoffs' 6th principle:
     "Finally, it is necessary, given the circumstances
     that command its application, that the system be
     easy to use, requiring neither mental strain nor
     the knowledge of a long series of rules to observe."
He was writing about ordinary soldiers, from his experience with the French Army.
As a segue to today, in an EFF talk at RWC2015, they were talking about the EFF's CUP or Crypto Usability Prize.  Apparently the cutoff point is 3 minutes.  If you can't explain how to get in and up and running with the privacy tool, you're out.  I once measured raw users getting up and going over chat with Skype, it took 3 minutes from start to talking.
Usability is the  factor in security.
Well, it isn't as simple as one class v. another.  There are bad warrant officers as well as bad commissioned officers.
Legend has it [0] that fighter ace Adolf Galland told Goering that he could win the Battle of Britain with a squadron of Spitfires.  Goering was a flake, but Canaris was a wiley fox, and also more on-point for this particular battle, and would have more likely listened to a noncom.   Having said that, it was early days, nobody had any clue what the other side was up to, and belief in own side was paramount.
I was told yesterday that during WWII the Germans had some success parking submarines over undersea cables from UK to US, and using acoustics to pick up traffic!?  Anyone got any references to that?
[0] by which I mean, it's disputed if that actually happened.

@_date: 2015-01-07 08:06:50
@_author: ianG 
@_subject: [Cryptography] open hardware as a defence against state-level 
As much discussed here, it may be that open designs to defend against state level attacks are moving down the deployment stack into the hardware level:
  RT: The NSA has been trying to go after Tor users for a while now, but seemingly with little success. Do you think enough has been done to ensure this project will be NSA surveillance proof?
TL: The Tor network stands secure to the best of our knowledge, but we have taken the opportunity to armor the protocol further since this new application obviously will become a high priority target for intelligence agencies. In addition to the encryption Tor uses peers on the Slur network will have another layer of encryption based on a different line of mathematics. We?ve also built an open-source processor with security features designed to protect both the Tor relay and slow market applications. This is achieved by separating those processes from the host operating system with hardware-anchored cryptographic isolation. The system on chip is based on an OpenSPARC T1 by Sun Microsystems with substantial enhancements to the hypervisor and two cryptographic co-processors. That will be released in about a month and the designs for the development board and the logic of the system on chip will be of course open source.

@_date: 2015-01-12 11:46:15
@_author: ianG 
@_subject: [Cryptography] open hardware as a defence against state-level 
Seems like you are letting the perfect be the enemy of the good. Defence in depth.  Defence against fierce & persistent attacks is not about defeating the enemy totally & utterly but about raising the cost of the easy attacks to just above the cost of the next easy attack. Rinse & repeat.
ps; old story about 2 guys running away from a tiger.  The one who stops to put on running shoes was the one who told the tale.

@_date: 2015-01-13 21:48:31
@_author: ianG 
@_subject: [Cryptography] simple codebook for passwords 
Following is a simple codebook design for ordinary users.  Discuss ;)
  The simple plastic card that goes in your wallet for easy to remember very strong passwords picture of qwertycard
Step 1: Type in the code letters shown on the 'spacebar' of the card Password sh(/J3Hq
Step 2: Choose a single secret word for all your passwords
Example: ENIGMA. Type in the code characters for each letter of ENIGMA
Password sh(/J3HqAfQsu.
Step 3: Type in the code characters for each letter of the website you are using
Example:  use the code characters for each letter of AMAZON
Password sh(/J3HqAfQsu..u.rqf

@_date: 2015-01-16 16:03:27
@_author: ianG 
@_subject: [Cryptography] Summary: compression before encryption 
It occurs to me that what we need, and I may be talked out my posterior here, is keyed compression.  If the problem is distinctive header cribs, then cause those headers / first few hundred bytes to be encrypted in some form of light scale encryption, just enough to break any realistic sense of cribs.
You might be saying something similar.  I like the idea also of integrating this with a HMAC, or at least analogising it.  Question of whether to expand AE so it is now AE&C for Compression as well I'll dangle for others.

@_date: 2015-01-20 20:30:51
@_author: ianG 
@_subject: [Cryptography] actual NSA protocol docs to mine... 
This appears to be a document that describes an NSA protocol:
Which might give some valuable clues as to what we should be doing! Only skimmed so far, but h/t to Adam Shostack who tweeted that we should look at the amount of random data stuffed into the packets.

@_date: 2015-01-26 12:47:28
@_author: ianG 
@_subject: [Cryptography] Android's Secure ADB as a security hole 
Only if that requirement is maintained.  The thing about any successful system is that it gets extended to be used in other places.
It's a little bit of a dilemma.  Do you go now with the cheap & chearful or do you plan for success, and double your costs upfront?

@_date: 2015-01-26 13:07:39
@_author: ianG 
@_subject: [Cryptography] The Crypto Pi 
It's interesting isn't it.  This whole entropy accounting idea seems to be an implausible base for computing, and as a practical matter, in our field of endeavour, isn't required.  The notion of cryptographic security is sufficient.
The notion that removing a bit also removes a bit of entropy appears to be a "bound" argument.  You can't "remove a bit of entropy."  It's clearly not entropy once you touch it, and because output is protected by hashes, etc, there is also no clear mathematical relationship between the entropy pool and the output of the RNG.
If a bit of entropy is touched or relied upon, then adding in another bit certainly overwhelms any reduction that might be imagined.  But because entropy is expensive this one-bit-equality leads to unreliable design (blocking), so it's a missed opportunity.
Meanwhile, from a security & attack perspective, there is a slightly more compelling argument that an attacker can copy the state at time N and roll it forward.  The "replace entropy after use" argument seems to mitigate that attack nicely.
All of this has led me to change the design around.  In my current RNG I don't keep a central pool at all, just a mixer that demands fresh entropy as required from all collectors.  The major rule then is that the collectors aren't allowed to block at all, and it this is a problem for them, they have to keep a pool, or return nix;  their choice.
ps; talking about collecting entropy is fraught by definition.  We need a better language, but I guess we're stuck with it for now.  I like John Denker's version which is 'surprisal'.

@_date: 2015-01-26 13:12:29
@_author: ianG 
@_subject: [Cryptography] The Crypto Pi 
Once upon a time, it might have meant that.  Now it doesn't.
What it "means" is more to do with whatever is implemented by the primary unix system you are using, and as it happens, Linux and BSD do differently.  So meaning is out the window, we're back to implementation.
My advice:  applications should use /dev/urandom.
ps; more long-winded stuff here

@_date: 2015-01-26 13:56:52
@_author: ianG 
@_subject: [Cryptography] please help give advice on starting a project 
First thing to understand is that 'trust' is a very difficult and essentially human word.  It does not translate to tech terms, and typically the word has been so destroyed in tech world that you'll never be able to have a sensible conversation about it.
So, try to replace 'trust' with something else.  Perhaps reliability of claims?  Security of investments?  Social standing?  Or, whenever you use the word trust, say this:  "Alice trusts Bob for WHAT?"
Yes, western financial world credit scoring.  It's not necessarily applicable everywhere, though.
OK, so the claim you are trying to make over Bob is something like "Alice thinks Bob is worth X in credit" ?  Maybe it is more precise like "Alice agrees to extent Bob X in credit."
This is the crux of the bitcoin world's biggest problem.  It's the next frontier, IMHO, although few agree.  What follows is all my personal opinion, and do not assume I'm unbiased, as I've been involved and am involved in some things I mention below.
There is a long history in identifying users with cryptographic keys. OpenPGP and x.509 certificates are used in this fashion.  However these systems bungle the semantics (meaning) needed for actual use.  SPKI was also a system that went in this direction.  Although I know little about it, it appears to have at least explored the semantics issue more than the others.
What we can conclude from the above history is that an identity framework built from a key-centric vision is not adequate.
Then, in an alternate approach, CAcert the open CA, built an identity system based technologically on a LAMPS stack that records the KYC ("know your customer") data, as typed in by a worldwide network of assurers.  Going beyond the traditional tech limits of the concept of identity, they added an arbitration layer which ensures robustness in claims:  assurers can be arbitrated against if they say the wrong thing.   They also added ways to communicate reliable claims albeit in simple non-tech terms.
What is probably clear from the CAcert experiment is that the tech stack they used wasn't adequate to reach human use cases either, but the semantic layer was much stronger and can IMHO support the use cases that people might come up with.
For a first time implementation?  For that I have no easy answer, if I did I'd build it :)  What I can say is that my current system has elements of all the things mentioned above.  It melds CAcert to keys and heads in the direction of SPKI semantics, and some new stuff.  It is quite complicated, big.
If you are a student looking to make a system in this area, I'd suggest you slice out only a part of the system.  The interesting space to me (?) would be semantics of claims.  If you can make a system that composes multiple claims together and the result is still parsable and interpretable to humans, that would be pretty good.  E.g., above Alice extends a line of credit to Bob.  What would your system do if Bob's other friends also do the same:  Carol, Dave, Eve.  If Bob can rely on 4 lines of credit, can a statement be created that aggregates that into a single composed claim?
(By that I mean, leave out the crypto.  Making yet another key-centric id system might not work for you.)
This is part of the question of what I call semantics:  what is the claim that is made?  The problem consists loosely of many parts. Firstly you have to collect a claim.  Secondly, communicate it. Thirdly, parse it! which means by program and by human.  Next, compose it which means adding multiple claims together to make new claims.
Finally you have to test the claim;  what happens when the claim turns out to be unreliable?  This is the step that everyone typically bungles and therefore their system is unfounded, just a fantasy.
Only once you have all these elements in place do you start to see the concept of what we humans call trust emerge.
Right, what are the claims that matter?
Irrelevant, don't assume a technical solution until you are much deeper into your design.
That's solved by your identity solution.  It isn't much of a solution if it cannot figure out sybils and forgeries :)
That also may be something you want to leave until later.  Measure the result against that requirement?
As I say, all the above is in my opinion.  I work directly in that area, for what it is worth.

@_date: 2015-01-28 15:10:18
@_author: ianG 
@_subject: [Cryptography] traffic analysis 
There was once a company (ZKS) that offered a client download that did a sort of late 1990s mix of Tor,Chat,Nyms,mail privacy stuff.  In order to do it properly they wanted you to have a pipe would would consume X bandwidth always, so traffic analysis was hard.  I recall it had a setting on it for 64k, 128k, 256k.
Unfortunately, the users thought that actually, they'd rather preserve that bandwidth for ... other things :)  Inter alia, users don't really pay for privacy, so the company didn't get very far selling their product.  I hear they did manage to get into the firewall business tho.
Doing traffic obfuscation by filling out is an expensive solution when the rest of the world spends most of its time trying to optimise.

@_date: 2015-01-29 16:22:14
@_author: ianG 
@_subject: [Cryptography] traffic analysis 
Outer HMAC is what I assumed.  So, yes, at least a shared key for the HMAC is required (nod to DJ).
A good protocol can drop incoming packets with bit errors anyway. Another good principle is that ones traffic is indistinguishable from random.  You can send a random packet, and any attempt at the HMAC will fail.  Or you can send random data within the inner encryption envelope that says "drop".  As there is no return packet necessary, there is no particular timing issue.
In my thinking, what we want is a slow channel and a fast channel.  The slow channel moves stuff occasionally.  The fast channel moves stuff quickly.  When the fast stuff starts moving, the slow channel backs off.   When the time is quiet, the slow channel puts out some splutter.  One can think of the fast channel as the business channel and the slow channel as the movies recommendations.
(This of course assumes an application.  And it's not perfect, much fast traffic will cause spikes.)
That I don't follow.

@_date: 2015-01-29 22:58:09
@_author: ianG 
@_subject: [Cryptography] How the CIA Made Google 
There's no smoking gun.  The business about pre-google funding was ho-hum.  All funding is tied, I'm even happy that this funding looks like it did some good, whereas most funding is so damned skewed and corrupted, it does harm.  Yes, GF I'm talking to you.
What is significant in the article is enough linkage to see the revolving doors -- jobs -- going on between these organisations.  It wasn't clear to me just how many people were in common across these orgs, and that only happens when they're all part of the same tribe; same agenda, same team, same interests.
What it paints is a picture that google and others like it (recall that list of 8 or 9 big names in the original Snowden slide, many of them mentioned) are as much a part of the team as not.  What to make of that .. depends.
I'd say that point is starting to shift.  Pre-Snowden, people believed that google defended their users' privacy, and google was given significant respect in just stating it.  Post-Snowden, I suspect they are going to have to prove every point they want to claim.
They aren't handing data to NSA?  Show us, how?  The executive doesn't take their agenda from 'Highlands Forum' ?  Show us, that you don't. The pipes between data centers are encrypted by keys that aren't being leaked -- where's the evidence?  Your CSO doesn't have a phone in his shoe?  Let's see!
This question is bigger than this group.  Nobody pays a blind bit of attention to this group, so it matters not what is said here.  But, elsewhere, the hallowed name of google and its "don't be evil" mantra has somewhat morphed over time, bit by bit.  The ordinary people are starting to be negative.  Now it's more like "trust but verify" perhaps.
What's that worth?  IDK.  Cash cow economics are a fearful thing, ask any new-normal-insolvent-bank.  Ask Skype, who still are an unparalleled milking machine, even after the clear and undeniable evidence that they breached their commitment to users privacy by shipping it all across to the NSA.
If one is a shareholder of google, this is a remote possibility, not scary at all.
On the other hand, if USA companies depend on foreign sales, this could get a bit nasty.  Has anyone been tracking the blockback from international sales post-Snowden?  Hopefully with more depth than "industry associations" lobbyists or journos...

@_date: 2015-01-30 09:10:34
@_author: ianG 
@_subject: [Cryptography] How the CIA Made Google 
It has to do with thinking outside the data/tech box.
Which granted, these companies will have trouble with, because the employment profile is 'entanglement with tech'.  NSA likes that, it's an easy target.

@_date: 2015-01-30 09:34:25
@_author: ianG 
@_subject: [Cryptography] traffic analysis 
Oh I see.  A second reading made it clear.  You're talking about the base rate fallacy.
Yes, if there is any uncertainty, but I'm assuming that there is no uncertainty.  I'm thinking these assumptions:
1.  the packets are indistinguishable from random.
2.  receiver takes the last N bytes and assumes it is a HMAC.  If it calculates, it is good, if not, throw.
3.  inside the encryption packet there is a further authenticator e.g. public-key sig at the application level.
Due to DOS and performance reasons, you can dial the HMAC down a fair bit, the public-key sig will have your back.  SHA1 which I've used in the past is way overkill, I'd reckon about 8 bytes would do it.
The hard part is 1. indistinguishable from random.  Because you're doing this over IP packets there are IP# and port# considerations (how many clients, routers, etc).
Also, for testing and usability you actually want to know what is going wrong, and this notion deliberately hides that.  Note also Jerry's rather interesting comment that we need include anti-DOS as a requirement, that is pretty challenging.  We also want to include anti-DOS detection at least where mucky intermediates just cause the packets to drop because they can't do deep-packet inspection on them.
All of these things have tended to be broad giveaways as to the type of traffic.  So you can only go so far before you're dancing on a pin.

@_date: 2015-01-31 12:27:43
@_author: ianG 
@_subject: [Cryptography] best practices considered bad term 
That term!
The term 'best practices' is deceptive.  Best practices emerges in a sector for some topic that nobody cares enough about to compete on, but all are agreed that something is needed.
The process of creating a 'best practices' is mostly a lowest common denominator one -- what can the majority agree on and therefore force consensus on the minority?
Once 'best practices' is in place, it takes a whole lot to change the stability.  A new practice has to be demonstrably better to all, in ROI terms, otherwise those who don't see the benefit will be incentivised to spend up to the cost of the new practice to avoid it being added. That's a whole lotta pushback against change.
Consequently, this cost makes 'best practices' an unchanging, always behind--the-curve beast.  It therefore only works in the absence of an aggressive attacker, or one so benign it can be built into loss rates. It's a sucker for compliance / liability dumping.  It's certainly not 'security' in the sense of securing the interests of the customer.
It's a term best seen as lazy practices, or "the least we can get away without being outed as negligent by a sleeping public."
For this reason, where people want good practices, some of us have been pushing the term 'better' as in the BetterCrypto.org project in Vienna.
As a wider philosophical question, is it even appropriate to promote or accept 'best practices' in the security world?  It's presence is almost a complete proof that we're not doing security, we're instead participating in a rain dance or voodoo for purposes of avoiding security.

@_date: 2015-01-31 12:39:12
@_author: ianG 
@_subject: [Cryptography] De-Anonymizing 
Right -- pay with a tracking mechanism, get tracked.
I generally pay with cash at the local supermarket.  But I have one of those store tracking things that a mate gave to me.  So his purchasing pattern is intermingled with mine.  Which is probably not a lot of confusion, can be stripped out if it is known.
Perhaps we could do a mix?  If we each meet once a month, and we throw our supermarket tracking things into a basket, mix it up, then everyone takes a random one.  Next month, remix.
The tracking would be pretty hard.  And, as the reward is on the volume, we'd all share in the combined result.  A bit of a disadvantage if I'm the biggest payer, but it's a livable compromise.
Tracking mixes might be rendered useless if we end up with no cash systems though, as, the payment card could be used to inform the real purchasing buyer.

@_date: 2015-07-26 17:15:51
@_author: ianG 
@_subject: [Cryptography] What thou crypto hath wrought - /Ulterior States/ 
If anyone fancies a lazy Sunday hour, there is a new documentary out which explores the political & economic implications of what Bitcoin has achieved, or will achieve.
Although normally politics and economics of Bitcoin would only be marginally on-topic for this list, Satoshi aired the invention here back in late 2008, so we possibly retain a special affinity.

@_date: 2015-08-01 02:37:22
@_author: ianG 
@_subject: [Cryptography] 
=?utf-8?q?Blockchain?=
Nice summary!
My view:
tl;dr - they're not.  They are just mouthing the word 'blockchain' so they can experiment in peace.  They'll tell us what they are really doing when they figure it out.

@_date: 2015-06-02 11:54:26
@_author: ianG 
@_subject: [Cryptography] open questions in secure protocol design? 
Nah.  You can't make the point like that.  You have to make it from the context of 2007-2008 timeframe.
   What was the environment like then?  nobody cared.
   Did Curve25519 even exist?  Yes.  But safecurves.cr.yp.to didn't.
   Where the problems then known?  No, not widespreadly.
   Did a reasonably good engineer have access to this info?  No - he talked here, and nobody blinked.
   Did he pick a ciphersuite that lasted a good while?  Yes - in year 7.
Picking a bad curve is a given - we only have to wait long enough for that to come true.  The engineering comes in being prepared for that day.  Until recently, very few have thought about this problem.
Apparently they've got something.  But whether it is good enough to move I don't know.
What they might want to do is to develop a future algorithm, so that they are ready for the event when it happens.  Trying to push people to change beforehand without clear evidence (losses) will be hard, here cue in the recent debate about the block size.
I just checked, three I asked haven't switched and don't have much intention to do so.
So, think about the protocol for a bit.  Let's say we put into Bitcoin a new feature such that the chief scientist can launch a packet into the blockchain that causes the key algorithm to change to say Curve25519. (Wave away the mining consensus problem for now.)
Is that going to fix it?  No.  These keys aren't ephemeral like AES, where as you noted there has been progress.  They aren't even short-lifetime like certs that expire every 2 years.  These are keys holding *value* and some of these keys remain dormant for years.  Indeed there is a sub-sector called cold storage built around this opportunity.
So, we can probably rule out 1TCS as a complete answer because just shipping a better version in a new protocol doesn't say anything about the older keys -- which is the real problem they have.  Same with classical view on 'algorithmic agility' which handwaves about the existing product out there.  Legacy keys is such a dominating issue, that the engineering probably wants to focus on that, and when that is solved, see what else is left.
Absent a key expiry concept or a key pensioning system or a revocation method ... this is a hard problem.
I doubt.  Most all the world has no idea what crypto Bitcoin uses, and the altCoins have barely scratched at Bitcoin, regardless of what difference they pretend.  Why would a "stronger key" make a difference, absent any actual key attacks?
Empirical evidence on the ground suggests they want an easier-to-implement key not a stronger one.  Including a better RNG ;-)

@_date: 2015-06-04 14:16:58
@_author: ianG 
@_subject: [Cryptography] Forget the Enigma Code; 
It's hard to have a debate of any value based on country statistics.
But yes, I agree that my claim was not only sarcastic ("out of reach" is the best?) and also literary licence :)
Dragging this back to crypto.  Let's say that you are liable for your posted copy of e-rights.  Or Satoshi was liable for his earlier bitcoin code.  Or, DJB is liable for his snuffle.
In the market for open use, wide-purpose, published software, the developer cannot be aware of the future use of the software.  So he is unable to charge a price that reflects liability.
In such a market, the only price and liability that clears is zero. There is a clear stability in the market for open software where zero liability meets zero price.
"You can use this software for free but you suspend the Hammurabi clause."
When however we move away from that zero cross, the conditions become murkier.  If Hammurabi liability is applied for a contracted piece of software and a price is paid, then a meeting of minds was reached.  In which case it is entirely possible that both sides can be forced to accept fitness of purpose, etc.  In which case, the price charged has to rise to cover the liability.
So very quickly, I predict, charged software will price itself out of the market, and only free/open software will remain.  Some would consider this a good thing.  Bring it on?

@_date: 2015-06-09 19:21:10
@_author: ianG 
@_subject: [Cryptography] Bitcoin sidechains to use Schnorr (was: Open 
Last night there was a presentation [0] on bitcoin's sidechains that said, copying from twitter:
      * Also replaced ECDSA with Schnorr
         - Efficient (non-accountable) multisig
         - Batch verification (2x speedup)
See here [1] for more of what that means, but your central point is taken - the crypto needs an update.  As it happens, they are not changing the curve, just the signature.
But, let's talk about the real issues:  engineering and deployment. It's easy to pick another cipher suite, it's difficult to deploy it. This change likely will deploy in sidechains only for the short and medium term, so the mainnet bitcoin will probably have to wait a while.   It may never change.
Indeed hypothetically if the curves/crypto in mainnet never change [2], and weaknesses are discovered, then there might be a force that migrates all value over time to new chains.  This is a complicated thing, and I'll go out on a limb and say, it might actually be a good thing as much as a bad thing.
Each of these new sidechains can use a new selection of crypto. Probably most will use the same selection of crypto, indeed it is going to be pretty rare to change.
Now, given this migration path from bitcoin to sidechains, and given the fundamental premise of the bitcoin concept that it should be hard to change, I'd suggest that they (the devs) are naturally going to prefer one suite and only one suite for any given chain.
Let's say you decided that algorithm agility was good, universally and therefore you wanted to put it in.  If you want to employ agility you have a real hard time designing something that actually added value.  In part because you're not improving the security of old keys, in other part because of the downgrade attack - imagine having the possibility of two signature mechanics coming from one key, not happy at all, because you get the full weakness of both!
And in other part because as soon as you start fiddling with the crypto, you find it's actually not the biggest concern, and full review of all concerns are driving you to build entirely new software sets.  I.e., look at yesterday's talk.
In short, we want sidechains or altCoins, depending on our religion.
In short, 1TCS.  Bitcoin may be a good engineering example of where it dominates as the solution, and algorithm agility does not.
[0] Greg Maxwell, there is what appears to be a practice run here  for those who weren't at the developers meet-up.
[1] [2] Greg mentioned in the talk there is a soft fork going through now to do with 'signature minutea'.

@_date: 2015-06-11 11:16:47
@_author: ianG 
@_subject: [Cryptography] Bitcoin sidechains to use Schnorr 
That's hugely valuable - listening to talks is just too much work for most, and referring back to text is what makes it work.

@_date: 2015-06-12 01:39:15
@_author: ianG 
@_subject: [Cryptography] Logjam (Peter Fairbrother) 
(Peter Fairbrother asked me to forward the following to list, words below not mine)
I don't know why, but I haven't been able to collect emails from the list, or send the list emails, for a while. I don't even know whether I am still subscribed to the list.
If I had, there are a couple of issues I would have raised - I have skimmed the archives and found no mention of either, but the thread titles don't always reveal much about the content, so if I am repeating others' words or thoughts please forgive me.
First, the Logjam attack, and the state-level attacks mentioned in the logjam paper. Logjam, like FREAK, degrades crypto suites, in this case TLS DH to 512-bit "export grade" crypto. It's a bit more sophisticated than FREAK, but not much.
7th principle: Holes for "good guys" are holes for bad guys too.
8th principle: In code, nothing ever really goes away.
(big nod to Jerry Leichter here)
More controversial, and quite possibly more damaging, is this:
"Threats from state-level adversaries.
Millions of HTTPS, SSH, and VPN servers all use the same prime numbers for Diffie-Hellman key exchange. Practitioners believed this was safe as long as new key exchange messages were generated for every connection.
However, the first step in the number field sieve  the most efficient algorithm for breaking a Diffie-Hellman connection  is dependent only on this prime. After this first step, an attacker can quickly break individual connections.
A close reading of published NSA leaks shows that the agency's attacks on VPNs are consistent with having achieved a break [of the single, most common 1024-bit prime]."
I wonder whether the "state level threat" of breaking common 1024-bit DH primes is the "major breakthrough" which NSA told Congress about a few years ago, for which they got all that lovely extra money.
If so, the people who in 2013 were supporting the idea of replacing 2048-bit RSA with ubiquitous 1024-bit DH in order to provide FS look a bit silly ..
[ the major browsers supported 1024-bit DH but 2048-bit RSA, perhaps due to people mistakenly thinking that DH keys needed to be half the size of RSA keys - it might be interesting to see where that rumour came from.
To quote Peter Gutmann, posting here:
"It's a debate between two groups, the security practitioners, "we'd like a PFS solution as soon as we can, and given currently-deployed infrastructure DH-1024 seems to be the best bet", and the theoreticians, "only a theoretically perfect solution is acceptable, even if it takes us forever to get it"." ]
.. as the only people who could partially break 2048-bit RSA were the major agencies (gimme the private keys sunshine, or go to jail), the same ones who could almost universally break 1024-bit DH, but without the hassle of warrants or anyone else knowing about it ..
By the way, this was just after Snowden, when Google and the like were moving to 2048-bit RSA, and other people were running around like headless chickens saying "we must do something".
NSA must have been laughing all the way to the bank.

@_date: 2015-06-12 01:40:28
@_author: ianG 
@_subject: [Cryptography] proposed ITAR changes 
(second of two that Peter wanted forward, words his)
Second issue, proposed US ITAR changes. New regs, for comment, not yet in law or in force.
Actually, it says, for the first time explicitly, that publishing widely on the internet would be enough to put data into the public domain. Sounds good?
However, there is a great big kicker: posting technical data for the first time would be an export, and you wouldn't be allowed to do it without prior authorization [17].
Reposting already-posted technical data is also making it available, and you wouldn't be allowed to do that unless the initial posting was Neither would you be allowed to sell a book or magazine or periodical, even within the US, unless it had been made available with an authorisation [23].
So, in the US people wouldn't be allowed to make technical data available even by publishing technical data in book form without prior authorisation. Phil Zimmerman's trick, publishing the source to PGP in printed form to put it in the public domain, would no longer work.
In fact, you wouldn't be able to send 100-year-old science textbooks overseas unless you knew that they have been authorised. Nor could you sell them in the US if/once you had been reliably informed that they had not been authorised.
Talk about wildly overinclusive laws ..
There is also some trickery about redefining software as an item, rather than as data; one effect of which is to put software which is the result of fundamental research into the control regime.
Of course, as "fundamental research" only means research done in the US by US centers of learning, or US Government funded ..
I get confused, but it would seem to me that eg if there is a crypto conference in the US with published proceedings, the publishers would need export permission for the work of foreign authors, but not the work of most US authors.
[17] To get pernickity: data which has been made publicly available, including by widespread posting, would be exempt.
However, data which hadn't been made available with proper authorisation would not be exempt.
If you saw some posted data or data in a book, and you didn't actually know that it hadn't been released with proper authorisation, you couldn't be prosecuted for reposting it, or selling the books it was in. Though you could be prevented from doing it again, if someone told you its initial release has not been authorised.
 127.1
(a) * * *
(6) To export, reexport, retransfer, or otherwise make available to the public technical data or software if such person has knowledge that the technical data or software was made publicly available without an authorization described in  120.11(b) of this subchapter.

@_date: 2015-03-01 08:16:56
@_author: ianG 
@_subject: [Cryptography] DIME // Pending Questions // Seeking Your Input 
Sure, JSON might beat XML, and JSON in binary would be a good thing.
But, they are both *general* data formats and one thing we know from security is that we don't want general, we want specific.  Close off stuff not open it up.
Right, we need numbers, length-data items and everything can be constructed from that.
If one wants to use JSON in binary as a starting point, sure, do that. But call it MySON for My Security Object Notation :)  The point being that you can save on a bit of software if you want to, but there isn't that much software involved, and being lazy and secure at the same time isn't a happy mix.
Another possible way to go -- if your religion is "JSON saves" -- is to look at ProtocolBuffers or the other forms out there (I forget the names of the others but I guess they are equally good.   The reason for looking at these is that if you are trying to save effort, you are far better off if you can get a leg-up on the parsing of objects by generating classes for multiple languages using the tools.
(I've never actually worked with these things...)

@_date: 2015-03-04 17:40:17
@_author: ianG 
@_subject: [Cryptography] FREAK attack 
Hear hear, singing to the choir and all that.
But battle is still raging with the IETF groups, who should be crypies and techies.
Two reasons have been advanced as to why they think there should be 'choice' in cryptographic protocols.
Firstly, for backup in case of the primary suite's failure.  I find this difficult to deal with because nobody there has a plan or view on how the 'backup' is to be deployed.  And nor has deployment actually worked out well for us, it's pretty much all required "re-install".
Also, the notion that a well-written modern suite would suddenly spring a leak is just not matched in history.  In practice, as today's attack shows, the breach is typically contributed by the presence of multiple suites, not their absence.  Today's breach, entirely due.
Secondly, the notion that is advanced is that countries are sufficiently wise & advanced to demand their own suites, their own secure net as it were.  This is the GOST argument.
I just don't understand the temptation to listen to this.  We've now got revelations that the local big state has slid in bad RNGs, financed bad patches, caught intercepting fedexes with critical hardware, runs a complete shadow interception network, and engages in cyber-destruction of industrial equipment.  And, we want to give states the ability to change our crypto?  Huh?
I think this debate will rumble on, but we will have to face these arguments head-on and battle through them.  Until we win the argument, IETF will continue to create and push out standard protocols with weaknesses built in, and industry will continue to pay for this folly.

@_date: 2015-03-10 19:24:40
@_author: ianG 
@_subject: [Cryptography] FREAK attack 
This is the part that IETF WGs do not have a handle on.
In the spirit of being positive perhaps the Security section of all future RFCs should now include a section on how to take away old ciphers/modes/suites... ?
Don't get me started.... ;)

@_date: 2015-03-10 19:46:34
@_author: ianG 
@_subject: [Cryptography] FREAK attack 
Ha!  Excellent thought experiment.  So, you are setting a 15 year timeframe?  Well, bear in mind that *all* of your other suites would also have to achieve that ;) but here goes:
V. Intr. Term.    Suite.
1. 1994  2004     RSA1, DES, MD5, CBC
2. 1999  2009     DSA1/DH, T-DES, SHA1, CBC, HMAC
3. 2004  2014     RSA2, AES128, SHA256, (???), HMAC
4. 2009  2019     EC???, (???)
5. 2014  2024     EC???, Chacha20, Poly1305
6. 2019  2029     EC???, CAESAR
Announce versions being N and N+1.  Pick the latest one agreed.  Only implement 2 versions in the packet.
ps; the reason for the ??? is absence of sufficient knowledge beyond a 10m post.

@_date: 2015-03-10 19:52:38
@_author: ianG 
@_subject: [Cryptography] FREAK attack 
Nobody's been working on RSA factoring since 2010, when all the uni projects switched to mining Bitcoin ;-)
ps; this is only semi-mirthful.  I have suggested that the PoW algorithm should be something that could be more usefully used by the rest of society, like house-heating, but that suggestion seems to be philosophically blocked by a misreading of economics that says that the material used for uniqueness (paper, hashing) should be of NO use to the rest of society otherwise Gresham's law kicks in.

@_date: 2015-03-12 13:36:51
@_author: ianG 
@_subject: [Cryptography] Securing cryptocurrencies 
Right, so thinking of this as a mathematical puzzle or a cryptography game isn't really better than marginal.  Peter Todd also suggested that we're testing SHA256 but that is only extraordinarily interesting for about 100 cryptographers...  the other 7.999.999.900 of us are a bit ho-hum over the point of that.
So, think outside the box.  Here are my suggestions:
Room-heaters.  Build mining boxes that do 500w, 1kw, 2kw as room heaters and sell them for winter.  Those people who have to run electric heaters anyway will get a buzz out of an occasional lottery win.  Extra points if the heater plays a bingo chime.  Also, this does a nice distribution of hardware because your average family isn't going to be seriously mining these things for profit -- so we improve the distribution, the checking over the big miners.
Digital signing / encryption accelerators.  Instead of SHA256 which is boringly useless and fast, build an RSA 4098 variant.  E.g., change the crypto algorithm to something that is now "dual purpose".  The point here is you run it for 6m as a PoW box and then sell it to a corporation that does lots of RSA.  Better than scrap :)
(Quite what the algorithm of choice would be here, I don't know. Password crunching, of both forms... would require some thought, or 5 mins asking someone at Akamai.)
More here.
It's the problem that everyone loves to attack.  I've spent a lot of time on it too, and my answer is ... change everything.  But I'm still left with the fundamental coordination problem of a split/fork that has to be resolved at efficient cost.  Working on that very occasionally...

@_date: 2015-03-23 19:34:08
@_author: ianG 
@_subject: [Cryptography] "Most Americans Don't Mind Being on Candid 
Personally I think the surveillance genie is out of the bottle and will never go back.  Over the long term, technology trumps law, not the other way around.
However, what I am not keen on - actually what I think is the threat to the very nature of the democratic and free societies we were taught about in school - is that the intelligence agencies then use their broad and well-funded surveillance powers on everyone, *and then hand the data over to the police*.
That way lies the police state.  And for that, there is no excuse, because we were all (*) alive and aware when the Berlin Wall came down.
Spying on everyone is fine - for national security.  But there is a long-standing taboo between mixing national / military affairs with domestic / criminal affairs.
(*) ok, I know that's not technically true, but ...

@_date: 2015-03-23 19:38:10
@_author: ianG 
@_subject: [Cryptography] FFS 
Hell yeah.
A bunch of us have been using the term 'protocol forward security'.

@_date: 2015-03-24 01:12:57
@_author: ianG 
@_subject: [Cryptography] OPENSSL FREAK 
The answer is in parts.
1.  In the 1990s it was believed that cipher agility was a good thing. Everyone had the right to propose their own pet algorithm and get it in there.  (Since then, we've figured out this is a very bad idea...)
1.b  There was a notion that having extra ciphers was good because we could always switch over if the need ever arose...
2.  Nobody created a plan, or a protocol, or a ceremony, or anything that actually told us how 1 billion browser users and 1 million server sysadmins would actually ... switch.  So when the time came, the switch couldn't be used, and wasn't relevant.
2.b  And of course, there was no plan/process/ceremony/desire to retire any algorithms.

@_date: 2015-03-24 22:30:24
@_author: ianG 
@_subject: [Cryptography] "Most Americans Don't Mind Being on Candid 
It's easy to construct a story.  It's also easy to lie.  And it;s inevitable that they go off the rails if there is no control.
So where to draw the line?  Where is the place they must not go?
If that isn't clear then it's all over.  The line must be observable, measurable and hard.

@_date: 2015-03-24 23:00:15
@_author: ianG 
@_subject: [Cryptography] OPENSSL FREAK 
He :) well, what you're saying isn't so different, so, I'll move on.
Right.  Everyone did their own thing.  That's what I mean by the absence of a plan.
Have a look at the protocol.  If a HELLO packet arrives, every implementation knows what to do.  If a USE AES256 arrives, every node knows what to do.
If a SWITCH NOW packet arrives, what would we do?
There is no answer to that.  There isn't even a SWITCH NOW packet...
And, everyone's scratching their heads saying, wait, iang, you looney, that makes no sense at all.
Now go helicopter:  There's a mechanism to PUT IN the algorithms and CHOOSE the algorithms.  But no mechanism to TAKE THEM OUT, nor to SWITCH.
So obviously, uncontrolled growth was the order of the day, and at the micro level, down in the dirt, we get results like this:
The reason the browsers kept the export ciphers is because they haven't got a way to get rid of them *in the legacy servers*.

@_date: 2015-03-26 12:29:48
@_author: ianG 
@_subject: [Cryptography] "Most Americans Don't Mind Being on Candid 
Correct.  And what you might be missing here is that your brain and mine and everyone else's on this list is soon going to be replaced by a set of brains with different perspectives on the whole mess:  *youth*.
Spend some time with kids.  Teenagers.  The way they use technology is radically different to what we expect.  Their expectations are in some cases stunningly different and they literally break our assumptions and therefore *destroy our security models*.
(And that's just the rich/west/white/wasp sector.  Move a cultural barrier or three and it all changes again.  Hell, just cross the Atlantic and the other side has a diametrically orthogonal view as to who the enemy of privacy is.)
Which is not to say this is good.  Nor bad.  It is, and our task here in designing security and cryptography and surveillance and all so forth is made much harder because we're also shooting at a moving target.
I think you're conflating human's visceral reaction to control with the technological advances.  They both exist, they interact heavily, but they are independent in origin, and neither can be stopped.
No, they are offensive to our generation, the one brought up on _1984_.   The newer generation is a bit vague on it all, they've got snapchat, they've been using nyms for 15 years, mobile identity in their pockets, crowding and flashmobs, permanent recording of them and their friends, boarding, and all that.
I don't think it is fair.  Nor an excuse.  I just think it is, it's a done deal.  There is no way you or anyone can unwind what technology made possible.  Or if there is, I'm interested to hear it, but it actually has to be realistic, not foot stomping and harkenings back to a more innocent age.
No, that isn't candid camera, that's in your face camera, that's paparazzi surveillance.  That's aggression, bordering on assault, a completely different issue.
Not so many people mind if they are on candid camera, what they mind about is having the ability to control how their image is utilised That's invasive.  That's crossing a line, that's entering into your own Sure.  People are talking about it.  But what to do?
How do you apply the brakes when you're not in the vehicle?  That's completely missing the reality of it - you have a well-funded 100k personnel and growing rapidly organisation that is breathing life into the starved old physical industrial military complex.  It routinely lies to all the regulators of same, in court.
Do you think it cares one jot what you think?  The 'people' think?
About the only thing you can do -- my thoughts only -- is to ring fence it.  By applying pressure to all the other agencies around the place that are currently, still, beholden to the regulatory structure called And that, before it is too late.
Yes - the only thing we have to fear is fear itself.
The fact that it is a fiction doesn't stop it being one of the most powerful weapons used against own people we've ever seen.
It is the world we live in.  Philosophically, we either abandon those who don't realise the irrational trap they let themselves get caught in, or we defend them.
This same debate has erupted over on OpenPGP.  Who is pgp for?  Is it for the masses, or is it for the experts?
What does 'pretty good privacy' mean, really, and who is it?
Strawman proposal:  you can't do both.  You have to pick.

@_date: 2015-03-26 14:17:46
@_author: ianG 
@_subject: [Cryptography] Drop Zone: P2P E-commerce paper 
What does this mean -- does "testnet" have the capability for a "secure communication channel" ?  I'm unfamiliar with that, that would be a significant variation on testing the mainnet...
So, my attack would be to (a) set up a seller of contraband stuff, which I simply steal from the evidence locker, (b) do all that stuff you mention above, and (c) surround the dead letter boxes with lots of cameras and other sensors.
(d) Sell, profit, then after a year go roll up all my entrapped & identified victims.  Oh, and refill the evidence locker.
(e) Rinse & repeat.
As far as I can see, the attack doesn't even damage my reputation score?!

@_date: 2015-03-28 13:37:35
@_author: ianG 
@_subject: [Cryptography] OPENSSL FREAK 
My point can then be interpreted as, until you find a way to disable the bad thing of insecure ciphers, cipher agility earns the title.

@_date: 2015-03-28 13:51:42
@_author: ianG 
@_subject: [Cryptography] Drop Zone: P2P E-commerce paper 
Digressing... As a general observation in financial cryptography, general purpose trading networks seem to require three elements:
     messaging,
     file store, and
     payments,
each of which have to be secured.  Looking at the evolution of variations such as Ethereum, they have added those other two to the "standard" blockchain approach pioneered by Bitcoin.  Whereas in the latter Bitcoin there is a sense of using some other disjoint feature.
What strikes me is that, for those with long memories, Mojo Nation might have actually had the feature set more or less right.
OK, you're securing the supplier not the buyer.  I get it.
(With a nod to other posters, you might want to try out your concept with wine & fags before putting your futures at risk with the exciting

@_date: 2015-03-28 21:35:15
@_author: ianG 
@_subject: [Cryptography] OPENSSL FREAK 
Yes.  Do that.  Not upgrading the protocol, not starting over is also a bad thing.  Indeed, it's seems to be about an order of magnitude bigger badder thing, according to discovered bugs.
(Note that this assumption is a core assumption in the anti-agility approach - we are probably needing to replace our entire protocol every 5-10 years.  If we could write protocols as well as we wrote algorithms, we wouldn't need to worry about it.)

@_date: 2015-03-28 22:55:13
@_author: ianG 
@_subject: [Cryptography] OPENSSL FREAK 
Right, and now we're back to:  how do you switch from one cipher to another?  How do you turn off a cipher?
If usability is a concern (and of course it must be) then the user cannot have any input into this, because asking the user to do anything to do with crypto is both error prone and opens up a huge security weakness - the downgrade attack.
So we're more or less required to consider some form of automatic upgrade mechanism.  As Apple, Microsoft, etc have been working on for a decade now.
In which case, may as well upgrade the whole bloody lot.
 From a general life-cycle approach, there are no times or processes or aspects where algorithm agility plays any real full part.  Certainly it plays a checkbox / audit part in that the question "what happens when the alg breaks" is answered by "algorithm agility."  But it plays no part in real life security, as enjoyed by users.
Fans of algorithm agility need to lay out their life-cycle vision, and refer to empirical evidence that it was possible, it happened, it was the right thing to do, and it worked.

@_date: 2015-03-30 18:49:12
@_author: ianG 
@_subject: [Cryptography] Drop Zone: P2P E-commerce paper 
It feels like there is an entire generation of people running around and blockchaining everything:  messaging, insurance, passports, chocolate brownies, their kid sister, ... simply because!?
If blockchain is your hammer...

@_date: 2015-03-30 18:59:44
@_author: ianG 
@_subject: [Cryptography] OPENSSL FREAK 
So, on the one hand you claim incredible authority on this question, on the other hand, you decline to put that authority to good use in answering the challenge.
Clearly, if you are tuning the ciphersuites for millions of users, you are also installing the later releases of SSL as they come out of the SSL bug machine we've seen develop since 2011.  Right?
So you've cracked the upgrade nut.  You also know which algorithms introduced in the last 10 years caused any problems.  Correct?  And you know the ratio of bugs in protocol to emergency algorithm switches. Estimates?  You also know why it is your customers have any problem with export cipher downgrades.  Etc.
The penny drops.  Your business has huge incentives to support old customers.  The 'old customer' problem can be seen as the inability to upgrade broken/old SSL versions.  So if we were to do algorithm agility properly in SSL -- if we put the conceptual switch in to SSL to turn off the old algorithms as is demanded by the engineering -- we'd also solve the 'old customer' problem.
Unleashing all that energy to other more productive uses.
Do we have a dichotomy?  How much of the support for algorithm agility is due to the major companies handling the 'old customer' problem actually benefiting from complexity?
I mean, here, those huge business incentives can't be slouched at.  But they only effect you and your partner big companies.  For everyone else, we want a *cheaper* result.

@_date: 2015-05-02 18:21:30
@_author: ianG 
@_subject: [Cryptography] "Trust in digital certificate ecosystem eroding" 
It's pretty much fixed if we re-label the institutions involved as to their actions not their marketing.  The browser vendors are the top level CAs.  Users actually can trust Mozilla to some extent, Apple to a greater extent, Microsoft to a lesser extent, and google to whatever extent you desire.  Users choose their browser, and do so more or less consciously and with some base of information.
Then, under the top-level CAs, they have processes to choose a number of sub-CAs that the the top-level CAs feel comfortable outsourcing the real certificate issuing part to. CAs do their RA ("Registration Authority") process by checking the documents and policies of the sub-CAs, and adding them to the root list.
The CAs are public brand names, they have exposure, and when they stuff up they are incentivised to repair their brand and sharpen their act. This process works, i.e., was shown to work just recently when Mozilla-CA dropped its Chinese subsidiary CA CNNIC.
The system works.  Indeed, it is the only way it can work, because the CAs have the brand and jealously guard it.  Until the 4 CAs above -- google, Microsoft, Mozilla, Apple -- have a change of heart, and start sharing the branding on the chrome for the sub-CAs, then the system can't really change, the CAs have to make the decisions.  Nod to Bill, who says more!
ps; we sometimes call the 4 top level CAs as ber-CAs just to make the distinction between the marketing term of CA and the reality.
pps; to go deeper into this discussion, we'd have to talk about liability, but I'm trying not to be too depressed today so I'd rather not.

@_date: 2015-05-02 18:51:45
@_author: ianG 
@_subject: [Cryptography] Threat analysis - "Meeting Snowden in Princeton" by 
Because he's written such a comprehensive and timely threat summary over the intelligence community as a threat actor, here is Ross Anderson's post in full, for our record:
Meeting Snowden in Princeton
2015-05-02 Cryptology, Internet censorship, Legal issues, Open-source security, Politics, Privacy technology, Protocols, Security economics, Security engineering, Security psychology, Usability   Ross Anderson	
Im at Princeton where Ed Snowden is due to speak by live video link in a few minutes, and have a discussion with Bart Gellmann.
Yesterday he spent four hours with a group of cryptographers from industry and academia, of which I was privileged to be one. The topic was the possible and likely countermeasures, both legal and technical, against state surveillance. Ed attended as the Snobot, a telepresence robot that let him speak to us, listen and move round the room, from a studio in Moscow. As well as over a dozen cryptographers there was at least one lawyer and at least one journalist familiar with the leaked documents. Yesterdays meeting was under the Chatham House rule, so I may not say who said what; any new disclosures may have been made by Snowden, or by one of the journalists, or by one of the cryptographers who has assisted journalists with the material. Although most of what was discussed has probably appeared already in one place or another, as a matter of prudence Im publishing these notes on the blog while Im enjoying US first-amendment rights, and will sanitise them from my laptop before coming back through UK customs.
The problem of state surveillance is a global one rather than an NSA issue, and has been growing for years, along with public awareness of it. But we learned a lot from the leaks; for example, wiretaps on the communications between data centres were something nobody thought of; and it might do no harm to think a bit more about the backhaul in CDNs. (A website that runs TLS to a CDN and then bareback to the main server is actually worse than nothing, as we lose the ability to shame them.) Of course the agencies will go for the low-hanging fruit. Second, we also got some reassurance; for example, TLS works, unless the agencies have managed to steal or coerce the private keys, or hack the end systems. (This is a complex discussion given CDNs, problems with the CA ecology and bugs like Heartbleed.) And its a matter of record that Ed trusted his life to Tor, because he saw from the other side that it worked.
Third, the leaks give us a clear view of an intelligence analysts workflow. She will mainly look in Xkeyscore which is the Google of 5eyes comint; its a federated system hoovering up masses of stuff not just from 5eyes own assets but from other countries where the NSA cooperates or pays for access. Data are ingested into a vast rolling buffer; an analyst can run a federated search, using a selector (such as an IP address) or fingerprint (something that can be matched against the traffic). There are other such systems: Dancing oasis is the middle eastern version. Some xkeyscore assets are actually compromised third-party systems; there are multiple cases of rooted SMS servers that are queried in place and the results exfiltrated. Others involve vast infrastructure, like Tempora. If data in Xkeyscore are marked as of interest, theys moved to Pinwale to be memorialised for 5+ years. This is one function of the MDRs (massive data repositories, now more tactfully renamed mission data repositories) like Utah. At present storage is behind ingestion. Xkeyscore buffer times just depend on volumes and what storage they managed to install, plus what they manage to filter out.
As for crypto capabilities, a lot of stuff is decrypted automatically on ingest (e.g. using a stolen cert, presumably a private key obtained through hacking). Else the analyst sends the ciphertext to CES and they either decrypt it or say they cant. Theres no evidence of a wow cryptanalysis; it was key theft, or an implant, or a predicted RNG or supply-chain interference. Cryptanalysis has been seen of RC4, but not of elliptic curve crypto, and theres no sign of exploits against other commonly used algorithms. Of course, the vendors of some products have been coopted, notably skype. Homegrown crypto is routinely problematic, but properly implemented crypto keeps the agency out; gpg ciphertexts with RSA 1024 were returned as fails.
With IKE the NSA were interested in getting the original handshakes, harvesting them all systematically worldwide. These are databased and indexed. The quantum type attacks were common against non-crypto traffic; its easy to spam a poisoned link. However there is no evidence at all of active attacks on cryptographic protocols, or of any break-and-poison attack on crypto links. It is however possible that the hacking crew can use your cryptography to go after your end system rather than the content, if for example your crypto software has a buffer overflow.
What else might we learn from the disclosures when designing and implementing crypto? Well, read the disclosures and use your brain. Why did GCHQ bother stealing all the SIM card keys for Iceland from Gemalto, unless they have access to the local GSM radio links? Just look at the roof panels on US or UK embassies, that look like concrete but are actually transparent to RF. So when designing a protocol ask yourself whether a local listener is a serious consideration.
In addition to the Gemalto case, Belgacom is another case of hacking X to get at Y. The kind of attack here is now completely routine: you look for the HR spreadsheet in corporate email traffic, use this to identify the sysadmins, then chain your way in. Companies need to have some clue if theyre to stop attacks like this succeeding almost trivially. By routinely hacking companies of interest, the agencies are comprehensively undermining the security of critical infrastructure, and claim its a nobody but us capability. however thats not going to last; other countries will catch up.
Would opportunistic encryption help, such as using unauthenticated Diffie-Hellman everwhere? Quite probably; but governments might then simply compel the big service forms to make the seeds predictable. At present, key theft is probably more common than key compulsion in US operations (though other countries may be different). If the US government ever does use compelled certs, its more likely to be the FBI than the NSA, because of the latters focus on foreign targets. The FBI will occasionally buy hacked servers to run in place as honeypots, but Stuxnet and Flame used stolen certs. Bear in mind that anyone outside the USA has zero rights under US law.
Is it sensible to use medium-security systems such as Skype to hide traffic, even though they will give law enforcement access? For example, an NGO contacting people in one of the Stans might not want to incriminate them by using cryptography. The problem with this is that systems like Skype will give access not just to the FBI but to all sorts of really unsavoury police forces.
FBI operations can be opaque because of the care they take with parallel construction; the Lavabit case was maybe an example. It could have been easy to steal the key, but then how would the intercepted content have been used in court? In practice, there are tons of convictions made on the basis of cargo manifests, travel plans, calendars and other such plaintext data about which a suitable story can be told. The FBI considers it to be good practice to just grab all traffic data and memorialise it forever.
The NSA is even more cautious than the FBI, and wont use top exploits against clueful targets unless it really matters. Intelligence services are at least aware of the risk of losing a capability, unlike vanilla law enforcement, who once they have a tool will use it against absolutely everybody.
Using network intrusion detection against bad actors is very much like the attack / defence evolution seen in the anti-virus business. A system called Tutelage uses Xkeyscore infrastructure and matches network traffic against signatures, just like AV, but it has the same weaknesses. Script kiddies are easily identifiable from their script signatures via Xkeyscore, but the real bad actors know how to change network signatures, just as modern malware uses packers to become highly Cooperation with companies on network intrusion detection is tied up with liability games. DDoS attacks from Iran spooked US banks, which invited the government in to snoop on their networks, but above all wanted liability protection.
Usability is critical. Lots of good crypto never got widely adopted as it was too hard to use; think of PGP. On the other hand, Tails is horrifically vulnerable to traditional endpoint attacks, but you can give it as a package to journalists to use so they wont make so many mistakes. The source has to think How can I protect myself? which makes it really hard, especially for a source without a crypto and security background. You just cant trust random journalists to be clueful about everything from scripting to airgaps. Come to think of it, a naive source shouldnt trust their life to securedrop; he should use gpg before he sends stuff to it but he wont figure out that its a good idea to suppress key IDs. Engineers who design stuff for whistleblowers and journalists must be really thoughtful and careful if they want to ensure their users wont die when they screw up. The goal should be that no single error should be fatal, and so long as their failures arent compounded the users will stay alive. Bear in mind that non-roman-language countries use numeric passwords, and often just 8 digits. And being a target can really change the way you operate. For example, password managers are great, but not for someone like Ed, as they put too many of the eggs in one basket. If youre a target, create a memory castle, or a token that can be destroyed on short notice. If youre a target like Ed, you have to compartmentalise.
On the policy front, one of the eye-openers was the scale of intelligence sharing  its not just 5 eyes, but 15 or 35 or even 65 once you count all the countries sharing stuff with the NSA. So how does governance work? Quite simply, the NSA doesnt care about policy. Their OGC has 100 lawyers whose job is to enable the mission; to figure out loopholes or new interpretations of the law that let stuff get done. How do you restrain this? Could you use courts in other countries, that have stronger human-rights law? The precedents are not encouraging. New Zealands GCSB was sharing intel with Bangladesh agencies while the NZ government was investigating them for human-rights abuses. Ramstein in Germany is involved in all the drone killings, as fibre is needed to keep latency down low enough for remote vehicle pilots. The problem is that the intelligence agencies figure out ways to shield the authorities from culpability, and this should not happen.
Jurisdiction is a big soft spot. When will CDNs get tapped on the shoulder by local law enforcement in dodgy countries? Can you lock stuff out of particular jurisdictions, so your stuff doesnt end up in Egypt just for load-balancing reasons? Can the NSA force data to be rehomed in a friendly jurisdiction, e.g. by a light DoS? Then they request stuff from a partner rather than collecting it.
The spooks lawyers play games saying for example that they dumped content, but if you know IP address and file size you often have it; and IP address is a good enough pseudonym for most intel / LE use. They deny that they outsource to do legal arbitrage (e.g. NSA spies on Brits and GCHQ returns the favour by spying on Americans). Are they telling the truth? In theory there will be an MOU between NSA and the partner agency stipulating respect for each others laws, but there can be caveats, such as a classified version which says this is not a binding legal document. The sad fact is that law and legislators are losing the capability to hold people in the intelligence world to account, and also losing the appetite for it.
The deepest problem is that the system architecture that has evolved in recent years holds masses of information on many people with no intelligence value, but with vast potential for political abuse.
Traditional law enforcement worked on individualised suspicion; end-system compromise is better than mass search. Ed is on the record as leaving to the journalists all decisions about what targeted attacks to talk about, as many of them are against real bad people, and as a matter of principle we dont want to stop targeted attacks.
Interference with crypto in academia and industry is longstanding. People who intern with a clearance get a lifetime obligation when they go through indoctrination (yes, thats what its called), and this includes pre-publication review of anything relevant they write. The prepublication review board (PRB) at the CIA is notoriously unresponsive and you have to litigate to write a book. There are also specific programmes to recruit cryptographers, with a view to having friendly insiders in companies that might use or deploy crypto.
The export control mechanisms are also used as an early warning mechanism, to tip off the agency that kit X will be shipped to country Y on date Z. Then the technicians can insert an implant without anyone at the exporting company knowing a thing. This is usually much better than getting stuff Trojanned by the vendor.
Western governments are foolish to think they can develop NOBUS (no-one but us) technology and press the stop button when things go wrong, as this might not be true for ever. Stuxnet was highly targeted and carefully delivered but it ended up in Indonesia too. Developing countries talk of our first-mover advantage in carbon industrialisation, and push back when we ask them to burn less coal. They will make the same security arguments as our governments and use the same techniques, but without the same standards of care. Bear in mind, on the equities issue, that attack is way way easier than defence. So is cyber-war plausible? Politically no, but at the expert level it might eventually be so. Eventually something scary will happen, and then infrastructure companies will care more, but its doubtful that anyone will do a sufficiently coordinated attack on enough diverse plant through different firewalls and so on to pose a major threat to life.
How can we push back on the poisoning of the crypto/security community? We have to accept that some people are pro-NSA while others are pro-humanity. Some researchers do responsible disclosure while others devise zero-days and sell them to the NSA or Vupen. We can push back a bit by blocking papers from conferences or otherwise denying academic credit where researchers prefer cash or patriotism to responsible disclosure, but that only goes so far. People who can pay for a new kitchen with their first exploit sale can get very patriotic; NSA contractors have a higher standard of living than academics. Its best to develop a culture where people with and without clearances agree that crypto must be open and robust. The FREAK attack was based on export crypto of the 1990s.
We must also strengthen post-national norms in academia, while in the software world we need transparency, not just in the sense of open source but of business relationships too. Open source makes it harder for security companies to sell different versions of the product to people we like and people we hate. And the NSA may have thought dual-EC was OK because they were so close to RSA; a sceptical purchaser should have observed how many government speakers help them out at the RSA Secret laws are pure poison; government lawyers claim authority and act on it, and we dont know about it. Transparency about what governments can and cant do is vital.
On the technical front, we cant replace the existing infrastructure, so it wont be possible in the short term to give people mobile phones that cant be tracked. However it is possible to layer new communications systems on top of what already exists, as with the new generation of messaging apps that support end-to-end crypto with no key escrow. As for whether such systems take off on a large enough scale to make a difference, ultimately it will all be about incentives.
(by Ross Anderson)

@_date: 2015-05-05 01:09:34
@_author: ianG 
@_subject: [Cryptography] replacing the whole sodding lot 
One of the snarky predictions I made to the SSL/PKI/CA cabal when they refused to respond to security issues was that at some point the whole lot would be replaced.  It took a while, but there are now some green shoots.  This is spectacularly good, and I will say that all of the nasty things I've said about this company are forgiven.  I'll still say them, but ALL is FORGIVEN:
  A QUIC update on Googles experimental transport
Last year we announced QUIC, a UDP-based transport protocol for the modern Internet.  Over the last quarter, weve been increasing the amount of traffic to Google services that is served over QUIC and analyzing QUIC performance at scale. Results so far are positive, with the data showing that QUIC provides a real performance improvement over TCP thanks to QUIC's lower-latency connection establishment, improved congestion control, and better loss recovery.
For latency-sensitive services like web search, the largest gains come from zero-round-trip connection establishment. The standard way to do secure web browsing involves communicating over TCP + TLS, which requires 2 to 3 round trips with a server to establish a secure connection before the browser can request the actual web page. QUIC is designed so that if a client has talked to a given server before, it can can start sending data without any round trips, which makes web pages load faster. The data shows that 75% percent of connections can take advantage of QUICs zero-round-trip feature. Even on a well-optimized site like Google Search, where connections are often pre-established, we still see a 3% improvement in mean page load time with QUIC.
Another substantial gain for QUIC is improved congestion control and loss recovery. Packet sequence numbers are never reused when retransmitting a packet. This avoids ambiguity about which packets have been received and avoids dreaded retransmission timeouts. As a result, QUIC outshines TCP under poor network conditions, shaving a full second off the Google Search page load time for the slowest 1% of connections.    These benefits are even more apparent for video services like YouTube. Users report 30% fewer rebuffers when watching videos over QUIC. This means less time spent staring at the spinner and more time watching videos.
Where do we go from here? Today, roughly half of all requests from Chrome to Google servers are served over QUIC and were continuing to ramp up QUIC traffic, eventually making it the default transport from Google clients  both Chrome and mobile apps  to Google servers. We plan to formally propose QUIC to the IETF as an Internet standard but we have some housekeeping to do first, like changing the wire format and updating our reference implementation from SPDY-over-QUIC to HTTP2-over-QUIC. In the coming months, we also plan to work on lowering handshake overhead to allow better server-side scalability, improving forward error correction and congestion control, and adding support for multipath connections.

@_date: 2015-05-09 12:18:13
@_author: ianG 
@_subject: [Cryptography] NIST Workshop on Elliptic Curve Cryptography 
Workshop on Elliptic Curve Cryptography Standards
June 11-12, 2015
Agenda now available!
The National Institute of Standards and Technology (NIST) will host a Workshop on Elliptic Curve Cryptography Standards at NIST headquarters in Gaithersburg, MD on June 11-12, 2015.  The workshop will provide a venue to engage the cryptographic community, including academia, industry, and government users to discuss possible approaches to promote the adoption of secure, interoperable and efficient elliptic curve Register by June 4, 2015.  There is no on-site registration for meetings held at NIST.
Agenda, registration and workshop details are available at the workshop website:  iang (as forwarded by Russ to [saag])

@_date: 2015-05-12 01:16:42
@_author: ianG 
@_subject: [Cryptography] [cryptography] NIST Workshop on Elliptic Curve 
John Kelsey, chief of something or other at NIST, gave a pretty comprehensive talk on the NSA issue for NIST at Real World Crypto in Janaury [0].  My take-away is that they are taking it seriously.
 From memory, there wasn't anything directly spotted for the ECC stuff, but there has been this rising tide of demand for new curves ... so maybe now is the time.
If you're saying, can the academics stumble across something that the NSA had beforehand, well, of course.  But I'm not sure that's what you mean.
Yeah, curves look much harder than hashes and ciphers.  But is there a better option?
[0]

@_date: 2015-05-12 05:50:11
@_author: ianG 
@_subject: [Cryptography] books on SSL/TLS (the protocol not deployment) 
Can anyone recommend reasonable books to learn the protocol of SSL/TLS from scratch (entry level, for a grad student level)?
Back 15 years ago, Eric Rescorla's would have been good, but it doesn't seem to have been updated since.

@_date: 2015-05-12 15:13:40
@_author: ianG 
@_subject: [Cryptography] AEAD modes for signed ciphertext 
lots of snipping,
( This is what we call "squaring Zooko's triangle".  You can google on ZT which basically describes why it is that cryptographic ids don't work for humans.  The challenge is to build a composite system that provides users what they want, and to link that downwards into the keys.  Also you might want to check out petnames, and capabilities in general. )
( OK, so this is what I call the 'introduction problem' - how it is that Alice's agent is introduced to Bob's agent such that, post-introduction, when the former sees the latter, it alerts Alice to her petname for Bob. )
Yup.  Now, as you've handwaved about how Mallory acquires Alice's messages, we have to assume for the attack that Mallory can read any of Alice's messages and do the same.
Which then implies that any message cannot be self-authenticating.  So now we need to examine how authentication is done.  It's done by the What is going on here then is that you are allowing the signature to be replaced (easy) but the user is not informed.  Therefore this is not a cryptographic issue but a UI issue.  Your user has to be informed of who signed the message, and it this is bungled then ... your attack succeeds.
(This problem is the same problem existing in the browsers where the precise "who said what" is not clear, leading to phishing, and also with S/MIME, where the implementations bungle it by insisting that the outer envelope's sender in cleartext matches the cryptographic signer, simply because it isn't providing a proper UI that shows who signed the message.)
So, how are you informing Bob that the message came from Alice?  Or Mallory?  The "answer" in short is to make it obvious.
Right.  Ad infinitum.  Which indicates actually ... any message can be sent by anyone, and Bob isn't noticing who they come from.
It's the only solution as far as I can see.  Only Bob "knows" who Alice is, the software just handles the linkage and displays Bob's petname for Alice (etc), and doesn't handle context of what that might mean.
This solves nothing, because it's just another "sig" and can be stripped off as well.
(As an aside, in 2004 or so, I did AES with CBC [0] but now that it's 2015 I use ChaCha20/Poly1305 and I will use CAESAR when it comes out, or SHA3 if it holds to its promise, when either of these come out.)
Yep.  But Mallory can craft any email, including "I AM ALICE."  Tech can't stop that.  It can only show the true source of the message.
(Consider this possibility.  Alice sits next to Mallory in the HQ. Alice's comset breaks.  She needs to get the message "withdraw immediately" to Bob, or else all is lost.  So she asks Mallory to send the message "withdraw immediately, THIS IS ALICE" to Bob.  If she can't do that, she'll consider the system broken, and Bob will be surrounded and killed...)
Others may differ in opinions, but I think your problem is a UI problem.   And your task is to integrate into the security UI of the system.
[0]

@_date: 2015-05-12 15:27:21
@_author: ianG 
@_subject: [Cryptography] Is there a good algorithm providing 
Some African countries are in the process of increasing penalties for homosexuality, including in some cases the death sentence.
If one wanted to use e.g., Wikipedia to reach out to victims (being the homosexuals) and evade the attackers (being the police in those countries) then using HTTPS over the browsing would be a good measure.
It is unlikely that the police there would mount an attack that sophisticated that they could match the page lengths of different Wikipedia articles.  They'd have to index the whole lot, and then index the linkages to try and winnow out the non-related pages of the same length.  Etc.
Yes it could be done... And if it was done, then's the time to improve things.  Never let the perfect be the enemy of the good.
BTW, for anyone doubting the ability of such things to achieve change, here's an old article claiming that the Coloured Revolutions of five or so years ago in Northern Africa to Middle East were triggered by wikileaks of documents delivered by Pvte. Manning.

@_date: 2015-05-13 13:27:49
@_author: ianG 
@_subject: [Cryptography] AEAD modes for signed ciphertext 
People misunderstand ZT.  In the excitement of discovering a solution, what they fail to realise is that the conceptualisation of the problem was that which allowed for a solution to emerge.
It is for this reason that we introduced the phrase "squaring the triangle."
The point of ZT isn't that there is no solution at all, but that it is conjectured that there is no solution using one single technical component.
Therefore, you have to construct a solution out of components. Therefore you have to make compromises, design decisions, incur costs.
E.g., the wikipedia article argues that Szabo has proven it wrong.  Yet his solution proposes replicated databases, voting, vulnerability to conspiracy, etc.  A perfect example of compromise from composition of But yes, it seems that the current generation of bitcoiners have learnt that proclaiming success and moving on is a better strategy than proving it.
(Hmmm  Seems I've already written this in the Talk section.)

@_date: 2015-05-15 16:02:43
@_author: ianG 
@_subject: [Cryptography] NIST Workshop on Elliptic Curve Cryptography 
Yes.  It's an odd sort of compact between corporations and USG for the latter's support for PKI.  If the USG also ran a big CA that could be used for MITMing other organisations, it would be a bit obvious, wouldn't it ;-)
Also, if the browsers listed a USG CA, what happens when the Chinese and Russians insist on theirs?  Actually that's how the Chinese one worked until an excuse was found to punish it.
The PKI - new playground for the great game?
You'd think ... a lot of things about PKI.

@_date: 2015-05-16 16:05:11
@_author: ianG 
@_subject: [Cryptography] Is there a good algorithm providing both 
All this is true.  But there is one thing that makes OS (opportunistic security) a winner whereas every other approach is a loser.
Opportunistic Security is the on-ramp.
It is practically impossible to move a totally unsecured platform like HTTP across to HTTPS.  For this we have evidence - when HTTPS was released, it immediately caused a bifurcation which opened up the downgrade attack to phishing which made HTTPS approximately child's play to defeat in secure browsing.
The problem then is how to get the 99% to start using SSL?  Because, HTTPS doesn't work as a security model unless pretty much all your traffic is on SSL.
The answer is, perversely:  opportunistic security, as an on-ramp.  If all of HTTP migrates to HTTPS opportunistically, it becomes much easier to upgrade the OS protocol by adding in certs and TOFU and what have you than without.  It's an up-switch, drive up the on-ramp, no more, users just absorb the improvement in the highway without having to do a thing.
Whereas if you pick and poke at any one connection and prove through decades of inscrutable wisdom that it is insecure, you're looking at leaves falling off the tree when the wind rustles through.  You aren't seeing tree, let along the forest.
It really isn't about whether any one connection can be attacked.  It's about the overall balance between protecting those we want to protect and not wasting energy on perfect security;  making it easy enough so that the medium-grade protection users deserve is available by default, and hard enough such that the dedicated attackers just go in and hack their machines.
To further strain the metaphors, right now, we're looking at deforestation.  We want to get back to sustainable logging.

@_date: 2015-05-16 16:10:30
@_author: ianG 
@_subject: [Cryptography] NIST Workshop on Elliptic Curve Cryptography 
That exists, as a technical solution, yes.  But for some reason it has not seen widespread adoption.

@_date: 2015-05-16 16:16:33
@_author: ianG 
@_subject: [Cryptography] NIST Workshop on Elliptic Curve Cryptography 
Suspicion is all we've got to work with.  'Proof' doesn't work when the opponent is a decade or so ahead in the math, and refuses to come to the court for any particular question to be tried.  If this were a court, we'd be trying questions on "balance of suspicion" not "beyond reasonable doubt."
Also, the NIST curves are old;  we have a decade's worth more knowledge.   That's worth quite a lot.  Let's use it.

@_date: 2015-05-17 11:12:18
@_author: ianG 
@_subject: [Cryptography] [cryptography] NIST Workshop on Elliptic Curve 
This is the prevailing view in software as well, but it's still wrong. Most of the old timers here also come from a world when hardware dictated many concerns.  It's still the prevailing view in software for protocol negotiation that hardware enforces the basic numbers, most protocols are still using hardware notions of "32 bit integers" and so forth, without understanding that we don't need to do that anymore inside protocols, and doing so results in complication that leads to In short, any protocol that sends a "32 bit hardware number" is out of date.
Mmmphh... SHA3... sponge... AE mode... can't wait...
That's how stream ciphers such as Chacha/Salsa work - they are block ciphers underneath with a stream API.  I do not know if that has changed for CAESAR, but maybe, maybe.
It's also how my software works - higher layer delivers datagrams throughout, but when it comes to sending them over UDP, it breaks them up and delivers them in smaller packets so as to be assembled at the other side.  This is because some of the datagrams are huge - photos, which can be up to a meg, say - but we don't reliably know which ones are huge beforehand.  It's also a tunable because some networks handle small packets (older networks are limited to 1500 bytes) and some networks handle up to jumbo UDP packets (64k), but there are also networks (cell) that top out between 20-40k.
But in short:  application software SHOULD treat datagrams as being of infinite size, and network delivery software SHOULD have a facility to break up MonsterGrams into BabyGrams.
Is this world about hardware talking to hardware?  If so, why should the software world care?
If the world of hardware requires to talk the software world, what is the balance whereby we make things worse for software, while making things fast for (some) hardware buyers?
It's clear that in the hardware world, the economic incentives are much more clearly aligned.  People actually get paid for each unit delivered!   But it's also the case that hardware world pushing a more efficient solution - for them - can create externalities that cause costs for the software world.
That might make sense if there was a way to communicate the extra number of rounds needed.  But in practice, most of the experience we've had has been to upgrade the entire software suite.  In which case it would be just as easy to up the protocol number to a better algorithm.
There's something intellectually fraudulent about predicting that in 10 years we will need to upgrade the algorithm from X to Y, but we won't need to replace the rest of the software.  Experience shows that 9 out of 10 security blunders are with the protocol, not with the algorithms, so let's deal with the big cases first, and then we can also sweep up the occasional algorithm weakness at the same time.
Right, it's not as if you're providing a dynamic capability.  Although there is the obvious complexity argument leading to a downgrade attack.

@_date: 2015-05-17 18:16:03
@_author: ianG 
@_subject: [Cryptography] [cryptography] NIST Workshop on Elliptic Curve 
What is the problem there?
I'm guessing it is because google/Android team are incredibly slow to update their copy of Java security files & implementations, and Oracle/Java are diabolically lackadaisical about the same...

@_date: 2015-05-17 20:15:59
@_author: Ian G 
@_subject: [Cryptography] rare sighting of that shy beast known as 
" The Trojan, TROJ_YAHOYAH, eventually downloads and decrypts a malicious image or decoy file. The downloaded images appear harmless and look similar to default wallpapers in Windows XP systems. However, encrypted into them via simple steganography is BKDR_YAHAMAM, a malware that steals data from the system, kills processes and services, deletes files and directories, puts systems to sleep, and performs other backdoor capabilities."

@_date: 2015-05-23 11:29:47
@_author: ianG 
@_subject: [Cryptography] I broke a cipher this week. 
There's obviously something wrong with the message that is out there. Why is it that an apparently good engineer doesn't get the message that it's better to work with the known good stuff?
This is a seriously interesting question, in that we keep coming back to it.  If we can't even convince the programming world that something like AES is orders of magnitude more secure than a home-built cipher, ... what does this say about slightly more complicated decisions?
Of which there are many.  Case in point being cipher suites, which can be hard to understand, and a wrong choice can leave it totally open.
I wonder if there is an experiment that could be run.  Take 100 good engineers, give them a box of algorithms and a cheat-sheet on how they work and why they are bad ... tell them to pick one.  Then see how many decide to start from scratch?
I just last night watched _The Imitation Game_.  Apparently WWII was won because one man had a girlfriend, so didn't follow orders.  Yes, whatever, it was fiction, but the story is indicative.
What is the chance that any decision is bungled for reasons we can't rationally predict?  If that chance is X what does this do to our security?
How many decisions N need we introduce into a system before it becomes worthless because the combined chance of a breach exceeds some threshold?
Yes, brave.  If the press gets hold of it, they will write the wrong story.

@_date: 2015-05-23 17:19:30
@_author: ianG 
@_subject: [Cryptography] open questions in secure protocol design? 
Today's thought while reading the WIP/draft from IETF on algorithmic agility [0].  Open questions in secure protocol design:
1.  One True Cipher Suite versus Algorithm Agility?
2.  Design:
    - One designer, small team or committee,
    - Big corp v. lone wolf,
    - open source v. proprietary,
    - patented algs v. open algs,
    - Amateurs v. professionals?
3.  Compromises:
    - Simplicity v. Features,
    - hardware v. software,
    - raw speed v. resource efficiency?
4.  Packet-oriented v. connection oriented?
5.  opportunistic v. externally authenticated v. delayed auth?
6.  Modes & switches v. mode-less, switch-less?
7.  E2e versus point2point.
8.  Alg breaches v. protocol Breaches v. bypasses?
9.  Security v. delivery?
It occurs to me that we now have enough history in open (internet) secure protocol to do a survey across protocols & time and discover whether there are any meaningful trends in the above open questions.
This might make a good masters topic for someone?
[0]

@_date: 2015-05-26 14:58:30
@_author: ianG 
@_subject: [Cryptography] I broke a cipher this week. 
So you're saying the script writer and director did a good job :)  No bad thing.
At the beginning, it said "based on a true story" not documentary.
A better question might be, did people enjoy it, and did they come away with a helpful view of our world?

@_date: 2015-05-26 15:35:29
@_author: ianG 
@_subject: [Cryptography] open questions in secure protocol design? 
I sense a little over-reaction here.  1TCS is part of the real world, it's been used, and it does the job.
What we might disagree on is which parts of the world are better suited to which pattern.  Clearly, the TLS camp is very well entrenched, so we can probably agree to call the TLS school one of the end points in the If we look at all the places where 1TCS works, it might be that it is a bit of everywhere, but the thing that stands out (for me) is that it works far better when you don't have to worry about world-wide scale, competition, permission, approval, consensus and all that.
E.g., PGP1,2 were happy with it, and it was only when PGP grew up and entered the OpenPGP working group that it acquired the agile mojo. Bitcoin are happy with it, and as a curious aside, they also promote the 'practice' that all miners should use only the one body of code.
So we might end up saying that the same designs that need IETF would also prefer agility.  I see correlation there.  Maybe, there's an underlying causality.
But I wouldn't say that the IETF causes agility, no more than I'd say that agility is the cause of the IETF or that the IETF is the owner of all rationality and design space and protocols and everything.  That would be ... an overstretch.
More likely there is an underlying factor that is causal.  Something about the way the IETF is constructed is also something about why the people found at IETF need agility.
ps; does anyone know what the NSA's view on this is?  Yes, I know, then you have to kill me, but all in the pursuit of knowledge!

@_date: 2015-05-29 15:23:21
@_author: ianG 
@_subject: [Cryptography] open questions in secure protocol design? 
Yup.  On the one hand, 1TCS forces you to have a way of upgrading.
On the other, the varied alternate methods do not force you to have a way of upgrading the protocol.  Because of agility, you can simply assume that the protocol itself can switch internally.
So that looks like an advantage.  But, it's a chimera.  It just shifts the pieces around the board.  You still need a way of pushing a config or negotiation upgrade out to the users.
In this sense, there is zero difference - you need a way to push an upgrade out to users regardless of which approach you take, and if you don't have it, then you are screwed.
The only difference then is that Algorithm Agility allows you to assume it away, whereas 1TCS forces you to consider it, by removing the crutch.

@_date: 2015-05-29 15:30:21
@_author: ianG 
@_subject: [Cryptography] open questions in secure protocol design? 
That's really what I would like to inspire, hence the subject line.  We need someone to sit down and look at this history seriously and objectively.
For my money, things that worked well are these:
    PGP to 2.6.
    Skype
    Bitcoin
    Silent Circle [0]
Now, we can criticise this list.  Sure.  But, let's eliminate the religious sniping, and then see what's left.  What can we learn?
[0] history isn't a judge yet, so perhaps it's added for novelty :) Also Jon has demurred, he mentioned an algorithm had to be changed.

@_date: 2015-05-29 15:39:43
@_author: ianG 
@_subject: [Cryptography] open questions in secure protocol design? 
Do you have a better cite for that?  I also wrote that somewhere, and I guess I'm just old, traditional, out of date, but I do like to cite people.
On generalisms:  we are talking generalisms.  If people can't cope with the limitations of these models, and cannot dance between the generalisms and the particularities of real problems, then they probably shouldn't be doing crypto.
The odds & evens protocol pattern, in that a protocol could be precisely odd or even.
(And, an implementation would therefore handle two protocol versions, being one odd and one even, with distance one between them. 1,2 -> 2,3 -> 3,4...)
Yes.  This is all about that.  Making the designer think.  Because typically, they are not, they are following on from "best practices" that are out of date, without having realised that knowledge has moved on.
There is fairly clearly a spectrum of choices for the perceptive designer to choose from.
    TLS 400 flowers <--> MTIs + backups <--> odds&evens <--> 1TCS.
This of course is another stylised generalisation.  The presentation is of a model.  When we get to reality, the designer has to make a decision on the particularities; take it from model to implementation.

@_date: 2015-05-29 19:36:24
@_author: ianG 
@_subject: [Cryptography] Uniform Data Fingerprint 
Some comments - on the whole this is a good start!
2.1.  Last para seems to conflate two issues being the age/replacement and the weak/substitution.  Either way we arrive at the same conclusion, that the fingerprint mechanism should include some degree of signal that indicates which one it is.
I think I'd write it somewhat differently, words to effect:
Fingerprint formats have had several problems in the past.  There has been a proliferation of formats which has led to a potential confusion between the algorithm to be used on a particular format.  In particular, where an algorithm has also become weak, such as MD5, it is possible to do a substitution attack.
Therefore, representations MUST reserve the first 5 bits as an algorithm identifier Section 3.1.1.
I'm a bit disturbed by the MIME content type but I can't quite put my finger on what's the difficulty.  One thing that might help is to define a default type in the words of the text that means "no information/context is implied."
     An empty Content-ID can be used if no MIME content is to
     be delivered, but the colon ':' must always be present.
It would also be helpful (to me?) to specify some basic MIME types. E.g., things like:
     The following MIME types are reserved:
        text/plain
        openpgp-v5-key
        SMIME-v1
        openpgp-v4-cleartext-signed
etc (just making it up as I go).
Can I suggest that M and S be output as m and s?  In this way we signal to the eye more easily what is going on:
     mB2GK-6DUF5
(and a caveat that it is a typographical convention only, it is case independent, and implementations must accept leading upper case).
I'm unsure about section 4.  What's the point in just talking about it?   E.g., if we want a word list, why not introduce it, just copy the PGP word list into an appendix and provide some text as to how it works.

@_date: 2015-05-30 02:24:04
@_author: ianG 
@_subject: [Cryptography] open questions in secure protocol design? 
Out by 18 years - that's gpg, not pgp.  2.6 was marked for demise in 1997 when pgp5 was released to the world 1997.  Those crazy Dutch.
Which latter pgp5 had the new shiny algorithmic agility upgrade, dammit.
A+ agree.  It should be RSA only.  They should have dropped the rest after the patent expired.  Those crazy Germans.  I see you agree in follow-on post.  But this:
 > The idea of "One True Ciphersuite" complicates the elimination of
 > outmoded ciphers that should no longer be supported.
Huh?  What is this?  Blame the victim?  I am pretty sure the author of gpg isn't blaming me because he can't get rid of DSA/Elgamal ;)
Well.  That isn't to deny that it worked, and it worked well, and it walloped the competition, and and and.  In some part, Skype's success was due to the fact that the engineering of the protocol was not weighed down by costly practices.
post 2009, possibly.  I'm assuming that the audit report from about 2006 is indicative until about 2009.  Those crazy Americans.  It wasn't the engineers that decided to do that, for the beautification of the protocol.
Except, you changed the topic.  Coming back to the topic ... do you disagree that Bitcoin uses one alg for each function?  Oh wait, your post is about how you agree that it's only using one alg.
You're real disagreement is that you don't like that it is using one alg, and predict it will therefore melt-down :)
So, we're now dividing along the lines of FUD.  If what you fear comes true, you're right, but there's always a version change.  If what you fear doesn't come true, I'm right, but there's always tomorrow.
    "How can Bitcoin avoid these sorts of attacks in perpetuity?"
Actually, I suspect regardless of our views, Bitcoin is locked into the 1TCS for now.  Because if they add another algorithm, it is totally worthless until every client has got it.  Which to paraphrase Peter "will make them think."  And we will benefit from the experience.
So, just on that proprietary "no comment" it may be that we end up deciding that where algorithm agility has to be used, and 1TCS can't be used is ... protocols written jointly in WGs.
But that's not the same as protocols approved in WGs, protocols written by a leader in a WG, protocols written by a corp then published, and etc

@_date: 2015-05-30 12:31:41
@_author: ianG 
@_subject: [Cryptography] open questions in secure protocol design? 
OK, but the problem we are talking about here is not what curve to pick now, but how do you change the curve in 7 years, when the original is considered or proves weak.
And the corollary to that discussion is that allowing the addition of new key types *without a strategy for deprecation and rollover* is a pathway to hell.
Not an issue.  Part of our hope here is challenge 'professional' opinion that is out of date.
Right.  It's easy to talk about it key duality on paper.  I'm not sure anyone's got any track record in it.
Why should auth be connection oriented?
(I agree that auth will probably be session oriented, but recent work with say QUIC and mosh is towards making the session agile across IPs. The motive for this comes from mobile.  What I don't know is how well it Point2point unsecures (decrypts) at each node so each node can be a vulnerability.  End2end requires each node to have an independent and reliable auth method over every other node ... a challenging ask.
Good answers.  I should have reversed the order of the points, everyone got choked on

@_date: 2015-05-30 14:38:15
@_author: ianG 
@_subject: [Cryptography] Forget the Enigma Code; 
The Swiss used to have a rule that any officer of a bank that went bankrupt could be criminally indicted just on that.  I'm not sure if that is still the case, or even relevant as banks aren't going bankrupt like they used to...
The problem with this is that it creates a pot of honey that both insiders and outsiders can steal.  So you now add governance load to the system, and the one thing we know from the last decades of governance improvements is that we don't have a good track record of doing that ...
So how does this work - the programmers fly on the first flight?  The programmers stand under the rocket?  On the launch pad?
What about the 1000th flight of an airbus 380 that goes down due to a number being out of range because of a committee that changed a spec in a protocol from a company in China that copied an IETF protocol that was designed for remote control lawnmowers?
Finally, a reason to be a slave of a builder than a son of a builder :)
Now, the obvious problem here is this:  it will add terrific costs to the process.  Programming will now look like American health care - the best in the world but also many times more expensive and out of reach of most.  So innovation will come to a great screaming halt, the economy will stall and all the fun guys will migrate to the countries that aren't beaten up in secret treaty negotiations.
As the ancient chinese curse has it, be careful what you wish for.

@_date: 2015-05-31 15:16:38
@_author: ianG 
@_subject: [Cryptography] open questions in secure protocol design? 
(Hmmm.. thinking about this, there are two SPOFs here - the cipher suite, and the "body" that designs the next one.  Whereas arguably the 1000 flowers method and other points in between are less vulnerable to these SPOFs.  Point to note in discussions.)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ !
Threat analysis should really be a subset of risk analysis.  In that art, we should be allocating likelihoods to all our threats.  As it happens, the likelihood of a sudden fail is below the noise level, assuming we chose wisely, of which the counter-example to wisdom is WEP.
If a threat is so unlikely we can't measure it, the typical response from risk analysis is to accept the risk.  This is the right choice because of our psychological bias to over-react on scary risks and under-react on non-scary risks;  to counteract that fear, we need to show data, and if the data approximates zero, we call it zero.
This is a general problem.  It isn't solved by any of the systems we talk about, because we don't have a way to send out a "quick" instruction like that [0].
(a) If we look at the canonical case here of certificate revocation as a quick instruction, it's pretty clear in 2010s that it doesn't work for CA-breach, so the super-CA has to revoke the CA with a browser software (b) If we look at the recent dozen TLS breaches, there was no quick response.  It started at 3.5 years, and went down [1].  I'm not sure what the figures are for the last one, but I'd be surprised (and encouraged) if was down to less than a year.
(c) his system involves upgrade in *new devices only* and all old devices are left stuck on old protocols.  As we're talking IoTs these things might have a 20 year lifetime ...
My understanding of his method is that the software handles two suites.   So he's covered as long as N-1 fails, N is still good and N+1 goes bad in delivery.  N+2 carries great incentives tho :)
If however he gets a catastrophic fail in N, and moves to N+1 which was broken, then he's screwed.
However, this is the old joke about carrying a bomb on a plane - the chances of their being another bomb on a plane at the same time are next to zero, so everyone should do it [2] !
Also, a system that employs a cryptosystem will be typically plagued by higher layer threats, and if it is properly designed, it will model a catostrophic failure of the crypto and build in defence in depth.  E.g., in payments systems, there are typically controls on the cash-out agents so that they can't easily deliver a million bucks because someone broke the card crypto.
So much of security design in the 1990s was predicated on "we know what we're doing so we'll teach the users to do what we do..."
Right!  If you're steering fails, you can clamp on the breaks and limit the damage.  If your brakes fail, you're screwed.  And so is anyone in front of you...
Having redundant brakes covers a lot of the other risks as well.
Which works for cars, roadtrains, etc.  But it doesn't work for planes.   Planes have redundant steering and engines, but crappy brakes.
I think that's pretty easy to answer for the general Internet security case.  If someone is capable of *attacking your cryptography* then (a) they are well easily capable of hacking your platforms, and (b) we are talking about a very clueful attacker who is against a very high value target, so we should be demanding more money for the product.
For everyone/everything else:  take on some small risks.
The security high tide watermark is reached when the general attacker sees 50:50 between cryptographic attack versus platform attack.  We then build the crypto a bit stronger, higher, up to the 100 year flood level.
That.  This is *all about protocol design informed by threat analysis*.
[0] In the 1990s there was a system called Mondex that had been told by their regulating CB to make sure they were safe.  So they designed in a second ciphersuite and developed the techniques to switch over in case of algorithm meltdown.  They never had to use it, indeed their system failed in general at market level.  The alternate systems that I knew of (it's all proprietary/secret) used defence in depth mechanisms to deal with algorithmic failure, and stuck more to 1TCS.
[1] Arbitrarily, I'm calling 80% deployment as the benchmark for the renegotiation breach.  This works because all I'm doing here is measuring the diameter of the OODA loop.
[0] Eat this email before going through security.

@_date: 2015-11-02 23:24:26
@_author: ianG 
@_subject: [Cryptography] RIP Tommy Flowers 
In this case, it looks like the victors went further than writing history, they also bombed it into oblivion.

@_date: 2015-11-06 01:26:46
@_author: ianG 
@_subject: [Cryptography] How programming language design can help us 
If I may be so bold, there is no paradox.  The reason for the difference in approach is founded in information.
In the safety critical world, we have pretty much universal agreement on what is an unsafe thing and what is a safe thing.
In contrast, in the information security world, we have dozens or even hundreds of tribes touting one view of security in conflict with another.  There is no agreement, and there is no easy way to find agreement, on what means "Secure" and what means "Insecure".
At least, at the level that is achieved by the safety people.
Hence... it becomes a battleground of "my view of what security means."   This often but not always relates to what I'm selling today.  Either way, given the uncertainty, it is appropriate for me to use any argument I can to push my view.  And if I like C (or I have a 1mloc code base) then C can be written securely, and you'd be daft not to believe me.
ps; great post!

@_date: 2015-11-06 01:33:43
@_author: ianG 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
1.  I doubt anyone would study such a thing because it's obscure.  But this would change if there was a big popular system using it.
2.  I'd be staggered if there was a connection.  Eg an attack as brash as that would speak to huge problems - probably in both algorithms.
3.  In contrast, I'd say, just don't do that.  At a minimum take the one key and expand it into two keys.  Or exchange more material, really in this day and age, key material is cheap.  (OK, so I suspect you're reviewing an actual system...)

@_date: 2015-11-06 01:47:11
@_author: ianG 
@_subject: [Cryptography] Why Rijndael ? 
The AES process was a competition.  The last 5 contenders were all heavily trawled over by all the cryptography groups in the last 5 and many others.  There was no mention at all by any of them that any of the 5 were in any way unsuitable - everyone was pretty convinced we'd got 5 best of class algorithms and any would be a good choice.
In terms of your question - the competition was an independent operation.  NIST made the final choice, sure, but the competition surfaced that we had 5 good algos.
The winner was chosen on the margin - good in all categories whereas the others seemed to have some more darker sides.
I don't recall really at the time, but as an anecdote, my team doing Java Cryptix at the time predicted before the end that Rijndael would be the winner.  Well, we were a bit involved as coders of the test suite and some of the algos, but certainly NIST wasn't listening to us on the crypto :)
AES has been subject to a lot of attacks ... it's even lost a few bits of strength in dramatic circumstances.  But the NIST process itself has not been scrutinised as far as I know.
SO, it is possible that the Rijndael team were a put-up job.  But we still got a best of class algorithm.  We got an algorithm that could in that weirdo case still only be hacked by the NSA.  And even then, looking at the situation, we'd be likely saying that they need to run their supercomputers at overdrive and turn off the lights in Utah for a week to crack one SSL session.  Pointless - the real game is hacking RSA.  And the real real game is in hacking the code in your browser, or the IETF WGs or the OS suppliers or any of 100 easier targets.
If there is one thing that is rock solid strong, it's the encryption

@_date: 2015-11-07 10:14:49
@_author: ianG 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
That's quite controversial.
  * It sort of posits that homegrown crypto causes people to act more dangerously, the so-called "false sense of security" ... but this isn't really how the world works.  In the real world, we run through the list of risks, do what we can to mitigate them, and then move on.  Those that we fail to close of, we accept.  We're at risk all the time against everything, it's just a set of different levels.
  * As a practical observation, home grown crypto is often shown to have weaknesses due to the simple lack of knowledge - fair enough given the limited time the home gardener has to pick up the skills.  But what is not shown very often is that the home grown crypto led to actual monetary damages, and that is because:
  * Homegrown crypto is a far better thing than no crypto, and while it's not being attacked by an actual person with cryptanalytic experience, it's knocking hordes of scammers, script kiddies, criminal gangs and what-have-you.  There still remains a tight negative correlation between cryptanalytic skills and criminal gangs, although the publication of academic results does tend to lead to the development of script exploits leading to potential dangers.
  * people in the crypto business like to say "homegrown crypto is a bad idea" so that they get the job.  But this is about their security not yours.  Unfortunately, the professionals who say that often can't do risk analysis so they can't see that their approach can take away important elements of your other security.  Leaving you worse off than if you focus on what is really hurting your business.
Sounds ok.
Ah.  So maybe what you really want is a library that exposes an encrypt&auth method.  This is called Authenticated Encryption or AEAD, and is a thing.  It's not so well standardised as yet, there is an active competition to provide it.  Probably the algorithm that is most popular in this vein is Chacha/Poly but even that is two algorithms (BTW, calling the HMAC-SHA1 'signing' isn't quite right.  Signing is reserved for public key ops, as they have long term reliability, whereas the use of HMACs is considered to be ephemeral, temporary, good enough to stop an attacker in the next minute.  Authentication is the verb most used there.)
Hmmm... you haven't established a harm as yet.  Maybe you know something else, but it doesn't seem like a huge problem that they're putting some extra encryption in place?
If you're moving them from an old deprecated library to a newer supported one, ok.  OTOH, if you're moving them from some standard inhouse thing that they already have, and are just adding another external dependency, well, that could cause issues on its own.

@_date: 2015-11-10 23:43:27
@_author: ianG 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
That's a little unkind, although I'm not sure to whom.  The late 1990s and 2000s was about encrypt & MAC and we kept getting it wrong.  At some stage we figured out the problem was as much crypto interactions as programming, so we threw the whole lot back over the fence and yelled to the cryptologers on the other side:
Compose it!  Throw back an AEAD algorithm!  We'll buy you a beer!

@_date: 2015-11-14 22:16:17
@_author: ianG 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
Now, *that* is novel.  Criminal gangs in the bitcoin world are learning linear/differential cryptanalysis!  This is a milestone of some Interesting!  Is there a cite for this?  Not that I don't believe you but it becomes much more useful evidence if we have something we can point at.
Good.  But in order to damn the homegrown crypto, you'd actually have to do some analysis.  How much did they spend on the homegrown crypto, how much would a professional job have cost, and how much did you charge to "change some software and config" ?
Then, we would have to integrate time into the calculations.  It could be that they saved people's bacon for several years on some lightweight crypto that cost a few thousand, in which case they can write that off against a benefit of protection over years.  If the thefts amounted to a few dozen bitcoins, then it could be well in profit.
IDK the answer to these questions - what I do know that a crack of crypto isn't evidence of bad engineering nor bad choices in production of homebrew crypto, by itself.
Yep.  One thing I have noticed is that encouraging people to use some homegrown stuff actually gets them into the business.  Then, step by step they professionalise.
In the alternate, the cryptoguild approach of must-be-perfect before you're allowed to send your first packet tends to be such a high barrier that most devs choose no crypto.
Perhaps we need an award for most promising homebrew crypto protocols?

@_date: 2015-11-16 00:10:09
@_author: ianG 
@_subject: [Cryptography] ratcheting DH strengths over time 
One of the things that is pretty clear now is that passing paramaters across to end users is just asking for trouble.  The end-users don't know what they mean, the configuration becomes a nightmare, it opens up downgrade attacks, consumes sysadm time, generates little or no measurable security and etc etc.
In times gone past, choosing public key strengths was left to the users, but this is a hangover from the days of PGP where you were expected to know what sizes meant.  Those days are gone and even RSA key sizes have been somewhat limited between various and many limits.  e.g., choose outside 2048-4096 and you risk running into one barrier or another.  But the notion that this number should be taken entirely out of users hands is not as yet popular.
However, there is an area where we might be able to win over the cultural argument:  DH key exchange.  In the light of the recent suggestion that NSA is cracking weak DH keys, we're again facing the same old failure:  We lack a forward thinking plan to improve the crypto over time to cope with circumstances.
IOW, it's easy to point out the negligence of letting users choose numbers, but how to remove the option?
Satoshi Nakamoto introduced the idea of a ratcheting difficulty [0], which increased the hardness of PoW by adding a few bits every now and then.  Likewise he introduced the idea of a schedule of mining rewards, the 25BTC thing that halves every now and then.  Both of these seem to have worked, in the brutal sense that they got us well into the next few years, and the next set of problems were revealed.
How could we do this in a DH protocol?  I would suggest a schedule over time.  Most or all of our implementations have a timebase available. Something like this:
2015 - 1024
2016 - 1280
2017 - 1536
2018 - 1792
Or, it could be done by starting with a constant for year 0 and adding another constant for every later year.
Then, some rules:
1.  I look up my year-size, and initiate with that number.
2.  I will accept any later year if initiated against.
3.  If asked for a bigger size, I'll have to generate/resend a new DH number.  This will happen on new year time, with dodgy timebases, and those sysadms that want to timetravel.
What difficulties are there?
  1. How would we change the schedule?  That is simple - the schedule will be fixed for the current version of the software.  Each new version comes out with its own schedule.  If you're talking v3 you use the v3 schedule.  Upgrades are deferred to v4.
  2. The obvious bug of a missing timebase:   means that the client can't initiate correctly but it could just initiate with any number, and respond to the other's demand.  Two clients without timebase will then end up stuck at their default, but that's the situation now, so it isn't a loss.
  3. Testing is a bit of a nuisance as we'd have to wait until a year passes before watching them all click up.  So something of a faster ratchet would make sense - 20 bits every month in the above schedule.
Thoughts?  This isn't crypto per se, it's cryptography policy.  It's the sort of thing that should become standard in WG dox.
[0] by that I mean, got it working on a large scale, not that she was the first person to suggest it.

@_date: 2015-11-17 15:01:14
@_author: ianG 
@_subject: [Cryptography] [FORGED] Re: ratcheting DH strengths over time 
If you mean, the client does a work thing and tests how big a key it can create in say 10s, then that is a metric, but it's pretty loose.  The problem here is that you're measuring your CPU, whereas what we want to predict is the attacker's work difficulty.
Unless you really mean "blockchain based" and then everyone is fighting to increase the difficulty.  I'm not seeing why that would help.  But I have proposed elsewhere that the PoW function should really be a function over checking & signing RSA signatures, so we could more economically use the past-life mining boxes for fast SSL.
Yes - but how do you get the protocol designers to agree to use 2048 only?  The point I'm trying to reach is where there is *no user config required* which means that the protocol designer has to lay it out for probably 20 years.

@_date: 2015-11-18 01:13:42
@_author: ianG 
@_subject: [Cryptography] Satoshi's PGP key. 
Why would they?  Do *they* have any need to prove themselves?  If I was Satoshi, which I am every second Tuesday, I'd be scared to have to authenticate my very words, it would mean that my words meant nothing, only my persona.  That way lies madness.
This surprising (?) unknown (?) property is similar that used to make root lists work - distro by other means.  It is also exactly the same property that makes Ricardian Contracts strong without any external referent - the document is distributed from person to person until it is so widespread that it is hard to introduce a "false" one.
We should ask her to try that experiment ;)
Coming back to the need to prove words, it is notable that people tend to ignore the mail because they are unsure if the source is valid.  Do we have a situation where SN could write a great idea and have it be ignored because it isn't signed?  Or, if he wrote a daft idea for say blocksize enlargement, signing would cause the daft idea to become gospel?

@_date: 2015-11-21 11:38:10
@_author: ianG 
@_subject: [Cryptography] [FORGED] Re: ratcheting DH strengths over time 
This one specifically refers to numerology, but in an appropriate way, with citations and shakedowns ;-)
The third response produces a reasonable level of security: ECDHE, as actually implemented, doesn't allow any small groups. Properly implementing ECDHE with NIST P-256 isn't easy, but all of the critical pitfalls here are eliminated by next-generation ECC. The performance of NIST P-256 is already acceptable for many sites, and I expect the performance of next-generation ECC to be acceptable for almost all sites.
One can mix and match responses: for example, use ECDHE with a server-specific elliptic curve. But it's better to use ECDHE with a larger shared curve. Compared to a server-specific curve, a larger shared curve saves bandwidth; in most situations ends up saving CPU time; and has larger benefits against all known attacks.
(by djb)

@_date: 2015-11-21 12:41:06
@_author: ianG 
@_subject: [Cryptography] 
=?utf-8?q?d_would-be_terrorists_with_encryption?=
Wait - are you giving any credence to any statements official or otherwise?  Why is that?  I can't recall the last time I saw a statement that wasn't broken by standard levels of challenge and skepticism.
At this stage, it would seem an impossible task for any of the agencies concerned to change their culture to make a statement, official or unofficial, that was in any way reliable or useful to the public.
e.g., 1. Look at the trouble NIST is in trying to come up with a credible statement.
e.g., 2. Notice the recent Russian plane that went down - for the first time that I can remember, there was deliberate suppression of the causes by/in the Russian media.  They mostly said "we don't know", whereas in the west, MSM immediately slots in the fashion statement threat de jure.   and E.g., 3. Paris was caused by crypto, syrian refugees and weak gun control over civilians.
e.g., 4. recent NSA statement on EC - we still don't know what it means or what to do, other than generally panic.
tl;dr - believe nothing that the agencies or MSM write about.  Reverse it and you'll likely find more reliability.
Indeed, this is standard military training, or was in my day.  For example, if engaged in a firefight, we knew that the enemy would also know pretty soon what we would know, so things like location, time, casualty numbers, and estimates of the enemy strength were not encrypted, because if encrypted, they would give the enemy cribs with which to crack the codes.
(OK, so this assumes that the soldiers on the ground were using pencil & paper codes rather than encrypted radios, but the principles still apply.)
Launching a drone missile has to be taken at a high level because they're engaged in an illegal act.  Decisions to commit war crimes cannot be easily delegated, because soldiers are trained not to conduct illegal acts, they have to be ordered to do so [0].
The problem here is a peculiarly American one, with some leak-over to 5-eyes.  In short, USA agencies are obsessed with sigint.  Breaking this obsession down to parts,
  1. historical success (Enigma, Purple and so forth).
  2. technological advantage (Silicon Valley effect).
  3. economies of scale in listening to everything.
  4. economies of scale in employing mathematicians.
  5. industrial-military-crypto complex - the ability of large defence suppliers to convince the employment of strategies that support large defence suppliers,
  6. hubris and the ability to get locked into ones own OODA.
I'm not scientific enough to apply percentages to those factors, and there may be others, but it's a pretty big cultural trap the USA is in.   What is perhaps more interesting is that 6. totally undermines 1 and diminishes 4 to nearly worthless.  2 is dying with the rise of China. And 3. is easily defeated by "radio silence" or burner phones.
What's left is 5 -- which appears to be getting stronger and stronger, if the rise of cyberwarfare divisions in security & defence firms is any [0] Soldiers are also trained to refuse illegal orders, but today is not the day for grunt-level philosophy ;)

@_date: 2015-11-21 14:37:14
@_author: ianG 
@_subject: [Cryptography] ratcheting DH strengths over time 
It makes sense - to them.  To me, it doesn't make sense to follow that path because we simply don't know enough to reliably make that balanced equation, and that's even before bringing in time.  See DJB's post on that, which seems to slam all and sundry on poor thinking, but is dotted with helpful conclusions like [0]:
      To summarize, here's what's actually known
      about the per-key cost of breaking RSA keys:
      "It's complicated."
Increase the rounds?  Most symmetrics have some sense of that already. Chacha has it built in as 8, 12, 20, and the AES candidate discussion had long and sometimes warm arguments about the effect of more rounds.
[0] Which is a *must read* for this mailing list.

@_date: 2015-11-21 14:47:44
@_author: ianG 
@_subject: [Cryptography] ratcheting DH strengths over time 
OK, you can do that - but the problem here is that your machine isn't a reliable predictor of the attacker's machine.  Indeed it isn't even a vaguely handwavy aligned predictor.  So what you're really doing is setting the number to what the user can tolerate, which might or might not deliver the security desired.  I think this is fine if you're not protecting much, but if you're actually protecting something important, then not so fine.

@_date: 2015-11-21 14:49:01
@_author: ianG 
@_subject: [Cryptography] ratcheting DH strengths over time 
I see two benefits to this approach - one is that it "works" in and of itself to some degree of approximation - we can reasonably map out a curve of expected strength and we can call that a better prediction than say aiming at a hypothetical point in the future.
The second is realpolitik, that it might well force the designer to think about the bigger problem - the future.  The big flaw that has emerged in the last 5 years, as evidenced by all (?) the crypto fails we've seen, without exception (?), have been caused by "old stuff" that we knew should have been deprecated and removed ages ago.
With apologies to Jon, that's the world we live in.
So, what can we do to encourage protocol designers to start planning for end-of-life?  That's a very general question of course, because we're hacking what amounts to a 'best practice' to handwave the future away, which take decades to change.  But, if we turn around and say "use a ECDH schedule rather than choice or a too-big number" we're actually forcing them to pick that schedule.  Which then gets them thinking about the end-of-life problem.
As a partial answer, if I was an engineer of one of these devices, I'd be pulling the crypto from some well-known stack that would therefore have the schedule built in already.  I'd also expect the testing to be provided as well.
Also, if one reads the other thread "Long-term security" it seems that
1. the IoTarget has to keep going elsewise the owner freezes...
2. automatic updates are not the answer;
3. expiry is the least worst of the bad options;
In contrast, this does suggest a 4th option:  the device gets slower and slower.  E.g., it appears to be wearing out ;)
But, yes, I agree that the IoT device has to keep working, so the notion of arbitrary increases out to infinity doesn't seem like a good idea. Something like 1x -> 2x -> 3x and stop.
Also, there is nothing about the approach that means everyone has to follow suit.  If the IoT manufacturer just hacks his cryptostack to turn off scheduled increases, and he does so on the controllers as well, he's lost nothing except "compliance with an IETF WG" which he's happy to ignore anyway.
Yes - this is the challenge, to get the protocol engineers to think of the future EoL scenario at a crypto level.  But I think, to use Jerry's phrase, we will only succeed if we keep banging the drum.
Yes - but even that is solved by the notion that we should be replacing things within a reasonable timeframe.  Although this knowledge of DH primes sharing was known once upon a time, it was forgotten -- only to be reminded by some attacks.
So, again, we are in a situation where our knowledge (typically) gets better in the long-run.  Attacks also get better in the long-run.  So we are again forced to conclude that what we did 10 years ago, no matter how carefully, will need refresh in some sense or other.
If the Internet of Targets world refuses to think about these things, that is their engineering decision.  But we on the Internet of software shouldn't live up to that excuse.  We should be better than that.

@_date: 2015-11-26 16:34:50
@_author: ianG 
@_subject: [Cryptography] Dells are shipping with a rogue root level CA 
Assuming this is a serious question - very hard. The key to the question here would be "what question do you want answered by the audit?"
In practice, the question that *is* answered in typical CA audits is:
     "is the CA compliant with a big long list of things
     that CAs have agreed are important?"
What should be asked is
     "how does this serve (secure) the user?"
But for various historical and structural-institutional reasons, that will never be asked in a formal audit process.  It was however half-asked in an audit process known as DRC, so we can say that it isn't an unaskable, at least.
(much much) more here:
Much more than you wanted to know ;)
Now, the question you might ask is ... why ?

@_date: 2015-11-27 01:12:04
@_author: ianG 
@_subject: [Cryptography] Fighting fear (of encryption) with fear (of bad 
Here's one answer:
You need to watch it all the way to the end.  Sorry, I don't normally recommend vids, but this one is worth the effort.  And it answers your question ;)

@_date: 2015-10-05 12:03:16
@_author: ianG 
@_subject: [Cryptography] blockchain and trustworthy computing 
This seems to solve the problem for me, but trustworthy computing was always about being able to run my code on someone else's facility.  For example, I-as-movie-mogul run my movie on your TV, after payment received.
Right - so leaving aside the "stick the probe in the pipe" solution - let's assume that the basic sensors are trustworthy.
Then, if EPA or California or whoever were to mandate that a private & certified blockchain were run on the car, and the sensors all followed a standard API, would we have a platform we (the EPA) could trust?
Right.  But we solved that problem with the odometer already - you're not allowed to rewind your odometer any more.
I'm speaking very hypothetically, imagining a future.  If people are uncomfortable with cars as a thought platform, think of a plane, far less scary :)  We already have much stronger and secure standards for plane automation systems, so it should be a snap to add in a blockchain (*).
(*) I'm kidding of course...
The primary purpose *was* to provide that.  But there are at least two groups who differ in their view of the future.  One is the financial world, which are being teased by the notion of disruption, so their plan is to use the blockchain-as-invention, internally, in permissioned fashion.  The other is the smart contract heavy crowd led by Ethereum, which hope to build a generalised computing platform.
Right - similar claim - at the hypothetical.
So the claim here would be that if our VW has a blockchain, then we can't stop others from evaluating the EPA program on the fly and then doing other computations to trick the vehicle into compliance mode. Which is precisely the problem that got us here in the first place.
Right, ok, I see that.  That's a serious limitation of 'trustworthy' computing.  You get your calculation but your inputs can be gamed in real time without you knowing.  Fail.
To get around this the EPA would have to certify every other program on the blockchain, and then we'd be back to walled garden ... might as well use the old methods.
Right.  It doesn't take many changed assumptions to collapse the blockchain design.  It is quite an elegant result.
Thanks!  Thinking of the gaming attack has certainly clarified my thinking.

@_date: 2015-10-05 12:16:23
@_author: ianG 
@_subject: [Cryptography] blockchain and trustworthy computing 
I'm not pushing for a BIP to upturn mainnet, rather I'm thinking about the world of private blockchains run for local or specific purposes. I.e., the invention of the blockchain, not the one that runs BTC.
Is that still disagreeable?
Right, something like that.  To extend my (somewhat challenged) analogy of the VW problem, a blockchain can be a private permissioned one that is running inside the car, on all the distributed brain chips.  EPA is one of the signatories that can access, so it dials in and loads up a program (e.g., smart contract) and does some computations.
In this sense, the EPA is an SPV client user.  It's relying that the car has a proper private blockchain on it, but once past that hurdle, it's free to run trustworthy programs on there.
(See other post to Natanael that destroys my argument...)
Yep - big problem with mainnet.  Fundamental problem here is that energy enjoys economies of scale.  If all you are doing is converting energy into zeroes, then next door to a Chinese powerplant is the best deal going.  If anyone's read the novel Accelerando, there are better alternatives coming, for now we're stuck with China.
But that's another debate - I'm specifically looking at other blockchains, used for private or multiple purposes.
What is the difference between trustless computing and trustworthy Have we even defined the terms to wide consensus satisfaction?  I'd be surprised if so, we haven't even got a good consensus on trust, how can we negate- or -worth something we don't have shared understanding of?

@_date: 2015-10-08 23:03:41
@_author: ianG 
@_subject: [Cryptography] Spec for SSLv1 
Snarky question:  Who grumbled?
More or less snarky:  Are they grumbling today on the TCPINC list?

@_date: 2015-10-09 15:52:55
@_author: ianG 
@_subject: [Cryptography] blockchain and trustworthy computing 
Ahhh... I get the distinction.  Of course, ... how slow am I.
I'm hypothetically envisaging a car with many CPUs in IoT form, each interfacing with their particular device (brakes, carburretion, entertainment, locks, etc) but also running a blockchain to handle (Maybe this is what 21c are trying to do, maybe not.  IBM are also toying in this IoT direction, as are others.)
Any signatory can put programs (smart contracts) into the car's blockchain to get run.  By 'proper' I mean it is certified by EPA or other agency, according to some procedure / requirements they design, like NIST or CC approvals.
In the same way that the SPV knows that things were done accurately - it sends in its transactions, gets them back, is happy.  As to what is happening under the hood, for that it relies on various institutional defences (auto-manufacturer honesty, annual vehicle checks).  In this sense, trust.
Alternatively, the rules could say that each agency that is interested must also put its own full node into the chain.  Now it gets to verify directly.  This node would build on the tradition of the odometer.
Indeed - note however that I didn't say "unlimited" as there are no such things as unlimited economies of scale, just a mix of economies and diseconomies to scale, which kick in at different points.
So, obviously, the hard limit to mining currently would appear to be the max output of the power station that one is sited next to.  The soft limit would be how much of that 100% the miner could ease across to mining, depending on the various business and economic factors.
(This reminds me of what we wrote in that hated article of 2011 on Gresham's Law and Bitcoin, "Building on the scenario of misallocated power costs by hobbyists or other users, what possibility is there for a simple power cost of zero?")
What you're saying is that the soft limit at any power station is well below the total mining needs, so decentralisation is still present.  Is there any research or investigation on the numbers on this new China Right, I see the distinction.  But the needs that both are trying to fulfill are more or less the same, albeit through different approaches.   Trustless or verifiable computing dominates trustworthy computing, in that it has at least one less component to trust.
So maybe my claim is more that Blockchain has eliminated the need for trustworthy computing by moving to verifiable computing?
ps; this is all to skip aside from the criticism that the term 'trustless' suggests a perfect result, which we know as inadequate, even a dangerous assumption.

@_date: 2015-10-09 17:27:29
@_author: ianG 
@_subject: [Cryptography] Reproducible results, scientific method, 
On that, James Randi makes for some interesting reading including on social engineering (the security connection):
Another group that is well known to be easy to fool is doctors.  They are the number 1 target for boiler room operations, so I hear.
Hallelujah to that!

@_date: 2015-10-10 16:40:18
@_author: ianG 
@_subject: [Cryptography] Collisions w/SHA-1 ~$100,000 TODAY 
Can anyone provide a pointy-eared boss description of what a *freestart* collision is?

@_date: 2015-10-10 19:14:08
@_author: ianG 
@_subject: [Cryptography] Collisions w/SHA-1 ~$100,000 TODAY 
Thanks to Scott and Philipp for (almost identical) answers.  That I I guess the next question would be, how long we expect the freestart limitation to last as a meaningful barrier to full SHA1 collision attacks.
It is fascinating to watch.  11 years after the Shandong Hashquake, SHA1 is still saying "I'm not dead yet!"

@_date: 2015-10-13 21:13:26
@_author: ianG 
@_subject: [Cryptography] (fwd) a tale of software maintenance: OpenSSL and 
List:       openbsd-tech
[Download message RAW]
In case you need an OpenSSL anecdote to scare your co-workers with...
Many of you may remember from your crypto class in college that DES has 16 'weak' keys that have group-like properties; check wikipedia for a longer explanation.
These are not generally considered a problem: in any sane situation, keys for DES are generated with a CSPRNG (cryptographically secure random number generator).  Since there are 2^56 possible keys, the odds of hitting one of these is 1 in 2^52.  That's "both you and your computer were--independently--struck by lightening this year" territory.
So, the *serious* recommendation by the cryptographic community is to ignore the possibility of getting a weak key: don't check for them. If you get one either
a) your random number generator is bad, like *Debian* bad, and
    you're *totally screwed* already: checking for weak DES keys is
    putting new vinyl on the Titanic's deck's chairs, OR
b) wow, you're unlucky!  Sorry about the lightening; you should buy a
    lottery ticket! ...but don't worry, the attacker was just going to
    brute force your DES keys anyway!
You're more likely to get the check wrong than to ever hit one of them.
Huh, that's a funny way to phrase it...
So OpenSSL has _optional_ code to reject attempts to use weak DES keys.   It, sanely, is *not* enabled by default; if you want it you have to compile with -DEVP_CHECK_DES_KEY.
Last Thursday it was reported to the openssl-dev mailing list by Ben Kaduk that there was a defect in this optional code: it had a syntax error and didn't even compile.  It had a typo of "!!" instead of "||":
      if (DES_set_key_checked(&deskey[0], &data(ctx)->ks1)
          !! DES_set_key_checked(&deskey[1], &data(ctx)->ks2))
This syntax error was present in the _original_ commit: the code in the  had _never_ been compiled.
This code was commited in 2004.
(stop screaming and catch your breath)
The LibreSSL response?  The  and code in them have been deleted.
The OpenSSL response?  The code... that in 11 years had never been used... for a deprecated cipher... was *fixed* on Saturday, retaining the

@_date: 2015-10-21 16:42:40
@_author: ianG 
@_subject: [Cryptography] Other obvious issues being ignored? 
I'm guessing this relates to older military shared key systems where misuse of the system could provide substantial clues as to the 'codenooks' of the day.

@_date: 2015-10-23 16:18:57
@_author: ianG 
@_subject: [Cryptography] "We need crypto code training" and other obviosities. 
Taking on the Devil's Advocate here!
Well, pilots and surgeons do have rather a long history of killing people.  Or worse, leaving them maimed and angry...
As has been pointed out frequently albeit unpopularly, the record of people dying and being harmed from bad crypto is not written.  It's popularist, anecdotal and often self-serving for the industry, but basically it is as credible as tabloid news.
(I'm being serious here - the security industry talks a good talk but has little evidence written down to back up their noise.  This is why I write a list of CA threats at  so that at least in my own little corner we have something written, however A challenge!
So, you say that.  But what do you mean, in practical terms?  Some * I'm no academic, but I don't think I've come across a course in * SANS doesn't run a course like that (I'm told), which is kind of an indication that there isn't an easy commercial play here.
* There are a few books, a few guides, and so forth but I'm not keen to point at them yet.
* then there is every Uni's crypto course.  One, per Uni, it seems. Which might or might not mix in some programming.  I can't think offhand of a University which is renowned for teaching great crypto coding.
I'm not saying we don't need such a thing.  I am suggesting we haven't got such a thing - so how confident can we be that it is the right answer?
If I think of my crypto students over time, none of them were ever taught a formal course in crypto programming.  In all cases it was like, here's this problem, here's some tips on implementations, go at it.  My last student managed Salsa/ChaCha, Poly, RNG and a DH key exchange, building on about 1 years worth of CS with Java and no prior crypto experience - using papers and direction and net and sweat and tears. Not from "a course" athough she did return from internship and do the formal university crypto101 course afterwards.
If I had to have a stab at what this means, I'd say that crypto programming is 90% good programming, 9% good business understanding and 1% crypto.
Just to be flagrant, of course!  What say others?
If you've got seriously good programming skills you can pick up the crypto as long as we stick to black boxing.  Sure, if we get into some of the crazier stuff, all bets are off, but that's not recommended even if you can understand the stuff.  Most or all of the things we should be using are black boxed.
On the other hand if you've got great crypto skills and aren't a good programmer ... well, you're screwed.  IMHO.  If you've got no business skills, we're screwed.
I'm not saying such a thing as a crypto programming course or whatever "training" means isn't needed - or wouldn't make a difference - just that we don't seem to have it, so I'm a bit skeptical that we got as far as we did if it was entirely necessary.

@_date: 2015-10-23 22:08:42
@_author: ianG 
@_subject: [Cryptography] Other obvious issues being ignored? 
All this is true.  It has always been true, ever since RSADSI convinced Netscape that it couldn't be the one true signer, oh no, that had to be an open market, led by the company that RSADSI or its employees started...
This problem is *not a security problem* but *an institutional problem*.
The space is formed of a number of interlocking institutions that have concreted power relationships and accreted power in order to hold the browsing public to a commercial deal.  In one sense this is good because it got some security out to the users, which had to be paid for in some sense or other.
In the wider sense however, the browsing public is totally screwed because the model is early 1990s at best, and has had to carry the weight of 20 odd years of architectural folly.
But, the edifice has been built.  You can no more convince Mozilla to do something different than you can convince the whitehouse to stop sending out drones to solve its foreign policy itch.
In a sense, the proof that this is an institutional problem not a security problem is google - it was the only organisation that was on hook for all parts of the problem. It therefore had the liability at all levels, and the incentive to move all levels. Nobody else had that, nobody else could move, no matter how screwed they were.
The question then isn't how to fix the security.  We've always known how to do that.  The question is how to fix *the institutions* or more precisely how to re-wire them such that they work for the user.  Or at least, so they don't screw the user over.  That I don't have the answer to.

@_date: 2015-10-24 12:08:02
@_author: ianG 
@_subject: [Cryptography] "We need crypto code training" and other 
For your imagined sins - perhaps you or anyone could post a topics list?
In the sense of, if we knew what a crypto programming course looked like, we could possibly predict its impact better.  Something like:
Day 1: hashes & HMACs
Day 2: symmetric ciphers
Day 3: public key crypto
Day 4: key exchange

@_date: 2015-10-24 12:21:41
@_author: ianG 
@_subject: [Cryptography] Security Standard for Safety Critical Software 
(I don't know of one off hand, but see below...)
The theoretical problem with dealing with a security standard is that it is a byzantine field.  That is, the attacker is aggressive, can read our documents including our standards, and model his attack to suit.
As a consequence of this, the attacks evolve.
Following OODA, the attacks can evolve faster than the standards can evolve.  In effect, in such an environment, good security means paying more attention to what the attackers are doing this year, not last decade.
In such an environment, there isn't a passive enough base to write and rely on a standard in the long run.
Cyber-security especially is becoming much more dynamic, over the last 5-10 years or so.  One can imagine other environments where it is mostly passive in technique such as home robbery.  But it's just not where Internet and cyber security are at the moment.

@_date: 2015-10-24 18:52:21
@_author: ianG 
@_subject: [Cryptography] attacks on packet length may be surprisingly good: 
Phonotactic Reconstruction of Encrypted VoIP Conversations:
             Hookt on fon-iks
In this work, we unveil new privacy threats against Voice-over-IP (VoIP) communications. Although prior work has shown that the interaction of variable bit-rate codecs and length-preserving stream ciphers leaks information, we show that the threat is more serious than previously thought. In particular, we derive *approximate transcripts* of encrypted VoIP conversations by segmenting an observed packet stream into subsequences representing individual phonemes and classifying those subsequences by the phonemes they encode. Drawing on insights from the computational linguistics and speech recognition communities, we apply novel techniques for unmasking parts of the conversation. We believe our ability to do so underscores the importance of designing secure (yet efficient) ways to protect the confidentiality of VoIP conversations.
My emphasis - I'd love to see some examples... iang

@_date: 2015-10-25 00:29:48
@_author: ianG 
@_subject: [Cryptography] Other obvious issues being ignored? 
The problem with talking about RNGs is that they are in a special space in cryptography - there is no easy black box testing.  No trial params we can put in to show it is working.  This makes them hard to get right, hard to prove and *hard to teach*.
Not surprisingly, opinion has changed a lot over time as to how to deal with RNGs.
Right now, the favoured approach is:  don't do it.  Considered & better advice is to leave RNGs to the core heavy specialists who can spend a lot of time on getting it right.
Or as I say it:  use what your platform provides [0].  Others say it more simply:  use /dev/urandom [1].
Now, that disposes of the second point - you shouldn't be making up your own RNG anyway.
The first point - depleting /dev/*random - is a good one because it actually points at a flaw in the interface for Linux's /dev/random. Linux still sticks to the old concept of delivering "entropy" when our thinking has moved on dramatically since those days.  Now we deliver "cryptographically secure random numbers" and plenty of them.  This is a strictly wider set than "entropy" but also a more tractable one because it avoids the rather traumatic implications of entropy, eg, "running out."  So, in short, Linux needs to update to the modern thinking here and telling people to program around Linux is probably wasted effort [2].
tl;dr - use /dev/urandom (Linux's /dev/random is broken if it runs out).
[0] [1] [2] I think it was FreeBSD that first fixed the /dev/*random to do a pure CSRNG, and I for one hated it.  It took me probably a decade to figure out that it was indeed the right thing to do...

@_date: 2015-10-25 12:42:27
@_author: ianG 
@_subject: [Cryptography] composing EC & RSA encryption? 
The recent "distancing" news from NSA concerning ECC and their view that QC is coming sooner [0] rather than later has somewhat upset things.
Before, we seemed comfortable with the trend to ECC as the future.  Now, it's not clear.
Yet bets have to be made - a protocol invented today should probably want to survive between 10 and 20 years if we draw from the OODA loop of good upgraders versus lazy laggards.  I'm at the moment wondering which way a new OpenPGP PK algorithm would go, which should preferably last even longer because of its traffic matter.
Is it possible / reasonable / practical to compose the two together into one algorithm?  And thus achieve some sort of agnostic defence against future developments that favour a break in one over the other?
An EC/RSA signing form is easy - just make one signature in RSA and one in EC, and we're done.  At least at a trivial level, this works, although I imagine it might be possible to do better - interesting work for a grad student perhaps.
But what about encryption?  Doing that in parallel makes it weaker, it would have to be done in serial.
If one encrypts using RSA and then EC, does that run into problems with Does the ordering matter?  If there is an easy break in EC would it matter much if it were the first or the second?
Or, do we copy the triple DES construction and do EC-RSA-EC?  Chosen that way because the EC part is faster...
As a side comment, it does rather seem that there will be pressure to preserve the key length as an option in PK protocols.  Which is IMHO unfortunate because it opens the door to weak keylengths in future downgrade attacks instead of an orderly length upgrade process to deal with the lazy laggards problem.  But that fight seems unwinnable for the [0] a confirming commentary on NSA position is here:

@_date: 2015-10-25 22:38:00
@_author: ianG 
@_subject: [Cryptography] composing EC & RSA encryption? 
That's it.  Of course.
Right.  Actually someone (Jon?) has in the past suggested that the message digest of the two randoms is better than the XOR.  But this is a detailed refinement.
So, let's say a WG is arguing as it does for the right to have multiple public key algorithms.  "We simply must have ECC and RSA because you never know what is going to happen..."
Could we cut that gordian knot by saying, you can have your cake and eat it too:
Cake(x) = EC(x) ^ RSA(x)
ps; I'm ignoring the size differences, and assuming that the RSA is padded out correctly, etc.

@_date: 2015-10-28 12:31:21
@_author: ianG 
@_subject: [Cryptography] [FORGED] The Energy Budget for Wireless Security 
Yes - this is another reason why I suggest that the protocol designer take such decisions away from the users.
The benefit for one user in "choosing better" does not outweigh the cost for all users in spinning wheels and choosing worse.

@_date: 2015-10-28 15:07:39
@_author: ianG 
@_subject: [Cryptography] composing EC & RSA encryption? 
My worry really isn't QC - the notion that the aussies can build a million qubit machine in 5 years only makes sense if they're in Queensland, where we have it on good authority that the dope they grow is better than God's Dope [0].
My real worry is that the working groups are going to *add more algorithms* and thus make the protocols more brittle.  They are not going to take away.  They've been aware of this flaw in their process for a decade now [1], but still have no real consensus on how to deprecate a cipher suite and rollover to modern stuff.
Thought for today:
     Every RFC Security section has to have the rollover plan.
Point being that ad hoc thinking & talking about problems in the future hasn't worked.  But maybe if we force the RFC authors to make a stab at it in words, we can see what works and what doesn't in 2025?
Yep, same as Jerry.
While we're having fun here [2], one more:  Adi Shamir's secret sharing is supposed to be information theoretic secure - not just computationally secure.  Is there mileage here?  How does it perform against QC?
[0] inside reference to an old popular aussie song
[1] Steven M. Bellovin and Eric K. Rescorla. Deploying a new hash algorithm. In Proceedings of NDSS '06, 2006.
Abstract: The strength of hash functions such as MD5 and SHA-1 has been called into question as a result of recent discoveries. Regardless of whether or not it is necessary to move away from those now, it is clear that it will be necessary to do so in the not-too-distant future. This poses a number of challenges, especially for certificate-based protocols. We analyze a number of protocols, including S/MIME and TLS. All require protocol or implementation changes. We explain the necessary changes, show how the conversion can be done, and list what measures should be taken immediately.
[2] The penny drops. This is why the NSA is illegally harvesting all the Internet it can get. Because it will then be able to go back and break all those RSA keys while the machines are waiting for hi-prio traffic ... and decrypt huge continents-worth of secret stuff ... of people who thought they were secured. Run the traffic through the word-list detector and build a bigger graph of who was a bad boy 20 years back...
So, it's willing to run the risk of breaking the law because this bounty won't be around forever. It ends once everyone realises all their past comms is going to be harvested.
Problem is, this kicker also kicks them - if the Chinese & Russkies are busy hoovering up all the traffic now from borked routers around Washington DC and all military companies, then it will bite back much That's why the announcement has a sense of panic - the USG is in the worst position of all because its data is more valuable. Hold your breath while USG moves to lower the boom of ITAR on all QC.
Hence a further (advanced) question isn't just over RSA (the bulk) but also over RSA + PFS (protocol forward secrecy) modes...

@_date: 2015-09-02 16:47:59
@_author: ianG 
@_subject: [Cryptography] NSA looking for quantum-computing resistant 
Seems to be some discordance on that one - any chance of a cite?

@_date: 2015-09-02 17:19:42
@_author: ianG 
@_subject: [Cryptography] NSA looking for quantum-computing resistant 
Here's my summary of the facts & conclusions (?) so far.  As always looking for correction.
1.  NSA has mandate to protect USG agencies.  It also has a mission to breach everyone (else) but let's ignore that for the moment.
2.  NSA knows more about quantum than anyone else, in the sense that it has the budget to know, and has been spending that budget.
3.  (we suspect/agree) NSA is worried about quantum.
4.  NSA guidelines protect out to a 25 years (h/t to Ryan).  So if NSA can't rule out a quantum attack in the 25 year++ horizon, then they have to protect against a quantum attack.
5.  Current rule of thumb is that a quantum attack reduces the bit-strength of an algorithm by the square-root - much like a birthday 6.  So in short, take previous minimum strengths (128 baseline, etc) and double (to baseline 256, etc).
Commentary,  So, what does this mean for everyone else?  Not a lot.
The reason it doesn't matter is this:  WYTM?  The NSA is mandated to protect US government agencies and not the rest of the world.  Following the normal approach to threat modelling, they built their list of threats, not your list of threats [0].
Their list of threats include a very well funded Chinese / Russian attack.  E.g., state of the art, monster-grade quantum supercomputer with a billion dollar price tag.  Which is only going to be used against the juciest of targets - the USA.  Let's call this the Bletchley Park Attack (h/t to Tom).
Our list of threats doesn't include that computer or that government. Because, if any government wants our data, they'll spend $1000 to hire a local thief, not $1000000000 to deploy their monster machine on mere civilians [1].
The NSA, by its own methodology and logic and customer, cannot afford to be wrong on this.  Everyone else can afford to wait, and we can also afford to be wrong.  Wait and see.  When ordinary people (botnet operators) can buy quantum computers that can crack keys, we'll know about it.
That's not to say that people won't upgrade.  All the other governments and supra-national orgs like IETF will fall into line with NSA's threat model because their approach is best practices, not security modelling.
But there's no need, there's no hurry, and if you spend a dime on it, you wasted that dime, and the opportunity to spend it on your real threats.
[0] the key flaw in our reasoning is quite old:  using someone else's threat model and not realising it's wrong for you.  A common failing. Obligatory old post asking, What's your Threat Model?
[1] XKCD on Security is a better reference

@_date: 2015-09-02 18:15:01
@_author: ianG 
@_subject: [Cryptography] NSA looking for quantum-computing resistant 
Hmmm, which could be considered a weak keys test, if one was worried about the generality of the result.
Excellent summary, thanks!

@_date: 2015-09-03 19:25:14
@_author: ianG 
@_subject: [Cryptography] Feedback welcome on an idea 
There are two problems with deniability that I'm aware of:
One is the OTR ("off-the-record" a protocol for Jabber) trap in which there is some conceptual claim that because a message can be changed, you can claim it is therefore "not yours".  This actually makes your position worse if ever you needed to deny something, so don't do that.
The second is K6, or the need to be able to operate the thing under stress.  Kerckhoffs was writing about soldiers using pen & paper ciphers so he was trying to make the process easy.  If it wasn't easy, they would just write the messages in the clear, which is what you do when someone is shooting at you or even when the officer is shouting at you.
In the narrower context of deniability, it doesn't work if the method, or initiation of it, won't work under stress.  If you have to remember to do it, you're in trouble - humans don't remember well under stress.
The general principle here is that a good security mechanism is like a seatbelt - it's there if used continuously, it's useless if you have to remember to put it on as you see the accident coming.

@_date: 2015-09-04 23:01:42
@_author: ianG 
@_subject: [Cryptography] Checking for the inadvertent use of test keys 
This is becoming a good argument not to do this in Peter's code.  It will never do what is wanted to any strong degree, and will invoke all sorts of visions for improvement.  Until that day when someone dicovers a very cunning hack, and the wrong sort of fame starts knocking.
Kick it up the stack.  Make the caller deal with it, or carry the

@_date: 2015-09-19 15:47:50
@_author: ianG 
@_subject: [Cryptography] millions of Ashley Madison bcrypt hashes cracked 
Yes, I think you're right, but:
People pointing out you can't get an SSN is interesting counter-evidence.  And, there is plenty of evidence that people are screwed by their banks, and not a few court cases.
Phishing has been running wild for a decade now.
So why do we think AshMad is a first?  I think it is because it is the first time we've got a readily identifiable tribe as victim, rather than dispersed individuals, and, the damages are not in dispute.
A continuing-forever problem we have with security as a business is that it is very hard to put a number on the damages.  Without knowing what you are trying to achieve, it's basically a voodoo art to know how much to spend.  If you go out and start calculating direct damages as incurred by businesses for hacks, the information is incredibly sparse.
Now we see people popping up and phoning their divorce lawyer, or suiciding in at least one case.
That other factor - that we ignore damages to individuals, can be put down to the syndrome of "it won't happen to me."  Which is actually rational.  If the event is so rare that it becomes news, then it likely won't happen to you.  What we should be concerned about is the event that is so frequent it doesn't become news.
Right.  Which brings to mind that infamous Sydney Morning Herald headline after the Starr report came out, around 1998:
"Thank god we got all the convicts and they got all the puritans."
Yep - a question.
Heartbleed had an effect.  If you believe in improving security, does this mean you believe in more Heartbleeds?

@_date: 2015-09-19 15:58:55
@_author: ianG 
@_subject: [Cryptography] The default password of '1234' 
I've been calling that click-thru syndrome since forever.
The essential point is correct - the web browsing model is broken because we taught the users to ignore it.  It can't be remedied.  This is why every CA is the same, and there is race to the bottom in CA security.
But no, it can't be fixed, only papered over.  There are these colours available at your local wallpaper store:
   * everything has to go HTTPS (yay say the dodgy CAs) or
   * we have to replace the browser security model
    (over our dead bodies, say the CAs, and they've fought
     that battle to prove it with CABForum).
Which is why I started promoting HTTPS Everywhere in 2005.

@_date: 2015-09-29 09:44:40
@_author: ianG 
@_subject: [Cryptography] The default password of '1234' 
Yes, on all HTML pages, and yes, the bottom line is that many pages will be using certificates that are self-signed and are thus vulnerable to MITM.
Expect?  Well, in crypto we do what we can for free..  It's beyond clear that we can do encryption for free.  (See Ryan'd thread on Cycles for an example, or see the TCPInc project, or SSH or Skype or modern chat programs, etc.)  So the principle is that we should always encrypt.
What we can't do "for free" is authentication.  For that we need application and/or user help.
(old timers on the list can stop reading now, you've seen this discussion a thousand times...)
The perspective is complicated and historical.  HTTPS was designed to be a complete solution - assuming that you only used HTTPS.  But the day it was tried way back in 1994, the "high performance" sites of the times discovered that it actually cost a lot in performance.  HTTPS wasn't free.
So they bifurcated the web site.  They put only the secure stuff on HTTPS and went back to the non-secure stuff being on HTTP [0].
To solve this problem the user was "taught" to see the difference and secure herself.  Except, the teaching didn't stick.  Now, this was particularly exacerbated because the HTTP site started first, and only after the user was "qualified" was she eased across to HTTPS (to enter credit card, etc).
Which means that the second HTTPS phase was now vulnerable to a downgrade attack to HTTP, which succeeded if the user didn't follow her How often does the user not follow her "training" ?  Well, studies seem to put the error rate at around half [1].
What does that mean?  Well - it means that in the presence of on attack, there is a 50% chance the user won't notice, and enter her secret info into a scammer's site.
50% failure means in cryptographic terms 1 bit security, so HTTPS as delivered in browsers, provides about 1 bit of security, if one follows the above logic.
Hence phishing is an attractive attack, which first started in 2003-ish against HTTPS [2].
The only way out of this is to go HTTPS Everywhere - and then force the browser to correctly deal with unauthenticated versus authenticated sites.  This puts the website back to being one site not two, and thus authenticated from the start, and therefore the browser itself can see the downgrade attack.
(There's a bit more reasoning based on browser politics which we can leave for another day.)
[0]  And, the user was taught that she didn't need encryption for the other stuff.  And, it was declared to be best practices.  And, and and. What was secure and what was non-secure is an interesting other discussion we can ignore for now, the point is that there were now two channels where the security model assumed one.
[1] I'm approximating here, so I'll go with half, you can plug in your own numbers.
[2] to be fair, phishing is a lot harder than the above 1 bit claim above.  I think we'd be safe saying that HTTPS in secure browsing delivers no more than about 10 bits of strength.  But also in cryptographic thinking, we're typically doomists and end-of-world-ists, so 1 bits is actually about fair for a lower bound for security planning.

@_date: 2015-09-30 23:22:10
@_author: ianG 
@_subject: [Cryptography] blockchain and trustworthy computing 
============================== START ==============================
Thought experiment.
The blockchain [0] provides a way to do verifiable computing [1].
Popularly under the tag of "smart contracts" these little scripts can be executed on every node, and the ability of the nodes to come together and find consensus on the state or results provides a way to not only compute, but also know that we have verifiably computed.
Verifiable computing was a long treasured goal - but proved elusive outside an academically restricted set of assumptions.
If we have verifiable computing finally in the blockchain - a thesis - does this mean we now have a trustworthy computing platform?
To cast back to prior efforts at trustworthy computing platforms, they all more or less failed because they moved the pieces around the board, they didn't actually provide a neutral, independent and fair definition of what was trustworthy.  E.g., creating special computing cells within CPUs created the concern that those who created the cells could always manipulate the results.  Hence, even if you believed Intel wouldn't cheat, there wasn't the combined faith in the system to incentivise the many large investments that inventors hoped for to make it fly.
In practical terms, if I run a personal blockchain on my laptop, home computer, TV, android tablet, iPhone, and xWatch, have I created a trustworthy computing platform?  In the process, has the hardware-I-Don't-Trust conundrum been solved?
Or, for a more hypothetical example, if I have an EPA-tester running on the blockchain calculating in turn over the various cars that are providing the nodes, does this solve the VW problem?
[0] I'm talking about the conceptual idea, as implemented in various techs.
[1] Some more found here:

@_date: 2016-04-05 02:51:18
@_author: ianG 
@_subject: [Cryptography] Transcript Collision Attacks: Breaking 
*/Transcript Collision Attacks: Breaking Authentication in TLS, IKE, and SSH/*(Karthikeyan Bhargavan and Gatan Leurent), In Network and Distributed System Security Symposium (NDSS), 2016.
AbstractIn response to high-profile attacks that exploit hash function collisions, software vendors have started to phase out the use of MD5 and SHA-1 in third-party digital signature applications such as X.509 certificates. However, weak hash constructions continue to be used in various cryptographic constructions within mainstream protocols such as TLS, IKE, and SSH, because practitioners argue that their use in these protocols relies only on second preimage resistance, and hence is unaffected by collisions. This paper systematically investigates and debunks this argument.
We identify a new class of transcript collision attacks on key exchange protocols that rely on efficient collision-finding algorithms on the underlying hash constructions.
We implement and demonstrate concrete credential forwarding attacks on TLS 1.2 client authentication, TLS 1.3 server authentication, and TLS channel bindings. We describe almost-practical impersonation and downgrade
attacks in TLS 1.1, IKEv2 and SSH-2. As far as we know, these are the first collision-based attacks on the cryptographic constructions used in these popular protocols.
Our practical attacks on TLS were responsibly disclosed (under the name SLOTH) and have resulted in security updates to several TLS libraries. Our analysis demonstrates the urgent need for disabling all uses of weak hash functions in mainstream protocols, and our recommendations have been incorporated in the upcoming Token Binding and TLS 1.3 protocols.

@_date: 2016-04-07 02:05:35
@_author: ianG 
@_subject: [Cryptography] the Herival tip - hut 6 
While researching something else, I came across David Rees, 1918 - 2013.
The German operators of the Enigma machines were told which three rotors and which settings to use each day, but they had to choose the starting positions of the rotors and indicate their choices via the first three letters of their first messages. John Herivel, another of Welchman's recruits from Sidney Sussex College, predicted in February 1940 that some German operators might use short cuts that could be exploited by the Bletchley Park codebreakers.
For three months, this insight produced no result; but in May 1940 some of the German operators began to make the predicted mistakes, and David and his fellow codebreakers were able to use the technique known as the "Herivel tip" to break Enigma ciphers for some critical months from May 1940. The initial breakthrough generated roars of excitement in Hut 6, but it had to be kept secret for at least 40 years. David remarked in 2000 that "the Herivel tip was one of the seminal discoveries of the second world war". He thought that Herivel should have received more credit.
Herivel's great insight came to him one evening in February 1940 whilst he was dozing in front of his landlady's fire. It was that stressed or lazy operators who had set the rings when the rotors were in the machine, may then have left ring setting at the top, or near the top, and used those three letters for the first message of the day.[13]
For each transmitted message, the sending operator would follow a standard procedure. From September 1938 he would use an initial position to encrypt the indicator, and send it in clear, followed by the message key that had been enciphered at that setting. Suppose the initial position, the ground setting (German: Grundstellung) was GKX for example, he would then use Enigma with the rotors set to GKX to encrypt the message setting, which he might choose to be RTQ; which might encrypt to LLP. (Before May 1940 the encrypted message setting was repeated, but this makes no difference to Herivel's insight.) The operator would then turn his rotors to RTQ and encrypt the actual message. Thus the preamble to the message would be the unencrypted ground setting (GKX) followed by the encrypted message setting (LLP). A receiving Enigma operator could use this information to recover the message setting and then decrypt the message.
The ground setting (GKX in the above example) should have been chosen at random, but Herivel reasoned that if an operator were lazy, or in a hurry, or otherwise under pressure, he might simply use whatever rotor setting was currently showing on the machine.[12] If this was the first message of the day, and the operator had set the ring settings with the rotors already inside the machine, then the rotor position currently showing on the machine could well be the ring setting itself, or else be very close to it. (If this situation occurred in the above example, then GKX would be the ring setting, or close to it).
Polish cryptographers used the idea at PC Bruno during the Phoney War.[14]
The effect predicted by Herivel did not immediately show up in the Enigma traffic,[13] however, and Bletchley Park had to continue to rely on a different technique to get into Enigma: the method of "perforated sheets", which had been passed on by Polish cryptologists. The situation changed on 1 May 1940, when the Germans changed their indicating procedure, rendering the perforated sheet method obsolete. Hut 6 was suddenly unable to decrypt Enigma.
Fortunately for the codebreakers, the pattern predicted by the Herivel tip began to manifest itself soon after on 10 May, when the Germans invaded the Netherlands and Belgium. David Rees spotted a cluster in the indicators,[12] and on 22 May an Air Force message sent on 20 May was decoded, the first since the change in procedure.[16] The Herivel tip was used in combination with another class of operator mistake, known as "cillies", to solve the settings and decipher the messages.[13][17] This method was used for several months until specialised codebreaking machines designed by Alan Turing, the so-called "bombes", were ready for

@_date: 2016-08-20 07:04:07
@_author: ianG 
@_subject: [Cryptography] BBC to deploy detection vans to snoop on 
In other words, we define what we can fix, then we re-work the threat model to ignore that which we can't fix.  Then we teach the users that this is their problem, their fault, they've been socially engineered.
Right.  There is this incredible bias in cryptology to even ignore actual results that breach our shared acceptable threat model.  This I posted last year and it disappeared without a trace ... which was really odd - the authors claim to have actually recovered encrypted conversation from so-called metadata n VoIP.
-------- Forwarded Message --------
Phonotactic Reconstruction of Encrypted VoIP Conversations:
              Hookt on fon-iks
In this work, we unveil new privacy threats against Voice-over-IP (VoIP) communications. Although prior work has shown that the interaction of variable bit-rate codecs and length-preserving stream ciphers leaks information, we show that the threat is more serious than previously thought. In particular, we derive *approximate transcripts* of encrypted VoIP conversations by segmenting an observed packet stream into subsequences representing individual phonemes and classifying those subsequences by the phonemes they encode. Drawing on insights from the computational linguistics and speech recognition communities, we apply novel techniques for unmasking parts of the conversation. We believe our ability to do so underscores the importance of designing secure (yet efficient) ways to protect the confidentiality of VoIP conversations.
Hell yeah!
There was a company called Zero Knowledge Systems back in around 1999 that bombed.  One of the reasons it bombed was because it insisted that its message streams would be 128kbps continuous.  At the time that was a non-starter.  But now, we need to re-evaluate such techniques.
For my part I'd say we should be obfuscating, not eliminating. Bandwidth and so forth is still an issue.  But this becomes an open research topic, I suspect.

@_date: 2016-02-16 01:19:15
@_author: ianG 
@_subject: [Cryptography] 
=?utf-8?q?of_Suite_B?=
A careful reading will suggest two things.  Firstly, the NSA is now removing choice where it can - one only curve, one only SHA, one only AES.  It's also looking askance at RSA/DH family, and askance at numbers other than 3072.
Why?  Interoperability.  Which means, in the NSA's assessment, the ability for more people to connect over the same cryptostuff delivers more security than the fantasy benefit of encouraging people to play their security like they study for their multiple choice exams for a SANS G-thing.
(my words, not theirs :)
Next - NSA are planning a replacement for this CNSA.  It's an interim minimised suite to deliver just enough oopmh to get the NSA into a clearer information frame than today.  I'll go out on a limb and say they're planning to mandate a new suite in 7 years time, which is about the right balance between the deployment cost of now, and the 20 year survivability they announce in the document.
These two developments are as it should be:  remove choice as much as possible from the users.  The NSA are the experts in what a suite should look like, none of their users knows better (by law, as it happens).
And, plan to replace the entire bloody suite in about 7 years, because that's the time required for our knowledge to tip from good through rusty to out-of-date.

@_date: 2016-02-16 01:43:16
@_author: ianG 
@_subject: [Cryptography] Proof that the NSA does not have a quantum 
Well, not transferred at least.  But the moment they are transferred, you can expect all hell to break loose.  Hardly your average NSA clandestine operation ;)
If NSA had a QC machine it would be the nation's best kept secret.  It would be like Engima, Bletchley Park, Purple and all that.  Massive rings of control surrounding it to make sure it never never EVER leaked out, whole departments running deception operations.
Secondly, $500m is chumpchange.  Thirdly, the NSA can hack into so many bitcoin computers it could probably pick up a few mil by lunchtime.
OK, that's not $500m, but the NSA is so deep into the world's financial system that it can practically create money.  Any time it needs funds all it has to do is steal the pots of various known and identified vulnerable parties - drugs barons, oligarchs in exile, US politicians who haven't approved NSA budget increases, Syrian despots.  The usual It will *never happen*.  The other dirty secret is that these supposedly anonymous and private payment systems (cof) have always been well supported by *some* agencies but not by others.  One of the reasons that e-gold went quietly into the night is that digging too deep found too many "interesting characters" from both sides of the tracks.  All of the spooks everywhere have a vested interest in keeping things like Bitcoin alive.  It's not personal, moral or legal, it's just the way the great game works.
The notion that someone was busted for a poker game raises just one question in my mind - what was he really being busted for?

@_date: 2016-02-22 23:51:57
@_author: ianG 
@_subject: [Cryptography] Cyber Command - we have nothing to fear, but... 
... Most examples of idiocracy there are classified and can't be blogged about, but to illustrate how truly bad the problems are, let me share an unclassified story of idiots and red tape.
A senior network exploitation operator noticed one day that the organization had deployed a large number of devices on an unclassified network.  He said to himself:
     Wow - I know our targets frequently misconfigure
     these devices and leave default services enabled.
     I wonder if our contract administrators staffed
     by the lowest bidder have done this too?
The operator decided to check, but realized the pen testing a DoD network without authorization could be a criminal offense.  He's a smart guy so he didn't penetration test anything.  He simply walked up to the device and started typing at the keypad.  Just be looking at options on the on-screen display, he confirmed that default services (including an incredibly insecure embedded HTTP server) were enabled.
The operator then emailed IT to let them know.  IT first said that the entire system was configured securely and he was wrong.  HTTP services were in fact disabled they said.  So he opened up a web browser on his system and navigated to the web page (which did not require authentication).  The web server would have accepted default credentials that would have given him additional access.  The operator knew the default passwords since he used them to regularly hack others (with authorization).  But he stopped short of logging in, knowing that this would be a big deal.  IT summarily ignored him and simply stopped answering emails.
When IT ignored him, he emailed security.  Rather than security contacting IT to address the vulnerabilities in Internet connected DoD systems, they opened up an investigation into the operator's actions. Security noted in their report that connecting to a web server officially involves making a TCP connection, which is sort of technically a port scan.  And port scanning sounds a lot like hacking. Oh yeah, you can see where this is going.  This senior CNE operator who hacks other nation-states for a living found a glaring vulnerability Cyber Command/NSA's own infrastructure.  They should have given this guy a medal.
But yeah, he didn't get a medal.  Instead he got a reprimand.  A written f*cking reprimand.  And that was the beginning of the end for him.  He started looking for a new job and no longer works for them.  He was one of the best operators I've ever had the pleasure of working with.
So go ahead and tell me all about how Cyber Command rewards creativity, problem solving and outside the box thinking. But meet me in a SCIF to do it.  I've got a hundred more stories like this that I can't share in open forum.  This didn't happen a decade ago, it was less than two years ago.  Are things getting better? Maybe. But according to the people I'm still talking to they are changing at a glacial pace (if at all).

@_date: 2016-02-26 22:36:35
@_author: Ian G 
@_subject: [Cryptography] From Nicaragua to Snowden - why no national 
Long article on why IETF and similar bodies should *not* pander to national bodies in adopting encryption algorithms.
III. CHINESE WIRELESS TRANSMISSION STANDARDS AND THEIR COUNTERPARTS The standards with which this Note is concerned show the truth behind Smoots claim that the information technology industry uses every kind of standardization process imaginable.93 The three relevant Chinese standards are WLAN Authentication and Privacy Infrastructure (WAPI), Ultra HighThroughput WLAN & its counterpart Enhanced Ultra High-Throughput WLAN (UHT/EUHT), and ZUC  taken together, the Encryption Standards. The table below lays out basic information about the standards, their applications, and their foreign competition.
The first row of standards94 all pertain to WLAN systems, as is evident from the formal names of both WAPI and UHT.95 most basic, WLAN refers to a system of connecting two or more devices96 without the need for wires between them.97 As network connectivity has become an integral part of using a computer, wireless networks have grown in number and popularity.98 A wireless network allows quick and convenient access to a network. A common use of WLAN systems is to connect a laptop to an access point for the World Wide Web.99 WLAN can also be used to connect a small set of devices (such as a smart phone, laptop, tablet, video game console, and television set) into a home multi-media entertainment system.100 Instead of wires, WLAN uses radio frequencies to transmit data between connected devices.101
802.11 is a set of internationally recognized standards that facilitate WLAN connectivity.102 The Institute of Electrical and Electronics Engineers (IEEE), a formal standards development organization based in New York City,103 created 802.11.104 IEEE continually modifies 802.11, incorporating new security and transmission techniques.105 In 1999, when 802.11a was approved by the International Organization for Standardization as a formal international standard, it had a maximum transfer rate of fifty-four megabits per second.106 The upcoming revision, 802.11ad, has a maximum transfer rate of seven gigabits per second  almost 130 times as fast.107 Users and businesses benefit from faster internet connections. Higher speeds make the Internet more economically viable as a business transmission medium, a field once dominated by man-carried or animal-carried letters.108 The added convenience of radio-enabled wireless networks raises significant security issues. Interception of or tampering with radio waves is trivial to anyone with a radio.109 By intercepting radio waves, an unauthorized person can effectively eavesdrop on the other parties, and for example, uncover a private password or Social Security Number transmitted over the network. An unauthorized person could also tamper with the signal and trick other devices into thinking his own system has authorization it does not actually have  and here, that person could enter private networks, such as a restricted intranet upon which a company stores its trade secrets or other private and sensitive information.110 Identity thieves often target unsecured wireless networks to steal identifying information.111 Early versions of 802.11 used an encryption scheme known as Wired Equivalent Privacy (WEP).112 WEP was intended to bring to wireless transmissions a level of security which would compete with more secure wired transmissions, and thus prevent eavesdropping on, and tampering with, private signals.113 Every transmission subject to WEP underwent a two-stage process of encryption at its point of origin, and the receiver would reverse the process to decrypt and access the information.114 The communicating parties shared a secret key upon which the entire process relied; without the proper key, the information could not be decrypted. 115 However, in 2001, researchers discovered significant security flaws in WEPs encryption scheme.116 Thieves and other unauthorized persons could easily exploit these flaws to gain access to encrypted transmissions.117 After these discoveries, the IEEE 802.11 Task Group on Security began significant changes to WEP to plug the holes in security.118 These changes culminated in the Wi-Fi Protected Access scheme (WPA).119 In 2004, the IEEE integrated WPA into the 802.11 set of standards as 802.11i.120
The rift between WAPI and 802.11 revolves around the standards respective handling of security. WAPI is an offshoot of the WEP-encrypted versions of 802.11, born of Chinese dissatisfaction with the security flaws in WEP.121 The Standardization Administration of China (SAC) initially approved WAPI in May 2003 to become effective later in December of that year.122 The core of WAPI is a redone security scheme. The Chinese claim that WAPIs encryption rectifies the security deficiencies inherent in WEP.123 A necessary secret encryption algorithm controls WAPIs security scheme.124 The Chinese state provides only a half-dozen Chinese companies with access to the algorithm.125 Any company seeking to integrate WAPI into its radio designs would thus have to negotiate with one of those six companies. Additionally, 802.11 and WAPI are mutually incompatible.126
During 2003 and 2004, the Chinese government planned to instate WAPI as a mandatory standard.127 By June 2004, all WLAN devices would be required to support WAPI.128 The United States government formally protested the mandatory standard.129 Perhaps more importantly, information technology giants Intel, Texas Instruments, and Broadcom promised to cease sales of any product affected by WAPI.130 Craig Barret, Intel CEO, personally visited Beijing in an attempt to resolve the crisis.131 Amid the tension, China agreed to indefinitely postpone government enforcement of mandatory compliance with WAPI during bilateral trade negotiations with the United States.132 However, the United States Trade Representatives 2013 Report on Technical Barriers to Trade said that, as of 2011, Chinas Ministry of Industry and Information Technology (MIIT) remained unwilling to approve any Internetenabled mobile handsets or similar hand-held wireless devices unless the devices were WAPI-enabled.133
The UHT/EUHT standards follow in much of the same vein as WAPI. UHT/ EUHT are Chinese domestic alternatives to the internationally-accepted 802.11n standard.134 The Chinese claim that UHT/EUHT can coexist with 802.11.135 However, because UHT/EUHT both operate on the same frequency as their 802.11 counterparts, a device operating on one standard may cause considerable interference with the transmissions of a device operating on the other standard.136 A European information technology standards organization concluded that adequate coexistence between UHT/EUHT standards based devices and devices based on standard 802.11 is not possible.137 The United States Trade Representative has also expressed concerns about incompatibility between UHT/EUHT and 802.11.138
  4G LTE differs from the above standards in that it is designed for use in mobile smartphones, as opposed to use in laptops or other larger devices. 139 The 4G LTE set of standards is developed by the 3rd Generation Partnership Project (3GPP).140 Although 3GPP is an industry-specific standards organization, instead of a general formal standards organization like IEEE, 3GPP controls the 4G LTE standards and promulgates enhancements to the set, similar to the various iterations of 802.11x developed by IEEE.141 3GPP developed 4G LTE in part through recommendations from the Next Generation Mobile Networks initiative  of which China Mobile Communications Corporation is a member.142 With Sprint Corporations cessation of support for WiMAX in 2012, all American smartphone carriers now support 4G LTE standards exclusively.143 The market has thus established 4G LTE as a de facto ZUC is an additional encryption system operating over the top of 4G LTE.145 The Data Assurance and Communication Security Center (DCS) of the Chinese Academy of Sciences is developing the standard, and held the first international workshop on ZUC in December 2010.146 In 2011, 3GPP approved ZUC as one of several voluntary encryption standards.147 In early 2012, Chinas MIIT informally announced that networks and mobile devices operating on Chinas TD-LTE standard must only use domestic-developed encryption algorithms, a set that includes ZUC.148 At subsequent bilateral negotiations between the US and China, China agreed not to mandate a specific encryption standard.149 The US Trade Representative is still closely monitoring ZUC developments.150
[Long snip on why the WTO is likely to conclude against USA and for China, citing US - Nicaragua as precedent, and /national security/ as the right of sovereigns to break markets.]
VII. CONCLUSION A WTO Panel, in a dispute over the Encryption Standards invoking Article XXIs national security exception, is very likely to produce a dual ruling akin to the GATT Panel Report in US  Nicaragua: that China has breached its obligations, yet that breach is justified under Article XXIs national security exception. Any ruling to the contrary would require the Panel to ignore the terms of reference set in US  Nicaragua and rule on the validity or motivation of Chinas invocation of Article XXI. As national security goes to the core of a sovereigns responsibility, the consequences of a new formal interpretation of Article XXI would be severe  and beyond the scope of this Note.

@_date: 2016-02-28 14:18:23
@_author: ianG 
@_subject: [Cryptography] From Nicaragua to Snowden - why no national 
Just as an aside... Belgians designed the algorithm, the code was written by a Brazilian, the Java test rig was created by an Australian.   The Dutch in Anguilla did something too, can't quite recall what tho.   The other 4 contenders (or 29) were from many countries and spent a lot of time analysing the 5 leaders.  All 5 leaders were thought to be state of the art of the time.
    Seems non-national to me :)
    The Americans ran the comp and chose the winner.  The algorithm however is only mandated for US Government agencies, not for anyone else.  Unlike many other standards bodies that also mandate against Sure we know there are always going to be conspiracy theories, but frankly, the full NIST comp is as open as any other process we can invent.  Again, in SHA3.
But that's not an interesting point.  An interesting point is that labelling a bad process won't save it.
Ya.  So the overall problem with consensus seeking is that everything must be considered, everyone must have a say.  As we can see on this thread and in every other bikeshed, the result is a grand unified theory of cryptopolitics, rather than ... a ciphersuite, which is what users need.
Anyone can bring a pander-request to the table, and lobby for it.  Crap or not.  Anyone can bring a small RFC for no clear purpose, or a RNG that is pre-broken, or argue that an opponent isn't following the latest academic attack that is still warm from printing, or or or...
Ontologising the inputs or outputs isn't going to help.
There's no way around this.  If you're going to allow *everyone* a voice, then you'll end up with the pander-list.  Which is fine in ordinary commercial protocols like HTTP but not good in security protocols where we have known, aggressive, enemies that don't follow the laws of their land nor the laws of polite IETF society.  AKA the spooks.   Add countries to that.
The only alternatives I can think of are diktat of some form.  Of those, I prefer the following diktat:
One person of known history and pedigree is chosen to design the crypto suite.  That person does it.  That suite lasts for 7 years.  If it goes down in flames, so does that person's rep.  Sucks, but the incentive to take care is clear.
If nobody cares, then do an experiment?  This is why I consistently argued in TCPINC that David Mazieres' group should do it. The whole lot.   The notion of starting a competition between 2 opposing designs has the unfortunate consequence of handing the NSA another 2-4 years grace in mass surveillance.
Nobody should care what the suite is in TCPINC because the stuff can be MITM'd by definition.  I know that doesn't stop people reaching for the paintbrush, but we should hold the line on general security:  choosing any suite is good for TCPINC, choosing two suites is to misunderstand everything about the general nature of MITM, threat modelling, security.
( for others:  TCPINC is the generic name for bootstrapping a lightweight opportunistic encryption protocol within TCP so the masses can get out from the yolk of permanent global mass surveillance. )
TLS is a challenge which cannot be surmounted because the process is already too entrenched.  Granted.  No point in arguing about that.

@_date: 2016-01-09 22:22:08
@_author: ianG 
@_subject: [Cryptography] Verisimilitrust 
The problem with this is that it has been acknowledged in the past by at least Mozilla that if there is due process - court order or user permission - then it is OK to MITM.  As this is simply awful but legal behaviour, this is OK, and it applies to all CAs in all countries.  And we typically don't write policies that say "we'll ignore court orders" because those courts then strike them down nor do we typically enter into criminal conspiracies to tell our partners to ignore court orders...
In contrast there was a bit a push to try and ban MITM boxes on the grounds that even employee permission isn't sufficient for spying on Part of the problem is probably that Mozilla just buried the debate. Back when I was in that group, they'd been caught twice running the secret conversations internally, so they know things and act on things that we have no idea over nor can govern over.  Now they're caught in the open and have to explain all that wasn't explained in the past.
If this theory is true, then they've likely been permitting MITM boxes for friendly governments for a while...  So this just amounts to normal WASPish or WEIRD discrimination against one of the Darkistans.
If you go back to the early versions of PKI documents, they talked about how the acceptance of the roots was based on an entry into clear agreements and obligations to do various checks which would then be a foundation for your risk analysis, leading to a decision to proceed - aka trust - conducted by the relying parties.
Since then, the "popular" PKI for secure browsing found itself reversing much of that.  Agreements were made opaque, liability was buried, and users were stripped of the analytical tools to make any sort of risk analysis.  E.g., for most of Verisign's life you weren't allowed to rely unless you had agreed to their contracts, which basically said you had no chance in suing them.  Hobson's choice.
One could argue (and it is argued) that this was the only way to make secure browsing work - strip the users of the tools to do the analysis - because they had proven beyond a doubt incapable of doing it.
But sadly or otherwise, this means that trust is no longer part of the system, or at least that trust that the PKI originally envisaged, pre 1994.
Trust is a human decision of taking a risk that things will turn out right in the future based on past information.  In contrast, that which is called "trust" in PKI is precisely the reverse:  the browser hides it all and you carry on regardless.  If trust turns up at all it is that some browsers once told you that "you trust this site" ... which of course is bogus.
In practice, users make a trust decision precisely once - when they download their browser or purchase their new computer.  They trust their vendor to get it right - one decision leading to trust over years of usage on Apple, Mozilla, Google and Microsoft platforms.  Although that trust is pretty poorly informed and mixed up with other things, it's actually the trust that goes with branding, so it's not zero, it's quite rational and well studied in marketing.
Which gives us another clue to the word trust - users literally cannot trust what they do not know, and they statistically do not know the names of the CAs, or if they do, they don't know what they do.
Which all goes on to say, pretty much anyone who's using the word "trust" in the CA / PKI world is in a state of sin.  That's ok, it's whatever you have to do to collect a pay cheque.  Everyone has to make compromises in professional life.
Yes - so the muddled thinking that goes on in the browsers is because they have to live with the cognitive dissonance that the browser policy departments are the certification authorities over the certification authorities, but they can't call themselves CAs because that'll upset their chums over at the club.
Yup.  Inevitable.  Notice how they got into the revocation game?  How soon will it be before someone throws a switch inside and starts delivering signed root lists?  Oops, there is one that already does ;)
Well, the scaling problem is harder.  Getting certs out at Internet-personal-scale is a hard problem, so maybe some money has to be made somewhere to make it work.  But yes, one day someone in the browsers will figure out that certs that are too easy to spoof are no different from certs that people create themselves.
White certs should be the same - ones that are indistinguishable from HTTP.  But the problem here is that they got it into their heads that a cert means a signal of non-MITM-ness, not of equivalence to HTTP.  Oh

@_date: 2016-01-10 02:24:22
@_author: ianG 
@_subject: [Cryptography] simple guidelines to using message digests 
Looking at Zooko et al's formative thoughts at [1] it is possible to suggest some conclusions.
If we start with the table at Figure 1 we can see by eyeball there is a clear trend towards hashes losing their collision protection after some period of time.  Leading to observation 1: Calculating the batting average [2] for all hashes before SHA2 (around 2000), I get 14 years before your average message digest is out.
Then, if we look at Figure 2 we can see ... a sea of green!  Except for one outlier, hashes are safe against 2nd pre-image.
Which leads to the possible suggestion of guidelines in use of secure message digests.  Observation 2:
     1.  pick a modern message digest.
     2.  fix for collision resistance by (eg) using a nonce.
     3.  TRUST YOUR HASH !!!
This is empirical observation - we now have 25 years experience in hashes on which to lean, so we should use it.
One might seize on the outlier - Snefru-2 - of which Zooko et al says/:
    //"/That single exception is the second-oldest secure hash function
    ever designed,//Snefru//, which was designed in 1989 and 1990, and
    which turned out to be vulnerable to differential cryptanalysis.
    Differential cryptanalysis was discovered (by the open research
    community) in 1990."/
The problem with this is that it is literally an outlier, and there is no engineering sanity in creating protocol elements for dealing with such rare conditions, especially when there is plenty of other noise going on such as frequent protocol and coding breaks. Observation 3 would then be, there is very little support for algorithm agility here.  Do not code up algorithm agility for hashes, alone.
It would be very interesting to see the same depth of research in other algorithm classes.
[1] See disclaimer and warning - we're not meant to read it unless we [2] I'm using the cricket formula:
batting average = (21 1 5 14 12 11 13 14 10 14 19 9 15) / 11

@_date: 2016-01-10 16:50:12
@_author: ianG 
@_subject: [Cryptography] Verisimilitrust 
Then, the requirement wasn't for strong MITM protection because mail order or traditional stores already suffer MITM.
So, I disagree.  The WebPKI was designed to use some technology that happened to have been at hand at the time.  Retail just happened to be the driver for it, so the security model was crafted to make retail appear so that it needed that technology.
Which is not true.  SSH, Skype, and all the other chat programs out there...
:)  Let's not move the goalposts.  The original topic was the use of the word trust.

@_date: 2016-07-24 16:16:14
@_author: ianG 
@_subject: [Cryptography] Code is Cruel -- The DAO 
In more accounting terms, the typical defence against this is to implement double entry bookkeeping, so that the value cannot be "created" out of thin air, it must come from somewhere, and be shown to have existed as a deliberate act.  Typically a system that profers an account style (as typical in Ethereum) should have a double entry backend behind the scenes.
In more CS terms, any form of transactional processing likely needs some form of locking, somewhere.
Another issue is an unwritten law of financial cryptography: you always debit the sender's account first, then credit the receiver's account. So if there is a division attack, you can destroy money not create it.
Finally, the dichotimy between code-is-cruel and consensus-is-community is now revealed.  The solution is to set the contract up front and to state what the conditions of a hard fork are as a dispute.
Interesting!  Yes, Ethereum deliberately goes against Hal & Satoshi's conservatism and calls for the full turing experience.

@_date: 2016-03-06 20:29:24
@_author: ianG 
@_subject: [Cryptography] DROWN attack on SSLv2 enabled servers 
The fundamental systemic problem I believe is that few of the crypto projects have a holistic view to upgrade.  Instead, they've preferred to travel with the false sirens of algorithm agility.
Yet again, we find that a bug cannot be fixed by utilising algorithmic agility - because the breaks are 9:1 or better in the protocol not the Then, as the breaks are dominated by protocol breaks, the requirement is mass upgrade.  Which needs to be a carefully managed mitigation built in to the protocol.  In requirements terms, the system must have the capability to be upgraded, en masse.
Further, if hypothetically there were a failure in an algorithm (there was one once, RC4 I think) the situation is still that a mass upgrade is the best solution because it dominates the process.  It is easier or should be easier to upgrade than to adjust the algorithm params, and you get the latest stuff thrown in at the same time!
Mass upgrade is easier said than done - it's taken years for Microsoft and Apple to bed it in.  But it's much harder if you don't face up to it.

@_date: 2016-03-08 04:35:01
@_author: ianG 
@_subject: [Cryptography] EFF amicus brief in support of Apple 
The only thing in common between digital signatures and human manuscript signatures is the word 'signature'. Calling reverse RSA encryptions as signatures was probably one of the worse mistakes cryptography has ever I doubt.  If you look at the legal thrust of the usage, it's likely no-where presented that Apple has signed over a document for you to read and verify and then rely upon.
E.g., if Apple were to open up the walled garden to other suppliers, would there likely be a change?  Very unlikely, it would just be announced one day in the tech media and users would not notice the Another example - if Apple's alleged "intent" to meet their contract goes sour, would you be able to sue for damages?  Extremely unlikely. They've had their lawyers working those agreements for a long time, and the liability will be set to zero.  It's not even clear that there is a real contract there for all those free programs.
Another - if they were really signing, wouldn't they sign their T&C?
In contrast, what Apple has done is to use an authentication and authorisation technology to deliver an approved piece of code to the phone, which relies on that tech to know the code came from the mothership.
Which isn't that much used.  A failed experiment in society.  Which is why the world is now seeing finger or pen signing electronic devices. It's not because the users are stupid and won't adopt the smartcard science fiction stuff, it's because the plastic-pen-on-screen devices do actually capture intent.  Something no smartcard thing could really manage.
Right - it may well have the legal standard of a signature.  But it doesn't make any sense for Apple to use the technology for signing. It's quite fine being used as an authentication and authorisation device from Apple central to the phone.  No user needs to see it.  And if no user sees it, is it really a signature?

@_date: 2016-03-08 05:24:34
@_author: ianG 
@_subject: [Cryptography] McAfee: NSA Juniper backdoor used by China to 
I'm not sure if the author nailed it by logic alone or not, but I was told a long time ago that this is indeed the process:
Likely, more than one.  Post-Snowden, I wrote up the model we developed Once we knew there was a process of injecting personnel into our critical areas, once we knew what to look for, it was a lot easier to spot the spooks.  It is perversely pleasing to know that we as a group spotted a dodgy character within by applying the model, kept him away from the critical systems, rooted him out of the organisation over time, and later got credible evidence he was working for the intelligence However, it's really quite hard to operate under this sort of threat level.  The model eventually fell apart because after it had been handed on to the 3rd generation of defenders, they had lost the understanding.   In large part because it wasn't documented, kept so secret we didn't even dare write it down.

@_date: 2016-03-18 00:10:54
@_author: ianG 
@_subject: [Cryptography] Would open source solve current security issues? 
I tracked the 'renegotiation' cycle over its announcement, fix, distro and finally to a measurement of 80% patched here:
The measurement of that OODA loop was 41 months.  Now, we can quibble about the numbers.  It's one story and we'd need a dozen or so to get reliable data.
It does however suggest some questions:
   Was open source an important factor in the slowness to patch?
   Would Microsoft have been able to do it faster?
   Would Apple?
   Would Android?
It may be that with enough eyeballs, all bugs are shallow.  It may also be that with enough open source, you can still drown in a one inch deep ocean of upgrade treacle.
The ways I've mitigated the supply chain treacle in the past are:
   a. build upgrade into the security model.
   b. shorten the supply chain (absorb that package!)
   c. do your own security stack.
These things of course involve taking on some more risk, in exchange for getting rid of other risks.  It's not clear that these mitigations cause breaks any more than say the open source package spaghetti weaving process, but when they do break, it's a lot faster to get the fix out to (Note that I've not tested these out to large scale.)
Yes, significant learning there.  Has anyone any notion of the time between discovery, announcement, and say 80% patch rate for the modern generation of issues such as Heartbleed?
Right.  IoT.  Another pertinent question to ask is what other processes do we know about that haven't even begun to think about the supply chain aspects of delivery of security?
Nudge, nudge.
And, if you had to choose between a lunchtime Dagwood sandwich of dodgy algorithms versus a quagmire supply chain mixed into a bowl of spaghetti packages ... pick one ... which would you pick?

@_date: 2016-03-18 00:21:56
@_author: ianG 
@_subject: [Cryptography] ZDNet: "US government pushed tech firms to hand 
So, first, you register the crypto code with the government, because we won CW-1.
Then they come after you for the source code in secret court.
Did we really win CW-1?
Who are we fooling?

@_date: 2016-03-27 02:01:28
@_author: ianG 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
I'd be really interested in a more precise measurement of the cost of your analysis of the patent situation.  "A considerable amount of time" ... "some deliberation" ... "a few months of work"
Would it be possible to take your case study and turn it into metrics like man-months and costs?
Then, we could multiple this across industry.  And come up with a cost of patents to the world.

@_date: 2016-03-27 02:18:50
@_author: ianG 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
AES standardisation worked.  Monoculture worked - if AES was a guide. SHA continues to demonstrate this.  In fact, any reasonably tested good block cipher would have worked in the last 30 years - DES has still not been broken, any of the five AES contenders would have worked.  Same with the SHAs.
The question is - does monoculture on one algorithm bring more costs that the alternate?  History leans *strongly* towards monoculture in algorithms.  IETF's TLS is a casebook study in the alphabet soup of algorithmic agility, but it's not the only data point.
Is monoculture therefore golden?  No.  By no means - it's a risk decision.  We all know we're putting our one egg in one basket and feeding the world.  It's just that over time, this risk is lower than any other we know of - like Churchill's democracy, monoculture in algorithms is a really bad idea, but it's the least bad of the rest of the ideas.

@_date: 2016-03-27 17:13:54
@_author: ianG 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
As you elegantly show, it's not really sustainable to assess the risks mathematically because we're into the infinites.
Well, I know cryptographers try to push the meme that a couple of bits off the strength of a cipher means it is "broken" but we don't need to take that as gospel :)  If AES is broken that means many messages can be broken.  It doesn't automatically mean all are broken - although it possibly means that all are vulnerable.
Another factor to take into account is that
(a) as above, AES being broken doesn't mean the messages are read.  We know this because even clear text messages aren't being attacked/read as seen in recent European news.
(b) even if not broken, the messages are far more likely to be "broken" by other means.  The most common attack on cryptographic systems isn't by the hacker or MITM, it's your own people.
I saw an estimate somewhere that intelligence analysts only need to see about 2% of the traffic to divine the general gist.
Which is to suggest, if any messages are broken, then that's sufficient to incur a very high cost.  Someone estimated that the cost of Heartbleed re-work was in the vicinity of $500 million - and that's without any particular evidence of any breaches.  Which is enough to suggest that a partial break or a full break of 1/k might incur the full costs of "infinity" regardless.
Typically what people do when the probability is vanishingly small is to accept the risk.  This is indeed what everyone does with crypto anyway - it is very complicated to use, and often goes wrong for inane reasons (try teaching your grandma to do online banking securely).  How we handle this is to simply accept the risk that it will go wrong and she might be hacked/phished whatever.
Now, if that's how we handle the application, the same standard should be applied to the crypto.  Get the probability to so small as to be below noise, then accept any residual risk.  This process is already familiar with for example with prime search for RSA.
This approximately says:  AES has zero probability of being broken before the next cycle of major upgrades.  Full stop.
Well, I think the cost of maintaining a fallback is probably below the cost of maintaining k alternatives in good order.  Also, the key difference might not be the direct cost, but the fact that once the k are out there, the maintainer can wash hands of any additional costs and say "your problem" to the users.  Whereas a fallback has to be deployed - maintainer costs.
As a sort of sideswipe to this alternative, with k alternatives, it is easy to avoid any k-1/k breaks:  Simply XOR all k together.
(Some systems even do that, although I've only seen systems that do that with two algorithms.)
Why wouldn't we do that?  There are performance arguments which are interesting because in userland this isn't really a blocker, it's only an issue for the mass-connection servers.  But even there it's not really so much of an issue any more, so if we *really cared* about algorithm breaks we'd carry the cost.
The philosophical argument however is also compelling.  If XOR was an effective way to do things, the cryptographers would do it inside their black boxes.  They don't.  So it's obviously not appealing to cryptographers, and not only not appealing, it goes against the general dictum of "don't write your own algorithms" which can be better read as "the cryptographers did know what they were doing, don't second guess them."
Which all is to say it is important to evaluate the risks of a k-algorithm solution.  But I don't think it exists just because we think one of the k might fail.  There is other stuff going on...
IMHO we have enough data - we've now got about 25 years of mass Internet crypto.  But what we lack is a full reading of the experiences.  There are a few factors that need to be added into the mix above.
1.  Almost all breaks are protocol breaks, or higher.
2.  Because the (higher) breaks happen all the time, well-used systems already include a fast upgrade method.
3.  The fast upgrade method is typically well understood by the users (albeit often turned off).
4.  When there is an algorithm break, the most likely fix is to use the fast upgrade method to turn off any broken algorithm in a k-alg set. Indeed, most users won't even know that this break is "different."
5.  Notwithstanding the incur-infinite-cost argument, most users are unaffected by any break, and the cost to them is the rework cost.
6.  The same methods can be used to switch to the fallback.
7.  All large scale security systems work to the greater good, not the perfect protection for every user.  There is already sacrifice of security built in because users are using the same set of features.
This suggests that the protocol be locally monocultural.  That is, protocol version N be strictly monocultural (if that's the term of art that Peter is presenting) and version N+1 be countercultural - the fallback.
But version N+1 can be waiting in the wings, it doesn't need to be deployed, it can just be sitting there ready for the call in fast upgrade.
Right - but what is the complexity in the real world telling us? There's been almost no algorithm breaks.  Lots of protocol and higher breaks.  I like Figure 2 here to make that point:
with apologies to Zooko for spying on his works in progress.
I think there's room to lean a bit more on the algorithms and deploy a bit more resource up the stack.

@_date: 2016-03-27 17:26:51
@_author: ianG 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
There is some sort of sex appeal in being able to field and play with multiple algorithms.  Terming this as boring crypto gets right to the heart of the sort of geeky amusement that you see going on in the protocol lists - geeks defending their right to have selection of algorithms using all sorts of arcane arguments.
In practice, most of it is just a myth that was inherited from the 1990s.
I want boring crypto.  I want the sex appeal to move up the stack to protocols - which still needs work - and into the app which is where almost all the security problems occur.
What is an issue is, how do we choose the monoculturalist-in-chief?  DJB got this one by default because he was the one who created the boring set with that goal in mind, in advance of the market realising this is the way to go.  But this set - any set - has a shelf life.
In about 5 years we're going to want to choose a new king.  As soon as an organistion like IETF adopts this process, we'll be shifting the burden to the future prince in waiting.

@_date: 2016-03-28 12:01:16
@_author: ianG 
@_subject: [Cryptography] Mixing public key crypto systems? 
Packaging two key technologies into one is something that the post-quantum folks have thought about.  If you think of for example the certificate business, how would you ensure that it keeps running in the face of a sudden shift in quantum capabilities.  E.g., someone reveals that NSA has built a 256 bit EC cruncher...
(Personally, I'd say, nothing ever happens that fast in the cryptanalysis world, let's wait for that evidence.  But opinions are all over the map on this one.)

@_date: 2016-03-28 13:24:14
@_author: ianG 
@_subject: [Cryptography] More Bad Govt Shit To Fight (Burner Verizon FBI) 
Another thing that surprised me was the number of bogus claims made by women through the courts in adverse divorce/separation cases.  The lawyer who told me about this was nonplussed, it's lucrative work.
ObCrypto - how do you protect your secrets from such a threat?  It is threats like this that make SnapChat's disappearing pictures a good thing.  The solution is something like that you have to (a) integrate other human aspects into the security model and (b) accept the strength of the solution is around 8 bits not 128 bits.
I'm in part responding to this because this is real security - protecting people who have clear harms.  Not like the normal cryptographic theoretic stuff we use ex-military threat models to protect credit card transactions.
I did some security review recently for an org that does something like the above-mentioned stuff, on a massive scale.  One of the threats was that the victims' activity could be tracked through billing records which were often available to people in household and people at work, who were the attackers.  Given the nature of the abuse involved, it's a "hell-yeah!" threat rating.
The challenge is to be able to communicate using some device that others have access to without leaving a trail.  E.g., SMS over a standard dumb phone without leaving phone numbers in the billing records.
There's probably a million people in USA alone who desperately need this protection in one way or another, without counting the teenagers who are just sharing mucky kid stuff away from their over-protective parents.
And that's exactly the protection that the feds want to take away.

@_date: 2016-03-30 10:30:47
@_author: ianG 
@_subject: [Cryptography] Gates are cheap. Should cipher design change? 
I'm not sure why others find this so, but the reason I find it so is because every protocol I've looked at in recent times has been a datagram protocol.  By that, I don't mean UDP / layer 3 but that the application at layer 7 has a requirement to send a particular packet of known size across to the other party.
So much so that I've been scratching my head for about a decade now trying to think of a pure stream oriented requirement and the only one I can come up with is SSH - in terminal mode.  But even that is more a byte-datagram protocol, and it's "only" that because that's the semantics of a Unix terminal as designed back in the 1970s.  People who are older than me will recall that before Unix established that paradigm, terminals were typically datagram-oriented.  And from a pure security/tracking perspective, byte-by-byte SSH is a nuisance, we'd much rather a single datagram for the line that is typed.
E.g.,  Sound & Video - lossy datagrams.  Logs - time-ordered append stream of datagrams.  Backups - set of datagrams.
Another aspect is that at least in my programming, although we use streams in OO a lot, we always convert in and out of streams several times, so the packet and its size is well impressed on the software. Even up to a 100k photo from a phone, software is happy to work with a single packet and not worry too much until post-mature optimisation time comes, or it has to slice the packet down to UDP size for transmission.
(NB, I'm leaving out the reliability aspects of protocols in the above.)
 From software complexity pov, I'd actually say we want arbitrary sized, not even "powers of 2".
But we could work with powers of 2.  There has been a recent shift towards hiding length, as that can be used to figure out what part of the protocol is being used.
One of the security protocols out there uses 1024 byte datagrams (I forget its name).  That's it.  But this isn't totally perfect when it comes to larger packets because sending 10 x 1024 in 10ms reveals that you are really sending a 10240 sized packet.  Which generates some signature.  Now we're into conflicting desires: anti-trackability versus preservation of bandwidth (== mobile airtime).
As a future direction, SHA3's expanding block is enheartening, including it's AE cipher mode. I can't recall its name but SHA3 is where I would start today for a new generation crypto suite.  One reason is because of the sponge feature and dial-in size, another reason is that the suite covers the basic symmetric family needs.
I have a different set of friends, I don't let them do crypto in anything but own code ;)  In high level application space, packages are a threat.  Hardware is just another package.

@_date: 2016-03-30 10:32:16
@_author: ianG 
@_subject: [Cryptography] Gates are cheap. Should cipher design change? 
Curious - what requirements should NIST have had?
And, what are tweaks?

@_date: 2016-05-05 08:44:46
@_author: ianG 
@_subject: [Cryptography] Proof-of-Satoshi fails Proof-of-Proof. 
That ain't gonna happen, sorry folks!  Not to rag at RAH, I'm just picking up his perfect foil, and for reasons he'll wryly smile to: Physics. Humanity. Frailty. Complexity. Of the sort that we've all being talking about since forever on this list and many others.
Let's break it down.
Firstly, we all on this list know that cryptographic keys prove that a private key did a maths transform that a public key can confirm.  Full Stop.
What cryptographic proofs do not confirm is that a human said something meaningful to another human. Indeed, the more that the Bitcoin community and the tabloid press demand a proof-of-spend and examine the results they're given, the more it demonstrates how humans seem to be isolated by cryptography not joined.
In theory, keys are mathware, humans are wetware and the two do not easily mix.
How does this play out in real life?  We know that the human experiment known as cryptographic signing has failed. We know that there is at least one tiny little country - Estonia - clinging to the European dream of using smart cards to identify humans, but statistically the world has failed to make human signing with public key cryptography work. People write books about this, I simply point it out as a significant data point of where many thousands of people really really tried to use keys to prove meaningful human things.  And failed.
Let's get more topical. There are strident, demanding calls for people who make statements concerning the identity of one said Satoshi Nakamoto to back those statements up with cryptographic proof. Yet these demands are .. unfounded, and that is the kindest thing that could be said about them. Why?
Anyone offering information to the world has no necessary call to offer more information. When I say that Craig Wright was the leader of the team known as Satoshi Nakamoto, I do not contract to say more. Nor did Gavin or Jon or others in any sense contract to say more than they did. They don't owe anyone anything. Even if they made errors, it is not on them to correct them. "Extraordinary claims calls for extraordinary proof" is only a standard for academia, it has little place in human affairs, especially in that democratic tradition known as open discourse, nor in the human standards of proof that have been honed over a thousand years of legal history.
In fact, I contracted to say less - as well all do, when we join the encryption business, we covenant to keep peoples' privacy. When I started what became Project Prometheus a few years ago, I promoted their privacy as a goal - because the team known as Satoshi Nakamoto asked for their privacy by posting here in 2008 and disappearing entirely 2 years later. Now, when I come out and say that Craig Wright was the leader of Satoshi Nakamoto, it is only because he himself finally announced it. I remain committed to privacy even if the community Satoshi wrought is revealing themselves to be a pack of rabid statist wolves looking to rip the wool off of the backs of the sheep that they call their customers and future users.
Sorry, guys, it gets worse, and I hope the Bitcoin community dissolves itself in collective shame as to their inability to even contemplate protecting their own.
As we know in cryptographic affairs, key management is hard. Keys can be lost. Misplaced. Traded.  Breached and stolen.  Keys can be spoofed - we have an entire cryptographic security system called SSL/HTTPS which is blighted by phishing, based on misuse of cryptographic proof of identity. Let's not go into the details, but I shall revise here FTR the claim of secure browsing: the identities are cryptographically proven. Which apparent claim does not reveal itself to the humans in sufficient reliability in order to defeat basic common or garden social engineering. If the IETF's biggest, bravest and most educated can fail to protect the browsing public from the obvious, known and counted threat, what hope the rest?
Even if the above were not sufficient, let me get precise and particular as to why the Proof-of-Satoshi is dead-on-arrival. There are several facts which apply in this case.
Firstly, Satoshi Nakamoto is not one human being. It is or was a team. Craig Wright named one person in his recent communications, being the late Dave Kleinman. Craig did not name others, nor should I. While he was the quintessential genius who had the original idea for Bitcoin and wrote the lion's share of the code, Craig could not have done it alone. Satoshi Nakamoto was a team effort.
Indeed, a sort of proof is right there in front of you - when you look at Craig Wright, you do not see Satoshi. When you look at Satoshi Nakamoto, you're seeing some measure of the influence of Dave Kleinman, and it isn't possible for Dave to prove anything anymore to anyone. Team Satoshi is ephemeral, and no cryptographic multisig can now capture those that aren't around any more.
This team effort was one of a most severe cost to all members of that team, and only privacy is holding us back from recognising it.
Further, the keys that controlled critical parts were moved several times between various persons. Which is to say that control of the keys does not indicate more than the holder being trustworthy to the goals of the team at a point in time.  Even if Craig manages to sign over a coin, it does not and cannot prove he is "the one," only that he was at one point in time a trusted member of the team. Albeit, the team that he founded, but a wise leader controls for all risks, including those risks posed by the leader himself.
More: control at any time does not necessarily indicate ownership, either in the minds of the team nor in the eyes of the law.  Recalling the reports of late 2015, can you rule out that the keys haven't been Finally, as has been reported, the headline bulk of the value is controlled by a trust. Any movement of those coins needs to operate according to trust rules; if not, then we are in a state of sin. What that means is not something that can be described in mathematical terms, but it can certainly be described in hysterical terms - the logic de jure of the Bitcoin community.  As an aside, I really strongly suggest that the Bitcoin community not press for the breaking of the trust.  If unsure on this point, ask your miners to explain that old curse "be careful what you wish for."  Breaking the trust is way off the scale of what anyone will desire.
I suggest that it is therefore impossible for any reasonable person to conclude that a "spend" of a Bitcoin coin proves anything beyond that the erstwhile signer was at some point in some way related to a key.  A host of factors make the 'proof' too impractical to describe at a press or media level. And, if we have to call in opposing experts to argue the case, what's the point of the "proof"?
It is with incredible sadness that I watch an entire community misunderstand the lesson that Satoshi originally taught - trust in mathematics to prove accountancy. Yes, cryptography can prove that a coin is available and disposable pending an attempt to further dispose it. But the Bitcoin design was deliberately weak when it came to proof of persons. Especially, when it comes to known and now revealed weaknesses in the persona once known as Satoshi Nakamoto, there is no proof in mathematics that can satisfy that community's yearning for yet another meal.
By all means, take that lamb for yet another feast of slaughter, but do not soil the good name of mathematics for your Pavlovian hunger.
iang, CARS.
ps; after writing this, I stumbled across:
pps; This post reflects no commercial agenda or position of myself or any person related to me.  I have no position in BTC and have never had any BTC other than a few pence lost in some test wallet somewhere.

@_date: 2016-05-08 22:24:44
@_author: ianG 
@_subject: [Cryptography] russian spies using steganography? 
Bezrukov and Vavilova communicated with the SVR using digital steganography: they would post images online that contained messages hidden in the pixels, encoded using an algorithm written for them by the SVR. A message the FBI believes was sent in 2007 to Bezrukov by SVR headquarters was decoded as follows: Got your note and signal. No info in our files about E.F., BT, DK, RR. Agree with your proposal to use Farmer to start building network of students in DC. Your relationship with Parrot looks very promising as a valid source of info from US power circles. To start working on him professionally we need all available details on his background, current position, habits, contacts, opportunities, etc.

@_date: 2016-11-05 14:29:38
@_author: ianG 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
So, as a breed, we continue to miss the big picture and focus on the details - mail is insecure, why didn't PGP work, surely we can't overcome the shame of email, and we must simply must HTTPSise the net before the Ruskies come and kill us all in our beds.
In the above, "we should at least encrypt the dox..." is just laughable.   I mean, I share the guy's pain - but it's missing the threat by a planetary mile.
The big picture is this:  the node is the threat, not the wire.  This case as 99% of the threat evidence out there is all about hacking some server and scarfing up everything, *or* some insider threat leaking the Or both - with the news that 5 intelligence services were likely (99%) to have hacked Hillary's private servers, and wikileaks likely getting their leaks from insiders.
Which is to say, we could paper the planet with wire encryption - pure PGP mail and HTTPS as standard - and we'd not move the threat needle by more than 1%.
Here's a new data point from Wired - how long did it take the browser manufacturers to respond to the bleedingly obvious failed GUI of the padlock?  20 years.
That article is the Good, the Bad and the Ugly of security thinking. Count the years - SSL and secure browsing invented in 1994, and the GUI was screwed by Netscape 1.0.  Now, in 2014, a browser manufacturer starts to seriously think about how to present the user a message.
Yeah.  In effect, using completely clear email would have probably assisted her cause.  She hid in the noise, and her own intel services, chartered to protect the government's secrets, didn't spot it.
What screwed her was that the node was breached.  6 times.
Which was why she wasn't allowed to have a private server in the first

@_date: 2016-11-08 01:57:16
@_author: ianG 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
Yes - it's a leak.  There is a rebellion going on in the FBI.
Of course, there is no evidence to confirm it.  Nor is there any evidence to confirm anything Snowden said about the NSA.  Nor has the White House confirmed that wikileaks maildrops are essentially accurate, or identified the ones that have been changed.  Nor has Sweden admitted that its case against Snowden is made up.  Nor nor nor.
OK, so at this stage, James Comey is fighting a battle that means he will lie even when he's telling the truth.  Look at the stakes.  One has to weigh his situation with the context of the week.
Whatever he purports has zero meaning, against the stakes of the game he's in today, tomorrow.
Wikileaks are releasing what they have, but it looks like they got it from another source.
The other intel agencies will not release it unless provoked.  They're intel agencies - their job is done in the shadows.  They got the product, the extracted, they fed up to their masters.  Now kicks in the next task - to make sure their tracks are covered.
No intel agency will admit to hacking, ever.  We're still waiting for the NSA to admit to spinning the centrifuges, right?  It makes zero sense for any intel agency to admit anything anytime anywhere.
Most or all intel agencies won't futz with the American election.  Most or all foreign governments will not have a preference for one or other candidate.  Most or all governments will recoil with horror at the accusation that they are interfering with the American election.
So, no, they won't release it.  Nor admit it.  Ever.
Unfortunately, the NSA, the FBI and the various other counter-intelligence agencies which are tasked at protecting the government are not going to see that as any more than self-serving bluster.  And in court - if it ever were to get there - it would be demolished.  That alone would send the perp to jail.  E.g., if the answer to a few upsets within is that we go it alone, that means every agency, every secretary, every sysadmin who thinks he can do better than the NSA ... has carte blanche.

@_date: 2016-11-08 02:01:15
@_author: ianG 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
Well, actually, I'm accepting the point that something "should be done" but also I'm making the point that within the year we knew there were problems with the model.  And the chickens came home to roost mid 2000s.
And it took to 2014 for one browser manufacturer to think it was broken.
You can't show that causality.  Until mid 2000s, there was only a vague handwavy correlation between "claimed secure" and "unknown threat." Which matches at both 1 and 0.
I've heard that story.  I've read that story.  I don't believe it.  I've never seen any evidence that an Internet merchant said "I don't have the confidence to take a credit card because ..."
It makes zero sense.  At the time, credit cards were being lifted by insiders - talk was of waiters taking credit cards and swiping them away from the customer.  Yet there were no waitresses on the Internet, *and* in both scenarios, the customers were covered and the merchants would accept the chargeback.  Fraud was quite high in those days!
What I have heard is, that which drove the web in those early days was porn.  And, what the porn operators were afraid of was not theft of CC but showing porn to under 18s.  When credit cards were offered, with encryption, they were given a two-way pass.  Only over-18 year olds had the card, and the encryption stopped the rest.  Bingo!
Of course, within the month they dropped the encryption over the content because it was too slow, and just left it for the credit card.  Breaking the model.
And, chargebacks on porn were ridiculously high.  Way higher than any other threat.  They would have had to advertised credit cards on billboards to even move the needle on actual theft of credit cards over the wire.
I'm quite happy to be shown some evidence and shown to be wrong.  I'm quite happy to believe that the sellers of encryption convinced the buyers of encryption they didn't have the confidence.
But merchants?  For a start they didn't exist as a movement, and for next, they were all arbitrage operators and 'confidence' was the least of their worries.  It all sounds so surreal.

@_date: 2016-11-08 02:04:16
@_author: ianG 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
But this rabbit hole goes further - unless the app decodes in a sandbox and can prove that nothing else can get into the sandbox, the bottom line is that Huma's erstwhile 'end-node' is still a node.
All PGP does is eliminate all the visible, easy intermediate nodes, maybe.  E.g., if it is a usable interface, then the mail is decrypted into IMAP so it can be read and searched and so forth... so even then the nodes might be reduced but the vulnerable ones are still present.
However you do the analysis, it still ends up that the best strategy for the attacker is to ignore the wire and hack the node.  So the only defensive strategy that actually faces up to the attacker is to defend the node.  Start from the node.
Most all security work starts from the wire - it's that old CIA thing that's embedded into the consciousness.  It's that box we have to get out of.
Well, no.  Insider threats are beyond cryptography only when you're in cryptothinking's cryptobox.  Consider Bitcoin - it's a cryptographic solution to an insider threat, that of Ivan issuing more money or accepting false tender.  SSH and PGP could be considered as criticism of the insider vulnerability of PKI.  Shamir secret sharing, ring or group signatures, etc.
Yeah - in that at the very least, we should expect the counter-intel services to have provided a hack-proof box to Hillary that she chose not to use.
ach - because to be usable, an email client has to connect to IMAP and analyse all the mail in the clear.  This is of course to damn crypto for email's pre-historic architecture, which is why a lot of people say that email can never be secured, and should be abandoned wholesale.
In essence I'm saying that we *could* talk about PGP having been used to encrypt all the data and have it encrypted at rest on the box.  But in practice, it never seems to work out that way.
To be worth anything it has to work for 1++ bn people.   The paper designs of the security industry are routinely crushed between the unstoppable force of the hacker and the immovable rock of Kherckhoffs' 6th.
Idle question - did Hillary have a hardware token?

@_date: 2016-11-08 21:27:03
@_author: Ian G 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
Right, that's why I called it the Good, the Bad and the Ugly.
So, the movement to come up with a new GUI arrangement, and the move to HTTPS everywhere are known good things.  The reason for this, and the painful unwinding of the meme of "open content is meant to be open" over the last decade was because there is no way that phishing can be dealt with until it is HTTPS all the time.  Once everything is in HTTPS, then every server is effectively identified, and the browser is responsible for not getting tricked into phishing.
As an aside, this strategy of HTTPS everywhere was identified in 2005.  Can you say OODA?
The cost of this is in the cert.  The slavish obsequiousness to CAs is going to cause massive heartache, as techs running toy servers have to keep running off to that one open CA, and these people - the influential techies running departments - will fight until they get effective self-signed pinning interfaces.
That's the bad.

@_date: 2016-11-09 11:07:00
@_author: Ian G 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
I think you're probably right on the motivations.  However I think that foreign countries would likely stick to propaganda, which is an accepted means of peddling influence.
This isn't to say that the elections will be easy - there is insider attack.
Maybe David Chaum was right.  Even those we can't figure out a correct cryptographic solution to the problem of political voting, we should still try.

@_date: 2016-11-09 14:38:30
@_author: Ian G 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
Both?  It's charged times, so we know that both sides are going to play it to the hilt.  There were calls that the "Russians did it" on the other side of the Atlantic, again with zero evidence.
It suggests that their legal counsel assessed the chances of them being drowned in court?  If Hillary had won, she'd have sent in the boys to clean up the opposition.  This time with feeling.  GC wouldn't take that I don't think we can determine much from any statements in the press.  All we can really do is to take all the leaks and correlate them, look for trends, and eliminate them for stupidity.
It has been suggested only part in sarcasm that the way to gain credibility is to take public information and call it a leak...
Yes I think you might be half-right on that one. There are claims that Russia is engaged in standard levels of aggressive propaganda. E.g.,
Still zero evidence, but motive is clear.  The one-sided bellicosity is sufficient reason to get involved.
Whether propaganda qualifies as interference, I don't know.  If it does, then unfortunately all the media, government, and leadership are also interfering.  I think it's that ugly side of the democratic coin - when we do it, it's rights, freedom of speech, democracy and apple pie.  When they do it, it's an attack on our rights, our freedoms, our democracy.
The question that I have, outside propaganda is whether there is actually a cyberthreat to the process?  Whether a foreign power would enter into the voting machines and hack them left or right as desired?  I suggest that is nonsense in this particular case, but it's hard to prove an absence.  We can only wait for evidence of positive interference.
Well, the security officers within each department generally handle that, using the processes laid down in the security manuals.  They let Obama have his blackberry.  I'm sure if enough pressure were brought to bear they would have built a private server situation for State Dept.
But seems like they never got told to do that.
In summary - I think there is merit in looking at how cryptography could have changed the situation.
1.  Hillary's use of private server was an attempt to deal with one threat, although what that was was never clear to me.  But it opened her up to another threat - hacking.  At a simplistic level, I think the answer is clear - don't do that.  At deeper level, we should be delivering systems that don't lead the users to taking such drastic steps, and then making their situation worse.
2.  The sense of Russians hacking the electoral process leads us to look at reliable voting systems.  Thinking about our current infosec posture, that this is something that cryptography can't provide the answer to, I think we've got it wrong.  Because (a) if we don't secure the voting system then someone else will hack it and steal it.  And there's plenty of underground and anecdotal evidence that this is going on.
And (b) we need to get away from this impossibility thing. Probability works for human systems, too.  If we can make it improbable that a vote is tampered with, that's still a win, for those times in the majority where we got the true positive.

@_date: 2016-11-13 15:00:11
@_author: ianG 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
Yes it was:
SSL has two way authentication built in.  The other way is called client certificates.  With full authentication, there isn't a way for the phisher's website to ask for a login & password, and to then go and use them on the primary site.
Also, the original model, pre-Netscape 1.0 was that the user was supposed to authenticate the CA, HTTPS and the website name.  In the original PKI concept the user was expected to be part of the security model.  What Netscape discovered however was that they were moving from a technically savvy audience to a non-tech audience.  So the approach as built up over a decade of writing and thinking was ... broken before it saw full release.
(You might not like the solution or the implementation might be weak, but I think I'm answering your direct question.)
So, even if you don't think that SSL was designed to prevent MITM of the nature of phishing, it has another effect which is quite bad:  it is the security system of record.  It's existence blocks others, blocks evolution.
This is not a cryptographic thing - but an institutional thing.  SSL's in place, and so are the RFCs, the CAs, the committees, the training, the certs, the CABForums, the corps, the legals, the contracts, the devs, the thinking, the beliefs and last but not least, the lines of code.  Putting in place another security system (whatever that is) is an inordinately difficult task.
So yes, we can blame SSL for not addressing phishing.  But some part of the causality is outside the technology.  In part we're blaming the people for not recognising phishing as a threat to their users.  But the people will never accept the blame so it's pointless to write it out like that.  It's actually politer to say "SSL doesn't defend."
So, in 2005 when phishing took off from a broken start in 2003 ... nothing much happened.
What technology would you put in place today to stop these threats?
Assuming we recognise Phishing to be a threat, surely we'd put something in place, right?

@_date: 2016-11-13 15:24:58
@_author: ianG 
@_subject: [Cryptography] Need a better name 
In my world it's called a smart contract, but I can understand that isn't going to make you happy [0].  Indeed, it isn't really a smart contract but a smart object or even better a networked or consensus object.
Things to think about:
  * what happens when private key is compromised?  For this reason the smart object idea is nice - the object can accept a new key under some   * what happens when the key is re-used for two documents?  Is it then by definition the same?  or different?  Or?
  * from my thinking, the initial creation of the object seems best to be empty.  And then the first instance is pushed into the object as first action.  Which means we might likely need an immutable chain of events.  But this might be too far away from your use case.

@_date: 2016-11-13 19:39:35
@_author: ianG 
@_subject: [Cryptography] October 28th is now National Cryptography Day 
Good idea.  But why national?  Why is this always an American thing?
Serious question - what would an international or non-national cryptography day look like?
What is the most important message ever protected or not protected? What day was it sent?

@_date: 2016-11-14 19:52:51
@_author: ianG 
@_subject: [Cryptography] highlights of crypto history 
Point.  I recall the Mary Queen of Scots ciphers which were decrypted and heads rolled as a result, literally.  [0]
Hard for the Germans not to appreciate the impact as well ;)  It's nicely international.
The consequences were quite dramatic.  It also has the advantage of being quite recent, with understandable consequences, whereas the historical ones might be less
For the rest - they are mostly non-crypto.
Does anyone have any contact with IACR?  11 January 1917 offers an enticing date coming up...  It might also be something the other side can get behind.
[0]

@_date: 2016-11-14 22:07:42
@_author: ianG 
@_subject: [Cryptography] On the deployment of client-side certs 
That is the question.
I think the headline problem is basically down to the poor GUI, the poor management of client certs.  Which is down to the lack of interest in the browser vendors developing it, or experimenting with alternates such as self-signed certs.  Which is down to the CAs being terrified of anyone futzing with their business model.
It's actually quite easy.  I programmed a fully client cert site in PHP about 6 years back, for the experience.  In the end I concluded it was as easy as programming up with passwords, and a lot more friendly and usable for the users.  It is of course much more secure at the cryptographic level.
No more lost passwords!  Which seems to be about half the user support costs and a big security risk eliminated.  Of course what this does is shift the burden across to the users needing to get/keep certs, so the work becomes much more beneficial if one has several sites to program up, and share the bounty.
Hence the strategy that emerged was to pull the users on to one key site that they all had to go through with client certs to get access.  Then the rest came for free.

@_date: 2016-11-23 13:16:33
@_author: ianG 
@_subject: [Cryptography] combining lots of lousy RNGs ... or not 
If you can undo the random, then it ain't random.  It's shared.
Using the definition of "random == unpredictable to adversary".
Which is to say that we draw the security envelope around the RNG such that if that security envelope is breached, then all bets are off - attacker owns the entire machine, and won't bother with the RNG, will just instruct you which knife you are to use in your upcoming

@_date: 2016-11-27 14:02:16
@_author: ianG 
@_subject: [Cryptography] Is Ron right on randomness 
It should read from /dev/urandom [1].
That covers all Mac OSX, all Linux and all Android, which makes for the majority of devices.
Left over is Microsoft, iOS and misc?  Push them to add a file device?
[1] The economics is that the platform can more easily solve this problem than the application, and should solve this problem.

@_date: 2016-11-28 12:20:25
@_author: ianG 
@_subject: [Cryptography] randomness for libraries, e.g. OpenSSL 
Which suggests a further question - what is a platform, and what is not a platform?  The short answer is that OpenSSL is not a platform.  So we don't need to dig any further in this thread.  (In other threads we might see a sense of the CPU-as-platform versus the OS-as-platform, but that doesn't effect us here.)
That.  Rather than spending energy on solving the problem in code, which is a really hard objective because of many factors, some of which are listed below, put the energy into Inveiglement.
Browbeat the ones without /dev/urandom to add it, or an equivalent. Browbeat the ones with /dev/urandom to surface their threat model and their security model and improve their best efforts.
So, to do a proper RNG it seems that we have to have access to hardware.   E.g., Denker's unplugged microphone port is one such.  But in order to gain blessing against John von Neumann's curse, one has to enter the dirty world of hardware.  A library that has side-effects like accessing random pieces of hardware or relying on CPU opcodes is an unclean beast.   Such decisions should be reserved for the OS or the application.
I'd say this is Context more than Con.  Because of e) the library isn't in a position to solve the "just work" problem.
I think,
a) the random v. urandom war is over.  Equality won, they are both the same.  FreeBSD got it right, in that of the practically zero users who understand and need the difference, they should do some navel gazing, and if that doesn't help, all of them are capable of solving it elsewhere.  Of the rest, they should use urandom.
b) the offer of no guarantee of quality is actually commensurate with the guarantee of security and the guarantee of reliability - nothing is entirely secure, and nothing is entirely reliable.
By adopting a "best efforts" approach, the efforts get better;  by trying to measure the asymptoticalities of security, reliability and entropy ... *and* deliver these to the outside world in a digestible simple number, we seem to get distracted by the numerology and forget the goal.  The world is more Pareto - we want a better result here without compromising the better results in other places - than c) the guarantee of availability will trump the desires for quality in every real world competition.  It's up to our best efforts to ensure that quality remains upwards not downwards.

@_date: 2016-11-28 12:38:01
@_author: ianG 
@_subject: [Cryptography] Is Ron right on randomness 
Yes absolutely!  That is a platform responsibility - see the thread with John Denker where he says:
    By way of example, here is something that might go into such
    a specification:  There should be *one device* ... or if for
    back-compatibility there are two, they should behave the same.
    The device should guarantee 100% availability and 100% high
    quality, high enough for all practical purposes.
    Let's be clear:  a proper RNG device should never block, *and*
    there should never be any temptation -- or even possibility --
    of using the device (or the corresponding intra-kernel function
    call) before the RNG is well and truly initialized.
This is the only interface or promise that makes sense to the general purpose app, library, developer.
So, you are throwing the responsibility back to OpenSSL to assume that the platform hasn't well seeded.  That's what I'd advise against because (insert John's list) the OpenSSL only has the vaguest understanding of what Linux is up to at any one point in time / release / etc.  That's not to say the devs don't know more, but what goes into the code benefits by being simple and standardised.
Note also that random and urandom are the same on *BSD which I guess goes for all Mac OSX and all Android.  E.g., a far larger slice of the world than Linux, albeit the client side not the server side.
Not in my opinion.  Linux should provide good random numbers (unpredictable to the adversary) from urandom.  End of story.  If it doesn't, the user is screwed, and Linux is broken.
It's not efficient for any general app or general library to second guess this problem.  Only the paranoid can afford the luxury of solving the RNG problem themselves, and OpenSSL is a general purposes crypto library delivering to general purpose applications.
ps; taking up Rich's challenge to reduce N+1 to N ;-)

@_date: 2016-11-30 06:15:02
@_author: ianG 
@_subject: [Cryptography] randomness for libraries, e.g. OpenSSL 
Right.  So Intel provided the hardware and it has weaknesses.  This is the same as other vendors providing the hardware, all hardware RNGs come with weaknesses.  Hence the need to pool & mix.
The answer circulates around an assumption - what is the platform that is the appropriate place to fix the RNG problem?  Is it Intel, which is clearly a platform and it fixes the problem with RDRAND, or is it OpenSSL which apps see as a crypto platform, or is it Linux?
The answer is I think - the platform that is lowest layer, in software, and in open source.  Everything below is out-of-control hardware, so can be seen as a separate source of entropy.  Everything above is user software.
Which makes it Linux.  Or *BSD or Windows.  Is where the problem should most efficiently be solved.
Right.  When it gets to hardware, we are not only looking for an audit, we are totally dependent on it.  Which is not a good thing - and hardware therefore has this special status - mysterious, vaguely hopeful, branded.
Sure - but a standard source at a platform level like urandom is not easy for Intel to futz with.  It is issues like this that make the platform of efficient rectification open source.
If the RNG mix is in the OS, then Intel's chip has to look at the code, analyse where the RNGs are being mixed, and then dive in and futz with the instructions as they are coming out of the mixer.
Crazy, but doable in some corner of the 8bn transisters.  IIRC the permathread from about 3 years back, this argument came up when RDRAND was being treated as a post-mix action in Linux.  As RDRAND was now the last thing, it was somewhat more credible that an internal analysis could trigger just on RDRAND.
At the time I think it was acknowledged.  And RDRAND got pushed back upstream into the mix - or so it was intimated at the time.  Ideally, RDRAND should be "just another pluggable mixer".  And to amuse this group here, Linux could also be customarily pushing a series of changes ... and forcing the NSA to live-update its internal microcode to keep up :)

@_date: 2016-11-30 09:55:29
@_author: ianG 
@_subject: [Cryptography] RWC 2017 Programme 
The program for RWC 2017 in New York in January 2017 is now out...
It looks like it is going to be a great program, so please
register, and pass this information along to anyone else you
feel will be interested.
Local information etc and registration details are here...

@_date: 2016-10-01 22:40:44
@_author: ianG 
@_subject: [Cryptography] another security vulnerability / travesty 
If we want to reach groups that are familiar with old tools, we have to give them new tools that feel the same.
So, what would it take to invent a cyber-fax?
   Both - Install - click to run.
   Alice tells Bob to cyber-fax it to domain + code.
   Bob enters the document, the domain + code.  It returns a new code, which Bob tells to Alice.
   Alice types in the code, and out pops the document.
The reason is probably that the post leaves the hands of the postman and is therefore out of the security envelope, whereas the phone lines are a security domain that are point to point and therefore are reliably secure.  Wire tapping is a big crime whereas stealing someone's mail from the mailbox is not.
Peer to peer.
We lost the war a decade or two back.  Now we're paying the cost.

@_date: 2016-10-01 23:45:21
@_author: ianG 
@_subject: [Cryptography] Insecure email might be an even bigger problem 
i know this is an old thread and I'm behind a month, but ... really?
As far as I know, the dems and the repubs have been waging a never-ending war against each other including hacking servers, election machines, the whole works.  This is rarely reported because they have so much dirt on each other they both know that to report would be end of life as they know it.
So, if the FBI are now reporting that a foreign power is attacking US elections ... surely this can only be for a reason that is unrelated to the actual topic - hacking elections.
I am reminded of the UN's attempt to define terrorism, which got sent back 6 times by the State Dept. because "we do that."
iang, colour me cynical.

@_date: 2016-10-02 01:19:32
@_author: ianG 
@_subject: [Cryptography] Recommendations for short AES passphrases 
Easily remembered.  Easily typed in.
An awful lot depends on who is using the system, and where.
In talking about BASE 58 or BASE X or any similar system you have automatically limited yourself to a developer audience.  Who are Think about a phone. If it's a smart phone we are talking about an alphabetic keyboard.  And any other keys are a pain to type because we have to keep hitting the meta keys.  Plus, on a phone, all keys are vulnerable to the fat-finger problem.
So in this context, it is easier to give the users several alpha words in one case (26 + space) ... an entire extra word (26^4) is easier than switching to digits (10^4).
Then, if it's a "feature" or dumb phone we're back to digits.  And, an extra 4 digits is easier than switching to letters.  Try it some time...
If you don't give the users the method that works for their device, and for them .. they desert, and you lose all security.
A 4 digit PIN is more secure than a 20 x BASE58 character password that is never used.

@_date: 2016-10-03 00:14:39
@_author: ianG 
@_subject: [Cryptography] distrusted root CA: WoSign 
Peter's rhetoric is actually a bit soft.  No security protocol can be analysed outside the context of the users and institutions who field it.   The real place any change is blocked is CA/B Forum, the browsers, the IETF, the CAs, the auditors and the software creators, all of whom are locked in a deadly embrace.  And they like it that way, the incumbent people within are mostly not volunteers as we might imagine in the open source world, but instead are employed and paid to promote the model.
So why would they change?
CAB Forum was born in darkness, and grew in darkness.  They didn't open it up until after they had laid down a new framework for standards that locked in the old model even tighter.  Even when they opened it up, after two or three years of secret policy preparation, they carefully made sure that no open or outside or user-oriented voice would be able to change things.
This reminds me of the "where's your patch?" rhetoric.  The problem with this lazy slap down is that a person could spend months writing a patch and have it rejected.  It is pretty clear for example that if we send in a patch for Chrome to implement say Jerry's idea or any of 100 ideas proposed, it would go nowhere.
This doesn't mean you're wrong.  You could be right.  But the difference between that and building the franchise, locking out others, is nothing.
PKI is a tribe.  There is no way to change it.  And it is totally pointless to even talk to the tribe about their religion.
The only thing that can be done is to bypass it.  Totally.
They will never displace.  Any sufficiently good technology (and there are a few) will not displace PKI-secured browsing but bypass it and create an entirely new system.
CT is making its mark.  What is poignant is that it took a company with google's resources and position to do it.  The notion that even google had to work hard at it puts the lie to the notion that any one less could make changes.

@_date: 2016-10-04 22:48:38
@_author: ianG 
@_subject: [Cryptography] distrusted root CA: WoSign 
Their paycheques?
Not sure how one converts a lazy slapdown to being called lazy, but rhetoric isn't everyone's cup of tea.
And, if you want people to open the kimino and claim how many years each has tried to improve the system, let me add my ask:  we also divide by the salary we took while doing it.
Right.  We're now at the point where the reality hits - everything that is said by one side or the other is rhetoric.  Neither side can reach the other.  That's the point I was making.
You spent 4 years?  I probably spent 6 years following the "Mozilla open route."  Guess what - they're locked tight.  The openness at Mozilla is a travesty.  The last straw for me was not the CA/B Forum secret policies but the fact that Mozilla were routinely, regularly and secretly having private discussions with CAs about matters that were being debated openly - as if the open debate was important.
If even Mozilla can't be open, there's no hope.
And you won't.  If I were to propose a solution like e.g. how we proposed certificate pinning back in the mid 2000s, the CA side (I won't say you) will come back and say, no that doesn't solve the problem.  Or, go see the IETF.  Or propose it to CA/BForum.  Or, that'll never work. Or, it's been tried.  Or, or, or.
Part - the honest part - of the problem is that we haven't decided who's problem we're solving.
If I could hazard a guess, you're solving google's problem.  I'm uninterested in that.  I'd like to solve the users' problem.  Pretty tough to square that triangle.
QUIC was looking pretty nice ... but I hear they've added TLS 1.3 for the Kex.  Darn it.
TCPINC was looking great until somebody had the bright idea of putting TLS 1.3 into TCP.  I'm not sure where that is right now, I kind of got depressed when I realised the game theory of it - it was trivial for the NSA to just stuff exactly N sybils in there and keep it in deadlock.
QUIC and no CAs.  Bloody brilliant.  TCPINC and bootstrap upwards.  Awesome.
Remember - I'm not saying what the change is to be, not any more, now that I've shown after a 6 year personal effort that no change is possible.  (Granted, not without a google.)
But, assuming someone wants to change - what is the objective of the change?  Who's security is being threatened?
Let's try one.  Mass surveillance.  Thousands of hacked Cisco and Juniper boxes (or however the NSA do it) are scarfing up all the traffic of the net they can copy.
By copying.  That's byte for byte, a technique that works because there is approximately zero crypto on the net.  How do we fix that?
Put crypto on the net.  Something like all browsers doing ADH to all servers would be ... like a year's work for interns, right?  It's a known mode, right?
What's the current ratio of open web traffic to encrypted web traffic?
Would ADH set back the NSA ?  Yes it would, it would force them to go active and attack the nodes they wanted, risking they were spotted. Which I'm quite happy to let them risk, because they'll take that risk if they are convinced it's bad guys they're chasing.
Dude.  We could talk for 4 years and never reach.

@_date: 2016-10-08 01:10:05
@_author: ianG 
@_subject: [Cryptography] Suggestion that a bad crypto tool led to a 
Turkey coup plotters' use of 'amateur' app helped unveil their network
Turkish authorities identified thousands of undercover Glenist operatives, whom they blame for the failed coup, after cracking messaging app ByLock
Turkish authorities were able to trace thousands of people they accuse of participating in an underground network linked to last months failed military coup by cracking the weak security features of a little-known smartphone messaging app.
Security experts who looked at the app, known as ByLock, at the request of Reuters said it appeared to be the work of amateur software developers and had left important information about its users unencrypted.
A senior Turkish official said Turkish intelligence cracked the app earlier this year and was able to use it to trace tens of thousands of members of a religious movement the government blames for last months failed coup.
Members of the group stopped using the app several months ago after realising it had been compromised, but it still made it easier to swiftly purge tens of thousands of teachers, police, soldiers and justice officials in the wake of the coup.
The ByLock data made it possible for us to map their network  at least a large part of it, a senior Turkish official said. What I can say is that a large number of people identified via ByLock were directly involved in the coup attempt.
The Turkish official said ByLock may have been created by the Glenists themselves so they could communicate. However, experts consulted by Reuters were not able to verify this.
ByLock is an insecure messaging application that is not widely used today, Tim Strazzere, director of mobile research at US-Israeli security firm SentinelOne told Reuters. Anyone who wanted to reverse-engineer the app could do so in minutes.
More than a dozen security and messaging experts contacted by Reuters had never heard of ByLock until it was mentioned in recent days by the Turkish authorities.
According to Matthew Green, a cryptologist and assistant professor of computer science at Johns Hopkins University in the US who examined the apps code after being contacted by Reuters, the ByLock network generates a private security key for each device, intended to keep users But these keys are sent to a central server along with user passwords in plain, unencrypted text, meaning that anyone who can break into the server can decrypt the message traffic, he said.
From what I can tell it was either an amateur app (most likely) or something that someone wrote for the purpose, he said in an email.
The ByLock messaging app appears to have been launched in 2014 on both Apple and the Google Play app stores, only to be removed by the developers later the same year. New versions subsequently appeared on less secure app downloading websites targeting Android, Windows Phone and Blackberry users.
An anonymous blogpost in November 2014 purporting to be from the developer claims ByLock had attracted around 1 million users, making it difficult to maintain, in part because the app had come under attack from unnamed Middle East countries.
Even if it had reached a million users, that would still make it minuscule compared with mainstream smartphone messaging apps like Facebook Messenger or WhatsApp, which each have around a billion users worldwide, or iMessage, the messaging app available on all Apple iPhones.
According to some websites that allowed users to download ByLock, and to the security certificate inside the software itself, the author of the app was listed as David Keynes of Beaverton, Oregon. Reuters was unable to locate anyone matching that name or verify whether this identity is Starting in May 2015, Turkeys intelligence agency was able to identify close to 40,000 undercover Glenist operatives, including 600 ranking military personnel, by mapping connections between ByLock users, the Turkish official said.
However, the Turkish official said that while ByLock helped the intelligence agency identify Glens wider network, it was not used for planning the coup itself. Once Glen network members realised ByLock had been compromised they stopped using it, the official said.
Instead, the coup plotters seem to have switched to the far more secure WhatsApp by the time they launched their putsch. While WhatsApp encryption is harder to crack from the outside than ByLock, the authorities have been able to access messages sent that night by getting their hands on the phones of detained plotters.
Transcripts published by Turkish media show officers coordinating troops movements in WhatsApp chat groups. With thousands of people in a single WhatsApp chat, it only takes one person to get captured while their phone is unlocked to discover every planned detail, said Dan Guido, head of New York-based information security firm Trail of Bits.

@_date: 2016-10-12 21:23:23
@_author: ianG 
@_subject: [Cryptography] Blockchain to Secure Nuclear Weapons? 
While I don't know for sure, it's not clear to me that Guardtime technology is blockchain.  It's more like hashchain, but that is something that has been discussed many times going back to the early 1990s.

@_date: 2016-10-26 23:21:56
@_author: ianG 
@_subject: [Cryptography] A PKI without CRLs or OCSP 
I don't wish to be harsh, ... but there's your mistake, right there - you took a tool and bashed it over a problem.  It's the other way around.
Start with identity.  Figure out what it is.  (That will take a (*))
Compare your model of identity to your use case or business model.  From this extract some requirements and threat models and security models (whirl around this loop like a million times).  When all that settles, pick some tools.  (Another (*))
I guarantee if you do this properly, you won't be thinking about blockchain nor PKI.
Yes, like PKI.  PKI was originally designed by telcos to deliver certificates over the telephone, one per household, as the newfangled digital-to-analog-and-back-again modem did it's work of downloading the day's electronic mail.
Then, as the family browsed their mail on their single standalone pre-PC, the certificate would authenticate the sender - offline.
Unfortunately, while that particular model was possibly relevant and promising in the 1970s and 1980s, including with Janet, ACSnet, UUCP and all its !!! the offline model died with the Internet.
It's all online now.  Need another design.
Nice hack - but the business model of the CA includes making sure that never happens.
So, if we can skip past the blockchain-is-your-hammer... what is it that you are really trying to solve?
(*) year

@_date: 2016-10-26 23:40:50
@_author: ianG 
@_subject: [Cryptography] How to prove Wikileaks' emails aren't altered 
This is pretty important in the modern life.  If we think of something like the goals of OTR and Snapchat, it is clear that ephemeral communications are very important.
It becomes a choice of security designs to fight for ephemeral communications, to preserve them, if that is what your user base needs (kids doing sexting, whistleblowers leaking documents, human rights reporters uploading evidence, spouses heading for divorce).
On the one hand it is clear that this is literally impossible given our modern definitions of cryptographic security.
On the other hand, it is clear that DKIM's notion of "tenuous" is dangerous, as was Pvte. Manning's reliance on OTR, every divorce case that surfaced 3-year old IMs taken out of context, and not a few human rights workers.
Somewhere between these two impossible and unacceptable poles, there is some work to be done.

@_date: 2017-04-03 00:37:15
@_author: iang 
@_subject: [Cryptography] stegophone 
Right, this is what I meant by a 'duress' device.  But when I say this will further complicate everyone's life, it is because I suspect that such a design doesn't work _in practice_.  By which I mean, when grandma as owner of the device is faced by (a) customs or (b) extortionist or (c) burglar or (d) husband, she has a low probability of operating the device, *and* carrying off that there is no secret or stegophonic mode.
Depending on the nature of the attacker, her fear and confusion in getting it wrong could be damning, could indeed be much worse than if she just handed the whole open phone over.  Interrogators, prosecutors, border guards are trained in spotting stress.  Is grandma?

@_date: 2017-04-03 01:01:34
@_author: iang 
@_subject: [Cryptography] Regulations of Tempest protections of buildings 
It worked!  Your electromagnetic emissions in the visible band were not able to escape the room...

@_date: 2017-04-10 17:31:22
@_author: iang 
@_subject: [Cryptography] Regulations of Tempest protections of buildings 
Depends on which country.  Not sure about up to the minute, but it was the law in Austria that every household had to have a radio, nominally to hear emergency broadcasts.  The law dates back to Anschluss, and was thought to be required so that everyone could hear Hitler's speeches in the comfort of own home.
The law was still in use because of radio/TV licensing.  You weren't allowed to claim you didn't have a radio, so you had to be licensed....  They had dropped the law in Germany, but they had their own issues to deal with ...
Of course, all countries are a bit different, but radio isn't necessarily a logical or sensible regulatory environment.  Like Ham radio operators yelling at you if you start using crypto...

@_date: 2017-12-11 14:26:35
@_author: iang 
@_subject: [Cryptography] Altcoin volume 
(apologies, very old reply slipped through cracks)
Gresham's Law applies when the two units have substantial other purposes. e.g., cows for eating or milking. Electrons on a blockchain don't have much substantial other purpose.
But mining does - the electricity consumed in mining has lots of purposes. So one could look at the differences in efficiency of the mining process. If one is comparing say Bitcoin segwit and Bitcoin cash, then there is no real difference in their mining process so little to say. One could predict that Ethereum & Bitcoin could compete on efficient mining, especially if PoS ever takes off. Or wait until next year's DPOS contenders take flight.
ps ...which segues into this old paper:
Bitcoin & Gresham's Law - the economic inevitability of Collapse
Which in short predicted that (a) Bitcoin was vulnerable to botnet / stolen electricity, and (b) entry into market by thieves would toxify the marketplace, and (c) Bitcoin would eventually collapse within if it The paper was lambasted as with all anti-Bitcoin papers. But it has recently emerged that the botnet threat of (a) was very real in the day (PG and I had no insight into actual data) and it was the ASICs that tipped the balance back to honest mining. (Not the GPUs, they were equally vulnerable.)
For (b) check out the Bitcoin politics. For (c), wait :-)

@_date: 2017-02-25 16:26:27
@_author: Ian G 
@_subject: [Cryptography] Schneier's Internet Security Agency - bad idea 
Bruce Schneier has recently published an impassioned plea for a United States Federal Internet Security Agency, which would likely gain control of civilian cryptography, among many other munitions.  The essay is impassioned, it is much longer than his normal 2 pagers, which signals something - belief, preparedness, foundation?
Poignantly, the link in the Crypto-gram was broken, use the above one.  You should read it, but let me summarise.
Schneier's basic argument is that the Internet of Things is becoming too big and too dangerous to be ignored.  He uses the metaphor of building an Internet-sized robot, which I think is a great picture of something too big and dangerous to ignore any more.
As we're all agreed, security is hard, and the market has failed to solve it.  Therefore, Schneier suggests, we need a non-market solution.  Which is, by implication, a government agency.
Quite fairly, he points out that the US government isn't structured to deal with this because the problem is spread across too many departments.
Where he is quite right is that the problem will be seriously considered at USG level - we already know that Trump's impressive list of executive orders included one on cyber-security, and people close to the USG are reaching out for ideas.
These are claims I think we can agree on:  that the IoT trainwreck to be is on the tracks and picking up speed, and the USG is going to do something.
But then, concluding that a government agency is the solution to this does not follow.  For three reasons, in increasing fundamentality:
1.   Bringing it all under one roof doesn't work, and that goes especially for the USG, which famously always fails to coordinate.  For cynical example, it has about 15 intelligence agencies, and its attempts to unify them all post-911 just resulted in the creation of another intelligence agency.  For other example which Schneier highlights, Americans are still paying for the problem of DHS which was basically that solution - bring the problem of securing the 'homeland' under one roof.
2. I think we can agree that the market hasn't solved the problem.  But it is a fallacy that this implies the government has to then step in.  As a matter of objective reality, governments can't solve some problems, and governments can make some problems worse.  Which is why we have bad wars and bad legislation, something that even Schneier admits with DCMA.
Unconvinced?  Look at what the DHS/CBP has done with the so-called muslim ban:  they are now searching people's phones and other devices for 'expressions (un)aligned to US values' or some such nonsense.  This is damage done, spilt milk, but let me cry out the reasons:
    1/ the security community is upset, which means we will now start thinking about 'duress' devices which will further complicate everyone's life.  Also, nobody in the field will want to work with DHS/CBP on this for fear of tarring their reputation.
    2/ Worse, all the people who actually do want to harm others (e.g. terrorists but also murderers, fraudsters, baby-snatchers, whoever) now know about it, and will not bring compromising devices across the border.  Or they'll start creating legends - and if you think about it, the more nefarious you are, the easier it is to create a legend, and the harder it is for the border guard to see it's a legend.
So the only consistent, predictable outcome is that searching devices will harm innocent people - companies and individuals that have their hardware compromised by CBP must now replace them because of security breach, and reset any compromised passwords. Corrupt or prejudiced officers will be empowered.  People will be slowed down.
This negative signal to the world can never be repaired!  Worse, it will make Americans absolutely unsafer because by using the tool, CBP has destroyed its efficacy in most all the useful cases and made it harmful in most all the non-useful cases.  It might not be absolutely the worst thing DHS could have done, but it's got a place in the top 10.
3. The final and fundamental reason why this is wrong comes down to thinking about who knows what to do, which is known in economic circles as the market(s) in insufficient information.
In the now-canonical paper "The Market for Lemons," George Akerlof argued that when the buyer does not know the quality of a used car, the direct sales market does not clear, and institutions arise to solve that problem:  used-car warranties, sales yards with brand, regulations, etc.
Akerlof shared the Nobel Prize for this paper, so the insight is widely accepted as being useful - but the Market for Lemons was premised on one important caveat, that the seller knew what the state of the car was.
This critical point becomes much clearer if you consider the works of the other two papers cited in that year's Nobel Prize.
Rothschild and Stiglitz wrote on the market for insurance, which they identified as the reverse of Lemons - the insurer being the seller did not know the quality of the goods, whereas the buyer did know the state of what he was trying to insure.  A mirror image, if you like, and together, economists called these markets in asymmetric information.  As But as we are logical people, we know that where there is an asymmetry, there are two other choices.  There is not only the case where both buyer and seller know, there is also a null case - where neither buyer nor the seller know the quality of the good. In this case, there is no information - a mirror doesn't work when the light is off.
Which brings us to Spence, the third laureate of that year, who showed that in a market where neither side knows the quality of the good, _signals_ can emerge to guide us, but they can be as false as they are truthful.  Indeed that was part of his argument - a good signal is one that can be interpreted by both sides, but could be interpreted incorrectly by one or even both sides.
Spence doesn't dispute Akerlof's claim that institutions arise, and indeed his first example was the undergraduate degree, a very clear institution.  What he disputes is that the signal of the institution is correct in some objective manner - he shows that under some circumstances of inadequate feedback over quality, the institution can sustain without any reference to quality.
That is, we all believe in the institution because it turns out we don't know what the problem is, and we are happier passing our responsibility off somehow.  E.g., to another party; in the market for undergrad degrees, everyone passes off the quality argument to someone else:  the student to the university & employer, the university to the student and employer, and the employer to the student and the university.  This works, is sustainable, but has no quality anywhere in the argument.  So quality drifts...
And so it is with a government agency for all of Internet cybersecurity.  We can all believe in it, and we can all pass on the responsibility for the signal to someone else.  See where I'm going here?  The government will pass on the responsibility for absence of success to someone else:  its people aren't the experts, terrorists aren't playing doggy with phones any more, the APTs are smarter than us, the Russians are interfering with our democracy again, etc, etc.
And one thing that government agencies are objectively good at is saying that more money will solve the problem.  So more money will be thrown at the problem, guaranteeing that the institution will sustain, while the responsibility for success will be necessarily handed on to next year's The fundamental problem here is that we don't have a solution. We can outline the problem, but there is no solution in sight that fits the general needs.  And, if we create a government agency without having that solution in sight, we'll just be creating another problem.  Remember DHS?  They are now a problem, they are now arguing against the cybersecurity of your phone, and we still no closer to a coherent concept of "border control."
Schneier's argument relies, in a sense, on asking the question: what's the least bad thing we could do, when we don't know what to do?  Schneier says that the market has failed, and what we do with market failure is create a government agency to implement a solution to the But what's that solution?  Cybersecurity is not like airplanes or cars or radio spectrum - for all of those 'market failures' we have a clearly delineated and standardised solution:  careful design, crash-test dummies or auctions.
I say that creating a government agency will objectively create a new problem, because government agencies are good at growing in uncertainty, and we haven't got a solution to hand to this agency, only more problems, more uncertainty, and more potential for big agency spends.
Curiously, we - the security industry - have been sitting on this controversy for some time.  Is the market for security one of Lemons, in which case an institution can objectively find a solution to market failure, or is it a market for Silver Bullets, in which case institutions can exist but their existence says little or nothing about the problem?  And it's been a tough intellectual puzzle, if it was that easy, we'd have agreed by now.  There's even a Workshop on Economics & Information Security, and it hasn't resolved the Lemons debate, nor come up with a clear plan to solve the wider problem of the economics of information security - which makes for a nice problem to have, if you're an academic.  We might ponder whether institutions like WEIS sustain because they are the institutional solutions in Akerlof's model, or because they're the signals in Spence's model?  Poignancy all around.
So let me propose an objective test.  Let's say this:  if we can put a random or otherwise independently chosen group of experts in a room, and they can come to consensus on a solution, then we're in a market for Lemons - an institution can arise, and they've chosen it for trial.
In the alternate, if the experts can't come to consensus, we're in a market for silver bullets.
What happens in a market for silver bullets?  Once all the dust settles, I suggest that an institution arises, but it arises because of the money - the solution is the one that supports the biggest lobbiest.  Industry wins, but the user does not.
Basically, the one with the most influence - paid or otherwise - gets their solution mandated.
Who might that be?  RSA?  Symantec?  Boeing?  SANS?  NIST?  NSA? BlackRock?  CIA?  We can't tell right now because bidding hasn't started - We can't predict who's going to reach deeper and further into the pocket for the lobbying.  But I am predicting that an agency solution will go to that entity that pays for the most influence.
Is that how we're going to solve cybersecurity?  I don't think so - but, and I think Schneier is right on this - we're going to find out.  I think the desperation for a solution will cause the cries for a new, single cross-government agency will rise.
I say - resist.
As of now, we are, and so is Pres. Trump.  Not only did the leaked draft of a cybersecurity executive order not suggest anything like an agency, it was the first EO to be delayed and deferred.  POTUS appears to have got that message at least - we don't know what to do, so best bet right now is to do nothing impetuous, and ask for more research.
Let's see who's right.
I'd urge you all to choose sides on this, because our Internet - our security, our crypto, our institution - hangs in the balance. Choose sides.  Prove me wrong.  Because it's a damn sight better if you can prove me wrong than the alternate.
iang, seller of silver bullets, voodoo spells, snake oil and other charms

@_date: 2017-02-26 13:01:37
@_author: Ian G 
@_subject: [Cryptography] Schneier's Internet Security Agency - bad idea 
The market hasn't solved the problem because (a) we haven't waited long enough and/or (b) the market hasn't found a large enough space to justify the extra work.
Consider Apple.  Let's say Apple decides to solve the problem of IoT.  It collects an A-list of devices, crafts a SOHO security mechanism, and launches a suite of stuff.  Bingo, solved.  By the market.
To rephrase, Apple could do this and nobody would blink an eye.  The reason they could do it is that Apple has the margins that permit them to take the long view and craft a franchise that keeps providing, which feeds into their loyal customer base.  But to do that, Apple have to find a set of devices that sell in the 100mm+ range.
Which is to say, the market might solve this problem.  All we know right now is the market hasn't solved it, yet, today.  Tomorrow?
Yes that is a cure often suggested.  But, not only would it kill the existing IoT manufacturers, it would put a wet blanket on the open source world as Kevin mentions, and also impact the licensed software world.  As Richard suggests.  So it's not really a solution, unless killing that patient and rest of the ward as well is considered a solution.
Maybe, we want our cake and eat it too.  But also, even if such were to be introduced, would it really solve the problem?  Or would we just encourage a grey market in non-strictly attachable IoTs which would cause the same damage, and add to the mix cost of trying to make owners strictly liable?  It hasn't really worked for the music/films industry.  Or pharma as Kevin mentioned.
Also, the legal world is very good at crafting legal arrangements that appear to protect the public but don't.  The easy go-to solution here is to do white label imports into the country and expect the importer to go bust any time the problems happen.  Or, slightly more sophisticated, sell the camera raw, with firmware that is only capable of downloading some open source software.  Seller is strictly responsible for the hardware, buyer is strictly responsible for the open source software...
I'm not saying (forcefully) that liability is a hopeless direction, rather that we don't know enough about what works, and we know that's obviously got some pretty dramatic downsides.  And putting that in the hands of a government agency isn't going to find us the balance.

@_date: 2017-01-14 13:46:04
@_author: Ian G 
@_subject: [Cryptography] 33C3: cash :-) attacks ! 
55 minutes of fun!
Two of the attacks (at least) rely on shared libraries which results in a shared cache area in LLC (last level cache), and the dramatic difference between a cache hit and a cache miss for memory access. The attacker can watch what happens to his view of the cash.  Or, the attacker can pre-load the cache and see how the victim responds.
   ==> So, if we turn off shared libraries ... does that mitigate those Their target attack focussed on AES T-Tables which have pre-computed look-up tables, which turn keys into indexes.  Listen and profit!
   ==> Back to pure code algorithms?
The attack(s) are statistically measurable by looking at the high cache activity.  But they were able to slow down the attack and tune for stealth, and avoid the detection.
The attack on the android keyboard/password is again a table-lookup.  By identifying the table in the code, they were able to attack the addressing and reverse the locations of the key presses, leading to the letters being pressed.
   ==> passwords without table look-ups?
The covert channel description of delivering 0s and 1s via cache stuffing "prime & probe" is really fun.  I'm not convinced it is realistic but it's a great example of sexing up a presentation. 40-75KBps between AWS vms allowed them to mount an SSH connection over the covert channel.
They also exploited direct kernel direct memory mapping to reverse out the physical addresses from virtual addresses.  Allows attacker to find kernel drivers.  Point of this was not clear to me, it related to other And a bunch of other attacks related to other instructions.  Their conclusion was that there is so much leakage going on with many instructions that simple instruction level mitigations won't help.
   == Don't use VMs?
Not sure how well this will work with mobile phones...

@_date: 2017-01-14 22:05:27
@_author: Ian G 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
Were the failures due to electrical failures?  Or was there a specific cyber attack element that was taking out the gear?
I recommend the good engineer acquire the necessary skills and do it Because of the issues you (and Peter and Jerry) list, the success rate of outsourcing to experts isn't high, so can a good engineer learn enough to leap a rather low bar?  Typically, yes.  Is it state of the art?  No.  Is state of the art available?  No.
Is the alternate a secure system, or a standards system that delivers too late and too little, or nothing?
What's the compromise?  Typically, it takes a couple of good engineers to put together a cheap & cheerful security protocol for a narrow use case.  Or it takes an industry to make a standard that's over generalised and introduces security impedance mismatches so it typically can't also ever meet state of the art.  Clear economics point to "do it

@_date: 2017-01-31 17:11:11
@_author: Ian G 
@_subject: [Cryptography] Cryptocurrency Exchange without a trusted third 
(I'm pretty sure I also figured that one out in the past but can't find my writeup.)
You might want to look at hash locking:
Now back to the thread on disputing the subject line ;)

@_date: 2017-07-01 15:00:36
@_author: iang 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Security is about risk analysis.  Take the likelihood of the attack and mulitply it by the cost of damage, both for your chosen users. Then order them according to largest first.  Start at the top. Ignore the bottom.
Anyone who has that threat model generally has a squillion other problems.  I'd say OpenSSL security is for those who are the vast majority of OpenSSL users - the browsers and servers.

@_date: 2017-07-03 20:15:26
@_author: iang 
@_subject: [Cryptography] Satoshi's Trump Card 
Yes.  The Satoshi has become a God.  No matter that the brand has been dispersed and/or destroyed, no matter the logic or the agenda or the humanity, the brand has taken on an impossible standard that no mortal can meet.
Unfortunately the public won't be shaken from that.
And, unfortunately, the bitcoin community has moved on in its own agenda.  There are now serious monetary interests involved and they are quite happy to fight for their wallet, and no stinkin stashi wannabe dare get in their way.
So, to live in interesting times.
It is somewhat elusive to think back.  From the very beginning a very special sort of person believed in the Satoshi invention.  Not me, I thought it was ... unsustainable.  I was not special enough. :-)
But some people on this very list did, and dived in.  Then others followed.
And a community was born.  But that community also had its own style.  Like a genetic algorithm, living, growing, that community has eaten its way to its current phase.  So moveth the herd.
That which we all see now is the evolution of what was begun in Jan 2009.
Not wishing to be boring, but one of the difficulties in getting close to the god that is Satoshi was that it was a team.  They carefully worked all the posts and the paper to make it good.  And to strip out personality.  To leave the objectivity in.  All very perfect and professional and they may be ruing the day, now they or he or she has to live up to that standard.
Yeah.  What always upset me about Bitcoin was that the users' interests were never taken seriously.  Still not.  Oh well.  They're right and I'm

@_date: 2017-06-01 07:02:48
@_author: iang 
@_subject: [Cryptography] Password rules and salt ... or not 
How much easier than the latest password non-fix would it be to write a public key implementation and just label it 'advanced password' for those who can't shake their addiction?

@_date: 2017-06-01 07:20:14
@_author: iang 
@_subject: [Cryptography] athletic event verification? 
In my time looking at what people loosely call 'identity', I've come to the conclusion that the only useful statement is what one person says about another.  There is no point in logging what one person says about The only useful thing a person can assert is "this is me/my key" being a necessary precondition for anyone to say anything about that person.
iow, Identity is an Edge Protocol.  More with pictures here:

@_date: 2017-06-08 09:10:37
@_author: iang 
@_subject: [Cryptography] stego mechanism used in real life (presumably), 
I agree with the observation of the shift, but I take issue with the notion of "society's own enforcement mechanisms".  As far as I can see, it isn't society that is putting in wholesale enforcement mechanisms, it's a small subset that are working outside the bounds of society.
In long-standing principle, societies have more or less accepted the need for spying on *foreign* enemies but drawn the line at spying on own citizens.  This is well tested in history.  For local spying you need an investigation, a warrant, a court, a process.  The barrier is high.  Things like yellow dots, the equity ratio of 10:1 offence to defence at NSA, also the 19 agencies secret sharing and deception to courts, show that the historical defences of civil society are being subverted.
And, it is more or less worse in other countries.  It used to be the notion in pre-1990s times that the agencies spying on own people was reserved for the evil enemy - the Stazi, McCarthy, KGB, Hoover.  But now it seems to be trotted out with regularity that if the terrorists are achieving, of course we'll undermine society to fix that.  C.f., May's recent comments about willingness to reduce fundamental rights of 60 million in exchange for 6.
So I would prefer to say, what we are seeing is a shift towards society protecting itself against the attacks of agencies that are now out of control of the democratic population.
That's just me.  I'm not society.  But neither am I content when entitled members of society in agencies think society is right and it's ok to go local because we're the good guys.
General society didn't need end to end encryption until this shift happened.  40 bit CA-mediated crypto did the job for credit cards nicely enough.  Nice to have, but there was no serious privacy threat on the tubes.  Now there is a big shift happening - those that are listening are using the information.  It's not there yet, but if the trend for open intel sharing continues, society will need end to end encryption just to survive.

@_date: 2017-06-09 07:06:25
@_author: iang 
@_subject: [Cryptography] stego mechanism used in real life (presumably), 
Technically, this is doable and the experts are the intel agencies who rewrite their raw intel to mask the source.
However, there is a drawback with this.  Once so done, it isn't the source material.  So it is no longer of so much use as evidence against the leakee.  That is, the leakee can simply say, "that's not our material, we didn't write that" and they'd be right.
The process of whistle blowing depends heavily on the use of internal evidence that is strong.  So the original document has to be undeniable.  (Which is to say, if the right document is denied, it compounds by adding a lie which may later turn into a perjury...)
Another issue is that we're dealing with a leaker who's aware of some tradecraft but we're also dealing with journalists who aren't so aware.  Maybe surprising to many here, but I also had forgotten about the yellow dots, and may well have made the mistake of using a photocopier.  Now that I think about it, I'd use a camera then an OCR ... but very quickly we're slowing down the whole process so much, and we're overcomplicating it.  Did they put in word-errors? Did I destroy the camera?
Remember K6 - I'm in a rush, I need to get it out quickly, else I'll cut past the process and end up with zero security.

@_date: 2017-06-09 07:24:01
@_author: iang 
@_subject: [Cryptography] Fwd: stego mechanism used in real life 
I prefer to think about what's really going on rather than layers of deception.  This way we can design a system that meets needs, not strike hidden icebergs.
Western society does not tacitly consent to spying on self.  What they do is trust the spies to follow the rules.
But there is a flaw with that arrangement - secrecy.  The spies have to do all in secrecy, and in such level of secrecy they don't know themselves what they are doing.  This makes it easy to capture - by themselves, by outside elements, sometimes by the enemy.
It also makes it easy for them to cross the line without informing the public.  To cross the line and put up window dressing or emperor's new As we've now seen countless times, accelerated with Snowden but by no means unique, the western intel agencies (and others) have frequently crossed the line of the law, and have dressed it up somehow.  When that dressing is ripped off, and the public sees what is really going on, changes are made.
For example, to drag the cyberpolitik back to *crypto*:  when we discovered that NSA was collecting all the Internet with watchers in every node, that was clearly illegal.  (We desperately need e2e and tcpinc, where is it?)  The "dress" they put on the emperor at the time was that they weren't collecting because they weren't looking. Once the lie was exposed to the public it was clearly illegal. There are I hear court cases advancing.
That is not tacit consent.  That's illegal behaviour, and society failing to see it isn't in any way tacit consent.
As a further thing:  what the agencies do *all the time* is manipulate the victim into believing that the victim somehow allowed this to happen.  It's your fault!  Tacit consent is something the agencies will sell to the public.  "Yes, but terrorism, danger, fear.... you know we have to do this, and you agree."
So, no - don't be fooled.  The agencies tell you to tacitly consent.  Say no.
What is of interest is, once we figure it out, why don't we fix the problem?  This is pure power.  The government fights fiercely to defend itself, as all cornered animals do.  Now, if the government fights and wins, e.g. quotes "state secret" doctrine so we can't enforce society's values, we lose.
Again, what's that?  We lost.  But, the agencies will then come back around and say "look, see, the courts said it was OK, you've agreed, you've given tacit consent."
I call BS.  If you accept the "tacit consent" doctrine, you've been

@_date: 2017-06-20 14:29:29
@_author: iang 
@_subject: [Cryptography] Trustworthiness 
There are better paradigms.  Whether you call them 'remedies' would depend on how broken you think the system is.
In practice, anything labelled with trust was a bit of a bait&switch.  The notion that people could "trust" systems was a misuse of the word.  In practice people rely on systems, not trust them.  You don't trust your car to get you to work, except euphemistically instead you rely on it.
In contrast, newer generation systems will use attributes or attestations [1] and not certificates [2].  These attestations will be like:
"Alice asserts X about Bob to standard Y"/Alice/
Then, systems will collect many things said by Alice, and by Bob. When a question arises there will be a process - first collect everything we know about the target subject, then filter it for relevance.  Then present or analyse or something.
As you can see, this ain't a business model, as it's directly between peers.  So it's taking a while for developers and orgs to get on board.
[1] [2]

@_date: 2017-06-21 13:50:21
@_author: iang 
@_subject: [Cryptography] doing traffic analysis for good - analysing TLS 
(not clear to me what is the key data here that indicates malware, but interesting story nonetheless.  Only a few snippets copied below...)
...by analyzing millions TLS flows, malware samples and packet captures, Anderson and McGrew found that the unencrypted metadata in a TLS flow contains fingerprints that attackers cannot hide, even with encryption. TLS is really good at obscuring plain text, but by doing so it also creates a complex set of observable parameters that engineers like McGrew and Anderson can use to train their data model.
For instance, when a TLS flow begins, it starts with a handshake. The client (like your Chrome browser) sends a ClientHello message to the server its trying to reach (like Facebook). The hello message includes a list of parameters, like what cipher suite to use, what versions are acceptable and a list of optional extensions.
ETA examines the ClientHello exchange, which holds many fingerprints that can be used to determine what traffic is malware.
TLS metadata like the ClientHello are not encrypted, because they transfer back and forth before the encrypted messages begin. This means Andersons model can analyze the unencrypted data with no knowledge of what is actually inside the message. And the model will then accurately categorize what traffic is malware and what is benign.
According to Andersons latest testing, not only does this approach preserve user privacy by not breaking encryption, but tests of ETA against large samples of  network data and malware samples show promising results for its accuracy. Using only NetFlow features, ETA catches malware about 67 percent of the time. When ETA is fed those NetFlow features with additional feature sets like Service Packet Length (SPL), DNS, TLS metadata, HTTP and others, the accuracy jumps up to more than 99 percent.

@_date: 2017-06-22 00:43:38
@_author: iang 
@_subject: [Cryptography] Trustworthiness 
This is what I call compliance.  I resist calling this trust.  To me, trust involves me taking an analysis, making a decision, taking on a risk, and then living with the consequences - reward or loss.
Wherever one talks about a Trusted XBlahSomething, we ultimately end up with no choice.

@_date: 2017-06-23 11:07:39
@_author: iang 
@_subject: [Cryptography] Trustworthiness 
This.  In that, it is more sensical to _trust_ the producers, and _rely_ on the machine.  To my mind, trust is a complicated risk analysis that a human makes on another human given some information about the past, and a risk in the future.
To apply that to a machine makes no sense (although I grant that it might be possible in a gambling machine context).  The machine is either working as advertised or it is broken.
Yup.  Where it gets interesting is when we move to a more complicated scenario.  E.g., a mixed service offering like uber.  We trust the driver to do the right thing, and we trust uber to behave well, but we rely on their software to find us a car, and get it here in the 7 minutes the app says.
We rely on Uber to make sure that the driver's care is reliable - there's little to no risk there.  We trust that the traffic won't stuff things up - but there is some risk that the driver can't fight her way So another requirement of 'trust' is that there has to be a risk. No risk, it's a belief or knowledge or certainty.  Nothing to consider.
As an aside, I wrote lengthily about trust here:
But this is part of an extended Identity Cycle that aims to figure out what the future of personal identity is.  So it's context-specific being Part 2 only.  It's also rather light on crypto, but I suppose Trust has been abused in crypto so much that we have little choice but to keep an eye on it...

@_date: 2017-06-23 11:18:55
@_author: iang 
@_subject: [Cryptography] Trustworthiness 
Yep - agree there.
I don't agree that trust is transitive... In your analogies in "Misunderstanding Trust" you use the example of the pilot trusting the chimp and therefore by extension the passengers end up trusting the chimp.  That I disagree with.  The passengers are trusting the pilot always, including to make the right decisions w.r.t. autopilots & chimps.
Only if the pilot were to walk back and propose to the passengers that they have a choice of deciding to trust the chimp to fly (and he bails out with the last parachute to make the choice real) or not - would they have been given the choice.  And if they choose to accept the pilot's offer, and the chimp flies them on, then they have transited their trust from the pilot to the chimp.
But if they don't know, they've done nothing with their trust.  If they do know and they have no choice, they have not decided to trust the chimp - they've complied with the reality that the chimp is now flying the plane.
Yes - we rely on things that always work.  Unless they're broken. But that's a statistical thing, so reliance just includes a certain percentage of brokenness.

@_date: 2017-06-29 00:36:07
@_author: iang 
@_subject: [Cryptography] OpenSSL CSPRNG work 
I would say precisely the reverse.  If you're a security expert you have a good chance in hell of doing a good PRNG.  If you're a user you have no chance.
I use chance here quite productively - hashes and encryption algorithms are delivered with testing numbers, so as long as you get the numbers to work, you're in good company.  Literally.  Users can do this.
Whereas PRNGs come with no good numbers.  There is no objective positive test.  There are only negative tests.  So the only way to know your PRNG is good is with a lot of care, and reference to the state of the art in thinking about PRNGs.
This is not for users.  For them, trusting a platform is far far safer, by orders of magnitude.
Now, you say now they are at the mercy of platforms.  This is true. But this mercy scales.  Millions of users will be dependent, and will observe and will yell loudly and get it fixed.  Each platform. And, those fixes will flow to all the other millions without them noticing.
I'm with Thomas on this one.  Both of them, and a host of others.

@_date: 2017-03-12 20:29:14
@_author: iang 
@_subject: [Cryptography] On New York's new "Cybersecurity Requirements 
It somewhat depends on what is meant by 'useful' ... or, useful to whom?
An objective look at security would say that it means to deliver to the users.  But a more cynical view would be that the users aren't the customers, instead, the corporates are the customers of security.  As long as the corporate is safe, then everyone is happy.
In this sense, the threat to the corporates includes, as well as hacking, also lawsuits, investigations, audits, loss of reputation, harrassment in the media and fines.
If for hypothetical argument, the cost of the hacking was low (to the corporate!) and the cost of the fallout was high, then the better strategy would be to reduce the cost of fallout.
The older strategy was then met by keeping all hacks a secret.  As this has fallen out of favour, what seems to emerge is a compliance approach:  as long as the corporate followed a well-accepted prescription, then the corporate has done little wrong and has been subjected to an act of nature or of God, and should deserve our compassion not our scorn.
Then, corporates need that standard to set their permissable and acceptable actions.  If the regulator can be persuaded to draft such a regulation, that would fit the bill.  A court would find it hard to rule against a hacked corporate that followed the regulation to the letter.
Right, it doesn't need to be different to what is already done, and it doesn't need to change the breaches.  It just needs to be standardised so it can protect the corporate.
Right.  Who speaks for the user?

@_date: 2017-03-19 16:01:25
@_author: iang 
@_subject: [Cryptography] Crypto best practices 
To the warrior, attack is so much more fun than defence.
And, to the bureaucrat, saying No is so much safer than saying Yes.

@_date: 2017-03-28 09:55:55
@_author: iang 
@_subject: [Cryptography] encryption + authentication - waiting - chaining 
Hi John,
I'd rather just say - most every protocol is a datagram protocol at its heart, and as most every datagram is of uncertain length, a stream protocol is far better.  If you haven't got a sponge function to hand (hint, hint) then a stream cipher is probably the next best thing to use (e.g., chacha/poly).  If not, you're going to have to look for an AEAD algorithm, most of whom are a kludge to turn a block cipher into a stream cipher.  Worst option is, you're going to have to build a stream cipher yourself out of block ciphers and modes.  There be dragons.  Good Which is to say, I'd rather tackle the issue head-on:  The block ciphers may be great but they are the wrong tool.  The stream ciphers may be crap, but they are the right tool.  For extra points, a sponge function is the better way.
Hmm.  There is another approach.  The unrealised innovation in djb's *code* for chacha/poly is something I also do:  a tight vertical stack that does the entire thing required and only the thing.  Throw out the library and build the cryptobox as the complete thing.  The construction requires top to bottom coding adroiticity or some aggressive cut&paste.

@_date: 2017-05-06 23:28:38
@_author: iang 
@_subject: [Cryptography] Big ugly security problem in post-2008 Intel 
We had a similar eye opening experience back around 2007 when we were setting up rack machines for remote management.  These big name 2U machines had remote access software that was very hard to get going.  When we got into debugging and setting up the remote management software, we realised it was so badly done that it could not be secure.
This was mystifying, so we asked around ... and the answer that came back was that everyone knew they were insecure, and that's what customers wanted.  They wanted experts from the suppliers to come in and hack the machines to get access when something went wrong.  The customers (banks) didn't care about the suppliers getting access, that wasn't their threat model.
It was observations like that that led me to suggest that actually, there was much less hacking going on than we might have thought if we just assumed our threat models were reflective of the real world?  Or that the hacking was narrowly expertised, such as SQL injection, so low hanging fruit?  So maybe we were just overspending on security and tickets for the security theater was a better proposition?
Unpopular opinions!  I think there was an element of truth in all that, but two things changed - one was the evolution serious criminal gangs which industrialised the process.  The second was the rise of cyberwar... although the jury's out as to whether this was caused by e.g. Obama's OLYMPICGAMES or as a natural evolution, a tit for tat.
By the time we entered 2010s any peace dividend was pretty much spent.  But, by that time we were screwed because crap security was institutionalised and meanwhile so was good, industrialised thievery and It isn't one thing.  At a systemic level it is pointless chasing one bug, one observation.  As far as I can tell, it's been Effing broken for several decades, and it ain't gonna change coz of this one guy.

@_date: 2017-05-10 10:25:31
@_author: iang 
@_subject: [Cryptography] Big ugly security problem in post-2008 Intel 
Yes, this is why financial cryptography was always more fun.  It has the tightest feedback loop - get it wrong and you get robbed.  This tight feedback loop is mostly absent in other forms of cryptography such as privacy, retail commerce (aka SSL & passwords), political / natsec and military.  Which means, in financial cryptography as opposed to the others, we learn the fastest, assuming we're capable of that.
Right.  We always knew that the phones were an inadequate 2FA simply because they were relatively easy to attack.  And once the stakes were high enough, they were attacked.
Now, for online banking, this was sorta maybe ok because the online banks also had other security layers inside the banks, so using the phone was their cheapest widespread option.  But this fell apart for cryptocurrencies which typically did not have other layers of protection (ok, they had verbal protections like multisig and and and).
The interesting thing is that people wake up and blame the telco. But it was never the telco's threat model to protect bitcoin or online banking...

@_date: 2017-10-02 15:26:49
@_author: iang 
@_subject: [Cryptography] Crypto basic income 
Well -- it is high quality but not perfect and can be foiled. FBI and various state labs are in a funk because their unchallengeable DNA tests that put many in prison turned out to be bad.
That only makes sense if you have a gold definition of identity. Which, is unlikely :-)
Actually I hear it is fast getting cheaper and more retail. Doesn't make it a gold standard tho.
Yes, it is :)

@_date: 2017-10-02 15:38:24
@_author: iang 
@_subject: [Cryptography] Steemit 
The users' major and important information lies on the blockchain. So if the website were seized by some Feds, then the open source (recent) could be downloaded and another website started up again.
The more important thing about Steemit is that it shows how to do a Dapp. Along with Bitshares, it shows how to do a business that is entirely on the blockchain, serves people in a positive fashion, and does not actually need people t drink the blockchain koolaid in order to Steemit is pretty unique in its exposition of positive-sum blockchain businesses. Another that is starting up now with a 2nd version is OpenBazaar which is retail auctions of stuff for sale, for a range of crypto currencies.
In contrast, gaming (which is the big user) and Bitshares are zero-sum games. The house always wins. These are good for entertainment but they are not the foundation of an economy. By definition.
Yes, I'd agree. There are some sites which also access all the media. I haven't followed it that deeply.

@_date: 2017-10-02 16:02:34
@_author: iang 
@_subject: [Cryptography] Altcoin volume 
Or, if you are in FOSS the way it should work is that someone who has a bright idea should work on a new system in their own time. When it is getting somewhere they should get enlightened, steal time from the day job and their family life, hole up in a cave and make it so. As they're now unemployed and single their code moves fast. When the first prototype is released, the whole world collectively goes WOW and everyone starts to contribute, because that's what FOSS does. A team built, versions come out, people are very happy. Then a corporation comes along and hires the original author (who's very thin by now) just to only work on the product. Life is good. An economy emerges.
Or, if you're a financier, you take your hot slide deck to Sand Hill Road and trawl around trying to fight this month's meme because you actually developed a meme that isn't current for this month, so you have a *lot* of explaining to do. Eventually you find a VC who cuts you a little check for a huge piece and you spend the lot on hired programmers (lots nearby in the valley) and also replacing them as they seem to pop in and out of all the hot startups. Finally you put out a demo and your Series A investor convinces you why it is in your best interest to start the whole process again for your Series B round. Etc...
Or, maybe you're a traditionalist and you live in ma&pa's garage and program away with a little money from family. Eventually you get enough servers up to get going and bring in customers. It's all friends & family funded, no debt, no outside equity. But it grows slowly so there is frustration.
Then there is ICOs.
Without commenting on EOS for the moment, the ICO market seems to have boomed because of several factors: the shortage of "I get it" investors, the ocean of "I've a bright idea" kids, the compliance nightmare, the FOSS absence of a way to pay for people to do the right thing, the rise of the Ethereum price to the point where sudden new wealth had to diversify because it knew it was crazy, the shocking price of garages today, and the humongous promise of Ethereum versus the equally enourmous shortage of actual real measurable results.
Within that context, the EOS auction process (not ICO) was designed to solve some issues experienced in the past - primarily to neuter the pump & dump, and the recognition that some fair distribution over time is required in order to build a community. EOS auction process was also heavily informed by what went wrong (and right) with Bitshares (not enough money, pump & dump) and Steemit (too much concentration, pump & It is recognised, or should be recognised, that there is no one true way to do this. Apologies to all. Cryptography has created a chaotic market, and those who have read the old cypherpunk material should not be surprised that it's noisy and opinionated and still experimental :-)
ps; cryptography didn't really create this market. People created the market, and crypto just provided some essential components...

@_date: 2017-09-05 10:22:50
@_author: iang 
@_subject: [Cryptography] Finding Nemo's random seed 
When Pixar wanted to release its 2003 film Finding Nemo for Blu-ray 3D in 2012, the studio had to rerender the film to produce the 3D effects. The studio by then was no longer using the same animation software system, and it found that certain aspects of the original could not be emulated in its new software. The movement of seagrass, for instance, had been controlled by a random number generator, but there was no way to retrieve the original seed value for that generator. So animators manually replicated the plants movements frame by frame, a laborious process. The fact that the studio had lost access to its own film after less than a decade is a sobering commentary on the challenges of archiving computer-generated work.

@_date: 2017-09-05 14:53:31
@_author: iang 
@_subject: [Cryptography] Tezos 
Hi James,
My understanding is that Tezos is a DPOS or delegated proof of stake system, having adopted the mechanism invented by Dan Larimer for In short, DPOS separates out the problem of PoS into two layers:
 * a selected set of block producers (aka witnesses), and
 * an on-chain voting mechanism to vote in/out the producers proportional to stake.
This separation solves the "nothing at stake" problem. I've written an introductory high level description to DPOS here:
And here's a more practical take of the workings of the Steemit DPOS system, with numbers:
Assuming that the DPOS method has been captured faithfully within Tezos, the elections by the 50.1% are conducted through an on-chain smart contract mechanism and are therefore fully visible and auditable.
I don't follow Tezos so am unclear how they intend to start off, but it is a challenging problem. Let me describe how EOS intends to start off for some illumination on the issue (note, below).
As established precedent in 2009, the goal is to launch the chain without there being an operator. In EOS.io's design, there will be a release of the final software, and that will be the end of the responsibility of the bootstrapping software authors.
Then, the community (quite large) that surrounds EOS will compete to get it up and going. Many chains will start. The goal is to get one true chains. To achieve this we have a few things in the favour of quick  - Substantial network effects of the one true chain being worth a lot more to everyone than multiple small chains.
 - Substantial pressure on the producers to agree quickly on the chain so that they can start making money, and not trying to produce for all wannabe chains.
 - Finally, the software is constructed to include a threshold of 15% of the value in order to trigger advanced functions like payments. The software will include the list of EOS token holders and their value, and it will wait until 15% of that value votes on a confirmation to trigger advancment to the next phase.
It will be a lot of fun to watch.
:-) It also happens to be the fundamental problem of software. To write it we need to fund the devs. And once we have funding, we can take the risk of seeing if the ideas work out. Hopefully that will lead to making money.
The difference with the blockchain process is that first we make the money. Then we build it. Then we make the money. ;-)
Disclosure: I'm related to the EOS project and not to the Tezos project. They are approximately similar projects at a helicopter level. Both have sold tokens for something north of $200mm, both use DPOS, and both are proposing smart contracting blockchains.

@_date: 2017-09-05 15:51:47
@_author: iang 
@_subject: [Cryptography] What's happening with Intel's SGX? 
Yes, R3, the banking consortium for blockchain-like things, is using SGX to add a better privacy option to its Corda product.
ps; disclosure, I had something to do with the early architectural work of Corda but not the SGX part.

@_date: 2017-09-05 20:48:32
@_author: iang 
@_subject: [Cryptography] What happens when a smart contract forks? 
This is not only a good question, but an important one:
Both forks have valid coins, so you doubled your coins. However the value generally splits Presumably unevenly, presumably summed to 1. But oddly the sum of the value of the 2 coins can surge to greater than the original, just on the buzz.
This actually an OK result. We know precisely what happens - 2 for 1 split. We know the accounting, we know what that means simply because we know that a coin has no meaning, so two coins has two no meanings. The only thing is that supply and demand bounce around a bit, which is fine, another day on the markets, exciting for some but always balanced at the end of the day. We've now run the experiment twice and the world hasn't stopped spinning.
But there is another question - what happens to a smart contract that is on a chain that forks?
We do not and can not know the answer to this question in a completeness sense - because we don't know a priori what every smart contract does. We do know what a cryptocurrency does, a priori, because all it does is move numbers around. But a smart contract is more complicated - we don't know whether it has issued rights, whether it has imposed obligations, liabilities etc.
In short, it would be the same technical effect as the coin, there are now two contracts. Which means two sets of rights and two sets of obligations, two sets of liabilities.
If you hold the obligations and they suddenly double on you, are you capable of meeting those? Are bankrupt, are you reneging, are you doubling your workload?
If you hold the two sets of rights, you might be happy but at the other end of the rights are two sets of obligations, and the holder of those might be ins strife. Or, you might find yourself delivered with two sets of rights that are wasting because you can't consume them fast enough, if one considers the futures market.
I think the brutal conclusion is that you can't safely do rights/liabilities/obligations in smart contracts on any chain that has a chance of splitting.
Brutally, simplistically, this probably means you shouldn't do smart contracts on Bitcoin or Ethereum. At least until you've resolved how you deal with the fork.
In example terms, imagine Ethereum were to fork today - what would happen to all the ERC-20 ICOs?

@_date: 2017-09-25 12:13:57
@_author: iang 
@_subject: [Cryptography] Altcoin volume 
Hi James,
There can only be one winner with zero semantics. Two cryptos with zero semantics have no differentiation, and thus the primary mover is likely to squeeze all copies out through network effects.
Winning then becomes a process of differentiation. Which can be ephemeral such as the altCoins or it can be technological such as the enhanced chains like Ethereum. To which we can now add the newcomers like Tezos and EOS (disclosure; I work with these guys), which seek to do things better again.
Then you've got the forked Bitcoins - Bitcoin Cash and Bitcoin Segwit. They are competing on more subtle technological differences, on scaling, and on politics.
As to who is winning, ... wait and see :-)
Yes, this is one of the lessons of DPOS. In practice, the limit of 21 producers or witnesses or delegates is informed both by community (in)attention to the process, and also by the desire to reduce finality of transactions down (2/3 of the producers therefore 42 seconds or so).
I don't think China will upset Bitcoin's hegemony, unless they go after the hash farms. If that happens, Bitcoin has a big problem. More rumblings here:  lol... It is somewhat surreal that new tokens are refusing sales to both US persons and to Chinese persons.
Ha! Putting biased corporate hat on: Building, building, building. Alpha-grade testnet is supposed to be happening as we speak. Devs are being brought in. Much conference.
For those who don't follow: EOS has a novel coin distribution strategy that was designed to mimic mining and to eliminate the pump and dump: with a 1 year broad distribution using daily auctions. This won't be complete until mid next year, at which point the tokens will be switched off, the community will start up its chain, and the real EOS tokens will be enabled.

@_date: 2018-01-03 22:52:51
@_author: iang 
@_subject: [Cryptography] crypto leak to Iranians via Iraqis 
James Risen
3 STELLAR WIND
IN THE SUMMER of 2003, the New York Times named a new Washington bureau chief: Philip Taubman, an old friend of Bill Kellers. Taubman had been the Timess Moscow bureau chief when Keller won a Pulitzer Prize as a correspondent there. Now Taubman was Kellers man in Washington.
Taubman and I developed a friendly relationship. He had covered national security and intelligence matters earlier in his career, and he seemed eager for scoops. But by 2004, I began to disagree with some of his decisions. That spring, I learned that the Bush administration had discovered that Ahmad Chalabi, the neoconservatives golden boy in Iraq, had told an Iranian intelligence official that the National Security Agency had broken Iranian codes.
That was a huge betrayal by the man some senior Bush administration officials had once considered installing as the leader of Iraq. But after I called the CIA and NSA for comment, NSA Director Michael Hayden called Taubman and asked him not to run the story. Hayden argued that even though Chalabi had told the Iranians that the U.S. had broken their codes, it wasnt clear the Iranians believed him, and they were still using the same communications systems.
Taubman agreed, and we sat on the story until the CIA public affairs office called and told him that someone else was reporting it, and that we should no longer feel bound not to publish. I was upset that I had lost an exclusive, and I believed that Haydens arguments against publication had been designed simply to save the White House from embarrassment over Chalabi.
(The rest is worth reading too!)

@_date: 2018-01-04 21:30:30
@_author: iang 
@_subject: [Cryptography] Bitcoin theft and the future of cryptocurrencies 
One would think, modulo hype.
The Bitcoin community has bought into what might be called the "security of the chain" fallacy - that the security of the chain is more important than the security of the users. To an extent this aligns with price increases, use cases and the like.
Part of the flaw inherent in the approach is what we grey hairs used to call WYTM - who are you threatened by, how, why? If you are threatened by the state, then that's one thing. If you are going to be robbed by your spouse in a legally sponsored attack called divorce, that's another thing. Cryptocurrency are mostly worried about one set of risks that could be said to be lower risk than average John Doe cares about.
Since early days, hardware wallets, cold storage and multisig have been proposed. But these are hard to use.
I am working on a hybrid model where cold storage is issued to a (governable, reversable) derivative issue that is managed by a TTP that is "us".
It's complex of course to get the mix right. Part of the problem is whatever design you come up with, there needs to be some reversibility built in at some point. Which kinda flies in the face of raw blockchain. So blockchain needs to compromise, as do the people.

@_date: 2018-11-03 19:12:31
@_author: iang 
@_subject: [Cryptography] Why the NSA called me... 
Amusing read about source, crypto, cooperation and the NSA.
Why the NSA Called Me After Midnight and Requested My Source Code
The story behind my top secret coffee cup
Please listen carefully and dont hang up. Those were the first words this unknown male caller said to me when my brother handed me the phone.
It was the July 4th weekend, 2000, give or take a day, and Mr. X knew to say that first because he was calling me after midnight at my brothers house in Connecticut. This was beyond creepy because I lived in California and nobody knew I was in Connecticut except for my immediate family, who were all there with me in the house. I had only arrived the day before as I do most years about this time for our annual family picnic.
Why was this guy calling me?
  It was a matter of national security.

@_date: 2018-11-11 16:45:40
@_author: iang 
@_subject: [Cryptography] Early data mining efforts by the KGB defeated the 
How to explain the KGBs amazing success identifying CIA agents in the Paranoid CIA heads blamed Soviet moles, but the real reason for the repeated disasters was much simpler
follow us in feedly
JONATHAN HASLAM
SEPTEMBER 26, 2015 2:45PM (UTC)
As the Cold War drew to a close with the fall of the Berlin Wall in November 1989, those at CIA headquarters in Langley, Virginia, finally hoped to resolve many long-standing puzzles.
The most important of which was how officers in the field under diplomatic and deep cover stationed across the globe were readily identified by the KGB. As a consequence, covert operations had to be aborted as local agents were pinpointed and CIA personnel compromised or, indeed, had their lives thrown into jeopardy.
The problem dated from the mid-'70s, the very time that James Angleton, the paranoid head of agency counterintelligence, was at last ushered out of office, to the relief of conscientious officers hitherto cast under a dark cloud of suspicion, their promotion delayed or, worse still, denied, and in some cases entire careers wrecked.
But could Angleton have been right? Some consistently maintained so, notably the late Bruce Bagley. Their argument was simple. How could these disasters have happened with such regularity if the agency had not been penetrated by Soviet moles?

@_date: 2018-11-01 01:40:13
@_author: iang 
@_subject: [Cryptography] Ten years ago today 
============================== START ==============================
People like Hal and Bear thought it was great. Guys like James and I thought it was daft. Can't recall what happened next ;-)

@_date: 2019-12-03 00:02:28
@_author: iang 
@_subject: [Cryptography] 795-bit factoring and discrete logarithms [NMBRTHRY] 
*Subject:* 	
795-bit factoring and discrete logarithms *From:* 	
Emmanuel Thom? <[log in to unmask] *Reply To:* 	
Number Theory List <[log in to unmask] Emmanuel Thom? <[log in to unmask] *Date:* 	
Mon, 2 Dec 2019 13:53:58 +0100
*Content-Type:* 	
*Parts/Attachments:* 	
text/plain (91 Dear number theorists,
We are pleased to announce the factorization of RSA-240, from RSA's challenge
list, and the computation of a discrete logarithm of the same size (795 bits):
RSA-240 = 124620366781718784065835044608106590434820374651678805754818788883289666801188210855036039570272508747509864768438458621054865537970253930571891217684318286362846948405301614416430468066875699415246993185704183030512549594371372159029236099
 ????????= 509435952285839914555051023580843714132648382024111473186660296521821206469746700620316443478873837606252372049619334517
 ????????* 244624208838318150567813139024002896653802092578931401452041221336558477095178155258218897735030590669041302045908071447
Let p = RSA-240 + 49204 be the first safe prime above RSA-240. We chose
as a target the encoding of the sentence "The magic words are still
Squeamish Ossifrage" (in reference to the factorization of RSA-129 [1]):
target_str="The magic words are still Squeamish Ossifrage"
target_hex=`echo -n $target_str | xxd -p -c 256`
target=`echo "ibase=16; $target_hex" | BC_LINE_LENGTH=0 bc`
target = 774356626343973985966622216006087686926705588649958206166317147722421706101723470351970238538755049093424997
we have with generator g = 5:
log(target) = 92603135928144195363094955331732855502961099191437611616729420475898744562365366788100548099072093487548258752802923326447367244150096121629264809207598195062213366889859186681126928982506005127728321426751244111412371767375547225045851716
which can be checked with 5^926...716 = target mod p.
The previous records were RSA-768 (768 bits) in December 2009 [2], and
a 768-bit prime discrete logarithm in June 2016 [3].
It is the first time that two records for integer factorization and discrete
logarithm are broken together, moreover with the same hardware and software.
Both computations were performed with the Number Field Sieve algorithm,
using the open-source CADO-NFS software [4].
The sum of the computation time for both records is roughly 4000
core-years, using Intel Xeon Gold 6130 CPUs as a reference (2.1GHz).
A rough breakdown of the time spent in the main computation steps is as
 ????RSA-240 sieving:  800 physical core-years
 ????RSA-240 matrix:   100 physical core-years
 ????DLP-240 sieving: 2400 physical core-years
 ????DLP-240 matrix:   700 physical core-years
The computation times above are well below the time that was spent with
the previous 768-bit records. To measure how much of this can be
attributed to Moore's law, we ran our software on machines that are
identical to those cited in the 768-bit DLP computation [3], and reach
the conclusion that sieving for our new record size on these old machines
would have taken 25% less time than the reported sieving time of the
768-bit DLP computation.
Another estimation can be made with the rough complexity ratio given by
the L_N(1/3,(64/9)^(1/3)) formula that, up to (1+o(1)) factors in the
exponent, is customarily taken as an estimation of the expected hardness
increase from one computation to the next. This would suggest that
795-bit computations should be 2.25 times harder than 768-bit
computations. Taking this into account, and still using identical
hardware, our computation was 3 times faster than the expected time that
would have been extrapolated from previous records.
The acceleration can be attributed to various algorithmic improvements
that were implemented for these computations.  The CADO-NFS
implementation was also vastly improved.
We used computer resources of the Grid'5000 experimental testbed in
France (INRIA, CNRS, and partner institutions) [5], of the EXPLOR
computing center at Universit?? de Lorraine, Nancy, France [6], an
allocation of computing hours on the PRACE research infrastructure using
resources at the Juelich supercomputing center in Germany [7], as well as
computer equipment gifted by Cisco Systems, Inc. to the University of
More details will be given in a forthcoming scientific publication.
Fabrice Boudot, ??ducation Nationale and Universit?? de Limoges, France
Pierrick Gaudry, CNRS, Nancy, France
Aurore Guillevic, INRIA, Nancy, France
Nadia Heninger, University of Pennsylvania and University of California, San Diego, United States
Emmanuel Thom??, INRIA, Nancy, France
Paul Zimmermann, INRIA, Nancy, France
[5]

@_date: 2019-12-06 00:18:23
@_author: iang 
@_subject: [Cryptography] that endangered species, the email mitm, 
Being one of those who has said repeatedly for decades that we got the whole threat model thing precisely backwards, because there is approximately zero evidence of that mythical beast, the MITM, can only present myself for castigation - here's an actual MITM over email spotted in the wild.? Which of course now justifies 3 decades of trying and failing to secure email... We must try & fail harder.
Tech by VICE Hackers Trick Venture Capital Firm Into Sending Them $1 Million
A Chinese VC firm and an Israeli startup had the money stolen right out from under their noses thanks to spoofed emails and bogus domains.
by Karl Bode Dec 5 2019, 1:00pm
Security researchers at Check Point say the company has uncovered evidence that Chinese hackers managed to hijack $1 million in seed money during a wire transfer between a Chinese venture capital firm and an Israeli startup?without either side realizing anything was wrong.
The VC firm and the startup, whose names Check Point hasn?t released, reached out to the security firm after the funds failed to arrive. Once Check Point dug into the details, it discovered a man in the middle attack that took a lot of planning and plenty of patience.
After analyzing the server logs, emails, and the computers involved in correspondence between the companies, Check Point noticed some abnormalities. Some of the emails, analysts discovered, had been modified. Others hadn?t even been written by either organization.
After seeing the original email thread announcing the upcoming multi-million dollar seeding fund, the hacker took action. Instead of monitoring subsequent emails by creating an auto forwarding rule (standard practice in traditional attacks), the hacker started by creating two lookalike domains.
?The first domain was essentially the same as the Israeli startup domain, but with an additional ?s? added to the end of the domain name,? Check Point said. ?The second domain closely resembled that of the Chinese VC company, but once again added an ?s? to the end of the domain  From there, the attacker sent two emails with the same subject header as the original email?one posing as the starup?s CEO from the copycat startup domain?and a second sent to the Israeli startup from the copycat Chinese VC firm domain, spoofing the email address of the VC account That opened the door to a man in the middle attack whereby every email sent by each side of the exchange was in reality sent to the attacker, who then edited the emails to include bogus information and banking details, then forwarded them from each lookalike domain to its original Throughout this process, the hacker sent a total of 18 emails to the Chinese VC firm and 14 to the Israeli startup ahead of the compromised bank transfer. At one point, the VC account manager and startup CEO scheduled a meeting in Shanghai, putting the hijack at risk. So the hacker sent emails to both sides, making up different excuses to cancel the meeting:
?Patience, attention to detail and good reconnaissance on the part of the attacker made this attack a success,? Check Point said.
After successfully using the man in the middle attack to hijack the funds, the attacker, who still hasn?t been identified beyond his origins in Hong Kong, tried to go after another round of VC investment money. The CFO of the Israeli startup continues to receive one email a month from the spoofed CEO account, urging him to conduct another wire transaction, Check Point said.
The security firm says there?s several things companies can do to avoid a similar fate.
That includes adding a second verification by calling the person who asked for the transfer, keeping audit and access logs for at least six months to ensure the integrity of your email infrastructure, retaining as much evidence as possible when dealing with suspected hackers, and using tools to help spot duplicative, phony domains.

@_date: 2019-07-21 13:20:03
@_author: iang 
@_subject: [Cryptography] Accounting Resource Location (was: Questions of 
Hey Phillip,
any update?? do you have any other writings on this idea?
The deployment is hard because accounting moves slower than glaciers.? Having looked at this idea for the same era, my conclusion is that there is no way to do "just that."? Instead, it has to be piggy-backed onto some other application or business process.? In other words, it is not so much an invention as a design pattern that should be slipped stealth-wise into the business and allowed to spread outwards.? I've recently been consulting with someone who is doing just that, it turns out to be very valuable when one has the right incentives.
The basic ideas should be hard to patent - it is similar or same or related to what Todd Boyle was talking about in the late 1990s on his web site now cached here:
His design was called the Shared Transaction Repository, altho being an accountant and not a cryptoplumber, he didn't get deep into the magic of hashes and exactly how to share the docs.? He coined the term "triple entry accounting" which I wrote about later:
Which also was something of an inspiration for Bitcoin in some mythical sense.? Having said all that, one can easily convince the patent office of some of the detailed implementation issues, and chances are that is already happening.? Doesn't make it right, but patents aren't right, they are rights.
More recently, R3's Richard Brown has been marketing the notion that "I know that what you see is what I see" as an aphorism leading to the benefits of known equivalence as well as integrity.? It turns out that when done in certain fields such as clearing & settlement, there are dramatic short cuts one can take with the old systems if we know that there are no reconciliation issues to bite us.? Coincidentally, my description of these processes fell out of a design for payments that was intended for exactly that - trading in real time gross settlement, without the clearance & settlement nightmare.
Yes it would work.? That challenge is to not to build it, but to encourage adoption.? This is chicken & egg.? Someone has to take the lead and start posting these.
Then, as soon as you start posting these, the worryworts come out of the woodwork.? Privacy, security, compliance, taxman, patriotism, ... everyone's got their criticism.? So part of the puzzle is to find a space where it can be protected for long enough to keep back the no-dooers.? Like mPesa was protected.? Not sure what that is tho.
Business wise, it could be loyalty points - the receipt QR could lead to not only the digital receipt but also include the extra points needed to get the "free coffee after 6" deal.? Wrap the app around it and start As above, I do not think just investing will work.? What you need is to convince someone who is already doing something quite close to get a bit closer.? We need to borrow someone else's chicken, make some quick evolutionary hacks, and then try for our first new egg.
Any update?
ps; and did you resolve the 4-4-4 vs 5-3-5 question?

@_date: 2019-09-12 23:29:26
@_author: iang 
@_subject: [Cryptography] TRNGs as open source design semiconductors 
The concept of RNs (random numbers) is pretty vexing, and it doesn't seem to bend to the normal security thinking.? Even though an open source chip might be better, it still presents a supply chain attack and when it comes to actual production, chip manufacture is anything but Because of issues like this, thinking in RN generation ("true" or pseudorandom or otherwise) has changed somewhat.? In short, RNs should be primarily directed in software, and the sources or seeds for this should be diversified out to different producers.? If there is a good hardware source, use it.? If there are several, use them all.? The, the software task reduces to combining/mixing many hopefully independent Once that is accepted, the task is now different.? In the hardware sense, we don't care about a perfect TRNG any more, we now care about many okRNGs, with the emphasis on independent not truethity.
Which leads to the notion that actually, a simple hardware design, or many designs, could be a good thing.? There is no particular reason why hardware manufacturers couldn't add in a small RNG into the left over 0.1% of chip area.
Then software could XOR them all.
Right. So if we have multiple sources into a software mixer, then all that babbling disappears and any good-enough source becomes good enough.
Any source on a chip can support crypto as long as there are more than 1 source, and they get mixed in software.? The more the merrier.? The slappier the sloppier, we'll take them all.
ps old rant:

@_date: 2020-01-07 16:12:32
@_author: iang 
@_subject: [Cryptography] Dan Geer: nothing can make online trustworthy 
I demur. I suspect this view is based on (a) assuming that a thing is either completely trusted or it is not. That is a misuse of the word trust. And (b) that a thing can be trustworthy.
(a) Trust is a risk analysis informed by many things - experience, memory, rewards & punishment, reputation, recommendations, etc.? Some would say it is the essence of childhood.? E.g., learning how to trust is growing up. It is a risk, always, and therefore a negative outcome is always possible. For that potential cost there needs to be a reward, which pays out most times. The outcome of trust is never binary, always uncertain, should be profitable, but always a risk.
Eg, trust is like gambling when you are the house.
(b) Hence, it is ideally suited to person to person interaction, and trust is something that shines when people use it on people. When people trust machines like "online" it is a form of animism - pretending the object is like a person, in order to analyse the risk. This kind of works in some settings and contexts but not in others. Eg machines break trust because statistically they break; companies break trust because the deceive.
So, applying trust and its associated things like trustworthiness or trustlessness to a machine like net or blockchain is not how the brain was designed. Necessarily, in an adversarial world, this will not work out so well.
OTOH, we do know how to connect persons to persons online. If we ignore the "online" part and make sure the persons are trusted, then we can do that trust of the other person, only online.
But, people online are (often) untrustworthy. The problem therefore isn't how to make online trustworthy, it's how to make people trustworthy. And for that, looking at tech is the wrong place. Go back to RAH's old writings and look at anthropology. Ask where there is trust, and look at that.
A book?
(trust me, I'm) iang

@_date: 2020-11-13 18:09:29
@_author: iang 
@_subject: [Cryptography] Encrypting messages with artificial bacterial 
For your amusement :)
Encrypting messages with artificial bacterial receptors
A method for encrypting messages using engineered bacteria and different fluorescently labeled synthetic receptors is described. We show that the binding of DNA-based artificial receptors to E. coli expressing His-tagged outer membrane protein C (His-OmpC) induces a F?rster resonance energy transfer (FRET) between the dyes, which results in the generation of a unique fluorescence fingerprint. Because the bacteria continuously divide, the emission pattern generated by the modified bacteria dynamically changes, enabling the system to produce encryption keys that change with time. Thus, this development indicates the potential contribution of live-cell-based encryption systems to the emerging area of information protection at the molecular level.
Keywords: artificial receptors; cell surface modification; fluorescent probes; molecular cryptography

@_date: 2020-11-14 01:17:54
@_author: iang 
@_subject: [Cryptography] IPsec DH parameters, other flaws 
coming late to this party... but I'll bet the permathread will be running for a decade.
The key is to start asking who, not how. It's clear that the IETF/etc was setup to allow vendors to duke it out. Which opened the way for NSA & friends to futz up the security groups with targetted interventions. In short to bring the process to a standstill where their security was So the answer is to not do that - not do committees, working groups, and not rely on the good faith of participants. When there is an attacker who is prepared to outspend you and out-faith you, you have to change the process. In this context, the who cannot be a committee.
The who has to be individuals / tight teams.
Now the how.
The NIST AES process showed one way: a knock down competition. Set the requirements, invite open submissions. Only one proposal wins. Set a schedule. Stick to it. Thunderdome. 30 proposals enter, 1 leaves.
Another way (also shown by NIST) is to contract someone credible to do the job. This was done with SHA2. To date, criticisms have been glancing, including as proven with SHA3 competition.
Ofc, one could criticise and say this can only be done with very simple designs that have nothing to do with hiding information. But actually, there are many people & teams who we could probably widespread trust to do a good job, given strong requirements.
The problem here might be how to stop nefarious agencies (NSA) from spiking the project while in gestation. Here, strong requirements, transparent schedule and many well known observers can help.
ps; See you in another decade.

@_date: 2020-11-14 02:16:24
@_author: iang 
@_subject: [Cryptography] Possible reason why password usage rules are 
Military crypto is very different to civilian crypto.? In the latter, there is a sense that some random Alice has to talk to some random Bob, and they don't know each other and can't trust each other so have to go through some form of ceremony before they can be permitted to know and trust each other.
In the military it's the other way around. Every soldier needs to know and trust all other soldiers in the same military. For example (and this applies to my time many decades ago) it is the case that any soldier on the ground should be able to call in indirect fire support from any assets in the area - which includes aircraft and ships.
In order to enable this, the crypto system had to be centralised. Recalling that (a) until recently it wasn't likely that soldiers could carry sophisticated tools that could do things like public key encryption and (b) security at the sharp end was tactical, which meant you only had to keep the secrets for O(day). Especially, any battle info was already compromised because the enemy was on the receiving and and knew what you were likely sending.
The answer then was a very large centralised distribution of books, which changed over like daily or every 6 hours. Everyone had the same books, and everyone had to synchronise changeover at the same time (and same zone) as stated in Zulu time (GMT).
The same thing is seen at the unit level - the CO sends out a patrol from one squad, and it has to come back in through another squad. Ofc they both have to have the same code words. Which means codewords need to be rotated daily as everyone has them, if anyone gets captured he only needs to hold out for a day.
Bear in mind that military crypto has a much longer history, numbering in centuries. Whereas civilian crypto up-ticked in the 80s (Unix delivered DES as an early mistake) and only became really important in the 90s. All the expertise was initially military - even the guys who were involved in Unix security eg Morris were connected to military and IC.
So assumptions were picked up and not questioned. The Internet had to find out the hard way that we had to discard the received wisdom and rebuilt it from 1st principles.

@_date: 2020-11-26 21:48:22
@_author: iang 
@_subject: [Cryptography] Second Swiss firm allegedly sold encrypted spying 
Swiss public television, SRF, has found a second company besides Crypto AG was involved in manufacturing manipulated devices allegedly used for spying by foreign intelligence.
November 26, 2020 - 11:34
According to SRF sources, the Swiss company Omnisec AG had ties to US intelligence services. This follows revelations in February by SRF, German television ZDF and The Washington Post that Zug-based firm Crypto AG was at the heart of a huge international spying operation led by the CIA, and to a lesser extent by the German BND spy agency. Omnisec was one of the largest competitors of Crypto AG.
Swiss cryptologist and professor Ueli Maurer was a consultant for Omnisec for years and told SRF that in 1989 US intelligence services (National Security Agency) contacted Omnisec through him.
Of concern are the OC-500 series devices. Devices were sold to several Swiss federal agencies. However, Swiss authorities only noticed the devices weren't secure in the mid-2000s.
