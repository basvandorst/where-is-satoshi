
@_date: 2013-10-21 18:20:08
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Mail Lists In the Post-Snowden Era 
There has at least been publicly done research on the matter. See for
example this March 2011 blog post by Bruce Schneier,
which references a web page which unfortunately since then appears to
have been taken off the 'Net, but The Wayback Machine has a copy at
I doubt a major government agency could not do something similar, and
probably better.

@_date: 2013-10-22 10:57:25
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Mail Lists In the Post-Snowden Era 
On 21 Oct 2013 15:31 -0400, from grarpamp at gmail.com (grarpamp):
I never intended for my post to indicate otherwise. It was simply
intended to point out a fairly recent example of public research in
the matter, in response to Devin's comment. I apologize if I did not
make that sufficiently clear.

@_date: 2014-08-02 11:43:09
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
On 1 Aug 2014 11:42 -0700, from trevp at trevp.net (Trevor Perrin):
I get the feeling that this is comparing apples to oranges. AES-256
uses 14 rounds, AES-128 uses 10 rounds. There's your 40% difference
right there. Now, there were reasons for going for more rounds with
AES-256 than with AES-128, but to me, they do not appear to be
intrinsic to the key length _itself_, but rather a function of known
attacks on the cipher for a given key length. For another example,
neither Blowfish, Twofish or Threefish have different numbers of
rounds for the varying key lengths, although Threefish has more rounds
for the largest block size.
In general, this discussion seems a lot like it is about choosing the
One Curve to rule them all. What if that isn't possible, let alone
Personally, _as a systems designer and developer_, I'd prefer to have
a few options. Not many, mind you, but a few. Sort of like AES does by
specifying 128, 192 and 256 bit key lengths; I can go with "fast, and
sufficient for almost any needs" (128 bits) or I can accept the
performance penalty and use "even more" if for some reason I feel 128
bits of key is not sufficient (very long term secrecy needs against a
potentially determined and powerful adversary, for example). I could
then make the argument in each specific instance why I'd choose one
over the other and what considerations went into the trade-off between
key length and encryption/decryption throughput on a particular class
of hardware.
The way to do that with ECC would appear to be to have multiple curves
with different work factors which I can trust to not be bongoed. Don't
force me to use a comparatively fast WF128 curve for very long term
secrets which are rarely accessed, for example, or a much slower WF256
curve for things that pretty much only need to be kept secret while
they are actually _in transit_ (for example, because other parts of
the protocol make them useless for more than one time usage).
128 and 256 bits are common enough key lengths to, in my mind, warrant
widely vetted (and well described how chosen, and how those
considerations stem from algorithm requirements) corresponding work
factor curves. 192 bits, maybe. Longer keys, _possibly_, but by then
you could easily argue that we are well into tin-foil hat territory;
even a meet-in-the-middle attack against a 256 bit key appears totally
unrealistic to me, so at that point, other kinds of attacks than
directly against properly implemented cryptography appear much more

@_date: 2014-08-03 15:06:57
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
On 3 Aug 2014 14:13 +0100, from iang at iang.org (ianG):
I would argue that this depends on the type of software, and the type
of user.
Certain types of software probably can usefully allow the user to pick
among options; other types of software might, by making the user go
well out of their way; others would choose based on what they key is
going to be used for (PFS, ephemeral, long-term; active or passive
adversary expected; military or diplomatic secrets, controversial or
non-controversial web browsing; ...); yet other types of software
definitely should not offer a choice at all, and just stick to one or
If I, even as someone who _is_ interested in all this, save an
encrypted, password-protected spreadsheet, I don't want to have to
consider the minutiae of the key length choice or key derivation
algorithm; just make a choice for me already, preferably an
interoperable and secure one.
If the "weakest" option is "good enough for most purposes", as for
example is the case with AES' weakest option being a 128-bit key, I
don't see giving choices _to those who are in a position of making an
informed selection among them_ as being a major problem. Who that is
depends on the target audience.
If against a determined attacker the weakest option is little better
than a Caesar cipher, then yes, that's _asking_ for trouble.
If curves can be chosen that correspond, at least roughly, to commonly
used symmetric key lengths, then the choice between those curves
becomes similar to picking 128, 192 or 256 bit keys for AES. That's a
choice a systems designer, even one without extensive knowledge of
cryptography, can grasp: either the shorter key with faster crypto, or
the longer key with slower crypto. The performance difference can be
stated (and hopefully explained well), tested against acceptable user
experience criteria on target classes of hardware, and possibly given
time code can be optimized such that the difference is reduced. The
implication of either choice on an attacker, particularly a passive
attacker, can to some degree be quantified. These are matters about
which a technical discussion can be held, and an appropriate choice
made in each instance. Boss wants to be able to tick "256-bit security
throughout"? Just pick that curve and key length, and live with the
performance penalty. Boss wants to be able to show off the
performance? Pick the faster options, and optimize the code like
you're (and the boss is) a madman. And so on.
Hence my position that, as a designer or implementer, having _a few_
_clear_ choices each with _known_ advantages and drawbacks provides
choices for those who want them. Assuming that the weakest choice is
"good enough" (for some definition of "good enough"), then no matter
what choice is made out of those the result will be "good enough".
That is within the constraints of the particular implementation, but
the implementation should essentially be unaffected by whether one
curve/key length or another is chosen; the assumption being that the
algorithm itself is the same.
We can't prevent software writers from giving choice to users who are
not in a position of being able to make an informed decision. It _is_
possible to give software writers a set of options such that none of
the options offered are terrible for any reasonable scenario, which
means that even _if_ someone makes an uninformed decision the
consequences aren't horrible (fail safe!). Don't discount the
possibility of a designer or implementer making an uninformed decision
on which alternative to go with, either; again, if all choices are
reasonable, the results might not be great, but they also won't be
Indeed, that is definitely one approach for some types of software,
particularly those that target regular end-users.

@_date: 2014-08-08 16:45:16
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
On 8 Aug 2014 11:14 -0400, from crypto.jmk at gmail.com (John Kelsey):
Well, that's pretty much the point I'm making, or at least tried to
_Personally_, most of the time I'd rather have "slow by default" over
"insecure by default", but I am certainly _not_ advocating insecure
alternatives being offered in the first place. The choices I see would
be somewhat like choosing between "good" and "better" security (or
"good" and "somewhat worse" performance, whichever way you prefer
looking at it). Not choosing between "mediocre", "good" and "great"
security, because if you do that then yes someone _will_ choose the
"mediocre security" option for one reason or another, and in that case
they are often better off _knowing_ that their data is unprotected
against a determined adversary. All options should be secure against
reasonably practical attacks for the foreseeable future; some options
could be offered which are _more_ secure, but come at a performance
penalty. We can never guard completely against large quantum leaps in
the relevant fields, but we can work within what we know about
currently and add a safety margin for the future which is reasonable
in both directions.
Perhaps now I'm showing off my ignorance in public, but I was under
the impression that choosing the curve is a bit like choosing the key
length, or perhaps even more apt choosing the value of _e_ in a RSA
implementation, in that it is a value that _only peripherally affects
the implementation itself_, by essentially being a constant somewhere
that must be chosen according to a set of criteria but which can be
replaced with another constant also chosen according to those criteria
without adversely affecting the _validity_ of the software. (The
security of the full cryptosystem as implemented being a different
matter, with some values offering higher levels of security than
others, but that happens everywhere in cryptography.)
Have I misunderstood on some fundamental level how this works?
And _implementors_ would of course always have the choice of only
implementing support for a single curve, assuming interoperability
across implementations of a standard is not a requirement. (If it is,
then obviously all curves mandated by the standard would need to be
supported in some fashion.)

@_date: 2014-08-12 08:50:48
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Dumb question -> 3AES? 
If that's your worry, then I'd say don't worry.
Copying Bruce Schneier [1]:
Even if the numbers he cites are off by something like _ten orders of
magnitude_, it doesn't matter. Brute force cracking of a 256-bit key
just isn't practical, and likely never will be practical, for reasons
of physics. The only way breaking a 256-bit key length cryptosystem
will ever be practical is if it provides _far_ less than par security
per bit of key material used, and the most obvious way to end up with
that is that it's storing something that allows deducing the key, or
the key is generated in a very un-random fashion (in which cases you'd
very likely be just as hosed using 3AES256).
Anyone targeting a cryptosystem that uses 256-bit keys will
_certainly_ choose some attack vector other than trying a brute force
search for the key. Hiring a few dozen thugs would be almost
infinitely cheaper, and vastly more likely to yield the desired
results (gaining access to the plaintext within a reasonable timeframe).
[1]

@_date: 2014-08-15 07:50:57
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Encryption opinion 
First off, kudos for recognizing your limitations, reaching out and
trying to do better. Lots of companies wouldn't.
Before you begin making changes, though, _please_ take some time to
_clearly define the threat model_ you are trying to protect against,
if you haven't already. Without a clearly defined threat model, it
becomes very difficult to tell whether the security provided is good
enough to meet the threat model. Also please be clear _to users_ about
the threat model you are attempting to protect your users against; I
would suggest publishing a set of documents outlining it (both in
layman's terms as well as a more in-depth technical discussion) on
your web site, someplace where it will be easy to find for those
Note that _academic_ 768-bit RSA factoring was reported in early 2010,
and that paper [1] suggests (section 3) that factoring a 1024-bit RSA
modulus _by academic effort_ is not likely possible before around 2015
and optimistically possible before around 2020. That would give a
1024-bit solution a few years of breathing room, at most, _against
academic efforts_.
That last part is important. We can most likely safely assume that
government agencies (not only in the USA) have access to significantly
more computing power, should they decide the communications is worth
the effort to decrypt, and it is also possible that they might have
access to methods of attack not known in the academic community. I
also believe it's safe to say that we know the NSA (and likely
others!) is vacuuming up everything they can get their hands on, and
that the NSA _specifically_ stores encrypted communications. That
means that any solution that may need to provide longer-term security
(as opposed to ephemeral security) must be able to withstand the
cryptanalysis and computing power of not only a large government
agency today, but in the case of high-value targets quite possibly
decades into the future. (If someone breaks into your house or mugs
you on the street and steals your cell phone, at least you'll know,
and can take steps to mitigate the damage. If someone sniffs your
encrypted traffic, stores and/or decrypts it, you won't necessarily
ever know, and hence can't take mitigating steps.)
Consider that NIST already in 2007 recommended [2] that a minimum of
80 bits of security shall be provided _until 2010_ for non-classified
data; between 2011 and 2030, 112 bits minimum security shall be
provided; and after 2030, 128 bits minimum. ([2], section 5.6.2.)
Looking at [2] table 2 (part of section 5.6.1), 80 bits of security is
provided by RSA-1024, 112 bits of security is provided by RSA-2048 and
128 bits of security is provided by RSA-3072. These numbers are not
necessarily current, but should be close enough. Obviously, we are
currently in the bracket where 2048-bit RSA would be recommended if
you are using RSA, providing 112 bits of security (brute force
symmetric key length equivalent).
[1] [2]

@_date: 2014-08-15 16:13:00
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
Does the A5 family of ciphers count as having been broken in living

@_date: 2014-08-16 11:38:49
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Cost of remembering a password 
Why should there have to be a marginal cost for remembering 'another'
password in the first place?
Off the top of my head, there are four or five passwords (which serve
as anything much more than signposts) that I actually _remember_. Two
are system login passwords where resorting to a password manager is
not practical; one is the passphrase to unlock the password manager's
It's nigh impossible to come up with passwords of high enough entropy
to withstand an offline attack, _especially_ within the limitations
sometimes enforced (maximum length, allowed character set, ...). And
that doesn't even touch on trying to remember any significant number
of such passwords. XKCD style (done right), Diceware and so on can
help generate quite secure passphrases, but you still have to remember
not just that one (or many) passphrase(s), but also _where to use each
Given how many people end up with passwords like "password1",
"12345678" and so on, I think it isn't as much _passwords_ that need
to be dealt with as password manager integration. All major web
browsers for example have the ability to locally store passwords used
(whether or not it's secure is a different matter and also depends a
lot on the user's chosen master password/passphrase), but what is
lacking is a _user friendly_, fully integrated, enabled by default
means to automatically generate and store secure passwords, and with
today's proliferation of different types of devices share passwords
between e.g. a desktop computer, a smartphone and a tablet.
Then, ideally, when I view a sign-up form that asks for a password,
the password field would have some sort of visible indication next to
it that allows me to automatically generate and store a secure
password for that particular web site. If there is no obvious
connection between the sign-up and login forms, in the login form I'd
be able to pick something along the lines of "for this site
(login.example.com), use the credentials previously used on
signup.example.com", perhaps offering by default the FQDNs for which
there are stored passwords which fall under the same second-level
domain (with handling of those top-level domains which use an
intermediate label to separate types of sites, as is the case for e.g.
.uk, .nz, and a handful of others).
A login form that currently looks like (Unicode and monospace font
needed for these mockups to look reasonable):
    Username: [my-username          ]
    Password: [???????????????      ]
              [ ? Sign in ]
could thus look a bit like the following, with the menu expanded:
    Username: [my-username          ]
    Password: [???????????????      ][?]
              [ ? Sign in ]            ?
 Use the password from signup.example.com |
 Use the password from     |
 Select among stored passwords...         |
 Generate a new password...               |
 Forget password on login.example.com...  |
                                       `------------------------------------------'
If that could be solved in a way that makes it easy to use even for
novices, and perhaps even allowing integration with an external
password manager for advanced users, I think we would already be _far_
along the way toward at least encouraging people to use _different_
passwords everywhere. And that, in itself, besides not requiring a
total revamp of lots of sites' authentication logic, would be a huge
win in practice.

@_date: 2014-08-16 17:59:13
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Cost of remembering a password 
Who said you'd be forced to use it at all? Password managers are great
for the vast majority of people for precisely the reason that they
allow the generation and use of long, high-entropy passwords by beings
who simply aren't wired to remember a large number of distinct, long,
high-entropy strings.
If you're worried about your password manager doing something it
shouldn't, then what's to say the browser can be trusted?

@_date: 2014-08-16 18:07:04
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Cost of remembering a password 
That, to me, sounds like a design matter, which would be easy to
design out of the question entirely if the desire to do so is there.
One could _for example_ require the master passphrase to be entered
before the passwords can be accessed or used other than on the sites
they are connected to, along with timing out sessions to the password
manager after a reasonable, perhaps configurable amount of time (with
the ability to time out the session immediately without affecting
anything else) forcing entry of the master passphrase before any
logins can be accessed.
Hardly rocket science. (And rocket science isn't that hard.)
The key would be to reduce what the user needs to remember to,
ideally, a single passphrase, rather than dozens of passwords or more;
and then integrate support for that everywhere (by allowing easy
access to it on password entry fields).

@_date: 2014-08-19 11:03:50
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] GPU Farm 
I'm not sure how good contemporary GPUs are at that sort of tasks, but
here's one: Factor RSA-309 or RSA-1024 (your pick).
It's quite a step up from RSA-768, but as pointed out by Tom Ritter in
successful factorization of (even a single) 1024-bit semiprime would
be a huge accomplishment that could potentially have far-reaching
implications, perhaps _particularly_ if done in the private sector.

@_date: 2014-12-18 22:09:25
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
I strongly doubt that would work at all. Anyone with an ounce of
knowledge of system engineering and development knows how complex
something like the equipment involved is. Numbers from Wikipedia and
just one example, but Linux 1.0.0 (March 1994) was 176k SLOC
(graspable). Linux 2.6.0 (December 2003) was 5.9M SLOC. Linux 3.10
(2013) is _15.8M SLOC_ total. _And that is just the kernel itself._
Once you throw in even what runs on my perimeter router (some version
of Linux, a web server, some custom front-end software for
configuration, IPv4 and IPv6 DHCP client and server, ...) the attack
surface in terms of lines of code in use balloons. And that's even
before considering more obscure threats like compromised CPU microcode
or the fact that Postscript is Turing complete.
And beside the possibility of a deliberately inserted backdoor, there
are likely to be genuine mistakes lurking inside _any_ large code
base. Many of those are not going to create exploitable bugs, but some
might. Some languages are more prone to this than others; for example,
you are more likely to inadvertantly write code that may suffer from
buffer overruns in C than in Java. Indexing errors are perhaps more
likely to occur in Java than in Ada. And so on. You only need to find
one such exploitable bug to get the crowbar in, but the vendor (and
certification institution) needs to find and fix _all_ of them. Static
analysis can help, but still has a ways to go.
Then there is the fact that if someone wants to hide a backdoor, it's
fairly easy to make it such that even looking for things out of the
ordinary in the equipment's behavior during operation won't reveal
much. Port knocking for instance is a technique which is fairly widely
used legitimately to expose services only to believed authorized
users, and with a long enough sequence, can be made almost arbitrarily
difficult to crack open. (It's not a perfect comparison, but a
knocking sequence verifying both source and target port numbers can
get a few hundred bits' worth of security with only a handful of
knocks each carrying 32 bits of entropy, and to anyone else's
equipment is likely to just look like the background noise of the
Internet these days so is unlikely to draw much attention. Throw in
special values in other rarely-used IP or TCP/UDP header fields and
you can get even more.) Without source code access finding something
like that would likely be near impossible to simply stumble upon if
the sequence is long enough and the code is deliberately written to
obfuscate that feature, and even with source code access, you'd still
need to know where among the millions of lines of code to look and be
able to recognize the code as being something out of the ordinary.
(What if the vendor sells built-in port knocking support as a
_feature_?) Also, particularly in C on known hardware, it's fairly
easy to do rather insidious stuff.

@_date: 2014-12-23 13:41:45
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
I believe that (multiple independent systems voting on what is to
become the final result) is done fairly regularly for anything
semi-critical in aviation, at least. There, however, it's more about
reliability in the face of random errors or malfunctions than against
a determined, intelligent attacker. And of course even that doesn't
protect from anywhere near the full spectre of possible malfunctions.

@_date: 2014-12-28 23:45:25
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] [cryptography] NSA Attacks on VPN, SSL, TLS, SSH, 
Some quick math says 188 ? 2^20 ~ 197 ? 10^6. Let's have a hash and I
think we can boil this one down to decimal versus binary prefixes.

@_date: 2014-07-19 21:11:26
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Steganography and bringing encryption to a piece 
There's another reason why that wouldn't really work. The database can
be local; storage these days is relatively cheap. Let's say we have
the ciphertext words HOT, RESEARCHER, PRETTY, TIME, SUSPICIOUS. Try
this; each sentence contains one of the words:
"I'd say that 55+ C is too hot for a disk. The best approach entirely
depends on the skills and equipment in possession of the researcher,
or whoever is configuring the emulator. It gives a pretty error
message and dies. Wow, that was a long time ago! We are suspicious of
people who stay up too late."
A message like that would stand out a little, wouldn't it? Yet, I just
copied five relatively innocous sentences from various discussion
forum posts, blog posts and emails I happened to have around the
computer, and fixed one obvious typo. It just doesn't look like
something a human would write, and I'm pretty sure even a poorly
trained, relatively naiive Bayesian filter would pick it out as
warranting a closer look.
Project Gutenberg has published (in April 2010) a DL DVD containing
almost 30,000 books in less than 8 GB, which should be plenty. Many
computers these days even have enough RAM that all of that could be
held in _cache_.
Making the resultant text _reliably_ look like something a human would
write is likely a much more difficult problem to solve.

@_date: 2014-05-30 21:53:13
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] How secure are hashed passwords? 
I'm _pretty_ certain that when I reset my password after this recent
breach, the form said the maximum allowed length was a measly 20
characters. But they won't even give me a simple password change form
without going through all the hoopla of a full "forgotten"-style
password-reset procedure, which I am not inclined to do. (Just give me
a form already where I can provide my current and a new password!)
Although I haven't confirmed this, according to one of the screenshots
on [1], Paypal has the same inane limitation of maximum 20 characters.
[1]

@_date: 2014-11-02 11:13:43
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
On 1 Nov 2014 15:40 -0700, from mitch at niftyegg.com (Tom Mitchell):
A recent study [1] did find that memory errors are much more common
than previously thought. PDF page 11, section 7 conclusion 1:
Also note that memory modules experiencing detected, uncorrectable
errors are normally replaced, which means that any given DIMM will
only experience a single uncorrectable error during its effective
lifetime, which would likely lower the number of uncorrectable errors
reported compared to what would have been seen had the module been
kept in use.
With RAM that does not employ error detection, there is no way to
detect any of those errors, meaning that the user would likely just
brush it off as a software bug causing a crash, perhaps reboot, and
keep working, possibly by the time the crashes are piling up simply
buy a new computer or, if they are particularly computer savvy,
reinstall the system from scratch (thus not solving the underlying
problem, of which they are likely to be unaware).
I personally find these figures, while not worthy of panicking, to be
significant enough to warrant attention for critical data, and have
gone with ECC RAM (which has yet to report any problems) myself.
The problem with software-level memory error detection is that even if
you are "lucky" in the sense that memory corruption, if it occurs,
hits precisely the data you are protecting (which is far from certain;
you are probably more likely to hit code or cache than a few thousand
bits of key material), if you have something like a stuck bit in RAM
you will be computing the error-detection data over already-incorrect
data. So while such schemes can detect errors that arise after the
data has been stored in RAM, they cannot detect errors that exist from
the beginning. As a somewhat extreme example, if I do something
trivial like memset(&ptr,0,1024) setting my 1024-byte block of memory
to all zeroes, but _a single bit is *stuck* on 0_, I can compute any
checksum over that block I want and it'll tell me all is well. If I
then fill that block by for example reading key data from disk, let's
just say that with 50% probability, I have a bad situation because RAM
now holds something other than what came from storage. _Reliably_
detecting the problem without hardware support is a non-trivial
problem; _at a minimum_, all memory-writing operations would need to
double-check the results in a way that is immune to caching.
[1]: Schroeder, Bianca; Pinheiro, Eduardo; Weber, Wolf-Dietrich
 (2009). "DRAM Errors in the Wild: A Large-Scale Field Study" (PDF).
 SIGMETRICS/Performance (ACM). ISBN 978-1-60558-511-6.

@_date: 2014-11-07 18:54:50
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
On 8 Nov 2014 02:27 +1300, from pgut001 at cs.auckland.ac.nz (Peter Gutmann):
I agree that if the attacker has enough access that they can flip bits
back and forth at will in step with code execution then you have a
much more serious problem. However, I think the above sequence
potentially misses one failure case, namely the stuck bits case which
I mentioned in my previous email in this thread.
Thankfully, stuck bits is fairly easy to protect against, since (and
especially with the above) you can likely assume non-intelligence. A
trivial way of checking this might be to on key storage normalize the
actual key data to some known format (this could be as simple as an
ASCII base-10 digits representation of the parameters in a fixed
order), compute a checksum over that data, and store that checksum
along with the stored (encrypted, marshalled, whatever strikes your
fancy) key material. As a final step before using the loaded key
material for crypto, normalize the key material that is about to be
used into the same format, compute the checksum over that, and compare
it to the stored checksum. In other words, true _end to end_
verification that the key material is what was originally generated or
at least stored. If they differ, then bail out with a prominent error

@_date: 2014-11-07 21:23:09
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
On 7 Nov 2014 16:01 -0500, from leichter at lrw.com (Jerry Leichter):
I can see three main possibilities here:
1. I misunderstood Peter's pseudocode and exactly what it does at each
step as well as all the steps when taken together. This seems the most
2. I failed to express myself properly.
3. You misunderstood what I meant with my comment.
Or possibly a combination of points 2 and 3 above.
What I meant was that if you load the key material into a memory
location which suffers from a stuck bit, then how does Peter's
proposed scheme detect that failure of RAM to hold what was intended
when each intermediate checksum gives a valid result (which basically
means that there are no bit _flips_ during operation)?
By adding an end-to-end checksum, with some defined format for what is
checksummed, all the way from storage to (in this case) the bignum
representation, we add the possibility of capturing that failure as

@_date: 2014-10-06 12:30:54
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] NSA versus DES etc.... 
On 3 Oct 2014 15:14 +1000, from dave at horsfall.org (Dave Horsfall):
While this has preciously little to do with cryptography, yes.
The Swedish Wikipedia article states that "between 1981 and 1994
approximately 4700 observations" of submarine-like objects were made,
presumably within Swedish territorial waters (I don't really feel like
digging out the Swedish government report cited as the source for
It's interesting to note that the lists differ; the Swedish-language
list gives Sep 15 2011 as the date for what appears to be the same
event that is listed by the English-language list as occuring on Sep
11 2011. It's possible that further scrutiny would uncover further
U-137/S-363 [1] is probably the most famous incident, but as can
trivially be seen, far from the only one.
 [1]:

@_date: 2014-09-13 11:44:13
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] keys, signatures, trust, identification, badges, 
In all honesty though, isn't that approximately the same thing as
trying to talk about the concept of a key belonging to a person, and
using the key fingerprint to validate that you have the right key?
In both instances we have a person, we have something _about_ that
person (a public key, a name, an email address, an employment
relationship, ...), and we want to validate that the two match by
using a third thing (a photo ID, a key fingerprint, a business card,
Not saying it's the same thing, but it appears to me to be _rather
closely related_ when you remove the layers of technology and just
look at the problem being solved in the abstract.
By the way, Swedish official ID numbers (similar to US SSNs, but not
quite) are ten digits long. Twelve if you use the full form, which
often is only used for storage (the first two or four digits is the
year of birth, with or without the century; until the early 00s, it
was common to specify only the last two digits of the year, but using
the full year has caught on in many uses since then). They started out
one digit shorter, and then a check digit was added once computerized
databases became more common.

@_date: 2015-04-11 11:52:50
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] upgrade mechanisms and policies 
On what basis however can we assume that a hypothetical future TLS 1.5
will be "better" (either in some objectively measurable sense, or in
every sense) than a likewise TLS 1.4?
The above is certainly a valid argument to consider, but it falls
apart pretty quickly if we don't at the very least define what
"better" actually _means_. Newer does not necessarily mean better,
especially in the security field, and in fact something that has stood
the test of time may actually be _better_ than something entirely
Even just because such a hypothetical TLS 1.5 would have a larger
number of algorithms to choose from than 1.4 (in the name of backward
compatibility) that does not necessarily make it better. (Anywhere
there is mutual automated negotiation and choosing between, for some
meaning of the terms, "better" and "worse" options, there exists the
possibility of downgrade attacks like the one we have just seen,
whether in the face of implementation bugs or where the negotiation
can be disrupted.)

@_date: 2015-04-17 09:39:04
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] upgrade mechanisms and policies 
Confidentiality is meaningless if you don't know that you are
communicating with the entity that you believe you are communicating
with, and not someone passing traffic along.
Suppose Alice and Bob want to communicate in such a way that Eve and
Mallory cannot know _what_ is being communicated. (For simplicity's
sake, let's say that Alice and Bob are fine with Eve and Mallory
knowing _that_ they are communicating with each other; they want
message confidentiality, not communications secrecy.) By having an
authenticated, encrypted channel to transport the data, this is easy,
but Alice and Bob somehow need to authenticate each other initially.
If this authentication is persistent at the endpoints and is tied to
something that only each of Alice and Bob knows (such as their
respective private keys), then they can be confident that after they
have verified that the other endpoint is the intended one, as long as
that value (say, a key fingerprint) remains the same, everything is
very likely fine; Eve the passive attacker can see that they are
communicating (which was okay in their threat model), and Mallory the
active attacker could in theory insert himself in the middle but that
would invalidate the previous endpoint authentication between Alice
and Bob, alerting both.
If Mallory can insert himself in the middle, to Alice _appearing_ as
Bob and to Bob _appearing_ as Alice, then you have no real
confidentiality, even if the link is encrypted. That's the situation
you get with encryption without authentication. Incidentally, it's
also what you have with e-mail opportunistic transport-level
encryption without certificate validation; it protects against passive
eavesdropping, which is a step up from everything being in plain text,
but it does not offer protection against active attackers.

@_date: 2015-04-18 20:53:20
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] upgrade mechanisms and policies 
While I do believe the points you're making are valid to some degree,
they are not what I see when I talk to non-technical people. Many
don't even care about blanket mass nation-state surveillance. More
than one person I have tried to introduce to the concept has
_literally_ responded with something very close to "if they think I am
that interesting, then for all I care they can listen in". This even
when suggesting things that can be done with next to _zero_ negative
impact for the individual, such as using OTR on top of already
established IM communications, or installing a browser plugin like
HTTPS Everywhere. These are things that take minutes or less to set
up, are basically maintenance-free, yet still help improve (and
certainly don't detract from) the security of their communications. We
aren't even talking about things like Certificate Patrol, which _can_
cause user inconvenience in some legitimate cases.
That's where the threat acceptance I suggested comes from. My
experience is simply that _people don't care if the fact that Alice
and Bob are communicating is known._ They might, however, care about
the content of those communications being known; what has been
jokingly referred to as the NSA "DICKPICS" program.
This is also most likely a major reason why we don't have a huge
uproar in Sweden _right now_ because, in spite of the ECJ decision _a
year ago_ that the EU Data Retention directive violates _basic human
rights_ by virtue of enacting blanket mass nation-state surveillance
(of communications metadata), the Swedish law (which the Swedish
government claimed was only enacted because we were _forced_ to do so
by the EU, never mind that Sweden was one of the driving countries
behind the directive) that requires blanket mass surveillance and
storage of communications metadata _is still in place and in effect_,
and the government claiming that this is not a problem.

@_date: 2015-08-28 18:38:02
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] A thought about backdoors and quantuum-resistant 
I don't know, and I don't know enough to even know where to begin
Why does that follow? It is my understanding that based on current
knowledge, quantum computing, when applied to symmetric cryptography,
causes the security level to drop to the square root of what it used
to be. So a cipher offering a 128-bit security level now offers a
64-bit security level (because sqrt(2^128) = 2^64) against an
adversary that has a sufficiently powerful quantum computer that they
are willing to throw at the problem. Which is a Bad Thing (tm).
However, a today 256-bit security level cipher in this hypothetical
quantum computing world "only" offers the equivalent of 128-bit
security, which is Not Great (tm) but certainly not Terrible (r).
So _symmetric_ cryptography is the easy part to solve: we just need to
double the key lengths, and figure out what that means in practice. In
situations where in a no-quantum-computing world we might have used
AES-256 or AES-128, we might use XES-512 and XES-256 [1] respectively
for a similar effective security level. In this hypothetical future
world, symmetric keys remain short enough that key management is not
significantly complicated compared to what it is like today.
[1] XES is obviously the next great thing after sliced bread:
eXtensible Encryption Standard. Because anything extensible is by
definition great. I mean, just look at how easy XML is!

@_date: 2015-12-04 11:53:33
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] TrueCrypt issues 
On 3 Dec 2015 19:51 +0000, from sacanoid at gmail.com (SacanoID):
You reformat and restore your most recent backup. You _do_ have
backups, right?
I don't use TrueCrypt (never have), but it sounds like the container
has somehow become corrupted (which might possibly related to that
unplugged cable you found). Once a FDE container becomes corrupted,
it's often down to sheer luck whether you are able or not to recover
the data. Simply enough, if the corruption hits the "right" spots, and
assuming that the cryptography is proper and implemented correctly,
even what would otherwise be trivial corruption affecting just a
single file or folder (and often overcome fairly easily using common
data recovery software) can easily lead to total data loss.
Given the symptoms you describe, it appears that you have found
yourself on the wrong side of this luck.
I won't say that recovery is _impossible_ at this point; I don't know
enough to make such a statement. But at this point, it certainly
sounds like restoring from backup is the quickest way to get your data
This type of failure is a reason why, when using full-disk encryption,
solid backups is even _more important_ than otherwise (and they are
already important in every case, because in the case of Our Data vs
Universe, Universe has yet to lose).
What you _should_ have done, before you started trying to fix this,
was to make a low-level copy of your TrueCrypt container, and kept one
of the copies pristine while working with the other. That would have
allowed you to roll back any changes made and get back to the original
broken state ("fixing" any additional corruption introduced by your
attempts at recovery). If you want to keep troubleshooting, you
_absolutely_ should do that now, before taking any further steps.

@_date: 2015-12-04 12:00:19
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] JSC notifies on introduction of National 
On 3 Dec 2015 03:44 +0000, from cryptography at dukhovni.org (Viktor Dukhovni):
Can anyone please explain what the exception to the rule is supposed
to mean?
Does it mean that if the computer running the software doing the
encryption is physically located in Kazakhstan, then the "national
security certificate" is not or does not need to be used? If so, then
even if we were to take the "this is for your own protection" argument
at face value, how can this possibly help?
If it means that only systems outside of Kazakhstan need to use the
certificate, then how can that be mandated, given that it would be out
of the Kazakhstan government's jurisdiction?
No matter how I read that exception, I can't figure out what it's
supposed to mean.

@_date: 2015-12-05 16:25:46
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] TrueCrypt issues 
On 5 Dec 2015 16:01 +0000, from sacanoid at gmail.com (SacanoID):
Sorry to be blunt, but frankly, why should I (or we, on this list)
care more about your data than you apparently do?
_Having a solid backup regimen_ is the _first_ step in ensuring that
one is able to recover from storage mishaps. And storage mishaps
happen. (Now, if you'd _had_ backups and had the unfortunate situation
of the backup media going bad as you were attempting to restore the
backup, it would perhaps have been a different matter. But this list
_still_ probably wouldn't be the best place in which to seek help.)
Yes, the NTFS MFT is corrupted; your tools are telling you that much.
The TrueCrypt container may very well be corrupted beyond the header
that you restored. To what extent either is corrupted is impossible to
tell based on the information you have provided, but the sector number
and the MFT offset indicated would imply a storage device of at least
3*10^17 bytes (about 314,000 TB), which is clearly implausible.
I don't think you need a cryptography expert; rather, you need a data
recovery specialist, or possibly someone who knows the ins and outs of
TrueCrypt as a product and its on-disk format. Or, if that is out of
your budget, at least data recovery software. Now that you are able to
open the TrueCrypt container, the fact that the data is encrypted
on-disk should have limited bearing on any further problems, _if_
(which is a very big if) there is no further data corruption.

@_date: 2015-12-20 22:21:05
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Questions about crypto that lay people want to 
Additionally, your bank has very different requirements for secure
identification of the person performing a transaction than those
involved, for example, in voting. This comes down not really to
confidentiality, as much as anonymity and auditability.
Your bank wants to make sure that you and only you (and those you
authorize, but that's in some ways the same thing and in other ways
completely unrelated) are able to take action with regards to your
bank account. An easy way to do that is to have some secure method of
In this case, HTTPS is nothing more than the transport, just like
signed forms can be sent back and forth in the mail (hopefully within
A basic set of requirements for the voting process in most democratic
systems is that we can, at a minimum, ensure that:
1. a single person can cast no more than one vote in a given election,
2. that only people authorized to vote in the election are able to
   vote or otherwise influence the outcome,
3. that the final results are auditable,
4. that anyone can review the process,
5. _and_ at the same time that it is not possible to tell how (e.g.,
   for which candidate) any one specific voter _actually voted_
   (unless they disclose it themselves, but even then, there should be
   no way for a third party to _verify_ the claim)
I think most people can agree that these are all goals worth striving
for in a pick-your-government kind of voting process, regardless of
how it is implemented.
This set of requirements is fairly trivial to meet when using a
physical process like voting in person at a voting station of some
kind, but is decidedly non-trivial in an electronic process,
particularly one performed remotely such as from one's home over the
For a start, when voting remotely, no matter how secure the transport
(postal mail or HTTPS over TCP/IP alike), you can give no _guarantees_
that the person voting is alone (or at the very least offered a
realistic choice of being alone) when they commit to their choice. Not
being able to guarantee confidentiality in one's choice when voting,
even if all else is _perfect_, means that several of the listed
requirements are violated. Specifically in the list above, it casts
serious doubt on  and  and outright fails Of course in an ideal world, we would also like to be able to verify
that _our particular vote_ was included in the final results, without
violating any of the other requirements listed above.

@_date: 2015-02-02 15:52:51
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Wrong uses of filesystem encryption 
So, if your threat model includes that, then don't use fully automated
encryption solutions.
The key to every security decision is to first decide what you are
trying to protect against, such that proper protective measures can be
taken. If in your threat model this scenario is likely, you have to
accept some loss of convenience if you want to protect against it.
It's either that, or enjoy the convenience while accepting the risk.
Encryption does you no good if the adversary has the ciphertext _and_
the key, or has the ciphertext and is able to recover the key,
especially if the algorithm is known or can be reverse engineered. Cue
movie DVD CSS DRM for just one example.
However, it does a lot of good against an adversary that _doesn't_
have access to both of those. Examples of which are:
* Properly done crypto, which actually uses a decent key not kept in
  plaintext together with the ciphertext
* Offline (or otherwise separate) storage
The first will cover the case where you are actually using the data.
The second will cover cases like backups.

@_date: 2015-02-16 19:51:04
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] phishing attack again - $300m in losses? 
I keep wondering about this; people repeatedly fall for exactly the
type of trap you are describing (as well as some others, obviously,
but let's deal with one problem at a time), yet I don't see even any
real _attempts_ at mitigation. Is there any MUA out there that, say,
categorizes sender email addresses? I'm thinking something like "red =
beware, new/unknown/suspicious address", "yellow = an address you have
had limited dealings with in the past" and "green (or whatever
standard color) = an address with which you have corresponded several
times in the recent past". Or some sort of symbols to aid those with
color-vision disabilities. That sounds like it could greatly reduce,
albeit admittedly far from eliminate, the attack vector you are
describing. If it can be combined with DKIM, SPF and perhaps even
cryptographic identities like matching up against PGP public keys, all
the better.
Something like that probably wouldn't do much to protect against
directed attacks, but it would probably do a _lot_ to reduce the
problem of random phishing going on. And even that would seem to me to
be a big win.

@_date: 2015-02-22 22:00:32
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] trojans in the firmware 
Besides what others have already said, it allows you (as in the device
manufacturer or firmware vendor) to efficiently implement the "SECURE
ERASE" command. Just generate a new key and overwrite the old one with
the new one. Voila, the data has now gone to Where Documents Sometimes
Go. No need to overwrite every sector (has the added benefit of saving
time, particularly in the case of HDDs, and write cycles, particularly
in the case of SSDs) and it ensures that _all_ data remnants are
rendered unusable. Even for example cells that are no longer able to
take and hold a new state become unusable for recovery attempts,
because all you're getting is the ciphertext where the key has been
discarded. If the crypto is any good, that means you get nothing
useful at all.
While I personally wouldn't rely exclusively on a self-encrypting
storage device for security, it _does_ add one more layer of defense
in depth. And for certain use cases, just _might_ be good enough all
on its own.
The bad part is that for all that advertising, it could just be AES in
ECB mode with the same key for every block. Obviously, details matter,
but 99.9% of even the people who are looking for something like that
see "256-bit AES" and think "great, unbreakable!".

@_date: 2015-01-02 16:42:25
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] on brute forcing 3DES to attack SIMs 
On 1 Jan 2015 23:21 +0000, from iang at iang.org (ianG):
Or a bit of math. I'll admit, the "." tripped me up as well, so I ran
the numbers. All of the below is to within experimental error.
If we read "245.760 Mcrypt/sec" as 245M760 crypt/sec, then we have
done in three days:
245M/sec ? 3d ? 86400 sec/d ~ 63.5e6 M = 63.5e12 = 6.35e13 encryptions
If instead we read it as 245760M crypt/sec, then we have done:
245G/sec ? 3d ? 86400 sec/d ~ 63.5e9 M = 63.5e15 = 6.35e16 encryptions
We know that:
2^55 ~ 3.6e16
2^56 ~ 7.2e16
And of course:
6.35e16 ~ [3.6e16 .. 7.2e16]
We do know that back in 1998, 56-bit DES was brute forced in less than
three days using custom hardware costing $250k (that's the EFF DES
Cracker). Working backwards, Deep Crack tried some 1.8e11 keys per
second (or about 9.63e7 keys per second per cracker chip, roundable to
1e8 keys per second per chip).
Which of course means that brute-forcing single-DES in three days
today (by trying 2.45e9 keys/second), on almost any class of hardware,
is certainly not a major achievement; only about one order of
magnitude faster after 15 years of technology advancement.
Hence, nothing to see here.

@_date: 2015-07-08 13:42:16
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Anti-clipper team re-assembles 
On 8 Jul 2015 02:09 +0000, from rsalz at akamai.com (Salz, Rich):
Or stated otherwise, if the crypto itself (either the algorithm or the
implementation) is _broken by design_, then it doesn't matter how
great a password you are using: you can _never_ get good security with
such a system, because the weakest link becomes uncontrollable.
In which case, if anything, we are making higher entropy passwords
basically pointless, at least against certain classes of attackers;
right along with introducing vulnerabilities that are potentially
exploitable by _other_ adversaries as well. The math doesn't care
whether it's Government A or Criminal Organization B or School Kid C
or Intended Recipient D trying to gain access.
**I agree that real-world password entropy in many cases is at best
dismal. But that's no excuse for making other (unrelated) things
_worse_.** If anything, it should be an argument for making the most
that we reasonably can out of what little entropy we do have.
Additionally, password entropy is, to a large degree, controllable by
the end user; algorithm design and sometimes even selection is not. I
can _choose_ to use a long, high-entropy passphrase kept only in my
head and accept the disadvantages that comes with that, knowing that
the disadvantages to any attacker trying to gain access to my data
through at least some means are even greater. I can use a long-running
PBKDF to stretch the available entropy further. With backdoored
crypto, I can't say any of that, because at least certain attackers
won't need to even touch my passphrase.
If there's one thing I think history has taught us that is relevant
here, then it is that weaknesses designed to allow only a certain
class of attackers do, sooner or later, allow the same attack for
other classes of attackers as well. No need to invoke Kerckhoffs,
even; Moore and Murphy are more familiar to most people, and serve to
illustrate the point well enough in many cases. Or go ahead and watch
the original _War Games_ movie, or consider Logjam.

@_date: 2015-07-12 15:16:53
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Ad hoc "exceptional access" discussion at 
There's always the possibility of just asking said non-tekkies:
- If the government can't keep their secrets safe (even ignoring
various insider attacks like Manning or Snowden, let alone that which
happens at the hands of disgruntled law enforcement officers or
curious medical practitioners; see e.g. the recent US _Office of
Personnel Management_ breach, or the illicit telephone wiretapping
mess in Greece a few years ago which AFAIK hasn't ever been attributed
to anyone),
- If the companies that make software designed to allow spying on
people can't maintain security (see e.g. the recent _Hacking Team_
- If large multinational corporations can't maintain security (see
e.g. the recent _Sony_ episode),
- _Then why_ should we trust any of those to, in addition to their own
secrets, keep _our_ secrets safe? Why should _I_ trust them to keep
_my_ secrets safe?
I obviously might not be able to do _better_ (and frankly, am unlikely
to be able to do significantly better) than any of the above, but at
least I'm not creating an _additional_ extreme-value-target treasure
trove which I then fail to adequately protect.
I sometimes compare data encryption to locking your house. (Most
people accept that, even though they aren't doing anything illicit in
their homes, they don't want strangers rummaging through their
belongings.) In that comparison, "exceptional access" would be a sort
of global master key that allows trivial unlocking of _any_ locked
door, in such a way that does not trigger any alarm system or anything
else similar that the home owner might have installed.

@_date: 2015-06-01 11:09:38
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] open questions in secure protocol design? 
FUD. For all my personal preference for non-Microsoft OSes, there are
two facts at hand in this case:
1. Windows XP was the first version of Windows that introduced product
activation. and . Prior
versions could be installed freely without involving Microsoft at all;
Windows XP and later requires Microsoft involvement to install and
maintain a fully functional operating system, to varying degrees
depending on the specific version and edition of Windows.
2. Current versions of Windows are available in both retail box sets,
OEM bundling, and for licensing and downloading over the Internet. See
e.g. . MSDN
and TechNet subscriptions require renewal to remain current, but as
far as I know that is not any major departure from how it has been for
a long time. While MSDN and TechNet software these days is offered
through web site downloads rather than being shipped on physical
media, that does not change the licensing situation (and just like you
could keep or copy the physical media, you can download software from
MSDN/TechNet while your subscription is current and keep it
afterwards; technical, not necessarily legal, "can").

@_date: 2015-06-13 22:21:19
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] let's do something intelligent about md5sum! 
For the people not in the know, this is often termed "technical debt".
This is something that the people writing the code to move away from
simply using crypt() for password storage encryption on *nix
(specifically as far as I am aware Linux) had to contend with. It was
easier there, though, in part because the encrypted password field was
not a fixed-length field the way a 128-bit MD5 hash is fixed-length.
The solution they came up with was fairly elegant, particularly given
the constraints (pure text file, likely not wanting to alter the data
meta-format itself by introducing a new field, ...): _prepend an
identifier to the value, which is invalid in the old scheme, stating
which algorithm was used to generate it._
crypt() already had 12 bits of salt, expressed as two base64 bytes,
prepended to the encrypted value (itself expressed as base64), so this
was easy to do by using a combination that could never be valid Base64
(the format became `$` followed by a numeral followed by `$` followed
by the encrypted or hashed value as further defined by the specific
scheme; if the first character of the stored value is not `$` then you
know it's old-style crypt()). These days we have a handful of
algorithms, each with its own identifier; tools that validate
passwords need to be aware of the old schemes; but tools that _set_
passwords only need to know about any _one_ of these algorithms
(ideally, the most secure one, for some value of secure). Thus, over
time, as people change their passwords, the data gets migrated
_transparently_ to, hopefully, stronger algorithms. It also allows for
the possibility of administrators being able to disable the old
schemes once those are no longer needed.
If we are going to break things anyway, a similar approach could, in
principle, be used for hash values. Reserve, say, two bytes prepended
to the hash (65,536 options should be enough for anybody, right?)
which specifies the specific hash in use.
The only major downside is that you need some sort of coordination in
assigning the identifiers, and in the case of storing hashes in a
database or similar that the maximum length of the field is unknowable
ahead of time. (The maximum output length given a _particular set_ of
algorithms can certainly be knowable, however, and moving from a
128-bit output hash length to a 256-bit output hash length is already
a schema breaking change with fixed-length storage fields, so no major
change there.) An additional downside is that software that needs to
verify the hashes need to support several different algorithms, but
with a reasonably sized set, that is managable. (Think something like
perhaps MD5 and SHA1 for backwards compatibility, and SHA256, SHA512,
BLAKE2 and SHA3 moving forward. While not everyone would get their pet
hash algorithm, it seems like a reasonable starting set _could be
agreed on_.)
With something like this, software can also reasonably intelligently
handle the case where the indicated hash algorithm or scheme is not

@_date: 2015-06-19 15:14:15
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] password fatigue;  was: Lastpass 
If we are dreaming up new (exterior) hardware designs anyway, then why
not make it such that it has a USB port (allowing any old USB cable to
connect it to a computer), presents itself to the host as a HID device
acting as a keyboard (much like a Yubikey does) and has a physically
triggered action to send data selected in its software to the host?
You'd have to solve synchronization in a secure manner, but that's a
problem with all of these types of devices. Bonus points if it allows
the connection of an external keyboard to it; try hooking up a Yubikey
to this device and using that for a part of your master password.
I'm thinking a simple password manager UI where you indicate in the
software to for example "send password", then press this physical
button on the device and the software transmits the password for the
current account _as if you had typed it on a keyboard_.
Throw in keyboard layout selection for the output and it seems like
you have something that could be made reasonably small (even with a
physical keyboard on the device, certainly small enough to fit in a
reasonably-sized pocket) _and_ can reasonably be known to not leak
data (because you can trivially have it input a password into a text
editor, for example, and verify that it does what it should; also, it
requires both being plugged into a computer _and_ physical action on
part of the owner to legitimately transmit anything).
Any takers?

@_date: 2015-06-21 20:57:56
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] password fatigue;  was: Lastpass 
Far from all systems have Bluetooth hardware. Particularly, it is
uncommon on desktop systems. While laptops are common in many
settings, pure desktop systems still serve a large, legitimate niche
even in new installations.
The idea with using wired USB was to force the user to make an
informed decision to have the device interact with _the intended
system_, and not some other one that merely happened to be in radio
receiver range at the time.

@_date: 2015-03-05 12:06:06
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] FREAK attack 
On 5 Mar 2015 08:23 +1100, from dave at horsfall.org (Dave Horsfall):
Um -- no. An easy way to MITM someone is to set up an open access
wireless network with a tempting name (this gets demonstrated every
now and then in practice, sometimes even in environments where people
_should_ be security-conscious), and on it, intercept all DNS traffic
and modify it appropriately (or reply with your own canned responses
in a mixed authoritative/caching/modifying DNS setup). This works
especially well if you are just looking to MITM "anyone", as opposed
to "this individual in particular" which may take some more effort.
In such a scenario, the MITM'er still don't have access to the plain
text of encrypted communications, but _can_ intercept the vast
majority of outgoing connections (anything that relies on DNS and,
possibly, DNS not secured using DNSSEC). That'd normally, given
certificate pinning and similar technologies (which means you can't
easily MITM the TLS cryptography itself), give you a dump of the
encrypted transport stream, which supposedly is difficult to decrypt.
But given that you can _also_ intercept the SSL/TLS _handshake_ and
modify it to trick the parties into using easily breakable crypto, you
_significantly_ lower the bar for a successful breach of
Corporate SSL MITM'ing is different among other reasons because in
such an environment you control at least one of the endpoints -- the
client -- in addition to the middle point (the one doing the MITM),
allowing establishing trust between the client and the MITM.

@_date: 2015-03-22 13:08:18
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Kali Linux security is a joke! 
While I don't know about Kali Linux specifically, there are two things
that we do know:
1. Kali Linux is based on Debian.
2. Current Debian's "Release" file, which together with its GPG
signature forms the heart of Apt security, provides MD5, SHA1 _and_
SHA256 hashes of each listed file, plus the size of the file.
See for example ftp://ftp.debian.org/debian/dists/wheezy/Release and
search for "MD5:", "SHA1:" and "SHA256:" (the colons are important for
meaningful hits).
Finding a collision against MD5 might well be feasible if you are in a
position of controlling both files (collision attack rather than
preimage attack). Finding a valid collision against _all of_ MD5, SHA1
and SHA256 _simultaneously_ would be a lot more difficult; it would
be, at the very least, as difficult as finding a collision against the
strongest of the algorithms involved. At that point, it seems more
reasonable to just intercept the request for the Release and
Release.gpg files, and attack the single hash provided by the detached
GPG signature.
Also the fact that multiple hashes are already provided indicates that
the infrastructure is in place to support additional hashes; if a
crippling vulnerability in, or even the possibility of a practical
attack on, any of the algorithms involved were to be found, at the
very least it should be practical to add another hash algorithm and
choose to either maintain or retire the broken algorithm.
At least Debian provides the relevant GPG public keys in the installed
system, which limits our trust bootstrapping problem to the initial
download. If you have a trusted Debian system already installed, you
can verify the authenticity of the Debian downloads without relying on
anything not secured by the package manager, as described for example
at . The
ISOs are distributed along with MD5, SHA1, SHA256 and SHA512 hashes,
each of which signed by the distribution role key. Hence basically if
you have a trusted version of an OpenPGP implementation, and can gain
access to trusted role key fingerprints (perhaps from someone else who
already has Debian installed), you can meaningfully establish a
reasonable degree of trust in the installation images.
Which is probably about as close as is practical for most people to
come to a "trusted operating system installation".

@_date: 2015-03-23 17:49:46
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Kali Linux security is a joke! 
It looks to me like that blog post is basically about the CA problem,
and the fact that in the absence of trust in the full set of CAs TLS
protects mainly against passive attacks and a small set of active
But in all fairness: exactly what of that is solved by going with HTTP
rather than HTTPS?
In HTTP, all traffic is in the clear, available for anyone to copy or
modify anywhere along the route. You are relying on everyone being
honest and passing along the traffic unmodified.
In HTTPS, a _passive_ attacker will only see the ciphertext of web
pages. They will see, through DNS and SNI, which hosts you are
visiting, and in the case of reasonably small, publicly available
hosts might be able to determine which pages you're looking at. But in
HTTP, they were _certainly_ able to do that, right along with
responding with a HTTP response of their own, and in the case of HTTP,
there was no way for the end user to tell that it has happened at all.
An _active_ attacker capable of injecting DNS responses, tricking the
client or server into downgrading the connection security, or
something similar, can obviously use that ability to take over a HTTPS
session just as well as they could take over a HTTP session. But
suddenly there exists the possibility to introduce another obstacle:
certificate pinning (client- or server-side) or validation against
what is seen by others for the same host. Combine a plugin like HTTPS
Everywhere (with or without the SSL Observatory feature turned on)
with one like Certificate Patrol on the client side, and configure
HSTS on the server side if applicable. Suddenly, without making large
changes on either side, _passive attacks became vastly more
difficult_. By forcing the attacker to become _active_ you increase
the risk and/or cost to the attacker at little inconvenience to the
user. Specifically, it significantly increases the risk that the
attack will be noticed, or increases the cost in terms of resources
for gaining access to the content of the communication.
Now, if an attacker has reason and ability to target you specifically,
they still might. But all of a sudden they are now risking exposure,
and they have to expend (for example computational) resources which,
after all, are limited.
Is that good or bad for security?

@_date: 2015-03-31 20:11:47
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Cipher death notes (was: Re: Fwd: OPENSSL FREAK) 
Maybe I'm missing the utter simplicity here, but there's something I
don't quite understand. Hopefully you can enlighten me in the matter.
Suppose that the feature you describe was widely implemented. Suppose
also that the relevant primitive gets broken in a meaningful manner,
_but_ that the ability to break said primitive has not yet spread to
the masses. Maybe the break was limited to a fairly specific case, but
still bad enough to warrant sunsetting the primitive in question on a
rapid schedule (perhaps something like for example that we find an
efficient way to factor one out of every ten thousand RSA moduli, for
some given value of efficient, but there is no good test for such
moduli). Maybe the break requires a large amount of computational
resources, making it out of the reach of all but the very most
resourceful adversaries. Maybe it requires a combination of multiple
such factors before the break becomes feasible. Or whatever.
The end result of the above is the same: we have reason to believe
that YFTLA (Your Favorite Three-Letter Agency, for some given values
of "Three" and "Letter") may have the capability to break said
cryptographic primitive at least in select cases, but it remains well
out of the reach of the general public, let alone any one individual.
What would entice said TLA to publish the break, rather than
(attempting to) keep the ability to themselves and maybe a few of
their closest allies? What would entice them to break the specific
"negative cert authority" key (which gains them nothing except the
ability to force the primitive in question out of use, depriving them
of their ability to break the security of the system), rather than,
say in the case of public key cryptography or cryptographic hashes,
the root cert of Comodo or Verisign (which gains them potentially
_very_ significant leverage) or the PGP key of a high-value target?
Your proposal is certainly an interesting idea for academic exercises,
where the whole idea would presumably be to publish and get wide
dissemination of the result, but how does it help with adversaries who
keep the capability to themselves and/or secret/classified for the
purposes of making good use of the break?

@_date: 2015-05-03 14:35:05
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] "Trust in digital certificate ecosystem eroding" 
On 2 May 2015 10:00 +0100, from hyc at symas.com (Howard Chu):
The problem with this is (and many other approaches that burden end
users with security-critical decisions) that the vast majority of
users simply want to proceed to  or
whatever other site they were trying to get to. So they will click
"proceed" or whatever the button is labelled, all the while thinking
"why are you bugging me? just get out of my way, computer!".
And how likely is the average user to make a correct judgment if, say,
the CA for the web site for their bank (the certificate for which they
accepted God knows when way back) changes from, say, "VeriSign, Inc."
to "ValiCert, Inc.", or even "VISA"? Just look at how many end-users
fall for even the worst examples of impersonating various banks and
large companies in spam email.
It would take _considerable_ (re-)training of users to actually take
security warnings seriously, and to reduce the number of false
warnings. I run the Certificate Patrol add-on for Firefox myself, to
catch instances of sites changing certificates unexpectedly, and I
probably get literally _several popups a day_ saying that a site has
changed its certificate unexpectedly. Often even not about the site I
am actively visiting, but some third party site. (And I don't lurk in
many of the dark back alleys of the Internet.) I consider myself
reasonably technically adept, both as far as computers _and_
cryptography goes, and I sometimes have difficulty judging how to
respond to those popups. How would a regular user react, and how would
they come to a decision about whether to accept or reject the new

@_date: 2015-05-03 17:22:21
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] "Trust in digital certificate ecosystem eroding" 
On 3 May 2015 15:45 -0000, from johnl at iecc.com (John Levine):
That's basically my point, and to my mind it completely invalidates
Howard Chu's suggestion that "Every time you hit a new web site,
prompt for whether to trust it's chain or not". The user simply isn't
in a position to make such a determination, and much less so when the
certificate chain _changes_, perhaps in the middle of a session. And
that's assuming that the chain actually changes _and_ that the change
of the certificate chain actually means something important. Look at
how several CAs are currently changing from a non-SHA256 to a SHA256
cert, labelled as such in the certificate CN. That's a change of the
certificate chain, but to something like five or six nines of the
users, it's a complete non-issue.
As for  specifically, feel free to
substitute anything. The user wants to get the job done (whatever "the
job" might happen to be). So now "ValiCert, Inc." (just to take one
example out of my browser's trust store) is vouching for their bank's
web site rather than "VeriSign, Inc."; the vast majority of users are
unlikely to even recognize what a change of CA means, much less be
able to make an informed decision about what to do, and as you point
out the user likely just wants to get the job done. So we _cannot_ put
the burden on the user; it has to be placed somewhere else.
As for what that "somewhere else" might actually be: Widely deployed
HSTS and DNS-based certificate pinning (with a service identifier of
some kind, perhaps per-port) in combination with DNSSEC for reasonably
high-value sites seems to me to at least have a chance of working,
because at least it puts the authoritative information where it
belongs: with the owner of the site. HTTPS is already per-hostname
(with allowances for multiple host names, but not for sub-hostname
subdividing of certificates) so DNS seems a quite acceptable metadata
delivery mechanism, and DNSSEC (while imperfect) can help against
spoofing of DNS responses and is something that we actually _have_
albeit not widely deployed at present. I don't know what the current
status on DNS-based certificate pinning is, but this seems like
something that, _if the desire to do so exists_, we could deploy
widely (both in client software and on servers) within a matter of a
few weeks to a few months. Aside from DNSSEC, it's a fairly minimal
change on the server side, and some client code to do an additional
DNS request and do something intelligent with the response. (It would
also be a non-breaking change; all present clients would keep working
the same way they do currently, and new clients could take advantage
of the site owner's statement of which certificate(s) to trust.) It
also seems to allow for intelligent handling for non-interactive
services, including SMTP STARTTLS.
It almost certainly wouldn't be perfect, but it seems to me to be
something that we could actually do with what we currently have to
improve the ecosystem situation a great deal.

@_date: 2015-05-26 09:53:20
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] open questions in secure protocol design? 
In all fairness, had it started out as an Algorithm Agility approach
but (still) with only a single cipher specified to begin with, given
that home networking equipment rarely gets firmware updates and is
often more or less abandoned by the vendor relatively soon after it
makes it onto the market, it is quite possible that the situation
would still have been the same: lots of devices capable of using only
a single, broken means of ensuring transmission confidentiality.
Algorithm agility doesn't really help much if you don't have a plan
for effectively implementing it in practice. Which pretty much means
you need to have a plan to both support the devices involved as well
as for how to push upgrades to users. Plus again the problem of ending
support for cipher suites that turn out to be less secure than
Having multiple cipher suites, on the other hand, just means an
attacker has to find a way to trick the device into using one of the
less secure cipher suites. Cue FREAK, POODLE, Logjam and friends.

@_date: 2015-11-16 17:49:07
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] ratcheting DH strengths over time 
That would seem to depend on where the perceived weakest link is. Take
keylength.com's numbers, for example: if I plug in a 2048 bit
factoring modulus size, I get back various figures that point toward a
100-128 bit work factor. In other words, for symmetric crypto, a
fairly good match for AES-128: neither is obviously weaker than the
However, both 192-bit and 256-bit AES are readily available,
standardized and well understood. Again taking keylength.com's numbers
at face value, to match a 256-bit AES encryption we'd need to use RSA
with moduli anywhere between 15,000 and 50,000 bits. This is _clearly_
_far beyond_ what counts as practical in anything but extremely
specialized workloads.
It would be trivial to set a date, or threshold, by which conformant
implementations should switch from, say, 128-bit AES to 192-bit AES.
This can be done _well_ in advance, long before any practical attack
on 128-bit AES is known, simply such that the symmetric cipher does
not become the weakest link. This seems reasonable to me because the
performance penalty of going with longer symmetric keys, for example
in the case of AES, is much smaller than the performance penalty of
going with longer asymmetric keys.
(Or if we're worried about quantum computers, we could just go to
256-bit AES at once for a 2^128 WF for a quantum capable adversary,
and skip the intermediate step. But then again we would need to solve
the key exchange problem in the post-quantum-computers world, or we
are back to the steel walls and paper door.)
I don't know what sort of impact this would have on embedded systems,
but for applications that have access to decent amounts of processing
power, the switch from AES-128 to even AES-256 should for most
practical purposes barely be noticable, and if it is noticable and a
problem, it is at least _relatively_ easy to throw more hardware at
the problem. (Not quite so easy with embedded systems.)
_Yes_, an attacker will go after the whole system. But I don't think
that's a reason for not _making each primitive as strong as is
reasonable_ within a given set of performance criteria on a given set
of hardware.

@_date: 2015-11-18 23:21:13
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] A new, 
Are you saying that you expect us to wade through 1,152 lines of C
code to figure out how your "simple" algorithm (implemented in "self
documented" code) works? And what on Earth is Base64 doing in there?
Sounds a bit like cipher block chaining, doesn't it? Except in this
case the block is a single byte, which massively limits the amount of
diffusion you can get.
Um, no. Running a random application doesn't really prove anything,
and it _certainly_ doesn't prove that the algorithm is in any way
secure. I see _no_ past contributions here at least from you; you're
giving us over a thousand lines of code, with several numerical tables
with somewhat random-looking constants, and you're doing a fair bit of
funny stuff in there with arrays and pointers. At least I am not going
to be reviewing that code at the drop of a hat to make sure there's
nothing odd going on in there before I run it on my system, so I'm not
running it.
You haven't explained anything, and you definitely isn't being very
specific. What problem do you see with common algorithms? How does
your algorithm address those without being worse in other regards
(confidentiality, performance, etc.)? What types of attacks does your
proposed algorithm withstand? What types of attacks is it vulnerable
to? How does your algorithm work, _specifically_? (Hint: C code is
_not_ a lingua franca of cryptography.)
Um; no. Every practical cryptosystem allows for changing keys, and
many require new keys for each transaction. Look at OpenPGP, TLS, etc.
In practice symmetrical algorithms tend to be used with a new key for
each message. While the input key is often unchanged within a given
message, I don't see how your system is any different in this regard;
every encryption or decryption needs the input plaintext/ciphertext
and a key, as a starting point. An attacker can pick a key for a given
ciphertext just as well as the intended recipient can.
Key schedules is an old concept. Some algorithms, like Blowfish, even
use a slow key schedule to slow down attackers while providing good
performance for normal use.
The combination of single byte blocks and mutating the internal key
state sounds suspiciously like 8-bit RC4.
Salts aren't used in encryption. Salts (and peppers) are used in
hashing. Maybe you refer to IVs, which is an inherent component in
block chaining modes like CBC?
So, then, more or less 8-bit-block CBC.
An attacker using a related-key attack can just as easily pick related
keys taking any additional generated "key" material into account. A
key checksum might be useful in some specific cases, but not for added
security (unless you can describe the threat model it protects
against, and how it protects against that, _without_ making other
types of attacks easier). Related-key attacks aren't very useful
outside of highly specific situations; other types of attacks are more
interesting to look at. Also compare DES' 56-bit keys which were
commonly expressed as 64 bits with the parity bits all set to zero (or
parity, like a checksum).
At the very least parts of the plaintext is assumed to be known to the
attacker. You'd be surprised how much can be guessed based on
metadata. File headers are trivially guessable, and even if not, there
is only a small number in common use. Salutations don't come in too
many different variants. And so on. The intended recipient obviously
can't have the plaintext any more than an attacker does; they will
have the ciphertext (which is fully known to the attacker) and the key
(which the attacker can guess). Don't say "impossible"; state the
specific work factor for the best attack you have been able to devise,
against full round _and_ reduced round variants of your algorithm, and
describe the attacks in question as well as their requirements (memory
and encryption operations) and constraints ("works only if the
numerical values of the two related keys have a multiplicative product
ending in 7 in hexadecimal, and then only during the full moon").
So what exactly is _wrong_ with RSA, DH, ECC, AES, 3DES, IDEA,
Threefish, Salsa20, or any of the huge number of existing, well
studied, well understood algorihms by people known in the field? And
how are you fixing those issues with your algorithm, again, without
introducing new issues?
Let alone actual implementations using those.
Read through what Bruce Schneier has written in 1998 at
There is even a handy-dandy checklist in there. It probably isn't
perfect (nothing is), but it's a good start. Let's hear what you have
to say about each of those points.

@_date: 2015-11-19 09:29:54
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] A new, 
Neither of those are algorithms. They don't have keys, in a strict
sense. They do specify ways of agreeing on keys to be used with
agreed-upon algorithms, like how my web browser used TLS to agree on
AES-128-GCM and a shared 128-bit key with a web server just now.
So it's an IV. If it isn't, you need to explain how it is not, rather
than handwave the issue away with basically "it's something different
that I cannot explain". Nobody is going to be satisfied with such a
Any encryption algorithm that does not meet this design criteria is so
fundamentally broken that it should never get past the first brief
review by the person designing it. It's called the avalanche effect,
and is caused by diffusion; on average, a single bit change anywhere
in the key or an input block should cause about half the bits of the
output to change.
_How?_ Don't just say "this is a precaution against X"; _explain_ how
your algorithm without this step is vulnerable to X, and how your
algorithm _with_ this step is _no longer_ vulnerable to the same
Again, any encryption algorithm that does not give a huge change in
the output for a small change in the input is fundamentally broken.
This is not a point of advantage, it is an absolute requirement.
Where did you get the number 255 from? And why is the number of
outputs not modulo the block size (which in your case sounds like it
is 8 bits, but you haven't been explicit about that)?
Excuse me? Are you actually comparing your work, which you propose as
a serious cipher, to a simple polyalphabetic substitution cipher from
the mid-1500s, which you call a _cryptanalytic attack_ worthy of
consideration today?
Again, this is an absolute requirement in any serious cipher. An
encryption algorithm can be generalized as a mapping from plaintext
blocks p to ciphertext blocks c (and vice versa) with the key k being
the selector: e(p,k) = p->c where the operation of the "->" mapping is
selected based on k. If you change either p or k, then the output
_must_ change. Because we generally want d(e(p,k),k) = p in encryption
algorithms (that is, that the encryption is reversible, not a trap
door function), the mapping "->" must operate on a strictly one to one
basis, selected only by the key.
In the words of Wikipedia: "In CBC mode, each block of plaintext is
XORed with the previous ciphertext block before being encrypted." What
you describe sounds awfully similar. Updating the internal key state
while working is not unheard of; consider RC4 for just one example.
Again, _how_ does this help protect against the attacks you list? Show
how your algorithm without any specific step is vulnerable to some
attack, and how introducing the specific step _at the very least_
renders that attack inconsequential.
...which it isn't in encryption. Again, salts are used in hashes, and
even there, not in algorithms, but in implementations.
Again, this is a trivial requirement of any serious encryption
algorithm. Ideally, there should exist no attack better than
exhaustive key search; in practice, we usually find some attacks that
moderately lower the complexity (like how 128-bit key AES can be
broken in 2^126.1 computational complexity using the biclique attack,
rather than with 2^128 complexity in an ideal 128-bit key algorithm
with results expected after about 2^127 operations). Generally
speaking, any algorithm that has any practical attacks significantly
better than exhaustive key search is considered fundamentally broken.
No. For any two eight-bit quantities A and B, there exists an
eight-bit value C such that A xor B = C. (This is trivial boolean
algebra.) For three values A, B and C, we have A xor B xor C = D, and
can thus reduce to A xor E = D for E = B xor C. Merely successive xor
operations are thus equivalent to one xor operation with some other
value, no matter how many xors you stack on top of each other.
Using plain xors will also likely leak internal key state with any
reasonably guessable plaintext. For example, if you have a US-ASCII
plaintext input and a completely random key, the output ciphertext
will have the high bit of each byte set to the corresponding bit of
the internal key state, and the remaining bits will have their search
space vastly reduced. Encodings like UTF-16 will only exacerbate this
effect in most cases. A serious encryption algorithm must be equally
secure regardless of the plaintext.
I still haven't seen any description of how your algorithm actually

@_date: 2015-11-19 09:30:00
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] A new, 
You haven't explained anything. You haven't cited any previous works,
you haven't given any examples of attacks, you haven't explained how
the algorithm works (and again, no, C code _does not count_), you have
not explained how your algorithm is similar or different to anything
else, you use terms (nonce, salt) that are not commonly used in
discussions of encryption algorithms without even telling us
specifically what you mean, and you have no previous track record of
contributing here.
You haven't even told us some of the most _basic_ parameters about
your algorithm: number of rounds, length of key, and specific
performance (for example, cycles per byte on a given architecture). If
you have, then please, do point it out, because I haven't seen it. (If
you have, well, nobody is perfect; I might have missed it somehow.)
Nobody has analyzed your algorithm (unless there's something in the
moderation queue that I have yet not seen, but that's doubtful).
People are looking at how you present it.
Then describe specifically what that would involve, and explain why
you did not make those changes before proposing your algorithm.
Actually, there are many more. Key space, _effective_ key space, basic
operation (encryption or decryption) complexity, key setup complexity,
block size, ... And that's just some I can think of off the top of my
Given infinite memory, every encryption algorithm short of a one-time
pad is trivially breakable. If you can store 2^256 (keys) rows of
2^128 (blocks) columns of 128 bits (block size) each, breaking AES-256
becomes an exercise in _table lookup_.
On 19 Nov 2015 02:25 +0200, from ikizir at gmail.com (Ismail Kizir):
Cryptanalysis is not something you worry about in code, it's something
you worry about in algorithm design. In code, you might worry about
things like side-channel attacks, timing attacks and so on, but the
algorithm can be designed in a way that makes these attack vectors
more or less difficult to avoid.
Doesn't this change the algorithm, since the _only_ description of the
algorithm we have so far is this _one_ _implementation_ of it? What
are the security implications of this change? If it is a beneficial
change that you know about, why did you opt against it?
Nobody (to within experimental error) is going to wade through over a
thousand lines of C code to figure out how your algorithm works. The
onus is on you to describe how it works.
If your algorithm was accepted for anything resembling a serious
cryptography conference, you should have all this material already
because it should have been the first things asked for when you
submitted the paper to be presented at the conference if it wasn't
already in that paper.

@_date: 2015-11-23 21:06:10
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Fighting fear (of encryption) with fear (of bad 
The reference to Star Trek may be apt, but also, in my opinion, in a
manner of speaking inappropriate for the example. In the Star Trek
universe, encryption is often trivially breakable, yet it is still
somehow considered useful (as evidenced by its continued use).
Consider  by user Kyle
Jones, which starts out stating that:
For this reason, _encryption_ as used in the Star Trek universe is
perhaps a better metaphor for backdoored encryption.
And of course, let's not go into what goes for "downloading" data in
the Star Trek universe.

@_date: 2015-11-28 14:28:13
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] And it goes on... 
For those who want to learn more about who Bruce Hoffman is and his
professional career:

@_date: 2015-10-21 12:59:07
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Other obvious issues being ignored? 
This is drifting a little away from strictly being about cryptography,
but something like a standardized " nooptimize" attached to a
function to tell the compiler to not optimize the code in that
function at all but rather do as close to a one-to-one mapping from
the source code to machine language constructs, would probably go a
long way. (It wouldn't have to be a pragma; it could be a keyword, or
some other type of marker. The key part is what it would express, not
the mechanics of how to express it.)
It would not be perfect, for some of the reasons already outlined
(things like the possibility of data remnants in kernel memory after
context switching), but much of this is already outside the scope of
the language in the first place. It's easy to imagine a scenario in
which some piece of software works with security-critical data but
doesn't lock that page in RAM first; in which case the data may very
well get swapped out to disk by the OS, completely unprotected. The
language can't protect against that either. What the language _can_ be
used for is to express the programmer's intentions.
A function marked as such would be translated to the closest possible
machine language representation that does what the code says, even if
the compiler would normally optimize parts of the code away. This
would include things like treating every variable access as if the
variable was marked "volatile", no rearrangement of trivial operations
even if the observable end result would be the same, overwrite all
scratch registers on function exit, and so on.

@_date: 2015-10-22 08:27:39
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] How programming language design can help us 
That's C++, not C. I respectfully point you towards
In your mind, particularly in practice, what is the difference between
"the behavior is undefined if..." and "the result will be an undefined
value if..."? Do you have an example of any compiler the behavior of
which doesn't fall into the latter category already when faced with a
situation like this?
Yes, _technically_ "the behavior of Operation X is undefined in case
of ABC" means the compiler is free to do absolutely whatever it
pleases in that situation. _In practice_, there are only a few
possible outcomes, largely depending on what exactly Operation X and
the condion ABC is.
And of course, saying that (the operation or the result of) x<>|y|, or
allows for hardware architectures that transparently do x>>|y| when
asked to do x< It would be even better to specify that if the right argument is I can think of situations in which a specified result is just as bad
as an undefined result, when the specified result is not what you
would normally expect. _Especially_ in cryptography-related code.
C doesn't have exceptions. Or at least, didn't the last time I looked.
C++ does, but C++ is a different language from C. They are similar in
many ways, but it's also possible to write code in one that doesn't
compile in the other, or worse, compiles cleanly in both but has
different behavior in one compared to the other. Again, even though
they have many similarities, the two are different languages.
The second point brings us right back to the original question: how on
Earth is the compiler supposed to know where it is allowed to optimize
and where it isn't? We'd have to tell it somehow. Since _most_
applications benefit from the optimizations, and even applications
that need security benefit from optimizations in many other places in
the code, the reasonable strategy would be to have some way of telling
the compiler which parts we _don't_ want it to optimize (at all, or
just not optimize away). We don't want to build all of Mozilla without
optimizations just because a small portion of the cryptography-related
code needs specific guarantees about which operations get done.
If, as you propose above as the "even better" solution, an operation
that normally returns a value suddenly returns 0 where 0 is not the
desired value, how does that lead towards getting the right answer
when the language standard not saying what should be done (which is
_not_ the same thing as that a compiler vendor doesn't need to pick
some behavior) doesn't?
Like I said in my other post in this thread, add something like
" nooptimize" to the language. That would tell the compiler
that the such-marked block is off limits to optimizations, ensuring
that what the programmer expresses is what actually gets done.
We can't in source code do much of anything about what the CPU does
with the machine language instructions that building the source code
results in, but we can in source code and the language standard do a
lot about the transformation from source code to machine language.
Even the C++ standard you linked to mentions "implementation-defined"
behavior in a number of places (see for example  7.1.5.2 page 109, 
7.2 section 5 page 111,  7.4 page 123,  7.5 section 2 page 124, 
8.5.3 page 148, ...; a simple search for "implementation-defined" will
give you plenty of relevant hits).
If you ban undefined behavior from the standard because different
systems might do it differently, then by consequence you also must ban
implementation-defined behavior, because from the point of view of the
language standard, those are very close to the same thing.
Which of course means that the people who _are_ legitimately assuming
that the behavior defined by their particular compiler won't suddenly
change, will likely see _subtle_ breaking changes the next time they
update their compiler, to a version which follows such a revised
"Undefined behavior" is "we aren't saying what you should do" whereas
"implementation-defined behavior" is "we are saying that this part is
up to you to decide and possibly document". In both cases, the
standard defers the decision of the specific source code construct to
machine language transformation to someone else.
The language can help a lot here, by allowing the programmer to
specify that a function should not be passed through the optimizer
during the compilation phase. That would leave only optimizations done
in hardware, which seem like a much tougher nut to crack from the
language design perspective.
It isn't terribly difficult (just very different from what most
programmers are used to) to write code that executes in constant time,
as long as the compiler doesn't try to pull the rug out from under
your feet by rewriting the code.
Assuming that you can tell the compiler which data should be zeroised.
Just because data is copied somewhere and left there doesn't mean it
should be wiped. Case in point: writes to video memory.
Forgive my ignorance, but how on Earth can programming language design
help against attacks like TEMPEST?

@_date: 2015-10-24 17:13:58
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] How programming language design can help us 
Actually, it looks to me like Bertrand wasn't referring to your
program at all, but to Ray's.
I did use gcc -S, with a version of Ray's code massaged into
compilable form (_very_ similar to the one Brent posted). I did again
now, with that posted by Bertrand.
Specifically, I ran the C-to-assembler transformation of Betrand's
code several times. Once with default optimizations ("gcc -Wall
-pedantic -S test.c"), once with no optimizations ("-O0" added), once
with few optimizations ("-O1"), once with some more optimizations
("-O2") and once with extreme optimizations ("-O3"), and looked at the
output assembler.
When running with the default settings, which [1] claims is -O0, the
output assembler looks like one would expect. Here is what gcc, with
optimizations disabled, has transformed the lines
    z = x + y;    /* undefined in case of overflow */
    if (z < 0){
        printf("overflow at line %d\n", __LINE__);
        exit(1);
    }
    printf("positive result is %d\n", z);
        movl    -8(%rbp), %eax
        movl    -4(%rbp), %edx
        addl    %edx, %eax
        movl    %eax, -12(%rbp)
        cmpl    $0, -12(%rbp)
        jns     .L4
        movl    $13, %esi
        movl    $.LC0, %edi
        movl    $0, %eax
        call    printf
        movl    $1, %edi
        call    exit
        movl    -12(%rbp), %eax
        movl    %eax, %esi
        movl    $.LC1, %edi
        movl    $0, %eax
        call    printf
        movl    $0, %eax
        leave
.LC0 refers to the "overflow" format string, and .LC1 refers to the
"positive result" format string.
Note that, just like one would expect, there is a "cmpl $0, ..." and
"jns" in there, to skip over the part that, if the value obtained by
adding the two (loaded into %eax and %edx right after the .L3 label)
is negative, prints the "overflow" message and exits.
And quite right, when I compile and link with no specific optimization
settings, my system too prints "overflow at line 13". Built with gcc
(Debian 4.7.2-5) 4.7.2 on an up-to-date Debian Wheezy. "diff" says the
output of running GCC with no -O parameter and with -O0 is identical.
By the time we move into -O1 territory, the assembler code looks
_vastly_ different, and indeed the compiler has determined that the
check is redundant and optimized out the whole thing, leaving us with
a program that unconditionally prints the "overflow" message. When
running with -O1, this is the _full_ assembler output that I get when
using the above compiler (note that -O1 specifies optimizations above
and beyond the default):
        .file   "test.c"
        .section        .rodata.str1.1,"aMS", at progbits,1
        .string "overflow at line %d\n"
        .text
        .globl  main
        .type   main,         .cfi_startproc
        subq    $8, %rsp
        .cfi_def_cfa_offset 16
        movl    $13, %esi
        movl    $.LC0, %edi
        movl    $0, %eax
        call    printf
        movl    $1, %edi
        call    exit
        .cfi_endproc
        .size   main, .-main
        .ident  "GCC: (Debian 4.7.2-5) 4.7.2"
        .section        .note.GNU-stack,"", at progbits
If I replace the rvalue for the y assignment with "INT_MAX - argc",
then compile with -O1, the compiler keeps the check and running the
program again quite rightly prints "overflow at line 13" because it
obviously cannot fully evaluate the expressions involved at compile
If I add "volatile" to the variable declarations, as has been
suggested several times in this thread, but keep the original rvalue
expression, and recompile with -O3, I still get exactly the same
result: "overflow at line 13", because now I have _specifically_ told
the compiler that it cannot optimize out those accesses. The assembler
output in this case is significantly more difficult to follow at a
glance, but does refer to both string constants, and it has three
"testl"/"js" instruction pairs, seemingly corresponding to the three
comparison expressions in the C source code (testing x, y and z,
respectively, for less than 0).
Now. Is this a problem with the C language standard, with GCC, or is
it a problem with how people use GCC?
I think a case can be made for that, once you deviate from default
settings, the problem is no longer with the software, but with the
user, if those non-default settings have unintended consequences. I
also think a case can be made for that if the language provides a
means to do help the compiler understand which parts are critical (in
C's case, for example, the "volatile" keyword attached to a variable
declaration), and you don't use that, and you have told the compiler
to optimize, then if the compiler _can_ accurately determine the value
of an expression at compile time it is well within its right to do so
and optimize accordingly, even if the program otherwise would rely on
non-observable side effects of those operations.
The fact that a program _relies_ on _non-observable side effects_ is
something that _the programmer_ must take into consideration; it
cannot reasonably be the task of the compiler to know that one
_particular_ assignment of a value to a variable that is never read
again (and one that quite possibly goes out of scope immediately
afterwards) must not be optimized away.
This is why I think that a standardized way to tell the compiler
_that_ a particular chunk of code, perhaps at the function level,
_must not_ be optimized, would help a lot. (Would it completely
eliminate the problem? No, of course it would not. But it's a
low-cost, low-risk option that would seem to move us a great deal in
the right direction.) It would give an output from the compiler much
more like that obtained when running GCC with -O0, even when compiling
the whole package with say -O2 or -O3, without needing to compile
specific modules with separate settings (which would be brittle). When
running GCC with -O0, at least for this trivial program, we have
something that is pretty close to the "one to one mapping" between C
and assembler (or machine language) that I have discussed previously
in this thread: each C language expression maps to one or a set of
assembly operations, _and_ it's possible to look at the C code on one
side and the assembler code on the other, and directly map expressions
between them.
Or you could just mark the critical variables "volatile", forcing any
compliant compiler to always perform those accesses, even if they
appear to the optimizer as redundant.
 [1]:

@_date: 2015-10-30 10:38:03
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] letter versus spirit of the law ... Eventus 
This appears to me to oversimplify what actually happened during the
Ariane 5 launch failure in 1996.
According to [1], the main cause of the failure seems to have been a
combination of trying to return one type of data where another was
expected (an error code where flight data was expected), and the two
redundant units running exactly the same software and thus having the
exact same problem of returning one type of data where another was
The error that caused an error code to be returned, in turn, could
only happen because the physical properties of the Ariane 5 and its
trajectory were different from those of the Ariane 4, but the software
running on its on-board computers had not been adjusted accordingly.
In other words, the software that ultimately was the trigger for the
breakup worked as intended (but obviously not as required): a problem
was detected where certain values were not within the expected ranges,
because the range of valid values had not been adjusted to match the
changes in hardware. This led to an error condition being returned.
Downstream, this _error code_ was interpreted as flight data, which it
should never have been. _This incorrect interpretation_ of the value
that was returned led to the chain of events that eventually broke up
the launch vehicle. Had the error code been treated as an error code,
rather than as flight data, it seems likely to me that things would
have happened quite differently.
**This type of errors cannot be solved by the programming language.**
They are remedied by ensuring that redundant systems cannot reasonably
exhibit the same problems (for example by requiring at least two
different, fully independent development efforts targetting different
architectures), by software review at various stages, by requirements
review, and by ensuring that any software that is re-used is properly
adjusted to match _all_ changed requirements. And of course, making
absolutely certain at every call site that an error return cannot
_possibly_ be confused for a valid data value, for example by
returning status and results separately. "Exceptions" versus "error
codes" is simply the mechanics of how to ensure that last; at a
technical level, "exceptions" (like those in .NET, Ada, Java, C++,
etc.) are just a special kind of glorified return values which are
handled automatically rather than manually by the programmer.
And frankly, as expensive as even blowing up a spacecraft is, if the
alternative is continuing powered flight on an uncontrollable
trajectory, controlled destruction is almost always the less bad
choice. (The initial Ariane 5 breakup was however not controlled, but
due to excessive aerodynamic forces.) Compare the space shuttle
Challenger disaster [2], where the SRBs were destroyed by the range
safety officer after the orbiter broke up, to ensure that they did not
come crashing down under power.
 [1]:  [2]:

@_date: 2015-09-29 21:18:57
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Future GPG/PGP 
You don't need a replacement for GnuPG to go beyond 4096-bit RSA, even
if we limit the discussion to implementations of OpenPGP. In fact, RFC
4880  15 explicitly says:
and RFC 4880  13.5 says:
Plain GnuPG (the implementation) already supports two sets of
asymmetric encryption algorithms: RSA/RSA and DH/DSS. I doubt that
adding a third would require any major architectural changes, although
it obviously would require writing the code to implement that
additional algorithm or set of algorithms.
The maximum key length is almost certainly not more than a constant
set somewhere, intended primarily to ensure your average user doesn't
pick obscene key lengths like a 64k bit RSA modulus. Maximum
asymmetric key lengths in the PGP family have increased over time; I
recall reading somewhere that PGP 1.x only supported a maximum of 384
bits of RSA, and I distinctly remember PGP 2.6.2i topping out at 2047
bits (not 2048, due to a bug). I think PGP 5.0 was the version that
raised the maximum to a 4096 bit modulus in the case of RSA.
(Just because I could, when I replaced a SSH connection key recently,
I went with a 6144 bit modulus. Neither system involved is
particularly underpowered by modern standards, but the SSH
authentication using that key is _noticably_ slow.)

@_date: 2016-04-21 09:57:19
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Security on TRIM for full-disk encrypted SSDs 
I believe Valmiky is referring to the fact that an attacker would be
able to tell which portions of the drive hold data and which do not,
which would allow them to concentrate their crypto-breaking efforts on
the portions that actually hold data. Data in well-known locations can
also provide likely candidates for known-plaintext attacks.
That said, I don't really see why an attacker would choose to attack a
random block on the drive, rather than (say) the header which often
has well-known data in well-known locations. For the huge majority of
users even who use full disk encryption, assuming the attacker really
wants access to the data and not just a quick exchange of cash for a
piece of hardware they didn't have the day before, attacking the
passphrase is almost certainly easier than attacking the crypto.

@_date: 2016-08-11 16:19:12
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Public-key auth as envisaged by first-year 
So RFC 1149 actually anticipated RFCs 6973 and 7258?
Admittedly, RFC 6973 seems to not discuss the non-electronic aspects
of threat migitations, seemingly reducing that whole issue to only
data minimization, which might help in non-electronic transfer but by
itself doesn't solve the problem.
(Yes, this post is a little tongue in cheek.)

@_date: 2016-08-11 20:10:28
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Public-key auth as envisaged by first-year 
One thing that strikes me about the scenario you describe (low
powered, point-to-point radio links over short distances perhaps using
omnidirectional antennas) is that the link budget would be appropriate
for the initial link distance (because to begin with, we assume no
MITM of any kind is present). Hence, two devices 100 meters apart have
a link budget that is appropriate for communication over 100 meters
plus change. Call these two initial devices talking to each other
Alice and Bob.
If someone injects a MITM, call it Mallory, in between Alice and Bob,
on a wireless network, then wouldn't Alice and Bob _still be able to
communicate directly_? Unless Mallory is doubly active (not only
functioning as a MITM attacker to intercept and handle communication
supposedly being passed between Alice and Bob, but also disrupting
attempts at direct communication between Alice and Bob), Alice should
still hear Bob's transmissions and Bob should still hear Alice's. So
if Alice sends a message to Bob, which Mallory is able to MITM and
where Mallory responds back to Alice, then _at the very least_ Alice
should also hear _Bob's_ response presumably with a slight delay
compared to that from Mallory, _and_ it's possible that Bob would hear
_Mallory's_ response.
Even if Mallory detects Alice transmitting to Bob the _instant_ the
source and target station IDs (in whatever form) passes Mallory, and
_instantly_ starts jamming that transmission in such a way that
Mallory can still receive what Alice is transmitting, in that case at
the very least Bob will get _tons_ of garbled data over the wireless
network, which is easy to check for. And technology isn't instant;
even more so if it needs to be field re-programmable.
It's not like, the way it can be done with a cable splice, Mallory can
_selectively drop_ traffic originally intended by Alice to be sent to
Bob, or vice versa. The radio waves don't really care that Mallory is
listening; Alice's radio transmission will happily barge on toward Bob
while Mallory is processing and responding to it back to Alice. So
even if Mallory beats Bob to responding to Alice's request, Bob _too_
will respond in the same way.
I'm not sure if this can be exploited, but intuitively, it seems like
**it should be possible to at least _detect_ Mallory's presence in a
scenario similar to what you describe simply by the fact that messages
are _consistently_ responded to twice.** At that point, obviously
Mallory is already a part of the network, but this kind of
abnormalities in the message-passing could be enough to sound the
well-famed intruder alert. Especially in a controlled environment, if
a node that you _should_ hear suddenly goes silent, or starts
responding twice to everything you pass to it, that _in itself_ could
be interesting information for a monitoring station _even if_ it isn't
necessarily caused by any malicious party at all. It wouldn't be very
hard to stretch this out to _all_ nodes on the network monitoring the
traffic that they can hear, and sounding the alarm if they hear nodes
consistently starting to respond twice to the same message.

@_date: 2016-08-12 12:13:56
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Public-key auth as envisaged by first-year 
Good points all. About that last; for anything below several
gigahertz, a dish antenna with sufficient gain (directionality) to
matter would be very large; quite likely large enough to easily be
seen at the distances under consideration. So it wouldn't exactly be a
_stealthy_ device.

@_date: 2016-02-20 11:30:42
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Apple 3rd Party dilemma 
The problem with this kind of argument is that large portions of the
general public does not share this sentiment at all. It's quite
possible that a large fraction of the people on this list would agree
to a security trade-off like that, but the membership on this list is
very clearly not a representative subsample of the general population.
Here's a thought experiment for you. First, look at how many people
have smartphones.
Then, look at how many of those have data on their smartphone that
would be difficult or impossible to replace if it were lost.
Now subtract the number of people who regularly make backups (to a
_trusted, secure_ data store!) of the data on their smartphones.
Notice how little the group size changed in the last step?
I expect that with services like Apple's iCloud, the proportion of
people who at least _back up_ their data is slightly higher than with
say PCs, but unless the data is securely encrypted _before_ it leaves
the phone, and decryption requires access to a high-grade secret that
both is not bound to the physical phone and the vendor (in this case
Apple) does _not_ have access to, that fails at least the "secure"
data store test. And generally speaking, people are poor at handling
high-grade cryptographic secrets, not to mention their ability of
coming up with them.
The average John or Jane Doe _wants_ their data to be recoverable if
something happens to the device it is stored on. A device that, by
design, makes data recovery after a malfunction or accident difficult
enough to effectively be impossible, is going to be considered broken
by lots of people -- especially people who had their three-year-old
drop their phone into the bath tub just to see what would happen.

@_date: 2016-02-24 22:02:45
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Practicality of codebook in current-day secret 
Why do the competitors have access to the communications in the first
How does needing to refer to some kind of code book, whether
electronic or on paper, mitigate this threat? Ideally in a way that,
say, an active VPN and a (possibly further encrypted) corporate IM
service cannot, _if_ real-time communications with others during the
negotiations are even needed?
Remember that the idea of cryptography is to transform large secrets
(sensitive plaintext, for some definition of sensitive) into small
secrets (keys), because small secrets are easier to keep secret.
It seems to me that surely an adversary which is able to subvert and
monitor an active VPN link in more or less real time is quite capable
enough to get their hands on a code book as well.
This ties back into the above point on whether real-time communication
is needed. Why does "essential instructions and responses" need to be
exchanged between the negotiator and the far-away manager during
negotiations? Why can't the negotiator have a mandate to _negotiate_,
within some given bounds? ("Get the best offer you can, Joe, but
whatever you do, do not accept a bid below $42 trillion.")
If the manager does not trust the negotiator with such a mandate,
maybe the manager should go to the negotiations instead.
Cryptography can be useful, but it should be used to solve real
problems. Your proposed hypothetical situation where a code book could
be useful seems like one where plenty of easier options exist.

@_date: 2016-02-26 15:31:30
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Apple: graphically show users that they are 
Okay, so you want to somehow visualize to the user the fact that the
Internet is a dangerous place. Because, yes, indeed, the Internet _is_
a dangerous place; I am not arguing that point, in fact I have made
the same argument on a number of occasions.
This does however have the obvious problem of: **what is the end user
supposed to _do_ with that information?**
We tried going down a very similar route with software firewalls on
individual PCs that popped up warnings about every little thing.
(Anybody remember ZoneAlarm?) It might have been a good idea in
theory, in the eyes of an engineer familiar with TCP/IP, but it didn't
work terribly well in practice with a variety of types of users with
hugely varying sets of computer knowledge.
Telling people that everything is doom and gloom, while giving them no
constructive avenue for improving the situation, especially in a
situation where there is no single clear responsible entity, is rarely
conducive to an environment where those issues get resolved.
For an end-user appliance meant to cater to broad groups, such as
software running on a smartphone or home computer, it seems better to
work on any relevant false positive and false negative ratios, and
silently drop anything that is detected as malicious as early in the
processing chain as possible (that last to reduce the risk of any bugs
themselves allowing compromise of the device).
86-year old Aunt Jane isn't interested in a visualization of "current
network danger level"; she wants the phone, tablet, computer, toaster,
refrigerator, lightbulb, whatever to _just work_ in the environment
it's designed to work in. Including the good, the bad and the ugly.
The engineer who _wants_ to know this isn't interested in an aggregate
"current network danger level"; they are much more likely to want the
gory details.
In neither case can I see how your proposal helps anyone.
Learned helplessness is not something to encourage, at all.

@_date: 2016-07-12 15:39:48
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Putin goes full Stasi; 
On 8 Jul 2016 07:15 -0700, from hbaker1 at pipeline.com (Henry Baker):
According to the EDRi (unverified claim), the EU isn't all that far
behind Russia in this.
Quoting from

@_date: 2016-07-13 08:45:54
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] The Laws (was the principles) of secure 
Honestly, I think these are already too many. There is probably room
for quite a bit of combining these into fewer points that cover the
same ground. For example,  ("only those you trust can betray you")
and  ("holes for good guys are holes for bad guys too"), possibly
along with  ("it's all about who is in control") can probably be
summed up in one point, perhaps "technology does not discriminate
based on objective intent or subjective opinion". There's no need for
points that cover essentially the same ground; that's what elaboration
beyond the sound-bite is for.
Also, one that I absolutely think is missing is that _security always
favors the attacker_. (The defender has to defend against _every_
threat; the attacker only has to find _one_ viable attack vector that
is not adequately defended against. That attack vector might not even
allow for a full breach; if it allows the attacker to get a toe-hold,
it might be enough to compromise the security of the system. It also
might not be in a place where you expect security problems at all.)
Hence security in depth, your proposed  "design for future
threats" (which I agree with), and observations like Schneier's Law.
And of course, what happened to  in your list?

@_date: 2016-07-16 13:01:13
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] The Laws (was the principles) of secure 
**...and can I be certain that nobody has tampered with the data in
the meantime?**
Your variant covers confidentiality and availability, which are only
two sides (albeit very relevant ones) of the security triangle; there
is also authenticity.
The extreme example, of course, is again the one-time pad: textbook
OTPs, if done right, provide confidentiality and availability, but not
authenticity. For that you need something like an OTP plus MAC.
So how about, instead of "security is a boolean", chalk it down to the
slightly longer but IMO much more descriptive: **If you think that the
answer to "is it secure?" is "yes", then the correct answer most
likely is "no".**
Based on the reasoning that an _unqualified_ "yes" answer to an
_unqualified_ "is it secure?" is virtually guaranteed to be wrong,
based on the idea that there is some attack vector you are not
considering or not fully defending against or which is outside of the
scope of the system under consideration. A "no" answer, on the other
hand, allows for (and even points at the likelihood of) the
possibility of there being _any_ attack vector that you are not (a)
considering or (b) properly protecting against within the scope of the
Even if that attack vector is something mundane such as rubber hose
cryptanalysis or denial of service.
Someone who is willing to spend the time to determine what might be a
reasonable threat model won't be asking an unqualified "is it
secure?", but rather a qualified "given this threat model, is this
system secure against this threat?". The answer to _that_ might very
well legitimately be "yes". _For example_, I could very likely
legitimately answer "is the data stored well-encrypted on magnetic
HDDs where the FDE metadata has been overwritten by random data,
secure against disclosure from decommissioned drives sent in for
electronics recycling to within a budget of $10,000?" with an actual
"yes", because any remnants of data will be random-looking garbage,
encrypted but where the decryption key has been thrown away.

@_date: 2016-06-13 10:45:06
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Determining TLS session keys from the hypervisor 
One thing I was thinking about when hearing about this attack is SSH.
It seems to me that the same kind of attack could apply to virtually
every kind of key negotiation similar to TLS, and SSH certainly would
seem to fall into that family. What's more, taking over a SSH session
seems more likely to be able to get a toehold into a system because of
how often SSH is used for remote administration.
I guess it's the old adage again: if an untrusted party has
unrestricted physical access, then it's not possible to fully secure
the system.

@_date: 2016-06-23 15:41:17
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] What to put in a new cryptography course 
Add to this: The heart of cryptography is NOT anonymity. Cryptography
can very well provide all three (or some combination of them) _iff_
the cryptosystem is properly designed (but as we know, that's the
exception rather than the rule).
It's not _directly_ related to the subject of cryptography, and might
not warrant a module all of its own, and is somewhat related to the
above point. Consider touching on the subject of _leaky metadata_.
Cryptography can make a wide range of problems much easier to solve
(but can, of course, also make a wide range of otherwise trivial
issues much harder to solve), but it can't protect what it isn't
tasked to protect. Data required for message routing; (network and
disk) I/O timing along a transmission path; (relative) message sizes;
... all of that needs something other than pure cryptography. (Notice
how I didn't mention unencrypted headers in e-mail?) DNS, SNI, ...
Also logs; mail logs, web server logs, system audit logs, ...
That's something that I find to often be missing from introductory
material. You can give someone a screwdriver, but if they don't know
when a screwdriver is a useful tool and when it is not, then they are
bound to try to hammer a nail in with it some day. Sometimes you
really need a hammer or for that matter a wrench, rather than a

@_date: 2016-03-18 22:45:38
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Apple GovtOS/FBiOS & Proof of Work 
Let's hope then that there are forever never any legitimate security
vulnerabilities discovered in the relevant software that need to be
patched in a hurry.
Remember the "goto fail;" bug from about two years ago? [1] [2] [3]
[4] Which was in plain sight in _publicly available_ code for anyone
who really cared enough to go over the code? [5] (And I assume that
multiple countries' intelligence agencies go over code like that with
a fine-toothed comb continuously, looking for things they might be
able to take advantage of. Even finding one such bug a year would
probably make it worth the programmer time, and I expect the rate to
be far higher.) It was an absolutely trivial fix to a high-impact
vulnerability once it had been spotted, with what I would consider
_no_ possibility of unintended side effects from the fix; exactly the
kind of update that in an ideal world (which, of course, we do not
live in) should be heading out the door about five minutes after the
programmer clicks "commit".
And of course, the presence of any "fiddlable" sections implies that
there is data in the firmware that doesn't really do anything at all.
You can't go around flipping bits in highly specific machine code or
even data used by it and expect nothing to happen; it has to be a
portion that just sits around unused. Is that something we really want
to encourage?
I can sympathize with the idea, and for some very specific situations
it might even have merit, but for general purpose software that at
times can very easily have a legitimate need to be updated quickly?
Not so much.
 [1]:  [2]:  [3]:  [4]:  [5]:

@_date: 2016-11-09 19:58:50
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
On 8 Nov 2016 10:10 +0000, from pgut001 at cs.auckland.ac.nz (Peter Gutmann):
That line of reasoning is brought up time and again here. Let me ask
just one _totally honest_ question: Was SSL ever designed to protect
against such threats? (Not "sold as a way to", but "designed to".) Yes
or no? If yes, then since when?
Because if it wasn't, the above seems to me to be somewhat akin to
blaming window manufacturers because someone can take a chainsaw or a
sledgehammer to a wooden door and enter a house that way, despite the
fact that the family pet is properly cared for.
That's not to say that the threats you list are irrelevant, and that
they should be ignored in today's world; only that we can't expect
some technology to, except perhaps by pure chance, solve a problem
that _it was never even designed to solve_.

@_date: 2016-11-12 14:41:14
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
On 8 Nov 2016 01:57 +0100, from iang at iang.org (ianG):
Snowden? Did you mean to refer to Assange?

@_date: 2016-11-22 09:14:50
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] combining lots of lousy RNGs ... or not 
The problem with this line of reasoning is that it requires that the
concept of "trust" is determinable in the general case by anyone, and
that everyone looking at the same data reaches the same conclusion.
_That is not the case._ One entity may perfectly well trust some
technology or implementation, while at the same time another entity
has complete distrust in that same technology or implementation. As
such, the reasoning falls apart.
RDRAND may be for all practical intents and purposes perfectly secure.
It may also trivially be completely broken by anyone with knowledge of
exactly how it works. It could also have been designed to be secure,
but a flaw in the design or implementation makes it vulnerable to some
attack. The point is, _we don't know which._ And in the absence of
that knowledge, people are going to look at it (or any other
technology) and come to different conclusions about to what extent it
is trustworthy. Part of those conclusions are going to be based on
solid, quantifiable things like a well-designed threat model and
thorough analysis of how the technology in question holds up against
that threat model; and part of them are going to be based on paranoia
or blind trust, whether appropriate or misplaced.
So in a situation where there are many distinct, disparate trust
zones, and there is no one point among those that everyone can agree
is trustworthy, meaning there is no technology that everyone can agree
is sufficiently secure for some specific use (in this case,
cryptographically secure random number generation), _what is the best
we can do?_

@_date: 2016-11-27 15:32:54
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] RNG design principles 
So the idea is for the "randomize" command to work in a manner similar
to the TPM registers? (Where new_value = hash(old_value || input), or
something very much like that.)

@_date: 2016-10-03 20:26:45
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] French credit card has time-varying CVC 
On 3 Oct 2016 11:50 -0700, from hbaker1 at pipeline.com (Henry Baker):
So it's the CVC, not the PIN, that is time-variant. That's a pretty
big difference.

@_date: 2016-10-17 20:37:15
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] constructing and exploiting trapdoored DH/DSA 
I'm not Hanno, nor do I claim in the least to be answering in Hanno's
place, but to me, there's a pretty big gap between
"demonstrat[ing] that constructing and exploiting trapdoored primes
for Diffie-Hellman and DSA is feasible for 1024-bit keys with modern
academic computing resources."
"the NSA broke trillions of encrypted connections"
The former shows that something is possible, which implies that it
_might_ have happened, either in select situations or wholesale.
The latter claims that the same something not only is possible, but
_did happen_ and _was exploited_, and additionally in this specific
case goes even further by claiming to explain _how_ it happened. The
use of the word "trillions" also implies that it wasn't a rare
There's a _big_ difference between those two.
Yes, it seems pretty clear that we need to move beyond 1024 bits for a
whole host of reasons, just like how 512-768 bits became insufficient
many years ago. No, that is not helped by sensationalist propaganda,
but rather by increasing default sizes to something like 1536 or 2048
bits or moving to different algorithms with different security versus
performance properties (elliptic curves, anyone?). A case for that can
_clearly_ be made without mentioning nation-state adversaries. IMO,
particularly _people on this list should know better._

@_date: 2016-10-26 09:05:30
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] How to prove Wikileaks' emails aren't altered 
This looks interesting:
Basically, get  (Debian/Ubuntu seems to
use the package name python{,3}-dkim), then pipe the RFC 822 formatted
message into `dkimverify`.

@_date: 2016-09-17 10:37:01
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Recommendations in lieu of short AES passphrases 
On the other hand, people are _far_ more likely to notice that their
wallet is missing than that someone has surreptitiously logged into
their e-mail or social media account. While I don't advocate the
trivial means of writing passwords down (see [1] for some actual
suggestions; feedback on that page welcome!), if someone _has_ to
write a password down for some reason, I'd rather it be on a piece of
paper kept in their wallet on their person, than on a post-it attached
to the keyboard or screen, or in a text file kept on their computer's
 [1]:

@_date: 2016-09-18 20:43:41
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Recommendations in lieu of short AES passphrases 
I certainly expected people here to disagree with some of that.
Let me just point out first that what I wrote on that page isn't meant
as the end-all, be-all, protects-against-everything set of advice.
It's nowhere near that. It's meant to be _reasonably actionable_ to
protect people in general against password-class attacks.
Also, I'm not saying that passwords, especially static passwords, is
an ideal solution to the problem of establishing an identity. But they
are here, and we're basically going to have to live with them for a
long time still, because for most purposes, as a concept, they are
_good enough_.
Of course that won't be the last vulnerability. And you'll notice I
have a _huge_ reservation on that page about online password storage
services. Locally running software isn't _necessarily_ safer, but if
there's malware on your system capturing passwords (or even all
keystrokes), then for passwords, you are screwed almost no matter what
you do that doesn't involve only one-time passwords; and you are
probably screwed anyway, but for non-password-related reasons.
But if an online password management solution allows users to use
secure passwords, as opposed to ones straight out of the Top 100
lists, then doing so is _probably_ a step in the right direction.
I could have gone into plenty more detail, but the page was already
becoming quite long, and I also wanted something that people _actually
might read_. True enough, no method (including 2FA) is a panacea, and
they have different security properties. The question I was trying to
answer with the part on 2FA was _how the average user might be able to
strengthen their login credentials beyond a simple password without
breaking a leg_. Porting someone's cell phone number to get access to
their SMS traffic is a very much targeted attack, and relatively
overt. If you are the type of person who might be the target of such
methods of attack (which, admittedly, likely constitutes a significant
fraction of this list's membership), then plan accordingly. The
average person on the street probably isn't going to be the target of
such an attack, yet could benefit from their Facebook account not
having "password1" or "qwertyuiop" as a password.
I don't even talk about encryption keys on that page. I suppose the
closest would be passwords to FDE containers, which would fall under
"passwords that you have to remember". The only place where that page
talks about encryption keys is in reference to locally stored password
manager databases, and that's only as essentially an aside.
I _do_ talk about passwords in terms of bits of cryptographic strength
in order to give a framework for how to compare different types of
passwords. It's largely meaningless to talk about a Diceware password
in terms of number of characters, but it's very sensible to talk about
it in terms of bits of strength against a brute-force attack executed
by someone who knows that you are using Diceware passwords.
This is a good point, and one I might very well consider adding.
That's easy to say, but most people aren't willing to have a
completely separate, air-gapped computer for their financial stuff,
let alone Facebooking, and if they did, those computers would very
likely quickly get the same malware as their main system anyway simply
because they don't have the discipline to maintain that level of
separation. Those people can _still_ be well served by protecting
their Facebook account with a decent password. Again, this is intended
for common people, not those with a threat model that can quite well
include nation state level adversaries. Much of it may apply _also_ to
the latter group, but more by virtue of being reasonable advice that
lots of people can adapt to their own situation than because I wrote
it to specifically target that group.
Ah, yes. It bugs me out at work every few months when I have to change
my password; but only one out of, I think, four(!). Gives me headaches
every time until I get them all back in sync and get all sessions
reestablished. Windows and Active Directory, I'm looking at you.
Indeed, and that's part of the point; give users an idea of _how_ to
take care regarding passwords. Saying something like the above might
make for a better introduction, though.

@_date: 2016-09-18 20:49:23
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Recommendations for short AES passphrases 
It's also assuming that these "keys" being tested are actually PGP or
GnuPG private key passphrases. OpenPGP s2k is deliberately relatively
computationally intensive, though I don't know how it compares to a
modern PBKDF.
I did some math for a reply I later threw away, based on the numbers
at [1], and came up with a single 2.2 GHz AMD Opteron 8354 being able
to test somewhere close to 2.5M keys per second in AES/ECB mode with a
single block of data.
 [1]:  also

@_date: 2016-09-19 07:37:01
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Recommendations in lieu of short AES passphrases 
Curious. You disagree with what I suggest, then suggest some of the
same things yourself.

@_date: 2017-04-03 08:25:51
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Removal of spaces in NIST Draft SP-800-63B 
On 2 Apr 2017 15:17 -0400, from kevin.w.wall at gmail.com (Kevin W. Wall):
Which has/have a tendency to break legitimate workflows, including
non-automated usage of a password manager. I copy and paste usernames
and passwords from my password manager into the web browser all the
time, in part because I don't quite trust automation to always get it
correct. At least if I mess up myself, I know (or am able to figure
out quickly) which two accounts are involved and can go change those
passwords without having to guess too much.
If pasting into password fields is broken, I will have to choose a far
less secure password, because really, there is no way I'm going to
type a 50+ upper/lower/digits/symbols/hieroglyphs password manually
every time. Either that, or I go with a competing service. (Yes, I
_know_ that 50+ is overkill, but I'm already using a password manager,
so why not add a decent safety margin? It's not like it makes it any
Please don't ever encourage breaking standard workflows, including
copy and paste.

@_date: 2017-04-04 16:55:47
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Interesting new TLS RFC draft 
On 4 Apr 2017 09:09 +0000, from pgut001 at cs.auckland.ac.nz (Peter Gutmann):
Unfortunately, particularly sections 4.1 and 8 make it a little too
obvious even for a document dated April 1st. The I-D also doesn't have
much discussion in light of BCP 188. It could likely be argued that
section 3.2 addresses BCP 188, albeit only indirectly; a more direct
discussion would be appropriate.
Are the given authors' names real, though? "Y. Crypto" reads kind of
funny, but I suppose in some parts of the world, Yolo Crypto _could_
be an actual person's name.

@_date: 2017-04-04 16:59:38
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Does anyone here know PAM? 
On 4 Apr 2017 10:04 -0400, from phill at hallambaker.com (Phillip Hallam-Baker):
I haven't tried it myself, but something like
 looks like a decent starting
point. It is described as a way to "Unlock GnuPG keys on login" by
being "A PAM module that hands over your login password to

@_date: 2017-04-04 21:40:45
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Tempest and limits on receiving 
On 4 Apr 2017 14:34 -0500, from wilson at math.wisc.edu (Bob Wilson):
I don't see how what you are saying contradicts such a statement. The
fact that you can or can't buy an off-the-shelf receiver that covers a
particular frequency range has no immediate bearing on whether you are
allowed or not allowed to shield against RF or EMI. To a similar tune,
I can't readily buy an aviation band transmitter, but that doesn't
mean I'm not allowed to listen to aviation transmissions. I may or may
not be allowed to _convey the content of_ such transmissions, however.
Also, have you checked out the market for SDR (software-defined radio)
equipment lately? I'm pretty sure that you, without too much trouble,
can find ones that allow receiving your pet frequency.
That's not shortwave. Shortwave (technically the HF range) is 3-30
MHz, and in colloquial speech it is sometimes extended to something
like 1.5-30 MHz or thereabouts, but 100 MHz is well beyond shortwave
(and has very different propagation characteristics, including e.g.
that reliable ionospheric propagation basically doesn't happen;
sporadic E-layer propagation doesn't count as reliable). The microwave
range, for comparison, begins around 2 GHz or so.

@_date: 2017-08-14 16:51:43
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] NIST SP 800-63-3 
Besides the points already raised in relation to this; while some
entities may be able to afford a HSM and surrounding hardware and
infrastructure, how many "hobby" or even "subject matter enthusiast"
sites will?
Memory-hard salted hashes can be implemented relatively easily purely
in software running on top of virtualized platforms running on top of
commodity hardware, while still getting most of the security benefits.
I don't think HSMs have that property.
So a secret stored in a HSM _might_ be reasonable for the US
government, but memory-hard salted hashes can help all the rest of us.
And of course, nothing in 800-63 _prevents_ you from storing the
secret in a HSM if you want to, does it...?

@_date: 2017-12-29 09:55:05
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Fast handling of IP Address changes for HTTPS 
That's probably the best option for self-hosting, IMO. There are
options out there that don't even cost as much as $4/month, if low
cost is your primary objective and especially if you're willing to
trade some performance for a lower price.  is a
good place to start looking if you want a cheap VPS.
Or just get a cheap web hosting package somewhere and you won't have
to deal with the hassle of keeping the server running and secure.
Otherwise, while I don't use it myself, supposedly Let's Encrypt
supports validation via DNS. That might be worth looking into as well.

@_date: 2017-01-01 16:34:48
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
On 1 Jan 2017 08:10 -0500, from allenpmd at gmail.com (Allen):
So a smart meter perhaps has a way to shut off electricity remotely.
Personally, I can see the validity of such a use case from the power
company's point of view. But that also implies that it has (or at the
very least a very strong _should_ have) a way to turn the power back
on remotely.
Unless the meter is bricked, assuming that the disconnection was
illegitimate, a quick phone call to the power company should be enough
to get the ball rolling on getting power restored in short order. If a
malicious actor (Mallory, say) is able to shut off power again after
that, then it is to a very large degree the power company's problem.
As for a fire being tracked back to a malfunctioning smart meter, I'm
with Jerry; that has nothing to do with whether the meter is smart or
dumb. A malfunctioning dumb meter, given the same set of
circumstances, could all but certainly fail in the same manner. That's
why we have fuses, which are designed and intended to cut the power
before an excessive current flow causes excessive additional damage.
They don't always succeed, but really, they tend to succeed more often
than not. (And I don't know what is customary in the US, but every
electrical installation I've had the opportunity to look at, including
that to my own house, has incoming feed fuses _before_ the meter, so
even if there's a dead short in the meter, there's a limit to how much
current can be drawn before the fuses trip.)
I think it was Bruce Schneier who said it, but it doesn't really
matter who did: _It's in the news because it barely ever happens._
Don't worry about what's in the news, worry about everything that
_isn't_ in the news but still happens.

@_date: 2017-01-03 12:17:25
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
On 2 Jan 2017 23:44 +0000, from pgut001 at cs.auckland.ac.nz (Peter Gutmann):
_I actually think that's the big issue._ We can figure out how to
create a product (electricity meter) that meets some specifications
(implements protocol X as described by standard ABX-1234, resistant to
attacks Y, Z and Z'). Cryptography can play a part there. Frankly,
this is probably the easy part.
But how often are meters replaced? I suspect a ten year timeframe for
widely fielding a new product is actually perfectly realistic, if not
actually optimistic. I wouldn't be the least bit surprised if the
average electricity meter sees 10-20 years of service life in the
field before being replaced.
As a personal anecdote, I had the electricity meter on my house moved
recently, as the above-ground service feed was replaced with a
below-ground cable and I at the same time opted to have the meter
moved to a physically more convenient location. At a guess, based on
rate of electricity use, I would estimate that the meter is on the
order of 10-15 years old. I specifically asked the power utility
company people, mostly out of curiosity, if they were going to replace
the existing meter, or just move the existing one, and the answer was
along the lines of "why would we do anything but move it?". That's
_recently_ as in _a few months ago_.
Heck, it probably took those 10-20 years for utility companies to
replace "dumb" meters with "smart" meters en masse, despite the
obvious savings from the utilities' point of view in not needing to
regularly send someone to physically read each meter, with the risk of
errors that inevitably introduces. (We can argue all day about various
vulnerabilities, but from the utility's point of view, that _is_ a
pretty nice feature to have.)
There's a lot of inertia to overcome in fielding a new standard,
_especially_ when the new product required doesn't offer a "killer
feature" (pun not intended) for the entity making the decision on
which product to procure.

@_date: 2017-07-01 21:32:50
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Another way of putting this would probably be:
"Having an attacker inserting a back door into an operating system
kernel is not an unreasonable threat model for some people."
That might be true, particularly for some values of "some people".
(Yes, there are ways that might work to backdoor /dev/urandom that
don't involve fiddling with the kernel. But unless you are horribly
incompetent, all of those should still require either root privileges
on the system in question, or physical access. In both of those cases,
if someone really _wants_ to mess with you, you are pretty much
screwed _anyway_, with or without OpenSSL.)
But is that really a threat that OpenSSL should try to defend against?
Is it even something that OpenSSL meaningfully _can_ defend against?
If you can't even trust the operating system kernel, then why should
you trust OpenSSL running on top of that kernel? Why should you trust
_anything_ running on top of that kernel?
If your threat model legitimately includes people messing with the
random number generator in the kernel in order to exfiltrate data,
then maybe you shouldn't be running security-critical stuff on
$2/month OpenVZ VPSes...

@_date: 2017-07-10 18:10:59
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] A software for combining text files to obtain 
Would you mind elaborating a little on how that becomes a problem? My
first reaction when I saw the description was that this is _too_
simple, there's probably a flaw in there somewhere if you just look at
it for a bit (no pun intended), but I haven't analyzed the proposal in
depth so can't really comment on what, if indeed any, that would be.
However, if "around 50%" of bit sequences of any given length have a
particular bit set 0, then isn't that just what we'd want in a random

@_date: 2017-05-12 07:47:25
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Bizarre behavior of a non-smart mobile phone 
To what end?
someone's phone to behave the way you describe? The only thing I can
think of that it _would_ accomplish would be to annoy random people
that you communicate with on at least a semi-regular basis (enough so
to store their phone numbers in your phone's address book). It also
does so in a way that is utterly trivial to trace back to you, _which
allows you to rectify the issue_ on way or another.
I wouldn't be the least bit surprised if membership or participation
on this list is a significant attention flag in some places. But I
also wouldn't be too quick to dismiss the old adage "don't explain by
malice that which can be adquately explained by incompetence". That,
and "extraordinary claims require extraordinary evidence".
Buggy software running on the phone where you happen to hit some edge
case for whatever reason, or a physically or electrically faulty phone
(a loose solder joint, maybe?) sounds to me like a _far more plausible
explanation_ for what you are seeing, than software manipulation as a
targetted attack by an adversary. That doesn't mean that I summarily
dismiss even the possibility that it _could_ be the case, but it
definitely wouldn't be the first hypothesis I reach for to explain the
data you have presented.
That said, I honestly fail to see what this has to do with

@_date: 2017-05-26 17:41:12
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] key lengths in different places 
DES has a 64 bit key, of which eight bits (one bit per byte) was
traditionally used for parity and has no cryptographic value, being
ignored by the cipher. Thus 56 bits of key material are used for
3DES has several different keying modes, but can be used with three
independent DES keys (Wikipedia refers to this as "keying option 1").
That is nominally 3 x 64 bits = 192 bits of key material, but because
only 56 bits of each key is used, only 3 x 56 bits = 168 bits of key
material is used. This is often referred to as 168-bit 3DES, but
calling it 192-bit 3DES is as valid as saying that DES uses a 64-bit
key; that is, not completely correct, but at least has some basis in
3DES with three independent keys is vulnerable to a meet-in-the-middle
attack, which reduces the _effective_ security to that of two
applications of DES, corresponding to 2 x 56 = 112 bits of security
for 168 bits of actual key material. The security level is thus the
same as a cipher where no such shortcut exists but which uses a 112
bit key. The work factor for a brute force attack is thus 2^112.
All of "192 bits", "168 bits" and to some extent "112 bits" are thus
valid answers to the question "what is the key size of 3DES"; it all
depends on what specific metric you are looking at.
Assuming triple DES with three independent keys, 192 bits is the size
of the physical key; 168 bits is the amount of key material used; and
112 bits is the work factor for breaking the resulting encryption by
brute force.
Most often we are interested either in the amount of key material
actually used, or the work factor; so calling 3DES with three
independent keys as using a 168-bit key, or as having a 112-bit work
factor, is likely the most useful. The combination of this also works,
obviously; 3DES with three independent keys takes 168 bits of key
material to deliver the security of a 112-bit work factor.

@_date: 2017-09-02 13:17:59
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] TEXTCOMBINE-REV, 
pseudo-randomness in practice (replacing an earlier retracted software)
On 1 Sep 2017 22:05 +0200, from mok-kong.shen at t-online.de (mok-kong shen):
On the other hand, why should we knowingly give users _less_ than the
best we can reasonably achieve within the relevant engineering
It's hardly a valid argument to say that "this class of users don't
expect any significant security, so let's use MD5/DES/RC4/whatever
instead of an algorithm believed to be secure". A valid argument can
_possibly_ be made that the restrictions imposed e.g. by the hardware
_requires_ the use of less computationally intensive algorithms
(which, in turn, does not necessarily imply that those algorithms are
less secure; only that they have different properties), but your
proposal, to me, does not appear to fall into such a category.
Maybe you've posted that before, and I've missed it, but: What use
case does your idea target which is not covered by some other,
existing, well-studied algorithm for deriving unpredictable data?

@_date: 2017-09-14 14:45:38
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] After Equifax pwning, 
I believe Germany might be a decent example to look at too, with their
rather recent history and reasonably large population. Apparently,
even the German tax agency will issue _multiple distinct_
identification numbers to be used by _the same individual_ but for
different purposes.
When you're designing systems pretty much from the ground up and _on
purpose_ to make it difficult to correlate and aggregate data between
different systems and databases, that leads to some interesting design
choices being made.
Legal requirements being one aspect of those.

@_date: 2018-07-17 08:02:22
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] storage encryption 
Well, what's the threat model? You say that this is for an "activist
group", but that term alone spans a huge spectrum already (both in
terms of what those people are doing or planning on doing, and in
terms of the number of people involved), and the response to each
point on that spectrum can also cover a pretty broad range.
It's hard to propose solutions when the problem isn't clearly defined.

@_date: 2018-07-30 19:42:25
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] how to encrypt for the very long term? 
Well, if that's the biggest issue you're seeing with GPG, then it's
easy to use passphrases that provide sufficient randomness to
withstand brute-force attacks. That the S2K is fast doesn't matter if
the S provides as much randomness as the ultimate K and thus attacking
the S confers no advantage to the attacker over attacking the K even
if the S2K was instantaneous, _and_ attacking the K is impractical.
It only takes a 20 words long properly generated Diceword passphrase
to provide in excess of 256 bits worth of randomness (20 * log2(6^5) =
258). Add a few more words because your dice probably aren't perfectly
balanced, and 22-24 Diceware words should be plenty enough to match
the theoretical security of a 256-bit key. With EFF's dictionary, that
can be compressed into 60-72 alphabetic characters; themselves, if
chosen at random (obviously not the case for Diceware, as 6^5 < 26^3
by about half), providing between log2(26^60) ~ 282 and log2(26^72) ~
338 bits of randomness. With the standard English Diceware dictionary,
the passphrase ends up being around 104-125 characters; or you could
use the dice values directly for 100-120 digits' worth of passphrase
yielding 258-310 bits' worth of randomness.
At that point, I really don't see how you'd need to worry about
attacks on the cryptographic primitives. Some form of rubber hose
cryptanalysis becomes a very much more viable option well before that
One big upside of using an established solution like GPG (or more
generally OpenPGP) is that it's unlikely to go away in a hurry.

@_date: 2018-11-10 11:08:39
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Seeking recommendations for a dedicated 
On 9 Nov 2018 16:01 -0800, from ron at flownet.com (Ron Garret):
You might want to have a look at Hetzner. I have two VPSes with them and although I haven't talked to them on
the phone (the service tier I selected does not include telephone
support), their services have been just about rock solid for me since
I signed up with them about a year and a half ago. The few times I've
had reason to submit support tickets they have always been dealt with
promptly and professionally. The VPSes I have, despite them being the
cheapest VPS tier available at the time, are also plenty fast.
If a VPS isn't your cup of tea, they also offer various options for
managed or colocated physical servers at reasonable price points.
Their datacenters are located in Germany and Finland.
No affiliation whatsoever; just a happy customer.

@_date: 2019-08-16 15:00:10
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Well, that only took ten years 
Not sure about 10% of LE's volume capability, but Scott Helme blogged
about this back in January at [1], mentioning Buypass [2] as one
alternative to Let's Encrypt which provides free DV certs via ACME
validation [3]. Buypass even provides sample command lines for EFF's
Certbot client at [4], linked to from their product page (so about as
official as it gets).
 [1]:  [2]:  [3]:  [4]:

@_date: 2019-03-25 09:02:43
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Clinton email issues 
Even if it is the longer one, if I'm not mistaken, log2(36^25) ~ 129.
For a birthday attack by a powerful adversary, and where presumably
the attacker can generate numerous keys and possibly somehow
deliberately craft the generated keys in order to get (closer to) a
desired fingerprint, is that really sufficient?

@_date: 2020-02-23 11:37:30
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Apple's 13-month certificate policy 
Alternatively, certainly Certbot has its built-in web server
functionality, and you can use its --pre-hook and --post-hook to punch
a temporary hole in the firewall for port 80 for the few seconds that
the validation process requires. I'm not sure about other ACME
clients, but I suspect they have similar functionality as well.
DNS TXT authentication nicely bypasses the need for such host-local
changes, but if for some reason you can't (or don't want to) use that,
then the above is a possible alternative that doesn't require running
a full-time fully-functional web server on the host in question,
thereby significantly reducing the attack surface.

@_date: 2020-01-12 13:37:30
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] improved identification of non-targets 
Mode-A/C/S transponders (which is what you'll find in any typical
civilian aircraft, besides more recently ADS-B Out) already respond to
"anybody who asks". If you're not happy with the answer, nothing says
you can't ask again, or wait for someone else to ask and listen to the
ADS-B isn't Mode-S. The 24-bit identifier you have in mind is Mode-S;
I'm not sure what exactly ADS-B uses for an identifier, it _might_
even be the same value, but the two technologies are different. Once
you consider ADS-B In, they are _very_ different.
Mode-S already gives aircraft identity (which can be mapped to a
flight number in commercial aviation, via filed flightplans which are
required for Instrument Flight Rules traffic already; in turn,
virtually all commercial air traffic is IFR) and standard-atmosphere
pressure altitude (pressure altitude corresponding to a ground level
static pressure of 1013.25 hPa). It also gives the transponder code
("squawk" code) assigned by air traffic control to that flight and
selectable by the pilot. The latter is a four-digit octal number,
thereby allowing for 8^4 combinations, a handful of which are reserved
for specific purposes (including emergency situations such as radio
failure and illegal interference) but most are free for assignment by
Mode-S transponders have been required in large swaths of airspace for
a long time. ADS-B is making inroads as of recently.
Don't forget the timing issue. Transponders send the Mode-S packet in
response to radar interrogation; in congested airspace (such as in the
vicinity of airports, which also often happens to be in close
proximity to radar transmitters) this needs to happen quickly enough
that it can't be confused for one from another aircraft. Also, the
nonce would need to come from the ground station; if the aircraft can
select the nonce, little keeps them from picking them ahead of time.
It's absolutely possible for an aircraft to be in radar range, and
therefore its transponder interrogated, from multiple ground stations
at once. Once you add airborne radar stations to the mix, things get
even more complicated. I very strongly suspect that there's a standard
document somewhere which specifies a hard real-time limit to
interrogation responses.
If an aircraft isn't allowed to operate in your airspace and has a
working transponder, it will show up on secondary radar and you just
have your ATC not clear it into your airspace. That's standard
procedure already, and pilots flying in controlled airspace are
legally required to comply with ATC instructions anyway except in a
few specific cases that don't apply during normal operation and which
basically boil down to pilots' final authority in emergency
situations. If pilots don't follow ATC instructions and enter
controlled or restricted airspace without proper clearance, there are
already procedures in place for how to deal with that which almost
certainly _don't_ involve missile batteries as a first or even second
If the aircraft has its transponder turned off for whatever reason, it
doesn't really matter how that transponder is designed, because it's
not going to respond to interrogations anyway. At that point your best
bet is primary radar, with all its drawbacks. (There's good reason why
ATC relies primarily on secondary radar.)
Do keep in mind that regulations already require that pilots are able
to turn off every piece of electrical equipment on the aircraft from
the cockpit (even if doing so requires pulling a circuit breaker).
While hopefully extremely rare, there are potential situations where a
problem with the transponder might actually require turning it off.
As for the recent case of Ukrainian Intl 752, at least according to
what I've seen publicly stated so far, the flight was flying its
cleared departure, on a proper flightplan, with everything in order
and the transponder turned on (otherwise it would not, for example,
have shown on Flightradar24). It departed Tehran international
airport, which (at least according to Wikipedia) is owned by the
government of Iran, so the government absolutely could have access to
ATC's transponder code assignments, the actual departure time, and the
flightplan which would specify the airframe used; the departure point
and heading would also match the flight's departure runway. There's
probably more than this. While of course none of this is _conclusive_
evidence in favor of a given radar return being a commercial passenger
flight, that's a _lot_ of things adding up in favor of it being one.
A simple NOTAM (Notice to Airmen) closing the relevant airspace to
traffic could potentially have prevented the PS752 disaster. Pilots
already review NOTAMs before departure (or Dispatch does it for them,
but it's ultimately the responsibility of the Pilot In Command to
ensure it has been done) when selecting the route to fly; any airspace
closure NOTAM would have been a huge red flag, especially if they
included airspace near the airport's normal departure corridors.
Aviation, _especially_ commercial aviation, has a huge number of
checks and balances in place precisely to prevent any one person's
mistake from turning into a disaster, because it's been well known
since the 1970s that even highly trained, experienced people make
unintentional mistakes as well as (far less often, thankfully)
intentional errors. (Look up cockpit resource management for just one
aspect of this.) Some of those checks and balances are technological
in nature; others are non-technological; others mix the two.
You can add at the very least TCAS (Traffic Collision Avoidance
System) requirements to that list. I'm willing to bet that despite
those being uncommon, too-close encounters with other aircraft are a
_lot_ more common than too-trigger-happy missile crews operating in
airspace not closed to traffic.

@_date: 2020-01-13 21:28:15
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] improved identification of non-targets 
I never said that they would need to close _all_ of the airspace,
though of course they could do that if they really wanted to (and as
you say take the effects). They could easily leave a narrow corridor
open for arriving and departing flights, while closing the surrounding
airspace. The corridors could be made however long and at whichever
altitudes the authorities consider appropriate at the time, and could
be changed quickly if necessary. Airspace closures of various shapes
and sizes is nothing unusual.
Or, just tell ATC not to clear flights for certain instrument arrival
or departure routes. You don't even need to tell the ATC controllers
why you're doing that, let alone the pilots, though telling the ATC
controllers will allow them to give better assistance should an
aircraft develop an emergency. A separate NOTAM could simply inform
pilots that into and out of a given airport, normally available routes
A, B, C and D are unavailable, leaving E and F. They could even do
this on a per-carrier basis if they really wanted, especially if they
do it through ATC. Again, standard procedure, nothing unusual, pilots
already deal with variations of this every day.
A strong limiting factor in actual airport capacity is going to be
wake turbulence anyway, so if done right, neither of this needs to
affect actual throughput capacity much.
IFR traffic is procedurally required to have a filed and approved
flightplan; ATC won't let IFR traffic onto the runway without a
flightplan. At some point prior to departure, the flight will be
assigned a transponder code. I don't know what ATC will do if the
flight does not squawk its assigned code, but due to simply the risk
of conflicting code use, they will have to do _something_ in such a
situation. And in case of doubt, ATC can always request that pilots
IDENT (which sends a specific signal back to the ground station) to
highlight the flight on their displays, helping provide positive
identification of a radar return. That's sometimes used in cases of
radio failure, too, to establish whether an aircraft can _receive_ but
not _transmit_ on voice radio, or if they also cannot receive others'
That's already done all the time. Aircraft crossing airspace
boundaries will typically identify with ATC giving their flight number
and current altitude, as well as their target altitude if they are
climbing or descending. This allows ATC to identify the flight; again,
they can always request an IDENT in case of doubt.

@_date: 2020-01-17 09:36:43
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Dieharder & symmetric cryptosystems 
Which _exact_ version of the Linux kernel are you running? At a
minimum: Upstream version as reported by uname, distribution,
distribution release, and distribution kernel package variant and

@_date: 2020-11-21 13:47:46
@_author: Michael =?utf-8?B?S2rDtnJsaW5n?= 
@_subject: [Cryptography] Possible reason why password usage rules are 
Even NIST did away with the suggestion/requirement for forced periodic
password changes years ago now. Last I looked, their current
requirements for passwords were far more sane in a modern environment.
Passwords (and passphrases) as a concept aren't perfect, but I agree
with what someone said earlier; even if we come up with a perfect
replacement that has no downsides (sincerely: good luck), we will
still be stuck with the concept of passwords for a long time to come.
Therefore, it seems to me to make sense to not have the perfect be the
enemy of the good enough; and to find ways to actually encourage both
developers and end-users to take steps that we _know_ work for
protecting password-protected assets.
For example: using long, unique, randomly generated passwords _works_,
and we do have the tools (which, admittedly, can be improved in
places) to make using such passwords less awkward. For actual
high-value assets where the reduced usability can be justified by the
increased security, different types of one-time codes or separate
authentication tokens such as for example key fobs, smartcards or
Yubikeys can help raise the bar further for an attacker.
Similarly, proper hashing of passwords before storage _works_, and
that's something that can be implemented without really changing
_anything_ for the user.
Once you're already doing something like, say, using a password
manager instead of remembering and typing in every password by hand,
assuming support for long passwords, the usability difference between
a 10-character password and a 60-character password is effectively
negligible. I have unique, random >60 character passwords to log in to
a few _news site subscriptions_, where about the worst an attacker
could do is to quite literally read the news; not because such a
password is in any way necessary to protect such an account, but
because I might as well. It pretty much guarantees that whatever else
might go wrong, my password there definitely won't be the weak link.
