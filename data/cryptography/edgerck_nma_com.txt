
@_date: 2005-08-26 15:44:57
@_author: Ed Gerck 
@_subject: instant lottery cards too, Re: reading PINs in "secure" mailers without 
Years ago, I could read "instant win" lottery cards and still leave them
as new by using the laser photoacoustic effect. A low-power chopped laser
beam is focused and line-scans the target while a microphone picks up
the acoustic waves caused by differential absorption of the laser light
as it sweeps the line. By phase-shifting the received acoustic signal
versus the chopped light signal (they have the same frequency), you
can read at different depths of the target. Adjusting to hearing at
the depth of the paper substrate, below the covering ink, all markings
could be read as if the covering ink did not exist, line by line.
The apparatus could be built today by something like $500, I believe,
using parts readily available. Distributors of the "instant lottery"
cards could, without detection, separate the winning cards.
Unlike ATM cards, there are no cards that must be stolen at the same
time for the attack to be successful.
Ed Gerck

@_date: 2005-12-06 22:44:05
@_author: Ed Gerck 
@_subject: X.509 / PKI, PGP, and IBE Secure Email Technologies 
X.509 / PKI (Public-Key Infrastructure), PGP (Pretty Good Privacy)
and IBE (Identity-Based Encryption) promise privacy and security
for email. But comparing these systems has been like comparing apples
with speedboats and wingbats. A speedboat is a bad apple, and so on.
To help develop a common yardstick, I would like feedback (also by
private email) on a list of desirable secure email features as well
as a list of attacks or problems, with a corresponding score card for
the secure email technologies X.509 / PKI, PGP and IBE. The paper
is at Ed Gerck

@_date: 2005-12-07 12:37:04
@_author: Ed Gerck 
@_subject: X.509 / PKI, PGP, and IBE Secure Email Technologies 
James and Aram, thanks for your comments. My reply is inlined
Boats are storm rated, however. The point here is how can we
"storm rate" secure email systems?
For example, it might be a good idea to have different tables for
attacks vs. problems (for boats: resisting pirates versus
resisting high waves). For example P13 (must pre-enroll recipients) may
be a problem and it really is something different compared to an
attack, i.e., a problem leading to an attack such as P12 (open message
However, in order to make some progress in this, I felt a need to
simplify things into + and -. This makes sense also in terms of
attacks vs problems (for email, maybe not for boats!) because a
secure email system that has a lot of problems will likely be used
badly (insecurely) or not at all -- which opens room for attacks.
So, both problems and attacks are negative for security.
What I'm saying is that we can, and do in many other cases,
compare different systems in terms of their features and
shortcomings. Irrespectively of how we may further classify them,
a feature is + and a shortcoming is -. For example, that's how your
car value is estimated when you sell it -- by how many negative and
positive points it gets. It doesn't matter if the negative points
came from a dent or an absence of air bags, they all count negatively.
I'm looking into each product individually and building a collective
metric based on technologically-neutral specifications for + and -.
To do this, the paper asks two questions:
(1) what are the product's capabilities in terms of a list of desired
features (the +), and
(2) what are the product's shortcomings in terms of a list of attack/
problems (the -),
for each main market product using those technologies and assigns each
score (+ or -) to the technology used by that product. At the end
of this algorithm, scanning enough products, we have a list of all
capabilities and all shortcomings that affect each technology.
Of course, each feature and problem/attack is intended as optional --
you can pick and choose what you want from the list, to make your
own subset of the score card, valid for your needs (and amount of
money you want to pay).
These lists are an intrinsic "yardstick" that we can use to both rate each
technology and also, if we want, each individual product. For example, a
product that has 80% of the positive score and 20% of the negative score
for that technology is possibly better than a product using the same
technology that would have 20% of the positive and 80% of the negative
for that technology. We can, in the same way, compare products using
different technologies, comparing their score cards.
For example P1: giving others the ability to get your private
key at a server without your knowledge or consent.
Anything we can do to negate "after the fact investigations" is
useful , such as protecting the email headers (communication
security, not just message security). The paper's score card
reflects several aspects of this, in its positive and
negative scores.
For example P1 to P8.
Depends on your use. An X.509 identity cert or a PGP cert
can be made as secure as you wish to pay for. The real
question, however, that is addressed by the paper is
how useful are they in terms of email security? How do
you compare them and which one or which product to choose
from? What are the trade-offs?
Store and forward makes it reliable -- nothing needs to be
100% online 100% of the time (which is, of course, a totally
improbable event).
Ed Gerck

@_date: 2005-12-08 11:30:36
@_author: Ed Gerck 
@_subject: X.509 / PKI, PGP, and IBE Secure Email Technologies 
Regarding PKI, the X.509 idea is not just to automate the process of reliance
but to do so without introducing vulnerabilities in the threat model considered
in the CPS.
What's a bit of a struggle, still, is that many people do not fully realize
that the CPS is outside the scope of PKI. This is both a solution (makes the
X.509 effort independent of local needs) and a big problem, as CAs (writers
of the CPS) have the power to write almost anything they want, including
their notorious DISCLAIMER (where _near_ everything of value to the subscriber
is disclaimed, while _everything_ of value to the user is disclaimed).
That's why its useful to compare X.509 / PKI, PGP, and IBE technologies
for secure email, to know what are the trade-offs.
By comparing the capabilities and faults of the secure email products
per technology used, these and other problems come up in the score card.
Ed Gerck

@_date: 2005-12-08 17:10:20
@_author: Ed Gerck 
@_subject: X.509 / PKI, PGP, and IBE Secure Email Technologies 
I believe that's what I wrote above. This rather old point (known to the X.509
authors, as one can read in their documents) is why X.509 simplifies what it
provides to the least possible _to_automate_ and puts all the local and human-
based security decisions in the CPS.
(The fact that the CPS is declared to be out of scope of X.509 is both a
solution and a BIG problem as I mentioned previously.)
PGP is public-key email without PKI. So is IBE. And yet neither of them has
all the identical, same basic components that PKI also needs. Now, when you
look at the paper on email security at
you see that the issue of what components PKI needs (or not) is not
relevant to the analysis.
 > ... as in my oft repeated description of a crook attacking the
What you say is not really about X.509 or PKI, it's about the CPS. If the CPS
says it restricts the cert to the assertion that the email address was timely
responsive to a random challenge when the cert was issued, then relying
on anything else (e.g., that the email address is owned or operated by an
honest person or by a person who bears a name similar to that mailbox's username)
is unwarranted.
Ed Gerck

@_date: 2005-12-09 16:10:54
@_author: Ed Gerck 
@_subject: X.509 / PKI, PGP, and IBE Secure Email Technologies 
I think that's where PKI got it wrong in several parts and not
just the CPS. It started with the simplest (because it was meant to
work for a global RA -- remember X.500?) and then complexity was
added. Today, in the most recent PKIX dialogues, even RFC authors
often disagree on what is meant in the RFCs. Not to mention the
As another example, at least one IBE offer does not talk about
key lifetime at all -- in fact, the documentation online talks
about using the same key for _all_ future communications. When this,
of course, fails and key expiration is introduced, it will be
over an existing baseline... a patch. Key revocation will be
even harder to introduce in IBE.
As new capabilities conflict with the old, the end result of this
approach seems to ne a lot of patched in complexity and vulnerabilities.
It seems better to start with a performance specification for the full
system. The code can follow the specs as close as possible for
each version, the specs can change too, but at least the grand
picture should exist beforehand. This is what this thread's subject
paper is about, the grand picture for secure email and why aren't
we there yet (Phil's PGP is almost 15 years old) -- what's missing.
BTW, there's a new version out for the "X.509 / PKI, PGP, and IBE
Secure Email Technologies" paper and Blog comments in the site as well,
at Ed Gerck

@_date: 2005-12-10 14:54:12
@_author: Ed Gerck 
@_subject: X.509 / PKI, PGP, and IBE Secure Email Technologies 
The benefits of not always requiring direct online transactions has been
pointed out before in this thread, in terms of anonymity, availability and
reliability. What happens when you get a message and the direct, online
connection isn't there? You can' decrypt it even though it you need to?
Digital certs (X.509 and PGP) are useful when the key owner is not online.
There is a world when this not only happens but is also useful. BTW, this
is recognized in IBE as well.
A couple additional comments:
 > the baseline analysis, threat/vulnerability models, etc ... start with
 > the simplest and then build the incremental pieces .... frequently
 > looking at justification for the additional complexity.
 >
 > when doing the original design and architecture you frequently start
 > with the overall objective and do a comprehensive design (to try and
 > avoid having things fall thru the cracks).
Agreed, and that's where a baseline analysis really fails to reveal a
design's pros and cons -- because it follows a different path. Seems
logical but denies the design's own logic (which did NOT use a baseline
approach to begin with, on purpose).
Therefore, when I look into X.509 / PKI issues, or secure email issues,
a baseline analysis is not so very useful.
 > the trusted third party certification authority is selling digital
 > certificates to key owners for the benefit of relying parties.
The RPs are not part of the contract. Without CAs, there's no "key
owner" in PKI. It's for the benefit (and reduction of liability)
of the key owners.
Ed Gerck

@_date: 2005-12-16 10:11:28
@_author: Ed Gerck 
@_subject: X.509 / PKI, PGP, and IBE Secure Email Technologies 
Yes. Your observation on out-of-band PGP key verification
is very important and actually exemplifies what Werner
wrote. Exactly because there's no trust model defined
a priori, uses can choose the model they want including
one-on-one trust.
This is important because it eliminates the need for a
common root of trust -- with a significant usability
If the web of trust is used, the sender and recipient must
a priori trust each other's key signers, requiring a
common root of trust -- that may not even exist to begin
So, instead of worrying about what trust model PGP uses,
the answer is that you can use any trust model you want --
including a hierarchical trust model as used with X.509.
Jon Callas and I had several conversations on trust in
May '97, when Jon visited me for two weeks while I was
in Brazil at the time, I think before the OpenPGP WG was
even working on these issues. This is one of the comments
Jon wrote in a listserv then, with a great insight that
might be useful today:
   As I understand it, then, I've been thinking about some
   of the wrong issues. For example, I have been wondering
   about how exactly the trust model works, and what trust
   model can possibly do all the things Dr Gerck is claiming.
   I think my confusion comes from my asking the wrong
   question. The real answer seems to be, 'what trust model
   would you like?' There is a built in notion (the
   'archetypical model' in the abstract class) of the meta-
   rules that a trust model has to follow, but I might buy a
   trust model from someone and add that, design my own, or
   even augment one I bought. Thus, I can ask for a
   fingerprint and check it against the FBI, Scotland Yard,
   and Surite databases, check their PGP key to make sure
   that it was signed my Mother Theresa, ask for a letter of
   recommendation from either the Pope or the Dalai Lama
   (except during Ramadan, when only approval by the Taliban
   will do), and then reject them out of hand if I haven't had
   my second cup of coffee.
Ed Gerck

@_date: 2005-12-22 12:53:48
@_author: Ed Gerck 
@_subject: Comparison of secure email technologies 
Thanks for the comments. A new version of the work paper
"Comparison Of Secure Email Technologies X.509 / PKI, PGP, and IBE"
is available at The Blog (link in the paper page) contains the most relevant
public input; private input is also appreciated.
Comments are welcome.
Ed Gerck

@_date: 2005-07-06 22:19:17
@_author: Ed Gerck 
@_subject: [Fwd: VirtualGoods Workshop in Florence: Deadline for Submission, 
Reply-To: 	Juergen Nuetzel Dear Members of the VirtualGoods mailing list,
this e-mail is a kindly reminder for the deadline (July 20th) of the
3rd VirtualGoods workshop in Florence, Italy.
This year the workshop is part of the Axmedis conference (30 Nov - 2 Dec
2005) See the VirtualGoods CFP for details and guidelines:
Juergen Nuetzel

@_date: 2005-07-15 11:31:56
@_author: Ed Gerck 
@_subject: EMV and Re: mother's maiden names... 
Well, the "acceptable risk" concept  that appears in these two
threads has been for a long time an euphemism for that business
model that shifts the burden of fraud to the customer.
The dirty little secret of the credit card industry is that they
are very happy with 10% of credit card fraud, over the Internet or not.
In fact, if they would reduce fraud to _zero_ today, their revenue
would decrease as well as their profits. So, there is really no
incentive to reduce fraud. On the contrary, keeping the status
quo is just fine.
This is so because of insurance -- up to a certain level,
which is well  within the operational boundaries of course,
a fraudulent transaction does not go unpaid through VISA,
American Express or Mastercard servers.  The transaction is
fully paid, with its insurance cost paid by the merchant and,
ultimately, by the customer.
Thus, the credit card industry has successfully turned fraud into
a sale.  This is the same attitude reported to me by a car manufacturer
representative when I was talking to him about simple techniques
to reduce car theft -- to which he said: "A car stolen is a car sold."
In fact, a car stolen will need replacement that will be provided by
insurance or by the customer working again to buy another car.  While
the stolen car continues to generate revenue for the manufacturer
in service and parts.
Whenever we see continued fraud, we should be certain: the defrauded
is profiting from it.  Because no company will accept a continued  loss
without doing anything to reduce it. Arguments such as "we don't
want to reduce the fraud level because it would cost more to reduce the
fraud than the fraud costs" are just a marketing way to say that
a fraud has become a sale.
Because fraud is an hemorrage that adds up, while efforts to fix it --
if done correctly -- are mostly an up front cost that is incurred only
once.  So, to accept fraud debits is to accept that there is also a credit
that continuously compensates the debit. Which credit ultimately flows
from the customer -- just like in car theft.
What is to blame? Not only the twisted ethics behind this attitude but
also that traditional security school of thought which focus on risk,
surveillance and insurance as the solution to security problems.
There is no consideration of what trust really would mean in terms of
bits and machines[*], no  consideration that the insurance model of
security cannot scale in Internet volumes and cannot even be ethically
"A fraud is a sale" is the only outcome possible from using such security
school of thought.  Also sometimes referred to as "acceptable risk" --
acceptable indeed, because it is paid for.
Ed Gerck
[*] Unless the concept of trust in communication systems is defined in
terms of bits and machines, while also making sense for humans, it really
cannot be applied to e-commerce. And there are some who use trust as a
synonym for authorization. This may work in a network, where a trusted
user is a user authorized by management to use some resources. But it
does not work across trust boundaries, or in the Internet, with no
common reporting point possible.

@_date: 2005-07-16 10:36:24
@_author: Ed Gerck 
@_subject: EMV and Re: mother's maiden names... 
Thanks for some private comments. What I posted is a short
summary of a number of arguments. It's not an absolute position,
or an expose' of the credit card industry. Rather, it's a wake-
up call -- The time has come to really face the issues of
information security seriously, without isolating them with
insurance at the cost of the consumers. Why? Because the
insurance model will not scale as the Internet and ecommerce
In other words, "CardSystems Exposes 40 Million Identities"
as a harbinger. Now that we know more about the facts in this
recent case, expect more to come unless we begin to improve
our security paradigm.
Yes, public opinion and credit card companies can and will
force companies that process credit card data to increase
their security. However, as my comments show, how about the
"acceptable risk" concept that turns fraud into sales?
Do As I Say, Not As I Do?
By weakly fighting fraud, aren't we allowing fraud systems
to become stronger and stronger, just like any biological
threat? The parasites are also fighting for survival. We're
allowing even email to be so degraded that fax and snail
mail are now becoming atractive again.
Ed Gerck

@_date: 2005-06-02 17:02:50
@_author: Ed Gerck 
@_subject: Citibank discloses private information to improve security 
> This will change,.  I predict that the banks will end up
I think that the odds are better, historically, for the following
Banks will shift to the users the cost and liability for phishing,
for good or for bad, and they will not finance any add-ons
otherwise they would have the liability for those add-ons failing.
The solution for phishing will have to come from developers and
will be adopted by banks as long as (in this order): (1) the bank's
liability is zero, and (2) it works.

@_date: 2005-06-13 16:24:02
@_author: Ed Gerck 
@_subject: expanding a password into many keys 
You need to go beyond the scope of simple-minded PKCS recommendations
to calculate keys from passwords. If you want to improve security,
just adding padding and salt is not enough.
Yes, of course, your code should add padding, so that the sha1 argument
always has the same, fixed, length for any password and key name.
Further, as you know, passwords (especially if chosen by a user)
have low entropy... let's say 10 ~ 40 bits. Key names (constrained
by natural language) should also have low entropy per character.
The end result is that a dictionary attack could be quite easy to do,
if you are not careful on several fronts.  You need to:
- define your threat model;
- warn users about bad passwords (not all bad pwds can be detected!);
- prevent really bad passwords from being used (ditto);
- prevent easy key names (ditto);
- estimate minimum lengths for passwords AND key names as a function
   of all the above -- including the threat model;
- provide for key management, with revocation, expiration and roll-over,
   before you face these needs without planning.
Ed Gerck

@_date: 2005-05-26 11:24:12
@_author: Ed Gerck 
@_subject: Citibank discloses private information to improve security 
In an effort to stop phishing emails, Citibank is including in a plaintext
email the full name of the account holder and the last four digits of the
ATM card.
Not only are these personal identifiers sent in an insecure communication,
such use is not authorized by the person they identify. Therefore, I believe
that some points need to be made in regard to right to privacy and security
It's the usual tactic of pushing the liability to the user. The account
holder gets the full liability for the "security" procedure used by
the bank.
A better solution, along the same lines, would have been for Citibank to
ask from their account holders when they login for Internet banking,
whether they would like to set up a three- or four-character combination
to be used in all emails from the bank to the account holder. This
combination would not be static, because it could be changed by the user
at will, and would not identify the user in any other way.
Private, identifying information of customers have been used before
by banks for customer login. The account holder's name, the ATM card
number, the account number, and the SSN have all been used, and abandoned,
for Internet banking login. Why? Because of the increased exposure
creating additional risks.
Now, with the unilateral disclosure by Citibank of the account holder's
name as used in the account and the last four digits of the ATM number,
Citibank is back tracking its own advances in user login (when they
abandoned those identifiers).
Of course, banks consider the ATM card their property, as well as the
number they contain. However, the ATM card number is a unique personal
identifier and should not be disclosed in a plaintext email without
A much better solution (see above) exists, even using plaintext email --
use a codeword that is agreed beforehand with the user. This would be
a win-win solution, with no additional privacy and security risk.
Or is email becoming even more insecure, with our private information
being more and more disclosed by those who should actually guard it,
in the name of security?
Ed Gerck

@_date: 2005-05-26 14:19:09
@_author: Ed Gerck 
@_subject: Citibank discloses private information to improve security 
Suppose you choose "A4RT" as your codeword. The codeword has no privacy concern
(it does not identify you) and is dynamic -- you can change it at will, if you
suspect someone else got it.
Compare with the other two identifiers that Citibank is using. Your full name
is private and static. The ATM's last-four is private and static too (unless
you want the burden to change your card often).

@_date: 2005-05-27 10:16:17
@_author: Ed Gerck 
@_subject: Citibank discloses private information to improve security 
Wells Fargo reported to me some time ago that they tried using digitally
signed S/MIME email messages and it did not work even for their _own employees_.
Also, in an effort to make their certs more valuable, CAs have made digitally
signed messages imply too much -- much more than they warrant or can even represent.
There are now all sorts of legal implications tied to PKI signatures, in my opinion
largely exagerated and casuistic.
If someone forges a digitally signed Citibank message, or convincingly spoofs
it, the liability might be too large to even think of it.
Using a non-signed codeword that the user has defined beforehand allows the
user to have a first proof that the message is legitimate. Since the user
chooses it, there is no privacy concern or liability for the bank. Of course,
here trust decreases with time -- a fresh codeword is more valuable. But if
the user can refresh it at will, each user will have the security that he wants.

@_date: 2005-11-29 09:51:54
@_author: Ed Gerck 
@_subject: Call for papers -- IS-TSPQ 2006 
CALL FOR PAPERS
                  First International Workshop on
   Interoperability Solutions to Trust, Security, Policies and QoS
                  for Enhanced Enterprise Systems
                           (IS-TSPQ 2006)
                           In the frame of
                 Second International Conference on
      Interoperability for Enterprise Software and Applications
                              (I-ESA)
                          Bordeaux, France
                          March 21st, 2006
                     SCOPE:
With the increasing demands from the networked economy and government,
interoperability has become a strategic factor for enterprise software
and applications. In the context of collaboration between enterprises
and their business services, several interoperability issues stem from
non-functional aspects (NFA). The non-functional aspects are introduced
to provide separation of concerns between the main functions of enterprise
software and the supporting themes that cause modification of the main
functional behaviour. Traditionally, this is applied to supporting
technology that addresses, for example, quality of service, security and
dependability, but it may also involve business value, business policies,
and trust.
The IS-TSPQ 2006 workshop objective is to explore architectures, models,
systems, and utilization for non-functional aspects, especially addressing
the new requirements on interoperability. Space is given for building
understanding of the non-functional aspects themselves and improve the
shared understanding of future solutions for non-functional aspect
The IS-TSPQ 2006 workshop is hosted by the Second International Conference
on Interoperability of Enterprise Software and Applications (I-ESA)
organized by the INTEROP NoE. The workshop aims to bring together
researchers and practitioners.
   Topics:
In keeping with the focus on interoperability and non-functional aspects,
the IS-TSPQ 2006 workshop especially encourages original unpublished papers
addressing the following areas:
     - modelling of enterprises and their collaboration;
     - interoperability architectures and models;
     - negotiation mechanisms and representations of agreements that support
       interoperability;
     - challenges from the strategic business needs;
     - alignment of business needs and computing support; and
     - linking the above to trusted, dependable infrastructure solutions.
General papers on these topics will be welcome, but it would be particularly
valuable for papers to relate to the target domains of:
     - Trust and Trust Models, Reputation, and Privacy on data integration
       and inter-enterprise computing;
     - eContracting, contract knowledge management, business commitment
       monitoring and fulfilment, and the ontologies of contracts;
     - Non-Functional Aspects, Quality of Service (QoS), Quality Attributes;
     - Information Security, Performance, Reliability and Availability;
     - Digital Rights and Policy Management, Compliance, regulatory
       environments, corporate governance, and Policy Frameworks; and
     - Business Value, Business processes, Risk Management and Asset
       Management.
   SUBMISSION GUIDELINES:
Submissions must be no longer than 12 pages in length and should follow
the guidelines given at
Authors are requested to submit their manuscripts electronically in PDF
using the paper submission tool available at the workshop web page.
The workshop proceedings will be published after the conference (and will be
sent by post to the registered participants). Papers will be included in the
proceedings only if they are presented by one of the authors at the
The final, cameraready papers are accepted by the publisher as Word files
   GENERAL INFORMATION:
For more information please visit the web site at:
       IMPORTANT DATES:
  Papers due: January 5, 2006
  Acceptance: February 1, 2006
  Papers for participant proceedings: February 23, 2006
  Workshop : March 21, 2006
  Final papers due: April 10, 2006

@_date: 2005-09-13 15:17:56
@_author: Ed Gerck 
@_subject: Another entry in the internet security hall of shame.... 
Read in an email from a website:
You'll need to send us your CC information via regular email or fax.  I
would suggest splitting up your CC info if you send it to us via email in
two separate emails for security.

@_date: 2005-09-30 13:51:12
@_author: Ed Gerck 
@_subject: announcing email-security.net 
I'd like to get list feedback on the opening discussion paper
at email-security.net, which is a technical development forum
dedicated to a fresh exploration of the Internet email security
issues of today. Comments and paper contributions on the theme
of email security are welcome. Papers will be peer-reviewed
before publication. Product and service listings are also
welcome, search-engine style (short pitch + link).
Ed Gerck

@_date: 2006-04-27 04:25:36
@_author: Ed Gerck 
@_subject: History and definition of the term 'principal'? 
tmcghan quoted:
Calling a key a "principal" (and saying that a key "speaks") is just
a poetic language used in SDSI/SPKI. The goal was to eliminate liability
by using keys as syntactic elements - a digital signature reduced to
mathematics. This did not, however, turn out to be a real-world model
because someone must have allowed the software to use that key or, at least,
turned the computer on (even if by a cron job).
Usually (but not always consistently) cryptography's use of "principal" is
not what the dictionary says.
Here, principal conveys the idea of "owning or operating".
In this sense, SDSI is somewhat right -- the private key seems to
operate the signature -- but fails to recognize that, ultimately, the key
by itself cannot operate(or own) anything.
Being responsible for an account, or creating keys or passwords, is within
the idea of "owing or operating".
Ed Gerck

@_date: 2006-08-08 14:49:21
@_author: Ed Gerck 
@_subject: [IP] more on  Can you be compelled to give a password? 
The worst-case setting for the user is likely to be when the coercer can
do all that you said and has the time/resources to do them. However, if
the distress password is strong (ie, not breakable within the time/resources
available to the coercer), the distress password can be used (for example)
to create a key that decrypts a part of the code in the binary data that
says the distress password expired at an earlier date -- whereas the access
password would create a key that decrypts another part of the code.
There are other possibilities as well. For example, if the binary data
contains code that requires connection to a server (for example, to supply
the calculation of some function), that server can prevent any further
access, even if the access password is entered, after the distress password
is given. The data becomes inaccessible even if the coercer has the binary data.
Another possibility is to combine the above with threshold cryptography.
Ed Gerck

@_date: 2006-08-08 22:10:03
@_author: Ed Gerck 
@_subject: [IP] more on Can you be compelled to give a password? 
Not likely for the same data. After all, the data is protected by
a password that "expired".
A debugger cannot decrypt without the key, which is produced only
with the access password.

@_date: 2006-02-23 13:56:47
@_author: Ed Gerck 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
This story (in addition to the daily headlines) seems to make the case that
the available techniques for secure email (hushmail, outlook/pki and pgp) do
NOT actually work.
Ed Gerck

@_date: 2006-02-23 16:31:47
@_author: Ed Gerck 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
Usability should by now be recognized as the key issue for security -
namely, if users can't use it, it doesn't actually work.
And what I heard in the story is that even savvy users such as Phil Z
(who'd have no problem with key management) don't use it often.
BTW, just to show that usability is king, could you please send me an
encrypted email -- I even let you choose any secure method that you want.
Ed Gerck

@_date: 2006-02-24 06:49:03
@_author: Ed Gerck 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
This IS one of the sticky points ;-) If postal mail would work this way,
you'd have to ask me to send you an envelope before you can send me mail.
This is counter-intuitive to users.
Your next questions could well be how do you know my key is really mine...
how do you know it was not revoked ...all of which are additional sticky points.
In the postal mail world, how'd you know the envelope is really from me or
that it is secure?
Ed Gerck

@_date: 2006-02-24 07:50:34
@_author: Ed Gerck 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
Well, the observation (as I hear the NPR piece) is that it HAS been hard
to grasp.
Further, the comparison with "looking up an address in an address book" is
also not even close to the level of hassle that users need to go through with
PGP (and PKI). Please google "Why Johnny Can't Encrypt: A Usability Evaluation
of PGP 5.0" and comments in the Usability section of
Last time I looked, a lot of PGP keys in keyservers are useless because users
(most often) simply forgot their passphrase...
Out-of-band is good. But, again, the hassle factor...
Yes, but since you don't need to ask for one... no problem. You just use your
own envelope to send postal mail to me. The PKI problem is that it runs backwards
to normal mail flow -- you need to ask me for my envelope before you can send me a
secure message. IBE doesn't have this problem, even though it has key escrow.
Ed Gerck

@_date: 2006-02-24 10:44:23
@_author: Ed Gerck 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
Missing keys that do not exist or do not work (user forgot passphrase or
revoked) are still missing keys, no? Considering how few users use PGP,
we must assume that nearly all users have no keys.
After 15 years of PGP and PKI evolution, users still say it's just not working.
The problem seems to be the methods, not the implementations. Notwithstanding
people that do "the good thing".
Perhaps I wasn't clear -- with postal mail you just write my name and address
in YOUR envelope and it gets to me. With PGP and PKI you have to ask for MY
"envelope" first; further, MY public-key creates the secure envelope that you
now need to trust with YOUR secret...
My $0.02: If we want to make email encryption viable (ie, user-level viable)
then we should make sure that people who want to read a secure communication
should NOT have to do anything before receiving it. Having to publish my key
creates sender's hassle too ...to find the key.
BTW, users should NOT be trusted to handle keys, much less to handle them
properly. This is what the users themselves are saying and exemplifying in
15 years of experiments.
Ed Gerck

@_date: 2006-02-24 11:01:44
@_author: Ed Gerck 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
Actually, when I wrote "it does not actually work" I meant all three things:
1. It can't be done as a user would like to do it; note also that even experts
do it incorrectly (it's just too many detail devils).
2. When a user does it, the user does not really know if it was done right.
3. It is too difficult for users to use and (worse) most users who use it
do it incorrectly.
We have some choices. We can continue to say that it works and just wait
for users to get educated someday. Or, we can say that there is no x (x = market,
need, risk, point) -- and that's why no user bothers with it. Or, we can try
to understand what's it that users reject and work around it. My opinion I
already out upfront: users reject the whole model; it's not "natural" to
ask me for my envelope before you can send me a letter.
(btw, name and mail address are not the envelope -- they are routing
information. My public-key is the envelope analogue.)
Ed Gerck

@_date: 2006-02-25 13:11:55
@_author: Ed Gerck 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
We all agree that having to use name and address are NOT the problem,
for email or postal mail. Both can also deliver a letter just with
the address ("CURRENT RESIDENT" junk mail, for example).
The problem is that pesky public-key. A public-key such as
[2. application/pgp-keys]...
is N O T user-friendly.
Arguments that people give each other their cell phone numbers, for example,
and even though there isn't a cell phone directory people use cell phones
well, also forget the user's point of view when comparing a phone number with
a public-key.
Finally, the properties of MY public-key will directly affect the confidentiality
properties of YOUR envelope. For example, if (on purpose or by force) my public-key
enables a covert channel (eg, weak key, key escrow, shared private key), YOUR
envelope is compromised from the start and you have no way of knowing it. This is
quite different from an address, which single purpose is to route the communication.
That's I said the postal analogue of the public-key is the envelope.
I get junk mail all the time at two different postal addresses, without ever
having published either of them. Again, addresses and names are user friendly
(for better or for worse) while public-keys are not -- in addition to their
different security roles (see above).
That's another notorious area where users can't be trusted -- and that's why
companies lock down their OSes -- or, should a company really allow each user
to choose their desired OS? Apart from compatibility issues, which also do
not allow users to  freely choose even the OS in their homes ("Junior wants
to play his games too" scenario).
Ed Gerck

@_date: 2006-02-28 17:32:05
@_author: Ed Gerck 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
I did quite not get the irony/humor. All I'm saying about an email
address is that (1) it does not work as an envelope (hiding contents); and
(2) there's no big problem in using it. You publish your email address
every time you send an email from it, which may also make it searchable.
The distinction should be obvious if you try to tell someone your public-key
over the phone, byte by byte for 1024 bits, versus telling her your
8-digit cell phone number.
Ed Gerck

@_date: 2006-01-19 17:49:39
@_author: Ed Gerck 
@_subject: surveillance, Re: long-term GPG signing key 
Thanks. Survelillance technology is now almost 6 years ahead of April, 1999,
when the cited Report to the Director General for Research of the European
Parliament was issued.
Today, surveillance is not just a political problem or a concern for
someone involved in illegal activities, or just about breaking my own
privacy. Surveillance has become an ubiquitious threat to the right to
privacy and duty of confidence to others whom I have the legal or moral
obligation to protect, dramatically increasing the probability of
disclosure by eliminating the "need to know" block usually applied to
reduce disclosure risk. Untrustworthy individuals exist and are hard to
detect in any organization, including federal and law enforcement agencies
and at any government level. The "need to know" policy, which would be
the  barrier to prevent more individuals to be exposed to the critical
information, directly reducing the probability of disclosure, is silently
destroyed by surveillance.
Thinking about IT security needs in the XXI century, the solution of using
encryption and document control to prevent surveillance and secret-disclosure
would seem to impose itself.
Despite the apparent simplicity and widespread availability of public-key
cryptography, PGP and X.509 S/MIME, less than 5% of all email is encrypted.
Banks won't even consider using encryption for sending out monthly statements
and notices. It's not just the mounting problem with email fraud schemes such
as spoofing and phishing. Banks discovered that not even their own employees
were willing to use encryption.
The real security question of the XXI century is easy-of-use -- that the
security solution will actually be used takes precedence over any potential
benefits. In this context, the subject of email security is being discussed at
 -- please take a look at the Blog and Papers sections.
Contributions are welcome. A comparison of current email technologies is
presented at Ed Gerck

@_date: 2006-07-11 02:57:51
@_author: Ed Gerck 
@_subject: Call for Papers for the 4th VirtualGoods Workshop in Leeds 
C A L L   F O R   P A P E R S
               The 4th International Workshop for
            Technology, Economy and Legal Aspects of
                         Virtual Goods
              Organized by the GI Working Group ECOM
                        and in parallel with
                      IFIP Working Group 6.11
            Communication Systems in Electronic Commerce
       December 13 -15, 2006 on AXMEDIS 2006 in Leeds, England
                                ---------------------------------
Full version:  Topics of interest include, but are not restricted to, the following aspects:

@_date: 2006-07-12 20:46:52
@_author: Ed Gerck 
@_subject: Interesting bit of a quote 
Thanks for the quote. But "That which was not recorded did
not happen" and the other two points can, and IMO should, also
be taken in the positive sense that you need recorded, credible,
audited evidence in order to support business in case arguments
(as they do) arise. Trust depends on parallel channels. So
based, trust actually reduces liability.
The knife cuts the other way too, and that's why unrevocably
expiring documents that can be so treated (legally and business
wise) is also necessary to reduce liability.
Ed Gerck

@_date: 2006-07-28 12:58:20
@_author: Ed Gerck 
@_subject: [IP] more on  Can you be compelled to give a password? 
the Subject says it all. This might be of interest
here, for comments.
The answer is definitely NO even for the naive user,
just requiring the tech-savvy for set up. Several
examples are possible.
John Smith can set two passwords, one for normal use
and the other when in distress. The distress password
may simply announce that the data is expired or, more
creatively, also make the data unreadable.
John Smith can also set two passwords, one of them
unknown to him but known to a third-party (that
John S does not have to trust) that is subject to
a different jurisdiction /or rules /or is in another
place. John Smith may comply with any demand to
disclose his password but such a demand may not be
effective for the third-party.
John Smith can have the data, encrypted with a key
controlled by his password, sitting on some Internet
server somewhere. John S never carries the data
and anyone finding the data does not know to whom it
belongs to.
John Smith can also use keys with short expiration
dates in order to circumvent by delay tactics any
demand to reveal their passwords, during which time
the password expires.
Of course, this is not really a safe heaven for
criminals because criminal activity is often detected
and evidenced by its "outside" effects, including
Ed Gerck

@_date: 2006-06-15 23:23:33
@_author: Ed Gerck 
@_subject: free e-voting software available?! 
For non-commercial use, ZMAIL offers free voting software
and service. The secure ballot may have a Release Time (cannot
be read before) and an Expiration Time (cannot be read after),
defining when voting begins and ends. Verified voter
registration is included. Candidates and voters know that the
votes will remain secret until the election closes, and can be
verified, without the need to trust a third-party or proxy service.
For small elections, the received ballots can be printed and
manually tallied, with or without identifying voter information,
as desired. The printed ballots can be audited and stored.
For large elections, or for commercial use, the entire election work
can be automated and third-party audited.
More info at:
   Election Manager and Voter registration at:
   Ed Gerck

@_date: 2006-03-15 14:52:15
@_author: Ed Gerck 
@_subject: Zfone and ZRTP :: encryption for voip protocols 
"...it achieves security without reliance on a PKI, key certification,
trust models, certificate authorities, or key management..."
Good. But, uf course, there's a trust model and you need to rely on it.
"...allows the detection of man-in-the-middle (MiTM) attacks by
displaying a short authentication string for the users to read and
compare over the phone."
Depends on the trust model. May not work.
Ed Gerck

@_date: 2006-03-24 10:11:34
@_author: Ed Gerck 
@_subject: Entropy Definition (was Re: passphrases with more than 160 bits 
Someone mentioned Physics in this discussion and this
was for me a motivation to point out something that
has been forgotten by Shannon, Kolmogorov, Chaitin
and in this thread.
Even though Shannon's data entropy formula looks like an
absolute measure (there is no reference included), the often
confusing fact is that it does depend on a reference. The
reference is the probability model that you assume to fit
the data ensemble. You can have the same data ensemble and
many different (infinite) probability models that fit that
data ensemble, each one giving you a valid but different
entropy value. For example, if a source sends the number "1"
1,000 times in a row, what would be the source's entropy?
Aram's assertion that the "sequence of bytes from 1-256" has
maximum entropy would be right if that sequence came as one of
the possible outcomes of a neutron counter with a 256-byte
register. Someone's assertion that any data has entropy X
can be countered by finding a different probability model that
also fits the data, even if the entropy is higher (!). In short,
a data entropy value involves an arbitrary constant.
The situation, which seems confusing, improves when we realize
that only differences in data entropy can be actually measured,
when the arbitrary constant can be canceled -- if we are careful.
In practice, because data security studies usually (and often
wrongly!) suppose a closed system, then, so to say automatically,
only difference states of a single system are ever considered.
Under such circumstances, the probability model is well-defined
and the arbitrary constant *always* cancel. However, data systems
are not really closed, probability models are not always ergodic
or even accurate. Therefore, due care must be exercised when
using data entropy.
I don't want to go into too much detail here, which results
will be available elsewhere, but it is useful to take a brief
look into Physics.
In Physics, Thermodynamics, entropy is a potential [1].
As is usual for a potential, only *differences* in entropy
between different states can be measured. Since the entropy
is a potential, it is associated with a *state*, not with
a process. That is, it is possible to determine the entropy
difference regardless of the actual process which the system
may have performed, even whether the process was reversible or
These are quite general properties. What I'm suggesting is
that the idea that entropy depends on a reference also applies
to data entropy, not just the entropy of a fluid, and it solves
the apparent contradictions (often somewhat acid) found in data
entropy discussions. It also explains why data entropy seems
confusing and contradictory to use. It may actually be a much
more powerful tool for data security than currently used.
Ed Gerck
[1] For example, J. Kestin, A Course in Thermodynamics, Blaisdell,

@_date: 2006-05-24 17:28:30
@_author: Ed Gerck 
@_subject: Is AES better than RC4 
Please note that my email was way different in scope. My opening
sentence, where I basically said that it does not make much sense
to compare RC4 with AES, was cut in your quote -- but here it is:
"AES has more uses and use modes than RC4, in addition to the fact that
it encrypts more than one byte at once. Having said that, it is curious
to note the following misconceptions:"
BTW, discarding the first 100's of bytes in RC4 is easy, fast, and
has nothing to with lack of "key agility". And, if you do it, you don't
even have to hash the key (ie, you must EITHER hash the key OR discard the
first bytes).
Cheers, Ed Gerck

@_date: 2006-09-28 12:34:24
@_author: Ed Gerck 
@_subject: Circle Bank plays with two-factor authentication 
Circle Bank is using a coordinate matrix to let
users pick three letters according to a grid, to be
entered together with their username and password.
The matrix is sent by email, with the user's account
sign on ID in plaintext.
Worse, the matrix is pretty useless for the majority of users,
with less usability than anything else I saw in a long time.
This is what the email says:
   The following is your Two Factor code for Online Banking for
   username (sign on ID changed here for privacy reasons).  You will be
   required to enter the grid values associated with the three
   Two Factor boxes presented with each sign-on to Online Banking.
   Please save and store this Matrix in a safe yet accessible place.
   The required entries will be different each time you sign-on.
                 Two Factor Matrix
     A    B    C    D    E    F    G    H
     _    _    _    _    _    _    _    _
1    0    8    4    2    1    1    7    5
2    7    4    9    9    2    4    2    0
3    3    6    0    6    9    9    0    6
4    6    4    5    1    4    6    8    4
5    1    7    6    8    6    5    9    2
These are the additional instructions in the site:
   Check your e-mail for receipt of the Two Factor Matrix which should
   be delivered within 2-3 minutes of activation. You can save the
   e-mail to your desktop for easy access or print the matrix.
   However, do not write your sign on ID and password on this matrix ?
   treat it securely as you do with a Debit or ATM card.
   Go back to the online banking sign on page and type in your sign
   on ID, password, and the three coordinates from your Two Factor
   Matrix. These three coordinates are randomly selected each time
   you sign on, so remember to keep your matrix secure and easily
   accessible.
Well, the bank itself already compromised both the sign on ID
and the matrix by sending them in an email. All that's left
now is a password, which a nice phishing email giving the
correct sign on ID might easily get.
When questioned about this, the bank's response is that this
scheme was designed by the people that design their web site
and had passed their auditing.
Of course, a compromise now would be entirely the user's fault

@_date: 2006-09-29 00:32:12
@_author: Ed Gerck 
@_subject: Circle Bank plays with two-factor authentication 
The first condition for security is usability. I consider this to be
Users have difficulty already with something as simple as username/pwd.
Here, the user is additionally requested to find three numbers that match
(for example) G5:H1:D3, out of 40 matrix positions in 8 columns and 5 rows.
Anyone who has played battleship knows that matrix searching takes time and
mistakes happen.
The screen is likely to time out while the user is looking for the 3 numbers,
so that the user has to start again, possibly with another new time out. The
user may also make a parallax mistake, getting a wrong number. After the user
logs in the session times out after a while, requiring the same procedure anew.
Users will have a hard time using this. But I don't think there is so
much of a need to advocate for the users here -- they will just go back
to phone service (which costs much more for the bank). Eventually, because
of cost, something with higher usability will have to be used.
The introduction of a USB interface for SecurID was caused by user rejection
of a much simpler procedure -- the user just had to read the two-factor code
off a display.
We agree they should not have included the sign on ID. It is not such a
quick fix, however, to delete it from the message because different
accounts may share the same email address and the user would not know
what matrix to use for what account. But such a simple, clear mistake is
actually a harbinger -- there are other clear mistakes there. But which
cannot be solved.
For example, the scheme (contrary to SecurID) has no protection against
an insider threat (the highest risk). The matrix combinations are fully
known in advance from the bank side (and there are only 999 of them [*]).
Further, it does not allow the usual bank security policy of separating
development (inside knowledge) from operations (the bank's servers).
Watching a couple authentication events for a user should be enough to
find which matrix the user was assigned to, allowing the next authentication
event to be fully predictable without any cooperation from or attack on the
After the severe usability burden of this scheme, one would think that
the threat model would be more robust -- to pay for your troubles.
There are, of course, also the outside threats. Contrary to what
people think, it's very common and very easy to intercept email.
ISPs can do it without trace. Companies do it all the time for
their employees. Of course, ISPs and employers already show trusted
functionality to the user but the use of insecure email here
multiplies the inside threat opportunity against the user.
There's also the question of plausible deniability. If the user's
username/pwd is compromised today, it's easy to argue it was not
safe to begin with. With this scheme, people (and the user) might
think the user is more protected -- when the user may actually
be more exposed.
Shifting the burden to the user is tempting. But, contrary to risks,
shifting the usability burden is less tolerable to users. As
technologists we cannot just do the math and say -- it works! This
was the same mistake of email encryption. That the system can actually
be used turns out to be more important than any security promise.
Ed Gerck
(*) Apparently, at most. Their 3-digit matrix counter, also included
in the message (!), can index at most 999 pages.

@_date: 2007-08-08 14:41:52
@_author: Ed Gerck 
@_subject: unintended consequences? 
No change, notwithstanding anecdotal references on fiber bending
as used for tapping.
Tapping a fiber can be done without much notice by matching the
index of refraction outside the outer fiber layer, after abrasion
and etching to reach that layer. There is no need for bending,
which might not be physically possible (eg, in a thick cable bundle),
would increase propagation losses beyond that caused by the tapped
signal power itself, and might create detectable backward
propagating waves (BPWs are monitored to detect fiber breach).
Low-loss taps are essential. A tap must extract a portion of
the through-signal. This, however, should not have the effect of
significantly reducing the level of the remaining signal. For
example, if one-quarter of the incident signal is extracted, then
there is a 1.25 db loss in the remaining through-signal, which
can easily be detected.
Ed Gerck

@_date: 2007-08-15 18:32:10
@_author: Ed Gerck 
@_subject: New DoD encryption mandate 
The first is simply a MSFT Vista requirement for BitLocker file
encryption. The second is for example present in ACER laptops
(Aspire 5920) as eLock -- it allows you to protect and then
unlock storage devices that can be mounted as a file system when
plugged into the trusted system (the laptop), or keep them locked

@_date: 2007-08-16 15:38:27
@_author: Ed Gerck 
@_subject: Skype new IT protection measure 
We've heard it so many times: "There's nothing to worry about."
Now, Skype adds a new IT protection measure -- "love":
  "The Skype system has not crashed or been victim of a cyber
  attack. We love our customers too much to let that happen."
Of course, these two phrases are a non sequitur. No amount of
a company's "love" for customers will prevent their IT system from
crashing or hackers from attacking and being successful. At the
very least, Skype is making users uneasy with such statements.
What's happening, and that's why Skype wrote about "love", is
that Skype users worldwide cannot call or hear voicemail for
many hours now.
The visible error is that users cannot login -- hence can't call,
etc. While this could understandable, what is not understandable
is Skype's love declaration.
BTW, one may wonder what is really happening. Any other reports?
Ed Gerck

@_date: 2007-08-19 12:10:39
@_author: Ed Gerck 
@_subject: a new way to build quantum computers? 
It's a mater of (lack of) journalism English. The first paragraph phrase:
    "The new technique can crack even complex codes in a matter
    of seconds."
should have been written as:
    "The new technique may crack even complex codes in a matter
    of seconds."
The scientific authors, I believe, were more careful. Their technique
still has all the basic problems of QC built in.

@_date: 2007-12-07 10:15:50
@_author: Ed Gerck 
@_subject: Flaws in OpenSSL FIPS Object Module 
Peter cites an important difference. You may be able to see but you can't tell.
However, one can still easily reverse-engineer to find the vulnerability
and then present an exploit saying "There's something we noticed here when
the code is executed with this input...".
The conclusion holds that closed-source is now less of a reasonable argument
in terms of /protecting/ source code.
Software-as-a-Service (SaaS), though, would still work in terms of
protecting source code, though, as all you have is a "service oracle" that
does not necessarily reveal code details or flaws. SaaS could be supplied
remotely or locally, with a secure processor card or secure USB-processor.
Ed Gerck

@_date: 2007-12-10 11:56:40
@_author: Ed Gerck 
@_subject: Flaws in OpenSSL FIPS Object Module 
Enter Reality 2.0. Yesterday, security was based on authority --
on some particular agency or expert. Today, security is /also/ based
on anyone else that can point out non-compliance, and solutions.
The integrity of the FIPS program, and any other evaluation process,
can only increase when [x] are also able (entirely on their own and
not by a mandate) to point out non-compliance of evaluated products

@_date: 2007-12-11 11:28:03
@_author: Ed Gerck 
@_subject: PlayStation 3 predicts next US president 
Thanks, Allen. Interestingly, digital signatures do provide what
notaries can't provide in this case. Even though a digital signature
binds a document to a key, there are known legal frameworks that can
be used to bind the key to a person.
Ed Gerck

@_date: 2007-12-26 09:57:52
@_author: Ed Gerck 
@_subject: 2008: The year of hack the vote? 
The answer is NO, and that is so because it's different.
In elections, you must have a "Chinese wall" between the voter and the ballot. If I get the vote I don't know who the voter is, if I get the voter I don't know what the vote is. And that doesn't happen in e-commerce. In e-commerce I have a traceable credit card. I have a traceable name, I have an address for delivery. Anything that's bought must be delivered. I have a pattern of buying, if you go to Amazon.com, they will suggest the next book to you if you want, based on what you bought. They may know a lot more about you than you think they know.
And so there is a basic difference between e-commerce and Internet voting, which must not be ignored, otherwise ignorance is bliss, we don't see it.
In e-commerce there must be no privacy, the merchant must know who I am, my credit card must be valid. There are laws against [fraud in] this. So there is a basic divide here, which you need to take into account. There is a paradigm shift, there is a very strong technological point which those on the political side don't see, because that's natural. And there is a very strong political side that us, on the technological side don't see. For us, yes, voter participation is very good, or don't we all care if voter participation may decrease?
So the point that I wanted to make is that it [Internet voting] is not as easy [as in e-commerce], because it's a fundamentally different problem. The solution is not the same, what we have today [for e-commerce] does not transpose, and the solution, the final comment, the solution that we have today for e-commerce is not cryptography, is insurance, for 20 percent of fraud that is the Internet fraud in credit cards. And how is that paid? By us, cardholders, we socialize the cost. Imagine telling, yes, you were elected president, but you know, there was a fraud, here is our insurance policy. You collect your million dollars, next time play again. You know, we cannot socialize fraud in elections. We cannot accept 20 percent of fraud paid for by insurance, which is what happens today. We did solve the e-commerce security problem, by putting in insurance. We can not solve it that way [for elections].
(from my Brookings Symposium comment, Washington, DC, January 2000).
Ed Gerck

@_date: 2007-02-02 12:11:31
@_author: Ed Gerck 
@_subject: convenience vs risk -- US public elections by email and beyond 
The social aspects of ease-of-use versus security are well-known.
People would rather use something that works than something that
is secure but hard to use. Ease-of-use trumps risks.
What is less recognized, even though it seems intuitive, is that
convenience (even though costlier and harder to use) can also make
people ignore risks. Convenience trumps ease-of-use, which trumps
For example, people will often send a cell phone text message
that requires dozens of button-clicks, costs money and is less
secure (US Rep. Mark Foley case)... than do a one click, free
phone call. We all use regular email even though it is totally
insecure -- because it's convenient.
Convenience has a lot to do with "personal comfort". It is often
more comfortable to send a text message or email than call and
actually speak with the person.
That you can do it on your own time, or save time, is a very
important component for personal comfort. A convenience store,
for example, sells items that saves the consumer a stop or
separate trip to the grocery store.
What happens when convenience is ignored? If convenient ways are
not available?
Let me note that opposition to any type of e-voting has led to
public elections in the US being carried out via regular email
in 2006.
It may be hard to imagine why opposition to e-voting would in any
way make adoption of email voting more likely.
It happens because voting is useful and voters want to vote.
Therefore, voters will find ways that are not safe but convenient
and available ...if more convenient and safe ways are blocked.
We already discovered that for the system to be usable is more
important than any security promises that might be made. Security
innovation has often improved usability -- for example, even though
public-key cryptography is hard to use by end-users, it represented
a major usability improvement for IT administrators. Usable
security is a major area of innovation today.
We are discovering that convenience is an even stronger force to
bring about innovation.
How about paper voting? It does not prevent large-scale fraud, which
has been a complement to paper elections for over a century, and is
not convenient. Lacks personal comfort, personal use of time. Lack
of convenience (not lack of security) will, eventually, kill paper
Regarding voting, our future is pretty obvious. Online voting
will be mainstream, and is already here in the public and private
sectors. But, to be secure, it should not happen with regular
email, e-commerce web sites, or current "trust me" e-voting machines
The socially responsible thing to do regarding voting is, thus, to
develop online voting so that it is secure _and_ easy to use. It
already has the top quality that paper voting and e-voting machines
(DRE) cannot have: convenience.
But the real-world voting security problem is very hard. Voting is an
open-loop process with an intrinsic "vote gap", such that no one may
know for sure what the vote cast actually was -- unless one is willing
to  sacrifice the privacy of the vote.
A solution [1], however, exists, where one can fully preserve privacy
and security, if a small (as small as you need) margin of error is
accepted. Because the margin of error can be made as small as
one needs and is willing to pay, it is not really relevant. Even when
all operational procedures and flaws including fraud and bugs are
taken into account.
The solution is technologically neutral but has more chances for
success, and less cost, with online voting. Which just adds to the
winning hand for online voting, led by convenience.
I would like to invite your comments on this, to help build the trust
and integrity that our election system needs -- together with the
convenience that voters want. Personal replies are welcome. I am
thinking of opening a blog for such dialogue. Moderators are welcome
Ed Gerck
[1] Based on a general, information-theory model of voting that applies
to any technology, first presented in 2001. See
Provides any desired number of independent records, which are readily
available to be reviewed by observers, without ever linking voters to

@_date: 2007-02-05 14:05:13
@_author: Ed Gerck 
@_subject: Intuitive cryptography that's also practical and secure. 
It is possible and has been done by Safevote, the first time in 2001.
The solution also prevents vote selling. The solution was verified and
approved by the Swedish Ministry of Justice.
This is how it works. Voters are allowed to cast as many ballots as
desired but only the last ballot is counted (this is called the CL product
option). If anyone forces or rewards the voter for voting in a certain way,
and even watches the voter vote, the voter may always vote again afterwards
and effectively erase the former vote when in privacy. The coercer would have
to follow the voter 24/7 to prevent this.
There is a second method, also used by Safevote in 2001 and positively
evaluated by the Swedish Ministry of Justice. Voters can use the
Internet to vote but also in a supervised environment, a precinct, where the
voter is alone to vote. The vote cast at the precinct trumps the vote
cast elsewhere, which allows the voter an easy recourse in case of
difficulty (spouse, etc.).
This is often ignored by opponents of online voting, that online voting
does not eliminate precinct voting; it just allows it to be sent online
as well in a controlled environment. This also means that no one
needs to buy a computer or have Internet connection to vote -- there's
no "digital divide". People can continue to use the precinct and vote
as usual.
About the screen picture issue, Safevote allows voters to print all
pages of the ballot, and all ballot choices made by the voter. However,
the server provides the ballot pages in such a way that the voter cannot
prove (except to himself when voting) how the voter actually voted. This
procedure also helps prevent vote selling and coercion. The voter cannot
produce a non-repudiable proof of how the voter voted.
Ed Gerck

@_date: 2007-02-07 08:46:32
@_author: Ed Gerck 
@_subject: convenience vs risk -- US public elections by email and beyond 
Thanks for all the comments in and off list. A revised write-up is
available at More examples where convenience trumps ease-of-use, and risk, will be added
from time to time. Please check back. Comments and suggestions are welcome.
Ed Gerck

@_date: 2007-02-13 16:21:50
@_author: Ed Gerck 
@_subject: Failure of PKI in messaging 
The solution is simpler than it seems.
Let's first look at one scenario that is already working
and use it as an example to show how the email scenario
may work.
Banks are already, and securely, sending and receiving
online messages to/from their clients. This is done by
a web interface, after the user logs in to their account.
Web user login can be based on a number of two-factor and
mutualauthentication solutions, some of them quite ineffective
to prevent phishing but, nonetheless, better than what the
email PKI model provides.
What's missing with the email PKI model?
While the bank is asking to be authenticated by the user, it
does so by asking the user to rely on a number of third-party
references that are actually unreliable (ie, by being without
recourse, warrantless, unverifiable, and chosen by the purported
sender in what may be a con game). The bank would never allow
the user to be authenticated under the same assumptions!
So, what's missing in the email PKI model is two-sidedness.
It is essential to have a reference point that the user trusts.
In the web messaging example already used by banks, this is
provided by the user login -- the user trusts that that is their
account -- their name is correct, their balance and transactions
are correct.
I am using this insight in a secure email solution that provides
just that -- a reference point that the user trusts, both sending
and receiving email. Without such reference point, the user can
easily fall prey to con games. Trust begins as "self-trust". Anyone
interested in trying it out, please send me a personal email with
application info.
Ed Gerck

@_date: 2007-02-13 22:10:09
@_author: Ed Gerck 
@_subject: Failure of PKI in messaging 
That's not banking. Banks and their clients already have a trusted
relationship. The banks webmail interface leverages this to provide
a trust reference that the user can easily verify (yes, this is my
name and balance). That's why it works, and that's what is missing
in the bank PKI email model -- what's that relationship buying you?
Email for banks should thus leverage the relationship, rather than
present an ab initio communication.
It is not true that you can't secure first communications. It is just
harder and _not_ necessary for banks (because the client already knows
the bank and vice versa).
Ed Gerck

@_date: 2007-02-15 14:47:05
@_author: Ed Gerck 
@_subject: BETA solution, Re: Failure of PKI in messaging 
The application info is just so I can verify your requirements.
The solution is in BETA and does not use Java, Flash, stored cookies,
or ActiveX. Works in Linux, Mac, and Win. There's also a javascript-
free version (earlier BETA).
The solution is available free (for personal use)
at Summary is available at  and how it works at
The question is: Why should I trust it?
Zmail actually reduces the amount of trust by not storing your usercode,
password, or keys anywhere. This makes sense for zmail, and is an incentive
to actually do it, to reduce risk -- anyone breaking into any zmail server,
even physically, will not find any key or credential material for any user
and, hence, cannot decrypt any user area (the user area keeps the address book
and contact keys, all encrypted using the user keys that are not there), or
user messages collected from ISPs.
This is more than X.509 or PGP can do, as the private-key must be exposed
Next, let's see what zmail does. It creates a point-to-point encrypted
channel, with authentication, delivery and control mechanisms that you define.
It's a secure routing/delivery system, working as an add-on interface (so it
does not change how you use email).
The message itself could be encrypted by you and just delivered by zmail

@_date: 2007-02-16 14:59:31
@_author: Ed Gerck 
@_subject: BETA solution, Re: Failure of PKI in messaging 
N O W H E R E, as it says above.
There's no need to replace PGP or S/MIME. After all, less than 5% of all email
is encrypted using them. What's needed is to offer an option for the other
95% that could be encrypted and authenticated.
Because you have to trust zmail less (the two quotes above), and also because
you have to trust the recipient less (the return receipt, for example). In
addition, you have to trust your platform less (no private-key that is stored
in your computer; ZSentryID can be used to render key-logging ineffective).
In short, the less you have trust everyone (including your own computer),
the better the trust model is -- what you trust is what can break your
security, when it fails.
Ed Gerck

@_date: 2007-01-26 17:54:52
@_author: Ed Gerck 
@_subject: Intuitive cryptography that's also practical and secure. 
The first problem of voting is that neither side (paper vote vs e-vote)
accepts that voting is hard to do right -- and that we have not done
it yet. Paper is not the "gold standard" of voting.
The real-world voting problem is actually much harder than people think.
Voting is an open-loop process with an intrinsic "vote gap", such that
no one may know for sure what the vote cast actually was -- unless one
is willing to sacrifice the privacy of the vote. This problem is
A solution [1], however, exists, where one can fully preserve privacy
and security, if a small (as small as you need) margin of error is
accepted. Because the margin of error can be made as small as
one needs and is willing to pay, it is not really relevant. Even when
all operational procedures and flaws including fraud and bugs are
taken into account.
The solution seems fairly intuitive. In fact, it was used about 500
years by the Mogul in India to prevent fraud.
The solution is also technologically neutral, but has more chances for
success, and less cost, with e-voting.
Ed Gerck
[1] In Shannon's cryptography terms, the solution reduces the probability
of existence of a covert channel to a value as close to zero as we want.
This is done by adding different channels of information, as intentional
redundancy. See I can provide more details on the fraud model, in case of interest.

@_date: 2007-01-27 11:46:17
@_author: Ed Gerck 
@_subject: Intuitive cryptography that's also practical and secure. 
[Perry, please use this one if possible]
You mentioned in your blog about the crypto solutions for voting and
that they have been largely ignored. The reason is that they are either
solutions to artificially contrived situations that would be impractical
in real life, or postulate conditions such as threshold trust to protect
voter privacy that would not work in real life. Technology-oriented
colleagues are not even aware why threshold trust would not work in
Thus, the first problem of voting is that neither side (paper vote vs
e-vote accepts that voting is hard to do right -- and that we have not
done it yet.
The real-world voting problem is actually much harder than people think.
Voting is an open-loop process with an intrinsic "vote gap", such that
no one may know for sure what the vote cast actually was -- unless one
is willing to sacrifice the privacy of the vote. This problem is
A solution [1], however, exists, where one can fully preserve privacy
and security, if a small (as small as you need) margin of error is
accepted. Because the margin of error can be made as small as
one needs and is willing to pay, it is not really relevant. Even when
all operational procedures and flaws including fraud and bugs are
taken into account.
The solution seems fairly intuitive. In fact, it was used about 500
years by the Mogul in India to prevent fraud.
The solution is also technologically neutral, but has more chances for
success, and less cost, with e-voting.
Ed Gerck
[1] In Shannon's cryptography terms, the solution reduces the probability
of existence of a covert channel to a value as close to zero as we want.
The covert channel is composed of several MITM channels between the voter
registration, the voter, the ballot box, and the tally accumulator. This
is done by adding different channels of information, as intentional
redundancy. See I can provide more details on the fraud model, for those who are

@_date: 2007-07-02 11:36:33
@_author: Ed Gerck 
@_subject: a fraud is a sale, Re: The bank fraud blame game 
Yes. Today, under current practice, there's actually a strong
incentive to keep existing fraud levels than to try to "scrub
it out" -- fraud has become a sale:
   in 2001, the last year enough HARD data was available,
   their revenue stream from fraud was USD $550 Million.
   That all came from chargeback fees against the merchants.
   And since it was fraud, the merchants lost the product
   and the income from the product along with the shipping
   costs and the chargeback fees. Merchants, of course, have
   no choice but to pass those losses on to the honest customers.
in See also Ed Gerck

@_date: 2007-07-14 11:43:53
@_author: Ed Gerck 
@_subject: improving ssh 
SSH (OpenSSH) is routinely used in secure access for remote server
maintenance. However, as I see it, SSH has a number of security issues
that have not been addressed (as far I know), which create unnecessary
Some issues could be minimized by turning off password authentication,
which is not practical in many cases. Other issues can be addressed by
additional means, for example:
1. firewall port-knocking to block scanning and attacks
2. firewall logging and IP disabling for repeated attacks (prevent DoS,
block dictionary attacks)
3. pre- and post-filtering to prevent SSH from advertising itself and
server OS
4. block empty authentication requests
5. block sending host key fingerprint for invalid or no username
6. drop SSH reply (send no response) for invalid or no username
I believe it would be better to solve them in SSH itself, as one would
not have to change the environment in order to further secure SSH.
Changing firewall rules, for example, is not always portable and may
have unintended consequences.
So, I'd like to get list input (also by personal email if you think your
comment might be out of scope here),  on issues  above and if you have other SSH security issues that you would like to see solved /in SSH/.
Ed Gerck

@_date: 2007-07-16 20:41:44
@_author: Ed Gerck 
@_subject: improving ssh 
Perhaps not the way they are solved today (see above), and that IS
the problem. For example, the lack of good crypto solutions to protocol
bootstrap contributes significantly to security holes 1-7.
Ed Gerck

@_date: 2007-07-19 17:23:44
@_author: Ed Gerck 
@_subject: summary, Re: improving ssh 
Thanks everyone for the feedback. There are now some
ideas how things could be improved using crypto. I prepared a summary of the public and private responses, and clarifications, at:
Comments are welcome in here (if crypto) an in the blog in
Ed Gerck

@_date: 2007-06-21 12:05:50
@_author: Ed Gerck 
@_subject: question re practical use of secret sharing 
Why? A copyright-protected work is still copyright-protected,
encrypted or not.
It is just as with any reversible encoding of a copyright-
protected work, such as magnetic domain encoding when storing it
in a hard disk.
Now, if you pass a copyright-protected work through an irreversible
hash function, it would be hard to claim the result to be
Ed Gerck

@_date: 2007-06-30 12:04:07
@_author: Ed Gerck 
@_subject: Quantum Cryptography 
============================== START ==============================
As a physicist, with a doctorate in quantum optics, I want to
add my agreement to Steve's comment. And extend his comment to
note that quantum cryptography (QC) is much more than QKD and even
more than the qbit theories used today to represent information
in terms of entangled states.
The model of information in QC will certainly evolve. Today,
the rather naive security assumptions in QC (and QKD based on
QC) might just reflect equally naive security assumptions
found in today's conventional cryptography. [1]
I would suggest QC as a very fruitful area of research, and one
that can add much insight back into conventional cryptography.
Ed Gerck
[1] For example, the rather common idea that risk can be defined
independent of trust or even, that the IT concept of trust is just
some kind of authorization. Yes, it is true that in a closed network
a trusted user can be described as a user that is authorized by Z to
do X within Y, but in an open network there is really no "Z" to
authorize anything. It is not that trust evaporates in an open
network -- naive representations just can't describe it. Or, the
equally embarrassing question of what happens when you connect
two trusted systems. Are they, together, more trusted, less
trusted, or equally trusted? Again, naive representations of trust
just can't answer this. No wonder that QC has problems there as

@_date: 2007-10-26 09:14:37
@_author: Ed Gerck 
@_subject: Password vs data entropy 
Eliminating all other variables, such as the hash algorithm used
to derive a key from  the password (see previous thread) and
the computational load of the decryption effort per trial, one
might think that the entropy of the password alone would
determine the attack effort needed to decrypt the value
being protected (the payload).
However, the real question to ask is not the password's
(Shannon) entropy but the workload necessary to find the
To give a few examples:
- if the value being protected can be efficiently
tested (eg, by using it as a key to decrypt a short English
text), then the workload necessary to find the password will
be reduced even if the password entropy is high.
- the workload necessary to find the password may be actually
trivial if the password is generated with a non-flat distribution
(even though the entropy of the distribution may be quite high,
such as 128 bit).
- if salt is not used, or used poorly, a dictionary attack
can be quite effective to reduce the workload.
What matters here is the expected cost of password search,
not the password or payload Shannon entropy. For some pointers
on this discussion, and why high Shannon entropy does not
mean high workload, see
Ed Gerck

@_date: 2008-04-07 08:53:44
@_author: Ed Gerck 
@_subject: Still locked up Shannon crypto work? 
"Consider Shannon. He didn?t do just information theory. Several
years before, he did some other good things and some which are still
locked up in the security of cryptography."
Shannon's crypto work that is "still [1986] locked up"? This was
said (*) by Richard W. Hamming on March 7, 1986. Hamming,
who died when he was almost 83 years old in 1998, was then a
Professor at the Naval Postgraduate School in Monterey, California.
He was also a retired Bell Labs scientist.
Does anyone about this or what it could be? Or if Hamming was
(*) (BTW, this was a great talk!)
Ed Gerck

@_date: 2008-04-16 08:55:00
@_author: Ed Gerck 
@_subject: 2factor 
He's likely called Paul McGough, of Washington, DC, and ignores that SSL prevents MITM. It gets worse after this.

@_date: 2008-04-28 11:04:34
@_author: Ed Gerck 
@_subject: "Designing and implementing malicious hardware" 
Each chip does not have to be 100% independent, and does not have to be used 100% of the time.
Assuming a random selection of both outputs and chips for testing, and a finite set of possible outputs, it is possible to calculate what sampling ratio would provide an adequate confidence level -- a good guess is 5% sampling.
This should not create a significant impact on average speed, as 95% of the time the untested samples would not have to wait for verification (from the slower chips). One could also trust-certify each chip based on its positive, long term performance -- which could allow that chip to run with much less sampling, or none at all.
In general, this approach is based on the properties of trust when viewed in terms of Shannon's IT method, as explained in [*]. Trust is seen not as a subjective property, but as something that can be communicated and measured. One of the resulting rules is that trust cannot be communicated by self-assertions (ie, asking the same chip) [**]. Trust can be positive (what we call trust), negative (distrust), and zero (atrust -- there is no trust value associated with the information, neither trust nor distrust). More in [*].
Ed Gerck
  References:
[*] [**] Ken's paper title (op. cit.) is, thus, identified to be part of the very con game described in the paper.

@_date: 2008-04-28 12:58:23
@_author: Ed Gerck 
@_subject: "Designing and implementing malicious hardware" 
Provided you have access to enough chip diversity so as to build a correction channel with sufficient capacity, Shannon's Tenth Theorem assures you that it is possible to reduce the effect of bad chips on the output to an error rate /as close to zero/ as you desire. There is no lower, limiting value but zero.
Statistical independence is not required to be 100%. Events are not required to be randomly flat either. Sampling is required to  be independent, but also not 100%.
The counter-point is that the existence of a violation can be tested within a desired confidence level, which confidence level is dynamic.
The more comparison channels you have, and the more independent they are, the harder it is to compromise them /at the same time/.
In regard to time, one strategy is indeed to watch 100% of the time but for random windows of certain lengths and intervals. The duty ratio for a certain desired detection threshold depends on the correction channel total capacity, the signal dynamics, and some other variables. Different implementations will allow for different duty ratios for the same error detection capability.
Except as above; using a correction channel with enough capacity the problem can /always/ be solved (ie, with an error rate as close to zero as desired).
As above, the problem is solvable (existence proof provided by Shannon's Tenth Theorem).  It is not a matter of whether it works -- the solution exists; it's a matter of implementation.
Ed Gerck

@_date: 2008-04-28 13:21:06
@_author: Ed Gerck 
@_subject: "Designing and implementing malicious hardware" 
Yet, Shannons' tenth theorem can be proven without a hypothesis that noise is random, or that the signal is anything in particular.
Using intuition, because no formality is really needed, just consider that the noise is a well-defined sinus function. The error-correcting channel provides the same sinus function in counter phase. You will see that the less random the noise is, the easier it gets. Not the other around.
How about an active adversary? You just need to consider the adversary's reaction time and make sure that the error-correcting channel has enough capacity to counter-react within that reaction time. For chip fabrication, this may be quite long.
Ed Gerck

@_date: 2008-01-22 10:38:24
@_author: Ed Gerck 
@_subject: SSL/TLS and port 587 
I would like to address and request comments on the use of SSL/TLS and port 587 for email security.
The often expressed idea that SSL/TLS and port 587 are somehow able to prevent warrantless wiretapping and so on, or protect any private communications, is IMO simply not supported by facts.
Warrantless wiretapping and so on, and private communications eavesdropping are done more efficiently and covertly directly at the ISPs (hence the name "warrantless wiretapping"), where SSL/TLS protection does NOT apply. There is a security gap at every negotiated SSL/TLS session.
It is misleading to claim that port 587 solves the security problem of email eavesdropping, and gives people a false sense of security. It is worse than using a 56-bit DES key -- the email is in plaintext where it is most vulnerable.
Ed Gerck

@_date: 2008-01-22 21:49:32
@_author: Ed Gerck 
@_subject: SSL/TLS and port 587 
It is common with those who think that the threat model is
"traversing the public Internet". As I commented in the
second paragraph, an attack at the ISP (where SSL/TLS is
of no help) has been the dominant threat -- and that is
why one of the main problems is called "warrantless
wiretapping". Further, because US law does /not/ protect
data at rest, anyone claiming "authorized process" (which
the ISP itself may) can eavesdrop without any required
For examples on claiming that SSL/TLS can protect email
privacy, see the commercial email security product by
 (now with google):
"Postini?s Encryption Manager Policy-Enforced TLS has successfully
met SEI?s email security needs, protecting communications where they
are most vulnerable ? traversing the public Internet. [sic]".
in In another page at postini.com, we can read: "With TLS,
we will be able to securely send and receive confidential
documents with our clients who support TLS." While this
part is 100% correct, it is not relevant for the security
of those documents, as they sit in plaintext at the ISPs.
Also, in the current thread on Comcast blocking port 25 at Farber's
IP list, and in previous threads here, using TLS/SSL has been promoted
to help "cease to become low hanging fruit for reading or public
dissemination", and to prevent a "private contractor's (ISP) misuse or
loss/exposure of your data". However, having a port 587 TLS connection
to my ISP (eg, gmail) is not going to make my email more or less
protected at that ISP, and is not going to prevent wiretapping.
Of course, SSL/TLS is very successful in e-commerce. But SSL/TLS is
not an email authentication and encryption solution, and fails for
email where the risk is higher.
Ed Gerck

@_date: 2008-01-23 06:27:39
@_author: Ed Gerck 
@_subject: SSL/TLS and port 587 
Yes. Caveats apply but SSL/TLS is useful and simple for this purpose.
The problem is when it is generalized from the particular case where
it helps (above) to general use, and as a solution to prevent wireless
wiretapping. For example, as in this comment from a data center/network
Now, personally, with all the publicly available info regarding
warrantless wiretapping and so on, why any private communications should
be "in the clear" I just don't know. Even my MTA offers up SSL or TLS to
other MTA's when advertising its capabilities. The RFC is there, use it
as they say.
Ed Gerck

@_date: 2008-01-23 08:10:01
@_author: Ed Gerck 
@_subject: SSL/TLS and port 587 
As you wrote in your blog, "users really need to read those boring
[ISP] licenses carefully."
ISP service terms grant the disclosure right on the basis of
something broadly called "valid legal process" or any such
term as defined /by the ISP/. Management access to the account
(including email data) is a valid legal process (authorized by the
service terms as a private contract) that can be used without
any required formality, for example to verify compliance to the
service terms or something else [1].
Frequently, "common sense" and "standard use" are used to
justify such access but, technically, no justification is
actually needed.
Further, when an ISP such as google says "Google does not share
or reveal email content or personal information with third
parties." one usually forgets that (1) third parties may actually
mean everyone on the planet but you; (2) third parties also
have third parties; and (3)  is recursive.
Mr. Councilman's case and his lawyer's declaration that "Congress
recognized that any time you store communication, there is an
inherent loss of privacy" was not in your blog, though. Did I
miss something?
Ed Gerck
[1] in  :
Of course, the law and common sense dictate some exceptions. These exceptions include requests by users that Google's support staff access their email messages in order to diagnose problems; when Google is required by law to do so; and when we are compelled to disclose personal information because we reasonably believe it's necessary in order to protect the rights, property or safety of Google, its users and the public. For full details, please refer to the "When we may disclose your personal information" section of our privacy policy. These exceptions are standard across the industry and are necessary for email providers to assist their users and to meet legal requirements.

@_date: 2008-01-23 09:39:33
@_author: Ed Gerck 
@_subject: SSL/TLS and port 587 
First, there is no confusion here; I was simply addressing both
issues as in my original question to the list:
   The often expressed idea that SSL/TLS and port 587 are
   somehow able to prevent warrantless wiretapping and so on, or
   protect any private communications, is IMO simply not
   supported by facts.
Second, those two issues are not as orthogonal as one might
think. After all, an ISP is already collaborating in the
case of a warrantless wiretap. So, where would the tap
take place:
1. where the email is encrypted, or
2. where the email is not encrypted.
Considering the objective of the tap, and the expenses incurred
to do it, it seems quite improbable to choose Thanks for Mr. Councilman's case update. I mentioned it only
because it shows what does happen and the economic motivations
for it, none of which could have been prevented by SSL/TLS
protecting email submission.
Ed Gerck

@_date: 2008-07-01 10:28:43
@_author: Ed Gerck 
@_subject: The wisdom of the ill informed 
[Moderator's note: I'll let Ed have the last word. I'm sure everyone
knows what I'd say anyway. --Perry]
Since you are not fully aware how Wells Fargo operates, let me clarify. What you say below is true for users entering the system /today/:
No. Any Wells Fargo user today that has an /older/ account (eg, opened in 2001), can login with their numeric PINs if that is how their online access was done then and they did not change it.
So, even though WF /today/ does not accept /new/ users to use only numbers for their password, WF is happy to continue to accept /older/ rules, including accepting the PIN for online account login.
Their website today is what they use today. Older account users that have not changed their login can still use their PINs for login. I know one company that used way back when their numeric PIN for login, because that's what WF told them to do, and that just very recently changed to a safer password.
While it is good that WF has improved its rules, it would better if they had made it compulsory for all users (not just newer) to renew their passwords when the rules started prohibiting using only numbers and /not/ requiring the PIN for first login.
I imagine that there are lots of sites out there that have likewise improved their front-end password acceptance rules but have not bothered to ask all their users to renew their passwords, and thus force compliance with newer, safer rules.
WF does that, still today, for their most valued customers -- their older customers. May our words be a good warning for them!
When you do the math, you will see that knowing a few hundred invalid accounts will not considerably reduce your search space for the comparison we are talking about. Remember, we are talking about 4-digit PINs that have a search space of 9,000 choices (before you complain about the count, note that all 0xxx combinations are usually not accepted as a valid PIN for registration) versus an account number that is a sparse space with 12-digits and that (by the sheer number of valid users) must have at least /millions/ of valid accounts.
Please check with actual banks. Bank users logging in from a static IP account are treated differently by the servers than users from a dynamic IP account. As they should.
The dialogue disconnect here is classical in cryptography, as we all have probably seen in practice. In the extreme, but not too uncommon position, a crypto guy cries for a "better" solution (which, more often than not, is either not usable or too expensive) while dismissing a number of perfectly valid but incomplete solutions that, when used together, could mount a good-enough (and affordable) defense. Many people have frequently made this point here, including yourself with EV certs.
Yes, blocking by IP is not a panacea, and may fail to block, but when it works it is mostly correct (and if it's not, it errs on the side of caution). It should certainly be in everyone's toolbox.
But blocking-by-IP is just one possible pattern, as I comment below:
Not everyone has 4-digit PINs (9,000 choices, not 10,000) and, as banks update their practices (but not Wells Fargo!) the search space increases and the problem goes away.
Nonetheless, if a system such as reported by Allan in this thread, uses 6-digit passwords to protect an email message, this does NOT mean that someone could break it in seconds. It all depends on the control system (and its effectiveness) to prevent many multiple tries in a short time.
It's not just the search space that counts but how fast you can search it. Why can EC use shorter keys than PKC, for the same level of security against brute search?
I understand you simply jumped to conclusions here and before. My citation on Wells Fargo was and is (today) correct. My reply to Dan was also a valid (even if not perfect!) method, that can be used in combination with other methods as I suggested.
As Father Gracian suggests in his book (recommend reading it) "The Art of Worldly Wisdom", never complain. So, I won't. Anyone who is using a public dialogue to mine the gold of truth can very well understand that a few stones that also come through will just make the gold even more valuable.
Best regards,
Ed Gerck

@_date: 2008-06-02 09:37:00
@_author: Ed Gerck 
@_subject: Can we copy trust? 
In the essay "Better Than Free", Kevin Kelly debates which concepts hold value online, and how to monetize those values. See Kelly's point can be very useful: *When copies are free, you need to sell things which can not be copied.*
The problem that I see and present to this list is when he discusses qualities that can't be copied and considers "trust" as something that cannot be copied.
Well, in the digital economy we had to learn how to copy trust and we did. For example, SSL would not work if trust could not be copied.
How do we copy trust? By recognizing that because trust cannot be communicated by self-assertions (*), trust cannot be copied by self-assertions either.
To trust something, you need to receive information from sources OTHER than the source you want to trust, and from as many other sources as necessary according to the extent of the trust you want. With more trust extent, you are more likely to need more independent sources of To copy trust, all you do is copy the information from those channels in a verifiable way and add that to the original channel information. We do this all the time in scientific work: we provide our findings, we provide the way to reproduce the findings, and we provide the published references that anyone can verify.
To copy trust in the digital economy, we provide  digital signatures from one or more third-parties that most people will trust.
This is how SSL works. The site provides a digital certificate signed by a CA that most browsers trust, providing an independent channel to verify that the web address is correct -- in addition to what the browser's location line says.
Ed Gerck
(*) "Trust as qualified reliance on information" in  and
Digital Certificates: Applied Internet Security by J. Feghhi, J. Feghhi and P. Williams, Addison-Wesley, ISBN 0-20-130980-7, 1998.

@_date: 2008-06-02 12:29:53
@_author: Ed Gerck 
@_subject: Can we copy trust? 
A copy is something identical. So, in fact you can copy that server cert to another server that has the same domain (load balancing), and it will work. Web admins do it all the time. The user will not notice any difference in how the SSL will work.
Another point: When we talk about a copy, we're technically talking about a transmission. To copy a web page to your hard disk is to transmit bits from the web server to your disk. To say that we cannot copy trust would, thus, be the same as to say that we cannot transmit trust. But we can and do transmit trust -- we just have to do it right (see refs in previous post). Similarly, we have to do it right when we transmit data (for example, if we don't have enough bandwidth or if there is too much noise, the data will be not be 100% transferred).
Ed Gerck

@_date: 2008-06-02 18:24:49
@_author: Ed Gerck 
@_subject: Can we copy trust? 
Recognition = a channel of information
memory = a channel of information
When you look at trust in various contexts, you will still find the need to receive information from sources OTHER than the source you want to trust. You may use these channels under different names, such as memory which is a special type of output that serves as input at a later point in time.
The distinguishing aspect between information and trust is this: "trust is that which is essential to a communication channel but cannot be transferred from a source to a destination using that channel". In other words, self-assertions cannot transfer trust. "Trust me" is, actually, a good indication not to trust.
Yes, where recognition is the OTHER channel that tells you that the value (given in the original channel) is correct. Just the value by itself is not useful for communicating trust -- you also need something else (eg, a digital sig) to provide the OTHER channel of Ed Gerck

@_date: 2008-06-02 23:53:44
@_author: Ed Gerck 
@_subject: Can we copy trust? 
Trust is indeed expressed by relationships. And those relationships can be transmitted with proper consideration -- just not in your example. In the case of SSL certs, a simple file copy is enough.
Ed Gerck
Did you have a chance yet to read Kelly's paper? In that paper, he is looking for stuff that can't be copied -- because he hopes that such stuff is scarce and valuable. "When copies are free, you need to sell things which can not be copied."
Kelly says that we can't copy trust. So, if I have 100 servers for the domain example.com does this mean that I have to buy 100 trusted SSL certs from the CA? Or, is any copy of the SSL cert as trustworthy as the original?

@_date: 2008-06-03 09:03:26
@_author: Ed Gerck 
@_subject: Can we copy trust? 
And we (Kelly and I) were talking about copying trust, where a copy is (as usual) a reproduction, a replication of an original. If you are copying trust from a domain, as represented by a SSL cert signed by a trusted CA, it should be a reproduction of /that/ trust  -- not trust on a different domain.
If you want to "copy" trust to a different domain, then we need to transfer the trust. This is also /possible/, as you know, as long as the issuing CA has set the "CA bit" in the SSL certificate. Object Signing CA certs must have the Object Signing CA bit set.
In summary, in SSL you can both copy and transfer trust. Without further evidence, which can be provided in pvt if desired by anyone, (1) SSL is not such only example in the Internet; and (2) we can likewise copy and transfer trust in our social interactions, not just in our digital interactions.
Ed Gerck

@_date: 2008-06-03 09:30:30
@_author: Ed Gerck 
@_subject: Can we copy trust? 
Yes, and the OTHER channels needed for trust are exactly those time-defined channels that you set up as you "get to know them over time". Each interaction, each phrase, each email exchanged is another Still, you can be talking to "Doris" in a p2p interaction over months and never realize it's actually Boris. This can happen in personal meetings as well, not just online.
The point being that (1) you need those other channels and can recognize them even if you are just in a p2p interaction; and (2) be careful because whatever channels you have, they will only span a certain, limited extent in the interaction that you want to trust, so your reliance space must be contained within that extent.
Shannon's information theory is a general approach that, even though it has  limitations as any other model, has allowed researchers to deal with both social and technical aspects of trust.
The important point, contrary to what PKI did, is to base the technical definition of trust on the social mediation of trust that we have learned over thousands of years.
Thus, when we look at linguistics and other areas where we find expressions of social experience and communication in a culture, we see that the unique, defining aspect of trust is that trust on something or someone needs /OTHER/ channels of information (where memory is also a channel) than the information channel we want to trust.
This social-linguistic observation transfers directly to the definition we can use with information theory for the technical aspect of trust, allowing the /same/ model of trust to be used in both worlds, as:
"trust is that which is essential to a communication channel but cannot be transferred from a source to a destination using that channel".
 From this abstract definition, you can instantiate a definition that applies to any desired context that you want -- social and/or technical -- while assuring that they all have the same model of trust. Examples are provided at the top of As usual, information is defined as: "information is that which is transferred from a source to a destination". If the same information is already present at the destination, there is no transfer. That's why information is surprise; there's no surprise if the information already exists at the destination.
You can use different models. I believe that trust is a more fundamental model than negotiation, as we can have trust without Ed Gerck

@_date: 2008-06-03 10:05:47
@_author: Ed Gerck 
@_subject: Can we copy trust? 
From the viewpoint of the user (which is the viewpoint used by Kelly), we see that trust can be copied when different users, accessing different servers for the same domain, do not know that they are using different copies of the /same/ SSL cert. In fact, no copy is less of an original than the original itself!
We see that the trust relationship represented by that SSL cert can be copied without any loss, as many times as you wish (for the possible dismay of the CA). If the CA bit is set, trust can even be transferred to multiple domains, and the trust represented by each such SSL cert in each domain can be copied without limit as well.
As to another point of your comment, the problem most people have with PKI is not that SSL does not work. SSL does not even need PKI.
The problem can be explained in terms of extent of trust. If you don't define your extent of trust in a CA, for example in your acceptance policy of records signed by certs from a CA, you may run into difficulties. The difficulties are /solved/ (within your risk model) when you correctly define the extent of trust -- rather than just taking a "trust in all matters" attitude.
For example, even though I do not trust a CA's CRLs, I may trust that CA to prevent rogue use of its private-key for signing end-user certs. This trust, limited by this extent, can be used in automating use of certs from that CA -- for example, only accept signatures from end-user certs of that CA if the cert is less than 31 days old (or, 15 days -- whatever your risk model says).
Ed Gerck

@_date: 2008-06-29 15:32:56
@_author: Ed Gerck 
@_subject: The wisdom of the ill informed 
Not so fast. Bank PINs are usually just 4 numeric characters long and yet they are considered /safe/ even for web access to the account (where a physical card is not required).
Why? Because after 4 tries the access is blocked for your IP number (in some cases after 3 tries).
The question is not only how many combinations you have but also how much time you need to try enough combinations so that you can succeed.
I'm not defending the designers of that email system, as I do not know any specifics -- I'm just pointing out that what you mention is not necessarily a problem and may be even safer than secure online banking Ed Gerck

@_date: 2008-06-30 11:30:44
@_author: Ed Gerck 
@_subject: The wisdom of the ill informed 
This is, indeed, a possible attack considering that the same IP may be legitimately used by different users behind NAT firewalls and/or with dynamic IPs. However, there are a number of reasons, and evidence, why this attack can be (and has been) prevented even for a short PIN:
1. there is a much higher number of combinations in a 12-digit account 2. banks are able to selectively block IP numbers for the /same/ browser and /same/ PIN after 4 or 3 wrong attempts, with a small false detection probability for other users of the same IP (who are not blocked). I know one online system that has been using such method for protecting webmail accounts, with several attacks logged but no compromise and no false detection complaints in 4 years.
3. some banks reported that in order to satisfy FFIEC requirements for two-factor authentication, but without requiring the customer to use anything else (eg, a dongle or a "battle ship map"), they were detecting the IP, browser information and use patterns as part of the authentication procedure. This directly enables  above.
I also note that the security problem with short PINs is not much different than that with passwords, as users notoriously choose passwords that are easy to guess. However, an online system that is not controlled by the attacker is able to likewise prevent multiple password tries, or multiple account tries for the same password.
Ed Gerck

@_date: 2008-06-30 12:55:16
@_author: Ed Gerck 
@_subject: The wisdom of the ill informed 
We are in agreement. Even short PINs could be safe in a bank-side authenticated (no MITM) SSL connection with 128-bit encryption. What's also needed is to block multiple attempts after 3 or 4 tries, in both the ATM and the SSL online scenarios.
Ed Gerck

@_date: 2008-06-30 18:50:49
@_author: Ed Gerck 
@_subject: The wisdom of the ill informed 
============================== START ==============================
You may well think that "You're completely wrong here," as you wrote. However, a first evidence that I'm correct is that the online banking system has /not/ collapsed under this attack (Dan's point) in many years... even though bad guys do have access to large blocks of different IP numbers, etc.
Wells Fargo allows PINs for user authentication. Passwords are optional and PINs are used for password setting. This is just to name one key US bank.
 > I suspect that currently invalid accounts are probably even cheaper
 > than valid ones
we all know that invalid accounts are of no use to attack, so this issue is not relevant here.
But let me address your other points.
 > I'm sure you will now go on about some other way to evade Dan's
 > crucial point, but it should be obvious to almost anyone that you're
 > not thinking like the bad guys. If you really want to go on about
 > this, though, I'll let you have as much rope as you like, though
 > only for a post or two as I don't want to bore people.
(don't worry, you never bore people)
Dan's question has to do with how to protect online access from multiple tries on the account number for a given PIN. Of course, the reverse (repeated use of the same account for different wrong PINs) can easily trigger a block.
As I replied to Dan, a counter-measure is for the server to selectively block IP numbers for the /same/ browser and /same/ PIN after 4 or 3 wrong attempts.
You present a valid objection in that there are people hijacking huge IP blocks for brief periods for spamming. People also hijack vast numbers of zombie machines. Either technology is easily used to prevent block-by-IP from doing squat for you, you wrote.
Not so fast.  Block-by-IP is not that useless. Many anti-spam blacklists use block-by-IP and it works. Further, if the PIN is held constant (eg, a common PIN such as 1111) and the IP as well as the browser identification are changed while different account numbers are targeted, this pattern can trigger a block by that PIN that repeatedly (3 or more times) causes an access error, for any IP number and browser. Excessive errors/minute can also trigger inspection and blocks.
You can find many other ways to try to trick the system. For example, you can space out the attacks and rotate the trivial PINs to reduce suspicion -- but you will also reduce the number of tries per hour that you can perform for each account.
What makes a good difference in preventing an attack as mentioned by Dan is to /not/ allow weak passwords in the first place! But, because this is not really possible with PIN systems (even with 6 digits), the security designer can detect attack patterns and use them to trigger a block even for an a priori unknown IP.
Ed Gerck

@_date: 2008-05-04 18:24:00
@_author: Ed Gerck 
@_subject: User interface, security, and "simplicity" 
(on Kerckhoffs's rules)
Yes. Usability should be the  property of a secure system.
Conventional security thinking says that usability and security are like a seesaw; if usability goes up, security must go down, and vice-versa. This apparent antinomy actually works as a synergy: with more usability in a secure system, security increases. With less usability in a secure system, security decreases. A secure system that is not usable will be left aside by users.
Ed Gerck

@_date: 2009-02-21 11:33:32
@_author: Ed Gerck 
@_subject: Solving password problems one at a time, Re: The password-reset paradox 
In a business, one must write down the passwords and one must have a duplicate copy of it, with further backup, where management can access it. This is SOP.
This is done not just in case the proverbial truck hits the employee, or fire strikes the building, or for the disgruntled cases, but because people do forget and a company cannot be at the same time responsible to the shareholders for its daily operations and not be responsible for the passwords that pretty much define how those daily operations are run.
The idea that people should not write their passwords is thus silly from the security viewpoint of assuring availability and also for another reason. Users cannot be trusted to follow instructions. So, if one's security depends on their users following instructions, then something is wrong from the start.
Solving password problems one at a time.
I submit that the most important password problem is not that someone may find it written somewhere. The most important password problem is that people forget it. So, writing it down and taking the easy precaution of not keeping next to the computer solves the most important problem with not even a comparably significant downside. Having automatic, secure, and self-managed password recovery and password reset (in case the password cannot be recovered) apps are also part of this I see the second most important problem in passwords to be that they usually have low entropy -- ie, passwords are usually easily guessable or easy to find in a quick search.
The next two important problems in passwords are absence of mutual authentication (anti-phishing) and absence of two-factor authentication.
To solve these three problems, at the same time, we have been experimenting since 2000 with a scheme where the Username/Password login is divided in two phases. In different applications in several countries over nine years, this has been tested with many hundreds of thousands of users and further improved. (you can also test it if you want). It has just recently been applied for TLS SMTP authentication where both the email address and the user's common name are also authenticated (as with X.509/PKI but without the certificates).
This is how it works, both for the UI and the engine behind it.
(UI in use since 2000, for web access control and authorization) After you enter a usercode in the first screen, you are presented with a second screen to enter your password. The usercode is a mnemonic 6-character code such as HB75RC (randomly generated, you receive from the server upon registration). Your password is freely choosen by you upon registration.That second screen also has something that you and the correct server know but that you did not disclose in the first screen -- we can use a simple three-letter combination ABC, for example. You use this to visually authenticate the server above the SSL layer. A rogue server would not know this combination, which allays spoofing considerations -- if you do not see the correct three-letter combination, do not enter your password.
(UI in use since 2008, TLS SMTP, aka SMTPS, authentication). The SMTP Username is your email address, while the SMTP Password is obtained by the user writing in sequence the usercode and the password. With TLS SMTP, encryption is on from the start (implict SSL), so that neither the Username or the Password are ever sent in the clear.
(UI 2008 version, web access control) Same as the TLS SMTP case, where a three-letter combination is provided for user anti-spoofing verification after the username (email address) is entered. In trust terms, the user does not trust the server with anything but the email address (which is public information) until the server has shown that it can be trusted (to that extent) by replying with the expected three-letter combination.
In all cases, because the usercode is not controlled by the user and is random, it adds a known and independently generated amount of entropy to the Password.
With a six-character (to be within the mnemonic range) usercode, usability considerations (no letter case, no symbols, overload "0" with "O", "1" with "I", for example), will reduce the entropy that can be added to (say) 35 bits. Considering that the average poor, short password chosen by users has between 20 and 40 bits of entropy, the end result is expected to have from 55 to 75 bits of entropy, which is quite strong. This can be made larger by, for example, refusing to accept passwords that are less than 8 characters long, by and adding more characters to the usercode alphabet and/or usercode (a 7-character code can still be mnemonic and human friendly).
The fourth problem, and the last important password problem that would still remain, is the vulnerability of password lists themselves, that could be downloaded and cracked given enough time, outside the access protections of online login (three-strikes and you're out). This is also solved in our scheme by using implicit passwords from a digital certificate calculation. There are no username and password lists to be attacked in the first place. No target, hence not threat.
In other words, to solve the fourth password problem we shift the information security solution space. From the yet-unsolved security problem of protecting servers and clients against penetration attacks to a connection reliability problem that is easily solved today.
This approach of solving password problems one at a time, shows that the "big problem" of passwords is now reduced to rather trivial data management functions -- no longer usability or data security functions.
Usability considerations still must be applied, of course, but not to solve the security problem. I submit that trying to solve the security problem while facing usability restrictions is what has prevented success so far.
Comments are welcome. More at Best regards,
Ed Gerck
ed at gerck.com

@_date: 2009-02-23 13:30:32
@_author: Ed Gerck 
@_subject: Solving password problems one at a time, Re: The password-reset 
Thanks for the comment. The BofA SiteKey attack you mention does not work for the web access scheme I mentioned because the usercode is private and random with a very large search space, and is always sent after SSL starts (hence, remains private). The attacker has a where the user sends the email address to get the three letters -- which is trivial to bypass.
I'm referring to SMTP authentication with implicit SSL. The same usercode|password combination is used here as well, but the usercode is prepended to the password while the username is the email address. In this case, there is no anti-phishing needed.
This case has the  same BofA SiteKey vulnerability. However, if that is bothersome, the scheme can also send a timed nonce to a cell phone, which is unknown to the attacker. This is explained elsewhere in (there are different solutions for different threat models)
If the threat model is that you can "learn or know the RNG a given site is using" then the answer is to use a hardware RNG.
The point is that two passwords would still not have an entropy value that you can trust, as it all would depend on user input.
That data is just a key that is the same for /all/ users. It is not user-specific. its knowledge does not provide information to attack any Sorry if it wasn't clear. Please have a second reading.
Ed Gerck

@_date: 2009-02-23 14:40:43
@_author: Ed Gerck 
@_subject: Solving password problems one at a time, Re: The password-reset 
Humans tend to notice patterns. We  easily notice mispelngs. Your experience may be different but we found out in testing that three-letters can be made large enough to become a visually noticeable Reversing the point, the fact that a user can ignore the three-letters is useful if the user forgets them. The last thing users want is one more hassle. The idea is to give users a way to allay spoofing concerns, if they so want and are motivated to, or learn to be motivated. Mark Twain's cat was afraid of the cold stove.
Ed Gerck

@_date: 2009-02-23 17:23:33
@_author: Ed Gerck 
@_subject: Solving password problems one at a time, Re: The password-reset 
What usercode? The point you are missing is that there are 2^35 private usercodes and you have no idea which one matches the email address that you want to sent your phishing email to.
The other points, including the  TLS SMTP login I mentioned, might be clearer with an example. I'll be happy to provide you with a test account.
Ed Gerck
