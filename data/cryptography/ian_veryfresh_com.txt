
@_date: 2002-06-21 11:48:00
@_author: Ian Clelland 
@_subject: RSA getting rid of trusted third parties? 
But haven't browsers supported ceritificate chaining for years? As far as I can tell, that's all this is - RSA issues you a cert which says that you are trusted to create additional certificates (presumably just for entities within your organisation).
The trust model doesn't break down just because anyone can create a valid X.509 certificate. There still has to be a valid chain of trust leading back to a trusted party (RSA, in this case). If that trust is abused, then RSA can revoke your cert and break the chain.
Ian Clelland

@_date: 2002-06-21 14:30:51
@_author: Ian Clelland 
@_subject: RSA getting rid of trusted third parties? 
I hope that they would reserve the right to revoke the certificate before it expires. There has to be a way for RSA to say that 'we no longer trust the entity posessing this certificate'. Even if a company has paid for the certificate, it should still be revocable in the event of breach of contract, or loss/theft of the certificate.
That's a good point, but I think it's more of an argument that the browser-certificate model was already broken, not that this new service suddenly changes anything.
Ian Clelland

@_date: 2002-06-21 14:59:31
@_author: Ian Clelland 
@_subject: RSA getting rid of trusted third parties? 
It makes sense, though, that a company should be able to issue certificates for servers belonging to various departments within the company. The organisation knows its own internals far better than RSA does. Why should RSA/Verisign/whoever be responsible for signing such certificates? I see no benefit to having such a wide, flat web of trust.
What root CAs are good at, and what they should be doing, is authenticating the organisation itself. They can verify that the organisation exists as described, and that the private key really is controlled by someone authorised within that organisation. This makes them fairly well suited to handing out certificates for the public face of the organisation.
The high cost of this process, though, means that organisations tend to have very few secure servers, and if they need to secure any machines for internal use, they're not going to ask a root CA to do it; they'll just make one and sign it themselves, and probably put a note on the page which says, "We know that your browser will claim this cert is invalid; just accept it anyway."
This sort of practice is what leads to all of the help-desk calls, and is probably more damaging to PKI, ultimately, than letting RSA issue a certificate which says, "The owner of this certificate is trusted to sign certificates within the organisation XYZ".
If the alternative is to have people conditioned to simply click "Proceed" whenever they see an unrecognised signer, I'd much rather have this system.
They already have such a root certificate in your browser. Nowhere in the press release do they say that they will let anyone and everyone get the CA software and start signing certificates for every site and its dog.
This shouldn't be a problem, as long as the signing certificate can only be used to sign certificates within that organisation. In that case, if one does get compromised, then that company has a major PR problem, but it's not the end of the world for everyone else. They should have taken better care of the keys.
My whole argument, of course, rests on the assumption that these certificates can be restricted in this way. I don't know enough about the format of X.509 certs to say for sure that this is true. Someone on this list must, though.
Ian Clelland

@_date: 2002-06-24 09:07:15
@_author: Ian Clelland 
@_subject: X.509, SSL & security of decentalized certification 
The global DN hierarchy has many of the same problems as the DNS. Your example of IBM  and IBM UK encounters the same problem with X.500 DNs. "c=US, o=IBM" is not related at all to "c=UK, o=IBM". Most organisations, though, do not have multiple roots in DN-space, and those that do would have little difficulty in getting each of those roots verified by a global root CA.
So there is a real problem, then, if third-level certificates can only be authenticated based on their X.500 DN, but the browser has no method of associating the IP address or DNS name with any DN.
That's really unfortunate, and looks like the system of CAs and certs for the web has been crippled from the beginning.
Ian Clelland

@_date: 2002-03-01 23:39:59
@_author: ian@veryfresh.com 
@_subject: Interesting new cipher patent 
How is this different than a standard stream cipher?
Except for the key length, of 2^20 bits (compared to 2^6 to 2^8 in
conventional stream ciphers), and the mention of reversibility, I
can't see the difference.
Of course, having such a large key means that you have 128KB of key
material to generate and securely distribute (I hope there would not
have to be a new key for every message,) as you suggest, in the same
cumbersome manner as a one time pad.
I don't see where your requirement for reversibility comes in. My
understanding of stream ciphers is that you would not want a reversible
PRNG, since compromise of the PRNG state would mean that all past
communications can now be decrypted.
With regards to the period of this cipher, it would be ridiculously
long, as you say, but so are much smaller ciphers.. A 2^8-bit PRNG
already has a period of length 2^2^8, or 10^77, which is a lot of data
to send using a single key.
I would be interested to know if this conjectured RNG has different
properties than a conventional PRNG, or if I have misread your
description, and a stream cipher is not actually what you were
Ian Clelland

@_date: 2002-09-21 01:22:50
@_author: Ian Clelland 
@_subject: unforgeable optical tokens? 
I think you're right here; in order for the challenges to be reproducable, the locations / angles that the reader uses would have to be discrete, probably by some sort of stepper motor. However, if the readers are autonomous (and each one needs to see the physical token once in order to identify it later,) then every reader could be calibrated differently, and would therefore use one relatively small subset of locations / angles out of a large number of subsets.
I wonder if an analysis of the diffraction patterns produced by passing light though a token like this would provide enough information to reconstruct the internal 3-D shape... it strikes me as being a problem similar to X-ray crystallography.
Ian Clelland

@_date: 2003-04-18 04:39:04
@_author: Ian Clelland 
@_subject: DMCA Crypto Software 
Can you post at least an overview of this formula, or describe how it is
My take on the math is this: Suppose that a 90 minute movie contains
1620 'polymorphic frames,' each version of which movie contains one of
two alternate versions of each such frame. Then every version of the
movie has effectively been watermarked with a 1620-bit id. All an
attacker needs to do to release a watermark-stripped version is to find
both versions of each of the 1620 key frames, and put together a new
version of the movie, with a random selection for each frame. This new
version cannot be traced back to any particular source, so the attacker
cannot be identified.
If an attacker can only obtain a small number of 'official' versions
with which to work, then his chances of success depend to a large extent
on the total number of official versions in existence. (Someone, whether
the company releasing the movie, or the company doing the watermarking,
is going to have to keep track of all of the official versions, and whom
they are licensed to).
If the attacker can only obtain four versions, then he will have access
to both versions of 15/16 of the polymorphic frames (assuming that
watermark bits appear random). Knowing their positions within the movie
file, he can randomise them, effectively obscuring 1508 bits of the
watermark. However, the remaining 102 bits will identify the four source
files with very high probability.
Assuming that there are 1 billion legitimate copies of the movie
(roughly 2^30), then the odds are only 2^30/2^102 = 1 in 2^72, or 1 in
4*10^21, that any other copy shares those 102 bits with the four source
With five source files, the situation changes a bit - there are now only
51 identifying bits, and a 1 in 2^21 (1 in 2*10^6) chance that another
source file might match. Still not good, though.
With a sixth source, the attacker has cut the number of identifying bits
to 25. In this scenario, you can expect there to be about 32 copies out
of 1 billion which share those bits. That's 26 others on top of his 6.
Much better odds, but still not a lot to hide behind.
Of course, the attack only gets better with each additional source, and
after a while (after about lg 1620 = 11 sources,) it is very likely that
an attacker can put together all versions of every polymorphic frames,
and there is no way to track it at all.
Anyway, I'm very curious to know where the original numbers (4*10^-10
and 10^20) came from. Or perhaps my reasoning is just way off on this.
Ian Clelland

@_date: 2005-08-04 11:37:44
@_author: Ian Clelland 
@_subject: Query about hash function capability 
Why not just include a canonicalization step at the beginning of the
hash that is designed to ignore rotation?
For example, if you can define an ordering on the set of possible inputs to the hash, then you can rotate any input to the point where it is the "smallest" (or "largest") that it can be, and then hash *that* Ian Clelland
