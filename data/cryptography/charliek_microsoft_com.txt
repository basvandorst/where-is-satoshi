
@_date: 2005-03-14 18:28:55
@_author: Charlie Kaufman 
@_subject: I'll show you mine if you show me, er, mine 
James A. Donald said:
Unpatented will be your biggest hurdle.
I collaborated on the development of a strong password protocol with the
explicit goal of having such a protocol that was not patented. For
details, see:
But while we got our employers to agree not to patent the algorithm,
neither we nor they are willing to defend it against infringement claims
by others. (It also has not been extensively reviewed. There is no
particular motivation for anyone to do so since its performance is
inferior to other schemes and its patent status is uncertain.)
Basically, there is no way to establish that any technology is
unpatented. The best you can do is hide behind someone with deeper
pockets than you do who is doing the same thing. Like hiding behind IBM
when using Linux.

@_date: 2005-03-23 17:11:47
@_author: Charlie Kaufman 
@_subject: Propping up SHA-1 (or MD5) 
All hash functions I'm aware of consist of an inner compression function
that hashes a fixed size block of data into a smaller fixed size block
and an outer composition function that applies the inner function
iteratively to the variable length data to be hashed. Essentially you're
proposing a modification to the outer layer of the hash construction.
All of the standard hash functions since MD4 have been constructed so
that a collision in the inner compression function is likely to lead to
a collision in the hash function. MD2 did not have that property. It
computed a cheap checksum of the variable length data in parallel with
the digesting process and digested the checksum following the data. I
have often wondered whether such a cheap addition would strengthen the
newer hashes. (It would fix the suffixing attacks that motivated the
development of HMAC).
It's not obvious whether this would make the functions more secure or
just make them harder to analyze. Perhaps someone from the research
community could comment on why the checksum was removed in the evolution
from MD2 to MD4.
Your proposed encoding has the disadvantage that it would require two
passes over the message being digested. This would be bad news for
hardware implementations and should be avoided if possible.
You note with the construction:
H'(x)=Random || H(Random || x)
(reminiscent of the salted hash calculation for UNIX passwords) that the
hash gets longer. The hash need not get longer. If you have 40 random
bits and the first 120 bits of H(Random || x), you match the size of
SHA-1 and get improved security against most practical attacks. If your
system depends on a fixed length hash, you're in trouble already because
the fixed length is probably 128 bits and the world is headed toward
A problem that does exist with this construction is that some uses of
hash functions assume that if you hash the same data you get the same
hash (or indirectly, that if you sign the same data you get the same
signature). In particular, you now need separate functions for
generating a hash and for checking one.
-----Original Message-----
[mailto:owner-cryptography at metzdowd.com] On Behalf Of Ben Laurie
Sent: Monday, March 21, 2005 3:57 AM
It was suggested at the SAAG meeting at the Minneapolis IETF that a way to deal with weakness in hash functions was to create a new hash function from the old like so:
H'(x)=Random || H(Random || x)
However, this allows an attacker to play with Random (the advice I've seen is that if one is going to use an IV with a hash function, then one
should transfer the IV with integrity checks to deny attackers this Another objection is that this construction changes the API at the sender end, which could lead to a great deal of complexity when the use of the hash API is deeply embedded.
A third is that the length of the hash is changed, which could break existing protocols.
Musing on these points, I wondered about the construction:
H'(x)=H(H(x) || H(H(x) || x))
which doesn't allow an attacker any choice, doesn't change APIs and doesn't change the length of the hash. Does this have any merit? Note that this is essentially an HMAC where the key is H(x). I omitted the padding because it seems to me that this actually makes HMAC weaker against the current attacks.

@_date: 2005-03-24 10:59:03
@_author: Charlie Kaufman 
@_subject: Propping up SHA-1 (or MD5) 
Whether these various tricks help depends on the technical details of
the attacks found. I hope that the bit twiddling crypto types who are
finding the attacks are going to propose something to fix them.
There are probably cheaper fixes than the 2x or 3x performance loss of
your algorithm down in the inner loops of these algorithms (such as the
change from SHA to SHA-1) and that these will come out. I'm reluctant to
jump on the SHA-256 bandwagon or to come up with some ad hoc fix until a
more thorough analysis is done. SHA-256 was designed before these
attacks were known and probably has related flaws (though they are even
less likely to be practically exploitable). We have the luxury of having
the current break being largely theoretical, so waiting even a year for
the mathematicians is probably OK. But it's never too early to start
preparing for a new algorithm - perhaps with a new hash size - in our
protocols. Further, given that lots of attacks (past and present) are
not exploitable if every hashed quantity includes some value chosen by a
trusted party and unpredictable by an attacker, it seems reasonable to
consider that as a desirable characteristic as we design our protocols.
p.s. Your formulae below have unbalanced parentheses, but I can guess
what you probably meant.
-----Original Message-----
Sent: Thursday, March 24, 2005 2:39 AM
I suggested in a later version these two constructions:
H'(x)=H(H(x || H(0 || x) || H(0 || x))
H'(x)=H(H(x || H(0 || x) || H(1 || x))
which only require a single pass (but, unfortunately, two or three different instances of the hash). This seems similar to the mechanism used in MD2, except the checksum is expensive.

@_date: 2009-02-21 20:26:01
@_author: Charlie Kaufman 
@_subject: The password-reset paradox 
I would assume (hope?) that when you have an OTP token, you get two factor
authentication and don't stop needing a password. You would need a password
either to unlock the OTP device or to enter alongside the OTP value. Otherwise,
someone who finds your token can impersonate you.
Assuming that's true, OTP tokens add costs by introducing new failure modes (e.g.,
I lost it, I ran it through the washing machine, etc.). I suspect a similar study
would find that the cost of the OTP token would be $500-$700/yr. even if the
device itself only cost $5. After all, passwords are free!
-----Original Message-----
Sent: Thursday, February 19, 2009 5:36 AM
There are a variety of password cost-estimation surveys floating around that
put the cost of password resets at $100-200 per user per year, depending on
which survey you use (Gartner says so, it must be true).
You can get OTP tokens as little as $5.  Barely anyone uses them.
Can anyone explain why, if the cost of password resets is so high, banks and
the like don't want to spend $5 (plus one-off background infrastructure costs
and whatnot) on a token like this?
(My guess is that the password-reset cost estimates are coming from the same
place as software and music piracy figures, but I'd still be interested in any
information anyone can provide).
