
@_date: 2002-08-11 19:09:22
@_author: Peter Fairbrother 
@_subject: TCPA/Palladium -- likely future implications (Re: dangers of 
That was a software keylogger (actually two software keyloggers), not
hardware. (IMO Scarfo's lawyers should never have dealt, assuming the evidence was
necessary for a conviction, but the FBI statement about the techniques used
was probably too obfuscated for them - it took me a good week to understand
it. I emailed them, but got no reply.
Incidently, Nicky Scarfo used his father's prison number for the password,
so a well researched directed dictionary attack would have worked anyway.)
The FBI reputedly can (usually, on Windows boxen) now install similar
software keyloggers remotely, without needing to break in.

@_date: 2002-12-05 02:17:30
@_author: Peter Fairbrother 
@_subject: DBCs now issued by DMT 
OK, suppose we've got a bank that issues bearer "money".
Who owns the bank? It should be owned by bearer shares, of course.
Can any clever person here devise such a protocol?
I'd guess that all the Bank's finances should be available to anyone who
asks. That should include an accounting of all the "money" issued. And not
be reliant on one computer to keep the records.
Or the propounders wanting to: make a profit/control the bank?

@_date: 2002-07-31 20:07:16
@_author: Peter Fairbrother 
@_subject: Challenge to David Wagner on TCPA 
============================== START ==============================
The wise general will plan his defences according to his opponent's
capabilities, not according to his opponent's avowed intentions.
However, in this case the intention to attack with all available weapons has
not been well hidden. There may be some dupes who honestly profess that no
attack is planned, and some naif's who cannot or will not see the wood, but
they will reap the whirlwind.
My humble opinion,

@_date: 2002-06-26 04:05:30
@_author: Peter Fairbrother 
@_subject: Ross's TCPA paper  
Neither do I. If one-way functions exist there is no such symmetry. While in theory such
functions cannot be proved to exist, in practice they are used a lot in
crypto etc.

@_date: 2003-12-04 23:44:58
@_author: Peter Fairbrother 
@_subject: Searching for uncopyable key made of sparkles in plastic 
(catching up on old posts)
Not a ref as such, more a bit of trivia.
A similar system was used to verify SALT. Russian ICBM's etc had sparkles
glued to them, and from time to time US people would test them to see if
they were the same missiles.
I don't know what the Russians did to the US missiles, but I think it was
the same.

@_date: 2003-12-06 18:05:54
@_author: Peter Fairbrother 
@_subject: safety of Pohlig-Hellman with a common modulus? 
As far as I can tell it's safe - the main danger is that it that if an
attacker does the work to calculate the factor base for an index calculus
attack, the factor base is useful for attacking all ciphertext which uses
the modulus. It's fairly easy to find an individual discreet log with a
factor base, so such an attacker would get a bigger return on investment.
Two simple ways around that - use longer prime moduli, or change the modulus
from time to time.
The attack on RSA has no equivalent here. That attack involves using a key
pair to find phi(n) or a divisor of it, but in Pohlig-Hellman the value of
phi(p) is not secret (= p-1).
I am presently using Pohlig-Hellman in a construction for universal
re-encryption, taking advantage of it's key-multiplicative property. See
 or
 for the messy math details,
my application is in the online observable SFS for m-o-o-t.
Pohlig-Hellman is still however very slow compared to modern symmetric
ciphers, and in most cases where P-H is used a group cipher with the
required key properties would be more efficient.
No such cipher exists, but I am thinking of building one. I have already got
a few indications of interest, if anyone else wants to contribute please let
me know. I'm not committed to doing it, but if enough people want it..

@_date: 2003-12-06 18:15:03
@_author: Peter Fairbrother 
@_subject: safety of Pohlig-Hellman with a common modulus? 
As far as I can tell it's safe - the main danger is that it that if an
attacker does the work to calculate the factor base for an index calculus
attack, the factor base is useful for attacking all ciphertext which uses
the modulus. It's fairly easy to find an individual discreet log with a
factor base, so such an attacker would get a bigger return on investment.
Sorry, the above is complete nonsense, and only applies in a few situations.
There are some chosen plaintext attacks, and especially adaptive chosen
plaintext attacks, but they apply whether or not the modulus is shared.
But P-H with a shared modulus is pretty much as safe as with different
moduli, afaict.

@_date: 2003-12-07 03:58:06
@_author: Peter Fairbrother 
@_subject: safety of Pohlig-Hellman with a common modulus? 
Nope. In P-H there is no g. A ciphertext is M^k mod p. An attacker won't
know k, and usually won't know M, but see below. I don't know what the
problem is called, but it isn't DLP. Anyone?
Not usually.  In general index calculus attacks don't work on P-H, except in
chosen plaintext attacks (where the chosen plaintext sort-of substitutes for
When using P-H I usually pre-encrypt data in any old symmetric cipher with a
random IV and any old key, to avoid known plaintext attacks. There are
several other attacks to be aware of, including some nasty
adaptive-chosen-plaintext and chosen-ciphertext attacks.

@_date: 2003-12-07 17:32:43
@_author: Peter Fairbrother 
@_subject: safety of Pohlig-Hellman with a common modulus? 
Duuuh. I _knew_ that. I've even proposed changing p from time to time to
limit the take from an IC attack. Dumb of me.
Too much beer, no coffee, got a brainstorm and couldn't see the wood for the
trees... Sorry.

@_date: 2003-06-27 19:00:48
@_author: Peter Fairbrother 
@_subject: pubkeys for p and g 
Can you give me a ref to where they say that? I'd like to know exactly what
they are claiming. Perhaps they are encrypting the DH secrets with RSA keys to provide some
recipient authentication?
Or perhaps they are using DH instead of RSA for their public keys?
(I assume a and b the usual DH secrets)
You mean "all public keys are good generators mod all public keys"
This won't work, for instance, the N's in RSA keys can't be prime. The e's
can be, but there is then no way that I can think of to ensure that an e is
a generator of a sufficiently large subgroup of another, unknown at
generation, e. It might be possible to use some algorithm to find a suitable g, but that
doesn't conform to your/ their stipulation.

@_date: 2003-06-27 19:00:59
@_author: Peter Fairbrother 
@_subject: pubkeys for p and g 
It's not exactly DH, but if you used the e of a RSA key as g, and the N as
p, that would actually work. It's only one RSA key tho'.

@_date: 2003-11-18 05:47:08
@_author: Peter Fairbrother 
@_subject: A-B-a-b encryption 
The protocol is called the Shamir three-pass protocol. It needs a
commutative cipher.
Probably the only cipher that it can be securely used with is called the
Pohlig-Hellman cipher, a simple exponentiating cipher over Zp.
Whether it's a symmetric cipher is a matter of precise definition, though
despite the encryption and decryption keys being different I would consider
it such. A better term might be a secret-key cipher. It's quite easy to find
the decryption key d from the encryption key e:
d*e = 1 mod (p-1)
C = M^e mod p
M = C^d mod p
p should be a "safe" (= 2q+1, q prime) prime, and all keys used should be
odd and !=q. There is an ECC variant. There are lots of things to watch out
for in implementations.
I'm trying to develop (or find? anyone?) a secure symmetric cipher which is
a group, where if you know A and B you can find a key C that decrypts
B(A(M)), but that's a different story.

@_date: 2003-11-26 20:47:01
@_author: Peter Fairbrother 
@_subject: Are there...one-way encryption algorithms 
A useful property for all sorts of things. I'm using P-H to improve Golle et
al's universal encryption methods,
 but it's a pity that
Pohlig-Hellman is still slow, and that there isn't a faster algorithm with
similar properties.
There's lots of potential uses for one of those :)

@_date: 2003-11-26 20:50:44
@_author: Peter Fairbrother 
@_subject: lockable trapdoor one-way function 
Does anyone know of a trapdoor one-way function whose trapdoor can be locked
after use?
It can be done with secure hardware and/or distributed trust, just delete
the trapdoor key, and prove (somehow?) you've deleted it.
It looks hard to do in "trust-the-math-only" mode...

@_date: 2003-10-02 23:58:50
@_author: Peter Fairbrother 
@_subject: quantum hype 
I've just discovered that that won't work. Eve can get sufficient
information to make any classical error correction or entropy distillation
techniques unuseable.
See:  You have to use QPA instead, which has far too many theoretical assumptions
for my trust.

@_date: 2003-09-21 13:37:21
@_author: Peter Fairbrother 
@_subject: quantum hype 
There are lots of types of QC. I'll just mention two.
In "classic" QC Alice generates polarised photons at randomly chosen either
"+" or "x" polarisations. Bob measures the received photons using a randomly
chosen polarisation, and tells Alice whether the measurement polarisation he
chose was "+" or "x", on a authenticated but non-secret channel. Alice
replies with a list of correct choices, and the shared secret is calculated
according as to whether the "+" polarisations are horizontal or vertical,
similar for the "slant" polarisations.
If the channel is authentic then a MitM is hard - but not impossible. The
"no-cloning" theorem is all very well, but physics actually allows imperfect
cloning of up to 5/6 of the photons while retaining polarisation, and this
should be allowed for as well as the noise calculations. I don't know of any
existing OTS equipment that does that.
A lasing medium can in theory clone photons with up to 5/6 of them retaining
enough polarisation data to use as above, though in practice the noise is
usually high.
There is also another less noisy cloning technique which has recently been
done in laboratories, though it doubles the photon's wavelength, which would
be noticeable, and I can't see ofhand how in practice to half the wavelength
again without losing polarisation (except perhaps using changing
gravitational fields and the like); but there is no theory that says that
that can't be done.
In another type of QC Alice and Bob agree on the measurement angles (any
angles, not just multiples of 45 deg) they will use, and Alice generates a
pair of entangled photons, sending one to Bob. Both measure the individual
photons at that angle, and the shared secret is generated according to
whether the photons pass the filter.
If the agreed-on measurement angles are kept secret, and noise bounds etc
are obeyed, then a MitM is hard as before except the theoretical maximum
ratio of "clonable" photons is lower - but it isn't much use, except as an
"otp key multiplier".
There are a zillion variations on these themes, and other types of QC. For
instance Alice can send Bob data rather than generating a random shared
secret, and without a separate channel, if she generates the quantum string
using a preshared secret. Mallory can get 1/2 of the bits, but AONT's can
defend against that, and if properly implemented no MitM is possible.
And so on.

@_date: 2003-09-21 17:51:56
@_author: Peter Fairbrother 
@_subject: quantum hype 
Had two requests for links (and some scepticism) about this already. Try:
for an article and some ref's (though I'm not even sure if the paper
referred to is the one I'm thinking of, the one with wavelength doubling. I
though it was published earlier this year).
I'll try and post some better links later.

@_date: 2003-09-22 21:37:47
@_author: Peter Fairbrother 
@_subject: quantum hype 
Agreed. It's not a break, though it does make it harder. Many people think
the no-cloning theorem says you can't clone photons at all. Most COTS QC
gear only "works" under that false assumption.
Then there's the noise/error rates - in practice it's very hard to get > 60%
single photon detection rates, even under the most favourable conditions,
and low error rates are hard to get too.
I tend to the opinion, without sufficient justification and knowledge to
make it more than an opinion, that most COTS QC products are probably secure
today in practice, but claims for theoretical security are overblown.
There may be yet another problem which I should mention. First, I'd like to
state that I'm not a quantum mechanic, and I find the math and theory quite
hard, so don't rely too much on this.
I'm not certain that the 5/6 figure is a universal physical limit. It may
just be an artifact of the particular unitary transform used in that
specific cloning process.
It _may_ be possible for the cloner to get some information about which
photons were cloned incorrectly. This is tricky, and I don't know if it's
right - it involves non-interactive measurement of virtual states, kind of.
Another possibility is to imperfectly clone the photon more than once.
The no-cloning theorem per se doesn't disallow these, it only disallows
perfect cloning, but other physics might.
QC's unbreakability isn't based on a "hard problem", it's based on the
physical impossibility of perfect cloning. But exactly what that
impossibility means in practice, I wouldn't like to say. You can't clone
every photon. Can you only clone 5/6 of photons? Or 99.99999...% of them? It
may be the latter.
BTW, you can decrease the wavelength of a photon by bouncing it off moving

@_date: 2003-09-23 00:38:26
@_author: Peter Fairbrother 
@_subject: quantum hype 
I never suggested it was very practical, but:
Trap it in a cavity between two parallel mirrors, and shrink the cavity. It
doesn't matter (within reason) how fast you shrink it, just how much.

@_date: 2003-09-26 11:58:43
@_author: Peter Fairbrother 
@_subject: Can Eve repeat? 
Thanks for the interesting link.
That's pretty much what I was talking about when I said that it may be
possible to clone an arbitrarily large proportion of photons - and that
Quantum Cryptography may not actually be secure.
For instance, you can clone a "virtual" photon and do an interaction-free
measurement comparing the now-cloned photon and the photon in it's uncloned
state. If they don't match, the photon was incorrectly cloned. You may only
be able to correctly clone 5/6 of the photons, but that way you know which
photons were correctly cloned.
It may also be possible to clone an arbitrarily large proportion of photons,
ie approaching all of them, by measuring the incorrectly cloned photons and
their clones or transforming to get the original photon back, then trying to
clone again. Other methods are perhaps possible too.
The "no-cloning rule" says that no unitary transform will take two quantum
waveforms, one unknown, and generate two wavefoms with the same state as the
unknown waveform. It's probably true (anent some non-linear transform), but
it _doesn't_ say that there isn't another way to clone quanta, perhaps using
three waveforms, or "virtual" waveforms, or generating a new quantum from
interaction-free measurement of the original quantum waveform.
IMO far too much reliance has been placed on it, or perhaps people just
misunderstood what it says.
It reminds me a bit of the "cd's are better than vinyl" dispute - the cd
guys said that Nyquist theorem showed that cd's reproduced music to above
human hearing, but it doesn't, it just shows that less-than-Nyquist sampling
rates have extra problems. And it only applies to steady states, not music.
And so on.
And the "no-cloning rule" _doesn't_ say it's impossible to clone. Perhaps it
should be called somthing else.

@_date: 2003-09-28 15:43:10
@_author: Peter Fairbrother 
@_subject: quantum hype 
I promised some links about the 5/6 cloning figure. You've had a few
experimental ones, here are some theory ones.
Cloning machines:
Theoretically optimal cloning machines:
1/6 disturbance is theoretically optimal, both as a QC interception strategy
and "it's an optimal cloning machine":
A different approach to the 1/6 figure (2/3 cloned correctly, the 1/3
imperfectly cloned still has a 50% chance of being right):
That lot is pretty much indisputed...
...except for the "optimal" part; and that's a sideways argument anyway -
the math and physics theory are right as far as they go, just that they
didn't consider everything.
It may be possible to clone better than those "optimal" solutions,
especially in the classic QC case, or get more information like which
photons were cloned correctly, and perhaps to as near perfection as you
like, but that is in dispute. Actually it's a pretty friendly dispute,
people mostly say "I don't know"*. I'll post some more links on that later.
*unless someone mentions non-linear transformations. Which is a different
dispute really.

@_date: 2003-09-30 01:34:22
@_author: Peter Fairbrother 
@_subject: Can Eve repeat? 
Caveat - I'm slightly drunk, and in a quitting-nicotine state...
IFM's can detect classical measurements, without changing the classical
measurement (eg the presence of an obstacle in a path). They may change the
quantum state of the classical measurement, but that doesn't matter as far
as changing it's classical aspect goes.
What I was referring to was using IFM and quantum Zeno techniques to detect
_which_ quanta were incorrectly cloned by a UQCM. That would increase the
Shannon information available to Eve, and allow her to either stop
transmission of the incorrectly cloned quanta, or, and this is an unknown
step, to reconstruct the incident quantum and try again to clone it.
The simple apparatus is just a UQCM in one limb of a Mach-Zender
interferometer. That's enough to increase Eve's Shannon infomation. The QZ
apparatus I can't describe, but I can well imagine one that would allow
near-perfect detection of incorrect clones.
But this follows your quote:
"On the other hand the method do allow performing some non-demolition
measurements. It might be momentum-exchange free and energy-exchange free."
You are claiming too much, and ignoring what the paper actually says. Bad
boy! Spank! It might be changing-polarisation-free too. I'm pretty sure it
can be.
Not entirely. IFM techniques can also eg compare the incident photon with
the same photon in a "has-been-cloned" state. The theory is slightly
different, which was why I said "sort of" in my earlier post on the subject.
Not as you describe. But the earlier "it would defy Einsteinian causality"
(without a detailed explanation of how) argument has been shown to be wrong,
and there is now nothing I know of that will prevent EV IFM-like techniques
working to detect which quanta are incorrectly cloned. Or recreating the
original photon (though there might be limits on that).
Yep. Nope. Maybe. I forget what the second statement was. I've got a feeling
that Ivan had a point tho'.

@_date: 2004-06-07 11:30:07
@_author: Peter Fairbrother 
@_subject: Article on passwords in Wired News 
My (European) bank uses "memorable information", an alphanumeric string
provided by me, and they ask for three randomly chosen characters when
authenticating online. There is also a fixed password.
Not terribly secure, or terribly one-time, but it would defeat a simple
keylogger or shoulder surfing attack, for instance. It doesn't give me the
warm fuzzies, but it does mean I would use a dodgy terminal at least once if
I was stuck in the badlands (and then change passwords etc.).

@_date: 2005-08-08 02:21:55
@_author: Peter Fairbrother 
@_subject: solving the wrong problem 
Didn't the people who did US/USSR nuclear arms verification do something
very similar, except the characterised surface was sparkles in plastic
painted on the missile rather than paper?

@_date: 2005-08-08 17:59:34
@_author: Peter Fairbrother 
@_subject: solving the wrong problem 
Talking of solving the wrong problem, that's a pretty bad metric - forging
should cost the damage an extra warhead would do, rather than the cost of an
extra warhead. That's got to be in the trillions, rather than a few hundred
thousand for another warhead.

@_date: 2005-07-10 00:13:42
@_author: Peter Fairbrother 
@_subject: Why Blockbuster looks at your ID. 
I agree that it might well reduce costs and fraud - but how will it improve
privacy? Your name is already on the card ... and the issuer will still have
a list of your transactions.
Not having to show ID may save annoyance, but it doesn't significantly
improve privacy.

@_date: 2005-07-10 00:12:14
@_author: Peter Fairbrother 
@_subject: Why Blockbuster looks at your ID. 
yes ...
 I'm not too sure what you mean.
 In the UK the merchant is not usually liable for card-present fraud.
 There has been / is about to be a change to the liability of the merchant,
usually to the effect that if a fraud is successful because the merchant
hasn't installed PIN equipment then they will be liable. A few banks are
making merchants liable for all fraud if PIN equipment has not been
EMV said the change would begin on 1st Jan, but the banks haven't all
implemented it yet. Many did so on 1st July.
The change occurs in the contract between the aquiring banks and the
merchants, not the law; the legality of the change is questionable, but as
it is basically just a way to encourage retailers to install PIN equipment
it has not been challenged afaik.
There is no change in the merchant's liability if he has installed Chip n'
PIN equipment - the tales circulating of all merchants becoming liable for
all frauds are simply not true.
 There will also be a change in the way fraud claims are dealt with, to the
almost certain disadvantage of the cardholder, as there is no physical
signature to contest and at least in the first instance the issuers
determine the "facts".
 However I am not aware of any changes to the law.
 There was a very recent Banking Ombudsman case where the cardholder had
been grossly negligent about her PIN security, but her liability was still
limited to ?50 (which is a statutory limit and applies to credit cards, but
not to debit cards - although it is in practice applied to them too).
Usually the ?50 limit is not charged by the issuing bank.
 However the customer eventually pays for fraud anyway, in the form of
higher prices, so the issuer - merchant liability split is not of immediate
relevance to the customer. It should be tilted firmly against the banks IMO
though, as they are responsible for the system, not the merchants, who have
no say, as EMV + AmEx is an effective monopoly.
 BTW, one of my banks recently sent me a leaflet which said Chip n' PIN was
going to be introduced worldwide. Anyone know more about that?

@_date: 2005-07-11 20:49:09
@_author: Peter Fairbrother 
@_subject: EMV 
London Underground have a contactless system too, but it isn't used much. As
I remember it had a similar problem, but they may have changed that.
You take out your wallet with the card in and wave it over a palm-sized
yellow blob on the turnstile, but you don't have to open your wallet to
withdraw a token. Muggers and pickpockets keep a close eye out to see how fat your wallet is
and where you keep it ...

@_date: 2005-06-21 02:03:09
@_author: Peter Fairbrother 
@_subject: massive data theft at MasterCard processor 
No, it isn't! A handwritten signature is far better, it gives post-facto
evidence about who authorised the transaction - it is hard to fake a
signature so well that later analysis can't detect the forgery, and few
people would bother to do it that well anyway, while it is easy enough to
enter a PIN with "digital reproducibility".
Also there are several attacks on Chip n' PIN as deployed here in the UK,
starting with the fake reader attacks - for instance, a fake reader says you
are authorising a payment for $6.99 while in fact the card and PIN are being
used to authorise a transaction for $10,000 across the street. They get
quite complex, there's the double-dip, where the $6.99 transaction is also
made, and the delayed double dip, where a reader belonging to a crook makes
the $10,000 transaction several days later (the crook has to skip town with
the money in this attack - so far. Except of course he never existed in the
first place, and maybe ...).
Then there's probably a Bank-wide attack, where an expensive attack on one
card can break all the cards used by one bank - ouch! because the Banks
haven't actually issued cards that digitally sign the transaction (and it
would make little difference to many of the fake reader attacks if they
had), but just reuse one key or a key with an offset or XOR on the card to
generate a keyed hash of the transaction for authorisation.
There are some more classes of attacks too. It's a bit early to say about
many of them, but it looks like there are a goodly number of going-to-be
successful attacks.
This might not matter that much except to the banks, but the liability for
what appears to be a PIN-authorised transaction is being foisted off on the
cardholder, who has litle recourse to proof that he didn't make the
transaction when one of these attacks is made.
I don't have any Chip n' PIN cards, and I don't want any either. I'm
sticking with signatures.

@_date: 2005-03-29 08:45:20
@_author: Peter Fairbrother 
@_subject: aid worker stego 
I've been asked to advise an aid worker about stego. Potential major
government attacker.
I don't think there is much danger of severe torture, but I don't think
"innocent-until-proven-guilty" applies either, and suspicion should be
minimised or avoided.
I though about recommending Best/Drive -Crypt as they are supposedly
general-purpose encryption programs which can also do encrypted containers
which are undetectable, but I don't know if that's actually so, or which to
If he's using Windows will they clean up the temp and swap files?
An alternative is a stego program to hide data in eg images.  I don't know
which are the better ones now available, can anyone advise?
The other point is that the stego program itself will be visible on disk. Is
there a small stego program that you could eg hide in an image and somehow
bootstrap from something totally innocuous?
Any other ideas?

@_date: 2006-01-28 19:34:15
@_author: Peter Fairbrother 
@_subject: thoughts on one time pads 
Does a microwave oven do anything? I've been reading too much Tom Clancy ...
It does get rid of the stuff on the top, leaving a surface that a bit of
sanding would make irretrievable, and some flakes that could be burned
Another possibility might be to n-of-n [1] split the data up so you need to
have a whole disk rotation's worth in order to reconstruct any of it - that
might well make assured destruction a lot easier.
The repeatedly applied hammer would probably work well then, I doubt it's
that hard to destroy ~2^100 bits with a few blows to one track.
but the hot fiery furnace in the basement is probably still the best. :)
It used to be a fashion to have key signing parties when crypto people
gathered - and at several ones over the last few years I have seen CD's of
OTP data swapped instead. And DVD's are about the same price as CDs now.
I'm talking about the kind of careful people who get the message and do the
xor themselves, probably in shell script. No "applications".
They can easily change to using symmetric keys to save OTP material (using
some of the otp for the symmetric key) when large files are sent - "Here's
the porneo.mpg of Hillary Clinton [2], encrypted in AES with this key:
Often doubly encrypted, typically using both Blowfish and AES with different
keys, in case one of those ciphers has been covertly broken.
Hey, why not? It costs nothing.

@_date: 2006-06-07 19:38:23
@_author: Peter Fairbrother 
@_subject: Windows guru requested - Securing Windows  
Today the UK Home Office announced the public consultation on the Code of
Practice of Part 3 of RIPA. This is the first stage of the process by which
it can be brought into force. Part III of RIPA is the
law passed 6 years ago but not yet brought into force.
With the advent of GAK in the UK looking more and more likely, I am ramping
up the m-o-o-t project (  , but the website is
woefully out-of-date and the final form of the project may be rather
different to that described therein), which has been dormant for some time.
m-o-o-t's goal is to provide even the dumbest luser with the tools to avoid
and evade demands for keys in such a way that it is very hard for them to
mess up and to anything insecurely.
This will be either for free or at very low cost (might do something with a
USB stick which the user would have to buy, but not from us - software will
be free).
In a preliminary search for alternatives, I am seeking an answer to this
question. I know very little about Windows beyond that many people use it
and that source is not available, so be gentle with me please.
In an attempt to partially secure Windows for temporary use, ie when it's
being temporarily used in "secure mode", and to prevent data being stored in
softwarekeylogger, temp and swap files, would something like the following
be possible?
Bot from CD, create a memory FS, union mount it to the main windows fat-32
FS, with the fat-32 fs mounted read-only, boot Windows? That way any changes
to the files would be wiped out when the power was switched off, and the
fat-32 fs would remain untouched.
Mount a steganographic FS read/write on eg a USB key (or a different
partition) on / with a hard-to-guess name. Secret files should be saved to
this fs.

@_date: 2007-07-04 20:22:56
@_author: Peter Fairbrother 
@_subject: UK RIPA Pt 3 
The UK Home Office have just announced that they intend to bring the provisions of Pt 3 of the Regulation of Investigatory Powers Act 2000 into force on 1st October. This is the law that enables Policemen to demand keys to encrypted material, on pain of imprisonment, and without judicial approval of these demands.
There is one last Parliamentary process to go through, the approval of a code of practice, but as far as I know there has never been a case of one of these failing to pass - though a related one was withdrawn a few years ago. We will try to prevent it happening, the chances of success are against us but it is not impossible.
You are not required to keep keys indefinitely, or give up a key you don't have, but the rules regarding the assumption that you know a key at least partially reverse the normal burden of proof.
m-o-o-t will be there on the day. m-o-o-t is a freeware live CD containing OS and applications, including an ephemerally keyed messaging service, and a steganographic file system.
If anyone knows of any other technologies to defeat this coercive attack I would be glad to hear of them, and perhaps include them in m-o-o-t.

@_date: 2007-07-05 03:34:03
@_author: Peter Fairbrother 
@_subject: UK RIPA Pt 3 
I forgot to mention that Pt.3 also includes coercive demands for access keys - so for instance if Mr Bill Gates came to the UK, and if there was some existing question about Microsoft's behaviour in some perhaps current EU legal matter, Mr Gates could be required to give up the keys to the Microsoft internal US servers. Or go to jail.
Though I'd quite like to see that :), I don't think it would be entirely appropriate ...

@_date: 2010-03-23 15:29:40
@_author: Peter Fairbrother 
@_subject: Question regarding common modulus on elliptic curve cryptosystems 
Diffie-Hellman combined with Pohlig-Hellman can do what you describe.
It's a variation on  using DH (as a public key system rather than as a key agreement system) rather than El Gamal. If it's not obvious how to implement it ask offlist.
But I don't think that's what you need. PK is not the same thing as signatures. It's not "a commutative signing primitive".
That's not something which I've come across before, but maybe I could work one out ... I gather you need for the verifier to be unable to tell the order in which the signatures were applied? Can't think offhand of any other reason why you'd need one.
But I need to know exactly what you need.
Need coffee, it's raining .. dilemma, wet or unwoken?

@_date: 2013-10-01 18:39:11
@_author: Peter Fairbrother 
@_subject: [Cryptography] RSA equivalent key length/strength 
Agreed (though did you also check whether the supposed verification process actually matches the supposed generation process?).
Also agreed, NSA could not have reverse-engineered the parts of the generating process from "random" source to the curve's b component, ie they could not have started with a chosen b component and then generated the "random" source.
However they could easily have cherry-picked a result for b from trying several squillion source numbers. There is no real reason not to use something like the digits of pi as the source - which they did not do.
Also, the method by which the generators (and thus the actual groups in use, not the curves) were chosen is unclear.
Even assuming NSA tried their hardest to undermine the curve selection process, there is some doubt as to whether these two actual and easily verifiable failings in a supposedly "open" generation process are enough to make the final groups selected useful for NSA's nefarious purposes.
But there is a definite lack of clarity there.

@_date: 2013-10-01 21:13:01
@_author: Peter Fairbrother 
@_subject: [Cryptography] TLS2 
I have said this first part before:
Dan Boneh was talking at this years RSA cryptographers track about putting some sort of quantum-computer-resistant PK into browsers - maybe something like that should go into TLS2 as well?
We need to get the browser makers - Apple, Google, Microsoft, Mozilla - and the webservers - Apache, Microsoft, nginx - together and get them to agree "we must all implement this" before writing the RFC.
Also, the banks and the CA's should have an input. But not a say.
More rules:
IP-free, open source code,
no libraries (*all* functions internal to each suite)
a compiler which gives repeatable binary hashes so you can verify binary against source.
Note to Microsoft - open source does not always mean free. But in this case it must be free.
Maximum of four crypto suites.
Each suite has fixed algorithms, protocols, key and group sizes etc. Give them girls' names, not silly and incomplete crypto names - "This connection is protected by Alice".
Ability to add new suites as secure browser upgrade from browser supplier. ?New suites must be signed by working group?. Signed new suites must then be available immediately on all platforms, both browser and webserver.
Separate authentication and sessionkeysetup keys mandatory.
Maybe use existing X.509? but always for authentication only, never No client authentication. None. Zero.
That's too hard for an individual to manage - remembering passwords or whatever, yes, global authentication, no. That does not belong in TLS.
I specifically include this because the banks want it, now, in order to shift liability to their customers.
And as to passwords being near end-of-life? Rubbish. Keep the password database secure, give the user a username and only three password attempts, and all your GPUs and ASIC farms are worth nothing.

@_date: 2013-10-01 22:58:44
@_author: Peter Fairbrother 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
AES, the latest-and-greatest block cipher, comes in two main forms - AES-128 and AES-256.
AES-256 is supposed to have a brute force work factor of 2^256  - but we find that in fact it actually has a very similar work factor to that of AES-128, due to bad subkey scheduling.
Thing is, that bad subkey scheduling was introduced by NIST ... after Rijndael, which won the open block cipher competition with what seems to be all-the-way good scheduling, was transformed into AES by NIST.
So, why did NIST change the subkey scheduling?
I don't know.
Inquiring minds ...
NIST have previously changed cipher specs under NSA guidance, most famously for DES, with apparently good intentions then - but with NSA and it's two-faced mission, we always have to look at capabilities, not

@_date: 2013-10-07 02:13:26
@_author: Peter Fairbrother 
@_subject: [Cryptography] Sha3 
Seems the Keccac people take the position that Keccak is actually a way of creating hash functions, rather than a specific hash function - the created functions may be ridiculously strong, or far too weak.
It also seems NIST think a competition is a way of creating a hash function - rather than a way of competitively choosing one.
I didn't follow the competition, but I don't actually see anybody being right here. NIST is probably just being incompetent, not malicious, but their detractors have a point too.
The problem is that the competition was, or should have been, for a single [1] hash function, not for a way of creating hash functions - and in my opinion only a single actual hash function based on Keccak should have been allowed to enter.
I think that's what actually happened, and an actual function was entered. The Keccac people changed it a little between rounds, as is allowed, but by the final round the entries should all have been fixed in stone.
With that in mind, there is no way the hash which won the competition should be changed by NIST.
If NIST do start changing things - whatever the motive  - the benefits of openness and fairness of the competition are lost, as is the analysis done on the entries.
If NIST do start changing things, then nobody can say "SHA-3 was chosen by an open and fair competition".
And if that didn't happen, if a specific and well-defined hash was not entered, the competition was not open in the first place.
Now in the new SHA-4 competition TBA soon, an actual specific hash function based on Keccac may well be the winner - but then what is adopted will be what was actually entered.
The work done (for free!) by analysts during the competition will not be wasted on a changed specification.
[1] it should have been for a _single_ hash function, not two or 3 functions with different parameters. I know the two-security-level model is popular with NSA and the like, probably for historical "export" reasons, but it really doesn't make any sense for the consumer.
It is possible to make cryptography which we think is resistant to all possible/likely attacks. That is what the consumer wants and needs. One cryptography which he can trust in, resistant against both his baby sister and the NSA.
We can do that. In most cases that sort of cryptography doesn't take even measurable resources.
The sole and minimal benefit of having two functions (from a single family) - cheaper computation for low power devices, there are no other real benefits - is lost in the roar of the costs.
There is a case for having two or more systems - monocultures are brittle against failures, and like the Irish Potato Famine a single failure can be catastrophic - but two systems in the same family do not give the best protection against that.
The disadvantages of having two or more hash functions? For a start, people don't know what they are getting. They don't know how secure it will be - are you going to tell users whether they are using HASH_lite rather than HASH_strong every time? And expect them to understand that?
Second, most devices have to have different software for each function - and they have to be able to accept data and operations for more than one function as well, which opens up potential security holes.
I could go on, but I hope you get the point already.

@_date: 2013-10-07 02:13:22
@_author: Peter Fairbrother 
@_subject: [Cryptography] Sha3 
That may once have been mostly true, but no longer - now it's mostly false.
In almost every case nowadays the speed at which a device computes a SHA-3 hash doesn't matter at all. Devices are either way fast enough, or they can't use SHA-3 at all, whether or not it is made 50% faster.

@_date: 2013-09-06 02:19:43
@_author: Peter Fairbrother 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
BULLRUN seems to be just an overarching name for several wide programs to obtain plaintext of passively encrypted internet communications by many different methods.
While there seem to be many non-cryptographic attacks included in the BULLRUN program, of particular interest is the cryptographic attack mentioned in the Snowden papers and also hinted at in earlier US congressional manouverings for NSA funding.
The most obvious target of attack is some widespread implementation of SSL/TLS, and while it might just be an attack against a reduced keyspace, eg password-guessing or RNG compromise, I wonder whether NSA have actually made a big cryptographic break against some cipher, and if so, against what?
Candidate ciphers are:
and key establishment mechanisms:
I don't think a break in another cipher or KEM would be widespread enough to matter much. Assuming NSA (or possibly GCHQ) have made a big I don't think it's against 3DES or RC4, though the latter is used a lot more than people imagine.
AES? Maybe, but a break in AES would be a very big deal. I don't know whether hiding that would be politically acceptable.
RSA? Well, maybe indeed. Break even a few dozen RSA keys per month, and you get a goodly proportion of all internet encrypted traffic. It's just another advance on factorisation.
If you can break RSA you can probably break DH as well.
ECDH? Again quite possible, especially against the curves in use - but perhaps a more widespread break against ECDH is possible as well. The math says that it can be done starting with a given curve (though we don't know how to do it), and you only need to do the hard part once per My money? RSA.
But even so, double encrypting with two different ciphers (and using two different KEMs) seems a lot more respectable now.

@_date: 2013-09-06 19:33:25
@_author: Peter Fairbrother 
@_subject: [Cryptography] People should turn on PFS in TLS (was Re: Fwd: 
Any additional delay will be short - after all, if forward secrecy by ephemeral key setup (I hate the term PFS, there is nothing perfect about it) is not used then you have to use something else - usually RSA - For a desktop, laptop, or even a decent mobile the difference is not noticeable in practice if the server is fast enough.
However, while the case for forward secrecy is easy to make, implementing it may be a little dangerous - if NSA have broken ECDH then
using it only gives them plaintext they maybe didn't have before.
Personally, operating on the assumption that NSA have not made a crypto break is something I'm not prepared to do. I just don't know what that break is is. I think it's most likely RSA/DH or ECC, but could easily be I don't really care if the "break" is non-existent, irrelevant or disinformation - beefing up today's crypto is only hard in terms of getting people to choose a new updated crypto, and then getting people to implement it. This happens every so often anyway.
One point which has been mentioned, but perhaps not emphasised enough - if NSA have a secret backdoor into the main NIST ECC curves, then even if the fact of the backdoor was exposed - the method is pretty well known - without the secret constants no-one _else_ could break ECC.
So NSA could advocate the widespread use of ECC while still fulfilling their mission of protecting US gubbmint communications from enemies foreign and domestic. Just not from themselves.
Looking at timing, the FIPS 186-3 curves were introduced in July 2009 - the first hints that NSA had made a cryptanalytic break came in early to mid 2010.
I'm still leaning towards RSA, but ...

@_date: 2013-09-07 21:50:08
@_author: Peter Fairbrother 
@_subject: [Cryptography] In the face of "cooperative" end-points, 
But does it matter much? A cooperative endpoint can give plaintext no matter what encryption is used, not just session keys.
Okay, that might be a little harder to do in bulk - but perhaps not that much harder, depending on circumstances.
Maybe. Or maybe they have broken (the NIST curves for) ECDHE. Or maybe it's something else.
Whatever, I don't think they would be asking for $5.2 billion plus (for comparison, BULLRUN has an annual budget of $280 million) to spend on developing "advanced cryptanalytic capabilities" for which it is useful to "shape the worldwide cryptography marketplace to make it more tractable to" unless it was against some sort of key establishment mechanism in SSL/TLS.
I can't think of any other target which is worth that much money. Okay, maybe I'm ignoring the "never underestimate what the enemy is willing to spend" rule here, but..
Breaking a cipher like AES, 3DES or RC4 wouldn't give them nearly as much access to plaintext as breaking a KEM - they would have to break each ciphertext individually, whereas they would only need to break a KEM once.
And most of their interception is passive, they just listen - you generally need at least one plaintext/ciphertext pair to break a cipher and find a session key, and most often they don't have the plaintext, just the ciphertext.
You just need the right math (and/or maybe some input into curve choices) to break a PK KEM, and find *all* the session keys it is used for.
(the $5.2 billion figure is from a NSA request for additional congressional funding for "exciting new cryptanalytic capabilities" made a few years ago, and leaked by a congressman)

@_date: 2013-09-08 16:20:40
@_author: Peter Fairbrother 
@_subject: [Cryptography] A Likely Story! 
This is just a wild story, It isn't true. If we cryptographers found it was true we would all be totally gobsmacked.
The Beginning:
Sometime in 2008 the NSA - the United States National Security Agency, who employ many times more mathematicians than anyone else does - discovered a new mathematical way to factorise big numbers better.
It wasn't a huge advance, but it would be good enough for them to factorise several hundred 1024-bit-long numbers per month using some big computers they wanted to build.
In the form of RSA public keys, these 1024-bit numbers were (and sometimes still are) used to generate the session keys which encrypt and protect internet traffic.
A session key is the key which is used to encrypt the traffic between you and a website, using a normal cipher - it is a shared secret between you and the website.
Setting up a shared secret session key, when the communications used to set it up may also be intercepted, is quite difficult and involves considerable tricky math. That's where RSA and factorising comes in.
In 2008, when you saw a little padlock in your browser, the connection was almost always encrypted using a session key whose secrecy depends on the inability of anybody to factorise those 1024-bit RSA numbers.
They change every few years, but usually each big website only uses one RSA key per country  - so when the NSA factorised just one of those RSA keys it could easily find the session keys for all the internet sessions that website had made in that country for a couple of years.
Now the NSA had been collecting internet traffic for years, and when the big computers were built they would be able to see your past and present online banking, your secret medical history, the furlined handcuffs you bought online ..
The Dilemma:
So, did the NSA then go "Hooray, full steam ahead?" Not quite. The NSA has two somewhat conflicting missions: to be able to spy on people's communications, and to keep government communications secure.
On the one hand, if they continued to recommend that government people use 1024-bit RSA they could be accused of failing their mission to protect government communications.
On the other hand, if they told ordinary people not to use 1024-bit RSA, they could be accused of failing their mission to spy on people.
What to do?
Some Background:
Instead of using 1024-bit RSA to set up session keys, people could use a different way, called ECDHE. That stands for elliptic curve Diffie Hellman (ephemeral), the relevant bit here being "elliptic curve".
You can use any one of trillions of different elliptic curves,which should be chosen partly at random and partly so they are the right size and so on; but you can also start with some randomly-chosen numbers then work out a curve from those numbers. and you can use those random numbers to break the session key setup.
The other parts are: starting from the curve, you can't in practice find the numbers, it's beyond the capabilities of the computers we have. So those if you keep those random numbers you started with secret, only you can break the ECDHE mechanism. Nobody else can.
And the last part - it is convenient for everybody to use the same elliptic curve, or perhaps one or two curves for different purposes. So if you know the secret numbers for the curve, you can break everybody's key setup and get the secret session keys for all the traffic which uses those curves.
The Solution:
Make government people use ECDHE instead of RSA, but with the NSA's special backdoored elliptic curves. Ordinary people will follow suit.
This solves both problems - when people change to the new system the NSA can still break their internet sessions, and government communications are safe from other people (although the NSA can break US government communications easily - but hey, that's the price of doing business, and we're the NSA, right?).
Someone else might find the factoring improvement, but it is thought infeasible that someone else would be able to find the secret backdoor.
"Hooray, full steam ahead!"
That's the story.
The rest is just details - maybe the NSA somehow got NIST to put their special backdoored curves into NIST FIPS 186-3 recommendations in 2009, so people would use them rather than make up curves of their own - it is usual and convenient, but not strictly necessary, for ECDHE software to only be able too use a small selection of curves.
Maybe they asked the US Congress for several billion in extra funding in the 2010 budget to run the RSA-breakers.
Maybe they are building a new "data center" in Utah to use the session keys to decrypt the communications they have intercepted over the years.
Maybe they put those special backdoored curves into Suite B, their official requirements for US Government secret and top secret Or maybe they didn't. It's just a story, after all. The cryptography, while incomplete, is correct, and it may all seem plausible - but of course it isn't true.

@_date: 2013-09-09 17:39:01
@_author: Peter Fairbrother 
@_subject: [Cryptography] A Likely Story! 
Move along please, there is nothing to see here.
This is just a wild and disturbing story. It may upset you to read it, so please stop reading now.
You may have read a bit about the story in the papers or internet or elsewhere, but isn't actually true. Government Agencies do not try to break the internet's encryption, as used by Banks and Doctors and Commerce and Government Departments and even Government Agencies themselves - that wouldn't be sensible.
Besides which, there is no such agency as the NSA.
But ..
Take FIPS P-256 as an example. The only seed which has been published is s=  c49d3608 86e70493 6a6678e1 139d26b7 819f7e90 (the string they hashed and mashed in the process of deriving c).
I don't think they could reverse the perhaps rather overly-complicated hashing/mashing process, but they could certainly cherry-pick the s until they found one which gave a c which they could use.
c not being one of the usual parameters for an elliptic curve, I should explain that it was then used as c = a^3/b^2 mod p.
However the choice of p, r, a and G was not seeded, and the methods by which those were chosen are opaque.
I don't really know enough about ECC to say whether a perhaps cherry-picked c = a^3/b^2 mod p is enough that the resulting curve is secure against chosen curve attacks - but it does seem to me that there is a whole lot of legroom between a cherry-picked c and the final curve.
And as I said, it's only a story. We don't know much about what the NSA knows about chosen curve attacks, although we do know that they are possible. Don't go believing it, it will just upset you.
They wouldn't do that.

@_date: 2013-09-09 18:12:06
@_author: Peter Fairbrother 
@_subject: [Cryptography] Thoughts about keys 
I like to look at it the other way round, retrieving the correct name for a key.
You don't give someone your name, you give them an 80-bit key fingerprint. It looks something like m-NN4H-JS7Y-OTRH-GIRN. The m- is common to all, it just says this is one of that sort of hash.
There is only one to remember, your own.
The somebody uses the fingerprint in a semi-trusted (eg trusted not to give your email to spammers, but not trusted as far as giving the correct key goes) reverse lookup table, which is published and shared, and for which you write the entry and calculate the fingerprint by a long process to make say 20 bits more work.
Your entry would have your name, key, address, company, email address, twitter tag, facebook page, telephone number, photo, religious affiliation, claimed penis size, today's signed ephemeral DH or ECDHE keypart, and so on - whatever you want to put in it.
He then checks that you are someone he thinks you are, eg from the photo, checks the fingerprint, and if he wants to contact you he has already got your public key.
He cannot contact you without also getting your public key first - because you haven't given him your email address, just the hash.
[ That's what's planned for m-o-o-t (a CD-based live OS plus for secure-ish comms) anyway. As well, in m-o-o-t you can't contact anyone without checking the fingerprint, and you can't contact him in unencrypted form at all. Also the lookup uses a PIR system to avoid traffic analysis by lookup. It isn't available just now, so don't ask. ]

@_date: 2013-09-10 00:25:20
@_author: Peter Fairbrother 
@_subject: [Cryptography] [cryptography] SSH uses secp256/384r1 which has 
specified in SP800-90 for Dual EC DRBG!
I did not see that, and as far as I can tell there is no actual Also, the known possible subversion of DRBG did not involve curve selection, but selection of a point to be used in DRBG. I think Kristian G has posted about that.
As to elliptic curves, there are only two of significance, in terms of being widely used:  they are NIST P-256 and NIST P-384.
NIST P-224 is also occasionally used.
These are the same curves as the secp256/384r1 curves, and the same curves as almost any other 256-bit or 384-bit curves you might want to mention - eg the FIPS 186-3 curves, and so on.
These are all the same curves.
They all began in 1999 as the curves in the (NIST) RECOMMENDED ELLIPTIC CURVES FOR FEDERAL GOVERNMENT USE
The way they were selected is supposed to be pseudo-random based on SHA-1, though it's actually not quite like that (or not even close).
Full details, or at least all of the publicly available details about the curve selection process, are in the link, but as I wrote earlier:
"Take FIPS P-256 as an example. The only seed which has been published is s=  c49d3608 86e70493 6a6678e1 139d26b7 819f7e90 (the string they hashed and mashed in the process of deriving c).
I don't think they could reverse the perhaps rather overly-complicated hashing/mashing process, but they could certainly cherry-pick the s until they found one which gave a c which they could use.
c not being one of the usual parameters for an elliptic curve, I should explain that it was then used as c = a^3/b^2 mod p.
However the choice of p, r, a and G was not seeded, and the methods by which those were chosen are opaque.
I don't really know enough about ECC to say whether a perhaps cherry-picked c = a^3/b^2 mod p is enough to ensure that the resulting curve is secure against chosen curve attacks - but it does seem to me that there is a whole lot of wiggle room between a cherry-picked c and the final curve."

@_date: 2013-09-10 15:57:34
@_author: Peter Fairbrother 
@_subject: [Cryptography] What TLS ciphersuites are still OK? 
Yes - hard code them all to 1024-bit. Then dump TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 in the bin where it belongs.
Then replace it with a suite such as Would a non-cryptographer know what TLS_DHE2048_WITH_RSA2048_WITH_AES_128_GCM_SHA256 meant? No. So for heaven's sake call it Ben's_suite or something, with a nice logo or icon, not TLS_DHE2048_WITH_RSA2048_WITH_AES_128_GCM_SHA256.
They won't know what Ben's_suite means either, but they may trust you (or perhaps not, if you are still Working for Google ...)
The problem with TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 is that you don't know what you are getting.
[ The other problem is of course that the main browsers don't make it easy to find out which suite is actually in use ... :( ]
Hmmm, can a certificate have several keylengths to choose from? And, if the suite allows it, can a certificate have an RSA key for authentication and a different RSA key for session key setup (cf RIPA)?

@_date: 2013-09-10 18:08:26
@_author: Peter Fairbrother 
@_subject: [Cryptography] Thoughts about keys 
sorry, that should read "You don't give someone your address or telephone number". mea culpa. You can give them your name.
Yes - except of course you can have as many identities as you want. You create them yourself after all.
The only assurance given by the scheme is that if a person gave you a hash which he generated himself, and you match it with a string and that string matches what you know about the person (eg their name or photo), then no-one else can have MTM'd it.
(maybe the server returns two or three matches, as after a while there will be random birthday collisions. That's why you should check the string matches what you know about the person. But an attacker can't find a hash which matches a particular pre-chosen person by trying, it would take 2^100 work)
You can have one for business, one for pretty girls, one for ugly girls - you just have to remember them all (except maybe the one for ugly girls). Or you can write them down. Or put them on your business card.
The point is that for practical purposes the hash *is* your telephone number, and/or your email, and/or your facebook page - we just need to get everyone else to install the software to do the lookup, checking, translation etc automagically and behind the scenes in their telephones, browsers, email clients etc.
(this was originally designed only for use in a single semi-secure comms program suite - but I don't see why it couldn't be more widely used)
See above. It would take on average 2^79 operations each of which would require 2^20 work to find a matching hash, starting with a picture. Or even just starting with a name, or whatever.

@_date: 2013-09-10 18:09:14
@_author: Peter Fairbrother 
@_subject: [Cryptography] Squaring Zooko's triangle 
Sorry, I misspoke: you can of course give them your name, just not your telephone number or email address. You give them the hash instead of those.
And that will help an attacker how?
To use a hash to contact you Bob has to ask the semi-trusted server to find the hash and then return your matching input string - if he gets it wrong even in one place the server will return a different hash, or no hash at all.
Bob can't use a hash which doesn't match exactly.
Sound too restrictive? But Bob can't use a telephone number or email address which is wrong in one place, never mind four, either.
I was even thinking of using a 60-bit hash fingerprint (with a whole lot of extra work added, to make finding a matching tailored preimage about 2^100 or so total work), so a hash would look like s-NN4H-JS7Y-OTRH but I haven't convinced myself that that would work yet.
Mind you, I haven't ruled it out either. There is a flood attack, but it can be defeated by people paying a dollar to the server when they input a hash.

@_date: 2013-09-11 21:58:18
@_author: Peter Fairbrother 
@_subject: [Cryptography] Squaring Zooko's triangle 
That part is similar, though I go from 80 bits (actually 79.3 bits) to 100 bits ; and a GPG key fingerprint is similar too, though my mashes are shorter than either, in order to make them easy to input.
There is another difference, mashes are easy to write and input without error - the mash alphabet only has 31 characters; A-Z plus 0-9, but 0=O, 1=I=J=L, 2=Z, 5=S. If one of those is misread as another in the subset it doesn't matter when the mash is input. Capitalisation is also irrelevant.
However the main, big, huge difference is that a mash isn't just a hash of a public key - in fact as far as Alice, who doesn't understand public keys, is concerned:
It's just a secure VIOP number.
Maybe she needs an app to use the number on her iphone or googlephone. And another app to use it on her laptop or desktop - but the mash is your secure VOIP number.
Or it's a secure email address.
Or it's both.
Alice need not ever see the "real" voip IP address, or the real email address - and unless she's a cryptographer and hacker she simply won't be able to contact you without using strong authenticated end-to-end encryption - if the only address she has for you is your mash.
Contrast this with your proposal, or a PGP finger print. In order to use one of these, Alice has to have an email address or telephone number to begin with. She also has to find the key and compare it with the hash, in order to use it securely - but she can use the email address or telephone number without ever thinking about downloading or checking the public key.
That's just not possible is all you give out is mashes.
It's looking at the mash as an address, not as a public key or an adjunct to a public key service - which is why I think it's kind-of turning Zooko's Triangle on it's head (I had never heard of ZT before :( - but I know Zooko though, hi Zooko!).
Or maybe not, looking at the web I see ZT in several slightly different But it probably is turning the OP's problem - the napkin scribble - on it's head. You don't write your email and fingerprint on the napkin - just the mash.

@_date: 2013-09-14 16:53:38
@_author: Peter Fairbrother 
@_subject: [Cryptography] RSA equivalent key length/strength 
Recommendations are given herein as: symmetric_key_length -> recommended_equivalent_RSA_key_length, in bits.
Looking at Wikipedia,  I see:
"As of 2003 RSA Security claims that 1024-bit RSA keys are equivalent in strength to 80-bit symmetric keys, 2048-bit RSA keys to 112-bit symmetric keys and 3072-bit RSA keys to 128-bit symmetric keys. RSA claims that 1024-bit keys are likely to become crackable some time between 2006 and 2010 and that 2048-bit keys are sufficient until 2030. An RSA key length of 3072 bits should be used if security is required beyond 2030.[6]"
That page doesn't give any actual recommendations or long-term dates from RSA now. It gives the "traditional recommendations" 80 -> 1024 and 112 -> 2048, and a 2000 Lenstra/Verheul minimum commercial recommendation for 2010 of 78 -> 1369.
"NIST key management guidelines further suggest that 15360-bit RSA keys are equivalent in strength to 256-bit symmetric keys.[7]"
NIST also give the "traditional" recommendations, 80 -> 1024 and 112 -> 2048, plus 128 -> 3072, 192 -> 7680, 256 -> 15360.
I get that 1024 bits is about on the edge, about equivalent to 80 bits or a little less, and may be crackable either now or sometime soon.
But, I wonder, where do these longer equivalent figures come from?
I don't know, I'm just asking - and I chose Wikipedia because that's the general "wisdom".
Is this an area where NSA have "shaped the worldwide cryptography marketplace to make it more tractable to advanced cryptanalytic capabilities being developed by NSA/CSS", by perhaps greatly exaggerating the equivalent lengths?
And by emphasising the difficulty of using longer keys?
As I said, I do not know. I merely raise the possibility.
[ Personally, I recommend 1,536 bit RSA keys and DH primes for security to 2030, 2,048 if 1,536 is unavailable, 4,096 bits if paranoid/high value; and not using RSA at all for longer term security. I don't know whether someone will build that sort of quantum computer one day, but they might. ]

@_date: 2013-09-14 18:48:40
@_author: Peter Fairbrother 
@_subject: [Cryptography] RSA equivalent key length/strength 
Yes, some - but I don't believe that's enough. Historically, it would not have been (and wasn't) - it doesn't take account of algorithm I actually based the 1,536-bit figure on the old RSA factoring challenges, and how long it took to break them.
We are publicly at 768 bits now, and that's very expensive  - and, over the last twenty years the rate of public advance has been about 256 bits per decade.
So at that rate 1,536 bits would become possible but very expensive in 2043, and would still be impossible in 2030.
If 1,024 is possible but very expensive for NSA now, and 256 bits per decade is right, then 1,536 may just be on the verge of edging into possibility in 2030 - but I think progress is going to slow (unless they develop quantum computers).
We have already found many of the "easy-to-find" advances in theory.

@_date: 2013-09-23 18:35:12
@_author: Peter Fairbrother 
@_subject: [Cryptography] RSA equivalent key length/strength 
Lessee - a "forward secrecy solution" which either doesn't work now or won't work soon - so that it probably won't protect traffic made now for it's useful lifetime - versus - well, who said anything about theoretically perfect?
To hell with perfect. I won't even use the word when describing forward secrecy (unless it's an OTP).
If you just want a down-and-dirty 2048-bit FS solution which will work today, why not just have the websites sign a new RSA-2048 sub-certificate every day? Or every few hours? And delete the secret key, of course.
Forward secrecy doesn't have to be per-session.
Though frankly, I don't think ubiquitous 1024-bit FS without deployment of some software/RFC/standard is possible, and if so that deployment should also include a 2048-bit solution as well. And maybe 3072-bit and 4096-bit solutions too.
And please please please don't call them all the same thing - because they aren't.
But, the immediate question before the court of TLS now is - "do we recommend a 1024-bit FS solution?"
And I for one cannot say that you should. In fact I would be horrified if you did.

@_date: 2013-09-26 00:24:12
@_author: Peter Fairbrother 
@_subject: [Cryptography] RSA equivalent key length/strength 
No. No. No. Please, no? No. Nonononononono.
It's Summa (over i)  P_i.I_i where P_i is the protection provided to information i, and I_i is the importance of keeping information i Actually it's more complex than that, as the importance isn't a linear variable, and information isn't either - but there's a start.
Increasing i by increasing users may have little effect on the overall security, if the protecting the information they transmit isn't particularly valuable.
And saying that something is secure - which is what people who are not cryptographers think you are doing when you recommend that something - tends to increase I_i, the importance of the information to be protected.
And if the new system isn't secure against expensive attacks, then overall security may be lessened by it's introduction. Even if Users are I have about 30 internet passwords, only three of which are in any way important to me - those are the banking ones. I use a simple password for all the rest, because I don't much care if they are compromised.
But I use the same TLS for all these sites.
Now if that TLS is broken as far as likely attacks against the banks go, I care. I don't much care if it's secure against attacks against the other sites like my electricity and gas bills.
I might use TLS a lot more for non-banking sites, but I don't really require it to be secure for those. I do require it to be secure for banking.
And I'm sure that some people would like TLS to be secure against the NSA for, oh, let's say 10 years. Which 1024-bit DHE will not provide.
If you really want to recommend 1024-bit DHE, then call a spade a spade - for a start, it's EKS, ephemeral key setup. It doesn't offer much in the way of forward secrecy, and it offers nothing at all in the way of perfect forward secrecy.
It's a political stunt to perhaps make trawling attacks by NSA more expensive (in cases where the website has given NSA the master keys [*]) - but it may make targeted attacks by NSA cheaper and easier.
And in ten years NSA *will* be able to read all your 1024-bit DHE traffic, which it is storing right now against the day.
[*] does anyone else think it odd that the benefit of introducing 1024-bit DHE, as opposed to 2048-bit RSA, is only active when the webserver has given or will give NSA the keys? Just why is this being considered for recommendation?
Yes, stunt.

@_date: 2013-09-26 00:37:18
@_author: Peter Fairbrother 
@_subject: [Cryptography] forward-secrecy >=2048-bit in legacy 
A couple of points: all the big CAs will give you a new certificate with a new key for free (but revocation is your baby) - while it isn't something they do, can't they issue say two years worth of one-day certs for perhaps a little more than the price of a two-year cert?
In the UK we have a law called RIPA, part of which allows Plod to demand keys. They can demand keys used for encryption and for key setup - but they can't demand keys used only for authentication. I don't think they routinely demand keys from TLS/SSL webservers.
The point is that in an ordinary TLS session the RSA key is used for both secrecy and authentication - in any future TLS these functions should be split.
Also, Dan Boneh was talking at this years RSA cryptographers track about putting some sort of quantum-computer-resistant PK into browsers - maybe something like that should go into TLS2 as well?
You need to get the browser makers - Apple, Google, Microsoft, Mozilla - and the webservers - Apache, Microsoft, nginx - together and get them to agree "we must all implement this" before writing the RFC.

@_date: 2013-09-30 21:31:09
@_author: Peter Fairbrother 
@_subject: [Cryptography] RSA equivalent key length/strength 
Given that mostly security works (or it should), what's really important is where that security fails - and "good enough" security can drive out excellent security.
We can easily have excellent security in TLS (mk 2?) - the crypto part of TLS can be unbreakable, code to follow (hah!) - but 1024-bit DHE isn't say unbreakable for 10 years, far less for a lifetime.
We are only talking about security against an NSA-level opponent here. Is that significant?
Eg, Tor isn't robust against NSA-level opponents. Is OTR?
No, and you don't know how important your opponent thinks the information is either, and therefore what resources he might be willing or able to spend to get access to it - but we can make some crypto which (we think) is unbreakable.
No matter who or what resources, unbreakable. You can rely on the math.
And it doesn't usually cost any more than we are willing to pay - heck, the price is usually lost in the noise.
Zero crypto (theory) failures.
Ok, real-world systems won't ever meet that standard - but please don't hobble them with failure before they start trying.
Do you mean I-i's?
Ah, average, Which average might that be? Hmmm, independent distributions of two variables - are you going to average them, then multiply the averages?
That approximation doesn't actually work very well, mathematically speaking - as I'm sure you know.
I totally agree with zero configuration - and best fit - but you are missing the main point.
Would 1024-bit DHE give a reasonable expectation of say, ten years unbreakable by NSA?
If not, and Manning or Snowden wanted to use TLS, they would likely be Incidentally, would OTR pass that test?

@_date: 2014-04-04 23:44:17
@_author: Peter Fairbrother 
@_subject: [Cryptography] Dark Mail Alliance specs? 
Hi Jon,
Could we have a glimpse at the slides? and maybe some helpful blurb?

@_date: 2014-04-15 21:46:55
@_author: Peter Fairbrother 
@_subject: [Cryptography] I  don't get it. 
I  don't get it.
Apple has an unreachable code goto, and it hurts security. OpenSSL has a bounds check failure, and it hurts security (and even OpenBSD gets another hole).
(no, I am not a fan of Theo's security stance - but it has been better that most)
But as far as I can see, almost all of the big holes in the last ten years could have been caught by good code checkers.
I wonder who committed the OpenSSL heartbeat change.
I wonder whether anyone at all at OpenSSL thought about whether it would be more secure *not* to implement the heartbeat option in the first place. Or even if someone at any time thought about that.
Perhaps most of all, I wonder whether it would be a good idea to shoot all the gcc developers.
I am no expert in bugs, but it seems to me that about 99% of the reported security bugs and holes and so-on could be solved by having a secure checking compiler. Which checked for most of the known holes, or perhaps just even the top five.
A long long time ago, about 2002, I asked Ben Laurie for some advice about co-writers for security software. Amongst other things he said "don't let them us C++ - it's too powerful". The specific advice may have been meant only for me in my situation, but it contains a basic truth - languages are, or can be, too powerful.
So, perhaps a prechecker before the code goes to the compiler? To check security code (like OpenSSL) for the top five or ten holes?
Or best of all, I think we need better compilers. Better in the sense that they will only compile secure code. And which can prevent coders from doing bad things.
Coders are not gods, and it isn't illegal for a compiler to say "you can't do that".

@_date: 2014-04-17 13:10:24
@_author: Peter Fairbrother 
@_subject: [Cryptography] I  don't get it. 
Well that caused no end of kerfuffle, both on and off list.
Obviously I didn't mean it literally, if for no other reason that it wouldn't do any good - C and gcc are now far too well established.
But, suppose you could go back in time, and adjust C and gcc, preferably without assassinations. What would you change?
Well first of all, strings and arrays. I don't want a bounds-unchecked string or array, not ever - even if bounds checking is resource-consuming and there is no possibility of out-of-range inputs, someone else might come along later and change the code.
I don't want to have to even think about whether a string or array is bounds checked; just whether the default behaviour when an out-of-range input is detected is suitable.
So, where would you change it? K+R? ANSI? I don't know, but in 35 years they never got around to fixing it. First there was strncpy etc, which didn't quite fix strcpy, Then there was strlcpy (on some systems) which didn't quite fix it either.
More important, when they introduced strncpy they *left strcopy in*. If they had meant to do it properly they would have at least deprecated and eventually removed strcpy, but they didn't, they left it "up to the So it's no surprise we get late-night unchecked-bounds bugs like heartbleed.
(BTW my preferred fix would be dynamic length strings and automatic array checking as default, with automatic string length checking and dynamic array sizing as alternatives. It can't be that hard, they do dynamic lengths OK with files - why not with strings?)
Malloc and free? Only in kernels and kexts, maybe drivers at a pinch.
Another thing I'd like to see as a (mandatory) goal would be repeatable compilation, so if you used the same source, compiler and compiler settings then you got the same object code out.
Any other suggestions?

@_date: 2014-04-18 13:53:06
@_author: Peter Fairbrother 
@_subject: [Cryptography] I  don't get it. 
That would be called bounds-checked strings and arrays. And no non-bounds-checked strings and arrays.
And 99% of the time bounds-checked strings and arrays would do the job just fine. But C programmers don't use them 99% of the time - in fact they don't use them at all, because mostly they don't exist - so they might as well use the sharp malloc/memcpy as the almost equally sharp string/array commands
You know why I titled this thread "I don't get it"? Because I really don't understand why no-one has fixed this. It has been a known problem for 30 plus years, and it is responsible for well over half of all the known security bugs.
So why haven't they fixed it?

@_date: 2014-04-18 13:53:02
@_author: Peter Fairbrother 
@_subject: [Cryptography] bounded pointers in C 
I can't see the point in trying to get away from the necessity of source modification - the idea after all is to have all strings and arrays and so on bounds checked, but if a programmer uses malloc and memcpy (the smoking gnu in the Heartbleed bug) instead of a declaration of a string and a bounded strcpy, then all bet are off anyway.
Now if C had proper bounded pointers for strings and arrays, then using malloc and memcpy would throw out a big flag - why on Earth are you using this dangerous stuff instead of the nice safe string/array commands? - but as C doesn't have nice safe string/array commands ...

@_date: 2014-04-18 19:36:38
@_author: Peter Fairbrother 
@_subject: [Cryptography] bounded pointers in C 
I think, in order to prevent C programmers, and all the old C code, from using the same old unchecked shit in all it's forms, we actually do have to make incompatible changes.
It is not just required, it is a requirement; so that at least all that old code will be put through a code checker.
It's 30 years overdue for the standard everyday implementations of strings and arrays to be bounds-checked.
But you can't do only that nowadays, as people use malloc, memcpy etc to manipulate what are really strings and arrays, perhaps because they are more convenient or give better performance, or perhaps because that's just what they are used to.
And all those other unbounds-checked options have to be checked out too.
Now that's not to say that we can't simply replace the str,,, where they have been used - but that is not enough.

@_date: 2014-04-18 21:54:02
@_author: Peter Fairbrother 
@_subject: [Cryptography] It's all K&R's fault 
No, I'd argue that you *can't* use it reasonably.
You write code, you are a genius, and your code is perfect. Then some non-genius comes along to maintain your code, and because you haven't used safe code constructs he makes a mistake.
But he's not a genius, and he isn't supposed to be one.
Assuming you had a choice of whether to use the constructs that only geniuses can use safely or the safer constructs which more normal coders can use safely, and then assuming that you knew or could presume that your code would be maintained by non-geniuses - then the fault would be yours, for choosing the wrong constructs.
Or perhaps before that, for choosing the wrong language, C.
So why didn't you, or they, object? I still don't get it.
There have been eleventy-million kinds of whining about this here, and twenty-leven-million mostly-non-fixes, but who has said, or better insisted, that eg strcpy() etc should be permanently removed from C, and replaced with a bounds-checking version?
Because *that* is what is needed. Nothing else will do.

@_date: 2014-01-03 18:34:35
@_author: Peter Fairbrother 
@_subject: [Cryptography] nuclear arming codes 
One technique I'm told is still in use is to affix glitter in random patterns in clear epoxy to missile and/or warhead parts, then shine lights on it from variable positions and compare the return sparkle images to a known set for that patch. Very hard to forge.
Typically used to uniquely identify missile parts with a view to counting them, with a friendly guy shining the lights in situ, in some circumstances it can also be used with an enemy guy shining the lights in a remote challenge-response fashion to ensure that the patch is still intact - eg break the glass substrate which locks the core in its silo and you break the patch.

@_date: 2014-01-16 12:48:21
@_author: Peter Fairbrother 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
Also, while we don't have details of the contract, RSA have not denied the initial charge - that they were paid $10 million to include something as their default. Doesn't matter what.
That's not what they are supposed to be paid for - they are supposed to be paid to protect their customers secrets.
And accepting money to do something which might detract from that - heck, something which could have no other likely reason for the payment than being a backdoor - if there is another possible reason for NSA making the payment I haven't heard about or thought of it - is fraud, pure and simple.
It's probably some other crimes as well.
A few years ago I was overwhelmed to see Whit Diffie, Ron Rivest and Adi Shamir at a RSA conference. Today - not so. I'd like to see them all refuse to attend.
I'd also like to see a permanent boycott, not just for one year - forever, destroy RSA, both the company (make EMC sell it, to wither) and the conference - which would at least generate another conference of standing similar to what RSA once had.
Hmmm, re moral rights in copyright - could one of R, S or A prevent EMC from using their initials?

@_date: 2014-07-26 21:04:10
@_author: Peter Fairbrother 
@_subject: [Cryptography] propaganda on "hurdles for law enforcement" 
There is a another, different hypothesis - that the lump of data is the same lump of data, possibly re-encrypted, as another lump of data somewhere else.
Perhaps we need a new definition of (pseudo-) random for that situation.
On a personal note, I have been struggling with this idea, in terms of cover traffic, for the last 9 or 10 years - but I haven't gotten anywhere much beyond the obvious, nothing noticeably brilliant :(
IMO. the whole subject of cover traffic needs to be investigated much further, and with rigor.
Take, as an example, a steganographic filing system where the files are kept in a public cloud, and it is easy for an observer to see when encrypted files are stored and recovered.
For simplicity, assume all files are the same size (padding, concatenation of associated files, etc).
In the cloud there will be real files which contain real data, and cover files which do not. The files, real or cover, may be re-encrypted from time to time, and perhaps returned to the cloud under different To an observer the operation of re-encryption should be the indistinguishable from the operations of writing, modifying, or deleting [1] a file.
In one instance of this there are multiple copies of real files and cover traffic files, so that an observer who can see traffic cannot imediately identify which files are real and which are not. The user may also keep a local pool of files to make real-world timimg information less useful.
The problem now is the pattern of file accesses. If the user loads ten files every time he starts a session, it is much harder to hide the real files than if he does not.
Ideally, file accesses should look random - but this is not practicable, and if we don't want the user to have to wait days to open a file, it is perhaps not even possible.
Now comes the only smart part I have found:
Somewhat easier ( or perhaps a _lot_ harder ) than generating lots and lots of random traffic, the covertraffic generator might generate lesser amountys of *suspicious but bogus* patterns of file accesses.
That can perhaps lessen bandwidth, but it ain't easy.
[1] though the encrypted file will always, always be available ...

@_date: 2014-07-27 07:12:53
@_author: Peter Fairbrother 
@_subject: [Cryptography] Directed Covertraffic was: propaganda on "hurdles 
That's fine, if you have the circumstances and resources.
But suppose that a OTP is not possible, perhaps for difficulty of key exchange reasons. An attacker might well want to find out whether a pre-arranged real random string (we assume the attacker knows the string), whose presence sets off the bombs, was sent.
But it is the string which sets off the bombs!!!, and the sender does not want to be caught, so he can't send it in clear. The sender might encrypt the sequence with a nondeterministic encryption, and then it may be super-encrypted for the link; the sender may not want the link operator to know what was sent - even though it is just a random string.
The point here is that even though the string is random, it is significant, it has a real-world meaning derived from context which is not related to it's Shannon etc entropies.
I think we need to make that clear, this string is different from any other random string of the same size - even though it is a real random To go back a bit, let's also suppose there are bandwidth restrictions, so you can only send say 100 packets per day. Further suppose you need to send say 20 real packets per day and, as these are urgent, you have to send a packet at very short notice, in a short time interval, say 10 seconds. Obviously, you can't send a packet every ten seconds.
Now suppose an attacker observes or causes an event - the attacker wants to know whether the system needs to respond to the stimulus. How do you hide that behaviour?
One technique which might help would be to respond to any stimulus, whether the system needs to or not.
That is an example of what I call directed covertraffic - it doesn't cover everything with a constant random flow (perhaps because we can't do that, or it's too expensive), but fake traffic created to cover only specific aspects of the real comms flow on the channel.
Another example of directed covertraffic would be where you want to hide some suspicious activity - you make fake patterns of packets (or whatever is observable by an attacker) which look like suspicious activity. Done properly, the attacker can't tell which is real.
Unfortunately I haven't gotten much further in developing a theory of directed covertraffic - well, a little, but not as far as I would like. Einstein once said "I need more maths" - I know how he felt.
(no, I am not comparing myself to Einstein!)

@_date: 2014-06-15 10:28:09
@_author: Peter Fairbrother 
@_subject: [Cryptography] End-to-End, One-to-Many, Encryption Question 
>
Here lies a greater problem - the secure deletion of data once stored in a cloud is practically impossible. The user does not know how many copies have been made, so he can't be sure they have all been deleted.
I say practically impossible rather than actually impossible only because a user can upload ciphertext only, and not upload the key, and on deletion of the key that deletion can be secure (for some value of "secure"). But then if eg you want perfect security and use an OTP, is the data actually stored in the cloud?

@_date: 2014-06-15 19:29:02
@_author: Peter Fairbrother 
@_subject: [Cryptography] End-to-End, One-to-Many, Encryption Question 
All symmetric block ciphers (eg otp is a group, but it does not have a
mitm attack) - but a block cipher whose keys formed a group would be
very useful, and doubling key size will usually defeat that attack, and
can often be done at minimal or no cost.
The easiest group block cipher I know of is Pohlig-Hellman, a simple
modular exponentiation cipher, which can be done in either the group Z_p
or the group of quadratic residues mod p - but the computational
requirements are large.
Actually it is not usually necessary to double key size, the storage
requirements of the meet-in-the-middle attack can be very large, eg 2^64
for a 128-bit key - it is not really necessary to use a 256-bit key,
2^128 fast storage is pretty well unthinkable.
If any cipher designers are listening, can we have a secure group block
cipher with less work please? It won't be as famous as AES, but it will
get used, unlike most ciphers you design.

@_date: 2014-06-26 13:08:06
@_author: Peter Fairbrother 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
No. Not only no, but - we have a language problem here. You seem to be using the word protocol for something other than a defined single way in which cryptographic items interact.
Of course, for loosely-defined use of the word "protocols", that "defined single way" may frequently involve eg a choice of ciphers, a choice of key exchange mechanisms, a choice of encryption and/ or authentication methods and/or methodologies, even a choice of whether to encrypt or not -
So first of all, TCPcrypt is not a protocol, in the strict sense of a defined single way in which cryptographic items interact.
Second, it is my belief that we as cryptographers have a duty to ensure that even the luser know-nothing know-it-all annoying helpline clogging masturbating useless bastards' communications are actually secure. All of them. Always.
So we should have a strict protocol called simply eg Alice. Which defines the cipher to be used, the key establishment mechanism, the authentication methods etc - as the best we can do.
And if it gets broken, even a little, we say so, and say sorry, and replace it with Bob. Alice is broken, sigh, long live Bob!
BTW, I'm not suggesting only one such strict protocol. But each of them should have names, and absolutely *everything* should be strictly defined, no alternatives, before the name is first used.
I can't see any better, on indeed any other, way for us to perform our duty.
Which includes confirming the expectation of those lusers that we will actually provide them with a secure cryptographic solution. A solution which is always secure, against everything, from kid sisters to the And that if we don't, if we messed up, we will say so - and try to do better next time.
As I think you might guess, no. Agile because the server might enforce a better cipher? but maybe the server is pwned ..
Or maybe so the server can just enforce the use of a worse cipher?
We don't, and can't expect the luser to make the right choice -- beyond occasionally perhaps choosing to use our software.

@_date: 2014-06-27 14:31:33
@_author: Peter Fairbrother 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
For a keyed hash, maybe [1] - but for a cipher, how do you decrypt?
[1] though not necessarily - both inputs for the XOR are derived from the same plaintext. It's the same problem for a cascaded cipher - it is highly likely that the cascade is stronger than either cipher, but not necessarily so.
See M. Maurer and J. L. Massey, Cascade ciphers: The importance of being first, Journal of Cryptology, vol. 6, no. 1, pp. 55?61, 1993.

@_date: 2014-06-27 14:32:06
@_author: Peter Fairbrother 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
I was almost convinced, for a moment. Two or maybe three suites, only SHALLs allowed, so there is no question of whether a suite is installed or fit for purpose - sounds good.
But who decides when to stop using an algorithm suite?  The luser client? The boss server?
It's certainly not the cryptologist.

@_date: 2014-06-27 18:53:21
@_author: Peter Fairbrother 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
As I said: It's certainly not the cryptologist who decides that the use of a suite is insecure, and therefore disallowed.
[ Hmmm, an aside - a point of naming. We have algorithms like ciphers and modes - we have protocols like key establishment methods and packet contents, and so on - we have suites which combine algorithms with protocols to give something like TLS_RSA_WITH_3DES_EDE_CBC_SHA:  and we have something, like SSL/TLS which I don't know a good name for, which is maybe a metaprotocol, or even a language - any suggestions? ]
But if there is only one suite, and it's called Alice, and the crypto community says "Alice is broken", then the luser can install Bob and get an annoying notification when he can only use Alice.
And, with a bit of psychology, he will blame the servers which still use Alice rather than blaming Bob - and the servers will quickly change to using Bob.
Well, maybe.
But at least the luser will have some idea of what is going on.
{yes, we have responsibilities to the servers too. That's a different issue though}

@_date: 2014-03-25 22:26:12
@_author: Peter Fairbrother 
@_subject: [Cryptography] Dark Mail Alliance specs? 
Does anyone have any more info on DarkMail?
Depends. Some thoughts on what might work in ten years or less:
Improved Email:
1) to eventually get a majority of all email sent end-to-end encrypted to a minimum security standard, such that active measures are needed to intercept and read it.
2) to be usable in a highly secure manner if and when that is required.
3) to resist demands for decryptions and for keys.
4) to be as future-proof as possible.
5) we think anonymity is not immediately practicable.
In order to achieve these objectives the following requirements must be met:
** Legal requirements:
To be entirely open source and free, as in a BSD or similar license.
    If not then Wassenaar and/or other crypto export requirements may apply, defeating objective 1)
** Software requirements:
A] be as widely compatible as possible, so that many people will use it
B] be easy to use, indeed almost transparent to the user, so that many people will use it
C) well-developed email reader and webmail clients are essential.
    If they aren't consumer-grade consumers won't use them;  writing non-consumer-grade clients is just a waste of time and effort.
D) must be compatible with normal email, but default to encrypted mode
E) must be cheap to install and operate, and not require normal email servers to do anything
** Cryptographic and security requirements:
a) an automatic key server, probably distributed/shared. We don't want the user to have to do anything to obtain a relevant key, otherwise he may not bother [1]
b) ephemeral keys, signature-only keys [2]
c) high-secure mode must look the same as low-secure mode to an attacker
d) clear distinction to the user between security modes in use
[1] A suggestion, a distributed key service.
Each keyserver accepts keys from (and generated by) users, sends them a confirm message to the email address attached to the key, then on receipt of the (signed) confirm reply adds the key to the shared list.
Each shared list entry consists of: email address, server, date added, key. The list is hashed and updated between servers a bit like the bitcoin list (which might also pay for the key servers, eg the right to send spam).
When a user wants to send an email he contacts his list server, the recipient's list server, and another list server chosen at random and asks each for the key. The recipient's key server also replies with a signed-by-the-recipient ephemeral key as well as the recipient's key.
If there is only one key for the email address, and the three responses match, and the sender's own copy of the recipient's key (if he has one) all match, then he uses the signed and dated ephemeral key provided by the recipients key server.
The replies from the servers are all signed, so if they don't match we want to know why - the replies can then be published, so if a server cheats then it can be found out and shamed.
There is a little more, eg when there is no key or more than one key attached to a single email address, but that's basically how to find a new correspondent's key from his email address.
Note that the key servers are separate from the email servers which just work in the normal way.
Though perhaps the security level of the key server isn't that great, it's better than nothing. If the user wants better key security he can get it in many ways, eg by sharing keys in person, which can be displayed in a menu somewhere onscreen.
[2] ephemeral keys for resistance to subject-matter-key demands, signature-only keys to prevent legal demands for keys which authenticate the ephemeral keys.
Ephemeral keys are updated automatically, the user should need to make no input to update the keys.

@_date: 2014-03-25 23:21:41
@_author: Peter Fairbrother 
@_subject: [Cryptography] Dark Mail Alliance specs? 
They use "cloud" and "security" in one _name_?  Wow.

@_date: 2014-03-26 01:14:13
@_author: Peter Fairbrother 
@_subject: [Cryptography] Dark Mail Alliance specs? 
The moderator refused my last post and I'm trying to rewrite it, but I can't help but feel - WTF?  S/MIME vs PGP?
PGP sucks. Sorry, but it sucks in usability terms. What is it supposed
to do? Make encrypted email widely usable?  does it do that?
Is S/MIME any better? Phuleeze. No.
Now that I have offended everybody, perhaps we can start again, with the idea that lusers are our target users?
We need good marketing. That's learner users (or lazy users), for a start.
[as an aside, look at what RANK-XEROX did when they developed the WIMP
GUI. They looked at how people actually used the programs. If we don't
do that sort of thing ...]
So the budget should include $2,000,000 or more for marketing. Good
marketing people are expensive, far more so than good people like us...
As to PGP and S/MIME, expunge both from your existence, and you will be a much happier man. Neither is actually worth bothering about.
They have already failed to do the job.
Such as?
ps, giving 14 pages of links isn't really a good idea. One or two perhaps, but 14? I for one am not going to look at all of them, and I very much doubt that anyone else is going to.
--Peter Fairbrother

@_date: 2014-03-26 23:05:03
@_author: Peter Fairbrother 
@_subject: [Cryptography] Dark Mail Alliance specs? 
I agree, especially with the comments and vulnerabilities you make below, and there are also issues with malware and spam detection.
However, I am of the firm opinion that if it isn't compatible with ordinary email then it won't get widely adopted, full stop.
So the only option, as I see it, is to make it interoperable, but to also close those holes - which may take some doing, especially the sync The rest is bad, but doable - eg attachments and clickable links are sent in a single chunk to the browser, which has no access to the contents of any other emails.
The browser may be insecure - but the email client need not be. The idea here is not to make the system securer, just to make email as secure as Not all messages need a very high level of security - the functionality of those that do need that level of security can be limited.
I'd like to repeat my first two objectives:
1) to eventually get a majority of all email sent end-to-end encrypted to a minimum security standard, such that active measures are needed to intercept and read it.
2) to be usable in a highly secure manner if and when that is required.
Now S/MIME fails to do the first, and PGP fails to do the second [1]  -- mostly or solely simply because very few people implement and use them.
[1] except in very limited cases, but eg read about Snowden trying to get the journalists to use PGP
--Peter Fairbrother

@_date: 2014-03-29 17:47:23
@_author: Peter Fairbrother 
@_subject: [Cryptography] OpenPGP and trust 
who to?
over the clear-text link, so the computer can
who can send instructions?
and so on.
I think you should maybe forget the crypto bit, and just write down what you want to do, in plain language, as clearly as you can.
Ignore how you intend to do it, just write down what you want to do - I don't want to see the words signature, authentication anywhere in it. Are there users? What can they do? What can't they do? Are there administrators? what can they do? is there anything they can't do? is there a super-root?
If nothing else, it will make it clear in your mind - but more likely, you will then see that there is an obvious way to do what you want to do.
(  OT when I design an electronic device, almost the last thing I do is design the circuit. First I decide what it is supposed to do, then I design the human interface and the front panel, then the power supply and the case - the actual detailed circuit comes last of all.
Now all through this I have been keeping the circuit in mind - what sort of things can be done with electronics? there is no point in designing something which can't work. Does it need a gazillion controls? then a few knobs won't do, and maybe a graphical and/or touchscreen interface, or a full-blown immersive simulator, is necessary. How big is the power supply and circuit going to be? it has to fit in the case.
After that I have a good idea of what the circuit is supposed to do, and actually designing it is so straightforward that mostly it's just boring.
A similar design methodology should be used for security products (and software products).  Start with the purpose of the product, then the human and electronic interfaces, then the hardware and last of all the detailed crypto or code. Oh yes, you think about the crypto all the way through, but only in terms of what is possible and what resources it will take - worry about the detailed crypto (or code) last  )

@_date: 2014-03-29 18:03:10
@_author: Peter Fairbrother 
@_subject: [Cryptography] Dark Mail Alliance specs? 
Or by work.
But the first objective is:  "1) to eventually get a majority of all email sent end-to-end encrypted to a minimum security standard".
Older folks, and especially businesses, just plain ain't going to give up on email.
A different objective might be "to get a majority of asynchronous internet communications sent in end-to-end encrypted form", and while that's one I'd support, it's a much bigger bite.
Note that of the social media systems you mention, only a small proportion of skype calls are end-to-end encrypted (by a secret proprietary system), and even that may be changing or have already changed where you are, Gubbmint wants to at least be able to see Skype Here in the UK there have recently been calls to change the law to make interception of the other media you mentioned easier by installing "deep packet inspection" boxes, controlled by GCHQ, to most internet lines..
Yup. However end-to-end encryption does increase cost, if only a little -  so perhaps encrypted message may be seen as more valuable than unencrypted messages.

@_date: 2014-05-30 02:01:26
@_author: Peter Fairbrother 
@_subject: [Cryptography] What is going on with TrueCrypt? 
I discard outright any possibility of it being an outside website hack - too hard, an attacker would need access to the TC website, the Sourceforge TC site, and to the code signing key.
The "Warrant Canary" theory doesn't seem to make a whole lot of sense either. It's possible, but why recommend BitLocker? When did someone have time to write all those code changes?
The theory which makes most sense to me is that it was an at least partly commercially-motivated self-takedown by the devs.
The recent change in name on the otherwise "same old code and binary signing key" is possibly significant here - the developers, or perhaps just some of them, may want to start up a commercial product in the new The devs' commercial aspirations are well-known, witness the previous license issues, the failed crowdfunding and donations campaigns, the "TrueCrypt Developers LLC" registered in Nevada (thanks to Piergiorgio Sartor for that info). And they already own a good chunk of the the IP rights in the TrueCrypt source.
The ending of the project was graceful, to some extent at least - people were not left with unrecoverable archives, and temporarily acceptable but not-as-good alternatives were suggested. A whole lot of work went into that.
It is obvious that this wasn't done in the heat of the moment - it must have taken at least several weeks to do the code revisions for the 7.2 release. There have also been hints (eg the robots.txt file) for some months that something might be happening.
The only reason I can think of for doing all that work is maintaining reputation (or technical reputation at least - TrueCrypt devs are not exactly known for being people people, or for being particularly into "free open source" either).
No reasons why the code is/may be broken are given. Actually the "WARNING: Using TrueCrypt is not secure as it may contain unfixed security issues" does not even actually say TrueCrypt is broken, just that it may be.
And any unfixed issues might be fixed later, in the commercial version.
( Which would have been independently audited... at no cost to TrueCrypt... )
just a theory,

@_date: 2014-05-31 05:30:18
@_author: Peter Fairbrother 
@_subject: [Cryptography] What is going on with TrueCrypt? 
cc'd to list, hope you don't mind
I agree - but I don't think the TC devs think that way.
I think they wanted to make some money out of TC, perhaps initially by selling it to Microsoft, but failed to do so - most recently by a crowdsourcing plan and a contributions campaign.
I think they have come to the conclusion that that TC won't make them any money as-is, and that's mainly why they pulled the plug.
Some of the devs may be hoping to produce a commercial version, and some of them may just be fed up of developing it for free.
TC has never been a completely free program, it has always had a tinge of half-hearted and cack-handed commercial ambition and possessiveness to it - witness the license problems, and the present apparent unwillingness to allow/support a fork.
BTW, I don't think the devs have the business ability to make a success of a commercial version. I don't think it is impossible, but I don't think the devs could do it.
Unless of course
TrueCrypt is
A_s it may contain unfixed security issues"

@_date: 2014-10-11 00:00:22
@_author: Peter Fairbrother 
@_subject: [Cryptography] Sonic.net implements DNSSEC, 
Being a Brit I know very little about US law, but in UK and EU law common carrier status isn't something that an ISP either does or does not have.
If a person (eg an ISP) is acting, in a particular case, only as a carrier of information for other people's data, then they may have common carrier status in that particular case; which is a defence against many civil actions and criminal charges, ranging from treason to copyright violation to libel or slander.
It is like they are saying that they are not responsible for the content of what they transmit, as they just carried it - just like the post office is not responsible for threats against the president or fruits of treason which are carried in the mail.
Roughly implicit in that is the idea that the person did not know what the content was, or that it was unlawful - but only roughly, not necessarily. Perhaps more implicit, but again not always necessarily so, is the idea that they must not discriminate, ie they must carry comms from anyone to anyone (as long as they get paid).
Persons may have to comply with other legislation in order to retain their common carrier status, and thus their immunity from civil and criminal liability - for instance, youtube must respond in timely fashion to DCMA takedown requests.
In most cases, ISPs do have common carrier status, and they value it In US statutory law common carrier status gives an ISP immunity to liability for copyright violations in third party content (DMCA), and against action for libel or slander in third party content (Communications Decency Act). The other immunities I mentioned are a mix of statutory and common law.
In the EU at least ISPs can also, for example, do spam filtering, and that does not affect their common carrier status, if it is done in order to facilitate the transmission of emails - they can reasonably say the email system would get completely clogged up if they didn't.
However when they start inspecting or censoring traffic for reasons other than facilitating the transmission of communications they may lose their common carrier status. This would leave them open to some civil suits and criminal prosecutions. In the UK/EU it would also be illegal interception if they looked at content, but not in the US.
Their T+C's are not usually immediately relevant to whether a person who passes on a communication has common carrier status.
(Net neutrality is kinda orthogonal to common carrier status - they don't really have that much to do with each other. Even if an ISP does deep packet inspection in order to decide whether to send a packet by the fast or the slow routes, that needn't necessarily affect its common carrier status. As long as the slow stuff gets there without inordinate delay, if the fast stuff gets there quicker then so what?
Common carrier status goes back a very long way; eg a shipping agent in India in the 1850's might offer a clipper service which would take ten weeks, or a barque service which would take twenty - but as long as he didn't discriminate based on factors other than price he would be a common carrier.
The censoring of communications by ISPs based on eg IP address or other communications metadata, rather than based on immediate inspection of content, is a slightly different, and thorny, issue, For instance if they bar access to hard-pron.com, for eg child-protection reasons, that is not interception of content (which would be illegal in the EU) - but it may cause them to lose common carrier status, not just for those comms, but for all comms. The law on all this is a bit unclear.)
As for Sonic.net and DNSSEC, no they do not have common carrier status in that respect. The DNSSEC communications are (presumably) between you and Sonic who run the DNSSEC server, so common carrier status would be impossible, and not relevant to the issue of whether you can sue them.
Sadly, as I know very little US law, I have no idea whether you can sue them or not.

@_date: 2014-09-04 04:16:16
@_author: Peter Fairbrother 
@_subject: [Cryptography] What is the difference between a code and a cipher? 
Old chestnut - is it being intended to be hard to decrypt? Having a variable key? Something else?
I don't know whether there is a definitive answer, but opinions are sought.

@_date: 2014-09-04 20:54:30
@_author: Peter Fairbrother 
@_subject: [Cryptography] What is the difference between a code and a 
I think we can agree what codes are, in modern terms.
Outside of cryptography, the concept of codes as reversible fixed 1:1 mappings between an input set and an output set is used frequently - light is encoded as pixels, bits are encoded as pits in optical media or as magnetic domains on hard drives, grey codes, Morse code, and so on.
There are some partial exceptions like erasure codes, where the forward mapping is a semi-randomised injection rather than a bijection, and lossy codes as in .mp3 etc which are surjections, but idea of a fixed (in the sense of non-keyed) reversible mapping is pretty universal.
I don't think we can get around that modern usage of the word codes in crypto, and more, I don't think we should, or need to.
Defining ciphers in modern terms is a bit trickier.
In classical cryptography there were once codes, as exemplified in the code book in which an entry might be a word or phrase; and ciphers, which worked at the letter level, either substituting or rearranging the However in modern cryptography that classical concept of a cipher is no longer useful. The meaning of the word "cipher" is drifting, and I was really wondering how people use it today, rather than in historical/classical terms.
I think there was a drift towards "ciphers are meant to conceal information unless you have the key/code book", whereas codes might not be, and indeed nowadays are usually not, about concealment.
However, just speaking personally, nowadays I use "cipher" in technical discussions to mean a variable-keyed transformation which is supposed to be hard to reverse without the use of the key, and which is used to conceal information.
Yes, I also still use it in the classical sense in phrases like "monoalphabetic substitution cipher" when discussing classical crypto - but not when discussing modern crypto.
So to me the modern difference is, a cipher has a key, whereas a code does not. I think that's an actual quote, but I don't know who said it.
What I was really wondering is how widely other crypto people use the same modern definition?
Or what other definitions they use, in modern technical discussion?
I know NSA still use the word/letter distinction, but that's NSA. They do some things in a somewhat unusual and old-fashioned way.

@_date: 2014-09-22 00:14:22
@_author: Peter Fairbrother 
@_subject: [Cryptography] new wiretap resistance in iOS 8? 
Aaarrrghh, not that old bollocks again.
"Out of proportion"? - bear in mind Robert Morris's second rule: "Never underestimate the attention, risk, money and time that an opponent will put into reading traffic."
Plus remember, we don't ever really know the full resources of an attacker, or how effective they are.
The costs might be well out of proportion - but the attacker might still be willing to pay his.
Security is only good if, in practice, it resists an attacker's attempts to break it.
Now what might be good enough in one case might not be good enough in another; if for example an attacker can employ extended resources in the second case.
If an attacker can only employ effective extended resources in limited numbers, well you could say "it sucks to be the loser, but most of us are safe" -
- but I will not say that. I will say instead that most of us are at risk.
Nor do I subscribe to the idea that security has to have any significant cost to the user - modern encryption is essentially free and unbreakable, why can't we do the same with the rest of our systems? Especially software systems - the cost of distributing software is lost in the noise.
The reason why the security behind those systems isn't essentially free and unbreakable lies mostly in those who design them - they are not security minded. They make things which are not secure but which are popular and easy to use, and thus those things get used - we have to make those popular things secure.
The converse is that secure products have to have the same popularity and ease of use.
If a luser has to RTFM, it don't fukken work.

@_date: 2014-09-22 17:41:22
@_author: Peter Fairbrother 
@_subject: [Cryptography] new wiretap resistance in iOS 8? 
It is most definitely a rule - it's about what you have to do to
successfully defend against an attacker.
Ignore it at your peril - look at what happened when the Nazis did just
Even if that were true - and I do not doubt role of economic
restrictions on breaking security - it has nothing to do with the
proportionality of costs to an attacker and a defender.
However it is not _immediately_ true - there are a lot of reasons why an attacker might spend more than he gains.
Most obviously, he might not know what he will gain until he has done
the attack. There might be maintenance reasons - maintaining capacity in case it is needed later. There might be what the USAnians call pork reasons.
And then there is the issue of value - if an attacker only has enough
resources to break 10 comms, he will use it to break 10 comms. Now we
might think that that is a ridiculous amount to spend to break each of
the broken comms, but to that attacker it's all he has, and the value of those broken comms to him could be better than nothing.
If those resources are a dedicated AES-cracker, then he is going to use
it to crack AES. It's not as if it can be reused for something else.
I'll mention that I do not think it is _necessarily_ true in the long
term either, but I won't go into that.
So, that experience tells us exactly what about what attackers know
about eg attacks which are not presently deployed because they are more
And if we defended against the attacks which are presently deployed,
apart from deploying the more expensive attacks they already know, don't you think they might come up with some new ones?
  We know
We knew that before, if we were paying attention at all - Robert
Morris's first rule of cryptanalysis, always look for plaintext.
(If you didn't already know, Robert Morris was the Chief Scientist at NSA)
Oh dear.

@_date: 2015-01-10 00:37:46
@_author: Peter Fairbrother 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
No ref's, but it seems unlikely - they could more easily have used EM techniques on the cables.
If they were over the repeaters though, those had relays, whose clicking could perhaps have been detected acoustically.
I find it curious that the Germans didn't cut the transatlantic cables though - they knew where they were. Perhaps they didn't want to cut off an intelligence source?
The British (and Germans) both cut most of the UK-continent cables, though I have heard the British left one from Ireland, as they were reading the traffic.

@_date: 2015-03-01 23:45:23
@_author: Peter Fairbrother 
@_subject: [Cryptography] DIME // Pending Questions // Seeking Your Input 
Had a very quick look, seems like DIME is an email-like message-passing service of some kind - though it is not email, it will not work on normal email channels, it is not compatible with email - with end-to-end encryption and some attempt to prevent traffic tracing and to provide forward secrecy.
Neither the end-to-end encryption nor the traffic obfuscation is particularly robust - the encryption is subject to MITM attack, and the traffic obfuscation is at best partial, and depends on trusting the sender's and the recipient's end servers, presumably operated by other people than the sender or intended recipient.
Sender x sends a message through server X to recipient y's server Y. Roughly, server X will serve clients using TLD X, and server Y will serve clients using TLD Y, as both sending and receiving servers. Clients can be offline when messages are passed.
To deter MITM there is a complicated system of "signets", where two key signatures are obtained from somewhere - a CA? and the sender's server? the recipient's server? I am unclear about this.
In any case, it would seem that this requirement to fetch (at least) 2 signets per message compromises the traffic tracing obfuscation - the servers holding the signets would know who sent the request, and the intended recipient it was meant for.
In order to make message tracing a little harder, an observer of traffic between server X and server Y cannot immediately tell which of server Y's clients the traffic is for.
However there is no delay or batching, so a passive observer of person x and his server X can immediately tell that x has sent a message to server Y, though he cannot immediately tell which of server Y's clients the message is for.
This system is obviously not robust against compromise of the recipient server, including legal requirements to store traffic data (formerly a becoming-ubiquitous legal requirement in the EU, now the legal status of traffic data retention is a bit indeterminate - though anyone operating a server in the UK today, for instance, would be required to log the relevant  traffic data and store it for 12 months.)
I think the sender's server X is not intended to know which of server Y's clients the traffic is for, just the recipient's server - however, the requirements for obtaining signets and perhaps DH portions also make server X susceptible to attack based on server compromise, and probably server traffic data retention requirements as well.
Also, according to figure 1, server Y sends some data direct to the sender. I am unclear about what data is sent, or why. so it is possible server Y does not know for sure who the sender is, but it seems likely.
Again, from figure 1, it seems some data is sent from server X directly to the recipient - so server X is likely to know who the recipient
There appears to be some form of forward secrecy, but details are lacking - as far a I can tell the recipient y seems to have a permanent, or at least semi-permanent, DH secret; so for instance message confidentiality is not secure against legal or rubberhose demands for keys.
As I said, I only had a very quick look - is that about right?

@_date: 2015-03-02 12:24:27
@_author: Peter Fairbrother 
@_subject: [Cryptography] DIME // Pending Questions // Seeking Your Input 
Had a further look, Some more feedback, based on:
p.20 "Only the author and recipient can decrypt an entire message."
For forward secrecy and protection against key demands, only the intended recipient should be able to decrypt message content, NOT the p.20 "Messages are tree structured and content encryption is per leaf with independent keys for each leaf,
permitting access to individual parts of the message without having to process other parts"
yes, but .. why do you need independent keys? surely the only person who can see any message content should be the intended recipient, and you only need one key for that?
p.23 diagram You seem to have at least four independent symmetrical keys in the message format, but you only have two actors who are required to decrypt anything - the intended recipient's dimeserver, who decrypts the recipient's name (and perhaps a dimeserver-to-dimeserver transfer layer of encryption) and the intended recipient.
The recipient's name is short enough that it could perhaps be encrypted (by the sender) using the receiving dimeserver's public key directly, not a symmetric key.
for bounce messages, this could include the sender's name. Or the message could include an ID from the sending dimeserver, which the sending server could decrypt or look up if needed in order to identify the sender's name for bounces, spam reduction, etc.
p.27 "The DIME security model depends upon the reliability and security of the global DNS system. For this reason we strongly
recommended organizations use DNSSEC"
shouldn't that be REQUIRE, not recommend, if(?) the security model depends on it?
p.34 "PART 5: SIGNET DATA FORMAT
alma mater field
gender field"
dick size field? Too damn many fields.
p.59 "Specialized payloads are structured differently from encrypted NO NO NO. All payloads MUST be treated the same.
p.67 "9 This is an aspect of D/MIME that would benefit from community feedback. The current plan is to allow a message which uses the same submitted once using DMAP, plus the individual key slot and signature symmetric keys to be values for each recipient. The submission server would assemble the pieces, and then the full contents would be transferred separately between servers over DMTP. "
I don't know what that refers to, but the ONLY time two messages should use the same key is when it is the same message to the same recipient. Ie, never.
This also helps a little with spam, which I don't see many references to in the draft.
"Users who want to avoid fingerprinting of the contents would need to submit a separate copy for each recipient."
tut tut.
p.67 "10 Should we define different display types for the different MIME content types? And possibly even differentiate a few of the subtypes, like text/plain and
text/html, so a client can distinguish which display chunk it should retrieve for display purposes? This would leak information about what information a
message is carrying, and make them easier to fingerprint, but could allow a client to avoid downloading a video message if it didn?t support video (for example
on a mobile device). Even if we did add this, there would be a generic catchall chunk type implementations could use if they didn?t like the All this MIME stuff - should be nothing to do with DIME. DIME just wraps the standard MIME stuff and content up and encrypts it as a single chunk. Let the user's normal email client deal with the MIME stuff. If the dimeserver has to deal with it, the dimeserver gets to know too much.
More importantly, the dimeserver gets to know too much in _every_ case, not just in partial download cases.
To avoid downloading a large video, the client can download the first say 16 kB of the message, where the MIME metadata and plaintext version is, and decrypt it. If you want to get fancy, then stick a bit more metadata in there, like sizes, start points, and so on. Then the client decides what to download, and the dimeserver just gets to know what was downloaded - and only when a partial download is performed.
However there is a case for treating signatures differently, so that eg spam reduction can be done etc - but then, the signatures should be in DIME format and appended to the message, not in MIME format and included in the (single) main encrypted chunk.
You could also add the extra content type data in the message signature block, if you wish. Would require a small change to the email client, but I think you'll need that anyway.
p.68 "DMTP is intentionally simplistic." oh no it isn't !! it's far too complex already. The whole of DIME is.
KISS  ;)
Signet stuff - well, data on the use of signets seems to be missing. I am not clear about the signets.
p.97 "A user?s reliance on an associated organization server can be at three different service trust levels, selectable by the user"
NO NO NO _NO_ *NO*
A luser (a learner user) doesn't understand these options, and can't be expected to. Hell, I don't understand them myself.
Do not give him a choice - you decide, you take the responsibility for getting it right. That's your job, not his.

@_date: 2015-03-02 22:27:38
@_author: Peter Fairbrother 
@_subject: [Cryptography] DIME // Pending Questions // Seeking Your Input 
Agreed. All you need is enough data for senders to identify the person.
I kinda liked the image field too - treat it a bit like an icon. Optional, of course.
Plus whatever the cryptographic protocol needs. That should be enough.
And in general, that's all the data which should be in the signet, nothing else belongs there. There are reasons to put more data, eg things like snail-mail addresses, in some types of signets - but not in DIME signets.

@_date: 2015-03-05 13:58:35
@_author: Peter Fairbrother 
@_subject: [Cryptography] FREAK attack 
> Sounds some what theoretical. Given the existence of certificates
 > that allow corporate proxy servers to 'inspect' SSL traffic,
Can someone  explain how that works please?
I can see a corporate proxy doing a MITM with the external SSL using a certificate signed by a corporate CA installed on all the internal Is that what he means? I may have missed something.
If it is, it isn't the certificate which is the plaintext hole, it's the corporate CA installed on the corporate machines..

@_date: 2015-03-05 13:58:42
@_author: Peter Fairbrother 
@_subject: [Cryptography] FREAK attack 
Couldn't agree more :)
To which, add:
4. Occam's razor applies to protocol design too - the simpler it is, the less points of attack there are.
5. A system that's hard to use doesn't get used. Good user interfaces are essential. Users don't RTFM, so don't expect them to.
And we have the beginnings of a modern security software design philosophy.
Any additions?

@_date: 2015-03-05 16:16:19
@_author: Peter Fairbrother 
@_subject: [Cryptography] DIME // Pending Questions // Seeking Your Input 
I probably didn't read it in enough detail  :(
However, even "splitting" leaks too much potentially-private
It adds a lot of unnecessary and unwanted complexity for
the luser, many of whom who will not understand it, or how to use it securely, even if it is simple.
It adds many places where attacks can be launched by malicious actors.
And most important, for a DIME-type signet which will be used for email,
it isn't necessary.
Now if the signet is to be used in a wider context, to encrypt eg voip,
chat, and other end-to-end applications as well as email, perhaps do
your online banking and call taxis, as a single universal user key
signet, then there is a case for optional fields of the type you
mention, with all sorts of personal data in them - but that is _very_
much bigger project than DIME. Then the signet structure should be
designed for that use, rather than for use in DIME, and even designing
that sort of signet would be a big project.
However if you are just encrypting email then there is no need for all
this, and it just adds complexity and places to attack - for instance,
should the fields be searchable?
This is actually quite a big question, suppose I give someone my
DIME-mail address, but I don't want it generally known - if I have my
name in there, and if even the name field is searchable, then ...
So, what do you *need* in an email-replacement signet? Basically, data
which tells the potential sender that the signet is, or purports to be,
the right signet, and that is all. You don't need any more than that.
You are adding all this other stuff - why?  What are the split signets for?
To allow someone to say "I warrant that this is my data, see, it has my
signature on it"? Can't they just do that in a (signed) DIME-mail?
Some other reason?
Making the fields splittable, and having different types of signet, are
just adding modes and options, which we know are not good protocol
design properties.
Who has access to the splits? To a particular split, or to a list of
splits or search of splits on DIMEserver X, or a global search or list?
If you are going to introduce splits then you are going to have to
answer all these questions, and more - and you are going to get some of
the answers wrong for some users, no matter what you choose, because
there are no right answers.
Besides which, you don't and won't have any actual real control of who obtains signets, or of the use of search tools on the signets.
Or control of when two split signets are logically linked; avoiding that is a nightmare of rat's nest complexity and probably theoretically, and certainly practically, impossible.
Just putting that data in there is a *FUKKEN* *GODSEND* for the
NSA/5I's, it will allow then to easily fill in their global
emailaddress-humandetails database.
The best way to avoid this, and actually the only way to avoid this, is
simple and easy - no unnecessary personal data in the signet. Which
means no splits, only one signet, with minimum (and optional) data in
the signet.
If there is no data to collect, no-one can collect it.
But more important, there is no _need_ for split signets; proof: today's
email addresses don't have them, and yet email is widely adopted.
If a user absolutely needs two different signets, just let him have two
signets. If he really wants to put his address, sexual and terrorist preferences, and preferred dick size in there, and you want to let him, have a "comments" field.
Here is another, linked, question; are you giving up the right to lie about who you are, or stay anonymous, in your email? It seems you need a CA signet in order to receive DIME mail, but can you get one without proving who you really are to the CA?
If not, it seems DIME is pretty bad choice as a ubiquitous replacement for email.
I don't understand what the org signets are for. Can't you just use normal CA certs? You are using them for FS anyway ..
and it's TOO DAMN COMPLEX as it is.

@_date: 2015-03-05 21:09:44
@_author: Peter Fairbrother 
@_subject: [Cryptography] DIME // Pending Questions // Seeking Your Input 
Eh, wot abaht FTP?
Tim's Web had huge advantages for the user - does DIME, or some overarching new infrastructure, have any such advantages?
Let's compare DIME with a fairly minimal and simple email encryption environment. In our simple environment encrypted email is sent over normal email channels. There are public key "signet"s for the users, which are kept in a distributed directory on a server somewhere.
Perhaps the @ is replaced by &, or there is something else in the address, in order to distinguish email addresses which will only accept encrypted email.
With a bit of handwavium over MITM for now (maybe I'll get into that later), a sender collects a public key "signet" from the directory server, and uses that to encrypt the message including attachments, signature, whatever, including the "from" field [1].
He transfers the encrypted message to his email provider via TLS. They then send it to the recipient's email server, hopefully via TLS as well. The recipient then collects it via TLS.
So, what does this system need in order to be implemented? It needs mail servers to use TLS between client and server, which is not a huge stretch. It needs a single or better several distributed directory servers. It needs updates to the client's email programs, or webmail with a hidden key program, probably a browser extension.
Cypherpunks write code? Get to it chaps!
DIME requires all these (?except the directory server?), but it also requires each user to employ a DIMEserver.
Afaict, DIME would need a critical mass of DIMEservers to be in existence just to start being useful. I might be wrong about that, but..
And I cannot see any great security reason or advantage for having the DIME servers. Ok, they might be required to use TLS server-to-server, but apart from that?
Is the benefit worth the cost?
Advantages of the simple system: it is compatible with ordinary email. It does not require any changes to existing email servers, or ask them to do anything they don't normally do, though it would be good if they always used TLS.
It encrypts email, it does the same job as DIME to provide FS, it actually does more than DIME does to hide metadata.
And most important, it does not require every potential sender to employ a dedicated DIME email server.
As to MITM, I don't understand the DIME anti-MITM mechanism (it isn't fully documented), but as far as I can tell there is no reason why the simple system can't provide exactly the same functionality.
[1] or perhaps the "envelope-from" field contains "simple-encrypted-email" - I am no POP/SMTP email expert, but the idea is that only the recipient (and the sender's server) can see who the sender is.
hmm, can you configure today's mail servers to only accept TLS connections?

@_date: 2015-03-07 19:17:01
@_author: Peter Fairbrother 
@_subject: [Cryptography] DIME // Pending Questions // Seeking Your Input 
Well, it's a nice name. Whether there is anything good about them is a different matter - I haven't seen any evidence of that, but maybe it's Indeed. But there is a very big IF in there ..
It's almost as if it was designed by NSA to leak as much information as possible, be hard to implement, and be easy to attack.
It leaks user details (the split signets).
It leaks MIME and other message details.
It requires new server software for existing email servers.
It requires a whole new new server infrastructure.
It requires a user to give out his real name to a CA in order to use it.
It requires a user to trust a server.
It requires a user to make security decisions the average user is incapable of.
It has a huge attack surface.
None of these are necessary in order to provide any of the advertised functionalities in DIME.
In short, it stinks.

@_date: 2015-03-11 21:08:51
@_author: Peter Fairbrother 
@_subject: [Cryptography] DIME // Pending Questions // Seeking Your Input 
did you mean "tying"?
Thanks for replying.
Just picking on the obvious points first; optional can be used, and will be. If it gets used, it will get abused.
The first law - if data isn't collected it can't be stolen. Do you NEED all this data stored to get the system working? Does it even give you any significant advantage? I say no.
Ordinary email seems to work quite well with just a  field, which people can lie or use aliases, stage names etc in. An optional image field, maybe - think of the icons used in web lists, chat etc - but no more, not for a ubiquitous secure messaging system.
As for getting to pick who you trust, that's simply wrong - if I want to send you a message using DIME, I have to trust whoever you have decided to trust.
More, I have to trust them to some unknown extent - for instance, I don't know whether you are using a paranoid or a simple security level (and even if I did, it wouldn't help any if I am a typical user who has not RTFM), whether your server has access to my MIME data, my user details - whether it even knows who I am, or purport to be.
You do not seem to understand the high-level design of a system like an encrypted email system - writing the code is not the hard part, this is the hard part: and it needs to be done, and done right, before any code or specs can be written.
I append some first-thought notes on such a system. Possibly one of the first things you will see is that there is no place for a trusted server in this system.
I think you have in mind that you will operate such a server in DIME, but there you forget the second rule - only people you trust can betray It seems to me that all of the objectionable bits of DIME - the MIME situation, the split signets,the multiple user security levels, the need for new server software, the need to trust a server - are all there in order to justify, or as consequences of, the desired new server If we throw that new server architecture out, then we might be able to do something useful to increase privacy, which will be cheaper, more readily introduced, easier to use, and more acceptable than DIME.
And a hell of a lot securer.
Sent via email?

@_date: 2015-03-20 13:11:21
@_author: Peter Fairbrother 
@_subject: [Cryptography] FFS 
I abhor the term "perfect forward secrecy", as being inaccurate unless an OTP is used for perfection -
- and unless then, remembering that for forward secrecy you have to delete any working materials which have come into contact with plaintext or key, you then kill the recipient ("I'd tell you but then I'd have to kill you") -
- and unless, as you yourself know the contents of the message, so you then kill yourself.
This is of course ridiculous, or at best over-enthusiastic.
I propose instead the term Full Forward Secrecy, and the corresponding acronym FFS, to describe a system where an OTP is not used, and the secrecy is based on some sort of computational complexity, whether real or imagined, instead.
This arose in the context of the question "if you have good encryption which is otherwise computationally forward-secret, is it still forward-secret if the recipient keeps a copy of the message after reception and initial reading?".
Of course, if the sender keeps a copy, the question is similar.
So, I propose the term - but I don't know quite what it should mean.
--Peter Fairbrother
Today I saw a solar eclipse!!
Spring is sprung
The grass is riz
I wonder where
The birdies is?
"The bird is on the wing",
I heard - but
That's absurd
The wing is on the bird.
