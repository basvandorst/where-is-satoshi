
@_date: 2003-04-11 01:19:33
@_author: Thor Lancelot Simon 
@_subject: PKI Test Data available: 5,000,000 RSA keys/CSRs/certs 
[Moderator's note: for those that need it, this is a very valuable set
of test data for PKI systems. --Perry]
Current requirements for deployment of software or hardware into
certain environments using X.509 certificates for authentication
(e.g. sale of certain types of cryptographic products to the U.S. DoD)
present a rather high bar for handling of certain PKI operations, e.g.
operations involving Certificate Revocation Lists, which may number
in the dozens and include hundreds of thousands of entries per list at
any time.
My employer (ReefEdge, Inc.) was recently faced with the need to test one of our products to said requirements.  We quickly realized that there was no suitable collection of test data, and that producing one would require a significant amount of computational effort and storage
space.  The Computer Science Department at the Stevens Institute of
Technology was kind enough to lend us the use of a cluster of 60
Pentium-IIIs running NetBSD, with about 1TB of disk available for
our use.
We are now making the data available for general use.  We request
that if you use this data to develop or test your product, you consider:
1) Making a donation, even a small one, to Stevens Tech, designated
   for the Computer Science Department.  You can arrange to make such
   a gift by calling the Office of Development and External Affairs
   at Stevens Tech at +1 201 216 5225.
   You could spend tens of thousands of dollars and months, weeks,
   or at least days (if you happen to have dozens of fast CPUs and
   a lot of storage handy) to generate this data yourself, so you    can sell your products into markets worth tens or hundreds of
   millions of dollars.  Or you could just use the data we're making    available out of the goodness of our hearts.  Wouldn't it make sense    to give us a little something to encourage us to continue to do so?     It's tax-deductable.
2) Giving credit to the Stevens Tech Computer Science Department
   in the documentation, advertising, or accompanying materials for
   your product, for example:  "This product was tested using PKI-Test
   version 1.0.0 from the Stevens Institute of Technology Department
   of Computer Science."
If you use this data in your research, we request that that you give us appropriate credit.  As mentioned above, the name of the current package is "PKI-Test Version 1.0.0", and it is available to you courtesy of the CS Department at the Stevens Institute of Technology.
PKI-Test version 1.0.0 is available from ftp.cs.stevens-tech.edu in
the directory /pub/PKI-Test.  The file "README" in that directory
gives the layout of the information.  Please don't just try to download
the tarfiles of all of the data that are in the top level of the
"tarfiles" subdirectory; they are over 10GB in total size and will
probably make your machine eat its filesystem if you try to extract
them anyhow; they contain thousands of directories and over 15,000,000
individual files.  See the README; other subdirectories contain all
the same data neatly packaged up for you in sets of 1000 files each.

@_date: 2003-04-11 13:50:07
@_author: Thor Lancelot Simon 
@_subject: don't like the price? change the barcode! 
Does it?  It seems to me that it is only an example, as we've seen
so many recently, of an age-old con retooled for new technology.
Barcodes were never intended to be anything but a replacement -- in
fact, originally, merely a supplement -- for old-style peel-and-stick
printed price labels (I speak with some authority on this as my mother
was the Nabisco representative on the original bar-code committee).  Any
teenager with a label gun can practice this attack against old-style
arabic-numeral price labels -- and many have, and do -- so it does not
exactly surprise me that there's a barcode equivalent.  Ultimately,
the cashier is responsible for verifying that the price on an item
has not been changed; that's true no matter how the item is labelled,
and requires some common sense on the part of the cashier.  The inability
of cashiers to always make the right decision about whether they're
being conned or not is part of the usual allowance for shrinkage at
any retail operation.
Of course, the automated self-service cashier locations now showing
up at some stores are another story entirely; those attempt to use
weight and size to validate that an item matches its barcode, but
even those simple sanity checks turn out to be extremely difficult
to apply without a high false result rate.  There really is no good
substitute for the human being in the loop in this application, at
least not yet.

@_date: 2003-04-30 20:00:18
@_author: Thor Lancelot Simon 
@_subject: eWeek: Cryptography Guru Paul Kocher Speaks Out 
============================== START ==============================
I'll note that this is not too different from the problem of unauthorized
duplication of actual film prints of movies -- except that access to good
35mm prints of movies is slightly (but only slightly!) harder to arrange
(it typically involves bribing only a single theater employee, and often
a fairly junior one at that) and that the actual duplication cost might
actually be lower, in an ongoing operation.
Despite some efforts in this area by the Mafia in years past *and* the
semi-widespread availability of 8mm and even 16mm prints of movies no
longer in first-run theatrical release through public libraries for a
few decades, duplication of film prints has not appeared to pose a major
problem for the studios.  This would seem to suggest that raising the
level of investment and technical sophistication required beyond mere
triviality is, in fact, the correct target to shoot for, from the
studios' point of view; and that Kocher is probably really doing a
reasonable job at that with his proposal.

@_date: 2003-08-25 21:15:00
@_author: Thor Lancelot Simon 
@_subject: PRNG design document? 
That's not quite right.
1) Various entities have already had various versions of    OpenSSL FIPS-140-2 certified.
2) It is permissible to use a non-Approved deterministic
   RNG for IV generation, though not for keying.
Since it's permissible to rekey the Approved PRNG, and there is no
requirement for _how_ it is rekeyed save that the input must not have
demonstrably less entropy than the output, it is possible to use, if
not Yarrow, a _very_ similar design by using an entropy pool collecting
input from one or more hardware sources to periodically rekey the
Approved X9.17 generator.
I am informed that in the past, implementations using Yarrow have, in
fact, been certified, passing the code examination in the lab by
documenting that Yarrow's output stage is, in fact, algorithmically
equivalent to the X9.17 generator.  Unfortunately, since those products
were certified, there have been some particularly ill-considered
interpretations of the X9.17 RNG specification by NIST which I believe
would now make it impossible to have a Yarrow implementation certified;
but you can get very close.

@_date: 2003-08-29 15:45:50
@_author: Thor Lancelot Simon 
@_subject: PRNG design document? 
I think there's some confusion of terminology here.  A "time", Ti for each
iteration of the algorithm, is one of the inputs to the X9.17 generator
(otherwise, you might as well just use DES/3DES in any chaining or feedback
mode, for all practical purposes).  However, it has always been permitted
to use a free-running counter instead of the time, and indeed the current interpretation by NIST *requires* that a counter, not the time, be used.
As for keying, you're allowed to key with whatever you want, whenever you
want, but at least from my conversations with a number of people during a
recent certification, you'd better be prepared to explain why your source
of key material is strong.
One implementation with which I was involved essentially rekeyed the
generator as soon as enough entropy had accumulated from a hardware
source; another rekeyed it depending on the number of output blocks.
Both approaches are permissible.
I do have some more thoughts on the quality of the various generators
the standard allows but I haven't had time to get them down in writing;
I'll try to do so before this thread is totally stale...

@_date: 2003-12-06 03:15:18
@_author: Thor Lancelot Simon 
@_subject: "Tales from the crypto" 
I'd like to see a copy of the "tales from the crypto" section of the Cray
supercomputer FAQ.  The version of the FAQ available on the web describes
it as "not for public distribution".  I'd like to read it because I think it
might shed some light on just what nobody at all, no, really, nobody, not
ever, nowhere, did with a comparatively immense amount of computing power;
but that's a secondary reason; I'd also like to read it because I've heard
here and there that there are some really great stories in there, if only...
well, you get the idea.
Anyone have one to share?

@_date: 2003-07-08 16:32:41
@_author: Thor Lancelot Simon 
@_subject: LibTomNet [v0.01] 
The problem isn't just the interface that OpenSSL presents to the
application programmer (which is lousy, and which in a lot of cases is
totally undocumented; it also has the "Kerberos problem" which is to say
that to do cryptographically necessary things it is often necessary to use internal or deprecated functions directly, and these change from
release to release... ugh!) it's also how it's implemented.
I trimmed OpenSSL down to just TLSv1 and only the FIPS-140 conformant
algorithms for a FIPS-140 conformance project at ReefEdge (and yes,
we did then have that version of OpenSSL tested and certified, but no,
you can't have it for free ;-)).  It was not so hard, but it was
immensely time-consuming and I had to learn a totally unreasonable
amount about OpenSSL's internals to actually ensure that all the
nonconformant algorithms were disabled (in some cases it would have
been impractical to not build them at all, unfortunately).
The result was still several hundred kilobytes -- actually, I don't
have exact numbers handy but I believe it was more than a megabyte
in size.  OpenSSL is not the TLS implementation I would use if I had any other free option that offered reasonable performance. :-(

@_date: 2003-07-08 17:42:10
@_author: Thor Lancelot Simon 
@_subject: LibTomNet [v0.01] 
I believe the Certicom library is somewhere around there in size, and
it is a pretty extensive implementation.  Costs money though. ;-)
It's also hideously overabstracted.  That, to my mind, is why it's both
hard to use and hard to maintain.  Unfortunately, its "API" is the only
one that is in wide use on Unix systems, which means that any alternative
would probably be forced to duplicate a frightening amount of OpenSSL's
internal complexity in order to present its _external_ complexity.

@_date: 2003-06-28 18:55:49
@_author: Thor Lancelot Simon 
@_subject: Attacking networks using DHCP, DNS - probably kills DNSSEC 
This problem is old and well-understood.  It is why there is work
in the IETF to combine the acquisition of a DHCP lease with the
acquisition of an initial IPsec SA to integrity-protect that
It's not easy for me to see why anyone would expect anything *but*
that MITM attacks against client systems that are entirely
configured by DHCP would be practical.  If the DHCP client and
server share no cryptographic guarantee of trust...
..oh, I'm sorry, I forgot that the anacephalic have fallen for
"you can magic up trust out of nowhere" about ten times in
succession in my immediately previous area of work, 802.11. :-)
Where I used to work, at ReefEdge, we disposed of the 802.11
security garbage and used a TLS-based solution that was not
entirely unlike PIC, dispensing temporary credentials for use
with IKE to users based on their legacy authentication.  As the
designer and maintainer of this system, I became *very* cognizant
of DHCP-based and DNS-based attacks, and very skeptical of the
sort of proposal someone brought be every few days suggesting
that some later establishment of a trust relationship could
overcome a successful MITM attack on one of the early stages of
the client's "boot up and get SA" negotiation.
(of course, I also became very skeptical of many other folks'
"use legacy credentials to bootstrap IKE" techniques; there
are implementations out there in widespread use which default
to only authentication methods that are trivially MITMed, and
at least one I can think of that _can not be configured_ to
do standard IKE in a secure way.  Ouch! But the simultaneous
IKE and DHCP proposal I read a few years ago at the London
IETF seemed pretty sound.)

@_date: 2003-05-01 12:36:41
@_author: Thor Lancelot Simon 
@_subject: eWeek: Cryptography Guru Paul Kocher Speaks Out 
No, it's not; there are three ways the work gets cheap:
1) You automate it; there's tons of last-generation automated equipment
   available for duplicating 35mm long-roll film and always has been,
   because it's a fairly cutthroat business and equipment is upgraded
   frequently to keep costs down.
2) You do it semi-manually and pay your workers a pittance.  The operations
   involved (loading film, loading chemistry, simple time and temperature
   control) are simple enough that they can essentially be done by
   minimum-wage workers.
3) You ship your first internegative to India and let them deal with how
   to make your cheap, stolen prints.
The cost is higher than that of burning a DVD-R, I'll grant you, but it
is not so high as to dwarf the setup costs, or even come close.  I'm
persuaded by your argument that the "keep duplicating forever" property
of the attacks we've seen described on Kocher's system is more serious
than I originally suggested, but generally speaking, I think the analogy
to unauthorized duplication of actual film prints is a useful one, because
it demonstrates that what the industry really has to fear is unauthorized
duplication of one or two copies each by a billion anklebiters; large-
scale duplication by organized criminals is a nuisance, but pretty much
only that, when compared to billions of copies circulated without payment
to billions' of folks one-friend-each.

@_date: 2003-11-26 20:27:08
@_author: Thor Lancelot Simon 
@_subject: Open Source Embedded SSL - Export Questions 
RC4 is extremely weak for some applications.  A block cipher is greatly
There isn't _quite_ a speed/strength tradeoff in cryptography, but any
time you choose algorithms based purely on speed, you'd better get really,
really suspicious about the strength of what you're producing.

@_date: 2003-11-26 23:06:00
@_author: Thor Lancelot Simon 
@_subject: Open Source Embedded SSL - Export Questions 
Sorry if I mislead -- that was intended as two separate statements, and I
was also in a bit of a hurry.
Yes, of course there are applications for which a stream cipher is preferable
to a block cipher.  However, in my experience, programmers often choose RC4
(or another fast stream cipher) by using speed as their only criterion, and
then end up applying it in applications in which a block cipher would be a
better choice.

@_date: 2003-10-01 16:54:35
@_author: Thor Lancelot Simon 
@_subject: Monoculture 
In that case, I don't see why you don't bend your efforts towards
producing an open-source implementation of TLS that doesn't suck.
If you insist on not using ESP to encapsulate the packets -- which in
my opinion is a silly restriction to put on yourself; the ESP encapsulation
is extremely simple, to the point that one of my former employers has a
fully functional implementation that works well at moderate data rates
on an 8088 running MS-DOS! -- TLS is probably exactly what you're looking
Note that it's *entirely* possible to use ESP without using IKE for the
user/host authentication and key exchange.  Nothing is preventing you
from using TLS or its moral equiavalent to exchange keys -- and looking
at some of the open-source IKE implementations, it's easy to see how
this would be a tempting choice.  Indeed, there's no reason your ESP
implementation would need to live in the kernel; I already know of more
than one that simple grabs packets using the kernel's tunnel driver, for
portability reasons.
However, if for what seem to me to be very arbitrary reasons you insist on
using an encapsulation that's not ESP, I urge you to use TLS for the whole
thing.  As I and others have pointed out here, if you're willing to *pay* for it, you can have your choice of TLS implementations that are simple, secure, and well under 100K.  Compare and contrast with the behemoth that is OpenSSL and it's easy to see why you wouldn't want to use the open-source implementation that is available to you now, but there is no reason you could not produce one yourself that was much less awful.
You say that you object to existing protocols because you want simplicity
and performance.  I say that it's not reasonable of you to blame the
failures of the existing *open-source implementations* of those protocols
on the protocols themselves.  I think that both the multiple good, small,
simple commercial SSL/TLS implementations and the two MS-DOS IPsec
implementations are good examples that demonstrate that what you should
object to, more properly, is lousy software design and implementation on
the part of many open-source protocol implementors, not lousy protocol design
in cases where the protocol design is actually quite good.  So if you're
going to set out to fix something, I think if you're trying to fix the
protocols, you're wasting your effort -- there are existing, widely
peer-reviewed and accepted protocols that are *already* about as simple
as they can get and still be secure the way users actually use them in the
real world.  I think that it would make a lot more sense to fix the lousy implementation quality instead; that way you seem much more likely to achieve your security, performance, and simplicity goals.

@_date: 2003-10-02 11:56:53
@_author: Thor Lancelot Simon 
@_subject: Monoculture 
A C++ implementation will be much less useful to many potential users;
perhaps the most underserved set of potential SSL/TLS users is in the
embedded space, and they often can't afford to, or won't, carry a C++
runtime around with them.  We learned this lesson with FreSSH and
I would strongly recommend a C implementation with an optional C++
interface, if C++ is the way you want to go.
Also, I'd consider, for simplicity's sake, at least at first, implementing
*only* TLS, and *only* the required ciphers/MACs (actually, using others'
implementations of the ciphers/MACs, even the OpenSSL or cryptlib ones,
is probably not just acceptable but actually a _really good idea_.)  The
major problems with OpenSSL are, from my point of view, caused by severe
overengineering in the areas of:
1) Configuration
2) ASN.1/X.509 handling
3) Tightly-coupled support for the many diverse variants of SSL/TLS.
As far as what OpenSSL does, if you simply abandon outright any hope of
acting as a certificate authority, etc. you can punt a huge amount of
complexity; if you punt SSL, you'll lose quite a bit more.  As far as the
programming interface goes, I'd read Eric's book and then think hard about
what people actually use SSL/TLS for in the real world.  It's horrifying
to note that OpenSSL doesn't even have a published interface for a some of
these operations.  For example, there is no simple way to do the most
common certificate validation operation: take a certificate and an optional
chain, and check that the certificate is signed by an accepted root CA, or
that each certificate in the chain has the signing property and that the
chain reaches that CA -- which would be okay if OpenSSL did the validation
for you automatically, but it doesn't, really.
1) Creates a socket-like connection object
2) Allows configuration of the expected identity of the party at the other
   end, and, optionally, parameters like acceptable cipher suite
3) Connects, returning error if the identity doesn't match.  It's
   probably a good idea to require the application to explicitly
   do another function call validating the connection if it decides to
   continue despite an identity mismatch; this will avoid a common,
   and dangerous, programmer errog.
4) Provides select/read operations thereafter.
Would serve the purposes of 90+% of client applications.  On the server
side, you want a bit more, and you may want a slightly finer-grained
extended interface for the client, but still, you can catch a _huge_
fraction of what people do now with only the interface listed above.

@_date: 2003-10-03 17:35:46
@_author: Thor Lancelot Simon 
@_subject: Choosing an implementation language 
I strongly disagree.  While an implementation in a typesafe language
would be nice, such implementations are already available -- one's
packaged with Java, for instance.
restated as "The world needs a simple, portable SSL/TLS implementation that's not OpenSSL, because the size and complexity of OpenSSL has been responsible for slowing the pace of SSL/TLS deployment and for a large number of security holes."
For practical purposes, if such an implementation is to be useful to
the majority of the people who would use it to build products in the
real world, it needs to be in C or _possibly_ C++; those are the only
languages for which compilers *and* runtime environments exist
essentially everywhere.  Coming from a background building routers and
things like routers, I can also tell you that if you're going to
require carrying a C++ runtime around, a lot of people building embedded
devices will simply not give you the time of day.
An implementation in a safe language would be _nice_, but religion
aside (please!) it's a cold hard fact that very few products that
people actually use are written in such languages -- if you leave Java
(which already has an SSL implementation) out, "very few" becomes
"essentially zero".  And if we're interested in improving the security
of not only our pet projects, but of the interconnected world in
general, it seems to me that producing a good, simple, comprehensible,
small implementation *and getting it into as many products as possible*
would be one of the better possible goals to work towards.

@_date: 2003-10-04 11:42:02
@_author: Thor Lancelot Simon 
@_subject: Monoculture 
No.  You can't do it in one step, and you have to use functions that are
marked in OpenSSL's header files as not being part of the official API.
mod_ssl has a convenience function that's confusingly named just like the
OpenSSL library functions that deals with this -- of course, it should be
part of OpenSSL itself, but at least as of 0.9.6 it was not.

@_date: 2003-10-05 15:19:05
@_author: Thor Lancelot Simon 
@_subject: Monoculture 
I have to apologize -- I was not entirely correct in my initial statement,
but without access to the source tree I did most of my OpenSSL work in
(it belongs to a former employer) it took me a while to retrace my steps
and realize I was not quite right.
On the client side, though the documentation's poor, you're correct: there
_is_ a way to validate a certificate and chain you've received from the peer
in one step.  (I note that there is now reference in the header files to
some AUTOCHAIN stuff that I don't recall from earlier versions of OpenSSL,
but that ssl_verify_cert_chain is *still* not part of the public API; it's in ssl_locl.h).
On the server side (or, indeed, on the client side, if the client side needs to follow a chain to reach a trusted CA, and thus needs to load chain certificates) there's no API for loading a cert and its entire chain in one shot, and indeed to do so AFAICT you must use functions that are not part of the public API.  See SSL_CTX_use_certificate_chain() in the mod_ssl sources (which appears much simpler in mod_ssl 2.8 than what I remember working with -- perhaps the OpenSSL API *has* improved!) and SSL_use_certificate_file, SSL_CTX_use_certificate_file, and SSL_CTX_use_certificate_chain_file in the OpenSSL sources.  And then note that *all* of the example code gets this
stuff wrong -- if it even bothers to do server certificate validation at
I can't lose my impression that some of the chain-handling functions moved
from ssl_locl.h to ssl.h between 0.9.6 and 0.9.7 but I don't have a 0.9.6
tree handy nor the time to sift through it.  Sigh.  I wish I had some of
my code from the last time I tackled this issue with OpenSSL at hand, but
unfortunately I don't own it, so I do not.
The complexity and instability of the API for this stuff, and the fact that
we're both rooting around *in the OpenSSL source code* to figure out which
bits of it are public and which are internal, and in which version of
OpenSSL, when the operations at hand (loading and validating chains of
certificates, from the cert for the peer's identity up to the cert from
which trust derives) is a pretty good example, itself, of why I don't care
for OpenSSL.  I spent a long time working on the X.509 support in Pluto,
too, and though I don't really care for it either it does have the decided
advantage that it appears to be designed in the right direction: from "what
are the end-user's needs?" instead of "what is the structure of the
underlying protocol or software abstraction?"

@_date: 2003-10-12 04:27:58
@_author: Thor Lancelot Simon 
@_subject: Trusting the Tools - was Re: Open Source ... 
Not too good.  If I knew what the target processor were, I think I could
arrange to do some damage to most general-purpose operating systems; they
all have to do some of the same fundamental things.
This is a bit more sophisticated than what Thompson's compiler did, but
it's the same basic idea.  There are some basic operations (in particular
on the MMU) that you can recognize regardless of their specific form and
subvert in a progammatic manner such that it's highly likely that you can
exploit the resulting weakness at a later date, I think.

@_date: 2003-10-19 13:39:00
@_author: Thor Lancelot Simon 
@_subject: WYTM? 
I believe the VanDyke implementation also supports X.509, and interoperates
with the ssh.com code.  It was also my perception that, at the time, the
VanDyke guy was basically shouted down when trying to discuss the utility
of X.509 for this purpose and put his marbles back in his cloth sack and
went home.
I see lack of any chained trust mechanism as _the_ major weakness of the
SSH protocol.  X.509 is not exactly pleasant, but it is what has emerged as
the standard for identity certificates and it is functional for that
purpose, and there are many implementations available; there are even
multiple implementations available for the SSH protocol.  I have to regard
the lack of certificate/chain-of-trust support in the SSH protocol as a
highly negative result of a knee-jerk reaction to the very _mention_ of
an X.500 series standard on the working group mailing list, by people who
did not offer any functional alternative seemingly because they thought
the laughable status quo ante -- with *no* way to validate the certificate
presented by a given peer on initial contact -- was fine.  It's a shame
that dsniff and the other toolkits for attacking that protocol weakness
did not exist at the time.

@_date: 2003-10-22 19:02:07
@_author: Thor Lancelot Simon 
@_subject: SSL, client certs, and MITM (was WYTM?) 
Can you please posit an *exact* situation in which a man-in-the-middle
could steal the client's credit card number even in the presence of a
valid server certificate?  Can you please explain *exactly* how using a
client-side certificate rather than some other form of client authentication
would prevent this?

@_date: 2003-09-02 12:33:29
@_author: Thor Lancelot Simon 
@_subject: PRNG design document? 
Well, it certainly doesn't forbid it; again, a simple approach is to treat
the seed as part of the key material and replace it when sufficient entropy
is available from hardware sources.

@_date: 2003-09-03 13:26:38
@_author: Thor Lancelot Simon 
@_subject: PRNG design document? 
Unfortunately, unless something has changed since the proposed RNG Known Answer Tests were temporarily withdrawn, some of that set of derived requirements would make it impossible to have an implementation that actually used the time in Ti certified.  I pointed this out to NIST informally through one of the test labs and was, essentially, told "too bad".
It is particualrly amusing that the way the RNG tests were originally specified, they essentially required the algorithm to diverge
from all published specifications by adding an additional step, that of
checking to see if a Ti value had explicitly been specified for testing
purposes; that Ti value was then to be treated as a counter and incremented
once per round.   I pointed this out and was met with plain old lack of
comprehension: in fact, I was told that "since it computes the same
function from its inputs to its outputs, it must be the same algorithm".
This basically made my jaw drop, but since I didn't feel like arguing
about fundamental computer science with the people who were going to be
testing my implementation I left it alone. :-/

@_date: 2003-09-08 16:25:44
@_author: Thor Lancelot Simon 
@_subject: OpenSSL *source* to get FIPS 140-2 Level 1 certification 
SSL's not certifiable, period.
TLS has been held to be certifiable, and products using TLS have been
certified.  However, it's necessary to disable any use of MD5 in the
certificate validation path.  When I had a version of OpenSSL certified
for use in a product at my former employer, I had to whack the OpenSSL
source to throw an error if in FIPS mode and any part of the certificate
validation path called the MD5 functions.  Perhaps this has been done
in the version currently undergoing certification.  You'll also need
certificates that use SHA1 as the signing algorithm, which some public
CAs cannot provide (though most can, and will if the certificate request
itself uses SHA1 as the signing algorithm).
The use of MD5 in the TLS protocol itself is okay, because it is always
used in combination with SHA1 in the PRF.  We got explicit guidance from
NIST on this issue.

@_date: 2003-09-15 13:25:02
@_author: Thor Lancelot Simon 
@_subject: OpenSSL *source* to get FIPS 140-2 Level 1 certification 
Unfortunately, another key set of words is "single user".  This would seem
to significantly limit the value of a software-only certification...

@_date: 2004-06-15 02:32:56
@_author: Thor Lancelot Simon 
@_subject: Is finding security holes a good idea? 
I don't believe that the premise above is valid.  To believe it, I think
I'd have to hold that there were no correlation between bugs I found and
bugs that others were likely to find; and a lot of experience tells me
very much the opposite.

@_date: 2004-06-16 13:30:48
@_author: Thor Lancelot Simon 
@_subject: Is finding security holes a good idea? 
Indeed it is -- and unless I misunderstand, you're claiming that there
is _not_ such a degree of overlap.
I think most people would tend to agree that humans working in the same
field generally work in similar ways; some, of course, are innovative
and exceptional, but in general most run-of-the-mill system programmers
have a lot of the same tools in their mental toolboxes and use them in much the same way; and some of the time, even the innovative and
exceptional ones work in the same way as us drudges.
This, to me, makes your claim extremely counterintuitive and questionable;
it contradicts not only my intuition but my experience.  I can't even
begin to count the number of bugs I've found by inspection of code (with
some other purpose in mind), forgotten to tell coworkers about or to fix
"right" such that the fixes could be committed, and then seen others
discover when they happened to cast their eyes over the same code fragment
days, weeks, or months later.  And I have deliberately audited large
sections of code, prepared fixes, paused a couple of days or weeks to test
my results, and seen others deliberately or accidentally find and fix (or,
worse, exploit) the same bugs I'd laboriously churned up.
If you won't grant that humans experienced in a given field tend to think
in similar ways, fine.  We'll just have to agree to disagree; but I think
you'll have a hard time making your case to anyone who _does_ believe that,
which I think is most people.  If you do grant it, I think it behooves you
to explain why you don't believe that's the case as regards finding bugs;
or to withdraw your original claim, which is contingent upon it.

@_date: 2004-06-16 17:35:18
@_author: Thor Lancelot Simon 
@_subject: Is finding security holes a good idea? 
Actually, I think that in this regard the answer lies in a sort of
critical mass of interest.  Ideas about where to look for what sort
of bug (or which sections of code to revisit, perhaps because a
relevant new fundamental technique has appeared in the literature, or
someone else's parser has added a popular feature, or because a protocol
specification has changed) tend to propagate through the population of
experts who might find bugs slowly at first, and then faster and faster
in what's probably an exponential way.
This suggests to me that if I happen to be out in front of the curve in
casting my eyes over code fragment _X_ or _Y_ (either because of random
luck or because I'm at the front of the wave of interest), I'd really
better fix what I can, and make that fix public; because trailing out
behind me are many, many more people each of whom has _some_ chance of
finding the bug that is correlated in a nonzero way with my having found
I think that this sort of thing is going to turn out to be _very_ hard
to tease out evidence for or against using naive studies of bug commission,
discovery, or rediscovery rates; but it is my intuition based on many years
of making, finding, and fixing bugs, and of watching others eventually
redo my work in the cases in which I'd managed to fail to let them know
about it.  I would argue that in fact this pattern is not the exception;
it is the rule.

@_date: 2005-12-22 02:02:43
@_author: Thor Lancelot Simon 
@_subject: browser vendors and CAs agreeing on high-assurance certificates 
Ought we forget that two such certificates were issued to a party
(identity, AFAIK, still unknown) claiming to be Microsoft?  What,
exactly, do you think that party's plans for those certificates
were -- and why, exactly, do you think they were inocuous?
  Thor Lancelot Simon	                                     tls at rek.tjls.com
  "We cannot usually in social life pursue a single value or a single moral
   aim, untroubled by the need to compromise with others."      - H.L.A. Hart

@_date: 2005-06-26 19:56:30
@_author: Thor Lancelot Simon 
@_subject: AES cache timing attack 
Sorry I didn't respond to this earlier -- I was on vacation last week.
It is simple to practice this attack against an 802.11 network that is
behind a NAT or routing firewall, rather than just a simple Ethernet
bridge.  It's only moderately harder when the 802.11 network is separated
from the public Internet by a proxy firewall.
In fact, I can run it right here in my house:
50 different 802.11 networks as I scan the buildings across the street
with a reasonably tight antenna.  Many of those networks are connected
to the public Internet by a cablemodem network to which I also have
A small number of those networks use WPA with AES (I'm lucky I have so
many networks to choose from; this isn't common on residentail networks --
So, to obtain encryption timing data, I can:
1) Do two quick tcpdumps, imported them into SPSS, and look for the
   best-correlated wireless and wired activity times.  This gives me a
   good guess as to which wireless network had which public address (it
   also, presumably, gives me en/deryption timing data, for known, even,
   but not chosen plaintext; but that's just gravy).
2) Look at the tcpdump for the wired segment again (or run a new one) to
   find some open TCP connections.  These will pretty much all correspond
   to open TCP connections on the "inside"; routing firewalls and almost
   all NAT boxes will just pass packets sent from the outside straight
   through (a proxy firewall will require an application-layer attack)
3) Send duplicate ACKs (or wholly spurious ACKs) to the public address
   of the firewall in question and watch the wireless segment to see
   how long it takes to encrypt them.  Oh, by the way, they're chosen
   plaintext...

@_date: 2006-08-09 08:29:56
@_author: Thor Lancelot Simon 
@_subject: SSL Cert Prices & Notes 
You certainly can, if slipshod practices end up _costing_
you money.
Has CAcert stopped writing certificates with no DN yet?
Has CAcert stopped writing essentially unverifiable (or,
if you prefer to think of it that way, forensics-hostile)
CN-only certificates on the basis of a single email exchange
Has CAcert stopped using MD5 in all their signatures yet?

@_date: 2006-12-03 20:26:07
@_author: Thor Lancelot Simon 
@_subject: cellphones as room bugs 
It's been a while since I built ISDN equipment but I do not think this
is correct: can you show me how, exactly, one uses Q.931 to instruct the
other endpoint to go off-hook?

@_date: 2006-12-27 14:10:10
@_author: Thor Lancelot Simon 
@_subject: How important is FIPS 140-2 Level 1 cert? 
This (braindamaged) requirements change was brought in by the creation of
a Known Answer Test for the cipher-based RNG.  Prior to the addition of
that test, one could add additional entropy by changing the seed value at
each iteration of the generator.  But that makes it, of course, impossible
to get Known Answers that confirm that the generator actually imlements
the standard.  So suddenly the alternate form of the generator -- in my
opinion much less secure -- which uses a monotonically-increasing counter
for the seed, was the only permitted form.
I have yet to hear of anyone who has found a test lab that will certify
a generator implementation that uses the mono counter for the KAT suite
but a random seed in normal operation.  For good reason, labs are usually
very leery of algorithm implementations that come with a "special test
However, you are free to change the actual key for the generator as often
as you like.  I'm not sure why OpenSSL doesn't implement "fork protection"
that way, for example -- or does it use the MAC-based generator instead?

@_date: 2006-07-04 17:49:25
@_author: Thor Lancelot Simon 
@_subject: Use of TPM chip for RNG? 
See one of the examples in my other message today in this thread (subject
changed as an aid to new readers) for an example of why you should *not*
trust such certifications as evidence that the RNG is any good.
Summary: I have encountered one such RNG that was FIPS-140 certified as
a Deterministic RNG but whose "hardware" inputs the vendor refused to
disclose, which I find extremely suspicious.  It is possible to get a
DRNG certified without careful analysis of what its input is; I have
personally seen this happen and heard of more instances even after NIST
gave specific guidance to the contrary.

@_date: 2006-07-04 17:46:20
@_author: Thor Lancelot Simon 
@_subject: Dirty Secrets of "noise based" RNGs 
Do you actually know of publically available documentation on the design
and implementation of *any* of these "noise based" RNGs?  I have spent
some time looking, and I do not.
Here is what I do know:
1) There's one exception: Hifn documents the RNG used on their 65xx and
   can, upon request, provide documentation on exactly how the version
   on the common 79xx chips differs from this design.  They also provide
   a fairly good analysis (practical and theoretical) of the design's
   strength.
2) Hifn used to make this documentation publically available but access
   to it now requires permission from Hifn sales -- it has been password
   protected on their public web site.  In other words, after years of
   design wins based on little but open-source friendliness (after all,
   Hifn's chips are no faster, often slower, than others', and notoriously
   buggy) they are now, at least on this issue, biting the hand that feeds
   them.
3) Broadcom makes no RNG documentation, much less analysis, publically
   available.  If you're using their RNG without NDA documentation that
   may or may not even exist, it's on a "trust us...really!" basis.
4) Neither does any other crypto vendor for whose products open-source
   drivers are available, AFAICT.
5) Some general-purpose CPU and motherboard chipset vendors include RNGs
   in their product.  Intel used to do so, and had a very good analysis
   of their product available.  But then they muddied the water by making
   it impossible to tell which chips had real RNGs on them and which just
   had junk registers sampling who knows what -- probably bus noise in
   some cases.  And they now call the RNG product "end of life".
   AMD has an RNG on their host chipset for Opteron, as they did on their
   last server chipset for Athlon MP.  But they do not document how it
   works nor provide any analysis of its strength.
   I have not had time to investigate the situation vis-a-vis VIA.  I am
   told it's somewhat better, but I was told the Broadcom stuff was
   trustworthy, too, and then I found out that the person who said so did
   not really have documentation either!
6) I have run into one implementation of an "RNG" on a crypto processor
   from a major vendor that is actually clearly, once one reads between
   the lines of its documentation, an X9.31 Deterministic RNG using the
   symmetric crypto functionality of the chip.  The vendor's documentation
   is silent as to what the actual entropy source is, and they *did not
   respond to a direct inquiry* on the subject.  This product is FIPS-140
   certified; but it was clearly designed *only* to pass certification,
   and for obvious reasons, you should not trust it!
   A good FIPS-140 test lab should follow the guidance from NIST that the
   input source to the D. RNG must not contain less entropy than the
   output.  But it is possible to sneak almost anything past a test lab
   if you're crafty about it and this vendor's refusal to disclose to a
   high-volume customer where the input bits come from is really scary.
These all add up to "vendors are doing things with their 'noise-based'
RNGs that should *really* scare you".  If you are specifying such a RNG
for deployment, and you have any leverage over the vendor who makes it,
I strongly urge you to make disclosure of how it works, including any
analysis they've done, a condition of your use of their product.  The
Intel and Hifn white papers are good examples of what *every* vendor
should be willing to publically disclose, if their RNG design does not
give them something to hide.

@_date: 2006-07-27 20:40:29
@_author: Thor Lancelot Simon 
@_subject: Crypto to defend chip IP: snake oil or good idea? 
So, you sign the public key the chip generated, and inject the _signed_
key back into the chip, then package and ship it.  This is how the SDK
for IBM's crypto processors determines that it is talking to the genuine
IBM product.  It is a good idea, and it also leaves the chip set up for
you with a preloaded master secret (its private key) for encrypting other
keys for reuse in insecure environments, which is really handy.
But do we really think that general-purpose CPUs or DSPs are going to
be packaged in the kind of enclosure IBM uses to protect the private keys
inside its cryptographic modules?

@_date: 2006-07-28 13:08:45
@_author: Thor Lancelot Simon 
@_subject: Crypto to defend chip IP: snake oil or good idea? 
I don't get it.  How is there "no increase in vulnerability and threat"
if a manufacturer of counterfeit / copy chips can simply read the already
generated private key out of a legitimate chip (because it's not protected
by a tamperproof module, and the "significant post-fab security handling"
has been eliminated) and make as many chips with that private key as he
may care to?
Why should I believe it's any harder to steal the private key than to
steal a "static serial number"?

@_date: 2006-07-28 19:16:59
@_author: Thor Lancelot Simon 
@_subject: Crypto to defend chip IP: snake oil or good idea? 
The simple, cost-effective solution, then, would seem to be to generate
"static serial numbers" like cipher keys -- with sufficient randomness
and length that their sequence cannot be predicted.  I still do not see
the advantage (except to Certicom, who would doubtless like to charge a
bunch of money for their "20-40k gate crypto code") of using asymmetric
cryptography in this application.

@_date: 2006-07-28 22:19:43
@_author: Thor Lancelot Simon 
@_subject: Crypto to defend chip IP: snake oil or good idea? 
[...] The issue is with unnecessary complexity that yields (still, to my
eye) no demonstrable security benefit in the applications for which
the Certicom press release claimed it was intended.  As I said before,
I think the basic "chip generates key pair, public key signed during
manufacturing" solution is a very clever one -- but only to problems
which _also_ justify the cost of a very serious tamper-proofing effort
aimed at protecting the private key, and where it is a requirement of
the application that the original fab _itself_ not have that key as
part of the manufacturing process, e.g. where it will be used as a
master secret for persistent storage of other keys.  In other words,
for devices like IBM's cryptographic modules.  But for the purpose
Certicom claimed (and you seem also to be claiming) it's suited for:
As Perry said, chip fabs have plenty of diagnostic equipment that
would extract an RSA private key every bit as easily as it would
extract a private serial number, which means that the additional cost
of 20-40 gates, plus IP licensing, plus... for a cryptographic engine
is strictly wasted.  I am a happy Certicom customer but I certainly
wouldn't buy _this_ product from them.

@_date: 2006-07-29 16:24:12
@_author: Thor Lancelot Simon 
@_subject: Noise sources: multi-oscillator vs. semiconductor noise? 
I am working on a semi-experimental hardware RNG project.  I note that
the publically available designs, e.g. or  or the others
listed at  all use semiconductor
junction noise as the source -- diode or transistor avalanche noise.
But all the modern commercial designs with which I'm familiar (which
include the Intel, the Hifn, one Motorola design, and some others that
are not publically documented) are all multiple-oscillator designs, in
which some number (usually 2 or 3) of undisciplined oscillators of close
design frequency drift against one another, and an ADC is used to sample
the resulting output waveform.
I cannot find any public, rigorous discussion of why such a design might
be preferable to the semiconductor noise type of design -- but I have to
assume the people designing the commercial sources have all converged on
similar designs for _some_ reason.
Can someone point me to a discussion of the advantages or disadvantages
of either design type in the literature?  I am not interested in the
theoretical advantages of other, costlier sources such as radioactive-decay
or "more direct" (than junction noise) quantum or thermal noise sources;
I just want to understand why all the public domain designs are of one
type, and all the commercial designs of the other.

@_date: 2006-05-19 14:52:13
@_author: Thor Lancelot Simon 
@_subject: Crypto hardware with secure key storage 
I'm trying to investigate which of the current high-end PCI crypto
accellerators include secure storage of key material -- that is, the
use model where one loads, say, an RSA private key or key for a symmetric
cipher into the device one, receives a reference, and can later, even
after device power down, tell the card "use key with reference X for this
I realize that there are ways to do this without actual persistent storage
on the card, e.g. encryption of the key with a symmetric cipher using a
secret key stored in the card, which allows the cleartext key to be disposed
of so long as the card can be told "okay, decrypt and use this key" in the
future.  That's fine, too.
I've run into some vendors who claim to support "secure key storage"
but turn out to mean something else by it.  I'm specifically looking
for a device that accellerates pubkey operations and is aimed at SSL.
If people with experience with particular hardware want to share that
with me in private rather than broadcasting it to the list, that's fine,
too; I'm just trying to select a device to meet an immediate need and
am okay with not shouting out a comparison of vendor capabilites to the
entire world (though I do think it is regrettable that there's a lack
of information on this kind of device capability anywhere public).

@_date: 2006-10-06 17:29:21
@_author: Thor Lancelot Simon 
@_subject: TPM & disk crypto 
And the TPM knows that your BIOS has not lied about the checksum of grub

@_date: 2006-09-13 11:54:36
@_author: Thor Lancelot Simon 
@_subject: Exponent 3 damage spreads... 
DNSSEC seems to be not-uncommonly used to secure dynamic updates,
which is not the most common DNS feature in the world but it is not
so uncommon either.

@_date: 2006-09-14 13:00:00
@_author: Thor Lancelot Simon 
@_subject: RSA SecurID SID800 Token vulnerable by design 
[... a long message including much of what I can only regard as
 outright advertising for RSA, irrelevant to the actual technical
 weakness in the SID800 USB token that Hadmut described, and which
 Vin's message purportedly disputes.  It would be nice if, when confronted
 with such a "response" in the future, the moderator of this list would
 return it to its author with the requirement that the marketeering be
 stripped out before the actual content be forwarded to this list!  I
 have snipped everything irrelevant to my own response. ... ]
As well he should have, because they are utterly irrelevant to the
genuine design flaw which he pointed out, and which Vin seeks to
minimize (by burying it in irelevancies?) here.
And so it was.  Vin simply handwaves away the fact that if RSA's client
software can poll the token and retrieve the current OTP, so can any
malicious software running on the host to which the token is attached.
It is not correct to suggest that perhaps this could be done only once,
when the token were first plugged in to the host system's USB port,
because USB *by design* allows the host to cut and restore power to
devices under software control, so that the SID800 can, even if it
somehow is intended to only allow token retrieval "once upon plug-in
and once only" (something Vin seems to imply, but does not directly
state) simply be repeatedly tricked into thinking that it has just been
plugged in.
"Good cooking is about full-bodied flavor, not wire rope or persian
kittens"; but let's leave the irrelevant analogy aside and stick to the
facts that seem to be discussed here, I suppose.
Vin claims that the user "instructs the SecureID to load one OTP
token-code directly into the paste buffer".  This is a very, very odd
claim, because it implies that the user communicates directly with a
USB peripheral and "instructs" _the peripheral_ to autonomously "load"
a token-code -- some bytes -- into an area in host memory that is used
by the host operating system's user interface.  We should note that,
unlike Firewire, USB *does not include a mechanism by which a
peripheral on the bus may initiate a DMA transfer into the memory of
the host system that is the master of the bus* so clearly what Vin
claims cannot be strictly true.  What, then, should we think that it
likely means?  I think he must mean something like this:
  "The user instructs the RSA-supplied application code running on the
   host system to retrieve one token code from the SecureID across the
   USB bus, and place that retrieved token code into the paste buffer."
If that is not what Vin means, I think that he should respond and say
exactly what he does mean, in a way that does not make reference to
mythical properties that USB peripherals do not have.
Now, consider what it means that it is even _possible_ for the RSA-
supplied application to retrieve a token code from the SID800 in this
way.  It means that by, at worst, cutting and restoring power to the
USB port in question, malicious software can retrieve *a new, current
token code* *any time it wants to do so*.  In other words, while, with
traditional SecureID tokens, it is possible for malicious software to
steal token codes typed by the user into a compromised host system _when
the user types them_, and by engaging in a man-in-the-middle scheme
impersonate the intended target system to the user _that once_ (since
SecureID token codes can not be used twice), this new system does, in
fact, open up the gaping new security hole Hamisch claims it does:
   With this USB-connected token, malicious software on the host can poll
   the token and retrieve a *new* token-code *any time it wants to*,
   without the user's intervention and with no need to implement a complex
   man-in-the-middle attack on the user to avoid the user's notice.
And here we see that Vin evidently does not understand the nature of the
vulnerability RSA has created with their USB-connected token and host
software, because he seems to think that the problem is that the software,
by design, places a single token-code in the paste buffer; as he notes,
malicious software that could steal that code could just as easily steal
a code from a traditional SecureID token typed in by the user.
But that is not the problem!
The problem is that _because there is an interface to poll the token for
a code across the USB bus_, malicious software can *repeatedly* steal new
token codes *any time it wants to*.  This means that it can steal codes
when the user is not even attempting to authenticate, and so the SecureID
server's traditional countermeasure of denying near-simultaneous attempts
to use the same token code becomes *entirely worthless* for there will be
no genuing, user-initiated authentication attempt for the attacker's
submission of the stolen code to be simultaneous *with*.
It seems clear, in fact, that whether RSA has burned the "OTP paste"
feature into the token's firmware or not, any USB-connected token that
works even remotely like the SecureID token is vulnerable to this kind
of attack.  As long as token codes can be requested by the host system
across the USB bus and are passed back by the token, without the user's intervention, an attacker can use software on the host to steal token
codes when the user is *not* present to cause a simultaneous
authentication attempt (and thus raise an alarm on the server) and use
them, along with a keylogged password, at will.
It is noteworthy that a token that requires *any* kind of intervention
by the user -- even something as simple as a press of a button on the
token which illuminates when the host presents the token-code request --
is invulnerable to such an attack, potentially at a much lower cost in
terms of user interface difficulty than the traditional SecureID token
where the user's action is to type in the entire token-code by hand.
It is regrettable that instead of wasting time (and, evidently, the
time of its consultants) trying to handwave away the severe security
problem with the SID800 design, RSA does not turn its efforts to the
design of tokens which use simpler mechanisms (like the aforementioned
button-press) to ensure that the user is present, and thus maintain
security while not imposing on the user the onerous requirement of
typing in the whole code.
Simply to clear the air, I would like to ask Vin to answer the following
simple question: did you compose and send your response to Hadmut at the
request or suggestion of someone at RSA, or was the idea to send it
entirely your own?
P.S. I am not and never have been a consultant to, shareholder in, nor
     employee of, RSA nor any other company in the token or smart-card
     market.  If what I write is emphatic, that is probably because of
     my intense dislike for long, equivocating messages full of what I
     can only regard as marketing language, when they appear on technical
     lists in response to genuine technical contributions.

@_date: 2007-04-05 19:26:31
@_author: Thor Lancelot Simon 
@_subject: DNSSEC to be strangled at birth. 
That is, of course, false, and presumably is _exactly_ why DHS wants
the root signing key: because, with it, one can sign the appropriate
chain of keys to forge records for any zone one likes.
Plus, now that applications are keeping public keys for services in
the DNS, one can, in fact, forge those entries and thus conduct man in
the middle surveillance on anyone dumb enough to use DNS alone as a
trust conveyor for those protocols (e.g. SSH and quite possibly soon
I know you understand this stuff well enough to know these risks exist.
I'm curious why you'd minimize them.

@_date: 2007-04-05 19:54:24
@_author: Thor Lancelot Simon 
@_subject: DNSSEC to be strangled at birth. 
You're missing the point.  The root just signs itself a new .net key,
and then uses that to sign a new furble.net key, and so forth.  No
unusual key use is required.
It's a hierarchy of trust: if you have the top, you have it all, and
you can forge anything you like, including the keys used to sign the
application key records used to encrypt user data, where they are
present in the system.

@_date: 2007-04-05 21:13:41
@_author: Thor Lancelot Simon 
@_subject: DNSSEC to be strangled at birth. 
You assume the new .net key (and what's signed with it) would be
supplied to all users of the DNS, rather than used for a targeted
attack on one user (or a small number of users).  Why assume the
potential adversary will restrict himself to the dumbest possible
way to use the new tools you're about to hand him?
Do you really think that the administrator of the _average_ DNS
client would notice that a new key for .net showed up?  It's trivial
to inject forged UDP packets, after all, so it is hardly the case
that one has to give the new forged key chain to every DNS server along the way in order to run a nasty MITM attack on a client.

@_date: 2007-08-20 13:16:19
@_author: Thor Lancelot Simon 
@_subject: Skype new IT protection measure 
One wonders if it was their attorneys who suggested that they confirm
categorically that "x OR y" -- never mind that no malicious activities
being "attributed" (by whom? to whom? "attribute" takes a subject and an
object, and neither is optional!  note the passive voice!) doesn't mean,
of course, that none occurred, nor than none were discovered by others
or even by Skype itself.
Reads like classic corporate weasel-wording to me.

@_date: 2007-12-13 20:29:47
@_author: Thor Lancelot Simon 
@_subject: Flaws in OpenSSL FIPS Object Module 
In fact, I was in the middle of a FIPS-140 certification at level 2
a number of years ago when the Known Answer Test for the X9.17 block
cipher based PRNG was introduced.  One unanticipated side effect of
this test was to make it impossible to actually use a clock or free
running counter as the counter in the PRNG, since the KAT expected
the simplistic "increment counter by 1 every time a block is extracted"
behavior chosen by most implementers.
Of course, that mode is _less_ secure (because the internal state is
more predictable) than the other, but given the choice between "validate
PRNG using special mode, run it using normal mode" or "validate PRNG
using special mode, run it using special mode" I know I'd pretty much
always take the latter.  In fact, the test lab we were using told us
they were quite skeptical about the former as well.
Fortunately, the requirement for the PRNG KAT was delayed long enough
to let us get our code out the door without having to actually choose
either of the unpalatble ways.  But it does highlight a certain tension
in the process: they want to know that algorithms have predictable
(correct) results, but RNGs are supposed to have unpredicatable (correct)
results.  So any PRNG that is testable as part of the certification
process pretty much _has to_ have two modes, and bugs like this may
be more likely to occur in normal operation.

@_date: 2007-12-14 13:27:57
@_author: Thor Lancelot Simon 
@_subject: Flaws in OpenSSL FIPS Object Module 
The PRNG test which requires DT to be run as a monotonic counter is, in
fact, a known-answer test.  To run that test at all, if one's PRNG is
implemented (as was permitted prior to the introduction of that test)
with DT as an actual clock, one has to alter the mode of operation of
the PRNG in a rather fundamental way.
I'm not sure what you're suggesting, but I think you misunderstand the
difference between the validation tests for AES and those for the PRNG.
The validation tests for AES do not dictate internal implementation details
of the DUT beyond the extent specified in the controlling standards, and
thus a conformant AES implementation cannot need to have its internals
rearranged in order to be attached to the test fixture.
That is *not* the case for the X9.17 PRNG and the test now specified for
it, which means that the test, itself, creates new opportunities for very
nasty bugs, or requires those who implemented the PRNG using a clock or
free-running counter to reimplement using a simple counter.  I tend to
think that this is a regrettable decrease in the strength of the RNG
but that, regardless, implementations of an algorithm should be required
to actually run the validated algorithm, not some other one that is close,
except that part of the algorithm's guts were swapped out for testing.

@_date: 2007-01-28 12:47:18
@_author: Thor Lancelot Simon 
@_subject: OT: SSL certificate chain problems 
That doesn't make sense to me -- the end-of-chain (server or client)
certificate won't be signed by _both_ the old and new root, I wouldn't
think (does x.509 even make this possible)?
That means that for a party trying to validate a certificate signed by
the new root, but who has only the old root, the new root's certificate
will be a necessary intermediate step in the chain to the old root, which
that party trusts (assuming the new root is signed by the old root, that
Or do I misunderstand?

@_date: 2007-07-01 16:59:56
@_author: Thor Lancelot Simon 
@_subject: The bank fraud blame game 
Who hasn't?  Oh, I'm sorry -- I meant to say: who, outside of the
set of producers and consumers of security snake oil aimed at
financial institutions, hasn't?
Regular readers will recall the SecurID discussion of about a
year ago, when an individual who appeared to be a paid consultant
to RSA vigorously put forth the notion that secure devices which
required the user to actually do something to authenticate a
transaction were _not_ what was needed -- to the shock and awe
of most readers of, and writers to, the thread here, at least
as I would summarize the discussion.

@_date: 2007-05-09 15:35:44
@_author: Thor Lancelot Simon 
@_subject: More info in my AES128-CBC question 
ESP does not chain blocks across packets.  One could produce an ESP
implementation that did so, but there is really no good reason for
that, and as has been widely discussed, an implementation SHOULD use
a PRNG to generate the IV for each packet.

@_date: 2007-05-28 01:03:51
@_author: Thor Lancelot Simon 
@_subject: 307 digit number factored 
I don't buy it.  I build HTTP load balancers for a living, and for
basically all of our customers who use our HTTPS accelleration at all,
the cost of 1024-bit RSA is already, by a hefty margin, with hardware
assist, the limiting factor for performance.  Look at the specs on
some of the common accelelrator families sometime: 2048 bit is going to
be quite a bit worse.
Busy web sites that rely on HTTPS are going to pay a fairly heavy price
for using longer keys, and not just in cycles: the few hardware solutions
still on the market that can stash keys in secure storage, of course, can
stash exactly half as many 2048-bit keys as 1024-bit ones.  Users who care
about HTTPS performance aren't as rare, I think, as you think.
What's more frustrating is the slow rate at which accellerator vendors
have moved ECC products towards market.  That's not going to help with
adoption any.

@_date: 2007-09-02 18:49:15
@_author: Thor Lancelot Simon 
@_subject: debunking snake oil 
That's right, you have.  As I recall, the last time you posted here was
when you tried to defend RSA's decision to sell no-human-interaction
tokens.  At that time, I asked you whether you were posting for yourself
or whether someone at RSA had asked you to post here, and you declined
to respond.
I think it's important that we know, when flaws in commercial
cryptographic products are being discussed, what the interests of the
parties to the discussion are.  So, I'll ask again, as I did last time:
when you post here, both in this instance and in past instances, is it
at your own behest, or that of RSA?

@_date: 2007-09-03 19:15:47
@_author: Thor Lancelot Simon 
@_subject: debunking snake oil 
[And a couple of hundred more lines -- but no actual direct answer to
 the question!]
I'll try again: yes, you've identified yourself as a consultant to RSA.
When you have posted here, both in this most recent thread and in other
threads, in particular the SecurID 800 thread, has it been at your own
behest, or that of RSA?
In other words, when you post here defending RSA products against
criticism, often with very emphatic language and in a way that belittles
the person making the criticism rather than engaging with the actual
technical critique, can we assume that it is not the case that RSA
asked you to do so?  Or is it, in fact, sometimes the case that RSA
asks you to post about their products here, and thus we should read your
words as being RSA's words?
I don't think it's an unreasonable question, and I ask it one more time
because, despite all the vitriol you directed at me (including the rather
odd choice to refer to me by my middle name rather than in a more normal
way) you did not, in fact, answer it.

@_date: 2008-08-09 21:45:05
@_author: Thor Lancelot Simon 
@_subject: security questions 
When I worked at DEC, in 1991, at least one internal purchasing system
used this method of authentication.  As a summer hire, I couldn't use it,
but my boss had to authenticate this way whenever he made any major
equipment order or transfer for our group.  IIRC, it used personal data
already available to DEC -- so they didn't have to ask their employees
for it -- emergency contact phone numbers, names of other insured parties
on their health care, license plates of cars authorized to park in the work
lot, etc -- and asked a small number of random questions for each
I thought it was pretty clever.  I still do, actually.

@_date: 2008-01-31 15:46:47
@_author: Thor Lancelot Simon 
@_subject: Gutmann Soundwave Therapy 
There is no valid reason to ship snake oil cryptography (at any moment).
There is no standard but a high standard which is appropriate for
Since SSL was already available, there was no excuse to do anything
It seems that you still don't understand those things, or you would not
complain about them even at this far removed date.  How unfortunate.

@_date: 2008-07-14 10:42:23
@_author: Thor Lancelot Simon 
@_subject: Mifare 
Really?  From a cryptographic -- not a political -- point of view, what
exactly is wrong with DNSSEC or WPA?
WPA certainly seems to be quite widely deployed.

@_date: 2008-05-01 13:50:09
@_author: Thor Lancelot Simon 
@_subject: User interface, security, and "simplicity" 
It's fashionable in some circles (including, it seems, this one) to bash
IPsec (particularly IKE) and tout SSL VPNs (particularly OpenVPN) on what
are basically user interface grounds.
I cannot help repeatedly noting that -- I believe more so than with actual
IPsec deployments, whether with or without IKE -- OpenVPN deployments are
often configured in hideously insecure ways.  This is no more the fault of
OpenVPN's designers, of course, than the ghastly configuration interfaces
imposed by many IKE impledmentations are the fault of IPsec's designers.
See, for example, which is the official documentation from the popular "pfsense"
firewall/NAT/VPN package on configuring OpenVPN for use with clients.  Of
particular note:
The upshot is that, indeed, at least as shown here, this particular
configuration frontend to OpenVPN is very easy to configure -- if you
are willing to settle for much less security than OpenVPN was designed
to provide, and much less than, if you're naive about cryptography, you
probably think you're getting.
Gee, that's funny, that's one of the problems with IPsec implementations
that people always cite when they tout SSL VPNs (the other is that some
firewalls can't be configured to pass IP protocol 50 for ESP -- but, of
course, ESP can be tunneled in UDP, in a standard way, and that's been
true for years now).
I am left with the strong suspicion that SSL VPNs are "easier to configure
and use" because a large percentage of their user population simply is not
very sensitive to how much security is actually provided.  Someone said
"have a firewall", they set up a firewall.  Someone says "I can't get in
through the firewall, set up a VPN", they set up a VPN.  For their purposes
IP over DNS might serve just as well -- and if enough other people said it
was secure, they'd probably get all defensive if you said it wasn't, at
least not how they'd configured it.
One could think of it, I suppose, as a combination of drinking the Kool
Aid and buying the snake oil -- drinking the snake oil?  Whatever one calls
it one should be very careful of its effects on the popular consciousness
when trying to understand what user preferences for this security product
over that one actually mean.

@_date: 2008-05-04 13:46:36
@_author: Thor Lancelot Simon 
@_subject: User interface, security, and "simplicity" 
And yet there's no underlying technical reason why it is any easier to
configure than IPsec is; it is all a matter of the configuration interface
provided by your chosen SSL VPN (in this case, OpenVPN) or IPsec
I find it amusing (but somewhat sad) that in fact one can find basically
the same set of flaws in each, but they're considered damning in IPsec
while they're handwaved away or overlooked in SSL VPNs.  Of course you
(Perry) or I can configure either IPsec or OpenVPN in a safe and sane way;
and, of course, there are some VPN packages of either type (IPsec or SSL
VPN) which have configuration interfaces so bad that we _couldn't_, in
fact, set them up safely -- because they prevent safe, sane configuration.
The problem is that whether you or I _can_ set software X up safely isn't
the question that matters.  The question that matters is "_will_ a naive
user who does not understand the underlying security questions set software
X up securely".
And, in fact, most VPN software of any type fails this test.  My concern
is that an excessive focus on "how hard is it to set this thing up?" can
seriously obscure the important second half of the question "and if you
set it up in the easiest possible way, is it safe?"

@_date: 2008-05-04 22:24:13
@_author: Thor Lancelot Simon 
@_subject: User interface, security, and "simplicity" 
No.  Your claim sounds plausible because it's a much, much stronger form
of a claim which almost always _is_ true:  "If there is a wrong way to
do it, _some_ end users will do it wrong."
But that is not the same claim as "If there is a wrong way to do it,
_most_ end users will do it wrong", a claim which usually seems to be
made because someone who understood cryptography but not human factors
just decided that the problem he didn't know how to solve wasn't
important because he didn't know how to solve it.
The fact that that mistake (in essence, assuming "it is necessary that
most users will get it wrong" instead of "it is possible that most users
will get it wrong) is not pointed out when it is, so often, made, is,
indeed, the typical excuse for security software not bothering to supply
a good user interface such that most of the time, most users get it
That in no way means that such a user interface is not desirable, any
more than low standards in the area mean that it is not possible.
I believe that those who supply security products have a responsibility
to consider the knowledge, experience, and tendencies of their likely
users to the greatest extent to which they're able, and supply products
which will function properly _as users are likely to apply them_.  I
believe that not considering those questions at all is irresponsible
and in some cases much worse than that.  Pretending that the questions
don't exist is _definitely_ worse than irresponsible; I've quit jobs
when asked to behave that way, in the past, and I'd probably do so

@_date: 2008-09-22 12:03:31
@_author: Thor Lancelot Simon 
@_subject: Lava lamp random number generator made useful? 
I looked into this at moderate length about two years ago.  One very
attractive choice was the cheapest Motorola Coldfire with their onboard
crypto block, because you get the hashing for free and don't waste host
resources transferring in data you'll then distill by hash -- or hashing
As a source of random numbers, I was figuring to use one of the publically
available thermal noise designs plus the cheapest HiFn PCI crypto chip
(which features a multi-oscillator RNG I'm reasonably familiar with) since
the Coldfire with crypto has both USB and PCI on it.

@_date: 2009-08-25 13:54:32
@_author: Thor Lancelot Simon 
@_subject: SHA-1 and Git (was Re: [tahoe-dev] Tahoe-LAFS key management, 
Look at the difference between the time it requires to add an algorithm
to OpenSSL and the time it requires to add a new SSL or TLS version to
OpenSSL.  Or should we expect TLS 1.2 support any day now?  If earlier
TLS versions had been designed to allow the hash functions in the PRF
to be swapped out, the exercise of recovering from new horrible problems
with SHA1 would be vastly simpler, easier, and far quicker.  It is just
not the case that the software development exercise of implementing a
new protocol is on a scale with that of implementing a new cipher or hash
function -- it is far, far larger, and that, alone, seems to me to be
sufficient reason to design protocols so that algorithms and algorithm
parameter sizes are not fixed.

@_date: 2009-08-27 11:23:41
@_author: Thor Lancelot Simon 
@_subject: SHA-1 and Git (was Re: [tahoe-dev] Tahoe-LAFS key management, 
I think we're largely talking past one another.  As regards "new horrible
problems" I meant simply that if there _are_ "new horrible problems_ such
that we need to switch away from SHA1 in the TLS PRF, the design mistakes
made in TLS 1.1 will make it much harder.
As I read Ben's comments, they were _advocating_ those kinds of design
mistakes, advocating hard-wiring particular algorithms or their parameter
sizes into protocols, because -- as I understood him -- both replacing
and algorithm and replacing a whole protocol are just "software upgrades"
and all software upgrades are alike.
Well, I don't think it's true that all software upgrades are alike in the
relevant way.  In fact, it is radically harder to replace an entire
protocol, even with a related one, than to drop a new algorithm into an
existing, properly-designed protocol.  It may be no different for _users_,
but the difference for _implementers_ is vast, and that greatly delays the
availability of the relevant software upgrade to users, which is not a
good thing.
I think the current TLS 1.2 debacle is about the best evidence of this one
could ask for.  If TLS 1.{0,1} had been designed to make the hash functions
pluggagle everywhere they're used, then users would have new software which
didn't rely on SHA1 (even in a way we currently think is still safe)
available now, rather than having to wait quite a bit longer before the
possibility of upgrading even arose.

@_date: 2009-01-26 15:30:49
@_author: Thor Lancelot Simon 
@_subject: Obama's secure PDA 
I know no specific details but strongly suspect the difference in
requirements, and thus certifications, stems from the likelyhood that
the device stores (even very briefly) email and cached web objects, but
does not store voice communications.

@_date: 2009-01-30 13:49:56
@_author: Thor Lancelot Simon 
@_subject: full-disk subversion standards released 
People have funny notions of "ownership", don't they?
It's very clear to me that I don't own my desktop machine at my office;
my employer does.  But even if TCG were to punch out a useful, reasonable
standard (which I do not think they have done in any case so far), the
policy problem of how to ensure that my desktop machine's actual owner
could enforce its ownership of that machine against me, while the retailer
who sold me my desktop machine at home -- which I do own -- or for that
matter the U.S. Government, can't enforce _its_ "ownership" of my own
machine against me; that's a real problem, and solutions to it are useful.
Given such solutions, frameworks like what TCG is chartered to build are
in fact good and useful.  I don't think it's right to blame the tool (or
the implementation details of a particular instance of a particular kind
of tool) for the idiot carpenter.

@_date: 2009-01-30 20:35:28
@_author: Thor Lancelot Simon 
@_subject: full-disk subversion standards released 
Okay.  In that case, please, explain to me why you are not opposed
to the the manufacture and sale of digital computers.
More gently: it seems to me that there is an "only" missing from your
sentence above, or else it is almost by necessity a straw-man argument:
it will, if consistently applied as you have stated it, hold against
various tools I do not believe you actually oppose the manufacture or
sale of, such as printing presses, guns, and door locks.
Many of TCG's documents purport to specify mechanisms that are in fact
generally useful for beneficial purposes, such as boot-time validation
of software environments, secure storage of cryptographic keys, or
low-bandwidth generation of good random numbers.
Do you actually mean that such things should not be built, or only that
you are suspicious of TCG's intent in building them?
In text I've snipped, you claimed to describe TCG's charter.  I must
admit that I don't know if they even actually have such a document.
But, on the other hand, they describe their own purpose like this
(these are their actual words):
"The Trusted Computing Group (TCG) is a not-for-profit organization formed to
develop, define, and promote open standards for hardware-enabled trusted
computing and security technologies, including hardware building blocks and
software interfaces, across multiple platforms, peripherals, and devices. TCG
specifications will enable more secure computing environments without
compromising functional integrity, privacy, or individual rights. The primary
goal is to help users protect their information assets (data, passwords, keys,
etc.) from compromise due to external software attack and physical theft."
I happen to think that if those _stated_ goals were achieved, that would
be a good thing, and that there are in fact hardware and software mechanisms
that could help achieve them -- some of which TCG has made stabs at
specifying, though they've generally missed the mark.
Leaving aside your assertions about TCG's _actual_ goals -- which may be
correct -- are you really of the position that what's described above,
no matter who were to build it nor how well, would be only useful for
"monopoly and totalitarianism"?

@_date: 2009-03-06 12:30:02
@_author: Thor Lancelot Simon 
@_subject: full-disk subversion standards released 
But this, itself, is valuable.  Given trivial support in the operating system
kernel, it eliminates one of the most common key-theft attack vectors
against webservers.
I must admit I'm curious whether the TPM vendors are licensing the relevant
IBM patent on what amounts to any wrapping of cryptographic keys using
encryption - I can only assume they are.

@_date: 2009-03-06 14:01:26
@_author: Thor Lancelot Simon 
@_subject: full-disk subversion standards released 
Almost no web servers run with passwords on their private key files.
Believe me.  I build server load balancers for a living and I see a _lot_
of customer web servers -- this is how it is.
No, no there's not.  In fact, I solicited information here about crypto
accellerators with onboard persistent key memory ("secure key storage")
about two years ago and got basically no responses except pointers to
the same old, discontinued or obsolete products I was trying to replace.

@_date: 2009-03-16 14:31:11
@_author: Thor Lancelot Simon 
@_subject: full-disk subversion standards released 
Nitrox doesn't have onboard key memory.  Cavium's FIPS140 certified
Nitrox board-level solutions include a smartcard and a bunch of
additional hardware and software which implement (among other things)
secure key storage -- but these are a world apart from the run of the
mill Nitrox parts one finds embedded in all kinds of commonplace
devices.  They also provide an API which is tailored for FIPS140 compliance:
good if you need it, far from ideal for the common case for web servers, and
very different from the standard set of tools one gets for the bare Nitrox
There are of course similar board-level solutions using BCM582x as the
crypto core.  But in terms of cost and complexity I might as well just
use custom hardware -- I'd probably come out ahead.  And you can't just
_ignore_ performance, nor new algorithms, so eventually using very old
crypto cores makes the whole thing fail to fly.  (If "moderate"
performance will suffice, I note that NBMK Encryption will still sell
you the old NetOctave NSP2000, which is a pretty nice design that has
onboard key storage but lacks AES, larger SHA variants, and other modern
To the extent of my knowledge there are currently _no_ generally
available, general-purpose crypto accellerator chip-level products with
onboard key storage or key wrapping support, with the exception of parts
first sold more than 5 years ago and being shipped now from old stock.
This was once a somewhat common feature on accellerators targetted at
the SSL/IPsec market.  That appears to no longer be the case.

@_date: 2009-05-07 22:17:42
@_author: Thor Lancelot Simon 
@_subject: bcm586x has onboard key storage 
I don't have any details of how it works (and I don't know how hard
it would be to get Broadcom to cough them up -- they seem better about
this lately than they used to be) but looking at the bcm586x product
announcement, I see they added onboard key storage.

@_date: 2010-08-11 12:36:11
@_author: Thor Lancelot Simon 
@_subject: A mighty fortress is our PKI, Part II 
If you want to see a PKI tragedy in the making, have a look at the CRLs
used by the US DoD.

@_date: 2010-08-14 17:59:21
@_author: Thor Lancelot Simon 
@_subject: Has there been a change in US banking regulations recently? 
Indeed.  The way forward would seem to be ECC, but show me a load balancer
or even a dedicated SSL offload device which supports ECC.  I'm not even
certain the popular clients, which are usually well ahead of everything
else in terms of cryptography support, can cope with it.  The only place
it seems to be consistently used is in proprietary client/server software
for mobile devices, as has been the case for years.

@_date: 2010-08-27 14:02:28
@_author: Thor Lancelot Simon 
@_subject: questions about RNGs and FIPS 140 
I am surprised you'd have trouble with this at any lab.  Isn't there
specific guidance on this in the DTRs?  My 10-years-rusty recollection
is that, specifically, the input used to key the Approved RNG may not
contain provably less entropy than the Approved RNG's output, or words
very close to that in effect.

@_date: 2010-08-28 23:10:34
@_author: Thor Lancelot Simon 
@_subject: questions about RNGs and FIPS 140 
That doesn't make any sense.  DT in that generator is really meant to
serve the role of a counter, and, in fact, the test harness for that
generator *requires* it to be a counter.
The seed for that generator is K.

@_date: 2010-08-29 23:36:17
@_author: Thor Lancelot Simon 
@_subject: questions about RNGs and FIPS 140 
I'm sorry, I don't buy it.  I am aware that some labs will not allow the
use of actual time and date in DT to feed in additional entropy as the
generator runs.  But when this discussion started, as far as I can tell
you were claiming that some lab does not allow the use of non-deterministic
entropy sources to seed the X9.17 generator *at all*.
I don't believe that, because it amounts to telling you how and when to
set K, which is the key used to key the cipher that is the core of this
DRNG, and the how and when that you'd have to, in this case, be told,
would appear to directly contradict the Derived Test Requirements.
Believe me, I was quite annoyed the first time I discovered I could
not actually use the real date and time in DT, since that is the only
measure that provides any resistance to keystream recovery in this
generator between rekeyings.  I think I've mentioned it before on
this list.  But that does not mean that you can't key the 9.17 generator
from a hardware entropy source; it is really another question entirely.

@_date: 2010-07-14 03:35:29
@_author: Thor Lancelot Simon 
@_subject: Intel to also add RNG 
I couldn't say, as regards AMD's chipset RNG.  Intel's, however, was on
an optional component of one of their motherboard chipsets.  Many
motherboard vendors chose to buy that component from other sources, who
implemented something register-compatible to the Intel part but with
the RNG register not actually connected to a random number source.
Worse, when Intel increased chipset integration and pulled the optional
chip "into" one of the host bridge chips, they did the exact same thing.
The basic problem was that the register indicating presence-of-RNG was
not on the same piece of silicon (originally) as the actual RNG.  So the
register really indicated only that this Intel chipset *was capable of
interfacing to the chip with the RNG on it*; nothing more.
Worse, a lot of people read noise -- but not really random noise --
from those notional RNG registers and persuaded themselves that since
the output wasn't continuous, there must really be an RNG present.

@_date: 2010-10-06 16:43:45
@_author: Thor Lancelot Simon 
@_subject: Formal notice given of rearrangement of deck chairs on RMS 
You may have missed the next sentence of Mozilla's statement:
That is, no matter how long your root key is (the previous sentence
stated the requirements about _that_) you may not use it to sign any
end-entity certificate whose key size is < 2048 bits.
Now they have everything they need to prevent HTTPS Everywhere.

@_date: 2010-09-14 17:33:58
@_author: Thor Lancelot Simon 
@_subject: Folly of looking at CA cert lifetimes 
I don't understand.  The original text seems to be talking about *server*
certificate lifetimes, and how much shorter they are than CA cert
lifetimes.  What does that have to do with "a thousand times no" about
some proposition to do with CA cert lifetimes?
In other words, if CA key lifetimes are longer than indicated by their
X.509 properties, it seems to me that just makes the quoted text about
the relationship between server and CA key lifetimes even more true.

@_date: 2010-09-29 16:03:18
@_author: Thor Lancelot Simon 
@_subject: 2048 bits, damn the electrons! [rt@openssl.org: [openssl.org 
See below, which includes a handy pointer to the Microsoft and Mozilla
policy statements "requiring" CAs to cease signing anything shorter than
2048 bits.
As I think I said last week -- was it last week? -- it's my belief that
cutting everything on the Web over to 2048 bits rather than, say, 1280
or 1536 bits in the near term will be a significant net loss of security,
since the huge increase in computation required will delay or prevent the
deployment of "SSL everywhere".
These certificates (the end-site ones) have lifetimes of about 3 years
maximum.  Who here thinks 1280 bit keys will be factored by 2014?  *Sigh*.
----- Forwarded message from Rob Stradling via RT

@_date: 2010-09-30 11:41:18
@_author: Thor Lancelot Simon 
@_subject: 2048 bits, damn the electrons! [rt@openssl.org: [openssl.org 
At 1024 bits, it is not.  But you are looking at a factor of *9* increase
in computational cost when you go immediately to 2048 bits.  At that point,
the bottleneck for many applications shifts, particularly those which are
served by offload engines specifically to move the bottleneck so it's not
RSA in the first place.
Also, consider devices such as deep-inspection firewalls or application
traffic managers which must by their nature offload SSL processing in
order to inspect and possibly modify data before application servers see it.  The inspection or modification function often does not parallelize
nearly as well as the web application logic itself, and so it is often
not practical to handle it in a distributed way and "just add more CPU".
At present, these devices use the highest performance modular-math ASICs
available and can just about keep up with current web applications'
transaction rates.  Make the modular math an order of magnitude slower
and suddenly you will find you can't put these devices in front of some
applications at all.
This too will hinder the deployment of "SSL everywhere", and handwaving
about how for some particular application, the bottleneck won't be at
the front-end server even if it is an order of magnitude slower for it
to do the RSA operation itself will not make that problem go away.

@_date: 2010-09-30 14:16:31
@_author: Thor Lancelot Simon 
@_subject: 2048 bits, damn the electrons! [rt@openssl.org: [openssl.org 
[I wrote]:
No, I don't mean that, because if the administrator of site _X_ decides
to do SSL processing on a front-end device instead of on the HTTP servers,
for whatever reason, that is simply not a MITM attack.
To characterize it as one is basically obfuscatory.
When I talk about "SSL everywhere" being an immediate opportunity, I mean
that, from my point of view, it looks like there's a growing realization
that _for current key sizes and server workloads_, for many high transaction
rate sites like Gmail, using SSL is basically free -- so you might as well,
and we all end up better off.
Mutliplying the cost of the SSL session negotiation by a small factor will
change that for a few sites, but multiplying it by a factor somewhere from
8 to 11 (depending on different measurements posted here in previous
discussions) will change it for a lot more.
That's very unfortunate, from my point of view, because I believe it is
a much greater net good to have most or almost all HTTP traffic encrypted
than it is for individual websites to have keys that expire in 3 years,
but are resistant to factoring for 20 years.
The balance is just plain different for end keys and CA keys.  A
one-size-fits-all approach using the key length appropriate for the CA
will hinder universal deployment of SSL/TLS at the end sites.  That is
not a good thing.

@_date: 2010-09-30 13:32:38
@_author: Thor Lancelot Simon 
@_subject: 2048 bits, damn the electrons! [rt@openssl.org: [openssl.org 
This is a neat idea!  But it means changing the TLS standard, yes?

@_date: 2013-09-07 15:44:01
@_author: Thor Lancelot Simon 
@_subject: [Cryptography] Washington Post: Google racing to encrypt links 
One wonders whether, if what we read around here lately is much guide,
they still believe they can get link encryption systems that are
robust against the only adversary likely to be attacking their North
American links?

@_date: 2013-09-08 15:10:45
@_author: Thor Lancelot Simon 
@_subject: [Cryptography] Techniques for malevolent crypto hardware (Re: 
I'd go for leaking symmetric cipher key bits into exposed RNG output:
nonces, explicit IVs, and the like.  Crypto hardware with "macro" or
"record" operations (ESP or TLS record/packet handling as a single
operation; TLS or IKE handshake, etc.) offers ample opportunities for
this, but surely it could be arranged even with simpler hardware that
just happens to accellerate both, let's say, AES and random number

@_date: 2013-09-08 15:55:52
@_author: Thor Lancelot Simon 
@_subject: [Cryptography] Techniques for malevolent crypto hardware 
Well, I guess it depends what your definition of "blatant" is.  Treating
the crypto hardware as a black box, it would be freaking hard to detect,
no?  And not so easy even if you're willing to go at the thing at the
gate level.  You could end up forced to examine everything attached to
any of your crypto chip's I/Os, too, and it goes rapidly downhill from
When we build protocols that have data elements we *expect* to be random,
and rely on cryptographic primitives whose outputs we expect to be
indistinguishable from random, we kind of set ourselves up for this
type of attack.
Not that I see an easy way not to.
I also wonder -- again, not entirely my own idea, my whiteboard partner
can speak up for himself if he wants to -- about whether we're going
to make ourselves better or worse off by rushing to the "safety" of
PFS ciphersuites, which, with their reliance on DH, in the absence of
good RNGs may make it *easier* for the adversary to recover our eventual
symmetric-cipher keys, rather than harder!

@_date: 2013-09-12 22:38:27
@_author: Thor Lancelot Simon 
@_subject: [Cryptography] Thoughts on hardware randomness sources 
We looked briefly at this during one of my efforts to improve
entropy availability on small or embedded systems running NetBSD.
I was inspired by the insight on pages 12-13 of  :
So I tore through the system looking for anything even indirectly
caused by physical variation or human interaction that we could measure
and inject into the entropy pool -- what's listed above, power plug
and battery state change / charge level, physical addresses of VM
system pages faulted in at various intervals, skew between clocks
potentially derived from separate oscillators on the motherboard, etc.
And at least on some systems, we do a pretty decent job getting those in.
The audio subsystem actually posed *two* obvious opportunities: amplifier
noise from channels with high final stage gain but connected by a mixer
to muted inputs, and clock skew between system timers and audio sample
clocks.  The former requires a lot of interaction with specific audio
hardware at a low level, and with a million different wirings of input to
mixer to ADC, it looks hard (though surely not impossible) to quickly
code up anything generally useful.  The latter would be easier, and it
has the advantage you can do it opportunistically any time the audio
subsystem is doing anything *else*, without even touching the actual
sample data.
Unfortunately, both of them burn power like the pumps at Fukushima,
which makes them poorly suited for the small systems with few other
sources of entropy which were one of my major targets for this.  So they
are still sitting on some back back back burner.  Someday, perhaps...

@_date: 2013-09-12 22:48:27
@_author: Thor Lancelot Simon 
@_subject: [Cryptography] Radioactive random numbers 
Or that a design wasn't sabotaged intentionally wasn't sabotaged
accidentally while dropping it into place in a slightly different
product.  I've always thought highly of the design of the Hifn RNG
block, and the outside analysis of it which they published, but years
ago at Reefedge we found a bug in its integration into a popular Hifn
crypto processor that evidently had slipped through the cracks -- I
discussed it in more detail last year at
 .

@_date: 2014-02-05 20:05:51
@_author: Thor Lancelot Simon 
@_subject: [Cryptography] Mac OS 10.7.5 Random Numbers 
Oh, neat.  Now, after 13 years of wondering, I know who got Yarrow through
a FIPS-140 test lab -- and which lab.
(I'd heard through the grapevine that this had been done despite the fact
 that it seemed clearly counter to the DTRs in effect at the time; but I
 never dug up the details.)

@_date: 2015-12-22 19:09:38
@_author: Thor Lancelot Simon 
@_subject: [Cryptography] Juniper & Dual_EC_DRBG 
I'm quite curious where they chose to leak the output -- the obvious place,
for this general kind of attack, is in the explicit IVs carried in every
IPsec ESP packet, for instance, but Dual_EC is too slow to use for IV
generation in this application.
One of the nonces in an early IKE message?

@_date: 2015-12-22 19:38:22
@_author: Thor Lancelot Simon 
@_subject: [Cryptography] Juniper & Dual_EC_DRBG 
Adding to my puzzlement: I just checked my recollection, and indeed
the Juniper ScreenOS devices (e.g. SSG) use Cavium Nitrox crypto ASICs.
I am more familiar with SSL than IPsec microcode loads for Nitrox, but
I am 90% sure that just as is the case for SSL, Nitrox provides "macro"
operations that do entire messages of the IKE handshake, and it definitey
does ESP record processing one-shot.  Given the latencies involved in
accessing an accelerator of this kind, you'd be nuts to use the raw
crypto ops instead. Nitrox has an onboard noise source and X9.31 RNG and uses it in its
record and macro ops to fill in the random fields of messages.
So I am just not sure what would have been generated by the system RNG
nor how to leak it: the accellerator should be generating all the random
fields of all the messages and stamping them in for you, and certainly
it should be generating the actual session keys.
So what's being generated by the system RNG and how is it being leaked?

@_date: 2015-12-24 20:39:54
@_author: Thor Lancelot Simon 
@_subject: [Cryptography] Juniper & Dual_EC_DRBG 
I think you're on the wrong path here: why would anyone bother to
subvert the system RNG if the crypto accellerator were already subverted?
What I'm asking is *how subverting the system RNG* led to loss of
confidentiality for VPN sessions, *given that the system appears to
use an accelerator which has its own RNG and stamps that RNG's output
into packets*.

@_date: 2015-12-25 12:30:08
@_author: Thor Lancelot Simon 
@_subject: [Cryptography] Juniper & Dual_EC_DRBG 
I believe even the lowest-end ScreenOS devices used (the lowest-end)
Cavium accellerators for packet encryption/decryption.  Certainly if
you peel apart the smaller SSG boxes you'll find a Cavium chip in there.
I think I still can't talk about any prices anyone might have paid
for these parts, but I can say that there's a reason the CN1010 and its
even-littler siblings remained in production long after Cavium had moved
on to newer and faster things.
Discussions with some friends and former colleagues suggest that Cavium may
have had some kind of IKE support in very early IPsec microcode releases
but that later releases were ESP/AH only.  I also found some notes from
when I was doing SSL stuff with these chips that make it clear that in
a number of operations, the caller in fact explicitly supplied random values
to the chip (even if Cavium's software toolkit may have earlier _obtained_
those same values from the onboard RNG and buffered them up for later use).
The operations in question would be pretty directly analogous to IKE Phase 1,
so I think all in all it does make sense that -- even with a crypto chip
in use, and whether or not it is or ever was used for IKE -- the first few
messages of the IKE exchange are where to look for the RNG state leak, and
it's a fair assumption that key material used in IKE came from the software,
not the accelerator's, RNG.

@_date: 2016-01-11 12:47:51
@_author: Thor Lancelot Simon 
@_subject: [Cryptography] Juniper tells USG all ScreenOS devices are EOL 
According to the list at  (select the "VPN
Concentrator" category), on January 6 Juniper informed DISA that every
single previously approved ScreenOS device was end-of-life and, as I
read the attached memo, DISA consequently mandated they all be removed
from service by January 31.
It appears _someone_ has more confidence in (or more chutzpah about) the
operational security surrounding the development process for Juniper's
JunOS and "SA" based VPN products; they have been left certified until
October, 2017.
