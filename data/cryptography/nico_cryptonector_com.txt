
@_date: 2013-12-11 16:58:36
@_author: Nico Williams 
@_subject: [Cryptography] Kindle as crypto hardware 
A keyed backdoor is tolerable if the key(s) (but, in the case of
Dual_EC, singular) are extremely well protected.  The more you use the
backdoors, the harder it is to protect their keys.  Routine cooperation
with LEA that involves use of these backdoors is therefore risky, and
considering the scope of these keys, I'd say "extremely risky".  For if
these keys are accessed routinely then a Snowden will eventually have a
chance to copy them.
(I'd like to picture a trusted key module like device built like a tank,
in vault in a bunker in Fort Knox, with two-man activated physical keys,
protected by Marines, requiring written order from several bigwigs, high
security clearance of the user, and verbal confirmation of the order via
land-line telephone.  That'd be OK and manageable if the keys were
rarely used, but otherwise, I picture a number of operators accessing
these "airgapped" devices very frequently.  Auditing procedures have to
be airtight, IGs have to be able to audit, ...  More likely security is
much too lax.)
(It's very likely (IMO) that Dual_EC was just originally a sort of key
escrow system for U.S. government-internal (e.g., military) purposes,
not a backdoor to be foisted on the public.  Not a terrible idea, as
long as the key is closely held and so on.)

@_date: 2013-12-13 13:02:27
@_author: Nico Williams 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
Yes, to reprise the /dev/*random robustness thread, we need as many
inputs to the CSPRNG as possible.  Heck, even a constant seed and a seed
saved from the previous boot.  As long as the PRNG is cryptographically
secure and at least one source of boot-time (and subsequent) entropy is
predictable by would-be attackers, this should be good enough.

@_date: 2013-12-13 17:17:37
@_author: Nico Williams 
@_subject: [Cryptography] An alternative electro-mechanical entropy source 
I know this thread got campy and all, but, it's one thing to trust
RDRAND and use its outputs without any further post-processing[*], and
it's another to trust the rest of the CPU.  It's true that one cannot
possibly fully test, say, 64-bit multiplication... but on the whole the
CPU has to implement deterministic operations reliably, and it's too far
down the stack to robustly implement the Ken Thompson attack[**][***].
So, yes, it is reasonable to trust a CPU but not its RNG.
The correct approach to RDRAND is to make it but one of several inputs
to a CSPRNG.  That is should be so is so clear now (and, IIRC the
consensus, if one were to try to determine one, of the /dev/*random
robustness thread) that we really should stop retreading over this.
[*] Or with entirely deterministic post-processing, if that processing
    is easy for the adversary to match.
[**] I've been reminded once that Thompson wasn't the first to think of
     it, and he'd given credit.  Still, you'll all understand when I
     refer to it as the Ken Thompson attack.
[***] Unless, I suppose, the CPU has another small computed bolted on
      that can operate independently of the CPU and which could flash
      the CPU's microcode.  "Hmmm".
Physical security.  It still matters.  (I.e., tamper evident seals, et
cetera).  You have to trust HW within some perimeter.
As long as they can't influence it at high enough frequencies...
Accelerometers are a decent source of entropy for mobile devices, though
probably not all the time (might use too much battery, or the device
might be close enough to at-rest relative to the local gravity well for
an extended period of time).
Except hopefully using parts usually already found in every computer.
Like mic inputs (see turbid), or just plain interrupt jitter + cycle
counters / hi-res clocks.  Plus a baked-in seed, plus a seed saved at

@_date: 2013-11-01 00:20:19
@_author: Nico Williams 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
I've asked this before and maybe we can make it very short and sweet:
   How hard is an attacker fitting your threat model[*] willing to work
   to attack you via your RNG?
   [*] The person answering this question gets to pick their threat model.
Considering Dual_EC, assuming it's an attack on the wider community (as
opposed to a secret self-key escrow that happens to also escrow other
Dual_EC users' keys)...  the answer appears to be: not too hard.  This
particular attacker, well-funded and all, apparently wanted to have to
see just 32 bytes of RNG output to be able to recover its state with
little effort.
That's NOT evidence that no attacker is willing to work much harder than
that to attack you via your RNG.  But it's suggestive.
Consider RNGs specifically designed to make some attack plausible:
 - Dual_EC-like -> observe small amount of output (plausibly, as TLS
   nonces) and you're done.  Easy.
 - Not like Dual_EC -> might have to observe lots of outputs (less
   plausible without subverting application implementations as well),
   and possibly have to gather data on the victim (e.g., CPUID, ...),
   mount some active attacks, and so on just to get to a brute-force
   search within reach.
   Of course, if the attacker can do some of this off-line, then maybe
   it's not a big deal to them.
   If then can only do this in real-time (e.g., active attacks), then it
   may be too much work (and if the RNG is good enough, infeasibly too
   much) relative to the alternatives:
    break in & install keylogger, content yourselve with traffic
    analysis + whatever cleartext you can get a hold of, bribe someone,
    use social engieering, use a rubber hose, ...
Now consider an RNG where there's multiple sources of entropy at least
one of which would be too much work or impossible for the attacker to
compromise in any way, and where the RNG mixes its sources, extracts,
and stretches entropy in cryptographically-secure ways.  In this case
attacking via the RNG is not likely going to be of interest.
Quantifying this will depend on the sources of entropy, but it's easy to
see that with just a few (seed file, RDRAND, jitter, ...) it quickly
becomes impossible for the attacker to use this path.
Note that robustnes (in the sense that got these threads going recently)
is important in this case: assuming the attacker has all initial state
and slowly (better, quickly) loses track of some entropy inputs, then
the attacker should get no meaningful improvement over brute-force for
guessing next RNG outputs.
In other words:
 - be liberal as to entropy sources -- the more the better, at least one
   of them decent,
 - be conservative in wanting as many of them and as much output from
   them as possible while low-balling estimates of total entropy,
 - be conservative as to mixer/pool/extractor/stretcher crypto design,
   not only in /dev/*random, but in apps consuming it as well (to
   protect themselves against a weak /dev/*random).
   (Note that it's not good to constantly poll for new entropy: that
   would be bad for power management.)
This should be clear.
Related tangent: it will also help if we start designing protocols to
need/use fewer nonces (and random IVs) so as to limit subliminal

@_date: 2013-11-01 11:11:31
@_author: Nico Williams 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
Right.  I did not state goals for attack complexity, but clearly 2^64
work factor is in the domain of the feasible for targeted attacks and
not for bulk attacks.  So we should want 2^128 as a baseline.
A high-quality entropy source with 1 bit/s bandwidth will get the host
up to that baseline in just over two minutes of run-time.  Even jitter
alone can provide higher bandwidth than that.  But this highlights the
importance of delaying things like ssh host authentication key
generation (and even ephemeral DH keys) until we have reached that goal.
A blocking-until-properly-seeded-then-non-blocking stretched RNG device
would be very nice to have!  (I manage this on some systems by holding
up boot progress until /dev/random (and urandom) are seeded.)
However, it's not always possible to wait until a very conservative
estimate of entropy reaches 128 bits: some kit might not reach it in
acceptable times.  How conservative to be in this estimate may well have
to be a configuration knob.  It might help to have RNG interfaces that
allow applications to state a mininum entropy requirement, a
Yes.  But then you're probably already taking good care to build that
machine with a decent source of entropy + a decent seed just to be sure,
and you might even use network entropy beacons for extra assurance.
I deliberately mentioned both nonces and random IVs because both can
be used as subliminal channels to leak RNG outputs.
An attacker might try to get you to run an application that uses RNG
outputs as nonces so they can observe RNG outputs directly.  You might
not notice this in an audit.  Remember, the easiest way for a Dual_EC-
like backdoored RNG to be used cheaply is to use its outputs as TLS
nonces.  This probably means it actually happens somewhere.
Eliminating or minimizing these subliminal channels makes it harder to
attack via the RNG.  Without them the attacker has only a brute-force
search attack -- that might still be feasible (e.g., if the attacker
gets you to use an RNG with very few bits of entropy), depending on your
RNG construction, but eliminating Dual_EC-type attacks is an
Right.  That was Perry's suggestion, a few weeks back, for a new cipher
mode construction: transmit a counter (which presumably is checked
against a sequence number window) and use it to derive the random IV for
CBC decryption.

@_date: 2013-11-01 18:08:29
@_author: Nico Williams 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
An ssh-scan is still a first use from the point of view of the service.
And from the point of view of the user doing the scan.
Hmmmm, well, ssh to localhost should be special.  If you're connecting
to / accepting on 127.0.0.1:22 or ::1:22 then the client a) shouldn't
care what the host key is, b) if the server doesn't yet have a key then
it could generate one for just this use and not any others.
(And, for ssh w/ GSS, ssh to localhost should replace "localhost" with
the host's hostname.)
It's a bug that ssh to localhost:22 is not special.

@_date: 2013-11-04 10:52:51
@_author: Nico Williams 
@_subject: [Cryptography] DNSSEC = completely unnecessary? 
Answer: Something closer to a real PKI with one root and much less
complexity than PKIX.  And if you also use the TLS server PKI then you
get two factors for authenticating servers to users.
Not so; see DANE [RFC6698].
DNSSEC has issues (e.g., slow deployment, bandwidth-amplification), but
it is not a waste of time, and together with DANE, DNSSEC provides
robust security (or can, assuming other things are done right, like
selection of public key algorithms and key sizes).

@_date: 2013-11-04 14:27:32
@_author: Nico Williams 
@_subject: [Cryptography] DNSSEC = completely unnecessary? 
"some type of cert. verfication" is exactly what goes on.  And it's
weak.  It's weak because:
 - there's no single root for the PKI and it's not hierarchical -- there
   are many, many roots that can MITM each others' customers to their
   relying parties, and,
 - name constraints didn't get implemented
Whereas DNSSEC gets both of those things right from the word 'go'.
There may be multiple registrars for a zone, and often there are, but
it's not possible for each registrar to issue MITM certificates, say,
and you'll notice immediately if a registrar steps on another's (and
their customers') toes.
It's done, and has been for a long time.  It sucks.
Because the TLS server PKI isn't, and it sucks.
DNSSEC doesn't take 10x more time and effort to implement and deploy
than the TLS server PKI.  Deployment of anything new like this is
asymptotic, but eventually we'll get close enough.
Because encrypting everything is expensive (computationally, which is to
say: expensive in terms of *energy*, which is to say expensive as in
$$$$).  Static content should simply be hashed and their hashes be
included in the hrefs to the static content, all of this built into
Merkle hash chains, effectively, and only dynamic content and static
content roots encrypted and authenticated.
Because you want assurance that the certified key corresponds to the
certified name.  Verifying ownership of names is hard.
Security is not cheap, much less free.  Backing the whole enchilada are
the law and courts (including international law).  None of it cheap or

@_date: 2013-11-04 16:51:25
@_author: Nico Williams 
@_subject: [Cryptography] /dev/random is not robust 
Not at all.  Encrypted swap should be added after local filesystems are
mounted and *RNG state seeded.  (And, by implication, it should be
possible to run without swap.)
Break up and re-arrange bring-up as needed to get things right.
Also, *RNG state seeding could be done at multiple points in time.
Where there's RDRAND-like HW entropy sources then use those immediately
upon startup, else use a seed file from the RAMfs miniroot, then re-seed
when / (and /var) is (are) mounted, then re-seed when device drivers for
other HW entropy sources are loaded, then re-seed using jitter data,
then re-seed using interrupt data...
Aside: can the bootloader do anything to provide the kernel with entropy?
Also, ASLR should use /dev/urandom, and perhaps init and friends should
re-exec themselves after proper seeding.
A bigger problem on RDRAND-less tick-less systems is power management:
waking up to sample HW entropy is not an option.  This might lead to
higher latency for /dev/random on such systems.  But I think this is
manageable, besides /dev/*random should work like a periodically
re-seeded SRNG, so there should be no need for /dev/random to block once
properly seeded.

@_date: 2013-11-07 16:06:30
@_author: Nico Williams 
@_subject: [Cryptography] randomness +- entropy 
That's *not* supposed to be the case.  That is, a good PRNG does not
allow an attacker that observes some of the PRNG's output to use it to
guess future outputs.  Obviously the security of a PRNG will decrease as
the attacker observes more and more outputs, but, given a random and
unpredictable (high-entropy) initial state of N bits, the PRNG's
resistance to such attacks will be reduced by one bit (N--) for each bit
observed by the attacker!
A PRNG with n bits of high-entropy state should provide as-good-as-
brute-force protection.  Allowing the attacker to observe one output of
the PRNG should reduce the attacker's work factor by 2^-n, not by a
factor of 2!
The PRNG needs an estimate of security relative to attackers that get to
observe some (many, most, all) of the PRNG's outputs.  This estimate is
not the same thing as an estimate of the PRNG's state's entropy.  This
estimate of strength should go down very slowly as outputs are produced,
and it should go up sharply when new trusted seeds are consumed.
An estimate of entropy is useful for protection against generating keys
from a not-yet-(or unsafely)-seeded PRNG.  Once the PRNG is seeded with
with enough entropy, the PRNG's state's entropy estimate should be
largely irrelevant because the more interesting question becomes: how
resistant is the PRNG is to guessing by attackers that get to observe
its outputs.
I.e., ssh-keygen might want to demand /dev/urandom outputs with N>256 bits
of entropy and 2^256 brute-force-equivalent cryptanalysis resistance.
Periodic re-seeding with high-quality seeds is necessary for robustness
reasons: to recover quickly from state compromises (e.g., a sysadmin
with root access using a kernel debugger to get at the PRNG's state and
using this to escalate privilege once the debugging session is over).
Ideally /dev/urandom gets re-seeded on every read using HW RNG outputs.
Then occasional state compromises become almost a non-issue; sustained
compromise == complete compromise.

@_date: 2013-11-07 16:28:37
@_author: Nico Williams 
@_subject: [Cryptography] CD bootable Linux (was randomness +- entropy) 
12 years ago, in the aftermath of a time-bomb attack on a bank by a
disgruntled [ex-]employee, I co-wrote recommendations on how to deal
with that in the future.  Those were pre-TPM days, and one part of our
proposal was to investigate booting from read-only media or secure NFS
(also non-existent back then; nowadays one might consider diskless iSCSI
booting) as a way to prevent time bombs being left in boot images.
We also proposed -and implemented- making privileged access to systems
much more closely audited as a deterrent.  The read-only media concept
made, effectively, for an easy-to-audit boot media update process (but
it required additional physical security protections).
Read-only media is a poor-man's TPM-measured boot media.  Highly
Anyways, the take-away: we need HW RNGs on all systems that need to boot
from read-only media, or even TPM-measured media (because any next-boot-
seed must be measured or be considered not trusted, and because updating
the TPM every time the system boots/shuts down very much weakens the
TPM-measured boot proposition.
It also so happens that we should want HW RNGs on *every* system.
We also want CPU cycle counters, hi-res timers, and so on so that there
are some sources of entropy (jitter) that aren't (can't be) potentially-
backdoored HW RNGs.
That's it.  ARM and other CPU designers: give us both of those things!!
Thanks for the link, that's very interesting.

@_date: 2013-11-07 19:46:05
@_author: Nico Williams 
@_subject: [Cryptography] randomness +- entropy 
That's a strawman.  I never said that's a definition of a PRNG (good,
bad, whatever).
I also never said anything about provability.
I was only arguing that consuming n bits of PRNG output != lowering the
PRNG's "entropy" by n bits.
Think DTrace.  The debugger might only be permitted to read.

@_date: 2013-11-08 15:31:03
@_author: Nico Williams 
@_subject: [Cryptography] randomness +- entropy 
Good.  We could argue about how slowly entropy gets consumed as outputs
of a properly seeded PRNG/SRNG get produced, which I think should be...
...incredibly slowly, at least by 2^-n for every unit of output where n
is the number of bits of entropy estimated in the RNG's state.  So if
you have 256 bits of entropy to start with you'll never in a million
years (figure of speech) end up with less than 180 or so bits of entropy
left.  Add to that periodic TRNG inputs to provide fast recovery from
state compromises and what more could we want?
It only needs to have high entropy, unpredictable state.  Given a
mixer/extractor design that does not cause entropy to go down much at
all just because output bits are extracted... it doesn't matter if
(an SRNG, basically) as opposed to just a TRNG.  With proper crypto no
harm could come from that, nor could anyone distinguish one from the
Given RNGs whose states' entropy is difficult to consume there should be
no real difference between /dev/random and /dev/urandom once they've
been properly initialized, especially if they are both designed to be to
recover quickly from one-time state compromises.  Or, if there should be
a difference, a) what is it, b) why should I care, c) how could I tell?
If indeed there is no need for there to be a difference between the two
[once properly initialized] then why do we have the two interfaces?  And
also, (2a) would follow naturally.
The key here is "once properly initialized".
 - /dev/random should block whenever it hasn't had fresh entropy mixed
   in in the past N seconds or if it has not yet been properly seeded.
   I.e., guarantee strong and robust outputs.
 - /dev/urandom should block if it has not yet been properly seeded,
   then it should never block again.
   I.e., guarantee strong outputs.
 - Both should get fresh entropy mixed in frequently (at least as
   frequently as outputs are demanded).
 - Nothing in the boot sequence should need entropy before /dev/urandom
   has been seeded properly.  Anything that does should be modified not
   to.
   For example, ASLR should not require strong RNG outputs until
   /dev/urandom has been seeded, and then init(1M) and any other
   long-running processes should restart.  (Solaris' init(1M) can be
   restarted, so this is not farfetched.)  Seeding of the RNG should be
   done as early as possible in the boot sequence.  Sources of entropy
   should include:
    - RDRAND or similar
    - jitter (requires hi-res CPU cycle counters and timers)
    - async event timing (interrupts)
    - a seed in the RAMfs boot image
    - a seed in /var saved at last boot
    - a seed in /var saved at last shutdown
    - datetime, CPUID, ...
    - network entropy servers
   in roughly that order.  Only jitter entropy should be trusted for
   estimating initial RNG state entropy, but some of each of the others
   must be required.
Once seeded, and if frequently reseeded then it's the same as
contract where each open file descriptor represents an instance of a
seeded-one-time-only PRNG, but I don't think anyone needs this, and
anyways, most apps open, read, close /dev/urandom rather than keep it
Not really.  The two pseudo-devices have to be reasonable
high-bandwidth, and [once properly seeded] secure under fairly broad
threat models.  So starting from that premise the goal is a
fast-but-secure mixer/extractor with TRNG inputs fed to the mixer as
often as possible (though not on a timer, for power mamangement reasons,
so much as on demand as read()s are done).
If entropy isn't consumed linearly (but way sub-linearly) with demand
then I don't see this trade-off.  You just agreed with my statement that
that is a desirable property.
No, we should demand 128 bits of entropy before anything useful can be
done with /dev/urandom's output.  The only question is: whence the
entropy -- we always end up at this very first square.
Right!  We're not extracting entropy.  We're extracting outputs from an

@_date: 2013-11-24 19:41:07
@_author: Nico Williams 
@_subject: [Cryptography] Explaining PK to grandma (Re:  (no subject)) 
Here's two attempts to explain PK to grandma:
 - It's like the postal service: your mail is secure -let's pretend, for
   this analogy- provided you write the correct recipient addresses on
   the envelopes, otherwise someone else might get it delivered to them
   incorrectly.
 - Same analogy, only this time your correspondents' addresses are
   barcoded and you must affix barcode stamps (or print the barcodes) on
   the envelopes.  If someone replaces the barcodes in your addressbook,
   how would you notice?  If you don't, your mail goes to an MITM.
   This one is a better analogy, methinks.  It clearly illustrates the
   difficulty of bootstrapping trust (how to find a peer's address), it
   clearly illustrates MITM attacks, and it's a clear enough analog of
   RSA encryption.
   Postal addresses in many countries just aren't unwieldy enough to
   make this analogy believable, but this isn't universally so.  Maybe
   grandma has many correspondents in the UK and she can't quite
   recognize/comprehend/lookup their postcodes.
Both analogies also describe the security properties of a trusted third
party system (here the postal service).  Postal mail is only as secure
as the mailboxes, the postal buildings, trucks, and so on, ultimately
only as secure as the postal service itself -- paper envelopes by
themselves are not secure (at best tamper-evident), quite clearly, and
even grandma can grasp this.  And surely grandma will understand that
the postal service will do the government's bidding.  So all of the
major PK security issues are covered.

@_date: 2013-11-25 10:11:40
@_author: Nico Williams 
@_subject: [Cryptography] Explaining PK to grandma 
Sealing an envelope and dropping it into a mailbox is an analog for
encryption.  It's a lousy one, I know, but we're not going to get
perfect off-line world analogs for crypto, and what we need are not so
much explanations of low-level details of crypto but high-level aspects
of security.
That's not the right analogy.  The recipient will only get the mail if
she addressed it correctly, and only they will be able to open the
envelope.  This analogy covers even physical security aspects of crypto.
The thing that causes complexity is the unwieldiness of cryptographic
keys.  If we could recall keys we'd not have passwords, and we wouldn't
have much need for PKI.

@_date: 2013-11-25 11:38:09
@_author: Nico Williams 
@_subject: [Cryptography] Explaining PK to grandma 
The problem with lockboxes for analogies is that we don't use them for
communication; that's also a plus in that they show the unwieldiness of
crypto.  But lockboxes *can* be asymmetric, as in drop-safes: anyone can
drop something in, but only the holder of the key can take out the
Ditto open padlocks: you can close someone else's padlock (if it's not
locked yet), but you can't unlock it without the key.
Perhaps we should combine the postal service and padlock analogies: you
send letters (and open padlocks for the responses) in padlocked boxes.
The combination shows the unwieldiness of crypto and matches the rough
outlines of PK.
I don't yet have a padlock-like analogy for PK signatures, only for PK
Anyways, we're talking about necessarily imperfect analogies.
The proposition that any one analogy is good (or bac) can probably be
tested -to some degree- on real people.  Perhaps we should make a list
of analogies and gather some anecdotal (at first) evidence for their
Measuring an analogy's utility is no doubt not trivial, so some thought
would have to go into that first.  Perhaps we could have a standard
questionaire to measure a person's understanding of the analogized
subject matter before and after being exposed to the analogy.  (I'm sure
there is literature on how to do this correctly.  Clearly polling is not
an area where I have any expertise.)  I'm not sure this is worth doing
at all, but having decent grandmanalogies on hand might be useful for
discussions of policy where the general public is the audience.

@_date: 2013-11-25 14:28:39
@_author: Nico Williams 
@_subject: [Cryptography] Email is unsecurable 
E-mail has been not secure for... 40 years.  So what?  It works well
enough for a lot of things, and nothing else we've yet seen would work
as well for some uses (e.g., fora like this one).
It'd be better to incrementally deploy more secure protocols for
specific use cases (IM, video chat, ...) move use cases off of email
where/as possible.  In fact, we're doing that all the time: with web
services and IM for example.
E-mail generally cannot be secured, I think this is true.  The
anonymization concepts discussed in this forum theoretically work, but
they aren't likely to be widely adopted.
And as to mass-adoption keep in mind that only a few thousand (or a few
tens of thousands) of people at most can really be expected to review/
audit/build/run their software stacks.  Which is to say that pretty much
everyone will necessarily be running bits subject to backdooring.  It
might be interesting to consider cross-border commercial certifications
for software stacks, but I doubt those would be feasible for a long
time, and to be meaningful they'd have to include certifications from a
variety of countries, some friendly and some hostile to the end-user's
(not that that matters, for as we all know, Oceania hasn't in fact
always been at war with Eastasia).
(Among other things, certifications are massively expensive, in large
part due to their opportunity costs, some of them relating to their very
negative impact on development schedules.  End-users aren't going to pay
for them.)

@_date: 2013-11-25 19:10:21
@_author: Nico Williams 
@_subject: [Cryptography] Explaining PK to grandma 
I just finished trying this analogy on several teenagers, and the
verdict is that it works.  The difficulties of using crypto properly
became most evident when I threw in the need for a 411-type white pages
(It will be difficult to try it on grandma, but only because she's far
away, and hard of hearing.)
Some salient points:
 - symmetric crypto is easily understood (decoder rings and all that)
 - the padlock analogy works (whether you get open padlocks from your
   peers or enter a public code for them into off-the-shelf padlocks, or
   "print" them, as you suggest)
 - the postal service part works, but
 - what really drove the point home was the 411 online white pages
   concept.
 - the very next question was: "so why not always do the symmetric
   thing?", so I explained how pair-wise keying fails to scale.
   "oh"
I should note that I've previously been able to explain plain old DH
using a pencil and a napkin.  DH is easy to explain, and easy to
understand.  It's fun to see the lightbulb go off!
Signatures.  I don't know of a good analogy for signatures.  Anyone?

@_date: 2013-11-26 09:35:38
@_author: Nico Williams 
@_subject: [Cryptography] Explaining PK to grandma 
Padlocks make good analogs for PK encryption because substantial
physical effort is needed to break them, *and* the tamper resistance of
boxes and padlocks can always be improved, but wax seals have very
little resistance to copying and it cannot be improved much in any easy
way.  Otherwise both would be decent analogs.
Oh, could be.  There's really no meatspace analog for digital
signatures: any integrity protection seal stops serving its purpose once
broken, and they must be broken to get at the contents, and
authenticating physical integrity protection seals is hard.
I can see how one could create protocols based on padlocks that can be
used to implement something like perishable digital signatures after the
fact, but the analogy then gets way out of hand and loses all its
utility (except as a way of showing that "crypto is hard").

@_date: 2013-11-26 11:12:35
@_author: Nico Williams 
@_subject: [Cryptography] Explaining PK to grandma 
Please be careful with attributions.  The above was written by "Lodewijk
andr? de la porte" , not me.

@_date: 2013-11-26 11:17:54
@_author: Nico Williams 
@_subject: [Cryptography] Explaining PK to grandma 
But users have to understand the risks [inherent in driving a killing
machine such as a car, or sending sensitive data over any one
I believe users need to know, and be educated if need be, about scams
(e.g., phishing) and how to recognize when they are at risk.  Some
details necessarily bleed through the abstractions ("cars burn gas").
When I tried out the padlock analogy last yesterday, my audience got the
MITM problem mainly when I mentioned relying on 411 as an online
directory.  Finding analogies that make real risks evident to the
uninitiated is important, provided we can find such analogies of course.

@_date: 2013-11-26 11:58:49
@_author: Nico Williams 
@_subject: [Cryptography] Explaining PK to grandma 
I've already stated that I don't think e-mail can be secured.  That
makes it easier to educate users: don't put much faith into what you get
in your inbox.
As for IM and web services, the best we can hope for is for users to
know that they're at least trusting the vendor of the app/device, and we
should apply things like DANE and pinning (and stronger TLS) to get as
close as possible to "secure" for those services.  That's reasonably
feasible.  I doubt we'll do much better as to mass consumption.

@_date: 2013-11-26 18:26:10
@_author: Nico Williams 
@_subject: [Cryptography] Explaining PK to grandma 
Right, secure e-mail is a bit of an oxymoron.  You can have other secure
things, but e-mail, not so much.  And do explain this go grandma:
    all your base belong to government [and your vendor(s) [*]]
    [*] If she can grok any explanation of "vendor".
And as someone pointed out, if you wish to store bits and pieces of non-
e-mail secure conversations, pretty soon you're right back at something
that looks like e-mail, but as long as exchanges between peers are
interactive you have a shot at being secure modulo caveats that grandma
can understand (see above).

@_date: 2013-10-05 19:37:55
@_author: Nico Williams 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
More like: use a KDF and separate keys (obtained by applying a KDF to
a root key) for separate but related purposes.
For example, if you have a full-duplex pipe with a single pre-shared
secret key then: a) you should want separate keys for each direction
(so you don't need a direction flag in the messages to deal with
reflection attacks), b) you should derive a new set of keys for each
"connection" if there are multiple connections between the same two
peers.  And if you're using an AEAD-by-generic-composition cipher mode
then you'll want separate keys for data authentication vs. data
The KDF might well be SHA256, but doesn't have to be.  Depending on
characteristics of the original key you might need a more complex KDF
(e.g., a PBKDF if the original is a human-memorable password).  This
(and various other details about accepted KDF technology that I'm
eliding) is the reason that you should want to think of a KDF rather
than a hash function.
Suppose some day you want to switch to a cipher with a different key
size.  If all you have to do is tell the KDF how large the key is,
then it's easy, but if you have to change the KDF along with the
cipher then you have more work to do, work that might or might not be
easy.  Being able to treat the protocol elements as modular has
significant advantages -and some pitfalls- over more monolythic

@_date: 2013-10-06 10:02:13
@_author: Nico Williams 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
Algorithm agility makes it possible to add and remove algorithms.  Both,
addition and removal, are made difficult by the fact that it is
difficult to update deployed code.  Removal is made much more difficult
still by the need to remain interoperable with legacy that has been
deployed and won't be updated fast enough.  I don't know what can be
done about this.  Auto-update is one part of the answer, but it can't
work for everything.
I like the idea of having a CRL-like (or OCSP-like?) system for
"revoking" algorithms.  This might -in some cases- do nothing more
than warn the user, or -in other cases- trigger auto-update checks.
But, really, legacy is a huge problem that we barely know how to
ameliorate a little.  It still seems likely that legacy code will
continue to remain deployed for much longer than the advertised
service lifetime of the same code (see XP, for example), and for at
least a few more product lifecycles (i.e., another 10-15 years
before we come up with a good solution).

@_date: 2013-10-07 12:12:12
@_author: Nico Williams 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
Note, btw, that Keccak is very much like a KDF, and KDFs generally
produce variable length output.  In fact, the HKDF construction
[RFC5869] is rather similar to the sponge concept underlying Keccak.  It
was the use of SHA-256 as a KDF [but not in an HKDF-like construction]
that I was objecting to.
As Jerry Leichter said, that's a really nice idea.  My IANAC concern
would be that there might be greatly diminished returns past some number
of rounds relative to the sorts of future attacks that that might
drastically weaken AES.  There are also issues with cipher modes to
worry about, so that on the whole I would still like to have algorithm
agility (though I don't think you were arguing against it!); but the
addition of a cipher strength knob might well be useful.
You're quite right that with CPU support for AES it will be very
difficult to justify switching to any other cipher...  There's always
3AES (a form of round count, but a layer up, and with much bigger step
sizes).  I suspect it's not AES we'll have problems with, but everything
else (asymmetric crypto and cipher modes most likely).

@_date: 2013-10-11 10:53:03
@_author: Nico Williams 
@_subject: [Cryptography] prism-proof email in the degenerate case 
Me too.  I've been telling people that all PRISM will accomplish
regarding the bad guys is to get them to use dead drops, such as comment
posting on any of millions of blogs -- low bandwidth, undetectable.  The
technique in this thread makes the use of a dead drop obvious, and adds
significantly to the recipient's work load, but in exchange brings the
bandwidth up to more usable levels.
Either way the communicating peers must pre-agree a number of things --
a traffic analysis achilles point, but it's one-time vulnerability, and
chances are people who would communicate this way already have such
Each server/list is a channel.  Pre-agree on channels or use hashes.  If
the latter then the hashes have to be of {sender, recipient}, else one
party has a lot of work to do, but then again, using just the sender or
just the recipient helps protect the other party against traffic
analysis.  Assuming there are millions of "channels" then maybe
something like
    H({sender, truncate(H(recipient), log2(number-of-channels-to check))})
will do just fine.  And truncate(H(recipient, log2(num-channels))) can
be used for introduction purposes.
The number of servers/lists divides the total work to do to receive a
But then the sender can't quite prove that they didn't send anything.
In a rubber hose attack this could be a problem.  This also applies to
recipients: they can be observed fetching messages, and they can be
observed expending power trying to find ones addressed to them.
Also, there's no DoS protection: flooding the lists with bogus messages
is a DoS on recipients.

@_date: 2013-10-15 19:35:07
@_author: Nico Williams 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
Performance matters in the real world.  Adding layers is generally nice
from a security p.o.v., but not from an economics p.o.v.
If you want secure protocols deployed then they have to perform well
Now, what kind of layers are we talking about: multiple layers of crypto
in the same protocol, or multiple secure protocol layers (e.g., TLS over
IPsec)?  I suspect the former.  In either case AES-NI is the game to
Yes, that.
Because legacy and deployment issues get in the way.  Make one mistake
and getting over it will take at least half a decade.  Make N>3 mistakes
and we don't ever settle on a stable *and* secure state.  That's TLS.
We can't even start over.  The real world imposes constraints that make
fixing things hard.  Bill Sommerfeld's analogy (I think it was his) was
that what we do is like rebuilding a 747, full of passengers,
mid-flight, at 30,000ft.
That's not doing it justice though -- it's much, much worse than that.
If you get the APIs wrong, then fixing things takes much more time and
effort than if you merely get some protocol detail wrong -- protocol
details can be negotiated as long as changes to them don't bleed through
into the APIs.  When that happens it's like switching between unit
systems while rebuilding the 747 mid-flight.
The TLS re-negotiate vulnerability involved API design mistakes.  It
arose from not even thinking of TLS as having an API.  (Few people agree
with the proposition we need to think about abstract APIs when designing
Starting over is like moving the passengers from a dilapidated plane to
a newer one -- mid-flight, and possibly with the new plane mid-
construction.  This is why we don't start over.  Plus the situation
w.r.t. firewalls and proxies is such that the new plane has to look just
like the old one, just not dilapidated.
The complexity of the crypto pales by comparison to the complexity of
dealing with legacy and getting everyone to move on to the new thing.
And yet we're only now beginning to get being confident about the crypto
(constant-time block ciphers + AEAD and also some non-AEAD modes used
carefully + constant-time sponge hash functions and KDFs + safe,
constant-time ECC curves + good RNGs).
If you've gotten here you've missed the big picture because you're
probably doubling the cost of the crypto even though crypto weaknesses
are not the biggest problem -- not for symmetric key encryption anyways.
It's everything else, particularly the cipher modes, the infrastructure,
and so on.
Also, it's not clear to me that more crypto is the answer to all
problems.  Consider compression-related vulnerabilities...  We need
full-stack security *and* economic analysis and design.
No, it's legacy -> complexity.  Any brand new TLS-replacement protocol
will soon look like TLS unless we manage to get all the details just so.
Assuming we're near a stable understanding of how to build secure
protocols (see above), we could get all the details just so *once*.  But
how likely do you think that will be?
Well, PKI RSA-MD5 certs were involved in the Flame kit, no?  At least
some good guys got hit (since it spread outside its suspected original
target).  And the bad guys (who probably think they are the good guys)
were hit.  Strictly speaking crypto vulns have resulted in real-world
attacks.  Most real-world vulns are not in crypto nor crypto protocols,
so it's a fair argument that there's not much to worry about in the
And anyways, absence of evidence is not evidence of absence.
Well, I think DJB has this right.  The best way to minimize latency
would be a combination of DNS[SEC], ECC, and UDP with ECDH-based key
exchange and authentication, and TCP Fast Open-like fast reconnect
functionality.  UDP because TCP hasn't aged so well, because -unlike any
new transports- it traverses middle-boxes, because [...].
Layers can cooperate to reduce round trips.  For example, GSS-API has a
channel binding input and "PROT_READY", which allow for intersting round
trip optimizations.  A transport with TCP Fast Open-like functionality
too can lead to the common case being zero round trips to setup new
connections, the next common case being a single round trip, and the
worst case being two.  This is not something we can make happen for TCP
and TLS concurrently: ETOOHARD, too many moving parts.
"Always secure" is not really that easy.  Ignoring the economic costs,
there's the introduction problem, and the fact that sloppy CAs and
registrars (no matter how few) make PKI lame.  At best you can do
opportunistic anonymous key exchange and encryption -- better than
nothing, as they say[0], but not enough either since a) there's UI
issues (see the whole browser status bar lock icon saga), b) as long as
unauthenticated usage prevails the middle boxes that sprout up after you
can force you to stick to that usage.
[0] Ahem, see RFC5386.

@_date: 2013-10-18 18:08:34
@_author: Nico Williams 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
The problem is that many apps expect /dev/urandom never to block.  This
is a severe problem if such an app is invoked early in boot and blocks
the rest of the bootup procedure.  But, then again, that would be a
serious bug, therefore blocking until seeded would be very useful
behavior: it would allow one to find such bugs.
Now, once seeded, /dev/urandom should not block again (apps that use
should get periodically reseeded.

@_date: 2013-10-18 19:15:15
@_author: Nico Williams 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
Didn't I say that in exactly the next sentence?!
You went to all the trouble of quoting this one sentence separately from
the preceding and following sentences, then corrected me with the
contents of the very following sentence...  Why?

@_date: 2013-10-31 01:57:10
@_author: Nico Williams 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
I've been wishing it were so for many years.  Certification can be
everything to a vendor, and when the standards and labs require a bad
result, the vendor provides it.  Management demands it, the engineers
that don't like it move on to other projects, and the ones that don't
know better do what they're told.  It's just too difficult to argue with
anyone about the requirements that must be met to get a certification.
Or at least that's the perception.  That perception is wrong: I've
actually succeeded in getting NIST to change a proposed requirement in
the past (or so I was told after the fact, verbally; my emailed comments
actually went unanswered at the time).  But not that long ago colleagues
running a product past certification were emphatic that changing the
requirements was just too difficult/expensive/not worthwhile.
Standards can be a source of systemic risk.  This is true even in the
absence of conspiracies: just because of inertia.  Standards can also be
a source of stability and can prevent serious mistakes -- also because
of inertia.  Lack of standards is worse.
It's a reasonable explanation.  For 2004.  It's still believable in 2013
because most of us don't deal in FIPS-anything, so we can (and do) shrug
it all off.  But it's also a bit embarrassing.  As someone here recently
said (Peter Gutmann, IIRC), we thought "oh, you were serious about
that?!".  But, yes, standards with certification lab enforcement are
serious, therefore we should not shrug them off.  AND, in order to do
that, the standards bodies in question have to be accessible.

@_date: 2013-09-12 14:04:08
@_author: Nico Williams 
@_subject: [Cryptography] [cryptography] very little is missing for 
Note: you don't just want BTNS, you also want RFC5660 -- "IPsec
channels".  You also want to define a channel binding for such channels
(this is trivial).
To summarize: IPsec protects discrete *packets*, not discrete packet
*flows*.  This means that -depending on configuration- you might be
using IPsec to talk to some peer at some address at one moment, and the
next you might be talking to a different peer at the same address, and
you'd never know the difference.  IPsec channels consist of ensuring
that the peer's ID never changes during the life of a given packet flow
(e.g., TCP connection).  BTNS pretty much requires IPsec configurations
of that make you vulnerable in this way.  I think it should be obvious
now that "IPsec channels" is a necessary part of any BTNS

@_date: 2013-09-12 14:35:02
@_author: Nico Williams 
@_subject: [Cryptography] Killing two IV related birds with one stone 
I like this, and I've wondered about this in the past as well.  But note
that this only works for ordered {octet, datagram} streams.  It can't
work for DTLS, for example, or GSS-API, or Kerberos, or ESP, ....
This can be implemented today anywhere that explicit IVs are needed;
there's only a need for the peer to know the seed if they need to be
able to verify that you're not leaking through IVs.  Of course, we
should want nodes to verify that their peers are not leaking through
There's still nonces that are needed at key exchange and authentication
time that can still leak key material / PRNG state.  I don't think you
can get rid of all covert channels...  And anyways, your peers could
just use out-of-band methods of leaking session keys and such.
BTW, Kerberos generally uses confounders instead of IVs.  Confounders
are just explicit IVs sent encrypted.  Confounders leak just as much
(but no more) than explicit IVs, so confounding is a bit pointless --
worse, it wastes resources: one extra block encryption/decryption
operation per-message.

@_date: 2013-09-12 14:53:28
@_author: Nico Williams 
@_subject: [Cryptography] Summary of the discussion so far 
Privacy relative to PRISMs is a political problem first and foremost.
The PRIM operators, if you'll recall, have a monopoly on the use of
force.  They have the rubber hoses.  No crypto can get you out of that
I'm extremely skeptical of anti-PRISM plans.  I'd start with:
 - open source protocols
 - two or more implementations of each protocol, preferably one or more
   being open source
 - build with multiple build tools, examine their output[*]
 - run on minimal OSes, on minimal hardware [**]
After that... well, you have to trust counter-parties, trusted third
parties, ...  It get iffy real quick.
The simplest protocols to make PRISM-proof are ones where there's only
one end-point.  E.g., filesystems.  Like Tahoe-LAFS, ZFS, and so on.
One end-point -> no counter-parties nor third parties to compromise.
The one end-point (or multiple instances of it) is still susceptible to
lots of attacks, including local attacks involving plain old dumb
security bugs.
Next simplest: real-time messaging (so OTR is workable).
Traffic analysis can't really be defeated, not in detail.
On the other hand, the PRISMs can't catch low-bandwidth communications
over dead drops.  The Internet is full of dead drops.  This makes one
wonder why bother with PRISMs.  Part of the answer is that as long as
the PRISMs were secret the bad guys might have used weak privacy
protection methods.  But PRISMs had to exist by the same logic that all
major WWII powers had to have atomic weapons programs (and they all
did): if it could be built, it must be, and adversaries with the
requisite resources must be assumed to have built their own.
Anti-PRISM seems intractable to me.
[*] Oops, this is really hard; only a handful of end-users will ever do
    this.  The goal is to defeat the Thonpson attack -- Thompson trojans
    bit-rot; using multiple build tools and dissassembly tools would be
    one way to increase the bit-rot speed.
[**] Also insanely difficult.  Not gonna happen for most people; the
     ones who manage it will still be susceptible to traffic analysis
     and, if of interest, rubber hose cryptanalysis.

@_date: 2013-09-12 15:11:31
@_author: Nico Williams 
@_subject: [Cryptography] Why prefer symmetric crypto over public key 
My $.02:
 - protocols based entirely on symmetric keying are either PSK or a
   flavor of Needham-Schroeder (e.g., Kerberos)
 - neither PSK nor Needham-Schroeder scale
    - PSK fails to scale for obvious reasons
    - Kerberos could scale if there were TLD realm operators, but there
      aren't any, and there can't be because they would have too much
      power, thus no one would trust them (see below)
    - Kerberos could scale with a web of trust (PGP-like), but managing
      that web would be difficult, and realms that are widely trusted
      are... much too powerful (see below)
 - Kerberos KDCs have even more privileged a position than PKIX CAs:
   they can impersonate you to others and vice-versa (therefore they can
   MITM you) and they can recover all your session keys (unless you use
   PFS) even when they don't MITM you.
   This is necessarily so for any symmetric key only protocol.
 - To get past this requires PK crypto.  It's unavoidable.
 - Life will look a bit bleak for a while once we get to quantum machine
   cryptopocalypse...

@_date: 2013-09-13 15:46:58
@_author: Nico Williams 
@_subject: [Cryptography] Summary of the discussion so far 
First: you can probably be observed using them.  Unless too many people
use mix networks you might just end up attracting unwanted attention:
more passive surveillance, maybe even active attacks (at the limit very
physical attacks).
Second: I suspect that to be most effective the mix network also has to
be most inconvenient (high latency, for example).  That probably means
mix networks won't be popular enough to help with the first problem.
Third: the mix network had better cross multiple jurisdictions that are
not accustomed to cooperating with each other.  This seems very
difficult to arrange.
I'd love to be disabused of the above though.

@_date: 2013-09-13 16:14:56
@_author: Nico Williams 
@_subject: [Cryptography] [cryptography] very little is missing for 
I think you're arguing that active attacks are not a concern.  That's
probably right today w.r.t. PRISMs.   And definitely wrong as to cafe
shop wifi.
The threat model is the key.  If you don't care about active attacks,
then you can get BTNS with minimal effort.  This is quite true.
At least some times we need to care about active attacks.
It's hard to know exactly why BTNS failed, but I can think of:
 - It was decades too late; it (and IPsec channels) should have been
   there from the word (RFC1825, 1995), and even then it would have been
   too late to compete with TLS given that the latter required zero
   kernel code additions while the former required lots.
 - I only needed it as an optimization for NFS security at a time when
   few customers really cared about deploying secure NFS because Linux
   lacked mature support for it.  It's hard to justify a bunch of work
   on multiple OSes for an optimization to something few customers used
   even if they should have been using it.
 - Just do it all in user-land has pretty much won.  Any user-land
   protocol you can think of, from TLS, to DJB's MinimaLT, to -heck-
   even IKE and ESP over UDP, will be easier to implement and deploy
   than anything that requires matching kernel implementations in
   multiple OSes.
   You see this come up *all* the time in Apps WG.  People want SCTP,
   but for various reasons (NAAAAAATTTS) they can't, so they resort to
   putting an entire SCTP or SCTP-like stack in user-land and run it
   over UDP.  Heck, there's entire TCP/IP user-land stacks designed to
   go faster than any general-purpose OS kernel's TCP/IP stack does.
   Yeah, this is a variant of the first reason.
There's probably other reasons; listing them all might be useful.  These
three were probably enough to doom the project.
The IPsec channel part is not really much more complex than, say,
"connected" UDP sockets.  But utter simplicity four years ago was
insufficient -- it needed to have been there two decades ago.

@_date: 2013-09-25 15:31:15
@_author: Nico Williams 
@_subject: [Cryptography] The hypothetical random number generator backdoor 
Note that Kerberos "confounds": it encrypts it's nonces for AES in CTS
mode (similar to CBC).  Confounding makes it harder to exploit a
backdoored RNG if the exploit is made easier by the ability to see RNG
outputs as nonces.  I'm not sure how much harder though: presumably in
the worst case the attacker has the victim's device's seed somehow
(e.g., from a MAC address, purchase records, ...), and can search its
output via boot and iteration counter searches (the details depend on
the PRNG construction, obviously).  Seeing an RNG output in the clear
probably helps, but the attacker could design the PRNG such that they
don't need to.
Now, there's a proposal to drop confounding for new cipher suites in
Kerberos.  Among other things doing so would improve performance.  It
would also make analysis of the new cipher suites easier, as they'd
match what other standard protocols do.
Of course, I'd rather implementations have a strong enough RNG and SRNG

@_date: 2014-04-02 10:59:52
@_author: Nico Williams 
@_subject: [Cryptography] IPsec is worse than unusable (Re: TLS/DTLS Use Cases) 
Actually, IPsec fails to do exactly what you say.  That is a key failure
of IPsec's.  There are no "IPsec sockets", nor "TCP w/ IPsec sockets".
A TCP SYN and SYN+ACK (etc...) for the same TCP connection can easily be
sent to different peers altogether even though using IPsec!
The reason is a combination of statelessness in IPsec[*] and that
authenticating IP addresses is ETOOHARD, so the typical IPsec
configuration says "anyone with certs from these issuers can claim any
IP addresses from these ranges".  This fails to scale.
Plus IP addresses are meaningless as security identifiers to users and
applications.  connect() to domainnames -with IPsec doing the hard work
under the covers- would have been better.
There are ways to fix all this, but it's ETOOLATE for IPsec.
And anyways, in-band key exchange and authentication (TLS, SSH, and
friends) have won out over out-of-band (IKE) key exchange.  Tragic or
not, it is what it is.
[*] IPsec is a layer below TCP: a "secure" analog to IP.  IP is
    stateless and unaware of TCP (and other) state.  The same is true of
    IPsec.
PS: I know I must sound like a broken record about this, but it's worth
    repeating to all and sundry in the security industry.  IPsec is a
    great case study in critical interface design failures leading to
    market failure for a security protocol.  If we are to learn from
    failure we must identify, study, and teach failures.  I'm sorry to
    pick on you.
    Perhaps it's worth writing a paper/RFC on this topic just so we can
    just point at it every time misconceptions about IPsec come up.  But
    there's no positive value in writing such a thing.

@_date: 2014-04-02 18:33:09
@_author: Nico Williams 
@_subject: [Cryptography] IPsec is worse than unusable (Re: TLS/DTLS Use 
Can you share some details?  Off-list perhaps.  I'd greatly appreciate
The basic thing to implement is RFC5660.  Then you can add APIs on top.
Tracking IP address overlaps is not the issue.  See below.
Sure, I grant that and _wish_ it had gone differently.
In terms of scalable deployment SSH and TLS have IPsec beat by light
years.  There's no contest.
Even in terms of actual security offered, a deployed protocol that has
issues (e.g., TLS) is clearly much better than a non-deployed protocol
that has issued (e.g., IPsec).
Even if IPsec deployment as specified had become widespread, its
security offering would be necessarily much less than that of TLS
because of the issues I described.  Unless, of course, widespread
deployment were to lead to those issues being noticed and addressed,
but since it didn't happen...
Partially done already :)  RFC5660 covers some of this, specifically the
lack of binding of packet flows to the initial node IDs for the life of
the packet flow.  RFC5660 also specifies a [local] fix for this.
RFC5660 made it past WG, IETF, and IESG reviews.  The work item also
passed WG charter review.  If you don't agree, well, it's a request for
comments, please send some!  :)
The right place to send comments today would be the ipsecme WG and the
RFC author(s) (singular in this case, and yours truly, but I'm on the
ipsecme list already).

@_date: 2014-04-02 18:57:43
@_author: Nico Williams 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve numbers 
[This thread is getting repetitive :/]
"Nothing up my sleeve number" == very little freedom in selecting that
number.  Ways to limit such freedom:
 - pick small numbers (e.g., 2)
 - pick an irrational number from a small set of them (pi, e, and
   sqrt(2) -- all easily available and computed), then an appropriate
   length prefix of its mantissa; no seeking
The first won't look random; the second will.  Neither will or could be
random, as explained earlier.
(The golden number is the irrational that's easiest for me to remember
how to compute: it's the limit of the ratio of consecutive Fibonacci
numbers as the Fibonacci number index goes to infinity.  That's also
easier to approximate by hand than pi and e: just find the Fibonacci
numbers for sufficiently large N and N+1, then divided them.  I like
Phi for this and other reasons.  But really, just a few well-known
irrationals will do.)
Or just compute each digit until you have enough.  It's simple stuff:
The recent threads about safe curves went into this in some detail.  DJB
calls this "rigidity".  See the list archives and

@_date: 2014-04-05 18:23:28
@_author: Nico Williams 
@_subject: [Cryptography] TLS/DTLS Use Cases 
Add WebSocket and it's back to being datagram-like.
In so far as AJAX pages make lots of small requests and get small
responses, HTTP is still datagram-ish anyways.
Also, HTTP is just about the worst datagram protocol ever.  There's no
XID, so responses have to be sent in the same order as requests over
any one keptalive TCP connection.  Yuck.  (When I've brought this up
in the context of HTTPbis I've been told to go away.)

@_date: 2014-04-05 18:30:26
@_author: Nico Williams 
@_subject: [Cryptography] TLS/DTLS Use Cases 
Add WebSocket and it's back to being datagram-like.
In so far as AJAX pages make lots of small requests and get small
responses, HTTP is still datagram-ish anyways.
Also, HTTP is just about the worst datagram protocol ever.  There's no
XID, so responses have to be sent in the same order as requests over
any one keptalive TCP connection.  Yuck.  (When I've brought this up
in the context of HTTPbis I've been told to go away.)

@_date: 2014-04-08 14:21:20
@_author: Nico Williams 
@_subject: [Cryptography] TLS/DTLS Use Cases 
It was added as a new minor version of the protocol.  That would tend to
indicate (to me anyways) that it wasn't an afterthought.
Are you saying that HTTP/1.0 was elegant?  Well, I suppose it was, if we
ignore all the complexity of text, line-oriented headers (the two ways
to express multiple header values, continuation lines, verbosity).  The
elegant part is the REST/CRUD aspects, IMO.
Anyways, the part that interests me here is that there's still no
interest in fixing this, particularly when HTTP/2.0 is so much about
performance.  Perhaps the lesson is that we don't learn from our
(I haven't checked recently, so it's possible that this has been
addressed since.  I sure hope so!)

@_date: 2014-04-08 14:33:42
@_author: Nico Williams 
@_subject: [Cryptography] [cryptography] The Heartbleed Bug is a serious 
The first part (gather data) is OK.  The second I thought was said
facetiously.  It is flawed, indeed, but it's also true that people have
a hard time weighing intangibles.
I don't know how we can measure anything here.  How do you know if your
private keys were stolen via this bug?  It should be possible to
establish whether key theft was feasible, but establishing whether they
were stolen might require evidence of use of stolen keys, and that might
be very difficult to come by.  We shouldn't wait for evidence of use of
stolen keys!

@_date: 2014-12-15 14:01:38
@_author: Nico Williams 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
Most Unix-ish OSes have something like it (for some it's just "xattrs"
with smallish payloads, either way, it's easy to overlook).
And who ever bothers looking in lost+found?
Hiding places are not in short supply.
And elsewhere.  Archive files, SQLite3 files, ... it's all roughly
equivalent to "a filesystem" comparable to that used here.  Encrypting
it is easy too.

@_date: 2014-12-22 21:38:45
@_author: Nico Williams 
@_subject: [Cryptography] Certificates and PKI 
CAs weren't necessarily a bad solution.  Unconstrained naming definitely
DNSSEC (and therefore DANE) has that critical feature that PKIX only has
as-specified but never will as-deployed: naming constraints.
RAs might well be as awful as CAs.  But at least they'll be constrained.
Then there's naming.  x.500 naming is just. such. a. disaster.
People -perhaps every literate human with an Internet connection- are
conversant with domainnames.  Perhaps three people outside this list
understand x.500 naming.
Naming constraints is PKIX's last-mile problem.
If you look at it this way, which horse has a better chance of winning?

@_date: 2014-12-23 10:16:11
@_author: Nico Williams 
@_subject: [Cryptography] Certificates and PKI 
That's a different problem that PKIX naming is also susceptible to
(probably any naming scheme where "labels" of any sort are used would
DNSSEC/DANE has a simpler last mile problem than the problems that
plague PKIX as-deployed in the WebPKI.  The future is DNSSEC's.

@_date: 2014-12-23 14:27:38
@_author: Nico Williams 
@_subject: [Cryptography] Certificates and PKI 
You already have to trust registries/registrars to a large degree.
Trusting them to properly safeguard their DNSSEC private keys isn't
asking that much more of them, gives them less power than unconstrained
CAs[*], doesn't prevent CT for DNSSEC (so we can also keep registrars
honest, so that the additional extent to which we must trust them isn't
intolerably more).
In the long run it also means one fewer sort of entity to trust: the
unconstrained CAs, as hopefully they will go the way of the dodo.  In
the medium term we might even see registrars and CAs merge, producing
that same long term result sooner.
No real new problems arise as to authenticating domain owners either,
just as to enrollment.
[*] Unconstrained because unconstrainable, because name constraints were
    never deployed in time and as critical.

@_date: 2014-12-23 14:36:03
@_author: Nico Williams 
@_subject: [Cryptography] Certificates and PKI 
DNSSEC/DANE puts registries into the same position as a
properly-constrained CA (something that doesn't exist).
CT is more than just a mitigation against lack of name constraints.
It's applicable to any kind of PKI.  DNSSEC is a kind of PKI.  CT should
be applicable to DNSSEC.
I'm already practicing the cheer.  I'm not much of a gymnast, but I'll
even wave around some pom-poms if it'll help.
To the extent that browsers come with pre-loaded "important" site pins,
why do we need EV?  But whatever, and DANE is orthogonal to that.
It's a lower barrier to entry for small players.

@_date: 2014-12-23 15:00:37
@_author: Nico Williams 
@_subject: [Cryptography] Certificates and PKI 
People are definitely conversant with domainnames: they know how to read
them, how to type the ones they know well enough, they know how to
communicate these verbally, and so on.
[Phishing is orthogonal to DNS: any naming scheme would be subject to
 phishing, even plain old brick-n-mortar store signs are.]
In fact, domainnames are all that users ever see as far as naming of
services -- that and URLs and e-mail addresses containing domainnames.
Users never see PKIX names without drilling into the certificate used by
a service.  In that case the first thing the user see's is the service's
domainname, and then the user is left on their own as far as making
heads or tails of the certificate and its chain.  Even the best PKIX
GUIs can't actually say much more than "the chain validates to some
trust anchor and none of the certificates in it are expired".  With CT
they can also say "and the CAs participate in CT auditing".  The user
has no clue though if the service's owners really wanted to use that
cert and the corresponding private key.  No clue.
PKIX naming lacks even a decent textual form!  Much less a canonical
one.  (Really, I'm not joking.  See RFC4514.)  PKIX names can't reliably
be typed in.  Even in a GUI, displaying PKIX names is non-trivial.
x.400/x.500 naming is useless without conventions and naming constraints
that don't exist (and can't be deployed), and some relation to e-mail
addresses and service names that *also* don't exist in the wild.  X.400
addressing was not that uncommon in the 90s; I remember HP OpenMail.
Even in the 90s it was ironic that to exchange e-mail with the rest of
the world one needed gateways to SMTP and RFC822 addressing (sendmail
rewrite rules for this were something else, and if you've not seen them,
then count yourself lucky).
But since the web -since HTTP and the web- there's no popular service
that can even be named with a user-visible x.500 name.
DNS naming has naming conventions and rigid name constraints, and had
them from day 1.
There's no question as to which is superior.  And nothing is being
proposed to replace DNS naming, while DNS becomes more entrenched every
day.  We could conceivably replace DNS.  We can't conceivably replace
DNS-style domain naming.
And it can't get much simpler than DNS domain naming either.  If users
can't handle that ever, well, we're just going to have to be app-
centric, and goodbye to the Web.  Since the web isn't going away, I
submit that users are managing just fine (yes, yes, phishing, but see
No.  I fully expect to continue to toast to the universal deployment of
the latter for a long time.  DNSSEC deployment has much better universal
deployment prospects.  Particularly when we consider DANE stapling.
A migration path is clear.  Eventually critical mass will be reached.

@_date: 2014-12-29 22:18:10
@_author: Nico Williams 
@_subject: [Cryptography] Certificates and PKI 
It doesn't seem likely that there would be a zone cut between
mxhost.example.com and _.mxhost.example.com, or between
_.mxhost.example.com and _._.mxhost.example.com.
Now, the resolver might not be in a good position to know where to stop
looking for zone cuts, but the application might be.
Anyways, 5 would be the max; 4 would be the likely max, so just one more
query (still, 33% more, unless we know where to stop QName
Also, we could have zones that must always cut...  It's might be too
late to go there, but it'd have been convenient for QName minization.
As to latency, it can be traded off for load by parallelizing queries,
though that's probably scarier than the extra latency.
Another problem is that this means that stapling DANE is not enough:
the client will have to check for zone cuts that don't appear in the
stapled data.  Again, some heuristics about where we can expect no zone
cuts will help; in the majority of cases that should suffice.
OK, so CT for DNSSEC has to log not just delegations signed (or made at
all, signed or not) by the parent, but also all would-be
out-of-bailiwick RRs served by the parent, and related NSEC3 RRs as
Identifying requirements for CT for DNSSEC is a good thing..
Incidentally, if the WebPKI were strictly hierarchical, then we could
say the same exact thing about CT for PKIX, no?  It's good to have
problems that the WebPKI doesn't have because the problems it has are so
much bigger...

@_date: 2014-03-06 18:53:35
@_author: Nico Williams 
@_subject: [Cryptography] GnuTLS -- time to look at the diff. 
People can also write crappy code without gotos, and there's plenty of
crappy code without goto (I'm sure you can find plenty of crappy
Python, JavaScript, ... code).
That's silly.  Some languages have no goto.  And others use lambda as
the ultimate goto (get the reference?).  Spaghetti code is spaghetti
code -- goto is not necessary.  Just witness the constant complaints
about callback hell in JavaScript -- tell me you find it all easy to
read, but I warn you now: I won't believe you.
Right, because users love applications that recover from error
conditions by (from the user's point of view) crashing.  And
developers love libraries that do that to their apps.
Look, this is no different than the never ending arguments about
exceptions versus explicit error checking.  Using a single goto label
for error handling is a fine pattern, and not unlike what Go does for
handling otherwise-unhandled exceptions.  I wonder what you think of
Go-style exception handling... and if you like it, why one shouldn't
try to approximate it when writing in C.
(Sure, you might well tell us not to write in C, but there's an
enormous base of code written in C that we can't all just abandon.  We
don't all always get to pick the languages we program in, so "don't
use C" isn't all that useful as advice goes -- someone has to deal
with the existing code base.)
I use it: a) when the code base I'm working with does (and so maybe
I've got to stick to the upstream's style), b) when I own the code
base and using goto results in code that's easier to read.
I find code that indents many levels difficult to read.  I also find
it difficult to read code where everything is refactored into
three-line functions just to avoid many levels of indentation and
goto.  At some point you're dealing with enough complexity that these
considerations cease to be a big deal -- and then you *really* want
strong software engineering processes.  I'd say TLS involves such a
level of complexity.
Meanwhile I've seen (and found) many, many security bugs, of many
different types, so I'll concern myself with software engineering
processes for preventing these in the first place.  You are free to be
shallow in your own response, though I'd rather you were more

@_date: 2014-03-07 11:22:04
@_author: Nico Williams 
@_subject: [Cryptography] GnuTLS -- time to look at the diff. 
Memory management is not the only problem.  One could (and with
CoreFoundation, Apple engineers often do) resort to abstractions that
make memory management easy (ref-counted).  There is often more than
just memory management to take care of: you might need to record
detailed error information (more than just returning an error code)
somewhere, you might need to map error codes.  Whenever you have
repeated error-handling you want to apply the DRY principle, and
that's what goto failure helps with.
Consider some alternatives:
    ret = func(...);
    if (ret != success) {
        /* cleanup */
        ...
    }
    ret = ...
    if (ret != success) {
        /* cleanup */
        /* oops!  I'd better not forget to check that this does
everything that needed to be done, plus any additional cleanup work
needed since the previous cleanup section! */
        ...
    }
Yuck.   Really, that's what you want?  You don't think that's error-prone?  Or:
    if ((ret = ...) != success ||
        (ret = ...) != success ||
        ...) {
        /* single cleanup section; look ma'!  no gotos in sight!  no
needless repetition either */
        ...
    }
But now you have assignments in if conditions (which many people will
complain about; can't win eh).  And source-level debuggers don't make
stepping through this sort of code easy (but then, real programmers
don't use source-level debuggers, so maybe this is a non-issue).
On the contrary, clearly the concern about memory deallocation shows a
concern about correctness.  That they screwed up is not evidence that
they didn't care about correctness.
Goto isn't the problem.  Cargo cult can be a problem; never using
gotos is cargo cult (as opposed to not using them to write spaghetti
code, which is a different matter altogether).  Refexive goto-hate is
inconsistent unless you also apply it to callback-hell, or, really,
any LISP or Scheme or similar, because, really, lambdas are the
ultimate goto, and unsurprisingly you can write spaghetti code in just
about any Turing-complete language.  I put a premium on code
readability: if it's hard to read your code, it's that much easier for
me to miss a bug like this when reviewing it -- this is universally a
good rule of thumb, cutting across all languages.
Manual memory management also isn't the problem.  Sometimes you really
want manual memory management in security-critical components: because
you need to be able to characterize performance under load (think
DDoS), or because you want secrets wiped from memory as soon as
possible.  And sometimes you need it because you're working in a very
constrained environment (such as embedded devices).  Good engineering
can handle manual memory management.
The problem here was a lack of testing and possibly incomplete or
incompletely-followed software engineering processes (maybe also
deficient static analysis tools) (but I repeat myself).

@_date: 2014-03-07 12:07:52
@_author: Nico Williams 
@_subject: [Cryptography] Bounties 
IMO: slim to none.
Why wouldn't criminals want to make off with this bounty, if they
thought they could?  What are the odds no criminals thought to look
and see?  For that matter, what are the odds that insiders at the
exchanges in question weren't involved?  Insiders are generally the
biggest threat!
Occam's razor and all that.
Consider the incompetence displayed by the various exchanges.
Something bad was bound to happen.
And if it was a covert state op, well, so much the better: better the
crisis come sooner (so we can limit the damage) than later.  Why
aren't we celebrating these failures?
There's a common misconception that fiat currencies are the worst
currencies, or that non-fiat currencies won't be subject to money
creation via fractional reserve lending, or that maturity matching
will solve all our problems w.r.t. systemic banking crises.  Or that
somehow cryptocurrencies can blunt Leviathan (spoiler: it can't,
because Leviathan has a de facto monopoly on the use of force).  This
is a mistake.  Currencies get their value from others' willingness (in
the near and far future) to take money from you and give you services
and products in exchange, and this depends on there being people in
the future to trade with.  Today's global financial crisis is really a
demographic crisis -there are not enough young people to trade with,
to invest in-, not a crisis of fiat currencies.  Those who think fiat
currencies are teh worst are likely to see conspiracy theories in the
failures of alternatives.  I just think that fiat currencies, coupled
with a constitutional democracy, are the worst currencies, except for
all the others.
We will end up having cryptocurrencies in widespread use, of this I'm
sure.  I'm not sure that they won't be fiat cryptocurrencies (I don't
think that need be an oxymoron) or that we won't still have fractional
reserve lending.
We'll need to think about using multiple devices to execute
transactions: for escrow purposes, and to mitigate local security
issues.  This business of having "hot wallets" at exchanges is rather
scary to me.  Yes, I know, online banking and payment with fiat
currencies is no better.  We're still at square  when it comes to
online payment security.  Heck, we're still at square  when it comes
to plain old paper checks.
If only smartcards for payments were a viable option...

@_date: 2014-03-07 15:19:02
@_author: Nico Williams 
@_subject: [Cryptography] GnuTLS -- time to look at the diff. 
Exactly.  It doesn't speak well for Java that you need all this extra
ceremony to get a simple "bail out" pattern.
 - compilers that whine about constant loop conditions (oh but why?!)
 - unnecessary and distracting additional indentation...
 - ...which increases the pressure to refactor into pointless 3-line
 - ...which increases the number of units to think about / devalues
the function/method as a unit to think about.
The pressure to indent / pointlessly refactor is a big deal, a very
big deal.  We're talking about codebases between hundreds of kloc and
tens of mloc, and engineer teams with turnover rates too high relative
to the size of the codebases.  That means that the task of _reading_
the bloody code has to be made simple.  The use of goto in the Apple
code achieved that, and all the alternatives being proposed would undo
that achievement (for shame).
Making that code easy to read wasn't sufficient to avoid a bug because
no one actually read the result of a merge (or something along those
lines) carefully enough, and that might not have been sufficient
because humans are fallible.  Lack of testing did this in, NOT this
use of goto.
Yeah, yeah, statement expressions, yeah, yeah :)  I like them, they're
nice.  They're not universally available, and neither are
indefinite-extent closures (Blocks).  And we're stuck with C, so of
course we want these things.  (And CoreFoundation, which proves you
can get decent data structures and memory management in C.)
Well, no, it's not easy: you _have_ to initialize them because they
don't auto-initialize.
One can do something about this: put all local variables in a local
struct and memclear it at function entry.  Of course, that doesn't
work for pointers (since zero pointer values need not be zero bit
patterns, though for most architectures they are zero bit patterns, so
maybe you won't care).  Or you could use a macro to define local
variables and auto-initialize them to zero (which doesn't work for
structs, but hey, at least that's a bit of progress).  Of course, now
the code doesn't really look like C... (maybe that's a feature).
Seriously: the goto was not the problem.  Building a lot of machinery
to avoid a non-problem is only going to cause problems.
Admit: software engineering is hard.  There are no obvious, simple
answers.  There are no non-obvious, simple answers either.  Stop
looking for simple answers: there aren't any.  Software engineering
requires discipline.  Software engineering is expensive.  You get what
you pay for.
If you're not paying for discipline then you'll need a scapegoat; I
suppose goto is as good as any other scapegoat.
But I'll call you on it.

@_date: 2014-03-10 12:57:04
@_author: Nico Williams 
@_subject: [Cryptography] End-to-End Protocols and Wasp Nests 
Yes, but with public test servers and/or open source test services.
Simple ones could test edge cases that don't require complex
environment setup (e.g., signing with the wrong private key, vs.
CRIME/BEAST), but with some sophistication even CRIME/BEAST could be
done.  I could imagine a wifi router hacked to provide a wasp nest...
just download a wasp image, install, power up, test.

@_date: 2014-03-11 16:01:45
@_author: Nico Williams 
@_subject: [Cryptography] GnuTLS -- time to look at the diff. 
Er, yes, agreed, but the standards generally say these things, just
not in a way that can be easily extracted for the purpose of building
a wasp nest / test suite.
Perhaps we need to consider a more formal standards-writing language,
but there's a lot of resistance to that (see the recent discussions
about JSON schema languages in the IETF JSON WG).  A more realistic
alternative might be to produce an Informational follow-on to any
standard like TLS that has a description of all the test cases related
to violations of requirements in the standard.
Agreed.  Code needs to be readable.  I don't see how the Apple goto
failure style hurts readability.  The accident happened (or could
have) for reasons unrelated to style.

@_date: 2014-03-11 17:29:52
@_author: Nico Williams 
@_subject: [Cryptography] RC4 again (actual security, 
[OT] Yeah, well, they'll tell you "wrong WG"  :)
But point taken.  This issue comes up a lot, and the answer is
generally resistance, as you point out.
I am definitely starting to think that HTTPS w/ DHE ciphersuites +
renego is the way to go: passive attackers can only use packet sizes
and timing to guess that renego is happening, and active attackers get
found out (probabilistically).  Add in DNSSEC and DANE and upgrading
to strong authentication is then relatively easy.
Of course, we're still missing DHE ciphersuites with ECC DH and modern
ciphers/modes.  Hmmm.

@_date: 2014-03-11 18:14:03
@_author: Nico Williams 
@_subject: [Cryptography] RC4 again (actual security, 
Sigh.  I meant, TLS_ECDH_anon_WITH_AES_xxx_GCM_SHAxxx and other such
I.e., you can't do anon ECDH in TLS and get a modern cipher with a
modern cipher mode and modern PRF.  This has been mentioned a few
times.  I've just asked the TLS WG about it.  The registry has
codepoints reserved for allocation via Standards Action and
Specification Required, so I ought to be able to register these
missing ciphersuites with an individual submission I-D with intended
status Informational.  I'll do so soon.  But I'm giving the WG a
chance to say they want it as a WG work item.

@_date: 2014-03-16 23:39:45
@_author: Nico Williams 
@_subject: [Cryptography] Client certificates as a defense against MITM 
This isn't new.  IIRC in
 (but I
may be thinking of the wrong doc).  See the various versions and its
Google's channel bound cookies basically do something like using user
certs for protecting session identifiers (cookies).  There's no MITM
detection, just protection against theft of cookies (e.g., possibly by
future MITMs, but also CRIME/BEAST type attacks).
In general one can use channel binding to detect MITMs when
authenticating at least the other peer to oneself.  Using channel
binding to detect MITMs when authenticating to the other side without
authenticating it back is harder.
The following is partly a restatement of what you say, really.
Q: Suppose you use a user cert in a TLS handshake, what do you know at
the end of the handshake (assuming it doesn't fail) beyond what you
would have known without having used a user certificate?
A: Only[*] that if there is an MITM they can't impersonate you
-specifically: your user cert's public key- to the real server!
That's nice, but not enough.
Now how will you know that the real server sees you as authenticated
to them and, therefore, that there was no MITM?  Well, suppose the
server is your bank, and you're looking at your bank account balances
and transaction logs: if you didn't authenticate to your bank but to
an active attacker then the attacker will have to know an awful lot
about you for you to realize that they're not your bank.  The attacker
might get to observe your intentions (e.g., move money about), but
they won't get to change your orders, and you'll eventually recognize
that something went wrong.
That's very nice, really -- not ideal, but much better than nothing.
You still have to enroll your public key at some point for this to
work... but we're making progress.  But there are some issues:
 - If you're going to use user certs this way, there'd better no be an
online password-based ("security question") login recovery procedure
that the MITM could trick you into using.  Otherwise there really
isn't any MITM protection from using user certs.
 - If the server is using a PKI to authenticate you (as opposed to
merely knowing your public key or a pre-shared cert for it) and you're
using a PKI to authenticate them... then two CAs (possibly one) will
be able to MITM you.  So it's important that the server know your key,
not just your name.
I.e., protection against MITM using client user certs is not trivial.
Actually, this reminds me of something...  We're updating SASL/GS2 to
relax some requirements so that OAuth, SAML-EC, and friends can be
used as GSS-API and SASL mechanisms.  The requirement relates to
channel binding and mutual authentication.  What this reminds me of is
that we probably should add security considerations text explaining
that channel binding can still provide a modicum of value (see above)
even when there is no mutual authentication.  (Note well: I'm using
"mutual authentication" in a sense that is a bit of a term of art that
is well understood on the IETF KITTEN WG mailing list.)
[*] That's assuming your private key remains secure and the TLS
protocol works as advertised.

@_date: 2014-03-16 23:58:34
@_author: Nico Williams 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
That would be great.  It'd also be a double-edged sword...  A bit like
microprogramming: how do you validate that black box?  But mostly a
good thing, I think, and we're going to need it if we're ever to
replace AES.  In the long-term, as one ciphersuite wins over the rest,
algorithm agility will become harder and harder to actually use as

@_date: 2014-03-17 16:00:11
@_author: Nico Williams 
@_subject: [Cryptography] Client certificates as a defense against MITM 
The idea is: you generate an ephemeral client keypair (and cert) for
every "origin" and you use it to "authenticate" the client in TLS, the
server then binds that public key into the web cookies it sets when
you login (with a typical username&password form, or whatever else),
and then every time you use the same cookies the server verifies that
the TLS user credentials you used match what's bound into the cookies.
This provides protection against cookie theft/compromise.

@_date: 2014-03-17 17:49:49
@_author: Nico Williams 
@_subject: [Cryptography] Apple's Early Random PRNG 
The rpi has one, it turns out.  But yeah, it needs to be on-CPU, so
it's never not there due to choice of peripherals.
If it really matters to you then you should ask your lawyers.  Though
if it makes you happy, there's a long trail of embedded RNGs, not all
at Intel, much predating RDRAND.  Which means that you might happier
to pay the cost of the legal research knowing there's likely to be
good enough prior art or that the patent holders that exist are the
type who won't come after you (a very subjective call, to be sure).
It's not transistor budget.  It's designing a circuit that will
reliably produce real entropy (specifically, that can be credibly
characterized as or shown to do so) without being too easy to force
into an all-ones or all-zeros state (or other trivial pattern).  A
circuit that will not significantly increase bad silicon rates.
It's an engineering process more than anything.  The die space needed
can be fairly minimal.
This is a large fixed cost, zero marginal cost.  If you find good
enough public-domain designs, maybe the fixed cost is very low, but if
you start from scratch or are full of FUD then you might find this
fixed cost to be ample.
There's also the cost of failure to take into account.  Avoiding
failure will mean following a more rigorous engineering (including
testing) process.
That's... the SOC market.

@_date: 2014-03-17 18:15:45
@_author: Nico Williams 
@_subject: [Cryptography] The role of the IETF in security of the 
the net?
The crypto wars ended eons ago.
Security has always been spectacularly hard.
Over in the real world we want to scale, which is why we use TTPs.  It
sucks, but then, Internet commerce is sooo nice.  We all want to buy
or sell stuff (goods, services), trading with perfect strangers, yet
we have some expectations of legal support.  Well, it's difficult to
scale beyond your immediate circle of friends and family without TTPs.
 That's also true offline.  And sure, the TTPs can MITM you.  Well,
yeah, it's the price you pay.  You can do all sorts of things to make
it harder on the TTPs to screw you over, but you as long as you have
the introduction problem and you want to scale, you'll have TTPs to
kick around.
How much experience do you have in the IETF?  What you say does not
reflect the reality I inhabit.
Later you say:
Say what?  You think Skype is better/more secure for not having been
standardized?  So you trust Skype more than various IETF security
protocols?  I find it hard to take that seriously.  I've no idea what
Skype does on any given day, and less what it will do the next.
And BTW, the IETF doesn't always do design by committee, the IETF
standardizes protocols that participants want to; if you bring a fully
formed protocol to the IETF that the community is interested in then
they'll standardize it after reviewing it -- chances of zero changes
are low, but the design will not have been by committee.  Really, the
IETF has its faults, and who knows, maybe there are NSA/GCHQ moles
pushing the consensus around, but please don't let's generalize so
Also, the cost of bringing a protocol to the IETF is rather low: it's
the cost of the labor needed to write the documents, rally interest
(it's a volunteer organization), and go through the process.  You
don't even need to pay the IETF one penny if you don't every choose to
attend an _actual_, physical meeting.  This cost is as low as it gets,
though it isn't low, i know.  You can always just manage to get
interest and a community going outside the IETF, effectively making
your own SDO -- plenty have done it.  And there are alternative SDOs.
Heck, SSL was a protocol brought to the IETF by an outsider, not
designed by committee at all.  Kerberos was too.  Ditto NFS, SSH, ...
Sure, after two or three decades they look like monsters, but that's
not because of committee, it's because of the impossibly high cost of
flag days.  The design-by-committee parts come after initial
standardization, when many people use the protocol and therefore have
a stake in its evolution.  I don't see how you avoid this.  It's
called "organic growth", or "entropy" if you like.
Can we _please_, pretty please, with sugar and cherries on top[*]
raise the signal-to-noise ratio on this list?  How can we work to
improve security when our lists become a DDoS on ourselves?
I miss the days when Perry moderated aggressively to tamp down
retreading and when he'd edit posts to note that a given thread was
[*] I hate sugar, but still.

@_date: 2014-03-18 15:26:58
@_author: Nico Williams 
@_subject: [Cryptography] Thoughts about keys 
Indeed.  Then you only need to trust the two people on the path to
anyone you're trying to talk to.  Also, you have to account for the
fact that people do (and will continue to) keep multiple online
identities -- I'm not sure how that complicates things, if at all.
Well, if you can do any web-o'-trust routing then you're presumably
using something PGPish -- you have keying.  Therefore you have no
excuse not to encrypt.

@_date: 2014-03-19 13:08:44
@_author: Nico Williams 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
The antecedents of "this" and "the former" seem to be the same,
therefore that makes no sense.
Anyways, fixed-keys [EC]DH is orders of magnitude easier (and, with
DJB curves) cheaper than just about any other type of PK key exchange
and authentication.  Plus you get to implement LoF/TOFU if you like,
and you can trivially bind-in a PFS exchange (ephemeral ECDH keys).
You could look at the old and very obsolete NFS AUTH_DH (later, the
GSS mech_dh mechanism) code...  Or DJB's modern variant of the... the
same idea.  You can see the simplicity there.
So I don't understand just what you're trying to say.  Can you clarify?
Also, I'm decoding "the older performance-dominated model of
PK-signed-key-setup + RR-key-encrypted" == RSA key transport TLS
ciphersuites.  I get that.  I don't get the "performance-dominated"
part, unless you mean that "RSA key transport" is cheaper than signed
PFS, which it used to be, but with modern curves that's no longer
true, and anyways, with fixed ECDH keys you get plenty of value.
DJB addresses the security concerns of ECDH key reuse.  You should
read the relevant papers.  For the key sizes we're discussing there's
no issue for any realistic reuse rates.
To save the TCP handshake cost.
If instead of ISNs + 3-way handshake you use a non-TCP 3-way handshake
with ECDH keys you get the same properties as TCP's 3-way handshake.
And then you get to implement TCP Fast Open like zero-message new
connection establishment.
You get more though.  You can easily implement things like mosh's
mobility semantics by using UDP (which is precisely why mosh uses
I.e., IPsec, IP mobility, SCTP, these things have failed, and TCP is
not useful for long-lived connections in a mobile world.  Therefore
UDP looks like the best hope.
You do get to (must!) re-implement everything that TCP does regarding
congestion control, minus obsolete cruft.  And for better or worse you
get to do it in user-land (there's a lot written about user-land
TCP/IP stacks for high-performance servers, so don't discount the idea
out of hand).
Not surprising.  [Hopefully] good!  (i.e., if you implemented sane
congestion control, then good! else bad!)
Hopefully as we move more protocols onto UDP the routers/router admins
will drop the preference for TCP.  It may be touch-and-go for a while,
but c'est la vie.
Alternatives would be to develop TCP options for grafting
MinimaLT/CurveCP onto TCP, including mosh-like mobility support.
_That_ would be a good idea, if it's at all feasible (I'll have to
refresh my recollection of constraints on TCP options).  Then we'd not
have to worry about the middle boxes' preferences.  We'd have a bit
more overhead, and a lot of TCP's remaining problems (unless we
further evolved it with yet more options).
BTW, now you see why SDOs like the IETF don't just throw out old
broken things: it's bloody hard to deploy new things given the middle
boxes that exist now.  You can't build a new, parallel Internet.
Paraphrasing Bill Sommerfeld, we have to redesign and rebuild the
aging airplane as we fly it at cruise altitude and cruise speed.

@_date: 2014-03-19 16:02:49
@_author: Nico Williams 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Eh, this is if you know the one bit of the plaintext encrypted with
RSA, but that plaintext is generally a randomly chosen key for a
symmetric cryptosystem used to protect the real plaintext of the
message.  Here you've determined the plaintext of a 1-bit message (or
of 1 bit of a longer message), but not one bit of the "plaintext" fed
to RSA.  That's not relevant here.
Ian's concern is that using the same pair of ECDH key pairs repeatedly
makes it easier to recover the private keys or the session keys.  To
be sure, any PK cryptosystem where you "encrypt to" a public key that
is literally public... is subject to chosen plaintext attacks (by
definition) and therefore had better be resistant to them...

@_date: 2014-03-19 19:39:11
@_author: Nico Williams 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
That's great, but PSK doesn't scale, and Needham-Schroeder (Kerberos)
has scale and trust issues that are not easily resolved without
sprinkling some PK/PKI.
Worse, in Needham-Schroeder (Kerberos) the TTPs (KDCs) are even more
powerful than the TTPs in PKI.  If two peers use certs with chaining up
to different root issuers, then you may need (e.g., if using TLS) MITM
CAs in both of them to MITM the two peers.  In Needham-Schroeder any KDC
in the trust path between two peers can MITM them -- even if you
sprinkle some PK dust (PKCROSS) you still end up with the last hop
realm's KDC's being able to MITM the two peers.
Also, PKI leaves evidence of MITM CAs, whereas Needham-Schroeder doesn't
You have two realistic choices: you can have the level of security you
want (for online shopping, email, IM, ...) using some PK, or you can
have not much security at all outside your home or enterprise network.
Whatever security considerations of any one cryptosystem might be are
just that: security considerations [to pay careful attention to].
There's no real alternative to using at least some PK crypto.
(Experience with Kerberos will convince you of this.)

@_date: 2014-03-19 21:53:40
@_author: Nico Williams 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Fair snark; we're retreading.  This particular issue does not affect
CurveCP.  Ian's comment/question was:
which is a reference, IIUHC, to whether the implementation (of whatever
the PK cryptosystem be) is susceptible to side-channel attacks (usually
by not being constant-time).  That would certainly be a problem, but it
shouldn't be a problem for DJB's curves (I suppose one can implement
them in non-constant time ways, but one shouldn't...).
(For example:
claims that the curve25519 donna implementation is not constant-time,
therefore it's not safe to use it for anything other than key exchange
with ephemeral keys.  Clearly, if the first assertion is correct, then
the latter follows.
The curve25519 donna implementation page claims it's constant-time
though, and DJB's implementation is.)
My point to Ian is that security considerations abound, and there's no
need to frown on PK in general, or CurveCP in particular, over his
concern about PK key pair reuse.  After all, non-constant-time AES
implementations are as fatal to security as non-constant-time PK (RSA,
DH, ECDH, ...) implementations can be.  Instead what one has to do is
use the cryptosystems correctly and use appropriate implementations.
Perhaps Ian has a *different* problem in mind than leakage via time

@_date: 2014-03-20 10:49:07
@_author: Nico Williams 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Quite true.  However, those accounts are generally established using
TLS, not using PSK (the first time anyways, and probably never), so
there's some PK in there.  Which was my point: it's hard to get by in a
pure symmetric crypto world.
I'm not promoting PKI.  See above.
Any time you have a TTP introducer -no matter what the protocol- you
have MITM potential.  Anytime you use key agreement with keys you can't
authenticate somehow, you have MITM potential.

@_date: 2014-03-25 23:01:58
@_author: Nico Williams 
@_subject: [Cryptography] On mobile passwordless logins and established 
Your approach is Persona-like, which IMO puts it on the right track even
though Persona failed.
My variation would be as follows:
 - clients enroll public keys for site-specific ad-hoc identities using
   per-site random PK key pairs
 - a user's devices can share these keys using a protocol for the
   purpose or can enrol multiple (1 per-device) keys for each identity
 - to the extent that sites wish to allow it their users could use their
   IDs elsewhere; for this a light-weight short-lived/fresh cert issued
   by the site's online issuer should suffice.
Note: no PKI in sight for client IDs.
I.e., Persona-like, with extensive device support.
LoF/TOFU can be made workable, such that any MITM had better have been
in the middle at enrolment time and thereafter (for at least the
observed TACK periods for server credentials).  Insist on tack periods
of at least a week, use periodically so as not to miss rollovers, done.
No PKI in sight.  DANE and PKI for server auth don't hurt, to be sure,
but the combination of DANE, PKI, and LoF/TOFU would be quite strong by
comparison to the lame TLS server PKI.
The key is the ability to win trust over time.  Roughly how off-line
natural "authentication" works: recognition (here, online, with PK) +
trust building.
No passwords other than device unlock and/or recovery passwords.
No PKI.  Just Persona-like certs (if that; bare public keys should work
too).  If a site knows you well (e.g., you've made payments to them with
a credit card) they might vouch for you.
 - Sites don't want a standard enrolment protocol that doesn't allow
   them to have highly customized interactions with prospective new
   users.  But any enrolment protocol would have to have a component
   that is completely standard and automated in mobile devices,
   otherwise we'd either not have universality or the UI would be very
   user un-friendly.
 - Mobile device OS vendors would have to agree on something rather
   important.  Either identity key sharing or facilitation of multiple
   key enrolments.  The latter actually wouldn't require much from
   mobile OS vendors... but it'd require more of users in terms of
   enrolment interactions.
   We'd still want to be able to share LoF/TOFU and other persistent
   state across devices, for extra security, but then we'd really need
   the OS vendors to cooperate.
 - Key rollover should be fine, but account recovery if you lose all
   your devices and recovery passwords -or if your devices are
   compromised- could be anywhere from difficult to impossible.  In the
   worst-case scenario the user would need new IDs and might not be able
   to do much to revoke compromised ones.
Persona just closed shop, so maybe this won't fly either, but IMO this
is the right approach.
I find them wanting.  They don't really help with the server
authentication problem, which is the hard one.
Many reasons:
 - There's no need for a PKI for this, but TLS' user cert functionality
   is PKIX-based.  All pain, no gain.
 - Poor application/library integration on the client side.
 - Most importantly: lack of an enrolment protocol for ad-hoc
   identities.
See Persona.
a) enrolment has to be standard
b) sharing of identity keys and/ enrolment of alternate keys
   (per-device) needs to be standard
c) account recovery / revocation processes need to be solidly designed
It's the correct approach, but getting all the relevant players to agree
is going to be like pulling teeth.  But then, who knows, it could
happen!  I'm willing to put some effort into it.

@_date: 2014-03-27 22:04:17
@_author: Nico Williams 
@_subject: [Cryptography] Cryptographic protocol composition (Re: BLAKE2: 
...is not trivial.
Careful: IPsec might not mean what you think it does.  It's easy to
think that it must mean a lot more than it does.
In brief: IPsec authenticates source and destination addresses of
packets, but since IP address<->node assignments have a lot of churn,
the policies concerning how to authenticate an IP address are... overly
broad.  In particular the common approach is to say that any node with
a certificate from CA such-and-such gets to claim any IP address from
such and such CIDR blocks.  Which means a) this doesn't scale to the
Internet, b) authorized mobile users can bump others off their assigned
addresses and take over their TCP connections.
Briefer: IPsec is utterly useless at authenticating the entities that we care
about at higher layers.
At least for large/active enough networks.
The "bump 'em off and take over" attack can be prevented locally...[with
a method not yet implemented anywhere, so, not even that.
At scale, end-to-end IPsec as it exists (i.e., as implemented) today is
barely better than nothing[*].  Actually, not even, because as
implemented one generally can't even express a policy that keys should
be exchanged and used even with peers that cannot be authenticated,
therefore at scale end-to-end IPsec cannot be used at all, never mind to
good effect.  Trying to use IPsec this way might only increase latency.
Even with the relevant fixes (BTNS and connection latching) implemented
and deployed IPsec is difficult to use opportunistically (NOTE: not in
the sense of DNSSEC-based discovery) because of the need to attempt -and
timeout!- an IKE SA exchange.  Opportunistic out-of-band key exchange
tivially falls to DoS attacks.
Layering/combining multiple cryptographic protocols is not that much
easier than layering cryptographic algorithms within one protocol.  The
first problem you run into after you get past impedance mismatches is
inappropriate APIs.
And then there's the question of who pays.  All that extra layering/
crypto has non-trivial costs, especially for large services (think of
Google).  With the end of Moore's law we'll probably see more of the
likes of AES-NI: special-purpose on-die HW, but resources are still
OTOH, if you don't have to scale to Internet scale, sure, IPsec with
very static policies + TLS above that will at least function.  But
you'll probably be using roughly the same PKIs for both, so you'll get
no extra protection, and you'll be wasting resources in the process.
There is value in composing cryptographic protocols, but mostly only so
as to reduce workload rather than increasing security.  Though anything
that improves performance/reduces costs of cryptographic protocols has
the salutary effect of increasing their deployment.
[*] Pun intended.  Now we just need an IETF working group named PUN.

@_date: 2015-02-10 11:54:32
@_author: Nico Williams 
@_subject: [Cryptography] Do capabilities work?  Do ACLs work? 
Capabilities in the academic (i.e., not Linux) sense are great, but we
don't have enough real-world experience with them because, as you say,
ACLs won out.  iOS uses a limited form of capabilities, and this is
clearly superior to Android's privilege model.  But does the iOS model
scale out to large enterprises?  Where Windows and Linux rule, all we
have are ACLs, and to a much, much smaller degree, some flavor of MAC.
Users generally don't want to think about ACLs.  Someone has to though.
Also, from a management perspective, understanding capability usage can
be very difficult (or so I imagine, since I lack real-world experience
with capabilities at an enterprise scale).  Ditto ACLs, but if mgmt can
audit ACLs at a coarse-enough level (something like "share-level ACLs",
if you wish), then they can manage.  See below.
iOS-like systems could certainly use time-limited capabilities, IMO, yes.
There are a lot of things we could do.  The app isolation model leads to
interaction being moderated by capabilities and/or ACLs, but the main
thing there is application isolation.  Interaction moderation via
priviliges, of course, is a bad idea.
Exactly.  Understanding such a system requires inquiring as to its
internal run-time state if it uses capabilities, but not if it uses ACLs
[presumably stored in external state].
The indirection to find locations costs a lot.  Caching of location
information is a partial solution.
Revocation is hard, but that's because revocation is hard (once you give
some entity access, they have at least an analog hole).

@_date: 2015-02-11 11:49:59
@_author: Nico Williams 
@_subject: [Cryptography] Do capabilities work?  Do ACLs work? 
In particular, ACLs can be audited, while auditing capability tokens
requires looking at running state of entire systems.  One of these is
not practical!
Sure capability token usage can be audited, but while that can answer
questions about the past, it doesn't say enough about potential future
Capability tokens are a great mechanism for things that can legitimately
fly under the radar of auditors asking questions like "what can this
user do?".
Capability tokens are not a good mechanism for expressing auditable policy.

@_date: 2015-02-11 12:03:55
@_author: Nico Williams 
@_subject: [Cryptography] Do capabilities work? Do ACLs work? 
Yes, this is a problem.  Authorization information has to be possible to
aggregate in semantically meaningful ways to be useful for answering
meaningful questions such as "what can this employee do?".
This tends to argue for adding coarse-grained authorization (without
removing fine-grained authorization).
Transitivity isn't the problem so much as that traditional ACLs and user
grouping mechanism don't express semantics of interest such as "this is
a group of direct reports for some team/sub-team" and "delegate granting
authority to team X manager(s)".  That and the aggregation problem
mentioned above.
Well, but capability tokens can be passed around, no?  Impersonation
happens to be a common mechanism.  So now we need to express policy
about who can be given authorization to do any particular thing to any
particular resource, and this begins to resemble ACLs.  And/or you can
audit the state of a running system (which is difficult).

@_date: 2015-01-04 02:31:10
@_author: Nico Williams 
@_subject: [Cryptography] 
=?utf-8?q?ything=3F?=
IPsec[*] effectively has no programming interface.  One might think that
that's the correct design (security should be invisible), but since all
IPsec sees from the application is IP addresses (ah, there is an
interface), there's NO relation to the entities that applications
generally want to authenticate.  Which means that IPsec can't provide
meaningful authentication.
IF the sockets APIs dealt in higher-layer/level identities (and
credentials), THEN IPsec could be useful.  But there's no
AF_SERVICE_NAME and so on.
(I hear some implementors want to use DANE to automagically configure
IPsec on the fly, which is a neat idea, but it's a patch, not a design.
The local recursive resolver would do the work of detecting IPsec
applicability and modifying IPsec configuration.)
As it is IPsec is even worse than I paint it above, because IPsec isn't
even aware of logical packet flows.  If you connect() (assume TCP here)
to a service using an IP address and port number, each packet for that
TCP connection might be exchanged with a different peer[**] each time,
even though it all [appear to] have the same IP address!
That didn't have to be so, but no one bothered to implement APIs between
TCP (and UDP) and IPsec.  It could have been done though; see RFC5660
(which came too late).
And it's even worse still: there's often no way for the application to
even know whether IPsec protection is available, who the peer is (even
assuming it couldn't change), nor request IPsec protection, for that
matter.  There are few exceptions to this (e.g., Solaris'/Illumos'
IP_SEC_OPT socket option).
IPsec *could* have been the "what you would design if you had a blank
piece of paper", but such a design would have to include real APIs that
deal in higher-layer/level identities and credentials, and that deal in
packet flows.  Today, for IPsec, it's too late to fix this.
[*] That's the proper capitalization.  _I_ don't care about that, but
    others very much do, fyi and fwiw.
[**] Because IPsec deals in IP addresses at the ESP level, and because
     that's so obnoxious and difficult to deal with (since a lot of
     nodes renumber often but seldom change _names_), we often see IPsec
     authorization rules like "any peer with a certificate from that CA
     can claim any IP address from these CIDR blocks".
The very name indicates that the intention was to have an API that looks
like the native sockets.  Various things get in the way of that, and it
took a long time to get to where TLS APIs look like that, but that seems
to have been the intention, in which case: what's the problem?  Aside
from never having gotten quite that, that is.
Mind you, an API that looks like sockets but deals in *names* could use
IPsec, TCPINC, TLS, or something else -- that choice wouldn't be so
critical, because the key is dealing in *names*, because that's what
people deal with.
It's been done.  For HTTPS, for example, it was trivial (protocol-wise).
For protocols where we used the StartTLS pattern it was a bit harder,
but not that much.
"Secure Socket Layer".  That sounds like it was meant to evoke a
SOCK_SECURE_STREAM (and DGRAM for that matter), or SOCK_* with a socket
option for "secure", or even no socket option (it should always be
The main difficulty is that we'd have needed new AFs: AF_SERVER_NAME,
AF_SERVICE_NAME, AF_USER_NAME, and maybe others, as well as socket
options for pointing the system at one's private/secret key credentials
(or passwords, or...).  None of that exists.
Also, fitting the native OS file descriptor/handle I/O system basically
meant putting TLS in "the kernel", really, in the native OS.  But that
didn't happen either (though there have been a few "kssl" kernel
modules), at least for the post-authentication phase.
Yes, that they are.  Especially when it comes to naming, because the
WebPKI has had so many problems (the use of x.500 naming, for example,
the lack of naming constraints on CAs, for another).
The octet stream part is utterly trivial by comparison to the naming
And multiplexing for X11 display forwarding and so on.  Turns out that
that requires flow control at two layers in the stack, and this doesn't
work so well.
And the opposite too.  Since there were no suitable APIs and no one
cared to standardize them...
Yes, IPsec is useful mostly for VPNs, not end-to-end security.
That's easy.  Authentication is hard because naming is hard.
Is that so?  APIs for CNLP, IIRC, dealt with network addresses, not
names.  Which means they (ISO network protocols) were going to have the
same failures.  It's not just the protocol, it's the APIs.
The naming schemes that sucked least (DNS and name at domain RFC822 style
addressing) didn't come in until very late in the picture of IP and ISO
development.  Everybody missed that boat because the sockets APIs were
designed in an era of rather small networks, and this screw-up (hiding
higher-layer naming from the OS) got baked-in, persisted, and will
continue to be part of our lives for many, many more years.  Because
compatibility.  Because handling DNS and such in "the kernel" is "hard"

@_date: 2015-01-04 10:29:11
@_author: Nico Williams 
@_subject: [Cryptography] 
Another option would [have been] for IPsec (and now TCPINC) to provide
anonymous channels with channel binding data that apps could bind into
app-layer authentication.
This too didn't happen.  Channel binding was first mentioned in a handwavy
manner in 1992 or so, back when CAT WG was working on RFC1508.  We didn't
get around to anything like a formal description until more than a decade
later.  That's quite late...

@_date: 2015-01-04 19:14:12
@_author: Nico Williams 
@_subject: [Cryptography] 
Oh, some functions have been added, like accept4(), so that O_CLOEXEC could
be set atomically.
All we really need are new "address families" and new socket options.
Also, as with the IP_SEC_OPT socket option, I'd say that asking for PROTECT
gets you that (or an error) and bypasses the horrible SPD.  At least to get
started.  This means stronger coupling between the transport layers and
IPsec too: even "connected" UDP sockets should get equivalent protection
for all their packets, with the same peer, and even SOCK_RAW should get
ancillary data suitable for application-level logical packet flow

@_date: 2015-01-04 19:18:08
@_author: Nico Williams 
@_subject: [Cryptography] 
For an implementation strategy for the complex bits of authentication I'd
go with upcalls (a l gssd) or PF_KEY/netlink type IPC protocols (but I
repeat myself).  The session security layer should be in "the kernel".

@_date: 2015-01-04 19:35:02
@_author: Nico Williams 
@_subject: [Cryptography] 
=?utf-8?q?ything=3F?=
Exactly.  Security must be easier than OpenSSL's API, much, *much*
easier.  But it can't be entirely transparent.  The user(s) has(have) to
play some role, and human-meaningful _names_ are the key.
Naming must be simple, and it doesn't get much simpler than DNS (for
hosts) and name at domain for most other things.  Add confusable script
detection and protection against that and I don't think we'll do much
better on that front.
The sooner we accept this, the better.

@_date: 2015-01-05 14:07:33
@_author: Nico Williams 
@_subject: [Cryptography] Why aren't we using SSH for everything? 
on this? I  can reach out to the various parties I know of and kick off a
mailing list...
I cannot contribute source code at this time, but otherwise I'd be happy to
participate.  I can contribute API design ideas for dealing with the
complexity of naming (ideas that have been around for a while and partially
implemented).  If it snowballs i can then consider contributing source code.
There was a discussion (in a review of the PKCS UTI I-D) just says ago
on the ietf at ietf.org and saag lists about the role of the IETF in
networking-related APIs.  It is my view that some Internet protocols (among
them IPsec and TLS, but maybe not so much SSHv2) should include abstract
APIs or discussion of API patterns for them.  At some point we ought to
have a more substantial discussion about this complete with an I-D to
address this much like we do security considerations.

@_date: 2015-01-07 00:31:13
@_author: Nico Williams 
@_subject: [Cryptography] SSH vulnerability when using passwords 
When SSH itself is being used to login with a password, then this is not
so (in all the SSHv2 userauth cases a whole password is sent in one go,
not one character at a time, interactively).
But when one runs, say, "sudo" or some other application that turns off
tty echo and reads a password from the tty, then the other end of the
tty (whether it's SSH or something else) generally doesn't know this and
feeds characters as they are typed.  If an attacker knows that the user
is doing this then they can glean some information from timing of
Propagating echo on/off (and cooked/raw) mode backwards from the tty all
the way to the first client is the correct answer, IMO.  Failing that,
mosh-style heuristics / output prediction should help the client detect
that the tty is in echo off (and cooked or otherwise line-oriented)
mode, so that the client nearest the real TTY (terminal emulator,
really) can read a whole line before sending anything.
I ought to know if it's possible for pty masters to tell what mode the
other side of the pty is in, but all I can recall (and a brief search)
is that the master doesn't get messages about such changes, though it
can poll the pts.  One would think that TIOCPKT mode would have
supported this, but evidently not...  (caveat emptor: brief reply,
little research).

@_date: 2015-01-07 00:34:07
@_author: Nico Williams 
@_subject: [Cryptography] SSH vulnerability when using passwords 
Neither sends a password a character at a time.  RFC4256 is designed
with PAM in mind, which also doesn't have the application feed passwords
one character at a time to the API.

@_date: 2015-01-12 11:59:14
@_author: Nico Williams 
@_subject: [Cryptography] open hardware as a defence against state-level 
Perhaps someday we'll see something like small, cheap, portable 3D
printers, but for ICs.  Such printers would make it easier to implement
countermeasures.  Of course a printer could be compromised and look for
patterns in which to insert backdoors, but that might well prove easy to

@_date: 2016-12-01 18:39:46
@_author: Nico Williams 
@_subject: [Cryptography] OpenSSL and random 
The problem is that if it's a library or a language run-time (e.g.,
Python's here), and if that library/run-time only seeds an internal PRNG
once, and the process using this is long-lived...  it might not be just
innocuous things.  How would you know?  Or maybe it's innocuous today
but not in the next release.
It might be nice to be get an indication of entropy quality from the OS.
At minimum a boolean (true -> real entropy, false -> meh entropy).
_Perhaps_ also an indication of when was the last time new entropy was
stirred in.  (Anything more would be overkill and hard to use well.)
Keying a hash table function will work well enough with meh entropy.
Keying a critical cryptosystem with meh entropy will not.

@_date: 2016-12-01 21:47:02
@_author: Nico Williams 
@_subject: [Cryptography] OpenSSL and random 
I meant: in the API.  A dmesg does the app no good.
Python could use lame rng seeds for hash table randomization, note the
lameness, and reseed later when cryptographically-secure an rng is

@_date: 2016-12-02 11:26:50
@_author: Nico Williams 
@_subject: [Cryptography] OpenSSL and random 
For a sufficiently-low number of bits we'd have a number of recognizable
SSH host keys and such.  How low is that?  16 bits is too low for sure.
20 bits is still too low.  30 bits is probably also too low in that, if
you compute all the SSH host keys that could result, store them, then
scan the network for them, and there are many devices generating SSH
hostkeys with 30 bits or less...  64 bits is quite safe from this sort
of attack.  So we're looking at a minimum well above 30 and closer to
What I really want is an API that lets me specify my app's minimum
entropy requirement, and returns an error if that minimum cannot be met.
Two or three values will do: "meh" and "cryptographically-secure", or
"meh", "low-value", and "high-value cryptographically-secure".  "Meh"
would always be available.  A number of required bits of entropy will
also do, but IMO it's overkill.
One more thing: entropy in a CSPRNG pool is never depleted, only
stretched.  Once seeded then pool must be available for
cryptograpically-secure outputs without blocking ever again.

@_date: 2016-11-28 21:48:06
@_author: Nico Williams 
@_subject: [Cryptography] randomness for libraries, e.g. OpenSSL 
Agreed.  We should use it.
Adding other sources of entropy is a no-brainer.
Well, but not *everone* has to have that capability.  Many can
understand the design.  Very few can tear down a chip and verify it.
That a few labs do would be a lot better than that none do.  Granted,
that might not be saying much if it's just two labs subject to nation
state pressure, but it's still better than nothing.
There's a limit to how much intelligence can be baked into an ME's
flash.  One may have to resort to constantly updating such a protocol
and code for it, but that would be a very worst-case scenario.  (Not
unlike gaming and cheating, actually..., but it'd be hard for Intel to
"cheat" so much.)  In any case, a remote source of entropy can't raise
local entropy, but it can't hurt either (assuming a decent CSPRNG).
Remote sources of entropy can be all sorts of innocent-looking things,
such as search engine stats, news pages, and so on.

@_date: 2017-12-14 16:09:03
@_author: Nico Williams 
@_subject: [Cryptography] Privacy-preserving wireless communication? 
In OP's case pairing is done over wired connections, so there's no
scalability issue w.r.t. number of announcements.  If the MAC in the
announcement does not match, the recipient does nothing.
How about a DoS-resistant variation on your announcement, with a
pseudo-randomized ID/sequence number:
  announcement = {{nonce, PRF(shared_secret, peer_ID, seqnum)},
                  MAC(shared_secret, {nonce, PRF(peer_ID, seqnum)})}
This allows the sender and recipient to keep some per-peer state
(sequence number) that allows them to pre-compute a number of
PRF(shared_secret, peer_ID, seqnum) and thus prevent a DoS by sending
lots of announcements.
A seqnum resync announcement can be processed only when the user presses
an appropriate button.  Devices could do this automatically, but
rate-limit resyncs.
For peer IDs use public keys (or hashes thereof).  Names can be purely
local for applications like OP's.  Alternatively, exchange peer IDs
during (or after) pairing.
If you need to pair over wireless you could use (EC)DH with an
interactive challenge/response with challenges displayed to the user
only.  The difficulty that arises in wireless pairing is that either the
public keys are per-device (in which case pseudonymous) or per-pairing
(in which case anonymous because ephemeral), but!...
...the problem with per-pairing public keys is that now there's no way
to decide which wireless pairing announcements to ignore withou more
user interaction (more than merely pressing a button, confirming a
challenge, and pressing a button again), I think.

@_date: 2017-12-21 10:37:16
@_author: Nico Williams 
@_subject: [Cryptography] Rubber-hose resistance? 
If that happens you just dispose of the devices.
A simple defense against this sort of attack is to carry just a
raspberry pi and an SD card with a minimal OS and content.  You can
always buy a new SD card, download an image, and build yourself a remote
access terminal.  It's pretty simple.  It's not likely that customs will
have a hardware attach for every SBC out there, and you can always
inspect it, as these computers are very small and their boards highly
If you stay at a hotel then chances are you can just display onto the
room's TV with an HDMI cable.  Or you can carry a 14" portable display

@_date: 2017-12-22 10:51:39
@_author: Nico Williams 
@_subject: [Cryptography] Rubber-hose resistance? 
TSA?  Pfft.  They'll just swab the stuff to see if it's got any traces
of explosives.  I've never seen TSA do more than that.  The agents
running the x-rays don't know who you are, so they won't target
people because of their being "interesting".
Customs is a completely different story.  Customs is not interested in
detecting explosives and weapons (TSA's remit) so much as any and all
contraband.  U.S. customs cannot keep U.S. persons from entering, but
they could keep those devices for a while.  If you're not a U.S. person
they might deny you entry, or allow you to enter and still take those
However, U.S. customs officers don't search everyone, and they don't
x-ray everyone's bags (they're not even setup for that), so they won't
even know that you have a non-standard computer with you.  Instead they
select people for additional scrutiny based on: their records about the
persons, suspicious behavior, or just your telling an immigration
officer that you are carrying something of, say, agricultural interest.
If they search you, it will almost certainly be because the immigration
officer selected you for a search.
"Candy" -> no problem; "prepared food (e.g., a sandwich)" -> search;
"chocolate eggs with a surprise inside" -> search and likely a _massive_
My guess is that U.S. customs is not likely to keep your devices unless
you're a person of interest to them to begin with.  I fancy that all of
us on this list are persons of interest, but honestly, that's probably
not true, at least not as to U.S. persons, and probably not as to
foreigners either -- they have much more interesting things to worry
about that crypto nerds.
Anyways, there's no need to keep these things "wired together" in your
baggage :)  It's just: 1 SBC, 1 portable display, 1 keyboard, 1 mouse.  If
the mouse and keyboard are wired that's 5 cables (two for power), else
it's 3.  That's not remotely out of the ordinary.  I travel domestically
with more cables than that and a proper laptop or two, and never has
that been a problem.

@_date: 2017-12-22 11:26:16
@_author: Nico Williams 
@_subject: [Cryptography] Rubber-hose resistance? 
Why on Earth would you carry the SBC _powered_ through a border?  Or
even _at all_?  It's not a smartphone.  If you're not using it, you just
don't power it.  You don't sleep it.  You don't hibernate it.  You just
halt it, remove power, and put it away.

@_date: 2017-11-13 20:30:17
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
Protocol buffers is remarkably similar to... ASN.1 w/ DER rules.  Makes
one wonder why even bother building a new thing that's not new at all :(
But then, there's been nothing new since S-expressions...  And nothing
new since ASN.1, and nothing new since BER/PER/XER.  And yet there's
always one more of these wheel reinvention events around the corner.
The big problem with ASN.1 for many, many years was the lack of access
to the standard, and the lack of tools.  If you want to reinvent this
wheel because of that _now_, well, please don't: you'll be building
tools that don't exist for your wheel, when you could be using off-the-
shelf ones, or building new ones, for ASN.1/whatever.
Well, no, ASN.1 itself does not do that.  DER (and CER) -- encoding
rules for ASN.1 -- do, but not all encoding rules do.
In any case, we've learned not to depend on canonical encodings, so
canonical encodings are neither here nor there.  We just don't build
protocols/applications that require repeatable canonical encodings.
If you ever see a proposal that would need canonical encodings, well,
you can inform its authors of just how wrong they are (unless there's
something truly new and they aren't).
(You'll notice that we said no to canonical JSON.  It's just one huge,
low-reward rathole.)
It's perfectly possible to produce JER -- JSON Encoding Rules for ASN.1.
I'm sure someone's done it...  I'd rather nothing think about YER
The big problem with JSON is the need to quote/dequote strings, and lack
of streaming support.  Binary JSON encodings (CBOR, ...) take care of
I wouldn't say that DER is particularly efficient.  I mean, it's not,
because it's got all this tagging redundancy, and the variable length
encoding of lengths + definite-length encoding of everything is a nasty
If you want efficiency, PER is probably your ticket.  You might want a
four-byte aligned variant of PER, aka XDR (seriously, XDR is very close
to that!).
As for human readability... I'd rather use encodings that can be
formatted nicely and which have powerful tooling.  XML has XSLT/XPath --
yeah, XML is super verbose, but XSLT is super convenient.  JSON has jq,
which is as much pithier than XSLT as JSON is than XML (more, actually,
but I may be biased, being a jq maintainer).  But JSON is horrible as
UI, much more so than XML.  I don't really think YAML is right either.
Personally I'd go with SQLite3 (SQL) for application configuration.
This removes the need for any kind of human readable/authorable document
format, and still gives you the power of a declarative programming
language to deal with configuration.  Win-win.
But now we're far, far afield from crypto :(

@_date: 2017-11-14 15:34:09
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
Heimdal has an actual, honest-to-goodness ASN.1 compiler with BER and
DER support.  We should break it out into a separate repository too.
Heimdal's ASN.1 compiler compiles ASN.1 to C, and is very easy to use.
It also has an option to compile to an interpreted, bytecode-like
template that produces much less object code.
Not that DER is a fantastic encoding.  PER is *much* better.  But that
there is no excude to reinvent this wheel nowadays.
Reimplement, sure.  Reinvent?  Please spare the rest of us having to
implement yet another encoding.
Really, inventing a new encoding imposes a large burden on the rest of
the world, and usually only because the re-inventor(s) couldn't be
bothered to burden themselves with the cognitive load of reading and
understanding an existing spec.  Don't do it!

@_date: 2017-11-14 16:00:34
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
Can we be a bit more pedantically correct?  ASN.1 is *syntax* and has an
associated set of encoding rules.
Those rules include:
 - tag-length-value (TLV) encoding rules that suck: BER, DER, CER
 - PER -- an XDR-like set of rules, but with 1-byte alignment (vs.
   4-byte alignment for XDR)
   XDR is also very inefficient at encoding optional fields and booleans
   in general since it does not pack booleans and the implied booleans
   for optional fields.  Whereas PER does pack booleans into bitfields,
   so PER is very efficient.
 - XER -- XML Encoding Rules.
   Yes, ASN.1 is just syntax, allowing any number of encodings, even
   XML.
   This is so much so that there's even an ASN.1/PER-based compression
   scheme for XML (FastInfoSet).
Confusing ASN.1 and DER is how we lead others to say "screw ASN.1, I'm
going to do a brand new thing", and then we all get burdened with that
new thing.  Hello Protocol Buffers (which, ironically, is
DER sucks because it's a definite-length encoding with variable-sized
lengths, which means that before you can encode your data structure you
must compute the encoded size, or alternatively that you must traverse
the structure in post-order to encode it from the end and realloc
buffers as you go.  Either way sucks, though if you have a compiler and
run-time then the only thing you'll observe is that encoding is
not-online (though decoding is).
BER doesn't have this problem in that you can choose either indefinite-
length encoding or to not minimize the encodings of lengths.  And if you
choose indefinite-length encoding then BER is online for encoding.
PER is as good as it gets, even today, with the only tunable of interest
being its alignment (which is 1-octet, but it would probably be faster
with 4-octet alignment).
The only downside to PER -- the reason we don't use it universally -- is
that you really do need tools that can compile a complete ASN.1 module,
and these did not exist for a long time, not as open source code
anyways.  Of course, the situation is better now, but it's too late.
Though it's never too late to say NO to new encodings.
(I wouldn't say no to things like CBOR or JSONB, since those are
specifically tailored to JSON, which exists.  But please, no new
Protocol Buffers alikes.)
BER/DER/CER were definitely NOT designed to match 1980s CPUs.
As for ASN.1 the *notation*, the faster CPUs get, the less reason there
is to not use it, since it's just a compile-time thing.
As for encoding rules, PER, or PER with a four-octet alignment, is still
probably the best (fastest, most compact, most efficient) encoding.
Certainly, if the data compresses well, and is full of octet or
character strings, then applying a compression function will help.  But
compression is still orthogonal to encoding rules, because you'll still
need those.
Protocol buffers is a TLV encoding with definite-length encoding --
i.e., just a variation on ASN.1's DER.
I don't recall the details, but if it doesn't use variable length
encodings, that would be a win, though still not a huge win as long as
definite-length encoding is part of the system.
If you're looking at "ASN.1" (meaning DER) and think "wow that sucks",
what you want to do is look at PER and think of how you might improve it
(there isn't much room for improving PER, really).
It's perfectly fine to produce CBOR or JSONB when you're dealing with
JSON data.  Since JSON exists, you have to deal with it.
It's perfectly fine to look for alternate encodings of XML, and for the
same reason.  (And people have.  Again, see FastInfoSet.)
But when you don't have a pre-existing thing to deal with, and the only
problem you're facing is lack of tooling for your chosen programming
language, well, there's no excuse for building your own.  First do the
research, pick an existing specification, and write tools for that.
Alternatively, if you're lucky enough to find suitable tools, use them
and save yourself the burden of building them.

@_date: 2017-11-14 17:35:41
@_author: Nico Williams 
@_subject: [Cryptography] [FORGED] Re: Is ASN.1 still the thing? 
Why should it have been a problem at all?  If you have a decent compiler
and run-time, like, e.g., Heimdal does, then ASN.1 and BER/DER can be an
Then by all means use XDR.
Just for the record (and, I know, I'm repeating myself here), XDR
(syntax and encoding) is basically a subset of ASN.1 (syntax) and PER
(packed encoding rules) with 4-octet alignment and no packing of
If you can do XDR, you're not far from doing ASN.1/PER.
If you think XDR is simple, then why would you not also think the same
of ASN.1/PER?  The only obvious answer is "lack of tooling".  Even XDR
doesn't come with that much tooling either -- there's the rather lame
rpcgen(1) program, libxdr, and not much else.  XDR is just simple enough
that one can code it from memory -- a very nice advantage, to be sure.
(The very simple krb5_ret_*() and krb5_store_*() functions in Heimdal
can be trivially used to implement XDR protocols without having a
compiler.  This is not by design.  It's just an accident -- a very happy
accident -- born of XDR's utter obviousness and simplicity.  So there's
a lot to say for XDR!  But when you have good compilers then this
simplicity advantage disolves away.)
If nothing else, the ASN.1 specifications series, x.68x (ASN.1) and
x.69x (encoding rules) are extremely well-written and readable, with
decades of experience fine-tuning.  That too is an advantate, especially
now that those are free (and now have been for years).
It is DER (and CER, and to a lesser degree, BER) that sucks, not ASN.1.
IF you ever must invent your own encoding (please don't), just remember
that TLV ("self-describing") is garbage.  TLV encodings have unnecessary
redundancy, lead to serious bugs in hand-coded implementations, and are
all-around just a lazy way to avoid compiling a description of the data.
The only nice thing about TLV is that when you don't know the type of
some data, you can still get a feel for its shape, and so you can have
tools like asn1dump that don't need to know what type of data you want
to dump.  But it's still better to [and usually inexcusable not to] know
the type of the data anyways.

@_date: 2017-11-14 17:37:25
@_author: Nico Williams 
@_subject: [Cryptography] [FORGED] Re:  Is ASN.1 still the thing? 
*En*coding in DER is not streaming.  And only in DER.
*De*coding is.
And ASN.1 != DER.

@_date: 2017-11-14 21:23:32
@_author: Nico Williams 
@_subject: [Cryptography] [FORGED] Re: Is ASN.1 still the thing? 
By using extensibility markers.  Which Protocol Buffers.. does not have.
  People ::= SEQUENCE {
    name UTF8String,
    address UTF8String,
    ...
  }
That ... says "expect extensions, which will be added here".  The ITU-T
thought a lot about extensibility.
You're saying Protocol Buffers exists because ASN.1 types are not
extensible?!  I hope you're wrong, because otherwise that's a lot of egg
to wipe off one's face.  You really do want to look at what came before
before you dispose of it.
  People ::= SEQUENCE {
    name UTF8String,
    address UTF8String,
    ...,
    firstName UTF8String,
    lastName UTF8String
  }
Hah!  Protocol Buffers *needs* tags.  ASN.1 *doesn't*.  Sure,
BER/DER/CER use tags, but you can use the automatic tags feature and
never write them down in actual ASN.1 modules.  And PER does NOT even
use tags on the wire.
So, to recap: ASN.1 got you covered and gives you choices of encodings.
You could even make JSON Encoding Rules (then apply CBOR when you
realize that JSON kinda sucks, and you're back to a binary encoding).
Heck, today I learned of a textual encoding rules for ASN.1: GSER.
Surprise!, it's a bit JSON-ish.
And you could even make a PBER -- Protocol Buffers Encoding Rules for
Some may think about X and then think that other people haven't thought
about X before.  Some may be wrong.
Some may not research the literature before they start coding.  Some may
needlessly reinvent wheels.  Some may even reinvent wheels badly.  Do
not expect this to end though -- it seems to be human nature.  But you
could do your part today: learn, then teach, this lesson, that you
should look before you go.

@_date: 2017-11-14 21:29:23
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
We even had that!!!  It's called ONC RPC.  Or DCE RPC.  Or whatever the
Apollo thing was called.  And there's more, I'm sure.  Lots of complete
solutions.  Remember SOAP, anyone?  That was totally the last complete
solution we were ever going to need.  All of these are actually still in
In two years there will be some other "complete solution".
And what if you need to interop with a whole stack of things?  Maybe you
need an NFSv4 implementation (ONC RPC/XDR) and some MSRPC of some sort
(basically DCE RPC) and some SOAP-ish thing, and...  You'll be spending
enormous amounts of time just reading all the specs, finding tools,
building the ones you can't find, building FFI bindings for the ones you
can.  You'll then curse whoever thought they should add that Nth
"complete solution".
I don't even want to look at this Avro thing.  I bet I'll find lots of
terrible choices were made by people who could not take the time to
learn what came before.  I might be surprised though, but I'm still not
enthused to go look.  I'd rather hope it's good and not find out until
the day I'm forced to.

@_date: 2017-11-14 21:37:06
@_author: Nico Williams 
@_subject: [Cryptography] [FORGED] Re: Is ASN.1 still the thing? 
And before Avro or whatever other thing you could have picked?  No,
you're right, it was better to reinvent.
Someone saw no tools and chose to build tools.  Great!  But they also
chose to invent a new syntax, encoding, RPC service locator, etc.
That's not great -- that's a terrible disaster for every else.
The least they could have done is used some other non-ASN.1 system.
ASN.1 is alive and kicking.  You have the wrong attitude.  In our world
code never dies.  It sticks around forever, being a pest to whoever must
maintain it.  But by means let's add to that pile of code more code,
more legacy.
Yeah, you could say I resent this mess.  Been traumatized by it.  How
many encoders/decoders have you had to wallow in, maintain, or author?

@_date: 2017-11-15 10:47:00
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
I don't know of any cases where one does this, but I can imagine one:
replicating a logical stream of operations on a Merkle hash tree DB,
with a hash/MAC/signature of the final Merkle hash tree root.  The
logical stream being a compression of the block-level stream, this is
worthwhile just on account of that when the goal is to replicate the
same Merkle hash tree on the recipient side.  (Of course, one should
also integrity-protect the logical stream in this case.)  This would,
indeed, be a legitimate application of canonical encodings.
(I can't think of any such DBs or filesystems.  And in a distributed,
highly concurrent DB/FS world, a single Merkle hash tree root is
basically unobtainium.  In centralized systems it could be done.  If
ZFS, for example, were more of a CAS FS, meaning that physical locations
did not affect its Merkle hash tree, then the ZFS root hashes would be
more meaningful and reproduceable, as in git, and thus replication of
the ZFS intent log (ZIL) rather than computing deltas as in "zfs send"
would be interesting indeed!)
In security protocols, however, we do not need canonical encodings
because in the common case you have an input message and its hash/MAC/
signature and you just verify that rather than reconstructing the input
message from other data.
DER *only* gets you a canonical encoding.  Every time.  There's no
licensing requirements.  There are free compilers and libraries.  If you
meant PER:
I don't recall whether PER produces canonical encodings.  I imagine it
could, since it's very similar to XDR as I've explained -- there aren't
a lot of choices to make, and if there are any, one could standardize a
variant that leaves no choices (just as DER is such a variant of BER).
It's true that there aren't many (any?) open source implementations of
PER.  It's a chicken-egg situation: not much uses PER, so not many tools
support PER.  We should fix this not by creating a replacement for PER
but by creating the tools as we need them.
If I had the time I'd teach Heimdal's ASN.1 compiler support for PER,
XDR, GSER (ugh), XER, and maybe even some JSON encoding rules (mostly so
I can decode BER/DER, reencode in JSON, then use jq(1) to deal with the
content :)

@_date: 2017-11-15 11:10:12
@_author: Nico Williams 
@_subject: [Cryptography] [FORGED] Re: Is ASN.1 still the thing? 
Mind you, just using BER/DER/CER is not sufficient, since a decoder is
free to produce an error when it sees unexpected SEQUENCE fields.  And
for CHOICEs and SETs the extensibility markers are even more important.
ISTR reading some of the history behind ASN.1's extensibility rules.
They were motivated by both, PER and the existence of BER decoders that
complained about unexpected fields.  Of course, there are other
extensibility mechanisms that do not require special support from the
notation / encoding rules, such as "typed holes" (sequences of
{type_id, inner_value}).
What's interesting about that history is that there's been nothing new
since.  Ignoring ASN.1 is a really bad idea whatever one's reason for
ignoring it.
Everything we've discussed in this thread, from TLV vs. XDR-like vs.
textual vs. markup encodings, canonical vs. not, online vs. not,
extensibility, notation, mapping to/from other schemes (e.g., XML
Schema) -- it's all there in ASN.1's history, and, indeed, in the
x.68x and x.69x specification series.  We have decades of experience
with ASN.1 in the ITU-T and IETF, and in the communities that
implement protocols that use it.
When you reinvent a wheel with this much history, you will almost
certainly miss important ideas if you don't look at that history.
And, with this much history, you might as well just _use_ ASN.1 and only
build new tools as needed.  New encoding rules make sense for interop
with existing other systems, but that's about it.

@_date: 2017-11-15 14:58:06
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
I'll have to check the spec.
Good to know.
You can always not use the IOS.  It's mostly just syntactic sugar that
allows you to automatically wrap/unwrap things in typed holes.  You can
just hand-write the code for that -- it's simple enough.
An ASN.1 compiler that only implements the base x.680 goes a long, long
I bet PB doesn't have anything like the IOS...

@_date: 2017-11-15 15:03:57
@_author: Nico Williams 
@_subject: [Cryptography] [FORGED] Re: Is ASN.1 still the thing? 
Right, but IIRC it was not in the 1984 version, or there were such
implementations back then.  And in any case, non-TLV encodings like PER
require knowledge of extensibility markers, thus they were added.
IIRC ASN.1's creators never expected to add something like PER, but they
did it because of complaints from the IETF crowd about the silliness of
TLV encodings, about the superiority of "bits on the wire" specs to TLV.
The IETF participants who complained were right, and the ITU-T was right
to respond by adding PER.  (Which goes to show that the ASN.1 community
was and probably still is responsive.)
(I wasn't there.  This is memory of reading about this history.)

@_date: 2017-11-15 17:18:04
@_author: Nico Williams 
@_subject: [Cryptography] [FORGED] Re: Is ASN.1 still the thing? 
Excellent!  OER looks pretty good; I hope it always encodes lengths in
fixed sizes so that it can be online for encoding.  I'm pretty sure that
in the face of extensions, any PER-like encoding cannot be online for
encoding, since lengths become necessary, unless extensions get chunked
(which I forget if PER does).
Is JSER JSON Encoding Rules?  The only things I can find on that call it
Indeed.  The haters gonna hate, but they don't have a leg to stand on.
Honestly, I don't really like the use of tagging in ASN.1 modules as
it's just a distraction and people do sometimes make mistakes in their
choices of tags, and the syntax is a bit clunky, especially for the IOS.
But other than that I can't think of a legitimate complaint about ASN.1,
not even lack of tooling, since anything new one constructs instead of
ASN.1 will always also lack tooling initially.  The only way in which
[lack of] tooling makes a difference is for existing alternatives to

@_date: 2017-11-15 18:32:43
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
That's true of all TLV encodings.  The issue is redundancy in the
context of nesting of structures, which leads to lots of lengths that
you must make sure to sanity-check before ever using them.
(This is just as true of protocol buffers as of DER, because PB is a TLV
The bottom line is that if you use *tools* to compile IDLs, then there's
no security concern from the use of TLV (or any other) encodings.
OTOH, if you're going to be hand-crafting a codec, well, first, best
not! and second, don't! and third, yeah, TLV is a viper pit.
"insist ... that fields not repeat" -- that's a tall order in JSON-land!
I'm not sure what you mean about a "normalized form".  Canonical form?
But JSON doesn't have a canonical form.
So I don't see those grounds for believing that JSON is less a viper pit
than DER.
I suspect that an analysis of actual security vulnerabilities in DER and
JSON codecs will show that JSON is in fact safer, unless we limit
ourselves to vulnerabilities in the past decade, then it might all be
There were lots of old hand-coded DER codecs in the 90s that needed to
have their bugs shaken out, and mostly I think they have.  Perhaps the
only real difference for hand-crafted DER vs JSON codecs is that overall
know-how in the industry having improved had improved dramatically by
the time JSON came along.
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Exactly: it's a solved problem, unless you decide to re-invent solutions
to it, because then it's on you to write secure codecs all over again.

@_date: 2017-11-15 18:53:19
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
Well, x.509/PKIX does suck this way in that it says to use DER for
computing signatures, but doesn't say to use DER for the actual
TBSCertificate as issued.  This means that you do have to decode and
re-encode in order to verify signatures.  That's _PKIX_'s fault, not
DER's.  (In practice I suspect all issues only use DER anyways.)
You could say that having as much rope as N>1 encoding rules to choose
from is the problem, but we were always bound to have a multiplicity of
encoding rules.  Picking from among those is like picking from among
those and XDR and PB and XML and JSON/CBOR/BSON/whatever -- pick
As to the issues with Real values not round-tripping, well, x.509 does
not use Real, so I'm not sure what he meant.  Maybe SPKI did??
And yes, in general, one cannot expect anything other than smallish
integers (32- or 64-bit, signed or unsigned) to round-trip.  This issue
comes up all the time in many open source communities I participate in,
such as jq or SQLite3.  People are always surprised by IEEE754 issues.

@_date: 2017-11-16 15:38:28
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
If this is what relying parties do, then maybe it's time to update PKIX
to say that TBSCertificate MUST be encoded in DER and drop the language
about having to [re-]encode in DER before verifying the signature.
Alternatively simply state that the TBSCertificate MUST be delivered in
the same form as the input to the signature function.  Who cares if it's
BER or DER or CER, as long as a) the RP can decode it, b) there's no
need to re-encode to verify the signature?
Sometimes the spec has to reflect reality.
Tero's post does make me think that this is mostly something we'd see
only in the context of something like LDAP re-encoding.  Clearly this
can't really happen in any other case: CAs themselves must sign DER if
the PKIX language is to work at all, or else CAs sign whatever is also
always delivered to users (and thence RPs).
As for the LDAP server re-encoding case, it's easy enough to push the
re-encoding requirement to LDAP clients that care *if* there are any
such servers left anywhere.  Or to the servers and say they "MUST NOT
re-encode TBSCertificate", or even just say that in PKIX and be done.
I guess it's time to take this to the appropriate list at the IETF...

@_date: 2017-11-17 09:54:26
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
IEEE754 issues are encoding-agnostic.  This kind of thing comes up all
the time in JSON tooling, in ECMAScript, in RDBMSes, and many other
things besides.  S-expressions wouldn't be immune.  If you want floating
point numbers to interop and round-trip then all implementations must
use arbitrary precision floating point software implementations.

@_date: 2017-11-17 14:06:59
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
"specified to work" -- specification vs. reality.
The moment you parse into an IEEE754 representation and use that as the
canonical form for subsequent output, you've failed to round-trip.
Serialization formats generally don't know anything about IEEE754, and
rightly so.  And not everyone uses IEEE754 for in-memory representation
of real numbers.  Numbers then might not round-trip real numbers because
the source uses an arbitrary precision library (or unums, or...) and the
other peer uses IEEE754.
If you'd like some examples with JSON, go look at issues tagged ieee754
on  .

@_date: 2017-11-17 15:02:39
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
IEEE754 is not all that exists as to floating point number
representation.  There are others.  JSON, encoding rules for ASN.1, and
so on, generally try not to be constrained by IEEE754 -- they might not
even mention it.  RFC7159 mentions this issue, and that's it -- no MUST,
no SHOULD.
For security protocols this should be a non-issue though: we don't use
real numbers.  But someone said they were advised to stay away from
ASN.1 because of this issue when designing SPKI -- that's just nonsense :/

@_date: 2017-11-18 15:51:02
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
OK, thanks for that history.  It's too bad he misunderstood.  Reals are
irrelevant to PKIs, and ASN.1 and its encoding rules have nothing to do
with IEEE754 issues any more than any other set of encoding rules.

@_date: 2017-11-19 19:31:40
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
No, you've written 4 BER/DER codecs.  But that's NOT ASN.1, that's just
BER/DER.  All TLV encodings are garbage.  That includes protocol
buffers.  ASN.1 itself is fine (especially if you ignore all character
string types other than UTF8String), and some of the other encoding
rules are just fine.  It's the encoding rules that are great -- or

@_date: 2017-11-19 19:34:07
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
Did you not read?  The problem is that not all codecs use the same
parsed representation of real numbers.
It'd be useless to addressing this problem unless you also had the ITU-T
dictate that only IEEE754 may be used to represent parsed real numbers.
Which in turn would mean no arbitrary precision real numbers, and no
unums, ...

@_date: 2017-11-21 15:05:05
@_author: Nico Williams 
@_subject: [Cryptography] [FORGED] Re: Is ASN.1 still the thing? 
Sure.  But again, would you rather respond to this by:
a) building a new spec and tools for it,
b) building tools for the existing spec
If you do (a), chances are you'll do something like what Google did with
Protocol Buffers: i.e., miss important considerations.  And worse:
you'll burden the rest of us with building yet more tools.
If you do (b) you have less work to do.
If you listen to PHB and friends who say "ASN.1 is bad" when what they
mean "BER/DER/CER are bad", then you will probably choose (a).
There are few wheels like PER out there at all.  The closest is XDR.
You will not make a serious mistake in picking XDR, though the tools
that exist for it are not very good.
As others, and I, have been saying, XDR is a perfectly good choice in
most cases.

@_date: 2017-11-24 15:11:50
@_author: Nico Williams 
@_subject: [Cryptography] [FORGED] Re: Is ASN.1 still the thing? 
Please don't.  Don't use BER/DER/CER -- those are lousy, as is Protocol
Buffers, because they are all TLV encodings.
Use PER/OER/XDR or anything like them, but don't build something brand
new.  It'll be a burden on the rest of us.
JSON is _horrible_, mostly because of the need to escape characters in
strings, and the lack of a binary type.  Some binary JSON encodings are
Sure, you absolutely can generate ASN.1 or whatever from equivalent
languages, and there's a lot to say for doing that, especially if you
use XML or SQL, as then you can write a lot of tooling in functional/
declarative languages.  I do this sort of thing (in the latest case I
generate JSON schema descriptions from SQL schemata).
And, of course, you can easily produce a one-to-one mapping of a subset
of XML to/from ASN.1, so you really need not think of ASN.1 if you hate
the notation.
Easy to work around: chunk up any data types whose values can get that
big.  It's probably worth it too.  And if you have indeterminate-length
payloads, you'll have to chunk them up anyways, a la HTTP chunked
transfer encoding.

@_date: 2017-11-24 15:16:43
@_author: Nico Williams 
@_subject: [Cryptography] WIPEONFORK in Linux 4.14 
You can't get this without support from the device.  There's no way to
guarantee that an overwrite is an overwrite, or that there's no trace
left of the original.
It's better instead to have per-file encryption keys, then you can
forget those.  Of course, those keys have to be stored encrypted in some
other, master key, and since this all would go on disk... this doesn't
help all that much either as it's turtles all the way down (up), and so
eventually for secure deletion via decryption-key-forgetting you have to
actually change a master key and humanly forget the passphrase it was
derived from.
They don't work anyways.
Truly deleting data is *really* hard.

@_date: 2017-11-25 17:46:52
@_author: Nico Williams 
@_subject: [Cryptography] WIPEONFORK in Linux 4.14 
So there's a turtle holding a turtle ...
The bottom-most turtle, however, may have the same troubles ensuring
deltion as any other bottom-most turtle.  It's still not easyy.
Zeroing out blocks need not have the effect you seek.  This is why a
TRIM command was added so that devices could be told of a block's
Doesn't help more than encryption and key erasure, but since key erasure
itself ultimately depends on being about to overwrite actual blocks on
the storage device proper, it ultimately comes down to whether the
stack of filesystems, device drivers, and firmware, can manage to track
all the locations to overwrite, and then do it.
This is all very difficult to ensure in a simple stack in the absence of
VMs.  It's harder still to ensure when using VMs.
It's still, today, much easier to securely destroy an entire storage
device than it is to ensure secure deletion of a file.

@_date: 2017-11-25 21:57:54
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
ASN.1 is pretty clean, but BER/DER/CER are all crap (mainly the TLV
thing, and the poor choices made for canonicalization in DER and CER).
I've never been formally trained in ASN.1 either.
Are you referring to the EXPLICIT keyword?
Explicit tagging -> TLV nesting.  I.e., TLV' where V' is the underlying
TLV, so: TLTLV.
That's right: extra redundantly and ridiculously wasteful.
Here you get:
  Tag(CONTEXT, 0) || Length(Tag(UNIVERSAL, INTEGER) ||
                        Length() ||
                        <encoding of INTEGER 0, 1, or 2)
What was the point of using EXPLICIT tagging for that field and IMPLICIT
for the rest?  I don't know, and so far I can't think of an obvious
(While we're on the subject of mistakes in x.509, it's very unnatural to
 apply a signature to a subset of a PDU (in this case, tbsCertificate).
 tbsCertificate should have been an OCTET STRING containing the
 TBSCertificate.  But whatever.)
As for where this is described...  It's described in x.690, section
8.14.  It's also referred to in a few places in x.680.
 - x.690, 8.14:
   8.14 Encoding of a tagged value
   8.14.1 The encoding of a tagged value shall be derived from the complete
          encoding of the corresponding data value of the type appearing
          in the "TaggedType" notation (called the base encoding) as
          specified in 8.14.2 and 8.14.3.
   8.14.2 If implicit tagging (see ITU-T Rec. X.680 | ISO/IEC 8824-1,
          30.6) was not used in the definition of the type, the encoding
          shall be constructed and the contents octets shall be the
          complete base encoding.
   8.14.3 If implicit tagging was used in the definition of the type, then:
          a)     the encoding shall be constructed if the base encoding
                 is constructed, and shall be primitive otherwise; and
          b)     the contents octets shall be the same as the contents
                 octets of the base encoding.
This actually makes sense if you read the rest of the spec, but it's
also made more obvious by already knowing that EXPLICIT tagging means
one more layer of TLV while IMPLICIT tagging means replacing the tag
(but not the constructed/primitive bit) in the TLV encoding :(
(I'm probably coming across as defending ASN.1, but I want to make
 absolutely clear that BER/DER/CER are horrible.  My defense of ASN.1 is
 mostly about dissuading people from reinventing that wheel _badly_.  If
 you want to reinvent it, please don't make the TLV mistake again.)
And here's a couple of places where x.680 talks about tagging:
 - x.680, 30.5:
   All application of tags is either implicit tagging or explicit
   tagging.  Implicit tagging indicates, for those encoding rules which
   provide the option, that explicit identification of the original tag
   of the "Type" in the "TaggedType" is not needed during transfer.
     NOTE -- It can be useful to retain the old tag where this was
     universal class, and hence unambiguously identifies the old type
     without knowledge of the ASN.1 definition of the new type.  Minimum
     transfer octets is, however, normally achieved by the use of
     IMPLICIT.  An example of an encoding using IMPLICIT is given in
     ITU-T Rec. X.690 | ISO/IEC 8825-1.
 - x.680, E.2.12.5
   Textual use of IMPLICIT with every tag is generally found only in
   older specifications.  BER produces a less compact representation
   when explicit tagging is used than when implicit tagging is used.
   PER produces the same compact encoding in both cases.  With BER and
   explicit tagging, there is more visibility of the underlying type
   (INTEGER, REAL, BOOLEAN, etc.) in the encoded data.  These guidelines
   use implicit tagging in the examples whenever it is legal to do so.
   This may, depending on the encoding rules, result in a compact
   representation, which is highly desirable in some applications.  In
   other applications, compactness may be less important than, for
   example, the ability to carry out strong type-checking.  In the
   latter case, explicit tagging can be used.

@_date: 2017-11-25 22:06:18
@_author: Nico Williams 
@_subject: [Cryptography] WIPEONFORK in Linux 4.14 
Even back then there was bad block remapping.  Sure, there weren't going
to be a lot of bad blocks to remap, but still, it was an issue.
It's long been the case that absolutely ensuring secure deletion of a
file is difficult or impossible without destroying the device.
Or getting an error if it can't be done.
I agree with this.  Secure file deletion should be a proper system call,
ultimately implemented by the actual filesystem.  Until that happens, it
might as well not exist, and even then, getting it right is hard for all
the reasons given in this thread.
Ideally all SSDs should have a raw access mode, with no wear levelling,
so that one could use ZFS-like CoW filesystems that are SSD-aware and
can do wear levelling in software.  The less the firmware does, the
easier it is to test it and establish trust in it, even for a third
party.  The controllers on the SSDs should also be open to third party

@_date: 2017-11-26 10:57:32
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
When the value is the DEFAULT then the field should be left out -- it's
really like OPTIONAL where absence denotes the DEFAULT value.
I believe the correct answer is (1): nothing.  Though I can see how one
might think (3) is sensible, and also how decoders should accept (2) if
DEFAULT values were allowed to change over time.
EXPLICIT and IMPLICIT exist only because of and for TLV encodings.
TLV -> bad idea.  Don't do it!
(Someone asked me off-list for an explanation of why TLV encodings are
bad.  It's all in this thread already, but maybe a single post collating
those reasons might be useful to some.)
That's right, ANY was removed (and replaced with the information object
system -- horrible name).
Not much of a foot-gun.  One often needs extensibility, and either you
can do things like make the encoding extensible (e.g., see the ASN.1
extensibility marker and how that works in PER) or you can have "typed
holes" -- OCTET STRING wrappers of already-encoded values, along with a
type to identify the value.  Those are your basic choices.  Typed holes
work no matter what the encoding rules, so they're rather universal,
whereas an encoding need not provide any support for extensibility.

@_date: 2017-11-27 11:58:28
@_author: Nico Williams 
@_subject: [Cryptography] WIPEONFORK in Linux 4.14 
C libraries should provide a secure_malloc() (and calloc and realloc)
whose allocation can be freed with free(), and which means "don't write
to unencrypted swap" and "don't allow underprivileged debuggers to see
this".  A "wipe on fork()" variant, or a flags argument by which to
specify desired (critical!) allocation options would be nice.
pthread_atfork() suffices for fork-safety for userland PRNGs, though an
mmap() wipe-on-fork option does seem more likely to be more robust.

@_date: 2017-11-27 12:57:39
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
Oh, good analysis.  That's probably right, though some spec and mailing
list archive deep diving might be necessary to find out for sure.
Well, we do interop, so, "yes".  We have a large variety of
implementations of PKIX, after all.  Some use ASN.1 compilers, and some
use hand-coded codecs.

@_date: 2017-11-27 14:09:40
@_author: Nico Williams 
@_subject: [Cryptography] WIPEONFORK in Linux 4.14 
Right.  One should not, however, call syscall(2) to avoid libc stubs.
It's very dangerous for precisely this sort of reason.
Clever.  You can also just check that (my_saved_pid == getpid()), which
if you have a fast getpid() via a vdso, is cheap.

@_date: 2017-11-27 14:17:22
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
Adding a DEFAULTed field anywhere but past an extensibility marker is
*not* supposed to work for any encoding rules other than TLV rules.  And
for TLV rules what's expected to happen is that older peers will produce
an error when the new field is present because they don't expect it.
That is, this technique of retroactively adding a version field in the
front (if that's what happened here) will NOT work with XDR, PER, OER,
and similar.  It can, of course, work for things like XML, and for JSON
when the structure is represented as an object (but not when it is
represented as an array).
I don't follow this.  ASN.1 and its encoding rules have specifications.
You follow the specs, and it should work.  Or use tools that follow the
specs, and it should work.  It's no different than XDR or any other IDL.
The spec itself used to be proprietary!
Worse than that, the fact that the specs were proprietary meant that all
non-proprietary tools tended to use samizadata-style specs, riddled with
errors (e.g., the venerable, and broken, Layman's Guide to ASN.1).
Of course, that's long ago now, and now we do have open source tools and
public specs.

@_date: 2017-11-27 14:22:37
@_author: Nico Williams 
@_subject: [Cryptography] Is ASN.1 still the thing? 
"not coming back" != "going away".
Multics may not be coming back, but it's mostly gone.  ASN.1 may not be
coming back but it's still around, and will continue to be around for a
long time.  That's because there's a ton of legacy using ASN.1 and not
so much using Multics.
I'm all for a better scheme (you know already what I think of the TLV
xERs).  But it seems like every time someone sets out to make a better
scheme, they repeat mistakes already made before them.  Protocol Buffers
is a DER-like TLV.  Whoever designed Protocol Buffers evidently didn't
take the time to learn from the past.  And so now we have BER/DER _and_
PB to interface with.  Fantastic.

@_date: 2017-11-27 14:35:42
@_author: Nico Williams 
@_subject: [Cryptography] Rubber-hose resistance? 
If you were being extorted hard enough, you'd take a sedative to reduce
your heart rate back to the point where you could fool the device.
That's assuming there were no other way to fool the device (which, being
biometric, there would be).
There is no such thing as rubber-hose resistance.

@_date: 2017-11-27 21:19:28
@_author: Nico Williams 
@_subject: [Cryptography] WIPEONFORK in Linux 4.14 
Even now it's not possible to do certain things.  Setting up TLS
correctly with clone(2) without actually being inside the C library is
impossible.  I can point you at some interesting threads about that.

@_date: 2017-11-28 10:50:25
@_author: Nico Williams 
@_subject: [Cryptography] WIPEONFORK in Linux 4.14 
The case I was involved with involved making fork()/exec() of child
processes faster.  Normally one would use vfork() for this, but one
could imagine something closer to pthread_create() where if the "child"
thread exec()s then it creates a new process without replacing the
parent -- an "async vfork()" of sorts that is made possible by _not_
sharing a stack between the parent and child.
See: and: This *could* be done on Linux using clone(2), but it can't be done if
you'd call (on the child side) *any* C library functions that refer to
errno.  The reason is that you have to setup TLS (thread-local storage)
for the child, but a) the details of how to do this are NOT documented,
b) there are interactions with private internals of the chosen libc, so
you can't make this work unless you implement it _inside_ a C library.
Thus, some non-standard uses of clone(2) via syscall(2) are simply not
workable unless you're making use of them in the C library itself.
So I tend to think that anyone using clone(2) via syscall(2) gets what
they deserve.
But it's true that pthread_atfork() is a user-land mechanism, and
anyways refers to fork() but not esoteric uses of clone(2), and that a
kernel-land fork-safety mechanism would necessarily be more robust in
that you couldn't disable it no matter what you do in user-land.
(Well, I'm stretching things: one could always remap the wipe-on-fork
mappings as not wipe-on-fork, but presumably one wouldn't do that.
There may be a need for some clone(2) calls via syscall(2), but there's
no need to remap wipe-on-fork memory as not.  Wipe-on-fork is reliable.)
This makes wipe-on-fork a reasonable mechanism.  There may be others,
naturally, but we only need one that works reliably and _portably_.

@_date: 2017-11-28 15:02:49
@_author: Nico Williams 
@_subject: [Cryptography] Transparent remote file access 
Kerberos should really be updated to support (EC)DH for all exchanges,
with static service public keys plus forward security via an ephemeral
public key for both, the client and the service.  This would greatly
limit what a KDC operator could do, though they could still impersonate
users and services (just like Certification Authorities in PKI).  Add a
PAKE for initial credential acquisition and recovery from KDC compromise
can be quite simple.
This way Kerberos would be a lot like PKI, really, with short-lived
tickets (as compared to, in a PKI, fresh or non-fresh certificates, and
possibly fresh OCSP Responses, stapled or otherwise).
There's a great duality between Needham-Schroeder and PKI.
Needham-Schroeder, of course, is post-quantom if you use sufficiently
large symmetric keys.  Adding (EC)DH could be done in a way that does
not weaken the PQ nature of Needham-Schroeder (except maybe when
recovering when a KDC compromise).
On the other hand, all hierarchical systems fail the same way that the
web PKI did: politically/commercially you can't have one root.  DNSSEC
is the one exception so far, and perhaps we could use DNSSEC to
bootstrap cross-realm trusts in Kerberos, but this would be done with
PK, so there goes the PQ...
Although, given slow, big-keyed PQ PK, Needham-Schroeder could be a
useful way to amortize the cost of such PQ PK.  I wouldn't count
Kerberos out just yet.
But enough with that digression.
Kerberos does have PKINIT, which you can use with a token.
Q1:  Do you trust the file server(s)?  If so, why?
If you don't, then you should be using them only as object stores, doing
all file content (and directory) crypto in the client, using encryption
to users/groups via PK.  You don't need a particularly smart file server
for this.  But you do end up having to trust the clients a fair bit to
operate correctly, otherwise bad things can happen.
See Lustre for an illustration of the complexities of distributed
filesystems where the clients play a central role in correctness.
Alternatively, if you centralize all the metadata operations then the
server will have a lot more information for traffic analysis, even if
you encrypt actual file data on the clients.
You can always encrypt to public keys...
Preventing privilege escalation is extremely difficult.  You need lots
of isolation, not just in terms of processes, kit, and networks, but at
the organizational level too.

@_date: 2017-11-28 20:58:34
@_author: Nico Williams 
@_subject: [Cryptography] Intel Management Engine pwnd 
It uses a built-in, on-chip NIC.  The ME literally intercepts IP packets
that are protocol TCP with destination ports matching the ones that the
ME listens on.  If you don't connect that NIC to any networks, you'll be

@_date: 2018-12-02 17:35:54
@_author: Nico Williams 
@_subject: [Cryptography] [TLS] ETSI releases standards for enterprise 
Any key escrow system will have this property.  Given the session keys
(or a way to recover them) you can resume decrypted sessions.
If I had to I would build a corporate TLS 1.3 session key escrow system
as follows:
 - use a keyed PRF/PRNG to generate the ephemeral DH keys, with two
   inputs, a secret key shared with the escrow third party, and a number
   generated randomly:
   edh_key = DH_key_gen(seed = PRF+(escrowed_key, r = getrandom()));
 - log to the escrow third party {connection ID, random} for each
   connection (the connection ID can be a handshake transcript hash).
   (it's even safe to log the random number r in the clear, as it alone
   is insufficient for recovering session keys)
This would preserve all the properties of TLS 1.3 and would work for any
other version of TLS with EDH too, and also for any other protocols that
use ephemeral key agreement (SSHv2, IKE, ...).
It's more work to integrate this than to use RSA key transport with
escrowed RSA key orchestration, but it's one-time work (do it for about
six or so open source implementations and you've got 90+% coverage).
I'm sure upstreams would accept this sort of contribution as it is
better for everyone outside corporate environments if we can just stop
the pressure to go back to RSA key transport.  It's also better for
corporate environments, as insiders are the largest threat there, so
forward security is still a plus even in corporate environments.

@_date: 2018-12-04 11:43:14
@_author: Nico Williams 
@_subject: [Cryptography] [TLS] ETSI releases standards for enterprise 
Yes, but then you have to get interoperability using them, which means
patching clients and servers.  You can't 100% escrow this way.

@_date: 2018-02-01 11:08:41
@_author: Nico Williams 
@_subject: [Cryptography] canonicalizing unicode strings. 
That's still heuristics.
Perhaps the best heuristic would be to use an AI image recognizer
trained to look for homoglyph texts.  When it comes to phishing we need
close to 0% false negative rate, otherwise we end up with lots of
victims no matter what.
The point I was making is that mixed scripts are fine for identifiers,
but that one must disallow subsequent new identifiers that look the same
as existing identifiers (unless they are aliases of those).
Heuristics and/or occasional human intervention are needed because some
homoglyphs may need to be tolerated.  E.g., darn vs dam -- OK or not OK
as identifiers for different entities in the same namespace?

@_date: 2018-02-01 16:10:21
@_author: Nico Williams 
@_subject: [Cryptography] canonicalizing unicode strings. 
But usage evolves.  So, for example, in Korea adding an "ing" Latin
suffix is a common thing now.  Suppose that became popular among
Devangari users?  Or suppose they mix digits from Latin in otherwise
Devangari text.
And while I have no idea what "half Devangari and half Arabic would look
like", but I'm guessing that's not such a strange idea given that India
is quite the cultural melting pot.
Yeah, I know.  I assume a lot of script mixing involving two or more
non-Latin scripts are... difficult to enter (except by pasting :).
That's how people enter Kanji, only they type in hiragana instead of
Correct.  Without being able to view what you're entering, password
entry can be very difficult in some scripts (though if the input method
is predictable, maybe you can just use muscle memory...).
None of this means that one should reject mixed script new passwords.
However, users should be warned about difficulty of password entry.
And back to identifiers, again, we (for a value of "we" that roughly
means "IETF" here) shouldn't forbid script mixing for them either.
However, administrators certainly could in specific contexts.

@_date: 2018-02-01 21:22:28
@_author: Nico Williams 
@_subject: [Cryptography] canonicalizing unicode strings. 
You don't have to educate them as to mixed-script passwords.  First,
it's hard enough to enter that most won't even try.  Second, if it
doesn't work out then they'll go through whatever password reset
procedure, and for the most part won't be sad.

@_date: 2018-02-06 14:01:25
@_author: Nico Williams 
@_subject: [Cryptography] canonicalizing unicode strings. 
UTS has a confusables.txt file, which is the closes to what you're
asking for, and the UC's UTS *is* the proper vehicle for updates to

@_date: 2018-02-08 13:42:50
@_author: Nico Williams 
@_subject: [Cryptography] RISC-V branch predicting 
Elimination of side channels is really hard.
Elimination of observability of side channels is harder.
The problem with speculative execution is that abandoned speculated code
paths end up having observable effects in the form of cache line
Sharing less is one answer, but it's a non-trivial answer since we can
already do that by flushing caches on context switches and we know that
sucks, and we know too that adding tagged caches will cost a ton of
Everything that could be done here (other than nothing) has massive
costs that no one wants to incur (surprise).  Doing nothing has its own
less tangible cost: security.

@_date: 2018-01-03 14:54:35
@_author: Nico Williams 
@_subject: [Cryptography] Software patent lifetimes are the problem (Re: 
No, the problem with software patents in (no matter the country) is how
long-lived they are.
The idea of a patent lasting N years is to give the inventor time to
recoup their investment and make a tidy profit while eventually
allowing the public to make use of it without further royalties.
The problem is that an N may be reasonable in one industry while being
entirely unreasonable in another.
17-20 years seems logical enough for pharmaceuticals -- at least it's
But 17-20 years is several times too long for _software_.  Inventions in
software abound, and capitalizing on one can take much much less time
than with pharmaceuticals (not least because there's no FDA for
Heck, the effective under-patent-and-available lifetime of a drug is
much shorter than 17-20 years.
I feel that setting context-/industry-specific patent lifetimes is
probably the easiest path to effective patent law reform locally and
worldwide.  I think a strong case for this can easily be made based on
equity.  This is just intuition, and my intuition may be wrong.
One could also point to a plethora of software patents where the
invention has been ignored by the industry until the patent expired, and
then argue that no value of N makes sense for software.  But I feel that
an argument against software being patentable is less likely to prevail
than one for setting N smaller.
N = 8 seems like plenty good enough to me, though I would prefer a
smaller value, say, 5.

@_date: 2018-01-03 17:31:19
@_author: Nico Williams 
@_subject: [Cryptography] crypto leak to Iranians via Iraqis 
Some lessons:
0) Risen is a moron or disingeneous (or both).
1) Don't cry for Risen.
2) No really, don't cry for Risen!
3) Don't share secrets of this magnitude with people who don't have to
   know.  Chalabi almost certainly didn't have to know!
   (Of course, Chalabi could, if he didn't really know, have
   inadvertently revealed this to the Iranians too.  Even if he didn't
   know, but what he was told implied a break to the Iranians, then
   maybe he shouldn't have been told any such thing to begin with.)
   (Chalabi also could just not have known anything yet told the
   Iranians just to impress them with his claimed level of access.  That
   would still be a compromise, and a betrayal.)
4) If told your crypto is broken, you might want to take advantage of
   this information, and eventually test it, which brings us to:
There are plausible very good reasons for asking a journalist not to
reveal that the enemy knows a secret of this magnitude that go beyond
screwing the journalist or saving oneself embarrassment.  Here's one
reason, for example:
   Just because the enemy knows you broke their crypto doesn't mean that
   they know that you know that they know this.
   If the enemy is using knowledge of the break to feed you
   disinformation, they may also have to forego protection for lots of
   other traffic just to avoid tipping you to their knowledge of your
   break, so there can still be real intelligence to be had.
   Once they know you know they know you broke their crypto they have no
   reason to continue using any longer than it takes to deploy new
   crypto.
And besides, Risen himself lists another good reason: the Iranians might
not have believed what Chalabi told them.  So that's two plausible very
good reasons right there.
Risen should have had the courage to publish, or the else loyalty to
country not to.  Pick one.  Most likely he held back only to avoid
losing access.
Even if the only real reason to ask Risen to not publish was to save the
administration embarrassment, Risen couldn't have known this.

@_date: 2018-01-04 17:08:19
@_author: Nico Williams 
@_subject: [Cryptography] ROP gadgets => OOO gadgets == larger attack 
It's almost like we now have to make everything constant-time.  Our
world just got much more hostile.
And/or slow things way down.  Sharing less is probably the better
answer, except that it's not very practical, especially for consumer
Perhaps we can move all computation into a cloud and have slow and dumb
devices as thin clients.  That would be ironic here given that cloud
services are particularly impacted by these vulnerabilities.  But in any
case, I don't think we'll end up there.

@_date: 2018-01-04 19:06:16
@_author: Nico Williams 
@_subject: [Cryptography] Speculation re Intel HW cockup; 
I don't see how.  Perhaps jamesd was... speculating.
Speculative execution is necessarily side-effect-having by its potential
cache thrashing impact and ability to be impacted by cache thrashing.
This necessarily creates side-channels.  It seems unavoidable except by
having sandboxed caches, but that's probably not an option for many
reasons (including power consumption).
Facing pressure on the cache front I think CPU designers might
reconsider the UltraSPARC T-n approach of adding more execution units to
compensate for slow memory.  UltraSPARC was a last gasp of a dying
architecture, but the idea behind it might actually work better now.  Of
course, this approach is predicated on software being able to take
advantage of it, but for cloud kit it makes a lot of sense.
I think we'll see a few trends from this:
 - more programmer control over speculation
   (hardly a panacea, but better than nothing)
 - less speculation / slower systems
 - a push for faster RAM?
   (are there technologies on deck that could deliver fast RAM?)
 - more HW threads to compensate for smaller caches, less sharing,
   slower HW threads
   (was the UltraSPAC T series on the right track?)
 - for smartphones a less cache sharing but also less concurrency and/or
   slower app switching
But I'm just speculatin' here!

@_date: 2018-01-05 10:50:38
@_author: Nico Williams 
@_subject: [Cryptography] Speculation re Intel HW cockup; 
Fair enough, though this is basically "slow things down".
Also, speculation would have to be limited to accessing cache lines in
L1, since accessing cache lines in L2/L3 would evict a cache line from
L1, which in turn would be visible via timing.

@_date: 2018-01-05 10:53:51
@_author: Nico Williams 
@_subject: [Cryptography] Speculation considered harmful? 
It's very good for many cloud workloads that are inherently horizontally
scalable.  It's very bad for workloads that need high sequential

@_date: 2018-01-07 17:38:24
@_author: Nico Williams 
@_subject: [Cryptography] Speculation considered harmful? 
But it would then have to reload whatever they had contained before.
Eviction is still a side-effect.

@_date: 2018-01-08 22:58:22
@_author: Nico Williams 
@_subject: [Cryptography] Speculation considered harmful? 
Provided it doesn't speculate behind the compiler's back, you could just
disable speculation by having the compiler emit slower, more sequential
code.  That's the real idea of VLIW: let the compiler do more of the
work.  That was also the problem with VLIW: it's difficult to make the
compiler do that work.  But maybe LLVM and friends have become advanced
enough that it could work now, and maybe bitcode could be the new object
code so we could have non-stable VLIW ABIs.  In any case, it seems a bit
late for a second look at Itanium.

@_date: 2018-01-10 10:02:50
@_author: Nico Williams 
@_subject: [Cryptography] Speculation considered harmful? 
VLIW == no stable ABI.  We'd have to ship bitcode and re-optimize,
assemble, and link for every VLIW CPU model.
It wouldn't be the end of the world.  But first we'd need the compilers
that could handle it.

@_date: 2018-01-10 10:10:38
@_author: Nico Williams 
@_subject: [Cryptography] Speculation considered harmful? 
That's hardly changed.  And it's not just Unix.  It's all legacy.
Microsoft has had the same problem with Windows.  The Linux emulation
functionality in BSDs and Illumos has run into the same issue as well.
Your OS may be superior to the one you're emulating, but now you have to
emulate the very bugs your OS does not have by dint of being designed
        (epoll, I'm looking at you, you awful ode to NIH)
Legacy represents enormous future costs, but it's also a measure of past
success.  Not legacy, no success.  It can't even be avoided.

@_date: 2018-01-10 14:59:17
@_author: Nico Williams 
@_subject: [Cryptography] Speculation considered harmful? 
We accept JIT in lots of contexts, so why not just always?  I'm not
saying that *would* work.  I'm saying this concern might not be so
Before Meltdown/Spectre I'd have said (and did say whenever the topic
came up) that VLIW was dead.  Now there's a new opportunity for VLIW
research.  *Research*.  I'd not expect a revolution in CPU architecture.

@_date: 2018-01-11 13:20:27
@_author: Nico Williams 
@_subject: [Cryptography] Spectre -- would an L0 for speculation-only help? 
Suppose speculative execution never evicted cache lines in any cache,
except a special, _small_ (say, 8 cache lines) cache only used during
speculation.  Call this cache L0.
When a speculated thread is committed then all the cache lines in L0
loaded during speculation are moved to L1, resulting in evictions only
at commit time.
That is, speculative execution would have an L0 in its cache hierarchy,
while non-speculative execution would not.
L0/L1 would not be inclusive; L0 would never be loaded from L1.
L2/L3 misses might have to stop speculative execution if the cache
hierarchy is inclusive, but not otherwise.  I suspect that in order to
perform well L1 misses would have to not stop speculation in any case.
L0 would have to be teeny tiny -- it cannot cost too much die area.  But
it wouldn't have to be very large at all to have the desired effect of
allowing performant speculative execution with no side-effects on L1 for
abandoned speculation.
Is this crazy?  Workable?  If so, would there still be timing side-
channel attacks on speculative execution left unadressed?  Perhaps there
might be timing leaks via cache coherency effects?

@_date: 2018-01-12 12:10:05
@_author: Nico Williams 
@_subject: [Cryptography] Spectre -- would an L0 for speculation-only help? 
It wouldn't handle a speculation tree, indeed.
More than that, someone pointed out off-list that a CPU does not commit
to a thread of speculation, but to individual branches.  That's not
fatal, but it would require tagging L0 cache entries with the IP of the
instruction that first loaded them, and perhaps additional state to deal
with subsequent stores to the same line.  And it would require flushing
L0 at the start of speculation (i.e., often), though I was assuming that
would be necessary anyways.

@_date: 2018-01-29 11:26:57
@_author: Nico Williams 
@_subject: [Cryptography] I'll give the right answers to the right 
OK, sure, FP killed your on early Niagara.  And your workloads weren't
as embarrassingly parallel as the number of threads in your application
might have implied.
Another thing that is terrible on SPARC in general is register window
spills.  Niagara has a single window... so every function call takes a
spill, which is almost certainly less optimal that caller- or callee-
saved register protocols.  Turns out that aspect of SPARC sucked.
But the question is: can we build a better world on massive hardware
threading with less speculation?
I don't think Niagara provides a final answer to that question.
Ultimately it's all about whether we can more easily turn serial code
into parallel code, and that so far has been very difficult.
A hybrid CPU with a few fast threads along the lines of current x86_64
CPUs, and a bunch of slower CMT-style threads, might actually help the
market move towards CMT.

@_date: 2018-01-30 18:06:26
@_author: Nico Williams 
@_subject: [Cryptography] canonicalizing unicode strings. 
Well, a small portion of typesetting (there's no way to format text, no
way to control positioning on a page, etc.).
It was never going to be any other way.  This isn't a result of design
choices that went into Unicode.  It's a result of human scripts being...
as they are: organically evolved, and plentiful.  Perhaps some design
choices could have been made that reduced some of these homoglyph cases,
but there would have been annoying trade-offs anyways.
We had homoglyph problems before computers ('l' vs '1', for example).
They got worse not because of Unicode, but because the world is more
connected now.  Yes, we could, eg, have had a unified CJK codepoint
assignment set, but it turns out people didn't want that.
Basically, we just have to accept these issues and deal with them as
best we can: with code to heuristically detect phishing based on
homoglyphs, and code to fuzzily match Unicode identifiers.  Such code
has to evolve as scripts are added to Unicode and/or new homoglyph sets
are discovered (if we don't already know all of them).
No alternative to Unicode that somehow supports multiple scripts could
have been a very good choice for expressing identifiers.  And since
Unicode exists and is so widely used now, even if there was such an
alternative, we'd have to convert to/from that alternative and thus we'd
only just barely have moved the boundary at which these problems arise.
There's no point saying that this "makes Unicode a lousy base to use for
identifiers and passwords".
And for passwords homoglyphs are mostly a non-issue.  The primary issue
for passwords is the user's ability to enter them correctly on all their
devices.  And, of course, normalization is kinda required, since the
user generally has no control over pre-composition choices of the input
I'm not sure it's worthwhile to do this, and I think I'd likely object.
In gaming and social media (e.g., reddit) people like to use identifiers
that one would never see used in, e.g., corporate networks.  I've
objected before to SASL proscribing a variety of characters that people
like to use in gaming:   why should we say no to using, say, '*', in
gaming usernames[0]?  But homoglyph usernames are a problem, if nothing
else for phishing / impersonation reasons.
Algorithms for detection of homoglyph identifiers that match existing
ones is a more urgent need.
[0] Or ':'.  Yes, ':' is very very bad for Unix user/group names, but
    for SASL cids, who cares?

@_date: 2018-01-30 22:24:16
@_author: Nico Williams 
@_subject: [Cryptography] canonicalizing unicode strings. 
My sense (I wasn't there and I haven't researched what happened) is that
precomposition exists to make transcoding with ISO-8859 simpler, and to
make rendering easier for Hangul, though it might well also be useful
for other scripts too but that's beyond the limit of my knowledge of
Decomposition is clearly the better approach!
But even with only decomposed representations we'd have a normalization
problem for characters that involve more than three codepoints.
So I don't even resent normalization.  I've accepted it.  It's just a
part of life.  Ditto homoglyphs.  Processing text is hard; we're not in
ASCII-land anymore.
Bingo.  There's rules for IDNA, and then there are rules that registrars
should apply, which is not the same thing.  The DNS itself needs to
allow homoglyphs if for no other reason than that servers can't be
expected to do anything about them and neither would clients (though
user agents are another story); but registrars need to look out for
their customers.
I... rather like punycode, oddly.  It would have been good to re-use it
for email mailbox I18N.

@_date: 2018-01-31 10:49:21
@_author: Nico Williams 
@_subject: [Cryptography] canonicalizing unicode strings. 
That's why I did not propose that :)
And even if they use just one script.  Remember that 'l' and '1' look
similar in many fonts, and those are just in the plain old ASCII range.
Mixing Latin ligatures with Latin non-ligatures is not mixing scripts
Diacritic marks can make confusable characters too.
For string hashing, in particular, but for string hashing one admits
collisions, so one can choose to be very liberal in "canonicalizing"
characters for string hashing.  The biggest problem for string hashing
is that the hashing algorithm needs to be stable for persistent storage
(but can't be), so storage formats need to be able to version string
There is, actually.  See UTS  I expect the confusables.txt file to
grow over time, not least because Unicode is not closed to new scripts.

@_date: 2018-03-08 17:25:00
@_author: Nico Williams 
@_subject: [Cryptography] Mutually authenticated TLS 
In this case there's no problem because the necessary authorization data
is small, being just an OU in the DN.
Active Directory does this for Kerberos, but not PKI.  With PKI the
"PAC" is downloaded as needed, whereas with Kerberos it is included in
the ticket.
Does it work?  Yes.  Are there issues?  Definitely:
a) You won't have new grants until you get a new TGT and new service
   tickets -- whatever.  Windows does this periodically anyways.
   This will happen with the other approach too, since you want to cache
   the PAC anyways (so you don't refetch it too often; in some contexts
   that could happen way too frequently as you'll see below), but you're
   likely to have a much shorter TTL for fetched PACs than for Kerberos
   tickets.
b) Ticket bloat due to PACs is a serious issue.
   You see HTTP/Negotiate with huge headers for users who have many
   Windows group memberships.  This sometimes necessitates increasing
   header size limits on servers (ugh).
   Non-cookie-using apps will send an AP-REQ on *every* HTTP request, so
   this is tremendously wasteful.  Horrible.
On the other hand, having to fetch a PAC on the server side makes
authentication blocking when you need to fetch a new PAC.  This is not
exactly desirable...  It really is very nice that accepting Kerberos
authentication does not block...
One really does need to address (b).  The obvious thing to do is to use
cookies...  And, of course, TLS (which was needed anyways).  And
preferably also channel binding one way or another.  (Another thing to
do would be to have the acceptor return a new ticket to be used in
subsequent authentications, but this would require standards work.)
Yes.  Especially if certificates are fresh.
Speaking of fresh certs, I think we should just converge on fresh certs
and to hell with OCSP and CRLs.  This necessitates an online CA to
re-issue certs often, but so what.

@_date: 2018-03-09 18:18:44
@_author: Nico Williams 
@_subject: [Cryptography] Mutually authenticated TLS 
I mean, I like the idea of OCSP stapling, but fresh certs works without
having to change anything other than provisioning.  For servers that's a
fairly simple thing.  No, wait, it's not -- *nothing* is ever simple for
PKI applications, is it.  Way too many TLS implementations require
either restarting services to update certs or writing special
application code to support switching to new certs.  But still, that's
only boiling a lake, while adding OCSP stapling everywhere is more like
boiling an ocean.

@_date: 2018-03-09 18:37:12
@_author: Nico Williams 
@_subject: [Cryptography] On those spoofed domain names... 
We should stop, yes.
That's not what happened.
What happened is that human scripts and human politics are not simple,
and precluding all homoglyphs was a) never part of the UC's mission, b)
never plausibly and politically going to be part of the UC's mission.
Yes, CJK unification was a thing, but only for CJK, and it failed
We were always going to have a confusability problem anyways because of
typos and font confusability issues.  The problem isn't that the UC
didn't prevent confusability (it couldn't have).  It's that the
community didn't recognize the problem and write code and standards for
registries/registrars that would make it easier to cope with the
There's no need to cry over this.  Instead we need to demand that
registrars prevent registration of domains that are typo-, font-, and/or
homoglyph-confusable.  We also need to write code that does fuzzy
confusable matching.
This may be true, but it's also almost certainly true that they couldn't
wait to learn twenty years' worth of lessons in order to design a
bug-free Unicode that can encode hundreds of scripts, nor could those
lessons have been learned without actually shipping standards and code
during those decades.  A standard like Unicode cannot be born complete,
cannot ever be complete, and can only grow organically.
The alternative to Unicode is what we had before: a mess of smaller,
legacy character sets that could only encode one or two scripts, codeset
conversions galore, and forever-unhappy users.
Correct: because human scripts are *still evolving*!  How could it be
Unicode includes characters needed to encode ancient texts.  What's
wrong with that?
Dead languages and scripts are not entirely dead.  The need to be able
to express those in Unicode is real.
The UC's mission is not first and foremost to make the DNS better.
Never was.  Never could have been.

@_date: 2018-03-19 10:54:59
@_author: Nico Williams 
@_subject: [Cryptography] Typesetting vs. identifiers, 
I don't think we need subsets.  We just need standards (and code) to
detect confusable identifiers, then we can have first-come-first-served
policies with anti-confusable-squatting policies.
Even without Unicode confusables issues we all know about typo-
squatting.  This is a difficult issue even without Unicode in the
picture.  Nothing here is Unicode's fault, not really.
A list of sets of homoglyphs is feasible, but it will take time to come
up with something remotely complete.  It will also cost money.  And it
will have to be an ongoing process because Unicode is not closed to new
glyphs (it can't be, as it's not complete and never could be).

@_date: 2018-03-19 11:00:41
@_author: Nico Williams 
@_subject: [Cryptography] On those spoofed domain names... 
To some degree each community has to have its own standards.  The UC is
central here in that they have the expertise to produce a good first
approximation of sets of confusable glyphs.
As you and others and I point out here and elsewhere, confusable glyphs
are hardly the whole story.
Note that there's no Unicode in sight there.  One is a typo domain.

@_date: 2018-03-19 11:03:20
@_author: Nico Williams 
@_subject: [Cryptography] On those spoofed domain names... 
There are limits to how far you can go with this.  You can detect
confusable glyphs this way.  But you also need to detect typo-squatting
and even just plain old confusable words.

@_date: 2018-03-20 11:01:15
@_author: Nico Williams 
@_subject: [Cryptography] Typesetting vs. identifiers, 
Yes, but.  Either fluent CJK users have less trouble confusing CJK
glyphs, or they have as much trouble as non-CJK users might expect but
the rest of us non-CJK users are unaffected by that anyways :)
I suspect that for non-ideographic/hieroglyphic scripts the
confusability problem is closer to that which we find in {Latin, Greek,
Cyrillic}, and that it's actually feasible to identify sets of
confusable glyphs among such scripts.
If someone attempts to phish me with CJK confusables, I'll recognize...
that I can't read them and move on.  If someone attempts to phish me
with Cyrillic confusables, I might well be at a font's mercy.
I don't think one needs to look for confusables in {Latin, CJK} --
there won't be very any/many.  But it's much more likely that looking
for {Latin, Greek, Cyrillic} confusables in registrations would help
speakers of European languages.  Forbidding mixing of scripts in one
label is not likely to be feasible (e.g., in South Korea it's becoming
common to add "ing" to Hangul words).
I don't think there is zero value in collating a set of sets of
confusable [non-CJK] glyphs.
There may not be *enough* value in that: there's still typo-squatting
and other such attacks to worry about, so it feels like a losing battle.
But it's not yet clear to _me_ that there is insufficient value in
identifying sets of confusable glyphs.
Perhaps that is clear to _you_, but you may need to make an argument
that doesn't involve CJK to convince me :/
This is also why different registries should be able to have different
Of course, but an attacker won't make you type it.  They'll hope you

@_date: 2018-03-24 00:37:22
@_author: Nico Williams 
@_subject: [Cryptography] Does RISC V solve Spectre ? 
It definitely is more complex: for the compiler writers.  They now have
to repeat this exercise for every release of a CPU.  And software needs
to be recompiled for each CPU release, or else JITed -- which means it's
more complex for others too.
Also, it's not as trivial as you might think because the time to perform
a load is not predictable because of the various levels of cache, the
time to store isn't either because of cache coherency issues, and the
time to perform memory barriers and atomic memory operations isn't
predictable for the same reasons.  Meaning: a compiler can't quite
schedule things correctly.  Instead a compiler might have to generate
micro event loops where hopefully work can be found to do while waiting.
Ultimately a compiler might even generate code that speculates, leaving
you roughly at square one (though trading the horrible CPU update
considerations for software update considerations)!
This road is just no fun to travel, and that's probably why we haven't
gone that far on it.

@_date: 2018-03-25 22:22:21
@_author: Nico Williams 
@_subject: [Cryptography] Justice Dept. Revives Push to Mandate a Way to 
There are none.  At the end of the day the people with the guns win.
There is no crypto that beats rubber hose cryptanalysis.  This extends
to all technology.

@_date: 2018-03-26 23:09:14
@_author: Nico Williams 
@_subject: [Cryptography] Justice Dept. Revives Push to Mandate a Way to 
It does not.  If you're found to have steganographic software then it's
a reasonable inference that you are *using* it, and it's all downhill
from there.
This debate comes up periodically on this list.  Let's not retread.
Law enforcement types don't care that you think you're being clever.
Neither do judges.  Neither do legislators.  These are political
problems, not technical.  There are no technical solutions to political
problems -- the state is armed with more guns and law enforcement and
soldiers than you could ever hope to counter in anything other than the
most widespread popular revolt.
Everyone in this field ought to know all of this by now.
If you object to the CLOUD Act or anything like it, you have to
participate in the political process.  Perhaps, if you're a brilliant
lawyer or have a brilliant lawyer, and if the Supreme Court is amenable,
and if you have time and money to spend on it, you might prevail in
court, but I doubt this.  The same goes for just about every western
country (though often it's worse elsewhere than the U.S. for lack of a
Bill of Rights).
That is exceedingly hard to do, if not impossible other than by
"dropping off the grid".  Everything we do leaks enormous amounts of
metadata.  Metadata is all law enforcement really needs (if only they
understood this).  All of your messaging and web browsing, really, all
of your online activity, and most of your off-line activity (through
electronic payments) -- all of this leaks all the metadata needed to
know where to apply the rubber hose.  Unless you're willing to dispense
with all the amenities of modern life (few are), you leak metadata.
