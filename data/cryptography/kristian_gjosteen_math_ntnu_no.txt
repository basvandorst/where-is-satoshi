
@_date: 2013-11-26 21:36:21
@_author: =?iso-8859-1?Q?Kristian_Gj=F8steen?= 
@_subject: [Cryptography] Explaining PK to grandma 
26. nov. 2013 kl. 17:46 skrev Jon Callas :
I like seal. The seal is a thing clearly separate from you, just as the signing key is not inside your head.
Cryptographic signatures can be considered impossible-to-forge seals.
This makes it meaningful to talk about someone stealing, copying or misusing the seal (the signing key). You can even reason about placing your seal into a trusted locker in your local bank (your bank stores your signing key for you) or giving it to your secretary along with the authority to use the seal whenever you tell him to (inserting your smart card into your computer). Now you can explain all kinds of interesting trust situations in an intuitive way.
I like seal.

@_date: 2013-10-01 09:49:31
@_author: =?iso-8859-1?Q?Kristian_Gj=F8steen?= 
@_subject: [Cryptography] RSA equivalent key length/strength 
1. okt. 2013 kl. 02:00 skrev "James A. Donald" :
Checking the verification code may be a good idea.
I just checked that the verification process described in Appendix 5 in the document RECOMMENDED ELLIPTIC CURVES FOR FEDERAL GOVERNMENT USE, July 1999 ( accepts the NIST prime field curves listed in that document. Trivial python script follows.
I am certainly not the first non-US non-government person to check.
There is solid evidence that the US goverment does bad things. This isn't it.

@_date: 2013-10-02 18:18:31
@_author: =?iso-8859-1?Q?Kristian_Gj=F8steen?= 
@_subject: [Cryptography] RSA equivalent key length/strength 
2. okt. 2013 kl. 16:59 skrev John Kelsey :
Edlyn Teske [1] describes a way in which you select one curve and then find a second curve together with an isogeny (essentially a group homomorphism) to the first curve. The first curve is susceptible to Weil descent attacks, making it feasible to compute d.log.s on the curve. The other curve is not susceptible to Weil descent attacks.
You publish the latter curve, and keep the first curve and a description of the isogeny suitable for computation to yourself. When you want to compute a d.log. on the public curve, you use the isogeny to move it to your secret curve and then use Weil descent to find the d.log.
I suppose you could generate lots of such pairs of curves, and at the same time generate lots of curves from seeds. After a large number of generations, you find a collision. You now have your trapdoor curve. However, the amount of work should be about the square root of the field size.
Do we have something here?
(a) Weil descent (mostly) works over curves over composite-degree extension fields.
(b) Cryptographers worried about curves over (composite-degree) extension fields long before Weil descent attacks were discovered. (Some people like them because they speed things up slightly.)
(c) NIST's extension fields all have prime degree, which isn't optimal for Weil descent.
(d) NIST's fields are all too big, if we assume that NSA couldn't do 2^112 computations in 1999.
(e) This doesn't work for prime fields.
It seems that if there is a trapdoor built into NIST's (extension field) curves, NSA in 1999 was way ahead of where the open community is today in theory, and had computing power that we generally don't think they have today.
We have evidence of NSA doing bad things. This seems unlikely to be it.
[1] Edlyn Teske: An Elliptic Curve Trapdoor System. J. Cryptology 19(1): 115-133 (2006)

@_date: 2013-09-06 09:03:27
@_author: =?windows-1252?Q?Kristian_Gj=F8steen?= 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
5. sep. 2013 kl. 23:14 skrev Tim Dierks :
As a co-author of an analysis of Dual-EC-DRBG that did not emphasize this problem (we only stated that Q had to be chosen at random, Ferguson &co were right to emphasize this point), I would like to ask:
I mean, who on earth would be daft enough to use the slowest possible DRBG? If this is the best NSA can do, they are over-hyped.
(If you really do want to use Dual-EC-DRBG: truncate more than 16 bits, and don't use NSA's points, choose your own - at random.)

@_date: 2013-09-09 12:50:29
@_author: =?iso-8859-1?Q?Kristian_Gj=F8steen?= 
@_subject: [Cryptography] [cryptography] SSH uses secp256/384r1 which has 
9. sep. 2013 kl. 10:45 skrev Eugen Leitl :
The curves are standard NIST curves. They were the curves you used until about now. That they are the same everywhere is no surprise.
The "problem" with Dual-EC-DRBG was that a point that should have been generated verifiably at random was not generated verifiably at random. There's no reason to believe it wasn't, but it was a stupid mistake that should not have been made, and that has now been blown out of all proportion. Users, if there are any, should generate their own points verifiably at random.
If you reuse one or more points from Dual-EC-DRBG as generators in other standards, it is of no matter. Even if the points are carefully chosen, they cannot compromise those other standards. (DLOG is essentially independent of the generator.)
There's no reason to be paranoid, just because the NSA is out to get you.

@_date: 2013-09-25 14:29:18
@_author: =?iso-8859-1?Q?Kristian_Gj=F8steen?= 
@_subject: [Cryptography] RSA recommends against use of its own products. 
24. sep. 2013 kl. 18:01 skrev Jerry Leichter :
Choosing Dual-EC-DRBG has been a mistake for its entire lifetime, because it is so slow.
While some reasonable people seem to have a preference for cryptography based on number theory, I've never met anyone who would actually use Dual-EC-DRBG. (Blum-Blum-Shub-fanatics show up all the time, but they are all nutcases.)
I claim that RSA was either malicious, easily fooled or incompetent to use the generator. I will not buy anything from RSA in the future. Were I using RSA products or services, I would find replacements.
(For what it's worth, I discounted the press reports about a trapdoor in Dual-EC-DRBG because I didn't think anyone would be daft enough to use it. I was wrong.)

@_date: 2014-08-01 07:05:41
@_author: =?Windows-1252?Q?Kristian_Gj=F8steen?= 
@_subject: [Cryptography] [cryptography] Browser JS (client side) crypto 
29. juli 2014 kl. 01:00 skrev Jeffrey Goldberg :
Cryptography in JavaScript is obviously useful, but unnecessarily hard to do right.
There are applications that must be able to do proper crypto and that you want to run in the browser.
One example is remote voting, where fancy crypto must happen on the client. You can?t require people to install purpose-built applications, so you write the encryption code in JavaScript and deploy it via a web site. In principle, you can also distribute dedicated voting applications, or a browser extension that ignores the election web site?s crypto code and uses a local copy instead. This would allow users who care to increase their security.
There are applications that could use the equivalent of crypto in JavaScript. OpenID-like schemes based on passwords could benefit from client-side cryptography. Again, dedicated applications or browser extensions could provide increased security for users who care.
In a perfect world, browsers would expose a standard, useful API of cryptographic and mathematical primitives, which would make it much easier to deploy JavaScript cryptography. But that?s unlikely to happen. So we?ll continue to see people messing up random number generation etc.
People will obviously continue to apply JavaScript cryptography where it doesn't make sense.

@_date: 2014-08-02 18:52:45
@_author: =?windows-1252?Q?Kristian_Gj=F8steen?= 
@_subject: [Cryptography] [cryptography] Browser JS (client side) crypto 
1. aug. 2014 kl. 19:16 skrev Tony Arcieri :
No. The WebCrypto API is sufficient for some applications, but not for others (such as the voting application I mentioned.) Maybe you can twiddle with the DH interface to get exponentiation, but that?s hardly sufficient to do non-trivial cryptography.

@_date: 2014-08-27 12:22:10
@_author: =?iso-8859-1?Q?Kristian_Gj=F8steen?= 
@_subject: [Cryptography] Encryption opinion 
27. aug. 2014 kl. 03:29 skrev ianG :
This is the wrong way to think about such cryptographic protocols. Alice is the end-point, the bank is the other end-point, and the computer is between the end-points. With this point of view, internet banking (and many other applications) is easy to apply cryptograpic protocol analysis tools to. Also, with this point of view, a compromised computer is little different from a compromised network, so it is a MITM attack.
That said, I would say that MITM and MITB are informal terms used to describe classes of attacks. Quarrelling about what is and is not a MITM is sometimes fun and almost never productive.

@_date: 2014-01-23 09:55:22
@_author: =?iso-8859-1?Q?Kristian_Gj=F8steen?= 
@_subject: [Cryptography] Does PGP use sign-then-encrypt or 
23. jan. 2014 kl. 00:05 skrev Alexandre Anzala-Yamajako :
As usual, this is a well-studied problem. You need only include the sender and recipient identities together with the message, and then EtS and StE are both secure.
Obviously they have different properties: EtS ciphertexts reveal the sender (which may be both desirable or undesirable or both), while StE ciphertexts do not (probably not sufficient on its own).
It is at best an inefficient solution. (I have not verified that it is a solution.)

@_date: 2014-10-24 23:19:37
@_author: =?utf-8?Q?Kristian_Gj=C3=B8steen?= 
@_subject: [Cryptography] Simon, Speck and ISO 
24. okt. 2014 kl. 13.53 skrev Fedor Brunner :
I looked at these papers for two minutes, and as far as I can tell, they report attacks on reduced-round variants. Which is what you would expect.
What did I miss?

@_date: 2014-09-05 14:53:14
@_author: =?windows-1252?Q?Kristian_Gj=F8steen?= 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
5. sep. 2014 kl. 03:23 skrev Jonathan Katz :
No. As long as it?s hard to factor a 2000-bit RSA composite using feasible resources for a decade, lot?s of nice cryptography is possible.

@_date: 2014-09-15 21:55:25
@_author: =?iso-8859-1?Q?Kristian_Gj=F8steen?= 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
15. sep. 2014 kl. 12:17 skrev Miroslav Kratochvil :
I skimmed the paper. My impression is that you are over-selling it.
Based on existing work on decoding random linear codes, the best known algorithm for decoding a word of weight 32 encoded using a random code of length 8192 requires ~ 2^150 work.
We assume that regular words are as hard to decode as random words.
We assume that quasi-cyclic codes are as hard to decode as random codes, for regular words.
Then this paper has a theorem saying that we can rule out any distinguisher using time at most 2^63 with advantage 2^-10 that uses 2^30 bits of key stream.
Compare with AES-128 in counter mode. Using only 2^23 blocks, we can ignore collision attacks, and the best known attacks with advantage 2^-10 require work 2^118 or thereabouts. I would guess that AES has seen rather more analysis than the various assumptions that this scheme is based on.
The end result: If you want assurance, go with AES in counter mode, rather than this scheme.

@_date: 2015-01-17 11:03:42
@_author: =?utf-8?Q?Kristian_Gj=C3=B8steen?= 
@_subject: [Cryptography] coding for compression or secrecy or both or 
16. jan. 2015 kl. 20.02 skrev John Denker :
Obviously there is a reason why this is so.
Note that the modern standard of security is equivalent to what is called real-or-random: the adversary cannot tell if we encrypted a message of his choice or just a random bit string of the same length.
We may assume there?s not much ciphertext expansion going on. (I can?t think of a single system with significant ciphertext expansion that I?d call state-of-the-art.)
Obviously, almost all random strings are incompressible, so almost all encryptions of random strings will also be essentially incompressible.
If we have a system where we can usefully compress many or all of the ciphertexts we produce, there is an efficient algorithm that outputs messages that when encrypted gives ciphertexts that tend to compress.
We now have an adversary against the system. Run the algorithm to produce a message. Ask for an encryption of that message or a random string. Try to compress the resulting ciphertext. If it compresses, it?s an encryption of our message. If it does not compress, it?s an encryption of a random string.
Note that, per Katz? message from a few days ago, there are situations where this argument does not hold. One very stupid example. Suppose I only encrypt blocks of 0s or 1s. (In this setting, ECB mode is catastrophically insecure, but CBC mode works well.) I want to compress CBC-mode encryptions.
Recall the CBC mode encryption operation. To encrypt blocks m1, m2, ?, mn, choose a random iv c0, then compute c1, c2, ?, cn as ci = E(mi + c(i-1)). To decrypt, we compute
(*)	mi = D(ci) - c(i-1).
Now observe that we don?t need all the bits of c(i-1) to recover our mi, we only need the first bit, since mi is either all zeros or all ones. So from ci and one bit of c(i-1), we can recover mi.
The final trick is to observe that once we have mi and ci, we can recover all of c(i-1) by rewriting (*) to
(**)	c(i-1) = D(ci) - mi.
So we can decrypt the message given cn and one bit from each of c0, c1, ?, c(n-1). (Note that we decrypt back-to-front.) Ciphertexts compress nicely.
The constructions Katz referred to are much more sophisticated and quite clever, but I think they are essentially just as useless.
Why does the above incompressibility argument fail? The argument implicitly assumes that compressed ciphertexts can be correctly decrypted. After applying the above compression to an encryption of a random message, we will get a decryption failure.

@_date: 2015-06-02 22:24:42
@_author: =?utf-8?Q?Kristian_Gj=C3=B8steen?= 
@_subject: [Cryptography] Why is ECC secure? 
2. jun. 2015 kl. 07.35 skrev Bill Cox :
Itâs dumb.
Your mistake is kind of like wondering how DLP in finite fields can be hard when logarithms are easy to compute for real numbers.
Your second mistake is to expect someone to explain all this to you. You need to read a book. You need to make an effort. Or accept that others have made the effort and trust them.

@_date: 2015-05-30 11:34:17
@_author: =?utf-8?Q?Kristian_Gj=C3=B8steen?= 
@_subject: [Cryptography] Why is ECC secure? 
29. mai 2015 kl. 21.26 skrev Bill Cox :
There’s no proof of security, but those who pay attention to these things know that since 1985, there has been a significant amount of work on ECDLP and related problems. For some special cases, there has been significant progress (typically reducing the problem to a DLOG problem in some other group where better algorithms exist), but for non-special curves over prime fields, there has been essentially zero progress. Compare this to factoring and DLP where there has been significant progress since 1977.
Note also that «closeness» isn’t very relevant for ECDLP. For instance, you can «smoothly» deform an elliptic curve into a singular cubic curve where DLOG is trivial.

@_date: 2015-11-09 17:02:36
@_author: =?utf-8?B?S3Jpc3RpYW4gR2rDuHN0ZWVu?= 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
8. nov. 2015 kl. 18.43 skrev KrisztiÃ¡n PintÃ©r :
That is not a sound argument.
That expert cryptographers use constructions other than EtA is not surprising. These constructions typically arenât straight E&A, obviously.
EtA is a simple construction that if used more widely would have saved us all some bother. It is obviously better than E&A, and it is safer than AtE (fewer ways to mess things up). Â«Use EtAÂ» is still sound advice, except it should probably be modified to be Â«Use AEAD if you can, otherwise EtAÂ».

@_date: 2015-11-11 20:10:03
@_author: =?utf-8?B?S3Jpc3RpYW4gR2rDuHN0ZWVu?= 
@_subject: [Cryptography] where is the weakness?  related-key, 
11. nov. 2015 kl. 19.13 skrev John Denker :
That is true, but you donât understand the context in which the EtA advice originated.
At the time, we had a bunch of block cipher modes and stream ciphers that were good at providing security against chosen plaintext attacks. And we had a couple of MACs that were good at providing integrity.
Also, we realized that what we wanted was chosen ciphertext security.
It was reasonably clear that combining an IND-CPA scheme and a MAC was a reasonable approach. What was not (and obviously still isnât) clear was exactly how to combine them. Then we got a theorem saying EtA is always good, AtE is not always good and E&A is not good.
Which means that, unless you are prepared to think seriously about the security of your scheme, EtA is the way to go.
Later, we got AEAD and lots of other nice stuff, but still people mess up, people donât understand and people complain a lot.
(There is still some minor difference of opinion among experts, but that difference isnât whatâs reflected on this list. Also, the story isnât quite as simple as the above suggests. Also, when I say Â«weÂ» above, I should mention that I wasnât a cryptographer back then, so that Â«weÂ» doesnât include me, strictly speaking.)

@_date: 2015-11-12 11:32:13
@_author: =?utf-8?B?S3Jpc3RpYW4gR2rDuHN0ZWVu?= 
@_subject: [Cryptography] Post Quantum Crypto 
12. nov. 2015 kl. 09.45 skrev Philipp GÃ¼hring :
You should probably have noticed that D-Wave is building a machine that isnât suitable for running Shorâs algorithm.
As far as I know, it is an open problem if the D-Wave machine is suitable for anything. See Scott Aaronsonâs blog.
That is not to say that quantum computers wonât be a problem, but your premise is faulty, so your reasoning does not apply.
As for the question: How do we find a key exchange algorithm with the same properties as DH?
In general, you can turn any PKE with a reasonable message space into a two-move DH-like key exchange algorithm:
Alice generates a ephemeral key pair (ek, dk) and sends the public key ek to Bob.
Bob encrypts a random message x1 using Aliceâs key ek to get c. Bob sends c to Alice.
Alice decrypts c to get x1.
Now Bob and Alice can both compute k = H(ek, c, x1).
Alice erases dk and x1. Bob erases x1.
If the cryptosystem is reasonably secure, then this is secure against passive adversaries. You make it secure against active adversaries using digital signatures, just like with DH.
The interesting question is: Can you do better than this, say like (H)MQV? Can you get more symmetry?

@_date: 2015-09-23 18:56:54
@_author: =?utf-8?B?S3Jpc3RpYW4gR2rDuHN0ZWVu?= 
@_subject: [Cryptography] Non-Authenticated Key Agreement 
23. sep. 2015 kl. 20.04 skrev Philipp GÃ¼hring :
That is false.
If Aliceâ random number generator always outputs 3, Eve can easily deduce the shared secret, regardless of what Bob does.
The correct statement is that the parties end up with a randomly chosen key if Bob (the responder) follows the protocol (with some minor additions to the textbook version). They end up with a randomly chosen key if Alice (the initiator) follows the protocol and Bobâs value is independent of Aliceâ value. Note that Â«randomly chosenÂ» is not the same as Â«secureÂ».

@_date: 2016-08-09 07:45:39
@_author: =?utf-8?B?S3Jpc3RpYW4gR2rDuHN0ZWVu?= 
@_subject: [Cryptography] BBC to deploy detection vans to snoop 
9. aug. 2016 kl. 00.29 skrev Jerry Leichter :
No, cryptologists do not assume that it doesn’t really matter very much.
Cryptologists know very well that these issues can be very important for applications. What cryptologists also know is that crypto primitives usually cannot deal effectively with these issues, since crypto primitives must support many different application requirements.
This is obvious for application timing attacks. (There’s timing attacks on the primitives themselves, where the designer can contribute, of course, but that is different from application timing attacks.)
For things like message length, you could in theory mitigate the problem by padding everything to multiples of 512 bytes (or 1024, or 2048, or 31415, or …), but this is inefficient, since the crypto primitive designer does not in advance know the application requirements, and for any mitigation, probably I can come up with an application where that mitigation fails.
Yes, we have to deal with these things. But there’s a limit to what crypto primitives can do. Cryptologists are usually very explicit about what those limits are. Information security people and application designers may need to be better aware of these limits, of course.

@_date: 2016-08-31 06:34:17
@_author: =?utf-8?B?S3Jpc3RpYW4gR2rDuHN0ZWVu?= 
@_subject: [Cryptography] Key meshing (Re: [Crypto-practicum] Retire all 
30. aug. 2016 kl. 18.22 skrev Phillip Hallam-Baker :
You have now created a stateful cryptosystem. You have to remember the key between encryptions. This is inconvenient in many situations.
You have also made a system that needs a block cipher secure against related key attacks. As we know, a secure block cipher does not have to be secure against related key attacks, so you have to redo a lot of analysis.
Also, there’s this thing about the key schedule. Yes, modern computers can do key schedules real quick, but I still have other stuff for them to do, so not having to do key schedules is a good thing.
If you don’t care about cost, you can du stuff like
which would be secure if you use any decent hash function. Now you have a choice whether to include an IV or keep a state.

@_date: 2016-08-31 20:33:45
@_author: =?utf-8?B?S3Jpc3RpYW4gR2rDuHN0ZWVu?= 
@_subject: [Cryptography] MATH: Unlikely correctness of paper will break 
31. aug. 2016 kl. 10.33 skrev Georgi Guninski :
Can you find a mistake in the paper? I looked briefly at it, and the equations seemed correct to me.
Linearization is an interesting technique. The idea is that you have a system of non-linear equations, say u1 x^2 + v1 x  = w1 and u2 x^2 + v2 x = w2, and you want to solve it, but you cannot. However, you know that there is a solution.
Now replace the x^2 terms with y. You get the linear system of equations
with two unknowns and two equations. Since you know that there is a solution x0 to your original system, you know that y=x0^2 and x=x0 is a solution to the linear system.
The point is that if your linear system of equations has a unique solution, you can find this solution and it must be y=x0^2 and x=x0. So this is a method for solving certain non-linear systems of equations using linear equations. It has found reasonably interesting applications in cryptography.
However, if you have only one non-linear equation, this technique isn’t very useful, since there’s nothing to force any solution you find to be the correct solution.
The Chinese remainder theorem is also a massively useful theorem. It allows you to solve some equation modulo a bunch of relatively prime moduluses and then «glue» your results together to get solution(s) modulo the product of the moduluses. If everything is nice, you get a unique solution. However, if you cannot solve the equations (e.g. if you have a linear equation with two unknowns), trying to solve it modulo several moduluses doesn’t really help you, since the different equations are essentially independent.
I should also mention that this stuff modulo squares appear in a neat cryptosystem by Okamoto and Uchiyama. Paillier extended their construction into a properly useful cryptosystem. Later, at PKC 2006 Chevallier-Mames, Paillier and Pointcheval have an interesting cryptosystem modulo p^2.
The current paper, however, is a case of funny computations that aren’t really interesting.

@_date: 2016-09-29 22:22:10
@_author: =?utf-8?B?S3Jpc3RpYW4gR2rDuHN0ZWVu?= 
@_subject: [Cryptography] Posting the keys/certs for: Two distinct 
29. sep. 2016 kl. 18.18 skrev Ron Garret :
These keys aren’t weak, they are invalid. The parameters used are not according to the standard.
Verifying the parameters is somewhat expensive (should be about the same cost as generating a signature, and half the cost of verifying a signature). It is not immediately obvious that it makes sense to verify these parameters all the time in a TLS context. The CA should probably verify them before it vouches for the key.

@_date: 2016-09-30 06:41:57
@_author: =?utf-8?B?S3Jpc3RpYW4gR2rDuHN0ZWVu?= 
@_subject: [Cryptography] Posting the keys/certs for: Two distinct 
30. sep. 2016 kl. 08.06 skrev Peter Gutmann :
For this particular key, yes. But you can probably do similar attacks with keys that superficially look ok. If you want to detect invalid keys, you really need to verify the order of g (and the primality of p and q, and probably some more stuff). The standard talks about this.
As I said, I suspect OpenSSL is doing the right thing by not verifying all the parameters all the time.
