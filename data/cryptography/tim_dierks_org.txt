
@_date: 2002-08-12 12:12:19
@_author: Tim Dierks 
@_subject: Feasability of Palladium / TCPA 
I haven't done an in-depth reading of available information of Palladium/TCPA specifications, but my understanding is that there is a private key stored in hardware and a contiguous chain of trust that extends from the hardware/BIOS through the OS to an application, with each level validating the superior levels; specifically, the hardware/BIOS validates the OS as compliant, and then the OS validates an application's identity. The OS then vouches for the application to the BIOS, giving the application the ability to make indirect use of the hardware private key in order to engage in application-level communication with an attestation of the integrity of all levels (hardware, OS, application).
Here's my question: who thinks this can actually be made secure? Having spent some time looking at such security issues, developing software, and watching security alerts fly by for software, I think there's almost no chance that the attestations made by the OS can be very secure.
Given the nature of PC software platforms, with the size, complexity, and diversity of vendors, I think it is guaranteed that there will be software bugs that will allow attackers to run rogue code inside another application's address space or otherwise cloak themselves in another application's trust attestation. The attacker can thus get access to protected secrets or the ability to convince a remote system that they are an authentic & secure software instance.
Does anyone not believe that there's not going to any buffer-overflow type bugs in drivers for third-party sound cards that will allow an attacker to execute code as some other application? And what can be done about? It's certainly not viable to require dramatically more code review & security in applications & drivers: I don't think Microsoft & Intel are willing to sacrifice the economic viability of the platform on the throne of DRM. It's also not acceptable to customers to require that all code running on the platform be signed & authorized (unless you're willing to destroy the industry of software developers who are not large enough or reliable enough to get such signatures, including the entire open source industry). I don't believe it's technologically feasible to design a software platform that can reliably determine the complete chain of control resulting in an API entry-point being called. (Among other things, you can execute an attack without having your return address address on the stack.)
Bottom line, I don't think that TCPA or Palladium will be reliable enough to really constrain those who wish to penetrate it. Given that, I don't think that there will be as high a value put on the technology by vendors & rights holders (since their stuff can be stolen by others anyway). If there's not a high differential value to these architectures, then presumably the market will not support a high differential price, and the technologies will not see widespread acceptance.
  - Tim Dierks
PS - I'm looking for a job in or near New York City. See my resume at

@_date: 2002-08-12 15:28:15
@_author: Tim Dierks 
@_subject: Palladium: technical limits and implications 
The addition of an additional security ring with a secured, protected memory space does not, in my opinion, change the fact that such a ring cannot accurately determine that a particular request is consistant with any definable security policy. I do not think it is technologically feasible for ring -1 to determine, upon receiving a request, that the request was generated by trusted software operating in accordance with the intent of whomever signed it.
Specifically, let's presume that a Palladium-enabled application is being used for DRM; a secure & trusted application is asking its secure key manager to decrypt a content encryption key so it can access properly licensed code. The OS is valid & signed and the application is valid & signed. How can ring -1 distinguish a valid request from one which has been forged by rogue code which used a bug in the OS or any other trusted entity (the application, drivers, etc.)?
I think it's reasonable to presume that desktop operating systems which are under the control of end-users cannot be protected against privilege escalation attacks. All it takes is one sound card with a bug in a particular version of the driver to allow any attacker to go out and buy that card & install that driver and use the combination to execute code or access data beyond his privileges.
In the presence of successful privilege escalation attacks, an attacker can get access to any information which can be exposed to any privilige level he can escalate to. The attacker may not be able to access raw keys & other information directly managed by the TOR or the key manager, but those keys aren't really interesting anyway: all the interesting content & transactions will live in regular applications at lower security levels.
The only way I can see to prevent this is for the OS to never transfer control to any software which isn't signed, trusted and intact. The problem with this is that it's economically infeasible: it implies the death of small developers and open source, and that's a higher price than the market is willing to bear.
  - Tim
PS - I'm looking for a job in or near New York City. See my resume at

@_date: 2002-08-12 16:32:05
@_author: Tim Dierks 
@_subject: trade-offs of secure programming with Palladium (Re: 
I agree; I think the system as you describe it could work and would be secure, if correctly executed. However, I think it is infeasible to generally implement commercially viable software, especially in the consumer market, that will be secure under this model. Either the functionality will be too restricted to be accepted by the market, or there will be a set of software flaws that allow the system to be penetrated.
The challenge is to put all of the functionality which has access to content inside of a secure perimeter, while keeping the perimeter secure from any data leakage or privilege escalation. The perimeter must be very secure and well-understood from a security standpoint; for example, it seems implausible to me that any substantial portion of the Win32 API could be used from within the perimeter; thus, all user interface aspects of the application must be run through a complete security analysis with the presumption that everything outside of the perimeter is compromised and cannot be trusted. This includes all APIs & data.
I think we all know how difficult it is, even for security professionals, to produce correct systems that enforce any non-trivial set of security permissions. This is true even when the items to be protected and the software functionality are very simple and straightforward (such as key management systems). I think it entirely implausible that software developed by multimedia software engineers, managing large quantities of data in a multi-operation, multi-vendor environment, will be able to deliver a secure environment.
This is even more true when the attacker (the consumer) has control over the hardware & software environment. If a security bug is found & patched, the end user has no direct incentive to upgrade their installation; in fact, the most concerning end users (e.g., pirates) have every incentive to seek out and maintain installations with security faults. While a content or transaction server could refuse to conduct transactions with a user who has not upgraded their software, such a requirement can only increase the friction of commerce, a price that vendors & consumers might be quite unwilling to pay.
I'm sure that the whole system is secure in theory, but I believe that it cannot be securely implemented in practice and that the implied constraints on use & usability will be unpalatable to consumers and vendors.
  - Tim
PS - I'm looking for a job in or near New York City. See my resume at

@_date: 2002-08-26 13:14:56
@_author: Tim Dierks 
@_subject: PKI cost of ownership studies? 
I'm consulting for a company which would like to have a better understanding of the market's perception of PKI cost of ownership. Specifically, they hear a lot of numbers tossed around regarding people's perception of what it costs to set up a PKI, numbers in the $3 million and up area, and they'd like to better understand where those numbers come from.
I'm interested in getting a better understanding of where people may come up with these numbers and what they're composed of: software costs, integration services, building a secure vault, key ceremonies, CPS's, etc.?
Can anyone point me to available studies which might be influential in establishing perception of these costs?
Or better, is there anyone out there who's got a really good understanding of such market perception and who'd like to bill a few hours to answer some questions and brainstorm a little?
  - Tim Dierks
    tim at dierks.org

@_date: 2002-07-17 14:06:15
@_author: Tim Dierks 
@_subject: Secure E-Mail ASP-type systems? 
I'm looking to assemble a list of commercially available secure e-mail ASP solutions, with a particular focus on those vendors who make their solution available for sale for operation by the customer. (Think running your own Hushmail servers.)
I'm aware of Certified Mail, but there must be many alternatives out there.
If you know of such an ASP or vendor, please send me an e-mail and I'll summarize to the list.
  - Tim Dierks

@_date: 2002-09-20 10:18:48
@_author: Tim Dierks 
@_subject: Court Decision about russian hackers? 
This URL should provide enough info for you to find the case:
  - Tim

@_date: 2003-08-19 11:57:32
@_author: Tim Dierks 
@_subject: PRNG design document? 
Is there a definitive or highly recommended paper or book on the design of I'm assuming a cryptographic PRNG of the type in OpenSSL, PGP, etc., where entropic seeding data is accumulated into a pool and output is produced by operating on the pool with a secure hash or similar cryptographic algorithm.
I'm interested in whether there's a strong source on what the design considerations for how to process the input into the pool, mix & remix the pool, and generate output are. I'm familiar with a number of generators and many issues in building such things, but I don't know if I've seen a single design document.
  - Tim
PS - Either I've been dropped, the list is dead, or no one has had anything to say for the last month.

@_date: 2003-08-29 15:02:46
@_author: Tim Dierks 
@_subject: Beware of /dev/random on Mac OS X 
Can anyone who believes that only having 160 bits of entropy available is an interesting weakness tell me why? I'm currently of the belief that there's far too much entropy paranoia out there. Barring disclosure of the entropy pool, I'm not aware of any plausible attack that could occur if I (for example) generate a bunch of keys from a single 160-bit entropy seed, given that I believe a 160-bit value to be invulnerable to brute force for quite a long time. I can't imagine any situation in which the lack of reseeding is going to be the weakness in this scenario, but maybe I'm insufficiently imaginative.
  - Tim

@_date: 2003-08-29 15:43:40
@_author: Tim Dierks 
@_subject: PRNG design document? 
I'd like to thank everyone for their suggestions re: PRNG design documents. The most commonly suggested documents were:
    Peter Gutmann's paper on the subject:
           The Yarrow design document:
       Other links & suggestions:
    A link farm from David Wagner:
           The FIPS 186 generator:
              (appendix 3)
Allow me to clarify my problem a little. I'm commonly engaged to review source code for a security audit, some such programs include a random number generator, many of which are of ad-hoc design. The nature of such audits is that it's much more appealing to be able to say "here are three accepted guidelines that your generator violates" rather than "I haven't seen that before and I don't like it, you should replace it with something So I'm interested in such design guidelines, if they're available, which such a generator could be tested against. While the resources provided have been useful, it's only led me to where I was: that the only way to do so is to attempt to analyze the system for vulnerability to a collection of known I know a bunch of basic, obvious things that I can state (have a large enough internal state, generate output with a secure hash, etc.) and a bunch of other fuzzier notions that are harder to concretize (output should be dependent on a sufficient quantity of the internal pool, reseeding should affect a sufficent quantity of the internal pool, etc.). But I don't have a resource which attempts to canonically define minimal requirements for all these elements. (If I have missed such a list in skimming the broad resources available, I'd appreciate a note.)
Anyway, thanks to all.
  - Tim

@_date: 2003-12-27 12:09:06
@_author: Tim Dierks 
@_subject: Identity Based Encryption 
US Patent Application 20030081785, May 1, 2003:   - Tim

@_date: 2003-02-08 13:24:14
@_author: Tim Dierks 
@_subject: Columbia crypto box 
Here are three valid reasons for NSA (who provides communication security to NASA) to keep crypto algorithms secret:
  1. If one has a sufficiently good level of analysis in-house that additional cryptographic analysis has reached the level of diminishing returns, then there's little additional value to be gained from the community input resulting from disclosure. In such a situation, even if a cipher is secure enough to meet its goals based solely on secrecy of the key, the marginal security of keeping the algorithm secret is of value.
  2. Keeping an algorithm secret prevents your opponents from using it. If you have better algorithms than your opponents, this is of value.
  3. Keeping an algorithm secret may provide protection to design concepts and constraints, which will help you keep secret methods of cryptanalysis with which you are familiar, but that your opponents have not yet discovered (e.g. differential cryptanalysis).
There may be more valid reasons for treating the device as secret; some categories that come to mind include protecting non-cryptographic information, such as the capabilities of the communication channel. Also, many systems on the shuttle are obsolete by modern standards, and it's possible that the communications security is similarly aged.
  - Tim Dierks

@_date: 2003-07-07 18:31:20
@_author: Tim Dierks 
@_subject: Fwd: [IP] A Simpler, More Personal Key to Protect Online 
I wrote this for another list I'm on:
This system is based on an identity-based cryptography scheme developed by Dan Boneh with Matt Franklin. You can find a link to his paper "Identity based encryption from the Weil pairing" on Dr. Boneh's website,  .
The system allows any predetermined public value (e.g., an e-mail address) to be a public key. To encrypt a message, you do a mathematical operation as follows:
   EncM = E(M, pubKey, p)
   EncM is the encrypted message
   E is the encryption operation
   M is the message
   pubKey is the public key (e-mail address)
   p is a set of public domain parameters
The parameters p are a set of values which any subset of people can use to communicate with each other, but which must be predetermined by a trusted party and shared with all communicants. When the trusted third party creates the public domain parameters, there is a matched set of secret domain parameters (call them sp) which allow the trusted party to determine the matching secret key for any public key. Namely, in this system, for every pubKey there is a matching secKey which can be used to decrypt an encrypted message. The secret domain parameters are needed to be able to calculate secKey from pubKey:
   secKey = KD(pubKey, sp)
Where KD is the key derivation algorithm.
So, it all boils down to a system that's not dissimilar to a traditional CA-based public key system. In order for you to participate, you go to the trusted third party, they verify that you own the e-mail address you're claiming to possess (with whatever level of verification they insist upon), and if you do, they generate your secret key for you and send it to you. You can now decrypt messages which other people encrypt with that public key.
I don't think it's an interesting solution. I don't see any interesting application that's possible with this system which you couldn't do with existing public-key cryptography: for example, I could write a protocol & software where you could request a public key from a server for any e-mail address; if the user didn't already have an enrolled key, my trusted server would generate one and enroll it on their behalf. When they got an encrypted message, they could contact me, authenticate themselves, and I'd send them their secret key. The functionality ends up being pretty much the same, but you don't need goofy new crypto to accomplish it. Furthermore, no-one's bothered to deploy the system I describe (although it's obvious) which implies that market demand for such a system hasn't been held back by the fact that no one had figured out the math yet. All of this, on top of the fact that the private key, is, in essence, escrowed by the trusted third party, causes me to believe that this system doesn't fill an important unmet need.
  - Tim

@_date: 2003-07-08 17:47:10
@_author: Tim Dierks 
@_subject: Fwd: [IP] A Simpler, More Personal Key to Protect Online 
True, but how valuable is that, given that you can't send the actual message without contacting a server? I suppose one can construct theoretical scenarios where that's a benefit, but it seems to be a pretty narrow niche to me.
I retract the "goofy". My point was that the market is incredibly reluctant to adopt new technology: if you can solve a problem with components known to the marketplace, you're much more likely to be successful than if you invent something new. This is above and beyond any reluctance to adopt new cryptographic technology based on concerns about security.
Even if the Weil pairing is known to be 100% secure and tested, any new solution has to, as a practical matter, leap a huge hurdle to overcome available, well known alternatives. I've spent years attempting to get the market to accept alternative security solutions, and I can testify to how high that hurdle is. In my opinion, identity-based cryptography has insufficient upside to overcome that hurdle, especially given that it is not without its downsides (escrowed private keys, no protection against key   - Tim

@_date: 2003-07-08 17:54:23
@_author: Tim Dierks 
@_subject: LibTomNet [v0.01] 
IIRC, the embedded SSL library I wrote (with Chris Hawk) at Certicom was < 64K of 68K code (we originally wrote it for PalmOS devices), including all crypto, for a fully-compliant SSL 3.0 & X.509v3 implementation (client-side SSL only, with a profiled subset of SSL ciphersuites and X.509 features, of course). And it could run with a RAM usage of substantially less than 10K/connection. And we wrote it in less than a month (it was our third or fourth time implementing SSL and X.509, though).
The complete Certicom library is somewhat bigger, but it's got a lot of flexibility, (modular crypto interface, etc.), and code size wasn't a concern on desktop/server platforms.
  - Tim

@_date: 2003-06-03 11:14:46
@_author: Tim Dierks 
@_subject: Maybe It's Snake Oil All the Way Down 
I have my own opinion on what this assertion means. :-) I believe it intends to state that ssh is more successful because it is the only Internet crypto system which has captured a large share of its use base. This is probably true: I think the ratio of ssh to telnet is much higher than the ratio of https to http, pgp to unencrypted e-mail, or what have you.
However, I think SSL has been much more successful in general than SSH, if only because it's actually used as a transport layer building block rather than as a component of an application protocol. SSL is used for more Internet protocols than HTTP: it's the standardized way to secure POP, IMAP, SMTP, etc. It's also used by many databases and other application protocols. In addition, a large number of proprietary protocols and custom systems use SSL for security: I know that Certicom's SSL Plus product (which I originally wrote) is (or was) used to secure everything from submitting your taxes with TurboTax to slot machine jackpot notification protocols, to the tune of hundreds of customers. I'm sure that when you add in RSA's customers, those of other companies, and people using OpenSSL/SSLeay, you'll find that SSL is much more broadly used than ssh.
I'd guess that SSL is more broadly used, in a dollars-secured or data-secure metric, than any other Internet protocol. Most of these uses are not particularly visible to the consumer, or happen inside of enterprises. Of course, the big winners in the $-secured and data-secured categories are certainly systems inside of the financial industry and governmental systems.
  - Tim

@_date: 2003-06-06 15:04:49
@_author: Tim Dierks 
@_subject: Maybe It's Snake Oil All the Way Down 
I don't think this problem is easier to solve (or at least I sure don't know how to solve it). It seems to me that you could tell a user every time they go to a new site that it's a new site, and hope that users would recognize that e-g0ld.com shouldn't be "new", since they've been there before. However, people go to a large enough number of sites that they'd be seeing the "new" alert all the time, which leads me to believe that it wouldn't be taken seriously.
Fundamentally, making sure that people's perception of the identity of a web site matches the true identity of the web site has a technical component that is, at most, a small fraction of the problem and solution. Most of it is the social question of what it means for the identity to match and the UI problem of determining the user's intent (hard one, that), and/or allowing the user to easily and reliably match their intent against the "reality" of the true "identity".
Any problem that has as a component the fact that the glyphs for "lower-case L" and "one" look pretty similar isn't going to be easy to solve technologically.
  - Tim

@_date: 2003-06-06 15:07:45
@_author: Tim Dierks 
@_subject: Maybe It's Snake Oil All the Way Down 
This isn't an SSL flaw; this is an HTTPS flaw, and it is repaired by RFC 2817, which is, as far as I know, sadly unimplemented in the field.
  - Tim

@_date: 2003-06-08 18:03:29
@_author: Tim Dierks 
@_subject: An attack on paypal 
I don't think it's https that's broken, since https wasn't intended to solve the customer authentication / authorization problem (you could try to use SSL's client certificates for that, but no one ever intended client certificate authentication to be a generalized transaction problem).
When I responded to this before, I thought you were talking about the server auth problem, not the password problem. I continue to feel that the server authentication problem is a very hard problem to solve, since there's few hints to the browser as to what the user's intent is.
The password problem does need to be solved, but complaining that HTTPS or SSL doesn't solve it isn't any more relevant than complaining that it's not solved by HTML, HTTP, and/or browser or server implementations, since any and all of these are needed in producing a new solution which can function with real businesses and real users. Let's face it, passwords are so deeply ingrained into people's lives that nothing which is more complex in any way than passwords is going to have broad acceptance, and any consumer-driven company is going to consider "easy" to be more important that "secure".
Right now, my best idea for solving this problem is to:
  - Standardize an HTML input method for  which does an SPEKE (or similar) mutual authentication.
  - Get browser makers to design better ways to communicate to users that UI elements can be trusted. For example, a proposal I saw recently which would have the OS decorate the borders of "trusted" windows with facts or images that an attacker wouldn't be able to predict: the name of your dog, or whatever. (Sorry, can't locate a link right now, but I'd appreciate one.)
  - Combine the two to allow sites to provide a user-trustable UI to enter a password which cannot be sucked down.
  - Evangelize to users that this is better and that they should be suspicious of any situation where they used such interface once, but now it's gone.
I agree that the overall architecture is broken; the problem is that it's broken in more ways than can just be fixed with any change to TLS/SSL or HTTPS.
  - Tim

@_date: 2003-03-01 21:42:17
@_author: Tim Dierks 
@_subject: Wiretap Act Does Not Cover Message 'in Storage' For Short  
This would seem to imply to me that the wiretap act does not apply to any normal telephone conversation which is carried at any point in its transit by an electronic switch, including all cell phone calls and nearly all wireline calls, since any such switch places the data of the ongoing call in "storage" for a tiny fraction of a second.
  - Tim

@_date: 2003-03-02 19:44:06
@_author: Tim Dierks 
@_subject: Wiretap Act Does Not Cover Message 'in Storage' For Short 
You may be correct as to intent, but the originally forwarded article says:
 > The court relied on Konop
 > v. Hawaiian Airlines Inc., which held that no Wiretap Act
 > violation occurs when an electronic communication is
 > accessed while in storage, "even if the interception takes
 > place during a nanosecond 'juncture' of storage along the
 > path of transmission."  Case name is U.S. v. Councilman.
which includes the phrase "along the path of transmission."
In order to avoid overreaction to a nth-hand story, I've attempted to locate some primary sources.
Konop v. Hawaiian Airlines:
   My understanding is that Konop v. Hawaiian Airlines was a lawsuit by Robert Konop against his former employer, Hawaiian Airlines. Mr. Konop had operated a website where he published a variety of allegations about Hawaiian, and he restricted access to that site by username and password. A manager at Hawaiian gained the permission of two other employees of the airline to use their names in accessing the website; Konop found out about the access and sued Hawaiian. Among other grounds, he claimed that management viewing his site constituted an "interception" of electronic communications in violation of the wiretap act.
I won't go into any argument about the plausibility of this claim; I'll just summarize the legal proceedings thereafter. The federal district court which heard the case granted summary judgement against Konop on the wiretap claims; the 9th circuit court of appeals then reversed the district court's decision on the wiretap claims. Thereafter, the 9th circuit withdrew that opinion, then affirmed the district court's original judgement against Konop. Thus, the end result is that the wiretap claim does not hold. Why?
Amid other reasoning, the court refers to an old friend, Steve Jackson Games, Inc. v. United States Secret Service. In summary, the fifth circuit court determined that e-mail stored on a machine was not protected by the wiretap act, because an "electronic communication" cannot be "intercepted" in the same way that a "wire communication" can be. This reasoning has been upheld with respect to voicemail messages.
There is a footnote that specifically addresses the interesting question: that all electronic messages involve storage at some point, so the wiretap act is meaningless with respect to electronic communication. The crucial conclusion is:
United States of America vs. Bradford S. Councilman:
   The Government charged Mr. Councilman with conspiracy to violate the wiretap act. Apparently, they claim that he used the contents of electronic mail passing through his service for commercial gain.
The judge seems quite aware of the implications of the decision and the effect of the Konop precedent, but dismisses the charge.
Based upon this rationale, it seems that one cannot be convicted of violating the wiretap act unless one actually taps into electric signals. For example, it would seem to continue to be illegal to intercept 802.11 RF signals, but possible not be illegal to plug a cable into an ethernet hub and copy all traffic on the subnet (since most hubs "store" packets internally for transmission), and perfectly OK to subvert a router to forward copies of all packets to you.
I'd be interested in any opinions on how this affects the government's need to get specific wiretap warrants; I don't know if the law which makes illicit civilian wiretapping illegal is the same code which governs the government's ability (or lack thereof) to intercept communications.
  - Tim

@_date: 2003-03-05 14:40:20
@_author: Tim Dierks 
@_subject: Wiretap Act Does Not Cover Message 'in Storage' For Short 
Furthermore, it's apparently not illegal for a non-governmental actor to retrieve stored information which they have access to, although it might be illegal for them to wiretap a communication even if they had access to the physical medium over which it travels.
I disagree with "Somebody"'s claim; I don't think that claim would go anywhere in court, since a transmission clearly falls under the category of "wire communication", and it's clear that transmission lines are the very entities the wiretap act has always been intended to protect, so Congress' intent is quite clear, regardless of any argument about "storage".
  - Tim

@_date: 2003-03-07 13:40:56
@_author: Tim Dierks 
@_subject: Proven Primes 
Does anyone, in practice, care about the distinction, if the probability that the prime test has failed can be proved to be far less than the chance that a hardware failure has caused a false positive ECPP test? To restate the question: all calculation methods have a certain possibility of failure, whether due to human or mechanical error, however minute that possibility may be. If I can use a probabalistic primality test to reduce the possibility of error due to algorithm failure to a point that it's well below the possibility of error due to hardware failure, what's the practical difference?
  - Tim

@_date: 2003-05-12 21:18:25
@_author: Tim Dierks 
@_subject: economics of spam (Re: A Trial Balloon to Ban Email?) 
Assuming that a CPU costs $500 and that its value can be amortized over 2 years, CPU costs .0016 cents/second.
Based on the numbers enough, the revenue/spam sent is .044 cents. Thus, the breakeven point is 27.6 seconds/message: assuming other costs are minimal, you have to require > 27.6 seconds of CPU calculation from an email submittant to ruin the spamming business model.
A few thoughts on this:
  - You have to adjust the size of the calculation frequently to keep up with Moore's law (although the time/$500 CPU is constant, assuming constant profitability for spam)
  - If spammers have new technology or economies of scale available to them, it's going to adversely affect everyone else. (That is, if you're using an 18-month-old CPU and CPU-seconds cost you twice what they cost in the volume it costs spammers, your $500 computer will have to spend 2 minutes of time to calculate a token it takes a spammer 30 seconds to   - This is going to dramatically increase the costs of sending bulk e-mail for non-spammers: for example, I get airline specials a few times a week; they must send millions of these.
  - The CPU time required here is several orders of magnitude larger than the cryptographic costs associated with SSL, and SSL is not broadly accepted at least in part due to the CPU cost associated with with it; this implies to me that there will be substantial resistance.
  - The CPU costs associated with SSL engendered a substantial market in cryptographic accelerators intended to reduce the cost to do an RSA private key operation. Presumably, a system like this will create such a market for e-mail token accelerators: unfortunately, this is exactly the kind of new tech / economy of scale envisioned above: we may end up with a situation where a calculation which costs a spammer .044 cents will take the average user's CPU 10 minutes or more to calculate.
  - Tim

@_date: 2003-11-11 22:31:09
@_author: Tim Dierks 
@_subject: Clipper for luggage 
From the New York Times. Any guesses on how long it'll take before your local hacker will have a key which will open any piece of your luggage?
  - Tim
A Baggage Lock for You and the Federal Screeners
By JOE SHARKEY
Published: November 11, 2003
AIRLINE passengers will be able to lock checked bags confidently again starting tomorrow, thanks to a new customer-service initiative between private enterprise and the Transportation Security Administration.
Here's how the plan will work: Several major luggage and lock retailers in the United States will announce tomorrow the availability of new locks, made by various manufacturers, that T.S.A. inspectors will be able to readily identify and open on checked bags selected for hand searches at T.S.A. screeners in airports around the country have already been trained in using secure procedures to open the new certified locks when necessary, and relock them after inspecting bags.
"Literally since we began the process of screening every checked bag for explosives in December, one of the challenges has been the ability to get into bags without doing damage to them," said Brian Turmail, a spokesman for the T.S.A.
The system, developed in cooperation with the T.S.A. and the Travel Goods Association, a trade group, was designed around "a common set of standards that any company that manufactures, or is interested in manufacturing, luggage or luggage locks could follow that would allow T.S.A. screeners to open the bag without doing damage to the bag, in a manner that would allow the bag to stay secured afterwards,'' Mr. Turmail said. "In other words, we can open it, but no one else can."
The locks will be available in various manufacturers' designs. All will be geared around a uniform technology allowing them to be opened by T.S.A. inspectors using a combination of secure codes and special tools, according to John W. Vermilye, a former airline baggage-systems executive who developed the system through Travel Sentry, a company he set up for that All the locks will carry a red diamond-shaped logo to certify to screeners that they meet the Travel Sentry standards. Mr. Vermilye said his company would receive royalties from manufacturers.
The system will ensure that passengers using the locks will not have to worry about a lock being broken or a locked bag being damaged if it is selected for hand inspection. It will also mean more peace of mind for passengers worried about reports of increased pilferage from unlocked bags.
"The general feeling of airline passengers is, 'I don't like to have to keep my bags unlocked,' " added Mr. Vermilye, who once worked as a baggage handler. "As somebody in the business for 30 years, I don't like it either, because I know what goes on" in some baggage-handling areas, he said.
An industry study showed that 90 percent of air travelers are now leaving checked bags unlocked, whereas before this year about 66 percent of them said they always locked their bags.
"I travel all the time, and I always used to lock my bags" until this year, said Michael F. Anthony, the chairman and chief executive of Brookstone, a specialty retailer with 266 shops, including 30 in airports. Besides the worry about theft within the airline baggage-handling systems, Mr. Anthony said he was concerned on business trips about unlocked bags in the hands of cab and airport shuttle drivers, bellhops and others.
Brookstone airport shops are planning to introduce the chain's own brand of new locks with in-store promotions tomorrow, Mr. Anthony said. A package of two four-digit Brookstone combination locks costs $20. Luggage and other accessories with the lock standards incorporated also will begin moving soon onto shelves at Brookstone and other retailers.
Mr. Anthony said that the locks represented a needed air-travel customer-service breakthrough, "helping people reclaim a sense of security they had in the past" with their checked possessions.
The T.S.A. mandated screening of all checked bags starting last Dec. 31. Since then, most of the estimated 1.5 million bags checked daily in domestic airports have been inspected by bomb-detecting machinery - but about 10 percent of checked bags are opened and inspected by hand.
Initially, the T.S.A. planned to issue a blanket prohibition against locking bags, but the agency ultimately decided instead to merely suggest that passengers not lock them. The T.S.A. public directive on the subject says: "In some cases screeners will have to open your baggage as part of the screening process. If your bag is unlocked, then T.S.A. will simply open the bag and screen the bag. However, if the bag is locked and T.S.A. needs to open your bag, then locks may have to be broken. You may keep your bag locked if you choose, but T.S.A. is not liable for damage caused to locked bags that must be opened.''
With bags unlocked, many travelers, including business travelers who pack expensive electronic gear, worried that their checked possessions were far too vulnerable to theft, passing unlocked through T.S.A. hands and into the standard airline baggage-handling systems. Reports of pilferage rose this year, as did concern about who was legally responsible for claims of theft or damage, since both government and airline employees have custody of bags at various points.
Mr. Vermilye is a former head of baggage operations for Eastern Airlines who later worked as a top executive of the International Air Transport Association, a trade group for airlines worldwide. After 9/11, he was part of a team of industry consultants working with the T.S.A. to improve customer service.
Mr. Vermilye and Mr. Turmail at the T.S.A. agreed that the new system would probably make the screening chore easier for inspectors. "With this system, they know they don't have to break a lock or damage a bag. They go, 'Relax, I know I can open it.' It ceases to become an issue," Mr. Vermilye said.

@_date: 2003-11-16 10:30:33
@_author: Tim Dierks 
@_subject: XML-proof UIDs 
This is what GUIDs/UUIDs were designed for, and they're used broadly. They're standardized in ISO 11578 [1], although there's a very similar public description in an expired Internet Draft [2]. Microsoft also publishes a description of how they generate their GUIDs, but I can't find it right now.
  - Tim
PS - I'm looking for a full-time job. My resume is at  . Looking for architecture or technical management jobs; I'm in New York, NY, but I am willing to relocate.

@_date: 2003-11-16 10:06:27
@_author: Tim Dierks 
@_subject: A-B-a-b encryption 
I don't know what it's called, but it's vulnerable to man-in-the-middle unless you've got some way to authenticate the parties (because Alice has no way to tell if she's decrypting Bob's B(A(m)) or Mallet's M(A(m)).
And if you've got some way to authenticate the parties (a shared secret or a public key or something), it's probably easier to leverage that into an encryption key.
  - Tim
PS - I'm looking for a full-time job. My resume is at  . Looking for architecture or technical management jobs; I'm in New York, NY, but I am willing to relocate.

@_date: 2003-10-01 19:23:08
@_author: Tim Dierks 
@_subject: anonymous DH & MITM 
It does not, and most SSL/TLS implementations/installations do not support anonymous DH in order to avoid this attack. Many wish that anon DH was more broadly used as an intermediate security level between bare, insecure TCP & authenticated TLS, but this is not common at this time.
(Of course, it's not even clear what MITM means for an "anonymous" protocol, given that the layer in question makes no distinction between Bob & Mallet.)
  - Tim

@_date: 2003-10-01 22:49:44
@_author: Tim Dierks 
@_subject: anonymous DH & MITM 
Sure, although it's a chicken & egg thing: it's not the standard because the initial adopters & designers of SSL didn't have any use for it (not to mention the political strength of RSADSI in the era).
There are so many different categories of users that it's probably impossible to make any blanket statements about "most users". It's certainly true that a web e-commerce vendor doesn't have much use for self-signed certificates, since she knows that dialogs popping up warning customers that they have some problem they don't understand is going to lead to the loss of some small fraction of sales. (Not that she necessarily has any concern about the security implications: it's almost entirely a customer comfort and UI issue.)
  - Tim

@_date: 2003-10-02 12:36:18
@_author: Tim Dierks 
@_subject: anonymous DH & MITM 
In TLS, AnonDH offers forward secrecy, but there are no RSA certificate modes which do (except for ExportRSA). You can use ephemeral DH key agreement keys with static certified DSA keys, though.
To be clear, this is a protocol issue, not really a self-signed certs vs. DH issue. The only real difference between a self-signed cert and an ephemeral bare public key is that you've got proof of private key possession by somebody (if that matters to you), and the entity has bound a self-asserted name & attributes to the key. Also, our extant infrastructure makes it easier to cache a once-presented X.509 certificate for consistency with future transactions, and self-signed certs fit more cleanly into a hybrid model where some entities are trusted due to third-party certification and some are directly approved.
  - Tim

@_date: 2003-10-02 15:35:12
@_author: Tim Dierks 
@_subject: anonymous DH & MITM  
I think it's a tautology: there's no such thing as MITM if there's no such thing as identity. You're talking to the person you're talking to, and that's all you know.
Re: your chess problem, I think the reason it's not applicable is because the concept of "Alice" and "Bob", as distinct from "Mitch", has no role in an anonymous protocol: Alice completing a chess move with Mitch is just as valid as completing one with Bob.
  - Tim

@_date: 2003-10-03 14:23:24
@_author: Tim Dierks 
@_subject: anonymous DH & MITM  
They've got exactly that same assurance in a MITM situation: unfortunately, Mallet is the one other party who can read the message. If you extend the concept to say "but I want Bob to be the one who can read the message", you've discarded anonymity. And saying that "I want only one party to have access to my message" is digital rights management.
Even if you could make this assertion, how would you avoid something that I'll call the "Cyrano attack": that the person you're communicating with is not, in fact, the source of the witticisms you associate with his pseudonym? And how is that attack distinct from MITM?
  - Tim

@_date: 2003-10-03 17:27:36
@_author: Tim Dierks 
@_subject: anonymous DH & MITM  
In an authenticated protocol, you can have a risk model which includes the idea that an authorized person will not choose to share a secret with an unauthorized person. However, in an anonymous protocol, you cannot have such a risk model, because there's no such distinction.
Are you saying that you're defining a protocol with rules of behavior which cannot be enforced (namely that Mallet is breaking the protocol by forwarding the data)? Previously, you said that you were defining the thing to be controlled as the shared secret, but now you're extending it to any and all content transmitted over the link. Describing the format of communications between parties is in a "protocol"; what they do with those communications is in a "risk model" or "trust model".
No disagreement with this: if you can ever communicate over an unintermediated channel, you can detect previous or future intermediations. There are easier ways to do it than maintaining a hash of all session keys: you can just insist that the other party have the same public key they had the first time you spoke, and investigate if that becomes untrue (for example, ssh's authentication model).
This can be defeated by Mallet if he makes changes in his forwarding of communications (that either have no semantic effect or have whatever semantic effect he chooses to impose), but which causes the hashes to vary. He then posts statements re: his communications with each of Alice & Bob, so they'll see a match.
Or, alternately, he interposes himself between Alice & Bob and the bulletin board, which is possible within most understandings of the MITM threat. (Again, if Mallet can't do that, it implies that Alice & Bob have an unintermediated channel available: the bulletin board).
  - Tim

@_date: 2003-10-03 19:31:56
@_author: Tim Dierks 
@_subject: anonymous DH & MITM  
I'm lost in a twisty page of MITM passages, all alike.
My point was that in an anonymous protocol, for Alice to communicate with Mallet is equivalent to communicating with Bob, since the protocol is anonymous: there is no distinction. All the concept of MITM is intended to convey is that in an anonymous protocol, you don't know who you're talking to, period. Mallet having two conversations with Alice & Bob is equivalent to Mallet intermediating himself into a conversation between Alice & Bob.
If you have some unintermediated channel to speak with a known someone once, you can exchange a value or values which will allow you to authenticate each other forevermore and detect any intermediations in the past. But the fundamental truth is that there's no way to bootstrap a secure communication between two authenticated parties if all direct & indirect communications between those parties may be intermediated. (Call this the 'brain in a jar' hypothesis.)
  - Tim

@_date: 2003-10-13 15:25:22
@_author: Tim Dierks 
@_subject: WYTM? 
I think this is an interesting, insightful analysis, but I also think it's drawing a stronger contrast between the real world and the Internet threat model than is warranted.
It's true that a large number of machines are compromised, but they were generally compromised by malicious communications that came over the network. If correctly implemented systems had protected these machines from untrustworthy Internet data, they wouldn't have been compromised.
Similarly, the statement is true at large (many systems are compromised), but not necessarily true in the small (I'm fairly confident that my SSL endpoints are not compromised). This means that the threat model is valid for individuals who take care to make sure that they comply with its assumptions, even if it may be less valid for the Internet at large.
And it's true that we define the threat model to be as large as the problem we know how to solve: we protect against the things we know how to protect against, and don't address problems at this level that we don't know how to protect against at this level. This is no more incorrect than my buying clothes which will protect me from rain, but failing to consider shopping for clothes which will do a good job of protecting me from a nuclear blast: we don't know how to make such clothes, so we don't bother thinking about that risk in that environment. Similarly, we have no idea how to design a networking protocol to protect us from the endpoints having already been compromised, so we don't worry about that part of the problem in that space. Perhaps we worry about it in another space (firewalls, better OS coding, TCPA, passing laws).
So, I disagree: I don't think that the SSL model is wrong: it's the right model for the component of the full problem it looks to address. And I don't think that the Internet threat model has failed to address the problem of host compromise: the fact is that these host compromises resulted, in part, from the failure of operating systems and other software to adequately protect against threats described in the Internet threat model: namely, that data coming in over the network cannot be trusted.
That doesn't change the fact that we should worry about the risk in practice that those assumptions of endpoint security will not hold.
  - Tim

@_date: 2003-09-06 14:33:56
@_author: Tim Dierks 
@_subject: cryptographic ergodic sequence generators? 
It seems to me that this could be constructed with a block cipher with a block size n bits long by encrypting the values 0..2^n sequentially with a random key.
I'm sure that it would be possible to design a Feistel-based block cipher with variable block size, supporting some range of even values of n.
  - Tim

@_date: 2003-09-06 20:59:20
@_author: Tim Dierks 
@_subject: cryptographic ergodic sequence generators? 
I'm not a cryptographer, so I can only make a tentative proposal, but I believe you could make use of any secure keyed one-way function as the function f() in a 2-round Feistel cipher and end up with a secure construction. For example, let's use DES.
Let's say we wanted a 16-bit block cipher. Then f() must take a round # and a 8-bit input and translate it into an 8-bit output. It doesn't have to be a unique or reversible mapping.
Using DES, we could lay out the plaintext block as:
   round # | input
with the round # in the first 32 bits and the input in the last 32 bits (padded with zero bits), then encrypt and take the low byte of the output as the output of f().
Thus, to encrypt a 16-bit block:
  - Divide it into 2 halves, L0 & R0
  - L1 = R0
  - R1 = L0 ^ DES(1 | R0)
  - L2 = R1
  - R2 = L1 ^ DES(1 | R1)
Output = R2 | L2
This will take two DES operations per iteration, so it's not super-speedy, but it's pretty fast for most operations. It's reversible, so you know it will generate all values. And the construction works for any even-length value of n. If you want, you can probably use a faster function for f(), depending on your security requirements.
Here's a simple Perl script I wrote just to make sure I had it right:
use Crypt::DES;
$n = shift if (!defined($n) || $n < 2 || $n % 2 != 0 || $n > 32 || $ > 0) {
         die "Usage: $0 n\n2 <= n <= 32 && n even\n";
$key = pack("A8", rand());
$cipher = new Crypt::DES $key;
$hn = $n/2;
$fmask = (1 << ($n/2)) - 1;
sub f($$) {
         my ($round, $v) =          my $pt = pack("LL", $round, $v);
         my $ct = $cipher->encrypt($pt);
         my ($high, $low) = unpack("LL", $ct);
         return $low & $fmask;
sub E($) {
         my ($p) =          my $L = $p >> $hn;
         my $R = $p & $fmask;
         my $round;
         for $round (1..2) {
                 $Ln = $R;
                 $Rn = $L ^ f($r, $R);
                 $L = $Ln;
                 $R = $Rn;
         }
         return ($R << $hn) | $L;
foreach $v (0..(1<<$n)-1) {
         $o = E($v);
         print "$v => $o\n";
         if ($o >= 1<<$n) {
                 die "Too big";
         }
         if ($retvals{$o}++) {
                 die "Duplicate";
         }
This takes the length of the sequence in bits as a parameter.
This will take a while to run for any value of n much larger than 10 or so, but I've tested it for up to n = 16 and it works fine: generates a random ordering of the values 0..2^n-1.
  - Tim

@_date: 2003-09-06 21:20:44
@_author: Tim Dierks 
@_subject: lopsided Feistel (was: cryptographic ergodic sequence 
Of course, I'd forgotten about unbalanced Feistel networks.
Here's an updated script:
use Crypt::DES;
$n = shift if (!defined($n) || $n < 2 || $n > 32 || $ > 0) {
     die "Usage: $0 n\n2 <= n <= 32\n";
$key = pack("A8", rand());
$cipher = new Crypt::DES $key;
$tb = int(($n+1)/2);
$sb = $n - $tb;
$tmask = (1 << ($tb)) - 1;
$smask = (1 << ($sb)) - 1;
sub f($$) {
     my ($round, $v) =      my $pt = pack("LL", $round, $v);
     my $ct = $cipher->encrypt($pt);
     my ($high, $low) = unpack("LL", $ct);
     return $low & $tmask;
sub E($) {
     my ($p) =      my $L, $R, $Ln, $Rn, $round;
     for $round (1..2) {
         $L = $p >> $sb;
         $R = $p & $smask;
         $Ln = $R;
         $Rn = $L ^ f($r, $R);
         $p = ($Ln << $tb) | $Rn;
     }
     return $p;
foreach $v (0..(1<<$n)-1) {
     $o = E($v);
     print "$v => $o\n";
     if ($o >= 1<<$n) {
         die "Too big";
     }
     if ($retvals{$o}++) {
         die "Duplicate";
     }

@_date: 2004-08-18 14:01:20
@_author: Tim Dierks 
@_subject: MD5 collisions? 
How likely is it that this attack could be extended to creating
common-prefix collisions? This is equivalent to specifying two IVs for
the two hashes. Looking at X.509 ASN.1, it seems like it would be
pretty easy to construct a certificate request such that, if you could
predict the certificate serial number you were to be issued, you could
create a hash collision in an X.509 certificate:
You would construct two strings, each the same length, an even
multiple of 512 bits long, which contained the prefix of the
certificate you were going to be issued and the prefix of the
certificate you would like to claim to have been issued (P1 =
authentic prefix, P2 = forged prefix). The trailing part of each
prefix would be the first few bits of the value of n in the RSA key.
These first few bits can be different in P1 and P2, and randomly
selected, as long as they begin with a 1. Call the first few bits of n
in P1 "p1", and the first few bits of n in P2 "p2".
You would take IV1 = internal state of MD5 after hashing P1 and IV2 =
internal state of MD5 after hashing P2. You can tweak the first few
bits of n if you want IV1 and IV2 to have a particular relation, as
long as it's not infeasible to find that relation. Then find two
1024-bit 2-block messages, M1, and M2, such that they create a
collision. Define v1 = p1 concatentated with M1, and similarly for v2.
Calculate D = abs(v1-v2). Factor D, finding a 1024-bit prime that
evenly divides D ("i"). If you can't easily factor D, or it doesn't
yield a 1024-bit prime factor, find another collision pair. Now
construct the last bits of n (call them t), which will be used in both
requests, such that n1 = v1 concatentated with k and n2 = v2
concatenated with k are both equal to i times j1 and j2 (each 1024-bit
primes). I think it may be possible to acclerate it, but even if you
just have to calculate collisions until it's true, you should be able
to find a collision D and a value k that gives primes i, j1, and j2
once in every ~30 million collisions. (I'm taking a stab in the dark
that values of D that have a single 1024-bit factor have 1/300
density, but this is demonstrably worst-case.) If you can find such a
collision with 5 minutes of CPU time, the attack will take, at worst,
275 CPU-years. This is university-feasible.
Of course, I don't have any idea how hard it is to extend the known
attack with common IVs to one in which the IVs are not common.
I'd recommend that anyone operating a CA choose serial numbers which are
long & unpredictable; if you want some structure, append a random 128 bit
value to your structure.
- Tim

@_date: 2004-12-15 10:45:17
@_author: Tim Dierks 
@_subject: The Pointlessness of the MD5 'attacks' 
Here's an example, although I think it's a stupid one, and agree with
you that the MD5 attack, as it's currently known to work, isn't a
material problem (although it's a clear signal that one shouldn't use
I send you a binary (say, a library for doing AES encryption) which
you test exhaustively using black-box testing. The library is known
not to link against any external APIs (in fact, perhaps it's
implemented in a language and runtime with a decent security sandbox
model, e.g., Java). You then incorporate it into your application and
sign the whole thing with MD5+RSA to vouch for its accuracy.
I incorporate several copies of a suitable MD5 collision block in my
library, so one of them will be at the correct 64-byte block boundary.
I can then modify bits inside of my library, which car checked by the
library code and cause it to change the functionality of the library,
yet the signature will still verify.
This would be pretty easy to do as a proof-of-concept, but I don't
have the time.
- Tim

@_date: 2005-08-12 11:47:26
@_author: Tim Dierks 
@_subject: Number of rounds needed for perfect Feistel? 
I'm attempting to design a block cipher with an "odd" block size (34
bits). I'm planning to use a balanced Feistel structure with AES as the
function f(), padding the 17-bit input blocks to 128 bits with a pad
dependent on the round number, encrypting with a key, and extracting the
low 17 bits as the output of f().
If I use this structure, how many rounds do I need to use to be secure (or
can this structure be secure at all, aside from the obvious insecurity
issues of the small block size itself)? I've been told that a small number
of rounds is insecure (despite the fact that f() can be regarded as
"perfect") due to collisions in the output of f(). However, I don't
understand this attack precisely, so a reference would be appreciated.
 - Tim

@_date: 2005-08-12 15:53:10
@_author: Tim Dierks 
@_subject: Number of rounds needed for perfect Feistel? 
A Feistel network doesn't depend on lack of collision in f(). The Handbook
of Applied Cryptography,
 describes it pretty
 - Tim

@_date: 2005-08-24 13:48:47
@_author: Tim Dierks 
@_subject: Another entry in the internet security hall of shame.... 
[resending due to e-mail address / cryptography list membership issue]
iChat pops up the warning dialog whenever the password is sent to the
server, rather than used in a hash-based authentication protocol.
However, it warns even if the password is transmitted over an
authenticated SSL connection.
I'll leave it to you to decide if this is:
 - an iChat bug
 - a Google security problem
 - in need of better documentation
 - all of the above
 - none of the above
 - Tim

@_date: 2005-02-11 11:31:16
@_author: Tim Dierks 
@_subject: TLS session resume concurrency? 
I don't fully understand how you phrased the question in the two deleted
paragraphs, but this one accurately describes the SSL/TLS session cache:
it holds a shared secret derived from the original key exchange. For each
connection, completely new encryption & authentication keys are derived
from this shared secret and per-connection random nonces provided by each
party. One session can be safely reused for many connections, either
serially or in parallel. The session cache is also write-once: starting a
new connection from a session needn't update the cached secret or other
 - Tim

@_date: 2006-09-14 11:12:56
@_author: Tim Dierks 
@_subject: Exponent 3 damage spreads... 
This problem is just as likely or more likely if we were using XML to
encode the hash inside the RSA-encrypted blob (signature). The
equivalents would be:
Appended garbage:
  Valid-looking-hashGarbage here
  Valid-looking-hash[null byte]Garbage here
Interior garbage:
  Valid-looking-hash
or similar attacks. The problem is not XML or ASN.1: the problem is
that it's very demanding and tricky to write parsers that are
invulnerable to all the different kinds of malicious attack that are
out there.
If anything, I think XML is more vulnerable to such attacks because
its less-structured format makes it harder to write very strict
parsers. The actual way to design a system that was less vulnerable to
this attack would have been to use a much simpler data structure:
e.g., one could have said that the hashing algorithm must already be
known, so the size of the hash is known to be n bytes, and that the
data block should be a byte of value 0, followed by bytes of value FF,
with the last n bytes equal to the hash. Then it would have been a
no-brainer for anyone to write a precisely accurate parser and
validator, and we'd be less vulnerable to such oversights.
 - Tim

@_date: 2007-11-29 16:05:00
@_author: Tim Dierks 
@_subject: Open-source PAL 
A random thought that's been kicking around in my head: if someone were
looking for a project, an open-source permissive action link (
 is a good link, thank you
Mr. Bellovin) seems like it might be a great public resource: I suspect it's
something that some nuclear states could use some education on, but even if
the US is willing to share technology, the recipient may not really trust
the source.
As such, an open-source PAL technology might substantially improve global
 - Tim

@_date: 2008-08-08 15:52:47
@_author: Tim Dierks 
@_subject: OpenID/Debian PRNG/DNS Cache poisoning advisory 
[Sorry for duplicates, but I got multiple requests for a non-HTML
version, and I didn't want to fork the thread. Also sorry for
initially sending HTML; I didn't realize it was so abhorrent these
days. ]
Using this Bloom filter calculator:
 ,
plus the fact that there are 32,768 weak keys for every key type &
size, I get various sizes of necessary Bloom filter, based on how many
key type / sizes you want to check and various false positive rates:
 * 3 key types/sizes with 1e-6 false positive rate: 2826759 bits = 353 KB
 * 3 key types/sizes with 1e-9 false positive rate: 4240139 bits = 530 KB
 * 7 key types/sizes with 1e-6 false positive rate: 6595771 bits = 824 KB
 * 7 key types/sizes with 1e-9 false positive rate: 9893657 bits = 1237 KB
I presume that the first 3 & first 7 key type/sizes in this list
 are the best to
incorporate into the filter.
Is there any chance it would be feasible to get a list of all the weak
keys that were actually certified by browser-installed CAs, or those
weak certificates? Presumably, this list would be much smaller and
would be more effectively distributed in Bloom filter form.
 - Tim

@_date: 2008-08-27 14:45:02
@_author: Tim Dierks 
@_subject: Decimal encryption 
I believe the most straightforward thing to do is to build a balanced
4-round Feistel cipher [  ]
that uses AES as its mixing function, but which operates within a
field of 10^20; you can then encrypt a value within F_10^40 as a
single block operation (ECB mode), taking 4 AES operations and some
other math do to so.
In this usage, each 20-digit side of the cipher would be expressed as
a bit string with ~66 bits, zero-padded to make a 128-bit block. You
should also use the round number in the input; you can put it in the
top 2 bits of the block. This block would then be encrypted with AES,
resulting in a 128-bit output block. You would then reduce this
128-bit value modulo 10^20 to give you a 20-digit output value from
your f() function; that value can be added, modulo 10^20, into the
other 20-digit side of the network (or subtracted on decryption).
A couple of notes:
 - I believe 4 rounds should be secure, but someone else on this list
should validate this.
 - As simply described here, this is unbalanced, because 2^128 is not
an even multiple of 10^20, so some 20-digit output values of f() are
more likely than others. To avoid this problem, if the 128-bit result
of the AES encryption is less than 2^128 % 10^20
(63374607431768211456), reencrypt the 128-bit output block with AES
again and iterate. This will happen approximately one time in 5e18, so
it's not clear that it's a real vulnerability; it's certainly not a
performance problem.
Good luck; please feel free to ask if any of this isn't clear.
 - Tim

@_date: 2008-01-31 14:51:38
@_author: Tim Dierks 
@_subject: Fixing SSL (was Re: Dutch Transport Card Broken) 
I totally disagree that this is a material problem that is in any meaningful
way impeding the use of SSL client certificates (there are totally different
reasons that client certs aren't being widely adopted, but that's beside the
However, TLS supports what you want right now: just do the initial handshake
without client auth, then renegotiate after the session encryption starts.
The renegotiation will happen under the encrypted, identity-protected and
server-authenticated session, and client authentication can be requested in
the renegotiation; the client cert will then be confidential.
The reason nobody actually bothers to do this is because there's no customer
demand (see paragraph 1).
 - Tim

@_date: 2013-09-05 17:14:39
@_author: Tim Dierks 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
I believe it is Dual_EC_DRBG. The ProPublica
Classified N.S.A. memos appear to confirm that the fatal weakness,
discovered by two Microsoft cryptographers in 2007, was engineered by the
agency. The N.S.A. wrote the standard and aggressively pushed it on the
international group, privately calling the effort ?a challenge in finesse.?
This appears to describe the NIST SP 800-90 situation pretty precisely. I
found Schneier's contemporaneous article to be good at refreshing my
 - Tim

@_date: 2013-09-06 13:09:19
@_author: Tim Dierks 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
On Fri, Sep 6, 2013 at 3:03 AM, Kristian Gj?steen <
It's implemented in Windows and in a number of other libraries*; I can't
find any documentation on which points these implementations use. But I
agree that there's little technical reason to use it?however, who is to
know that a vendor couldn't be influenced to choose it?
In pursuing the list NIST validations, there's aa number of cases where
Dual_EC_DRBG is the only listed mode, but all of them (with one exception)
are issued to companies where they have other validations, generally on
similar products, so it just looks like they got multiple validations for
different modes. The one exception is Lancope, validation  which
validated their use of Dual_EC_DRBG, but no other modes. So it looks like
there's at least one implementation at use in the wild.
 - Tim
* - The implementors that NIST
RSA, Certicom, Cisco, Juniper, BlackBerry, OpenPeak, OpenSSL, Microsoft,
Mocana, ARX, Cummings Engineering Consultants, Catbird, Thales e-Security,
SafeLogic, Panzura, SafeNet, Kony, Riverbed, and Symantec. (I excluded
validations where the implementation clearly appears to be licensed, but
people can name it anything they want, and some of the above are probably
just OpenSSL forks, etc.)

@_date: 2013-09-11 13:44:23
@_author: Tim Dierks 
@_subject: [Cryptography] About those fingerprints ... 
When it comes to litigation or actual examination, it's been demonstrated
again and again that people can hide behind their own definitions of terms
that you thought were self-evident. For example, the NSA's definition of
"target", "collect", etc., which fly in the fact of common understanding,
and exploit the loopholes in English discourse. People can lie to you
without actually uttering a demonstrable falsehood or exposing themselves
to liability, unless you have the ability to cross-example the assertions.
I don't have a precise cite for the Apple claim, but let's take two
summaries: first, from Andrew "Apple made the bold, unaudited claim that it
will never save the fingerprint data outside of the A7 chip". Initial
questions: does this mean they won't send the data to third parties? How
about give third parties the ability to extract the data themselves? Does
the phrase "fingerprint data" include all data derived from the
fingerprint, such as minutiae?
second, from Macworld:
"the fingerprint data is encrypted and locked in the device?s new A7 chip,
that it?s never directly accessible to software and that it?s not stored on
Apple?s servers or backed up to iCloud". Similar questions: is the data
indirectly accessible? Is it stored on non-Apple servers? Etc.
Unless you can cross-examine the assertions with some kind of penalty for
dissembling, you can't be sure that an assertion means what you think or
hope it means, regardless of how straightforward and direct it sounds.
 - Tim

@_date: 2014-06-19 10:14:46
@_author: Tim Dierks 
@_subject: [Cryptography] [cryptography] Dual EC backdoor was patented by 
I wasn't at Certicom in 2005, but based on my knowledge of company strategy
(I was CTO 1998-2002), yes, the desire was to file on anything that could
be patented and figure out how valuable it was further down the line.
Certicom was a company with a patent IP strategy, so this makes a lot of
 - Tim

@_date: 2016-04-18 12:09:26
@_author: Tim Dierks 
@_subject: [Cryptography] Security engineering at Google 
Forgive me for this commercial interruption, but I wanted to let the list
know [1] that we're always looking for strong security-minded software
engineers at Google.
We handle a huge amount of high-value data for users and companies and we
take the trust they place in us very seriously. We're not only building the
infrastructure to keep that data secure and private within Google, but we
are also, in building out the Google Cloud platform, developing tools and
foundations for data security for every company that makes use of it. As
such, working here is an opportunity to contribute to the security of
billions of users, not only of Google products, but of systems run by
companies who build on top of our platform. We also have efforts which work
to make the Internet safer in general, with projects like malware detection
and notification, Certificate Transparency, unphishable authentication, and
I work with a team of software engineers here in NYC who are responsible
for data protection for Google and Google Cloud Platform, including key
management, request authorization, audit logging, and binary-to-source
provenance. We are also looking for software engineers to join the security
team in the SF Bay Area and the Seattle area, and we have outposts in other
offices, including London and Zurich. [2]
The best candidates for these roles will be strong software engineers who
have deep knowledge of security. We also hire into other kinds of roles
(incident response, analysis, etc.); I'm not personally as familiar with
those roles or opportunities, but I'm happy to try to make connections.
I'm always happy to discuss opportunities here, feel free to contact me at
my personal address (tim at dierks.org) or corporate (dierks at google.com).
Best wishes,
 - Tim
[1] - Thanks to Perry and Tamzen for allowing me to share this here; I
started my career at Google thanks to an email from Fritz Schneider on this
list in 2003 [3], so while I recognize the imposition of recruiting spam, I
also know that it's valuable for technical people to connect other
technical people with opportunities.
[2] - Job posting:
[3] -  (I'm
hoping this one is as tasteful, although it's longer).
