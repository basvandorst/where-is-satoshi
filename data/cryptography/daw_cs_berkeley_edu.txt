
@_date: 2002-08-01 17:28:43
@_author: David Wagner 
@_subject: building a true RNG 
You understood it perfectly.  Good point.
I didn't notice that problem.  Harrumph.
Thanks for catching my oversight!

@_date: 2002-07-27 18:19:25
@_author: David Wagner 
@_subject: building a true RNG 
That obviously can't help.  Introducing the notion of streams of data
can't make the problem any easier.  Just look at the first n bits of
output; they are going to be a deterministic function of the first n' bits
of input, and we can simply let f be the function mapping the first n'
bits of input to the first n bits of output.  (Or, the function mapping
the entire input stream to the first n bits of output.)  Then we're back
to the same problem: since there is no way to guarantee that the first
n bits of output will be uniform if the input distribution is arbitrary,
there is no way to solve the streaming problem, either.

@_date: 2002-07-28 10:46:07
@_author: David Wagner 
@_subject: building a true RNG 
I understand what you're trying to say, but this will not give a
general-purpose function that "doesn't waste entropy" regardless of the
input distribution.  This only works when the distribution on the input
stream consists of independent, memoryless samples from some distribution
on 8-bit values.

@_date: 2002-07-28 10:50:42
@_author: David Wagner 
@_subject: building a true RNG 
Nitpick: You can sample from such a set.  You can generate m randomx
values from this set with about 10m computations of SHA-1: simply pick
a random x, check whether SHA-1(x) has its first ten zeros, and if not
go back and pick another x until you find one that works.
However, your general point is correct.  I make no claims about whether
this is a serious problem in practice.  In practice, we would find it
pretty surprising if our entropy sources generated entropy samples that
happen to interact badly with SHA-1.  Normally, we'd expect our entropy
sources to be somehow "independent" of the details of SHA-1.  However,
there is no proof that such a thing can't happen, and the only way I know
to formalize this assumption is using the random oracle model.
In other words, my point is merely that this stuff is very hard to
justify in any rigorous, formal, or precise way.

@_date: 2002-07-28 13:52:06
@_author: David Wagner 
@_subject: building a true RNG 
Yes, sorry.
No, my point is stronger.  It's hard to justify even in the standard
"security against computationally-bounded adversaries" model.  I know
of *no* theoretically-rigorous justification for any practical entropy
sampling procedure without making unreasonable and untestable assumptions
about the input distribution, except in the random oracle model.

@_date: 2002-07-29 10:00:23
@_author: David Wagner 
@_subject: building a true RNG 
Oh dear.  On re-reading your message I now suspect that what you asked
is not what I originally thought you asked.  I see two questions here:
  Q1: If we cycle through all N-bit messages, are all
      2^N output values possible?
  Q2: If we cycle through all messages (possibly very long
      or very short), are all 2^N output values possible?
On first reading I thought you were asking Q1, but it now occurs to me
you were probably asking Q2.  I apologize profusely for any confusion
I may have caused.
Anyway, the answer to Q1 is likely to be "No".  I'd guess that the answer
to Q2 is probably "Yes", or close to it.
For the first scenario, if the hash behaves like a random oracle,
then one would expect only 2^N * (1 - 1/e) of the possible outputs to
be attainable.  Of course, the forbidden outputs are not easy to find;
the best way I know is by exhaustive search over all 2^N N-bit messages.
For the second scenario, if the hash behaves like a random oracle, then
one would expect all outputs to be attainable.  No significant deviation
from the random oracle model is known for SHA1 (well, with one exception
that doesn't appear to be relevant here).  That said, this is not a proof,
and my answers just represent my best guesses.

@_date: 2002-07-29 10:45:39
@_author: David Wagner 
@_subject: building a true RNG 
Nitpick: the last statement does not seem quite right to me.  I'm thinking
of the notion of a one-way permutation.  For instance, the RSA function
f(x) = x^3 mod n is conjectured to be a one-way permutation, assuming n
is a RSA modulus with unknown factorization and the RSA assumption holds.
(I'm being a little fast and loose with the formal details, but I hope
this conveys the idea.)
That said, I'm not claiming that the RSA function would make a good
cryptographic hash function.  Cryptographic hash functions are expected
to be a lot more than just one-way and collision-free.
I don't know of any good cryptographic hash function that comes with
a proof that all outputs are possible.  However, it might not be too
hard to come up with plausible examples.  For example, if we apply the
Luby-Rackoff construction (i.e., 3 rounds of a Feistel cipher), with
ideal hash functions in each round, does this have the desired properties?
It might.
On the gripping hand, I don't think this is a real issue in practice.
SHA1 is probably good enough for all practical purposes that I can
think of.

@_date: 2002-07-29 12:43:48
@_author: David Wagner 
@_subject: building a true RNG 
However, when used in this way, DES is not an especially good hash function.
For instance, it is easy to find collisions, to find pre-images, and so on.

@_date: 2002-07-29 13:26:55
@_author: David Wagner 
@_subject: building a true RNG 
One standard method is to use Davies-Meyer mode with a block cipher that
has a very strong key schedule and has a sufficiently large block size
(at least 128 bits).  I'm not sure I'd recommend doing this with AES,
as I'm not sure how well studied AES's key schedule is.  Personally,
if I had a choice, I'd prefer hash functions like SHA1, but if that's
not an option, Davies-Meyer might be a reasonable alternative.

@_date: 2002-07-29 14:48:05
@_author: David Wagner 
@_subject: building a true RNG 
Can you elaborate?  What would the computational complexity of this be?
Estimating the size of {x : f(h(x))=1} to within relative error e
requires O(1/e^2) evaluations of h, if I understand correctly.  If we
consider the difference between a hash function that can hit all of
S, and another hash function that misses only one element of S, we'll
need to resolve the size of these sets to within relative error 1/|S|.
That suggests we'll need |S|^2 evaluations of h, which is infeasible
for SHA1 and slower than the naive O(|S|) algorithm.

@_date: 2002-07-29 14:55:36
@_author: David Wagner 
@_subject: building a true RNG 
I believe you are referring to the state compromise attacks
described in the following paper:
  J. Kelsey, B. Schneier, D. Wagner, C. Hall,
  "Cryptanalytic Attacks on Pseudorandom Number Generators",
  FSE'98.  I once wrote a short note about the relevance of this to IPSec:

@_date: 2002-07-29 15:18:10
@_author: David Wagner 
@_subject: building a true RNG 
theoretically perfect "random oracle"
Well, I know this particular point wasn't central to your email, but
I'm not sure I agree with you on this small point.  I believe it should
be more or less straightforward to analyze the entropy preservation of
a random oracle (alas, so straightforward you probably won't find any
paper on it in the literature).
The problem can really be divided into two parts:
  1. Is our entropy crunching algorithm secure when used with
     a random oracle instead of SHA1?
  2. Does SHA1 behave enough like a random oracle that the answer
     to question 1. is in any way relevant to the real world?
I suspect that question 1. is not hard to answer in a formal, rigorous,
provable way.  It is only question 2. that is hard to answer.  It is
absolutely true that we have no proof for question 2.
That said, we should keep in mind that none of our cryptographic
algorithms (even 3DES) come with a proof of security.  All we have to
rest on is years of unsuccessful cryptanalysis.  This is true for 3DES,
and this is true for SHA1 (albeit less so than for DES).  Our faith
in SHA1 is based on the fact that years of analysis have found no
significant evidence that SHA1 doesn't behave like a random oracle,
and the hope that no new cryptanalytic breakthrough will be found.
We can also question the random oracle model.  The random oracle model
has problems.  However, for entropy generation, it's the best thing
we've got, and (as I argued in a previous post) there are good reasons to
believe that there are fundamental barriers to sound proofs of security
in the standard computational-theoretic model.
So, yes, I agree that entropy generation has not been well-analyzed in
the literature.  I agree that there are fundamental reasons to think it
will be hard to analyze in the standard model.  I don't agree that there
are fundamental barriers to analyzing it in the random oracle model.
I think you're absolutely right to point out gaps in our theoretical
understanding of entropy crunching.
Well, this is probably tangential again, but I'm not sure you're using
the right definition of entropy.  Remember, in the random oracle model,
the bad guys also have oracle access to the hash function, just like the
good guys.  (We have to assume the bad guys can compute the hash of any
message they like.)  Hence, any useful definition of entropy should be
made in the context where the random oracle is known, but the entropy
inputs are unknown to the attacker.
In other words, you want something more akin to the conditional entropy.
Define random variables:
  I = the entropy input,
  R = the random oracle,
  O = the output of the entropy crunching algorithm.
Then I claim the right measure is somehow closer to the conditional
entropy H(O | R) than to the unconditional entropy H(O).  (Caveat:
I'm using language very sloppily and informally here.)
That said, "entropy" is almost always used in a very informal way, and
I'm not sure that the notion of entropy is 100% meaningful here once you
look at the details.  If you want to formalize this, you instead should
ask whether a computationally-bounded adversary (with oracle access to
R) can distinguish O from a true-random source.  If we followed that
formulation, I think we'd find that the security of the algorithm does
depend on the entropy of the input.  I haven't worked out the details,
so maybe it's not reasonable for me to speculate like this, but I think
this project is closer to a homework problem for a graduate student than
to a open research problem.
I'd love to see a paper giving a solid theoretical analysis of these
issues.  (On the other hand, if I tried to write one, I'm not sure I
could get it published...)

@_date: 2002-10-24 11:46:11
@_author: David Wagner 
@_subject: collision resistance -- Re: Why is RMAC resistant to birthday attacks? 
Umm, that's basically what I said in my previous message to the
cryptography mailing list.  But my terminology was better chosen.
In case 2, calling this "the internal collision probability" is
very misleading; there is no event whose probability is the inverse
of the square root of the size of the internal state space.
Again, this is nothing new.  This is all very basic stuff, covered
in any good crypto textbook: e.g., _The Handbook of Applied Cryptography_.
You might want to take the time to read their chapters on hash functions
and message authentication before continuing this discussion.

@_date: 2003-09-13 14:43:32
@_author: David Wagner 
@_subject: quantum hype 
Quantum crypto only helps me exchange a key with whoever
is on the other end of the fibre optic link.  How do I know
that the person I exchanged a key with is the person I wanted
to exchange a key with?  I don't ... unless I can make extra
assumptions (such as that I have a guaranteed-authentic channel
to the party I want to communicate with).
If I can't make any physical assumptions about the authenticity
properties of the underlying channel, I can end up with a scenario
like this: I wanted to exchange a key securely with Bob, but instead,
unbeknownest to me, I ended up securely exchanging key with Mallet.
I believe the following is an accurate characterization:
 Quantum provides confidentiality (protection against eavesdropping),
 but only if you've already established authenticity (protection
 against man-in-the-middle attacks) some other way.
Tell me if I got anything wrong.

@_date: 2004-12-15 08:35:40
@_author: David Wagner 
@_subject: The Pointlessness of the MD5 "attacks" 
I guess I disagree.  Imagine that the code has some block cipher with
some S-boxes hardcoded into it.  The code uses this block cipher to
decrypt an associated ciphertext and outputs (or takes some action based
on) the resulting message.  This is an example of code that could be
used to fool a MD5 checksum.  Moreover, I don't have a great deal of
confidence that even a careful code inspection would cause the code to
be considered suspicious.  Consequently, I don't have great confidence
that such an attack would be detected.
I know it is tempting to think that, look, Wang et al only found a pair
of random-looking messages that collide; they didn't claim to find a pair
of meaningful messages that collide; and maybe we can hope that there is
no way to come up with a pair of meaningful-looking colliding messages.
But I think that kind of hope is unfounded, and acting on hope is
asking for trouble.  I believe the only safe course now is to assume
that MD5's collision resistance is totally broken.  If Wang et al can
find meaningless-looking collisions today, it seems all too likely that
someone else may be able to find meaningful-looking collisions tomorrow.
Hoping that the latter will be hard even though the former is known to
be easy seems too optimistic for my tastes.

@_date: 2004-12-15 09:12:49
@_author: David Wagner 
@_subject: The Pointlessness of the MD5 "attacks" 
Where this argument breaks down is that someone might have partial
but not total control over the binary.  This partial control might
not be enough for them to distribute a malicious version straightforwardly,
but just enough to exploit a MD5 collision.  It is hard to be confident
that such an attack scenario is impossible.
To give one contrived example, imagine that the Windows 2010 binary
comes with an image file that is displayed as part of the splash start
screen.  Imagine that the graphic designer is allowed to supply that
image, but the graphic designer has no other authorized access to the
source or binary of Windows.  Now a disgruntled graphic designer might
be able to arrange to find a MD5 collision MD5(img1) = MD5(img2) so that
img1 looks like an entirely reasonable Windows splash screen, but img2
contains some scrawled epithet ("Tired of Windows crashing all the time?
Try Linux!").  Or, even more contrived, imagine that img1.jpg looks
like a completely normal JPG file, but img2.jpg exploits some buffer
overrun in the startup screen's JPG decoder to overwrite the program's
image with some other malicious code.
Sure, these scenarios are contrived and unlikely.  But how do you
know that there is not some other (possibly more complex but less
contrived) scenario that you would consider more troubling?
I've got a better challenge: show me a convincing argument that no such
scenario exists.
What I'm trying to get at is that you've got the burden of proof
backwards.  Implicit in your challenge is the idea that we should
keep trusting MD5 until someone finds a convincing argument that it is
insecure in practice.  My argument is that this is much too trusting.
I believe that, given the theoretical results on MD5, we should not have
any trust whatsoever in the security of MD5 as a collision-resistant
hash until someone is able to offer a convincing argument that MD5 is
secure enough in practice despite its known weaknesses.
I could try to answer your challenge.  I might even be able to devise
some solution to your challenge that would satisfy you.  For instance,
maybe the image file attack above qualifies as a solution.  Or maybe
the S-box table attack in my previous email is good enough.  But I don't
really want to argue about whether I have found a valid answer to your
challenge.  I shouldn't be required to meet that burden -- the burden
of proof should be on whoever wants to believe that MD5 is secure.
Why should the burden be on MD5 defenders?  Not just because I said so.
Part of the reason is that there are just too many complex scenarios
to consider.  Suppose I conceded that I couldn't find a scenario you'd
accept.  What would that prove?  Very little.  Even if I can't think of
a suitable scenario for you off the top of my head, that doesn't mean
that with more thought I wouldn't find one.  Even if I spent a month
trying and still couldn't find one, that doesn't mean that others can't.
My experience is that if it is possible to find a theoretical attack with
one day's work, it is often possible to extend this to a more practical
attack with, say, one week's work.  Bruce Schneier puts this concisely:
"Attacks always get better."  Trusting in MD5's collision-resistance
amounts to assuming that "cryptanalysts of MD5 will get this far, but
no farther", and that seems like a pretty questionable assumption to me.

@_date: 2004-12-22 14:28:41
@_author: David Wagner 
@_subject: SSL/TLS passive sniffing 
[which says: "If you're going to use /dev/urandom then you might
    as well just not encrypt the session at all."]
That claim is totally bogus, and I doubt whether that poster has any
clue about this subject.  As far as we know, Linux's /dev/urandom is just
fine, once it has been seeded properly.  Pay no attention to those who
don't know what they are talking about.
(That poster wants you to believe that, since /dev/urandom uses a
cryptographic-strength pseudorandom number generator rather than a
true entropy source, it is useless.  Don't believe it.  The poster is
confused and his claims are wrong.)

@_date: 2004-11-30 19:22:29
@_author: David Wagner 
@_subject: SSL/TLS passive sniffing 
This sounds very confused.  Certs are public.  How would knowing a copy
of the server cert help me to decrypt SSL traffic that I have intercepted?
Now if I had a copy of the server's private key, that would help, but such
private keys are supposed to be closely held.
Or are you perhaps talking about some kind of active man-in-the-middle
attack, perhaps exploiting DNS spoofing?  It doesn't sound like it, since
you mentioned passive sniffing.
And it doesn't matter whether you use Diffie-Hellman or RSA with Verisign
certs; either way, SSL should be secure against passive eavesdropping.
I think you need to elaborate before we can give any sensible responses.

@_date: 2004-11-30 19:25:26
@_author: David Wagner 
@_subject: SSL/TLS passive sniffing 
No, that is not accurate.  Diffie-Hellman is also insecure if the "private
key" is revealed to the adversary.  The "private key" for Diffie-Hellman
is the private exponent.  If you learn the private exponent that one
endpoint used for a given connection, and if you have intercepted that
connection, you can derive the session key and decrypt the intercepted
Perhaps the distinction you had in mind is forward secrecy.  If you use
a different "private key" for every connection, then compromise of one
connection's "private key" won't affect other connections.  This is
true whether you use RSA or Diffie-Hellman.  The main difference is
that in Diffie-Hellman, "key generation" is cheap and easy (just an
exponentiation), while in RSA key generation is more expensive.

@_date: 2004-08-31 21:10:32
@_author: David Wagner 
@_subject: More problems with hash functions 
Doesn't work, alas.  A trivial adjustment to Joux's attack also defeats
your proposal.
Suppose M1 and M1' collide on the "blank initial state".  Let M2 be
arbitrary.  Then M1|M2 and M1'|M2 collide, and the final state after
processing them is  in both cases.  Now find messages M3 and
M3' that collide if processed starting from the common state .
Then you have four 3-block collisions for the cost of two: M1|M2|M3,
M1'|M2|M3, etc.
With this small tweak, Joux's attack will go through.  So, your scheme
offers little or no additional resistance to Joux's attack.

@_date: 2004-09-01 13:19:09
@_author: David Wagner 
@_subject: ?splints for broken hash functions 
This does not add any strength against Joux's attack.  One can find
collisions for this in 80*2^80 time with Joux's attack.
First, generate 2^80 collisions for the top line.  Find B1,B1* that
produce a collision, i.e., C(IV,B1)=C(IV,B1*)=V2.  Then, find B2,B2*
that produce a collision, i.e., C(V2,B2)=C(V2,B2*)=V3.  Continue to
find B3,B3*, ..., Bk,Bk*.  Note that we can combine this in any way
we like (e.g., B1, B2*, B3*, B4, .., Bk) to get 2^80 different messages
that all produce the same output in the top line (same H1).
Next, look at the bottom line.  For each of the 2^80 ways to combine
the above blocks, compute what output you get in the bottom line.
By the birthday paradox, you will find some pair that produce a
collision in the bottom line (same H2).  But that pair also produces
a collision in the top line (since all pairs collide in the top line),
so you have a collision for the whole hash (same H1,H2).
The same attack applies.  This construction is not secure against
Joux's attack, either.

@_date: 2004-09-08 13:37:22
@_author: David Wagner 
@_subject: Seth Schoen's Hard to Verify Signatures 
This signature scheme has an interesting property: once you've done
the lengthy computation, you can quickly prove to someone else that
the signature is valid.  In particular, there is a short and easy to
prove that the signature is valid, based on listing x, x^2, x^4, x^16,
x^256, x^65536, ..., x^e and giving a zero knowledge proof that your
list is correct.  The details can be found here (see esp. Section 3):
  D. Boneh, M. Naor, "Timed Commitments", CRYPTO 2000.
  The signer can also create a proof like this if he wants.  Note that
Boneh and Naor consider a somewhat similar but not equivalent notion of
timed signatures, which may also be of interest.

@_date: 2004-09-09 00:11:17
@_author: David Wagner 
@_subject: Seth Schoen's Hard to Verify Signatures 
Ahh, yes, I see.  You're absolutely right.  I was too quick to jump
to conclusions, but your argument convinces me.  I withdraw my claim
about the verifier being able to convince others of the validity of
the signature -- sorry about that.  Thanks.

@_date: 2005-12-04 16:44:18
@_author: David Wagner 
@_subject: RNG implementations and their problems 
This is a security feature.  If non-root programs could write to
entropy count and harm others on the system who rely on /dev/random.
By the way, rngd already does pretty much what you want.  Have you
looked at it?
It would be pretty easy to hack egd or prngd to periodically feed
the entropy they have gathered into /dev/random, using the appropriate
ioctl()s and root-level access.  Seems like that would be good enough.
It would also be trivial to write a 'cat'-like program that takes
data on stdin and uses the appropriate ioctl()s to write it to /dev/random.
Problem solved.
But I am skeptical that this problem is very widespread.  I doubt there
are many people who have found this to be a barrier.
I would also question why you are using /dev/random.  For most purposes,
Few applications truly need /dev/random.  Those applications that do need
for them, the "deplete the pool" behavior seems like the right semantics.
It is certainly true that there are many poorly-thought out applications
that use /dev/random even though /dev/urandom would have been a
better choice.  However, I just can't get too bent out of shape if
those applications suffer as a result of their questionable choice.
Those applications will just have to deal with the consequences of
their misdesign.  If it becomes a problem, maybe that will be sufficient
motivation for them to reconsider their use of /dev/random and switch
over to /dev/urandom, like they should have done in the first place.
Anyway, on the question of whether to use write() or ioctl() to update
the entropy pool from user land, my suspicion is that the current
semantics just hasn't been a very big problem for anyone, and so no
one has cared enough to bother writing code to change the behavior.
It's also not clear to me that current interface is problematic or
that your suggestion is a better choice.  But in any event, if you
are motivated enough to try to write code and submit patches that
would implement your preferred solution, you should probably take this
discussion over to the linux-kernel mailing list.
I'm not sure what this is referring to.  As far as I know, /dev/{u,}random
doesn't fail in silent or unsafe ways at runtime.

@_date: 2005-12-22 13:13:13
@_author: David Wagner 
@_subject: RNG quality verification 
Go tell whoever wrote your requirements that they (to be frank) don't
know what they're talking about.  What they're asking for doesn't make
any sense.  You should ask them what problem they're trying to solve.
Don't let them try to tell you how to solve it; you just need to know
the goal, not the mechanism.
The standard solution is to just not worry about this at all, and say
that it is the user's responsibility to choose good random numbers.
If the user fails to do so, they're the one who bears the costs of their
failure, so why should you care?
If the goal is to hold the hands of your users, then you might want to
think carefully about whether you want to be in that business, what are
the most likely failure modes, and what is the best way to deal with it.
(Trying to check whether their numbers are random probably isn't the best
answer.)  Most CA's have gravitated towards the opinion that that's not
something they can control, nor do they want to, nor should they -- and
that sounds reasonable to me.  But if you want to be in the hand-holding
business, you're going to have to do an awful lot more than just check
the random numbers.

@_date: 2005-01-09 15:53:00
@_author: David Wagner 
@_subject: Entropy and PRNGs 
I guess I have to take exception.  I disagree.  I think Ben Laurie's
paper is quite good.  I thought your criticisms missed some of the points
he was trying to make (these points are subtle, so this is completely
understandable).  Presumably his paper could be criticized as not clear
enough, since it seems it did not convey those points adequately, but
I don't think his paper is inaccurate.  I'll respond point-by-point.
Actually, I think Ben got it right.  Entropy depends on context.
The attacker might have extra context that allows him to narrow down
the possible values of the randomness samples.
For instance, imagine if we use packet inter-arrival times (measured down
to the nanosecond) as our randomness source.  From the point of view of
an outsider, there might a lot of entropy in these times, perhaps tens
of bits.  However, from the point of view of an attacker who can eavesdrop
on our local area network, there might be very little or no entropy.
This is the difference between unconditional and conditional entropy that
Ben was trying to introduce.  In information-theoretic notation, H(X)
vs H(X|Y).  Let X = packet inter-arrival time, and Y = everything seen by
a local eavesdropper, and you will see that H(X|Y) can be much smaller
than H(X).  Indeed, we can have H(X|Y) = 0 even if H(X) is very large.
This is Ben's point, and it is a good one.
Conditioned on everything known to the attacker, of course.
A counter is fine as long as there is only one machine in the universe
that will ever assign UUIDs.  However, if you want to do distributed
generation of UUIDs, then counters are insufficient because there is no
way to prevent overlap of two machine's counter spaces.
Perhaps what Ben should have said is that:
* Unconditional entropy is sufficient for UUIDs;
  conditional entropy is not needed.
* For centrally-assigned UUIDs, even unconditional entropy is unnecessary;
  a centrally-managed counter is fine.
* For distributed, unsynchronized assignment of UUIDs, unconditional
  entropy appears to be necessary and sufficient.
Ok, this seems like a fair criticism.
Another reasonable point.  Perhaps truerng would be a better name, then?

@_date: 2005-01-10 12:34:12
@_author: David Wagner 
@_subject: Entropy and PRNGs 
Ok.  I see that you were already well aware of the point Ben Laurie
was making, and indeed it was obvious to you.  Great.
But I have seen people for who this was definitely not obvious, and
who failed to recognize the distinction between the two concepts or
the need to use conditional entropy until it was pointed out to them.
I guess Ben's paper is going to be useful for them, but not for you.
You're right.  I take it back.  I accept your point about UUIDs.
There are schemes that avoid the need for randomness (entropy).
Thank you.

@_date: 2005-01-10 20:33:41
@_author: David Wagner 
@_subject: Simson Garfinkel analyses Skype - Open Society Institute 
In article <41E07994.5060004 at systemics.com> you write:
The answer appears to be, "no one knows".  The report accurately reports
that because the security mechanisms in Skype are secret, it is impossible
to analyze meaningfully its security.  Most of the discussion of the
potential risks and questions seems quite good to me.
But in one or two places the report says things like "A conversation on
Skype is vastly more private than a traditional analog or ISDN telephone"
and "Skype is more secure than today's VoIP systems".  I don't see any
basis for statements like this.  Unfortunately, I guess these sorts of
statements have to be viewed as blind guesswork.  Those claims probably
should have been omitted from the report, in my opinion -- there is
really no evidence either way.  Fortunately, these statements are the
exception and only appear in one or two places in the report.
All in all, a useful analysis.  Thanks for posting that.

@_date: 2005-01-27 15:22:09
@_author: David Wagner 
@_subject: Simson Garfinkel analyses Skype - Open Society Institute 
I don't buy it.  How do you know that Skype is "more secure", let alone
"vastly more private"?  Maybe Skype is just as insecure as those other
systems.  For all we know, maybe Skype is doing the moral equivalent
of encrypting with the all-zeros key, or using a repeating xor with a
many-time pad, or somesuch.  Without more information, we just don't know.
I'm sorry to pick nits, but I have to stand by my statement.  No matter
how atrociously bad other systems may be, I don't see any basis for saying
that Skype is any better.  It might be better, or it might be just as bad.
We don't know.

@_date: 2005-06-08 17:27:16
@_author: David Wagner 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
Because the page you downloaded in the clear contains the https: URL
in the post method.  How do you know that this is the right URL?  If
you got the page in the clear, you don't.  An attacker who can provide
a spoofed page (by DNS cache poisoning, "pharming", MITM attacks, or
any other method) could substitute a post URL that sends your sensitive
data to hackers-r-us.com.
That said, I don't see how adding an extra login page to click on helps.
If the front page is unencrypted, then a spoofed version of that page
can send you to the wrong place.  Sure, if users were to check SSL
certificates extremely carefully, they might be able to detect the funny
business -- but we know that users don't do this in practice.
Dan Bernstein has been warning of this risk for many years.
 at cr-yp-to
 at cr-yp-to
As far as I can tell, if the front page is unencrypted, and if the
attacker can mount DNS cache poisoning, "pharming", or other web spoofing
attacks -- then you're hosed.  Did I get something wrong?

@_date: 2005-03-06 22:39:06
@_author: David Wagner 
@_subject: comments wanted on gbde 
I took a look at that paper.  (I seem to recall looking briefly at actual
GBDE code two or three years ago, but I have since forgotten everything
I once knew about GBDE, so I was forced to read the paper and start
from scratch.)  Here are some thoughts.  Warning: they are based solely
on reading the above paper.  I have not put a great deal of thought into
this, nor have I examined any source code, so take everything with many
grains of NaCl.
My sense: GBDE isn't built the way I would have chosen to, and there are
many aspects that I consider inelegant, but it doesn't look obviously
broken.  If you use it appropriately, it looks fairly reasonable, as far
as I can tell.  I would be willing to use it for my own personal secrets.
I believe it is possible to use it in a way such that it will provide
adequate security.
However, I also believe it is possible -- and, perhaps, all too easy --
to use GBDE in a way that will not provide adequate security.  My biggest
fear is that safe usage is just hard enough that many users will end up
being insecure.  GBDE uses a passphrase to encrypt the disk.  If you can
guess the passphrase, you can decrypt the disk.  Now in theory, all we
have to do is tell users to pick a passphrase with at least 80 bits of
entropy.  However, in practice, this is a pipe dream.  As we know, users
often pick passphrases with very little entropy.  Practices vary widely,
but for many users, an estimate in the range of 10-40 bits is probably
reasonable.  Consequently, dictionary attacks are a very serious threat.
GBDE does not take any steps to defend against dictionary attacks.
In GBDE, a passphrase with b bits of entropy can be broken with 2^b AES
trial decryptions.  This means that people who are using passphrases
with only 10-40 bits of entropy may be screwed -- their data will not
be secure against any serious attack.  40-bit security is a very weak
level of protection.
So, this seems like it could be a real concern, depending on how GBDE
is used and how much security you need.  If you can train users to use
strong passphrases, GBDE should be fine, I would think.  For instance,
human rights workers might be willing to deal with the usability costs of
very long passphrases.  However, many users may not have enough discipline
to use very strong passphrases, and those users might be at risk.
The GBDE paper has a rather peculiar comment about dictionary attacks.
It mentions the possibility of using password stretching/strengthening
techniques (e.g., PKCS iteration to slow down dictionary attacks,
etc.), but rejects them on the basis that they are not able to ensure
perfect security in all cases.  That strikes me as a rather weird stance.
They in no case make things worse, and in many cases may provide a
measurable improvement.  I agree that such mechanisms are imperfect,
and there are severe limits on how much they can help, but they still
seem better than doing nothing about dictionary attacks.  This strikes
me as an unfortunate aspect of GBDE.
A second possible critique of GBDE is that the design seems a little
strange in several places.  There is no clear explanation of the threat
model GBDE is intended to protect against, or the security goals it is
intended to achieve.  The design is unnecessarily complex and baroque.
I should elaborate.  The threat model: What can we assume about the
attacker's capabilities?  What kinds of attacks are we trying to
defend against?  One scenario is where a hard disk is stolen, the
attacker gains access, and wants to read the data.  But what about
other threats?  The paper mentions the "cleaning lady" threat, where
an attacker gains physical access to the hard disk at multiple times,
without being detected.  Do we want to defend against such attacks?
If we do, we need chosen-ciphertext security, because otherwise the
attacker might be able to tamper with data on the disk and thereby learn
information about the plaintext.  The GBDE paper does not clearly say
whether GBDE is intended to design against this threat model, and I have
serious doubts about whether it would be secure against such a threat;
the modes are certainly not chosen-ciphertext secure.  So can we assume
we do not need to defend against such threats, and that if the attacker
ever gains access to the hard disk, then we will learn of this fact and
never use the hard disk again?  As another example, can we assume that
the communication between the computer and the hard disk is physically
secure, or might GBDE be used with NFS (for example)?
Also, the security goals are never precisely stated.  Is the only goal to
protect the confidentiality of stored data?  Do we also want to prevent
"traffic analysis", i.e., to prevent the attacker from identifying
recently-modified data?  Do we also want to protect the integrity of
the data?  I would expect that GBDE only achieves the first of these three
goals, but I haven't tried to analyze anything other than confidentiality.
A third potential complaint is that the design is unnecessarily complex.
GBDE invents new modes of operation, uses 1024-bit keys, and does other
strange things.  My feeling is that all of this is unnecessary and only
contributes to complexity.  It would have been simpler to just use AES,
rather than inventing new modes.  Complexity is a risk, because it
increases the chances of subtle mistakes due to interactions between
different pieces of the system.  New modes are also a risk.  The paper
talks about fears that AES might be insecure if used to encrypt a lot of
data, but I believe those fears are misplaced, and I suspect the risk
of greater complexity outweighs the possibility of successful attacks
on AES.  So the GBDE design is not exactly as I would have chosen,
if I were designing it.
That said, despite the unnecessary complexity and slightly baroque
design, I couldn't see any weaknesses introduced by this complexity.
The modes looked like they will probably be ok to me.  I would be very
surprised if GBDE gets broken by attacking the crypto head-on; attacks
on the crypto just don't seem like the most likely failure mode for GBDE.
A bigger concern is the implementation cost of this complexity.
More complex algorithms means you need more code, and that is more code
that you can get wrong.  A serious review of GBDE would need to look at
the implementation, but I did not do that.
Another potential risk is how carefully the keys are handled.  Are keys
zeroized quickly enough?  Could there be traces of a key left laying
around somewhere?  Could memory data structures (whether in kernel memory,
or in user memory) that hold a key get swapped out?  How careful are
the userland applications about handling data?  What happens when you
suspend a laptop to disk; do keys get copied to disk?  How carefully is
plaintext treated after it is decrypted (or before it is encrypted)?
Will applications that handle plaintext make copies into temporary
files, and if so, will those temporary files also be stored on an
encrypted partition?  How are backups made and protected?  And so on.
You get the idea.
In summary, the three biggest risks seem to be 1) dictionary attacks; 2)
unsafe use of the system, due to insufficient clarity about what GBDE can
and cannot achieve (e.g., threat models, security goals, etc.); 3) the
possibility of implementation flaws.  Unnecessary complexity could also be
a concern for some people, but I could not see any obvious problems there.
That said, despite all my criticisms above, I believe that, for the most
part, GBDE is a pretty reasonable design.  It may not be the most elegant
design possible, but if used appropriately, I believe it can provide a
useful level of security.  It is definitely not perfect, but in many
ways, it seems to be "Pretty Good".  In the grand scheme of things,
using GBDE is probably a heck of a lot better than doing nothing.
I am also mindful of the fact that Poul-Henning Kamp has actually built
the thing.  It is one thing to sit on the sidelines and take potshots
at a design; it is another thing entirely to put in the time and energy
to design, implement, and support a complete disk encryption system.
I have great respect for those who take the time to implement crypto
in a way that is usable, and I think we should be grateful to phk for
his efforts.  He has done the world a service in building GBDE.

@_date: 2005-03-21 11:26:59
@_author: David Wagner 
@_subject: Propping up SHA-1 (or MD5) 
Yes.  Suppose we use this for signing.  The crucial part is to have
the *signer* choose the Random value when computing the signature.
This may be secure even if H fails to be collision-resistant, because
even if an attacker finds a collision for H, he doesn't know which
Random value the signer is going to use.
More generally, we could try to use any universal one-way hash function
(UOWHF).  This concept is also known as target collision resistant (TCR).
It is natural to conjecture that H' is a UOWHF, i.e., is TCR, and this
may be true even if H is not collision-resistant.  Of course, there is
no proof of this, and this conjecture is speculative, but it does weaken
the assumptions we are making about our hash.
I have been advocating this kind of construction ever since hearing about
the hash cryptanalysis results last August.  Not everyone agrees with me,
and there is a lengthy discussion going on about this on the IRTF CFRG
working group.
      No, not if you use it right.  The way to use this is to have the signer
choose the value of Random, not anyone else.  A signer can play with Random
and maybe find collisions M,M', but in this case the signer will be viewed
as having signed both M and M', so this doesn't help the signer at all.
Shouldn't be a big deal for signing.  A much bigger deal is that this
changes the on-the-wire format.

@_date: 2005-03-25 08:47:46
@_author: David Wagner 
@_subject: What is to be said about pre-image resistance? 
Well, I'm not sure that the difference between 2^160 and 2^149
would be very significant in practice, even if there were some
redunction like this, but--
As far as I can tell, the pre-image resistance of SHA1 has not been
significantly threatened by these attacks, or at least, the authors
do not claim any results on pre-image resistance of SHA1.

@_date: 2005-03-25 08:55:32
@_author: David Wagner 
@_subject: Secure Science issues preview of their upcoming block cipher 
Well, that is completely non-responsive to the point Adam made.
You used the term "provably".  Where is your proof?
Did you understand the point Adam is making?  In this field, the term
"provably" means that there you have a mathematical proof.  Do you have
such a proof?  I'm awfully skeptical....
Will you retract the claim that SS2 is "provably just as secure as AES-128"?
As for your future hashes, will you be making similar claims?

@_date: 2005-03-25 09:02:40
@_author: David Wagner 
@_subject: Secure Science issues preview of their upcoming block cipher 
Actually, I think Adam is totally right.
Have you looked at their scheme?
  The way to come up with a cipher provably as secure as AES-128 is to use
AES-128 as part of your cipher -- but their scheme does not do anything
like that.
I am very skeptical about claims that they have a mathematical proof that
CS2-128 is as secure as AES-128.  I want to see the proof.

@_date: 2005-05-21 11:17:04
@_author: David Wagner 
@_subject: Microsoft info-cards to use blind signatures? 
What a strange criticism of Microsoft!  Here is something to know about
patents: many companies file patents all the time.  That doesn't mean
they are committing to build a product around every patent they file.
The fact that Microsoft hasn't pursued patent 5,768,385 tells you
essentially nothing about what they are going to do with this patent.
I wouldn't take patent filings as an indicator of intent or of future
business strategy.

@_date: 2005-05-21 14:53:20
@_author: David Wagner 
@_subject: DTV Content Protection (fwd from cripto@ecn.org) 
The final version of that paper is at
Quoting from the paper's conclusion section:
  To recover the center's master secret, an attacker needs 40 key pairs,
  and we point out a variety of ways to get them.  An attacker can reverse
  engineer 40 different HDCP video software utilities, he can break open
  40 devices and extract the keys via reverse engineering, or he can
  simply license the keys from the trusted center.  According to the
  HDCP License Agreement, device manufacturers can buy 10000 key pairs
  for $16000.  Given these 40 spanning keys, the master secret can be
  recovered in seconds.  So in essence, the trusted authority sells a
  large portion of its master secret to every HDCP licensee.
The $16,000 figure is taken from page 21 of
Of course, you have to sign an NDA, too, but I'm not sure whether
that would deter a serious bad guy.
So, in effect, the trusted center has agreed to sell its master secret
for $16,000 and a promise.
Thank you for your post.  It was chock-full of interesting information --
particularly the bits about DTCP, which I had never seen before.

@_date: 2005-11-16 13:10:54
@_author: David Wagner 
@_subject: timing attack countermeasures (nonrandom but unpredictable delays) 
Sadly, I don't think this works.
In many cases, the observed time depends both on the input and on some
other random noise.  In such cases, averaging attacks that use the same
input over and over again will continue to work, despite the use of
a pseudorandom input-dependent delay.  For instance, think of a timing
attack on AES, where the time compute the map X |--> AES(K,X) depends only
on K and X, but where the measured time depends on the computation time
(which might be a deterministic function of K and X) plus the network
latency (which is random).  Indeed, in this example even the computation
time might not be a deterministic function of K and X: it might depend
on the state of the cache, which might have some random component.
And there are many settings where you can average across many slightly
different inputs.
So I don't think this kind of approach is likely to anywhere, except
possibly in a few specialized settings.
In many cases, a better defense than random delays is to make the total
time constant and independent of the inputs.  Then averaging is pointless.
It's not always possible, but when it is, it's worth considering.

@_date: 2005-09-15 09:20:25
@_author: David Wagner 
@_subject: MIT talk: Special-Purpose Hardware for Integer Factoring 
The phrase "joint work with XXX" means that this was a collaboration
between XXX and the speaker.  If DJB wasn't part of the collaboration,
then of course he wouldn't be on that list.
This is different from a "related work" section, where one would cite
prior research.  But "joint work with" is not a citation; it's like
the list of authors on a paper.

@_date: 2005-09-19 15:32:24
@_author: David Wagner 
@_subject: Defending users of unprotected login pages with TrustBar 0.4.9.93 
But this isn't enough.  The only way for a user to be secure against such
attacks is to type in a https:-style URL into the address bar directly, or
to load a https:-style URL from a bookmark.  Users have to always remember
to type in  they must never use or they will be insecure.  Training users to follow this discipline is not
a trivial task.
I'm not sure it is fair to blame this solely on the web sites.
The problem is that the https: model for web security is broken, if
attackers are mounting active attacks, DNS spoofing, and other kinds of
man-in-the-middle attacks.  The problem is not with SSL; the problem is
with the model for how SSL is applied to solve the web security problem,
and with the user interaction model.  Fixing this probably requires
changes to web browsers and/or web servers.  So, a Hall of Shame seems
a little over the top to me, since there is no obvious way that the
web site could fix this on its own.
TrustBar's solution to this conundrum is a nice one.  I like it.
But it does require changing the web browser.
One thing that web sites could do to help is to always make
 work just as well as  and
then browser plug-ins could simply translate  ->
 for all sensitive sites.  Of course, web site
operators may be reluctant to take this step on performance grounds.

@_date: 2006-08-14 09:50:38
@_author: David Wagner 
@_subject: Solving systems of multivariate polynomials modulo 2^32 
Here is a trick that should solve these kinds of equations extremely
quickly.  First, you solve the system of equations modulo 2.  Then, for
each mod-2 solution, you try to extend it to a solution modulo 4.  Then,
for each mod-4 solution, you extend it to a solution modulo 8.  And so on.
This is sometimes known under the name "Hensel lifting".
Here's an example.  Suppose we have the equations:
    x*y + z       = 1
    x^3 + y^2 * z = 1
    x + y + z     = 0
Step 1: Find all solutions modulo 2.  This is easy: you just have to try
2^3 = 8 possible assignments and see which one satisfy the equations.  In
this case, the only solution is x=0, y=1, z=1 (mod 2).
Step 2: Find all solutions modulo 4.  This is again easy: since we know
x=0 (mod 2), then the only two possibilities for x mod 4 are 0 or 2.
Likewise, there are only two possibilities for y mod 4 and z mod 4.
Trying all 2^3 = 8 possible assignments, we find that the only two
solutions are x=0, y=3, z=1 (mod 4) and x=2, y=1, z=1 (mod 4).
Step 3. Find all solutions modulo 8.  First, take the mod-4 solution
x=0, y=3, z=1 (mod 4) and try extending it all 2^3 possible ways, to
see which of them lead to mod-8 solutions.  Then, take the other mod-4
solution x=2, y=1, z=1 (mod 4) and try extending it all 2^3 possible
ways, too.  The result is the set of mod-8 solutions.
Step 4. Find all solutions modulo 16.  etc.
We see that this requires performing 32 of these steps.  On average, we
expect about one feasible solution to remain possible after each step
(though this number may vary).  Each step requires testing 8 possible
assignments, times the number of assignments from the prior step.  Thus,
on the average case we can expect this to run very fast.
Note that this only works for Z_{2^32}.  It doesn't work for GF(2^32).

@_date: 2006-12-23 18:52:33
@_author: David Wagner 
@_subject: Startup to launch new random number generator from space 
Udhay Shankar reports:
Heh heh.  Pretty amusing.  I guess the founders haven't really thought
this through.  One problem with such a service, of course, is total
reliance upon Yuzoz: Yuzoz learns all your secret keys -- and so does
any hacker who figures out how to break into Yuzoz's servers.  That doesn't
sound like such a great deal -- especially considering that high-quality
random-number sources are not that hard to come by.
I guess we can take ill-conceived startups like this as a sign of
increasing awareness about the security risks and the need for security
solutions, even if there is some, err, lack of sophistication about how
to distinguish good security technology from bad.  (Quantum crypto seems
like another one for that camp.  Oracle's "Unbreakable" marketing slogan
was another good one.)

@_date: 2006-02-12 19:33:07
@_author: David Wagner 
@_subject: GnuTLS (libgrypt really) and Postfix 
This just shows the dangers of over-generalization.
Of course, we have to decide which is more important: integrity,
or availability.  I suspect that in the overwhelming majority (perhaps
all) of the cases where libgcrypt is used, integrity is more important
than availability.  If that is true, well, if in doubt, it's better to
fail closed than to fail open.
You rightly points out that there are important applications where
availability is more important than integrity.  However, I suspect
those cases are not too common when building Internet-connected desktop
I think the attitude that it's better to die than to risk letting an
attacker take control of the crypto library is defensible, in many cases.
Of course, it would be better for a crypto library to document this
assumption explicitly than to leave it up to users to discover it the
hard way, but I would not agree with the suggestion that this "exit before
failing open" stance is always inappropriate.

@_date: 2006-07-05 00:11:00
@_author: David Wagner 
@_subject: Irish eVoting Vetoed 
Agreed.  I think the quality of the technical analysis in the report
is a disappointment.  The report states the Commission's opinion that
the voting machine can be trusted.  However, the technical content
of the report is, in my opinion, starkly at odds with that conclusion:
  1) The report discloses several vulnerabilities that ought to raise
  questions about the security of the system.
  2) Moreover, there are frank admissions of major gaps in the
  Commission's analysis.  These revelations suggest to me that
  there is, at present, no rational basis for confidence in the
  correct operation of these voting machines.
After reading the report, I suspect that no one knows whether the system
is trustworthy or not (and that includes the Commission and all the
Commission's experts).  To be clear, I am not claiming that the system
is known to be untrustworthy; but neither is it known to be trustworthy.
It seems to me that the technical material in the report would better
support the conclusion that there is no convincing evidence that the
voting machines are fit for purpose.  I find the report's defense of the
voting machines unpersuasive.  It is puzzling to me why the Commission
is willing to recommend the voting machines.  I feel bad for any Irish
citizens who may be forced to rely on these machines in their election.
Let me share some quotes from the report that stuck out for me, along
with some commentary discussing my reaction to those quotes:
``The Commission has not conducted a line-by-line review of the software
embedded in the voting machine.'' -- p.60
  My comments: This is a confession that the Commission has no idea
  whether the system is trustworthy or not.  For all we know, a Trojan
  horse or malicious logic could be hidden somewhere in the source
  code.  The only way to detect such malicious code is by inspecting
  the source code.  When some of the source code is left uninspected,
  we have no way of knowing whether that uninspected part of the code
  might contain malicious logic.  Because the software is written in
  C, a single piece of malicious logic hidden anywhere in the code can
  subvert the working of the entire system.  Consequently, there is no
  rational basis for confidence that the system is free of backdoors.
  The Commission has no idea whether these voting machines might contain
  hidden backdoors -- and neither do I.
``further analysis, investigation and testing, and possibly amendment
of the C code [embedded in the voting machine] will be required.'' -- p.96
``the carrying out of substantive testing or verification of the system
lies beyond the scope of the Commissionts remit'' -- p.112
  My comments: The Commission recognizes that its own analysis of the
  source code is lacking.  Why are they willing to recommend the system
  before they have performed the analysis that would be needed to
  determine whether the system is trustworthy or not?
``The Commission has observed no mechanism within the system that would
enable operators, observers and voters to satisfy themselves independently
that the hardware and software of the voting machine are authentic and
that they are correct versions that have been tested and certified and
that have been approved for use by the electoral authorities.'' -- p.61
``it is not readily possible, nor is it required by prescribed procedures,
for operators or others to confirm independently that the version of the
C code installed on the voting machine or the programming/reading unit is
the correct version and to verify that it has not been altered.'' -- p.97
  My comments: This is a potentially significant vulnerability.  It is
  an admission that, if someone found a way of tampering with the code
  installed on the voting machines, election officials would have no way
  of detecting such tampering.
``data on ballot modules [e.g., electronic votes files] is not
cryptographically signed to prevent unauthorised alteration'' -- p.72
``The tests carried out by the Commission indicated that it would be
possible to access data, including votes, transmitted on CDs and to
alter the data without detection: [...] data, including votes, on CD is
not cryptographically signed to prevent unauthorized alteration [...]
There are thus significant hardware and data security vulnerabilities
associated with the use of CDs [...]'' -- p.88
  My comments: Another admission of a serious vulnerability in the system.
  The Commission has concluded that vote records can be tampered with
  while they are in transit, and that there is no way to detect such
  tampering, and that there are no satisfactory mitigations present.
  It seems to me that this vulnerability casts doubt on the integrity
  of the whole election.
``The storage location of a vote within each memory location of a ballot
module is determined pseudo-randomly, using the timer of the voting
machine as a seed in the case of the first vote to be stored. Thereafter
each vote is stored either immediately before or immediately after the
other votes that have already been stored, with the question of whether
it is stored before or after also being determined pseudo-randomly. If,
as further votes are stored, a vote cannot be stored before the other
votes as determined by this method, then it is stored after them (and vice
versa) until it is no longer possible to add votes to the ballot module.''

@_date: 2006-07-11 17:35:00
@_author: David Wagner 
@_subject: Factorization polynomially reducible to discrete log - known fact 
Be careful: when most people talk about the assumption that the
discrete log problem being hard, they usually are referring to the
hardness of discrete logs modulo a large prime.  In contrast, you
seem to be talking about the hardness of discrete logs modulo an
RSA modulus.  Those two things are not the same.
It is well-known that if you can solve discrete logs modulo a RSA
modulus N in polytime, then you can factor N in polytime.  This is
a standard result that is well-known to anyone who studies this field.
If you've re-discovered this result, you haven't got anything new.
The algorithm is very simple:
1. Choose a big random value x from some very broad range
   (say, {1,2,..,N^2}).
2. Pick a random element g (mod N).
3. Compute y = g^x (mod N).
4. Ask for the discrete log of y to the base g, and get back some
   answer x' such that y = g^x' (mod N).
5. Compute x-x'.  Note that x-x' is a multiple of phi(N), and
   it is highly likely that x-x' is non-zero.  It is well-known
   that given a non-zero multiple of phi(N), you can factor N in
   polynomial time.
There is no known proof that if you can factor N in polytime, you
can solve discrete logs modulo N in polynomial time.  (In practice,
if N is a 2048-bit RSA modulus that is a product of two 1024-bit
primes, if you can factor N, you can solve discrete logs modulo N
more efficiently by solving two discrete log problems modulo 1024-bit
prime numbers and then applying the Chinese remainder theorem.  But
the latter is still asymptotically superpolynomial.)
There is no known proof that if you can solve discrete logs modulo
a prime p in polytime, then you can factor a RSA modulus N in polytime.
There is no known proof that if you can factor a RSA modulus N in
polytime, then you can solve discrete logs modulo a prime p in polytime.
If you can solve any of the latter three problems, then you've got
something new, and many cryptographers will be interested.

@_date: 2006-07-11 17:50:06
@_author: David Wagner 
@_subject: Interesting bit of a quote 
No, it doesn't.  I think you've got it backwards.  That's not what SB1386
says.  SB1386 says that if a company conducts business in Caliornia and
has a system that includes personal information stored in unencrypted from
and if that company discovers or is notified of a breach of the security
that system, then the company must notify any California resident whose
unencrypted personal information was, or is reasonably believed to have
been, acquired by an unauthorized person. [*]
If you know or are notified that the security of your system has been
breached and if you know or have some reason to believe that someone
has received unauthorized access to unencrypted personal information
about California residents, then sure, you have to act on the presumption
that the personal information was spilled.  So what?  That seems awfully
reasonable to me.
In short, my reading of SB1386 is that companies only have to notify
customers if (a) they know or are notified of a security breach and
(b) they know or have reason to believe that this breach led to an
unauthorized disclosure of personal information.  In other words, SB1386
treats companies as innocent until there is some reason to believe that
they are guilty.  I don't know anything about SOX, but I think you've
mis-characterized SB1386.  Don't tar SB1386 with SOX-feathers.
[*] This is pretty close to an direct quote from Section 1798.82(a)
of California law.  See for yourself:

@_date: 2006-07-11 18:28:32
@_author: David Wagner 
@_subject: Interesting bit of a quote 
dan at geer.com
Well, are you sure you haven't confused what they're saying about SOX, vs
what they're saying about SB1386?  It's easy for me to believe that they'd
say this about SOX, but the plain language of SB1386 seems pretty clear.
(It would also be easy for me to believe that a General Counsel would
say that if you have knowledge of a breach of security in one of your
systems and reason to believe that an unauthorized individual gained
access to personal information as a result, then you must assume that
you have to notify every person whose data was stored in the system and
who may have been affected by the breach, unless you can prove that those
persons weren't affected by that breach.  But that's very different from
how you characterized SB1386.)
If General Counsels are really saying that SB1386 requires you to act
as if data has spilled, even in absence of any reason whatsoever to
think there has been any kind of security breach or unauthorized access,
merely because you don't have proof that it hasn't spilled -- then yes,
that does sound strange to me.  That is not my understanding of the
intent of SB1386, and it is not what the language of SB1386 seems to say.
Then again, maybe your General Counsels know something that I don't;
it's always possible that the text of the law is misleading, or that
I'm missing something.  They're the legal experts, not me.
Personally, my suggestion is as follows: The next time that a General
Counsel claims to you that SB1386 requires you to assume data has spilled
(even in absence of any reason to believe there has been a security
breach) until you can prove to the contrary, I suggest you quote from
the text of SB1386, and let us know how they respond.

@_date: 2006-07-12 09:55:34
@_author: David Wagner 
@_subject: Factorization polynomially reducible to discrete log - known 
When N is a large RSA modulus, there is a non-trivial probability that g
will be a generator (or that g will be such that x-x' lets you factor N).
The above is good enough for a polytime reduction.

@_date: 2006-06-12 18:15:02
@_author: David Wagner 
@_subject: Chinese WAPI protocol? 
As far as I can tell, WAPI (the Chinese proposal) uses proprietary
unpublished cryptographic algorithms.  The specification is secret
and confidential.  It uses the SMS4 block cipher, which is secret and
patented. [*]
I don't think that makes any sense, from a security point of view.
That's what got the 802.11 folks in trouble the last time.  If the
authors of WAPI won't make their spec and their algorithms, there is no
basis for trust in their scheme.  This is no way to design a standard,
and from the outside, it looks like adopting WAPI would be unwise.  It
was a bad idea the last time it was proposed, and it's still a bad idea.
Frankly, it's disappointing that any proposal that involves use of secret
homebrew crypto would be taken even the slightest bit seriously, no matter
what country's government is pushing it.  From a technical point of view,
it sounds like something that should have been rejected with prejudice
long ago.
[*] Contrary to what Adam Perez's email might suggest, Wikipedia does
not have a link to a specification of SMS4 or of WAPI.  Wikipedia has
an entry for SMS4, but about all it says is that not much is publicly
known about SMS4.

@_date: 2006-06-14 21:35:40
@_author: David Wagner 
@_subject: Chinese WAPI protocol? 
hank you to everyone who corrected the errors in my earlier post.  As has
been pointed out, the SMS4 block cipher was disclosed earlier this year.
Nonetheless, many of my concerns about the security of WAPI remain.
We already have a perfectly good solution out there; 802.11i is a good
scheme, and has been vetted by many folks.  In contrast, WAPI has
received very little analysis by security folks.  WAPI's underlying
block cipher is some special proprietary design that has never been
published in a peer-reviewed academic conference and does not seem to
have received much, if any, scrutiny from experts in block cipher design

@_date: 2006-09-08 17:50:07
@_author: David Wagner 
@_subject: Any opinions on Kryptor...? 
I have no clue whether the stream cipher in that paper is any good,
but the security analysis in the paper is basically nonsense.
The paper contains gibberish like "[axiom] A8 comes true by a relativistic
interpretation".  Your guess is as good as mine what that is supposed
to mean.  And the alleged "proof" of Theorem 1 is bogus.  In general, the
paper mimics the language of provable security, but without the content.
I would not recommend using a cipher that has not been published in an
appropriate peer-reviewed conference and scrutinized by the cryptography
community for several years.  Based on this, I would not recommend using
this stream cipher at this time.

@_date: 2006-09-16 16:28:53
@_author: David Wagner 
@_subject: Exponent 3 damage spreads... 
I agree with this comment, and with many of the other sensible
comments you have made in this thread.
I would modify what you said slightly: it may be reasonable to include a
field identifying the hash algorithm alongside the hash digest.  But apart
from that, throwing in additional optional parameters strikes me as just
asking for trouble.
It seems to me that e=3 is a distraction.  I think that these security
holes have revealed some more fundamental issues here that are independent
of the value of e you use.
It seems to me that the problems can be attributed to two problems:
(a) implementation bugs (failures to implement the spec faithfully); and
(b) ad hoc signatures schemes that have never been adequately validated.
In more detail:
  (a) Any implementation that doesn't check whether there is extra
  junk left over after the hash digest isn't implementing the PKCS
  standard correctly.  That's a bug in the implementation.  Of course,
  as we know, if you use buggy implementations that fail to implement
  the specification faithfully, all bets are off
  (b) The discussion of "parameter fields" in PKCS signatures
  illustrates a second, orthogonal problem.  If your implementation
  supports appending additional parameter fields of some general
  structure, then you have not implemented conventional PKCS
  signatures as they are usually understood; instead, you have implemented
  some extension.  That raises a natural question: Why should we think
  that the extended scheme is still secure?  I see no reason to think
  that throwing in additional parameters after the hash digest is a safe
  thing to do.  I suggest that part of the problem here is that people
  are using signature padding schemes that have not been validated and
  have not been proven secure.  These PKCS variants that allow you to
  include various optional ASN.1 crud alongside the hash digest have never
  been proven secure.  These days, using an ad hoc padding scheme that
  has not been proven secure is asking for trouble.  Why are people still
  deploying cryptographic schemes that haven't been properly validated?
I would suggest that there are two lessons we can learn from this
experience: (a) maybe more attention needs to be paid to verifying
that our implementations correctly implement the specification; and,
(b) maybe more attention needs to be paid to validating that the spec
defines a cryptographic mode of operation that is sensible and secure --
and provable security might be a good starting point for this.
Consequently, I think the focus on e=3 is misguided.  I think we should
be more concerned by the fact that our crypto implementations have
implementation bugs, and that our specs were never adequately validated.
This time, the impact of those failures may have been worse for signatures
using e=3, but it seems to me that this is more an accident than anything
particularly fundamental.  The latest problems with e=3 are just the symptom,
not the root cause.  I think it's worth putting some effort into treating
the root cause, not just the symptom.

@_date: 2007-04-19 12:46:54
@_author: David Wagner 
@_subject: AES128-CBC Question 
Yes.  If you encrypt two messages with a common prefix under the same key,
that fact will be readily apparent from the ciphertexts.  This may leak
information about the plaintext, depending upon the structure of your
messages.  Any decent crypto book will tell you about this weakness and
recommend against use of CBC with a fixed IV.  This is elementary stuff;
I think you may need to get someone with more experience in cryptography
advising you on these design questions.
Of course, the fact that someone else uses bad design (if that is
even correct) is not a good excuse for using poor practice yourself.
WEP does all sorts of crazy things, but that doesn't mean you should
copy what WEP does.

@_date: 2007-04-21 14:04:22
@_author: David Wagner 
@_subject: More info in my AES128-CBC question 
So the people who don't know anything about security are reluctant to
listen to those who do?  That's not a good sign.  It may be standard
operating procedure in groups like this, but that doesn't make it right.
It's still dysfunctional and dangerrous.  If the committee doesn't have
a commitment to security and is reluctant to listen to the experts,
that's a risk factor.
If you're sick and you go to a doctor, do you tell the doctor "you'd
better come up with some very clear arguments if you want me to follow
your advice"?  Do you tell your doctor "you'd better build a strong case
before I will listen to you"?  I would hope not.  That would be silly.
Doctors are medical professionals with a great deal of training and
expertise in the subject.  They can speak with authority when it comes
to your health.  So why do people with no training in security think
that they can freely ignore the advice of security professionals without
any negative consequences?
Wait a minute.  This reference to replacement attacks has me concerned.  Does the protocol use a message authentication code (MAC)?  I hope so.
If your protocol uses a MAC, and uses it properly, then replacement
attacks are not an issue, and the only issue with using a fixed IV is related to confidentiality.  If you don't use a MAC, you've got bigger
problems, and even random IVs won't be enough.

@_date: 2007-01-18 12:13:08
@_author: David Wagner 
@_subject: Private Key Generation from Passwords/phrases 
In article <45AFB67A.40300 at av8n.com> you write:
I disagree.  In the context of Physics, Shannon entropy may well be
the end-all and be-all of entropy measures, but in the context of
Cryptography, the situation is a little different.  In Cryptography,
there are multiple notions of entropy, and they're each useful in
different situations.
For this particular application, I would suspect that Pliam's workfactor
or Massey's "guessing entropy" could well be more accurate.  See, e.g.,
the following for a short summary and for references where you can learn
  Shannon entropy is often a reasonable first approximation -- it's
usually good enough for practical purposes.  But it's just an approximation,
and in some cases it can be misleading.
Example: Suppose I choose a random 256-bit AES key according to the
following distribution.  With probability 1/2, I use the all-zeros key.
Otherwise, I choose a random 256-bit key.  The Shannon entropy of this
distribution is approximately 129 bits.  However, it's a lousy way to
choose a key, because 50% of the time an adversary can break your
crypto immediately.  In other words, just because your crypto key has
129 bits of Shannon entropy doesn't mean that exhaustive keysearch will
require at least 2^129 trial decryptions.  This is one (contrived)
example where the Shannon entropy can be misleading.

@_date: 2007-01-22 16:22:32
@_author: David Wagner 
@_subject: analysis and implementation of LRW 
This is interesting.  Could you elaborate on this?  I suspect we could
all learn from the work the IEEE P1619 working group is doing.
I tried to trawl the P1619 mailing list archives to find some detailed
analysis on the topic of collisions, as you suggested, but I probably
wasn't looking in the right places.  The closest I found was this message:
  which estimates that if one continuously accesses the disk for 4.6
years (roughly the average life time of a disk), the chances of seeing
a collision are about 1/2^29.  Is that the analysis that triggered the
concern over collisions?
Are there modes that beat the birthday bound on collisions while using
a 128-bit block cipher?  Are they proven secure beyond the birthday bound?
I'm a little behind on the latest developments in modes of operation.
It would be interesting to hear more about any interesting technical
developments from the P1619 group.

@_date: 2007-01-23 10:43:04
@_author: David Wagner 
@_subject: analysis and implementation of LRW 
Peter Gutmann asks:
Alexander Klimov replies:
Huh.  Was that the reason?  I suspect there may have been more to it
than that.  The message at the cited URL basically says that if the
attacker somehow manages to learn half of the key material, then bad
things happen.  That alone isn't likely to lead to rejecting or accepting
a mode of operation, I would think.  You know the old joke.  Patient:
"Doctor, doctor, it hurts if I let the attacker learn part of my key."
Doctor: "Well, don't do that, then."
I should perhaps mention that there is some further background which
no one has brought up yet, and which may help to understand the IEEE
P1619 work.  I know there was a concern on the IEEE P1619 mailing list
that, if the encryption key is sitting in memory, when you suspend to
disk, if the suspend image is encrypted using that same key, then you
are encrypting the key under itself (C = E(K,K)).  Encrypting the key
under itself is in general a potentially unsafe thing to do.  For one
thing, it voids the security warranty; every provable security theorem
that I know of requires that the plaintext must not depend on the key.
For another thing, with some modes, there are known attacks where
encrypting the key under itself might reveal partial information about
the key.  LRW is one of those modes where there is such a known weakness.
I understand that the IEEE P1619 group came to the conclusion that it was
not reasonable to re-architect existing software to ensure that it would
never encrypt the key under itself.  This then creates a requirement that
the mode of operation must be safe to use even when encrypting a key
under itself.  That is indeed an interesting requirement, and one that
seems to legitimately rule out a number of existing modes of operation
for IEEE P1619.
With that background, I want to circle back to the message from Jim
Hughes.  I was aware of this encrypt-a-key-under-itself issue, and it's
an interesting one.  But Jim Hughes didn't cite that as the reason for
rejecting LRW; his mention of collisions made it sound to me like he
may have been thinking of something else.  Perhaps I misunderstood his
intent.  It might be helpful to have clarification on this: Jim, were
you suggesting that there is a second issue, or have I misunderstood you?
If there is an issue with collisions, I'd be interested to understand
it better.  Does anyone have any more information on this anywhere?
Does this refer to the birthday bound issue, that if you use a 128-bit
block cipher, then the security warranty is violated once you encrypt
close to 2^64 blocks of data?  Or is it something other than that?
My apologies if I've totally misunderstood the P1619 group's reasoning.
I suspect there may be a lot we can learn from IEEE P1619's effort.
Thanks to everyone for their comments.

@_date: 2007-01-23 18:23:50
@_author: David Wagner 
@_subject: analysis and implementation of LRW 
Thanks to everyone who responded with more information about IEEE
P1619.  Here are some of the additional links, with my reactions:
Andrea Pasquinucci points to:
Ben Laurie points to:
Wikipedia points to two concerns with LRW: (1) LRW isn't secure if you use
it to encrypt part of the key; (2) something having to do with collisions.
For these reasons, Wikipedia says that IEEE P1619 is moving to XEX-AES.
I think (1) is a valid concern and a legitimate reason for IEEE P1619
to move to another mode.  XEX-AES is a great mode and this seems like a
solid move for IEEE P1619.  XEX-AES rests on solid foundations, and there
are good grounds for confidence in its design.  I would add one caveat,
though.  I am not aware of any proof that XEX-AES -- or any other mode,
for that matter -- is secure when used to encrypt its own key.  This is
not a flaw in XEX-AES, but rather a generic property of standard models
of security for symmetric-key encryption.  So I wouldn't be inclined to
get too comfortable with the idea of encrypting the key under itself.
I'm not 100% certain I follow what (2) is trying to get at, but it
sounds to me like a non-issue.  One interpretation of (2) is that the
concern is that if part of the key is chosen in a non-uniform way (say,
as a password), then LRW is insecure.  Of course, you should not use any
mode in that way, and I don't know of anyone who suggests otherwise.
The remedy is straightforward: crypto keys should be truly uniform.
This is standard advice that applies to all modes of operation.
Another possible interpretation of (2) is that if you use LRW to encrypt
close to 2^64 blocks of plaintext, and if you are using a 128-bit block
cipher, then you have a significant chance of a birthday collision,
which may leak partial information about the plaintext or key.  That's
absolutely true, though it is pretty much a standard feature of any mode
of operation based on 128-bit block ciphers.  Standard advice is to change
keys long before that happens, and that advice doesn't seem terribly
hard to follow.  (See, e.g., my prior post on this subject for evidence
that this doesn't seem likely to be a serious problem for current disk
encryption applications.  That's fortunate for narrow-block cryptography,
because otherwise none of the solutions would be acceptable.)  So it
sounds like concern (2) is a bit of a red herring, and LRW is still ok
for applications that won't be used to encrypt the key or any material
derived from the key.
The good news out of IEEE P1619 is that a number of excellent modes of
operation are coming out of that effort, and other applications should
be able to take advantage of the good work that P1619 is doing.  This is
good stuff.
Disclaimer: Of course, LRW is of personal interest to me, so I'm sure
I'm biased.  Form your own opinions accordingly.

@_date: 2008-08-21 00:16:33
@_author: David Wagner 
@_subject: "Cube" cryptanalysis? 
I'm still absorbing Adi's new ideas, and I haven't looked at this in any
detail, so anything I say should be taken with an enormous grain of salt.
But, off-hand, I'd guess not.  I don't see anything that immediately
makes me worried about Skipjack, or AES for that matter.
In its most basic form, Adi Shamir's cube attack applies when some bit of
the output of the stream cipher (or block cipher, etc.) can be written as
a polynomial of the key and input such that the degree of the polynomial
is not too large.  One major innovation is that the attack applies even
if the number of terms in the polynomial is enormous -- say, way too
many to explicitly write down the polynomial.  When the degree is not
too large, Adi showed some powerful techniques for recovering the key.
Adi pointed out that this might be especially relevant to many LFSR-based
stream ciphers.  The reason is that many LFSR-based stream cipher use a
non-linear filter function of low degree.  Often, the key loading process
also has relatively low degree.  The LFSR itself is linear and hence does
not increase the degree.  The attack only seems directly relevant to a
subset of stream cipher architectures -- for instance, Adi mentioned that
he does not know how to apply it to some clock-controlled LFSR-based
stream ciphers, such as A5/1 -- but the class of stream ciphers it
applies to is an important and common class of stream ciphers.
In contrast, I don't expect this to threaten most modern block ciphers.
Most block ciphers contain enough rounds, and enough non-algebraic
structure in each round, to ensure that the degree of the resulting
polynomial will be large, so in those cases the attack does not seem
applicable.  But of course I could well be missing something, and it's
always possible there could be further advances.
It's a brilliant piece of research.  If you weren't at CRYPTO, you missed
an outstanding talk (and this wasn't the only one!).

@_date: 2008-02-09 17:04:28
@_author: David Wagner 
@_subject: Fixing SSL (was Re: Dutch Transport Card Broken) 
I'd be interested in hearing your take on why SSL client certs aren't
widely adopted.  It seems like they could potentially help with the
phishing problem (at least, the problem of theft of web authenticators

@_date: 2008-02-13 13:49:39
@_author: David Wagner 
@_subject: Toshiba shows 2Mbps hardware RNG 
all you need.
(Of course there are many Linux applications that use /dev/random
simply because they don't know any better, but that's a pretty weak
argument for a fast hardware RNG.)
A fast hardware RNG could be useful but I'm not convinced high
speed matters all that much for most applications.  Grab 128 bits
for a hardware RNG, feed it through AES-CTR to generate an unending
stream of pseudorandom bits -- that's good enough for most applications.
(Yes, I know there are exceptions where pseudorandomness is not
enough.  But even the exceptions rarely need true random numbers at
a rate of several Mbps.)

@_date: 2008-07-21 12:12:23
@_author: David Wagner 
@_subject: Looking through a modulo operation 
Yes.  I will show two attacks.  The results: Attack  is a
straightforward improvement to exhaustive search, and takes 2^52 work (~
10-20 CPU-years?).  Attack  is a more sophisticated meet-in-the-middle
attack, and takes 2^37 work and 2^35 space; the best instantiation I've
found looks like it would involve 16GB of RAM and a few days or weeks
of computation.  Both attacks recover the entire state of the PRNG,
given only a handful of known outputs.  It's possible there may be
other, better attacks.
My conclusion is that this PRNG is not cryptographically strong and
should not be used anywhere that you may face an adversary who is
motivated to break the PRNG.  In short, this PRNG is broken.
Attack  Given x mod N, there are only about 2^32/28233 = 2^17.2
possibilities for x, and it's easy to enumerate them all.  Suppose we
know x_1 mod N, x_2 mod N, ..., x_7 mod N.  Enumerate all
(2^17.2)^3 = 2^51.6 possibilities for x_1, x_2, and x_3.  For each
such possibility, you know 96 bits of output from a linear system,
and there are 96 unknowns, so you can solve for s_0.  Given s_0,
you can compute the predicted value of x_4, .., x_7 and compare them
to the observed values of x_4 mod N, etc.  If everything matches,
you've found the original seed s_0 and broken the PRNG.
This requires trying 2^51.6 possibilities, so it's a bit less than
2^51.6 work.  That's already a small enough number that the system
is not cryptographically secure.
This can be sped up slightly by noting that x_4 is a linear function
of x_1,x_2,x_3.  We can precompute the form of that linear function
and express it as a 32x96 matrix.  The best approach I can see will take
something like 500 CPU cycles to compute x_4 from x_1,..,x_3, so I'd
expect the total number of CPU cycles needed at something like 2^60.6
or so.  On a 4GHz machine, that's like 13 CPU-years.  Of course it is
trivially parallelizable.
Attack  If we were given inferred values of x_1, x_2, x_3, and x_4,
we could check them for consistency, since they represent 128
equations in 96 unknowns.  In particular, we can find a linear function
g from 128 bits to 32 bits such that g(x_1,x_2,x_3,x_4) = 0 if
x_1,..,x_4 are consistent with having been produced by this PRNG.
In fact, this can be broken down further, so that if x_1,..,x_4
are consistent, they will satisfy this equation:
  h(x_1,x_2) = h'(x_3,x_4).
Call this the check equation.  Here h,h' are linear functions from 64
bits to 32 bits, so if x_1,..,x_4 are not consistent, they have only a
2^-32 chance of satisfying this check equation.
Suppose we are given x_1 mod N, .., x_8 mod N.  We'll enumerate all
2^34.4 possibilities for x_1,x_2, compute h(x_1,x_2) for each, and
store them in a hash table or sorted list keyed on the 32-bit value
of h(x_1,x_2).  Once that table is built, we'll enumerate all 2^34.4
possibilities for x_3,x_4, compute h'(x_3,x_4) for each, and find
all occurrences in the table where
  h(x_1,x_2) = h'(x_3,x_4).
On average, we expect to find about 2^34.4/2^32 = 2^2.4 matches.
For each such match, compute the predicted value of x_5 as a function
of x_1,..,x_4 and see whether it agrees with the observed value of
x_5 mod N, etc., to weed out false matches.  In total, you'll need
to explore 2^34.4 + 2^34.4*2^2.4 ~= 2^37 possibilities, and you'll
need space for 2^34.4 entries in the table.
We can store the table as a sorted list.  This list will take up about
1600 GB, and you could buy enough hard disks for that at ~ $2000, say.
You can add an index in memory that tells you, for each value of
h(x_1,x_2), which block or sector on disk those entries of the list is
found at.  You'll need to make 2^34.4 random seeks into this hard disk
array.  With good disk head scheduling algorithms you might be able to
get the seek time down a bit, but even optimistically we're probably
still talking about a year of computation.
Fortunately, we can trade time for space.  With 16GB of RAM, we can
store 1/128th of the list: namely, all values where the low 7 bits
of h(x_1,x_2) are some fixed value V.  We cycle through all choices
of V, repeating the attack once for each V.  With this choice, I expect
the amount of time spent waiting for RAM to be comparable to the CPU
cost of the attack.  For each V, we have to evaluate a linear function
2^35.4 times.  It looks to me like this will take a few CPU-days.
Disclaimers: I have not checked any of the above analysis.  I have
not implemented it to test whether these attacks actually work.  Even
if the attacks do work, my estimates of the resources needed for the
attacks are a very rough estimate and should be taken with a large
grain of salt.  I didn't have time to do anything more than a cursory
analysis, so if there are errors in the above, please forgive me.

@_date: 2008-07-22 12:09:19
@_author: David Wagner 
@_subject: Looking through a modulo operation 
Well, that's good and broken then, isn't it?  No ifs about it.
It doesn't matter whether the implementation re-seeds frequently or
rarely.  It doesn't matter whether it leaks to you the first outputs
after initialization or only later ones.  If it's using a 32-bit seed,
this sucker is just plain broken, period.
The attack is trivial -- it's just exhaustive keysearch.
Attack: Given ~ 3 known outputs from the RNG, try all possible 32-bit
seeds.  You'll be able to recognize when your guess at the seed is correct
because the output from your guessed seed will match the observed output.
This assumes you know the offsets of your outputs (how many times the
RNG has been cranked before producing your known outputs), but even
if you only have partial information about that you can still make a
variant of this attack work.  The exhaustive search attack has to try
only 2^32 possibilities, so I'm guessing this attack should only take
minutes (at worst, hours) on a fast computer.
My earlier email about fancy attacks on this scheme is irrelevant.
There's no need to bother with fancy attacks, when the PRNG only uses
a 32-bit seed -- that's just blatantly insecure.  This is a textbook
error, one of the oldest mistakes in the book.  (For example, look
here:  Using this PRNG for security-critical purposes would not be wise.
I'll include your code snippet for seeding the PRNG below.  Note: I'm
assuming, per your comments, that "unsigned long" is a 32-bit type.

@_date: 2008-05-06 11:40:53
@_author: David Wagner 
@_subject: User interface, security, and "simplicity" 
In article <20080506010850.GC15920 at hn305c2n2.ms.com> you write:
This struck me as poor design, not good design.  Asking the user to
make these kinds of choices seems like the kind of thing that only a
cryptographer could consider sensible.  In this day and age, software
should not be asking users to choose ciphers.  Rather, the software
should just pick a sensible high-grade security level (e.g., AES-128,
RSA-1024 or RSA-2048) and go with that, and avoid bothering the user.
Why even offer "low" as an option?  (And this "export" business sounds
like a throwback to a decade ago; why is that still there?)
Good crypto is cheap.  Asking a user is expensive and risky.
Amen.  I know of quite a few software packages that could use more of
that philosophy.
It's too bad that today such elementary practices are something to brag
about.  Perhaps one day we'll be lucky enough that the answer to these
questions becomes more like "of course we use safe programming practices;
what kind of incompetent amateurs do you take us for?".

@_date: 2009-08-11 20:05:45
@_author: David Wagner 
@_subject: brute force physics Was: cleversafe... 
^^^^^^^^^^^^^^^^^^^
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
It is a commonly-held myth that quantum computers could solve
NP-complete problems, but most experts who have studied the issue
believe this is probably not the case.  There is no reason to think
that quantum computation could solve all NP problems in polynomial
time, and in fact, there are good reasons to believe this is
likely not the case.
(Do note that factoring is not NP-complete.)
See, e.g.
or for a more nuanced and deep treatment of these issues,

@_date: 2009-07-17 22:54:03
@_author: David Wagner 
@_subject: work factor calculation for brute-forcing crypto 
You may want to use the guessing entropy.
I've written about it before here:
 Christian Cachin's thesis is a wonderful introduction to entropy
measures, including the guessing entropy.
By the way, I'll make the obvious recommendation: Try to avoid
using a non-uniform random number generator to generate a cryptographic
key, if you can.

@_date: 2009-03-03 19:49:15
@_author: David Wagner 
@_subject: Activation protocol for tracking devices 
This does sound like it introduces novel risks.  I would suggest that
rather than spending too much energy on the cryptomath, it would make
sense to focus energy on the systems issues and the security requirements.
1) Is the system really intended to allow a single government agency
to deactivate a car, without permission from the owner of that car?
If so, that creates systematic risks that should be examined carefully.
Is there any chance of revising the security requirements, so that consent
of the owner is required?  Good requirements engineering may be able to
make as big a difference as any amount of crypto.
2) Strong audit logs would appear to be important.  In particular, here
are a few ideas.  One might require that anytime a car is deactivated,
a postcard is sent to the owner of that car letting them know of the
deactivation and who authorized it.  One could also require that an audit
log be kept of every deactivation event and who precisely authorized it,
and mandate that the owner of a car has the right to a copy of the audit
log for their own car at any point, without delay.
3) You might consider advocating an opt-out policy, where car owners
can turn off the functionality that allows deactivation of their car
without their permission, and/or turn off the tracking functionality.
4) You might want to ask about what protects the location privacy of
car operators.  Does this system provide a third party with the power
to track the movements of cars around the country?  That sounds like a
serious privacy risk to me.  What controls are there to protect privacy,
surveillance, or government abuse of power?
5) I would think that another possible security concern may be social
engineering: if DENATRAN has the power and is authorized to deactivate
cars, one tempting method to maliciously deactivate someone's car might
be to convince DENATRAN to deactivate it.  How will that be prevented?
What are the procedures that DENATRAN will follow before deactivating
a car?  Are these required by law or regulation?
6) Are there penalties for inadvertent, incorrect, or unauthorized
deactivation of a car?  One possibility might be to require that the
agency or the business pay a fee to the owner of the car if the owner's
car is improperly deactivated.  That might then put the onus of securing
the infrastructure on the folks who can do something about it.

@_date: 2009-10-22 12:14:17
@_author: David Wagner 
@_subject: Possibly questionable security decisions in DNS root management 
Could you elaborate?  I'm not sure what you're referring to or why it
would be quite risky to sign unrandomized messages.  Modern, well-designed
signature schemes are designed to resist chosen-message attack.  They do
not require the user of the signature scheme to randomize the messages
to be signed.  I'm not sure what discrepancy you're referring to.
Back to DNSSEC: The original criticism was that "DNSSEC has covert
channels".  So what?  If you're connected to the Internet, covert
channels are a fact of life, DNSSEC or no.  The added risk due to any
covert channels that DNSSEC may enable is somewhere between negligible
and none, as far as I can tell.  So I don't understand that criticism.

@_date: 2009-09-16 09:52:46
@_author: David Wagner 
@_subject: Detecting attempts to decrypt with incorrect secret key in OWASP 
Advice: if you're creating something for general-purpose use, at a minimum
make sure it provides authentication, integrity, *and* confidentiality.
A reasonable choice might be Encrypt-then-Authenticate where you first
encrypt with AES-CBC, then append a AES-CMAC message authentication
code on the ciphertext (including the IV).  As a bonus, this will detect
decrypting with the wrong key.
Use key separation -- with a pseudorandom function -- to derive the
encryption key and authentication key from the master crypto key
supplied by the user: e.g., Encryptkey = AES(K, all-zeros), Authkey =
AES(K, all-ones).
(You could replace AES-CMAC with SHA1-HMAC, but why would you want to?)
(From a cryptotheory point of view, what you want is a mode of operation
that meets IND-CCA2 /\ INT-CTXT, or at least IND-CCA2 /\ INT-PTXT.)
Advice: Provide one mode, and make it secure.  Try to avoid
configurability, because then someone will choose a poor configuration.
Suggestion: Provide documentation to warn the programmer against using a
password (or something based on a password) as the crypto key.  Provide
support for PBKDF2, with a reasonable default choice of parameters and
appropriate warnings in the documentation, for those who absolutely must
use a password-derived crypto key.
Recommendation: Read the book Practical Cryptography by Ferguson and
Schneier, or at least the chapters on message security.  It's the best
source I know to teach you some of the crypto-engineering practicum.
I would not recommend either of these.  It's better to use a MAC as I
suggest at the top of this email.
Whatever you do, don't choose your second MIC option: if the plaintext
comes from a low-entropy space, it leaks the value of the plaintext
(the plaintext can be recovered by brute-force search over all possible

@_date: 2009-09-16 17:19:53
@_author: David Wagner 
@_subject: Detecting attempts to decrypt with incorrect secret key in OWASP ESAPI 
I don't exactly follow the argument for using CCM mode instead
AES-CBC encryption followed by AES-CMAC, and I'm not familiar with
the political/perception arguments (who complains about the latter?),
but whatever.  It's hardly worth arguing over.  The cryptographic mode
of operation is unlikely to be the weakest link in your system, and the
security differences between CCM mode vs AES-CBC + AES-CMAC seem minor,
so it doesn't seem worth worrying too much about it: CCM mode seems good
enough.  I'm not sure I'm familiar with the arguments against EAX mode
(full disclosure: I'm a co-author on the EAX paper and hence probably
biased), but again, whatever.  These three choices are all good enough and
the security differences between them seem minor.  In my view, choosing
any of the three would be a reasonable choice.  Just my personal opinion.
Are you sure?  For vanilla CBC-MAC, the security proofs don't apply to
variable-length messages, and I recall that there are known attacks on
vanilla CBC-MAC when message lengths can vary (I'm not claiming those
attacks are necessarily realistic in all applications, but they may be).
AES-CMAC is a nice design that addresses this problem.  CMAC is based
upon CBC-MAC, but addresses the imperfections of vanilla CBC-MAC.
Personally, I wouldn't recommend vanilla CBC-MAC as a choice of message
authentication primitive; CMAC seems better in every dimension.  CMAC is
basically a CBC-MAC, but with all the details done right.  CMAC also
has the benefit that it has been standardized by NIST.
 Bottom line: If you're going to use a standalone CBC-based MAC together
with a standalone encryption algorithm, I'd recommend using CMAC as your
message authentication code, not AES-CBC.

@_date: 2009-09-17 12:42:26
@_author: David Wagner 
@_subject: Detecting attempts to decrypt with incorrect secret key in OWASP 
Given these choices, I'd suggest that you first encrypt with AES-CBC mode.
Then apply a message authentication code (MAC) to the whole ciphertext
(including the IV).  You then send the ciphertext followed the MAC digest.
SHA1-HMAC would be a reasonable choice of algorithm for message
authentication.  Sun's JCA appears to support SHA1-HMAC.
You'll want to use key separation to derive two separate keys.  So
if the key K is the master key, you could use
    Kenc  = SHA1-HMAC(K, "encrypt")
    Kauth = SHA1-HMAC(K, "authenticate")
or you could use
    Kenc  = AES-ECB(K, all-zeros)
    Kauth = AES-ECB(K, all-ones)
(Either is fine.)  Then use Kenc as the crypto key for AES-CBC encryption
and Kauth as the crypto key for SHA1-HMAC authentication.
If you are encrypting messages that will be sent over a two-way channel,
you'll probably want to either use a different crypto key for each
direction or include a direction bit in the inputs to the key separation

@_date: 2010-07-10 13:22:23
@_author: David Wagner 
@_subject: Question w.r.t. AES-CBC IV 
That's true of CBC mode, too, and almost any other encryption mode.
Encryption without authentication is dangerous; if you need to encrypt,
you almost always need message authentication as well.
(I will agree that CTR mode encryption without message authentication
is often even more dangerous than CBC mode encryption without message
authentication, but usually neither is a good idea.)
Setting that minor nitpick aside, the discussion here seems like good

@_date: 2010-07-21 11:49:04
@_author: David Wagner 
@_subject: A Fault Attack Construction Based On Rijmen's Chosen-Text 
For what it's worth, I read Vincent Rijmen's paper in the same way as
Jonathan Katz.  I don't think it's intended to be taken at face value;
if you took it seriously, one of us needs to read it again.  Rather,
I saw it as written with tongue embedded firmly in cheek: I took it as
a serious argument, hidden behind some gentle humor.
Vincent Rijmen could have written a sober, systematic critique of the
direction some of the field has gone in, carefully explaining in great
detail why some recent attack models are unrealistic.  That would have
been the safe, standard, and somewhat boring way to present such an
argument.  But instead Rijmen wrote a one-page lighthearted piece that
implicitly makes its point -- without ever having to come out and say it

@_date: 2010-07-23 13:53:33
@_author: David Wagner 
@_subject: Encryption and authentication modes 
Thanks for explaining the use case.
In addition to the dedicated AEAD modes (e.g., CCM, EAX, GCM) that
you already know about, you might look at encrypt-then-authenticate.
It might make a nice and simple solution for this particular use case.
In EtA, you encrypt with any secure encryption scheme, then append a
MAC that covers the entire ciphertext (including the IV).  For instance,
AES-CBC + CMAC is a fine combination.  It's pretty simple to implement.
I'm afraid that, for full security, secure encryption does require
either randomness or state.  The technical slogan is "deterministic
encryption schemes are not semantically secure".  A more down-to-earth
way to say it is that if you use a deterministic, stateless encryption
scheme to encrypt the same message twice, you'll get the same ciphertext
both times, which leaks some information to an eavesdropper.  In some
applications that might be OK, but for a general-purpose encryption
scheme, one might arguably prefer something that doesn't have such
an unnecessary weakness.
A weaker goal is graceful degradation: a weak random-number source
should not cause a catastrophic loss of security.  Some modes of
operation are more robust in the face of repeated or non-random IVs;
e.g., CBC mode is more robust than CTR mode.
Is obtaining proper randomness hard on your platform?  On most
desktop/server platforms, it is easy: just read from /dev/urandom.
If it's hard, there are various hardening schemes you can use to reduce
your dependence upon randomness.  I don't know whether they're worth the
effort/complexity/performance loss; that depends upon your usage scenario.
For instance, one cute hardening step you can do is to pick a separate
secret key for a PRF (e.g., CMAC), and then hash a random value together
with the message itself to obtain the IV for the encryption mode.  e.g.,
to encrypt message M:
  Encrypt(M):
  1. IV = CMAC(K0, random || M)
  2. C = AES-CBC-Encrypt(K1, IV, M)
  3. T = CMAC(K2, IV || C)
  4. return IV || C || T
(If you don't like storing 3 separate keys, use standard key separation
techniques: K0 = CMAC(K, 0), K1 = CMAC(K, 1), K2 = CMAC(K, 3).)
Of course, this hardening scheme does have a performance impact.

@_date: 2013-12-26 13:48:21
@_author: David Wagner 
@_subject: [Cryptography] RSA is dead. 
I believe I've managed to faithfully reconstruct the version
of Ping's code that contains the deliberately inserted bug.
If you would like to try your hand at finding the bug, you can
look at it yourself:
  I'm copying Ping, in case he wants to comment or add to
Some grounds rules that I'd request, if you want to try
this on your own:
1. Please don't post spoilers to the list.  If you think you've
    found a bug, email Ping and I privately (off-list), and I'll be
    happy to confirm your find, but please don't post it to the
    list (just in case others want to take a look too).
2. To help yourself avoid inadvertently coming across
    spoilers, please don't look at anything else on the web.
    Resist the temptation to Google for Pvote, check out the
    Pvote web site, or check out the links in the code.  You
    should have everything you need in this email.  We've
    made no attempt to conceal the details of the bug, so
    if you look at other resources on the web, you may
    come across other stuff that spoils the exercise.
3. I hope you'll think of this as something for your own
    own personal entertainment and edification.  We can't
    provide a controlled environment and we can't fully
    mimic the circumstances of the review over the Internet.
Here's some additional information that may help you.
We told reviewers that there exists at least one bug,
in Navigator.py, in a region that contains 100 lines of code.
I've marked the region using comments.  So, you are
free to focus on only that part of the code (I promise you
that we did not deliberately insert any bug anywhere else
outside that region).  Of course, I'm providing all the code,
because you may need to understand how it all interacts.
The original Pvote code was written to be as secure and
verifiable as we could make it; I'm giving you a modified
version that was modified to add a bug after the fact.
So, this is not some "obfuscated Python" contest
where the entire thing was designed to conceal a
malicious backdoor: it was designed to be secure, and
we added a backdoor only as an afterthought, as a way
to better understand the effectiveness of code review.
To help you conduct your code review, it might help to
start by understanding the Pvote design.  You can read
about the theory, design, and principles behind Pvote
in our published papers:
- An early version of Pvote, with many of the main ideas:
  paper:   Ping's slides: - Many improvements to the initial idea, and the final Pvote:
  paper:   slides: The Pvote code probably won't make sense without
understanding some aspects of its design and how it is
intended to be used, so this background material might
be helpful to you.
We also gave reviewers an assurance document, which
outlines the "assurance case" (a detailed argument describing
why we believe Pvote is secure and fit for purpose and free
of bugs).  Here's most of it:
  Why not all of it?  Because I'm lazy.  The full assurance
document contains the actual, unmodified Pvote code.  We wrote
the assurance document for the unmodified version of Pvote
(without the deliberately inserted bug), and the full assurance
document includes the code of the unmodified Pvote.  If you
were to look at that and compare it to the code I gave you
above, you could quickly identify the bug by just doing
a diff -- but that would completely defeat the purpose of the
exercise.  If I had copious free time, I'd modify the assurance
document to give you a modified document that matches
the modified code -- but I don't have time to do that.  So,
instead, I've just removed the part of the assurance
document that contained the region of the code where we
inserted our bug (namely, Navigator.py), and I'm giving you
the rest of the assurance document.
In the actual review, we provided reviewers with additional
resources that won't be available to you.  For instance, we
outlined for them the overall design principles of Pvote.  We
also were available to interactively answer questions, which
helped them quickly get up to speed on the code.  During
the part where we had them review the modified Pvote with
a bug inserted, we also answered their questions -- here's
what Ping wrote about how we handled that part:
 Since insider attacks are a major unaddressed threat in existing
 systems,
 we specifically wanted to experiment with this scenario. Therefore, we
 warned the reviewers to treat us as untrusted adversaries, and that we
 might not always tell the truth. However, since it was in everyone?s
 interest
 to use our limited time efficiently, we settled on a time-saving
 convention.
 We promised to truthfully answer any question about a factual matter
 that
 the reviewers could conceivably verify mechanically or by checking an
 independent source ? for example, questions about the Python language,
 about static properties of the code, about its runtime behaviour, and
 so on.
Of course, since this is something you're doing on your
own, you won't get the benefit of interacting with us and having
us answer questions for you (to save you time).  I realize this
does make code review harder.  My apologies.
You can assume that someone else has done some
runtime testing of the code.  We deliberately chose a bug
that would survive "Logic & Accuracy Testing" (a common
technique in elections, where election officials conduct a
test in advance where they cast some ballots, typically
chosen so that at least one vote has been cast for
each candidate, and then check that the system accurately
recorded and tallied those votes).  Focus on code review.

@_date: 2013-10-12 19:35:04
@_author: David Wagner 
@_subject: [Cryptography] Plug for crypto.stackexchange.com 
I've noticed quite a few questions on this list
recently of the form "How do I do X?" "What is
the right cryptographic primitive for goal X?" etc.
I'd like to plug the following site:
Cryptography Stack Exchange
It is an excellent place to post questions like
that and get helpful answers.  I encourage folks
to give it a try, if they have questions like the
ones I listed above.  By posting there, you will
not only get good answers, but those answers
will also be documented in a form that's well-suited
for others with the same problem to find and
benefit from.  I'm not trying to drive people
away from this mailing list, just pointing out
an additional resource that may be helpful.
Or, if you're feeling helpful and community-minded,
you can subscribe and help answer other people's
questions there.
(That site is like Stack Overflow, for those familiar
with Stack Overflow, except that it is focused on
cryptography.  There is also a site on information
security:  )
