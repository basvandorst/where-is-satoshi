
@_date: 2014-08-02 12:35:26
@_author: Rob Meijer 
@_subject: [Cryptography] Rumpelstiltskin treegraph sparsecap library 
Hi everyone,
My apologies is this mail is a bit dense and filled with external
references, I think its important to sketch the context to underline the
potential importance.
Some years ago I wrote a set proof of concept implementation of a set of
cooperating least authority providing user space file-systems for AppArmor
based Linux systems. This proof of concept was/is called MinorFS. For some
context, here is a Linux Journal article I wrote on this system 5 years
At the core of these file systems was a sparsecap (or password capability
if you prefer that term) file-system called capfs. This file-system was
based on an sqlite database with sparsecap to path mappings.
Some time later, I came up with an alternative hash based algorithm that
could possibly do away with the need of a database for capfs. After asking
feedback on this algorithm on the cap-talk mailing list, David
Barbour suggested I' d use HMAC instead of just SHA.
Resulting from this feedback, and driven by the idea that a library for
sparsecaps that give access to a DAG shaped authority structure might be
usefull for other things than just a rewrite of Minorfs::capfs,
I recently created a C++ (c++11) library that implements the algorithm,
using crypto++ for its hmac/sha2 crypto primitives.
This library basically implements the algorithm described here:
 Given the fact that my crypto knowledge and my knowledge regarding
implementation and usage pitfalls is relatively limited, I desperately need
a peer review on my Rumpleltree++ source code.  When the file-system in
finished, the logic in this library will become a pivotal part of the TCB
of any system built using the full set of file systems that will be layered
on them together with AppArmor. A rewrite of the original MinorFS that wil
aim at retrofitting the taming of shared mutable file system  provided by
MinorFS to non MinorFS aware applications in a way that should help
mitigate the effects that Trojans might have in a major way:
Thus, if anyone would be able and willing to contribute a peer review to
this library, you will be playing a crucial role in the ultimate goal of
creating a trojan free environment.

@_date: 2014-08-18 08:42:10
@_author: Rob Meijer 
@_subject: [Cryptography] Cost of remembering a password 
2014-08-16 13:38 GMT+02:00 Michael Kj?rling :
Then, ideally, when I view a sign-up form that asks for a password,
?Instead of storing passwords on a per-site basis, what about a poor-mans
hash-based hierarchy:
psk=?PBKDF2(secret, " i, n)
q3key=PBKDF2(psk,?PBKDF2(shared,"2014-q3", i, n)
password = BASE32(?PBKDF2(q3key,"login", i, n))
Such a hierarchy could be the first step to more fine-grained authorization
tokens. Fine-grained authorization tokens that could help in avoiding the
problems with using credentials as means of delegation between people.
contactskey =  ?PBKDF2(q3key,"contacts", i, n)?
readonlycontactskey = ?PBKDF2(contactskey,"read-only", i, n)
supplierscontactinfocap = BASE32(PBKDF2(readonlycontactskey,"suppliers", i,
So instead of: "Here are the credentials for my account, could you please
contact all product X suppliers for a quote on a quantity of Y of product
X", you would get: "Here is a sparsecap with the contact info for all our
suppliers, could you please contact all product X suppliers for a quote on
a quantity of Y of product X"
If browser plugin and site builders could agree on such a scheme, you would
have an amazing way to gain more usable security in the sense that you can
promote delegation that with passwords currently is discouraged out of
security considerations. So you either get considerably more work done with
a negligible decrease in security or, and this is likely more common, you
get significantly more security by not conflating delegation and abdication
as happens when you share regular passwords.

@_date: 2015-02-09 08:57:01
@_author: Rob Meijer 
@_subject: [Cryptography] What do we mean by Secure? 
2015-02-08 12:44 GMT+01:00 ianG :
?The more useful questions is: What are the most important security
attributes of your resources and what are your most important resources.
The fallacy of the treat model is best illustrated by the second/third lock
analogy. Ask a cop and a fireman the same question: "Should I use the
second and third lock on my door?"
?For the cop the thread model would be intruders and thus the answer would
be a definite yes. ?
?For the fireman the thread model would be ?fire and smoke and thus the
answer would be a definite no. When you look at the most valuable resource
(you and your family) and the most important security attributes
(survival), than you can start to look at the local crime and fire
statistics to try and calculate the most appropriate use of your locks.
Maybe if you live in an urban area on the US south you should use the locks
but if you live in the north of Europe not using the locks would maximize
the chances of survival. Maybe you should choose to use your additional
locks during the holiday season when  there are more house brake ins. Maybe
you should stop using these locks during the dry season. The base idea is,
it doesn't really matter how you die, dying is the least acceptable outcome
and thus the policy should be geared at minimizing the 'all cause'
probability of dying.

@_date: 2015-02-10 07:39:27
@_author: Rob Meijer 
@_subject: [Cryptography] What do we mean by Secure? 
2015-02-09 17:23 GMT+01:00 Phillip Hallam-Baker :
?That's an amazing metaphor. I'm definitely going to use that metaphor next
time I'll try to explain to my friends and family why I won't ever use
Skype ;-)

@_date: 2015-02-10 08:18:54
@_author: Rob Meijer 
@_subject: [Cryptography] What do we mean by Secure? 
2015-02-09 17:23 GMT+01:00 Phillip Hallam-Baker :
?Unfortunately due to different circumstances that have made me have to
rearrange my spare time and have made me reconsider on the forensic
aspects, I'm more than a year behind schedule so the system does still not
exist, ?but you might find my slides and speaker notes on MinorFS-2 for the
general gist of how a system like that could work at the FS level.
I was hoping the talk would have been available as video by now but
unfortunately the video material for the whole tent+day seems to have been
lost. I tried to make a video myself by narrating the slides, but I seem to
have the opposite of stage fright. While I have no issues speaking before
an audience, I seem unable to speak at my computer without an audience for
30+ minutes narrating the slides without  messing up all the time :-(
The file:
?has gotten all the slides twice, once without and once with the speaker
notes, so if you open that file and forward to slide 59 you should be able
to see both the slides and notes together from there.
The basic idea is that:
* You provide processes with private storage at different granularity
* ?You provide a private $TEMP to any process.
* By default, anything in the user's directory with a name that starts with
a dot maps to
   user+program granularity (other mappings are possible)
* By default, non dot files and directories map to user granularity (other
mappings are possible)
One thing that is not in the slides is the whole user versus owner problem,
but that's been discussed on this list recently, so the slides should give
you a general idea.
I know the file-system is only part of the rights granularity problem being
locked at the level of the user account, but at least its a start.

@_date: 2015-02-12 09:12:50
@_author: Rob Meijer 
@_subject: [Cryptography] Do capabilities work? Do ACLs work? 
2015-02-11 7:15 GMT+01:00 Peter Gutmann :
?I think that what most users want is more likely to be 'integrity'  than
'expression'.  Capabilities allow 'programmers' more and granularity
agnostic expression that is impractical or even impossible with ACL. In the
current technological reality, neither ACL nor capabilities can solve
'confidentiality'  in a way that yields a system that is both secure and
usable.  At least capabilities can get us 'integrity' part. The reason Unix
uses ACLs is because it was conceived in a time that it was suitable for
time sharing systems where the software was trusted but other users could
not. The model however is completely useless when as is mostly the case,
the users can be trusted but the software they are running can not.?

@_date: 2015-02-12 20:31:27
@_author: Rob Meijer 
@_subject: [Cryptography] Do capabilities work? Do ACLs work? 
2015-02-12 19:38 GMT+01:00 Jerry Leichter :
?ACL' s are not an extension of the UGO system, the UGO system is an
example of a trivial and limited ACL system. But anyway, lets not get in a
semantics discussion.  My point is that systems like these found their
origin in the days of timesharing and the days before malware. A time when
having access control at the granularity of groups and users made sense. In
that setting, the out of band attribute of resource approach to access
control made perfect sense. So now as a result of that legacy, we are still
pretty much stuck with a system that has become rather a kludge when trying
to solve the  security concerns that arise on systems where the software
run by the users is trusted significantly less than the users themselves.
In today's thread landscape I would argue,  without the burden of  backward
compatibility, the choice for capabilities would have been obvious and non
of these systems would have opted for ACL based systems.

@_date: 2015-02-16 11:49:48
@_author: Rob Meijer 
@_subject: [Cryptography] Capability Myths Demolished was: Do capabilities 
2015-02-16 7:22 GMT+01:00 Peter Gutmann :
?I found that the greatest problem with conceptions about capabilities, is
not in fact due to myths
surrounding capabilities or even myths surrounding ACL's. The ?greatest
problem with conceptions about capabilities comes from the fact that with
capabilities its obvious that particular unenforceable policies can not be
enforced, while ACL's can give the appearance of being able to enforce
policies that are in fact unenforceable.  So basically, if there are myths
that need to be demolished, it should probably be myths about what can and
what can't be theoretically enforced, independent of the whole
ACL/Capability divide.
Its pretty clear that ACL's can 'express' things that capabilities can't.
Its also pretty clear that ACL's can express things they can not enforce.
What isn't clear is the size or the form of intersection of policies that:
* Can not be expressed with capabilities.
* Can be enforced by ACLs.
* Have more than theoretical utility.
And in contrast, the  size or the form of intersection of policies that:
* Can be expressed by ACLs.
* Can not actually be enforced by ACL's
* Would have real utility if they could.
And finally we have the union of intersections of policies that:
1)  * Can be expressed with capabilities
     * Can not use central administration mandated by  use of ACL's due to
possible conflicting interest.
     * Have more than theoretical utility.
2) * Can be expressed with capabilities
    * Are so fine grained that use of ACL's would at best be impractical.
    * Have more than theoretical utility.
When I started out looking at capabilities, my expectation was that:
* A would be rather big
* B would be virtually zero
* C frankly never crossed my mind
Now after about a decade of trying to use capabilities in real systems, I'm
at the point that my expectations have changed to:
* A is not very big and for anything consumer related its probably zero.
* B is rather big and is a super-set of of the set containing all policies
that state 'do not delegate' at any level.
* C in the light of current day malware threats and mesh-up realities is
actually bigger and more relevant than its inverse.
But whatever view is (most) correct, its clear that myths that should be
addressed should first and foremost be those that being able to 'express'
something that for example would solve the CC problem , does in any way
actually solve the CC problem.

@_date: 2015-02-17 19:23:57
@_author: Rob Meijer 
@_subject: [Cryptography] Do capabilities work? Do ACLs work? 
2015-02-17 12:24 GMT+01:00 Jerry Leichter :
?Yet when you look at both tuples and file-systems as directional graphs,
the two worlds can meet once more in  (hopefully) multi-granular usable
Here is my attempt at an algorithm and library for use in a simple
(sparse)capability based file-system:
I don't have a background in cryptography so there is probably room
for improvement, but the base idea could be usable for tuples , file
systems and other DAG shaped singly attenuable structures.

@_date: 2015-02-19 20:38:07
@_author: Rob Meijer 
@_subject: [Cryptography] Passwords: Perfect, except for being Flawed 
2015-02-17 22:42 GMT+01:00 Kent Borg :
?Passwords are very much a flawed system for many reasons, and we should
definitely be able to do better.? The problem however with doing better is
that passwords serve two practical purposes in a poor way and fixing one
means braking the other for what we don't have a suitable alternative.
Passwords are tokens of authentication and pretty poor ones at that. We can
create better tokens of authentication, but there is a problem with that.
Passwords for many systems also are tokens of authorization. That is, they
are tokens that are usable to delegate authority by sharing them. Security
people will tell you that sharing passwords is bad practice but
anthropologists will tell you that delegation is essential to cooperative
patterns between people and is basically hard coded into our human
socio-genetic fabric. Basically passwords are both a bit like passports and
like keys and as each of those they suck. If you try to fix only the fact
that passwords make lousy passports, you will end up with passwords that
stop being keys and you will frustrate delegation and thus frustrate our
very built-in tendency to delegate. Before we can start to set out
replacing passwords with better tokens of authentication, we should first
take care of the genuine need for better tokens of authorization. That is,
we should start with fixing the important problem (better keys) first
before trying for the less important problem (better passports).

@_date: 2015-02-23 00:14:24
@_author: Rob Meijer 
@_subject: [Cryptography] Passwords: Perfect, except for being Flawed 
2015-02-19 17:18 GMT+01:00 ianG :
?Interesting. An other possible way to look at mobile phones could be as a
large keyring with (pseudo) anonymous? keys. So rather than using the phone
as single token for proving identity, it could be a granular tool for
proving our 'specific' authority. If all the authority can be revoked all
at once by a caretaker we control from our home, we could simple revoke all
of our phone's authority if ever we lost it or it got stolen.
Passwords are both (poor) tokens of authority and (poor) tokens for proving
identity. Two factor authentication makes for better tokens for proving
identity, but kills the (useful)  tokens of authority property that allows
us to share a password in order to delegate authority. If we can manage to
minimize the need for  tokens for proving identity and use tokens for
proving authority (capabilities) instead, than mobile phones could be an
interesting carriage for such tokens if we manage to get access control on
these devices sufficiently locked down to allow individual apps to keep
these authority tokens secret from each-other.
A mobile phone as secure capability wallet. Would be an interesting project
to work on ;-)

@_date: 2015-03-19 20:42:56
@_author: Rob Meijer 
@_subject: [Cryptography] Kali Linux security is a joke! 
2015-03-16 20:07 GMT+01:00 Henry Baker :
?Packaging security should be packager to user, not http(s) server to
http(s) client. Any packaging integrity system relying on 600+ CA's to be
uncompromised in inherently flawed.

@_date: 2015-03-23 08:32:45
@_author: Rob Meijer 
@_subject: [Cryptography] Kali Linux security is a joke! 
2015-03-23 7:09 GMT+01:00 Danny Mitchell :
?I couldn't agree more. Here is a bit of a rant I wrote a few months back
in response to some
of the undue praise that the 'HTTPS Everywhere' idea was getting:

@_date: 2015-10-15 10:41:18
@_author: Rob Meijer 
@_subject: [Cryptography] How does the size of a set of target results 
I am working on a opportunistic hashing implementation for a computer
forensic framework and I'm currently trying to objectively evaluate the
candidates for the hashing algorithm. Given that in computer forensics
there are many sources of 'known-good' and 'known-bad' that still use MD5
or a combination of SHA-1 and MD5 in the distribution of their data sets,
I'll need to come with convincing arguments to move forward from SHA-1 to
something like BLAKE2. I might even need convincing arguments to show if
MD5 is no longer acceptable. My current subjective take on this is that MD5
support should be considered depricated even for computer forensic purposes
and that I should probably aim for using a combination of SHA-1 with
BLAKE2-2bp to provide something of a transition path.
Collision-resistance arn't really in the threat model, but preimage
resistance of a sort is.  The thing is, given that most of the known-good
and known-bad hash sets are publicly available, it would be unacceptable if
it was practically possible to either create large amounts of known-bad
collisions as that would potentially create a DOS on the forensic process.
It also would be unacceptable if it was practically possible to modify
content to match any known-good hash as that would allow evidence to
falsely be discarded as known-good.
Now my question is: If a currently impractical preimage attack against a
hash function exists, but the threat model does not require a single hash
as result but a random hash from a large set of known hashes (in my case
somewhere between 2^26 and 2^28 individual hashes), how would the size of
this set of hashes influence the complexity of the attack; worst case?
Intuitively I would think if MD5 has a preimage attack of 2^116.9
complexity and a set of known hashes has worst-case size 2^28 entities,
than the worst-case combined attack complexity would be 2^88.9. But as
intuition has often proven to be a poor guide for me when cryptography is
concerned, this could be extremely pessimistic even to the extend that the
size of the hashset has zero impact on the complexity of the attack.
Could anyone share any insights on the true worst-case influence of the
size of the target hash set on the complexity of a pre-image attack ?

@_date: 2015-10-17 09:21:35
@_author: Rob Meijer 
@_subject: [Cryptography] How does the size of a set of target results 
2015-10-16 2:04 GMT+02:00 Ryan Carboni :
​No,  there are large sets of hashes that are distributed by organisations
spending time on categorizing files and distributing the hashes with
categorization to parties wanting to use these sets of hashes in a forensic
investigations. In the forensic investigation, these hash sets are then
used with the hashes calculated over files found on media in a set-theory
way in order to focus both the automated and manual part of the digital
forensic investigation.  Some of the hash sets are used as a "we can safely
ignore these files" type of set. These include files from OS distributions,
large collections of software packages, clipart collections, etc,etc.
Basically the stuff that you don't want to waste man-hours , CPU-cycles or
IO activity on.  Some sets are "A person needs to look at these" type of
set. These sets are things like collections of the hashes of child
pornographic images.
So there are three scenario's where a preimage attack could be a problem:
1) As an anti forensic technique, making an evidence containing file have a
hash from one of the  "we can safely ignore these files" set.
2) As denial of service attack on the human component of the investigation
by making a relatively large set of non-legally-problematic files match
with a hash from a "A person needs to look at these" set.
3) As part of a targeted anti-forensic attack. Making a file that can
exploit a flaw in a viewer for its file type match with a hash from a  "A
person needs to look at these" set.
So basically if an attacker is able to successfully do a first preimage
attack where the hash of the modified file matches "ANY ONE OF" of the
hashes from the large set of hashes (for what we mostly don't have the
original files).
Now my question is the following:
If the complexity of a first preimage attack for the used hasing algoritm
is defined as 2^{N}, and the size of the hash-set that our "ANY ONE OF"
implies is 2^{M}, what would be the worst case complexity of an  "ANY ONE
OF" geared first preimage attack?
I gather it would be smaller than 2^{N} yet probably not as small as the
naive assumption of 2^{N-M}.
I hope my question is making more sense now.​

@_date: 2015-10-26 06:32:47
@_author: Rob Meijer 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
2015-10-25 2:35 GMT+01:00 Ray Dillinger :
​The problem is: Undefined behaviour is where compiler builders get to make
optimisation work for you. Would you be OK with for example having to
specify each and every variable as volatile in order to make sure your
compiler would not be able to misoptimize?  Those kinds of things would be
needed for your Crypto C and I don't think they would be a price we should
be willing to pay. If you want to 'fix' it, you will need to start thinking
about a new programming language that does things like reverse the marking
burden. For example instead of marking a variable as volatile, it would
need to be volatile by default and get a nonvolatile marker assigned to it
explicitly by the programmer to show (s)he thinks he knows what (s)he is
doing.  Removing undefined behaviour altogether removes C's edge basically
when you compare it to languages that come with other safety guaranties
(like memory safety or capability security), so if that is the price you
are willing to pay, than go for it all the way and find or create a
memory-safe, capability-secure and undefined-behaviour-free language for

@_date: 2015-09-19 09:58:18
@_author: Rob Meijer 
@_subject: [Cryptography] Python Rumpelstiltskin-tree library + demo tool 
Just created a Python (3) port of RumpelTree++
â that I thought some of you might be interested in.â
Its a simple abstraction for facilitating the creation of Decomposable and
attenuatable sparse
designated  Rumpelstiltskin DAG
serialization based structures.
I've included a demo tool to play around with if you happen to use Linux or
some other Unix like OS. The tool lets you create a (or more if you like)
directory root node from a password. You can than create named sub
directory nodes under that root node or content nodes. The fun thing is
that each node comes with 3 ways to designate it:
* Through a combination of a parent node designation and its name
* Through a sparse capability
* Through an other sparse capability granting only read only access.
  second as encryption key.
All nodes are serialized to disk with AES using the second sparse
capability doubling as key and using a storage location derived from the
second sparse capability.
The tool is just a demonstrator , as real usage requires some kind of
server/client setup (or file-system/ user-process setup) where the server
can keep a key used for decomposition secret from the client
â, requires decent support for large data entities and requires race
condition protection to avoid messing up shared directory structures. The
tool however should give a nice idea of what the abstraction library can be
used for.â
I would be very much interested in any input I can get on this. I'm neither
a crypto expert nor a python expert, so I may be overlooking essential
things or making some blunders here. So please people, let me know âwhat
you think and/or what I could improve.ï»¿

@_date: 2016-08-31 01:48:32
@_author: Rob Meijer 
@_subject: [Cryptography] Capability Systems (was Re: ORWL - The First 
2016-08-30 23:37 GMT+02:00 Ben Laurie :
​The openat stuff looks interesting. It may be of interest to note that was
this discussion 10 years back:
that indirectly led to at that time was basically a workaround for openat
not allowing directory
file handles to be used as capabilties passible of unix domain sockets:
By the way, one thing that has been a growing concern with respect to the
use dir/file handles
in multi process setups is the fact that there doesn't seem to be a single
memory secure, let alone
capability secure programming language that supports good old file handle
passing over
unix domain sockets. Has the capsicum project given any thought about
providing a capability
based stack (like the AppArmor/MinorFS/E-language stack described in my
above article) ?
