
@_date: 2014-12-03 09:14:18
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Construction of cryptographic software. 
I don't know about that.  I think that Peter Gutmann's book "Cryptographic Security Architecture" is pretty good, and systematic, too.  It's also not written by committee, so it's an unfiltered view of one accomplished author of cryptographic and security software.

@_date: 2015-04-10 08:54:21
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Fwd: OPENSSL FREAK 
Good idea, except when the maker starts shipping out updates that have features that you don't want, e.g., surveillance.  In that case you're screwed if you do (unwanted features), screwed if you don't (liability).

@_date: 2015-02-09 08:45:12
@_author: Stephan Neuhaus 
@_subject: [Cryptography] What do we mean by Secure? 
I think that may be a bit too strong, in the sense that you probably can
make any reasonably complex software do something unintentional.
I like Dan Geer's definition: "Security is the absence of unmitigatable
surprises".  This covers, in one neat sentence, many aspects, for example:
* Most currently deployed systems have no formal specification (or could
be equipped with one in reasonable time), hence there are, strictly
speaking, no bugs, just surprises.
* As said above, you can probably make any reasonably complex software
do something unintentional, but "unintentional" isn't the problem,
really, because "unintentional" doesn't necessarily mean "bad". The
problem is, can you do something about unintentional behaviour when you
find out that it's also bad?

@_date: 2015-02-10 14:10:59
@_author: Stephan Neuhaus 
@_subject: [Cryptography] What do we mean by Secure? 
Or, to take an example that is probably much closer to what Richard meant, that there are no integers p and q such that p^2/q^2 = 2. Proving negatives is definitely possible in maths.

@_date: 2015-02-20 10:01:07
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Lenovo laptops with preloaded adware and an evil 
I think the chain of events went somewhat like this:
They (a) thought they really, really needed a way to increase their revenue, therefore (b) outsourced the whole thing to another company (because building things in-house is costly, don't you know?), (c) had neither the time nor the inclination to do a proper audit, and therefore (d) probably didn't even know about the extent to which Superfish compromised their customers' laptops.

@_date: 2015-02-25 16:10:56
@_author: Stephan Neuhaus 
@_subject: [Cryptography] information, Shannon, and quantum mechanics 
There's also the Feynman Lectures on Computation, where the author tackles questions like these (if I remember correctly, from a not-too-thorough reading many many years ago).
If I remember correctly (which I might not!), you could do reversible computations with zero energy, in the limit.

@_date: 2015-01-09 13:22:14
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Compression before encryption? 
Dear list,
I have come across the recommendation to "compress before you encrypt", on the grounds that this makes plaintext recognition through frequency analysis much harder.
However, compression algorithms surely have easily recognisable headers, right?  Also, I seem to recall a paper that did interesting things with encrypted compressed plaintext, but I can't recall any details.
So, does any one know what paper I might be referring to?  Or is there any other hard evidence (not personal opinion, however well-informed, please) that compression before encryption does or does not help?
Thanks in advance,

@_date: 2015-01-12 09:14:38
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Summary: compression before encryption 
Dear list,
thank you very much for your many answers; I'll summarise them here.
The question was whether compression before encryption helps security or
does more harm than good.
The consensus seems to be that while "it depends", it is more likely
that compression does not help security and might well harm it.
The reason is that compression may enable oracle attacks such as CRIME
(see  the first respondent to give
that link was Philipp Jovanovic philipp at jovanovic.io.) In this
chosen-plaintext attack, the attacker learns about a secret S by
observing the length of the resulting ciphertext.  The attacker uses a
CSRF to inject a plaintext P that is compressed together with the secret
S and then encrypted: Cipher = E(Compress(P, S)).  Now the attacker
observes Cipher' = E(Compress(P', S)). If the size of Cipher' is less
than the size of Cipher, then P' and S compressed better than P and S.
Knowing the compression algorithm allows the attacker to learn something
about, and eventually discover, S.
Where such attacks are feasible, compression should be disabled.  This
protects against CRIME even in the presence of CSRFs.
Another argument that was advanced was that if E(Compress(P)) were more
secure than E(P), then E would have a problem.  Technically, modern
ciphers have "to be resistant not just to known-plaintext but to
chosen-plaintext attacks, so hiding plaintext patterns doesn't really
get you much" (Peter Gutmann pgut001 at cs.auckland.ac.nz).
Another correspondent (natanael.l at gmail.com) raised the point that the
presence of plaintext patterns in an RC4-encrypted stream would allow
one to uncover biases in the key stream and ultimately the key, and
compressing the plaintext would hide these patterns.  But while that's
strictly true, surely that's just a matter of degree, and you're putting
your broken-down cart in front of your wheezing old horse. (See also
preceding paragraph.)
Another respondent (albert at puigsech.com) argued that
Encrypt-then-Compress was also an option ("has its pros"), but I'm
inclined to dismiss that since compression involves no secret, and so I
don't see how it can increase security post-encryption.
Finally, if injection is unlikely but compression highly beneficial for reasons having nothing to do with security (for example, if all you want is an encrypted channel to transfer your backups; the more general case being a one-way push or pull operation), then compression is very likely fine (noted by grarpamp at gmail.com).
In summary: compression *as a security device* is unlikely to benefit you, but likely to do you harm, so when in doubt, leave it out.

@_date: 2015-01-15 13:56:34
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Summary: compression before encryption 
Absolutely right. The article in question appeared in Cryptologia
Volume 10, Issue 2, 1986. DOI: 10.1080/0161-118691860912 URL:

@_date: 2015-07-09 10:45:40
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Anti-clipper team re-assembles 
Just a minor point of interest (but a genuine point of interest, not just a quip), why did you wait until after Snowden?

@_date: 2015-03-24 16:26:21
@_author: Stephan Neuhaus 
@_subject: [Cryptography] DIME // Pending Questions // Seeking Your Input 
In that case, you should probably steal ideas from IPFIX (RFC 5101 and
friends). That is a very simple binary format where it doesn't just say
"here is a 4-byte unsigned integer" but very specifically "this is an
IPv4 source address".
I've done an implementation for an IPFIX reader (a "collector" in
IPFIX-speak) and it's easy to get right and easy to get fast.
The only thing you should not copy from IPFIX is its really stupid
encoding of boolean values, where true is 1(!) and false is 2(!!).

@_date: 2015-03-25 16:35:35
@_author: Stephan Neuhaus 
@_subject: [Cryptography] How to crypto secure speed limit signs 
I'm not criticising, but instead trying to understand the situation.
Are you comparing the cost of creating a single QR code and signing that and then printing out hundreds of copies with the cost of creating many QR different codes, signing each one and then printing it out just once?
I think that the second alternative will be more expensive, but not because printing or signing is the deciding factor. Rather, I think that the difference in price comes when you have to train the people who install the signs that they should install the right signs in the right But I'll be interested to hear what others think.

@_date: 2015-03-27 08:22:11
@_author: Stephan Neuhaus 
@_subject: [Cryptography] "Most Americans Don't Mind Being on Candid 
While I totally agree with both your statements, a sufficiently bold devil's advocate might argue that, even though all your utterances may be recorded and scrutinised, you still have all the privacy in the world inside your own head.

@_date: 2016-07-06 18:47:37
@_author: Stephan Neuhaus 
@_subject: [Cryptography] What to put in a new cryptography course 
Depending on what you mean by that, the evidence for this is pretty thin.
Other than that, I've tried to find a way to teach ECC, but couldn't, at least not at the undergrad level.

@_date: 2016-07-11 12:03:07
@_author: Stephan Neuhaus 
@_subject: [Cryptography] What to put in a new cryptography course 
I don't think I did. I was obliquely referring to studies that claim to have found a link between complexity (as expressed by a metric) and vulnerability, but the metrics were very very weak. (The study is by Laurie Williams et al, and the correlations were on the order of 0.2. It's one of the few studies in the field.)
You mean like the complexity (and security) of AES?
We teach RSA with a high-level hand-waving explanation of why we think it's secure. We teach AES on the same principles. We even teach (not necessarily endorse) GCM, Gods help me. If we insisted on not teaching ANYTHING where we would have to rely on experts, then we'd have to leave out the whole of (modern) crypto.

@_date: 2016-09-27 12:35:19
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Fwd: Re:  [FORGED] Ada vs Rust vs safer C 
One interesting thing about this paper is that the authors argue that
"unstable code exists in 40% of the 8,575 Debian Wheezy packages that
contain C/C++ code", but of course that might mean that the remaining
60% contain no security checks at all. (I can't bring myself to believe
that fully 60% of all C/C++-containing Wheezy packages contain only
checks that have defined behaviour.)

@_date: 2018-08-09 19:57:17
@_author: Stephan Neuhaus 
@_subject: [Cryptography] PGP -- Can someone help me understand something? 
The short answer is that there are functions that are invertibe, but which are hard to invert. For example, I'm telling you that I know some x for which
   3^x mod 127 = 68.
Now I ask you, what is x? You can solve this problem easily, but it will probably be by trial and error, not by "working backwards". The point is that you can't just "work backwards", or if you can, fame and fortune await. So it's not always "a trivial issue to work backwards".
As always, the details are complicated, but I believe that this covers the essentials.

@_date: 2018-08-31 09:09:44
@_author: Stephan Neuhaus 
@_subject: [Cryptography] WireGuard 
I am not a fan of the "certificate authority model", for reasons we don't need to go into here, and I would contest your assertion that it is "less secure" than SSH's model, but in answer to your question, I refer you to the abstract of Peter Gutmann, Do Users Verify SSH Keys? Usenix :login; August 2011.

@_date: 2018-12-18 21:45:13
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Komitments 
If I understand your scheme, Bob can later claim to have committed to a different message.
For example, Bob has the message
s1 = The secret agent is Alice
He chooses some random r1 and computes and publishes w = H(s1 + r1)
When it later turns out that Dave is in fact the secret agent, he takes
s2 = The secret agent is David
and publishes s2 and r2 = s1 + r1 - s2. Since H(s2 + r2) = w = H(s1 + r1), Bob can thus "prove" that he knew that Dave had been the secret agent all along.
Did I misunderstand something?
You could perhaps save your scheme by using concatenation instead of addition. That looks as if it could work.

@_date: 2018-06-13 08:31:27
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Project Grace 
I'm not a PKI expert, but to me this looks more like a wish list. I'm not saying what they want to do is impossible, but I wonder how they plan to achieve "no certificates" and "supports TLS" at the same time. (Not TLS-PSK or TLS-SRP, mind you, they're talking about "the IETF standard for transport security, i.e. TLS.")
Also, one of the KPIs is "penetration testing with cryptanalysis", which seems vastly implausible. However, since the proposed way to measure these KPIs is to be "certifiable to ISO 27001:2013", it is certainly possible that this KPI will eventually be fulfilled.
TL;DR: colour me very skeptical.

@_date: 2018-05-16 14:43:11
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Vulnerability found in badly broken email apps 
In a slightly similar vein, I'm now seeing people on Twitter slamming GnuPG for either not using authenticated encryption (AE) or not failing hard when the authentication check fails. Such people will also generally point out that PGP is very old, as if that had anything to do with its security, or lack thereof.
If I understand correctly, GnuPG does on the one hand use a sort of AE, but does on the other hand indeed not fail hard when the authentication check fails, but at least on the third hand issues an error message, which, however, the email client then proceeds to ignore.
In thinking about this a bit more, I came to realise that this situation is the exactly same situation that occurs when using some of the shiny new AEAD ciphers such as GCM that have been mandated by TLS 1.3: the AEAD cipher will give you plaintext and only at the end do you realise that the authentication check has failed.
People say that AEAD does encrypt-then-MAC, on the grounds that the authentication token is computed on the ciphertext, not the plaintext. But if my analysis above is correct, then the caller of an AEAD API gets to decrypt (and therefore potentially act on) a message before its authenticity could be ascertained, which was exactly what encrypt-then-MAC was supposed to prevent, wasn't it?
If all that is true, I predict that we will see a number of Efail-like problems with AEAD ciphers.

@_date: 2018-10-15 16:38:40
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Random permutation model for encryption as a 
Dear list,
I'm teaching a class on IT security generally, and crypto plays not a small part in it. The original lecture had a lot of historical context and explained monalphabetic ciphers, simple polyalphabetic ciphers and transposition ciphers in great detail, and then went rather abruptly to AES, which could not be explained on that level any more.
Since the students only need to understand the *properties* of modern ciphers but not the details of their *construction*, and also since the detailed treatment of historical ciphers and their cryptanalysis had some students derive wrong ideas (e.g., being able to break AES by frequency analysis), I was looking for another way to explain modern ciphers to students in a way that would give them a useful mental model.
I came upon the random permutation model of ciphers. In this model, given a set P of possible plaintexts, and a set C of possible ciphertexts, a key selects a random permutation from P to C (and therefore |P| = |C|). This model is of course not an invention of mine; it's present in the Wikipedia page on block ciphers for example.
However, the set of possible n-bit keys contains only 2^n elements, whereas there are (2^b)! possible permutations of b-bit blocks, which is obviously vastly more if b is of the same order as n. Furthermore, not all permutations are good for block ciphers; e.g. those that have many fixed points (and most random permutations will have at least one) are evidently not well suited.
So the question is, is this a good model *for teaching*? It doesn't have to lead to theorems, and it may even be slightly inaccurate, as long as it prevents students from getting entirely wrong ideas Ã  la breaking AES with frequency analysis. I personally find it useful, but I have no idea what the students will think. Do you use other models?

@_date: 2018-10-16 08:29:13
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Random permutation model for encryption as a 
Dear list,
thanks for the many responses here. I need to emphasise though that the students will not be asked to analyse or construct block ciphers, so the inner workings of a cipher are wholly immaterial.  Some responses have been along the lines of "if you explained *this* particular way that ciphers are constructed, they'd have a better understanding of *that* property". These may be true, but they are not what I've been looking for. I've rather been looking for a mental black-box model of how ciphers work. I probably should have explained that better.
 From the responses, here is what I got so far:
* Use "pseudorandom functions" a la Lindell & Katz instead (in "Introduction to Modern Cryptography") (Thanks, Jon)
* Don't talk about permutations that aren't good for block ciphers -- if they're random (or well-chosen pseudorandom), they'll be good for crypto (thanks Peter, John, but see also Thierry's comment below)
* A topical Dilbert cartoon  (thanks Peter)
* Don't talk about "permutations of b-bit blocks" because it will make the students think that the bits in the block will be shuffled around. (thanks, John)
* A crypto *system* (as opposed to just a cipher) should use a different permutation for each block in the message, cf. IVs, chaining modes (thanks, John).
* Finding a good heuristic for the selection of 2^n permutations out of the (2^b)! possible ones is precisely the problem statement for a "good" block cipher (thanks, Thierry)
Very useful comments, thank you all very much!

@_date: 2018-09-03 09:09:12
@_author: Stephan Neuhaus 
@_subject: [Cryptography] WireGuard 
Sorry, my mistake. That was what I meant: I contest that the CA model is *more* secure than SSH's model.

@_date: 2019-01-14 09:12:42
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Government shutdown: TLS certificates not 
I made that point once (on this list even?), substituting my now 79-year-old fairly computer-savvy dad. For my troubles, I was told that I was a bad son and should have more faith in my dad! The implication was that the current PKI UI is fine, if only more people like me would take the time to explain what Peter Gutmann calls "PKI Theology" to people like my dad.
Somehow I retain my doubts about this.

@_date: 2019-07-08 07:25:17
@_author: Stephan Neuhaus 
@_subject: [Cryptography] [FORGED] Re: DDoS'ing PGP keys 
Waaaaay back when, I too noticed the dismally slow performance of PGP's key listing and management code. The way I remember it, it went:
for (each key in the keyfile, sequentially) {
   for (each signature on the key) {
     for (each key in the keyfile, sequentially) {
       if (id on key matches id on signature) {
         check signature
       }
     }
   }
So, not n^3 for PGP, but at least n^2.  I have no idea whether this kind of code is still in effect today, but I doubt it. Surely the entire codebase has been overhauled by now? Please?

@_date: 2019-09-30 06:41:37
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Crown Sterling debunked 
Another nitpick: The problem (as in "discrete-log problem") is rather to recover $n$ from $x$ and $x^n$, isn't it? Recovering $x$ from $n$ and $x^n$ would be called root-taking, IMO.
PS: My original post didn't make it through moderation because of top-posting (sorry about that) and seeing that we're already in a discussion where that post is being quoted, I'll refrain from reposting it.

@_date: 2020-08-25 12:39:21
@_author: Stephan Neuhaus 
@_subject: [Cryptography] any reviews of flowcrypt PGP for gmail? 
And if it doesn't get them (for example if, like me, you don't have a Google account and compile Signal from source[1]), it will run for about a month (I didn't check the exact period). And then it will count down about 10 days before it gives up the ghost. So the "demands weekly" update is in fact more of a "must-have monthly" update.
I have sympathy for the Signal developers. If there is a flaw in the software, they need to push updates, and push them fast. On the other hand, this makes it possible, under certain circumstances, to quickly push poisoned updates to targeted users. There is no good middle ground if you don't want to market yourself as a niche product. You're screwed either way.
[1] Before anyone jumps on this: I'm not doing this because I want to, but because precompiled versions of Signal are available on the official app stores only, and not, say, via F-Droid.

@_date: 2020-08-26 10:50:04
@_author: Stephan Neuhaus 
@_subject: [Cryptography] any reviews of flowcrypt PGP for gmail? 
Absolutely. I guess (but don't know, since 161 MiB are hard to verify) that if you pared Signal down to "just the encrypted SMS, Ma'am", you could do with much less, but that would then appeal only to geeks. And if you want to go for a broader audience, you'll have to include features that have nothing to do with secure messages, but exist only to forestall arguments like "I won't use Signal because it doesn't do ". For example, I can well imagine that some of the gunk comes from UI frameworks that exist only to make Signal look like other messengers, which in turn is important so that non-geek people can view Signal as a drop-in replacement for those other messengers.
That's a deliberate decision by Signal, and I for one applaud them for at least trying. I honestly don't want another geek-only tool that only geeks use.  Of course you may have a different opinion when your requirements are different.  If I were a dissident, I would probably not use Signal.
As for what the frequent updates update, I just don't know. When I do a git pull every month or so, I can always see many many changes. It seems to me that entire Java package hierarchies disappear, new ones appear, and there is much code churn. If this is correct, it would indicate that these aren't just bug fixes, but that the overall internal structure of the app is still very much in flux. I have no idea whether that is a good thing or not.
PS: A complete OS/9-68k install took 2 1.44 MiB diskettes in 1993, if I remember correctly. I'm not sure what a comparison of 25-year-old installs with contemporary ones can meaningfully achieve, except of course prove that one is getting progressively older :-)

@_date: 2020-02-05 12:36:29
@_author: Stephan Neuhaus 
@_subject: [Cryptography] SSL Certificates are expiring... 
I have been working with several IoT people and some solve this problem by issuing certs that expire in the year 9999 (i.e., never). Of course that solves the problem of the CA cert expiring, but on the other hand this is not how it was supposed to be.
And I hope that IoT developers realise that the PKI model of trust is not a good match for (much of) IoT security.

@_date: 2020-01-08 09:52:03
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Recent improvements on SHA-1 attacks 
But admittedly, the way that software systems are used these days, it's positively likely that among all of the users of a particular popular system (e.g., git) there are some where an adversary would think the investment worthwhile.
Oh sure, TLS will probably not be affected by this, but there are tons of protocols and file formats that will be. After reading PoC||GTFO, I'm thinking that there is almost nothing that can't also be a PDF.

@_date: 2020-06-09 17:47:11
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Zoom publishes draft cryptographic design for 
That was what I was thinking also, which is why I asked that question originally (or maybe we both hit on the same point, the messages to the list seem to cross each other). The d's seem to be very much equally distributed. If that is true, then you'll very probably get a d that's close to the bitsize of n with *any* suitable e.

@_date: 2020-03-02 06:28:50
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Ex-CIA Joshua Schulte Describes His Data/Crypto 
Can I ask you what your evidence for that claim is?

@_date: 2020-03-03 07:46:14
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Ex-CIA Joshua Schulte Describes His Data/Crypto 
Thanks for these kind words. I am asking because the reference that most people remember when they say something like this is Peter Gutmann's paper "Secure Deletion of Data from Magnetic and Solid-State Memory", from Usenix Security 1996. That paper has acquired various epilogues over time, the first reading, in part:
"[W]ith the ever-increasing data density on disk platters and a corresponding reduction in feature size and use of exotic techniques to record data on the medium, it's unlikely that anything can be recovered from any recent drive except perhaps a single level via basic error cancelling techniques."
So my question, perhaps more fully stated, is: would you like to dispute this claim, and if so, what evidence do you have for disputing it?

@_date: 2020-03-04 10:14:42
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Ex-CIA Joshua Schulte Describes His Data/Crypto 
I went there and found quotes like
"Daniel Feenberg, an economist at the private National Bureau of Economic Research, claims that the chances of overwritten data being recovered from a modern hard drive amount to "urban legend".[3] He also points to the "?18 1?2-minute gap" Rose Mary Woods created on a tape of Richard Nixon discussing the Watergate break-in. Erased information in the gap has not been recovered, and Feenberg claims doing so would be an easy task compared to recovery of a modern high density digital signal."
"On the other hand, according to the 2014 NIST Special Publication 800-88 Rev. 1 (p. 7): "For storage devices containing magnetic media, a single overwrite pass with a fixed pattern such as binary zeros typically hinders recovery of data even if state of the art laboratory techniques are applied to attempt to retrieve the data."[5] An analysis by Wright et al. of recovery techniques, including magnetic force microscopy, also concludes that a single wipe is all that is required for modern drives. They point out that the long time required for multiple wipes "has created a situation where many organisations ignore the issue all together ? resulting in data leaks and loss."
So I'm still not seeing how "overwritten data" (presumably on a modern, high-density drive) "can be recovered".

@_date: 2020-05-25 07:39:34
@_author: Stephan Neuhaus 
@_subject: [Cryptography] CMS or S/MIME test vectors 
Or you could adapt techniques from fuzzing to make your code fail and then see if the thing they failed on was really invalid or just an odd corner case. Fuzzing is really, really good these days at creating inputs that, when you squint, look valid but aren't. Or, of course, vice Of course you still need to check that the things your code *didn't* flag up as invalid are really all valid cases. That's going to be tough.

@_date: 2020-11-02 10:00:42
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Windows security leads to 0-day in Windows 
It seems strange to me that one of the better C/C++ compilers out there (Visual Studio) wouldn't see the potential for an integer overflow here. (To be sure, it's UNSIGNED integer overflow, which is at least not undefined behaviour. That might in fact be part of the problem here.) Also, it seems to me that fuzzing would have found this problem rather quickly. It's a very odd bug to have survived apparently since Win7.
Do you have any insights on how this bug remained in the code base for so long, and why none of the (reportedly excellent) static analysis components of Visual Studio have alerted to it?
PS: Windows has ioctls? I'd always thought they were a Unix specialty.

@_date: 2020-11-16 09:57:03
@_author: Stephan Neuhaus 
@_subject: [Cryptography] IPsec DH parameters, other flaws 
The killer to this approach will be, I suggest, interoperability.
The nice thing about symmetric crypto is that everyone agrees that there is just one basic building block, the block cipher[1], that from an interop perspective only has two design parameters: block size and key size. Compare that with, say, a protocol that does all the things that TCP does (which was one thing the OP wanted to replace). There are dozens of design parameters, and there will even be dozens of mutually incompatible *sets* of design parameters, depending on your architecture.
Also, if you want interoperability, you have to get the vendors on board. With block ciphers, we know that you can't really roll your own. With network protocols, there are many different, viable designs that are not interoperable. Vendors can, and, before the Internet took over, did, roll their own network protocols.
So let's say that NIST organises a competition and "TCPng" wins. So what? Why on Earth should anyone implement that thing when it cannot give them an edge? (I realise that this is an argument from incredulity, so I'd be happy to hear counterarguments.)
[1] or at least agrees that they can live with block ciphers as the building block

@_date: 2020-11-17 11:12:45
@_author: Stephan Neuhaus 
@_subject: [Cryptography] IPsec DH parameters, other flaws 
Yes, and the reason it works (i.e., gets transported through routers) is that the protocol underlying QUIC is IP, one of the things the OP *also* wanted to replace.

@_date: 2020-11-23 08:58:12
@_author: Stephan Neuhaus 
@_subject: [Cryptography] IPsec DH parameters, other flaws 
Here we come full circle in this subthread. Iang proposed a NIST-style competition for network protocols instead of IETF-style "rough consensus and working code", and I pointed out that if you need interop, NIST-style is not a good match and one probably needed something like IETF. Then you asked whether I meant something like QUIC.
And indeed I might have because QUIC was not adopted through a NIST-style competition. Yes, it was initially a Gogle-proposed protocol, but IMO it gained traction by getting it (slowly, slowly) through the IETF.

@_date: 2020-11-27 09:28:43
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Second Swiss firm allegedly sold encrypted 
I can confirm that this is mainline news in Switzerland (even though it is shadowed by news about its terrible handling of the Corona crisis). The Tagesanzeiger had a long article about this, and from what I remember about that article, they were somewhat confused about Ueli Maurer's role in vetting Onmisec's crypto algorithms.
 From memory (which may or may not be faulty):
It seems that Omnisec gave the impression that their productss had been vetted by Maurer, an ETH professor and renowned cryptographer. Maurer denies this, saying that because of scientific prudence he would never vet products that he didn't own. He says that his role was more that of a "second opinion". But then the Tagesanzeiger writes that "Omnisec disputes this, claiming that Maurer had "attacked" their algorithm". (Quote from memory, translation mine.)
I honestly don't see a contradiction here. You can, as part of a second opinion, try a few attacks on the crypto, and report that these failed. But that is far from a successful vetting of the algorithm or product.

@_date: 2020-10-16 08:42:38
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Secret sharing for family members 
>
 >>
 >>> On 2020-10-14 (288), at 20:50:51, ?ngel   >>>
 >>>> In these days of 'digital estates', where each of us
 >>>> controls a large swath of digital accounts,
 >> [ ?. ]
 >>
 >>>> Are there any products/services that *safely*
 >>>> handle these situations?
 >>> Store an offline copy (such as a CD) of the password
 >>> manager file holding all those digital estates in a safe.
 >>>
 >>> If the need arises that you are (temporarily or permanently)
 >>> unable to handle your 'digital estate', your family member may take the
 >>> safe key from you and access it.
 >>>
 >>> A physical solution seems preferable than a cryptonerd one here.
 >>>
 >>> Best
 >> Umm, when was the last time you bought a CD burning drive
 >> equipped laptop let alone PC?  Let alone blank CDs from a
 >> neighbourhood convenience store?  Let alone a safety deposit
 >> box, &tc, &tc.
 >> __outer
 >
 > If you are going to store it in a safe, printing on paper seems the
 > simplest.
If I'm not mistaken, many modern printers will store the printed document locally on a disk. If that's so, and if stealing the printer and recovering the key that way is in your attacker model, then copying the key by hand seems to be the way.
PS: The last time I bought a CD burning drive was two years ago. I don't see why that should be an issue (yet), at least in Switzerland.

@_date: 2020-10-21 08:56:34
@_author: Stephan Neuhaus 
@_subject: [Cryptography] Fun with printers, 
It might be, it might not be. A document might be cached deliberately. For example, there might be some flash memory on the thing, e.g., to enable resumption of printing in case of a power failure. This will be hard to check visually, given that these chips are small. A document might also be stored without explicit intent. For example, some chips that are used for processing an image may have nonvolatile memory built in, making it even harder to ascertain the absence of such memory. A modern device has so many CPUs in it that I certainly would never guarantee that at least one of them didn't come with some NVM where images or documents could be stored.
If your threat model includes someone getting the document containing important secrets off nonvolatile memory anywhere in your printer, why risk it?
