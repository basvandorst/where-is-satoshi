
@_date: 2008-01-23 09:29:07
@_author: Florian Weimer 
@_subject: SSL/TLS and port 587 
* Ed Gerck:
Huh?  Have you got a source for that?  This is he first time I've
heard of such claims.
Message submission over 587/TCP gives the receiver more leeway
regarding adjusting message contents to police (add a message ID,
check the Date and From headers, and so on).  The abuse management
contract is also different: once you accept a message over 587/TCP,
it's your fault (and your fault alone) if this message turns out to be
spam.  There's nothing related to confidentiality that I know of.

@_date: 2008-01-23 17:59:59
@_author: Florian Weimer 
@_subject: patent of the day 
* Saqib Ali:
Exactly.  Niels Provos, "Encrypting Virtual Memory", USENIX Security
2000, looks like something pretty close to prior art:
AFAICS, the patent does not reference the paper.

@_date: 2008-05-05 13:41:45
@_author: Florian Weimer 
@_subject: OpenSparc -- the open source chip (except for the crypto parts) 
* Perry E. Metzger:
Call me a speciest, but it's not clear if Rice's Theorem applies to
While Marcos' approach is somewhat off the mark ("source-code
equivalent that works for me" vs. "conformance of potentially
malicious code to a harmless spec"), keep in mind that object code
validation has been performed for safety-critical code for quite a
while.  The idea is that code for which some soundness property cannot
be shown simply fails validation.  It doesn't matter if the validator
is not clever enough, or if the code is actually bogus.
(And for most (all?) non-trivial software, source code acquisition
costs are way below validiation costs, so public availability of
source code is indeed a red herring.)

@_date: 2009-08-13 11:41:34
@_author: Florian Weimer 
@_subject: brute force physics Was: cleversafe... 
* David Wagner:
It's also possible to factor an n-bit number in O(n^k) integer
additions, substractions, multiplications, divisions and comparisons
to zero, for some smallish fixed value of k (an observations which is
due to Sch?nhage, IIRC).  So you have to look very closely at your
model of computation.  It turns out integer arithmetic isn't the
relevant one.  Until scalable quantum computers are available, it will
be difficult to forecast what the correct model will be.  There might
be practical limits not immediately apparent, similar to our lack of
means to build machine registers which can store integers in the
mathematical sense.

@_date: 2009-10-21 07:59:52
@_author: Florian Weimer 
@_subject: Possibly questionable security decisions in DNS root management 
* Perry E. Metzger:
As far as I know, only the following classes of DNS-related incidents
have been observed:
  (a) Non-malicious incorrect DNS responses from caches
      (a1) as the result of defective software
      (a2) due to misconfiguration
      (a3) as a means to generate revenue
      (a4) as a means to generate revenue, but informed consent
           of the affected party is disputed
      (a5) to implement local community standards
  (b) Compromised service provider infrastructure
      (b1) ISP caching resolvers
      (b2) ISP-provisioned routers/DNS proxies at customer sites
      (b3) authoritative name servers and networks around authoritative
           name servers
      (b4) as the result of registrar/registry data manipulation
  (c) DNS as a traffic amplifier, used for denial-of-service attacks
      both against DNS and non-DNS targets
  (d) in-protocol, non-spoofed DNS-based reflective attacks against
      authoritative servers
  (e) unclear incidents for which sufficient data is not available
The problem is that the "attacks" you mentioned are in class (e), but
likely belong to (a1) and (a2) if we had more insight into them.
Certainly, bad data itself is not proof of malicious intent.
(NB: (a1) does *not* include software using predictable query source
ports.  There does not appear to be corresponding attack activity.)
Well, this seems to be rather constructed to me.  You state that
DNSSEC is a game changer, and then it's indeed pretty unclear what
level of cryptographic protection is required.  But in reality, DNSSEC
adoption is not likely to change DNS usage patterns.  If there's an
effect, it will be due to the more rigid protocol specification and a
gradual phase-out of grossly non-compliant DNS implementations, and
not due to the cryptography involved.

@_date: 2009-10-21 08:12:21
@_author: Florian Weimer 
@_subject: Possibly questionable security decisions in DNS root management 
* Jack Lloyd:
And you better randomize some bits covered by RRSIGs on DS RRsets.
Directly signing data supplied by non-trusted source is quite risky.
(It turns out that the current signing schemes have not been designed
for this type of application, but the general crypto community is very
slow at realizing this discrepancy.)

@_date: 2009-10-21 08:24:29
@_author: Florian Weimer 
@_subject: Possibly questionable security decisions in DNS root management 
* Victor Duchovni:
There's only one required signature in a ". IN NS" response, so it
isn't as large as you suggest.  (And the priming response is already
larger than 600 bytes due to IPv6 records.)
DNSKEY RRsets are more interesting.  But in the end, this is not a DNS
problem, it's a lack of regulation of the IP layer.

@_date: 2009-10-21 08:27:11
@_author: Florian Weimer 
@_subject: Possibly questionable security decisions in DNS root management 
* John Gilmore:
And it's still not clear that it works.  No additional suite of
algorithms has been approved for DNSSEC yet.  Even the upcoming
SHA-256 change is, from an implementors perspective, a minor addition
to NSEC3 support because it has been tied to that pervasive protocol
change for political reasons.
Not really, most ccTLDs only pay out of generosity, if they pay at all
(and if you make enough fuss at your favorite TLD operator's annual
general meeting, they are likely to cease to pay, too).
Crap queries are one problem.  DNS is only efficient for regular DNS
resolution.  Caching breaks down if you use non-compliant or
compliant-to-broken-standards software.  There's also the annoying
little twist that about half of the client (resolver) population
unconditionally requests DNSSEC data, even if they are incapable of
processing it in any meaningful way (which means, in essence, no
incremental deployment on the authoritative server side).
There are some aspects of response sizes for which no full impact
analysis is publicly available.  I don't know if the 1024 bit decision
is guided by private analysis.  (It is somewhat at odds with my own

@_date: 2010-08-26 15:45:34
@_author: Florian Weimer 
@_subject: towards https everywhere and strict transport security 
* James A. Donald:
In this generality, this is not true at all.  You're confusing
handshakes with protocol layering.  You can do the latter without the
former.  For example, DNS uses UDP without introducing additional
round trips because there is no explicit handshake.  Lack of handshake
generally makes error recovery quite complex once there are multiple
protocol versions you need to support, but handshaking is *not* a
consequence of layering.

@_date: 2010-07-14 08:52:50
@_author: Florian Weimer 
@_subject: Encryption and authentication modes 
What's the current state of affairs regarding combined encryption and
authentication modes?
I've implemented draft-mcgrew-aead-aes-cbc-hmac-sha1-01 (I think, I
couldn't find test vectors), but I later came across CCM and EAX.  CCM
has the advantage of being NIST-reviewed.  EAX can do streaming (but
that's less useful when doing authentication).  Neither seems to be
widely implemented.  But both offer a considerable reduction in
per-message overhead when compared to the HMAC-SHA1/AES combination.
Are there any other alternatives to consider?  Are there any traps I
should be aware of when implementing CCM?

@_date: 2010-07-23 08:14:20
@_author: Florian Weimer 
@_subject: Encryption and authentication modes 
* David McGrew:
I just want to create a generic API which takes a key (most of the
time, a randomly generated session key) and can encrypt and decrypt
small blobs.  Application code should not need to worry about details
(except getting key management right, which is difficult enough).
More popular modes such as CBC lack this property, it's too easy to
misuse them.
I suppose a superficially similar primitive is contained in
Bernstein's NaCl library.  I was intrigued by its simplicity, but the
cryptographic algorithms it uses are a bit non-standard.
Right now, I would probably use it to forward session state through
browser URLs in areas which are not actually security-relevant.
Somewhat more sensitive applications are possible in the future.  In
no case I expect adversaries to actually have access to ciphertext.
To some degree, it's about being able to say "yes, we use encryption
for X, and it's algorithm Y", and I want to do it right without using
CMS or modern OpenPGP, with all the complexities that come with that.
When I said that CCM wasn't widely implemented, I was referring to the
fact that none of the cryptographic libraries on my system supports it
directly.  This is a pity because once you fix the parameters, it's
much simpler to use safely than pure encryption modes.
There seems to be one downside with the CCM instance specified towards
the end of the NIST standard, though: If you on an architecture where
sizeof(size_t) == 8, then your encryption function isn't total because
it can't accept the full range of possible input lengths---or you end
up with just 64 bit for the tag, which seems to be a bit on the small
side.  I'm not sure if this is a compelling reason to use EAX or GCM,
though---especially since we're strictly limited to 31 bit array
length in some places by software and not hardware (so this limitation
will be in place for a long time).
A mode which does not rely as much on proper randomness for the IV
would be somewhat nice to have, but it seems that the only choice
there is SIV, which has received less scrutiny than other modes.
Interesting.  But it will take about five years until our crypto code
would make use of a new hardware instruction, assuming that we'd
implement all the necessary pieces right now (thanks to desynchronized
software release cycles at several layers of the software stack).
Speed is of no particular concern anyway.  Increase in message size is
somewhat relevant (think about the URL case I mentioned), but only to
up to a degree.

@_date: 2010-09-22 08:16:13
@_author: Florian Weimer 
@_subject: Haystack redux 
* Adam Fields:
I wouldn't be surprised if there are plenty such tools in circulation
which are used by various dissident groups.  It's a cost-effective way
to infiltrate them.
The problem with such tools is that you can't really know how is
listening in on the proxies.  Even if the software itself contains no
backdoors, the service as a whole might still be compromised.  Even if
the proxies are trustworthy, your usage of the tool can very likely be
discovered by traffic analysis (and usage patterns as well, if you're
unlucky, and increasingly so if the service has low latency).
There is no technical solution to oppressive governments (or
non-trustworthy ISPs, for that matter).  After all, if you're
anonymous and oppressed, you're still oppressed.
