
@_date: 2002-02-09 23:52:19
@_author: Simon Josefsson 
@_subject: PGP & GPG compatibility 
I believe it would be fruitful to separate the secure email message
formats (S/MIME vs PGP/MIME, or perhaps CMS vs OpenPGP) from the key
trust mechanism (PKI CA vs PGP web of trust).  In theory I cannot see
why one decision need to affect the other, they could be orthogonal
issues.  Perhaps by reading the relevant standards creatively, a
mailer sending S/MIME messages but uses a OpenPGP implementation
locally is already possible.

@_date: 2002-11-08 18:45:53
@_author: Simon Josefsson 
@_subject: did you really expunge that key? 
I find that this thread doesn't discuss the threat model behind
"expunging" keys, and this statement finally triggered my question.
On which systems is all this really an issue, and when?  Which
operating systems "leak" memory between processes in this way?  Which
operating systems swap out processes to disk that can be read by
non-privileged users?  Which operating systems write core dumps that
can be read by non-privileged users?  My gut feeling tells me that if
you can allocate memory on a system, there are easier way to attack it.

@_date: 2002-11-09 06:01:25
@_author: Simon Josefsson 
@_subject: did you really expunge that key? 
The documentation for the function says it will check read access
permissions.  Isn't this permission check done properly?  I.e.,
disallow memory reads across processes owned by different users.  If
so, this should be reported and fixed.  The remaining situation seems
to be if ReadProcessMemory() on the running process "leak" data
initialized by dead processed owned by other users, any pointers to
information on this case would be appreciated.
If you can run a program as root, aren't there easier way to discover
passwords than allocating memory initialized by other processes?
E.g., attaching a debugger to /bin/login.
My point is that the software in general cannot solve this without
help from the operating system.  In particular, software cannot
protect itself from operating systems bugs that reveal secret data
handled by the software.  If you run security software on a insecure
host, you won't achieve security no matter how good the security
software is.  A pair of functions secure_memory_allocate() and
secure_memory_zeroize() that handle "volatile char*" data, together
with a compiler that respects the volatile property, seems like a
useful interface.  No doubt, this already exists.

@_date: 2003-06-29 23:49:05
@_author: Simon Josefsson 
@_subject: Attacking networks using DHCP, DNS - probably kills DNSSEC 
No, I believe only one of the following situations can occur:
* Your laptop see and uses the name "yahoo.com", and the DNS server
  translate them into yahoo.com.attackersdomain.com.  If your laptop
  knows the DNSSEC root key, the attacker cannot spoof yahoo.com since
  it doesn't know the yahoo.com key.  This attack is essentially a
  man-in-the-middle attack between you and your recursive DNS server.
* Your laptop see and uses the name "yahoo.com.attackersdomain.com".
  You may be able to verify this using your DNSSEC root key, if the
  attackersdomain.com people have set up DNSSEC for their spoofed
  entries, but unless you are using bad software or judgment, you will
  not confuse this for the real "yahoo.com".
Of course, everything fails if you ALSO get your DNSSEC root key from
the DHCP server, but in this case you shouldn't expect to be secure.
I wouldn't be surprised if some people suggest pushing the DNSSEC root
key via DHCP though, because alas, getting the right key into the
laptop in the first place is a difficult problem.

@_date: 2003-06-30 06:58:02
@_author: Simon Josefsson 
@_subject: Attacking networks using DHCP, DNS - probably kills DNSSEC NOT 
It can be a useful feature, but it does not circumvent DNSSEC in any
way, that I can see.  DNSSEC see yahoo.com.attackersdomain.com and can
verify that the IP addresses for that host are the one that the owner
of the y.c.a.c domain publishes, and that is what DNSSEC delivers.
The bad judgement I referred to was if your software, after DNSSEC
verification, confuses yahoo.com with yahoo.com.attackersdomain.com.
I think it is simpler to have the DNSSEC root key installed with the
DNSSEC software.  If someone can replace the root key in that
distribution channel, they could also modify your DNSSEC software, so
you are no worse off.

@_date: 2003-10-02 18:37:33
@_author: Simon Josefsson 
@_subject: Monoculture 
Several people have now suggested using TLS, but nobody seem to also
refute the arguments made earlier against building VPNs over TCP, in
I have to agree with many things in the paper; using TCP (as TLS does)
to tunnel TCP/UDP is a bad idea.  Off-the-shelf TLS may be a good
security protocol, but it is not a good VPN protocol.  Recommending
TLS without understanding, or caring about, the application domain
seem almost arrogant to me.
Admittedly, you could invent a datagram-based TLS, but this is not
widely implemented nor specified (although I vaguely recall WTLS) so
then you are back at square one as far as security analysis goes.

@_date: 2005-08-29 15:58:48
@_author: Simon Josefsson 
@_subject: Fwd: Tor security advisory: DH handshake flaw 
Right.  The attack I mentioned was a tangent off the Fermat test.
However, controlling the prime numbers seem to be comparable to
controlling the random number generator.  I.e., you have some access
to the subject's hardware, and want to trick the software into using
crackable parameters for RSA, DH etc.  If the application doesn't use
prime numbers without a proof, neither of these two attacks aren't
possible.  So this is actually a class of attacks.
No, the certificate is verifiable in deterministic polynomial time.
The test is probabilistic, though, but as long as it works, I don't
see why that matters.  However, I suspect the ANSI X9.80 or ISO 18032
paths are more promising.  I was just tossing out URLs.

@_date: 2005-08-29 17:32:47
@_author: Simon Josefsson 
@_subject: Fwd: Tor security advisory: DH handshake flaw 
I wonder if the original author didn't think of Carmichael numbers,
which are Fermat pseudoprime in every base.  Some applications,
e.g. Libgcrypt used by GnuPG, use Fermat tests, so if you have control
of the random number generator, I believe you could make GnuPG believe
it has found a prime when it only found a Carmichael number.
However, for Miller-Rabin, it has been proven that all composite
numbers pass the test for at most 1/4 of the possible bases.  So as
long as you do sufficiently many independent tests (different bases,
preferably chosen at random), I don't see how you could be fooled.
Doing the test for more than 1/4 of the bases (which would actually
prove the number prime, although without a succinct witness) for large
numbers is too expensive though.
One algorithm that results in a polynomially verifiable witness is:
Almost All Primes Can be Quickly Certified
Btw, I've been playing with prime proving in the past, and if you want
to specify a format for prime proofs that OpenSSL would understand, I
would consider supporting the same format in GnuTLS.  Trusting that
numbers are prime for cryptographic purposes should require a proof.
There are several prime proof formats, but I can't tell if they are
practical for this purpose.

@_date: 2005-08-31 10:42:41
@_author: Simon Josefsson 
@_subject: Fwd: Tor security advisory: DH handshake flaw 
Yes, but it doesn't produce certificates; the algorithm that I cited
do.  The algorithm to _verify_ the certificate was not probabilistic,
only the algorithm to _produce_ the certificates was probabilistic.
Btw, could you describe the threat scenario where you believe this
test would be useful?

@_date: 2005-11-07 22:50:16
@_author: Simon Josefsson 
@_subject: GnuTLS 1.2.9 
I thought that this might be of some interest; I just released a new
version of GnuTLS that disable RSA-MD5 for some X.509 uses by default.
See announcements and details below.
Feedback and suggestions are always welcome.
We are pleased to announce the availability of GnuTLS version 1.2.8.
GnuTLS is a modern C library that implement the standard network
security protocol Transport Layer Security (TLS), for use by network
This is the last non-bugfix release in the 1.2.x series.  We will open
the 1.3.x branch after this release.  The goal of 1.3.x will be to
merge work currently done on CVS branches, for TLS Pre-Shared-Keys and
TLS Inner Application.  Other planned improvements in 1.3.x are
system-independent resume data structures, modularization of the
bignum operations, and TLS OpenPGP improvements.
This release disable the RSA-MD5 algorithm when verifying untrusted
intermediary X.509 CA certificates.  This decision was made based on
the results in Lenstra, Wang and Weger's "Colliding X.509
Certificates".  This is discussed in more detail, including
instructions on how to re-enable the algorithm for application's that
need backwards compatibility, in:
Noteworthy changes since version 1.2.8:
- Documentation was updated and improved.
- RSA-MD2 is now supported for verifying digital signatures.
- Due to cryptographic advances, verifying untrusted X.509
  certificates signed with RSA-MD2 or RSA-MD5 will now fail with a
  GNUTLS_CERT_INSECURE_ALGORITHM verification output.  For
  applications that must remain interoperable, you can use the
  GNUTLS_VERIFY_ALLOW_SIGN_RSA_MD2 or GNUTLS_VERIFY_ALLOW_SIGN_RSA_MD5
  flags when verifying certificates.  Naturally, this is not
  recommended default behaviour for applications.  To enable the
  broken algorithms, call gnutls_certificate_set_verify_flags with the
  proper flag, to change the verification mode used by
  gnutls_certificate_verify_peers2.
- Make it possible to send empty data through gnutls_record_send,
  to align with the send(2) API.
- Some changes in the certificate receiving part of handshake to prevent
  some possible errors with non-blocking servers.
- Added numeric version symbols to permit simple CPP-based feature
  tests, suggested by Daniel Stenberg .
- The (experimental) low-level crypto alternative to libgcrypt used
  earlier (Nettle) has been replaced with crypto code from gnulib.
  This leads to easier re-use of these components in other projects,
  leading to more review and simpler maintenance.  The new configure
  parameter --with-builtin-crypto replace the old --with-nettle, and
  must be used if you wish to enable this functionality.  See README
  under "Experimental" for more information.  Internally, GnuTLS has
  been updated to use the new "Generic Crypto" API in gl/gc.h.  The
  API is similar to the old crypto/gc.h, because the gnulib code were
  based on GnuTLS's gc.h.
- Fix compiler warning in the "anonself" self test.
- API and ABI modifications:
gnutls_x509_crt_list_verify: Added 'const' to prototype in .
                             This doesn't reflect a change in behaviour,
                             so we don't break backwards compatibility.
GNUTLS_MAC_MD2: New gnutls_mac_algorithm_t value.
GNUTLS_DIG_MD2: New gnutls_digest_algorithm_t value.
GNUTLS_VERIFY_ALLOW_SIGN_RSA_MD5: New gnutls_certificate_verify_flags values.
                                  Use when calling
                                  gnutls_x509_crt_list_verify,
                                  gnutls_x509_crt_verify, or
                                  gnutls_certificate_set_verify_flags.
GNUTLS_CERT_INSECURE_ALGORITHM: New gnutls_certificate_status_t value,
                                used when broken signature algorithms
                                is used (currently RSA-MD2/MD5).
LIBGNUTLS_VERSION_NUMBER: New CPP symbols, indicating the GnuTLS
Improving GnuTLS is costly, but you can help!  We are looking for
organizations that find GnuTLS useful and wish to contribute back.
You can contribute by reporting bugs, improve the software, or donate
money or equipment.
Commercial support contracts for GnuTLS are available, and they help
finance continued maintenance.  Simon Josefsson Datakonsult, a
Stockholm based privately held company, is currently funding GnuTLS
maintenance.  We are always looking for interesting development
If you need help to use GnuTLS, or want to help others, you are
invited to join our help-gnutls mailing list, see:
The project page of the library is available at:
       (updated fastest)
Here are the compressed sources:
   (2.7MB)
  ftp://ftp.gnutls.org/pub/gnutls/gnutls-1.2.9.tar.bz2 (2.7MB)
Here are GPG detached signatures signed using key 0xB565716F:
    ftp://ftp.gnutls.org/pub/gnutls/gnutls-1.2.9.tar.bz2.sig
The software is cryptographically signed by the author using an
OpenPGP key identified by the following information:
  1280R/B565716F 2002-05-05 [expires: 2006-02-28]
  Key fingerprint = 0424 D4EE 81A0 E3D1 19C6  F835 EDA2 1E94 B565 716F
The key is available from:
    dns:b565716f.josefsson.org?TYPE=CERT
Here are the build reports for various platforms:
  Here are the SHA-1 checksums:
7229d094de83cabd572fcaab806ab3afc6b58959  gnutls-1.2.9.tar.bz2
fae5d7a5d84935406ba3ed6e2804a18cede6fcf1  gnutls-1.2.9.tar.bz2.sig
Nikos and Simon

@_date: 2005-11-09 17:27:12
@_author: Simon Josefsson 
@_subject: RSA-640 factored 
There are timing details in:
They claim they need 5 months of 80 machines with 2.2GHz processors.
Using these numbers, I think it would be interesting to come up with
an estimate of how expensive it would be to crack larger RSA keys for
someone who used the same software.  I'll make an attempt to do this
below, but I reckon I will make errors...  please correct me.
The complexity for the GNFS is roughly
O(exp(1.9(log n)^.3 * (log log n)^.66)
where n is the number to factor, according to
I'm not sure translating complexity into running time is reasonable,
but pending other ideas, this is a first sketch.
Let's input the numbers for 2^640:
octave:26> n=2^640
n =  4.5624e+192
octave:27> a=e^(1.923*(log(n))^(1/3)*(log(log(n)))^(2/3))
a =  1.7890e+21
And let's input them for 2^768:
octave:28> n=2^768
n =  1.5525e+231
octave:29> b=e^(1.923*(log(n))^(1/3)*(log(log(n)))^(2/3))
b =  1.0776e+23
Let's compute the difference:
octave:30> b/a
ans = 60.232
In other words, cracking a RSA-768 key would take 60 times as long,
assuming the running time scale exactly as the complexity (which is
So it seems, if you have 80*60 = 4800 machines, you would be able to
crack a RSA-768 key in 5 months.
Continuing this to 1024 bit keys...  (or rather 1023 since Octave
believe 2^1024=Inf)
octave:40> n=2^1023
n =  8.9885e+307
octave:41> c=e^(1.923*(log(n))^(1/3)*(log(log(n)))^(2/3))
c =  1.2827e+26
octave:42> c/a
ans =  7.1697e+04
I.e., RSA-1024 is about 70000 times as difficult as RSA-640 using
GNFS.  If you have 80*70000 = 5600000 machines, you would be able to
crack a 1024 bit RSA keys in 5 months.  Or put differently, if you had
10.000 CPUs it would take 5*80*70000/10000/12 = 233 years to factor a
RSA-1024 key.
I know there are many hidden assumptions here, and I probably made
mistakes when computing this.  Please point out flaws so we can get
accurate numbers.

@_date: 2005-11-09 18:33:35
@_author: Simon Josefsson 
@_subject: RSA-640 factored 
Can we deduct a complexity expression from it, that could be used to
(at least somewhat reliably) predict the cost of cracking RSA-768 or
or RSA-1024, based on the timing information given in this report?
The announcement doesn't say how much memory these machines had,
though, but perhaps that information can be disclosed.

@_date: 2005-09-01 15:04:43
@_author: Simon Josefsson 
@_subject: Fwd: Tor security advisory: DH handshake flaw 
If you control the random number generator, you control which
Miller-Rabin bases that are used too.
Of course, it must be realized that the threat scenario here is
slightly obscure.  The scenario I have been thinking about is when an
attacker has gained control of the hardware or kernel.  The attacker
might then be able to see when a crypto library requests randomness,
and return carefully constructed data to fool the user.  The
constructed data should be so the RSA/DH parameters become weak [for
the attacker].  The attacker may not be in a position to send the
generated prime back home over the network, and doing that may also be
detected by firewalls.  The target system might not even be networked.
Designing this fake random number generator is not trivial, and must
likely be done separately for each crypto library that is used.  If
software only used prime numbers that came with a prime certificate,
you combat this attack.
Too bad you can't mathematically certify that "real" randomness was
used in choosing the prime too.  Although perhaps you get pretty close
with algorithms that both generate a prime and a prime certificate in
one go.

@_date: 2006-02-08 18:47:52
@_author: Simon Josefsson 
@_subject: general defensive crypto coding principles 
I wonder whether this is really a good suggestion, considering
Krawczyk's paper that show that this construct is not generically
secure.  See .
I'd say this leads to complex code that is difficult to audit.
Consequently a questionable recommendation.
A 1024 bit RSA key is not twice as strong as a 512 bit RSA key, nor is
a 2048 bit RSA key twice as strong as a 1024 bit RSA key.  Factoring
algorithm don't scale that way.
I believe it is now well accepted that this idiom is a poor idea for
security products in particular, and perhaps also in general.  You
should _not_ be liberal in what you accept.
Are you sure the above is the reason for this?  Perhaps the sshd
simply didn't support 4096 bit keys.

@_date: 2006-02-11 12:36:52
@_author: Simon Josefsson 
@_subject: GnuTLS (libgrypt really) and Postfix 
We have made some efforts, but the design of libgcrypt has not
changed.  libgcrypt is problematic for two reasons:
  1) It invoke exit, as you have noticed.  While this only happen
     in extreme and fatal situations, and not during runtime,
     it is not that serious.  Yet, I agree it is poor design to
     do this in a library.
  2) If used in a threaded environment, it wants to have access to
     thread primitives.  The primary reason was for RNG pool locking
     (where it is critical), but I think the primitives are now used
     in other places too.  GnuTLS is thread agnostic, so it can't
     initialize libgcrypt properly.
I have decided that the best approach to solve this is to make GnuTLS
crypto-agnostic.  We now invoke libgcrypt through a simpler wrapper
API.  I have completed this for hashes, symmetric encryption, RNG, but
MPI and PK operations are not yet moved from the libgcrypt to the
general API.  We have started to integrate LibTomMath for the MPI
library, but not yet finished that work.  (In case you are wondering
why we didn't use GMP: it also calls exit, and may have blinding and
timing issues, otherwise I really like GMP.  I know of no other MPI
libraries with a free license and of similar high code quality that
We've made some progress on the RNG issue on the GnuTLS discussion
list.  I think the Linux /dev/urandom implementation is sub-optimal.
When used heavily, it depletes the /dev/random pool, causing
applications that access /dev/random to stall.  I believe /dev/urandom
should be changed, to be a PRNG (re-)seeded with strong randomness
from the /dev/random pool, rather than simply taking entropy from the
interface in Linux is rather unreadable IMHO.  The code for it should
not be as complicated it is today.
This is an area that is very much alive, and if any prospective
developers here want to help with this effort, that would be greatly
appreciated.  Please join the GnuTLS-developer list if you are

@_date: 2006-02-13 11:29:00
@_author: Simon Josefsson 
@_subject: GnuTLS (libgrypt really) and Postfix 
That /dev/random doesn't exist seem like a quite possible state to me.
The application would want to shut down gracefully when the library
detect that condition.  The application may be processing files in
different threads.
Further, a library is not in a good position to report errors.  A
users will sit there wondering why Postfix, or some other complex
application died, without any clues.  Returning an error and providing
a foo_strerror() function at least make it possible to report a useful
error to the user.
I would agree if we are only talking about truly fatal cases, like
asserts() to check explicit pre-conditions for a function, but I
disagree when we move into the area if easily anticipated problems.
However, looking at the code, it is possible for Postfix to handle
this.  They could have installed a log handler with libgcrypt, and
make sure to shut down gracefully if the log level is FATAL.  The
recommendation to avoid GnuTLS because libgcrypt calls exit suggest
that the Postfix developers didn't care to investigate how to use
GnuTLS and libgcrypt properly.  So I don't think there is any real
reason to change code in libgcrypt here.  Postfix could be changed, if
they care about GnuTLS/libgcrypt.
I'd say that the most flexible approach for a library is to write
thread-safe code that doesn't need access to mutexes to work properly.
Implementing the RNG functions like this is a challenge, and may
require kernel-level support (see below), but giving up and requiring
thread hooks seem sub-optimal.
That seem like a poor argument to me.  It may be valid for embedded
devices, but for most desktop PCs, Linux should provide a useful
It seems that it would be possible to write a new /dev/*random
implementation that is more useful by libgcrypt and other RNG

@_date: 2006-09-12 16:43:24
@_author: Simon Josefsson 
@_subject: Exponent 3 damage spreads... 
There are actually two problems to consider...
First, there is the situation by Bleichenbacher at Crypto 06 and
explained in:
That uses the fact that implementation doesn't check for data beyond
the end of the ASN.1 structure.  OpenSSL was vulnerable to this,
GnuTLS was not, see my analysis for GnuTLS on this at:
Eric already posted test vectors that trigger this problem.
The second problem is that the "parameters" field can ALSO be used to
store data that may be used to manipulate the signature value into
being a cube.  To my knowledge, this was discovered by Yutaka Oiwa,
Kazukuni Kobara, Hajime Watanabe.  I didn't attend Crypto 06, but as
far as I understand from Hal's post, this aspect was not discussed.
Their analysis isn't public yet, as far as I know.
Both OpenSSL and GnuTLS were vulnerable to the second problem.  My
discussion of this for GnuTLS is in:
When I read the OpenSSL advisory, I get the impression that it doesn't
quite spell out the second problem clearly, but if you look at the
patch to correct this:
+		/* Excess data can be used to create forgeries */
+		if(p != s+i)
+			{
+			RSAerr(RSA_F_RSA_VERIFY,RSA_R_BAD_SIGNATURE);
+			goto err;
+			}
+		/* Parameters to the signature algorithm can also be used to
+		   create forgeries */
+		if(sig->algor->parameter
+		   && sig->algor->parameter->type != V_ASN1_NULL)
+			{
+			RSAerr(RSA_F_RSA_VERIFY,RSA_R_BAD_SIGNATURE);
+			goto err;
+			}
You'll notice that there are two added checks, one check per problem.
Test vectors for this second problem are as below, created by Yutaka
jas at mocca:~/src/gnutls/tests$ cat pkcs1-pad-ok.pem
-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----
jas at mocca:~/src/gnutls/tests$ ../src/certtool -e < pkcs1-pad-ok.pem
Certificate[0]: C=JP,ST=Tokyo,O=TEST 2 CLIENT,CN=www2.example.jp
        Issued by: C=JP,O=CA TEST 1-4,CN=CA TEST 1-4
        Verifying against certificate[1].
        Verification output: Verified.
Certificate[1]: C=JP,O=CA TEST 1-4,CN=CA TEST 1-4
        Issued by: C=JP,O=CA TEST 1-4,CN=CA TEST 1-4
        Verification output: Verified.
jas at mocca:~/src/gnutls/tests$ cat pkcs1-pad-broken.pem
-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----
jas at mocca:~/src/gnutls/tests$ ../src/certtool -e < pkcs1-pad-broken.pem
Certificate[0]: C=JP,ST=Tokyo,O=TEST 2 CLIENT,CN=www2.example.jp
        Issued by: C=JP,O=CA TEST 1-4,CN=CA TEST 1-4
        Verifying against certificate[1].
        Verification output: Not verified.
Certificate[1]: C=JP,O=CA TEST 1-4,CN=CA TEST 1-4
        Issued by: C=JP,O=CA TEST 1-4,CN=CA TEST 1-4
        Verification output: Verified.
jas at mocca:~/src/gnutls/tests$
If the above commands were used with an older GnuTLS 'certtool', they
would claim that the chain in pkcs1-pad-broken.pem is valid.
For reference, the certificate key:
-----BEGIN RSA PRIVATE KEY-----
-----END RSA PRIVATE KEY-----

@_date: 2006-09-14 10:37:45
@_author: Simon Josefsson 
@_subject: Exponent 3 damage spreads... 
Yes.  Implementations that didn't validate the parameters field are
potentially vulnerable; the attacker can put garbage in the parameters
field to make the signature value a cube.  Look at the certificates I
I don't think so.  The contents of the parameters field depends on the
hash algorithm.  As far as I know (but I didn't read the scriptures),
for normal hashes like SHA-1 the parameters field should not be used.
Checking that it is empty shouldn't be a problem.
Or do you know of real certificates with a non-NULL parameters field
in the signature?
It is important to keep in mind that this only applies to incorrect
implementations that handle keys with e=3.  Using Debian's
extracted the issuer name of the CAs with e=3:
Issuer: C=US,O=Digital Signature Trust Co.,OU=DSTCA E1
Issuer: C=US,O=Digital Signature Trust Co.,OU=DSTCA E2
Issuer: C=US,O=Entrust.net,OU= incorp. by ref. limits liab.,OU=(c) 1999 Entrust.net Limited,CN=Entrust.net Client Certification Authority
Issuer: C=US,O=Entrust.net,OU= incorp. by ref. (limits liab.),OU=(c) 1999 Entrust.net Limited,CN=Entrust.net Secure Server Certification Authority
I'm not familiar with DST, so I wonder whether those two are widely
used.   doesn't use it.
That leaves two Entrust certificates.  At least
 is protected by the second certificate above,
so it may be in wide use.

@_date: 2006-09-14 19:28:37
@_author: Simon Josefsson 
@_subject: Exponent 3 damage spreads... 
I see nothing fatal yet.  RFC 2630 says this:
   The AlgorithmIdentifier parameters field is optional.  If present,
   the parameters field must contain an ASN.1 NULL.  Implementations
   should accept SHA-1 AlgorithmIdentifiers with absent parameters as
   well as NULL parameters.  Implementations should generate SHA-1
   AlgorithmIdentifiers with NULL parameters.
   The AlgorithmIdentifier parameters field must be present, and the
   parameters field must contain NULL.  Implementations may accept the
   MD5 AlgorithmIdentifiers with absent parameters as well as NULL
   parameters.
GnuTLS follows this and permits both an absent parameters field, or a
field containing an ASN.1 NULL value (0x0500).  In the latest release,
it does not permit anything else.
The attacker can chose one of two encodings: missing parameters field,
or a NULL parameters field.  That's not flexibility enough to make the
value a cube, I believe.
If so, the implementation will have to parse the parameters field and
make sure it is valid.  But as you note later on, that may not always
be possible...
Now this is interesting.  A parameters field that contain fields such
as a longer IV -- that can't be validated because there is no
structure in it -- may be abused to make the value a cube.
Deploying a hash widely isn't done easily, though.  GnuTLS only
support MD2, MD5, SHA-1 and RIPEMD (of which MD2/MD5 are by default
not used to verify signatures).
The signature is the cube root of a perfect cube, where the perfect
cube is the PKCS value 00 01 FF ... FF 00 ASN.1-DigestInfo.
An interesting question is how much work would there be in varying the
input to modify the hash in the PKCS string until the PKCS string
becomes a cube?  It probably takes quite a number of hashes.
I could be seriously confused, but as far as I have understood the
math so far, I would agree.
Wouldn't e=17 border on being problematic as well?  You'll need a
larger modulus though, but the approach appear to be the same...
Perhaps the modulus becomes sufficient large so that people aren't
using those modulus sizes, though.

@_date: 2006-09-18 10:39:14
@_author: Simon Josefsson 
@_subject: Why the exponent 3 error happened: 
That's not true -- PKCS implicitly require that check.  PKCS says
the verification algorithm should generating a new signature and then
compare them.  See RFC 3447 section 8.2.2.  That solves the problem.
Again, there is no problem in ASN.1 or PKCS that is being exploited
here, only an implementation flaw, even if it is an interesting one.
After reading  it
occurred to me that section 4.2 of it describes a somewhat related
problem, where the hash OID is modified instead.  That attack require
changes in specifications and implementations, to have the
implementation support the new hash OID.  But it suggests a potential
new problem too: if implementation don't verify that the parsed hash
OID length is correct.  E.g., an implementation that uses
memcmp (parsed-hash-oid, sha1-hash-oid,
         MIN (length (parsed-hash-oid), length (sha1-hash-oid)))
to recognize the hash algorithm used in the ASN.1 structure, it may
also be vulnerable: the parsed-hash-oid may contain "garbage", that
can be used to "forge" signatures against broken implementations,
similar to the two attacks discussed so far.  I don't know of any
implementations that do this, though.

@_date: 2006-09-21 18:28:17
@_author: Simon Josefsson 
@_subject: Exponent 3 damage spreads... 
Not using e=3 when generating a key seems like an easy sell.
A harder sell might be whether widely deployed implementations such as
TLS should start to reject signatures done with an e=3 RSA key.
What do people think, is there sufficient grounds for actually
_rejecting_ e=3 signatures?
One alternative would be to produce a warning, similar to what is
sometimes done for MD2 and MD5 today.
Btw, by default, OpenSSH's ssh-keygen appear to use e=35 (0x23..),
GnuPG (libgcrypt), GnuTLS and OpenSSL appear to all use e=65537, BIND
dnssec-keygen appear to use e=3.

@_date: 2006-09-22 15:54:35
@_author: Simon Josefsson 
@_subject: Exponent 3 damage spreads... 
We have at least three independent widely used implementations that
got things wrong: OpenSSL, Mozilla NSS, and GnuTLS.
However, note that this isn't a single problem; we are talking about
at least two related problems.  Some implementations are vulnerable to
only one of them.
The first problem was ignoring data _after_ the ASN.1 blob.
Vulnerable: OpenSSL, NSS?
The second problem was ignoring data _in_ the ASN.1 blob, in
particular, in the parameters field.  Vulnerable: OpenSSL, GnuTLS,
A several year old paper by Kaliski discussed using the ASN.1 OID to
store data in.  It has slightly different properties, but the lesson
in this context is that implementations must properly check the ASN.1
OID field too.
I hope that I convinced you that this isn't an open question.

@_date: 2006-09-25 10:29:27
@_author: Simon Josefsson 
@_subject: Exponent 3 damage spreads... 
Yes.  I'm only familiar with NSS as a user, not as a developer.  For
some reason, the Mozilla bug tracker hides information about this
problem from us, so it is difficult to track the code down.
I believe I identified the patch that solved the problem in NSS,
search for "350640" in:
The bug discussion is not public:
Possibly also bug reports 351079 and 351848 are related to the same
problem, but these bugs are also hidden.
The actual patch for 350640 is:
If some NSS developer could chime in, that would help.
I think you want to read up on free software license compatibilities,
and in particular OpenSSL vs GPL.  But this is a very different topic,
that we shouldn't pursue here...
At least some versions of PKCS does NOT say that, e.g., RFC 3447.
RFC 3447 essentially says to generate a new token and use memcmp().
Such implementations would not be vulnerable to any of the current
attacks, except the Kaliski ASN.1 OID attack (an attack that doesn't
work on existing implementations).
I believe the principle of "Be conservative in what you do; be liberal
in which you accept from others" is generally a bad advice.
The principle hides problems that should be fixed.  Hiding a problem
instead of fixing it typically enables bad things to happen.  We've
seen that lead to security problems for many years, this is just one
