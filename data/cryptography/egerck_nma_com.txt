
@_date: 2001-10-03 16:45:57
@_author: Ed Gerck 
@_subject: dejavu, Re: Hijackers' e-mails were unencrypted 
With all due respect to the need to vent our fears, may I remind
this list that we have all seen this before (that is, governments
trying to control crypto), from key-escrow to GAK, and we all
know that it will not work -- and for many reasons.  A main one
IMO is that it is simply impossible to prevent anyone from
sending an encrypted message to anyone else except by
controlling the receivers and the transmitters (as done in WWII,
for example). Since controlling receivers and transmitters is
now really impossible, all one can do is control routing and
addresses. I suggest this would be a much more efficient way
to reduce the misuse of our communication networks. For
example, if one email address under surveillance receives
email from X, Y and Z, then X, Y and Z will also be added
to the surveillance. Even if everything is encrypted, people
and computers can be verified.
In addition, we also need to avoid to add fuel to that misconception,
that  encryption is somehow  "dangerous" or should be controlled
as weapons are. The only function of a weapon is to inflict harm.
The only function of encryption is to  provide privacy.
Ed Gerck

@_date: 2001-10-05 08:38:00
@_author: Ed Gerck 
@_subject: dejavu, Re: Hijackers' e-mails were unencrypted 
My point is not that a government would not, but that a government
could not control the use of crypto.  It would not work.
My suggestion was that controlling routing and addresses would
be much more efficient and would NOT require new laws and
ersosion of communication privacy.
Maybe it's time to put sanity back into the DMCA crying.
In the infamous case of Microsoft vs. Stacker many years ago, when MS
was found guilty of using Stacker's code in a MS product, Stacker was
nonetheless found guilty of proving it by reverse engineering -- in a
notion similar to trespassing.
So, as stressed in that judicial case that predates DMCA, if I would get a
court order to reverse engineer the latest commercial "security solution"
and be allowed to publish the results, I would remain free and within
the legal limits. Otherwise, I would not -- DMCA or not.
Ed Gerck

@_date: 2002-08-30 13:30:42
@_author: Ed Gerck 
@_subject: Quantum computers inch closer? 
I'm a quantum physicist. Your argument is good but it has
nothing to do with quantum physics. The claim that feistel
ciphers are fairly secure against QC has to do with a
complex calculation that has no counterpart in a physical
system that could be used to "calculate" it. Not that the
calculation is not possible, but that it cannot be efficiently
transposed to a QC. Other ciphers may be a lot easier in this
regard  -- for example, there is a good similarity between
factoring the product of two primes and calculating
standing wave harmonics in a suitable quantum system.
Ed Gerck

@_date: 2002-12-16 09:45:41
@_author: Ed Gerck 
@_subject: Micropayments, redux 
What follows below is from my dialogue with Ron
earlier this year, when the design was still being
worked out as he told me, when he kindly answered
some of my remarks --  which I also report below.
This is a very interesting proposal that creates a
large aggregate value worth billing for (in terms
of all operational and overhead costs), but which
large value the user will pay *on average*.
The user has a limit, and one idea is that the user
would pre-pay it (which may raise questions about
creating a barrier against spontaneous buying but
could be presented as an authorized credit limit,
I think) and then spend the limit in thousands (or more)
of "peppercorn-worth" (i.e., very small value -- maybe
cents or fractions of cents)  transactions that would be
paid only *on average*.  That is, most of the peppercorn
transactions would go *unpaid* and *unprocessed* -- thus,
with near zero overhead. However, some transactions would
hit the "jackpot" and be charged with a multiplicative
factor that -- on average -- pays for all unpaid transactions
and overhead.
Thus, because of the limit and the prepay, this can be seen
as a game that has no possible underpaying strategy
for the user, and the bank would be happy to let the
user play it as often as he likes -- with the following
1. If there is no limit, then the well-known doubling
strategy would allow the user to, eventually, make the
bank lose -- the user getting a net profit.
2. If there is no prepaid amount, lucky users could quit
"while ahead" -- which would hurt the bank since those
users would be out of the pool to be charged, but they
have used the service.
3. The game is fair -- the bank will not "weigh the
wheel" (and hurt the users) and no one can compromise
the methods used by the bank (and hurt the bank).
Of course, if the wheel is not exactly balanced,
or if the house takes a cut in some other way,
then the user or the bank are losing ground at each
Another question, which answer I guess is more
market-related than crypto-related, is whether banks
will accept the liability of a losing streak ...for them.
Likewise, users may lack motivation to continue using
the system if they have a losing streak (i.e., if they run
out of their prepaid amount sooner than what they and
the bank expects, and pre-pay again, and again run out
of money sooner than expected, and again until they
give up to be on the losing side). The problem here
is that, all things being fair, the system depends on
unlimited time to average things out.  This can be
compensated, I'd expect, by adequate human monitoring
and insurance. As always, it is not only the math that makes
things work -- even though it's also the math.
All things considered, though, as I said above this is a
very interesting proposal because it does reduce
processing and overhead costs to near zero for a large
number of transactions. I'd refrain from saying "zero"
because there should be some auditing involved for
all transactions.
Ed Gerck

@_date: 2002-12-16 12:53:50
@_author: Ed Gerck 
@_subject: Micropayments, redux 
I'm happy you don't see any problems and I don't see
them either -- within the constraints I mentioned. But
if you work outside those   and  constraints
you would have problems, which is something you may
want to look further into.
For example, in reply to my constraint   you say:
 "This is expected to be roughly counterbalanced by the
 number of unlucky users who quite (sic) "while behind"."
but these events occur under different models. If there
is no prepayment (which is my point  then many users
can quit after few transactions and there is no statistical
barrier to limit this behavior. On the other hand, the number
of users who quit after being unlucky is a matter of statistics.
These are apples and speedboats. You ned to have an
implementation barrier to handle Ed Gerck

@_date: 2002-07-03 13:35:25
@_author: Ed Gerck 
@_subject: wrong data model -- Re: MS DRMOS Palladium -- The Trojan Horse OS 
There is no reason IMO to talk about economics when basic
properties are being ignored.
DRMOS will fail for pretty much the same basic reason that PKI
is failing. We are still trying to create an absolute reference
to measure "distance" in dataspace, when such reference
cannot exist by definition. Data is not an absolute property.
Choosing a reference, and even trying to enforce it, is illusory.
Distance can be measured without extrinsic references and this
is the only model that fits the properties that we need to assign
to data.
A wrong data model is being used, which nonetheless may still
sound intuitive. But one cannot revoke the law of gravity, even
though one might have a good market for such.
Ed Gerck

@_date: 2002-06-19 17:42:17
@_author: Ed Gerck 
@_subject: FC: E-voting paper analyzes "usability" problems of currentsystems 
[Moderator's note: I'm not sure I agree with Mr. Gerck's conclusion,
given that I don't think the proof is incorrect, but... --Perry]
The conclusion is incorrect. There is actually more than one way
to provide for ballot privacy and use effective audit trails in
electronic voting systems.
One way is to have a (sufficiently redundant) witness system
that records what the voter sees and approves as the ballot
is cast by the voter, without recording who the voter is. The
witness system can include independent witnesses controlled
by  every party or observer of the election. The vote tally result
can be verified with a confidence level as close to 100% as desired
by tallying a percentage of those witness records.  The theoretical
basis for such a system is Shannon's 10th theorem.  For a presentation,
see  Another way is to provide each voter with a double-blind
digital certificate that includes a nonce, and using homomorphic
enccryption for further protecting the voting pattern from
disclosing the voter's indentity (the Mafia attack) .  The nonce
allows for an effective audit trail per voter without disclosing the voter's
identity.  See  Ed Gerck

@_date: 2002-06-21 11:18:59
@_author: Ed Gerck 
@_subject: Shortcut digital signature verification failure 
A DoS would not pitch one client against one server. A distributed attack
using several clients could overcome any single server advantage.  A
scalable strategy would be a queue system for distributing load to
a pool of servers and a rating system for early rejection of repeated
bad queries from a source. The rating system would reset the source rating
after a pre-defined time, much like anti-congestion mechanisms on the Net.
Fast rejection of bogus signatures would help, but not alone.
Ed Gerck

@_date: 2002-06-26 16:56:12
@_author: Ed Gerck 
@_subject: TCPA / Palladium FAQ (was: Re: Ross's TCPA paper) 
Interesting Q&A paper and list comments. Three
additional comments:
1. DRM and privacy  look like apple and speedboats.
Privacy includes the option of not telling, which DRM
does not have.
2. Palladium looks like just another vaporware from
Microsoft, to preempt a market like when MS promised
Windows and killed IBM's OS/2 in the process.
3. Embedding keys in mass-produced chips has
great sales potential. Now we may have to upgrade
processors also because the key  is compromised ;-)
Ed Gerck
PS: We would be much better off with OS/2, IMO.

@_date: 2002-11-16 02:20:33
@_author: Ed Gerck 
@_subject: Secure Electronic and Internet Voting 
I want to spread the word about a newly published book
by Kluwer, where I have a chapter explaining Safevote's
technology and why we can do in voting (a much harder
problem) what e-commerce has not yet accomplished (it's
left as an exercise for the reader to figure out why e-commerce has not yet done it; hints by email if you wish). This book serves as a good introduction to other systems and some nay-sayers.  The book's URL is
With the US poised to test Internet voting in 2004/6, this book may provide useful, timely points for the discussion. We can't audit electrons but we can certainly
audit their pattern.
Ed Gerck

@_date: 2002-10-09 08:56:50
@_author: Ed Gerck 
@_subject: Microsoft marries RSA Security to Windows 
Tamper-resistant hardware is out, second channel with remote source is in.
Trust can be induced this way too, and better. There is no need for PRNG in plain
view, no seed value known. Delay time of 60 seconds (or more) is fine because
each one-time code applies only to one page served.
Please take a look at:
and Microsoft's move is good, RSA gets a good ride too, and the door may open
for a standards-based two-channel authentication method.
Ed Gerck

@_date: 2002-10-11 08:40:16
@_author: Ed Gerck 
@_subject: Microsoft marries RSA Security to Windows 
Great questions. Without aspiring to exhaust the answers, let me comment.
Cloning the cell phone has no effect unless you also have the credentials
to initiate the transaction. The cell phone cannot initiate the authentication
event. Of course, if you put a gun to the user's head you can get it all but
that is not the threat model.
A local solution on the PDA side is possible too, and may be helpful where
the mobile service may not work. However, it has less potential for wide
use. Today, 95% of all cell phones used in the US are SMS enabled.
Has no effect if the system is well-designed. It's possible to make it mandatory
(under strong crypto assurances) to enter the one-time code using the *same*
browser page provided in response to the authentication request -- which
page is supplied under server-authenticated SSL (no MITM).
No solution works everywhere. Cell phones are no exception. But it is
possible to design the system in a such a way that the user can use a different
access class (with less privileges, for example) if the cell phone does
not work. After all, the user is authenticated before the message is sent to
the cell phone.
That said, cell phone coverage is becoming ubiquitous and the solution also
works with pagers (while they still exist), email accounts (blackberrys) and
other means of communication -- including voice.
Let's be careful with generalizations. During the tragic events of 9/11, cell
phones emerged as the solution for communication  under a distributed terrorist
Second, as I hint somewhere above, the important point here is not to rely on
something that will never fail (which is, clearly, impossible) but to offer
the user a recourse -- alternative ways to get the job done, even under reduced
See above.
I see your argument backwards -- you are more likely to notice that
you lost or forgot your cell phone than a hardware token that you
seldom use, or even notice.  Cell phones also prevent a silent compromise
more effectively (as in theft, which involves no violence) because you
have a tendency to notice its absence -- the cell phone is sueful for many
other purposes!  And, you don't have to carry additional devices, that you
may lose or forget.
Additionally, just having my cell phone does not get you anywhere. You
also need my credentials to begin the authentication process and they are
NOT in my cell phone.
;-) nothing is free, but an SMS message is pretty close to that -- $0.05
to $0.10 per message. It's pay-as-you-go versus investing money
upfront on a hardware device that will also need replacement.
You use a pay phone or your wired phone. You can also use email.
It's costly, makes you carry an additional thing and -- most important
of all -- needs that pesky interface at the other end.
There is always room for evolution, and that's why we shan't run out of
work ;-)
However, not everyone wants to have an implant or carry a ring on their
finger -- which can be scanned and the subject targeted for a more serious
threat. My general remark on biometrics applies here -- when you are the
key (eg, your live fingerprint),  key compromise has the potential to be
much serious and harmful to you.
BTW, what is the main benefit of two-channel (as opposed to just two-factor)
authentication? The main benefit is that security can be assured even if the user's
credentials are compromised -- for example, by writing their passwords on stick-it
notes on their screen, or under their keyboards, or by using weak passwords, or
even having their passwords silently sniffed by malicious sofware/hardware,
problems that are very thorny  today and really have no solution but to add
another, independent, communication channel. Trust on authentication effectiveness
depends on using more than one channel, which is a general characteristic of trust
(   )
Ed Gerck

@_date: 2002-10-15 14:58:58
@_author: Ed Gerck 
@_subject: Microsoft marries RSA Security to Windows 
[I'm reducing the reply level to 2, for context please see former msg]
I like the medical record dialogue. But please note that what you wrote is
much stronger than asking "How did they get your hardware token too?"
because you could justifiably go for days without noticing that the hardware
token is missing but you (especially if you are an MD) would almost
immediately notice that your cell phone is missing. Traffic logs and call
parties for received and dialed calls could also be used to prove that you
indeed used your cell phone both before and after the improper access. Also,
if you lose your cell phone you are in a lot more trouble.
The point made here is that the aggregate value associated with the cell
phone used for receiving a SMS one-time code is always higher than that
associated with the hardware token (it is token +), hence its usefulness
in the security scheme. Denying possession of the cell phone would be
harder to do -- and easier to disprove -- than denying possession of the
hardware token.
The main objective of two-channel, two-factor authentication (as we
are discussing) is to prevent unauthorized access EVEN if the user's
credentials are compromised. This includes what you mentioned, in addition
to assuring authentication (i.e., preventing the user from cloning his account;
allowing enterprises to deny the unauthorized use of user's accounts).
Now, why should the second channel be provided ONLY by a hardware
token?  There is no such need, or security benefit.
The second channel can be provided by a hardware token, by an SMS-
enabled cell phone, by a pager or by ANY other means that creates a
second communication channel that is at least partially independent from
the first one. There is no requirement for the channels to be 100%
independent. Even though 100% independency is clearly desirable and can
be provided in some systems, it is hard to accomplish for a number of reasons
(indexing being one of them). In RSA SecurID, for example, the user's
PIN (which is a shared secret) is used both in the first channel (authenticating
the user) as well as in the second channel (authenticating the  passcode). Note also
that in SecurID systems without a PIN pad, the PIN is simply prefixed in plain
text to the random code and both are sent in the passcode.
The second channel could even be provided, for example, by an HTTPS (no
MITM) response in the same browser session (where the purported user
entered the correct credentials) if the response can be processed by an
independent means that is inacessible to others except the authorized user
(for example, a code book, an SMS query-response, a crypto calculator, etc.)
and the result fed back into the browser (i.e., as a challenge response).
There is nothing dowloaded on the cell phone.  Mobile RSA SecurID and
NMA ZSentryID are zero foot print applications.
BTW, requiring the download of a game or code opens another can of worms

@_date: 2002-10-22 10:29:49
@_author: Ed Gerck 
@_subject: Why is RMAC resistant to birthday attacks? 
Short answer:  Because the MAC tag is doubled in size.
Longer answer: The ?birthday paradox? says that if the MAC tag has t bits,
only 2^(t/2) queries to the MAC oracle are likely  needed in order to discover
two messages with the same tag, i.e., a ?collision,? from which forgeries
could easily be constructed. In RMAC, t is increased to 2t, so that
2^(2t/2) = 2^t and there is no reduction in the number of queries due to the
"birthday paradox". For example, for a MAC tag with 128-bit keys, the number
of queries that bound the chance of a forgery is still close to 128 bits. The
penalty is doubling the size of the MAC tag.
BTW, for MAC systems where collisions are prevented a priori, the
"birthday paradox" does not apply.
Ed Gerck

@_date: 2002-10-22 11:50:31
@_author: Ed Gerck 
@_subject: Why is RMAC resistant to birthday attacks? 
It does to (as you can read in the paper). BTW, the "easily" applies to the case
WITHOUT salt -- ie., without RMAC. But that's why RMAC was proposed ;-)
Ed Gerck

@_date: 2002-10-22 12:15:37
@_author: Ed Gerck 
@_subject: Why is RMAC resistant to birthday attacks? 
;-) please note that you already have one forgery...
BTW, it is important to look at the size of the internal chaining variable.
If it is 128-bit, this means that attacks with a 2^128 burden would likely
work. However, if only a subset of the MAC tag  is used OR if the
message to be hashed has a fixed length defined by the issuer, this is not
relevant. Only one of these conditions are needed.
Ed Gerck

@_date: 2002-10-22 12:31:47
@_author: Ed Gerck 
@_subject: Why is RMAC resistant to birthday attacks? 
My earlier comment to bear applies here as well -- this attack can be avoided
if only a subset of the MAC tag  is used OR if the message to be hashed has
a fixed length defined by the issuer. Only one of these conditions are needed.
except as above noted, which is easy to implement.
Ed Gerck

@_date: 2002-10-22 13:09:25
@_author: Ed Gerck 
@_subject: Why is RMAC resistant to birthday attacks? 
;-) your question was "Why is RMAC resistant to birthday attacks?"
Eve may just watch traffic that comes into her company's servers, knowing
the back-end plain text messages. No need to watch external networks. Eve
may also be, for example, one of those third-party monitoring services that
monitor traffic inside enterprise's networks for the purpose of "assuring security".
A birthday attack requires 2^(t/2) values, which looks surprising low -- hence
the name "paradox" (btw, this attack provides the mathematical model behind the
game of finding people with same birthday in a party, which works for a
surprisingly low number of people).  If you can get 2^(t/2) values, the attack
In an Internet message, datagrams can be inserted, dropped, duplicated, tampered
with or delivered out of order at the network layer (and often at the link layer). TCP
implements a reliable transport mechanism  and copes with the datagram unreliability
at the lower layers. However, TCP is unable to cope with a fraudulent datagram that is
crafted to pass TCP's protocol checks and is inserted into the datagram stream. That
datagram will be accepted by TCP and passed on to higher layers. A cryptographic
system operating  below TCP is needed to avoid this attack and filter out the deviant
datagrams -- and that's where you would use a MAC, if you want to protect each
datagram. It's not difficult, thus, to have more than 2^32 MACs in one message or
in a series of messages.
This is a scenario where it is not so difficult for an attacker to forge an acceptable
MAC for a datagram that was not sent in a given sequence, possibly tampering with
the upper-layer message and also making it more vulnerable to denial-of-service attacks.
Note that having a MAC above TCP does not prevent this attack, even though it can
detect it (and thus lead to a denial-of-service).
If birthday attack is a concern, RMAC is helpful. If not, then not.
Ed Gerck

@_date: 2002-10-22 14:32:55
@_author: Ed Gerck 
@_subject: Why is RMAC resistant to birthday attacks? 
Yes, subset -- not  a string with less N characters at the end. For example,
you can calculate the P subset as MAC mod P, for P smaller than
2^(bits in the MAC tag).
No. The attacker gets A and B, and sees that A = B. This does not mean
that a=b in  A = a mod P and B = b mod P.  The internal states are possibly
different even though the values seen by the attacker are the same.
Why do you think there is a "good chance"?
Note that all messages for which you can get a MAC have some fixed message
length M. The attacker cannot leverage a MAC value to calculate the state of
a M+1 length message -- exactly because this is prevented by making all messages
have length M.
Ed Gerck

@_date: 2002-10-22 15:06:28
@_author: Ed Gerck 
@_subject: Why is RMAC resistant to birthday attacks? 
No -- these are all independent things. One can build an RMAC wih SHA-1.
An RMAC does not have to use an HMAC scheme. One can also have an
HMAC hash-based MAC algorithm using a block cipher, that is not an RMAC.
That's is not the reason it was devised. The reason is to prevent a birthday attack
for 2^(t/2) tries on a MAC using a t-bit key. Needless to say, it also makes harder
to try a brute force attack.
Ed Gerck

@_date: 2002-10-22 16:59:37
@_author: Ed Gerck 
@_subject: Why is RMAC resistant to birthday attacks? 
A minor nit, but sometimes looking into why things were devised is helpful.
What I explained can be found in
and especially useful is the segment:
The RMAC algorithm was a refinement of the DMAC algorithm in which a random bit
string was exclusive-ORed into the second key and then appended to the resulting MAC
to form the tag. The birthday paradox in principle was no longer relevant, for, say, the
AES with 128 bit keys, because the tag would be doubled to 256 bits. Joux presented his
underlying security model and the properties that he had proven for RMAC: the number
of queries that bounded the chance of a forgery was relatively close to the number of 128
bit keys.
Ed Gerck

@_date: 2002-10-23 17:01:52
@_author: Ed Gerck 
@_subject: Why is RMAC resistant to birthday attacks? 
I think that there is a third (and dominating) possibility: this is a very bad MAC.
(A required property of MACs is providing a uniform distribution of values for a
change in any of the input bits, which makes the above sequence extremely
BTW, references for using MAC subsets OR fixed-length messages to prevent
guessing the internal chaining value should be straight forward to find in the
Ed Gerck

@_date: 2002-10-23 19:35:23
@_author: Ed Gerck 
@_subject: Why is RMAC resistant to birthday attacks? 
Actually, for any two (different) messages the internal collision probability
is bounded by the inverse of the SQUARE of the size of the internal state space.
You seem to say that even if some of the internal state is hidden from the MAC
tag, once an attacker sees a MAC collision he can deduct that an internal collision
occurred as well. If so, this is incorrect.
Not really. You can prevent internal collision attacks, for example, by using
the envelope method (e.g., HMAC) to set up the MAC message. In such a
case, having a previous message M the attacker can discover (e.g., by calculating
over a large number of messages) another message M* such that hash(M) =
hash(M*) -- i.e., an internal collision. However, finding out this internal collision
CANNOT be leveraged into subverting the receiving party in accepting M* as
Thus, without increasing the size of the internal search space AND without
preventing internal collisions by any other way, it is possible to prevent an
attack that would use an internal collision.
You seem to imply that it is harder to defend against an attacker who knows
less (only detects collisions), than against an attacker who knows more (also
knows the internal state).  The reverse is true, by logic.
Also, please note that those techniques, and also the envelope method, are indeed
useful to prevent attacks when an attacker can detect collisions in the internal
state -- as my example above exemplifies.
Ed Gerck

@_date: 2002-10-23 19:41:52
@_author: Ed Gerck 
@_subject: Why is RMAC resistant to birthday attacks? 
Thanks. I should have written "a usually required property". In general,
to have a good MAC, we require a good PRF.
Ed Gerck

@_date: 2002-10-24 10:13:12
@_author: Ed Gerck 
@_subject: Why is RMAC resistant to birthday attacks? 
Thanks again. I should have had some coffee at that time...I meant SQUARE ROOT.
As to the point you say is in question: "the only way to reduce the likelihood of internal
collisions is to increase the internal state space." -- this is clearly true but is NOT what
is in discussion here. The point is whether the only way to reduce the likelihood of
attacks based on MAC collisions is to increase the internal state space.  These
statements are not equivalent.
However, it was possible to reduce the likelihood of attacks based on MAC
collisions is to increase the internal state space.   This is what I was trying to
explain. More below...
It's always good to read more, and learn more. But what I'm saying is
written in many such papers, including some that are written for
a general audience:
----in Dr. Dobbs, April 1999.
The point is clear: WITHOUT increasing the internal search space of MD5,
MD5 is used in a way that vastly reduces the likelihood of attacks based on
MAC collisions.
Ed Gerck

@_date: 2002-10-24 10:33:35
@_author: Ed Gerck 
@_subject: Why is RMAC resistant to birthday attacks? 
... pls read this message with the edits below... missing "^" in exp and the word "WITHOUT"...still no coffee...
Thanks again. I should have had some coffee at that time...I meant SQUARE ROOT.
As to the point you say is in question: "the only way to reduce the likelihood of internal
collisions is to increase the internal state space." -- this is clearly true but is NOT what
is in discussion here. The point is whether the only way to reduce the likelihood of
attacks based on MAC collisions is to increase the internal state space.  These
statements are not equivalent.
However, it was possible to reduce the likelihood of attacks based on MAC
collisions WITHOUT increasing the internal state space.   This is what I was trying to explain. More below...
It's always good to read more, and learn more. But what I'm saying is
written in many such papers, including some that are written for
a general audience:
----in Dr. Dobbs, April 1999.
The point is clear: WITHOUT increasing the internal search space of MD5,
MD5 is used in a way that vastly reduces the likelihood of attacks based on
MAC collisions.
Ed Gerck

@_date: 2002-10-24 11:43:37
@_author: Ed Gerck 
@_subject: collision resistance -- Re: Why is RMAC resistant to birthday attacks? 
There seems to be a question about whether:
1. the internal collision probability of  a hash function is bounded by the
inverse of the size of its internal state space, or
2. the internal collision probability of a hash function is bounded by the
inverse of the square root of size of its internal state space.
If we assume that the hash function is a good one and thus its hash space
is uniformely distributed (a good hash function is a good PRF), then we can
For a hash function with an internal state space of size S, if we take n
messages x1, x2, ...xn, the probability P that there are i and j such that
hash(xi) = hash(xj), for xi <> xj, is
    P = 1 - (S!/( (S^n)*(S - n)!)
which can be approximated by
    P ~ 1 - e^(-n*(n - 1)/2^(S + 1) ).
We see above a n^2 factor which will translate into a factor with sqrt(2^S)
when we solve for n. For example, if we ask how many messages N we
need in order to have P > 0.5, we solve for n and the calculation gives:
    N ~ sqrt( 2*ln(2)*2^S ).
Thus, if we consider just two messages, affirmation  holds, because
P reduces to 1/S. If we consider n > 2 messages, affirmation  holds (the
birthday paradox).
Ed Gerck

@_date: 2002-10-24 12:01:08
@_author: Ed Gerck 
@_subject: collision resistance -- Re: Why is RMAC resistant to birthday  
The event is finding 1 collision out of n messages.
;-) I never said it was new. But since you apparently sided with  and I
sided with  I was commenting that -- for once -- we both seem to be
right. BTW, the first time I read those chapters was in '97 and I still go
back to them when I need to brush up on something. The HAC is a great
book and, as you probably know, it's 100% available online too.
Ed Gerck

@_date: 2002-10-25 15:22:47
@_author: Ed Gerck 
@_subject: more snake oil? [WAS: New uncrackable(?) encryption technique] 
Their problem is not hard -- it is just either slow to converge for
some methods or not simply uniquely determined (*). They consider
the cases that are not uniquely determined, which is equivalent to the
following problem:
       given Y solve for X in Y = X mod 11
(and I mean 11 as a good number for their problem space),
which has many answers. Indeed, the number of answers (?keys?)
that fit the equation is infinite. Since they know the only "X" that they
consider (quite arbitrarily) to be the "right" answer, they say that
you can't guess it -- hence it is unbreakable in their view. However,
their search space is very small and all functional exponential forms
can be tried in parallel with much better algorithms than what they
seem to use (*). This is not better than short passwords, so that one
probably does not even need to break in and snatch the file holding
the keys to the kingdom -- the coefficients that were used.
(*) For an example, see the Prony method comment and reference in  Ed Gerck

@_date: 2002-09-02 15:33:01
@_author: Ed Gerck 
@_subject: Quantum computers inch closer? 
The original poster was incorrect just in assuming that this would be an
effective method allowing Feistel ciphers to be broken.
The original poster is correct, however, in that a metric function can be defined
and used by a QC to calculate the distance between a random state and an
eigenstate with some desired properties, and thereby allow the QC to define
when that distance is zero -- which provides the needle-in-the-haystack solution,
even though each random state vector can be seen as a mixed state and will, with
higher probability, be representable by a linear combination of eigenvectors
with random coefficients, rather than by a single eigenvector.
Ed Gerck

@_date: 2002-09-02 20:25:31
@_author: Ed Gerck 
@_subject: Quantum computers inch closer? 
In other words, even though most of the time a QC will be dealing with
mixed states (ie, states that cannot be represented by a single eigenvector),
a QC can nonetheless use a metric function (such as loosely described
by the original poster) in order to arrive at the desired needle-in-the-haystack
solution -- that might be a single eigenvector.
As I commented at the time, and where I think we agree, the scheme does not
make Feistel ciphers easier to break by quantum computing.  It's not what a
"quantum algorithm". However, we need to recognize that the scheme suggested
is sound for any computer and a QC *is* a computer -- but it would be no better
for a QC than  an exhaustive search. In short, his  method had nothing "quantum"
about it.
Here, the essential point for an effective QC solution is not whether the
calculation is possible (which it is if it can be computed), but that it should
be capable of being efficiently transposed to a quantum system.  Breaking a
Feistel cipher cannot, breaking RSA PK can.
Ed Gerck

@_date: 2002-09-03 12:07:17
@_author: Ed Gerck 
@_subject: Quantum computers inch closer? 
a proof of existence does not allow one to automatically convert a classical
circuit to "the" corresponding quantum circuit, which was the original comment
by John. Devising QC algorithms from classical algorithms should not be
the best way to do it, either.
Ed Gerck

@_date: 2002-09-17 14:51:28
@_author: Ed Gerck 
@_subject: Cryptogram: Palladium Only for DRM 
It may be useful to start off with the observation that Palladium will not be
the answer for a platform that *the user* can trust.  However, Palladium
should raise awareness on the issue of what a user can trust, and what not.
Since a controling element has to lie outside the controled system, the solution
for a trustworthy system is indeed an independent module with processing
capability -- but which module the user should be able to control..
This may be a good, timely opening for a solution  in terms of a "write code"
approach, where an open source trustworthy (as opposed to trusted)
secure execution module TSEM (e.g., based on a JVM with permission
and access management) could be developed and -- possibly -- burned on a
chip set for a low cost system. The TSEM would require user-defined
signatures to define what is trustworthy to *the user*, which would set a higher
bar for security when compared with someone else defining what is
trustworthy to the user.  The TSEM could be made tamper-evident, too.
Note: This would not be in competition with NCipher's SEE, because NCipher's
product is for the high-end market and involves commercial warranties,
but NCipher's SEE module is IMO a good example.
Ed Gerck

@_date: 2002-09-18 10:04:08
@_author: Ed Gerck 
@_subject: Cryptogram: Palladium Only for DRM 
The question of "what is trust" might fill this listserver for months.
But, if we want to address some of the issues that Pd (and, to some
extent, PKI) forces on us then we must be clear what we mean when
we talk about  trust in a communication system -- what is a trusted
certificate, a trusted computer? Trusted for what? What happens
when I connect two computers that are trusted on matters of X --
are they trusted together on matters of X, less or more? What do
we mean by trustworthy?
I can send you some of my papers on this but the conclusion I arrived
is that in terms of a communication process, trust has nothing to do with
feelings or emotions.
Trust is qualified reliance on information, based on factors independent of
that information.
In short, trust needs multiple, independent channels to be communicated.
Trust cannot be induced by self-assertions -- like, "trust me!"  or "trust Pd!"
More precisely, "Trust is that which is essential to a communication channel
but cannot be transferred using that channel."  Please see the topic ?Trust Points?
by myself in ?Digital Certificates: Applied Internet Security? by Jalal Feghhi,
Jalil Feghhi and Peter Williams, Addison-Wesley, ISBN 0-20-130980-7, pages
194-195, 1998.
That said, the option of being *able* to define your own signatures on what
you decide to trust does not preclude you from deciding to rely on someone
else's signature.  BTW, this has been used for some time with a hardened version
of Netscape, where the browser does not use *any* root CA cert unless you sign
it first.
Thanks for your nice  comment ;-)
Ed Gerck

@_date: 2002-09-18 17:29:51
@_author: Ed Gerck 
@_subject: Cryptogram: Palladium Only for DRM 
what you said also looks good
A recent summary is at  Ed Gerck

@_date: 2002-09-22 14:11:42
@_author: Ed Gerck 
@_subject: unforgeable optical tokens? 
Local authentication still has several optical issues that need to be answered,
and which may limit the field usefullness of a device based on laser speckle.
For example, optical noise by both diffraction and interference effects is a
large problem -- a small scratch, dent, fiber, or other mark (even invisible,
but producing an optical phase change) could change all or most all of
the speckle field. The authors report that a 0.5mm hole produces a large
overall change -- which can be easily understood since the smaller the defect,
the larger the spatial effect (Fourier transform).
But temperature/humidity/cycle differences might be worse -- any dilation or
contraction created by a temperature/humidity/cycle difference between recording
time (in lab conditions) and the actual validation time (in field conditions) would
change the entire speckle field in a way which is not "geometric" -- you can't just
scale it up and down to search for a fit.
Also, one needs to recall that this is not a random field -- this IS a speckle field.
There is a definite higher probability for bunching at dark and white areas
(because of the scatter's form, sine function properties, laser coherence length,
etc). This intrinsic regularity can be used to reduce the search space to a much
lower space than what I saw suggested.  Taking into account loss of resolution
by vibration and positioning would also reduce the search space.
Finally, the speckle field will show autocorrelation properties related to the sphere's
size and size distribution, which will further reduce randomness. In fact, this is a
standard application of speckle: to measure the diameter statistics of small spheres.
Ed Gerck

@_date: 2003-04-21 13:59:16
@_author: Ed Gerck 
@_subject: DRM technology and policy 
Yes, and protecting computer programs also helps
improve security (if the program is what it should
be and the user is who the user should be).
However, when we talk about increasing protection for
online music or other downloadable we need to remember
the old saying in business:
"If you have never lost a receivable, your rules were too strict!"
BTW, Einar Stefferud (First Virtual co-founder) makes some
important points on this issue in the coming conference on virtual
goods in Ilmenau, Germany (questions to R?diger Grimm
Ed Gerck

@_date: 2003-08-27 07:36:01
@_author: Ed Gerck 
@_subject: blackmail / real world stego use 
A guy in Google can do it. In short, if Bob would set up (or use
internally) a www cache, reachable as a public service, which
cache quickly downloads all the pages of several sites by multiple
HTTP connections, the desired image being among them, and do
this for a time window that overlaps the desired target time, then
the desired image can be seen almost in real time by Bob,
OTOH, it is possible that the dutch man was traced not by a one
time download of the image but by many attempts to find it,
since the upload time of the image to the site was not exactly
known to him and time was of essence. In this case, the required
tracing capability would NOT need a large capability for packet
recording and correlation. It would just include finding 100's
(or 1000's) of identical access occurrences in surfola's incoming
server traffic, after surfola's server was tagged from the website's
The lesson seems to be that, like with other security tools,
anonymizing tools also need to be correctly used. Providing an
action pattern can break an anonymizer -- to identify is to look
for coherence.
Ed Gerck

@_date: 2003-08-27 16:49:39
@_author: Ed Gerck 
@_subject: blackmail / real world stego use 
No, the website's logs mentioned above belongs to the victim -- who had no
problems in fully cooperating with law enforcement.
surfola connects upstream to someone, who is tapped before the
victim posts the image.
Ed Gerck

@_date: 2003-12-01 16:10:07
@_author: Ed Gerck 
@_subject: RSA -- 2002 Turing Award Lecture Available Online (fwd) 
The 2002 Turing Award Lecture by the winners of ACM's most prestigious technical award is now available online in a variety of formats at: The 2002 Turing Award was presented on June 7, 2003, to Drs. Ronald L. Rivest, Adi Shamir and Leonard M. Adleman, the developers of the RSA encryption code, for their seminal contributions to the theory and practical application of public key cryptography.
Available for viewing are Dr. Ronald L. Rivest's presentation on the "Early Days of RSA", Dr. Adi Shamir's talk on "Cryptology: A Status Report", and  Dr. Leonard M. Adleman's address on "Pre-RSA."

@_date: 2003-12-08 10:05:43
@_author: Ed Gerck 
@_subject: yahoo to use public key technology for anti-spam 
Using your own SMTP from a dynamic IP (cable, DSL and modem access, for
example) fails because of (the brain-dead) black-listing of dynamic IP blocks to
prevent spam -- see
  and
Also, as seems to be the norm now, most viruses come with a primitive
SMTP engine built into them -- which, again, taints dynamic IPs (since
many home machines are inflected).
Ed Gerck

@_date: 2003-12-18 20:58:58
@_author: Ed Gerck 
@_subject: Quantum Crypto 
Well, one of our real problems is that in order to protect a system we need
to introduce targets in addition to the system's resources (the original targets)
that can come under attack, which additional targets increase complexity,
overhead and we cannot protect with 100% efficiency. Thus, paradoxically,
adding controls adds weakenesses.
For example, if we add a password list and an ACL to control access we
are adding targets -- that can be (and are) attacked. Another example is
the software itself, needed to control the access.
Quantum cryptography's promise is to solve this real problem by eliminating
some additional targets when compared to a conventional system.
The same, however, can be done without QC and that is, IMO, one of the
directions we need more work on. How can we reduce the number of additional
targets -- QC or not? This approach can provide provable benefits by directly
reducing the total number of targets. You can't attack a target that does not exist.
Ed Gerck

@_date: 2003-12-24 01:34:00
@_author: Ed Gerck 
@_subject: Non-repudiation (was RE: The PAIN mnemonic) 
Yes, the term "non-repudiation" has been badly misused in
old PKIX WG drafts (in spite of warnings by myself and
others) and some crypto works of reference -- usually
by well-intentioned but otherwise misguided people trying
to add "value" to digital certificates.
However, IMO non-repudiation refers to a useful and
essential cryptographic primitive. It does not mean the
affirmation of a truth (which is authentication). It means
the denial of a falsity -- such as:
(1) the ability to prevent the effective denial of an act (in
other words, denying the act becomes a falsity); or
(2) the ability to prevent the denial of the origin or delivery
of transactions.
Note that, except for a boolean system, the affirmation of
a truth is not the same as the denial of a falsity. Hence, the
usefulness of "non-repudiation" as a primitive. Take away
"non-repudiation" and you end up with a lesser "language"
with which to describe security processes.
Ed Gerck

@_date: 2003-02-18 17:37:38
@_author: Ed Gerck 
@_subject: AES-128 keys unique for fixed plaintext/ciphertext pair? 
The statement was for a plaintext/ciphertext pair, not for a random-bit/
random-bit pair. Thus, if we model it terms of a bijection on random-bit
pairs, we confuse the different statistics for plaintext, ciphertext, keys and
we include non-AES bijections. Hence, I believe that what we got so far is
a good result... but for a different problem.
In this case, it seems to me that we need to take into account the maximum
possible entropy for the plaintext as well as the entropy of the actual plaintext,
and the entropy of the keys. With these considerations, with the usual
assumption that AES is a random cipher, we can say indeed [*]:
"For each AES-128 plaintext/ciphertext (c,p) pair with length
equal to or larger than the unicity distance, there exists exactly
one key k such that c=AES-128-Encrypt(p, k)."
Ed Gerck
[*] If AES is a random cipher and if the unicity distance "n" calculated
by the usual expression n = H(K)/[|M| - H(M)] for a random cipher,
where the quantities are:
    H(K) = entropy of keys effectively used in encryption
M| = maximum possible entropy for the plaintext
    H(M) = entropy of actual message, the given plaintext
is equal to or smaller than the given ciphertext's length, then there
is only possible decipherment of the given ciphertext -- ie, there is
only one key k such that p=AES-128-Decrypt(c, k) and
c=AES-128-Encrypt(p, k).

@_date: 2003-02-18 19:23:06
@_author: Ed Gerck 
@_subject: AES-128 keys unique for fixed plaintext/ciphertext pair? 
The relevant aspect is that the plaintext and key statistics are the
determining factors as to whether the assertion is correct or not.
In your case, for example, with random keys and ASCII text in English,
one expects that a 128-bit ciphertext segment would NOT satisfy the
requirement for a unique solution -- which is 150 bits of ciphertext.
However, since most cipher systems begin with a "magic number" or
has a message format that begins with the usual "Received", "To:", "From:",
etc., it may be safer to consider a much lower unicity, for example less than
128 bits. In that case, even one block of AES would satisfy the requirements

@_date: 2003-02-19 14:18:13
@_author: Ed Gerck 
@_subject: AES-128 keys unique for fixed plaintext/ciphertext pair? 
The previous considerations hinted at but did not consider that a
plaintext/ciphertext pair is not only a random bit pair.
Also, if you consider plaintext to be random bits you're considering a very
special -- and least used -- subset of what plaintext can be. And, it's a
much easier problem to securely encrypt random bits.
The most interesting solution space for the problem, I submit, is in the
encryption of human-readable text such as English, for which the previous
considerations I read in this list do not apply, and provide a false sense of
strength. For this case, the proposition applies -- when qualified for  the
Ed Gerck

@_date: 2003-02-21 06:31:20
@_author: Ed Gerck 
@_subject: AES-128 keys unique for fixed plaintext/ciphertext pair? 
This may sound intuitive but is not correct. Shannon proved that if
"n" (bits, bytes, letters, etc.) is the unicity distance of a ciphersystem,
then ANY message  that is larger than "n" bits CAN be uniquely deciphered
from an analysis of its ciphertext -- even though that may require some
large (actually, unspecified) amount of work. Thus, the likelihood of
of two keys producing valid decipherments (as plaintexts that can be
enciphered to the same ciphertext, natural language or not), from the
same ciphertext is ZERO after the message length exceeds the unicity
distance -- otherwise the message could not be uniquely deciphered
after the unicity condition is reached, breaking Shannon's result.
Conversely, Shannon also proved that if the intercepted message has less
than "n" (bits, bytes, letters, etc.) of plaintext then the message CANNOT
be uniquely deciphered from an analysis of its ciphertext -- even by trying
all keys and using unbounded resources.
As above, it can. And the answer formulated in terms of the unicity
is valid for any plaintext/ciphertext pair, even for random bits. It
answers the question in all generality.
No cipher is theoretically secure above the unicity distance, even though
it may be practically secure.
The following is always true, for any possible plaintext bit pattern:
"For each AES-128 plaintext/ciphertext (c,p) pair with length
equal to or larger than the unicity distance, there exists exactly
one key k such that c=AES-128-Encrypt(p, k)."
Ed Gerck

@_date: 2003-02-28 15:10:09
@_author: Ed Gerck 
@_subject: double shot of snake oil, good conclusion 
In  this article on MS DRM states: "For example, it might be possible to
view a document but not to forward or print it."
This is, of course, blatantly false. Of course it can, by using a screenshot,
a camera, a cell phone with camera or, simply, human memory. With all
due respect, the claim is snake oil.
This is exactly what we in IT security must avoid. Insecure statements that
create a false sense of security -- not to mention a real sense of angst. This
statement, surely vetted by many people before it was printed, points out
how much we need to improve in terms of a real-world model for IT security.
And that is why, today, IT security failures are causing an estimated
loss of $60B/year (ASIS, PricewaterhouseCoopers, 2001).
The second shot of snake oil came when some people, without realizing
the trap, started to get alarmed by the snake oil shot  and started
speculating on "the chilling effect that such measures could have on
corporate whistleblowers" while others speculated on "another potentially
devastating effect", that the DRM could, via a loophole in the  DoJ
consent decree, allow Microsoft to withhold information about file
formats and APIs from other companies which are attempting to create
compatible or competitive products -- compatible, that is, with the first
shot of snake oil.
The good conclusion from all of this seems to be that while humans are the
weakest link in a virtuous security system, they can also help break a
non-virtuous security system -- DRM snake oil claims notwithstanding.
Ed Gerck

@_date: 2003-01-08 11:26:33
@_author: Ed Gerck 
@_subject: DeCSS, crypto, law, and economics 
Well, zone locking helps curb this because it *reduces* the market for each
copy. The finer the zone locking resolution, the more effort an attacker needs
to make in order to be able to trade more copies.
Ed Gerck

@_date: 2003-01-16 15:37:13
@_author: Ed Gerck 
@_subject: Copyright protection, DMCA, DRM and technology 
The Supreme Court has rejected a challenge to the Sonny Bono Law.
 Let's stir the pot.
Today, law is not the logic of ethics. It is the logic of power.
That said, let's recognize the power of technology that is also at play
here and look at the options that are left after the USSC decision. In
addition to the legal approach of allowing copyright owners to
selectively renounce their seemingly ever-engorgable rights (the Creative
Commons intiative by Lawrence Lessig), one may be able to provide
legal support for technology that -- rather omninously to some -- helps
users become trusted fair-user of copyrighted materials that are so
protected. DRM can be useful  to users.
Why would DRM be useful to users? Because it could reduce the need
for legislation which outright curbs fair-use under the argument that
fair-use is "out of control" in the digital world.
Essentially, I'm making the point that fair-use of copyrighted material
can be technologically enforced and controlled, *notwithstanding*
cooperation (or lack thereof) by the user -- and that is why the user
can be trusted by Jack Valenti.
This argument, in broader terms, could reduce the perception and the
need to have legislation such as the DMCA, that uses the legal system
to protect what technology allegedly cannot (*).
Technology's role is to create tools to make it nearly impossible for
users to profit from an abuse of fair use, which allows laws such as
the DCMA to be questioned under legal arguments  -- for example,
unfair restriction of a buyer's rights.
Ed Gerck
(*) In other words, if it is axiomatic that we do not need much in
terms of legislation to prevent users from doing what is
tecnologically near-to-impossible, then by making available a
technology providing an absence of means for users to
significantly abuse fair use so technologically controled, we
need less in terms of laws providing the control.

@_date: 2003-07-08 18:55:56
@_author: Ed Gerck 
@_subject: Fwd: [IP] A Simpler, More Personal Key to Protect Online Messages 
Show me an enterprise/person who would like to have their private keys
escrowed by a third-party, with all the liability/collusion/blackmail potential
that goes  with it, and I'll show you a client for VS.
There are IMO many (and better) schemes when you want your private keys
to be known by a TTP. Including PKI.
Ed Gerck

@_date: 2003-07-14 11:31:55
@_author: Ed Gerck 
@_subject: Announcing httpsy://, a YURL scheme 
"The browser verifies that the fingerprint in the URL matches the public key provided by the visited site. Certificates and Certificate Authorities are unnecessary. "
Spoofing? Man-in-the-middle? Revocation?
Also, in general, we find that one reference is not enough to induce trust. Self-references
cannot induce trust, either (Trust me!). Thus, it is misleading to let the introducer
determine the message target, in what you call the "y-property". Spoofing and
MITM become quite easy to do if you trust an introducer to tell you where to go.
Not that I believe CAs are essential (I don't, for reasons already presented in '97),
but unless the issues of spoofing, MITM and revocation are adequately handled
according to a threat model that is useful, communication cannot be considered
Ed Gerck

@_date: 2003-07-14 13:08:55
@_author: Ed Gerck 
@_subject: Announcing httpsy://, a YURL scheme 
To unwind my phrase above, IMO the threat model should adequately handle the
issues of spoofing, MITM and revocation in order to be useful. Otherwise,
communication cannot be considered secure.
As a counter-example, using an empty threat model does not qualify
for "secure" even though any implementation would meet an empty threat
model. Not including a recourse against probable attacks such as spoofing,
MITM and key compromise (revocation) is IMO actually insecure.
Ed Gerck

@_date: 2003-07-14 17:43:39
@_author: Ed Gerck 
@_subject: Announcing httpsy://, a YURL scheme 
I did not see the issues of spoofing, MITM and revocation being
addressed at all. For these threats, however, the attack descriptions
are well-known and rather easy to carry out.
But there are other issues. Let me exemplify with PGP, which is
one of the models you cite. In PGP there is no entity responsible if
(or when) something goes wrong (not even the user). The use of
PGP in a commercial situation has been difficult and may not
adequately protect the business interests involved, which usually
need to be guaranteed in well-defined contracts with loss responsibilities
and fines. Furthermore, PGP does not scale so well in size (because of the
asynchronous maintenance difficulties of the web of trust) and time
(because of the same maintenance problems reflected in the so-called
certificate revocation certificates, a CRL for PGP certificates).
You may find the same issues with httpsy -- however, as in PGP, within
a circle of close friends (or within a company/organization) this may not
be important.
Ed Gerck

@_date: 2003-07-15 10:28:58
@_author: Ed Gerck 
@_subject: Announcing httpsy://, a YURL scheme 
Maybe that's why CAs are still around...they do not tell you where to go. Instead,
there are two assertions that a CA should deliver in a certificate according to X.509:
(i) that the subject?s public-key has a working private-key counterpart somewhere, and
(ii) that the subject?s DN is unique to that CA.
These assertions should also be delivered without content disclaimers but are limited
in scope by the CPS. In addition, in both cases caveats apply. For example, in (a),
there are no warranties that the public/private key pair is not artifically weakened,
that the private key is actually in the possession of the named subject and that no
one else has obtained a copy of the private key. In (b), there are no warranties that
such DN contains the actual subject?s name, location or that the subject even exists
or has a correctly spelled name.
(From Overview of Certification Systems, E. Gerck, 1997, copy
at   )
Ed Gerck

@_date: 2003-07-15 10:37:47
@_author: Ed Gerck 
@_subject: Announcing httpsy://, a YURL scheme 
My point exactly. Trust can also be seen as that which can break your system.
By believing in *one* trusted introducer, a single source of information, a single
trusted source, you have no correction channel available.  One of the earliest
references to this principle can be found some five hundred years ago in the Hindu
governments of the Mogul period, who are known to have used at least three
parallel reporting channels to survey their provinces with some degree of reliability, notwithstanding the additional efforts. More in Ed Gerck

@_date: 2003-07-15 14:15:50
@_author: Ed Gerck 
@_subject: Announcing httpsy://, a YURL scheme 
This is what your documentation says about key revocation:
 "When using YURLs, sysadmins can shorten the lifetime of a
  certificate, change keys more frequently, and thus reduce
  their site's vulnerability to identity theft. Keys could even be
  changed at a frequency that would enable the site to forgo
  certificate revocation and Certificate Revocation Lists (CRLs).
Really? What prevents the attacker from having a rogue site
with the stolen key if there is nowhere to verify whether the
key is valid or not?
 "A YURL MUST provide all the information required to
 authenticate the target site. Authentication of the target
 site MUST ONLY rely on information contained in the
 YURL."
The YURL is the single point of control and that is a problem,
not a solution. The YURL must also be recognized as a single
point of failure -- i.e., no matter how trustworthy that single point
of control is, it may fail or be compromised and there is no recourse
available because it is the single point of control.
Ed Gerck

@_date: 2003-07-16 08:48:00
@_author: Ed Gerck 
@_subject: Announcing httpsy://, a YURL scheme 
You say that "By definition, Alice can't introduce him [Bob] to an inauthentic party,
because  whoever Alice introduces him [Bob] to, that's who Alice introduced him
[Bob] to."
IF Alice is trusted by Bob to introduce ONLY authentic parties, yes. And that is the
Ed Gerck

@_date: 2003-07-16 08:55:35
@_author: Ed Gerck 
@_subject: trust is not associative 
The thread "Announcing httpsy://, a YURL scheme"  has not dealt with the
issues of *order of introduction* that, however, play a major role in terms
of introduction and the development of trust.
Let trust be the operation *.
Suppose Jon trusts a CA and has his cert issued by that CA. After the cert
was issued, the CA decides to trust Khadaffi and grants Khadaffi access
and control to all of its issued certificate and CRL files, including Jon's of
course -- which was already issued.
This is represented by the result of
    (Jon*CA)*Khadaffi,         (1)
which is Ok because Jon trusts the CA before the CA trusts Khadaffi, and thus
Jon gets his cert from that CA. This means that Jon accepts to be introduced
by the CA.
Suppose now that Jon learns beforehand that the CA trusts Khadaffi and all
his data will be also know to Khadaffi if he decides to trust that CA and
that Khadaffi could revoke his cert at will (e.g., simulating an error).
Then, if Jon does not trust Khadaffi, he will not have his cert
issued by that CA.
This is now represented by the result of
    Jon*(CA*Khadaffi),     (2)
which is not Ok and Jon does not get his extrinsic cert from that CA.
This means that Jon DOES NOT accept to be introduced by the CA.
Of course, the result of (1) is not equal to (2). Trust depends on the event
sequence. Trust is not associative.
The same could be exemplified for competing businesses or competing
countries, as I comment in the paper at Or, you may trust your lawyer before you know he trusts your competitor
but not after you know it. Of course, you may never know that an untrustworthy
C of (A*B)*C exists (i.e., the confidence-leak problem) and you may forever trust
Aldrich Ames!
In short, a system that ignores that trust is not associative can make you rely on an
otherwise unacceptable introduction. OTOH, a system that makes you rely on
a single introduction is essentially setting you up for a single point of failure.
Ed Gerck

@_date: 2003-07-16 10:58:51
@_author: Ed Gerck 
@_subject: Announcing httpsy://, a YURL scheme 
No. Alice may simply always introduce Bob to a fraudster, independently of *who*
Bob wants to talk to.
BTW, IMO this thread has suffered the constant, excessive use of sweeping statements
and arguments. The way I see it, until the statement that "Authentication of the target site
MUST ONLY rely on information contained in the YURL"  is revisited, there is nothing
much to discuss since there is already a single point of failure that is fatally built into the
Ed Gerck

@_date: 2003-07-16 13:55:14
@_author: Ed Gerck 
@_subject: Announcing httpsy://, a YURL scheme 
Authentication that reads thoughts .... sites that you are "actually"
introduced to (MITM, anyone?) ...
I believe you would want to edit that content before I comment it,
as well as the one I am quoted above.
Ed Gerck

@_date: 2003-03-03 13:06:33
@_author: Ed Gerck 
@_subject: double shot of snake oil, good conclusion 
We are in agreement. When you read the whole paragraph that I wrote,
I believe it is clear that my comment was not whether the loophole existed
or not. My comment was that there was a much more limited implication
for whistle-blowing because DRM can't really control what humans do
and there is no commercial value in saying that a document that I see
cannot be printed or forwarded -- because it can.
And that's what my paragraph meant.
Ed Gerck

@_date: 2003-03-03 13:28:54
@_author: Ed Gerck 
@_subject: Comments/summary on unicity discussion 
The recent thread on "AES-128 keys unique for fixed plaintext/ciphertext
pair" included a discussion on unicity, with some broken dialogues. I wrote
-up a summary that I'm sending to this list as a possible seed for further
comments. I apologize for any mistakes or imprecision, as I'm not
trying to be as exact as possible -- just sufficiently exact for the purpose
at hand. I also provide below the online references for Shannon's  works
[Sha48, Sha49] that are important to this discussion.
The AES thread discussion is NOT included here.
1. WHAT IS UNICITY?
There are three different contexts to answer to this question!
1.a. Unicity Definition: Shannon [Sha49, page 693] defined "unicity
distance" (hereafter, "n") as the least amount of plaintext which can be
uniquely deciphered from the corresponding ciphertext, allowing one to
determine, without doubt, the key that was used for encryption. The
"amount" of plaintext (i.e., "n") can be measured in any units the user
may find convenient, such as bits, bytes, letters, symbols, etc. Actually,
Shannon used "letters" in his paper.
    NOTE 1: This is a definition. There is no proof involved here.
1.b. Unicity Model: As first given by Shannon [Sha49] under some restrictive
assumptions, specially the "random cipher" assumption, the mathematical
expression for unicity can be cast in the following unfolded expression
(his original expression was  n = H(K)/D, where D is the redundancy):
    n = H(K)/[|M| - H(M)]
where the quantities are:
    n = unicity; least message length that can be uniquely deciphered
    H(K) = entropy of keys used in encryption
M| = maximum possible entropy for the plaintext
    H(M) = entropy of actual message, the plaintext
and the entropies are calculated accordingly to the desired units (bits,
bytes, letters, symbols, etc.), which also define the unit for n.
    NOTE 1: The model for unicity has no probability error with a tail
    to infinity because only entropy values are used in the formula of n
    and by *definition* of  entropy the entropy is already a limit to
    infinity.
    NOTE 2: It does not matter how the attacker may try to decipher
    the message. The attacker can of course use brute-force and try
    out all keys or he can use short-cuts, it is his choice and he is entirely
    free to use any method he desires.  The work involved may be small,
    quite large or even unbounded -- the amount of work is actually
    unspecified.
    NOTE 3: Shannon's definition of "random cipher" was that "all
    decipherments must produce a random flat distribution over all
    bits in the plaintext space."
1.c. Unicity Value:  The numerical value of n. It is important not to
confuse a model with a measurement. Models predict measurements,
and do so within an error range. What is the the error range for
measuring n?
First, note that the model works for any ciphertext, any plaintext.
And for any such pairs, the result "n" is predicted by the model
even if an attacker has unbounded resources, including infinite time.
The value of "n" depends on the maximum possible entropy for the
plaintext, the plaintext entropy, the entropy of the keys and the
assumption that the cipher is a random cipher. Since all good
ciphers should be a random cipher, for those ciphers the model
provides a good approximation to what "n" actually is. The practical
difficulty of reliably estimating the plaintext entropy and even the key
entropy (which errors contribute to an error in "n") has nothing to
do with the model itself or its error for "n", but on the errors
for the quantities  on which it depends -- however, it's not so
hard to obtain good estimates and several are well-known.
    NOTE 1: Estimating the entropy of English (and other languages)
    has been the subject of considerable study. Various authors have
    measured H(M) for English texts and found values that lie between
    1.0 and 1.5. The standard value quoted is 1.2, close to average of
    the extreme values. Even though  each author has a different text,
    different preferred words, and different style preferences, we all
    come pretty close to the  entropy value of 1.2. However, XML text
    (which is in English) is more redundant than natural English and should
    have a lower entropy. On the other hand, English text that is sent
    by SMS in cell phones has messages such as "Chk tat 4 u 2",
    where the redundancy is reduced and the entropy should be higher.
    NOTE 2: The benefit of compression is to increase unicity even
    if the compression algorithm is fully known to the attacker. If the
    plaintext is compressed before encipherment, then we rightly
    expect its entropy per compressed character to increase -- even
    though its entropy per English character does not increase. This
    is often confusing and may provide the wrong impressions that
    nothing is gained by compression or that we may need to "hide"
    the compression algorithm from the attacker.
2. READING THE FINE PRINT
Of further importance and often ignored or even contradicted by
some statements in the literature such as "any cipher can be attacked by
exhaustively trying all possible keys", I usually like to call attention to
the fact that any cipher (including 56-bit-key DES) can be theoretically
secure against any attacker -- even an attacker with unbounded
resources -- when the cipher is used within its unicity. Not only the
One-Time Pad is theoretically secure, but any cipher can be theoretically
secure if used within the unicity distance. Thus, indeed there is a
theoretically secure defense even against brute-force attacks, which is to
work within the unicity limit of the cipher. And, it works for any cipher
that is a good random cipher -- irrespective of key-length or encryption
method used.
It is also important to note, as the literature has also not been very neat
in this regard, that unicity is always referred to the plaintext. However,
it may also be applied to indicate the least amount of ciphertext which
needs to be intercepted in order to attack the cipher -- within the
ciphertext/plaintext granularity. For example, for a simple OTP-cipher,
being sloppy works because one byte of ciphertext links back to one
byte of plaintext -- so, a unicity of n bytes implies n bytes of ciphertext.
For DES, however, the ciphertext must be considered in blocks of 8
bytes -- so, a unicity of n bytes implies a corresponding modular number
of 8 bytes.
3. ONLINE REFERENCES
[Sha48] Shannon, C. Communication Theory of Secrecy Systems. Bell Syst.
Tech. J., vol. 28, pp. 656-715, 1949.  See also
 for readable scanned images of
the complete original paper and Shannon's definition of "unicity distance" in
page 693.  Arnold called my attention to a typeset version of the paper at
[Sha49] Shannon, C. A Mathematical Theory of Communication. Bell Syst.
Tech. J., vol. 27, pp. 379-423, July 1948. See also
Anton also made available the following link, with notes he took for
Claude Crepeau's crypto course at McGill. See page 24 and following at
(Anton notes that it's not unlikely that there are errors in those notes).
Comments are welcome.
Ed Gerck

@_date: 2003-03-03 17:21:25
@_author: Ed Gerck 
@_subject: Scientists question electronic voting 
Henry Norr had an interesting article today at
Printing a paper receipt that the voter can see is a proposal that addresses
one of the major weaknesses of electronic voting. However, it creates
problems that are even harder to solve than the silent subversion of e-records.
For example, using the proposed system a voter can easily, by using a
small concealed camera or a cell phone with a camera, obtain a copy of
that receipt and use it to get money for the vote, or keep the job. And
no one would know or be able to trace it.
Of course, proponents of the paper ballot copy, like Peter Neumann and
Rebecca Mercuri, will tell you the same thing that Peter affirmed in an official
testimony  before the California Assembly Elections & Reapportionment Committee
on January 17, 2001, John Longville, Chair, session on touch-screen (DRE)
voting systems, as recorded by C-SPAN (video available):
  "...I have an additional constraint on it [a voter approved paper ballot produced
  by a DRE machine] that  it  is behind reflective glass so that if you try to
  photograph it with a little secret camera hidden in your tie so you can go out and
  sell your vote for a bottle of whiskey or whatever it is, you will get a blank image.
  Now this may sound ridiculous from the point of view of trying to protect the
  voter, but this problem of having a receipt in some way that verifies that what
  seems to be your vote actually was recorded properly, is a fundamental issue."
I was also in Sacramento that same day, and this was my reply, in the next panel,
also with a C-SPAN videotape:
  ".. I would like to point out that it is very hard sometimes to take opinions, even
  though from a valued expert, at face value. I was hearing the former panel [on
  touch screen DRE systems] and Peter Neumann, who is a man beyond all best
  qualifications, made the affirmation that we cannot photograph what we can see.
  As my background is in optics, with a doctorate in optics, I certainly know that is
  not correct. If we can see the ballot we can photograph it, some way or another."
But, look, it does not require a Ph.D. in physics to point out that what Peter says is
incorrect -- of course you can photograph what you see. In other words, Peter's
"solution" goes as much of this DRE discussion has also gone -- it's paying lip service
to science but refutes basic scientific principles and progress.  After all, what's the
scientific progress behind storing a piece of paper as evidence? And, by the way, are
not paper ballots what were mis-counted, mis-placed and lost in Florida?
Finally, what we see in this discussion is also exactly what we in IT security
know that we need to avoid. Insecure statements that create a false sense of
security -- not to mention a real sense of angst. This statement, surely vetted by
many people before it was printed, points out how much we need to improve in
terms of a real-world model for voting.
This opinion is my own, and is not a statement by any company.
Ed Gerck

@_date: 2003-03-06 09:02:48
@_author: Ed Gerck 
@_subject: Scientists question electronic voting 
This is not possible for current paper ballots, for several reasons. For
example, if you take a picture of your punch card as a proof of how you
voted, what is to prevent you -- after the picture is taken -- to punch
another hole for the same race and invalidate your vote? Or, to ask the
clerk for a second ballot, saying that you punched the wrong hole,
and vote for another candidate?  The same happens for optical scan
cards.  These "proofs" are easily deniable and, thus, have no value
to prove how the voter actually voted.
Likewise, electronically, there is no way that a voter could prove how he
voted, even if the confirmation screen does list all the choices that the voter
has chosen, if that screen has two buttons: "go back", "confirm", and a
suitable logic. After the voter presses "confirm" the voter sees a "thank you"
screen without any choices present. The logic canbe set up in such a way
in terms of key presses and intermediate states that even photographing
the mouse cursor on a pressed "confirm" button does not prove that the voter
did not take the mouse out and, instead, pressed the "go back" button to
change his choices.
On the other hand, photographing a paper receipt behind a glass, which
receipt is printed after your vote choices are final, is not readily deniable
because that receipt is printed only after you confirm your choices.
    To deny that receipt the voter would have to say that the machine erred,
    which, if proved otherwise, could lead to criminal charges (e.g., the
    machine would be taken off the polls and, after the polls close the
    machine would be tallied; if the electronic tally would agree with the
    paper tally, the voter would be in trouble).
Protection against providing voters a receipt, voluntary or not, is often
overlooked by those who are not familiar with election issues.  For
example, the first press release by MIT/Caltech principals after Nov/2000 said
that the solution would be to provide the voter with a receipt showing how
they voted. Later on, MIT/Caltech reformed that view and have been doing an
excellent job at what I see as a process of transforming elections from art
to science, which is a good development after Nov/2000.
Ed Gerck

@_date: 2003-03-06 09:38:25
@_author: Ed Gerck 
@_subject: double shot of snake oil, good conclusion 
It beats me that "users you basically trust" might also be "careless, stupid,
lazy or confused" ;-)
Your point might be better expressed as "the company security policy would
be followed even if you do NOT trust the users to do the right thing." But,
as we know, this only works if the users are not malicious, if social engineering
cannot be used, if there are no disgruntled employees, and other equally
improbable factors.
BTW, one of the arguments that Microsoft uses to motivate people to
be careful with unlawful copies of Microsoft products is that disgruntled
employees provide the bulk of all their investigations on piracy, and everyone
has disgruntled employees. We also know that insider threats are responsible
for 71% of computer fraud.
Thus, the lack of value of these type of controls is to harass the legitimate users
and give a false sense of security. It reminds me of a cartoon I saw recently,
where the general tells a secretary to shred the document, but make a copy
first for the files.
Ed Gerck

@_date: 2003-03-06 09:48:32
@_author: Ed Gerck 
@_subject: Scientists question electronic voting 
The electronic process can be made much harder to circumvent by
allowing voters to cast any number of ballots but counting only the last
ballot cast. Since a voter could always cast another vote after the one that
was so carefully filmed, there would be no value for such film.
BTW, a similar process happens in proxy voting for shareholders meeting,
where voters can send their vote (called a "proxy") before the meeting
but can also go to the meeting and vote any way they please -- trumping
the original vote.
Much work needs to be done, and tested, to protect the integrity of
public elections. Even with all such precautions, if  the choices made by
a voter are disclosed (ie, not just the tally for all voters) then a voter
can be identified by using an unlikely pattern -- and the Mafia has,
reportedly, used this method in Italy to force (and enforce) voter
choices in an otherwise private ballot.
Ed Gerck

@_date: 2003-03-06 12:15:03
@_author: Ed Gerck 
@_subject: Scientists question electronic voting 
Not necessarily. Current paper ballots do not offer you a way to record
*your* vote. You may even photograph your ballot but there is no way to
prove that *that* was the ballot you did cast. In the past, we had ballots with
different collors for each party ;-) so people could see if you were voting
Republican or Democrat, but this is no longer the case.
It's easier than one may think to have a reliable proof, if you can photograph
the ballot that you *did* cast (as in that proposal for printing a paper receipt
with your vote choices) -- just wait out of the poll place and demand the
film right there, or wait out of the poll place, hear the voter's voice right
then and get the image sent by the cell phone before the voter leaves the
poll booth.
Ed Gerck

@_date: 2003-03-06 12:20:57
@_author: Ed Gerck 
@_subject: Scientists question electronic voting 
No, as I commented before, voiding the vote in that proposal after the paper
receipt is printed is a serious matter -- it means that either the machine made
an error in recording the e-vote or (as it is oftentimes neglected) the machine
made an error in printing the vote. The voter's final choice and legally binding
confirmation is made before the printing. And that is where the problems
reside (the problems that we were trying to solve in the first place), in that
printed ballot. Plus the problem of the voter being able to photograph
that final receipt and present it as direct proof of voting, as the voter
leaves the poll place (with no chance for image processing) or by
an immediate link by cell phone (ditto).
Ed Gerck

@_date: 2003-03-06 12:25:34
@_author: Ed Gerck 
@_subject: multiple system - Re: Scientists question electronic voting 
The dual, and multiple, system can be done without paper ballot.
There is nothing "magic" about paper as a record medium. I
can send a link for a paper on this that was presented at the
Tomales Bay conference on voting systems last year, using Shannon's
Tenth Theorem as the theoretical background, introducing the idea
of multiple "witnesses". If two witnesses are not 100% mutually
dependent, the probability that both witnesses may fail at the same
time is smaller than that of any single witness to fail.
Ed Gerck

@_date: 2003-03-06 18:17:29
@_author: Ed Gerck 
@_subject: Scientists question electronic voting 
This brings in two other factors I have against this idea:
- a user should not be called upon to distrust the system that the user
is trusting in the first place.
- too many users may reject the paper receipt because they changed their
minds, making it impossible to say whether the e-vote was wrong or
correct based on the number of rejected e-votes.
This was in my first message, and some subsequent ones too:
"For example, using the proposed system a voter can easily, by using a
small concealed camera or a cell phone with a camera, obtain a copy of
that receipt and use it to get money for the vote, or keep the job. And
no one would know or be able to trace it."
Ed Gerck

@_date: 2003-03-07 13:21:23
@_author: Ed Gerck 
@_subject: Scientists question electronic voting 
Maybe you missed some of my comments before, but these problems
do not exist in current paper-ballot voting schemes. Why should
e-voting make it worse?
My target is the same level of voter privacy and election integrity that a
paper-ballot system has when ALL election clerks are honest and do not
commit errors. Please see Proc. Financial Cryptography 2001, p. 257 and
258 of my article on "Voting System Requirements", Springer Verlag.
Of all aspects that need to be improved when moving to an electronic
system, the most important is the suspicion or fear that thousands or even
millions of electronic records could be altered with a keystroke, from
a remote laptop or some untraceable source. This goes hand-in-hand
with questions about the  current "honor system" in voting systems,
where vendors make the machines and also operate them during an
election. It's the overall black box approach that needs to improved.
The "trust me!" approach has had several documented problems
in paper ballot systems and would present even more opportunities
for fraud or even plain simple errors in an electronic system.
The solution is to add multiple channels with at least some independence.
The paper channel is actually hard to secure and expensive to store
and process. Paper would also be a step backwards in terms of efficiency
and there is nothing magical about a paper copy that would make it
invulnerable to fraud/errors.
Ed Gerck

@_date: 2003-03-07 13:33:40
@_author: Ed Gerck 
@_subject: Scientists question electronic voting 
This is true in the UK, but legal authorization is required to do so. In
the US, OTOH, the paper voting systems today are done in such a way
that the privacy of the vote is immune even to a court order to disclose it.
Voters are not anonymous, as they must be identified and listed in the
voter list at each poll place, but it is impossible (or, should be) to link
a voter to a vote.  This imposes, for example, limits on the time-stamp
accuracy and other factors suhc as storage ordering that could help in
linking a voter to a vote.
Ed Gerck

@_date: 2003-03-07 13:38:11
@_author: Ed Gerck 
@_subject: Scientists question electronic voting 
The broken system is the *entire* system -- from voter registration,
to ballot presentation (butterfly?), ballot casting, ballot storage,
tallying, auditing, and reporting.
Brazil, 120 million voters, 100% electronic in 2002, close to 100%
since the 90's, no paper copy (and it failed when tried). BTW, the
3 nations with largest number of voters are, respectively:
- India
- Brazil
- US
Ed Gerck

@_date: 2003-03-07 14:08:16
@_author: Ed Gerck 
@_subject: double shot of snake oil, good conclusion 
I believe we are in agreement in many points. Microsoft's mistake was
to claim that "For example, it might be possible to view a document but
not to forward or print it."  As I commented, of course it is possible
to copy of forward it.  Thus, claiming that it isn't possible is snake oil
and I think we need to point it out.
I'd hope that the emphasis on trustworthy computing will help Microsoft
weed out these declarations and, thus, help set a higher standard.
Ed Gerck

@_date: 2003-03-24 15:57:18
@_author: Ed Gerck 
@_subject: Who's afraid of Mallory Wolf? 
I'm sorry to say it but MITM is neither a fable nor
restricted to laboratory demos. It's an attack available
today even to script kiddies.
For example, there is a possibility that some evil attacker
redirects the traffic from the user's computer to his own
computer by ARP spoofing. With the programs arpspoof,
dnsspoof and webmitm in the dsniff package it is possible
for a script kiddie to read the SSL traffic in cleartext (list
of commands available if there is list interest). For this attack
to work the user and the attacker must be on the same LAN
or ... the attacker could be somewhere else using a hacked
computer on the LAN -- which is not so hard to do ;-)
The only sign of the spoofing attack is that the user gets a
warning about the certificate that the attacker is presenting.
It's vital that the user does not proceed if this happens --
contrary to what you propose.
BTW, this is NOT the way to make paying for CA certs go
away. A technically correct way to do away with CA certs
and yet avoid MITM has been demonstrated to *exist*
(not by construction) in 1997, in what was called intrinsic
certification -- please see  Ed Gerck

@_date: 2003-03-24 23:20:48
@_author: Ed Gerck 
@_subject: Who's afraid of Mallory Wolf? 
I'm sorry, but no. The bug in MSIE, that prevented the correct
processing of cert path restraints and which led to easy MITM
attacks, has been fixed for some time now.  Consulting browser
statistics sites will show that the MSIE update in question,
fueled by the need for other security updates, is making
good progress.
I'm sorry but the "a priori truth" above is false .  Ignorance about
the flaw, that is now fixed, and the need to do a LAN attack (if
you  want not to mess with the DNS) have helped avert a major
public exploit. The hole is now fixed and the logic fails for this
reason as well.
There is a good reason -- MITM. AnonDH and self-signed
certs cannot prevent MITM.
But it is, please see the spoof/MITM method in my previous post.
Which, BTW, is rather old info in some circles (3 years?) and is
easy to do by script kiddies with no knowledge about anything we
are talking here -- they can simply do it. Anyone can do it.
I think Ian's post, with all due respect to Ian, reflects a misconception
about cert validation. The misconception is that cert validation can
be provided as an absolute reference -- it cannot. The *mathematical*
reasons are explained in the paper I cited. This misconception
was discussed some 6 years in the ssl-talk list and other lists, and
clarified at the time -- please see the archives. It was good, however,
to post this again and, again, to allow this to be clarified.
You are asking for the same thing that was asked, and answered,
6 years ago in the ssl-talk and other lists. There is a way to do it
and the way is not self-signed certs or SSL AnonDH.
Problem -- SSL AnonDH cannot prevent MITM. The solution is
not to deny the problem and ask "who cares about MITM?"
;-) If anyone comes across a way to explain it, that does not require study,
please let me know and I'll post it.
OTOH, some practical code is being developed, and has been sucessfully
tested in the past 3 years with up to 300,000 simultaneous users, which
may provide the example you ask for. Please write to me privately if you'd
like to use it.
Ed Gerck

@_date: 2003-03-25 10:52:03
@_author: Ed Gerck 
@_subject: Who's afraid of Mallory Wolf? 
This would still depend on what the paper calls "extrinsic references",
that are outside the dialogue and create opportunity for faults (intentional
or otherwise). The resulting problems for PGP are summarized in

@_date: 2003-03-25 10:55:08
@_author: Ed Gerck 
@_subject: Who's afraid of Mallory Wolf? 
Maybe we're talking about different MSIE bugs, which is not hard to do ;-)
I was referring to the MSIE bug that affects the SSL handshake in HTTPS,
from the context in discussion. BTW, HTTP has no provision to prevent
MITM in any case -- in fact, establishing a MITM is part of the HTTP
tool box and used in reverse proxies for example.

@_date: 2003-03-25 11:38:31
@_author: Ed Gerck 
@_subject: Who's afraid of Mallory Wolf? 
Let me summ up my earlier comments: Protection against
eavesdropping without MITM protection is not protection
against eavesdropping.
In addition,  when you talk about HTTPS traffic (1%) vs.
HTTP traffic (99%) on the Internet you are not talking
about user's choices -- where the user is the party at risk
in terms of their credit card number. You're talking about
web-admins failing to protect third-party information they
request. Current D&O liability laws, making the officers
of a corporation personally responsible for such irresponsible
behavior, will probably help correct this much more efficiently
than just a few of us decrying it.
My personal view is that ALL traffic SHOULD be encrypted,
MITM protected, and authenticated, with the possibility of
anonymous authentication if so desired. Of course, this is
not practical today -- yet. But we're working to get there.
BTW, a source once told me that about 5% of all email traffic
is encrypted. So, your 1% figure is also just a part of the picture.
Cheers --/Ed Gerck

@_date: 2003-03-25 12:48:08
@_author: Ed Gerck 
@_subject: Who's afraid of Mallory Wolf? 
Cost is not the point even though cost is low and within the reach of
script kiddies.
I agree with this. This is helpful. However, supporting this by
asking "Who's afraid of Mallory Wolf?" is IMO not helpful --
because we should all be afradi fo MITM attacks. It's not good
for security to deny an attack that is rather easy to do today.
Your proposal is, possibly, a good option to have. However, it does not:
provide a credible protection against eavesdropping. It is better than
ROT13, for sure.
Essentially, you're asking for encryption without an authenticated end-point.
This is acceptable. But I suggest that advancing your idea should not be
prefaced by denying or trying to hide the real problem of MITM attacks.
Ed Gerck

@_date: 2003-03-25 16:24:54
@_author: Ed Gerck 
@_subject: Who's afraid of Mallory Wolf? 
PGP's WoT already does that. To be clear, in PGP the entity that is attempting
to prove the linkage between a DN and a public key chooses which signatures
are acceptable, their "degree of trust", and how these signatures became
acceptable in the first place. BTW, a similar facility also exists in X.509, where
the entity that is attempting to prove the linkage may  accept or reject a CA
for that purpose (unfortunately, browsers make this decision "automatically"
for the user but it does not need to be so).
That said, the paper does not provide a way to implement the method I
suggested. The paper only shows that such a method should exist.
Ed Gerck

@_date: 2003-03-28 15:38:54
@_author: Ed Gerck 
@_subject: Run a remailer, go to jail? 
It would also outlaw pre-paid cell phones, that are anonymous
if you pay in cash and can be untraceable after a call. Not to
mention proxy servers. On the upside, it would ban spam ;-)
Ed Gerck

@_date: 2003-11-17 15:18:23
@_author: Ed Gerck 
@_subject: Are there... 
I am sorry to differ, but packaging the encryption-key along with
the ciphertext (even if part of the plaintext) will create additional
dependencies and reduce the search space of possible results. In
short, one should avoid sending any additional information about
the encryption key.
Ed Gerck

@_date: 2003-10-02 13:03:59
@_author: Ed Gerck 
@_subject: anonymous DH & MITM 
False. In fact, it is possible  to prove the existence of at least one open and
anonymous protocol that is immune to MITM in any given, feasible scenario
(ie, given a threat model).
Ed Gerck

@_date: 2003-10-03 15:44:01
@_author: Ed Gerck 
@_subject: how to defeat MITM using plain DH, Re: anonymous DH & MITM 
No. What you get is a shared key between Bob and Mallory and *another* shared
key between Alice and Mallory. This is important for many reasons.
First, it provides a way to detect that a MITM attack has occurred. For example,
if the MITM is not there at any time forth after key agreement, the DH-based encryption/decryption will not work since Alice and Bob did NOT share a
secret key when under the MITM attack. As another example, if Alice and Bob can
communicate using another channel even an ongoing MITM attack can be likewise
Second, and most importantly, this provides a provable way to defeat MITM using
plain DH. For a set of communication channels, not necessarily 100% independent
from each other, if the probability of successfully mounting a MITM attack is
a(i) < 1 for each channel i, then by using N channels of communication we can
make the probability of a successful MITM attack as small as we desire and, thus,
defeat a MITM attack even using plain DH [1]. Moreover, this method can present
an increasing challenge to Mallory's computing resources and timing, such that
the probability a(i) itself should further decrease with more channels. In other
words, Mallory can only juggle so many balls. I pointed this out some years ago at
the MCG list. It's possible to have at least one open and anonymous protocol
immune to MITM -- which I called multi-channel DH.
Ed Gerck
[1] In a stronger form, we can allow the probability of successfully mounting a
MITM attack to be a(i) = 1 for all except for one channel in the set and still can
make the probability of a succesfull MITM attack as small as we desire, so that
we can still defeat a MITM attack using plain DH.

@_date: 2003-10-06 11:12:54
@_author: Ed Gerck 
@_subject: how to defeat MITM using plain DH, Re: anonymous DH & MITM 
1948 sounds right? The mathematical basis for this approach is Shannon's
Tenth Theorem of 1948. We are creating a correction channel. BTW, the
main reason why I decided to point this out is because, even though this
thread has been going on for a long time, the possibility of defeating MITM
with plain DH was not being recognized. Perhaps we need more examples
like yours and Zooko's. Who else has more examples?
The question of anonimity seems to be still pending, as raised by Anton
and bear.  The problem here seems to be the definition of anonymity.
Are we willing to accept that anonymity must decrease over time as
a result of the very communication based on that anonymity? In other
words, anonymity is not a static property of a communicaiton channel.
I note also that in multi-channel DH the interest is in creating many session
keys with very small delay. Thus, using newspaper, commercial
radio, television, etc. is not so feasible. The keys are also ephemeral and
cost is an issue. The DH multi-channels need to be created in real-time and
at low cost for this approach to be practical.
Ed Gerck

@_date: 2003-09-03 17:03:20
@_author: Ed Gerck 
@_subject: Is cryptography where security took the wrong branch? 
Arguments such as "we don't want to reduce the fraud level because
it would cost more to reduce the fraud than the fraud costs" are just a
marketing way to say that a fraud has become a sale. Because fraud
is an hemorrhage that adds up, while efforts to fix it -- if done correctly

@_date: 2003-09-14 20:56:10
@_author: Ed Gerck 
@_subject: quantum hype 
This is not relevant when the technology is correctly used for Q key
transmission because the sender would not be in the dark (sorry for the
double pun) for so long.
This should not happen in a well-designed system. The sender sends
the random key in the Q channel in such a way that compromises in
key transmission are detected before the key is used.
That said, Q cryptography is something else and should not be confused
with Q key distribution.
Ed Gerck

@_date: 2003-09-24 17:40:38
@_author: Ed Gerck 
@_subject: why are CAs charging so much for certs anyway? (Re: End of the line  
Yes, there is a good reason for CAs to charge so much for certs.
I hope this posting is able to set this clear once and for all.
  FOREWORD: It's often said that a good lawyer should be able to argue
  both sides of an issue... Though I am not a lawyer, I believe it is
  instructive to see things from all perspectives. My answer may help see
  things from the CA side and IMO does not contain any exaggeration.
Of course, to properly answer the question I would need to write a
CA Business Plan, which should contemplate the various pros, cons,
pricing, and contingency plans. However, without daring to use much
time in such a dubious endeavor, let me just briefly discuss the CA
business model in order to better motivate the pricing strategy answer.
1. Product Liability to Clients: Zero.
 CAs provide certificates that have zero content, zero warranties,
 zero assurances and, hence, zero liability under any law system.
 This is a very good point for CAs, and it is difficult to imagine a
 legal business that could get to so close to this goal. Perhaps,
 chiromancy with consenting adults over a phone line could
 be similar, but with a lesser market.
2. Contract Liability to Users: Zero.
 Since the certificate's users (ie, historically known as the
 relying-parties) are not the ones that paid for the certificate to
 the CA (ie, the certificate was paid for by the subscriber), this
 means that the CA has no responsiblity or contractual obligation
 whatsoever to the certificate's users, hence zero liability.
3. After-Sales Support: Almost Zero.
 This is also a very good point. There is no maintenance, set-up,
 compatibility or other post-sales questions to worry about. The
 product also self-destructs so to say after a period of usually one
 year, so there is not even a marginal need to maintain compatible
 systems for diagnosis after one year. Regarding the eventual need to
 revoke a certificate, here we are forced to say that after-sales
 support is "almost zero". However, that is not a serious issue
 because certificate revocation has also no warranties or assurances,
 hence this freely provided service has no liabilities or obligations
 to the CA, not even to be expedite.
4. Product Recall: Zero.
 The subscriber cannot send back an issued certificate and decide to
 cancel his order because the certificate does not work on the new
 Gizmo v4.0 or equivalent browser, or just because it does not like
 it any more. Once the product is sold, the revenues are liquid.
5. Technical Regulation: Almost Zero.
 Certificates are technically regulated by X.509 but X.509 is very
 tolerant on almost all issues except purely syntatic issues which
 are handled blindfolded by software. Further, CAs can issue their
 very own operating laws (CPS - Certificate Practice Statement)
 according to their needs and profit rules. They can define all their
 operating parameters.
6. Legal Regulation: Almost Zero.
 The CA's CPS must be accepted by the client and the CA can change it
 at will, at any moment. Legislation, such as Illinois', already
 consider such self-made laws as legally binding in lieu of any
 legislation's mandated procedures (see a typical CA CPS).
7. Legal Mandatory Use: Possible.
 This is a very positive point for CAs. Legal initiatives may make it
 mandatory to use CAs (eg, TTPs) in order to allow certificates to be
 deployed. So, CAs would have captive markets in this positive
 scenario and the client would not be able to decide not to use a CA.
8. Matched Sales: Strongly Enforced.
 A CA can reach profitable agreements with a wide array of partners,
 such as financial agents, software producers, content providers,
 etc., in order to render its certificates strongly matched to the
 partner's products or services. This is easily cryptographically
 guaranteed and sounds reasonable when explained to customers. For
 example, software producer ACME can easily decide that its product
 Gizmo will only accept plug-ins signed by a specific CA -- allowing
 several legal avenues for matched sales.
9. Product Price: At Will.
 There is no reference in price for an array of 2 Kbytes. It can
 range from $5.00 to $500.00 or beyond. Since the market also has to
 accept matched sales as a natural procedure in this case, it is not
 difficult to organize different product classes so that essentially
 the same array of 2 Kbytes can have very profitable margins for
 high-end (ie, expensive) applications.
10. Insurance: Paid By The Client.
 To cover for those few cases where the CA could still be liable (ie,
 gross negligence, employee collusion, fraud, etc.) to its clients,
 it is accepted to ask for the client to pay for insurance against
 the CA's acts. Since the users have no coverage (they are not part
 of the contract and they are not considered innocent bystanders as
 with car accidents), such insurance will need to cover only the
 client.
PRO SUMMARY: CAs make very good sense as businesses, shareholder's
risk is low and the activities are essentially unregulated. Further, future
legislation cannot impose more burdens because it would be technically
CON SUMMARY: Of course, the problems of e-commerce are not solved
by the CA business model and the so-called relying-parties must rely on
themselves. Which might point out to a possible technology change over if
such market forces gain momentum, possibly also after a stage of apparent
PRICING STRATEGY: CAs should keep their prices high and find ways
to add price to current products (eg, offering insurance, different
certificate classes, benefits for CRL access, etc.) -- because the potentially
difficult mid-term future of such business impose the need for a large
ROI in a short time. This is probably not a long-term business activity.
Ed Gerck

@_date: 2004-04-07 13:40:29
@_author: Ed Gerck 
@_subject: Firm invites experts to punch holes in ballot software 
The principle here is that no one should be able to prove how the voter voted, not even the voter. Yes, votes need to be verified and voters are certainly one party that can do it. However, you never want to allow the voter to take any kind of "receipt" out of the voting station if that receipt can be used to determine how the voter voted, e.g. by matching a number or pattern on the ballot, even if to the voter. Otherwise, vote selling and coercion cannot be prevented.
Ed Gerck

@_date: 2004-04-08 09:17:39
@_author: Ed Gerck 
@_subject: voting 
a counterpoint...
:-) that's one S too many. For true believers, KIS is enough.
If the real vote (the thing that gets counted) is machine-read
from the OCR-B, and the voter is verifying the human-readable OCR-B text on the ballot, then how can one say the vote is really You end up trusting the machines after all, both for scanning as well as for tallying. In addition, the paper ballots could also be falsified and the totals would be wrong even if someone would have us believe that their machines are infallible.
Machines are not 100% efficient when counting paper ballots. There
are misreads, rejections, jamming, etc. The usual procedure is to feed
the ballots twice in the machine, for verification. What happens
if the result differs? Since you don't know which paper ballots were misread, you MUST end up having to count them ALL manually. Florida law,
for example, unequivocally requires a manual recount in a close election

@_date: 2004-04-15 18:58:46
@_author: Ed Gerck 
@_subject: voting 
The flaw in *both* cases is that it reduces the level of privacy protection
currently provided by paper ballots.
Currently, voter privacy is absolute in the US and does not depend
even on the will of the courts. For example,  there is no way for a
judge to assure that a voter under oath is telling the truth about how
they voted, or not. This effectively protects the secrecy of the ballot
and prevents coercion and intimidation in all cases.
Thus, while the assertion that "Only if all the trustees collude can
the election be defrauded" may seem to be reasonable at first glance, it
fails to protect the system in the case of a court order -- when all the
trustees are ordered to disclose whatever they know and control.
Also, the assertion that "All of this is possible while still m
aintaining voter secrecy and privacy essential to all public elections" is incorrect, for the same reason.
Moreover, the assertion that "Vote receipts cannot be used for vote selling or to coerce your vote" is also incorrect, for the same reason.
These shortcomings do not depend on any specific flaw of a shuffling
process, a TTP, or any other component of either system. Rather, it is a design flaw. A new election system should do "no harm" -- reducing the level of voter privacy and ballot secrecy should not be an acceptable trade-off for changing from paper to electronic records, or even
electronic verification.
Court challenges are a real scenario that election officials talk about and want to avoid. Without making voter privacy inherently safe from court
orders, voter privacy and ballot secrecy are at the mercy of casuistic, political and corruption influences -- either real or potential. When the stakes are high, we need fail-safe procedures.
Now, you may ask, is there any realistic possibility of a court order for all trustees to reveal their keys?
Yes, especially in a hot and contested election -- and not only Bush vs.
Gore. Many local elections are very close and last year an election
in California was decided by *one* vote. For example, the California Secretary of State asked this as an evaluation question, when they were testing voting systems for the 2000 Shadow Election Project.
The question was whether and to what extent the voting system could be broken under court order  ? for example, if some unqualified voters were wrongly allowed to vote in a tight election and there would be a court order to seek out and disqualify their votes under best efforts.
Perhaps a trustee could be chosen who would be immune even from a US
court order?
Well, not for a US election, which is 100% under state and/or federal But there are additional scenarios -- a bug, Trojan horse, worm and/or virus that infects the systems used by all trustees would also compromise voter secrecy and, thereby, election integrity.
Ed Gerck

@_date: 2004-04-16 11:43:57
@_author: Ed Gerck 
@_subject: voting 
The privacy, coercion, intimidation, vote selling and election integrity
problems begin with giving away a receipt that is linkable to a ballot. It is not relevant to the security problem whether a voter may destroy his receipt, so that some receipts may disappear. What is relevant is that voters may HAVE to keep their receipt or... suffer retaliation...
not get paid... lose their jobs... not get a promotion... etc. Also
relevant is that voters may WANT to keep their receipts, for the same
As long as this does not go against the 'first law' for public voting systems: voters must not be linkable to ballots.
The 'second law' also takes precedence: ballots are always secret, only
vote totals are known and are known only after the election ends.
There is no tradeoff prossible for voter privacy and ballot secrecy.
Take away one of them and the voting process is no longer a valid
measure. Serious voting system research efforts do not begin by
denying the requirements.
There is no such principle.
You don't have this option when the public at large is considered, for
a public election. You can do it in a private election for a club,
for example, but even then only if the bylaws allow it.
Ed Gerck

@_date: 2004-04-19 00:55:02
@_author: Ed Gerck 
@_subject: voting 
The lowest possible totals are per race, per ballot box. The 'second law' allows you to have such totals -- which are the election results for that race in that ballot box. For example, if there are two candidates (X and Y) in race A ,
two candidates (Z and W) in race B, and only one vote per candidate is allowed in each race, the election results for ballot box K might be:
Vote totals for race A in ballot box K:
  Votes for candidate X: 		  5
  Votes for candidate Y:   		 60
  Blank votes: 	         		 50
Vote totals for race B in ballot box K:
  Votes for candidate Z: 		 45
  Votes for candidate W:   		 50
  Blank votes: 	         		 20
Total ballots in ballot box K: 	   	115
Because only the vote totals are known for each race, a voter cannot be identified by recognizing a pre-defined, unlikely voting pattern in each race of a ballot. This exemplifies one reason why we need the 'second law' -- to preserve unlinkability between ballots and voters.
No. All you need is that there should be more than one voter
per ballot box. This is a rather trivial requirement to meet.
Ed Gerck

@_date: 2004-08-10 10:42:13
@_author: Ed Gerck 
@_subject: Microsoft .NET PRNG (fwd) 
The PRNG should be the least concern when using MSFT's cryptographic
provider. The MSFT report 140sp238.pdf says:
Not only RSAENH writes keys to a lower-security file system... it also does
not provide the encryption security to protect those keys. Because RSAENH
trusts Windows XP to provide that critical link in the security, RSAENH cannot
be trusted to provide the security. In addition, there is a third problem in
securing the keys, namely the security gap between RSAENH and Windows XP.
The most troubling aspect, however, is that RSAENH makes it easy to provide
a covert channel for key access. FIPS 140-1 Level 1 compliant.
Ed Gerck

@_date: 2004-08-27 00:12:57
@_author: Ed Gerck 
@_subject: system reliability -- Re: titles 
That sounds cute but I believe it is incorrect. Example: error-
correcting codes. The theory of error-correcting codes allows
information to be coded so that it can be recovered even after
significant corruption. This allows, for example, for
_secret-sharing_ with multiple systems so that no operating
system platform has enough information or enough power to even
allow a compromise. Such an application can be much more secure
than any operating system supporting it.
RAID is another example of a realiable system that is made out
of unreliable parts.
The human application of these principles is well-known in
information security and also supports the examples above. Humans
are notorious for breaking security systems. Humans are the
wetware equivalent of an operating system. A common solution for
the risk presented by humans is also _secret-sharing_: No person
may have access to classified information unless the person has
the appropriate security clearance and a need-to-know.
What this means is that the search for the "perfect" operating
system as the solution to security is backwards.
Ed Gerck

@_date: 2004-08-29 22:17:42
@_author: Ed Gerck 
@_subject: system reliability -- Re: titles 
If I have N independent platforms, the probability is smaller.
Ah, the word "trust". What makes you trust something cannot be
that something by itself. It needs to be provided in multiple,
independently as possible, channels. What may make me trust a
MD5 fingerprint is the fact that the code works according to
some test vectors I define.
That lecture needs to be understood after the word "trust" is
defined -- which, btw, the lecture never did.
  >>What this means is that the search for the "perfect" operating
This is true but only if the weakest link is isolated. If you have
a strand with three threads, the weakest thread will break first but
the other two threads will still hold. Increase the number of threads
to N >> 1 and the weakest thread is not really relevant any more. Of
course, the system will still fail under an excess stress, but not
because one thread (read, OS) failed.
Yes, humans AND data are the weakest links.
But, according to the theory of error-correcting codes, the influence
of the errors you mention can be reduced to a value as close to ZERO as
you desire.
Not if designed well. A good security system is not like a baloon
that pops with one shot.
When the heart confutes the mind, that man's hand confutes itself.
Ed Gerck

@_date: 2004-12-22 11:04:33
@_author: Ed Gerck 
@_subject: solution, Re: The Pointlessness of the MD5 "attacks" 
"we do not know how to create" != "we will not know how to create"
The fear of a possible (likely?) attack as described by Wagner should
be countered by a concrete solution, not by considering it a time bomb
with a hopefully long enough fuse.
I think such a concrete solution exists, still using MD5. Even though
MD5 is not collision-resistant. The solution applies to everything that
Ben says, as well.
If Microsoft chooses a salt value for an MD5-HMAC, which salt value
Microsoft does not disclose to the programmer and the world until the
file is (1) quality-controlled and (2) handled for distribution, the
programmer would NOT be able to find the collision. Security is easily
assured by Microsoft choosing the salt only after (1) QC. Distribution
of any software, or text, can be likewise protected -- just don't let
the attacker control everything.
The problem here is not MD5. The problem is allowing the attacker to
have too much power.
Ed Gerck

@_date: 2004-01-05 17:32:37
@_author: Ed Gerck 
@_subject: [Fwd: Re: Non-repudiation (was RE: The PAIN mnemonic)] 
In business, when repudiation of an act is anticipated we're reminded by
Nicholas Bohm (whose clear thinking I know and appreciate for 6 years)
that some lawyers find it useful to define "irrebuttable presumptions"  -- a
technique known to the law and capable of being instantiated in statute or contract.
For example, a legal "irrebuttable presumption" can take the form of a bank check
contract stating that a check (even though it can be *proven* a posteriori to be a
forgery) is payable by the bank if the account holder did not notify the bank to
repudiate the check *before* the check was presented to the bank for payment.
The requirement can be seen an "out-of-band" signal from the account holder to
the bank, which absence makes the check's payability an irrebuttable presumption
by the bank. In this case, as long as the check's signature does not look like a
(obvious) forgery and there is enough balance in the account, the bank has no
liability to that customer in paying the check. Note also that the effectiveness of
this method relies on an "indirect proof" -- the absence of a previous communication
makes the check payable.
Likewise, in a communication process, when repudiation of an act by a party is
anticipated, some system security designers find it useful to define "non-repudiation"
as a service that prevents the effective denial of an act. Thus, lawyers should
not squirm when we feel the same need they feel -- to provide for processes
that *can be* conclusive.

@_date: 2004-01-07 11:42:40
@_author: Ed Gerck 
@_subject: [Fwd: Re: Non-repudiation (was RE: The PAIN mnemonic)] 
Huh? Processes that can be conclusive are useful and do exist, I read here,
in the legal domain. It may not be so clear how such processes can exist in
the technical domain and that's why I'm posting ;-)
Using an information theory model, it's clear that authentication needs one
channel of information (e.g., the CA's public key, the password list) in addition
to the signal (e.g., a signed message, a username/password entry). Authentication
rests on the information channel being trusted (i.e., independently verifiable). In
this model, non-repudiation is different because it needs at least one additional
out-of-band signal (where authenticated absence of the signal is also effective).
BTW, that's why digital signatures per se are repudiable -- there's no second,
out-of-band signal.
An additional technical difference is that authentication promotes "strength of
evidence" while non-repudiation promotes "lack of repudiation of evidence".
The latter is intuitively recognized to be stronger because  a single, effective
denial of an act can rebuke any number of strong affirmations.
This also means, intuitively,  that another difference exists. Non-repudiation
should be harder to accomplish than authentication (you want more, you need
to pay more). However, to the  extent that the process *can be* conclusive,
non-repudiation may be worth it. Imagine the added costs, time and hassle
(going back to a real-world comparison) if your bank would have to call you
to confirm payment for every check you sign? This would be the case if
paying a check could not be cast as a conclusive process for the bank (i.e.,
without the possibility of an irrebuttable presumption of payability).

@_date: 2004-07-07 11:46:10
@_author: Ed Gerck 
@_subject: identification + Re: authentication and authorization 
I believe that a significant part of the problems discussed here is that
the three concepts named in the subject line are not well-defined. This
is not a question of semantics, it's a question of logical conditions
that are at present overlapping and inconsistent.
For example, much of what is called "identity theft" is actually
"authentication theft" -- the stolen credentials (SSN, driver's
license number, address, etc) are used to falsely *authenticate* a
fraudster (much like a stolen password), not to identify. Once we
understand this, a solution, thus, to what is called  "identity theft"
is to improve the *authentication mechanisms*, for example by using
two-factor authentication. Which has nothing to do with identification,
impersonation, or even the security of identification data.
In further clarifying the issue, it seems that what we need first is
a non-circular definition for identity. And, of course, we need a
definition that can be applied on the Internet.  Another important
goal is to permit a safe automatic processing of identification,
authentication and authorization [1].
Let me share with you my conclusion on this, in revisiting the
concept of identification some time ago. I found it useful to ask
the meta question -- what is identification, that we can identify it?
In short, a useful definition of identification should also work
reflexively and self-consistently [2].
In this context, what is "to identify"? I think that "to identify"
is to look for connections. Thus, in identification we should look
for logical and/or natural connections. For example:
- between a fingerprint and the person that has it,
- between a name and the person that answers by that name,
- between an Internet host and a URL that connects to it,
- between an idea and the way we can represent it in words,
- conversely, between words and the ideas they represent,
- etc.
Do you, the reader, agree?
If you agree you have just identified. If you do not agree, likewise
you have identified! The essence of identification is thus to find
connections -- where absence of connections also counts.
Identification can thus be understood not only in the sense of an
"identity" connection, but in the wider sense of "any" connection.
Which one to use is just a matter of protocol expression, need, cost
and (very importantly) privacy concerns.
The word "coherence" is useful here, meaning any natural or logical
connection. To identify is to look for coherence. Coherence with and
between a photo, a SSN, an email address, a public-key and other
attributes: *Identification is a measure of coherence*.
The same ideas can be applied to define "authentication" and
"authorization" in a self-consistent way, without overlapping with
each other.
Ed Gerck
[1] The effort should also aim to safely automate the process of reliance
by a relying-party. This requires path processing and any algorithm to
eliminate any violations of those policies (i.e., vulnerabilities) that
might be hard to recognize or difficult to foresee, which would
interfere with the goal of specifying a wholly automated process of
handling identification, authentication and authorization.
[2] This answer should be useful to the engineering development of all
Internet protocols, to all human communication modes, to all
information transfer models and anywhere one needs to reach beyond
one's own point in space and time.

@_date: 2004-07-09 14:56:20
@_author: Ed Gerck 
@_subject: identification + Re: authentication and authorization 
Thanks. That's why my suggestion is that techies should solve the real
problem (authentication theft) that is allowing identity theft to create
damage to the general public. What's the use of stolen identity data if
that data cannot be used to impersonate the victim? At most, it would be
a breach of privacy... but not a breach of access and data protected by
the access. Furthermore, if identity data is not used as authenticators,
they would not be so much available (and valuable!) to be stolen in the
first place.
BTW, the confusion between identification and authentication begins in
our circle. Just check, for example, The Handbook of Cryptography by
Menezes et. al.:
  	and entity authentication are used synonymously throughout this book."
Ed Gerck

@_date: 2004-07-21 12:39:42
@_author: Ed Gerck 
@_subject: RP -- Re: Using crypto against Phishing, Spoofing and Spamming... 
> This totally leaves out the relying-party ... which is the
 > primary beneficiary of the PKI model from being a part
 > of the contractual business process ... which would imply
 > little or no legal recourse if something went wrong.
 > ...
 > The PKI frequently creates a total disconnect between
 > the parties of the certification "contract" ... and the
 > relying parties ... which should have recourse in case
 > something went wrong aren't even a part of it.
The PKI model is not tied to any legal jurisdiction and is not a
business process. What is meant then by relying-party (RP) and
RP Reliance in X.509 and PKIX? I hope the text below, from a
work in progress submitted as an IETF ID, helps clarify this issue.
RP reliance is a technical term in PKIX [1]. The concept of RP reliance
is needed in PKIX/X.509 because not every aspect of certificate
management can be fully verified by an RP. In short, RP reliance is
that which can break the RP's security policy.
For example, a user verifying a certificate's revocation status
(e.g., using mechanisms such as querying a repository or receiving
notices delivered via a message service) will depend on the
conforming administration and management of that status by one or
more authoritative entities -- i.e., the user becomes a relying party
to these entities.
More generally, a user who depends on values provided by an entity
(e.g., CA) that seem reasonable on their face value to the user but
are especially hard to check by the user is an RP to that entity.
What does the RP rely on? The RP relies on entities (e.g., the CA)
following PKIX recommendations that the RP CANNOT verify. For
example, an RP does not rely on the CA for certificate path
processing (because the RP CAN verify it) but must rely on the CA for
ensuring that the CA's signing key is properly secure and not
compromised. The latter cannot be verified by the RP and becomes,
thus, a limitation for reliance.
RP reliance limitations are objectively defined in PKIX, without
reference to domain policies, user security policy, jurisdiction-
based rules of laws and contracts or anything else that needs to be
locally defined.
As a literal value, RP reliance on the revocation status of a
certificate is defined by a conforming certificate path procedure
leading to a bit value -- "revoked" or "not revoked". The literal bit
value depends both on processes that the RP CAN verify and processes
that the RP CANNOT verify. The latter defines the limitations of RP
reliance. The lesser the limitations of RP reliance, the lesser the
risk faced by the RP that RP reliance might be broken by factors
outside RP control.
The literal bit value has no associated semantics outside the scope
of PKIX. In other words, RP reliance is syntactic, not semantic. Any
semantic value (e.g., legal reliance, contractual obligations
concerning use, public policy, etc.) MUST lie outside the scope of
PKIX and is, for example, regulated by terms in the CA's CPS.
Ed Gerck
[1] A relying party (RP) is a user who processes a certificate chain, and
    then acts in reliance on the end-entity (EE) certificate issued by a
    CA (the issuer CA) and any associated revocation information.

@_date: 2004-07-28 15:25:42
@_author: Ed Gerck 
@_subject: The future of security 
Email end-to-end: PGP, PGP/MIME, S/MIME. Not tunnel SSL or SSL
at the end points.

@_date: 2004-05-27 17:06:39
@_author: Ed Gerck 
@_subject: The future of security 
Indeed, email is not so good anymore. When lack of message
security in email becomes clearer to the users, as clear as
spam is today, the value of email will approach zero.
Practically anyone can read the email you send and receive,
your ISP included. What's the fuss with google's gmail? Gmail's
differential is that they do not hide they will search through
your mailbox. Users are realizing that an email is like a postcard,
open for anyone to read and write on it. But encryption and
authentication are a hassle today, with less than 2% of all email
encrypted (sorry, can't cite the source I know).
The problem with current schemes has been that they only work
when both sender AND recipient already use the feature, which
probability is zero in the beginning of adoption. It's a chicken-
and-egg proposition. It is also a change to email. Even though the
existing ideas are sound in principle (e.g., PGP/MIME, S/MIME,
email gateways, etc.) they are all a replacement product with
many barriers for adoption.
Instead of a replacement, I believe that what we need is a
complement to solve the lack of message security in email
(including sender spoofing). Email is just the transport.  The
solution should be able to start from a single end user, should
require no change to records/software that end users do not
control, and should require no cooperation from email providers
and ISPs.
Cheers--/Ed Gerck

@_date: 2004-05-28 09:53:58
@_author: Ed Gerck 
@_subject: Yahoo releases internet standard draft for using DNS as public 
The main problem with this approach is revealed in a mind slip by Yahoo
themselves at  :
  For consumers, such as Yahoo! Mail users or a grandmother accessing email
  through a small mid-western ISP, industry support for sender authentication
  technologies will mean that they can start trusting email again
It's "industry support". We know what it means: multiple, conflicting
approaches, slow, fragmented adoption --> will not work. It would be better
if the solution does NOT need industry support at all, only user support. It
should use what is already available.
Cheers--/Ed Gerck

@_date: 2004-05-30 16:43:52
@_author: Ed Gerck 
@_subject: Yahoo releases internet standard draft for using DNS as public 
The alternative to change (ie, replacement) is complement. I mentioned that.
I laugh with you ;-)
S/MIME and PGP did NOT earn user support. What's wrong with them, we all
know and Martin exemplifies below:
That's cute but your suggestion may have missed the point. If the email
lacks a valid signature, there may be many causes. Today, within CA cert
rollover dates, your browser's root certs may just need an update. Absence
of a valid signature simply means you have less evidence of whom it's from,
not no evidence.
No -- DomainKeys has nothingf to do with 'email cryptography'. They are
S/MIME and PGP/MIME.

@_date: 2004-05-31 12:04:41
@_author: Ed Gerck 
@_subject: Yahoo releases internet standard draft for using DNS as public 
What you're saying is that based on only *two* bits of information (e.g., SSH=1
and SSL=0) for a given mail sender, the message could be authenticated well-enough
to be useful in the operational context.
I agree with this and that's why I think that conventional digital signatures
with 1024-bit keys are an overkill for common email. If the ugly blob of base64
rubbish is small enough, it should be tolearable.
The problem with asymmetric keys, though, is that faking short signatures is
too trivial for current cryptosystems.
Ed Gerck

@_date: 2004-11-07 10:51:39
@_author: Ed Gerck 
@_subject: When A Pencil And Paper Makes Sense 
Here are some things that can --and do-- go wrong with the scanned ballots:
- blank votes (where the voter could have made a mark but did not) can
be "voted" at will after the ballot is cast by the voter, and no one can
detect the fraud.
- by looking at the vote pattern, a "voter contract" to vote for a certain
candidate can be verified by a third-person (not necessarily a poll official,
could be a party observer) and the voter can be rewarded or punished (if the
pattern does not show up).
- in a two-candidate race, voters circle a candidate and write "not this one".
Should it not count, even though voter intent is clear?
- voters pause the pencil on an option, and decide not to mark it; notheless,
the optical reader reads it as a vote. Was the voter intent respected?
- the cost of just storing these paper ballots, also after the election runs,
is several millions of dollars for San Francisco for example.
- the cost of printing and the special paper, makes this sytem have a high
recurring cost, election after election, in addition to the mounting storage
cost for past elections.
The solution to secure voting is not the current generation of "trust me"
electronic voting machines either, with or without an added paper ballot that
the voter can verify. The solution begins, as I see it, to recognize the hard
information theory problem behind what seems to be a simple process. This
analysis, and solution, is outlined in
Ed Gerck

@_date: 2004-09-15 11:39:25
@_author: Ed Gerck 
@_subject: public-key: the wrong model for email? 
[Perry: please use this version, if possible]
Public-key cryptography burdens the recipient and puts the recipient in
charge, while the sender is at the recipient's mercy. Is this the right
model for email security? After all, the sender is the party at risk.
To clarify, my comment is not that PKC is not useful for email. I believe it
is, but not directly used as it is today.
What I am saying is that the problem is key distribution. I want to separate the
key distribution solution as directly done by PKC today (send the public-key) from
a possible general solution for PKC key distribution that does not require sending
the public-key. The point is that when PKC solves this problem directly, it solves
it in a way that's useful for webservers (the webserver does not have to
trust the client's certificate) but presents difficulties for email senders (the
sender has to trust the recipient's certificate). Email use of PKC treats the
recipient as a server.
I think that what we need is a key distribution method for email (that can still
use PKC and PKI) where, because the sender has all the risk, the sender defines how
email is secured and delivered. The recipient should always be able to receive
it, without too much burden or required previous work.
For example, let's see how FedEx works. The sender chooses the service and the
type of envelope to protect the message. The sender also chooses the instructions
that must be followed before the envelope can be opened by the recipient.
The recipient has no charge to pay for, or burden, in order to receive the
envelope, and does not have to do anything before the envelope is sent. The
recipient is able to verify the identity of the sender and, if so desired,
refuse the envelope. The recipient can open the envelope if and only if
the recipient is willing to follow the sender's instructions (e.g., providing
name, address, date, signature).
Yes, SSL and public-key encryption are and continue to be a success for web
servers. However, the security model for protecting email with public-key
cryptography seems to be backwards, technically and business wise. The sender,
who is the party at risk, has to trust a lock provided by the recipient (his
public-key) to protect her secrets (the message). If FedEx would provide message
security a la PGP, PGP/MIME or S/MIME email, the sender would have to convince
the recipient to pay and send in advance an envelope for the sender to use.
The sender, however, would never know whether the envelope indeed prevented
others from prying into its contents.
A better situation would arise if the lock (i.e., the envelope) would be controlled
by the sender. Moreover, the recipient is not the business driver who needs to
provide, pay for and protect the lock. The sender is the party who has the
motivation to spend money to provide and protect the lock.
In short, I find that public-key cryptography cannot solve the security issues of
email when used as it is today and, most importantly, cannot provide the needed
motivation for users, senders and recipients, to buy into it.
It's not a matter of improvements in usability, better pricing or user education
[1]. It simply does not work as it should work. That's also why, IMO, it does
not take off. It is using the wrong mathematics for the problem.
Does anyone know of any email system that would put the sender in charge?
Ed Gerck
[1] Public-key cryptography gives the impression that email message security can
be achieved quite simply. The public-key can be distributed at will, no need for
secrecy, and anyone can receive private and secure messages. The same procedure
being applied to each side, sender and receiver, both could immediately engage
in private and secure communication.

@_date: 2004-09-15 22:19:52
@_author: Ed Gerck 
@_subject: public-key: the wrong model for email? 
My question on this is not about trust, even though I usually have  many
questions on trust ;-)
Yes, PKC provides a workable solution for key distribution... when you
look at servers. For email, the PKC solution is not workable (hasn't been)
and gives a false impression of security. For example, the sender has no
way of knowing if the recipient's key is weak (in spite of its length)
or has some "key-access" feature. Nonetheless, the sender has to use that
The analogy here is with you sending a confidential document using a courier
you don't know and cannot verify. Would you?
Further, it is generally in the recipient's interest that the decision to
send document X using channel Y should be under the sender's control. Any
limitation or directive imposed by the recipient on the sender (such as:
use my public-key) can shift the burden of risk to the recipient (your key
was weak, hence I had a loss). Liability follows power. The current use of
PKC in email is neither good to the sender nor to the recipient.
To further clarify, my comment is not that PKC is not useful for email. I
believe it is, but not directly used as it is today. The PKC key distribution
solution is backwards for email.
Ed Gerck

@_date: 2004-09-16 02:05:15
@_author: Ed Gerck 
@_subject: public-key: the wrong model for email? 
With Voltage, all communications corresponding to the same public key can be
decrypted using the same private key, even if the user is offline. To me, this
sounds worse than the PKC problem of trusting the recipient's key. Voltage
also corresponds to mandatory key escrow, as you noted, with all its drawbacks.
Ed Gerck

@_date: 2004-09-16 11:53:48
@_author: Ed Gerck 
@_subject: public-key: the wrong model for email? 
> the issue then is what level do you trust the recipient, what is the
If the recipient cannot in good faith detect a key-access ware, or a
GAK-ware, or a Trojan, or a bug, why would a complete background
check of the recipient help?
Talking about trust, it is important to note that when the email is sent
the recipient is already trusted not to disclose. But even though the
recipient is trustworthy his environment may not be. It is not a matter of
personal trust  or "complete background checks". This may all be fine
and, unknown to the recipient, the key might be weak, on purpose or by
some key-access "feature" included in the software (unknown to the user).
Or, the PKC software may have a bug (as PGP recently disclosed).
Loss from disclosure is also something that is much more important for
the sender. If the recipient's public-key fails to be effective in
protecting the sender, the sender's information is compromised. That's
why I make the point that PKC for email has it backwards: the sender
should not be at the recipient's mercy.
PKC for email also reverses the usual business model, because the
recipient is not so interested in protecting the sender or paying
for the sender's security. The sender would.
Regarding the use of PKC to sign emails, I see no problems using
PKC. The sender has the private-key, has the incentive to keep it
secure, and uses it to sign when he so desires. The sender does not
need to rely on the recipient, or receive anything from the recipient,
in order to sign an email. The problem with PKC email signature is
PKI. However, email signature can also be done without PKI, by PGP.
Ed Gerck

@_date: 2004-09-16 12:05:57
@_author: Ed Gerck 
@_subject: public-key: the wrong model for email? 
Voltage actually does. It allows secure communication
without pre-registering the recipient.
Ed Gerck

@_date: 2004-09-16 17:23:01
@_author: Ed Gerck 
@_subject: public-key: the wrong model for email? 
How do you know when the right Bob shows up? And...why encrypt? The
email never left your station. Your method is equivalent to: send
anything to Bob at "unknown-recipient at foocorp.com". When Bob shows
up pray he is the right one and send email over ssl. You also have to
run an ssl server (or trust Bob's server key).
With Voltage, you encrypt the email to "unknown-recipient at foocorp.com"
and send it. The sender's work is done[*]. Yes, the other problems still
exist with Voltage.
Ed Gerck
[*] The recipient can decrypt the Voltage email only IF both the sender
and  recipient can refer to the same key generation parameters for the
recipient. This is a problem that I have not seen Voltage discuss. Users
in different or competing administration boundaries will not be able
to communicate with each other in general.

@_date: 2004-09-17 12:42:29
@_author: Ed Gerck 
@_subject: public-key: the wrong model for email? 
> ...
Good list, even though missing some points that are important here,
mentioned below.
The disclosure threat is that the message may be disclosed AFTER it is
decrypted (this may happen even without the recipient being at fault).
I am NOT talking about the disclosure threat. Except for the disclosure
threat, the threat model includes anything that is not under control
or cannot be directly verified by the sender.
The obvious strategy for the sender is to trust the least possible
(ideally, nothing) regarding message security.
Public-key encryption for email makes this difficult from the start.
With all the public-key fancy foot work (eg, CA certs, CRL, OCSP, etc.),
the sender still has to trust the public-key generated by the recipient
regarding its basic property to encrypt messages that can only be
decrypted by the recipient when the message arrives.
Yes, the sender can do a challenge-response using that key and confirm
that the recipient has the private-key. But what the sender does not
have, and cannot have, is any assurance that his messages are only
decryptable by the recipient. The sender has no way of knowing if the
recipient's public-key is weak (in spite of its great length), or has
some "key-access" feature, or bug, or has been revoked in the mean
time [1]. Trusting the recipient helps but the recipient may not even
know it (in spite of the recipient's best efforts).
This problem also affects SSH and anything that uses public-key crypto,
including smart-card generated keys. For email, however, it can break
message security in spite of the sender's and recipient's best efforts.
Since the sender is the party who bears most, if not all, the risk, my
question was whether email security could benefit by using a different
model. Public-key crypto could still be used, but possibly not as it
is today. Again, the problem is both technical and business-wise.
The attacker could read the first message and download the second message.
It could make it detectable, though (but not necessarily traceable).
Ed Gerck
[1] The security fault happens when you (in spite of all your best efforts) send
an email message using a public-key that is revoked (eg, because the private-key was
compromised) at the time the message is received, due to various delays such as in
message transport, certificate revocation, and compromise discovery. You simply have
no way of knowing, even if the key was not listed in a CRL at the very second you
sent the message, whether the key has not been compromised at the time the message
is received. It gets worse. If the private-key is compromised any time after the
message is sent, the message can be decrypted from a cache copy somewhere -- even
if the recipient keeps the decrypted copy safe.

@_date: 2004-09-18 17:29:19
@_author: Ed Gerck 
@_subject: public-key: the wrong model for email? 
The recipient was not assumed to be entirely incapable of securing his
software. The recipient can be trusted to do a number of things that are
basic, for example, to operate an email software. In fact, we can even
assume a pretty sophisticated recipient, trained to use all the security
email software systems commercially available. Still, the recipient will
be incapable of verifying whether his RSA private-key is weak or not. Thus,
even fully unwillingly and under best efforts, the recipient can put the
sender's message security at risk when using that recipient's RSA public-
The sender's situation can be improved mostly when the sender trusts the
least possible (ideally, nothing) regarding message security. In other
words, the sender would like to avoid anything that is not under control or
cannot be directly verified by the sender. Doing a background check of the
recipient is orthogonal to the PK problem. It does not help nor solve it.
Ed Gerck

@_date: 2004-09-18 18:05:05
@_author: Ed Gerck 
@_subject: public-key: the wrong model for email? 
I agree with you that more checks is usually better. But if you are talking
about someone else verifying the recipient's machine, we're talking about
what seems to me to be a much worse security risk. Who exactly would you
trust to verify your machine and potentially read your decrypted email and
other documents? A "neutral" third-party? Just allowing a third-party to
have access to my machine would go against a number of NDAs and security
policies that I routinely sign. Further, in terms of internal personnel doing
it, we know that 70% of the attacks are internal. The solution to my email
security problem should not be installing a back-door in your machine.
The leakage of a classified document has a number of aspects to consider
in order to prevent it, as we all know. From the sender's viewpoint, however,
what strategy should have the most impact in reducing leakage of a classified
document? It seems clear to me that it is in avoiding anything that is not
under control or cannot be directly verified by the sender. In other words,
it should be more effective to eliminate the sender's reliance on the
recipient's public-key (the sender cannot control or verify whether the key
is weak or not) than do yet another background check of the recipient operation.
Even if the recipient passes today, it may be vulnerable tomorrow. The
sender can't control it.
Cheers--/Ed Gerck

@_date: 2004-09-22 07:55:08
@_author: Ed Gerck 
@_subject: public-key: the wrong model for email? 
Yes. This thread is about the observation that, even if the recipient
manages keys perfectly well, the recipient may not know he is
compromising the sender's security. The sender is in the recipient's
hands with PKC, whereas the sender usually has most (if not all) the
Ed Gerck

@_date: 2005-02-03 11:29:08
@_author: Ed Gerck 
@_subject: Can you help develop crypto anti-spoofing/phishing tool ? 
Yes, because it makes the user notice what CAs the _browser_ has
decided the user _automatically_ accepts [1]. But there is a caveat. Can
you trust what trustbar shows you? And, of course, knowing what CA
is being used is also possible without trustbar but requires a couple
mouseclicks. Wouldn't it be better if Firefox/Mozilla simply
put the name of the CA next to the lock icon?
Ed Gerck
[1] see corresponding flaws noted in

@_date: 2005-02-08 12:45:11
@_author: Ed Gerck 
@_subject: Can you help develop crypto anti-spoofing/phishing tool ? 
In other words, if trustbar can be verified it can be trusted.
Redundancy is useful to qualify trust in information. Trusting the trustbar
code might be hard to qualify by itself (ie, source code verification) but
redundancy helps here [1]. Trust increases if the two channels trustbar and
browser CA status [2] agree with each other. Trustbar can become a trusted
verifier after positively checking with the browser CA status.
This would also help prevent one-sided attacks to trustbar, as one would need
to attack both trustbar and browser CA status,
Ed Gerck
[1] This is also my solution to the famous trust paradox proposed by Ken
Thompson in his " Reflections of Trusting Trust". Trust is earned, not
given. To trust Ken's code, I would first ask two or more programmers (who
I choose) to code the same function and submit their codes to tests. If they
provide the same answers for a series of inputs, including random inputs,
I would have a qualification for trusting (or not) Ken's code. This works
even without source code. Trust is not in the thing, it's how the thing works.
[2] Mozilla already shows the signing CA name when the mouse is over the lock
symbol in SSL. This is more readily visible than clicking with the right-button
and reading the cert.

@_date: 2005-02-08 19:15:52
@_author: Ed Gerck 
@_subject: Can you help develop crypto anti-spoofing/phishing tool ? 
I heartly disagree. If the N-outputs are continuously verified for coherence,
any difference readily stands out. The number N and the cost of always using those
N-outputs should, of course, be outweighed against the cost of failure to
detect an attack. Theoretically, however, there is always a finite number N
that can make the probability of such an attack _ as small as you please_.
The mathematical basis for this result was proven by Shannon more than 50 years
ago; the practical intuition for this result was demonstrated during the Mogul
period in India (more than 500 years ago), who are known to have used at least three
parallel reporting channels to survey their provinces with some degree of reliability,
notwithstanding the additional efforts to do so.
Aren't we talking about different things? A covert channel, looking at
the crypto-chip by itself, is demonstrably impossible to detect with
certainty. However, what I was talking about is NOT this situation.
You are looking at *one* crypto-chip, a single source of information, a single
trusted source, when you have no correction channel available.  I am
looking at N outputs, N sources of information (each one as independent as
possible but not necessarily 100% independent). You have no reference for
detecting a "spike", I have N-1.
Ed Gerck

@_date: 2005-01-10 16:24:45
@_author: Ed Gerck 
@_subject: Entropy and PRNGs 
Let me comment, John, that thermal noise is not random and is
not "real entropy" (btw, is there a "fake entropy" in your
There are several quantities that can be estimated in thermal
noise, reducing its entropy according to what you seem to expect
today. See "photon bunching", as an example that is usually ignored.
Another, even though trivial, example is due to the observation that
thermal noise is not white noise. Yet another observation is that no
noise is really white, because of causality (in other words, it's
duration must be finite). The noise that is due to photon fluctuations
in thermal background radiation, for another example, depends
also on the number of detectors used to measure it, as well as
single- or multiple-mode illumination, and both internal and external
noise sources.
Yes, it's entirely possible that someone in the future will know
more about your entropy source than you do today! Even thermal
OTOH, why are nuclear decay processes considered safe as a source
of entropy? Because the range of energies preclude knowing or
tampering with the internal state. These processes are, however,
not free from correlations either.
Ed Gerck

@_date: 2005-01-14 14:31:40
@_author: Ed Gerck 
@_subject: [Fwd: Call for Papers: Virtual Goods 2005] 
Dear Virtual Goods Community,
here is the link to the cfp:
Please feel free to distrubute it.
Best regards
Here is the text:
                  C A L L   F O R   P A P E R S
               The 3rd International Workshop for
        Technology, Economy, Social and Legal Aspects of
                         Virtual Goods
                         including the new
                       Virtual Goods Tutorial
              Organized by the GI Working Group ECOM
                        and in parallel with
                      IFIP Working Group 6.11
            Communication Systems in Electronic Commerce
                June 2 - 4, 2005, Ilmenau, Germany
                                ---------------------------------
Full version:  Topics of interest include, but are not restricted to, the following

@_date: 2005-03-05 09:32:49
@_author: Ed Gerck 
@_subject: two-factor authentication problems 
Current solutions for two-factor authentication may be weaker than they
seem. Let me present two cases, including SecurID, for comments.
1. First case, without a clock, take a look at:
  Because the algorithm MUST be sequence or counter-based, poor
transmission can cause repeated failures to resynch. Also, someone
could get your token and quickly generate dozens of numbers without
you knowing it -- when you use the token later on, your new number is
not accepted and could fall outside the resynch window (even for two
numbers in sequence).
The worse part, however, is that the server side can always fake your
authentication using a third-party because the server side can
always calculate ahead and generate "your next number" for that
third-party to enter -- the same number that you would get from your
token. So, if someone breaks into your file using "your" number --
who is responsible? The server side can always deny foul play.
2. SecurID:
The last comment above applies. The server side always know what
numbers your token should generate now and in for days in the
future (clock drift included) -- and that's how they are recognized.
So, again, if someone breaks into your file using "your" number --
who is responsible?
Ed Gerck

@_date: 2005-03-07 12:39:57
@_author: Ed Gerck 
@_subject: two-factor authentication problems 
This is a different attack. If you have someone outside auditing, they will
notice what you said but not what I said. A simple log verification will
show the response was NOT good in your case. What I said passes 100% all
auditing -- and the operator does not have to be corrupt.
