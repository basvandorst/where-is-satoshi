
@_date: 2013-08-31 17:00:01
@_author: John Kelsey 
@_subject: [Cryptography] NSA and cryptanalysis 
If I had to bet, I'd bet on bad rngs as the most likely source of a breakthrough in decrypting lots of encrypted traffic from different sources.

@_date: 2013-08-31 17:53:42
@_author: John Kelsey 
@_subject: [Cryptography] Functional specification for email client? 
I think it makes sense to separate out the user-level view of what happens (the first five or six points) from how it's implemented (the last few points, and any other implementation discussions).  In order for security to be usable, the user needs to know what he is being promised by the security mechanisms--not which digital signature scheme is being used or whether decoy messages are sent to frustrate traffic analysis.  If something arrives in my inbox with a from address of nobody at nowhere.com, then I need to know that this means that's who it came from.  If I mail something to nobody at nowhere.com, then I need to know that only the owner of that address will be able to read it.  I need to know that nobody should be able to know with whom I'm communicating.  But I don't need to know about keys, digital signatures, mix nets, etc.  That's what I want to know as a cryptographer, but as a user, I just want to know what the security system is promising and how reliable its promises are. My intuition is that binding the security promises to email addresses instead of identities is the right way to proceed.  I think this is something most people can understand, and more importantly it's something we can do with existing technology and no One True Name Authority In The Sky handing out certs.  One side issue here is that this system's email address space needs to somehow coexist with the big wide internet's address space.  It will really suck if someone else can get my gmail address n the secure system, but it will also be confusing if my inbox has a random assortment of secure and insecure emails, and I have to do some extra step to know which is which.

@_date: 2013-12-13 13:24:21
@_author: John Kelsey 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
Why not just XOR RD_RAND outputs with Yarrow outputs?  That guarantees strong results if either one is good.

@_date: 2013-12-16 12:35:37
@_author: John Kelsey 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
You are assuming a way, way more complicated and specialized bit of malevolent engineering in the RNG chip, at that point--one that only works on one OS RNG, and one that probably breaks every time there's an OS upgrade that touches the RNG.  Also, I have to guess that the CPU designer could find hundreds of easier ways to screw over my security.  What OS-based RNG could withstand having the CPU it's running on designed to defeat its security?

@_date: 2013-12-16 12:52:15
@_author: John Kelsey 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
It's interesting to ask where the biggest risks are here.  My intuition is that the OS RNGs are often not all that secure, especially soon after boot.  So my first recommendation would be to draw 256 or so bits from RD_RAND and feed them into the OS RNG, with zero assessed entropy.  That gets the OS RNG to a strong state right away if RD_RAND isn't weak.  (Even some ways it could be weak would be masked by using it this way.)  That all by itself would get you to a secure system if the Intel RNG is good.  You can then XOR the RD_RAND outputs into the RNG outputs to get (if the Intel RNG is as good as it claims) something close to full entropy outputs.  So long as the RD_RAND outputs aren't somehow predicting the OS RNG outputs, this can only improve the security of the OS RNG outputs.  There are obviously many other perfectly fine ways to use them together--for example, initializing a good DRBG with the OS's entropy, and then reseeding it before every output with bits from RD_RAND.  Or initializing a DRBG with RD_RAND and sending in 256 bits of additional input from the OS RNG after it starts working.  My sense is that the Intel RNG is almost certainly a huge improvement in security.  It's certainly smart to think about how to use it that doesn't fall apart if the RNG is broken somehow, but it's also smart to realize that what we have now apparently isn't all that great!

@_date: 2013-12-16 13:09:04
@_author: John Kelsey 
@_subject: [Cryptography] Kindle as crypto hardware 
The war on terror has always relied, for its popularity, on the idea that it's not being done to *us* but rather to *them*.  Spying on, disappearing, infiltrating, torturing, or killing scary Muslim foreigners (even if they're US citizens on US soil, like Jose Padilla) is something that can be sold to the American people because they don't think it will ever be done to them or people they care about.  And we've had awful stories about nasty things we've been doing in the war on terror for years now.  But since those were mostly being done to Muslims, and both parties were reassuring us that it was all done for our safety, there wasn't much outcry.  But handing over the spying data to the DEA and IRS, it's kinda hard to spin that as only targeted at other people.

@_date: 2013-12-30 16:26:19
@_author: John Kelsey 
@_subject: [Cryptography] TAO, NSA crypto backdoor program 
It's not "have you no decency," it's "have you no limits?"  The answer is apparently no.  The Snowden dlsclosures and related disclosures have shown NSA attacking:
a.  US government standards
b.  US companies' networks
c.  US companies' products
d.  US citizens' computers and private communications
d.  Allied governments
e.  Foreign companies' networks
f.   Foreign companies' products
g.  Foreigners communicating inside their own countries
h.  The UN and other NGOs
etc.  The justification for this was terrorism, but apparently there's not much evidence that it stopped any terrorism (kinda like the TSA), and it looks like it's been used for all kinds of other stuff--fighting the drug war, tracking down whistleblowers, spying on journalists, economic espionage, spying on negotiators before hammering out treaties on stuff like intellectual property, etc.  At some point, probably ultimately originating in 9/11, they seem to have gotten the message that there *were* no limits on what they were permitted to do--at the very least, that message seems to have gotten to NSA and CIA.  We have seen open violation of the written laws in domestic spying scandals and torture scandals, and nobody faced any legal consequences but the whistleblowers.  The first response to this needs to be to send the message to them that there are limits, that rules and laws apply to them.  That message needs to have teeth--subpoenas, drastic budget cuts, congressional hearings, the whole bit.  That isn't going to happen anytime soon--the leadership of the two big parties has zero interest in reiniing this stuff in.  Whether that's because of their genuine belief in the need for unlimited power for the spy agencies, or the *really detailed* files the spy agencies have on key members of both parties, I don't know.  But every time the spy agencies show that they can get away with *anything* and nobody faces any consequences, it becomes more plausible that there's something more than good salesmanship by the intelligence agencies going on.  The only way I see this happening is for there to be a popular movement against unlimited unaccountable intelligence agencies doing whatever they please in the US.  I would love to see this happen.  I'm kind-of worried that the way our media works, any such movement will be black-holed or marginalized or channeled into more acceptable-to-the-powerful issues.  And that only deals with the first step that's needed.  The US is certainly not the only government doing this crap.  Figuring out how to resist nation-state level attackers will be hard even if we can ever get our own government not to be among the attackers.

@_date: 2013-12-30 16:45:42
@_author: John Kelsey 
@_subject: [Cryptography] On Security Architecture, The Panopticon, 
Distinguish the different uses of bitcoins.  For buying things online, yeah, there's a demand for bitcoins based on wanting to be able to do online transactions.  To the extent there are transactions that lots of people prefer to do in bitcoins (whether legal or illegal), there will be a certain amount of demand for bitcoins based simply on how many bitcoins are either involved in a transaction right now, or are being held ready to be used for a transaction soon.  But bitcoins are online, and their volatility makes them an awful store of value.  (They can be a speculative investment, but they aren't where you'd want to park your retirement fund on your 60th birthday.) They're too volatile to be a good unit to price things in, too.  If bitcoins become a very important way to conduct business online, and they remain as volatile as they are now, I expect people will buy bitcoins only when they need to do an online transaction.  Their actual wealth will be kept in dollars or euros or something.  The inherent traceabiliity of Bitcoin probably creates more demand for holding bitcoins, as well, oddly enough. (You need to go through a laundry or something to get any decent anonymity, which means more latency in your purchases, which means more demand for bitcoins.)  I am pretty skeptical that much of the economic theory around monetary inflation or deflation applies very cleanly to bitcoins, given that it's all online, with very low transaction costs, and is likely used only to do specific transactions.

@_date: 2013-11-04 12:39:16
@_author: John Kelsey 
@_subject: [Cryptography] /dev/random is not robust 
...           [I hope I got the attribution right]
Yep, this is an unfixable problem. Suppose you have a program to get a keypair, either by generating it or receiving it over the net.  But your program has no entropy.  Another way of saying that is that I (the attacker) can write a program that can do anything your program can do, because I have access to the same information as your program has.  If your program generates a keypair, mine will generate the same one.
If your program makes an encrypted channel to get the keypair from some trusted server, my program will know the encryption key and can decrypt it.  My program can play man in the middle, too, because your program can't do anything my program can't also do.  Without entropy--enough entropy to make me do an impossible amount of work getting my program to run like yours--there's just no way to get your program to generate or retrieve a keypair my program can't also generate or retrieve.
Now, if there is local traffic I can't intercept, your program can feed that into its RNG.  If I also can't guess that local traffic, then your program has enough entropy to generate a keypair. Yep.  It seems like getting random secure starting seeds into devices would be a huge win here.  Then they can combine that with whatever information they have locally, and initialize their RNG, and then generate their keypair.

@_date: 2013-11-04 12:44:17
@_author: John Kelsey 
@_subject: [Cryptography] HTTP should be deprecated. 
Amen!  The default for anything going over a communications network should be encrypted and autheticated.  In the rare cases where that isn't appropriate for some reason, that should be the thing that requires justification.  Instead, the opposite seems to hold--the default is unencrypted and unauthenticated, and anyone who wants to add crypto has to show why it's necessary.  The sticking point here is key management, which is a big potential administrative pain in the ass.    But it's worth wondering if we could at least get widespread use of Diffie-Hellman + GCM as a default.  There is no key management there, and no defense against MIM attacks, but at least everything doesn't go out in the clear.  But at this point, most websites don't even support an https request.

@_date: 2013-11-05 17:31:12
@_author: John Kelsey 
@_subject: [Cryptography] randomness +- entropy 
Is there any way for a program to find out if /dev/urandom has been seeded properly?  It seems like the alternative for a developer is either hope /dev/urandom has gotten to a secure point before he reads his PRNG seed from it, or get his PRNG seed from /dev/urandom and potentially block, and also potentially make other stuff block.  But there isn't really any reason for that, right? If I want to initialize a cryptographic PRNG, or generate a RSA key, or whatever, I am shooting for computational security, which /dev/urandom should give me *once it has reached a secure state*.  I don't need full-entropy bits--I'm not generating a one-time pad or something.  I just need something that is impossible to guess without more computing power than my attacker has.

@_date: 2013-11-05 17:46:36
@_author: John Kelsey 
@_subject: [Cryptography] /dev/random is not robust 
Okay, but if you don't have some starting value that I don't know and can't guess, you can't establish a cryptographically secure connection with anyone to get them to send you random bits.      How do you establish a key I don't know with your randomness-providing TTP?  If you have a single secret value I don't know and can't guess, you can use this as a PRNG seed, and as long as I don't compromise your state somehow, you can keep generating outputs that I can't distinguish from random for as long as you like.  If you share a single secret value I don't know with some TTP, you can use this secret as an encryption and authentication key, and get the TTP to send you some randomness.  Then, if I observed the ciphertext from the TTP to you, your PRNG's security is exactly the same as if you just used that starting value as a PRNG seed.  If I didn't observe the message from the TTP down to you, then no secure connection was needed.  The TTP could send you random bits in the clear, and that would be fine, because I wouldn't know them.

@_date: 2013-11-06 18:40:18
@_author: John Kelsey 
@_subject: [Cryptography] randomness +- entropy 
If the distribution can ship with a unique secret seed value, then that resolves the uninitialized rng problem against any attacker who doesn't know that seed value.
To update the seed, I think it's sufficient to initialize /dev/urandom from the seed file and write the first 256 bits of output back to the seed file before any outputs are generated for anything else.  That guarantees that /dev/urandom never gets seeded the same way twice.  If possible it would also be nice to have some process wait for the /dev/urandom ready flag to be set (assuming one is added), and then get another 256 bits from /dev/urandom and write those to the seed file.  That ensures that the seed file eventually can become unpredictable even to someone who knows the starting value of the seed file.

@_date: 2013-11-06 19:16:30
@_author: John Kelsey 
@_subject: [Cryptography] randomness +- entropy 
It seems like this would allow stuff like OpenSSL to do the right thing (initialize from /dev/urandom, but only once it has reached a secure state) with no more performance impact than necessary.  Ideally, /dev/urandom would accumulate entropy till it had a lot and then catastrophically reseed and set its "ready" flag.  And then any crypto application could check the flag, and read its PRNG seed or starting value for its prime number search from /dev/urandom only when the flag was set.
It seems like the best way for things to work would be that /dev/urandom always gave cryptographically strong random numbers.  But if that isn't always going to be the case, then application programs that really need that should be able to check to see if they can safely draw a PRNG seed out of /dev/urandom yet, or if they need to wait or ask the user to do something.  The alternative is silent failures that lead to low-entropy keys and breakable systems.
I don't have any intuition for how much work this is, but it seems pretty critical.  Right now, if someone is generating a cryptographic key on a Linux system, there seems to be no way for them to generate that key from /dev/urandom safely, because they can't really know if /dev/urandom will be in a secure state when they need to generate their key.  I guess the right guidance to give them now is "generate your key from /dev/random."

@_date: 2013-11-11 08:47:33
@_author: John Kelsey 
@_subject: [Cryptography] suggestions for very very early initialization 
Yes, you're right.  I was thinking of each instance here, but for some reason wrote about distributions instead.  If we update the stored seed before we generate outputs for anything else, then repeating RNG outputs won't happen.  There's a nicer way to think of this, though.  Consider the persistent state as the state of a PRNG.  That PRNG is used *only* to provide inputs to other PRNGs.  Use CTR-DRBG or HMAC-DRBG from 90A and you have processing of additional inputs built in.  Then, each time you want to get some bits to add to the system PRNG, you make a Generate call to the persistent-state PRNG, and that updates the persistent state in a way that doesn't allow backtracking or repeated outputs.  Then, use the persistent PRNG's output as additional input for the system PRNG.   I hadn't really thought about how the seedfile or persistent PRNG would affect the /dev/urandom ready flag.  I'm still not sure what the right answer to that is.  On one hand, assuming the starting value of the seed file is unknown to the attacker, /dev/urandom outputs will be secure.  On the other hand, if the starting value of the seed file is known to the attacker (via insider attack, incompetence on the part of the manufacturer, or national security letter), we gain no security from it.  It seems like this works better as a safety net.   If we're talking about /dev/urandom, it should be getting new entropy from the OS over time.  I'm not a great fan of the /dev/random vs /dev/urandom model, to be honest.  I'd prefer to see everything go through a PRNG--perhaps CTR-DRBG with AES256, or HMAC-DRBG with SHA512.  The OS gets entropy from time to time, and queues it up, and eventually catastrophically reseeds the PRNG.  If there's a high speed hardware entropy source, then that can be done very quickly--perhaps even fast enough to provide full entropy outputs in principle.
A PRNG needs an entropy source, but that doesn't have to be a hardware RNG.  It doesn't even have to be on-board.  A smartcard with some persistent storage that can be updated over time can be initialized from a good entropy source once, and then keep running and generating pseudorandom outputs for a very long time.

@_date: 2013-11-11 08:57:37
@_author: John Kelsey 
@_subject: [Cryptography] NIST Randomness Beacon 
One beacon is not worth all that much, because of the trust issues.  (Along with not trusting NIST, you may also not perfectly trust everyone involved in the project, everyone who might have access to the machine running the beacon, or the security of the beacon itself.)  Many beacons run by mutually mistrustful entities under separate political regimes and ownership, with separate codebases and hardware, but using the same message format, could be worth a lot more.  Similarly, including the beacon in a protocol where each of us is exchanging random numbers to contribute to a seed is nice because it can be verified by people in the future, who aren't yet participants in the transaction.  Suppose we decide to build a combined seed to decide which ballot boxes to audit.  We do:
Alice the Democrat generates R[a] and sends hash(R[a])
Bob the Republican generates R[b] and sends hash(R[b])
They both reveal their random numbers and compute the agreed-upon seed S = hash(R[a] || R[b])
Now, Carol the Green, who wasn't a party to this whole thing, has to trust that Alice and Bob didn't collaborate.  Adding in a beacon input addresses this somewhat.
Alice and Bob do the same thing as above, but also include a commitment that they will take the NIST beacon value as of tomorrow at noon Eastern time and mix it in.  At noon tomorrow, they get R[beacon], reveal their random numbers, and derive
S = hash(R[a] || R[b] || R[beacon])
If Carol thinks Alice and Bob are collaborating with the beacon, this doesn't help.  But if she thinks it's easier for the local Republican and Democratic bosses to cooperate than for them to subvert the beacon, then this adds some assurance for Carol, as she audits the generation of this shared seed as an outsider.

@_date: 2013-11-11 13:58:49
@_author: John Kelsey 
@_subject: [Cryptography] SP800-90A B & C 
There are two separate issues here:
a.  Allowing additional input that's not credited with entropy, but which may add security.  b.  Allowing the combination of two or more approved, validated entropy sources.
I'm still not sure where we run into problems with (a) (there's some 140-2 guidance that requires callers of RNGs to be authenticated at higher validation levels--that may cause problems), and at least so far I don't have an actual example of a FIPS lab refusing to allow a 90A DRBG to use additional input from an off-module unauthenticated source, (if you have one, please let me know) but I think this is something we can address in guidance on 90A.  Dealing with (b) is going to have to wait for 90C to be finished.  It's relatively easy to allow this for entropy sources that live within some kind of separate boundaries, but not for entropy sources that have access to the same physical processes or internal state.  But combining independent entropy sources is something that should make it into 90C.  As an aside, most of the content of 90A, B, and C *did* go through a normal standardization process in X9F1.  And since then, we've had a public workshop and a couple rounds of public comment, trying to hammer out things that might cause problems.  So I'm not sure if this is a normal standards process, but it sure is allowing for a fair bit of public comment.

@_date: 2013-11-11 14:07:22
@_author: John Kelsey 
@_subject: [Cryptography] randomness +- entropy 
If we're talking about a PRNG (which /dev/urandom is), then there are really two cases of interest:
a.  The PRNG has accumulated too little entropy[1] to be in a secure state.  b.  The PRNG has accumulated enough entropy to be in a secure state--say 128 or more bits.
In case (b), if the PRNG is secure, there can be no harm in anyone seeing lots of outputs from it. Initializing your PRNG with 200 bits of entropy and then outputting a million bits leaves you perfectly fine in security terms.  In case (a), you have a big problem.  If your PRNG has accumulated 37 bits of entropy and you generate an output, you've lost all 37 bits of entropy, because I can guess the PRNG's state, and if my guess is right, I will be able to predict the outputs correctly.  This sets up the situation where you do something like
Feed in 50 bits of entropy
Generate an output
Feed in another 50 bits of entropy
Generate another output
Feed in another 50 bits of entropy
Generate another output
And you never get to a secure state, even though you've fed in 150 bits of entropy.  This is why Yarrow does catastrophic reseeding.  [1] I use "entropy" here in the sense of information not known to any attacker, not in the sense of fundamentally unknowable information like how many nuclei decayed in a given period of time.  Also, if you're computing the entropy, the right measure to use is min-entropy, not Shannon entropy.  That's -lg( P[max] ) where P[max] is the maximum probability of any possible input to the PRNG.

@_date: 2013-11-11 14:38:52
@_author: John Kelsey 
@_subject: [Cryptography] randomness +- entropy 
I can't think of many times when it's really appropriate to demand full entropy, rather than cryptographically secure bits.  It seems like having /dev/urandom be capable of *either* generating cryptographically secure bits *or* generating predictable bits forces application developers into either using /dev/random or crossing their fingers and using /dev/urandom.  And if lots of people are being security-conscious, they all have to use /dev/random, and it will block for a really long time.  If you imagine a choice between:
a.  A secure cryptographic PRNG (say CTR_DRBG using AES256) which is catastrophically reseeded whenever it's convenient and the entropy pool is assessed at more than 256 bits.  (The pool is then reset to 0.)
b.  /dev/urandom and /dev/random as they are now.
I'm not clear on what situations there are where (b) provides better practical security than (a).  Who will make use of this model?  I mean, it looks like a lot of real-world crypto developers now are using /dev/urandom (with crossing of fingers and rubbing of rabbits' feet) to generate their keys, rather than wait for /dev/random.  It's hard to imagine those guys trying to make use of a complicated multidimensional model of whom they want to trust and how much.  In practice, they're not even paying attention to your internal entropy estimates.  This problem must come up with every entropy source, right?  If I can suspect the hardware RNG is hacked, I can also suspect the kernel or BIOS or drive firmware on which you're relying for entropy from drive access timings is hacked.
The nice thing about RDRAND is that if the hardware RNG isn't cooked, it can be used to initialize /dev/urandom and /dev/random to secure states before anyone needs any outputs from them.  (The only reason to suspect it could be cooked is, basically, because it would be really convenient for the bad guys if it were cooked.)  Since there's widely-used crypto code generating keys from /dev/urandom, closing the hole where /dev/urandom might not be securely initialized yet when the keys are generated seems like it's a couple orders of magnitude more important than addressing the possibility that RDRAND might also be cooked.  One thing to consider: RDRAND is probably fast enough that you could just request a new 128 bits of output and stir it into /dev/urandom or /dev/random whenever an output is requested.  So in principle, you could not account for RDRAND as having any entropy at all, but use it in this way, and the system would work just like it did without RDRAND, but with the difference that you would never again have a situation where /dev/urandom had 20 bits of entropy and was used to generate an RSA keypair.  This would be annoying in the sense that /dev/random wouldn't get the speedup it should from having a fast hardware RNG on board, but it would represent a huge improvement in practical security.

@_date: 2013-11-11 14:49:44
@_author: John Kelsey 
@_subject: [Cryptography] HTTP should be deprecated. 
There are a lot of examples of public data where it's interesting to someone that you are looking it up.  There might be people who would like to know that you are really interested in public articles on staging of breast cancer, or protease inhibitors, or gender reassignment surgery.  Some of those people might not have your best interests at heart.  Anyway, encryption is just not that expensive, and we are clearly in an environment where lots of spying is going on.  My feeling is that the default for communications going over a network should be encrypted and authenticated, and *not* encrypting/authenticating it should require a justification.  That's the opposite of today, where the default is unprotected, and only when a case can be made for the data needing protection is there any thought that we might want to encrypt and authenticate it.

@_date: 2013-11-13 19:05:59
@_author: John Kelsey 
@_subject: [Cryptography] Fwd: Moving forward on improving HTTP's security 
So your solution is what?  Continue sending data in the clear?  Why not push to get TLS used everywhere, and also push for certificate transparency and EA certs to make it harder to do CA attacks?  Right now, the default is to send data out unencrypted over a network that is apparently being heavily spied on.  Turning on crypto by default isn't a perfect answer, but I think it's the best one we can reach quickly.

@_date: 2013-11-15 08:43:50
@_author: John Kelsey 
@_subject: [Cryptography] Moving forward on improving HTTP's security 
CAs can participate in MITM attacks, but there are additional measures that can make that behavior very likely to get caught.  And right now, most traffic doesn't even need a MITM attack, just eavesdropping to listen in on the unencrypted traffic. It seems to me that anything that gives us easy opportunistic encryption is about as vulnerable to MITM attacks as TLS with possibly-compromised CAs.

@_date: 2013-11-18 18:02:29
@_author: John Kelsey 
@_subject: [Cryptography] Moving forward on improving HTTP's security 
It seems like the clever bit of CT is the insight that some actions, like a CA signing a cert, are intended to be public, and so should be forced (via clever crypto) to take place in public.  This makes me wonder what other crypto actions should also take place in public, in a way that doesn't permit hiding them from the world.

@_date: 2013-11-23 10:14:30
@_author: John Kelsey 
@_subject: [Cryptography] Moving forward on improving HTTP's security 
NSA is a good model for the attacker, but there are a lot of attackers that aren't NSA, ranging from nosy neighbors to local cops to criminals to foreign governments to big companies and their ethics-free contractors.  Moving to TLS everywhere will make eavesdropping harder across the board, and will be more effective the more we apply additional defenses against mitm attacks.

@_date: 2013-11-26 20:46:54
@_author: John Kelsey 
@_subject: [Cryptography] Explaining PK to grandma 
All the user of the email system needs to know is that:
a.  Email to a given address can only be read by the owner of the address.
b.  Nobody can tell how much email you send or receive, or to/from whom.  It's as silly to expect most users to understand the crypto underlying this as it is to expect them to understand the financial instruments and markets that make their credit card accounts possible, or the power distribution network that keeps their lights on, or the mechanism of action of the medicine their doctor gave them for high blood pressure.  Most people find that stuff boring, many aren't bright enough to understand it very well, and nobody, no matter how bright and interested, has the time and energy to understand more than a tiny fraction of the stuff they use every day in any depth.

@_date: 2013-11-27 19:24:26
@_author: John Kelsey 
@_subject: [Cryptography] Email is unsecurable 
Are hardware trojans really a bigger practical threat than compromised software?

@_date: 2013-10-01 12:47:56
@_author: John Kelsey 
@_subject: [Cryptography] NIST about to weaken SHA3? 
I like the general idea here, but I suspect a vote at the end of a conference isn't going to yield great results.  I'd hate to see something the designers opposed get adopted because they were outvoted by (say) a larger team.
The Keccak designers proposed reducing the capacity.  You can find public statements about this online, including in the slides on their website.  Also, the capacity is a parameter defined in the standard to allow an easy to understand performance/security tradeoff.  Setting c=256 gives an across the board security level of 128 bits, if you believe the underlying Keccak permutation is good.  The actual technical question is whether an across the board 128 bit security level is sufficient for a hash function with a 256 bit output.  This weakens the proposed SHA3-256 relative to SHA256 in preimage resistance, where SHA256 is expected to provide 256 bits of preimage resistance.  If you think that 256 bit hash functions (which are normally used to achieve a 128 bit security level) should guarantee 256 bits of preimage resistance, then you should oppose the plan to reduce the capacity to 256 bits.  If you think a 256 bit hash function should only promise 128 bits of security, except in specific applicaitons like keyed hashes where it has been analyzed specifically and shown to get more, then you should (at least on technical grounds) like the proposal to reduce the capacity to 256 bits for a 256-bit hash output.

@_date: 2013-10-02 08:58:46
@_author: John Kelsey 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
What on Earth are you talking about?  AES' key schedule wasn't designed by NIST.  The only change NIST made to Rijndael was not including some of the alternative block sizes.  You can go look up the old Rijndael specs online if you want to verify this.

@_date: 2013-10-02 10:11:06
@_author: John Kelsey 
@_subject: [Cryptography] are ECDSA curves provably not cooked? (Re: RSA 
[Discussing how NSA might have generated weak curves via trying many choices till they hit a weak-curve class that only they knew how to solve.]
This general idea is a nice one.  It's basically a way of using Merkle's puzzles to build a private key into a cryptosystem.  But I think in general, you are going to have to do work equal to the security level of the thing you're trying to backdoor.  You have to break it once at its full security level, and then you get to amortize that break forever.  (Isn't there something like this you can do for discrete logs in general, though?)  Consider Dual EC DRBG.  You need a P, Q such that you know x that solves xP = Q, over (say) P-224.  So, you arbitrarily choose G = a generator for the group, and a scalar z, and then compute for
 j = 1 to 2^{112}:
Now, you have 2^{112} values in a group of 2^{224} values, right?  So with about another 2^{113} work, you can hit one of those with two arbitrary seeds, and you'll know the relationship between them.  But this takes a total of about 2^{113} work, so it's above the claimed secuity level of P-224.  I suspect this would be more useful for something at the 80 bit security level--a really resourceful attacker could probably do a 2^{80} search.

@_date: 2013-10-02 10:46:22
@_author: John Kelsey 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
Has anyone tried to systematically look at what has led to previous crypto failures?  That would inform us about where we need to be adding armor plate.  My impression (this may be the availability heuristic at work) is that:
a.  Most attacks come from protocol or mode failures, not so much crypto primitive failures.  That is, there's a reaction attack on the way CBC encryption and message padding play with your application, and it doesn't matter whether you're using AES or FEAL-8 for your block cipher.  b.  Overemphasis on performance (because it's measurable and security usually isn't) plays really badly with having stuff be impossible to get out of the field when it's in use.  Think of RC4 and DES and MD5 as examples.  c.  The ways I can see to avoid problems with crypto primitives are:
(1)  Overdesign against cryptanalysis (have lots of rounds)
(2)  Overdesign in security parameters (support only high security levels, use bigger than required RSA keys, etc.) (3)  Don't accept anything without a proof reducing the security of the whole thing down to something overdesigned in the sense of (1) or (2).

@_date: 2013-10-02 10:59:24
@_author: John Kelsey 
@_subject: [Cryptography] RSA equivalent key length/strength 
I don't know enough about elliptic curves to have an intelligent opinion on whether this is possible.  Has anyone worked out a way to do this?  The big question is how much work would have had to be done.  If you're talking about a birthday collision on the curve parameters, is that a collision on a 160 bit value, or on a 224 or 256 or 384 or 512 bit value?  I can believe NSA doing a 2^{80} search 15 years ago, but I think it would have had to be a top priority.  There is no way they were doing 2^{112} searches 15 years ago, as far as I can see.

@_date: 2013-10-04 10:12:17
@_author: John Kelsey 
@_subject: [Cryptography] Sha3 and selecting algorithms for speed 
Most applications of crypto shouldn't care much about performance of the symmetric crypto, as that's never the thing that matters for slowing things down.  But performance continues to matter in competitions and algorithm selection for at least three reasons:
a.  We can measure performance, whereas security is very hard to measure.  There are a gazillion ways to measure performance, but each one gives you an actual set of numbers.  Deciding whether JH or Grostl is more likely to fall to cryptanalytic attack in its lifetime is an exercise in reading lots of papers, extrapolating, and reading tea leaves.    b.  There are low-end environments where performance really does matter.  Those often have rather different properties than other environments--for example, RAM or ROM (for program code and S-boxes) may be at a premium.  c.  There are environments where someone is doing a whole lot of symmetric crypto at once--managing the crypto for lots of different connections, say.  In that case, your symmetric algorithm's speed may also have a practical impact.  (Though it's still likely to be swamped by your public key algorithms.)

@_date: 2013-10-04 11:26:39
@_author: John Kelsey 
@_subject: [Cryptography] Performance vs security 
There are specific algorithms where you have a pretty clear-cut security/performance tradeoff.  RSA and ECC both give you some choice of security level that has a big impact in terms of performance.  AES and SHA2 and eventually SHA3 offer you some secuirty level choices, but the difference in performance between them is relatively unimportant in most applications.  Probably the coolest thing about Keccak's capacity parameter is that it gives you an understandable performance/security tradeoff, but the difference in performance between c=256 and c=512 will probably not be noticable in 99% of applications.  Then there are algorithms that give you higher performance at the cost of more fragility.  The example I can think of here is GCM, which gives you a pretty fast authenticated encryption mode, but which really loses security in a hurry if you reuse an IV.
It seems like these two kinds of security/performance tradeoffs belong in different categories, somehow.

@_date: 2013-10-04 10:23:38
@_author: John Kelsey 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
Just a couple nitpicks: a.  Dobbertin wasn't doing a birthday (brute force collision) attack, but rather a collision attack from a chosen IV.  b.  Preimages with MD5 still are not practical.  What is practical is using the very efficient modern collision attacks to do a kind of herding attack, where you commit to one hash and later get some choice about which message gives that hash.  There are certainly papers whose only practical importance is getting a smart cryptographer tenure somewhere, and many of those involve proofs.  But there's also a lot of value in being able to look at a moderately complicated thing, like a hash function construction or a block cipher chaining mode, and show that the only way anything can go wrong with that construction is if some underlying cryptographic object has a flaw.  Smart people have proposed chaining modes that could be broken even when used with a strong block cipher.  You can hope that security proofs will keep us from doing that.  Now, sometimes the proofs are wrong, and almost always, they involve a lot of simplification of reality (like most proofs aren't going to take low-entropy RNG outputs into account).  But they still seem pretty valuable to me for real-world things.  Among other things, they give you a completely different way of looking at the security of a real-world thing, with different people looking over the proof and trying to attack things.

@_date: 2013-10-05 21:29:05
@_author: John Kelsey 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
One thing that seems clear to me:  When you talk about algorithm flexibility in a protocol or product, most people think you are talking about the ability to add algorithms.  Really, you are talking more about the ability to *remove* algorithms.  We still have stuff using MD5 and RC4 (and we'll probably have stuff using dual ec drbg years from now) because while our standards have lots of options and it's usually easy to add new ones, it's very hard to take any away.

@_date: 2013-10-06 11:26:11
@_author: John Kelsey 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
If we can't select ciphersuites that we are sure we will always be comfortable with (for at least some forseeable lifetime) then we urgently need the ability to *stop* using them at some point.  The examples of MD5 and RC4 make that pretty clear.  Ceasing to use one particular encryption algorithm in something like SSL/TLS should be the easiest case--we don't have to worry about old signatures/certificates using the outdated algorithm or anything.  And yet we can't reliably do even that.

@_date: 2013-10-06 23:41:20
@_author: John Kelsey 
@_subject: [Cryptography] Sha3 
They are talking about the change to their padding scheme, in which between 2 and 4 bits of extra padding are added to the padding scheme that was originally proposed for SHA3.  A hash function that works by processing r bits at a time till the whole message is processed (every hash function I can think of works like this) has to have a padding scheme, so that when someone tries to hash some message that's not a multiple of r bits long, the message gets padded out to r bits.  The only security relevance of the padding scheme is that it has to be invertible--given the padded string, there must always be exactly one input string that could have led to that padded string.  If it isn't invertible, then the padding scheme would introduce collisions.  For example, if your padding scheme was "append zeros until you get the message out to a multiple of r bits," I could get collisions on your hash function by taking some message that was not a multple of r bits, and appending one or more zeros to it.  Just appending a single one bit, followed by as many zeros as are needed to get to a multiple of r bits makes a fine padding scheme, so long as the one bit is appended to *every* message, even those which start out a multiple of r bits long.  The Keccak team proposed adding a few extra bits to their padding, to add support for tree hashing and to distinguish different fixed-length hash functions that used the same capacity internally.  They really just need to argue that they haven't somehow broken the padding so that it is no longer invertible
They're making this argument by pointing out that you could simply stick the fixed extra padding bits on the end of a message you processed with the original Keccak spec, and you would get the same result as what they are doing.  So if there is any problem introduced by sticking those extra bits at the end of the message before doing the old padding scheme, an attacker could have caused that same problem on the original Keccak by just sticking those extra bits on the end of messages before processing them with Keccak.

@_date: 2013-10-07 12:03:20
@_author: John Kelsey 
@_subject: [Cryptography] Iran and murder 
Alongside Phillip's comments, I'll just point out that assassination of key people is a tactic that the US and Israel probably don't have any particular advantages in.  It isn't in our interests to encourage a worldwide tacit acceptance of that stuff.  I suspect a lot of the broad principles we have been pushing (assassinations and drone bombings can be done anywhere, cyber attacks against foreign countries are okay when you're not at war, spying on everyone everywhere is perfectly acceptable policy) are in the short-term interests of various powerful people and factions in the US, but are absolutely horrible ideas when you consider the long-term interests of the US.  We are a big, rich, relatively free country with lots of government scientists and engineers (especially when you consider funding) and tons of our economy and our society moving online.  We are more vulnerable to widespread acceptance of these bad principles than almost anyone, ultimately,  But doing all these things has won larger budgets and temporary successes for specific people and agencies today, whereas the costs of all this will land on us all in the future.

@_date: 2013-10-09 22:18:43
@_author: John Kelsey 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
We know how to address one part of this problem--choose only algorithms whose design strength is large enough that there's not some relatively close by time when the algorithms will need to be swapped out.  That's not all that big a problem now--if you use, say, AES256 and SHA512 and ECC over P521, then even in the far future, your users need only fear cryptanalysis, not Moore's Law.  Really, even with 128-bit security level primitives, it will be a very long time until the brute-force attacks are a concern.  This is actually one thing we're kind-of on the road to doing right in standards now--we're moving away from barely-strong-enough crypto and toward crypto that's going to be strong for a long time to come. Protocol attacks are harder, because while we can choose a key length, modulus size, or sponge capacity to support a known security level, it's not so easy to make sure that a protocol doesn't have some kind of attack in it.  I think we've learned a lot about what can go wrong with protocols, and we can design them to be more ironclad than in the past, but we still can't guarantee we won't need to upgrade.  But I think this is an area that would be interesting to explore--what would need to happen in order to get more ironclad protocols?  A couple random thoughts:
a.  Layering secure protocols on top of one another might provide some redundancy, so that a flaw in one didn't undermine the security of the whole system.  b.  There are some principles we can apply that will make protocols harder to attack, like encrypt-then-MAC (to eliminate reaction attacks), nothing is allowed to need change its execution path or timing based on the key or plaintext, every message includes a sequence number and the hash of the previous message, etc.  This won't eliminate protocol attacks, but will make them less common.
c.  We could try to treat at least some kinds of protocols more like crypto algorithms, and expect to have them widely vetted before use.  What else?   ...
What we really need is some way to enforce protocol upgrades over time.  Ideally, there would be some notion that if you support version X of the protocol, this meant that you would not support any version lower than, say, X-2.  But I'm not sure how practical that is.

@_date: 2013-10-10 11:51:04
@_author: John Kelsey 
@_subject: [Cryptography] Iran and murder 
The problem with offensive cyberwarfare is that, given the imbalance between attackers and defenders and the expanding use of computer controls in all sorts of systems, a cyber war between two advanced countries will not decide anything militarily, but will leave both combattants much poorer than they were previously, cause some death and a lot of hardship and bitterness, and leave the actual hot war to be fought. Imagine a conflict that starts with both countries wrecking a lot of each others' infrastructure--causing refineries to burn, factories to wreck expensive equipment, nuclear plants to melt down, etc.  A week later, that phase of the war is over.  Both countries are, at that point, probalby 10-20% poorer than they were a week earlier.  Both countries have lots of really bitter people out for blood, because someone they care about was killed or their job's gone and their house burned down or whatever.  But probably there's been little actual degradation of their standard war-fighting ability.  Their civilian aviation system may be shut down, some planes may even have been crashed, but their bombers and fighters and missiles are mostly still working.  Fuel and spare parts may be hard to come by, but the military will certainly get first pick.  My guess is that what comes next is that the two countries have a standard hot war, but with the pleasant addition of a great depression sized economic collapse for both right in the middle of it.

@_date: 2013-10-10 12:06:21
@_author: John Kelsey 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
Just thinking out loud....
The administrative complexity of a cryptosystem is overwhelmingly in key management and identity management and all the rest of that stuff.  So imagine that we have a widely-used inner-level protocol that can use strong crypto, but also requires no external key management.  The purpose of the inner protocol is to provide a fallback layer of security, so that even an attack on the outer protocol (which is allowed to use more complicated key management) is unlikely to be able to cause an actual security problem.  On the other hand, in case of a problem with the inner protocol, the outer protocol should also provide protection against everything.
Without doing any key management or requiring some kind of reliable identity or memory of previous sessions, the best we can do in the inner protocol is an ephemeral Diffie-Hellman, so suppose we do this:  a.  Generate random a and send aG on curve P256
b.  Generate random b and send bG on curve P256
c.  Both sides derive the shared key abG, and then use SHAKE512(abG) to generate an AES key for messages in each direction.
d.  Each side keeps a sequence number to use as a nonce.  Both sides use AES-CCM with their sequence number and their sending key, and keep track of the sequence number of the most recent message received from the other side.  The point is, this is a protocol that happens *inside* the main security protocol.  This happens inside TLS or whatever.  An attack on TLS then leads to an attack on the whole application only if the TLS attack also lets you do man-in-the-middle attacks on the inner protocol, or if it exploits something about certificate/identity management done in the higher-level protocol.  (Ideally, within the inner protcol, you do some checking of the identity using a password or shared secret or something, but that's application-level stuff the inner and outer protocols don't know about.

@_date: 2013-10-10 15:54:26
@_author: John Kelsey 
@_subject: [Cryptography] prism-proof email in the degenerate case 
Having a public bulletin board of posted emails, plus a protocol for anonymously finding the ones your key can decrypt, seems like a pretty decent architecture for prism-proof email.  The tricky bit of crypto is in making access to the bulletin board both efficient and private.

@_date: 2013-10-10 17:08:20
@_author: John Kelsey 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
More random thoughts:
The minimal inner protocol would be something like this:
Using AES-CCM with a tag size of 32 bits, IVs constructed based on an implicit counter, and an AES-CMAC-based KDF, we do the following:
Sender: a.  Generate random 128 bit value R
b.  Use the KDF to compute K[S],N[S],K[R],N[R] = KDF(R, 128+96+128+96)
c.  Sender's 32-bit unsigned counter C[S] starts at 0.
d.  Compute IV[S,0] = <96 bits of binary 0s>||C[S]
e.  Send R, CCM(K[S],N[S],IV[S,0],sender_message[0])
a.  Receive R and derive K[S],N[S],K[R],N[R] from it as above.
b.  Set Receiver's counter C[R] = 0.
c.  Compute IV[R,0] = <96 bits of binary 0s>||C[R]
d.  Send CCM(K[R],N[R],IV[R,0],receiver_message[0])
and so on.  Note that in this protocol, we never send a key or IV or nonce.  The total communications overhead of the inner protocol is an extra 160 bits in the first message and an extra 32 bits thereafter.  We're assuming the outer protocol is taking care of message ordering and guaranteed delivery--otherwise, we need to do something more complicated involving replay windows and such, and probably have to send along the message counters.  This doesn't provide a huge amount of extra protection--if the attacker can recover more than a very small number of bits from the first message (attacking through the outer protocol), then the security of this protocol falls apart.  But it does give us a bare-minimum-cost inner layer of defenses, inside TLS or SSH or whatever other thing we're doing.  Both this and the previous protocol I sketched have the property that they expect to be able to generate random numbers.  There's a problem there, though--if the system RNG is weak or trapdoored, it could compromise both the inner and outer protocol at the same time.  One way around this is to have each endpoint that uses the inner protocol generate its own internal secret AES key, Q[i].  Then, when it's time to generate a random value, the endpoint asks the system RNG for a random number X, and computes E_Q(X).  If the attacker knows Q but the system RNG is secure, we're fine.  Similarly, if the attacker can predict X but doesn't know Q, we're fine.  Even when the attacker can choose the value of X, he can really only force the random value in the beginning of the protocol to repeat.  In this protocol, that doesn't do much harm.  The same idea works for the ECDH protocol I sketched earlier.  I request two 128 bit random values from the system RNG, X, X'.  I then use E_Q(X)||E_Q(X') as my ephemeral DH private key. If an attacker knows Q but the system RNG is secure, then we get an unpredictable value for the ECDH key agreement.  If an attacker knows X,X' but doesn't know Q, he doesn't know what my ECDH ephemeral private key is.  If he forces it to a repeated value, he still doesn't weaken anything except this run of the protocol--no long-term secret is leaked if AES isn't broken.  This is subject to endless tweaking and improvement.  But the basic idea seems really valuable:  a.  Design an inner protocol, whose job is to provide redundancy in security against attacks on the outer protocol.
b.  The inner protocol should be:
(i)  As cheap as possible in bandwidth and computational terms.
(ii) Flexible enough to be used extremely widely, implemented in most places, etc.  (iii) Administratively free, adding no key management or related burdens.
(iv) Free from revisions or updates, because the whole point of the inner protocol is to provide redundant security.  (That's part of "administratively free.")  (v)  There should be one or at most two versions (maybe something like the two I've sketched, but better thought out and analyzed).
c.  As much as possible, we want the security of the inner protocol to be independent of the security of the outer protocol.  (And we want this without wanting to know exactly what the outer protocol will look like.)  This means:
(i)  No shared keys or key material or identity strings or anything.
(ii) The inner protocol can't rely on the RNG being good.
(iii) Ideally, the crypto algorithms would be different, though that may impose too high a cost.  At least, we want as many of the likely failure modes to be different.  Comments?  I'm not all that concerned with the protocol being perfect, but what do you think of the idea of doing this as a way to add redundant security against protocol attacks?

@_date: 2013-10-10 18:32:55
@_author: John Kelsey 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
I'm assuming G is a systemwide shared parameter.  It doesn't prevent mitm--remember the idea here is to make a fairly lightweight protocol to run *inside* another crypto protocol like TLS.  The inner protocol mustn't add administrative requirements to the application, which means it can't need key management from some administrator or something.  The goal is to have an inner protocol which can run inside TLS or some similar thing, and which adds a layer of added security without the application getting more complicated by needing to worry about more keys or certificates or whatever.  Suppose we have this inner protocol running inside a TLS version that is subject to one of the CBC padding reaction attacks.  The inner protocol completely blocks that.  I probably wasn't clear in my writeup, but my idea was to have different keys in different directions--there is a NIST KDF that uses only AES as its crypto engine, so this is relatively easy to do using standard components.

@_date: 2013-10-10 20:42:25
@_author: John Kelsey 
@_subject: [Cryptography] prism-proof email in the degenerate case 
So the original idea was to send a copy of all the emails to everyone.  What I'm wanting to figure out is if there is a way to do this more efficiently, using a public bulletin board like scheme.  The goal here would be:
a.  Anyone in the system can add an email to the bulletin board, which I am assuming is public and cryptographically protected (using a hash chain to make it impossible for even the owner of the bulletin board to alter things once published).
b.  Anyone can run a protocol with the bulletin board which results in them getting only the encrypted emails addressed to them, and prevents the bulletin board operator from finding out which emails they got.
This sounds like something that some clever crypto protocol could do.  (It's related to the idea of searching on encrypted data.). And it would make an email system that was really resistant to tracing users.

@_date: 2013-10-11 10:41:38
@_author: John Kelsey 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
We were talking about how hard it is to solve crypto protocol problems by getting the protocol right the first time, so we don't end up with fielded stuff that's weak but can't practically be fixed.  One approach I can see to this is to have multiple layers of crypto protocols that are as independent as possible in security terms.  The hope is that flaws in one protocol will usually not get through the other layer, and so they won't lead to practical security flaws.  Actually getting the outer protocol right the first time would be better, but we haven't had great success with that so far. Maybe not, though I think a very lightweight version of the inner protocol adds only a few bits to the traffic used and a few AES encryptions to the workload.  I suspect most applications would never notice the difference.  (Even the version with the ECDH key agreement step would probably not add noticable overhead for most applications.)  On the other hand, I have no idea if anyone would use this.  I'm still at the level of thinking "what could be done to address this problem," not "how would you sell this?"

@_date: 2013-10-11 15:28:39
@_author: John Kelsey 
@_subject: [Cryptography] Key stretching 
This is a job for a key derivation function or a cryptographic prng.  I would use CTR-DRBG from 800-90 with AES256.  Or the extract-then-expand KDF based on HMAC-SHA512.

@_date: 2013-10-13 01:28:03
@_author: John Kelsey 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
The point is, we don't know how to make protocols that really are reliably secure against future attacks.  If we did, we'd just do that. My hope is that if we layer two of our best attempts at secure protocols on top of one another, then we will get security because the attacks will be hard to get through the composed protocols.  So maybe my protocol (or whatever inner protocol ends up being selected) isn't secure against everything, but as long as its weaknesses are covered up by the outer protocol, we still get a secure final result.  One requirement for this is that the inner protocol must not introduce new weaknesses.  I think that means it must not:
a.  Leak information about its plaintexts in its timing, error messages, or ciphertext sizes.  b.  Introduce ambiguities about how the plaintext is to be decrypted that could mess up the outer protocol's authentication.  I think we can accomplish (a) by not compressing the plaintext before processing it, by using crypto primitives that don't leak plaintext data in their timing, and by having the only error message that can ever be generated from the inner protocol be essentially a MAC failure or an out-of-sequence error.  I think (b) is pretty easy to accomplish with standard crypto, but maybe I'm missing something.  If using AES or P256 are the weak points in the protocol, that is a big win.  Right now, we aren't getting anywhere close to that.  And there's no reason either AES or P256 have to be used--I'm just looking for a simple, lightweight way to get as much security as possible inside some other protocol.

@_date: 2013-10-14 11:55:36
@_author: John Kelsey 
@_subject: [Cryptography] please dont weaken pre-image resistance of SHA3 
I guess I should preface this by saying I am speaking only for myself.  That's always true here--it's why I'm using my personal email address.  But in particular, right now, I'm not *allowed* to work.  But just speaking my own personal take on things....
We go pretty *overwhelming* feedback in this direction in the last three weeks.  (For the previous several months, we got almost no feedback about it at all, despite giving presentations and posting stuff on hash forum about our plans.).  But since we're shut down right now, we can't actually make any decisions or changes.  This is really frustrating on all kinds of levels.
Personally, I have looked at the technical arguments against the change and I don't really find any of them very convincing, for reasons I described at some length on the hash forum list, and that the Keccak designers also laid out in their post.  The core of that is that an attacker who can't do 2^{128} work can't do anything at all to SHA3 with a 256 bit capacity that he couldn't also do to SHA3 with a 512 bit capacity, including finding preimages.  But there's pretty much zero chance that we're going to put a standard out that most of the crypto community is uncomfortable with.  The normal process for a FIPS is that we would put out a draft and get 60 or 90 days of public comments.  As long as this issue is on the table, it's pretty obvious what the public comments would all be about.  The place to go for current comments, if you think more are necessary, is the hash forum list.  The mailing list is still working, but I think both the archives and the process of being added to the list are frozen thanks to the shutdown.  I haven't looked at the hash forum since we shut down, so when we get back there will be a flood of comments there.  The last I saw, the Keccak designers had their own proposal for changing what we put into the FIPS, but I don't know what people think about their proposal. --John, definitely speaking only for myself

@_date: 2013-10-15 03:09:58
@_author: John Kelsey 
@_subject: [Cryptography] please dont weaken pre-image resistance of SHA3 
If the capacity is c bits, then preimages are never more than 2^{c/2} bits.  So SHA3-512 as proposed in my CHES slides would have preimage resistance of 256 bits.

@_date: 2013-10-15 17:47:27
@_author: John Kelsey 
@_subject: [Cryptography] please dont weaken pre-image resistance of SHA3 
Yes.  The 2^{c/2} preimage attack on Keccak/SHA3 is a meet in the middle attack on the internal hash state, and it has nothing to do with the output size.  More broadly, anything you can do to a SHA3 version with much less than 2^{c/2} work, you could also do to *any* hash function with the same output size.

@_date: 2013-10-16 09:23:09
@_author: John Kelsey 
@_subject: [Cryptography] please dont weaken pre-image resistance of SHA3 
There is a small but noticable performance improvement for having SHA3-256 have a capacity of 256 bits.  There is a big performance improvement for having SHA3-512 have a capacity of 512 bits.  In both cases, these performance improvements come at the cost of allowing attacks which are above the security level we normally claim for the hash functions.  In the case of SHA3-512, it's hard to imagine any crypto application needing more than 256 bits of security, and almost nothing else in our crypto toolkit (NIST's or the bigger community's) tries to get higher security than that.  Personally, I think demanding a loss of performance to reach security levels higher than 256 bits is nuts.  It's trading real performance off against imaginary, cosmetic security.
In the case of SHA3-256, there's more of an argument to be made, because while you really ought not to be using a 256-bit hash function and expecting more than 128 bits of security, we do care about being able to reach security levels higher than 128 bits.  There are random number generators and key derivation functions which use a 256 bit hash to generate keys with 256 bits of security.  Now, the NIST functions that do this look fine with SHA3-256 in them, and I think yu could get proofs for their security.  (There is a proof of the PRF security of the Keccak PRF which probably applies, but I'm not sure it's as well nailed down as the indifferentiability bound.). As you said, somewhere there may be someone counting on that higher preimage resistance, though I don't know of any actual examples.

@_date: 2013-10-19 09:33:32
@_author: John Kelsey 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
My understanding from Nadia Henninger's talks (and discussions with her) was that many of the appliance network devices that had duplicate primes in their RSA keys were drawing random numbers from /dev/urandom on machines that weren't collecting much entropy.  If this is right, it's a pretty clear demonstration of what can go wrong when /dev/urandom won't block.  (There must be many, many more cases where the machines are collecting too much entropy to get duplicate primes in their RSA moduli, but they're still getting little enough entropy that an attacker can guess the first prime they generated and thus factor their RSA modulus.)
I think the most critical moment in the operation of any RNG is the decision to start producing outputs.  There is no way for the RNG to know whether these outputs will be used for something unimportant, or to generate the high-value keypair that this device will use for the rest of its lifetime.  For devices that do some crypto, it's probably relatively common to have the system generate a high-value keypair very soon after starting up, so that first output needs to be very likely to be secure.  It's worth thinking about what might be done to ensure the best possible chance of this working.   One other thing you can see from the duplicate RSA keys: many RNGs (I think instances of /dev/urandom drawn on by openSSL) do not incorporate any internal information that can work like a salt.  They should.  If my device has a device serial number, ethernet address, timestamp, etc., those should all be hashed into the entropy pool, with 0 entropy bits assessed.  An attacker trying to guess the state of the pool is in much the same position as an attacker trying to guess a password that was used to derive an encryption key, and any unique-to-this-instance information included works just like salt in a password hashing scheme.  In principle, you could imagine doing an expensive computation to reseed the RNG, but while that would make state guessing attacks a little harder, it would also have a pretty awful performance impact.

@_date: 2013-10-19 15:19:20
@_author: John Kelsey 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
You should think of this like salting a password hash, not like adding entropy.  The attacker can probably know most or all of this data, but he won't be able to just run his entropy pool state guessing attack once and then exploit it for everyone everywhere.  I think the problem we have now is built into the assumptions of /dev/random and /dev/urandom.  It looks like /dev/urandom is typically expected to both never block, and to always give cryptographically secure random bits.  Right now, when those two requirements are not compatible, it fails to give secure random bits.  Fixing that makes it block, which will presumably break some programs, maybe causing a big impact.  I think the problem is that random and urandom split the random number generation problem in the wrong place.  What we probably need is something like a best-try non-blocking random number generator, suitable for non-crypto things where you want really unpredictable values if they're available but you can live with less unpredictability if you have to--stuff like address space randomization might want this.  And then, we want a crypto random number generator that blocks only at the beginning when it doesn't have enough entropy, and otherwise manages its reseeds intelligently.  (Implicit in this:  While I get why people might like to have a full entropy source in some situations, I'm extremely skeptical that it adds much from a real security perspective.)
What would break if /dev/random became something that only provided cryptographic strength random bits instead of full entropy bits, but never blocked except at startup?  Would it be possible to convince developers to then only use /dev/urandom for non-cryptographic applications, and to use /dev/random when they needed cryotographic random bits?  I'm sure there is a ton of code out there that uses /dev/urandom the wrong way now, though, and a change like this wouldn't affect that at all.  For that, the better entropy collection and maybe some external seeding of distributions seem like the only easy fixes, assming you can't make /dev/urandom block.

@_date: 2013-10-21 20:07:25
@_author: John Kelsey 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
A formal security model is exactly what the /dev/random paper had.  Here's a much less formal one, from my paper with Bruce and David and Chris many years ago:
An RNG is in a secure state if it is generating outputs that no attacker can predict or distinguish (up to some bounds on the number of outputs) from ideal random bits.  If it is not in a secure state, then it must be in an insecure state.  For most decent RNGs, being in an insecure state means the attacker knows or can guess the entire internal state.  If an RNG is in a secure state, no additional input fed into it and no plausible amount of output from it should put the RNG into an insecure state.  (At some point, any deterministic rng must cycle, so there has to be a limit on the number of outputs, but that can be huge.).  The old DSA RNG and the RSAREF RNG both had problems with this.  In general, a good technique here is to hash the external input before combining it with the RNG state, or to generate a new RNG state using the RNG and then XOR the input into it.  If an RNG was previously in a secure state, but has been compromised and now is in an insecure state, then the attacker mustn't be able to go backward and learn previous states or predict previous outputs he hasn't seen, even if he is given some previous outputs and asked to predict others.  This is ensured by periodically applying a one-way function to the RNG state.  The simplest one to use is the RNG itself--just use the RNG to generate a new RNG state.  If an RNG is in an insecure state, it should be able to get to a secure state if given sufficient entropy (say 128 bits).  This is the reason for "catastrophic reseeding."  You want to put in 128 bits of entropy all at once, rather than trickling in a few bits, then generating another output, because an attacker who starts out knowing your state can guess the entropy input, and check his guess against your RNG output, and can iterate this process to keep you from ever reseeding.  Let's consider a system that starts up for the first time today, and very quickly generates a high value keypair.  We are concerned with whether the RNG is in a secure state at the moment the randomness is used to generate the keypair.  The attacker wants to guess the input to the key generation mechanism.  He buys several instances of the same device and reverse-engineers them and experimentally determines what entropy values they collect before the key generation is done.  He builds a predictive model.  Now, to understand the security of this system, we need to know something about how often his predictive model guesses right.  This works very much like a password guessing attack--the attacker runs his model to generate huge numbers of predicted entropy inputs for the RNG, sort of like a John the Ripper for the system's entropy collection mechanism.  Think of the RNG and the key generation mechanism as being a little like a password hash algorithm, with the public key (or the first prime factor of the RSA modulus) being the password hash.  The attacker has to run his entropy input guessing engine and check each guess against a given machine's public key.  Suppose the RNG doesn't include stuff like a MAC address or IP address.  Then the attacker need only run his entropy guessing routine once, and precompute some huge table of possible keys or prime factors to try.  Perhaps he will do some clever time/memory tradeoff here as well.  At any rate, if 50% of the devices' entropy inputs can be guessed with 2^{40} work, then the attacker does one big 2^{40} precomputation, and he can break 50% of the devices' keys, whenever he sees them.  It's just like unsalted password hashes.  Suppose the RNG hashes in a MAC address.  Immediately, the attacker has a worse life--now with the same amount of entropy, he must do a new 2^{40} search each time he encounters a new device with a public key.  It's like a salted password hash.  Suppose the device pings a dozen hosts on the internet and times the returns of the packets, and hashes those into its RNG state before it generates its keys along with the MAC address.  Now, an attacker who wants to recover the key probably needs to have been observing the device's local network at the time the key was generated, or it won't have enough information to predict the RNG state.  Supose the device has a factory-installed secret value of 128 bits, which is stored forever on the device.  If it hashes this secret into the RNG state before generating its key, then the attacker must compromise the device or the manufacturer to guess the key.  And so on.  For the initial state of the RNG, you get about 90% of the right intuitions for thinking about the attack by thinking about a password cracking attack.

@_date: 2013-10-24 10:54:05
@_author: John Kelsey 
@_subject: [Cryptography] "Death Note" elimination for hashes 
I like the idea of death notes for crypto primitives, but to make them practical, you need a couple of things:
a.  An alternative primitive you can switch to in your system.  If your choices are DES and RC4, and you''ve already sent out the DES death note, then you *can't* send out the RC4 death note without ceasing to work.  b.  A more efficient mechanism than needing to show the break, which is only workable for some algorithms.  What does the death note look like for an adaptive chosen plaintext attack on AES that breaks it with 2^{50} texts and 2^{100} work?  (b) is easy with a TTP or a set of trusted parties--each crypto primitive has an identifier that's a hash value, and the preimage of the hash is the death note.  (Though you are then stuck supporting that hash function forever at least for this one feature.  Though you could concatenate hashes and require the preimage for *all* of them.)  You could give that preimage to a dozen trusted parties, and *any* of them could send out the death note, which would then spread in a viral fashion.  (a) is really hard once a couple of primitives/modes are broken.  One reason so many sites are using RC4 only for encryption is because of the reaction attacks on AES-CBC encryption.  I guess the way to do this would be to *require* support for a bunch of ciphers/modes up front that were as different as possible--CAST128 in CFB-mode + HMAC-SHA256, Twofish-CCM, AES-GCM, SHA3 in duplex mode, and Salsa20 + Poly1305, say.  Then, a death note for any one of them still leaves you a lot of choices.

@_date: 2013-10-24 10:59:14
@_author: John Kelsey 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
We seem to be seeing a move toward commonly-used CPUs including hardware entropy sources.  With those, we're in a much better position.  There's always the possibility that the entropy source was cooked or flawed, but that's something you can engineer your way around reasonably well.  Suppose you have a cryptographic PRNG that you initialize with a seed like this:
a.  Get 256 bits of entropy from the OS.
b.  Get 256 bits of entropy from the hardware entropy source.
c.  Ping several hosts on the internet and measure the response time, and fold that into your seed.
d.  Fold your ethernet address, IP address, and serial number into the seed.
e.  Fold the installed-at-birth secret 128 bit value from your device into the seed.
Initialize a PRNG with all that, and the attacker is in an extremely hard place, as he has to be able to guess all five elements.  (d) isn't all that hard to guess, but the rest will in general be very hard to guess.

@_date: 2013-10-25 08:12:00
@_author: John Kelsey 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
This gets back to the threat model discussion.  If your attacker is watching you from the outside as you generate your key, then interacting with stuff over the local net won't help you much.  (You may get a bit or two of entropy from the attacker not being able to know exactly which clock-tick you were on when the interrupt was serviced, but not much.). If he's not watching you then, you can rule out a whole category of attackers.  Similarly, if you have some secret value that's available to any program on your machine, an attacker who can get onto your machine later can learn that.  But one who can't is just not able to guess your prng starting state.  What else can be done to rule out classes of attacker up front?

@_date: 2013-10-25 08:15:57
@_author: John Kelsey 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
I like this idea.  If my PRNG is in a secure state, I can give out random numbers to anyone who asks.  At first startup, it won't be possible to establish a secure connection yet (no entropy), but by asking some hosts for a random number, we ensure that if those messages aren't recorded, the attacker can't possibly guess our PRNG starting state.

@_date: 2013-10-28 14:54:59
@_author: John Kelsey 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
Right.  My point is that we want to consider the possibility that one or more of our sources aren't good, but we don't know which.  For any minimally decent PRNG, if we feed it 128 bits of entropy plus a million bits the attacker gets to choose, we should still get a secure PRNG state.  So, we get 256 bits of entropy from the OS.  If the OS can actually find that much entropy, then we're done--we are now in a secure state.  If not, though, the hardware entropy source can come to our rescue.  But suppose the hardware RNG is cooked or broken, and the OS isn't able to collect enough entropy.  Then, what should we do?  Well, let's think about our attacker.  What we really care about w.r.t. PRNG seeding is not the ultimate question of how unpredictable the seed is.  We care about how unpredictable it is to the attacker.  So, let's partition the attacker space into two categories--some attackers are watching your network traffic at the time you generate your key, and some aren't.  If we are sampling the high-speed counter every time a packet arrives and folding that into our PRNG seed, then it seems almost certain to me that an attacker who isn't watching your network traffic right then is going to be unable to reconstruct exactly when each packet arrived.  Attackers that are not monitoring your network traffic right then are just ruled out.  To make this work even better, do some kind of broadcast on the local network, asking any other devices to respond with 256 bits from /dev/urandom or the system PRNG.  An attacker who is eavesdropping will see this and you won't benefit from it, but attackers who aren't eavesdropping on your network traffic just don't know what was sent, and so you have a secure PRNG seed with respect to those attackers.  Now, let's partition the attacker space into two different categories--those who eventually get hold of your machine, and those who do not.  If there is a fixed 128 bit random number installed on your machine (different for every one), and you feed that into your PRNG seed, then the attackers who don't eventually get hold of your machine are lost--they simply don't know those 128 bits, and can't guess them, so your PRNG is secure w.r.t. them.  (A stored seed is even better, though it's not clear whether or not the previous PRNG states will be recoverable.  In its absense, I'm not sure if there's anything else that could be used to get some unique information from the machine that only someone on the machine could get.)  What we can get from this is that, even if the OS and hardware RNGs are weak, the only attackers who can exploit this are those who were eavesdropping on your net traffic right before you generated your keypair, and who got hold of your machine afterward.  Other attackers simply can't get anywhere.  I'd love to think of some more classes of attacker we could rule out entirely.  Even a high precision timestamp doesn't do much good, since there are only so many times this particular device could have generated their key.  Now, at last, we come to the "salt" inputs--ethernet address, IP address, serial number, etc.  (Timestamp is good here, too.)  These aren't intended to provide any entropy.  Instead, their goal is to work like salt in a password hashing scheme.  Let's suppose the attacker has a predictive model that lets him guess a lot, but not all, of our entropy.  By including the "salt" inputs, he is forced to rerun his attack on each machine with the same configuration, rather than being able to run his attack once and then look for matching keypairs from every machine with the same configuration.

@_date: 2013-10-29 23:59:53
@_author: John Kelsey 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
Wait, the FIPS labs refuse to let you put your own stuff into those additional inputs?  That's the whole *point* of having them in the DRBGs.  If you call generate with an additional input that is not guessable to the attacker, starting with a DRBG state the attacker knows, the DRBG is put into an unguessable-to-the-attacker state before the output bits are generated.  One thing that's probably causing some confusion somewhere is that 800-90A really just specifies deterministic algorithms.  90B (which is still being worked on) specifies entropy sources, and 90C (also still being worked on) specifies how to combine the two kinds of components to get a working random bit generator.  I gather the FIPS guys are trying to impose some kinds of requirements while they wait for these to be done.  But there is no way it makes sense to restrict the DRBG additional inputs to only coming from an authenticated on-device location.  Our goal is to have a trusted entropy source in there somewhere, since otherwise you can't really know you are starting up into a secure state.  But there should not be any restrictions on where the additional inputs can come from.  There's also an idea of combining entropy sources, which we're working on in 90C, but I don't think it made it into the current out-for-comment draft.  All three 800-90 documents are out for public comment right now.  If you want to make sure it's included in the official comments (which we will definitely be going through and addressing), email exactly what you did here to RBG_Comments at nist.gov
More broadly to everyone: If you see problems with how the FIPS validation process plays with the DRBGs, or other problems, email a formal comment in.

@_date: 2013-10-30 21:19:10
@_author: John Kelsey 
@_subject: [Cryptography] FIPS 140 testing hurting secure random bit 
Perhaps you have never worked in a large organization?  I know there are problems with validation, but that does not mean either that I automatically have the power to fix them, or that I know all the problems that people run into with FIPS validation.  At any rate, I didn't know that the labs were forbidding people freely putting in external input from unverified sources.  When I emailed around to other people working on RNG stuff and dealing with validation stuff today, they didn't know about it either. The reason I recommend making a formal comment is because the documents are open for public comment right now, and because those comments make it a lot easier to make a case that this is a problem that needs fixing.

@_date: 2013-10-30 23:28:18
@_author: John Kelsey 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
The part of the standard involving entropy sources isn't done yet, but when it is, RBGs really will have to ultimately be fed by an approved entropy source.  The alternative seems to be leaving people in the current situation, where there's more or less no way of knowing how much entropy is being collected, or where it's coming from.  If the entropy source is good, then the RBG should end up secure.  The restriction on external sources of additional input is pretty obviously a misunderstanding--someone somewhere got confused between entropy source inputs (which need to come from some trusted entropy source) and additional inputs (which can come from anywhere).  I'm not sure what that "do a deal with fips authenticated inputs"  bit is even supposed to mean.  But this kind of nonsense doesn't have to make sense, it just has to be entertaining.  That's true.  But it's also true that security is hard to get right.  Lots and lots of dumb policies and decisions have been accepted or imposed by people who thought they were doing something sensible, but were really making security weaker.  And the bit where people make up conspiracy theories to explain every such failure has zero chance of improving security.

@_date: 2013-09-01 13:32:06
@_author: John Kelsey 
@_subject: [Cryptography] NSA and cryptanalysis 
What I think we are worried about here are very widespread automated attacks, and they're passive (data is collected and then attacks are run offline).  All that constrains what attacks make sense in this context.  You need attacks that you can run in a reasonable time, with minimal requirements on the amount of plaintext or the specific values of plaintext.  The perfect example of an attack that works well here is a keysearch on DES; another example is the attack on WEP.
All the attacks we know of on reduced-round AES and AES-like ciphers require a lot of chosen plaintexts, or related key queries, or both.  There is no way to completely rule out some amazing new break of AES that makes the cipher fall open and drop your plaintext in the attacker's lap, but I don't see anything at all in the literature that supports that fear, and there are a *lot* of smart people trying to find new ways to attack or use AES-like designs.  So I put this at the bottom of my list of likely problems.
Some attacks on public key systems also require huge numbers of encryptions or specially formed ciphertexts that get sent to the target for decryption--we can ignore those for this discussion.  So we're looking at trying to factor an RSA modulus or to examine a lot of RSA encryptions to a particular public key (and maybe some signatures from that key) and try to get somewhere from that.  I don't know enough about the state of the art in factoring or attacking RSA to have a strong intuition about how likely this is.  I'm pretty skeptical, though--the people. know who are experts in this stuff don't seem especially worried.  However, a huge breakthrough in factoring would make for workable passive attacks of this kind, though it would have to be cheap enough to use to break each user's public key separately.  Finally, we have the randomness sources used to generate RSA and AES keys.  This, like symmetric cryptanalysis, is an area I know really well.  And my intuition (backed by plenty of examples) is that this is probably the place that is most likely to yield a practical offline attack of this kind.  When someone screws up the implementation of RSA or AES, they may at least notice some interoperability problems.  They will never notice this when they screw up their implementation so that RNG only gets 32 bits of entropy before generating the user's RSA keypair.  And if I know that your RSA key is likely to have one of these 2^{32} factors, I can make a passive attack work really well.

@_date: 2013-09-02 23:03:25
@_author: John Kelsey 
@_subject: [Cryptography] Backup is completely separate 
<521CE337.6030706
The backup access problem isn't just a crypto problem, it's a social/legal problem.  There ultimately needs to be some outside mechanism for using social or legal means to ensure that, say, my kids can get access to at least some of my encrypted files after I drop dead or land in the hospital in a coma.  Or that I can somehow convince someone that it's really me and I'd like access to the safe deposit box whose password I forgot and lost my backup copy of.  Or whatever.  This is complicated by the certainty that if someone has the power to get access to my encrypted data, they will inevitably be forced to do so by courts or national security letters, and will also be subject to extralegal pressures or attacks to make them turn over some keys.  I suspect the best that can be workably done now is to make any key escrow service's key accesses transparent and impossible to hide from the owner of the key, and then let users decide what should and shoudn't be escrowed.  But this isn't all that great an answer.

@_date: 2013-09-05 19:14:53
@_author: John Kelsey 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
First, I don't think it has anything to do with Dual EC DRGB.  Who uses it?  My impression is that most of the encryption that fits what's in the article is TLS/SSL.  That is what secures most encrypted content going online.  The easy way to compromise that in a passive attack is to compromise servers' private keys, via cryptanalysis or compromise or bad key generation.  For server side TLS using RSA, guessing just the client's random values ought to be enough to read the traffic.  For active attacks, getting alternative certs issued for a given host and playing man in the middle would work.  Where do the world's crypto random numbers come from?  My guess is some version of the Windows crypto api and /dev/random or /dev/urandom account for most of them.  What does most of the world's TLS?  OpenSSL and a few other libraries, is my guess.  But someone must have good data about this.  My broader question is, how the hell did a sysadmin in Hawaii get hold of something that had to be super secret?  He must have been stealing files from some very high ranking people.

@_date: 2013-09-06 01:04:31
@_author: John Kelsey 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
One thing I can say for certain:  NSA did not write SP 800-90.  (That was the implication of from the New York Times article.). That was mostly Elaine Barker, my coworker, with me providing a lot of text around technical issues.  The algorithms all came out of work from ANSI X9.82 Part 3, which Elaine also wrote with lots of input and text from me and the rest of the editing committee.  We worked with NSA people on this, and X9.82 Part 2 (the entropy source section) was written entirely by NSA people, but it isn't easy for me to see how anyone could stick a trapdoor in that.  We're still working with NSA people on SP 800-90B, which includes a lot of stuff on testing entropy sources.  When I started there was an RSA based algorithm which we eventually dropped (analogous to bbs), Dual EC DRBG, the hash drbg (which is still in 800-90 in a much-changed version for backward comatibility reasons, because the standardization process took forever and people started designing to the X9.82 standard before it was done--this was also the reason we ended up keeping the Dual EC DRBG) and an X9.17 based thing we got rid of.  A bunch of the symmetric stuff in there is mine--I made changes the the hash DRBG to address time/memory/data tradeoffs, and wrote the HMAC DRBG and CTR DRBG.  I also changed around the hash df and wrote the bc df.  It is possible Dual EC DRBG had its P and Q values generated to insert a trapdoor, though I don't think anyone really knows that (except the people who generated it, but they probably can't prove anything to us at this point).  It's also immensely slower than the other DRBGs, and I have a hard time seeing why anyone would use it.  (But if you do, you should generate your own P and Q.)
If you're trying to solve the problem of not trusting your entropy source, this is reasonable, but it doesn't exactly scale to normal users.  Entropy collection in software is a pain in the ass, and my guess is that the overwhelming majority of developers are happy to punt and just use the OS' random numbers.  That looks to be what happened with the Henninger and Lenstra results regarding lots of RSA keys with shared moduli.  That flaw implies a huge number of easily factored RSA keys out there, thanks to insufficient entropy and calling /dev/urandom on a system where it won't block even if it thinks it has zero entropy.  (This was a multi-level screw up, as I understand it, between a Linux version and OpenSSL and the implementors of various appliance network devices.). It would be smarter for any crypto software to use the OS source for some seed material, and then combine it with both unique or nearly unique information and anything that's likely to have some entropy, and use that to initialize a good PRNG.  (I'd use CTR DRBG.)   Your ethernet address doesn't have any entropy, but it works like a salt in a password hashing scheme--an attacker must now rerun his attack for each individual instance of the crypto software. In general, software that uses crypto should probably be trying to gather entropy wherever possible, not just trusting the OS.  And it should use that entropy plus the OS' entropy to seed its own PRNG.  And it should throw in any information that might distinguish this instance from other instances, to force any attackers to rerun their attack on each instance individually.  When I saw the keystore stuff, I thought "bad key generation."  Henninger found a bunch of RSA keys from the same make of devices that were sharing primes, thanks to really awful entropy collection.  If those devices had been collecting 40 bits of entropy for the first prime, she might never have found a pair of keys with a shared prime.  But someone using analogous methods might be able to generate 2^{40} primes ahead of time, and efficiently run each new RSA key sent in against that list and break a large fraction of the keys.
I think long-term public keys are the big weakness here, because they tend to be generated relatively early.  It's easy to see how you don't have any entropy in your pool a minute after starting up; it's much harder to see how that happens after a year of operation.

@_date: 2013-09-06 01:19:10
@_author: John Kelsey 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
I don't see what problem would actually be solved by dropping public key crypto in favor of symmetric only designs.  I mean, if the problem is that all public key systems are broken, then yeah, we will have to do something else.  But if the problem is bad key generation or bad implementations, those will be with us even after we abandon all the public key stuff.  And as Jon said, the trust problems get harder, not easier.  With only symmetric crypto, whoever acts as the introducer between Alice and Bob can read their traffic passively and undetectably.  With public key crypto, the introducer can do a man in the middle attack (an active attack) and risks detection, as Alice and Bob now have things signed by the introducer associating the wrong keys with Bob and Alice, respectively.

@_date: 2013-09-06 01:40:54
@_author: John Kelsey 
@_subject: [Cryptography] FIPS, NIST and ITAR questions 
You won't get a prf or stream cipher or prng or block cipher just out of collision resistance--you need some kind of pseudorandomness assumption.  We expect general purpose hash functions like Keccak to provide that, but it doesn't follow from the collision resistance assumption, for exactly the reason you gave there--it's possible to design collision-resistant functions that leak input or are predictable in some bits. The HMAC construction wouldn't give a PRF for your example of h(x) = sha512(x) || sha512(x)
A single output would be trivial to distinguish from a 1024 bit random number.

@_date: 2013-09-07 21:22:16
@_author: John Kelsey 
@_subject: [Cryptography] In the face of "cooperative" end-points, 
Your cryptosystem should be designed with the assumption that an attacker will record all old ciphertexts and try to break it later.  The whole point of encryption is to make that attack not scary.  We can never rule out future attacks, or secret ones now.  But we can move away from marginal key lengths and outdated, weak ciphers.  Getting people to do that is like pulling teeth, which is why we're still using RC4, and 1024-bit RSA keys and DH primes.

@_date: 2013-09-07 21:46:31
@_author: John Kelsey 
@_subject: [Cryptography] XORing plaintext with ciphertext 
It depends on the encryption scheme used.  For a stream cipher (including AES in counter or OFB mode), this yields the keystream.  If someone screws up and uses the same key and IV twice, you can use knowledge of the first plaintext to learn the second.  For other AES chaining modes, it's less scary, though if someone reuses their key and IV, knowing plaintext xor ciphertext from the first time the key,iv pair was used can reveal some plaintext from the second time it was used.

@_date: 2013-09-07 22:51:11
@_author: John Kelsey 
@_subject: [Cryptography] Why prefer symmetric crypto over public key 
Pairwise shared secrets are just about the only thing that scales worse than public key distribution by way of PGP key fingerprints on business cards.  The equivalent of CAs in an all-symmetric world is KDCs.  Instead of having the power to enable an active attack on you today, KDCs have the power to enable a passive attack on you forever.  If we want secure crypto that can be used by everyone, with minimal trust, public key is the only way to do it.  One pretty sensible thing to do is to remember keys established in previous sessions, and use those combined with the next session.  For example, if we do Diffie-Hellman today and establish a shared key K, we should both store that key, and we should try to reuse it next time as an additional input into our KDF.  That is, next time we use Diffie-Hellman to establish K1, then we get actual-key = KDF(K1, K, other protocol details).  That means that if even one session was established securely, the communications are secure (up to the symmetric crypto strength) forevermore.

@_date: 2013-09-07 23:06:44
@_author: John Kelsey 
@_subject: [Cryptography] [cryptography] Random number generation 
There are basically two ways your RNG can be cooked:
a.  It generates predictable values.  Any good cryptographic PRNG will do this if seeded by an attacker.  Any crypto PRNG seeded with too little entropy can also do this.  b.  It leaks its internal state in its output in some encrypted way.  Basically any cryptographic processing of the PRNG output is likely to clobber this. The only fix for (a) is to get enough entropy in your PRNG before generating outputs.  I suspect Intel's RNG and most other hardware RNGs are extremely likely to be better than any other source of entropy you can get on your computer, but you don't have to trust them 100%.  Instead, do whatever OS level collection you can, combine that with 256 bits from the Intel RNG, and throw in anything else likely to help--ethernet address, IP address, timestamp, anything you can get from the local network, etc.  Hash that all and feed it into a strong cryptographic PRNG--something like CTR-DRBG or HMAC-DRBG from SP 800-90.  If you do that, you will have guarded against both (a) and (b).

@_date: 2013-09-07 23:45:22
@_author: John Kelsey 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Let's suppose I design a block cipher such that, with a randomly generated key and 10,000 known plaintexts, I can recover that key.  For this to be useful in a world with relatively sophisticated cryptanalysts, I must have confidence that it is extremely hard to find my trapdoor, even when you can look closely at my cipher's design.   At this point, what I have is a trapdoor one-way function.  You generate a random key K and then compute E(K,i) for i = 1 to 10000.  The output of the one-way function is the ciphertext.  The input is K.  If nobody can break the cipher, then this is a one-way funciton.  If only I, who designed it, can break it, then it's a trapdoor one-way function.  At this point, I have a perfectly fine public key encryption system.  To send me a message, choose a random K, use it to encrypt 1 through 10000, and then send me the actual message encrypted after that in K.  If nobody but me can break the system, then this cipher works as my public key.  The assumption that matters here is that you know enough cryptanalysis that it would be hard to hide a practical attack from you.  If you don't know about differential cryptanalysis, I can do the master key cryptosystem, but only until you learn about it, at which point you will break my cipher.   But if you can, say, hide the only good linear characteristics for some cipher in its S-boxes in a way that is genuinely intractible for anyone else to find, then you have a public key cryptosystem. You can publish the algorithm for hiding new linear characteristics in an S-box--this becomes the keypair generation algorithm.  The private key is the linear characteristic that lets you break the cipher with (say) 10000 known plaintexts, the public key is the cipher definition.

@_date: 2013-09-08 16:08:58
@_author: John Kelsey 
@_subject: [Cryptography] Market demands for security (was Re: Opening 
As an aside:
a.  Things that just barely work, like standards groups, must in general be easier to sabotage in subtle ways than things that click along with great efficiency.  But they are also things that often fail with no help at all from anyone, so it's hard to tell.
b.  There really are tradeoffs between security and almost everything else.  If you start suspecting conspiracy every time someone is reluctant to make that tradeoff in the direction you prefer, you are going to spend your career suspecting everyone everywhere of being ant-security.  This is likely to be about as productive as going around suspecting everyone of being a secret communist or racist or something.

@_date: 2013-09-08 16:21:55
@_author: John Kelsey 
@_subject: [Cryptography] Techniques for malevolent crypto hardware 
In principle, the malevolent crypto accellerator could flip into weak mode (however that happens) only upon receiving a message for decryption with some specific value or property.  That would defeat any testing other than constant observation.  This is more or less the attack that keeps parallel testing of electronic voting machines from being a good answer to the security concerns about them.

@_date: 2013-09-08 18:16:45
@_author: John Kelsey 
@_subject: [Cryptography] Techniques for malevolent crypto hardware 
I don't think you can do anything useful in crypto without some good source of random bits.  If there is a private key somewhere (say, used for signing, or the public DH key used alongside the ephemeral one), you can combine the hash of that private key into your PRNG state.  The result is that if your entropy source is bad, you get security to someone who doesn't compromise your private key in the future, and if your entropy source is good, you get security even against someone who compromises your private key in the future (that is, you get perfect forward secrecy).

@_date: 2013-09-09 23:29:52
@_author: John Kelsey 
@_subject: [Cryptography] Random number generation influenced, HW RNG 
Giving raw access to the noise source outputs lets you test the source from the outside, and there is alot to be said for it.  But I am not sure how much it helps against tampered chips.  If I can tamper with the noise source in hardware to make it predictable, it seems like I should also be able to make it simulate the expected behavior.  I expect this is more complicated than, say, breaking the noise source and the internal testing mechanisms so that the RNG outputs a predictable output stream, but I am not sure it is all that much more complicated.  How expensive is a lightweight stream cipher keyed off the time and the CPU serial number or some such thing to generate pseudorandom bits?  How much more to go from that to a simulation of the expectdd behavior, perhaps based on the same circutry used in the unhacked version to test the noise source outputs?

@_date: 2013-09-10 15:59:38
@_author: John Kelsey 
@_subject: [Cryptography] Random number generation influenced, HW RNG 
I don't think simulating a physical source is itself a big challenge.  People simulate complicated probabilistic behavior all the time.  The challenge is going to be sticking it into the chip in a way that doesn't show up when the chip is taken apart in a lab.
How can we design the whole system so that some compromised or flawed pieces don't wreck us?  I don't know how to ensure my chip's hardware RNG isn't hacked, but I have some hope of working out a design that will be robust even if it is hacked.

@_date: 2013-09-10 16:45:23
@_author: John Kelsey 
@_subject: [Cryptography] Fw: how could ECC params be subverted & other 
I've talked a bit with someone I know with some background in the math here, who didn't see an obvious way for these curves to have been cooked.  I don't have the number theory to have an opinion, but if someone can point me to an explanation of how they might have been cooked, I would very much appreciate it.  I imagine any such potential backdoor will be very easy to get my management to take seriously right now.  And that three months ago, we would not have been at all worried.  I forsee a rather large change in institutional culture at NIST.  It sure looks now like Dual EC DRBG was backdoored from the leaks and news stories, though I don't know of any hard proof.  But we (NIST) have put SP 800-90 back up for public comment and have issued a bulletin telling people to stop using it until we figure out what to do about it.  (The alternatives are to remove it or to fix it.)
This DRBG was in the X9.82 document when I joined NIST and came onto the project in 2003.  If you go to our website (csrc.nist.gov) you can see old slides and documents, and you can check the wayback machine to verify we haven't changed them.  (We also had a public workshop, and participants may still have old copies of the documents.)  This shows the development of the standard over time.  I think the P and Q were the same in those old documents.  If so, this tells you how far back this effort goes--at least to 2003, perhaps further back.  I believe the X9.82 effort had been going on several years before that, though not making much progress--I'm not sure how long ago Dual EC DRBG was put in.  I paid very little attention to the number theoretic constructions (two originally, one in the final version) because I didn't and don't think I know enough number theory to evaluate them.
When we heard about the issue of selection of points (in an X9 meeting, I think in 2006 or early 2007), we discussed the issue, and it didn't seem like a serious threat.  I sure didn't think the people I was working with on the document were trying to slip weak stuff in.  Instead, it looked like they had generated some parameters randomly and hadn't worried about proving where the parameters came from.  This seemed like a really weird place to put a backdoor, because it was insanely slow, and it seemed unlikely to get any significant use.  And I, at least, had internalized the idea that we weren't going to get intentional bad advice or sabotage from another part of the federal government.  (Accidental screw-ups, sure, but not intentional engineered vulnerabilities.) At the time, the solution we came to--allow the use of the default points (which some people had allegedly baked into implementations in anticipation of the standard) and also allow generation of your own points, seemed adequate.    But that was assuming a different world than we live in, apparently.  If NSA had a program of intentionally inserting weaknesses in crypto standards, and inserted at least one into a NIST standard, then any potential backdoor parameters we have are scary as hell.  No way can we leave them in a standard people are expecting to use.  Having such a program burns a hell of a lot of bridges, too.  I still don't *know* if this is true--convincing newspaper stories often get stuff wrong, and convincing leaked documents may not be authentic.  But it sure looks like the way to bet, now.

@_date: 2013-09-12 19:59:51
@_author: John Kelsey 
@_subject: [Cryptography] People should turn on PFS in TLS (was Re: Fwd: 
I think this is completely wrong.
First, there aren't any secret constants to those curves, are there?  The complaint Dan Bermstein has about the NIST curves is that they (some of them) were generated using a verifiably random method, but that the seeds looked pretty random.  The idea here, if I understand it correctly, is that if the guys doing the generation knew of some property that made some of the choices of curves weak, they could have tried a huge number of seeds till they happened upon one that led to a weak curve.  If they could afford to try N seeds and do whatever examination of the curve was needed to check it for weakness, then the weak property they were looking for couldn't have had a probability much lower than about 1/N.  I think the curves were generated in 1999 (that's the date on the document I could find), so we are probably talking about less than 2^{80} operations total.  Unlike the case with the Dual EC generator, where a backdoor could have been installed with no risk that anyone else could discover it, in this case, they would have to generate curves until one fell in some weak curve class that they knew about, and they would have to hope nobody else ever discovered that weak curve class, lest all the federal users of ECC get broken at once.  The situation you are describing works for dual ec drbg, but not for the NIST curves, as best I understand things.

@_date: 2013-09-13 16:55:05
@_author: John Kelsey 
@_subject: [Cryptography] prism proof email, namespaces, and anonymity 
The more I think about it, the more important it seems that any anonymous email like communications system *not* include people who don't want to be part of it, and have lots of defenses to prevent its anonymous communications from becoming a nightmare for its participants.  If the goal is to make PRISM stop working and make the email part of the internet go dark for spies (which definitely includes a lot more than just US spies!), then this system has to be something that lots of people will want to use.  There should be multiple defenses against spam and phishing and other nasty things being sent in this system, with enough designed-in flexibility to deal with changes in attacker behavior over tome.  If someone can send participants in the system endless spam or credible death threats, then few people are going to want to participate, and that diminishes the privacy of everyone remaining in the system, along with just making the system a blight in general.  If nonparticipants start getting spam from the system, it will either be shunned or shut down, and at any rate won't have the kind of reputation that will move a lot of people onto the system.  An ironclad anonymous email system with 10,000 users is a whole lot less privacy-preserving than one with 10,000,000 users.  As revelations of more and more eavesdropping come out, we might actually see millions of users want to have something really secure and anonymous, but not if it's widely seen as a firehose o' spam.  A lot of the tools we use on the net everyday suffer from having been designed without thinking very far ahead into how they might be exploited or misused--hence spam, malware in PDF files, browser hijacking sorts of attacks, etc.  My thought is that we should be thinking of multiple independent defenses against spamming and malware and all the rest, because parasites adapt to their environment.  We can't count on "and then you go to jail" as a final step in any protocol, and we can't count on having some friendly utility read millions of peoples' mail to filter the spam if we want this to be secure.  So what can we count on to stop spam and malware and other nastiness?  Some thoughts off the top of my head.  Note that while I think all these can be done with crypto somehow, I am not thinking of how to do them yet, except in very general terms.  a.  You can't freely send messages to me unless you're on my whitelist.  b.  This means an additional step of sending me a request to be added to your whitelist.  This needs to be costly in something the sender cares about--money, processing power, reputation, solving a captcha, rate-limits to these requests, whatever.  (What if the system somehow limited you to only, say, five outstanding requests at a time?). c.  Make account creation costly somehow (processing, money, solving a captcha, whatever).  Or maybe make creating a receive-only account cheap but make it costly to have an account that can request to communicate with strangers.  d.  Make sending a message in general cost something.  Let receiver addresses indicate what proof of payment of the desired cost they require to accept emails.  e.  Enable some kind of reputation tracking for senders?  I'm not sure if this would work or be a good idea, but it's worth thinking about.  f.  All this needs to be made flexible, so that as attackers evolve, so can defenses.  Ideally, my ppe (prism proof email) address would carry an indication of what proofs your request to communicate needed to carry in order for me to consider it.  g.  The format of messages needs to be restricted to block malware, both the kind that wants to take over your machine and the kind that wants to help the attacker track you down.  Plain text email only?  Some richer format to allow foreign language support?  h.  Attachments should become links to files in an anonymizing cloud storage system.  Among other things, this will make it easier to limit the size of the emails in the system, which is important for ensuring anonymity without breaking stuff.  What else?  I see this as the defining thing that can kill an anonymous encrypted communications system--it can become a swamp of spam and malware and nutcases stalking people, and then nobody sensible will want to come within a hundred meters of it.  Alternatively, if users are *more* in control of who contacts them in the prism-proof scheme than with the current kind of email, we can get a lot more people joining.

@_date: 2013-09-13 17:00:05
@_author: John Kelsey 
@_subject: [Cryptography] one time pads 
Switching from AES to one-time pads to solve your practical cryptanalysis problems is silly.  It replaces a tractable algorithm selection problem with a godawful key management problem, when key management is almost certainly the practical weakness in any broken system designed by non-idiots.

@_date: 2013-09-14 17:59:42
@_author: John Kelsey 
@_subject: [Cryptography] Security is a total system problem (was Re: 
Also, if AES being insufficiently strong is our problem, we have a whole bunch of solutions easily at hand.  Superencrypt successively with, say, Serpent, Twofish, CAST, Salsa, and Keccak in duplex mode.  This has a performance cost, but it is orders of magnitude less overhead than switching to manual key distribution of one-time pads.  It's hard for me to think of a real world threat that is addressed better by a one-time pad than by something cheaper and less likely to get broken via human error or attacks on the key management mechanism.

@_date: 2013-09-14 18:12:50
@_author: John Kelsey 
@_subject: [Cryptography] real random numbers 
Your first two categories are talking about the distribution of entropy--we assume some unpredictability exists, and we want to quantify it in terms of bits of entropy per bit of output.  That's a useful distinction to make, and as you said, if you can get even a little entropy per bit and know how much you're getting, you can get something very close to ideal random bits out.
Your second two categories are talking about different kinds of sources--completely deterministic, or things that can have randomness but don't always.  That leaves out sources that always have a particular amount of entropy (or at least are always expected to!).  I'd say even the "squish" category can be useful in two ways:
a.  If you have sensible mechanisms for collecting entropy, they can't hurt and sometimes help.  For example, if you sample an external clock, most of the time, the answer may be deterministic, but once in awhile, you may get some actual entropy, in the sense that the clock drift is sufficient that the sampled value could have one of two values, and an attacker can't know which.  b.  If you sample enough squishes, you may accumulate a lot of entropy.  Some ring oscillator designs are built like this, hoping to occasionally sample the transition in value on one of the oscillators.  The idea is that the rest of the behavior of the oscillators might possibly be predicted by an attacker, but what value gets read when you sample a value that's transitioning between a 0 and a 1 is really random, changed by thermal noise.  I think the big problem with (b) is in quantifying the entropy you get.  I also think that (b) describes a lot of what commonly gets collected by the OS and put into the entropy pool.

@_date: 2013-09-15 10:19:01
@_author: John Kelsey 
@_subject: [Cryptography] real random numbers 
If you are using a strong cryptographic PRNG, you only really need to know the amount of entropy you've collected in two situations:
a.  When you want to instantiate the PRNG and start generating keys from it.
b.  When you want to reseed the PRNG and know you will get some benefit from doing so.  But those are pretty critical things, especially (a).  You need to know whether it is yet safe to generate your high-value keypair.  For that, you don't need super precise entropy estimates, but you do need at least a good first cut entropy estimate--does this input string have 20 bits of entropy or 120 bits?  My view is that all the song and dance in /dev/random with keeping track of the entropy in the pool as it flows in and out is not all that useful, but there's just no way around needing to estimate entropy to know if your PRNG is in a secure state or not.

@_date: 2013-09-15 10:50:38
@_author: John Kelsey 
@_subject: [Cryptography] prism proof email, namespaces, and anonymity 
This seems like the main way most people would want PPE to work--like email they have now, but much more secure and resistant to abuse.  In the overwhelming majority of cases, I know and want to know the people I'm talking with.  I just don't want to contents of those conversations or the names of people I'm talking with to be revealed to eavesdroppers.  And if I get an email from one of my regular correspondents, I'd like to know it came from him, rather than being spoofed from someone else.  For most people, I'm pretty sure the security problems with email are centered around the problem of getting unwanted communication from people you don't want to hear from, some of which may manage install malware on your computer, others of which want to waste your time with scam ads, etc.  A PPE scheme that solves that problem can get a lot more users than one that doesn't, and may even eventually take over from the current kind of email.

@_date: 2013-09-17 18:21:47
@_author: John Kelsey 
@_subject: [Cryptography] The paranoid approach to crypto-plumbing 
Encrypt then MAC has a couple of big advantages centering around the idea that you don't have to worry about reaction attacks, where I send you a possibly malformed ciphertext and your response (error message, acceptance, or even time differences in when you send an error message) tells me something about your secret internal state.

@_date: 2013-09-18 00:42:47
@_author: John Kelsey 
@_subject: [Cryptography] The paranoid approach to crypto-plumbing 
For hash functions, MACs, and signature schemes, simply concatenating hashes/MACs/signatures gives you at least the security of the stronger one.  Joux multicollisions simply tell us that concatenating two or more hashes of the same size doesn't improve their resistance to brute force collsion search much.  The only thing you have to be sure of there is that the MAC and signature functions aren't allowed access to each others' secret keys or internal random numbers.  Otherwise, MAC can always take the value of MAC key.  This is just
message, signature 1, signature 2
where the signatures are over the message only.  For encryption algorithms, superencryption works fine.  You can first encrypt with AES-CBC, then encrypt with Twofish-CFB, then with CAST5 in CFB mode.  Again, assuming you are not letting the algorithms know each others' internal state or keys, if any of these algorithms are resistant to chosen plaintext attacks, then the combination will be.  This doesn't guarantee that the combination will be any stronger than the strongest of these, but it will be no weaker.  (Biham and later Wagner had some clever attacks against some chaining modes using single-DES that showed that you wouldn't always get anything stronger than one of the ciphers, but if any of these layers is strong, then the whole encryption is strong.  An alternative approach is to construct a single super-block-cipher, say AES*Twofish*SERPENT, and use it in a standard chaining mode.  However, then you are still vulnerable to problems with your chaining mode--the CBC reaction attacks could still defeat a system that used AES*Twofish*SERPENT in CBC mode, but not AES-CBC followed by Twofish-CFB followed by SERPENT-CTR.  For key-encryption or transport, I think it's a little more complicated.  If I need six symmetric keys and want to use three public key methods (say ECDH, NTRU, RSA) to transport the key, I've got to figure out a way to get the benefit from all these key exchange mechanisms to all six symmetric keys, in a way that I'm sure will not leak any information about any of them.  Normally we would use a KDF for this, but we don't want to trust any one crypto algorithm not to screw us over.  I think we can get this if we assume that we can have multiple KDFs that have secrets from one another.  That is, I compute KDF1( key1, combined key exchange input) XOR KDF2( key2, combined key exchange input)
The reason the two KDFs need keys that are secret from each other is because otherwise, KDF1 could just duplicate KDF2 and we'd get an all-zero set of keys.  If KDF2 is strong, then KDF1 can't generate an output string with any relationship to KDF2's string when it doesn't know all the input KDF2 is getting.  I agree with Perry that this is probably padlocking a screen door.  On the other hand, if we want to do it, we want to make sure we guard against as many bad things as possible.  In particular, it would be easy to do this in such a way that we missed chaining mode/reaction type attacks.

@_date: 2013-09-18 00:46:15
@_author: John Kelsey 
@_subject: [Cryptography] The paranoid approach to crypto-plumbing 
Arggh!  Of course, this superencryption wouldn't help against the CBC padding attacks, because the attacker would learn plaintext without bothering with the other layers of encryption.  The only way to solve that is to preprocess the plaintext in some way that takes the attacker's power to induce a timing difference or error message away.

@_date: 2013-09-22 09:50:38
@_author: John Kelsey 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
If criminals circumvent the PKI to steal credit card numbers, this shows up as fraud and is noticed without any need for a Snowden.  Eavesdropping doesn't show up in such an obvious way.  Also, criminals find it harder to spend a few million up front before they get the first payoff.  Nor can they appeal to patriotism or compel compliance via the law.  It has to pay for itself ultimately, at least as well as email does.

@_date: 2013-09-23 15:45:03
@_author: John Kelsey 
@_subject: [Cryptography] Gilmore response to NSA mathematician's "make 
This does not go far enough.  The US government is not the only one inclined to steal information which it can reach, either because the information goes over wires the government can listen in on, or because the companies handling the data can be compelled or convinced to hand it over.  Right now, we're seeing leaks that confirm the serious efforts of one government to do this stuff, but it is absolutely silly to think the US is the only one doing it.  The right way to address this is to eliminate the need to trust almost anyone with your data.  If Google[1] has all your cleartext documents and emails, they can be compelled to turn them over, or they can decide to look at them for business reasons, or they can be infiltrated by employees or contractors who look at those emails and documents.  You are trusting a lot of people, and trusting a company to possibly behave against its economic interests and legal obligations, to safeguard your privacy.  If they have encrypted data only, you don't have to trust them.  It needs to be in their business interest to convince you that they *can't* betray you in most ways.  [1] I'm not picking on Google in particular--any US company may be compelled to turn over data they have.  I imagine the same is true of any German or Korean or Brazilian company, but I don't know the laws in those places.

@_date: 2013-09-24 19:48:38
@_author: John Kelsey 
@_subject: [Cryptography] The hypothetical random number generator backdoor 
We don't know that there is a backdoor in dual ec, but we know that there could be one of a particular form, and it works like you describe--if you know the backdoor, then given output[i], you can predict all future outputs.  The other kinds of weaknesses I would expect to see in the wild would be lack of entropy, which means that you can try some reasonable number of guesses about the value of the output and expect to have a decent probability of being right once.  This would work if you had a secure key I couldn't guess for k.  If the entropy is really low, though, I would still see duplicate outputs from time to time.  If the RNG has short cycles, this would also show up.
This works against the dual ec type backdoor, but does nothing for low entropy or short cycles.  Also, it's kind-of sensitive to how H() works.  If H(x) = P(x) xor x then this will be invertible.  Still, the basic idea is nice.  It would be interesting to see if you could prove something about its security.
How about:
nonce = r[1] xor H(r[2])?
The other thing that you can do is to XOR multiple RNGs together.  As long as at least one is unbroken and all other RNGs are independent of that one unbroken one, the resulting random outputs should be secure.

@_date: 2013-09-25 18:14:54
@_author: John Kelsey 
@_subject: [Cryptography] Gilmore response to NSA mathematician's "make 
There are many places where there is no way to provide the service without having access to the data, and probably storing it.  For those places, we are stuck with legal and professional and business safeguards.  You doctor should take notes when you see him, and can be compelled to give those notes up if he can access them to (for example) respond to a phone call asking to refill your medications.  There are rather complicated mechanisms you can imagine to protect your privacy in this situation, but it's hard to imagine them working well in practice.  For that situation, what we want is that the access to the information is transparent--the doctor can be compelled to give out information about his patients, but not without his knowledge, and ideally not without your knowledge.  But there are a lot of services which do not require that the providers have or collect information about you.  Cloud storage and email services don't need to have access to the plaintext data you are storing or sending with them.  If they have that information, they are subject to being forced to share it with a government, or deciding to share it with someone for their own business reasons, or having a dishonest employee steal it.  If they don't have that information because their service is designed so they don't have it, then they can't be forced to share it--whether with the FBI or the Bahraini government or with their biggest advertiser.  No change of management or policy or  law can make them change it.  Right now, there is a lot of interest in finding ways to avoid NSA surveillance.  In particular, Germans and Brazilians and Koreans would presumably rather not have their data made freely available to the US government under what appear to be no restrictions at all.  If US companies would like to keep the business of Germans and Brazilians and Koreans, they probably need to work out a way to convincingly show that they will safeguard that data even from the US government.

@_date: 2013-09-30 14:22:40
@_author: John Kelsey 
@_subject: [Cryptography] check-summed keys in secret ciphers? 
GOST was specified with S boxes that could be different for different applications, and you could choose s boxes to make GOST quite weak.  So that's one example.

@_date: 2013-09-30 18:24:12
@_author: John Kelsey 
@_subject: [Cryptography] RSA equivalent key length/strength 
Maybe you should check your code first?  A couple nist people verified that the curves were generated by the described process when the questions about the curves first came out.  Don't trust us, obviously--that's the whole point of the procedure.  But check your code, because the process worked right when we checked it.

@_date: 2013-09-30 18:35:24
@_author: John Kelsey 
@_subject: [Cryptography] RSA equivalent key length/strength 
Having read the mail you linked to, it doesn't say the curves weren't generated according to the claimed procedure.  Instead, it repeats Dan Bernstein's comment that the seed looks random, and that this would have allowed NSA to generate lots of curves till they found a bad one.  it looks to me like there is no new information here, and no evidence of wrongdoing that I can see.  If there is a weak curve class of greater than about 2^{80} that NSA knew about 15 years ago and were sure nobody were ever going to find that weak curve class and exploit it to break classified communications protected by it, then they could have generated 2^{80} or so seeds to hit that weak curve class.  What am I missing?  Do you have evidence that the NIST curves are cooked?  Because the message I saw didn't provide anything like that.

@_date: 2013-09-30 20:24:40
@_author: John Kelsey 
@_subject: [Cryptography] Sha3 
If you want to understand what's going on wrt SHA3, you might want to look at the nist website, where we have all the slide presentations we have been giving over the last six months detailing our plans.  There is a lively discussion going on at the hash forum on the topic.  This doesn't make as good a story as the new sha3 being some hell spawn cooked up in a basement at Fort Meade, but it does have the advantage that it has some connection to reality.
You might also want to look at what the Keccak designers said about what the capacities should be, to us (they put their slides up) and later to various crypto conferences.  Or not.

@_date: 2014-08-08 10:44:38
@_author: John Kelsey 
@_subject: [Cryptography] All dice are loaded? 
For DRBG seeding and related stuff like password selection via diceware, a small bias doesn't matter all that much.  Suppose 6 ends up having probability 10% too high--1/6+1/60.  In terms of min-entropy, this gives you
original: -lg(1/6) ~= 2.58 bits of min-entropy per roll. new: -lg(1/6 + 1/60) ~= 2.45 bits of min-entropy per roll.
With 30 dice rolls (what Diceware recommends) you only lose a couple bits of entropy.  That is, your final password should have min-entropy of 2.58*30=77.4 bits, but due to the flawed dice it has a min-entropy of 2.45*30=73.5 bits.  The most likely password (the one that is generated by rolling all sixes for every roll) becomes very slightly more likely, but still not likely enough that it makes a password cracker's job noticably easier.  It seems like this would be different in different dice or different manufacturers.  I mean, the only difference between the faces that's inherent in the design of standard 6-sided dice is that the faces of the higher numbers are very slightly lighter than the faces of the lower numbers, but I'd intuitively expect differences in the precise shape of the dice or manufacturing variation to matter more.  I rather strongly suspect there's a file drawer effect going on here--the many times someone ran the experiment for a given kind of dice and found no statistically significant difference, this was sufficient confirmation for a casino or classroom exercise, but nobody would bother publishing that as a result.  But who knows?

@_date: 2014-08-08 11:14:49
@_author: John Kelsey 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
I guess I don't see the point of this kind of user selection.  If all the options are secure, then you can use the same crypto for handling the payroll as you use for handling your child's birthday party invitation list.  If all the options aren't secure, then why on Earth are you offering the user insecure options? Where I see some benefit to giving the user (or sysadmin) the ability to configure the crypto that's used is:
a.  If there's some widely reported attack that means a particular ciphersuite is bad, you'd like it to be possible to turn that one off.  (It's much more important in this context to be able to turn a ciphersuite off than to be able to select it.)  b.  If your organization has some rules about which ciphersuites you use (like federal users preferring FIPS approved algorithms), you may want to be able to select the ciphersuite.  It may even be that occasionally, there's a good reason to select those ciphersuites, though I suspect most of the time, it will be pointy-haired bosses mandating or forbidding something they read about (and didn't quite understand) in the news.  c.  If there's a performance/security tradeoff, that might be worthwhile to expose to the user, but only if the performance option is still secure and the secure option still performs okay.  Are there other reasons to allow many choices?  I mean, the practical reason we have a gazillion choices for every security standard is because that's how standards group politics works--everyone champions their favorite algorithm.  This has been known to have some downsides now and again, though, so it would be nice to eliminate it except where there's a real benefit you can point to in having different choices about ciphersuites or algorithms or curves or whatever.  Yes, this.  If I go to the doctor with a sinus infection, I don't want him to write me a prescription with a complicated set of options based on whether I think the bacteria infecting me are gram negative or gram positive, how likely I think it is that they're producing beta-lactamase, etc.  I might specifically have a reason to exercise some choices ("I'm allergic to Sulfa drugs, so don't give me those"), but there's no value in giving me a bunch of options I don't understand.  One big downside of more options is that it means more things to test.  If your choices are, say, the FIPS ciphersuite and the DJB ciphersuite, then there are only a couple things to test.  If you allow an independent selection of every component, then testing gets a lot nastier.

@_date: 2014-08-08 11:20:16
@_author: John Kelsey 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
Does it make sense to have a small set of curves that everyone uses?  Or would it be better to have every application or even every user generate their own curve, using some process that would convince skeptics that the curves had been generated randomly?

@_date: 2014-02-01 11:01:36
@_author: John Kelsey 
@_subject: [Cryptography] Mac OS 10.7.5 Random Numbers 
(quoting from the BSD /dev/random man page)
This isn't quite right.  Once Yarrow is in a secure state, it will stay there unless the state is compromised.  If there is no more entropy coming in after that, it will never recover.

@_date: 2014-02-03 11:24:56
@_author: John Kelsey 
@_subject: [Cryptography] Mac OS 10.7.5 Random Numbers 
What attack do you think is made practical by having only a 160-bit PRNG state, instead of a 256-bit state?  Any validation process you come up with is going to have the same feature: once you've gotten something validated, changing it is, in general, going to mess up your validation.  Otherwise the validation means nothing, because you could get a crypto device validated using RSA2048 and SHA256, and then change it over to using RSA512 and MD5.  The exact same thing holds true if, instead of a formal validation process like FIPS 140 validation or CC evaluation, you simply hire someone (or get some volunteers) to do a careful review of the architecture, cryptographic protocols and algorithms, and code of some crypto product.  Suppose you have a crypto product, and you hire some high profile, trusted people to review the code and the design from top to bottom.  Their review only really applies to the thing they reviewed.  Once you start changing the algorithms, that original review doesn't say much.  If a bunch of really smart people review Truecrypt and give it a green light, and then next year Truecrypt changes a couple of their crypto algorithms and rewrites some of their code, that review isn't very informative of the new version of Truecrypt.  I don't know how much FIPS 140 labs reuse previous analysis when they revalidate something, but I don't see how you can change algorithms without doing some kind of revalidation, if you want that validation to mean anything at all.

@_date: 2014-02-03 11:44:03
@_author: John Kelsey 
@_subject: [Cryptography] Pre-image security of SHA-256 reduced to 
No, this is a really important distinction, and one that otherwise pretty savvy people get mixed up all the time.  Here's an example: If you have a smaller Keccak variant with a capacity of 80 bits, both collisions and preimages are easy to find with 2^{40} work.  However, suppose you have recovered a password file with lots of entries using this weak hash function.  The 2^{40} preimage attack doesn't help you *at all* in recovering passwords from the file.  Here's another example:  The old RSAREF PRNG used MD5 in counter mode (Hash_DRBG in SP 800-90 uses SHA1 or SHA2 in counter mode) to generate outputs.  So you'd get pseudorandom output like
where all the additions were mod 2^{512} or something.
Now, a preimage attack (find *any* input x* such that hash(C+j)=hash(x) is of neglible value in attacking the PRNG.  But an attack that lets you go backwards and learn C (or the high 64 bits of C) from these outputs also allows you to break the PRNG.
For more examples, consider the use of hash functions to construct KDFs, or consider HMAC.

@_date: 2014-02-03 11:56:32
@_author: John Kelsey 
@_subject: [Cryptography] Pre-image security of SHA-256 reduced to 16 
[This first part is me asking a question]
AES has a relatively lightweight key schedule.  Twofish can be implemented so that it can change keys very quickly, too.  I believe quite a few of the recent lightweight block ciphers that have been proposed also have very lightweight key schedules, so they might be worth a look.  The Keccak SHA3 proposal includes scaled-down designs.  Basically, you can think of the full Keccak permutation as a 5x5 matrix of 64-bit words which gives a 1600-bit permutation.  And you can define a smaller permutation based on a smaller power of two word size.  So, the 200-bit permutation is defined by making it a 5x5 array of 8-bit words.  They intended the scaled down versions both as toy versions of the hash functions to cryptanalyze, and as potentially useful algorithms for low-end environments.  I think you could also define a scaled-down version of the JH permutation pretty easily.  I know Skein also defined a 256-bit wide version of their algorithm, but didn't propose it for standardization--that might be another place to look for relatively smaller hash functions that didn't have to haul around a thousand plus bits of state.

@_date: 2014-02-03 12:01:45
@_author: John Kelsey 
@_subject: [Cryptography] Now it's personal -- Belgian cryptographer 
What was the flaw?  I remember the NIST statistics guys ran a bunch of tests that didn't make sense on the candidates (like encrypting random plaintexts and then doing statistics on the ciphertexts).  Was there some other problem alongside that?  That sure looked like a bunch of people doing statistics on stuff they didn't understand. and I don't think it had any impact, as I remember everyone calling it out as soon as the report came out.  --John

@_date: 2014-02-03 12:12:23
@_author: John Kelsey 
@_subject: [Cryptography] cheap sources of entropy 
The problem of using hard drives for entropy is one that I think demonstrates a lot of the ways entropy collection from general purpose stuff tends to go wrong.  Here's the pattern:
a.  Someone does a careful, in-depth analysis of the behavior of some component of general-purpose machines, like trying to really quantify the unpredictability in read times and trace it back to air turbulence inside the drive.  b.  People start using this analysis to estimate entropy.  (Or more honestly, they use it to assert enough entropy exists, since if it doesn't, they've got a pain-in-the-ass design problem they don't want.)  c.  Over time and across devices, the reality on which the original analysis was based is radically changed.  Some machines have networked drives.  Some have flash drives.  The drive hardware gets smarter, with bigger caches and more layers of caching.  The OS changes its behavior in ways that change everything.  And so on.  d.  Code developed and even tested for one environment run on some new environment, and don't get any entropy.  The software now getting insufficient entropy never even detects that this is the case.  And we get a bunch of keys with 16 bits of entropy in them. I think this is going to be the problem as long as we're counting on general-purpose devices to give us entropy.  Any analysis we do is only valid on the hardware and OS that we do it on, and yet it needs to be used (and will be used) in many very different environments.  It's one reason why I think dedicated hardware entropy sources like Intel and AMD are putting into their chips are a huge step in the right direction.

@_date: 2014-02-03 12:14:30
@_author: John Kelsey 
@_subject: [Cryptography] cheap sources of entropy 
Maybe this is just my lack of understanding coming out, but I'm having a hard time seeing how any crypto code is going to remain secure if the hypervisor controlling the VM it's running on is under an attacker's control.  --John

@_date: 2014-02-20 17:13:40
@_author: John Kelsey 
@_subject: [Cryptography] Mac OS 10.7.5 Random Numbers 
[This is from awhile back because I got busy with other stuff, but it seemed like there were a couple of points worth discussing a bit more. ]
The problem is, one part of what the labs do in their validation is verify that you are really using a specific set of algorithms, and (via known answer testing) that the algorithms are probably correctly implemented.  If you have a module validated with 3DES, and now you want to use AES, someone somewhere needs to go back and verify that your module does, in fact, correctly implement AES.  Adding algorithms and modes and such to the list of approved algorithms is relatively painless.  It's removing them that's hard.  But just because a new algorithm is approved and you claim to do it doesn't mean anyone has verified that you're really doing what you say, or that you got the implementation right.  This seems kind-of trivial, but I've heard that a surprising fraction of modules fail one of the algorithm tests.  And when you buy or download software that does crypto, you're usually left trusting that the documentation is telling you the truth about what crypto they're doing.  (If it's open source, you can at least check to see if they're *implementing* AES or RSA or whatever, but it's a lot of work to verify even that they're actually doing the crypto they claim they're doing.)  Yep.  Since so many people hated the idea of reducing the capacities, we decided to go back to the original capacities in the Keccak SHA3 submission for all the fixed hash functions.  If you want the faster performance from a smaller capacity hash, you can use one of the variable output-length modes (the SHAKEs) with a 128 bit security level, but the fixed length hashes all have the capacity = twice the output length.

@_date: 2014-02-20 17:18:31
@_author: John Kelsey 
@_subject: [Cryptography] Random numbers only once 
If the pool doesn't have enough entropy, it *has* to block or return an error code or something. Otherwise, we get back into the realm of the bug that led to all those appliance routers and firewalls sharing primes in their RSA moduli.
Now, if you have a hardware RNG and you're willing to trust it, you can avoid blocking, since the hardware RNG is surely going to be able to give you 256 bits of entropy very quickly, and you can use that to seed AES256 CTR-DRBG or SHA256 HMAC-DRBG, and from there you have as many bits as you need.  If you have a hardware RNG and don't want to trust it, then you're back to either blocking or generating output bits you don't fully trust.

@_date: 2014-02-21 00:56:06
@_author: John Kelsey 
@_subject: [Cryptography] Entropy Attacks! 
[I'm responding to a somewhat old message, because I think this is an interesting idea worth thinking more about]
Yes.  This attack comes up with trying to generate mutually agreed on random numbers, too.  If Alice and Bob agree to send each other random numbers and then hash them together to get a shared random, and Alice wants to control the low bit of the resulting random number, she waits till she sees Bob's random number and then tries a few (on average two, I think) random numbers till she gets a random number R[a] such that low bit of hash(R[a] || R[b]) == 1.  The work for this attack is exponential in the number of bits controled, so it's not going to be easy to control more than a small number of output bits in this way.  In that context, one solution is to get each participant to first commit to their random number, and only open the commitments (reveal the random numbers) when they've seen everyone's commitments.  We've thought about this a bit w.r.t. potential attacks on the NIST beacon.  (For example, we define the random result of each beacon message as the hash of the whole beacon value, including signature, because the signature is done by a crypto token which won't export its signature key to the host machine.  An attacker who takes over the host machine running the beacon can only try to control output bits via brute force as quickly as the token will produce signatures, and can't move that brute force search off to the cloud or something.)
However, I have a hard time seeing how this works as a realistic attack on an RNG.  For the attack to make sense, I have to have code running inside your system which controls one entropy source, and can see all the other entropy sources, and which can duplicate everything your OS's RNG is doing in order to exert control over a couple bits of your RNG output.  It sure seems like once I've got code running with that kind of access, I can do a lot nastier things to you, like leak your keys (or your RNG seeds) via some subliminal channel.  In general, once I've got my code where it's reading all your RNG inputs, I think I've pretty much won. I think there is benefit in using some kind of key stretching algorithm for this, but it's not to resist this kind of super powerful attacker.  Rather, it's to survive a situation where you are starting up your RNG with  a marginal amount of entropy.  If your attacker can't do more than 2^{80} work to recover your seed, and you have only 60 bits of entropy, then a 2^{20} iterated hash will move the attack just out of reach.  (Note: you ought never to *design* a system to seed your DRBG with a marginal amount of entropy, but this sort of technique could buy you a few extra bits of safety margin in case you end up with a marginal amount of entropy despite your best efforts.)  In general, for the kind of attack on a PRNG or DRBG where you are trying to guess the inputs, you get about 90% of the right intuitions for the attack by thinking about password cracking attacks.  Salt (IP or ethernet addresses) is a win for resisting this kind of attack, for example--it adds no additional entropy, but it forces the attacker to rerun his attack on each new instance of your RNG.  Similarly, making the mapping from entropy input to RNG seed computationally expensive buys you a little extra security.  And, just as with password hashing, if you have a weak password (insufficient input entropy), all this added benefit is inadequate.  By contrast, if you have a strong password (enough input entropy), neither of these steps is all that valuable.  Salt and high iteration counts are only really very important for marginal passwords (marginal amounts of input entropy).  Of course, most users are lucky to get to a marginal password in the face of modern password cracking techniques.  Yes, this is a variant of the entropy guessing attack.  The obvious problem with making initializing the RNG computationally expensive is that people really don't like waiting for their RNG to be ready to generate random bits.  (Note the pressure *not* to make /dev/urandom block when it doesn't have enough input entropy.  For that matter, note the use of /dev/urandom for crypto products and libraries that should know better.)  I wonder if you might even make some RNGs worse, in practice, by doing this.  If my system has to have a working RNG ten seconds after startup, I can either:
a.  Collect entropy for 10 seconds and seed my RNG.
b.  Collect entropy for 10-T seconds and then run my key stretching algorithm for T seconds.
Suppose we are getting 8 bits of entropy per second, and I set T=5 (so I run my key stretching function for 5 seconds).  I now am seeding with 40 bits of entropy, and running for five seconds maybe I'm putting another 25 bits of security into the system, but I am losing more by not incorporating the other 5 seconds' worth of entropy in.  We could incorprate late-arriving entropy into the RNG at the end of the key stretching algorithm, but that kills its value for blocking the attack you were describing above.

@_date: 2014-02-21 00:58:52
@_author: John Kelsey 
@_subject: [Cryptography] RNG exploits are stealthy 
This general phenomenon seems to me to be the strongest argument against using general purpose stuff that's lying around in your computer as an entropy source.  You can do this, analyze it, and get a secure system today, and over time, things can change that will be pretty much invisible to your software, but that will completely destroy your security.

@_date: 2014-02-21 01:12:20
@_author: John Kelsey 
@_subject: [Cryptography] Random numbers only once 
Well, it's not a requirement for 99%+ of uses of random numbers.  But before you generate a crypto key, you need at least 128 or so bits of entropy that haven't been used to generate any other outputs, so that you can seed an RNG.  If you don't have that, and you seed the RNG and generate the crypto key anyway, you get something that looks just like real crypto, except that there are attackers who can sign messages coming from you or read your encrypted mail, and every now and then some smartass academic will be factoring thousands of your devices' RSA keys because they all share primes.  The best answer I see to this is a hardware entropy source built into everything, combined with whatever entropy can be collected in time to seed a good DRBG.

@_date: 2014-02-21 01:18:22
@_author: John Kelsey 
@_subject: [Cryptography] Entropy Attacks! 
One other aside: If you were worried about the Intel RNG trying to carry out this attack, you could *start* by collecting 256 bits from RDRAND, and then collect entropy from the other sources until you are ready to seed your RNG.  If those other sources actually have any entropy, then the Intel RNG can't predict them, and so can't do anything with them.  This is probably pointless paranoia, since if the CPU you're running on is evil, it's probably impossible to get much security.  But assuming the Intel RNG is good, seeding your system RNG in this way would work just fine.

@_date: 2014-02-21 01:23:28
@_author: John Kelsey 
@_subject: [Cryptography] The ultimate random source 
As an aside, how much magnification do you need to observe Brownian motion of particles in a fluid?  I think there are iPhone microscopes for under a hundred bucks that might be powerful enough, but I'm not sure.  This wouldn't make a production system, but it would make a pretty cool high school science fair project!

@_date: 2014-01-02 20:31:00
@_author: John Kelsey 
@_subject: [Cryptography] Dual_EC_DRBG backdoor: a proof of concept 
If we replaced dual ec drbg's output function by taking the parity of the output point's scalar value, it looks to me like we'd have a secure drbg despite the potentially evil choice of P and Q, with whatever good theoretical properties came from dual ec drbg.

@_date: 2014-01-02 20:43:08
@_author: John Kelsey 
@_subject: [Cryptography] TAO, NSA crypto backdoor program 
Amen!  We can and should get the intelligence agencies in democratic countries back under some kind of control, but that only addresses one part of the problem.  National scale attackers are hard to defend against.  However, there are a couple important differences when it's the intelligence agencies of your own country:
a.  If you (an American) figure out that the Chinese are trying to slip malware into a product, you can ask for help from the US authorities.  How much help you will get is probably pretty uneven, but I have to guess that if Cisco or Microsoft asks the FBI for help because they think the Chinese government is trying to slip malware into their products,  they can probably get the attention of some pretty high level people.  If it's your own government attacking your products, you can't reasonably call the FBI, and you may very well be required by law to go along and keep quiet, or may believe you are required to do so. b.  Far more Americans are going to be willing to go along with the US government doing anything than with the Chinese government doing it.  The spies trying to plant a weakness have the advantage that they can rely on patriotism, future employment prospects, legal requirements, or simply on trust ("they must have a good reason to tell me to do this.").  A foreign intelligence agency has to rely on much more  blunt methods--bribes, blackmail, threats, etc.  --John (definitely speaking only for myself!)

@_date: 2014-01-03 12:45:06
@_author: John Kelsey 
@_subject: [Cryptography] Dual_EC_DRBG backdoor: a proof of concept 
No, Niels and Bruce and I were doing this in Yarrow-160 long before X9.82, and it was also how the RSAREF, ANSI X9.17 and DSA prngs worked even earlier.  I published a paper with Bruce, Niels, and Chris showing how this kind of PRNG can be attacked long before X9.82 had started, too.  This was one of about three models for doing random number generation floating around--alongside pool type designs like dev/random and hardware RNG designs.
Right, but the apparent sabotage of dual ec drbg wasn't all that subtle--it involved getting on the standards editing committee, and being trusted by the other members of that committee.  In a world where lots of people (including me) thought they were working to make public crypto stronger, not weaker.  Putting a backdoor in a standard they worked on involved spending all the goodwill and trust they had built up over the last decade or more, with consequences that will go a decade or two into the future.  That sounds silly to me.  Attackers use what they can find.  The NSA attacks that have come out aren't some kind of black magic, they're the same kind of stuff done by (or suspected of) other attackers, just with more resources.
There have been a lot of flawed
RNGs in the literature.  Were any intentional, or accidentally created but left in as a backdoor?  If so, by whom?  Absent an attack of conscience along the lines of Manning or Snowden, how would we know?

@_date: 2014-01-08 10:18:47
@_author: John Kelsey 
@_subject: [Cryptography] What is an attack, and what is not an attack? 
I think you're headed toward a lower bound estimate of what the real-world attacks look like, but we also need to consider likely and possible attacks.  For example, we have years of results on all-electronic voting machines that show that they generally have Swiss cheese like security, but I am not aware of any documented election fraud in the US based on exploiting these weaknesses.  It would be imprudent as hell to assume that these machines haven't been attacked and won't be, based on the absence of evidence for these attacks do far.  We should at least assume that lab-demonstrated attacks that would work in the field are representative of what NSA and similar agencies in other countries are up to.  Further, monetary damage isn't the only measure of interest.  What's the monetary value of millions of peoples' communications being vacuumed up for years?  I have no idea how to put a price on that, and absent whistleblowers we would not ever have noticed the scale and intrusiveness of that massive surveillance, but it's sure as hell an attack!  Criminals are motivated mostly by money, so they can probably be modeled in terms of costs and benefits.  Governments and terrorists and ideologues are motivated by something different, and you probably don't get a great model of them by thinking in terms of dollars of damage.  How much monetary damage is done when a nascent protest is silently sniffed out by arresting three or four ringleaders on trumped-up charges?  How do we measure the unwillingness of potential sources to talk to journalists given the massive surveillance?  Not in dollars.  This is always going to be a problem.  We know dual ec could have been backdoored, and that if it was, it could have been used to compromise a lit of communications.  We probably won't ever know how much was compromised, or whose, or even whether P and Q were generated to put a backdoor into the DRBG.  We know that all kinds of appliance Internet devices were generating keys with frightfully low amounts of entropy.  We don't know if this was ever exploited, or if there was any kind of operation to get/keep lame entropy collection in those products.  (Though a smart attacker would have hashed the Ethernet address into the pool before generating the keys, to make the weakness harder to see from outside.
And so on.  I don't know how many industrial accidents, crashed commercial websites, blackouts, etc., over the last decade have been the result of some subtle computer attack, but I'm sure the answer isn't 0.  Similarly, I don't know how many would-be protesters or opposition politicians have been shut down by these attacks over the world, but again, I can't imagine the answer is 0.  It isn't clear how your way of thinking about attacks captures any of that.

@_date: 2014-01-20 12:13:01
@_author: John Kelsey 
@_subject: [Cryptography] cheap sources of entropy 
The problem is, nobody makes *everything* they use.  A sufficiently resourceful attacker might attack your device on all kinds of levels, and you can't possibly check them all yourself.  This has even been worked out by people with s lot of resources--classified systems apparently use a lot of off the shelf components now, for economic reasons.  The folks who run those systems would love to be paranoid enough to verify everything in their system, but they can't--it would cost too much.  I spent some time running through this with e-voting, and the same problem exists everywhere--there's a limited amount you can do to verify your software and hardware if you want to be able to afford the final product, and even spending a lot more than you can afford, you can't really get trustworthy software or hardware.  The plausible solutions there (which may be replicatable) are:
a.  Use some kind of physical audit trail to allow detection of misbehavior.
b.  Use some kind of electronic audit trail to allow detection of misbehavior, using other machines to do the checking and assuming an attacker can't compromise everything.
c.  Use some clever cryptographic protocol to allow detection or prevention of misbehavior.  A good example of (b) is certificate transparency--we can't avoid trust in CAs if we want anything like conventional PKI, but we can make CA misbehavior *visible*.  It's possible to do similar things with crypto protocols, though I'm not sure how generally applicable they will be.  Chaum had this idea of an "observer" which could prevent double-spending of e-cash but couldn't otherwise do anything bad.  I wonder if something like that could be developed for various hardware crypto operations, so that at least you could get multiple independent machines checking on one another.

@_date: 2014-01-20 12:17:03
@_author: John Kelsey 
@_subject: [Cryptography] one-time pads 
One-time pads replace a relatively well understood cipher design problem with an almost intractible key management problem.  If you're not trying to keep secrets from adversaries living in the low Beyond, they're not much use.

@_date: 2014-01-20 12:24:17
@_author: John Kelsey 
@_subject: [Cryptography] HSM's 
I'm sure you can find some HSM out there that uses single-DES or a homegrown cipher instead of AES, MD5 instead of SHA1 or SHA2, 768-bit RSA keys, etc.  So, yeah, I'm sure you can find someone who will sell you an HSM that ignores NIST recommendations.

@_date: 2014-01-20 12:34:13
@_author: John Kelsey 
@_subject: [Cryptography] Fwd: [IP] RSA Response to Media Claims Regarding 
I think that lesson has been driven home pretty effectively by the Snowden disclosures, along with the related lesson for standards committees.  Even if current participants stay in place, their coworkers aren't going to trust them for a long time to come.  However, I am pretty damned skeptical about how many US companies will manage to resist a decade-long multi-million-dollar intelligence operation run against them by their own government.

@_date: 2014-01-20 12:49:32
@_author: John Kelsey 
@_subject: [Cryptography] RSA is dead. 
Perhaps this is the result of living in a government bubble for awhile, but I certainly saw and heard a lot of the bigger community who thought NSA's involvement in domestic crypto standards and companies was intended to improve security.  That's why NSA people were and are openly members of a bunch of standards committees, why people invited NSA guys to give talks and take part in competitions, why people were using stuff like SE Linux.  People have been using DSA, the NIST curves, SHA1, and SHA2 for many years, believing them secure--because the assumption was that NSA wasn't putting backdoored stuff out there.  That's part of the collateral damage of the dual ec drbg trapdoor.  They had spent 10-15 years trying to build a good relationship with the crypto and computer security community, and when this came out, they lost that relationship.  Researchers will still take their money, government agencies required by law to work with them will continue to do so, but the default assumption won't be "they're on our side" anymore.  The ultimate cost of that will be many times higher than however much was budgeted for the project that got the dual ec drbg into the world.  --John, definitely speaking only for myself

@_date: 2014-01-21 12:55:44
@_author: John Kelsey 
@_subject: [Cryptography] Auditing rngs 
It seems like it should be relatively straightforward to do a cut and choose style audit on a random bit generator.  However, the functionality you would need for this would also be a hell of an attack point, so it's a mixed bag.
Imagine you have an HSM that has its own entropy source.  We want to have it do something that requires randomness, say generate an RSA key.  So we do the following:
HSM:  a.  Generate two entropy strings from its hardware entropy source, each estimated to have 512 bits of entropy.  Call these E1 and E2.
b.  Output the SHA256 hash of each, call them H1 = hash(E1) and H2 = hash(E2). c.  Input two additional input strings, each estimated to have 256 bits of entropy--call them A1 and A2.
d.  Using HMAC DRBG, instantiate drbg1 from E1 with personalization string A1, and drbg2 from E2 with personalization string A2.  e.  Use each drbg to generate its own new RSA keypair.  Output keypair1 (from drbg1) and keypair2 (from drbg2).
f.  The user now knows H1, H2, A1, A2, keypair1, keypair2.  He challenges either keypair1 or keypair2.  Suppose he challenges keypair1.  Then the HSM reveals E1, and the user can redo all the computations needed to derive keypair1.  If he gets the same answer, he has some reason to trust keypair2.
You can imagine redoing this process many times to get more assurance.  However, it requires building a "leak the entropy input of my drbg" functionality into your HSM, which has some pretty obvious bad potential uses.  (Lots of security proofs have a call the attacker can make to compromise the secret state to model leakage, but normally you don't actually build that functionality into your module!)

@_date: 2014-01-21 16:33:31
@_author: John Kelsey 
@_subject: [Cryptography] Auditing rngs 
We're in the process of doing that.  But that doesn't give someone using the HSM any way to check that the internal RNG actually incorporated the additional input.  And what I'm describing here would probably not fit too well into a standard crypto module anyway, for several reasons.
Here's the problem I'm trying to solve:  From outside a device that's doing some crypto, it is impossible to determine whether the box is behaving according to spec or not w.r.t. a lot of crypto, because the box has secret values, like drbg states, keys, etc., inside which have to stay secret.  It's probably all but impossible to really be sure that a particular box is doing what you think it's doing, even with testing labs and code review and everything else we can throw at the problem.  If there is a hardware entropy source and a good drbg in the original design, an attacker can almost certainly tamper the design to produce random numbers that are predictable to an attacker.  Just chopping out all but 40 bits of the drbg's entropy would manage that nicely.  So, how can we get the crypto box to do what we want, and verify that it did what it was supposed to?  The best answer I can see is to do some kind of cut and choose protocol:  Have the box behave according to the spec two or more times, commit to its answers, and then reveal all but one of its internal sets of values.  So, we incorporate external input, and using this protocol, we can provide some evidence that the module incorporated that external input along with its own entropy string, and followed the spec it was supoosed to follow.  On the other hand, hardware security modules often handle public key operations, and having one generate a high-value keypair is pretty sensible. If you were going to generate a high value keypair for some system, you would probably want it generated inside dedicated crypto hardware to decrease the chances of it leaking.  This would not be something you would want in a general purpose crypto module at all.  Part of the protocol I am describing involves sending secret internal state values out of the module, which in general should just never ever be done.  But in this case, it's the best way I can see to prove to a skeptical user that the crypto module is actually doing what it is supposed to do.

@_date: 2014-01-21 16:36:30
@_author: John Kelsey 
@_subject: [Cryptography] Does PGP use sign-then-encrypt or 
Encrypt then sign has the big advantage that onthe receiving side, you can verify the signature before processing the ciphertext at all.  And that means you can avoid all kinds of chosen ciphertext attacks on your encryption mechanism, many of which are surprisingly effective.  (I'm thinking in terms of reaction attacks here--stuff where you mess up the last block of ciphertext, and learn something about the plaintext depending on whether your change messed up the block padding through CBC decryption.)

@_date: 2014-01-21 16:41:00
@_author: John Kelsey 
@_subject: [Cryptography] Auditing rngs 
If the HSM's entropy estimates are correct, or the additional input has as much entropy as is postulated, then the drbg gets to a secure starting point.  There isn't any test you can do on entropy source outputs that will guarantee that they have some claimed amount of entropy, so your complaint seems kinda unavoidable beyond that.

@_date: 2014-01-28 21:09:48
@_author: John Kelsey 
@_subject: [Cryptography] cheap sources of entropy 
On Jan 28, 2014, at 5:41 PM, Kriszti?n Pint?r  wrote
Unfortunately, pretty much all real-world systems have some time (often very soon after their first startup) when they have to generate some high value key.  To a first approximation, the only entropy estimate that really matters is the one used to decide whether there's enough entropy to generate that key.  We have worked examples of crypto libraries which don't bother making sure they have enough entropy (by reading /dev/random), but instead just draw a seed from /dev/urandom and hope for the best, so even getting your crypto libraries to bother to check if they have entropy is not trivial.  Fortuna is an elegant and clever solution to the wrong problem.

@_date: 2014-01-30 20:29:48
@_author: John Kelsey 
@_subject: [Cryptography] Hard Truths about the Hard Business of finding 
I very strongly disagree with this.  There is a tradeoff between purpose-built crypto hardware, and off-the-shelf computers and devices pressed into service to do crypto.  The purpose-built crypto hardware and software is a bigger target for very high end attackers, but it is also almost certain to be designed to be harder to tamper with in the field, and it's probably designed with security in mind to a far greater extent than general-purpose hardware and software.  Worse, if some commonplace software or hardware component becomes the thing everyone bases their entropy collection on, that will become a tempting point for a targeted attack, but the sound card manufacturer or whatever won't think they're primarily building a security product.  A dedicated crypto device can be designed to try to resist a lot of attacks that will pretty trivially compromise most off the shelf hardware and software devices, like side-channel attacks.  It normally will be resistant to compromise by someone who takes over the computer it's installed in or connected to.  It can have an entropy source that's purpose-designed and analyzed as an entropy source, reasonably resistant to intentional or accidental outside interference, etc.  For whatever it's worth, it can also be tested by some organization that validates hardware crypto devices.  Those validations all have problems, but they're probably better than no validation, which is the practical alternative.  How do you recognize when your source is no longer behaving according to the model you so carefully built of its behavior, if you aren't doing some kind of ongoing health testing?  That's something you are going to measure to try to build a model of your source.  But when someone else is trying to check to see if your model makes sense, they're probably going to do statistical testing.  Ideally, that would be carefully tuned to the best model of your source, but in reality, it will probably largely be off the shelf statistical tests, because that's what you can quickly lay your hands on, and expertise is expensive and rare.  Right.  The goal of your entropy source really needs to be to generate an impossible to guess seed for your PRNG, and then to periodically reseed it.  That means you can probably accept a relatively low rate of entropy produced per second, if you can know how much you are getting.

@_date: 2014-01-30 20:46:15
@_author: John Kelsey 
@_subject: [Cryptography] cheap sources of entropy 
Yes.  I think there is a combination of these two views that makes a lot of sense:  Have at least one entropy source that's either purpose-built (like the Intel source) or carefully designed and analyzed from off the shelf components (like Turbid).  And then, use that to seed your PRNG in a way that will be secure if that component's entropy estimate is correct.  But also fold in those outside sources, as much as you can get.  (Ideally, with some kind of entropy estimate so you can wait till you have 128 or 256 bits of entropy to start generating outputs.). Now, you get the combination that:
a.  If your purpose-built source is good, all is well.
b.  If not, your additional sources may still save the day.
One sideline of this:  If NSA or its Chinese equivalent slips a weakness into your purpose-built entropy source, they are unlikely to advertise this fact to the world.  So the entropy source may be quite secure to every attacker except them.  If they can't compromise your other sources of information, then you get secure random numbers, even if some other attacker knows those other sources.  (This is what I took to be James A Donald's point.). That's true, but I think this is in general a solvable problem.  It may be beyond our abilities to get a big, complicated software system that is impossible to compromise, but we should be able to get a good random number generator, in the same way we should be able to get strong encryption, signatures, etc.

@_date: 2014-01-31 19:26:56
@_author: John Kelsey 
@_subject: [Cryptography] Pre-image security of SHA-256 reduced to 16 
1.  Is the attack you care about finding a preimage, or inverting the
Just to define terms my terms:  Suppose I give you F(x).  If you can find
x, then you can invert the function.  If you can find *any* value y such
that F(x)=F(y), whether y=x or not, you're finding a preimage.  If you can
find that y, but you need F(x) and x to do it, and you have to find
y!=x, you're finding a second preimage.
If you care about making sure the function can't be inverted, then looking
at most preimage attacks on hash functions isn't too helpful, because
they're worried about solving a different (easier) problem than you care
about.  If you can invert functions,  you can find preimages, but not in
general the other way around.  It's really rare to see an inversion attack
on a hash function.
2.  Do you need a 256-bit wide state, or could you do with less?
Part of what makes SHA256 expensive is the need to process so many bits of
state and message.   It has to process 256 bits of hash state, and 512 bits
of message block for each compression function call.  That imposes a
big cost which you may not need to pay.  You are probably just hashing a
very small block over and over again, so it sucks to pay the cost of
processing 512 bits of input each time, especially if you really only need
to keep hashing (say) 128 bits of state.  Or do you need to process the
whole password string at each iteration?  (If so, you probably do need a
hash function.)
If you only need a function that can't be inverted over a smallish state,
you might want to look at block ciphers.  If you have a block cipher
E(k,x), you can get a pretty good one-way function from
F(k) = E(k,constant)
Finding k in this case equals recovering the key given a single
known plaintext.  So it's hard to invert if the block cipher is strong.
I suspect that AES with three rounds will give you more than 32 bits of
security against inversion attacks with a single known plaintext.
If you need preimage resistance, but you are looking at a fairly small
state, you might want to look at smaller variants of some hash functions.
 There are smaller Keccak variants defined.  The 200-bit permutation
version can give you more than 32 bits of preimage resistance with a
128-bit input size, and if you are only worried about a 32 bit security
level, you probably can get away with a lot fewer rounds--maybe 8 or 10.

@_date: 2014-07-09 11:18:56
@_author: John Kelsey 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
To the extent clearances do what they're supposed to do, they should indicate less risk of compromise to the project--less blackmail or bribery potential, for example.  An ongoing relationship with someone who wants to compromise the project (which could be NSA, or a US govt contractor, or another country, or a criminal organization, or ...) is a potential problem, but no one trying to infiltrate your project will tell you about those.  We have a kind of instinctive security notion of wanting to build a nice big wall with bad guys outside and good guys inside, but that doesn't really work too well.  Instead, we need processes that don't rely overmuch on any one person's integrity or competence.  (That protects against errors as well as malfeasance.)

@_date: 2014-07-12 08:29:06
@_author: John Kelsey 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
There isn't *one* enemy sitting in Ft Meade (or Mordor).  There are hundreds of potential enemies. Blackmail and bribery are generic techniques that can be used to compromise people--they can be used by the US government, foreign governments, private criminals, activists, *anyone*.  If the security clearance investigation excludes the people who would have been most susceptible to those techniques, then having passed it adds some value.  How much?  That, I don't know.  Maybe you should simply monitor packets coming from them to check if the evil bit is set?  If someone is a covert employee of the FBI on assignment to inflitrate your organization, they will not tell you about it.  Nor will they be the least bit worried about any court in the US upholding any kind of contractual obligation to tell you.  Similarly, if someone is under the thumb of the Chinese government thanks to those really revealing blackmail photos of their vacation in Thailand, they just aren't going to tell you who they are ultimately working for, because they *really* want to keep the guys holding those photos happy with them.  Federal employees have to disclose conflicts of interest--there is a yearly declaration involving your investments and arrangements and planned jobs and such.  I guess many companies do the same thing.  And this is worthwhile for what it gives you--it probably helps keep people from getting into situations where their personal interests and their job is in conflict.  But it doesn't keep them from having some covert interest which they have decided not to disclose.

@_date: 2014-07-14 16:51:46
@_author: John Kelsey 
@_subject: [Cryptography] VCAT report on NIST's process review 
The VCAT (one of our oversight committees) convened a panel of experts to look over our interactions with NSA in our past cryptographic standards, including Dual EC.  For those interested in the results and the materials posted, they can be found at

@_date: 2014-07-16 16:45:06
@_author: John Kelsey 
@_subject: [Cryptography] multi-key encryption of "meta" data 
(2) seems to me to make for a pretty unworkable system for normal people.  I think I could conceivably get my mom, dad, and sister to understand some specific security policy that was more-or-less global, but not to read through a security policy for every recipient they might want to send to.  The security policy that would make sense there would be something like:
a.  If it comes to you with my from: address through this system and shows up, it's really from me (or at least someone with my private key).
b.  Nothing sent through this system can be read by anyone along the way except the listed recipients.  c.  Nobody can determine that you are sending messages to me or receiving them from me by observing the communications in and out of the system.  More details should be available for those who want them, but don't require people to know or care about them to use the system securely.

@_date: 2014-07-16 16:48:02
@_author: John Kelsey 
@_subject: [Cryptography] [cryptography] hashes based on lots 
This is kind-of how I think about most of the clever new ways of building better password hashing functions.  There's often a lot of cleverness involved in making a provably memory-bound function, or a function that's got other nice guarantees that make it unlikely to be sped up much by using a bunch of graphics processors on your machine or something.  But ultimately, using passwords to derive cryptographic keys is such an ugly problem that even the best of these schemes are, as you said, a band-aid for a broken leg.

@_date: 2014-07-20 16:14:42
@_author: John Kelsey 
@_subject: [Cryptography] The role of the IETF in security of the 
I think there's a more fundamental point that touches both on this discussion and on the earlier one about volunteers with security clearances.  A huge amount of the important stuff going on in the world right now is built on volunteer efforts and is seriously short on money.  That's not just open source software--instead it ranges from discussion on the internet[1] to Wikipedia to standards groups to art and music to journalism.  That means that anyone with money can have an outsized effect on the world, by simply providing cash and hiring people to volunteer.  And anyone doing that can also influence what gets produced.  Volunteers provide what they want to provide, not what needs to be provided, and if you're letting your employees volunteer for something, you can definitely tell them they're not allowed to help with X, or are expected to help with Y in a way that moves what's being done in a desired direction[2].  The natural way to avoid this is to move in the direction of the folks who know a lot about infiltration and sabotage--the community of spooks and defense contractors and such who deal with classified information.  But that ends up with a lot of really wonderful stuff not getting done.  If the barrier to entry for working on an open source software project is a background check and an intrusive set of personal questions and financial disclosures, there won't be a hell of a lot of volunteers.  (And you'll have to wonder why the people who *did* volunteer were so interested in volunteering!)  My belief is that one of the things that makes really explosive improvements in the world to happen (like the internet) is that there aren't high barriers for new people to get involved.  The world's a better place when some nobody you've never heard of is allowed to hack together his own personal version of Unix, or invent a new programming language, or write a new editor, or do research in cryptography.  And that works when people can just get interested, start getting involved, and do useful and interesting and wonderful stuff.  That's pretty much the opposite of the kind of world we get if we go down the background check/security clearance path.  [1] IMO, most of the most insightful discussions going on on the net about politics, society, economics, science, etc., come from bloggers (and sometimes podcasters) who are, in general, not making a living producing those discussions.  At the high end, the participants are much smarter, and the very narrow ideological bounds of US media discussions don't apply.  At the low end, of course, the discussions are stupid, but you don't have to read those.  I think it's easy to find bloggers whose commentary is much, much smarter and more insightful than the editorial and op-ed pages of the New York Times or Washington Post, for example.  [2] Think about standards group participation for dozens of examples, ranging from getting your company's IP included in the standard to spiking a competitor's product by sticking a lot of painful extra steps into the standard.

@_date: 2014-07-20 16:30:19
@_author: John Kelsey 
@_subject: [Cryptography] multi-key encryption of "meta" data 
Imagine a completely trusted mail server used by everyone.  If you had such a thing, you could get what you want by having a protocol wherein each user connected once every day to the mail server over an encrypted channel (TLS), sent up a fixed amount of information, and pulled down a fixed amount of information.  No outsider would be able to determine whether you were sending/receiving any email--all they'd know would be that you *could* have sent/received email.  The two ways I can imagine making that work without a trusted mail server are either:
a.  Come up with a protocol so that the mail server doesn't know who got what.  (This looks hard to me--it's related to searching on encrypted data, but looks harder than that to me.)  b.  Replace the single mail server with some kind of network of nodes controlled by different entities.  What we know how to do right now is build a remailer network with some kind of longish delay, along with some kind of service that lets users drop information and chaff into/out of the system.

@_date: 2014-07-24 16:21:36
@_author: John Kelsey 
@_subject: [Cryptography] hard to trust all those root CAs 
Yeah, and the judge and prosecutor who get your case will be helpless before your clever skills at evading them, because they've never had to deal with literal-minded people trying transparent dodges to get around the law before.  You will doubtless enjoy the same success as tax protesters do when they end up in court.  And shortly thereafter, you'll enjoy an all-expenses-paid vacation with free room and board, courtesy of the US government.

@_date: 2014-07-28 12:07:31
@_author: John Kelsey 
@_subject: [Cryptography] propaganda on "hurdles for law enforcement" 
Yeah, Clipper didn't catch on during the Clinton administration, and post-Snowden, I would expect it to be much less popular.  On the other hand, mass surveillance is one of those issues that gets a very strong consensus among the people at the top, regardless of their political party--even if the public as a whole doesn't like the idea of mass surveillance, if both big parties' leadership supports it, who are you going to vote for, to register your disagreement?  (See also: the drug war, bailouts for large financial companies, aggressive foreign policy, impunity for the well-connected, etc.)  I don't think this is quite right.  Media focus on fear and outrage because they sell papers and draw eyeballs, and they're easy to do.  (Nothing's easier than drumming up outrage by example on the talking heads shows, which is why they do that when there's no other news.)  But people mostly don't live their lives like people scared to death of terrorists.  *Lots* of people fly in planes, travel overseas, go to big events like the Superbowl or the Boston Marathon, etc.  There's probably good poll data on this somewhere....

@_date: 2014-06-06 10:47:50
@_author: John Kelsey 
@_subject: [Cryptography] Is it mathematically provably impossible to 
First, I agree with you about warrant canaries.  I expect using one after receiving an NSL will get you locked up and charged, and the idea that maybe the supreme court will hear your appeal in a decade or so won't feed your family or get you out of prison in time to see your kids graduate high school.  (And since the powerful people in the US are about 99% lined up behind the surveillance state, it's not all that great a bet the SC will ever get around to hearing your appeal.)  And NSLs are the kindest form of persuasion you need to worry about.  Mobsters won't be concerned with any legal niceties at all. But putting the notaries in different countries, under different legal regimes, seems like a pretty good defense against both of these.  Will the governments of (say) Finland, Belgium, the US, Brazil, and India coordinate their legal pressure against the notary operators?

@_date: 2014-06-06 10:58:39
@_author: John Kelsey 
@_subject: [Cryptography] Is it mathematically provably impossible to 
This is the critical thing that is necessary.  Even if we somehow get the intelligence agencies and surveillance state in the US under some kind of control, that doesn't deal with the possibility of coercion or bribery against people running important services, by them (extralegally, but the intelligence services in the US seem to be largely above the law) or by private criminals or other governments.  So the services need to be designed to minimize the trust needed for their operators.  The ideal situation is that the operators of the service simply can't do very much harm without being caught--in that case, coercing or bribing them just doesn't pay off, and so it won't be done much.  This is actually one of the more frightening aspects of the NSA's policies.  There's this bizarre apparent assumption among NSA's defenders that they're going to be the only ones doing this stuff.  Instead, they're helping usher in a world where every government will actively be trying ot subvert security standards and software and coercing weakening of security in their own country.  And since the 800 lb gorilla did it first and was too rich and powerful to push back against, the precedent will be set.  The US will probably lose more than anyone else from this precedent, in the long run.  (As with the precedents we're setting w.r.t. drone warfare and targeted assassination.)  But it's a short-term winner in terms of increasing budgets, getting contracts, and getting elected, so we're probably going to keep doing it regardless of how bad an idea it ultimately is.

@_date: 2014-06-06 11:06:08
@_author: John Kelsey 
@_subject: [Cryptography] DOJ Wants to Expand Authority to Break Into 
Those companies do business in a great many countries.  Will each one be able to demand backdoors be inserted in all their software and keep quiet about it to all the others?  ISTM that this is the path to having almost no off the shelf products provide meaningful security.

@_date: 2014-06-09 12:50:49
@_author: John Kelsey 
@_subject: [Cryptography] Licensing OCB (RFC 7253) 
I thought the big issue with OCB was the overlapping patents that probably read on it--I think there were related patents filed by IBM (Jutla) and Gligor.  Again, I don't claim any expertise on this, but even if you license a patent from Rogaway, you still have to deal with the other patents that apply.
--John

@_date: 2014-06-09 13:06:08
@_author: John Kelsey 
@_subject: [Cryptography] Help investigate cell phone snooping by police 
Blocking the RF on the phone (say, wrapping it in aluminum foil while leaving it in the same room with you) doesn't actually solve the problem.  Most smartphones have a fair bit of memory they can use to record audio, so you may just force the malware on the phone to record the conversation now and send it up later.
Making sure the phone isn't in a position to eavesdrop on a conversation at all is a lot smarter.  The phone designers have already put a lot of effort into optimizing the performance of the phone as a microphone, so it seems like you could test how well things got picked up with various levels of sound shielding (like putting the phone into another room) by using the voice recorder and the speaker phone setting, and seeing if it can detect a noise somewhat louder than anyone's voice is likely to be through the padding.  I think a phone that was generically eavesdropping on you would massively shorten its battery life.  Processing voice and streaming it via the cell network is what phones are designed to do, and reception and battery life are two of the things the phones' designers focus on.  It's unlikely that a malware writer will get much better performance out of the phones than they can normally do for voice calls.  I wonder what the optimal strategy for widespread eavesdropping via smartphones is.  Even just listening and running some local pattern-matching for words of interest would probably have a noticable impact on battery life.  (Anyone have hard data on this?)  But given the willingness of the NSA to try to get at everything, it's interesting to ask what they might be able to do on a massive (non-targeted) scale with smartphones.

@_date: 2014-06-11 09:49:47
@_author: John Kelsey 
@_subject: [Cryptography] Bitcoin compute power (was Re: Aggregate 
Is this just a matter of fixed vs marginal cost?  (Once you have bought the rig, your marginal cost for continuing to mine coins is more or less the cost of power.)

@_date: 2014-06-18 09:44:14
@_author: John Kelsey 
@_subject: [Cryptography] What has Bitcoin achieved? 
Bitcoin in its current form isn't much of a dream for crypto anarchists, since traceability is built in and it takes a lot of effort to avoid it.  Zerocoin or some other replacement would fit better.  More generally, I think one of Ian's
points can be made more succinctly:  the claim that all governments are corrupt and evil is an easier sell in Venezuela than it is in Switzerland.

@_date: 2014-06-23 19:44:44
@_author: John Kelsey 
@_subject: [Cryptography] "Is FIPS 140-2 Actively harmful to software?" 
The first case looks like a genuine failure somewhere (or several somewheres) in the process.  But in the second case, I think you're looking at an inevitable consequence of validation--you can only validate what you see.  Let's imagine that there's some wonderful new open source crypto library, and you get a team of cryptographers and programmers to audit it as with the code audit of TrueCrypt that was going on, and they put out a statement that they checked it and it looks good.  That audit is only meaningful until the developers start changing the code.  A code audit of the current version of the library doesn't give anyone much assurance (or shouldn't) about later versions of the library.  If you want the assurance of the audit, you can't change the code!  I don't really see any way around that.  (Go ask current users of Skype about that.)  Similarly, if you have some validation process that just verifies that your crypto (hardware or software) really implements the algorithms it claims to implement, that validation isn't going to mean anything if you change your algorithms' implementations.  I don't see any way around this. The ideal situation w.r.t. a software validation would include a digital signature on the source code, right?  And then any change to the source code, or the part covered by the signature/audit, would automatically invalidate it.  You could imagine some ways to make extending the validation to include some changes more economical, and for all I know, maybe the FIPS labs do that.  But you've still got this problem:  If I can alter my code after it's been audited, I can add security-relevant bugs.  If that previous audit or validation or whatever meant anything, once I start changing my code, it doesn't mean much anymore.

@_date: 2014-06-23 20:49:32
@_author: John Kelsey 
@_subject: [Cryptography] Help please, 
There's a really fundamental problem somewhere in here.  People forget stuff.  They get old.  They die.  They get Alzheimer's.  They go crazy.  They disappear at sea.  And then, you need to decide who gets their information, or access to their accounts, or whatever.  Strong crypto gives us the means to decide who will have that access, but the simplest solution is nobody--once your password is not available, nobody can guess it, so nobody gets access to your encrypted drive.  Another simple solution is to define one TTP as an escrow agent, and let him decide who gets access.  And then, there's an incentive for all kinds of people to parasitize that system--policemen and spies looking to listen in on people (with or without tiresome hearings and warrants), lawyers (or their well-funded employers) wanting to do discovery, companies with effective lobbyists who want to get the rules for who gets access set up for their benefit, etc.  And yet, you ultimately need some kind of human judgment there.  A court that can declare me dead and give my wife and kids access to my encrypted data (or my account passwords) is also a court that can silently give the cops access to my encrypted data.  The current solution is that probably the NSA can access a lot of my encrypted data and can probably get access to any account I have in the US, but my family won't be able to get to it if I'm dead or incapacitated.  I suspect that the best that can be done here is to create a TTP (or network of TTPs) with the ability to grant some kind of escrow access, and force it to act in a public way.  But since probably every government in the world will instantly want to subvert that system, it's hard to see anyone trusting it much.

@_date: 2014-06-26 17:21:44
@_author: John Kelsey 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
There are a couple point I think are really important to think about if you're trying to get algorithm agility in a protocol:
a.  The security benefit of algorithm agility is entirely in being able to *stop* using some algorithms while still functioning.  This means that you only really get this benefit if you have at least two ciphersuites that are both SHALLs.  If everyone implements RC4 + CRC32 (the SHALL), and some people also implement AEC-GCM (the SHOULD), then when someone finally realizes that RC4+CRC32 is insecure, you can't actually get rid of it.  Indeed, version rollback attacks will work even on implementations that support AES-GCM, because they have to be able to interoperate with implementations that don't.  b.  A lot of the time, the "algorithm" that gets broken isn't a cryptographic primitive like AES, but rather some surrounding chaining mode or padding scheme or something.  This means that if there are two ciphersuites, they should be pretty different to minimize the overlap between them.  For example, you might do AES-GCM and 3DES-CBC+HMAC-SHA256.  The same idea applies to more ciphersuites.  c.  It's critical to make sure there's a simple way for a particular implementation to stop using a given algorithm, and that this is built into the protocol.  The ciphersuite negotiation probably needs to be really simple.

@_date: 2014-06-30 12:32:43
@_author: John Kelsey 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
I'm working on the assumption that we learn what's broken by seeing academic publications and demonstrated attacks.  So while we don't know today which ciphersuite will be broken, we expect to find out at some point in the future.  At that point, we can switch over to another ciphersuite, but only if that ciphersuite is also implemented in the field.  If it's an optional ciphersuite that is implemented by half the devices in the field, then we don't actually manage to switch over--indeed, we'll probably remain vulnerable to version rollback attacks since devices need to be able to communicate with other devices that only implemented RC4+CRC32.  Ideally, we should choose two ciphersuites that are rock solid and will remain so for decades to come--maybe AES256-GCM and Twofish256-CMAC or something.  You're assuming a somewhat different situation.  If we know that someone has attacked one of the ciphersuites, but don't know which, then we have an interesting problem.  I don't know how likely that is, though.  What we've seen in the past is that we know that there are problems with some fielded algorithms/ciphersuites, but we can't change because there are no alternatives in the field.  Some equipment froze DES or RC4 or MD5 in place, and it's very hard to get rid of it now.  That's a problem that we can address by having two mandatory algorithms defined, so we have something to switch over to.  The other problem is that people keep fielding crap that's been known to be flawed forever, because  they can make some elaborate argument that the currently-known weaknesses probably don't matter too much in this one case.  So people have kept fielding RC4 for years, despite all kinds of known statistical flaws in the keystream.  And similarly, people kept fielding DES, MD5, etc.  I think this is tightly bound up with this common failure mode in standards, where we end up including everything anyone has implemented.  So since a bunch of people are using something that's obviously unwise to keep using, that becomes part of the standard and gets propagated forward into the next decade of unwise use.  If you want to build one more complex algorithm instead of two alternatives, that's doable.  But you'll have a couple problems:  a.  There's a performance impact.  If you first encrypt each plaintext with 3DES in CBC-mode, and then superencrypt with AES in CTR mode, you have a lot more work per encrypted bit.  Probably that doesn't matter most places, but there are some places where it will matter.  b.  Some attacks (reaction attacks, compression side channels) work regardless of superencryption.  If someone does a reaction attack on your CBC-padding, superencrypting with CTR mode won't have any effect on the attack.  Now, we can probably avoid most of (b) by careful design.  But it's not so clear how to get away from (a).

@_date: 2014-06-30 12:41:36
@_author: John Kelsey 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
I'm working on the assumption that the knowledge that a given ciphersuite has problems will, in fact, get around and eventually lead to people being willing to turn off one of the ciphersuites, *if* there's a workable alternative.  I mean, there are a lot of ways this can go wrong, but right now, we see situations where people who listen to cryptographers (in the rare occasions when we mostly agree) still can't stop using stuff like RC4, because then they can't communicate with the rest of the world.  The main point I want to make:  Having one SHALL ciphersuite and lots of MAY or SHOULD ciphersuites doesn't really give you any reliable ability to recover if your SHALL ciphersuite is broken.  Having two or more SHALL ciphersuites gives you some chance that you can switch over to an unbroken ciphersuite.  More broadly, the kind of algorithm flexibility that matters for security is the ability to *stop* using an algorithm.  Adding Triple-Twofish-CMAC to 10% of the devices you communicate with doesn't do much good, if you can't stop using RC4+CRC32 because it's the only thing everyone implemented.

@_date: 2014-03-02 13:07:59
@_author: John Kelsey 
@_subject: [Cryptography] Are Tor hidden services really hidden? 
The Tor Users Get Routed paper does some pretty solid-looking calculations on how long it should take for a given attacker to deanonymize a given kind of user.  For resourceful attackers, it's a lot shorter time than you'd think.  I very much recommend the paper to everyone.
John Young has the paper up at --John Kelsey

@_date: 2014-03-02 13:22:00
@_author: John Kelsey 
@_subject: [Cryptography] Are Tor hidden services really hidden? 
Just as an aside, I strongly suspect the motivation to shut down online drug dealing and related stuff is:
a.  Rather weakly correlated with how much drugs are sold online via these services.
b.  Completely uncorrelated with externalities of selling the drugs online (which probably involves a lot less collateral damage to the world as a whole).
c.  Very strongly correlated with the amount of press coverage that the online drug dealing sites get.  Once bitcoin and anonymous online merchants got into the New York Times and onto CNN, some kind of response from the authorities was inevitable.  One lesson from the Users Get Routed paper was that an attacker willing to put significant resources in (significant from the perspective of a medium sized company; roundoff error in NSA's black budget) can probably deanonymize most Tor users within a few months.  Some of the leaked Snowden documents I've seen reported on have said that Tor was causing NSA/GCHQ problems.  If so, either they haven't yet gotten the resources allocated, or the attacks are a lot harder than the paper assumes.  Or it's disinformation put into a slide presentation that Snowden found.  Or it's out of date.  Or the people at NSA working on cracking Tor anonymity just weren't all that clever.  Or....
I think a fundamental problem which you see in Tor and Wikipedia and a lot of other wonderful bits of our world is that they're run on donated labor and resources.  That means that an attacker with pretty modest resources can have a huge impact on those projects, for good or ill, by just hiring people to volunter or donating resources.  --John

@_date: 2014-03-02 13:31:03
@_author: John Kelsey 
@_subject: [Cryptography] RAM memories as one source of entropy 
[Discussion of using off the shelf RAM chips to get entropy via uninitialized reads]
Amen!  These sources of "found" entropy seem like they can play a valuable role in providing some extra fallback security, but you can't really rely on them to seed your DRBG securely.  For that, you want something purpose-designed and analyzed.
...  Actually, it probably will.  It's not all that hard to get a fair bit of entropy.  The problem is designing a system that you *know* gives you a certain amount of entropy, and that scales so that millions of people can reliably use it, and it doesn't break the next time someone puts out a new OS revision or hardware version.

@_date: 2014-03-07 10:23:32
@_author: John Kelsey 
@_subject: [Cryptography] See??? Satoshi Nakamoto Smeared 
So the reporter found someone who might be the inventor of bitcoin, or might be a little crazy and just saying so, or might be sick of weirdos "tracking him down" because of his name and just be saying what he thinks will get the reporter to go away.  From what is in the article, how would we distinguish these possibilities?

@_date: 2014-03-07 10:28:05
@_author: John Kelsey 
@_subject: [Cryptography] XKCD on rubber-hose crypto 
The idea of beating answers out of someone is old enough that you don't have to cite its inventor.  Kings and bossmen of the ancient world whose names archaeologists don't even know had royal torturers on the payroll.

@_date: 2014-03-11 07:59:43
@_author: John Kelsey 
@_subject: [Cryptography] GnuTLS -- time to look at the diff. 
It seems like a more useful thing for the standards writers to do would be to produce a pretty comprehensive set of test cases (mostly things that should be rejected), and maybe offer a bounty on stuff that the protocol says should be rejected, but for which there is no test case exercising that bit of the code.  A nicer coding style or a more modern language won't do nearly so much to prevent problems as that will.

@_date: 2014-05-30 22:29:45
@_author: John Kelsey 
@_subject: [Cryptography] FW: RFC 7253 on The OCB Authenticated-Encryption 
I am certainly not an expert, but I was under the impression that there were patents by several different people/organizations that probably applied to OCB.

@_date: 2014-05-31 17:48:27
@_author: John Kelsey 
@_subject: [Cryptography] FW: RFC 7253 on The OCB Authenticated-Encryption 
Anyone know how OCB does when the user reuses a nonce?  That's a particular problem for GCM.

@_date: 2014-10-29 16:23:30
@_author: John Kelsey 
@_subject: [Cryptography] Best internet crypto clock 
You can solve one end of this problem with beacons--nobody could have known this information before this time.  You can do the same thing with public information that's unpredictable, like the complete contents of the New York Times front page, or today's sports scores or stock prices.
You can use a digital timestamping service to solve the other end--this information had to be available by this time.
I don't know about the kidnapping scenario, but consider some program that takes an RNG seed, or some experiment which requires some random inputs.  I use the beacon values for today at noon to run the experiment, and as soon as I have the results, I get them digitally timestamped--say, at 1PM today.  This binds the experiment in time--it can't have happened before noon today or after 1 PM today.

@_date: 2014-09-10 17:05:14
@_author: John Kelsey 
@_subject: [Cryptography] distributing fingerprints etc. via QR codes etc. 
Making it easy to go from "I am standing with this person I know" to "We have exchanged public keys" seems pretty useful in many situations, and that's something it looks like you can do with QR codes.  A useful hack, IMO, would be something that let me take someone's picture and a picture of their PK (via a QR code), so that email from them would bring up the right picture.

@_date: 2014-09-18 09:02:31
@_author: John Kelsey 
@_subject: [Cryptography] Simple non-invertible function? 
The simplest one way function I can think of is
F(x) = E(x,0)
That is, use the input as a block cipher key and encrypt a counter with it.  You could use a stream cipher in a similar way--use the input to your one way function as a key and generate n bits of key stream.  Neither of these are super lightweight, but they do give you one way functions whose security comes from the underlying crypto primitive.

@_date: 2015-08-05 09:16:39
@_author: John Kelsey 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
Rule of thumb:  Suppose you have two systems.  One runs along perfectly nearly all the time.  The other just barely works in the best of times and often breaks down under its own weight.  It's pretty easy to tell when the first system has been sabotaged, but really hard to tell when the second one has been sabotaged.  Most of our mechanisms for developing crypto standards are more like the second system than the first.

@_date: 2015-09-11 17:06:02
@_author: John Kelsey 
@_subject: [Cryptography] millions of Ashley Madison bcrypt hashes cracked 
I wonder how that ratio (90%+ of the women there were fake) compares with
other dating sites.  My uninformed guess is that it's probably comparable
to other sites.

@_date: 2016-09-01 09:40:43
@_author: crypto.jmk@gmail.com 
@_subject: [Cryptography] Key meshing (Re: [Crypto-practicum] Retire all 
The idea of altering the key as part of the chaining mode is more-or-less what a tweakable block cipher buys you--there is effectively a part of the key that is assumed to be known or chosen by the attacker, and that is designed to be changed quickly at minimal cost.

@_date: 2019-08-03 10:06:53
@_author: John Kelsey 
@_subject: [Cryptography] Software proposal for verifiable stateless 
Maybe this is a dumb question (I read your email but not the paper), but if nobody verifies the state chains, how does anyone rely on the computation results?  Why cant a mining node just make up incorrect results?

@_date: 2019-05-14 19:41:36
@_author: John Kelsey 
@_subject: [Cryptography] Dieharder & /dev/urandom 
The practical issue with /dev/urandom is that its never allowed to block, so in some extreme circumstances you could be getting output bits even though the system hasnt managed to collect any entropy.  This was apparently behind the finding a few years back of a bunch of appliance routers and firewalls whose RSA keys shared primes.  (This demonstrates a disastrous lack of entropy!) Note the the statistics of those systems /dev/urandom outputs would have been fine if checkedthe problem was only visible when you looked at many different machines outputs.

@_date: 2020-02-15 21:09:47
@_author: John Kelsey 
@_subject: [Cryptography] SSL Certificates are expiring... 
In security, "trusted" should be translated as "can screw me over."

@_date: 2020-01-29 16:30:36
@_author: John Kelsey 
@_subject: [Cryptography] Proper Entropy Source 
You need at least an approximate probability distribution for your source, based on a physical understanding of your source's behavior, to be able to make a sensible entropy estimate.  (Entropy isn't a property of a bitstring, it's a property of the process that generated it, so you need to understand that process.)  Given that model, you can find statistical tests that are great at estimating entropy.  But without the model all a black box estimator can do is give you an upper bound.
However, this kind of model is basically impossible for operating system sources--for those, you can make pretty plausible arguments that there is stuff no attacker can guess in there somewhere, but you can't get any kind of nice probability estimates based on a physical understanding because the source is too complicated to model well. The best you cam do is make some plausible bounds on an attacker's ability to guess things.
