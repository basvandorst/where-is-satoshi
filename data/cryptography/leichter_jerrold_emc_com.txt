
@_date: 2005-12-01 10:05:37
@_author: leichter_jerrold@emc.com 
@_subject: Broken SSL domain name trust model 
One can look at this in more general terms.  For validation to mean
what's validated has to be the semantically meaningful data - not some
incidental aspect of the transaction.  The SSL model was based on the
assumption that the URL was semantically meaningful, and further that any
other semantically meaningful data was irreversibly bound to it, so that if
the URL were valid, anything you read using that URL could also be assumed
to be equally valid.
This fails today in (at least) two different ways.  First, as you point out,
URL's are simply not semantically meaningful any more.  They are way too
complex, and they're used in ways nothing like what was envisioned when SSL
was designed.  In another dimension, things like cache poisoning attacks
lead to a situationd in which, even if the URL is valid, the information
you actually get when you try to use it may not be the information that was
thought to be irreversibly bound to it.
Perhaps the right thing to do is to go back to basics.  First off, there's
your observation that for payment systems, certificates have become a
solution in search of a problem:  If you can assume you have on-line access
- and today you can - then a certificate adds nothing but overhead.
The SSL certificate model is, I contend, getting to pretty much the same
state.  Who cares if you can validate a signature using entirely off-line
data?  You have to be on-line to have any need to do such a validation, and
you form so many connections to so many sites that another one to do a
validation would be lost in the noise anyway.
Imagine an entirely different model.  First off, we separate encryption
from authentication.  Many pages have absolutely no need for encryption
anyway.  Deliver them in the clear.  To validate them, do a secure hash,
and look up the secure hash in an on-line registry which returns to you
the "registered owner" of that page.  Consider the page valid if the
registered owner is who it ought to be.  What's a registered owner?  It
could be the URL (which you never have to see - the software will take
care of that).  It could be a company name, which you *do* see:  Use a
Trustbar-like mechanism in which the company name appears as metadata
which can be (a) checked against the registry; (b) displayed in some non-
alterable form.
The registry can also provide the public key of the registered owner, for
if you need to establish an encrypted session.  Also, for dynamically
pages - which can't be checked in the registry - you can use the public key
send a signed hash value along with a page.
Notice that a phisher can exactly duplicate a page on his own site, and it
well end up being considered valid - but he can't change the links, and he
can't change the public key.  So all he's done is provide another way to get
to the legitimate site.
The hash registries now obviously play a central role.  However, there are a
relatively small number of them and this is all they do.  So the SSL model
should work well for them:  They can be *designed* to match the original

@_date: 2005-12-02 15:03:20
@_author: leichter_jerrold@emc.com 
@_subject: Proving the randomness of a random number generator? 
It can't be *proved*, for any significant sense of that word, regardless of the availability of resources.  At best, you can - if you are lucky - prove *non-randomness*.  In practice, one makes attempts to prove non-randomness and, if "enough" of those fail - "enough" being determined by available resources - one just asserts randomness.
There are basically two kinds of tests one can do:

@_date: 2005-12-04 17:51:11
@_author: leichter_jerrold@emc.com 
@_subject: [Clips] Banks Seek Better Online-Security Tools  
Until a couple of months ago, I avoided doing anything of this sort at all.
Simple reasoning:  If I know I never do any financial stuff on-line, I can
safely delete any message from a bank or other financial institution.
Now, I pay some large bills - mortgage, credit cards - on line.  I just got
tired of the ever-increasing penalties for being even a day late in paying -
coupled with ever-more-unpredictable post office delivery times.  (Then
who can really say when the letter arrived at the credit card company?  You
have to accept their word for it, and they have every incentive to err in their own favor.)
I have consistently refused on-line delivery of statements, automated
paying, or anything of that sort.  I cannot at this point forsee a world in which I would trust these systems enough to willingly move in that direction.  (It
doesn't help that, for example, one credit-card site I use - AT&T Universal
sends an "invalid" certificate.  AT&T Universal has its own URL, but they
are owned by Citibank, so use the citibank.com certificate....)
Of course, increasingly one has little choice.  My employer doesn't provide
an option:  Pay "stubs" are on-line only.  Reimbursment reports likewise.
There are increasing hints of various "benefits" if you use the on-line
systems for banking and credit cards and such.  The next step - it won't
be long - will be charges for using the old paper systems.  How many people here still ask for paper airline tickets?  (I gave up on this one....)

@_date: 2005-12-05 11:49:41
@_author: leichter_jerrold@emc.com 
@_subject: Proving the randomness of a random number generator? 
That's not a definition of randomness except in terms of itself.  What does
"independent samples" mean?  For that matter, what's a "sample"?  It's an
element chosen at random from a sample space, no?
"All outcomes equally likely" is again simply a synonym:  "Equally likely"
comes down to "any of them could come out, and the one that does is chosen
at random".
Probability theory isn't going to help you here.  It takes the notion of
randomness as a starting point, not something to define - because you really
can't!  Randomness is defined by its properties within the theory; it
doesn't need anything else.
One can, in fact, argue plausibly that randomness doesn't "really" exist:  It's simply a reflection of lack of knowledge.  Even if you get down to the level of quantum mechanics, it's not so much that when an atom decays is random, it's that we don't - and, in fact, perhaps *can't* - have the knowledge of when that decay will happen ahead of time.  Once the decay has occurred, all the apparent randomness disappears.  If it was "real", where
did it go?  (It's easy to see where our *ignorance* went....)

@_date: 2005-12-08 09:50:17
@_author: leichter_jerrold@emc.com 
@_subject: Malicious chat bots 
[From Computerworld - see
               Security firm detects IM bot that chats with you
               Bot replies with messages such as 'lol no its
               not its a virus'
               News Story by Nancy Gohring
               DECEMBER 07, 2005
               (IDG NEWS SERVICE) - A
               new form of malicious instant-message bot is on the loose
               that talks back to the user, possibly signifying a
               potentially dangerous trend, an instant messaging security
               firm said.
               IMlogic Inc. issued the warning late yesterday after
               citing a recent example of such a malicious bot. On
               Monday, the company first published details of a new
               threat known as IM.Myspace04.AIM. Once the computer of an
               America Online Inc. IM user is infected, the bot sends
               messages to people on the infected user's buddy list,
               making the messages appear to come from the infected user.
               The user isn't aware that the messages are being sent. If
               recipients click on a URL sent with a message, they will
               also become infected and start spreading the virus.
               A bot is a program that can automatically interact with
               people or other programs. AOL, for example, has bots that
               let users ask questions via IM, such as directory queries,
               and the bot responds.
               The unusual part of this malicious bot is that it replies
               to messages. If a recipient responds after the initial
               message, the bot replies with messages such as "lol no its
               not its a virus" and "lol thats cool." Because the bot
               mimics a live user interaction, it could increase
               infection rates, IMlogic said.
               IMlogic continues to analyze this threat but so far it
               seems to only be propagating and not otherwise affecting
               users.
               An AOL spokesman said today that the company's IT staff
               has not yet seen the bot appear on its network. The
               company said it reminds its users not to click on links
               inside IM messages unless the user can confirm that he
               knows the sender and what is being sent.
               Some similar IM worms install spybots or keyloggers onto
               users' computers, said Sean Doherty, IMlogic's director of
               services in Europe, the Middle East and Africa. Such
               malicious programs record key strokes or other user
               activity in an effort to discover user passwords or other
               information.
               "What we're seeing with some of these worms is they vary
               quickly, so the initial one may be a probe to see how well
               it infected users, and then a later variant will be one
               that may put a spybot out," Doherty said. The initial worm
               could be essentially a proof of concept coming from the
               malware writers, he said.
               Computerworld staff writer Todd Weiss contributed to this
               article.

@_date: 2005-12-08 12:19:52
@_author: leichter_jerrold@emc.com 
@_subject: [Clips] Banks Seek Better Online-Security Tools  
Be aware that when you authorize direct deposit to your account, you are
also implicitly authorizing "direct withdrawal".  I found this out many
years ago when an employer accidentally issued paychecks for too much money.
My next bank statement showed the deposit, followed a day later by a
withdrawal to get back to the correct value.
Nothing on any direct deposit authorization form seems to mention this, and
I know of no way to block it - the authorizations are a unit, you can't
agree to one without the other.
Of course, first with check truncation by large institutions, and now
Check 21, the line between paper checks and electronic withdrawals has
rather difficult to define.  In theory, you do have the same recourse with
electronically transfered checks (Check 21) that you did with paper ones.
In practice, the copy you receive doesn't normally grant you that level of
recourse - you need an official copy (I forget the actual term in the law),
and unless you know to ask for it, your bank won't give it to you.
Check truncation - where you send a check to a credit card company, say, and
it turns it into a direct withdrawal, so you don't even get a copy of the
check back - is even more problematic.  If, as has happened to me more than
once, they misread the value on the check (typically, the error is to forget
to add .00), yes, the same amount is credited to your card bill as is
from your bank account - but you could be hit up for interest and various
penalties.  The only proof of what the check really said is in the hands of
the credit card company - and I'm not even sure what their obligations are
terms of retaining the image and making it available to you.
I wonder if these new processes have given the various financial
what they wanted, but could never get the courts to agree to, eliminate for
years:  Effectively destroying the legal enforceability of a mark of "In
payment" on a check.*
* If there is a pre-existing dispute between you and another party about
what you owe them, and you give them a check for the amount you claim they
owe that is marked "In Full Payment", if they cash it, they have legally
agreed that that check settles the dispute.  I've only had to use this once,
years back, when a landlord had for months "not gotten 'round to" paying me
a referral fee:  I took the referral fee out of my next month's rent and
marked it "In Full Payment".  I pointed this out to them, because I didn't
really want to go to court about this issue!  They refused to cash the
but by an amazing coincidence delivered my referral check a day later and
then asked me to replace the rent check.  This right remains there - but if
you can't get your hands on the check, it's very difficult to enforce!

@_date: 2005-12-12 14:03:26
@_author: leichter_jerrold@emc.com 
@_subject: crypto for the average programmer 
I can tell you a situation that applied in one system I worked on:  You
could go with SSL, which gets you into GPL'ed code, not to mention the known
complexities of using the SSL libraries correctly (steep learning curve); or
we could go commercial code that had fairly steep license fees.  The
decision was to use off-the-shelf where completely unencumbered (e.g., Gladman's AES implementation), build the rest ourselves.
BTW, there are other issues with SSL.  We needed to fit this implementation
in to an already-running system with minimal effect - but try to get people to use it.  Having to get certificates for SSL was a big hurdle.  Even creating
self-signed certs was a hassle.  The existing code ran directly over TCP,
and assumed a byte stream.  SSL is record-oriented.  This shows up, for example,
when your 1-byte ACK (of which we send many) turns into a 32-byte block (or even larger).
We weren't interested in "complete" security - we just needed to raise the level considerably.  Given the nature of the application, message authentication was not *that* big a deal - it could be put off.
SSL is a fine protocol, and on theoretical terms, yes, you probably want everything it provides.  But in practice it's too much.
BTW, there are some interesting "social" issues.  Before we implemented our own crypto layer, we recommended people go through ssh tunnels.  The product
was set up to allow that.  I made the argument "Do you really want us to provide your crypto?  We're not crypto experts.  But this was perceived as clunky, complicated ... it didn't make it look as if the *product*.  Those factors were ultimately seen as more important than the very highest level
of security.  You can *still* use ssh, of course....  (In fact, I was in a discussion with a military contractor who wanted to use the product.  The question came up of exactly how our crypto worked, whether it would be approvable for their application, etc.  My comment was:  Isn't NSA providing
you guys with encrypted links anyway?  Answer - sure, you're right; we don't
need to do application-level encryption.  If IPSEC were actually out there,
all sorts of nasty issues would just magically go away.)

@_date: 2005-12-18 15:54:24
@_author: leichter_jerrold@emc.com 
@_subject: browser vendors and CAs agreeing on high-assurance certificat 
...which raises the interesting question of whether there is a role here for
banks in their traditional role:  As introducers and trusted third parties.
Imagine a "E-commerce" front end:  Instead of little-guy.com buying a cert
which you are supposed to trust, they go to e-commerce.com and pay for a
link.  Everyone trusts e-commerce.com and its cert.  e-commerce provides a
guarantee of some sort to customers who go through it, and charges the
guys for the right.
Yup.  This is the role E-commerce.com would play.
Since e-commerce.com would actually be present in the transaction - as
to a distant cert authority - in principle it could charge in a way that
sense.  If it's mitigating risk, the cost should be proportional to the risk
i.e., the size of the transaction and what e-commerce knows about little-guy
and its history.

@_date: 2005-12-19 11:18:08
@_author: leichter_jerrold@emc.com 
@_subject: browser vendors and CAs agreeing on high-assurance certificat 
Well, yes, and eBay provides the same service.  But how much protection are
they providing for buyers?  I think Amazon will cover the first $100 a
customer paid.  eBay gives you a bit of protection if you go with PayPal,
but not a whole load - they rely on their reputation system.
e-commerce.com would bring up a page saying:  "We guarantee that
up to $nnn with this site will be to your satisfaction or your money back".
The merchant would specify the maximum dollar value, and pay e-commerce.com
based on the limit and, presumably, his reputation with e-commerce.  (This is one way it might be set up - there are certainly other ways.  And, even
in this style, the entire wording of the guarantee would be something agreed
upon between the seller and e-commerce.

@_date: 2005-12-23 12:00:49
@_author: leichter_jerrold@emc.com 
@_subject: browser vendors and CAs agreeing on high-assurance certificat 
are we
mail had the your BTW, illustrating points made here, the cert is for
but your link was to   So of course Firefox
generated a warning....

@_date: 2005-11-16 12:26:52
@_author: leichter_jerrold@emc.com 
@_subject: the effects of a spy 
Remember Clipper?  It had an NSA-designed 80-bit encryption algorithm.  One
interesting fact about it was that it appeared to be very aggressively
designed.  Most published algorithms will, for example, use (say) 5 rounds
beyond the point where differential cryptoanalysis stops giving you an
advantage.  Clipper, on the other hand, falls to differential cryptoanalysis
if you use even one less round than the specification calls for.
Why the NSA would design something so close to the edge has always been a
of a mystery (well, to me anyway).  One interpretation is that NSA simply
has a deeper understanding than outsiders of where the limits really are.
What to us looks like aggressive design, to them is reasonable and even
Or maybe ... the reasoning Perry mentions above applies here.  Any time you
field a system, there is a possibility that your opponents will get hold of
it.  In the case of Clipper, where the algorithm was intended to be
there's no "possibility" about it.  So why make it any stronger than you
Note that it still bespeaks a great deal of confidence in your understanding
of the design to skate *that* close to the edge.  One hopes that confidence
actually justified for cryptosystems:  It turned out, on the key escrow side
of the protocol design, NSA actually fell over the edge, and there was a
simple attack (Matt Blaze's work, as I recall).

@_date: 2005-11-17 11:07:17
@_author: leichter_jerrold@emc.com 
@_subject: timing attack countermeasures (nonrandom but unpredictable de 
Why do you need to separate f from f+d?  The attack is based on a timing variation that is a function of k and x, that's all.  Think of it this way:
Your implementation with the new d(k,x) added in is indistinguishable, in
externally visible behavior, from a *different* implementation f'(k,x)
which has the undesired property:  That the time is a function of the
Any attack that works against such an implementation works against yours.
Now, your model is actually general enough to allow for effective d(k,x)'s.
For example, suppose that d(k,x) = C - f(k,x), for some constant C.  Then
t(k,x) is just C - i.e., the computation is constant-time.
One can generalize this a bit:  f(k,x) in any real application isn't going
to have a unique value for every possible (k,x) pair (or even for every
possible x for fixed k, or k for fixed x).  Even if this were true in a theoretical sense, you couldn't possibly measure it finely enough.  The real attack
arises because of a combination of things:  f(k,x) is actually a function or k and
x (or can be made so by averaging); the size of f's range is significant fraction of the size of the domain of k, x, or (k,x), depending on what you are attacking; and, finally, that the inverses images of the elements of f's
range are fairly even in size.  These all arise because the nature of the attack is to use f(k,x) to determine that k (or x or (k,x)) is actually a member of some subset of the range of k (or ...), namely, the inverse image
of the observed value under f.  (The need for the last one can be seen by considering a function that sends f(0,x) to x and every other pair of values
to 1.  Then it's easy to attack the 0 key by computing the timing, but no information about any other key can be gained by timing attacks.)
If we think of your d() function as a "compensation function", then
is an "ideal" compensation function, which it may be impractical to use.
(The ideal compensation function is always available *in principle* because
we can set C = max over k,x f(k,x), compute naturally, then "compute d(k,x)"
by looking at the time elapsed for the function we just finished and delay
for C less that value.)  However, the analysis above shows that there may
be other useful compensation functions which, while they can't by their
nature provide the degree of security of the ideal compensation function, may
still be effective.  For example, suppose I have several different ways to compute the function to be protected, with differing timing characteristics;
but it's certain that for no input values to all the calculations take the maximum amount of time.  If I run all the algorithms in parallel and deliver
the first result that is available, I've reduced the range of f by
eliminating some of the largest values.  (Of course, one has to get the details right!)

@_date: 2005-11-22 07:05:04
@_author: leichter_jerrold@emc.com 
@_subject: timing attack countermeasures (nonrandom but unpredictable de 
Why would it matter?  None of the attacks depend on inverting f in any analytical sense.  They depend on making observations.  The assumption is
not that f is invertible, it's that it's countinous in some rough sense.
Well, yes ... but the point is to characterize such functions in some useful
way other than "they don't leak".  I suppose if d(k,x) were to be computed
as D(SHA1(k | x)) for some function D, timing information would be lost (assuming that your computation of SHA1 didn't leak!); but that's a very expensive way to do things:  SHA1 isn't all that much cheaper to compute
than an actual encryption.
Agreed.  The problem is to (a) characterize those properties; (b) attain
them at acceptable cost.

@_date: 2005-10-25 14:58:22
@_author: leichter_jerrold@emc.com 
@_subject: [fc-discuss] Financial Cryptography Update: On Digital Cash-l 
This is incorrect.  The law draws a distinction between recognized sellers of the good in question, and other sellers.  If you buy a washer from a guy who comes up to you and offers you a great deal on something from the back
of his truck, and it turns out to be stolen, you lose.  If you go to an
appliance store and buy a washer that turned out to be stolen, it's yours.  Buy a gold
ring from the salesman at the same store, and you better hope he didn't
steal As in any real-world situation, there are fuzzy areas at the edges; and
there are exceptions.  (Some more expensive objects transfer by title - mainly houses and cars.  You don't get any claim on the object unless you have a state-issued title.)  But the general intent is clear and reasonable.
This is no different from the case with cash today.  If there is a way to prove - in the legal sense, not some abstract mathematical sense - that a transfer took place, the legal system may reverse it.  This comes up in contexts like improper transfers of assets before a bankruptcy declaration,
or when people try to hide money during a divorce.

@_date: 2005-10-25 15:15:25
@_author: leichter_jerrold@emc.com 
@_subject: semi-preditcable OTPs 
To get perfect security in a OTP system, you need to add as much
equivocation from the keystream as is being removed by the plaintext.  It's generally calculated that each letter in English text adds between 2 and 3 bits of information.  Hence you only need to add 3 or so bits of randomness from
each key input to make the system secure.  Even with the biases, there was
probably easily enough randomness in the OTP's to make recovery at least impractical
(e.g., information leaks but so slowly that you never see enough input to
get any useful decryptions) and perhaps even be theoretically impossible.

@_date: 2006-04-27 12:07:20
@_author: leichter_jerrold@emc.com 
@_subject: VoIP and phishing 
New phishing scam model leverages VoIP
Novelty of dialing a phone number lures in the unwary
      News Story by Cara Garretson
APRIL 26, 2006
(NETWORK WORLD) - Small businesses and consumers aren't the only ones
enjoying the cost savings of switching to voice over IP
(VoIP). According to messaging security company Cloudmark Inc., phishers
have begun using the technology to help them steal personal and
financial information over the phone.
Earlier this month, San Francisco-based Cloudmark trapped an e-mailed
phishing attack in its security filters that appeared to come from a
small bank in a big city and directed recipients to verify their account
information by dialing a certain phone number. The Cloudmark user who
received the e-mail and alerted the company knew it was a phishing scam
because he's not a customer of this bank.
Usually phishing scams are e-mail messages that direct unwitting
recipients to a Web site where they're tricked into giving up their
personal or financial information. But because much of the public is
learning not to visit the Web sites these messages try to direct them
to, phishers believe asking recipients to dial a phone number instead is
novel enough that people will do it, says Adam O'Donnell, senior
research scientist at Cloudmark.
And that's where VoIP comes in. By simply acquiring a VoIP account,
associating it with a phone number and backing it up with an interactive
voice-recognition system and free PBX software running on a cheap PC,
phishers can build phone systems that appear as elaborate as those used
by banks, O'Donnell says. "They're leveraging the same economies that
make VoIP attractive for small businesses," he says.
Cloudmark has no proof that the phishing e-mail it snagged was using a
VoIP system, but O'Donnell says it's the only way that staging such an
attack could make economic sense for the phisher.
The company expects to see more of this new form of phishing. Once a
phished e-mail with a phone number is identified, Cloudmark's security
network can filter inbound e-mail messages and block those that contain
the number, says O'Donnell.
 							-- Jerry

@_date: 2006-04-27 16:58:43
@_author: leichter_jerrold@emc.com 
@_subject: VoIP and phishing 
To open a trouble ticket with IT where I work, you go to a Web page; or,
if you have problems using the network, you can use the phone.  When the
phone is replaced by one that use VoIP, just how will one report network
outages?  I can't wait....
The last I heard, it was fairly easy to *suppress* ANI (using games that
redirected calls the network saw as going to toll-free numbers), but
still difficult to *spoof* it.  Since ANI drives Telco billing - unlike
Caller ID, which is simply delivered to customers - the Telco's have an
interest in making it difficult to fake.  On the other hand, LD revenues
have been falling for years, so the funding to attack LD fraud has
probably been falling, too - given how many people now have "all you
can eat" plans, there's less and less reason to worry about them
I probably get an offer to refinance my mortgage every other week or
so.  The letters cite real information about me and my mortgage:  They
know its size, or at least the know the amount at the time I took out
the mortgage.
In low-income areas, there's a long history of fraudulent refinancing -
claiming you are getting a better loan for the person but really getting
him deeper and deeper in the hole while you pocket various fees.  I
wouldn't want bet that all the come-on letters I receive are legitimate!
The only difference between some of this stuff and phishing is the
medium used.

@_date: 2006-08-30 10:30:42
@_author: Leichter, Jerry 
@_subject: CRCs and passphrase hashing 
Look up the paper "Fingerprinting by random polynomials" by Michael Rabin.

@_date: 2006-08-30 10:21:21
@_author: Leichter, Jerry 
@_subject: skype not so anonymous... 
...maybe.  This article gets many fundamental details wrong.  For
one thing, Alexander wasn't "nabbed" - the very article they linked
that word to simply says he was "found".  But even ignoring that,
more recent newspaper articles leave all kinds of things unclear.
What we have is a publicity-seeking (not just in this instance, he
has a history of this kind of thing) private detective who made some
unverified (as of news reports 2 days ago, anyway) claims about having
found and seen Alexander in Sri Lanka.  If I remember the stories
correctly, the PI said *something* about Skype, the reporters asked
him if he'd tracked Alexander down through his use of Skype, and
the PI never quite answered.
Whether Skype is anonymous or not, I have no clue.  But this article
gives no useful evidence one way or another.

@_date: 2006-12-11 10:58:13
@_author: Leichter, Jerry 
@_subject: cellphones as room bugs 
It's tough to make any definitive statements, but note that there are
phones that hold a couple of hundred MP3-compressed songs.
Understandable speech takes much less than that.  (As I recall,
8Kbit/second is enough if all you need is to understand what is being
said, not recognize the speaker.  The processing power to do this is
pretty small on today's scale of things.)
If I were doing this, I'd transmit under two conditions:  A really close
cell tower (which allows you to crank the transmit power way down -
something phones do anyway) and, even better, while recharging.  The
latter would be particularly pernicious:  If you wait a couple of minutes
after the phone goes into the charger, it's highly unlikely anyone will
be looking at the phone, you can transmit without draining the battery,
and on most phones you won't even affect the charge time by all that
much - not that the victim is likely to notice, since most people have
no idea how long it actually takes to charge their phone:  They stick
it into the charger at some convenient time, and pull it out at some
later convenient time.
Another advantage the attacker has in this scenario is that he can
transmit when he can get away with it and reassemble the pieces at
leisure.  A normal phone conversation has to be done in one long
stretch, which forces the phone to continue to receive in and transmit
even when conditions are highly unfavorable.  You could combine with
with lower-than-normal transmit power, on the assumption that the
receiver could request a resend to fix up garbled data.

@_date: 2006-12-22 15:32:07
@_author: Leichter, Jerry 
@_subject: How important is FIPS 140-2 Level 1 cert? 
I think this was changed as FIPS 140 evolved.  Several things about the
random number generator evolved.  For example, in earlier versions, you
had to run some tests on your generator at every startup.  That
disappeared by FIPS 140-2.  (It makes sense for a hardware generator,
but never did for software.)
I don't have the actual text handy now, but to the best of my
recollection, there are now two approved PRNG's you can use, and the
way the text is written, what's mainly important is that you run the
internal state through the PRNG before exporting it.  You're definitely
free to set the starting state using any source of entropy you like.
I *think* you can add extra entropy along the way; though even if this
were not allowed, you could probably declare that you were restarting.
(Of course, this might allow the silly implementation that restarts
with state 0 on every call.  There's enough leaway in the wording of
the standard to allow a lab to toss out such a thing.)
This could happen, but probably not because of a disagreement between
the labs as such:  The interpretation of the standard changes over time.
In fact, it's the interpretations - which only insiders really get to
learn about - that really define what the thing means; the written
standard leaves way too much open, most especially for software.  (It's
reasonably clear what it means to isolate hardware - though beyond that
there are some pretty specific discussions of potting technology and
such - but isolation for software?  That whole area has been defined
by interpretation.)
For what it's worth, I've been involved in (parts of, never worked
through the whole process) both FIPS 140 and Common Criteria
validations.  The latter strike me as fairly vacuous:  Shoot your
arrow (write your code), paint circles around it (define your protection
profile), declare you shot a bull's eye (certified!).  FIPS 140 *can*
have some real teeth, but it can also be gamed - again, especially
for software.  All the real meat is in the definition of the envelope.
I can declare I have a FIPS 140 certification for my AES implementation -
and then use a completely separate, insecure implementation, just so
long as I use it for "data scrambling" instead of "encryption".  A good
lab will call you if you play too many games, but ultimately labs are
paid to complete certifications, not to block them.  So it's caveat
emptor:  The full certification report is available for the customer's
review.  If you're concerned, get a copy and read it.  If the vendor
was gaming the system, that will show.  If he made a really serious
effort, that will show, too.

@_date: 2006-12-25 08:36:23
@_author: Leichter, Jerry 
@_subject: Startup to launch new random number generator from space 
There are at least three ways this kind of thing could make sense:
If you just read the quotes in the article from the CEO, it's clear
that he's more concerned about marketing - and especially the connection
to space - than about usage in cryptography, or likely and particular
How about a nice pet rock laser-inscribed with random bits from Yuzoz?
Brings together light, earth, and space - a sure winner.

@_date: 2006-12-25 08:53:23
@_author: Leichter, Jerry 
@_subject: Security Implications of Using the Data Encryption Standard 
Bounds on brute-force attacks against DESX - DES with pre- and post-whitening
- were proved a number of years ago.  They can pretty easily move DES out
of the range of reasonable brute force attacks, especially if you change
the key reasonably often (but you can safely do thousands of blocks with
one key).
One can apply the same results to 3DES.  Curiously, as far as I know there
are to this day no stronger results on the strength of 3DES!
I find it interesting that no one seems to have actually made use of these
results in fielded systems.  Today, we can do 3DES at acceptable speeds in
most contexts - and one could argue that it gives better protection against
unknown attacks.  But it hasn't been so long since 3DES was really too
slow to be practical in many places, and straight DES was used instead,
despite the vulnerability to brute force.  DESX costs you two XOR's - very
cheap for what it buys you.
Question:  How does DUKPT generate its unique keys?  If it's using DES
on the previous key, or on a counter, or anything simple like that, at
best, it's making brute force a bit more expensive - one brute forces
a couple of transaction keys, then uses them to brute force the DUKPT
key stream.  (There are certainly ways to make this much harder, but I
wonder what they actually do.)

@_date: 2006-02-10 09:24:44
@_author: leichter_jerrold@emc.com 
@_subject: Nonrepudiation - in some sense 
firewalls that can "look inside" SSL sessions:
 	SSL Security that Maintains Non-Repudiation
 	SecureSphere can inspect the contents of both HTTP and HTTPS
 	(SSL) traffic.  SecureSphere delivers higher HTTPS performance
 	than competing reverse proxy point solutions because
 	SecureSphere decrypts SSL encrypted traffic but does not
 	terminate it. Therefore SecureSphere simply passes the encrypted
 	packets unchanged to the application or database server. This
 	eliminates the overhead of re-packaging (i.e. changing) the
 	communications, re-negotiating a new SSL connection to the
 	server, and re-encrypting the information. Moreover, it
 	maintains the non-repudiation of transactions since the
 	encrypted communication is between client and application with
 	no proxy acting as middleman.
 							-- Jerry

@_date: 2006-02-10 15:08:07
@_author: leichter_jerrold@emc.com 
@_subject: Nonrepudiation - in some sense 
makes Well, yes, that was my point:  Even if you somehow convince yourself that
provides non-repudation "since the encrypted communication is between client
and application", an appliance like this must have access to the server's
and by its very nature, is watching the entire transaction.  How can anyone
ever prove that a message came from the server when the appliance was in a
position to create exactly the same message?  If it's "non-repudiation" of
what the client sent, how does it matter whether there's a proxy in the
picture?  (Not to mention how few clients have certs to begin with.)
BTW, I got to this page from a Computerworld article about a system manager
who was up nights worrying that his whole security infrastructure ended up
providing a means for hackers to penetrate his system without him being
able to track them (because he couldn't see what they were doing inside
SSL sessions).  The Imperva appliance reassures him that they can no longer
I'll leave it to everyone on this list to try and come up with a reasonable
real-world binding of either (a) his fears; (b) the fix.

@_date: 2006-02-13 16:53:58
@_author: leichter_jerrold@emc.com 
@_subject: GnuTLS (libgrypt really) and Postfix 
And *this* shows the danger of false dichotomies.
A library can't possibly know what kind of applications it will be part of!
No, the "library thinks it can call exit()" is *always* inappropriate.
There are reasonable ways to deal with this kind of thing that are just as
safe, but allow general-purpose use.  For example:
Granted, a user *could* write code that leaked important information upon
being informed of an error.  But he would have to try.  And, frankly,
not a damn thing you can do to *prevent* that.  Most Unix systems these days
allow you to interpolate functions over standard library functions.  You
think you're calling exit(), or invoking kill()?  Hah, I've replaced them
my own functions.  So there.  (No interpolation?  Patching compiled code to
change where a function call goes is pretty easy.)
Of course, all this is nonsensical for an open-source library anyway!
You're kidding yourself if you think *any* programming practice will protect
you against a programmer who needs his program to do something that you
consider a bad idea.  But the whole approach is fundamentally wrong-headed.
The user of your library is *not* your enemy.  You should be cooperating
him, not trying to box him in.  If you treat him as your enemy, he'll either
choose another library - or find a way to work around your obstinacy.

@_date: 2006-02-27 14:22:04
@_author: leichter_jerrold@emc.com 
@_subject: DHS: Sony rootkit may lead to regulation 
DHS: Sony rootkit may lead to regulation U.S. officials aim to avoid future security threats caused by copy protection software
News Story by Robert McMillan
FEBRUARY 16, 2006 (IDG NEWS SERVICE) - A U.S.  Department of Homeland
official warned today that if software distributors continue to sell
with dangerous rootkit software, as Sony BMG Music Entertainment recently
legislation or regulation could follow.
"We need to think about how that situation could have been avoided in the
first place," said Jonathan Frenkel, director of law enforcement policy for
the DHS's Border and Transportation Security Directorate, speaking at the
Conference 2006 in San Jose. "Legislation or regulation may not be
in all cases, but it may be warranted in some circumstances."
Last year, Sony began distributing XCP (Extended Copy Protection) software
some of its products. The digital rights management software, which used
rootkit cloaking techniques normally employed by hackers, was later found to
be a security risk, and Sony was forced to recall millions of its CDs.
The incident quickly turned into a public relations disaster for Sony. It
attracted the attention of DHS officials, who met with Sony a few weeks
news of the rootkit was first published, Frenkel said. "The message was
certainly delivered in forceful terms that this was certainly not a useful
thing," he said.
While Sony's software was distributed without malicious intent, the DHS is
worried that a similar situation could occur again, this time with
more-serious consequences. "It's a potential vulnerability that's of strong
concern to the department," Frenkel said.
Though the DHS has no ability to implement the kind of regulation that
mentioned, the organization is attempting to increase industry awareness of
the rootkit problem, he said. "All we can do is, in essence, talk to them
embarrass them a little bit," Frenkel said.
In fact, this is not the first time the department has expressed concerns
the security of copy protection software. In November, the DHS's assistant
secretary for policy, Stewart Baker, warned copyright holders to be careful
how they protect their music and DVDs. "In the pursuit of protection of
intellectual property, it's important not to defeat or undermine the
measures that people need to adopt in these days," Baker said, according to
video posted to The Washington Post Web site.
Despite the Sony experience, the entertainment industry's use of rootkits
appears to be an ongoing problem. Earlier this week, security vendor
Corp. reported that it had discovered rootkit technology in the copy
protection system of the German DVD release of the American movie Mr. and
Mrs. Smith. The DVD is distributed in Germany by Kinowelt GmbH, according to
the Internet Movie Database.
Baker stopped short of mentioning Sony by name, but Frenkel did not. "The
recent Sony experience shows us that we need to be thinking about how to
ensure that consumers aren't surprised by what their software is programmed
do," he said.
Sony BMG officials could not immediately be reached for comment.

@_date: 2006-01-09 10:55:50
@_author: leichter_jerrold@emc.com 
@_subject: phone records for sale.  
Where two parties exchange information voluntarily, deciding who ought to
have control of what can get ... interesting.  Here's a more complex case:  Vendors have long claimed the right use their own customer lists for
marketing purposes.  But suppose you buy using a credit card.  Then information about your purchase is known not just to you and the vendor you dealt with, but
the credit card company (construed broadly - there's the issuing bank, the vendor's bank, various clearing houses...).  Can the credit card company use
the same information for marketing - selling, say, a list of a vendor's customers who used a credit card to the vendor's competitors?  The same vendors who claim that you have no right to tell them what they can do with the transaction information incidental to you doing business with them make
a very different set of arguments when its "their" information being sold by someone else.
This issue came up a number of years ago, but I haven't heard anything
about it.  I'm not sure how it came out - the credit card companies may have
decided to back off because the profit wasn't worth the conflicts.  We're in
the midst of battles, not yet resolved as far as I know, about whether a
search engine can let company A put ads up in reponse to searches for
competitor company B.  Can an ISP sell lists of people who visited ford.com
from among their customers to GM?
Information doesn't want to be free - in today's economy, information wants
be charged for everywhere, from everyone.

@_date: 2006-01-18 10:06:28
@_author: leichter_jerrold@emc.com 
@_subject: long-term GPG signing key  
But the collisions are after 2^32 *blocks*, not *bytes*.  So the number to
start with is 2^35 bytes.
So this correspondingly is 2^41.
And this is about 10^10/40 minutes.
8 hours.
Realistically, rekeying every half an hour is probably acceptable.  In fact,
even if an attacker built up a large fraction of a codebook, there is no
known way to leverage that into the actual key.  So you could rekey using
some fixed procedure, breaking the codebook attack without requiring any
changes to the underlying protocols (i.e., no extra data to transfer).
Something like running the key through a round of SHA should do the trick.
If it's agreed that this is done after the 2^30 block is sent/received, on
a 1GB network you're doing this every 20 minutes, with essentially no chance
of a practical codebook attack.
(Not that replacing 3-DES with AES isn't a good idea anyway - but if you
a fielded system, this may be the most practical alternative.)
Perhaps I'm being a bit fuzzy this morning, but wouldn't using counter mode
avoid the problem?  Now the collisions are known to be exactly 2^64 blocks
apart, regardless of the initial value for the counter.  Even at
that will take some time to become a problem.  (Of course, that *would* require redoing the protocol, at which point using AES might be more

@_date: 2006-01-18 10:17:35
@_author: leichter_jerrold@emc.com 
@_subject: quantum chip built 
There is little basis for any real estimates here.  First off, you should probably think of current qbit construction techniques as analogous to transistors.  If you looked at "number of transistors in a computer" and didn't know that IC's were on the way, you would make much smaller estimates
as to the sizes of practical machines in 1980, much less 2006.
But more fundamentally, qbits don't necessarily scale linearly.  Yes,
current algorithms may need some number of qbits to deal with a key of n bits, but
the tradeoff between time and "q-space" is not known.  (Then again, the tradeoff between time and space for *conventional* computation isn't known,
except for some particular algorithms.)  I believe there's a result that if any of some broad class of quantum computations can be done using n qbits,
it can also be done with just one (plus conventional bits).
I'm not sure I would be tHat confident.  There are too many unknowns - and
quantum computation has gone from "neat theoretical idea, but there's no possible way it could actually be done because of " to "well, yes, it can be done for a small number of bits but
they can't really scale it" in a very short period of time.
majordomo at metzdowd.com

@_date: 2006-01-18 16:22:52
@_author: leichter_jerrold@emc.com 
@_subject: quantum chip built 
I'm no expert myself.  I can say a few things, but take them with a grain of
I don't recall seeing any quantum encryption algorithms proposed.  Someone
have done so, of course - the field is moving quickly.  Our understanding of
quantum computation is very limited so far.  Quantum key exchange is one
pretty well-developed area.  The main other algorithms are variations of
search.  A number of years down the road, I'm sure both will be seen as
"obvious" applications of ideas that had been around for years.  (Quantum
exchange is the practical application of ideas from thought experiments
back to the birth of quantum mechanics.  Search algorithms are pretty
straightforward applications of the basic idea of quantization.  There was
never a reason to look at these things as computational mechanisms until
There is actually a limit to the number of distinct quantum states that
any system can have, based mainly on the *area*, not volume, of the system.
(In some sense, we seem to have a 2-space-dimensional universe!)  The limit
for an elementary particle is pretty small.
BTW, this has some interesting implications.  We usually argue that some
computation, while beyond our current reach, is "in principle" possible.
in fact one can compute a bound on the number of primitive computational
events that could have taken place since the creation of the universe.  If
a computation required more than that number of computations - think bit
flips, if you like - then "in principle" it would seem to be impossible, not
possible!  One can flip this around:  Suppose you wanted to do a brute-force
attack against a 128-bit key.  OK, that requires at least 2^128
steps.  Suppose you wanted the result in 100 years.  Then the computation
can't require a volume of space more than 100 light-years across.  (Well,
really 50.)  You can compute how many bit flips could take place in a volume
of space-time 100 light-years by 100 years across.  If it's less than 2^128,
then even "in principle", no such attack is possible.
I did some *very* rough calculations based on some published results - I
didn't have enough details or knowledge to do more than make a very rough
estimate - and it turns out that we are very near the "not possible in
principle" point.  If I remember right, a 128-bit key is, in principle, just
barely attackable in 100 years; but a 256-bit key is completely out of
So much for the snake-oil "my 1500-bit key is much more secure than your
256-bit key" claims!
There are some very recent - last couple of weeks - results on creating
entangled systems of 100's of thousands of particles.  (Hence my suggestion
that we are doing quantum "transistors", but will eventually do quantum

@_date: 2006-01-24 18:56:48
@_author: leichter_jerrold@emc.com 
@_subject: NSA explains how to redact documents electronically 
It's a continuous battle, of course.
One noticeable thing is the complexity of the procedure.  I suspect human error will prove a more likely cause of leaks than the actual mechanism,
which appears sound.
There are commercial tools available that strip metadata from Word
I'm surprised no one has done a "redacting workbench" yet.

@_date: 2006-01-29 18:42:24
@_author: leichter_jerrold@emc.com 
@_subject: thoughts on one time pads 
[CD destruction] As always, who are you defending against?  There are commercial "CD
whose effect - preserved islands with some destroyed material - is produced
by a much more prosaic approach:  The surface is covered with a grid of pits.
Only a small fraction of the surface is actually damaged, but no standard device will have any chance of reading the disk.  I suppose specialized hardware might do so, but even if it code, there's the question of the encoding format.  CD's are written with error-correcting codes which can recover from fairly significant damage - but if the damage exceeds their correction capability, they provide no information about what was there to begin with.
If you want to go further down the same route, grinding the whole surface of
the disk should work even better.
Of course, all this assumes that there's no way to polish or otherwise
the protective plastic.  Polishing should work if the scratches aren't too
deep.  (The pits produced by the CD shredder" I've seen look deep enough to make this difficult, but that's tough to do over the whole surface.)
Probably the best approach would be "better living through chemistry":  It should be possible to dissolve or otherwise degrade the plastic, leaving the
internal metallic surface - very thin and delicate - easy to destroy.  One would need to contact a chemist to determine the best way to do this.  (If
all else fails, sulfuric acid is likely pretty effective - if not something you want to keep around.)
Realistically, especially given the error-correcting code issues, anything that breaks the CD into a large number of small pieces probably puts any recovery into the "national lab" range - if even they could do it.

@_date: 2006-07-03 10:40:14
@_author: Leichter, Jerry 
@_subject: Use of TPM chip for RNG? 
Your example would, in fact, be as strong as any.  It's generally
considered a significant - often disqualifying - fault of a modern
cryptosystem if its output can be distinguished from that of a random
function.  Feeding the input back is a common method for testing for
such non-randomness, since the expected cycle length for random
functions can be calculated and many older cryptographic functions
showed weaknesses here.
You're damned if you do and damned if you don't.  Would you want to use a
hardware RNG that was *not* inside a tamper-proof package - i.e., inside
of a package that allows someone to tamper with it?
A "spiked" RNG of the kind you describe is at least somewhat fixable:
Choose a fixed secret key and encrypt the output of the generator with
the key before using it.  Assuming the cryptographic function you use is
good - and in the end you're almost certain to make that assumption
somewhere - the resulting bits can be treated as random.  (Note that you
don't ever have to share that key with anyone, nor do you have to fix it
for good.)  (And, yes, on a theoretical level, there is only one block's
worth of entropy in such a generator, so it's not so good.  Assuming the
same crypto algorithm throughout, one way or another, the best you can
get is the difficulty of a brute-force attack on the smaller of a key or
a block.  For repeated uses, an attack on the generator, of course,
may give you access to much more than one key.)
As has been discussed here previously, there are other ways to "spike"
hardware, including an RNG, that are much more insidious.  An RNG that
only covers a small fraction of the possible outputs is one possibility.
For example, r_i = Encrypt(key,i mod 2^32) will look quite random unless
you get more than 2^32 samples, but there's a trivial brute-force attack
against the output - which works just as well against the "encrypt before
using" fix.

@_date: 2006-07-04 12:45:24
@_author: leichter_jerrold@emc.com 
@_subject: Use of TPM chip for RNG? 
This assumes an odd definition of "tamper-proof":  I can't look inside,
but the bad guys can change it without my knowing.  There are such
things around - all too many of them; your typical Windows PC, for
most people, is a great examplar of the class - but no  one describes
them as "tamper-proof".  "Tamper-proof" means that *no one* can change
the thing.  Obviously, this is a matter of degree, and "tamper-resistant"
is a much better description.  But there are devices considered
"tamper-resistent" against very well-funded, very technologically
adept adversaries.
which is yet another issue, that of tamper-evident design.  If your
design isn't tamper-evident - which again is a matter of degree -
it's unlikely your pictures will do you much good against even a
moderately sophisticated attacker.  With physical access and no
tamper evidence, a couple of minutes with a USB stick is all that's
necessary to insert some rather nasty code, which you have little
hope of detecting, whether by physical or software means.

@_date: 2006-07-11 09:49:04
@_author: leichter_jerrold@emc.com 
@_subject: Interesting bit of a quote 
...from a round-table discussion on identity theft in the current
 	IDGNS: What are the new threats that people aren't thinking
 	about?
 	CEO Dean Drako, Sana Security Inc.: There has been a market
 	change over the last five-to-six years, primarily due to
 	Sarbanes-Oxley. It used to be that you actually trusted your
 	employees. What's changed -- and which is really kind of morally
 	and socially depressing -- is that now, the way the auditors
 	approach the problem, the way Sarbanes-Oxley approaches the
 	problem, is you actually put in systems assuming that you can't
 	trust anyone.  Everything has to be double-signoff or a
 	double-check in the process of how you organize all of the
 	financials of the company....
 							-- Jerry

@_date: 2006-07-11 13:02:27
@_author: Leichter, Jerry 
@_subject: Interesting bit of a quote 
There have always been parts of the business where you needed to enforce
things quite tightly - mainly those that handled cash or cash equivalents.
Other things were enforced more loosely.  The change is that so much is
now moving into the "tight enforcement" category - and not just because
of SOX.  For example, there's a large and growing business in reviewing
employee-submitted expenses.  These have always been subject to *some*
level of review, but now they are increasingly scanned by computer for
the smallest violations of policy.
Business ultimately depends on trust.  There's some study out there -
I don't recall a reference - that basically finds that the level of
trust is directly related to the level of economic success of an
economy.  There are costs associated with verification, some of them
easily quantifiable, some of them much harder to pin down.  The
difficulty is in making the tradeoffs.  We're now pushing way over
on the verification side, in a natural reaction to a series of major
frauds and scandals.

@_date: 2006-07-12 10:04:40
@_author: leichter_jerrold@emc.com 
@_subject: Interesting bit of a quote 
Another, very simple, example of the way that the assumptions of
auditing are increasingly at odds with reality can be seen in receipts.
Whenever I apply for a reimbursement of business expenses, I have to
provide original receipts.  Well ... just what *is* an "original
receipt" for an Amazon purchase?  Sure, I can print the page Amazon
gives me.  Then again, I can easily modify it to say anything I like.
Hotel receipts are all computer-printed these days.  Yes, some of them
still use pre-printed forms, but as the cost of color laser printers
continues to drop, eventually it will make no sense to order and stock
that stuff.  Restaurant receipts are printed on little slips of paper by
one of a small number of brands of printer with some easily set custom-
ization, readily available at low cost to anyone who cares to buy one.
Back in the days when receipts were often hand-written or typed on
good-quality letterhead forms, original receipts actually proved
something.  Yes, they could be faked, but doing so was difficult and
hardly worth the effort.  That's simply not true any more.
Interestingly, the auditors at my employer - and at many others, I'm
sure - have recognized this, and now accept fax images of all receipts.
However, the IRS still insists on "originals" in case of an audit.
Keeping all those little pieces of paper around until the IRS loses
interest (I've heard different ideas about how long is "safe" - either 3
or 7 years) is now *my* problem.  (If the IRS audits my employer, and
comes to me for receipts I don't have, the "business expense reimburse-
ments" covered by those missing receipts suddenly get reclassified as
"ordinary income", on which *I*, not my employer, now owe taxes - and
their good friends interest and penalties.)

@_date: 2006-07-13 11:40:02
@_author: leichter_jerrold@emc.com 
@_subject: Interesting bit of a quote 
That's a very interesting comparison.  I think it's a bit more subtle: We
two distinct phenomena here, and it's worth examining them more closely.
Phenomenon 1:
Phenomenon 2:

@_date: 2006-07-26 11:28:42
@_author: Leichter, Jerry 
@_subject: Crypto to defend chip IP: snake oil or good idea? 
What's completely unclear to me is exactly what "IP" is being protected
and from whom.  Without a better understanding of those issues, it's
impossible to analyze what they are doing.  (Certicom is pretty
experienced at this sort of stuff, so I wouldn't dismiss it out of
An example of a problem that *could* be solved:  I wish to create a
semi-custom chip, where the unique value is derived in substantial part
(but not completely) from a gate array.  I don't trust the chip
manufacturer - if I give him the information necessary to configure the
gate array, he may create extra chips.  So I have him build the whole
thing with an unconfigured gate array, then ship the unfinished chips to
a smaller-scale operation that I *do* trust to configure the array and
"pot" the chips to make reverse engineering very expensive.
No crypto so far.  But suppose I don't trust the second chip house
either.  So I add a crypto module on the address inputs.  The first chip
house gets the keys for the module; the second gets encrypted inputs.
Neither alone can create a finished chip.  The second chip house could,
in principle, reverse engineer the whole thing, but that's a rather
large investment, and I'll bet Certicom has some tricks to keep their
crypto core from being analyzed.  You can probably make the risk of
this attack much lower than the risk that the two chip houses collude.
Yes, I'm doing a lot of hand-waving about how the actual manufacturing
process could work.  But the general approach should be workable.

@_date: 2006-07-28 10:16:23
@_author: leichter_jerrold@emc.com 
@_subject: Recovering data from encrypted disks, broken CD's 
--Jerry
When encryption doesn't work
By Robert L. Mitchell on Wed, 07/26/2006 - 12:00pm
In my interview with Ontrack Data Recovery this week (see
Recovery specialists bring data back from the dead:
quite a bit hit the cutting room floor, including these three nuggets by
Mike Burmeister, director of engineering for data recovery:
Encrption can be broken
I was surprised to learn that Ontrack regularly recovers encrypted data
on systems where the user has lost the key. "There's only a couple of
technologies where we would run into a roadblock [such as] some of the
new laptops that have passwords that are tied to the media and to the
BIOS," says Burmeister. That raises the question: if they can do it, who
else can?
On encrypted systems that are more difficult to crack, OnTrack also has
a secret weapon. "Certain situations involve getting permission to get
help from the manufacturer," he says.
Broken CDs still yield data
Ontrack can also reassemble and recover data from CD-ROM discs that have
been broken into pieces. If you're using CDs for backups of sensitive
data, it's probably best to shred them.
Tapes work. People fail
Among the tape problems Ontrack sees most often are those related to
human errors, such as accidentally erased or formatted tapes.
"Formatting the wrong tapes is the most common [problem] by far.  The
other one is they back up over a tape that has information on it.  The
general thing is they back up the wrong data. We'll get the tape in and
they'll say, 'The data I thought was on this tape is not on it.'"
While those failures can be attributed to confusion, another failure is
the result of just plain laziness. "People run these backup processes
and they're not simple anymore. They run these large, complex tape
libraries and they call that good enough. They don't actually go through
the process of verifying [the tape]," Burmeister says. The result:
disaster strikes twice: once when the primary storage goes down and
again when the restore fails.
For more on how the technical challenge of recovery have raised the
stakes and what you can do to protect your data, see the story above.
Filed under : Security | Software | Storage
Robert L. Mitchell's blog
It's really too bad that ComputerWorld deems to edit these
explainations. Especially when you consider its all ELECTRONIC paper.
Posted on Thu, 07/27/2006 - 4:12pm| reply
CDs (and DVDs) are very effective targets for recovery, because they
have massive error correction and the data is self-identifying because
of the embedded sector IDs. It's quite possible to recover a CD that has
been shredded, not just broken.
A few years ago, there was academic research describing automated
reassembly of shredded documents by scanning the bits and matching the
rough edges of along the cuts. I'm sure that technology has improved,
The moral of the story is that physical destruction is hard. Grinding to
powder and heating past the Curie point are pretty reliable, but short
of that, it's tough. You're better off encrypting, as long as the key
actually is secret.
Posted on Thu, 07/27/2006 - 4:44pm| reply
Computer BIOS passwords: easy to recover by resetting or other direct
access to CMOS. You can do this at home.
Disk drive media passwords: hard to recover, but possible by direct
access to flash memory on the drive. This is tough to do at home, but
probably a breeze for OnTrack.
Disk drive built-in hardware encryption (which as far as I know is only
a Seagate feature so far) should be essentially impossible to recover,
unless Seagate has built in a back door, has fumbled the implementation,
or the password is simple enough to guess. Same is true for software-
based full-disk encryption: it can be invulnerable in the absence of
errors. Use it properly, and you'll never have to worry about your data
if the computer is lost or stolen.
Posted on Thu, 07/27/2006 - 4:54pm| reply
Surely it's far more common to use the BIOS to prevent a hard drive
being mounted in another device that to encrypt it.
As one of the other commentators says, the BIOS is pretty easy to get
into if you know what you are doing. Basing an encryption system on this
would inherit all its weaknesses.
Posted on Fri, 07/28/2006 - 7:53am| reply

@_date: 2006-06-05 12:45:17
@_author: leichter_jerrold@emc.com 
@_subject: Trusted path (was: status of SRP) 
I'm going to give a pessimistic answer here:  None of the above.
You're fighting the entire direction of development of display technologies
on end-user machines.  There are no fixed standards - everything is subject
to change and (we hope) improvement.  Applications regularly "improve on"
the standards, or imitate what others have done.
Use a specially shaped window?  Soon, other applications will start
imitating that as a flag for "important" data.  Customized images?  How
many people will set one.  And how hard will it be to fool them with a
nice-looking new app that tells you it has a whole library of images you
can use for your customized image?
There is simply no precedent for people making trust distinctions based on
user elements on the screen.  They see the screen as similar to a piece of
paper, and draw distinctions using the same kinds of rules we've
applied to paper:  Does it look professionally done?  Is it well written?
Does it have all the right logos on it?  *None* of these are helpful on the
Web, but that doesn't change how people react.
The only "trusted path" most people ever see is the Windows Ctrl/Alt/Delete
to enter a password.  That's not a good example:  The *dialog* it produces
is indistinguishable from other Windows dialogs.  You should only trust it
to the degree that you know you typed Ctrl/Alt/Delete, and haven't yet hit
enter.  There's no way to generalize this.
This is a human factors issue.  You have to look at what people actually
use to make trust distinctions.  As far as I can see, the only thing that
will really work is specialized hardware.  Vendors are already moving in
this kind of direction.  Some are adding fingerprint scanners, for example.
However, any *generally accessible* device is useless - an attacker can
get at them, too.  What's needed is some physically separate device, with
a trusted path between it and something controlled.  A physical button,
with a small LCD near it, with enough room for a simple prompt, and you
are probably fine.  Make *that* "part of the browser chrome" and you have

@_date: 2006-06-13 16:25:34
@_author: leichter_jerrold@emc.com 
@_subject: complexity classes and crypto algorithms 
This is an idea that keeps coming up.
Suppose you had such a thing - for example, a one-way hash in which you
could prove that calculating a preimage inverse as NP-complete.
First off you have a basic problem in definition:  You have to specify
*one* hash with *one* output size, but NP-completeness has to do with
asymptotic behavior.  For any hash producing a fixed-size output string,
there is a deterministic machine that runs in time O(1) that computes a
pre-image.  It's a rather large machine that does a table lookup.
So, suppose you take the obvious approach - all you'd get out of 3SAT -
and said that you had a family of hash functions H_i, each producing
an i-bit output; and you made an NP-completeness argument about that.
Then the family H_i' defined by:
would satisfy the same NP-completeness predicates, but would be of no
conceivable use to anyone.
Finally, even if you get around all of that, NP-completeness is a
statement about *worst case* behavior.  All it says is that *some*
instances of the problem are hard.  It could well be that "almost all"
instances are easy!  The simplex algorithm, for example, is actually
known to have instances that require exponential time, but is widely used because in practice it's "always" fast - an empirical
observation that has been confirmed by analysis which shows that
not only is it polynomial time on average (for some appropriate
notion of randomized inputs), but it's even true in a stronger sense ("smoothed complexity", which beyond the description as being "somewhere between average- and worst-case complexity" I haven't looked at).

@_date: 2006-06-14 17:57:59
@_author: leichter_jerrold@emc.com 
@_subject: Chinese WAPI protocol? 
Actually, they are not.  There is a special provision in the law under
which something submitted to the patent office can be declared secret.
You as the inventor are then no longer allowed to talk about it.  I think
you are granted the patent, but it cannot be published.
This provision has been applied in the past - we know about it because
the secrecy order was later (years later) lifted.  I don't believe
there is any way for someone on the outside to know how many patents
may have tripped over this provision.
Needless to say, this is a disaster for you if you are the patent
applicant and want to sell your product.  But there isn't much of
anything you can do about it.  I'm not sure what happens to the term
of a patent "hidden" in this way.
The above description is of US law.  It's likely that similar provisions
exist in other countries.

@_date: 2006-06-21 07:19:30
@_author: Leichter, Jerry 
@_subject: complexity classes and crypto algorithms 
You missed the point.  Yes, you can do this for any algorithm.  But
the security claims for, say, DES include the cost of doing the
NP-completeness, on the other hand, has "security claims" that are
asymptotic:  As n grows, the complexity grows more than any polynomial
in n.  The definition makes no sense for fixed n.  Knowing the
asymptotic complexity tells you nothing about the difficulty for any
particular n.  It simply makes no statements about difficulty for any
particular n.
That's why NP-completeness has no real cryptographic significance,
except perhaps as a pointer to problems that you *might* be able to
use as a basis for a cryptosystem.  (In fact, I don't know of any
cryptographic system in actual use that is based on an NP-complete
problem.  The best-known attempts in this direction - public-key
systems based on the knapsack problem - failed.  So even the heuristic
approach doesn't seem to have borne fruit.)

@_date: 2006-03-01 11:38:54
@_author: leichter_jerrold@emc.com 
@_subject: "Study shows how photonic decoys can foil hackers" 
Does anyone have an idea of what this is about?  (From Computerworld):
 							-- Jerry
FEBRUARY 23, 2006 (NETWORK WORLD) - A University of Toronto professor
and researcher has demonstrated for the first time a new technique for
safeguarding data transmitted over fiber-optic networks using quantum
Professor Hoi-Kwong Lo, a member of the school's Centre for Quantum
Information and Quantum Control, is the senior author of a study that
sheds light on using what's called a photonic decoy technique for
encrypting data.
Quantum cryptography is starting to be used by the military, banks and
other organizations that seek to better protect the data on their
networks.  This sort of cryptography uses photons to carry encryption
keys, which is considered safer than protecting data via traditional
methods that powerful computers can crack. Quantum cryptography is
based on fundamental laws of physics, such that merely observing a
quantum object alters it.
Lo's team used modified quantum key distribution equipment from Id
Quantique and a 9.3-mile fiber-optic link to demonstrate the use of
decoys in data transmissions and to alert receiving computers about
which photons were legit and which were phony.  The technique is
designed to support high key generation rates over long distances.
Lo's study is slated to appear in the Feb. 24 issue of Physical Review
Lo notes that existing products, such as those from Id Quantique and
MagiQ Technologies, are for point-to-point applications used by the
military and security-sensitive businesses.  "In the long run, one can
envision a global quantum cryptographic network, either based on
satellite relays or based on quantum repeaters," he says.
University researchers are fueling many advances in network
security. A University of Indiana professor recently revealed
technology for thwarting phishing and pharming culprits by using a
technique called active cookies.

@_date: 2006-03-20 15:51:11
@_author: leichter_jerrold@emc.com 
@_subject: Creativity and security 
I was tearing up some old credit card receipts recently - after all
these years, enough vendors continue to print full CC numbers on
receipts that I'm hesitant to just toss them as is, though I doubt there
are many dumpster divers looking for this stuff any more - when I found
a great example of why you don't want people applying their "creativity"
to security problems, at least not without a great deal of review.
You see, most vendors these days replace all but the last 4 digits of
the CC number on a receipt with X's.  But it must be boring to do the
same as everyone else, so some bright person at one vendor(*) decided
they were going to do it differently:  They X'd out *just the last four
digits*.  After all, who could guess the number from the 10,000
 							-- Jerry
(*) It was Build-A-Bear.  The receipt was at least a year old, so for
all I know they've long since fixed this.

@_date: 2006-03-21 09:58:43
@_author: leichter_jerrold@emc.com 
@_subject: pipad, was Re: bounded storage model - why is R organized as  
The issue would be:  Are there any dependencies amoung the bits of
pi that would make it easier to predict where an XOR of n streams of
bits taken from different positions actually come from - or, more
weakly, to predict subsequent bits.
I doubt anyone knows.  What would worry me is exactly the existence
of the algorithm that would make this approach workable:  A way to
compute the i'th digit of pi without computing all the earlier ones.
As a starter problem, how about a simpler version:  Take n=1!  That
is, the key is simply a starting position in pi - taken from a
suitably large set, say the first 2^256 bits of pi - and we use
as our one-time pad the bits of pi starting from there.  An
attackers problem now turns into:  Given a sequence of k successive
bits of pi taken from among the first 2^256 bits, can you do better
than chance in predicting the k+1'st bit?  The obvious approach of
searching through pi for matches doesn't look fruitful, but perhaps
we can do better.  Note that if pi *isn't* normal to base 2 - and
we still don't know if it is - this starter problem is soluable.
BTW, Bailey and Crandall's work - which led to this discussion -
ties the question of normality to questions about chaotic
sequences.  If the approach of using pi as a one-time pad
works, then all the systems based on chaotic generators
will suddenly deserve a closer look!  (Many fail for much
simpler reasons than relying on such a generator, but some
are untrustworthy not because we don't know of an attack
but because we have no clue how to tell if there is one.)
Mathematician's insult:  You're transcendental (dense and totally

@_date: 2006-03-22 10:16:54
@_author: leichter_jerrold@emc.com 
@_subject: pipad, was Re: bounded storage model - why is R organized as  
I agree 100% from a practical point of view.  Given that you would
have to use a very large prefix of pi - at least 2^128 bits, probably
more - just the large-number arithmetic needed pretty much guarantees
that you aren't going to get a competitive system.
I do find this interesting from a theoretical point of view, since it
*might* give us some kind of provable security.  We don't seem to have
any techniques that have any hope of proving security for any
conventional system.  What we have are reductions.  An approach like
this ties into a very rich body of mathematics, and *might* lead to
a path to a proof.  (Given the connection that this work has to
chaotic dynamical systems, there's even the outside possibility that
one might get a provably secure efficient random bit generator out of
such systems.)
I certainly wouldn't want to place any bets here.  In fact, my guess
is that this won't go through - that the best you'll get is a result
of the form:  The set of reals for which a system is *not* provably
secure has measure 0.  Unfortunately, either you can't write down
any *particular* r that works, or there are artificially constructed
r's that are too expensive to compute.

@_date: 2006-03-22 13:32:51
@_author: leichter_jerrold@emc.com 
@_subject: passphrases with more than 160 bits of entropy 
Shannon entropy is a property of a *source*, not a particular sequence
of values.  The entropy is derived from a sum of equivocations about
successive outputs.
If we read your "create a sequence...", then you've described a source -
a source with exactly one possible output.  All the probabilities will
be 1 for the actual value, 0 for all other values; the equivocations are
all 0.  So the resulting Shannon entropy is precisely 0.

@_date: 2006-03-22 15:32:44
@_author: leichter_jerrold@emc.com 
@_subject: PayPad 
PayPad ( is an initiative that seems to have JPMorganChase
Chase behind it to provide an alternative method for paying transactions
on line.  You buy a PayPad device, a small card reader with integrated
keypad.  It connects to your PC using USB.  To pay using PayPad at
a merchant that supports it, you select that as an option, swipe your
card, enter your PIN, and the data is (allegedly) sent encrypted
from the PayPad device direct to the merchant.
Advantage to the merchant:  It's a debit card transaction, and they
claim the transaction fees are half those of a credit card. Of course,
the consumer pays for everything:  The device itself (about $60), the
lack of "float".  It's not clear what kind of recourse you might have
in case of fraud.
It's sold as "the secure alternative to using your credit card
online".  Unfortunately, it has the problems long discussed on
this list:  The PayPad itself has no display.  It authorizes a
transaction the details of which are on your computer screen.
You have only the software's word for it that there is any
connection between what's on the screen and what's sent to the
merchant (or to someone else entirely).
Realistically, it's hard to see how this is any more secure than
a standard credit card transaction in an SSL session.  It's not
even clear that the card data is encrypted in the device - for
all we know, card data and pin are transfered over the USB to the
application you have to run on your PC, ready to be stolen by,
say, a targetted virus.  They do claim that you are protected in
another way:  "Your sensitive data never goes to the merchant or
into a database that can be hacked .... The encrypted transaction
is handled directly with your bank...."  (I guess banks don't
keep databases....)
Anyone know anything more about this effort?
 							-- Jerry

@_date: 2006-03-24 09:17:47
@_author: leichter_jerrold@emc.com 
@_subject: Linux RNG paper 
Interesting; I hadn't seen this definition before.  It's related to a
concept in traditional probability theory:  The probability of ruin.  If
I play some kind of gambling game, the usual analysis looks at "the
value of the game" strictly as my long-term expectation value.  If,
however, I have finite resources, it may be that I lose all of them
before I get to play long enough to make "long-term" a useful notion.
The current TV game show , Deal Or No Deal, is based on this:  I've yet
to see a banker's offer that equals, much less exceeds, the expected
value of the board.  However, given a player's finite resources - they
only get to play one game - the offers eventually become worth taking,
since the alternative is that you walk away with very little.  (For
that matter, insurance makes sense only because of this kind of
analysis:  The long-term expectation value of buying insurance *must*
be negative, or the insurance companies would go out of business -
but insurance can still be worth buying.)

@_date: 2006-03-24 16:31:49
@_author: leichter_jerrold@emc.com 
@_subject: Creativity and security 
sight You're underestimating human abilities when there is a reward present.
Back in the days when telephone calling cards were common, people used
to "shoulder surf", watching someone enter the card number and
memorizing it.  A traditional hazing in the military is to give the new
soldier a gun, then a few seconds later demand that he tell you the
serial number from memory.  Soldiers caught out on this ... only get
caught out once.
Besides, there's a lot less to remember than you think.  I don't know
how your chip-and-pin card encoding is done, but a credit card number is
16 digits, with the first 4 (6?) specifying the bank (with a small
number of banks covering most of the market - if you see a card from
an uncommon bank, you can ignore it) and the last digit a check digit.
So you need to remember one of a small number of banks, a name, and
11 digits - for the few seconds it takes for the customer to move on
and give you the chance to scrawl it on a piece of paper.  Hardly very

@_date: 2006-03-31 10:36:03
@_author: leichter_jerrold@emc.com 
@_subject: webcam encryption beats quasar encryption 
============================== START ==============================
Probably Rabin's work on beacons.  It explored the results of assuming
a universally available oracle providing the same stream of random bits
to everyone.  (If you think of the randomized Turing machine model
as a TM plus an oracle giving that machine a random bit stream, you
can think of this as a bunch of communicating TM's that get *the
same* random bit stream.)

@_date: 2006-05-01 11:26:38
@_author: leichter_jerrold@emc.com 
@_subject: PGP "master keys" 
What I've heard described as "the bull in the china shop theory of
security":  You can always buy new china, but the bull is dead meat.
(I'm pretty sure I heard this from Paul Karger, who probably picked it
up during his time at the Air Force.)
The dodge of creating phantom troops and then collecting their pay
checks has been around since Roman times.  No one has ever found a
way of detecting it cost-effectively.  However, it's also been known
forever that it's just about impossible to avoid detection indefinitely:
The officer who created the troops gets transferred, or retires, and
he has no way to maintain the fiction.  Or the troops themselves are
transferred. other events intervene.  So armies focus on making sure
they *eventually* find and severely and publicly punish anyone who tries
this, no matter how long it takes.  A large enough fraction of the
population is deterred to keep the problem under control.
A similar issue occurs in a civilian context, sometimes with fake
employees, other times with fake bills.  Often, these get found
because they rely on the person committing the fraud being there
every time a check arrives:  It's the check sitting around with no
one speaking for it that raises the alarm.  The long-standing
policy has been to *require* people in a position to handle those
checks to take their vacation.  (Of course, with direct deposit
of salaries, the form of the fraud, and what one needs to do to
detect it, have changed in detail - but probably not by much.)

@_date: 2006-05-05 17:24:33
@_author: leichter_jerrold@emc.com 
@_subject: Linux RNG paper 
That's way too strong.  Here's an implementation that preserves
block-level atomicity while providing integrity:  Corresponding to each
block, there are *two* checksums, A and B.
Writes to a given block must be atomic with respect to each other.
(No synchronization is needed between reads and writes.)
Granted, this algorithm has other problems.  But it shows that the three
requirements - user block size matches disk block size; block level
atomicity; and authentication - are not mutually exclusive.  (Actually,
I suppose one should add a fourth requirement, which this scheme also
realizes:  The size of a user block identifier is the same as the size
of the block id passed to disk.  Otherwise, one can keep the checksum
with each "block identifier".)

@_date: 2006-05-08 09:55:25
@_author: leichter_jerrold@emc.com 
@_subject: Get a boarding pass, steal someone's identity 
I've actually gone in the opposite direction:  I shred less than I used
to.  Grabbing this kind of information off stray pieces of paper in a
garbage can is buying retail.  It's so much easier these days to buy
wholesale, stealing hundreds of thousands to tens of millions of on-line
records in one shot.
It would be useful to get some idea of the chances one takes in throwing
identifying material out.  Everything in security is cost vs. benefit,
and the cost of shredding, while it appears low on a single-item basis,
adds up in annoyance.  And all too many of the companies I deal with
seem to make it ever harder.  Just yesterday, I threw out a couple of
letters having to do with incidental matters (e.g., an incorrect charge)
from a credit card provider.  Every one of them had my full card number
on it.  Some of them looked like the routine junk you get every month
and don't even look at twice before discarding.
Meanwhile, my statements contain my credit card number, in small but
easily readable numbers, *vertically* on the page - next to what appears
to be a bar code with the same information.  Even a cross-cut shredder
probably isn't sufficient to render that unreadable.
The entire infrastructure we've built based on a shared pseudo-secrets
is one of the walking dead.  For credit cards, the responsibility for
loss is on the card companies, where it belongs - and I let it stay
there.  I take basic reasonable care, but I'm unwilling to go any
further, since it can't possibly help me and I'm paying indirectly for
all the costs the credit card companies assume anyway (since they push
them off on the vendors, who then raise their prices).  As far as
identity theft as a general issue:  What little evidence there is as to
the way the identity thieves work today implies that nothing I'm likely
to do - absent obvious dumb moves - will change my odds of being
successfully hit by very much.

@_date: 2006-05-09 12:04:58
@_author: leichter_jerrold@emc.com 
@_subject: Piercing network anonymity in real time 
eTelemetry Locate                                      [Image]
      Locate dynamically discovers, correlates and archives the
      person behind the IP address--"the people layer"--to expedite
      forensic investigations and help comply with SOX. It
      approaches the issue of how to match a name to a network
      device from the identity side, a change from 802.1x and NAC
      methodologies.
      The Locate appliance sits passively on the network and
      analyzes packets in real time to garner ID info from sources
      like Active Directory, IM and e-mail traffic, then associates
      this data with network information.
      Once Locate is populated, IT can disconnect an individual with
      one click at the switch port level, a powerful tool for
      enforcing policy and halting the spread of infections. You
      also can connect to an end user's computer without asking for
      an IP address, track assets dynamically, stay in sync with
      Active Directory and other LDAP directories automatically, and
      archive network activity for forensic investigations.
      etelemetry.com/pdf/Interop_finalist_final.pdf
 							-- Jerry

@_date: 2006-05-12 18:49:47
@_author: leichter_jerrold@emc.com 
@_subject: "Consumers Losing Trust in Internet Banking" 
Summary:  The deluge of reports of problems at on-line banks is having
an effect.  Customer attitudes are increasing negative, and customers
mention concerns about security as worrying them.  The adoption rate
for internet banking has dropped to only 3.1% for the last quarter
of 2005, about matching the rate at which people drop their accounts.
Over all, 38% of Americans use Internet banking - compared to 75% of
Europeans.  (Europeans report a much higher level of confidence in
on-line banking.)
The full report is at
Eventually, all those voting feet will have an effect.  Perhaps we
don't need to despair of the market forcing better security.
 							-- Jerry

@_date: 2006-05-15 13:14:01
@_author: leichter_jerrold@emc.com 
@_subject: Piercing network anonymity in real time 
There's a difference between "can be done by someone skilled" and
"your IT can buy a box and have it running on your network this
afternoon".  The first basically means that most people, most of
the time, effectively have anonymity because it isn't worth anyone's
bother to figure out what they are up to.  With the second, information
about who you are, who you talk to, etc., etc., becomes a commodity -
a very *cheap* commodity.  "Safety in numbers" disappears.
It's always been possible to go to town hall and look up public records
like deeds - which often contain things like Social Security numbers,
bank account  numbers, etc.  Skilled experts - PI's - have made use of
this information for years.  There's no difference, in principle, when
that some information goes up on the web.  But that's not how most
people feel about it.

@_date: 2006-05-15 15:12:47
@_author: leichter_jerrold@emc.com 
@_subject: the meaning of linearity, was Re: picking a hash function to  
XOR is the same as addition mod 2.  The integers mod 2 form a field
with XOR as the addition operation and integer multiplication (mod 2,
though that has no effect in this case) as the multiplication.
If you think of a stream of n bits as a member of the vector space
of dimension n over the integers mod 2 treated as a field, then
adding two of these - the fundamental linear operation - is XOR'ing
them bit by bit.
The thing I've always wondered about stream ciphers is why we only
talk about linear ones.  A stream cipher is fundamentally constructed
of two things:  A stream of bits (alleged to be unpredictable) as
long as the plaintext; and a combining function that takes one
plaintext bit and one stream bit and produces a ciphertext bit.
The combining function has to conserve information.  If you only
combine single bits, there are only two possible functions:  XOR
and the complement of XOR.  But consider RC4:  It actually generates
a byte at a time.  We just choose to use that byte as a vector of
8 bits.  For plaintexts that are multiples of 8 bits long - just
about everything these days - there are many possible combining
functions.  Most aren't even close to linear.
Other than post by a guy - Terry someone or another - on sci.crypt
a number of years ago - I've never seen any work in this direction.
Is there stuff I'm not aware of?

@_date: 2006-05-19 17:31:49
@_author: leichter_jerrold@emc.com 
@_subject: statistical inferences and PRNG characterization 
That's not the way it's done.  Ignore for a moment that we have a sequence
(which is probably irrelevant for this purpose, but might not be).  Instead,
just imagine we have a large collection of values generated by the PRNG -
or, looked at another way, a large collection of values alleged to have been
drawn from a population with P(0) = 0.3 and P(1) = 0.7.  Now take a truely
random sample from that collection and ask the question:  What is the
probability that I would have seen this result, given that the collection
I'm drawing from is really taken from the alleged distribution?  You don't
need any information about *other* possible distributions.  (Not that there
aren't other questions you can ask.  Thus, if the collection could have
been drawn from either of two possible distributions, you can ask which
is more probable to have resulted in the random sample you saw.)
The randomness in the sampling is essential.  When you have it, you wipe out
any underlying bias in the way the collection was created.
Actually, what one tends to prove are things like:  If X is uniformally
randomly distributed over (0,1), then 2X is uniformally randomly distributed
over (0,2).  (On the other hand, X + X, while still random, is *not*
uniformally distributed.)  That's about as close as you are going to get
to a "proof of randomness".
Statistics in general require subtle reasoning.
That may or may not be a meaningful concept.  If I toss a coin, and
on the result, blow up a building - there is no way to repeat the blowing up
of the building, but still it's meaningful to say that the probability that
the building gets blown up is 50%.
majordomo at metzdowd.com

@_date: 2006-11-06 11:32:07
@_author: Leichter, Jerry 
@_subject: Can you keep a secret? This encrypted drive can... 
Just wondering about this little piece.  How did we get to 256-bit
AES as a requirement?  Just what threat out there justifies it?
There's no conceivable brute-force attack against 128-bit AES as far
out as we can see, so we're presumably begin paranoid about an analytic
attack.  But is there even the hint of an analytic attack against AES
that would (a) provide a practical way in to AES-128; (b) would not
provide a practical way into AES-256?  What little I've seen in the
way of proposed attacks on AES all go after the algebraic structure
(with no real success), and that structure is the same in both
AES-128 and AES-256.

@_date: 2006-11-07 11:47:16
@_author: Leichter, Jerry 
@_subject: Can you keep a secret? This encrypted drive can... 
Well, there's a very easy answer to that one:  Tell the manager
involved that the number is the price.  "You can have the industrial
grade one for 128 bucks, or the one done to MIL specs with gold plating
for 256 bucks."  :-)

@_date: 2006-11-08 17:58:41
@_author: Leichter, Jerry 
@_subject: Can you keep a secret? This encrypted drive can... 
Sorry, that doesn't make any sense.  If your HWRNG leaks 64 bits,
you might as well assume it leaks 256.  When it comes to leaks of
this sort, the only interesting numbers are "0" and "all".
No, SHA-1 is holding on (by a thread) because of differences in the
details of the algorithm - details it shares with SHA-256.  I
don't think anyone will seriously argue that if SHA-1 is shown to
be as vulnerable as we now know ND5 to be, then SHA-256 can still
be taken to be safe for more than a fairly short time.
Anything *could* happen, but you haven't actually shown that this
particular pattern has been playing itself out in the hash function
Such calculations are nonsense.  Moore's Law stops working at some
point, as you start to run out of electrons to run through all your
gates.  2^128 isn't just out of our current range; it's out of range
of any technology we have any inkling of today.
BTW, if you really want to push this to the ultimate, there is a
QM result that bounds that number of bit flips that can take
place within a given volume of space-time.  Suppose you start a
brute force attack, and want a result in 100 years.  The computation
must occur within a sphere of space time with spatial radius of
100 light years, and a time extension of 100 years.  (Of course,
this is a gross overestimate, since you presumably want the answer
to come back to you, which means the radius had better be at most
half that.  But this is all very rough anyway.)  When I saw some
results in this direction - sorry, I don't have a reference - I
did a *rough* computation of how many bit flips would fit into
that volume.  It turns out that you can just barely, in principle,
do a 128-bit brute force search - counting only the bit flips to
generate all the possible keys.  By 256 bits, this is completely
out of the question.

@_date: 2006-11-08 18:55:31
@_author: Leichter, Jerry 
@_subject: Can you keep a secret? This encrypted drive can... 
OK, so what argument will you make that, given one of these "leaky",
partially predictable, generators, 128 bits are "too few" but by some
magic 256 are "enough"?  If they really are "enough", why not generate
256 bits and mash them together into 128?
Funny thing about exponential curves in the real world:  They stop
being exponential eventually.

@_date: 2006-11-13 15:15:25
@_author: Leichter, Jerry 
@_subject: Citibank e-mail looks phishy 
They screw things up in other ways, too.  If you have an AT&T Universal
card, you're actually serviced by Citibank these days.  To get to your
account on line, you go to  which very nicely
accepts https connections, using a Verisign cert.  Unfortunately, the
cert is for  or some such address.  (Of course, then it
promptly redirects you to something on accountonline.com.)
I complained to them about this months ago, with (of course) no response.

@_date: 2006-11-14 18:21:38
@_author: Leichter, Jerry 
@_subject: Citibank e-mail looks phishy 
One of Henry Petroski's early books is "To Engineer Is Human: The Role
of Failure in Successful Design".  Petroski argues that we only learn
from failure.  Success tells us how to build exactly the same thing
the next time.  Failure is the inevitable result of pushing out beyond
what you already know.  (Wonderful book, highly recommended.)
It's a curiosity of the financial industries that they repeatedly
forget what they've learned!  Architects design buildings that stay
up.  Engineers build bridges that don't fail when the wind blows.
Doctors abandon treatments that kill patients and don't go back to
them.  In most fields, failures are translated in to "best practices"
that are used to produce codes and rules and educational methods and
such that avoid repeating those failures - and remain in force
pretty much forever (sometimes beyond their useful lifetime, but
that's a different problem).  In "lower finance", there are plenty
of such safety rules - e.g., the person who authorizes the check is
never the person who signs the check - that are followed pretty
consistently.  But the guys in "high finance" all think they know
With every successful shuttle flight, we "learned" that the shuttle
could be flown safely even with ring erosion, at low temperatures, etc.
It usually does take a disaster to change the mindset.

@_date: 2006-10-03 10:18:35
@_author: leichter_jerrold@emc.com 
@_subject: Circle Bank plays with two-factor authentication 
Nah - more clever than what I had (which was meant for an age when you
couldn't carry any computation with you, and things you interacted with
on a day by day basis didn't have displays).
GridCode's idea is quite clever, but the fact that it's ultimately a
simple substitution - a varying simple substitution, but of a fixed
value - seems dangerous.  No obvious (to me!) attacks, though....

@_date: 2006-10-12 16:50:13
@_author: Leichter, Jerry 
@_subject: handling weak keys using random selection and CSPRNGs 
Beyond that:  Are weak keys even detectable using a ciphertext-only
attack (beyond simply trying them - but that can be done with *any* small
set of keys)?  If not, what's the attack?  One could posit an attack
in which some of the plaintext is known, and an attacker could detect
the weak key from known ciphertext/plaintext pairs and then use the
detected key to attack the rest of the ciphertext.  But that's an odd
attack to defend against - why not just try all the weak keys (or,
again, any small subset of keys) and see if they work?
The only kind of weak key that would matter is one that leaves the
ciphertext mainly or entirely unchanged - e.g., leaves most of the
bits unchanged or most of them always flipped.  This suggests that,
rather than looking for weak keys as such, it might be worth it to
do "continuous online testing":  Compute the entropy of the generated
ciphertext, and its correlation with the plaintext, and sound an
alarm if what you're getting looks "wrong".  This might be a
worthwhile thing to have, not just for detecting weak keys, but
to detect all kinds of software and hardware failures.  Since it's
outside of the actual encryption datapath, a bug either fails to
sound an alarm when it should - leaving you where you were without
this new check - or sounds a false alarm, which unless it occurs
too often, shouldn't be such a big deal.

@_date: 2006-10-12 17:24:31
@_author: Leichter, Jerry 
@_subject: handling weak keys using random selection and CSPRNGs 
I'm not sure which direction you want "false positive" to refer to.  If
the input is already random-looking, then any encryption of it, good or
bad (other than very silly ones which grossly expand the input) will
also be random-looking.  So you'll never trip the "output doesn't look
random enough" detector.  On the other hand, some simple correlation
analysis between input and output *might* detect some simple failures.
(You could also look for the non-random pieces, like the headers on
JPG's.  Not sure it's really worth it - about the only time you're
going to find that is when the output and the input are essentially

@_date: 2006-10-13 11:18:34
@_author: Leichter, Jerry 
@_subject: handling weak keys using random selection and CSPRNGs 
Which weak keys would those be?  The DES weak keys are self-inverting:
Encryption and decryption are the same.  The only way to test whether
the ciphertext you are looking at was encrypted with a weak key is
to try to encrypt it again with each of the weak keys and see if
you get something that makes sense.  Of course, for exactly the same
cost, you could *decrypt* with all the weak keys.
For the semi-weak keys, the story is pretty much the same except that
you have pairs of keys to try.
Looking at Wikipedia's summary of cryptosystems with weak keys:
Are you aware of any cryptosystem with weak keys identifiable from
plaintext only?
I have no idea what this means.

@_date: 2006-09-04 10:14:24
@_author: Leichter, Jerry 
@_subject: Debunking the PGP backdoor myth for good. [was RE: Hypothesis: 
Z is universally used to represent the integers.  (From Zahlen, German
for numbers.)  In printed mathematics, Z used this way is taken from a
special "blackboard bold" font.  A common representation uses two
parallel strokes for the Z, with somewhat thickened horizontal bars.
(Back when math was typed on a typewriter, you produced this by typing
Z, backspacing almost but not quite all the way, then typing it again.)
The same font is also used for the reals (R), rationals (Q - from
quotient?) and the complexes (C).  The Hamiltonians are less common, but
you'll sometimes see an H from this font to name them.  N is sometimes
used for the natural numbers (positive integers).  (The naturals are not
much used beyond elementary-school texts....)  The other letters in the
font have no universal meaning, but get used in specialized areas.  I
think I've seen a black-board bold A used for an affine space, for
In all cases, the "usual" operations are assumed, so R is the reals as a
complete ordered field, Z is the ring of integers under the usual
addition and multiplication (with the usual ordering, though there is no
common formal name I know of for a ring with an associated ordering),
and so.
There are a bunch of associated notions, like Z_n (_ for subscript - TeX
notation) for the group of integers mod n under addition.  When n\p is a
prime, Z_p^* (^ for superscript) for the group of integers 1..p mod p
under multiplication.  Z_n is actually a ring under addition and
multiplication mod n, and Z_p a field, and where appropriate, they are
taken to be so.  Q_p is the p-adics, but that's getting specialized.
In ASCII, we don't of course have blackboard bold fonts, but Z is mainly
taken to be the integers, and Z_p is universally taken to be the
integers mod p, in discussions even vaguely related to integer
properties.  R and the others are less commonly used, and you'd have to
understand the context.
Mr. Mikle's notation is ... a bit odd.  What else might one conceivably
substitute for the integers in (Z, p, generator g)?  If it has to be the
integers, why describe this as a triplet?

@_date: 2006-09-07 10:50:24
@_author: Leichter, Jerry 
@_subject: Raw RSA 
If I hand you my public key, I have in effect handed you an oracle that
will compute c^d mod n for any c.  What you are asking is whether you
can then extract my private key e - which is exactly what the security
claims for RSA say you cannot do.  (Note that I chose to call my
public key d and by private key e - but since the two keys are
completely equivalent in RSA, that's just naming.)
RSA is multiplicative, so, yes, this follows easily unless the encoding
used prevents it.

@_date: 2006-09-08 10:40:04
@_author: Leichter, Jerry 
@_subject: Raw RSA 
I don't follow.  For RSA, the only difference between encryption and
decryption, and public and private key, and hence between chosen
plaintext and chosen ciphertext, is the arbitrary naming of one of
a pair of mutually-inverse values as the "private" key and the other
as the "public" key.

@_date: 2006-09-10 09:51:56
@_author: Leichter, Jerry 
@_subject: Raw RSA 
Let R(x) = x^k mod n - where k might be the public key - for encryption -
or the private key - for signing.  R(xy) = R(x)R(y) mod n.  For
encryption, this means little - since the encryption exponent is
public, anyone can compute R(xy) directly anyway.  But for signing,
it means that if I have my hands on signed copies of x and y, I can
forge a signature on xy.  Thus, if I am able to get signatures on a
good collection of primes, I can sign many messages easily.  Yet
another reason to sign hashes of messages, not raw messages - and
that the signer module should compute the hash itself, not rely on
the caller to do it.

@_date: 2006-09-14 14:48:54
@_author: Leichter, Jerry 
@_subject: RSA SecurID SID800 Token vulnerable by design 
I think this summarizes things nicely.  Moving to a higher level of
abstraction:  With a traditional token, if the correct value has been
entered, we can reasonably assume intent on the part of the human
being in possession of the token to identify himself, and thus take
responsibility for some set of actions.  With the additional "something
you know" password associated with the token, we can further reasonably
assume that the person in possession of the token is in fact the person
who has the right to possess that token.*
In the case of a software-readable USB token, *neither* assumption is
reasonable.  The resulting authentication is very different in kind.
Pressing the button supplies exactly the confirmation of intent that
was lost.  (However, it can't get you to the assumptions about "the
right person" having possession of the token.  The fingerprint scanning
technologies that one sees in some USB drives today would probably be
reasonable for that purpose - not that one has much information about
their false positive rates or how hard they really are to attack.
I don't know what the costs are, however - low enough for ~$100 drives,
maybe not low enough for an ID token.)
* Yes, there are attacks that render these assumptions invalid.
Nothing is perfect.

@_date: 2006-09-20 19:14:29
@_author: Leichter, Jerry 
@_subject: Did Hezbollah use SIGINT against Israel? 
Newspaper reports have claimed that many troops were sent into the
field with old equipment - including in particular 10+-year-old
communications equipment.  Something that was fielded in 1996 was
likely designed using the technology of the early '90's.  Portable
communications equipment built with that technology is probably not
secure today.

@_date: 2006-09-21 18:34:58
@_author: Leichter, Jerry 
@_subject: Exponent 3 damage spreads... 
This is a rather peculiar interpretation of the spec.  If I look at a C
specification and it tells me that an integer is a string of digits,
when I write a C compiler, am I permitted to say that "123@ can
be parsed as an "entirely contained" integer, with the "@ "beyond
the scope of the specification"?
The only reasonable reading of the text quoted above is that the D must
consist of, and *only* of, an ASN.1 value of the given type.
Granted, one or more implementations got this wrong.  (Has anyone looked
to see if all the incorrect code all descends from a common root, way
back when?)  Those implementations are as wrong as a C compiler that
skips "irrelevant junk" at the end of an integer constant.  I wouldn't
blame either the spec or the use of ASN.1.  The implementations are
just plain wrong.  There isn't even any plausible interpretation that
would make them right.  (On the one hand, they use all the data,
including the junk, in computing the signature.  On the other hand,
they ignore some of the data when extracting the digest.  It's
clearly one piece of data!)
Granted, this particular design, e=3 and all, *may* be particularly
vulnerable to mistakes.  Until we know whether this is *one* mistake
that was copied from implementation to implementation, or the same
mistake made by multiple developers, it's really premature to draw any
conclusions.  But assume that's so.  Then indeed it's prudent to avoid
e=3 - more because of the likely number of broken implementations out
there that will take forever to move out of active use than for any
other reason.  But let's not draw the wrong conclusion.  Bad code is bad
code.  The error in reasoning here is not all that different from what
happens in various kinds of attacks, where for example the length of a
contained record is marked as larger than the length of the containing
record, is not checked separately - and produces a buffer overflow.
This is also related to an error in the C library design.  When
comparing *for equality*, strcmp(s,t) - ignoring issues about possible
missing trailing NUL's - has a very simple semantics:  It compares two
*strings*.  The corresponding memcmp(s,t,n) is usually thought of as
similarly comparing two memory regions - but that's *not* what it does.
To really compare two memory regions, it would have to have accept two
memory regions as inputs - that is, have separate pointers and a lengths
to give memcmp(s,m,t,n).  As it stands, the programmer given two memory
regions is left with the task of coming up with the right value for n.
In fact, what he *should* do is first compare the two lengths and decide
"not equal" if they differ.  In practice, programmers often use the
minimum of the two lengths for n, which actually tests one region and
one leading substring of another.  Not what they had in mind - and
indeed earlier in this thread we saw exactly this produce buggy code.
(BTW, strncmp(), the fix for unterminated strings, has the same
problem:  Since it has a single maximum length, it no longer has a
simple semantics as a C string comparison function.)
I think you'd be hard pressed to find any spec that addresses such a
thing.  When a spec says "X is an A followed by a B", the "and nothing
else" is implied.
I think you're putting the blame in the wrong place.

@_date: 2006-09-22 10:44:28
@_author: Leichter, Jerry 
@_subject: Exponent 3 damage spreads... 
I have to disagree.  The spec describes an entire certificate, not one
field inside of it.  The issue isn't the TLV encoding of the DigestInfo
- fine, that subfield ended.  It's the next level in the hierarchy, the
D data field I think it was called.  TLV-encoded or not, it exists as a
called-out entity, since it is what the signature is run over.  And
we are told that D consists of DigestInfo and a digest value.  That's
it; no extra bytes at the end.  If this were at the end of the
certificate, and the code *completely ignored* the random trailing
junk - that is, it did *not* include it in the signature - then you
might have an argument.
I would expect a file *player* that pretty much linearly scans a
file, that's designed to be user-friendly, that runs in the
standards-free zone of Windows software, and that doesn't deal with
anything particularly critical, to have a "greedy algorithm" that
just decodes as it goes along.  Compare to Web browsers and their
casual acceptance of all kinds of broken HTML.  In both cases, I
would expect a validator to be much stricter - and I would certainly
expect a security-relevant piece of software, or even something like
a compiler, to be strict.

@_date: 2006-09-22 10:56:41
@_author: Leichter, Jerry 
@_subject: Exponent 3 damage spreads... 
I agree that there are two issues, and they need to be treated
properly.  The first - including data after the ASN.1 blob in the
signature computation but then ignoring it in determining the
semantics - is, I'll argue, an implementation error.  You list only
OpenSSL as definitely vulnerable, NSS as "?", so it sounds like
only one definitive example.  Even if NSS has the same problem, one
has to look at code provenance.  OSS efforts regularly share code,
and code to pick apart data fields is hardly kind of thing that
is going to inspire someone to go out and "do it better" - just
The second - embedded parameter fields - is a much deeper issue.
That's a protocol and cryptographic error.  The implementations
appear to be correctly implementing the semantics implied by the
spec - ignore parameters you don't care about.  This is common
practice, and a fine idea in *most* situations, since it allows
for extensions without breaking existing implementations.  As
we've seen, it's a really bad idea for signed fields with small
exponent, since they have an unexpected malleability.  The
advice to avoid small exponents is fine, but in fact I think this
is a special case of another principle:  Don't act as a signing
oracle.  That was known to be a bad idea quite some time ago,
and it's one reason we sign cryptographic hashes, not raw data.
The curiosity of this bit of misdesign is that it hashes the raw
data, but then, on the way to signing it, turns the field back
into (mainly) raw data!

@_date: 2006-09-28 14:33:57
@_author: Leichter, Jerry 
@_subject: A note on vendor reaction speed to the e=3 problem 
VMS has for years had a simple CHECKSUM command, which had a variant,
CHECKSUM/IMAGE, applicable only to executable image files.  It knew
enough about the syntax of executables to skip over irrelevant metadata
like link date and time.  (The checksums computed weren't cryptographic
- at least the last time I used it, many years ago.  The command was
created to use in patches to provide a quick verification that the file
being patched was "the right one".)  I've always found it surprising
that no one seems to have developed similar tools for Unix - with the
Gnu libraries for portable access to object/ executable files, it could
be done relatively easily.
The general issue here is how to checksum the information, rather than
the raw data.  XML signatures have horrendous problems with this
because XML has many equivalent ways to "say the same thing", and
there is enough information in an XML file for intermediate nodes to
be able to change representation without changing semantics - and for
various reasons, they do so.  (The XML guys try to deal with this by
defining a "canonical representation" for data, and sign *that*.
Unfortunately, there are two competing standards for the "canonical

@_date: 2006-09-28 16:40:28
@_author: Leichter, Jerry 
@_subject: Circle Bank plays with two-factor authentication 
Wow.  A variation of an idea I suggested back in the '70's....  The
problem then was with telephone calling cards.  As those of us old
enough will remember, at one time you didn't have a cell phone with you
at all times (or at any times).  You had to use these things called pay
phones.  Long distance calls were expensive, and you had to dump a whole
bunch of change in to make them work.  Very annoying.  So you got a
calling card, which often charged to your home phone number.  Calling
cards had a fixed PIN on them.  "Shoulder surfers" would hang around
heavily used phones - commuter train stations were a good spot - watch
as you entered your account number/PIN, memorize it on the spot and then
sell it.  These could move remarkably quickly - my wife's PIN was stolen
this way, and in use within seconds after she hung up.  Over the next
hour or so, until the fraud people picked it up, it was used to make
several hundred dollars worth of calls from several locations in New
Anyhow ... my suggestion was that a similar table be printed on the back
of the card.  (I would have put a multi-digit number at each
intersection point and only ask for one value.  All told, I'm not sure
which approach is better - but with good printing technology you can use
much smaller fonts than when you rely on people printing things out
themselves.)  I also suggested that the numbers be printed in a color -
light blue, red against a grey background - that would make it hard to
No one ever did anything like this with phone cards.  Interesting to see
the idea re-invented for a different purpose.  (Hmm, if I'd patented it,
the patent would be running out soon, even assuming I went for the
renewal.)  Now if only they hadn't done the actual implementation so

@_date: 2006-09-28 16:47:32
@_author: Leichter, Jerry 
@_subject: A note on vendor reaction speed to the e=3 problem 
I have yet to see a version of "sum" that understands object or
executable file syntax and skips the "noise" stuff.
Yes, but VMS allow you to abbreviate automatically to the shortest
unique name.  Four characters (for top-level commands) is guaranteed
to be sufficient at least among vendor-provided commands, so "chec"
would always be safe.  In practice, I think "ch" was probably enough,
beating out "sum".  :-)  Of course, you could also abbreviate
option names to the shortest unique value, so ch/i would almost
certainly have given you "checksum/image" in even fewer characters
than a hypothetical "sum -i" option for Unix.  :-) :-)

@_date: 2007-04-23 14:41:13
@_author: Leichter, Jerry 
@_subject: More info in my AES128-CBC question 
Some of the messages in this stream have demonstrated why it can be
difficult to get non-crypto people to listen to advice from crypto
experts:  Cryptography research is, by its nature, a pretty absolute
thing.  We find attacks, we try to eliminate them.  There's a strong
tendency to view *any* attack as significant, so *any* use of a
technique "known to be weak" is frowned on.
However, the issue isn't cryptography, it's security; and security
is a cost/benefit tradeoff.  Some of the other messages in this
thread have already made that point, by looking at some of the
specific tradeoffs that have to be made (usability, efficiency,
time to market, etc.)  However, in this particular case, one can
even analyze the threat quite directly.
Suppose we use AES128-CBC with a fixed IV.  It's clear that the only
vulnerability of concern occurs when a key is reused.  OK, where do
the keys come from?  We're told that they are session keys.  Assuming
that these are generated *correctly* - they are effectively random
independent variables - then you'd need to see 2^64 sessions to get
a 50% chance of a repeated key.  Note:  Note 2^64 *blocks* - something
you might actually get in a reasonable amount of time on the fastest
links - but 2^64 *sessions*.  Is that within the realm of interest
for this protocol?  Maybe, maybe not.  (Most likely not.)
A decent protocol will have authentication and some kind of anti-replay
mechanism.  Even if someone gets hold of two sessions that used the
same key, the authentication mechanism will block attempts to
merge data from the two sessions.  Alternatively, any anti-replay
mechanism will require carrying a nonce of some sort in the stream.
Realistically, this will be sent very early in the session, pretty
much ensuring that even with common keys and a common IV, there will
be little common data.  In fact, a practical recommendation might be
to put the nonce in the first block, in which case it ends up playing
the role of an IV and the whole discussion disappears.
In summary:  Yes, ideally one uses a random IV.  In practice, what this
adds - in many common protocol styles - is robustness of a sort, not
real additional security.  (However, actual robustness of cryptosystems -
robustness against all the common kinds of errors that people make in
design, implementation, fielding, and use - doesn't appear to be within
reach of current techniques.)  If possible, it's certainly better to use
the best practices known - and random or nonce IV's are among those - but getting defensive about how "no one is listening to the crypto
experts" is not appropriate here.  Save that for the really egregious
mistakes - of which there are plenty.

@_date: 2007-04-25 09:27:06
@_author: Leichter, Jerry 
@_subject: More info in my AES128-CBC question 
In the original proposal, the IV was *fixed*:  It was always 0.  As a
result, it wasn't communicated, so could not be manipulated.
Integrity enforcement is required for other reasons anyway (and, based
on later responses, was always part of the protocol).

@_date: 2007-04-27 17:13:44
@_author: Leichter, Jerry 
@_subject: More info in my AES128-CBC question 
This still doesn't describe how the attack works.  It's very special-
ized: A chosen-plaintext attack that can confirm the decryption of a a
previously intercepted block (but cannot directly reveal it).
In the following, all encryptions are with respect to a fixed (but
unknown to the attacker) key.  Earlier, the attacker intercepted I and
E(I xor X), where X is some unknown data and I is the IV that was used
in encrypting it.  (That is, I is simply the previous encrypted block.)
The attacker has a guess Y as to what X is and wishes to confirm it.  He
also knows J, the next IV to be used; and can choose the next block to
be encrypted.  He chooses Y xor J xor I.  The CBC encryptor sends:
The attacker examines this block as it's transmitted, and compares it
to E(I xor X).  If they are the same, then X == Y.
Notice that this requires true *chosen* plaintext - known plaintext does
you no good at all.  And you have to be able to choose any plaintext -
what you're stuffing in there will look essentially random, because J
and for that matter I are essentially random.  Since all you'll see is
the encrypted result, you have to be able to choose *all* the bits.
Since SSH sends the password at a guessable point in the stream, the
block containing is locatable and it has high value.  If an attacker
grabs it, he could try password guessing attacks "through" the
subsequent encryption.
Frankly, for SSH this isn't a very plausible attack, since it's not
clear how you could force chosen plaintext into an SSH session between
messages.  A later paper suggested that SSL is more vulnerable:
A browser plugin can insert data into an SSL protected session, so
might be able to cause information to leak.  Note that for this to be
an interesting attack, we have to posit that the plugin can be caused
to run within a browser during a secure session containing valuable
data, but also that the browser is sufficiently secure that the plugin
can't get at the data directly.  (The Java security model does try to
enforce this kind of separation.)
It's really tough to formalize exactly what is going on here, since
there's this whole funny notion of "knowing the next IV to be used in
time to choose the next plaintext".  I guess we assume that the
implementation doesn't allow you to change the contents of a message
once you've passed it down to the crypto layer; otherwise, you could
in principle apply this attack between every pair of CBC-encrypted
blocks.  Unrealistic?  There were attacks against OS 360 in which
you passed in a channel program, then modified it while it was being
executed, sometimes from the CPU, sometimes by causing one element
in the channel program to do I/O that overwrote another element before
the channel got there.  So I would certainly not dismiss this as
completely impossible.
What the RFC seems to be suggesting is that the first block of every
message be SSH_MSG_IGNORE.  Since the first block in any message is now
fixed, there's no way for the attacker to choose it.  Since the attacker
doesn't know the key, even knowing the IV used with this fixed message
doesn't tell him what IV will be used for the block after, the earliest
one he could specify.  The same IV will likely never by seen again
during the lifetime of the key, so knowing the output of encrypting the
fixed message with that IV is also useless.  Adding some random data to
the ignored block can be done but has no effect on the security.  Note
that the wire overhead is the same here as with sending a new IV: One
block.  You can avoid that by making the IV's computable by a party that
knows the key:  E.g., use E(n) as the IV for the n'th block.  Or even
simpler, take the saved IV and encrypt it an extra time before using it.
The advantage of the RFC's approach is that it works with older peers:
They messages they send remain vulnerable, but the messages a new
implementation sends them are not, but are completely understandable.
If you need to upgrade an already fielded protocol, this is the best you
can hope for - and it's actually very good.
Frankly, while I find this attack interesting as a clever bit of math
and certainly worth knowing about when designing protocols, it's not
something I'd worry about except in very unusual circumstances.

@_date: 2007-04-27 17:42:41
@_author: Leichter, Jerry 
@_subject: More info in my AES128-CBC question 
No, not really, for any reasonable interpretation I can make of
that term.  You can send a message that consists of enough 0 bytes
to be sure that the entire first block is fixed, and you've gotten
all the security you can get against the attack in question.  (If
you're using SSH_MSG_IGNORE to protect against traffic analysis, you
might want to do something different - but that's a completely
distinct attack and the security considerations are entirely

@_date: 2007-04-30 13:50:13
@_author: Leichter, Jerry 
@_subject: Quantum cryptography is hacked 
============================== START ==============================
"Simulation proves it's possible to eavesdrop on super-secure encrypted  							-- Jerry

@_date: 2007-08-23 08:28:28
@_author: Leichter, Jerry 
@_subject: interesting paper on the economics of security 
Information about used cars is available, too:  You can take the car
to an independent mechanic for evaluation - there are mechanics who,
in fact, establish their independence by doing *nothing* but such
inspections, so that there is no suspicion that they are creating
work for themselves.  Histories of cars are available on line.
General information about models of cars is also readily available.
However, there's a non-trivial cost to the consumer to get hold of
this information.  Enough people, enough of the time, are not willing
to pay that cost, to drive the marketplace.
I would argue that the situation is the same for security software.
Only a tiny fraction of the computer-using population reads reviews
of anti-virus software - or could understand anything beyond a table
of raw detection numbers if they had such reviews in their hands.
What drives anti-virus installation is normally (a) what can on
the computer when you bought it; (b) word of mouth with no real
basis in fact; (c) familiarity of the product name.
In fact, the anti-virus field - if you look at the major vendors -
delivers reasonable, and reasonably equivalent, products.  For the
vast majority of people, the difference between having no anti-virus
product and having any of the big ones far exceeds the practical
differences among them.  Where information asymmetry would arise
would be with a new, essentially ineffective, product which would
be pushed out with a large burst of advertising claims, viral
marketing, etc.  Because there are a number of competing incumbents,
however, there isn't much room for someone to play this game:  They
would have to sell so cheaply that the profit wouldn't be there.
The above is for the *PC* antivirus market.  The Mac antivirus market
provides an interesting counterpoint.  Not to get into arguments about
whether a Mac *can* get a virus, in practice, there are none in the
wild today.  So any Mac anti-virus based on scanning has an actual
value of ... nothing, since there is nothing to scan for.  (A
good behavioral monitor might make sense, though building up the
needed models without actual attack examples is difficult.)  Still,
people do sell Mac anti-virus scanners; they even advertise the
size of the scanning databases they come with (vaguely).  In that
submarket, asymmetry of information clearly plays a role.
However, let's go back to the more general question.  Anti-virus
programs can at least be tested - whether against huge (and thus
meaningless) collections of viruses, or against viruses that are
known to be threats.  But that's hardly the only security software
out there.  Encryption software is a hell of a lot harder to test,
and in fact I've yet to see a *meaningful* test outside of the
specialist literature.  Oh, people will talk about the ease of
use of the software, and they'll parrot the makers claims about
how many bits of key they use; but whether the thing provides any
actual security ... who knows?  Asymmetry of information is the
rule here, which is why "snake oil" continues to be sold regularly.
Other security products fall somewhere in between.  Firewalls
don't seem to get much testing, though their funtions should be
reasonably easy to test - and explain.  But firewalls seem to be
seen as part of the plumbing that most people don't, and don't
want to, know anything about.  Intrusion detection systems are,
as far as I can tell, basically black boxes.  The algorithms and
rules are proprietary, no one really knows how to test them, and
you buy on the reputation of the vendor.  Highly asymmetrical.

@_date: 2007-08-23 13:10:05
@_author: Leichter, Jerry 
@_subject: interesting paper on the economics of security  
The whole eBay model hearken's back to traditional approaches
centering on trusted third parties and "letters of introduction",
which morph into the eBay reputation system.  Not too surprising -
the problems are the same:  Dealing at very-long-arms-length with
others with whom you may never establish an ongoing relationship -
so that the traditional controls based on trade-offs between
today's gain and lost future dealings with the cheated customer
don't apply.
eBay is by no means perfect, but their reputation system does
surprisingly well.  It's not something that is likely to warm a
true anarchist's/pure capitalist's heart, though:  It works
because eBay - a government in most practical senses - polices
quite aggressively and sometimes arbitrarily.
I wonder if there's a business to be made in running an eBay-style
reputation system for web sites in general....

@_date: 2007-12-06 18:12:38
@_author: Leichter, Jerry 
@_subject: Intercepting Microsoft wireless keyboard communications 
Computerworld coverage at
The main protection against interception is the proprietary protocol,
which these guys were able to reverse engineer.  The exchange is
"encrypted" using a Caeser cipher (XOR with a single byte that is the
common key, which is the only secret in the system); they say they can
determine the right key within 30 characters or so.  Their current
hardware can read the data from 33 feet away; with a better antenna,
well over a hundred feet should be possible.  These things operate at
27 MHz, which will penetrate walls easily.
Reading multiple keyboards at once is possible and they already do it.
They are looking at injecting data into the stream - presumably not very
Many other brands of wireless keyboard may well be equally vulnerable.
 							-- Jerry

@_date: 2007-12-10 15:32:59
@_author: Leichter, Jerry 
@_subject: Intercepting Microsoft wireless keyboard communications 
Somewhere - perhaps in the Computerworld article - someone mentions that
some devices use Bluetooth, "and are therefore much more secure".
In practice, most Bluetooth devices don't even agree on a non-zero
key when pairing, so just using Bluetooth is no promise of anything.
Does anyone know how good Bluetooth security can potentially be -
and is it practically attainable in the low power/lost message
context that would be needed here?  How are some of the emerging
low-power protocols (e.g., ZigBee) dealing with this?

@_date: 2007-12-10 16:03:14
@_author: Leichter, Jerry 
@_subject: More on in-memory zeroisation 
I don't have te C89 spec handy (just the C99 spec, which is laid
out differently), but from what I recall, this construct guarantees
nothing of the sort.
Most standard library functions can be implemented as macros.  Using the
construct (f)(args) guarantees that you get the actual function f,
rather than the macro f.  However, that doesn't say anything about
whether f is actually invoked at run time.  That comes under the "acts
as if" rule:  If the compiler can prove that the state of the C
(notional) virtual machine is the same whether f is actually invoked or
not, it can elide the call.  Nothing says that memset() can't actually
be defined in the appropriate header, as a static (or, in C99, inline)
function.  Then the compiler can look at the implementation and "prove"
that a memset() to a dead variable can be elided....

@_date: 2007-12-10 16:17:38
@_author: Leichter, Jerry 
@_subject: Flaws in OpenSSL FIPS Object Module 
I was going to ask the same question.  My answer:  This proves yet again
how far we are from a servicable ability to produce secure software.
Software that's been through the FIPS process has been vetted to the limits
of our current abilities under the constraints of being even vaguely
commercially viable.  OpenSSL is open source software that's been around
for a long time, examined by many, many people.  It had a very rough
journey through the FIPS process, so was presumably checked even more
than software that just breezes through.  Even so ... it had a security
bug.  It's hard to suggest something that could have been done differently
to guarantee that this couldn't happen.  Anyone who might argue - as I'm
sure they will - that this "proves you should use commercial software
rather than OSS if you need security" is speaking nonsense - that's not
at all what this incident is about.
It is, of course, the height of irony that the bug was introduced in the
very process, and for the very purpose, of attaining FIPS compliance!

@_date: 2007-12-11 10:25:54
@_author: Leichter, Jerry 
@_subject: Intercepting Microsoft wireless keyboard communications 
Keep-alives are a bad idea.  It's interesting how they keep hitting
limitations of various tecshnologies.  Back in the '70's, I worked on
the design of some protocols to be used by terminals.  In Europe, the
only remote access mechanism you could use was X.25, provided by the
local PTT.  If you had two buildings across the street from each other,
you were legally forbidden from running a wire between them - the PTT
had a monopoly.  An X.25 charges were per packet - a keep-alive would
cost you a fortune.  In fact, sending ACK's could double your costs!
I don't know enough about low-power design to guess where the power
costs actually go.  If there is a significant cost in just starting
the transmitter, then sending two bytes might not use up anything
like twice the energy of sending just one (though it will certainly
use *some* more).
Realistically, some key codes are much, much more frequent than others.
A simple Huffman encoding of the keycodes should let you get an average
message length well below the two bytes that I think are the standard.
(Yes, there are variations due to keyboard usage, but even a fairly
generic encoding should give you reasonable compression.)
No Huffman code is a prefix of another, so you can tell when you've
reached the end of a code.  Then you can use the remaining bits of a
2-byte block for redundancy.  (Of course, some rare keys will require
sending an extra block.)  Just sending as many bits as you can of a
count of messages the keyboard thinks it sent would probably be a pretty
effective check.
These days, you can probably cost-effectively put enough memory
in a keyboard to make this possible.

@_date: 2007-12-11 15:38:11
@_author: Leichter, Jerry 
@_subject: PlayStation 3 predicts next US president 
This whole discussion has an air of unreality about it.
Historically, notary publics date from an era when most people couldn't
read or write, and hardly anyone could afford a lawyer.  How does
someone who can't read a document and can perhaps only scrawl an X
enter into a contract?  In the old days, he took the written contract
to a notary public, who would read it to him, explain it, make sure
he understood it, then stamp his scrawled "X".  The notary's stamp
asserted exactly the kind of thing that we discuss on this list as
missing from digital signatures:  That the particular person whose
"X" was attached (and who would be fully identified by the notary)
understood and assented to the contents of the contract.
Today, we assume that everyone can read, and where a contract is at all
complex, that everyone will have access to a lawyer.  (Of course, this
assumption is often invalid, but that's another story.)  The requirement
for a notary public's stamp is a faded vestige.  For certain important
documents, we still require that a notary sign off, but what exactly
that proves any more is rather vague.  Yes, in theory it binds a
signature to a particular person, with that signature being on a
particular document.  That latter is why the notary's stamp is a
physical stamp through the paper - hard(er) to fake.  Of course, most of
the time, the stamp is only applied to the last page of a multi-page
contract, so proves only that that last page was in the notary's hands -
replacing the early pages is no big deal.  I think I've seen notaries
initial every page, but I've also seen notaries who don't.
In practice, whenever I've needed to have a document notarized, a quick
look at some basic ID is about all that was involved.  It's quite easy
to get fake ID past a notary.  Given the trivial fee paid to a notary -
I think the limit is $2 in Connecticut - asking the notary to actually
add much of value is clearly a non-starter.
The financial industry has actually created its own system - I forget
the name, some like a Gold Bond Certification - that it requires for
certain "high-importance" transactions (e.g., a document asserting you
own some stock for which you've lost the certificates).  I've never
actually needed to get this - it's appeared as a requirement for
some alternative kinds of transactions on forms I've had to fill out
over the years - so I don't know exactly how it works.  However, it's
completely independent of the traditional notary public system, and
is run through commercial banks.
Trying to justify anything based on the current role of notary
publics - at least as it exists in the US - is a waste of time.

@_date: 2007-12-11 16:00:42
@_author: Leichter, Jerry 
@_subject: Flaws in OpenSSL FIPS Object Module 
Agreed.  In fact, this fits with an observation I've made in many
contexts in the past:  Any time you introduce a new mode of operation,
you are potentially introducing a new failure mode corresponding to
it as well.  Thus, bulkhead doors on sidewalks are unlikely to open
under you because the only mode of operation they try to support has
the doors opening upward.  I would be very leary of stepping on such
a door if it could *ever* be opened downward.
There's a famous story - perhaps apocryphal - from the time IBM
introduced some of the first disk packs.  They did great for a while,
but then started experiencing head crashes at a rate much higher than
had ever been seen in the development labs.  The labs, of course,
suspected production problems - but packs they brought in worked just
as well as the ones they'd worked with earlier.
Finally, someone sitting there, staring at one of the test packs and
at a crashed disk from a customer had a moment of insight.  There was
one difference between the two packs:  The labs pulled samples
directly off the production line.  Customers got packs that had gone
through QA.  The last thing QA did was put a "Passed" sticker on the
top disk of the pack.
So ... take a pack with a sticker and spin it up.  This puts G forces
on the sticker.  The glue under the sticker slowly begins to migrate.
Eventually, some of it goes flying off into the enclosure.  If it
gets under a head ... crash.
Actually, now that this failure mode has been demonstrated, it would
be a good idea to test for it.  It's harder to do with just binaries,
but possible - look at the recent analyses of the randomization in
Vista ASLR.

@_date: 2007-12-11 16:14:29
@_author: Leichter, Jerry 
@_subject: More on in-memory zeroisation 
In practice, with an existing compiler you are not in a position to
change, these kinds of games are necessary.  If you're careful, you look
at the generated code to make sure it does what you expect.
But this is a very bad - and potentially very dangerous - approach.
You're relying on the stupidity of the compiler - and on the compiler
not become more intelligent over time.  Are you really prepared to
re-check the generated code every time the compiler is rev'ed?
There sometimes needs to be an explicit way to tell the compiler that
some operation *must* be done in some way, no matter what the compiler
thinks it knows.  There's ample precedent for this.  For example,
floating point arithmetic doesn't exactly follow the usual laws of
arithmetic (e.g., it's not associative, if you consider overflows), some
if you know what you are doing in construction an FP algorithm, you have
to have a way to tell the compiler "Yes, I know you think you can
improve my code here, but just leave it alone, thank you very much."
And all programming languages that see numerical programming as within
their rubric provide standardized, documented ways to do just that.  C
have "volatile" so that you can tell the compiler that it may not elide
or move operations on a variable, even when those operations have no
effects visible in the C virtual machine.  (The qualifier was added
to support memory-mapped I/O, where there can be locations that look
like memory but have arbitrarily different semantics from normal
memory.)  And so on.
You can almost, but not quite, get the desired effect for memory zero-
ization with "volatile".  Something more is needed, and software that
will be used to write cryptographic algorithms needs access to that
"something more" (to be pinned down explicitly).

@_date: 2007-12-11 17:07:05
@_author: Leichter, Jerry 
@_subject: More on in-memory zeroisation 
If the function is defined as I suggested - as a static or inline - you
can, indeed, takes its address.  (In the case of an inline, this forces
the compiler to materialize a copy somewhere that it might not otherwise
have produced, but not to actually *use* that copy, except when you take
the address.)  You are allowed to invoke the function using the address
you just took.  However, what in that tells you that the compiler -
knowing exactly what code will be invoked - can't elide the call?
By the way, you might wonder what happens if two different CU's take
the address of memset and we then compare them.  In this kind of
implementation, they will be unequal - but in fact nothing in the
Standard says they can't be!  A clever compiler could have all kinds
of reasons to produce multiple copies of the same function.  All you
can say is that if two function pointers are equal, they point to the
same function.  No converse form is provable within the Standard.
You might try something like:
(I *think* I got that syntax right!)
Then you can invoke (*p_memset).  But if you do this in the same
compilation unit, a smart compiler that does value propagation could
determine that it knows where p_memset points, and that it knows what
the code there is, so it can go ahead and do its deeper analysis.
in one compilation unit and then:
will keep you safe from single-CU optimizations, but nothing in the
Standard says that's all there are.  Linker-based optimizations
could have the additional information that nowhere in the program
can p_memset be changed, and further that p_memset is allocated to
regular memory, and in principle the calls could be elided at that
point.  Mind you, I would be astounded if any compiler/linker system
actually attempted such an optimization ... but that doesn't make
it illegal within the language of the Standard.
In principle (I'll grant you, probably not in practice), it can
provie quite a bit - and certainly enough to justify eliding the

@_date: 2007-12-12 10:28:29
@_author: Leichter, Jerry 
@_subject: More on in-memory zeroisation 
Where does it say it *can't* define them?  How could a Standard-conforming
program tell the difference?  If no Standard-conforming program can
tell the difference between two implementations, it makes no difference
what you, as an omniscient external observer, might know - they are either
both compatible with the Standard, or neither is.
It makes not difference.
What do you think "the definition of pointer equality" actually is?  Keep
in mind that you need to find the definition *in the Standard*.  The
*mathematical* definition is irrelevant.
Look, I write practical programs all the time - mainly in C++ recently,
but the same principles apply.  My programs tend to be broadly portable
across different compilers and OS's.  I've been doing this for close to
30 years.  I stick to the published standards where possible, but there's
no way to avoid making assumptions that go beyond the standards in a few
cases:  Every standard I know of is incomplete, and no implementation I've
ever worked with is *really* 100% compliant.
It's one thing to point out a set of practical techniques for getting
certain kinds of things done.  It's another to make unsupportable arguments
that those practical techniques are guaranteed to work.  There's tons of
threaded code out there, for example.  Given the lack of any discussion of
threading in existing language standards, most of them skate on thin ice.
Some things are broadly agreed upon, and "quality of implementation"
requirements make it unlikely that a compiler will break them.  Other
things are widely believed by developers to have been agreed upon, but
have *not* really be agreed upon by providers.  Programs that rely on
these things - e.g., that C++ function-scope static initializers will be
run in a thread-safe way - will fail here and there, because in fact
compiler developers don't even try to support them.  Because of the ever-
growing importance of threaded programs, this situation is untenable, and
in fact the language groups are starting to grapple with how to incorporate
Security issues are a similar issue.  The fact is, secure programming
sometimes requires primitives that the standards simply don't provide.
Classic example:  For a long time, there was *no* safe way to use
sprintf(), since there was no a priori way of determining how long the
output string might be.  People had various hacks, but all of them could
be fooled, unless you pretty much re-implemented sprintf() yourself.
snprintf() fixed that.
There is, today, no way to guarantee that memset() will be run, within the
confines of the standard.  This is a relatively minor oversight - C has
seen such issues as important since volatile was introduced well before
the language was standardized.  I expect we'll see some help on this in
a future version.  In the meanwhile, it would be nice if compiler
developers would agree on some extra-Standard mechanisms.  The gcc hack
could be a first step - but it should be written down, not just something
a few insiders know about.  Standards are supposed to grow by standardi-
zing proven practice, not by innovation.
The problem with unsupportable assumptions that some hack or another
provides a solution is that they block *actual* solutions.  By all means
use them where necessary - but push for better approaches.

@_date: 2007-12-12 16:43:33
@_author: Leichter, Jerry 
@_subject: More on in-memory zeroisation 
I'm not sure what you are trying to prove here.  Yes, I believe that
in most implementations, this will print "Hello world\n".  Is it,
however, a strictly conforming program (I think that's the right
standardese) - i.e., are the results guaranteed to be the same on
all conforming implementations?  I think you'll find it difficult
to prove that.
BTW, it *might* not even be true in practice if you build your program
as multiple shared libraries!

@_date: 2007-12-14 14:36:01
@_author: Leichter, Jerry 
@_subject: More on in-memory zeroisation 
While good for existing crypto code, this is exactly the kind of thing
that's a problem.  We now have a well-distributed bit of folk knowledge
that memset(x,0,y) is treated specially by the compiler.  It isn't; this
"knowledge" is just repeated inaccurate rumor.  Fortunately, "not
treated specially" in this case defaults to a case that does what we
want - but it also means that if someone makes the "code has no effect"
analyzer smarter in some release of gcc, all of a sudden, these memset()
calls that we're relying on may suddenly just disappear from the
generated code.  How long before anyone notices?  It's not as if the
change log will show "optimize away dead calls to memset" - it will
likely contain some obscure comment like "improve recognition that type
B subtrees can be collapsed in phase 3".
The only *safe* way to write code like this - absent explicit support
in the standard - is with explicit support in each particular compiler.
Even something like:
 always_call memset
ugly as it is, would work.

@_date: 2007-12-19 09:40:33
@_author: Leichter, Jerry 
@_subject: crypto class design 
Your Buffer class is a step up from using a void*!  You're not really
using data typing effectively.  Define classes to encapsulate encrypted
and cleartext data; carefully decide what transitions are allowed among
them; and define your API around that.  Note that transitions include
creation and, particularly, deletion - the destructor for cleartext
should zero the memory.
The above is a simplification.  There are probably more than two
categories of data.  A better classification might be:  Encrypted,
cleartext but sensitive, non-sensitive.  In a financial setting,
"sensitive" may have subdivisions based, for example, on who is allowed
access.  Should there be some special datatype for keys, which are about
the most sensitive thing in the system?  (It should probably be the case
that the common public API's provide no way to export a key, just a way
to apply it.  Key management should be a separate API that most
applications don't even use, so you can be sure they can't (without
cheating, which is of course always possible in C++) leak them.)
As much as possible, make the actual rules that apply to any piece of
data in the program (a) transparent to someone reading the code; (b)
enforceable by a compiler or, second best, the API implementation.  In
the public API, concentrate on the data and the rules that govern it.
Particular crypto algorithms and various related choices should be
hidden within the implementation.  Not only should the API be easy to
use correctly, it should be as hard as possible to use *in*correctly!

@_date: 2007-02-04 23:27:00
@_author: Leichter, Jerry 
@_subject: data under one key, was Re: analysis and implementation of LRW 
...thus illustrating once again both the allure and the uselessness (in
almost all situations) of one-time pads.  Consider:  I have 4 GB of
data that must remain secure.  I'm afraid it may leak out.  So I
generate 4 GB of random bits, XOR them, and now have 4 GB of data
that's fully secure.  I can release it to the world.  The only
problem is ... what do I do with this 4 GB of random pad?  I need
to store *it* securely.  But if I can do that ... why couldn't I
store the 4 GB of original data security to begin with?
*At most*, if I use different, but as secure as I can make them,
methods for storing *both* 4 GB datasets, then someone would have to
get *both* to make any sense of the data.  In effect, I've broken my
secret into two shares, and only someone who can get both can read it.
I can break it into more shares if I want to - though if I want
information-theoretic security (presumably the goal here, since I'm
worried about future attacks against any technique that relies on
something weaker) each share will end up being the same size as
the data.
Of course, the same argument can be made for *any* cryptographic
technique!  The difference is that it seems somewhat easier to protect
a 128-bit key (or some other reasonable length anything beyond 256 is
just silly due to fundamental limits on computation:  At 256 bits, either
there is an analytic attack - which is just as likely at 2560 bits, or
running the entire universe as computer to do brute force attacks won't
give you the answer soon enough to matter) than a 4 GB one.  It's not
easy to make really solid sense of such a comparison, however, as our
ability to store more and more data in less and less space continues
for a couple of generations more.  When CD's first came out, 600 MB
seemed like more than anyone could imagine using as raw data.  These
days, that's not enough RAM to make a reasonable PC.
I would suggest that we look at how such data has traditionally been
kept safe.  We have thousands of years of experience in maintaining
physical security.  That's what we rely on to protect the *last* 70
years worth of X-ray plates.  In fact, the security on those is pretty
poor - up until a short while ago, when this stuff started to be digital
"from birth", at least the last couple of year's worth of X-rays were
sitting in a room in the basement of the hospital.  The room was
certainly locked, but it was hardly a bank vault.  Granted, in digital
form, this stuff is much easier to search, copy, etc. - but I doubt
that a determined individual would really have much trouble getting
copies of most people's medical records.  If nothing else, the combination
of strict hierarchies in hospitals - where the doctor is at the top -
with the genuine need to deal with emergencies makes social engineering
particularly easy.
Anyway ... while the question "how can we keep information secure for
70 years" has some theoretical interest, we have enough trouble knowing
how to keep digital information *accessible* for even 20 years that it's
hard to know where to reasonably start.  In fact, if you really want to
be sure those X-rays will be readable in 70 years, you're probably best
off today putting them on microfiche or using some similar technology.
Then put the 'fiche in a vault....

@_date: 2007-02-05 00:07:50
@_author: Leichter, Jerry 
@_subject: deriving multiple keys from one passphrase 
The property you presumably want is that knowing n generated keys gives
you "no information" about the n+1'st generated key.  The usual way
that's proposed to do this is to take the master secret, concatenate
some identifying data - say k encoded in 32 bits for the k'th secret -
and run the result though a cryptographic one-way function.  This
*seems* like it ought to work - "you can't invert the function", etc.,
etc. - but even if the one-way function has all the usual properties,
it's not clear they really give you the property you need.  (In fact,
the arguments have the flavor of older arguments that you can "easily"
construct a keyed checksum from a hash by "just" prepending, or
appending, secret keying material.  In fact, you need to use something
more clever, like HMAC.   Using HMAC here looks like a good try, but
it's no clear that it does the trick either.)
Of course, there is a primitive that does have the right property,
namely an encryption function.  Suppose we took k and decrypted it
with the secret master key.  Any good encryption function will
certainly have, as part of its assumed properties, that seeing the
decryption of n blocks gives you no information about the decryption
of any other block (at least as long as n is small compared to the
2^(block size), which it certainly will be).
He's perhaps relying Rabin's "fingerprinting using random polynomials"
results.  What it basically says is that, even though it's easy to
attack a CRC any *known* polynomial, if I choose a polynomial *at
random* and keep it secret, your chance of modifying something I
checksum with that CRC and getting away with it is essentially just
1 in 2^n (for an n-bit checksum).  Of course, I have to keep both
the polynomial and the checksum secret; and there is some math involved
to pick a primitive polynomial randomly.
However, coupling all this with seeding a PRNG makes it highly likely
that he's cross the line from "obviously no vulnerabilities" to "no
obvious vulnerabilities".

@_date: 2007-02-06 10:06:42
@_author: Leichter, Jerry 
@_subject: man in the middle, SSL 
Recall how SiteKey works:  When you register, you pick an image (from a
large collection) and a phrase.  Whenever you connect, the bank will
play back the image and phrase.  You aren't supposed to enter your
password until you see your own image and phrase.
The usability problem found in the study was that if you build a login
page with the image and phrase replaced by something else that seems to
go that - like a notification about a systems upgrade, or maybe an ad for
a bank service - most people (90%?) will just go ahead and enter their
password anyway.
Unfortunately, the "all ads all the time" nature of today's web sites
has conditioned people not to expect *anything* to remain constant.
We're used to judging the trustworthiness of those with interact with
in the real world by various invariant marks and other features.  If
you go to your bank and find the signs have all changed, you will at
the least be a bit suspicious.  At a web site - who would think twice?
SiteKey tries to use something that's invariant but unique to you.
That's a distinction people clearly don't make automatically.  Whether
with sufficient training and experience they will learn to do so
remains to be seen.  (BofA is very consistent in telling you *never*
to enter your password without first checking for your image and
phrase.  Clearly, though, it hasn't clicked for people.)
Of course, SiteKey isn't the full answer - if I know your login name,
I can try to log in to BofA and get a copy of your image and phrase.
What SiteKey at best prevents is broad-based non-personalized attacks.
Automating "skimming" of SiteKey information using some virus is a
plausible attack, and we'll see it eventually if it appears worth
someone's while.
Combined with some of the other reports coming out about the lack of
effectiveness of EV cert indicators (why *that* surprises anyone is
beyond me) and of pretty much every other technique that anyone has
proposed so far, it's clear that the battle against phishing is going
to be long and hard, and that victory is very far from clear.
In architecture, there is the notion of a building have "human scale".
Places built ignoring that notion feel overwhelming.  (Sometimes that's
the point, of course.)  The Internet, as it's evolved to this point,
clearly lacks "human scale".  People's intuitions quick responses, all
the things we've evolved and learned to deal with the real world, don't
match the world of the web.  Until we can figure out how to bring human
capabilities and limitations into the picture much more effectively and
thoroughly than we have so far, things are going to get much worse.

@_date: 2007-02-14 10:19:20
@_author: Leichter, Jerry 
@_subject: Failure of PKI in messaging 
This is an excellent point - completely obvious once made (and I know
you've made it before, but for whatever reason, the inverted relation-
ship between certifier and signer/relying party never quite sank in
for me).
It's interesting to follow up on this idea, because it shows just how
profound the problem is.  Imagine starting a business that ran a PKI
and did business the old way:  You would charge someone *presenting*
an alleged certificate for an "OK".  The "OK" would, for the fee paid,
provide insurance against the possibility of fraud.  (Presumably, the
fee would be based on the size of the insured transaction and level
of experience and trust you have in the signing party.)  It's to
your advantage to have many parties whose signatures you vouch for,
since that's what brings you customers; so you probably don't charge
that side of the business - though it helps someone to have a "high
trust" signature, since their customers will like paying a lower
premium to do assured business with them, so you could charge on
that side in some cases.  But, unlike the case today, since your
own money is at stake if you vouch for someone untrustworthy, you
can't just go hand certs out to anyone who shows up at your door.
In the business-to-business case, things have worked like this (more
or less) for years.  This is pretty much what Dun and Bradstreet do,
for example (though they don't do the actual insurance part - they
rely on their own reputation to provide as much assurance as is needed
for typical transactions).  But can we even imagine a situation in
which Internet shoppers were willing to *pay* - even a nominal amount
- for assurance that the Amazon page they hit really was Amazon's?
There are at least two levels of established practice in the way:
This analysis indicates yet again why this is, and will likely remain,
an intractable problem.

@_date: 2007-02-15 10:10:21
@_author: Leichter, Jerry 
@_subject: Failure of PKI in messaging 
On the other hand, the push/pull combination of spam and IM/SMS are well
on their way to killing Internet mail.  Spam being what it is, the
notion that "anyone can send mail to anyone" is naive.  Unsolicited mail
stands a good chance of ending up tossed by a spam filter.  The volume
of spam is so high that few people even bother to review the stuff
caught, if their mail provider even provides a mechanism to do that.
Meanwhile, the next generation of users is growing up on the immediacy
of IM and text messaging.  Mail is ... so 20th century.
I think the whole notion of decentralizing *everything* has turned out
to be a trap.  Yes, it makes for great cryptography and system design to
find ways to do without a trusted third party.  But the resulting
systems just don't fit the way people think and work.  Trust has
*always* been based on personal contact, extended to organizations that
work hard to have a "human face" on the one hand, and to various
human-scale, humanly-transparent ways of reifying and rendering portable
the smile and the handshake, from letters of credit to various business
rating organizations (D&B, BBB), and so on.  Replacing that with some
abstract cryptographic system that no one understands, no one can see or
touch - and that ultimately can only be perceived as trustworthy if it
comes from trustworthy institutions anyway - is just a non-starter.
With this shaky base, it should perhaps not come as a surprise that
after all these years of trying, we haven't managed to come up with
human interfaces to these systems that actually allow them to work
effectively in the human world.
Meanwhile, in real terms, it would be interesting to know what
percentage of Email these days flows *between* organizations, and what
percentage remains within individual organization's Exchange servers.
With all the rules already enforced by typical Exchange-using
organizations - not to mention all the new rules being added as
first "compliance" and now "evidence retention and destruction" regs
and the upcoming "information leakage management", more and more
Email systems are taking on the characteristics of the old closed
systems, with only a thin, closely watched pipe connecting them out
to the Internet.

@_date: 2007-02-16 17:05:19
@_author: Leichter, Jerry 
@_subject: New Credit Cards May Leak Personal Information 
This was reported a couple of months back.  (In fact, if you follow
the links, they get you to a draft version of the report from October
of last year.)
What struck me in this whole story was:

@_date: 2007-01-22 07:24:05
@_author: Leichter, Jerry 
@_subject: Private Key Generation from Passwords/phrases 
I've heard of one alleged case, over 20 years ago, of what appeared to
be an actual collision in Unix hashed passwords.  Some undergrads at
Yale somehow came into possession of the root password on the department
Unix server.  The story - I wasn't directly involved and can't vouch
for the details - was that one of the students involved noticed that
his hashed password exactly matched the root hashed password - including
the salt, of course.
It's interesting to look at some of the issues here.  The chance of a
matching pair of passwords *somewhere* gets you into birthday paradox
territory, so isn't all that unlikely; in fact, across the population
of Unix systems, even then, it was probably close to a certainty.  Of
course, knowing that two unspecified users, perhaps in distinct domains,
have the same hashed password, is not generally very useful.  The
chance of a match *with a particular user* - and of course root is the
user of greatest interest, though there would likely be other users
involved in administration whose passwords would be almost as useful
to know - is much less likely (linear as opposed to quadratic), and is
a possibility that is usually ignored:  If I know that root's hashed
password matched that of some user on another machine, what do I do
with that information?  Well ... in a university setting, I might
well be able to approach that other person and, especially in a more
innocent time, get him to share his password with me.
Even so, the probabilities are likely against me.  But I, again in the
world of 20+ years ago, there was another factor:  Dictionary attacks
were not considered plausible at the time, so there was little reason
to choose what we would today consider "good" passwords.  As I recall,
the root passwords on the Yale machines at that time were words - in
fact, names of ocean creatures.  (I think the compromised password was
"dolphin".)  Since students were also probaby choosing words from the
dictionary - and, within the confines of a single department at a single
school at a single time, were probably much more likely than random
chance would predict to pick the same word, as the same concepts and
words were "in the shared air" - the  effective search space was immensely smaller than that implied by the hashed password size.  In this setting,
the "chance dictionary search" becomes at least plausible.
A great illustration of the need to consider the full system setting!
(Note that against this particular "attack", a considerably larger
salt would have been quite effective at little cost.)

@_date: 2007-01-30 16:10:47
@_author: Leichter, Jerry 
@_subject: Intuitive cryptography that's also practical and secure. 
This is a common misconception.  The legal system does not rely on
lawyers, judges, members of Congress, and so on understanding how
technology or science works.  It doesn't rely on them coming to accept
the trustworthiness of the technology on any basis a technologist would
consider reasonable.  All it requires is that they accept the authority
of experts in the subject area, and that those experts agree "strongly
enough" that the mechanism is sound.
How many people understand DNA matching?  How much do you think *you*
understand about DNA matching?  Could you name a single reagent used in
doing a DNA match?  Could you distinguish between a good match and a bad
match?  If someone handed you one of those pictures of different bands
on an electrophoresis plate, could you tell if it was real or faked?
Does any of this influence your faith in the validity of DNA matching as
a forensic technology?
Just as DNA matching can be explained in very simple, if fundamentally
very limited terms, as something like fingerprint matching only more
sophisticated, one can easily explain hashing in pretty much the same
terms.  It would not be hard to find highly credentialed experts who
would testify as to the worth, applicability, and general acceptance by
those in the field, of the technique.  Sure, lawyers on the other side
of a case trying to gain acceptance for hashing could probably find
*someone* to cast doubt on it - but it's unlikely they would be very
good expert witnesses - and in the end that's what determines the
Well, there will always be tin-hatters out there who will doubt
absolutely everything.  We rely on the police to hold on to evidence
concerning the people charged with crimes - who are sometimes corrupt
cops, politicians who control police funds, etc., etc.  There are
procedural safeguards around the chain of custody of materials.
When it comes to records of decided cases, the courts hold on to this
stuff.  Just how secure are *their* facilities?  There is rarely reason
for anyone to mount a concerted attack against them.  If you're worrying
about the NSA modifying stored evidence, what makes you think they would
have much trouble mounting a black-bag attack against some court's
storage room somewhere?
There are a number of very troubling issues about this series of cases
and the way the courts have allowed them to be handled (so far; history
shows that the courts, just like the other branches of government, are
very protective of what they perceive as their domain of responsibility,
and they tend to take back their roles).  But I'm not particularly
concerned about the NSA using some secret technique to find a second
preimage of a hash of the evidence.  Of course, the practical
difficulties of even getting to the point of being able to compute a
hash over a large collection of papers, books, various kinds of records,
and likely some other pieces of physical evidence is considerable....

@_date: 2007-01-30 16:50:00
@_author: Leichter, Jerry 
@_subject: Intuitive cryptography that's also practical and secure. 
I doubt *anything* would eliminate the conspiracy theorists.  Intuitive
cryptography or otherwise, any convincing argument that the records
had *not* been tampered with would require careful examination - and
conspiracy theorists don't carefully examine evidence *against* their
Actually, it's well known that aliens controlled both Lee Harvey Oswald
and Jack Ruby - their control over Ruby was slipping, he was about to go
public revealing what he know, so having Ruby kill Oswald did a great
job of covering up the ongoing invasion.
These aliens presented a take-it-or-leave it surrender document to
President Truman at Area 51 shortly after WW II.  Kennedy was about to
start an aggressive campaign against them - as, later was Robert
Kennedy, which is why the aliens arranged his death, too....
(What was the name of the TV series a number of years back that was
built on this premise?  Not very good, but cleverly done.)

@_date: 2007-07-01 23:09:16
@_author: Leichter, Jerry 
@_subject: The bank fraud blame game 
You do realize that you've just come down to what the TPM guys want to
build?  (Of course, much of the driving force behind having TPM comes
from a rather different industry.  We're all happy when TPM can be
used to ensure that our banking transactions actually do what the bank
says it will do for a particular set of instructions issued by us and
no one else, not so happy when they ensure that our "music transactions"
act the same way....)
Realistically, the only way these kinds of devices could catch on would
be for them to be standardized.  No one would be willing to carry one
for their bank, another for their stock broker, a third for their
mortgage holder, a fourth for their credit card company, and so on.
But once they *are* standardized, almost the same potential for
undesireable uses appears as for TPM's.  What's to prevent the movie
download service requiring that you present your Universal Safe Access
Fob before they authorize you to watch a movie?  If the only significant
differences between this USAF and TPM is that the latter is more
convenient because more tightly tied to the machine, we might as well
have the convenience.
(This is why I find much of the discussion about TPM so surreal.  The
issue isn't the basic technology, which one way or another, in some form,
is going to get used.  It's how we limit the potential misuses....)

@_date: 2007-07-06 16:27:25
@_author: Leichter, Jerry 
@_subject: "What Banks Tell Online Customers About Their Security" 
America customer, but unlike her I've started using their on-line
services.  What got me to do it was descriptions of the increasing
vulnerability of traditional paper-based mechanisms:  If I pay a
credit card by mail, I leave in my mailbox an envelope with my
credit card account number, my address, a check with all the
banking information needed to transfer money - and probably a
bunch of other envelopes with similar information.  Yes, I could
carry it to a post box or even a post office, but the inconvenience
is getting pretty large at that point.  Meanwhile, the on-line
services have some unique security features of their own, like
the ability to send me an email notification when various conditions
are met, like large transactions.
 							-- Jerry
What Banks Tell Online Customers About Their Security
- Sarah D. Scalet, CIO
May 29, 2007
By the end of 2006, U.S. banks were supposed to have implemented "strong
authentication" for online banking - in other words, they needed to put
something besides a user name and password in between any old Internet
user and all the money in a customer's banking account.
The most obvious way to meet the guidance, issued by the U.S. Federal
Financial Institutions Examination Council (FFIEC), would have been to
issue one-time password devices or set up another form of two-factor
authentication.  But last summer, when I did a preliminary evaluation of
security offerings at the country's largest banks, I was pretty
unimpressed. (See Two-Factor Too Scarce at Consumer Banks
Since then, I've given up on getting a one-time-password device,
and have accepted the fact that banks are instead moving toward what
might diplomatically be called "creative" authentication.
(See Strong Authentication: Success Factors
 Given that
man-in-the-middle attacks can circumvent two-factor authentication, a
combination of device authentication, additional security questions and
extra fraud controls doesn't seem like a bad approach.
But, I wondered, almost six months past the FFIEC deadline, what are
banks telling customers about online security?  As the chief financial
officer of Chateau Scalet - and as a working mother about to have baby
No. 2 - I wanted to know if any of them could offer me enough assurance
that I would take the online banking plunge as a way to simplify my
life. I decided it was time to update my research from last year.
I called the call centers at each of the top three banks, identified
myself as a customer with a checking and savings account, and told them
I was interested in online banking but concerned about security. The
point, yes, was to see what type of security each bank had in
place. More than that, however, I wanted to see how well each bank was
able to communicate about security through its call center. After all,
what good is good security if you can't explain it to your customers?
Here's what I learned. Citibank My first call was to Citibank. I started
with my standard question: "How can I be assured that my online banking
transactions are secure and private?"  The call center rep said that
Citibank uses 128-bit encryption, which "verifies that you have a
maximum level of security." End of answer. Pause. I asked what kinds of
protections Citibank had in place for making sure that it would really
be me logging onto my account. "I'm sorry," he said, "but I don't
understand your question."
We had a language barrier, he and I. The call-center rep, in India, was
not a native English speaker. The call went poorly, and I have no way of
knowing whether this was because of our communications barrier or simply
because Citibank hadn't instructed him how to answer questions about
security. I repeated my question a couple times, and he finally said,
"Let me look into that, ma'am." I waited on hold more than a minute, and
when he came back, he told me I could go online and read all about
online banking. "All the information is there, ma'am," he said politely.
I kept prodding. I asked if Citibank offered tokens or did device
recognition of some sort, and he told me I could log on with a user name
and password.
"At any computer where I punch in my user name and password, I'll have
full access to my account?" I asked.
"Yes, ma'am, anyplace you have Internet access," he answered. He finally
did say that in certain situations I would be asked extra security
questions, but he wouldn't or couldn't explain when that happened or
why. I asked if it was unusual for him to field calls about security,
and he said yes. I finally ended the call in frustration.  Chase Next I
called Chase. This time I got a woman in Michigan, who at least didn't
try to shunt me off onto the Internet - well, at least right away. But
she seemed to interpret my every question about security as one about
how, precisely, I could sign up for online banking. In fact, the first
thing she did was congratulate me on being interested in the service.
When I asked how I could be assured that my transactions would be secure
and private, she said that when I signed up, I would select a user name
and password. "Once you're enrolled, as long as you're not giving out
your user ID and password, you should be safe," she said. At least she
said should and not will.
Then I asked if Chase would do any authentication beyond user name and
password, like identifying my computer or giving me a one-time password
device. She seemed to think that I was worried about the log-on process
being burdensome or confusing - and proceeded to make the process even
more burdensome and confusing, with a convoluted answer about speeding
up the telephone verification process. At one point, she had me so
utterly baffled that she asked, "Are you O.K.?"
One thing I did manage to glean - I think - is that there would be some
kind of activation code involved if tried to log on at a library or a
friend's house.  Her explanation: "It's called an activation code
because it's like a reset," she said. "That is for security purposes."
She said this code could be sent by e-mail or text message, or that I
could call in to get it. But she wouldn't or couldn't explain its
It wasn't until 10 minutes into the call that she mentioned that I might
have to answer extra security questions on occasion, and again, she
couldn't or didn't explain what these questions were for, or even
reassure me that the measures were there to protect me. When I asked
what would happen if someone else transferred money out of my account,
she said, "That's not going to happen, ma'am, unless you give that
information out to somebody." Then she warned me to be careful about
giving out my information - to merchants, of all places.
Credit her with being a diligent salesperson, though. Throughout the
process, she kept trying to get me to establish an online account, right
then and there, so that the first time I went onto Chase.com, all I'd
need would be that precious user name and password.  Bank of America My
call with Bank of America also got off to a rocky start. I wanted to
record all three phone calls. (Why not?  The banks do it for "quality
assurance purposes".) Both the Citibank and Chase representatives agreed
to this without hesitation. The Bank of America rep, however, put me on
hold for more than seven minutes, before coming back and saying I
couldn't record the call - something something the bank only records
calls for training purposes something something. Oh well. It didn't seem
worth arguing.
Things got better after that. When I asked how I could be assured that
my online transactions would be private and secure, the call center rep
seemed to understand exactly what I was asking. First, she said that I
should look for the lock at the bottom of my browser window, indicating
a secure site, and noted that the encryption that Bank of America uses
is "one of the highest."  (Neither of these are perfect indicators of
security, of course, but it's a logical place to start the
conversation.) Then, she told me that, usually, the only time my account
wouldn't be secure is if I gave out my user name and password, or
"answered a spam e-mail" where I clicked a link and entered my user name
and password. This made her the only rep to actually warn about phishing
attacks; she gets extra points for not using the silly term phishing.
Next, she launched into a very plain-English description of SiteKey,
Bank of America's system of allowing customers to verify that they are
at the valid website by selecting a picture that will come up each time
they log on. "If you don't see the picture, don't enter your password,"
she told me. She also explained that when I signed up for the first
time, I'd have to answer three extra security questions. If I (or anyone
else) ever tried to access my account from a different computer, I would
first be asked a security question.  If I answered correctly, I'd see my
security picture and then be asked for my user name and password. If I
answered it incorrectly a certain number of times, I would be locked out
and have to go through extra verification at the call center to have the
account unlocked.
Overall, I was impressed at how comfortable she was talking about
security. It seemed to be part of the training she had gone through, and
she also made several references to how she used the service
herself. Call it a subtle kind of marketing if you will, but I actually
liked to hear her admit, "A lot of times people say they have a hard
time getting into our site as opposed to other sites, and that's because
it's a very secure site."  The Verdict Here's the recap:
    1. Citibank: Call-center rep did not seem to understand my questions
       and tried to refer me to the website for answers.
    2. Chase: Call-center rep didn't offer clear explanations but kept
       trying to get me to sign up anyway.
    3. Bank of America: Call-center rep understood my questions,
       explained customer-facing security mechanisms and offered advice
       about how I could protect myself.
After the calls, I rang Larry Freed, president of the research group
ForeSee Results ( to see what he
thought. Freed is a former banking CTO who does a regular survey on
banking customer satisfaction in conjunction with Forbes.com. He has
told me in the past that customers who have not signed up for online
banking often cite security as a factor.
Online banking is a huge area of growth for banks - if they can get it
right.  According to Freed's latest survey, customers who are not doing
online banking report an overall satisfaction level of 70 on a scale of
0 to 100. For those who do online banking and bill pay, the satisfaction
level jumps to 79. What's more, those who are doing online banking and
bill pay are much more likely to purchase additional services from the
bank - 59 percent likely, rather than 36 percent.
Nevertheless, Freed didn't seem surprised that the banks, for the most
part, had so little to say about online security. "The education and
communication of security is not done very well," he said. "For
converting non-online banking customers, I think that's a critical
step. But it's a balance between putting the fear in them and educating
Right now, I'd say, the banks are doing neither.
As for me, if I had a Bank of America account already, I think I'd give
online banking a try. It's not so much that I'm convinced Bank of
America actually has better security than Citibank or Chase. The
call-center rep doesn't know that, and none of the banks are going to
talk about all their security mechanisms anyway. But I'm heartened that
they're teaching their call-center reps how to explain their security
mechanisms to customers. At this point in history, it's a sad fact that
merely being willing and able to talk about security in plain English
(even if they don't want the call to be recorded) puts Bank of America
well ahead of its competitors. That's just not enough to make me change
banks, though. Guess I'll keep buying stamps after all.
  2007 CXO Media Inc.

@_date: 2007-07-10 09:17:23
@_author: Leichter, Jerry 
@_subject: How the Greek cellphone network was tapped. 
It's going to be interesting to see the effect of the iPhone in this
area.  While nominally a closed system like all the handsets that
preceded it, in practice it's clear that people will find ways to load
their own code into the things.  (As of yesterday - less than two weeks
after the units shipped - people have already teased out how to get to
the debugging/code patching interface and have extracted the internal
passwords.  The community doing this would make a fascinating study in
and of itself - an international group coordinating through an open IM
line, tossing around ideas.)  There's plenty of CPU power available, and
a fairly standard environment.  (In fact, recent reports hint that the
chip contains a hardware accelerator for Java.)
Between encrypted VOIP over WIFI and eventually over broadband cell -
keeping people from running voice over their broadband connections is a
battle the telco's can't win in the long run - and just plain encrypted
cell phone calls, I think in a couple of years anyone who wants secure
phone connections will have them.  There will be tons of moaning about
it from governments - not to mention the telco's, though for them that
will be a triviality compared to all the other things they will lose
control over - but no one is going to be able to put this genie back
in the bottle.
Also, right now, the technology to build a cell phone is still
specialized and capital-intensive.  But today's leading-edge chip and
manufacturing technology is tomorrow's commodity.  Ten, twenty years
from now, anyone will be able to put together the equivalent of today's
iPhone, just as anyone can go down to Fry's today and build themselves
what was a high-end PC a couple of years ago.  You can't quite build
your own laptop yet, but can that be far off?  A "gray box" cellphone
might not compete with what you'll be able to buy from the leading-edge
guys of the day, but it will be easily capable of what's needed to do
secure calling.
So - who's going to write the first RFC for secure voice over cell, thus
circumventing the entire government/telco/PTT standards process?  We're
not quite ready for it to take off, but we're getting close.

@_date: 2007-07-10 09:28:20
@_author: Leichter, Jerry 
@_subject: Historical one-way hash functions 
So, you want to be able to prove in the future that you have some piece of
information today - without revealing that piece of information.  We all
know how to do that:  Widely publish today the one-way hash of the
Well ... it turns out this idea is old.  Very old.  In the 17th century,
scientists were very concerned about establishing priority; but they
also often wanted to delay publication so that they could continue to
work on the implications of their ideas without giving anyone else the
opportunity to do it.  Thus, in 1678, Robert Hooke published an idea he
had first developed in 1660.  Even then, he only published the
following:  ceiiinosssttuu.  Two years later, he revealed that this was
an anagram of the Latin phrase "Ut tensio sic uis" - "as the tension so
the power" - what we today call Hooke's Law of elastic deformation.
(This story appears in Henry Petroski's "The Evolution of Useful
 							-- Jerry

@_date: 2007-07-17 13:11:41
@_author: Leichter, Jerry 
@_subject: How the Greek cellphone network was tapped. 
I won't disagree with you here.  Most people don't perceive voice
monitoring as a threat to them - and if you're talking about monitoring
by many governments and by business intelligence snoopers, they are
perfectly correct.  (I say "many governments" because those governments
that actively monitor and control large portions of their citizenry
hardly make a secret of that fact, and citizens of those countries
just assume they might be overheard and act accordingly.  The citizens
of, for lack of a better general phrase, the Western democracies, are
quite right in their assessment that their governments really don't care
about what they are saying on the phone, unless they are part of a very
small subpopulation involved, whether legitimately or otherwise, in
politics or intelligence or a couple of other pretty well understood
Selling protection against voice snooping to most people under current
circumstances is like selling flood insurance to people living in the
desert.  If you're an insurance hacker - like a security hacker - you
can point out that flash floods *can* happen, but if they are so rare
that no one is likely to be affected in their lifetime, your sales
pitch *should* fail.
What will change things is not the technology but the perception of a
threat.  Forty years ago, the perceived threat from airplane hijacking
was that it was non-existent, and no one would consider paying the cost.
Today, we play a very significant cost.  The threat is certainly
greater, but the *perceived* threat is orders of magnitude beyond even
The moment the perceived threat from phone eavesdropping exceeds some
critical level, the market for solutions (good and, of course,
worthless) will materialize.  As you note, in the military and
intelligence community, the real and perceived threats have been there
for years.  And the crypto hackers will perceive a threat whether it
exists or not.
I'd guess that the next step will be in the business community.  All it
will take is one case where a deal is visibly lost because of "proven"
eavesdropping ("proven" in quotes because it's unlikely that there will
really be any proof - just a *perception* of a smoking gun - and in fact
it could well be that the trigger case will really be someone covering
his ass over a loss for entirely different reasons) and all of a sudden
there will be a demand for strong crypto on every Blackberry phone link.
Things have a way of spreading from there:  If the CEO's need this, then
maybe I need it, too.  If "it" is expensive or inconvenient, I may feel
the need, but I won't act on it.  But the CEO's will ensure that it
isn't inconvenient - they won't put up with anything that isn't
invisible to them - and technology will quickly drive down the cost.

@_date: 2007-06-11 11:24:16
@_author: Leichter, Jerry 
@_subject: Why self describing data formats: 
I suspect the main reason designers use self-describing formats is the
same reason Unix designers tend to go with all-ASCII formats:  It's
much easier to debug "by eye".  Whether this is really of significance
at any technical level is debateable.  At the social level, it's very
important.  We're right into "worse is better" territory:  Self-
describing and, especially, ASCII-based protocols and formats are much
easier to hack with.  It's much easier to recover from errors in a
self-describing format; it's much easier to make "reasonable" inter-
pretations of incorrect data (for better or worse).  Network lore makes
this a virtue:  "Be conservative in what you send, liberal in what you
accept."  (The first part gets honored in the breach all too often, and
of course, the second is a horrible prescription for cryptography or
security in general.)  So software to use such protocols and formats
gets developed faster, spreads more widely, and eventually you have an
accepted standard that's too expensive to replace.
The examples are rife.  HTML is a wonderful one:  It's a complex but
human-readable protocol that a large fraction (probaby a majority) of
generators get wrong - so there's a history of HTML readers ignoring
errors and "doing the best they can".  Again, this is a mixed bag - on
the on hand, the web would clearly have grown much more slowly without
it; on the other, the lack of standardization can cause, and has caused,
problems.  (IE6-only sites, raise your hands.)
Looked at objectively, it's hard to see why XML is even a reasonable
choice for many of its current uses.  (A markup language is supposed to
add semantic information over an existing body of data.  If most of
the content of a document is within the markup - true of probably the
majority of uses of XML today - something is very wrong.)  But it's
there, there are tons of ancilliary programs, so ... the question that
gets asked is not "why use XML?" but "why *not* use XML?"  (Now, if I
could only learn to relax and stop tearing my hear every time I read
some XML paper in which they use "semantics" to mean what everyone
else uses "syntax" for....)

@_date: 2007-06-13 16:56:08
@_author: Leichter, Jerry 
@_subject: Inadvertent Disclosure 
Interesting-looking article on how users of P2P networks end up sharing
much more than they expected:   							-- Jerry

@_date: 2007-06-22 11:33:38
@_author: Leichter, Jerry 
@_subject: Quantum Cryptography 
The unique thing the "Q" provides is the ability to detect eaves-
dropping.  I think a couple of weeks ago I forwarded a pointer to
a paper showing that there were some limits to this ability, but
even so, this is a unique feature that no combination of existing
primitives can provide.  One can argue about what this adds.  The
current approach of the QKD efforts is to assume that physical
constraints are sufficient to block MITM, while quantum contraints
block passive listening (which is assumed not to be preventable
using physical constraints).  It's the combination that gives you
One can argue about the reasonableness of this model - particularly
about the ability of physical limitations to block MITM.  It does
move the center of the problem, however - and into a region (physical
protection) in which there is much more experience and perhaps
some better intuition.  Valid or not, it certainly is easier to
give people the warm fuzzies by talking about physical protection
than by talking about math....
In the other direction, whether the ability to detect eavesdropping lets
you do anything interesting is, I think, an open question.  I wouldn't
dismiss it out of hand.  There's an old paper that posits related
primitive, Verify Once Memory:  Present it with a set of bits, and it
answers either Yes, that's the value stored in me or No, wrong value.
In either case, *the stored bits are irrevokably scrambled*.  (One
could, in principle, build such a thing with quantum bits, but beyond
the general suggestions in the original paper, no one has worked out how
to do this in detail.)  The paper uses this as a primitive to construct
"unforgeable" subway tokens:  Even if you buy a whole bunch of valid
tokens, and get hold of a whole bunch of used ones, you have no way
to construct a new one.  (One could probably go further - I don't
recall if the paper does - and have a "do the two of you match"
primitive, which would use quantum bits in both the token and the
token validator.  Then even if you had a token validator, you couldn't
create new tokens.  Obviously, in this case you don't want to scramble
the validator.)

@_date: 2007-06-22 20:21:25
@_author: Leichter, Jerry 
@_subject: Quantum Cryptography 
My previous message was not an attempt to defend the companies that are
out there trying to sell quantum cryptography.  They're clearly way out
ahead of any reasonable theory and are following in a great tradition
of offering crypto snake oil.  That some of them are doing it on *my*
money - i.e., by selling stuff to the government - hardly makes me
However, just because there are many people in there to make a buck,
and others who are naive about the state of the art - having come over
from a different field (not something new either; look of the
papers mathematicians published when public-key first came into public
view) - doesn't mean there might not be valid and potentially useful
ideas to be found here.  The question was:  Does QK as it currently
exists offer anything that isn't available with conventional crypto?
The answer is clearly yes.  It offers two things:
If you want to attack the vendors of quantum key distribution equipment
for selling high-priced snake oil, fine.  They are hardly alone in
this field - and if their equipment doesn't *add* security, at least
it doesn't seem to remove it (if you use it properly).  Likewise,
if you want to attack papers by physicists who don't understand the
problem, that's fine - they *should* be attacked, because that's the
way science works.  Many of these guys are quite clever, and they'll
All I'm responding to is the self-congratulating commentary whose
starting point is "these problems have all been solved, there's
nothing at all new here".  That's not true.
BTW, on the quantum subway tokens business:  In more modern terms,
what this was providing was unlinkable, untraceable e-coins which
could be spent exactly once, with *no* central database to check
against and none of this "well, we can't stop you from spending it
more than once, but if we ever notice, we'll learn all kinds of
nasty things about you".  (The coins were unlinkable and untraceable
because, in fact, they were *identical*.)  Now, of course, they
were also physical objects, not just collections of bits.  The same
is true of the photons used in quantum key exchange.  Otherwise,
it wouldn't work.  We're inherently dealing with a different model
here.  Where it ends up is anyone's guess at this point.

@_date: 2007-06-25 10:24:09
@_author: Leichter, Jerry 
@_subject: Free Rootkit with Every New Intel Machine 
Apple included TPM chips on their first round of Intel-based Macs.
Back in 2005, there were all sorts of stories floating around the net
about how Apple would use TPM to prevent OS X running on non-Apple
In fact:
Amit Singh, the author of the definitive reference on OS X internals,
has written and distributed an OS X driver for the TPM on those
machines that have it.  For all kinds of details, see his page at:

@_date: 2007-06-27 10:50:40
@_author: Leichter, Jerry 
@_subject: TPM, part 2 
All your data belong to us.  From Computerworld.
 							-- Jerry
Trusted Computing Group turns attention to storage
Chris Mellor
June 24, 2007 (TechWorld.com) The Trusted Computing Group has announced
a draft specification aimed at helping block unauthorized access to
sensitive data on hard drives, flash drives, tape cartridges and optical
disks. These devices won't release data unless the access request is
validated by their own on-drive security function.
David Hill, a principal in the Mesabi Group, said: "The public media
blares the loss of confidential information on large numbers of
individuals on what seems a daily basis, and that is only the tip of the
data breach iceberg for not having trusted storage. Trusted storage will
soon be seen as a necessity --not just a nice to have -- by all
The Trusted Computing Group (TCG) is a not-for-profit industry-standards
organization with the aim of enhancing the security of computers
operating in disparate platforms. Its draft, developed by more than 60
of the TCG's 2175 member companies, specifies an architecture which
defines how accessing devices could interact with storage devices to
prevent unwanted access.
Storage devices would interact with a trusted element in host systems,
generally a Trusted Platform Module (TPM), which is embedded into most
enterprise PCs. The trust and security functions from the specification
could be implemented by a combination of firmware and hardware on the
storage device. Platform-based applications can then utilize these
functions through a trusted command interface negotiated with the SCSI
and ATA standards committees.
Thus a server or PC application could issue access requests to a disk
drive and provide a key, random number or hash value. The drive hardware
and/or firmware checks that this is valid and then supplies the data,
decrypting it if necessary. Future versions of the SATA, SCSI and SAS
storage interfaces would be extended to support the commands and
parameters needed for such access validity checking.
Mark Re, Seagate Research SVP, said: "Putting trust and security
functions directly in the storage device is a novel idea, but that is
where the sensitive data resides. Implementing open, standards-based
security solutions for storage devices will help ensure that system
interoperability and manageability are greatly improved, from the
individual laptop to the corporate data center." Seagate already has an
encrypting drive.
Marcia Bencala, Hitachi GST's marketing and strategy VP, said:
"Hitachi's Travelstar mobile hard drives support bulk data encryption
today and we intend to incorporate the final Trusted Storage
Specification as a vital part of our future-generation products."
The TCG has formed a Key Management Services subgroup, to provide a
method to manage cryptographic keys.
Final TCG specifications will be published soon but companies could go
ahead and implement based on the draft spec.

@_date: 2007-06-27 10:54:26
@_author: Leichter, Jerry 
@_subject: The bank fraud blame game 
As always, banks look for ways to shift the risk of fraud to someone -
anyone - else.  The New Zealand banks have come up with some interesting
wrinkles oh this process.  From Computerworld.
 							-- Jerry
NZ banks demand a peek at customer PCs in fraud cases
Stephen Bell
June 26, 2007 (Computerworld New Zealand) Banks in New Zealand are
seeking access to customer PCs used for online banking transactions to
verify whether they have enough security protection.
Under the terms of a new banking Code of Practice, banks may request
access in the event of a disputed transaction to see if security
protection in is place and up to date.
The code, issued by the Bankers' Association last week after lengthy
drafting and consultation, now has a new section dealing with Internet
Liability for any loss resulting from unauthorized Internet banking
transactions rests with the customer if they have "used a computer or
device that does not have appropriate protective software and operating
system installed and up-to-date, [or] failed to take reasonable steps to
ensure that the protective systems, such as virus scanning, firewall,
antispyware, operating system and antispam software on [the] computer,
are up-to-date."
The code also adds: "We reserve the right to request access to your
computer or device in order to verify that you have taken all reasonable
steps to protect your computer or device and safeguard your secure
information in accordance with this code.
"If you refuse our request for access then we may refuse your claim."
InternetNZ was still reviewing the new code, last week, executive
director Keith Davidson told Computerworld.
"In general terms, InternetNZ has been encouraging all Internet users to
be more security conscious, especially ... to use up-to-date virus
checkers, spyware deletion tools and a robust firewall," Davidson says.
"The new code now places a clear obligation on users to comply with some
pragmatic security requirements, which does seem appropriate. If fraud
continues unabated, then undoubtedly banks would need to increase fees
to cover the costs of fraud," he says, so increasing security awareness
and compliance in advance is probably the better tactic for both banks
and their customers.
"Bank customers who are unhappy with the new rules may choose to
dispense with electronic banking altogether, and return to dealing with
tellers at the bank.  But it seems that electronic banking and in
particular Internet banking has become the convenient choice for
consumers," Davidson says.
The code also warns users that they could be liable for any loss if they
have chosen an obvious PIN or password, such as a consecutive sequence
of numbers, a birth date or a pet's name; disclosed a PIN or password to
a third party or kept a "written or electronic record" of it. Similar
warnings are already included in the section that deals with ATM and
PINs for Eftpos that was issued in 2002.
There is nothing in this clause allowing an electronic record to be held
in a password-protected cache -- a facility provided by some commercial
security applications.
For their part, the banks undertake to provide information on their
websites about appropriate tools and services for ensuring security, and
to tell customers where they can find this information when they sign up
for Internet banking.
"One issue we have raised with the Bankers Association in the past is
that banks should not initiate email contact with their customers,"
Davidson says.
The code allows banks to use unsolicited email among other media to
advise of changes in their arrangements with the customer, but Davidson
says they should only utilize their web-based mail systems.
"It is hardly surprising that some people fall victim to phishing email
scams when banks use email as a normal method of communication, and
therefore email can be perceived as a valid communication by end users,"
he says.

@_date: 2007-06-27 12:24:34
@_author: Leichter, Jerry 
@_subject: anti-RF window film 
Real life follows fiction?  There was a Law and Order episode a year or
two back in which a high-tech company used some alleged technology like
this - a fine mesh of wires over the windows.  (An important clue was
one of the detectives noticing that the mesh had been disturbed.
Someone had replaced the wires in a small region with black thread, then
hid a cell-phone repeater outside the window.  As I recall, the reason
for doing was just your typical hacker "you try to stop me, I'll get
around you" trick.)
There were also reports not that long ago of a paint that provided
RF shielding.  On a more refined basis, there was some kind of
material suitable for walls that had embedded antennas.  You cut
them for a particular frequency range, and they provided very good
shielding in that range.
There is clearly a demand for this kind of thing.  New technologies
are making a hash of the old (sometimes not so old!) rules.  Two
With all this going on, the desire to just finesse the whole problem
by physically blocking signals is certainly only going to grow.
Interesting times.

@_date: 2007-06-27 13:46:18
@_author: Leichter, Jerry 
@_subject: The bank fraud blame game  
Actually, we don't really disagree with the rest of your message, and
I'm not claiming some kind of conspiracy.  This isn't really a power
play because the banks hold all the cards.  Perhaps We're reading
different parts of the message I forwarded.  Consider:
OK, I could live with that as stated.  But:
The delay between when you were defrauded and when they request
access is unspecified.  Who knows what's happened in the meanwhile?
Perhaps as a result of my experience, I stopped using on-line banking,
and as a result decided it wasn't worth keeping all the (obviously
ineffective) software up to date.  This is just too open-ended a
requirement.  "All reasonable steps?"  Just what *are* all reasonable
steps?  I think I know more than most people about how to keep systems
secure, but I'd be at a loss to make a list that could reasonably
be called "all reasonable steps".  (Actually, my list would probably
include "don't use IE or Outlook".  Is that "reasonable"?)
On-line access is on its way to become a necessity.  EZ-Pass in New York
(electronic toll collection) now charges $2/month if you want them to
send you a printed statement - go for all on-line access, and it's free.
Hardly a "necessity" yet, but this is a harbinger.  (Meanwhile, the
percentage of EZ-Pass only lanes at toll plazas keeps rising.  You don't
*need* to use EZ-Pass, if you're willing to incur significant delays.)
This is not just wrong, it's *dangerously* wrong.
As we've discussed here many times, banks' mail messages are incredibly
hazardous, and teach entirely the wrong things.

@_date: 2007-05-01 17:30:33
@_author: Leichter, Jerry 
@_subject: 128 bit number T-shirt? 
Even more amusing:  It's now possible to get stamps produced from
your own picture.  Imagine sending letters in response to such
notices with such a stamp on the envelope....

@_date: 2007-05-09 18:04:20
@_author: Leichter, Jerry 
@_subject: More info in my AES128-CBC question 
I hope it's a cryptographically secure PRNG.  The attack doesn't require
any particular IV, just one known to an attacker ahead of time.
However, cryptographically secure RNG's are typically just as expensive
as doing a block encryption.  So why not just encrypt the IV once with
the session key before using it?  (This is the equivalent of pre-pending
a block of all 0's to each packet.)

@_date: 2007-05-09 18:11:03
@_author: Leichter, Jerry 
@_subject: More info in my AES128-CBC question 
Just being able to generate traffic over the link isn't enough to
carry out this attack.  You have to be able to get the sender to
encrypt a chosen block for you as the first thing in a packet.  How
would you do that?  Suppose there was an "echo" command that would
cause the receiver to send back (within the encrypted channel) whatever
data you asked.  Well, how do you get an "echo" command inserted into
the encrypted, presumably authenticated, flow going back the other
The browser SSL attack could work because plugin code runs *within* the
browser - which knows the key - and it can add material to the "red"
(plaintext) connection data.  How do you propose mounting the attack
given only access to the "black" connection data?
I'm not saying there couldn't be such an attack, or that it's not
worth defending against - just that it appears to be very hard to
pull off.

@_date: 2007-05-09 19:03:05
@_author: Leichter, Jerry 
@_subject: More info in my AES128-CBC question 
I guess my proposal was ambiguous.  You don't use the encryption of
the *initial* IV for each packet; you use the encryption of what you
would otherwise have used as the IV, i.e., the last ciphertext block
of the previous packet.  The IV of the packet after that is just as
variable as it ever was.  (If it were not, then CBC would be just
about useless:  The CBC encryption of, say, the second block of two
all 0 blocks would always be the same!)

@_date: 2007-05-13 22:19:56
@_author: Leichter, Jerry 
@_subject: More info in my AES128-CBC question 
I have no clue what this means.
Again, we're talking about a particular attack, which requires (a) not
known, but chosen plaintext; (b) more, *adaptive* chosen plaintext:  You
have to be in a position to choose the plaintext for the next block that
will be encrypted *after* you've seen the ciphertext of the previous
Nothing like e-mail is going to work, since even supposing you could grab
the last packet on the link and get your mail message to be the next
thing to get sent over the link:  The first block of a mail message as
a server delivers it is never going to be completely under your control,
and in fact it's unlikely you can control any of it.  (If what you meant
by "1-for-1 correspondence between plaintext and encrypted packets",
then I guess that mail doesn't come close.)
You can certainly come up with artificial scenarios where such an attack
would work.  For example, suppose you know that the victim it tailing
some file, and you can write to that file.  Then by appending to the
file you are inserting chosen plaintext into his datastream.  Maybe you
could come up with some analogous attack around an RSS feed, but I'm
skeptical - you have to be able to choose every byte of the first block
or the attack is impossible.  Further, suppose you can choose the first
block, but only some fraction of the time.  Well ... without some other
signal, you can't tell if the first block failed to match your target
because your target was wrong, or because you didn't manage to get your
block in.  So now you have to try repeatedly.  What was always a brute-
force search just got multiplied by some factor.
Again, it's not that this isn't a potentially significant attack.  It's
that the combination of special circumstances you need to pull it off -
a block known to have high value; a small enough set of possible values
for that block that you have hope of guessing it correctly before the
key is reused; enough pauses in the datastream to actually let you try
enough probes to get a significant probability of confirming a guess;
the ability to insert random packets into the plaintext repeatedly
without it being noticed - limits the plausible attack scenarios to a
rather small set.  One can worry about everything or one can try to
field real systems.

@_date: 2007-11-05 14:09:08
@_author: Leichter, Jerry 
@_subject: Hushmail in U.S. v. Tyler Stumbo 
In previous cases of the government somehow magically gaining access to
"securely encrypted" data, it eventually turned out that the government
had compromised the target's machine and installed a key logger, or some
other piece of software to record the relevant secret information.  So
far, I've seen no information ruling this kind of thing out.  It's in
the government's interest to keep its methodology as secret and
mysterious as it can.
A common mistake is looking at PGP or Hushmail or some other kind of
secure mail system and saying "only I can read my my mail.  Not even
close to true:  Unless you're doing all your decryption with a pencil
and a piece of paper, it's your *computer* that can read your mail.
And today's computers simply cannot be treated as trusted.
None of which argues against alternative possible scenarios, such as
the "turned" correspondent at the other end of the mail interchange.
The fact is, we just don't know how this information was obtained.
We *may* learn more as the result of discovery leading up to trial.
It's generally difficult for the government to keep out of the record
the methods they use to obtain evidence, as doing so will tend to
taint the evidence and make it inadmissible.  I'm sure there are
plenty of lawyers looking closely at how to struture things to keep
as many details hidden as possible, however.  The fact that information
came from a "confidential informant" has to be revealed, but the
identify of that informant can generally be kept concealed.  Someone
will argue that the decrypted data plays the role of the "confidential

@_date: 2007-11-13 14:06:51
@_author: Leichter, Jerry 
@_subject: People side-effects of increased security for on-line banking 
Sometimes the side-effects are as significant as the direct effects....
 							-- Jerry
Story from BBC NEWS: Fears over online banking checks By Mark Ward Technology Correspondent, BBC News website
Complicated security checks could be undermining confidence in online
banking, warn experts.  Security extras such as number fobs, card
readers and password checks might make consumers more wary of net bank
websites, they fear.  The warning comes as research shows how phishing
gangs are targeting attempts to grab customer login details.  But the UK
body overseeing net banking says figures show criminals are getting away
with less from online accounts.  Security check
In a bid to beat the bad guys many banks have added extra security
checks to the login name and password typically used to get access to an
Some, such as Lloyds, have trialled number generating key fobs and
Barclays is trialling chip and pin card readers. Others have tried
systems that check a customers PC and then ask that person to select
which image they chose from a set they were shown previously.
But, said Garry Sidaway from ID and authentication firm Tricipher, all
these checks could be making consumer more nervous about using online
"The banks have to make this channel secure," he said, "but there is
crumbling confidence in it."
Andrew Moloney, financial services market director for RSA Security,
said banks were well aware that their efforts to shore up security
around online banking could have a downside.  "It registers as a
concern," he said, "there could be too much security and there's a
danger of over-selling a new technology."  "This is not just about
combating fraud," he added. "It's about customer retention rates, user
experience and customer satisfaction."
The misgivings about beefed up security around online banking come as
the UK government's Get Safe Online campaign issues a survey which shows
the risks people are taking with login details.  These lax practices
could prove costly as cyber fraudsters gradually shift their attention
to Europe following moves in the US to combat phishing.  In late 2005
the US Federal Financial Institutions Examination Council (FFIEC) issued
guidelines which forced banks to do more to protect online accounts.
Phishing statistics show a rapid move by the fraudsters to European
banks and, said Mr Moloney, to smaller European banks using less
protection.  Lists of phishing targets gathered by security companies
show a huge shift away from big bank brands such as Citibank and Bank of
America to Sparkasse, VolksBank and many others.  A spokeswoman for the
Association for Payment and Clearing Services (Apacs) which oversees
online banking said its figures showed that the message about safe
banking was getting through.  Statistics released in October indicated
that online banking fraud (including phishing) for the first six months
of 2007 was down 67% over the previous year.  During the same time
period the number of phishing attacks rose by 42%.  "The reason we are
seeing that fall, despite the increase in phishing attacks, is because
consumers are becoming more aware of how to protect themselves," said
the spokeswoman.  "But," she added, "what we are still seeing happening
is people falling foul of phishing attacks."  The spokeswoman urged
people to be careful with login details to bank accounts and exercise
caution when using e-mail and the web.
Published: 2007/11/13 09:33:59 GMT
? BBC MMVII

@_date: 2007-11-14 10:46:01
@_author: Leichter, Jerry 
@_subject: Government Smart Card Initiative 
Little progress on government-wide smart card initiative, and little
November 14, 2007 (Computerworld) More than three years after a
presidential directive requiring federal government agencies to issue
new smart-card identity credentials to all employees and contractors,
progress on the mandate continues to be tediously slow.
Most agencies appear to have missed by a wide margin an October 27
deadline by which they were supposed to have completed background checks
and issued smart-ID credentials to all employees and contractors with 15
years or less of service.
The so-called Personal Identity Verification (PIV) cards are supposed to
be tamper-proof and to support biometric authentication features. PIV
cards are designed to control access to federal computer networks and
facilities and can be used across agencies. Federal agencies are
mandated to issue them to all employees and contractor under Homeland
Security Presidential Directive-12 of August 2004. Under the multi-phase
initiative, agencies have until October 2008 to issue PIV cards to all
their employees and contractors.
Several government agencies contacted for this story did not respond to
request for information on their implementation status. But an
inspection of publicly posted information at IDmanagement.gov, a federal
identity management site, showed that a large number of government
agencies had barely begun issuing the cards just prior to the October
Well below the Mendoza line
For example, as of Sept. 1, the U.S. Department of Commerce had not
issued even one PIV credential, though it listed over 40,000 employees
as requiring it. As of October 19, the Social Security Administration
had issued cards to 300 of its 65,000 employees, and to 429 of its
approximately 20,000 contractors. On July 1, the U.S. Department of
Energy had issued the new cards to 5 out of its 13,500 employees, and
not a single one to its 98,000 or so contractors.
Doing slightly better was the Department of State, which has issued the
new ID credentials to 4450 of its 19,865 employees and to more than a
quarter of its 7000 contractors by Sept. 14. Similarly, the Department
of Labor has issued cards to 6450 of its 15,600 employees and about 400
of its 3000 contractors as of Sept. 1
Though the numbers are a far cry from where the agencies were required
to be, they are not entirely unexpected. From the program's outset,
security analysts and government IT managers warned that agencies would
have a hard time meeting HSPD-12 implementation deadlines for a variety
of technological and logistical reasons.
"This is a classic example of politically established deadlines that are
not based on any reality at all. It is no more complicated than that,"
said Franklin Reeder an independent consultant and a former chief of
information policy at the U.S. Office of Management and Budget (OMB).
"As best as I can tell, HSPD-12 deadlines were set without any real
understanding of the enormity of what needed to be done or the costs"
involved in doing so, said Reeder, who is also chairman for the Center
for Internet Security.
The National Institute for Standards and Technology (NIST), which was
originally entrusted with the task of coming up with the technical
specifications for HSPD-12, did a great job in delivering the standards
on schedule, Reeder said. Since then, agencies have been left with the
unenviable task of trying in an unreasonably short time frame to replace
their existing physical and logical access infrastructures with that
required for the PIV cards, Reeder said.
"It's one of those situations where the technology itself is not
complicated, but it does comprise many different pieces that have to be
carefully integrated," said Hord Tipton, a former CIO with the
U.S. Department of the Interior. The task involves a lot of cooperation
between different groups within agencies that have traditionally not
worked with each other, such as human resources, physical security and
IT, he said, and sometimes it can also mean replacing ongoing agency
efforts with the standards mandated by HSPD-12. The biggest example of
this is the U.S. Department of Defense, which rolled out millions of its
own IDs, called Common Access Cards. Those were based on a different
standard, and the DoD is currently in the process of migrating their
system to the PIV standard.
Interoperability looms
In addition to the internal issues, agencies also need to make sure
their PIV card infrastructures are interoperable with those of other
government agencies, Tipton said. This raises a whole set of other
technology, standards, trust, control and political issues that agencies
need to navigate.
A shared service set up by the General Services Administration (GSA) to
help agencies enroll employees into the PIV program and issue the new
cards to them is also still in the process of ramping up according to
Neville Pattison, vice president of business development and government
affairs at smartcard vendor Gemalto Inc.
This may have had an impact on the 63 or so federal agencies,
representing over 800,000 government employees, that are depending on
GSA to issue PIV cards, he said. Pattison says he expects the GSA shared
service to eventually achieve a run rate of around 10,000 cards per day,
"but that's going to be a good five years" from now.
Some of the bigger agencies are also using the HSPD-12 mandate as an
opportunity to roll out robust long-term ID management programs
requiring considerably longer implementation schedules, Pattison said.
One example is the Department of Homeland Security, which has managed to
get the approval of the OMB for a full compliance deadline of 2010.
Larry Orluskie, a DHS spokesman, said the agency received OMB approval
for the revised implementation schedule "so that it could most
effectively develop and deploy a scalable agency-wide solution" that
would form the foundation for ongoing security efforts at DHS.

@_date: 2007-11-19 12:30:52
@_author: Leichter, Jerry 
@_subject: State of the art in hardware reverse-engineering 
Flylogic Engineering does some very interesting tampering with "tamper-
resistant" parts.  Most of those "secure USB sticks" you see around won't
last more than a couple of minutes with these guys.
See  							-- Jerry

@_date: 2007-10-03 10:41:55
@_author: Leichter, Jerry 
@_subject: Linus: Security is "people wanking around with their opinions" 
On one side, we have SeLinux, produced with at least the aid of the NSA.
SeLinux embodies the accepted knowledge about how to do security
"right".  This is a matter of engineering experience, not science.  The
fact is, very few things in this world are a matter of "science".
Science can provide answers, but it can't choose the questions for you.
In the case of security, you first have to choose your model of what
needs to be secured, and against what kind of attacks.  There's no
possible science here - science can help you by telling you where the
limits are, what impact some choices have on others, but ultimately what
you consider important to protect, and what kinds of attacks you
consider plausible enough to be worth the costs of preventing, are
judgements that science cannot make.  The NSA has tons of experience
here, along all the relevant dimensions.  But the judgements they make,
while appropriate to their circumstances, may make little sense in other
circumstances.  I'm quite willing to grant that, in the sphere in which
NSA works, SeLinux is a great solution.  But few of us live there.
So ... on the other side, we have those who focus on the difficulty with
actually configuring and using an SeLinux system.  This is a dimension
that doesn't particularly concern NSA:  They have legal and operational
requirements that *must* be met, and the way to deal with the complexity
is to throw trained people and money at the problem.  But hardly anyone
else is in a position to take that approach.  So the net result is that
people end up not using SeLinux.  Seeing this, others come along with
simpler-to-use approaches.  They don't solve the problems SeLinux
solves, but they do solve *some* real problems - and they are claimed to
be much more likely to be adopted.  (Adoption rates, at least, *can* be
measured.  You can complain all you like about what people *should* be
doing, but ultimately what they *are* doing is something you have to
measure in the real world - scientifically! - not just think about.)
Now, the security absolutists say "But you're getting people to adopt
something that doesn't *really* protect them."  Perhaps, though in the
words of George Orwell, "The best is the enemy of the good."
We see the same kinds of arguments in cryptography.  There are the
absolutists, who brand as snake oil anything that doesn't pass every
known test anyone has ever published, that hasn't had every individual
component fully vetted by people they trust (and ultimately, they trust
no one, so it ends up the only things they trust are things they created
themselves).  There are the true snake oil salesmen.  And there are
those who try to get something "good enough" out there:  Something that
will actually get used by more than a tiny fraction of the population
and will protect them against reasonable threats.  For myself, I long
ago decided that no data I have is so valuable that it needs to survive
an attack that costs more than, say, a few thousand dollars to pull off.
In fact, if we're talking about data that can't be identified up front -
e.g., if someone had to go through my encrypted files one at a time, not
knowing what was in them until they had decrypted them - the threshold
is dramatically lower.  I'd probably be happy if it cost more than $100
per file.  Even at those rates, there would be cheaper ways to get at
my stuff than attacking the cryptography.
Obviously, others will have different thresholds.  But thinking about
this kind of thing in monetary terms does help you get away from the
kind of nebulous "I want my stuff secure from any possible attack by
anyone" thinking.  So I don't trust WEP for anything, but I do trust WPA
- but I use SSH even over WPA links for many things.  It's cheap, it's
as easy to use as the alternatives - why not?  I have files encrypted
with what by today's standards are very weak algorithms.  If they get
broken, I've judged that my loss is trivial.  The old programs are quick
and easy to use and I just haven't gotten 'round to re-encrypting with
newer algorithms that, on today's machines, are fast enough and easy
enough to use.  I tend to zero out files before deleting them, just
because it's easy to do and it can't hurt.  On the other hand, I don't
go out of my way to use some 7-pass or - Lord save us from those who
can't even be bothered to read Peter Guttman's paper on this and
understand what he actually said - 35-pass erasure algorithm:  If I have
to worry about an attacker who are willing to use fancy data recovery
hardware to look for remnant magnetization, I've got other problems.
(BTW, it always amazes me that no modern system has picked up an old,
old idea from VMS:  You can set a marker on a file that causes the
system to over-write it when it's deleted.  Since the file system implementation does this, it makes no difference how the file came
to be deleted - it *will* get zeroed, or, actually, passed to a
loadable module that can perform some other kind of secure erasure.
Compare to Unix, where if I accidentally re-direct output over a file I
meant to erase before deletion, I'm screwed.)

@_date: 2007-10-04 18:48:49
@_author: Leichter, Jerry 
@_subject: Retailers try to push data responsibilities back to banks 
Retail group takes a swipe at PCI, puts card companies 'on notice'
Jaikumar Vijayan
October 04, 2007 (Computerworld) Simmering discontent within the retail
industry over the payment card industry (PCI) data security standards
erupted into the open this week with the National Retail Federation
(NRF) asking credit card companies to stop forcing retailers to store
payment card data.
In a tersely worded letter to the PCI Security Standards Council, which
oversees implementation of the standard, NRF CIO David Hogan asked
credit card companies to stop making retailers "jump through hoops to
create an impenetrable fortress" to protect card data. Instead,
"retailers want to eliminate the incentive for hackers to break into
their systems in the first place."
"With this letter, we are officially putting the credit card industry on
notice," Hogan said in a statement. The NRF, a trade association whose
membership includes most of the major retailers in the U.S., is the
national voice for about 1.4 million U.S retail establishments.
In an interview with Computerworld this morning, Hogan said the letter
was provoked by a "lot of frustration" in the industry about PCI
guidelines and the deadlines associated with implementing them. If the
goal of PCI is to protect credit card data, the easiest and most common
sense approach is to stop requiring merchants to store the data in the
first place, he said.
PCI is a data security standard mandated by Visa International Inc.,
MasterCard International Inc., American Express Co., Discover and the
Japan Credit Bureau. It requires companies to implement a set of
prescribed security controls for protecting cardholder data. Though the
requirements went into affect more than two years ago, a large number of
big retailers are still noncompliant because of a variety of issues that
include legacy system challenges, rules interpretation issues and
continuously evolving guidelines.
According to Hogan, credit card companies require retailers and others
accepting payment card transactions to store certain card data sometimes
for up to 18 months so that it can be retrieved in the event of
chargebacks and other disputes.
But rather than have thousands of retailers store the data, credit card
companies and their banks should do so, Hogan said. Retailers only need
an authorization code provided at the time of a sale to validate a
charge, and a receipt with truncated credit card information to handle
returns and refunds. If that were done, he said, most retailers probably
wouldn't store any cardholder data.
According to Hogan, under the current process, credit card companies and
their banks already have the information needed for retrieval purposes
and it should be their responsibility to store and protect the data. "It
is a very fundamental shift. But if you think about it, it is a very
common-sense approach."
PCI mandates are challenging retailers to build fortresses around credit
card data, he said. "We build these higher walls and the hackers bring
in taller ladders and this kind of keeps scaling up all the time."
Gartner Inc. analyst Avivah Litan said that the NRF letter makes a
"sound argument.
"It's totally reasonable to tell the banking system and payment system
that 'we don't want to store this data anymore,'" she said. "If they
aren't storing this data, many of these [PCI] requirements go away and
the scope of the compliance effort is much more restricted.
In an e-mailed comment, Bob Russo, general manager of the PCI security
standards council, said the body received the NRF letter yesterday and
will respond after reviewing it further. "However, it must be recognized
that the payment brands -- and not the Council -- operate the systems
underlying the payments process, as well as the compliance
programs. Because of this, Mr. Hogan should be directing his concerns to
those individual brands."
Jon Hurst, president of the Retailers Association of Massachusetts,
backed the NRF's position. With all of the attention paid to PCI, what's
gone unnoticed is the fact that card companies themselves require
certain amounts of data to be stored because of disputed transactions,
he said. If not for that requirement, many retailers -- especially the
large ones -- would probably not keep data and therefore wouldn't be
pressed to secure it, he said.
Prat Moghe, founder and CTO of Tizor Systems Inc., a Maynard,
Mass.-based security firm, called the NRF's demand political posturing
and said it would do little to improve retail security anytime soon.
"I think a lot of this is about moving culpability back to the credit
card companies and saying don't make this my problem alone," Moghe
said. "They seem to have realized that going on the defense as an
industry doesn't help. There is just more and more they have to do." By
speaking out aggressively at a time when retail industry information
security practices are under scrutiny by consumers and lawmakers, the
NRF is hoping to spread the liability for card data protection, he said.
Even if the NRF's demands were immediately met, it would take several
years before retailers could purge their systems and applications of
credit card data, he said. Over the years, retailers have collected and
stored credit card data in myriad systems and places -- including
relatively old legacy environments -- and they are just now realizing
the data can be a challenge, he said. Purging it can be a bigger
headache because the data is often inextricably linked to and used by a
variety of customer and marketing applications; simply removing it could
cause huge disruptions.
"We are not talking about one isolated system that stores all this
data," he said.
Until retailers can get rid of the data, they will need to continue to
implement PCI controls, whether they like it or not, Moghe said.
Under PCI, credit card companies have also already been pushing
retailers to purge their systems of some customer data, including the
card verification codes and PIN block data that is stored on magnetic
stripes on the back of payment cards.
According to Gartner Inc., Visa last year levied more than $4.5 million
in PCI noncompliance-related fines. At least some of that was aimed at
companies that were storing prohibited card data on their systems.
The NRF letter comes just days after the passage of a major Sept. 30 PCI
deadline after which merchants face fines ranging from $5,000 to $25,000
for noncompliance. Up to now, most of the fines levied have been on
breached entities or on companies that kept prohibited card data.

@_date: 2007-10-08 14:48:20
@_author: Leichter, Jerry 
@_subject: Trillian Secure IM 
This is an old argument.  I used to make it myself.  I even used to
believe it.  Unfortunately, it misses the essential truth:  The choice
is rarely between really strong cryptography and weak cryptography; it's
between weak cryptography and no cryptography at all.  What this
argument assumes is that people really *want* cryptography; that if you
give them nothing, they'll keep on asking for it; but if you give them
something weak, they'll stop asking and things will end there.  But in
point of fact hardly anyone knows enough to actually want cryptography.
Those who know enough will insist on the strong variety whether or not
the weak is available; while the rest will just continue with whatever
they have.
It's much better to analyze this in terms of the cost to the attacker
and the defender.  If the defender assigns relatively low value to his
messages, an attack that costs the attacker more than that low value is
of no interest.  Add in the fact that an attacker may have to break
multiple message streams before he gets to one that's worth anything at
Even something that takes a fraction of a second to decrypt raises the
bar considerably for an attacker who just surfs all conversations,
scanning for something of interest.  It's easy to search for a huge
number of keywords - or even much more complex patterns - in parallel at
multi-megabyte/second speeds with fgrep-like (Aho-Corasick) algorithms.
A little bit of decryption tossed in there changes the calculations
I'm not going to defend the design choices here because I have no idea
what the protocol constraints were, what the attack model was (or even
if anyone actually produced one), what the hardware base was assumed to
be at the time this was designed, etc.  Perhaps it's just dumb design;
perhaps this was the best they could do.  Could it be better?  Of
course.  Is it better to not put a front door on your house because
the only ones permitted for appearance's sake are wood and can be
broken easily?

@_date: 2007-10-08 18:48:43
@_author: Leichter, Jerry 
@_subject: Full Disk Encryption solutions selected for US Government use 
Well, if you believe a talk by Brian Snow at the NSA - see
 - our whole process has to
change to get assurance, from the beginnings of the design all the
way through the final product.
I suspect he's right - but I'm also pretty sure that the processes
involved will always be too expensive for most uses.  They'll even be
too expensive for the cases where you'd think they best apply - e.g.,
in protecting large financial transactions.  An analysis of the costs
vs. the risks will usually end up with the decision to spend less and
spread the risks around, whether through insurance or higher rates
or other means.
We keep being told that inspection after the fact will give us more
secure systems.  It never seems to work.  You'd think that the
experience of, say, the US auto industry - which was taught by the
Japanese that you have to build quality into your entire process, not
inspect *out* lack of quality at the end - would give us some hint
that after-the-fact inspection is not the way to go.
Given all that ... a FIPS 140-2 certification is actually a pretty
reasonable evaluation.  It can be because it's trying to deal with
a problem that can be constrained to a workable size.  You know what's
supposed to go in; you know what's supposed to come out.  (This
still works better for hardware than for software, though.)  Where
FIPS 140-2 breaks down is that ultimately all it can tell you is
that some constrained piece of the system works.  But it tells you
nothing, and *can* tell you nothing, about whether that piece is
being used in a proper, secure way.  (Again, this is somewhat easier
with hardware, because the system boundaries are much more sharply
defined - and because of the inflexibility of hardware, they are also
much smaller.)  Beyond this is Common Criteria, which can easily be
more about paperwork than anything real.
Until someone comes up with a new way to approach the problem, my
guess is that we'll see more stuff moved into hardware, with limited
security definitions above the hardware that we can have some faith
in - but as little of real value to be said above that as there is

@_date: 2007-10-12 11:04:15
@_author: Leichter, Jerry 
@_subject: Quantum Crytography to be used for Swiss elections 
No comment from me on the appropriateness.  From Computerworld.
 							-- Jerry
Quantum cryptography to secure ballots in Swiss election
Ellen Messmer
October 11, 2007 (Network World) Swiss officials are using quantum
cryptography technology to protect voting ballots cast in the Geneva
region of Switzerland during parliamentary elections to be held Oct. 21,
marking the first time this type of advanced encryption will be used for
election protection purposes.
Still considered an area of advanced research, quantum cryptography uses
photons to carry encryption keys to secure communications over
fiber-optic lines and can automatically detect if anyone is trying to
eavesdrop on a communications stream. For the Swiss ballot-collection
process, the quantum cryptography system made by id Quantique will be
used to secure the link between the central ballot-counting station in
downtown Geneva and a government data center in the suburbs.
"We would like to provide optimal security conditions for the work of
counting the ballots," said Robert Hensler, the Geneva State Chancellor,
in a statement issued today. "In this context, the value added by
quantum cryptography concerns not so much protection from outside
attempts to interfere as the ability to verify that the data have not
been corrupted in transit between entry and storage."
The use of quantum cryptography in the voting process will showcase
technology developed in Switzerland. The firm id Quantique, based in
Carouge, grew out of research done at the University of Geneva by
Professor Nicolas Gisin and his team back in the mid-1990s.
According to id Quantique's CEO Gregoire Ribordy, the firm's Cerberis
product, developed in collaboration with Australian company Senetas,
will be used for the point-to-point encryption of ballot information
sent over a telecommunications line from the central ballot-counting
station to the government data center.
Ribordy said the Swiss canton of Geneva -- there are 26 cantons
throughout all Switzerland -- has about 200,000 registered voters who
will either go to the polls on Oct. 21 and cast their vote, or vote by
mail. "The votes cast by mail are all collected in the days before the
election and all brought to the central counting station on Oct. 21,"
Ribordy said.
"Once the election is closed -- at noon on Sunday, Oct. 21 -- the sealed
ballot boxes of all the polling stations are brought to the central
counting station, where they are opened and where the votes are mixed
with the mail votes. Counting them is then manually done at the central
counting station. People counting the votes at this central station use
computers to transfer the counts to the data center of the canton of
Geneva," Ribordy explained.
He said the quantum cryptography system is ready to be put into
action. Ribordy doesn't think the high-speed link has been encrypted by
any means in the past, but he added that the IT department of the Swiss
government is not sharing a lot of information on certain details for
security reasons.
The use of quantum cryptography in the Swiss election marks the start of
the "SwissQuantum" project managed by Professor Gisin, with support from
the National Center of Competence in Quantum Photonics Research in
"Protection of the federal elections is of historical importance in the
sense that, after several years of development and experimentation, this
will be the first use of the 1GHz quantum encrypter, which is
transparent for the user, and an ordinary fiber-optic line to send data
endowed with relevance and purpose," said Professor Gisin in a prepared
statement. "So this occasion marks quantum technology's real debut."
The SwissQuantum project aims to set up a pilot communications network
throughout Geneva. Supporters compare it with that of the first Internet
links in the United States in the 1970s. The Swiss are also expected to
showcase the quantum cryptography project during the ITU Telecom World
event being held in Geneva this week.

@_date: 2007-10-12 11:27:43
@_author: Leichter, Jerry 
@_subject: 307 digit number factored 
"Past performance does not predict future results."
I don't think this is a particularly strong argument.  A
reasonable counter-argument is that we've been doing
number theory over the integers for hundreds of years,
while intensive work on computations over elliptic curves
goes back, what, 20 years at most?
Ultimately, we have no scientific basis for judging the
relative vulnerability of factoring, RSA, discrete
logarithms over integers, discrete logarithms over
elliptic curves, and so on to "breakthrough attacks".
(We can pretty well quantify their vulnerability to
*known* attacks as technology changes, and we have some
rough ideas about "evolutionary" attacks which don't
change the fundamental constraints but give you an
extra factor of 10, say.)
I don't see that at all.  There are multiple domains to
consider.  In terms of local communication, Ethernet is
continuing to deliver factors of 10 speedup every couple
of years.  10Gb/second Ethernet cards are approaching
the $100 mark.  That's an astoundingly fast interconnect.
If you're talking about long-distance communication,
about the only use case I know of where *speed*, as
opposed to *latency*, is a big deal these days is in
moving really big datasets.  It's hard to come up with
useful *computations* for which even something like DSL
isn't fast enough that latency completely dominates the
cost to the algorithm of communication.
In any case, how often do you send *keying material*?
The difference between 100 bytes and even 10K bytes is
lost in the noise for that rare operation on any
modern network.
Meanwhile, single-stream CPU speed has more or less
stalled.  Everyone is going for parallelism.  Thats's
fine for some problems, not helpful for others.  So in
fact it's the lack of progress in CPU speed that makes
for a stronger argument for the smaller operations of
EC than slower communications speeds!
I don't see that there's anything you can really back
that statement up with.  Mathematics takes time.  It also
tends to move in fits and starts, because it has all kinds
of unanticipated interconnections.  An advance in one field
may be found to have completely unexpected applicability to
another.  The proof of Fermat's Last Theorem - based on a
large number of deep results in many different fields, few
if any what you would think of as "number theory over the
integers" in any obvious sense - came together to enable
progress after hundreds of years.  This is hardly an
isolated example - it's just a very widely known one.
I quite agree the elliptic curve techniques let you get
by with much smaller parameters, which has any number of
advantages.  We have no particular reason to believe that
these techniques are *weaker* than techniques over the
integers, but I don't see that we really have any evidence
that they are *stronger* either.  As an engineering choice,
sure, go with the cheaper computation - there's no known
loss in doing so.  But don't try to convince yourself that
you've bought extra inherent security that way, because you
really don't know.

@_date: 2007-10-15 10:20:53
@_author: Leichter, Jerry 
@_subject: Password hashing 
As others have pointed out, with a large enough salt, dictionary attacks
become impossible.  But it's worth mentioning another issue:  People's
userid's do change and it's nice not to have the hashed passwords break
as a result.  (This is pretty counter-intuitive to users who change their names, and a disaster if a large organization needs to do a mass renaming
and somehow has to coordinate a mass password update at the same time.)

@_date: 2007-10-15 10:27:48
@_author: Leichter, Jerry 
@_subject: Quantum Crytography to be used for Swiss elections 
Only that we've been over this ground so many times before.
Ah, but this is a quantum system.  I think it's more a matter of
inducing correlations than a physical transfer.
Are trust, relevance, and purpose orthogonal variables?  That seems
unlikely.  So you need to trade of your ability to measure them.
Ah, there are some trustworthy photons.  Oops, we can trust them, but
we don't know if they are relevant.  Ah, there's a relevant photon....

@_date: 2007-10-22 17:36:50
@_author: Leichter, Jerry 
@_subject: Intelligent Redaction 
Actually, it looks as if Xerox has been doing a bunch of very
interesting work on the borderlines of security, privacy,
cryptography, and human factors.  I hadn't noticed it before.
Look, for example, at:
(Now, can anyone account for the bizarre very light gray pattern
of lines that appear behind the top half or so of this page?)

@_date: 2007-09-06 09:28:40
@_author: Leichter, Jerry 
@_subject: In all the talk of super computers there is not... 
He misapplied an incorrect estimate!  :-) The usual estimate - going
back to Shannon's original papers on information theory, actually - is
that natural English text has about 2.5 (I think it's usually given as
2.4) bits of entropy per *character*.  There are several problems here:

@_date: 2007-09-06 14:24:11
@_author: Leichter, Jerry 
@_subject: In all the talk of super computers there is not... 
Interesting paper - I hadn't seen that one, only the earlier one that
got an estimate - cited in this one - for 2.3 (not 2.4) bits per
character for samples of length 8 (*very* roughly).
Well, for *general purpose* algorithms, you can get a rough idea by
looking at how well the best compressors do.  zip deflate on a random
selection of English text I used managed to reduce the text to about 31%
of its original size.  You can't easily compare this to Shannon's 25%
estimate because zup had an easy job:  The input was 7-bit ASCII, the
top bit of every byte was always 0; and of the remaining 128 possible
bytes, at least 30 (probably more) never occur.  If we assume the
input text had only 70 possible characters in it, then there are
"really" only a little more than 6 bits of true entropy per byte
of input.  This brings the effective compression from the "smart"
parts of the algorithm down to about from 69% to 60%.
zip deflate isn't the state of the art in compression algorithms, but
nothing does all *that* much better.
I suspect the best first-try algorithm for generating attacks would be
an analogue of using a dictionary to guess passwords:  Extract phrases
of the appropriate length from the huge volume of data that is now
readily available on line.  This is likely to catch many pass phrases.
The example in the original message shows how to avoid such an attack:
Don't use "Mary had a little lamb, it's fleece was white as snow";
use a semantic equivalant "Mary had one tiny lamb, with fleece that
were white as snow".  One can probably generate many such variants
algorithmically with little trouble, though.  (What's hard is
eliminating the ones no human would likely use for deep semantic
reasons, but for an attack like this, generating extra ones only
cost you time.)
Probably out of reach today for reasonably long phrases, but I
wouldn't give it very much time.
(It would be interesting to do a detailed analysis for the often-
recommended approach of picking a phrase and using the first letters
of successive words.  Just the distribution of first letters of
words is probably biased, and what the correlation of successive
first letters looks like is anyone's guess - though given the
ready availability of data, it's trivially easy to compute.)

@_date: 2007-09-07 16:27:50
@_author: Leichter, Jerry 
@_subject: Seagate announces hardware FDE for laptop and desktop machines 
First off, it depends on how the thing is implemented.  Since the entire
drive is apparently encrypted, and you have to enter a password just to
boot from it, some of the support is in an extended BIOS or some very
early boot code, which is "below" any OS you might actually have on the
disk.  Once you get past that, though, it depends on what they provide.
If the boot-time password gets stored in the disk firmware and controls
all encryption and decryption for the "session", the OS would neither know nor care.  If the drivers have to get involved, or you *want* them
involved (e.g., because you want to use the disk hardware to do encryp-
tion with different sets of keys you assign to different files,
partitions, whatever the thing can support) then ... ask for something
reasonable: That the interface to the mechanism is published so that
someone can write the appropriate drivers.
Ah, yes, it's all a conspiracy to make you run Windows.
Grow up.  *If* the drive vendor keeps the mechanism secret, you have
cause for complaint.  But can you name a drive vendor who's done
anything like that in years?  What possible motivation could they
have?  (In fact, I believe Seagate has said they will publish the
You don't.  The general issue of how you can come to trust a piece
of cryptographic hardware has been discussed here before, and no one
has been able to suggest a way to do it.
Guess what:  Seagate makes the same point.  As one of the two remaining
drive vendors who are actually US-based (I forget who the other is),
they've pointed out to Congress that it might not be such a good thing
if DoD's and Homeland's and the FBI's secure disks were all based on
chips and firmware developed overseas (and particularly in China).  They
bring this up purely for patriotic reasons, of course.  If Congress sees
fit to provide a bit of protection, well, that's a national policy
issue, not Seagate's doing....  :-)
Of course, most of the world's countries will be faced choosing secure
devices developed and built in one of 3 or 4 countries, at least the
two largest of which have very well developed organizations to, err,
develop information in the national interest.
Who are you willing to trust?  How much are you willing to pay to avoid
trusting someone you would rather not trust?
Personally, if I were *that* concerned, I'd use an encrypted file system
on top of an FDE system, at least for the stuff I considered really

@_date: 2007-09-10 10:52:33
@_author: Leichter, Jerry 
@_subject: What is a proof? 
Few results are discovered using the same journey as their accepted
proofs - much less than the elegant, beautified proofs that will appear
in the texts a number of years down the road.
Many, many years ago, a mathematician I knew asked a bunch of us
then-youngsters:  What's a proof?  Having been brought up in the
formalist tradition, we came back with definitions along the lines
of "a series of statements each of which is either an axiom or can
be derived from earlier lines by such-and-such a process".
His answer was:  No, a proof is something that convinces you of the
truth of something.  Interpreting this a bit more broadly:  A
*mathematical* proof is something that convinces *the relevant
set of mathematicians* of the truth of something.  I don't want to
sound like some post-modernist with statements about the relativity
of truth, but the fact is that this really is what proofs are about.
The standards for acceptable proofs are defined by the specialists
in the field.  If your proof has been vetted by those specialists,
it's accepted.  Until then, it's a *proposed* proof.
The standards of what constitutes an accepted proof have changed
over the years ago.  Consider the statement:  Any polytope has one
side on which it will rest stably.  (You can formalize this as:  If,
for each face, you draw a line from the centroid of the polytope
perpendicular to that face, than for at least one face the line
passes through the face.)  We can prove this by observing that if
this were not true, then if we put the polytope on the ground, it
will roll forever.
In the 19th century, this was accepted as a proof.  More recently,
early in the 20th century, a group of mathematicians in Italy created
the field of algebraic geometry.  No one outside that group really
understood what they were up to or accepted their techniques.  In the
1960's, the area became very "hot", and a number of PhD dissertations
were produced by finding some of the old results, recasting them in
modern terms, and producing proofs that were acceptable in modern terms.
Back in the 70's, Lipton, DeMillo and Perlis argued that program
proving was headed in a fundamentally untenable direction:  What
made mathematical proofs convincing was exactly that they were
reviewed by people who were legitimately convincing.  But it
was highly unlikely that anyone would ever review a proof of a
non-trivial program.  So we would have little reason to believe
in it.
There are a series of papers by Koblitz and Menezes, starting with
"Another Look at 'Provable Security'", that (I think convincingly)
argue that formal proofs in cryptography have been oversold:  The
availability of a formal proof should almost always add little to
your belief in the soundness of a system, and the *absence* of
such a proof should do little to decrease your belief.  They
demonstrate a number of cases where systems have been modified in
order to make formal proofs possible, but one can plausibly argue
that the modifications actually weakened the system.  (The lack of
adequate review of many of those proofs is one reason.  Bellare
and Rogaway's proof of the security of OAEP-RSA was accepted for
seven years - and OAEP-RSA got written into multiple standards -
before Shoup showed the argument was incorrect.  Proofs by
Bellare and Rogaway are well written and understandable.  Even
so, no one apparently looked closely for all that time.  All too
many proofs in the "provable security" match your description of
impressively decorated jargon.  Those will likely never get
reviewed by anyone, perhaps not even the authors!)

@_date: 2007-09-11 17:18:27
@_author: Leichter, Jerry 
@_subject: Another Snake Oil Candidate 
What makes you call it snake oil?  At least the URL you point to says
very reasonable things:  It uses AES, not some home-brew encryption; the
keys are stored internally; the case is physically protected, and has
some kind of tampering sensor that wipes the stored keys when attacked.
In fact, they make some of the same points:
The management team lists some people who should know what they are
doing.  They have a FAQ which gives a fair amount of detail about
what they do.
I have nothing at all to do with this company - this is the first I've
heard of them - but it's hardly advancing the state of security if
even those who seem to be trying to do the right thing get tarred as
delivering snake-oil.
If you know something beyond the publicly-available information about
the company, let's hear it.  Otherwise, you owe them an apology -
whether they actually do live up to their own web site or not.

@_date: 2007-09-18 18:22:02
@_author: Leichter, Jerry 
@_subject: OK, shall we savage another security solution? 
Anyone know anything about the Yoggie Pico (  It claims
to do much more than the Ironkey, though the language is a bit less
"marketing-speak".  On the other hand, once I got through the
marketing stuff to the technical discussions at Ironkey, I ended
up with much more in the way of warm fuzzies than I do with Yoggie.
 							-- Jerry

@_date: 2007-09-19 16:02:06
@_author: Leichter, Jerry 
@_subject: OK, shall we savage another security solution? 
This is a representative of yet another class of "secure" USB devices:
Historically, NSA has apparently never liked software implementations of
cryptography - they wanted protected hardware.  Such hardware has been
prohibitively expensive until quite recently.  These devices show that
the price of such hardware is no longer a problem:  We can build very
secure, very small pieces of hardware for not a lot of money.  What to
*do* with those hardware capabilities is another question.  It's not
easy to fit them safely into systems - and what problems can they solve
in those systems.  Kingston and many other similar devices are a great
solution to a problem very real problem:  When my 2GB memory stick falls
out of my pocket, have I just given away 2GB of highly sensitive data
to anyone who finds the thing?  They are *not* any kind of solution to
the "how can I access my data safely on a possibly-compromised system"?
The Ironkey guys have attacked a broader problem, and while they haven't
completely solved it - it's not clear any solution exists! - they've
provided a capability that is potentially useful.  (They aren't unique -
people have built a bunch of devices that are basically outboard
Linux boxes that rely on a guest box to provide network connectivity,
a keyboard, and a screen.  But they have a commercially available low-
cost product.)
If you think about this in general terms, we're at the point where we
can avoid having to trust the CPU, memory, disks, programs, OS, etc.,
in the borrowed box, except to the degree that they give us access to
the screen and keyboard.  (The problem of securing connections that
go through a hostile intermediary we know how to solve.)  The keyboard
problem is intractable, though it would certainly be a step forward
if at least security information didn't go through there.  This could
be done either by having a small data entry mechanism on the secure
device itself, or by using some kind of challenge/response (an LCD
on the device supplies a random value - not readable in any way by
the connected machine - that you combine with your password before
typing it in.)  Maybe HDMI will actually have some use in providing
a secure path to the screen?  (Unlikely, unfortunately.)

@_date: 2007-09-20 12:59:56
@_author: Leichter, Jerry 
@_subject: OK, shall we savage another security solution? 
Perhaps.  Public systems usually don't have "unpluggable" keyboards.
If I have to carry my own, I'm well on my way to just having my
own portable system (which may be the way things end up anyway).

@_date: 2007-09-24 15:49:30
@_author: Leichter, Jerry 
@_subject: Goodby analogue hole, hello digital hole 
The movie studios live in fear of people stealing their product as it
all goes digital.  There's, of course, always the "analogue hole", the
point where the data goes to the display.  The industry defined an
all-digital, all-licensed-hardware path through HDMI which blocks this
path.  As we know, Vista goes out of its way to keep all that stuff
"safe from tampering".
But in this business, there's always someone who's defining different
hardware that cuts the other way.  At least one "someone" is the
DisplayLink alliance.  This is a group of vendors who are supporting a
protocol for connecting video displays to computers over various kinds
of generic connections.  At the moment, USB, Ethernet, and wireless are
on the list.  Products are beginning to appear - LG, for example,
recently announced the LG L206WU, a 1680x1050 display that connects over
USB.  A "virtual graphics card" drives the thing.  You can support up to
6 USB displays, in addition to your existing displays.  The limiting
factor seems to be the CPU.  Intel is involved with DisplayLink, and is
demoing "3D and HD Video on USB and Wireless USB displays" based on some
integrated support in the the Intel graphics hardware at an upcoming
conference.  Vista's Aero is supported.  The press release talks about
watching movies.  There's a reference in one article about the LG that
says watching Blu-Ray disks probably won't work well because so much of
the CPU is used up decoding the disk.  Obviously, if this is indeed the
limitation, it's a temporary one.
No mention anywhere of HDMI or any kind of deal with the movie
industry.  If DisplayLink takes off - and with Intel on the producing
side and at least LG, Toshiba, Kensington already announcing products
it's got a good chance - HDMI is going to have a tough time gaining
a place at the table; and the Blu-Ray/HD-DVD producers are quickly
going to find themselves having to choose whether they are going to
walk away from the vast majority of the market.
 							-- Jerry

@_date: 2007-09-24 16:12:30
@_author: Leichter, Jerry 
@_subject: using SRAM state as a source of randomness 
I don't think that's what they are suggesting.  My understanding is
that their experiments show that, for any *particular instance* of
a chip, you can divide memory into two categories:
In reality, this is a spectrum - e.g., some locations may come up as
0 95% of the time and as 1 the other 5% of the time.  You could make
a Class 3 for those; members of that class would likely be ignored
in the following.  Which class a particular memory cell on a particular
chip falls into appears to be due to random process variations during
manufacture, and is alleged to be unpredictable and fixed for the life
of the chip.
So presumably the model is:  Put each manufactured chip into a testing
device that repeatedly power cycles it and reads all of memory.  By
simply comparing values on multiple cycles, it assigns locations to
Class 1 or 2 (or 3, if you like).  Once you've done this enough to have
reasonable confidence in your assignments, you pick a bunch of Class 1
locations and use them for the id; and a bunch of Class 2 locations and
call them the entropy source.  You burn the chosen locations into ROM on
the chip.  At power up, the chip checks the ROM, and constructs an ID
from the list of Class 1 locations and a random value from the list of
Class 2 locations.  (Obviously, you want to be a bit more clever - e.g.,
if all your Class 1 locations hold the same value on every power up,
something is wrong with your assumptions and you reject the chip rather
than using an ID of all 0's or all 1's.  The paper is asserting that
this won't happen often enough to matter.)
This is only done during manufacturing.  Presumably it would be
integrated into the testing process, which you're doing anyway.
The unique ID stuff is clever, but it's not clear how much it gains
you:  Since you need to do some final per-device programming anyway to
identify the locations to be used for the ID, why not just burn in a
unique ID?  The random generator is clever, but the question is whether
"produces an unpredictable result" is really a stable characteristic
of memory.  For example, it could be that those memory locations
initially are quite random, but if they are used to hold constant
values for long periods of time during operation, may build up a
remnance that destroys the initial randomness.  Ultimately, the nice
thing being relied on here - random process variations - also make
the approach vulnerable to any change in the process.

@_date: 2008-04-09 12:59:06
@_author: Leichter, Jerry 
@_subject: 2factor 
Anyone know anything about a company called 2factor (2factor.com)?
They're pushing a system based on symmetric cryptography with, it
appears, some kind of trusted authority.  "Factor of 100 faster
than SSL".  "More secure, because it authenticates every message."
No real technical data I can find on the site, and I've never seen
a site with so little information about who's involved.  (Typically,
you at least get a list of the top execs.)  Some ex-spooks?  Pure
snake oil?  Somewhere in between?
 							-- Jerry

@_date: 2008-04-22 10:46:33
@_author: Leichter, Jerry 
@_subject: Cruising the stacks and finding stuff 
Interestingly, if you add physics to the picture, you can convert "no
practical brute force attack" into "no possible brute force attack given
known physics".  Current physical theories all place a granularity on
space and time:  There is a smallest unit of space beyond which you
can't subdivide things, and a smallest unit of time.  One place this
shows up, as an example:  It turns out give a volume of space, the
configuration with the maximum entropy for that volume of is exactly a
black hole with that volume, and its entropy turns out to be the area
of the black hole, in units of square Planck lengths.  So, in effect,
the smallest you can squeeze a bit is a Planck length by Planck length
square.  (Yes, even in 3-d space, the constraint is on an area - you'd
think the entropy would depend on the volume, but in fact it doesn't,
bizarre as that sounds.)
So suppose you wanted to build the ultimate computer to brute-force
DES.  Suppose you want your answer within 200 years.  Since information
can't propagate faster than light, anything further than 100 years
from the point where you pose the question is irrelevant - it can't
causally affect the result.  So you computer is (at most, we'll
ignore the time it takes to get parts of that space into the
computation) a 100-light-year diameter sphere that exists for 200
years.  This is a bounded piece of space-time, and can hold a huge,
but finite, number of bits which can flip at most a huge, but finite,
number of times.  If a computation requires more bit flips than that,
it cannot, even in *physical* principle, be carried out.
I ran across a paper discussing this a couple of years back, in a
different context.  The authors were the ones who made the argument
that we need to be wary of "in principle" arguments:  What's
possible "in principle" depends on what assumptions you make.  Given
an appropriate oracle, the halting problem is "in principle" easy to
The paper discussed something else, but I made some rough estimates
(details long forgotten) of the "in principle" limits on brute force
attacks.  As I recall, for a 100-year computation, a 128-bit key
is just barely attackable; a 256-bit key is way out of the realm of
possibility.  Given all the hand-waving in my calculation, I didn't
try to determine where in that range the cut-over occurs.  Someone
better than me at the physics should be able to compute much tighter
bounds.  Even if I'm off by quite a bit, it's certain that the key
lengths we are using today are already near fundamental physical
limits.  Brute force is simply not an interesting mode of attack
against decently engineered modern systems.
Of course, this says - and can say - absolutely nothing about the
possibility of analytic or side-channel or any of a variety of other
intelligent attacks....

@_date: 2008-04-23 12:34:56
@_author: Leichter, Jerry 
@_subject: no possible brute force Was: Cruising the stacks and finding 
Except that this doesn't quite work.  You can't actually have anywhere
near that many distinct states in that volume of space.  The physics
does indeed get bizarre here:  The maximum number of bits you can
store in a given volume of space is determined by that space's
*surface area*, not its volume.  So you actually "only" get around
1E70 elements and 4.5E112 operations.  :-)
Kind of.
For simple search problems, where there are no shortcuts and you really
can only do brute force, quadratic is the best you can do.
Of course.  But the mice are already running that computation.
You're looking at this backwards.  Suppose I want to store a large
amount of data in a given volume of space.  To be specific, I have a
unit diameter sphere in which to build my memory device.  Initially, I
fill it with magnetic domains, and see how many distinct bits I can
store.  That gets me some huge number.  I replace that with
racetrack-style memory, with information on the edges of the domains.
Then with single electrons, with bits stored in multiple quantum states.
Can I keep going indefinitely?  The answer is no:  There is in fact a
limit, and it turns out to be the number of bits equivalent to the
entropy of a black hole with a unit diameter sphere.  The entropy
isn't given by the number of measurements I can make on my black
hole - it's given by the number of possible precursor states that
can produce the given black hole.  In fact, if you try to shove that
much information into your initial volume, you will inevitably
produce a black hole - not a good idea if what you want is a storage
device, since the data will be there in some sense, but will be
The actual computation requires using either string theory or loop
quantum gravity or, presumably, some other theory that combines GR
and QM.  The entropy is proportional to the area by very general
arguments, but to figure out the units, you have to make some more
assumptions.  According to the "Black Hole Thermodynamics" article in
Wikipedia, string theory implies that the units are exactly the Planck
length squared, while loop quantum gravity makes it proportional to
the single free parameter in that theory.
No one really understands what this means in any deep sense.  It
certainly doesn't say that a Planck-length x Planck-length square
is "a minimal bit" in any physical sense.  That's just a suggestive
interpretation.  If it were even approximately true ... that's
a 2-d "object".  Why can't I just stack a whole bunch of them in
an arbitrary volume?
What is important is the profound general principle that's emerged from
70 years or so of work on QM, which physicists now take for granted but
hasn't really made it into the general conciousness:  The universe is a
fundamentally finite thing.  The amount of information that can be
"embedded" into any finite volume of space and time is bounded.  (Note
that in Newtonian mechanics, or in GR, the value of even the single
gravitational field is given by a real number at every point in space
and time, so an arbitrarily small volume of space-time can "hold" an
arbitrary amount of information.  This is not the case in QM; in fact,
it's inherently incompatible with it.  See the Bekenstein Bound.)  Given
this and a limit on the speed at which information can propagate, that
there are fundamental physical limits on any computation follows.  "All
the rest is just detail."  :-)

@_date: 2008-04-24 18:23:50
@_author: Leichter, Jerry 
@_subject: Declassified NSA publications 
Interesting stuff.  There's actually more there in some parallel
directories - there's an overview page at

@_date: 2008-04-25 11:09:31
@_author: Leichter, Jerry 
@_subject: "Designing and implementing malicious hardware" 
While analysis of the actual silicon will clearly have to be part of
any solution, it's going to be much harder than that:
Why would you believe that what they publish doesn't already contain the
attack circuitry?  How far would you have them go?  Publish the VHDL
specs as well?  That's exactly the level at which the writers of this
paper added their code - around a hundred lines added to a total of
11,000 or so that describe a very simple chip.  Going further, suppose
someone has managed to "spike" the VHDL toolchain - recall Ken
Thompson's classic "On Trusting Trust".  Given the funding potentially
available to the kinds of adversaries who might want to mount such
attacks, the possible entry points are many.
This is a very tough problem.

@_date: 2008-04-28 10:15:16
@_author: Leichter, Jerry 
@_subject: "Designing and implementing malicious hardware" 
I suspect the only heavy-weight defense is the same one we use against
the "Trusting Trust" hook-in-the-compiler attack:  Cross-compile on
as many compilers from as many sources as you can, on the assumption
that not all compilers contain the same "hook".  For hardware, this
would mean running multiple chips in parallel checking each others
states/outputs.  Architectures like that have been built for
reliability (e.g., Stratus), but generally they assume identical
processors.  Whether you can actually build such a thing with
deliberately different processors is an open question.  While in
theory someone could introduce the same "spike" into Intel, AMD,
and VIA chips, an attacker with that kind of capability is probably
already reading your mind directly anyway.
Of course, you'd end up with a machine no faster than your slowest
chip, and you'd have to worry about the correctness of the glue
circuitry that compares the results.  *Maybe* the NSA would build
such things for very special uses.  Whether it would be cheaper for
them to just build their own chip fab isn't at all clear.  (One thing
mentioned in the paper is that there are only 30 plants in the world
that can build leading-edge chips today, and that it simply isn't
practical any more to build your own.  I think the important issue
here is "leading edge".  Yes, if you need the best performance, you
have few choices.  But a chip with 5-year-old technology is still
very powerful - more than powerful enough for many uses.  When it
comes to "obsolete" technology, you may have more choices - and
of course next year's "5 year old technology" will be even more
powerful.  Yes, 5 years from now, there will only be 30 or so
plants with 2008 technology - but the stuff needed to build such a
plant will be available used, or as cheap versions of newer stuff,
so building your own will be much more practical.)

@_date: 2008-04-28 15:47:54
@_author: Leichter, Jerry 
@_subject: "Designing and implementing malicious hardware" 
I'm not sure how you would construct a probability distribution that's
useful for this purpose.  Consider the form of one attack demonstrated
in the paper:  If a particular 64-bit value appears in a network packet,
the code will jump to the immediately succeeding byte in the packet.
Let's for the sake of argument assume that you will never, by chance,
see this 64-bit value across all chip instances across the life of the
chip.  (If you don't think 64 bits is enough to ensure that, use 128
or 256 or whatever.)  Absent an attack, you'll never see any deviation
from the theoretical behavior.  Once, during the lifetime of the system,
an attack is mounted which, say, grabs a single AES key from memory and
inserts it into the next outgoing network packet.  That should take no
more than a few tens of instructions.  What's the probability of your
catching that with any kind of sampling?
I don't follow this.  Suppose the system has been running for 1 second,
and you decide to compare states.  The slower system has only completed
a tenth of the instructions completed by the faster.  You now have to
wait .9 seconds for the slower one to catch up before you have anything
to compare.
If you could quickly load the entire state of the faster system just
before the instruction whose results you want to compare into the
slower one, you would only have to wait one of the slower systems's
instruction times - but how would you do that?  Even assuming a
simple mapping between the full states of disparate systems, the
state is *huge* - all of memory, all the registers, hidden information
(cache entries, branch prediction buffers).  Yes, only a small amount
of it is "relevant" to the next instruction - but (a) how can you
find it; (b) how can you find it *given that the actual execution of
the next instruction may be arbitrarily different from what the
system model claims*?
Long term-performance against a targetted attack means nothing.
The papers look interesting and I'll have a look at them, but if you
want to measure trust, you have to have something to start with.  What
we are dealing with here is the difference between a random fault and
a targetted attack.  It's quite true that long experience with a chip
entitles you to trust that, given random data, it will most likely
produce the right results.  But no amount of testing can possibly
lead to proper trust that there isn't a special value that will induce
different behavior.

@_date: 2008-04-29 10:57:42
@_author: Leichter, Jerry 
@_subject: SSL and Malicious Hardware/Software 
It's not the first.  Blue Coat, a company that's been building various
Web optimization/filtering appliances for 12 years, does the same thing.
I'm sure there are others.
I'm very uncomfortable with the whole business.
Corporations will of course tell you it's their equipment and is there
for business purposes, and you have no expectation of privacy while
using it.  I can understand the issues they face:  Between various
regulatory laws that impinge on the white-hot topic of "data leakage"
and issues of workplace discrimination arising out of questionable
sites, they are under a great deal of pressure to control what goes over
their networks.  But if monitoring everything is the stance they have to
take, I would rather that they simply block encrypted connections
As this stuff gets rolled out, there *will* be legal issues.  On the
one hand, the whole industry is telling you "HTTPS to a secure web
site - see that green bar in your browser? - is secure and private".
On the other, your employer is doing a man-in-the-middle attack and,
without your knowing it, reading your discussions with your doctor.
Or maybe gaining access to your credit card accounts - and who knows
who in the IT department might be able to sneak a peak.
Careful companies will target these appliances at particular sites.
They'll want to be able to prove that they aren't watching you order
your medications on line, lest they run into ADA problems, for example.
It's going to be very interesting to see how this all plays out.  We've
got two major trends crashing headlong into each other.  One is toward
tighter and tighter control over what goes on on a company's machines
and networks, some of it forced by regulation, some of it "because we
can".  The other is the growing technological workarounds.  If I don't
like the rules on my company's network, I can buy over-the-air broadband
service and use it from my desk.  It's still too expensive for most
people today, but the price will come down rapidly.  Corporate IT will
try to close up machines to make that harder and harder to do, but at
the same time there's a growing push for IT to get out of the business
of buying, financing, and maintaining rapidly depreciating laptops.
Better to give employees a stipend and let them buy what they want -
and carry the risks.

@_date: 2008-08-06 12:17:53
@_author: Leichter, Jerry 
@_subject: security questions 
These kinds of questions used to bother me.  Then I realized that
*I could lie*.  As long as *I* remember that I answer "What is your
mother's maiden name" with "xyzzy", the site and I can be happy.
Well ... happier, anyway.  The only way to remain sane if you take
this approach is to use the same answer at every site that asks
these security questions.  But that's not good, especially since
most of these sites appear to make the *actual value you specified*
available to their call centers.  This is nice if you can't remember
the exact capitalization you used, but it does, of course, leak more
information that you'd rather have out there readily accessible.
For Web sites these days, I generate random strong passwords and keep
them on a keychain on my Mac.  Actually, the keychain gets synchronized
automatically across all my Mac's using .mac/MobileMe (for all their
flaws).  When I do this, I enter random values that I don't even
record for the security questions.  Should something go wrong, I'm
going to end up on the phone with a rep anyway, and they will have
some other method for authenticating me (or, of course, a clever
social-engineering attacker).
The only alternative I've seen to this whole approach is sold by
RSA (owned by EMC; I have nothing to do with the product, but will
note my association with the companies) which authenticates based on
real-world data.  For example, you might be asked where you got
coffee this morning if your credit card shows such a charge.  This
approach is apparently quite effective if used correctly - though
it does feel pretty creepy.  (They were watching me buy coffee?)

@_date: 2008-08-07 10:32:13
@_author: Leichter, Jerry 
@_subject: security questions 
As best I can determine - based on external observation, not insider
information - the evolution went something like this:

@_date: 2008-08-08 10:18:17
@_author: Jerrold Leichter 
@_subject: More man-in-the-middle'd SSL sessions on the way 
From an article about WAN optimization appliances in Computerworld:
good option for another reason: Because data is optimized in an 	 unencrypted state, privacy and security concerns arise. But vendors 	 such as Riverbed, Juniper Networks and Blue Coat Systems can serve 	as  a trusted "man in the middle" for optimizing data encrypted with 	SSL,  which is commonly used in applications with Web interfaces and 	other  Internet traffic. They terminate the encrypted session,
It may indeed be a useful capability - but widespread use will destroy  what little is left of the SSL trust model.
                                                         -- Jerry

@_date: 2008-08-08 10:55:43
@_author: Leichter, Jerry 
@_subject: security questions 
RSA sells a product that is based on such research.  I don't have
references; perhaps someone else does.
I think the accurate statement here is:  There's been some research on
this matter, and there are some reasonable implementations out there;
but there are also plenty of "me-too" implementations that are quite
In fact, I've personally never run into an implementation that I would
not consider worthless.  (Oddly, the list of questions that started
this discussion is one of the better ones I've seen.  Unfortunately,
what it demonstrates is that producing a useful implementation with
a decent amount of total entropy probably involves more setup time
than the average user will want to put up with.)
Actually, this cuts both ways.  Automated interfaces generally require
exact matches; at most, they will be case-blind.  This is appropriate
and understood for passwords.  It is inappropriate for what people
perceive as natural-text questions and answers.  When I first started
running into such systems, when asked for where I was born, I would
answer "New York" - or maybe "New York City", or maybe "NY" or "NYC".
I should have thought about the consequences of providing a natural-
text answer to a natural-text question - but I didn't.  Sure enough,
when I actually needed to reset my password - I ended up getting locked
out of the system because there was no way I could remember, 6 months
later, what exact answer I'd given.
A human being is more forgiving.  This makes the system more vulnerable
to social engineering - but it makes it actually useable.  The
tradeoff here is very difficult to make.  By its nature, a secondary
access system will be rarely used.  People may, by dint of repetition,
learn to parrot back exact answers, even a random bunch of characters,
if they have to use them every day.  There's no way anything but a
fuzzy match on meaning will work for an answer people have to give
once every couple of months - human memory simply doesn't work that
I learned my lesson and never provide actual answers to these questions
any more.

@_date: 2008-08-08 13:04:16
@_author: Leichter, Jerry 
@_subject: OpenID/Debian PRNG/DNS Cache poisoning advisory 
Since the list of bad keys is known and fairly short, one could
explicitly check for them in the browser code, without reference to
any external CRL.
Of course, the browser itself may not see the bad key - it may see key
for something that *contains* a bad key.  So such a check would not be
complete.  Still, it couldn't hurt.
One could put similar checks everywhere that keys are used.  Think of it
as the modern version of code that checks for and rejects DES weak and
semi-weak keys.  The more code out there that does the check, the faster
bad keys will be driven out of use.

@_date: 2008-08-08 15:52:07
@_author: Leichter, Jerry 
@_subject: OpenID/Debian PRNG/DNS Cache poisoning advisory 
You can get by with a lot less than 64 bits.  People see problems like
this and immediately think "birthday paradox", but there is no "birthday
paradox" here:  You aren't look for pairs in an ever-growing set,
you're looking for matches against a fixed set.  If you use 30-bit
hashes - giving you about a 120KB table - the chance that any given
key happens to hash to something in the table is one in a billion,
now and forever.  (Of course, if you use a given key repeatedly, and
it happens to be that 1 in a billion, it will hit every time.  So an
additional table of "known good keys that happen to collide" is worth
maintaining.  Even if you somehow built and maintained that table for
all the keys across all the systems in the world - how big would it
get, if only 1 in a billion keys world-wide got entered?)
Or just go off to one of a number of web sites that have a full table.
Many solutions are possible, when they only need to be invoked very,
very rarely.

@_date: 2008-08-08 16:51:10
@_author: Leichter, Jerry 
@_subject: OpenID/Debian PRNG/DNS Cache poisoning advisory 
You're right, of course - I considered 32,000 to be "vanishingly small"
compared to the number of hash values, but of course it isn't.  The
perils of looking at one number just as decimal and the other just in
exponential form....
In any case, I think it's clear that even for extremely conservative
"false hit" ratios, the table size is quite reasonable.  You wouldn't
want the table on your smart card or RFID chip, perhaps, but there even
a low-end "smartphone" would have no problems.

@_date: 2008-02-04 09:33:37
@_author: Leichter, Jerry 
@_subject: Gutmann Soundwave Therapy 
Commenting on just one portion:
If efficiency is your goal - and realistically it has to be *a* goal -
then you need to think about the semantics of what you're securing.  By
the nature of VOIP, there's very little semantic content in any given
packet, and because VOIP by its nature is a real-time protocol, that
semantic content loses all value in a very short time.  Is it really
worth 17% overhead to provide this level of authentication for data that
isn't, in and of itself, so significant?  At least two alternative
approach suggest themselves:
It's great to build generic encrypted tunnels that provide strong
security guarantees regardless of what you send through them - just as
it's great to provide generic stream protocols like TCP that don't care
what you use them for.  The whole point of this discussion has been
that, in some cases, the generic protocols aren't really what you need:
They don't provide quite the guarantees you need, and they impose
overhead that may be unacceptable in some cases.  The same argument
applies to cryptographic algorithms.  Yes, there is a greater danger if
cryptographic algorithms are misused:  Using TCP where it's inappropri-
ate *usually* just screws up your performance, while an inappropriate
cryptographic primitive may compromise your security.  Of course, if you
rely on TCP's "reliablity" in an inappropriate way, you can also get
into serious trouble - but that's more subtle and rare.  Then again,
actually mounting real attacks against some of the cryptographic
weaknesses we sometimes worry about is also pretty subtle and rare.
The NSA quote someone - Steve Bellovin? - has repeated comes to mind:
Amateurs talk about algorithms.  Professionals talk about economics.
Using DTLS for VOIP provides you with an extremely high level of
security, but costs you 50% packet overhead.  Is that worth it to you?
It really depends - and making an intelligent choice requires that
various alternatives along the cost/safety curve actually be available.

@_date: 2008-02-07 10:34:42
@_author: Leichter, Jerry 
@_subject: Gutmann Soundwave Therapy 
Great minds run in the same ruts.  :-)
*Blush*.  Talk about running in the same ruts.  I was specifically
talking about dealing with lossy datagram connections, but when I came
to making a suggestion, suggested one I'd previously considered for
non-lossy stream connections.  Streams are so much easier to reason
about - it's easy to get caught.  (It's also all too easy to forget
that no stream implementation really implements the abstract semantics
of a reliable stream - which is irrelevant in some cases, but very
significant in others.)
My suggestion for a quick fix:  There's some bound on the packet loss
rate beyond which your protocol will fail for other reasons.  If you
maintain separate MAC's for each k'th packet sent, and then deliver k
checksums periodically - with the collection of checksums itself MAC'ed,
a receiver should be able to check most of the checksums, and can reset
itself for the others (assuming you use a checksum with some kind of
prefix-extension property; you may have to send redundant information
to allow that, or allow the receiver to ask for more info to recover).
Obviously, if you *really* use every k'th packet to define what is in
fact a substream, an attacker can arrange to knock out the substream he
has chosen to attack.  So you use your encryptor to permute the
substreams, so there's no way to tell from the outside which packet is
part of which substream.  Also, you want to make sure that a packet
containing checksums is externally indistinguishable from one containing
data.  Finally, the checksum packet inherently has higher - and much
longer-lived - semantic value, so you want to be able to request that
*it* be resent.  Presumably protocols that are willing to survive data
loss still have some mechanism for control information and such that
*must* be delivered, even if delayed.
Tons of hand-waving there; at the least, you have to adjust k and
perhaps other parameters to trade off security and overhead.  I'm
pretty sure something along these lines could be done, but it's
certainly not off-the-shelf.

@_date: 2008-02-07 11:31:26
@_author: Leichter, Jerry 
@_subject: Gutmann Soundwave Therapy 
A system I designed has this property:  You can choose the key exchange
mechanism separately from the encryption mechanism.  In fact, the
end user can select this (though generally he chooses one of a number
of pre-defined options, which internally are just macros).  The
encryption mechanism is able to enforce a quality constraint on which
keying mechanisms it's willing to deal with - e.g., only the NULL
encryption mechanism is willing to accept the "NO_KEY" key exchange.
I did make a simplifying assumption that there is a linear ranking
of quality for keying mechanisms, so that what an encryptor actually
specifies is "at least this strength".  There's a similar assumed
ranking for encryption mechanisms.  Negotiation is done by having
each end specify which keying and encryption mechanisms it is
willing to use (those it implements, filtered by user-specified
constraints), and then choosing the "strongest" in the intersection
of the mechanisms common to both.  In principle, one could similarly
choose an authentication mechanism.
The linear ranking worked in the particular situation where I designed
this but isn't generalizable.  Without that, things get much more
complex - you lose the nice property of the current implementation
that the two ends need merely exchange what the implement, and then
proceed independently to choose the "best" among the available
choices (and always come to the same conclusions).
All of this ignores a significant issue:  Are keying and encryption
(and authentication) mechanisms really independent of each other?
I'm not aware of much work in this direction.  Most of what's out
there is negative results that, on the one hand, tell you that
general independence theorems are impossible; but on the other,
they tend to be based on clearly pathological combinations, which
hints that independence theorems *might* be possible, if we knew
how to constrain the different components to avoid the pathologies.

@_date: 2008-02-07 14:42:36
@_author: Leichter, Jerry 
@_subject: Gutmann Soundwave Therapy 
Thanks for the reference.
It lets the receiver to make a choice:  Deliver the data immediately,
avoiding the latency at the cost of possibly releasing bogus data (which
we'll find out about, and report, later); or hold off on releasing the
data until you know it's good, at the cost of introducing audible
artifacts.  In non-latency-sensitive designs, the prudent approach is to
never allow data out of the cryptographic envelope until you've
authenticated it.  Here, you should probably be willing to do that, on
the assumption that the "application layer" - a human being - will know
how to react if you tell him "authentication has failed, please
disregard what you heard in the last 10 seconds".  (If you record the
data, the human being doesn't have to rely on memory - you can tell him
exactly where things went south.)  There are certainly situation where
this isn't good enough - e.g., if you're telling a fighter pilot to fire
a missile, a fake command may be impossible to countermand in time to
avoid damage - but that's pretty rare.

@_date: 2008-02-08 09:12:33
@_author: Leichter, Jerry 
@_subject: Gutmann Soundwave Therapy 
I don't know.  Can you prove that your way of looking at it is valid?
After all, I can look at encryption as applying a PRF to a data
stream, and authentication as computing a keyed one-way function (or
something) - so is there anything to prove about whether I can choose
and combine them independently?  About whether Encrypt-then-MAC and
MAC-then-Encrypt are equivalent?
I should think by now that we've learned how delicate our cryptographic
primitives can be - and how difficult it can be to compose them in a
way that retains all their individual guarantees.

@_date: 2008-02-10 06:24:44
@_author: Leichter, Jerry 
@_subject: Fixing SSL (was Re: Dutch Transport Card Broken) 
While trying to find something else, I came across the following
This was work done a Xerox.  I was trying to find a different report
at Xerox in response to Peter Gutmann's comment that certificate aren't
used because they are impractical/unusable.  Parc has done some wonderful
work on deal with those problems.  See:
    Not "Internet scale", but in an enterprise, it should work.

@_date: 2008-02-11 10:46:09
@_author: Leichter, Jerry 
@_subject: Dilbert on security 
Today's Dilbert -
is right on point....
 							-- Jerry

@_date: 2008-02-13 14:04:50
@_author: Leichter, Jerry 
@_subject: Toshiba shows 2Mbps hardware RNG 
I wonder if they considered the possibility that the device will be
destroyed by a static discharge?
It's one thing to criticize a design about which you know nothing on
the basis of a broad, little-known or brand new, attack.  But the
fact that EMI can skew devices has been known for years.  Hardware
that may need to work in (deliberately or otherwise) high-EMI
environments has to be appropriately designed and shielded (just as
devices have for years been protected against static discharge
through multiple layers of protection, from the chip design itself
through ground straps for people handling them).
I know nothing at all about Toshiba or its designers.  Do you know
something that makes you think they are so incompetent that they
are unaware of well-known issues that arise in the design of the
kinds of devices they work with?
Why?  Writing "hard random" values over the previous data is neither
more nor less secure than writing zeroes, unless you descend to the
level of attacking the disk surface and making use of remnance effects.
Once you do that ... it's still not clear that writing random values is
better or worse than writing all zeroes!  (As Peter Gutmann showed
years ago, there are *highly technology-specific sets of patterns*
that do a better job than all zeroes, or all ones, or whatever.
There's little reason to believe that a random set of bits is good
for much of anything in this direction.)
If you're concerned about someone distinguishing between erased
data and real data ... if the real data is unencrypted, then the
game is over anyway.  If the real data is encrypted, you want the
erased data to look "exactly as random" as the encrypted real
data.  That is, if you believe that your AES-encrypted (say)
data can be distinguished from random bits without knowning the
key, then if you fill the erased blocks with *really* random bits,
the distinguisher will tell you exactly where the real data is!
Better to use exactly the same encryption algorithm to generate
your "random" erasure pattern.
BTW, even pretty average disks these days can write 50 MB/second,
or 200 times the rate at which this device can generate random bits.
OK, though given the computational overhead involved in generating
symmetric keys, it's hard to see the random number generation as the
throttling factor.
If you're talking about inserting fillers to thwart traffic analysis,
the same argument as for erasing disk blocks:  Either you believe
your encrypted packets can't be distinguished from random, in which
case you don't need the generator; or you are afraid they *can* be,
in which case you'd better not use the generator!
I don't buy it.  First off, the rates are pretty low - how many packets
per second do you send?  Second, the attacks involved are probably
impossible to counter using software, because the timing resolutions are
too small.  Maybe you can build random jitter into the hardware itself -
but that brings in all kinds of other issues.  (The hardware is, of
course, *already* introducing random jitter - that's the basis of the
attack.  Just adding more without getting rid of the bias that enables
the attacks is little help; at worst, it just requires the attacker to
take more samples to average away the extra noise.  But if you can get
rid of the bias - do you still need the random jitter?)
I think this is a technically interesting innovation, and if as a result
chips come with reasonable random number generators - that would be
good.  But I'm unconvinced that there are many real applications that
need data rates even approaching the numbers here.  (If it's as cheap to
produce 2 Mb/sec as 2 Kb/sec - then it can't hurt to have the higher
value.  As someone else remarked, we can always stir a whole bunch of
bits together into one to make attacks that much harder.  But there are
few cases where 2 Mb/sec solves a problem that 2 Kb/sec can't.)

@_date: 2008-02-22 08:30:53
@_author: Leichter, Jerry 
@_subject: cold boot attacks on disk encryption 
I believe something like this has been written into law.  The reporting
laws are all state laws, so of course vary.  The Federal laws often
have "safe harbor" provisions for encrypted data.
Regardless of the law, the broad public perception is that "encrypted"
means "safe".  After one too many embarrasments, corporations (and
governments) have learned that "Oh, yes, 150,000 credit card numbers
were stolen but there's no evidence anyone is using them" no longer
works as damage control; but "Oh, yes, 150,000 credit card numbers
were stolen but that's OK - they were encrypted" works fine.  (Note
that these announcements don't even bother to discuss what the
encryption mechanism might be - ROT13, anyone?)
Unfortunately, the technical nature of these results - combined with
the "We told you to encrypt everything to make it safe; now we tell
you encryption isn't safe" nature of the debate, is unlikely to produce anything positive in the general public sphere.  People will probably
just shrug their shoulders, figure nothing can be done, and move on.

@_date: 2008-02-22 08:38:35
@_author: Leichter, Jerry 
@_subject: cold boot attacks on disk encryption 
We've viewed "screen locked" and "sleep mode" (with forced screen lock
on wake) as equivalent to "off".  Clearly that's no longer a tenable
position.  Sensitive data in memory must be cleared or encrypted,
with decryption requiring externally-entered information, whenever
the screen is locked or sleep mode initiated.  This would actually
make them *safer* than the "off" state, since at least you know
your software can gain control while entering those states!
I suspect GPS chip sets will become a standard part of laptops
in the future.  One can imagine some interesting techniques based
on them.  Even now, most laptops have motion sensors (used to
"safe" the disks), which could be used.
I seem to recall some (IBM?) research in which you wore a ring
with an RFID-like chip in it.  Move away from your machine for
more than some preset time and it locks.  I'm sure we'll see
many similar ideas come into use.

@_date: 2008-01-02 16:42:41
@_author: Leichter, Jerry 
@_subject: Death of antivirus software imminent 
Virtualization has become the magic pixie dust of the decade.
When IBM originally developed VMM technology, security was not a primary
goal.  People expected the OS to provide security, and at the time it
was believed that OS's would be able to solve the security problems.
As far as I know, the first real tie of VMM's to security was in a DEC
project to build a VMM for the VAX that would be secure at the Orange
Book A2 level.  The primary argument for this was:  Existing OS's are
way too complex to verify (and in any case A2 required verified design,
which is impossible to apply to an already-existing design).  A VMM can
be small and simple enough to have a verified design, and because it
runs "under" the OS and can mediate all access to the hardware, it can
serve as a Reference Monitor.  The thing was actually built and met its
requirements (actually, it far exceeded some, especially on the
performance end), but died when DEC killed the VAX in favor of the
Today's VMM's are hardly the same thing.  They are built for perfor-
mance, power, and managability, not for security.  While certainly
smaller than full-blown Windows, say, they are hardly tiny any more.
Further, a major requirement of the VAX VMM was isolation:  The
different VM's could communicate only through network protocols.  No
shared devices, no shared file systems.  Not the kind of thing that
would be practical for the typical uses of today's crop of VM's.
The claim that VMM's provide high level security is trading on the
reputation of work done (and published) years ago which has little if
anything to do with the software actually being run.  Yes, even as they
stand, today's VMM's probably do provide better security than some -
many? - OS's.  Using a VM as resettable sandbox is a nice idea, where
you can use it.  (Of course, that means when you close down the sandbox,
you lose all your state.  Kind of hard to use when the whole point of
running an application like, say, an editor is to produce long-lived
state!  So you start making an exception here, an exception there
... and pretty soon the sand is spilled all over the floor and is in
your eyes.)
The distinction between a VMM and an OS is fuzzy anyway.  A VMM gives
you the illusion that you have a whole machine for yourself.  Go back
a read a description of a 1960's multi-user OS and you'll see the
very same language used.  If you want to argue that a small OS *can
be* made more secure than a huge OS, I'll agree.  But that's a size
distinction, not a VMM/OS distinction....

@_date: 2008-01-02 19:15:05
@_author: Leichter, Jerry 
@_subject: Death of antivirus software imminent 
It's not clear to me what threats this protects you against.  A Windows
virus would work within the Windows environment just as it always did.
If that's *your* working environment, it's just as contaminated as if
you were running Windows on bare metal.
Of course, if you're using the sandbox idea, you can throw out your
contaminated Windows environment periodically and start from fresh.
As always, you need to be in a position to throw *everything* out,
which can be rather painful.
A virus that could break through Windows, then through VMWare (with
or without SELinux), then actually do something in that environment
to establish itself more strongly, probably doesn't exist today - and
would be quite an interesting challenge.
That's a more reasonable approach.

@_date: 2008-01-04 09:44:40
@_author: Leichter, Jerry 
@_subject: Death of antivirus software imminent 
Ah, yes - the unexpected side-effect which happens to be positive.
If you read Garfinkle et al's paper on the detectability of VMM's -
and the low likelyhood of ever producing an undetectable VMM's - you
can see some similar things happening.  Some of the techniques are
fairly universal - e.g., those based on measuring TLB sizes, which
is likely to be usable on any machine that uses virtual memory.  But
many others are based on ugly botches in the x86 architecture (e.g.,
the user-mode instructions like SIDT which reveal privileged state)
or the absurd complexity and rough edges of many I/O devices.
For security in general, unexpected side-effects are almost always paths
to break into the system - think power and timing analysis for two great
examples.  I suppose we have to catch a break sometimes....

@_date: 2008-01-04 12:46:43
@_author: Leichter, Jerry 
@_subject: DRM for batteries 
For laptop batteries - which can cost $100 each - some might see it as
a win.  Of course, if you can eliminate the competition, you can also
raise prices.
The spec sheets have links to PDF description of the algorithm, but
distribution of that is restricted - talk to your salesman.
Can anyone make any sense of the following claim:
The only thing I can come up with is the old idea that you compute (say)
a 32-bit keyed MAC but then only use the bottom 16 bits.  This makes it
more difficult for an attacker to use the MAC on some data to determine
the key - on average, you'd probably need about 2^16 samples to give
a unique key.  This was use in some old X.something-or-other bank
hashing algorithm, which predates functions that we believe to be
Over all, I find it hard to see how such a product can really make
sense, however.  If there's enough money to make it worth trying to keep
the clone makers out, there's enough money in it for the clone makers to
be willing to invest in determining the secret information.  Given the
nature of the proposed solution, all batteries (or other protected
objects) have to have the same secret - break into one, and you can make
as many as you like, so any cost for breaking in is amortized over
however many of the things you can sell.  There's no effective way to
change the secret - even if you could somehow make a patch to the
devices involved, you couldn't change it in such a way that it would
refuse to use the batteries already in it (with the old secret).
Meanwhile, at $1.40 a unit, you can't make anything really tamper
protected.  (Given some of the reverse engineering expertise already out
there, it's not clear how "tamper protected" you can really make
something these days at *any* cost.  But you certainly can't do it on
the cheap.)
Still, I'm sure people will try - and life will become even more

@_date: 2008-01-04 17:47:33
@_author: Leichter, Jerry 
@_subject: Death of antivirus software imminent 
Why not just require that the senders of malign packets set the Evil Bit
in their IP headers?
How can you possibly require that encrypted traffic *generated by the
attackers* will allow itself to be inspected?  The NSA tried to do
that by concealing information about effective cryptographic algorithms
while providing algorithms it controlled.  But that horse has long
left the barn.  Effective algorithms are widely known and readily
available processors are easily fast enough to implement them.
If you require lawful code to use inspectable crypto, every time you
successfully inspect a datastream, you'll find - surprise! - that it
contains nothing objectionable.  Meanwhile, the streams you can't
"open up" will continue to contain all the dirty stuff.  And, of
course, if you attempt to "open" a stream and what you see looks
like random bits - is it because someone has given you a bogus
key, or because it's a compressed video stream?

@_date: 2008-01-04 18:23:21
@_author: Leichter, Jerry 
@_subject: Death of antivirus software imminent 
Just because it *looks* like SSL doesn't mean that the key it leaks to
you is actually valid.  And if it *is* actually valid, it doesn't mean
that there isn't a second layer of encryption inside the SSL session.
Go back to my example:  How will you distinguish between random bits
and a compressed video stream?  Do you assume that every codec in the
world will be registered?  How about a big scientific dataset of
floating point values?  Or some huge, validly formatted, spreadsheet
of such values?
And that doesn't even consider obvious countermeasures.  What happens
if you decrypt and see a bunch of ASCII values that follow the first
and second order statistics of English text?  Sure, encoding my
encrypted data like that costs me some overhead - but given the
speed of today's networks, who cares?
This train left the station a *long* time ago.

@_date: 2008-01-07 10:14:47
@_author: Leichter, Jerry 
@_subject: Death of antivirus software imminent  
While I agree with your general point, this particular argument
is a misuse of the McCabe score.  Replacing:
L:	Y'
*at the machine code level* should have absolutely no effect on the
complexity of the algorithm (beyond any delta between Y and Y').  If you
insist on computing your McCabe score from the generated code, and it
gives you a different answer, then the score you are deriving is
The whole point of measurements like McCabe is to measure the complexity
of the algorithm *as seen by a human being*.  Automated code transforma-
tions that no human being ever sees should not affect it.  Otherwise,
you're going to have to throw out all your optimizing compilers.  The
transformation above occurs not just in patching but in other contexts -
e.g., this might be adventageous, with Y and Y' semantically equivalent,
if Y contains a bunch of calls that can't be reached with short
call sequences when at their original location, but can be if they are
relocated to L.  Or there might be cache interference effects that are
avoided by relocating.
Roughly similar patterns are used in generating code for loops, where
the surface semantics might require two copies of a test (one at loop
entry, one at the bottom of the loop) but this transformation lets you
get by with a single copy.
As long as the algorithm developer's view is that control flows directly
from X to Y (and there are no incoming edges this is one node, no
matter how the compiler or patch generator decides to shake and bake it
into memory.

@_date: 2008-01-07 11:20:51
@_author: Leichter, Jerry 
@_subject: Death of antivirus software imminent  
It's not really my area, sorry.  I'm just looking at this from
a very general point of view.  The point of McCabe and similar
measures is to point you to areas likely to contain bugs, or
that are likely to be particularly costly to implement.  Since
it's *humans* who actually implement (and produced bugs), a
meaningful measure can't depend on things that human beings
don't see.  This kind of analysis can be very powerful.  Everyone
has heard of Galileo's experiment dropping balls of different
weights and proving they hit the ground at the same time - but how
many people are aware of his theoretical argument that this must
be the case?  It's very simple:  Suppose a 2lb ball drops faster
than a 1lb ball.  Take the 2lb ball and pull it into a dumbell
shape, with (almost) 1lb at each end, but the ends very close
together.  Presumably, it still drops at the speed of a 2lb
ball.  Now pull the halves apart a bit at a time, gradually
thining out the connecting segment.  Eventually, you have two
1lb ball connected by a thread.  Does that drop at the speed
of the individual 1lb balls, or at the speed of a 2lb ball?
Clearly, it has to be both - the 1lb and 2lb balls must drop
at the *same* speed!
I pretty sure the build environments that give you McCabe measures
automatically are pulling the information from the control
flow analysis in compiler front ends.  This is where basic
blocks and the edges connecting them are first extracted.
Computing McCabe is trivial at this point - and the structure
it is computed on will correspond pretty directly to what a
human being would have perceived.  As various optimizations
are applied, the structure will change - and there is no
reason to believe that the McCabe measure won't change along
the way, since preserving McCabe is hardly a goal of optimizing

@_date: 2008-01-07 12:13:54
@_author: Leichter, Jerry 
@_subject: Foibles of user "security" questions 
Reported on Computerworld recently:  To "improve security", a system
was modified to ask one of a set of fixed-form questions after the
password was entered.  Users had to provide the answers up front to
enroll.  One question:  Mother's maiden name.  User provides the
4-character answer.  System refuses to accept it:  Answer must have
at least 6 characters.
I can just see the day when someone's fingerprint is rejected as
"insufficiently complex".
 							-- Jerry

@_date: 2008-01-23 09:13:22
@_author: Leichter, Jerry 
@_subject: patent of the day 
Alternatively, it could be an attempt to preempt any other patents
in this area.  We'll have to see what Garfinkle does with the
BTW, I don't see this as an example of an absurd patent.  There might
well be prior art, but the idea of erasing information by deliberately
discarding a key is certainly not completely obvious except in
retrospect.  If you look at any traditional crypto text, you won't
find anything of this sort - it wasn't the kind of thing people had
worried about until fairly recently.

@_date: 2008-01-23 18:39:29
@_author: Leichter, Jerry 
@_subject: VaultID 
Anyone know anything about these guys?  (  They
are trying to implement one-time credit card numbers on devices
you take with you - initially cell phones and PDA's, eventually in
a credit card form factor.  The general idea seems good, but their
heavy reliance on fingerprint recogition is troubling (though it may
be appropriate in their particular application).
 							-- Jerry

@_date: 2008-07-01 10:57:14
@_author: Leichter, Jerry 
@_subject: The wisdom of the ill informed 
Let's think about the economics here.  What's the value of the information they are sending you to someone else?  What could they do with it?  Apply for your insurance payment?  You'll discover that rather rapidly when you try to apply.  Discover what medical equipment you're ordering?  Is cracking the cryptography here anything like the easiest way to to get that information?  It's a myth that medical information is private - too many different parties have access to it in the normal course of things.
On the flip side, how many people will have trouble remembering even a six-digit password?  (Keep in mind that, by the nature of the business you're talking about - medical supplies - many of the customers will be ill/old.)
Frankly, I find it rather impressive that they provide *any* degree of security.  Six digits may in fact be more than is justified, given the value-of-information/usability tradeoffs.

@_date: 2008-07-02 11:00:20
@_author: Leichter, Jerry 
@_subject: Strength in Complexity? 
The cynical among us might rephrase that as:  "The more complex and
awkward they can make a protocol, the better it will be at generating
future consulting work."  :-(
(I don't think that applies to your list, where the root causes have
more to do with design-by-committee and the consequent need to make
everyone happy.)

@_date: 2008-07-08 17:15:21
@_author: Leichter, Jerry 
@_subject: disks with hardware FDE 
I have no idea what they actually *do*, but the obvious way to get an IV
is to use the encryption of the block number.  Guaranteed known to
whoever needs to decrypt the disk block, and unique for each disk block.
(Using the disk block number itself as the IV is actually reasonably
safe, too, though it seems a bit too structured - one can imagine files
which have a leading count or even a copy of the disk block number in
each disk block leading to an initial zero input to the encryption.)
(I think one of Phil Rogoway's papers suggest this kind of approach for
a "safe" CBC API:  Given an existing CBC API that takes an IP as input,
instead build one that takes no explicit IP, but (a) maintains an
internal counter; (b) prepends the current counter value to the
supplied input and increments the counter; (c) supplies the underlying
API with an IP of 0.  The modified API can't be abused by accidentally
re-using an IP.)
Somehow we still haven't learned the lesson that the security can only
come from (a) published, vetted algorithms and modes; (b) a way to check
that the alleged algorithm is what the "black box" actually implements.
Of course, for all you know it implements the algorithm while hiding a
copy of the key away somewhere "just in case"....  But that's a whole
other problem.

@_date: 2008-07-09 12:05:14
@_author: Leichter, Jerry 
@_subject: Permanent Privacy - Are Snake Oil Patents a threat? 
Patent law and its interpretation - like all law - changes over time.
Through much of the early twentieth century, patent law was strongly
biased in favor of large companies.  A small inventor couldn't get any
effective quick relief against even obvious infringements - he had to
fight a long, drawn-out battle, at the end of which he probably didn't
end up with much anyway.  In reaction to such famous cases as the
much-infringed patent on FM radio, the law was changed and reinterpreted
in ways that gave the small inventor much more power.  Unfortunately,
patent trolls eventually made use of those same changes....
The last couple of decades have seen a series of cases that effectively
gutted the entire notion of "obvious to persons having ordinary skills
in the art."  As often happens with trends like this, if you look back
at the early cases that started the trend, the results may seem
reasonable - but over time, the whole thing gets out of control.
The Supreme Court, in a decision last year (the name and details of
which escape me), pretty much said "This has gone too far."  Specific-
ally, they said that applying a technique that is well known in one area
to another area may well be "obvious" and not eligible for patent
protection.  The Supreme Court can only decide on cases brought before
it, but the feeling seems to be that they are signaling a readiness to
breath new life into the "non-obviousness" requirement for a patent.
It'll be years before we see exactly how this all settles out.

@_date: 2008-07-09 18:58:33
@_author: Leichter, Jerry 
@_subject: "Securing the Network against Web-based Proxies" 
Ah, where the web is going.  8e6 Technologies sells a hardware box
that it claims does signature analysis to detect HTTP proxies and
blocks them.  It can also block HTTPS proxies "that do not have a
valid certificate" (whatever that means), as well as do such things
as block IM, force Google and Yahoo searches to be done in Safe
mode, and so on.
They're marketing this to the education community (with the typical
horror stories of the problems your school district can run into
if students use proxies to get around your rules).
What I find most interesting, though, is that the company, based
in California, has an overseas presence in exactly two other
countries:  Taiwan and China.  One doesn't need much imagination
to see what market they are going after there....
 							-- Jerry

@_date: 2008-07-15 18:33:10
@_author: Leichter, Jerry 
@_subject: how bad is IPETEE? 
For an interesting discussion of IPETEE, see:
Brief summary:  This is an initial discussion - the results of a
drinking session - that got leaked as an actual proposal.  The
guys behind it are involved with The Pirate Bay.  The goal is
to use some form of opportunistic encryption to make as much
Internet traffic as possible encrypted as quickly as possible -
which puts all kinds of constraints on a solution, which in
turn also necessarily weakens the solution (e.g., without some
required configuration, there's no way you can avoid MITM
attacks) and forces odd compromises.

@_date: 2008-07-24 11:48:03
@_author: Leichter, Jerry 
@_subject: BBC on Bletchly Park 
The URL for the full article - which includes pictures and a short
film segment - is:
 							-- Jerry
'Neglect' of Bletchley condemned
A call to save Bletchley Park has gone out from the UK's computer
More than 100 academics have signed a letter to The Times saying the
code-cracking centre and crucible of the UK computer industry deserves
They say Bletchley, Buckinghamshire, should be put on a secure financial
basis like other "great museums".
"We cannot allow this crucial and unique piece of both British and World
heritage to be neglected in this way," the letter to The Times said.
The academics were brought together by Dr Sue Black, head of the
computer science department at the University of Westminster, who was
moved to act after visiting Bletchley Park in early July.
"I went up there and felt quite upset by what I saw," she said.
Many of the buildings on the Bletchley estate were in a state of serious
disrepair, she said. One building, where code-breakers worked during
World War II, was falling apart, said Dr Black, and was protected by a
blue tarpaulin that was nailed down over it.
Describing Bletchley as a "gem", Dr Black said it was a "national
disgrace" that such a historic site was being allowed to fall into ruin.
"I do not know why they do not have funding as a national museum," she
The visit led her to contact other heads of computer science departments
at universities up and down the country. Within hours, she had hundreds
of responses - all of them backing her call.
Dr Black said she had been "overwhelmed" by the response which showed
the depth of feeling about Bletchley and the position it occupies in the
history of the computer age.
Bletchley Park is well known as the place where the Enigma codes were
broken but it is also the place where Colossus was created - a machine
that was the forerunner of many modern computers.
The engineers that worked on Colossus at Bletchley helped define and
develop the UK computer industry after WWII ended, said Dr Black.
What was needed, she said, was for Bletchley Park to get secure funding
from the government. Until recently the site was deemed ineligible for
Lottery funding that would help preserve it.
A change to the rules on who can get funds has led to negotiations with
the Lottery Fund. However, said Dr Black, it could still take up to a
year for funds to materialise.
In the meantime, said Dr Black, the site was falling into an ever worse
state of disrepair.
Story from BBC NEWS:
Published: 2008/07/24 00:48:21 GMT
? BBC MMVIII

@_date: 2008-06-01 20:06:17
@_author: Leichter, Jerry 
@_subject: Protection mail at rest 
Excellent idea!  I like it.
Of course, it's another piece of a distributed solution that you need
to keep running.  It would make for an interesting third-party
service.  (On the surface, letting a third party run this for you
seems hazardous, but as always your stuff is exposed on the way
to the forwarder whatever you do....
A forwarded like this as a pre-packaged EC2 VM, perhaps?

@_date: 2008-06-04 17:50:49
@_author: Leichter, Jerry 
@_subject: the joy of "enhanced" certs 
This message, shortly after our discussion of trust, makes me think of
the applicability of an aspect liguistic theory, namely speech acts.
Speech acts are expressions that go beyond simply communication to
actually produce real-world effects.  The classic example:  If I say
"John and Sarah are married", that's a bit of communication; I've passed
along to listeners my belief in the state of the world.  When a
minister, in the right circumstances, says "John and Sarah are married",
those words actually create the reality:  They *are* now married.
There are many more subtle examples.  A standard example is that of
a promise:  To be effective as a speech act, the promise must be
made in a way that makes it clear that the promiser is undertaking
some obligation, and the promiser must indeed take on that obligation.
There's a whole cultural context involved here in what is needed for
an obligation to exist and what it actually means to be obligated.
(Ultimately, the theory gets pushed to the point where it breaks;
but we don't have to go that far.)
In human-to-human communication, we naturally understand and apply the
distinction between speech acts and purely communicative speech.  It's
not that we can't be fooled - a person who speaks with authority is
often taken to have it, which may allow him to create speech acts he
should not be able to - but this is relatively rare.
When exchanging data with a machine, the line between communication and
speech acts gets very blurry.  (You can think of this as the blurry line
between data and program.)  When I go into a store and ask for
information, I see myself and the salesman as engaging in pure
communication.  There are definite, well-understood ways - socially and
even legally defined steps - that identify when I've crossed over into
speech acts and have, for example, taken on an obligation to pay for
something.  When, on the other hand, I look at a Web site, things are
not at all clear.  From my point of view, the data coming to my screen
is purely communication to me.  From the computer's point of view, the
HTML is all "speech acts," causing the computer to take some actions.
My clicks are all "speech acts" to the server.  Problems arise when what
I see as pure communication is somehow transformed, without my consent
or even knowledge, into speech acts that implicate *me*, rather than my
computer.  This happens all too easily, exactly because the boundary
between me and my computer is so permeable, in a Web world.
Receiving an SSL cert, in the proper context (corresponds to the URL
I typed, signed by a trusted CA), is supposed to be a speech act to
me as a human being:  It's supposed to cause me to believe that I've
reached the site I meant to reach.  (My machine, of course, doesn't
care - it has no beliefs and has nothing at risk.)  The reason the model
is so appealing is that it maps to normal human discourse.  If my friend
tells me "I'll bring dinner," I don't cook something while waiting for
him to arrive.
Unfortunately, as we've discussed here many times, the analogy is
deeply, fundamentally flawed.  SSL certs don't really work like trusted
referals from friends, and the very familiarity of the transactions is
what makes them so dangerous:  It makes it too easy for us to treat
something as a speech act when we really shouldn't.
Enhanced security certs simply follow the same line of reasoning.  They
will ultimately prove just as hazardous.
Going back to promises as speech acts:  When a politician promises to
improve the economy, we've all come to recognize that, although that's
in the *from* of a promise, it doesn't actually create any obligation.
"Improving the economy" isn't something anyone can actually do - even if
we could agree on what it means.  Such a promise is simply a way of
saying "I think the economy should be better".  Politicians make
statements in this form because at some level, even though we know
better, we *do* treat them as speech acts.  It's a many-millenia-long
struggle between those trying to rouse the rabble and the "rabble"
trying to avoid being improperly roused.
Somehow, we're going to need to develop a better way for humans to
understand which computer communications are "just information," and
which should be treated as speech acts.

@_date: 2008-06-09 11:54:20
@_author: Leichter, Jerry 
@_subject: Ransomware 
Computerworld reports:
on a call from Kaspersky Labs for help breaking encryption used by some
ransomeware:  Code that infects a system, uses a public key embedded in
the code to encrypt your files, then tells you you have to go to some
web site and pay for the decryption key.
Apparently earlier versions of this ransomware were broken because of a
faulty implementation of the encryption.  This one seems to get it
right.  It uses a 1024-bit RSA key.  Vesselin Bontchev, a long-time
antivirus developer at another company, claims that Kaspersky is just
looking for publicity:  The encryption in this case is done right and
there's no real hope of breaking it.
Speculation about this kind of attack has made the rounds for years.
It appears the speculations have now become reality.
 							-- Jerry

@_date: 2008-06-09 15:44:13
@_author: Leichter, Jerry 
@_subject: Ransomware 
Bontochev's comment as well.
Of course, there is one way this can be much worse than a disk crash:  A
clever bit of malware can sit there silently and encrypt files you don't
seem to be using much.  By the time it makes its ransom demands, you
may find you have to go back days or even weeks in your backups to get
valuable data back.
Even worse, targeted malwared could attack your backups.  If it encrypted
the data on the way to the backup device, it could survive silently for
months, by which time encrypting the live data and demanding the
ransom would be a very credible threat.  (Since many backup programs
already offer encryption, hooking it might just involve changing the
key.  It's always so nice when your opponent provides the mechanisms
needed to attack him....)

@_date: 2008-06-10 18:01:11
@_author: Leichter, Jerry 
@_subject: A slight defect in the truncated HMAC code... 
There's another ... issue with SNMPv3, this time with encryption keys.
The SNMPv3 standard defines a mechanism for converting an entered pass
phrase into an AES key.  The standard also specifies a(n appropriate)
minimum length for the pass phrase.
SNMP agents running in devices sold by a certain, err, very large vendor
do not enforce the minimum length, and it is in fact common to see
devices configured using short pass phrases.  Software that needs to
talk to such devices, and which *does* enforce the requirements of the
standard, will of course be unable to do so.  (Well, I suppose there is
almost certainly an equivalent pass phrase that's long enough, but
finding is impractical if the key derivation function is any good.)  So
such software must necessarily ignore the security requirements of the
standard as well.
Not only is it hard to define technically correct solutions to security
problems ... it's damn difficult to get them fielded!

@_date: 2008-06-11 11:53:54
@_author: Leichter, Jerry 
@_subject: Ransomware 
Returning to the point of the earlier question - why doesn't someone
pay the ransom once and then use the key to decrypt everyone's files:
Assuming, as seems reasonable, that there is a "session" key created
per machine and then encrypted with the public key, what you'd get
for your ransom money is the decryption of that one session key.
Enough to decrypt your files, not useful on any other machine.
There's absolutely no reason the blackmailer should ever reveal the
actual private key to anyone (short of rubber-hose treatment of some

@_date: 2008-06-11 15:04:21
@_author: Leichter, Jerry 
@_subject: Ransomware 
This is the first time I've seen any mention of RC4.  *If* they are
using RC4, and *if* they are using it incorrectly - then yes, this
would certainly work.  Apparently earlier versions of the same malware
made even more elementary cryptographic mistakes, and the encryption
was easily broken.  But they learned enough to avoid those mistakes
this time around.  Even if they screwed up on cipher and cipher mode
this time - expect them to do better the next time.

@_date: 2008-03-15 17:56:14
@_author: Leichter, Jerry 
@_subject: RNG for Padding 
It's a requirement of all modern cryptosystems that they be secure
against known-plaintext attacks.  This is for two reasons:

@_date: 2008-03-17 10:06:17
@_author: Leichter, Jerry 
@_subject: delegating SSL certificates 
Apple's Mail.app checks certs on SSL-based mail server connections.
It has the good - but also bad - feature that it *always* asks for
user approval if it gets a cert it doesn't like.
One ISP I've used for years (BestWeb) uses an *expired* self-signing
cert.  The "self-signed" part I could get around - it's possible to
add new CA's to Mail.app's list.  But there's no way to get it to
accept an expired cert automatically.  So ... every time Mail.app
starts up, it complains about the cert and asks me to approve it.
This stalls Mail's startup, and it fails to pick up mail - from any
server - until I tell it, OK, yes, go ahead.  The cert has now been
expired for over 2 years.  (You might well wonder why, if you're going
to use a self-signed cert, you *ever* let it expire - much less cut one,
like theirs, with a 1-year lifetime.  Since all you're getting with a
self-signed cert is "continuity of identity", expiration has no
positives, just negatives.  Perhaps they were planning to go out of
business in a year? :-) )
I've been in touch with BestWeb's support guys repeatedly.  Either
they just don't understand what I'm talking about, or I'll finally
get someone to understand, he'll ask me for details on which cert
is expired, I'll send them - and then nothing will happen.
Clueless.  Just to add to the amusement, *some* of their services - Web
mail, and through it tuning of their spam filters - are accessible
*only* through HTTP, not HTTPS.  These use the same credentials....
Perhaps I should just go with the flow and use unencrypted connections.
(Or get over my inertia, stop trying to get them to fix things, and
drop my connections to them at the next renewal....)

@_date: 2008-03-19 14:25:36
@_author: Leichter, Jerry 
@_subject: Firewire threat to FDE 
Just how would that help?  As I understand it, Firewire and PCMCIA
provide a way for a device to access memory directly.  The OS doesn't
have to do anything - in fact, it *can't* do anything.  Once your
attacker is on the bus with the ability to do read/write cycles to
memory, it's a bit late to start worrying about whether you allow
that device to be visible through the OS.
Note that disks have always had direct access to memory - DMA is the
way to get acceptable performance.  SATA ports - uncommon on portables,
very common on servers - would be just as much of a threat.  Same for
SCSI on older machines.
Normally, the CPU sets up DMA transfers - but it's up to the device to
follow the rules and not speak until recognized.  But there's no real
enforcement.  (Oh, if you start talking out of turn, you might hang
the bus or crash the system if you collide with something - but that's
like very rare, and hardly an effective protective measure.)
The only possible protection here is at the hardware level:  The
external interface controller must be able to run in a mode which
blocks externally-initiated memory transactions.  Unfortunately,
that may not be possible for some controllers.  Sure, the rules for
(say) SCSI might say that a target is only supposed to begin sending
after a request from an initiator - but it would take a rather
sophisticated state machine to make sure to match things up properly,
especially on a multi-point bus.

@_date: 2008-03-21 16:08:18
@_author: Leichter, Jerry 
@_subject: convergent encryption reconsidered 
The way "obvious in retrospect" applies here:  The vulnerability is
closely related to the power of probable plaintext attacks against
systems that are thought to be vulnerable only to known plaintext
attacks.  The general principle that needs to be applied is:  In any
cryptographic setting, if knowing the plaintext is sufficient to get
some information out of the system, then it will also be possible to get
information out of the system by guessing plaintext - and one must
assume that there will be cases where such guessing is "easy enough".

@_date: 2008-03-30 15:12:01
@_author: Leichter, Jerry 
@_subject: [p2p-hackers] convergent encryption reconsidered 
How would that help?
Both the ability of convergent encryption to eliminate duplicates,
and this attack, depend on there being a deterministic algorithm
that computes a key from the file contents.  Sure, if you use a
different salt for each file, the attack goes away - but so does
the de-duplication.  If you don't care about de-duplication, there
are simpler, cheaper ways to choose a key.

@_date: 2008-05-08 19:08:57
@_author: Leichter, Jerry 
@_subject: How far is the NSA ahead of the public crypto community? 
An interesting datapoint I've always had on this question:  Back in 1975
or so, a mathematician I knew (actually, he was a friend's PhD advisor)
left academia to go work for the NSA.  Obviously, he couldn't say
anything at all about what he would be doing.
The guy's specialty was algebraic geometry - a hot field at the time.
This is the area of mathematics that studied eliptic curves many years
before anyone realized they had any application to cryptography.  In
fact, it would be years before anyone on the outside could make any
kind of guess about what in the world the NSA would want a specialist
in algebraic geometry to do.  At the time, it was one of the purest
of the pure fields.
The friend he used to advise bumped into this guy a few years later
at a math conference.  He asked him how it felt not to be able to
publish openly.  The response:  When I was working at the university,
there were maybe 30 specialists in the world who read and understood
my papers.  There aren't quite as many now, but they really appreciate
what I do.

@_date: 2008-05-08 23:31:18
@_author: Leichter, Jerry 
@_subject: It seems being in an explosion isn't enough... 
On the other hand ... from a report in Computerworld, we have:

@_date: 2008-05-09 17:18:42
@_author: Leichter, Jerry 
@_subject: It seems being in an explosion isn't enough... 
Well, he's the guy who actually recovers data from the things.
I think the main issue here is that the older drives used much larger
magnetic domains on the disk, inherently providing a great deal of
physical redundancy, for those with the equipment to make use of it.
Also, the encodings were much simpler and the controllers much less
sophisticated.  Today the controller/head/disk are effectively a single
unit, tightly coupled by complex feedback loops.  The controller writes
data that it will be able to read, adjusting things based on what it
actually reads back.  I've been told - I can't verify this - that in
practical terms today, if you lose the controller, the data is toast:
Another nominally identical controller won't be able to read it.

@_date: 2008-05-11 07:27:42
@_author: Leichter, Jerry 
@_subject: FBI Worried as DoD Sol Counterfeit Networking Gear 
Note the reference to recent results on "spiking" hardware.  (From some
IDG journal - I forget which.)
 							-- Jerry
---------- Forwarded message ----------
FBI Worried as DoD Sold Counterfeit Networking Gear
Stephen Lawson and Robert McMillan, IDG News Service
Friday, May 09, 2008 5:10 PM PDT
The U.S. Federal Bureau of Investigation is taking the issue of
counterfeit Cisco equipment very seriously, according to a leaked FBI
presentation that underscores problems in the Cisco supply chain.
The presentation gives an overview of the FBI Cyber Division's effort to
crack down on counterfeit network hardware, the FBI said Friday in a
statement. "It was never intended for broad distribution across the
In late February the FBI broke up a counterfeit distribution network,
seizing an estimated US$3.5 million worth of components manufactured in
China. This two-year FBI effort, called Operation Cisco Raider, involved
15 investigations run out of nine FBI field offices.
According to the FBI presentation, the fake Cisco routers, switches and
cards were sold to the U.S. Navy, the U.S. Marine Corps., the U.S. Air
Force, the U.S. Federal Aviation Administration, and even the FBI
One slide refers to the problem as a "critical infrastructure threat."
The U.S. Department of Defense is taking the issue seriously. Since
2007, the Defense Advanced Research Projects Agency has funded a program
called Trust in IC, which does research in this area.
Last month, researcher Samuel King demonstrated how it was possible to
alter a computer chip to give attackers virtually undetectable back-door
access to a computer system.
King, an assistant professor in the University of Illinois at Urbana-
Champaign's computer science department, has argued that by tampering
with equipment, spies could open up a back door to sensitive military
In an interview on Friday, he said the slides show that this is clearly
something that has the FBI worried.
The Department of Defense is concerned, too. In 2005 its Science Board
cited concerns over just such an attack in a report.
Cisco believes the counterfeiting is being done to make money. The
company investigates and tests counterfeit equipment it finds and has
never found a "back door" in any counterfeit hardware or software, said
spokesman John Noh.  "Cisco is working with law enforcement agencies
around the world on this issue."
The company monitors its channel partners and will take action,
including termination of a contract, if it finds a partner selling
counterfeit equipment, he said. "Cisco Brand Protection coordinates and
collaborates with our sales organizations, including government sales,
across the world, and it's a very tight integration."
The best way for channel partners and customers to avoid counterfeit
products is to buy only from authorized channel partners and
distributors, Noh said.  They have the right to demand written proof
that a seller is authorized.
The FBI doesn't seem satisfied with this advice, however. According to
the presentation, Cisco's gold and silver partners have purchased
counterfeit equipment and sold it to the government and defense
Security researcher King believes that the government is better off
focusing on detection rather than trying to secure the IT supply chain,
because there are strong economic incentives to keep it open and
flexible -- even if this means there may be security problems. "There
are so many good reasons for this global supply chain; I just think
there's no way we can secure it."

@_date: 2008-05-30 15:04:34
@_author: Leichter, Jerry 
@_subject: Protection mail at rest 
At one time, mail delivery was done to the end-user's system, and all
mail was stored there.  These days, most people find it convenient to
leave their mail on a IMAP server:  It can be accessed from anywhere,
it can be on a system kept under controlled conditions (unlike a
laptop), and so on.  In fact, most people these days - even the
technically savvy - not only leave their mail on an IMAP server,
but let some provider deal with the headaches of maintaining that
So, most people's mail spends most of its life sitting on a disk owned,
managed, and controlled by some third party, whose responsibilities, not
to mention abilities, for keeping that stuff secure are unclear to say
the least.  On top of that, the legal protections for data held by a
third party are limited.
We have mechanisms for providing end-to-end encryption of messages.
Messages sent using, say, S/MIME are encrypted on the IMAP server
just as they are out on the net.  But this only helps for mail
exchanged between correspondents who both choose to use it.
Suppose I ask for a simpler thing:  That my mail, as stored in my
IMAP server, spends "most of its life" encrypted, inaccessible even
to whoever has access to the physical media on which the server
stores its mail.
Now, this is a funny goal.  If mail arrives unencrypted, anyone with
access to the data stream can copy it and do what they like.  It will
inevitably be buffered, even likely stored on a disk, in the raw,
unencrypted form.  We explicitly leave dealing with this out of the
equation - only end-to-end encryption can deal with it.
Here are two ways of implementation something in this direction:
 	1.  Client only.  The client, whenever it sees a new message,
 		(a) downloads it; (b) encrypts it using a secret key;
 		(c) stores the encrypted version back on the server;
 		(d) deletes the unencrypted version.  The client can
 		put the encrypted messages in a different folder, or
 		it can mark them with a header line.
 	2.  Server-assisted.  The client gives the server its public
 		key.  When a message arrives at the server, the
 		server (a) generates a "session" key; (b) encrypts
 		the message using the session key; (c) encrypts
 		the session key with the client's public key;
 		(d) adds a header containing the encrypted session
 		key to the encrypted message; (e) stores the
 		encrypted message.  The necessary work for
 		the client is obvious.
In each case, one would probably chose some headers to encrypt
separately - e.g., the subject - so that one could more easily pull
them out without decrypting the whole message.
Obviously, approach 2 greatly decreases the time that messages may
hang around unencrypted; but approach 1 can be implemented without
any cooperation from the IMAP provider, which allows it to be rolled
out even for those who use the large providers without having Google
and Hotmail and Yahoo! buy into it.
Does anyone know of existing work in this area?
 							-- Jerry

@_date: 2008-10-28 12:43:11
@_author: Leichter, Jerry 
@_subject: combining entropy 
This isn't enough.  Somehow, you have to state that the values emitted
on demand in any given round i (where a round consists of exactly one
demand on all N member and produces a single output result) cannot
receive any input from any other members.  Otherwise, if N=2 and member
0 produces true random values that member 1 can see before it responds
to the demand it received, then member 1 can cause the final result to
be anything it likes.
This is an attack that must be considered because you already want to
consider the case:
Stating this requirement formally seems to be quite difficult.  You can
easily make it very strong - the members are to be modeled as
probabilistic TM's with no input.  Then, certainly, no one can see
anyone else's value, since they can't see *anything*.  But you really
want to say something along the lines of "no malicious member can see
the value output by any non-malicious member", which gets you into
requiring an explicit failure model - which doesn't fit comfortably with
the underlying problem.
If the issue is how to make sure you get out at least all the randomness
that was there, where the only failures are that some of your sources
become predictable, the XOR is fine.  But once you allow for more
complicated failure/attack modes, it's really not clear what is going on
and what the model should to be.

@_date: 2008-10-28 15:55:50
@_author: Leichter, Jerry 
@_subject: combining entropy 
Rest of example omitted.  I'm not sure of the point.  Yes, there are
plenty of ways for correlation to sneak in.
As far as I can see, only the second piece I quoted is relevant, and it
essentially gets to the point:  The original problem isn't well posed.
It makes no sense *both* to say the sources and trusted *and* to say
that they may not deliver the expected entropy.  If I know the entropy of
all the sources, that inherently includes some notion of trust - call
it source trust:  I can trust them to have at least that much entropy.
I have to have that trust, because there is no way to measure the
(cryptographic) entropy.  (And don't say I can analyze how the source
is constructed, because then I'm left with the need to trust that what
I analyzed is actually still physically there - maybe an attacker has
replaced it!)
Given such sources it's easy to *state* what it would mean for them to
be independent:  Just that if I consider the source produced by
concatenating all the individual sources, its entropy is the sum of the
entropies of the constituents.  Of course, that's an entropy I can again
measure - at least in the limit - in the information theoretical sense,
but not in the cryptographic sense; another aspect of trust - call it
independence trust - has to enter here.
All that's fine, but how then are we supposed to construe a question
about what happens if some of the sources fail to deliver their rated
entropy?  That means that source trust must be discarded.  (Worse, as
the original problem is posed, I must discard source trust for *some
unknown subset of the sources*.)  But given that, why should I assume
that independence trust remains?
Sure, I can make additional assumptions.  If I'm concerned only about,
say, physical failures of sources implemented as well-isolated modules,
it might well be a reasonable thing to do.  In fact, this is essentially
the independent- failure model we use all the time in building reliable
physical systems.  Of course, as we know well, that model is completely
untenable when the concern is hostile attack, not random failure.  What
do you replace it with?
Consider the analogy with reliable distributed systems.  People have
basically only dealt with two models:
The Byzantine model is bizarre sounding, but it's just a way of expressing
a worst-case situation:  Maybe the failed modules act randomly but just by
bad luck they do the worst possible thing.
We're trying to define something different here.  Twenty-odd years ago,
Mike Fischer at Yale proposed some ideas in this direction (where
modules have access to true random numbers and the only thing a failed
module *cannot* do is determine what random values another module drew
unless that module chooses to make them available), but I don't recall
much that came out of this work (not that I specifically tried to keep
track).  That seems related to the underlying problem here.  (If we want
to ignore intelligent attacks, we get something close to the fail-stop
model, where a failed module can deliver anything that depends only on
its internal state and its private random number source.)
Yes.  I'm not sure what the sentence was originally supposed to mean,
but what I ended up saying didn't make a whole load of sense....

@_date: 2008-09-09 14:52:54
@_author: Jerrold Leichter 
@_subject: Bletchley Park restoration 
[Moderator's note: I posted on this earlier, but I really do want to
see Bletchley Park maintained... :) --Perry]
IBM and PGP have donated $100,000 to help restore and maintain  Bletchley Park as a museum.  This money is intended to get others  involved - millions more will be needed.
                                                         -- Jerry

@_date: 2008-09-11 10:52:24
@_author: Leichter, Jerry 
@_subject: street prices for digital goods? 
But this implies there is something very wrong with our current
thinking about attacks.
If, as is commonly assumed, hackers today are in this as a business,
and are driven by then the value of a credit card number is determined
exactly by the most money you can turn it into, by any approach.  If
I have a credit card number, I can turn it into money by selling it,
or alternatively I can buy stuff and sell that instead.
Now, there are costs involved with buying goods, receiving them,
and reselling them; and also there's some probability that the
credit card providers will notice my activity and block my
transactions.  (There's of course also the possibility that I
get caught and sent to jail!)  If the costs of doing this business
are fixed, I can drive them to zero by using enough credit cards,
and there are clearly plenty around - but see below.  So the only
significant issue is variable costs:  For every dollar I charge on
a card, I only get back some fraction of a dollar, based on my per-
transaction costs and the probability of my transaction getting
rejected.  This probability grows with the size of the transaction,
so the actual optimal strategy is complicated.
Still ... if you can *buy* a credit card number for a couple
of cents, its actually *value* can't be much higher.  Which
implies that something in the overall system makes it difficult
to monetize that card.  I'm not sure what all of them are, but
we can guess at some.  The card providers *must* be rather good
at blocking cards fairly quickly - at least when large amounts
of money are involved.  That is:  The probability of being
blocked must go up very rapidly with the size of the transaction,
forcing the optimal transaction size to be small.  If it's
small enough, then fixed costs per transaction become significant.
And something blocks the approach of "do many small transactions
against many cards" - presumably because these have to be done
in the real world, which means you need many people going to many
vendors picking up all kinds of physical objects.
Whatever the causes ... if it's cheap to *buy* credit card
numbers, they must not really be worth all that much!
                                                        -- Jerry

@_date: 2008-09-17 18:39:54
@_author: EMC IMAP 
@_subject: Cookie Monster 
Yet another web attack:
Apparently, this one was found and described over a year ago by Mike  Perry, who decided to release all the details when there was no  significant followup.  (Sidejacking was announced at about the same  time, and people apparently think the two attacks are the same; but  they aren't, and mechanisms to prevent sidejacking generally don't  block Cookie Monster.)
As I understand the attack, it's this:  Cookies can be marked Secure.   A Secure cookie can only be returned over an HTTPS session.  An cookie  not marked Secure can be returned over any session.  So:  If a site  puts security-sensitive data into a non-Secure cookie, an attacker who  can spoof DNS or otherwise grab sessions can send a HTTP page  allegedly from the site that set the cookie asking that it be returned  - and it will be.
It turns out hardly anyone bothers to mark their cookies secure.  In  Firefox, if you list your cookies, you can sort on the Secure field.   I only found a couple of cookies marked - mainly from American  Express, one of the few sites that gets this right.  (Bank of America,  for example, doesn't; Gmail with the new HTTPS-only setting does, but  other Google services don't.)
My own conclusion from this:  This is yet another indication that the  whole browser authentication model is irretrievably broken.  It's just  way too complex, with way too many moving parts which can interact in  dangerous ways.  The list of requirements for a "safe" Web application  - even just based on attacks known today - is so long that no one can  remember them all, much less check any substantial Web application to  see if it follows them.
We need a better approach.
                                                         -- Jerry

@_date: 2008-09-19 10:53:32
@_author: Leichter, Jerry 
@_subject: Cookie Monster 
a)  It depends on who you think it has to be secure against.  Typical
reasoning:  If it's effectively the *client's* information, why/from
whom do I need to protect it while it's on the *client's* machine?
After all, it can only be seen by the client and me.
b)  The way this attack is actually likely to be used is to steal a
"logged-in session".  If I have the cookie, and can MITM the stream
to the server, I can act "within the logged-in session."  I don't
need to be able to decrypt the cookied - the real client has no
need to (but in fact there isn't much point in encrypting, while at
rest, the nonce that identifies the "logged-in session.")
I put "logged-in session" in quotes in agreement with James Donald's
message on this subject.

@_date: 2008-09-19 17:16:26
@_author: Jerry Leichter 
@_subject: Lava lamp random number generator made useful? 
The Lava Lamp Random Number generator (at   generates true random numbers from the images of a couple of lava  lamps.  Of course, as a source of randomness for cryptographic  purposes, it's useless because it's visible to everyone (though I  suppose it might be used for Rabin's beacons).
At ThinkGeek, you can now, for only $6.99, buy yourself a USB-powered  mini lava lamp (see    "All you need" is some way to watch the thing - perhaps a USB camera -  and some software to extract random bits.  (This isn't *really* a lava  lamp - the lamp is filled with a fluid containing many small  reflective plastic chips, lit from below by a small incandescent bulb  which also generates the heat that keeps the fluid circulating.  From  any given vantage point, you get flashes as one of the plastic chips  gets into just the right position to give you a reflected view of the  bulb.  These should be pretty easy to extract, and should be quite   random.  Based on observation, the bit rate won't be very high - a bit  every couple of seconds - though perhaps you can use cameras at a  couple of vantage points.  Still, worth it for the bragging rights.)
An alternative, also at ThinkGeek, is a USB-powered Plasma Ball (at  .  The arc discharges should be even easier to convert into a  bitstream, though it's probably a more biased source than the lava  lamp, so will need more post-processing.
                                                         -- Jerry

@_date: 2008-09-22 10:36:48
@_author: Leichter, Jerry 
@_subject: once more, with feeling. 
The sitation today is (a) the decreasing usefulness of passwords -
those anyone has a chance of remembering are just to guessable in the
face of the kinds of massive intelligent brute force that's possible
today and (b) the inherently insecure password entry mechanisms that
we've trained people to use.  Perhaps the only solution is to attack
both problems at the same time:  Replace passwords with something
else, and use a different, more secure input mechanism at the same
The problem is what that "something else" should be.  Keyfobs with
one-time passwords are a good solution from the pure security point
of view, but (a) people find them annoying; (b) when used with
existing input mechanisms, as they pretty much universally are, are
subject to MITM attacks.  The equivalent technology on a USB plugin
is much easier on the user in some circumstances, but is subject to
some bad semantic attacks, as discussed here previously.  Also, it's
not a great solution for mobile devices.
DoD/government uses smartcards, but that's probably not acceptable to
the broad population.  There's been some playing around with cellphones
playing the role of smartcard, but cellphones are not inherently secure
either.  There's also the related problem of scalability to multiple
providers:  I only need one DoD card, which might be acceptable, but if
every secure web site wants to give me their own, I have a problem.  Of
course, various federated identity standards are already battling it
out, but uptake seems limited.  Besides, that can only be one element of
the solution - if I use a traditional password to get to my federated
identity token, I've made the old problem much worse, not better.
Some laptops and keyboards and even encrypted USB memory sticks are
getting fingerprint scanners as standard hardware.  *If* these
actually work as advertised - not a good bet, based on history so
far - these could be an interesting input mechanism.  Since there
are no expectations today that the fingerprint data will be
available to any web site that asks, one could perhaps establish
a standard for controlling this in an appropriate way, with a
built-in, unforgeable display.  With microphones and, increasingly,
cameras as widely-available components, one might define a similar
special input mode around them and look to voice or face recognition.
Or maybe we could even leverage the increasing interest in special
outside-the-main-OS basic displays one sees on laptops.  (I'm sure it
just thrills Microsoft to see Dell putting a tiny Linux implementation
in each laptop....)
These are all just possibilities, and whether any of them (or some other
approach) actually gains broad acceptance is, of course, totally up in
the air.  Right now, while in the aggregate the problems with ID theft
are bad and getting worse, relatively few individuals feel the pain,
nor is there much in the way to offer them.  Until one or the other
of these changes - and most likely, both - the old "password in some
window or another" model will likely stick around.
