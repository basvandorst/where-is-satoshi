
@_date: 2004-08-15 02:00:31
@_author: james hughes 
@_subject: Al Qaeda crypto reportedly fails the test 
If I understand your question correctly, in 1994 a VPN product was fielded that had this capability. It did not have any capability for static group or tunnel keys. It was only RSA/DH using DH for the tunnel key and RSA only for authentication. The device had "perfect forward secrecy" because the use of RSA disclosed nothing about the tunnel keys, and complete RSA secret disclosure would only divulge that the D-H was authentic. The DH private keys were use once random and the public parameters, well, public. The user could set the tunnel lifetime short or long, their choice.
In this case, the "code clerk" had no direct access to the key material and could not set static keys even if they tried. The box was not tamper resistant, but it was not easy to remove the keys even with physical access.
The device did not have a "group password" (current Cisco IPSEC vulnerability) and used an invitation scheme to bring new nodes in. Link to Cisco notice is here Once the system was fielded, pressure on the systems designer could not change this.
In essence, there was no code clerk. One can argue that the network administrator is the code clerk, but that person could still wire around the VPN device or attach a completely separate backdoor to to cause, as you say, "unbounded damage" in a way that does not compromise the comsec system.
This was one of the original proposals for IPSEC, but was not selected (but that is another story). Subsequent generations of this device are still being built and sold from So, as long as I have understood your question, such systems have existed for some time.

@_date: 2004-08-17 02:37:03
@_author: james hughes 
@_subject: CRYPTO2004 Rump Session Presentations, was Re: A collision in MD5' 
This is Jim Hughes, General Chair of CRYPTO2002. There are three significant Rump session papers on hash collisions that will be presented, including an update on this one (and about 40 other short papers on other aspects of cryptography). As the session firms up, more information it will be posted at
Barring technical or other difficulties, if you want to hear this from the horses mouth, the CRYPTO2004 Rump Session will be webcast at 7pm pacific Tuesday Aug 17 for as long as it takes. You may join us virtually using the following links (depending on the readers).
Internet Explorer
Microsoft media server
The players (for MS and Mac) are available from
I assume MS clients will be able to cope. I know that my MacOSX machine with Windows Media Player can use the mms: link. I welcome feedback from anyone using other readers on other platforms like Linux.
The server is currently up and running and is broadcasting a dark, empty, and silent hall. This should be more interesting after sunup Tuesday Santa Barbara time. You may expect sound near to the start This is our the conferences first webcast, and I hope that it works for you. If there are problems, I will apologize in advance.

@_date: 2004-08-17 16:54:21
@_author: james hughes 
@_subject: CRYPTO2004 Rump Session Presentations, was Re: A collision in MD5' 
I have 2 items of note for this list.
1. The web site is updated with program and the times.
2. I was typing fast, and mistyped my title. I am General Chair this year, not 2002 as was stated.

@_date: 2004-12-16 19:39:14
@_author: james hughes 
@_subject: Fwd: The PoinFULLness of the MD5 'attacks' 
For this discussion, I think we are missing the point here...
1. With a rogue binary distribution with correct hash, this is -at least- a denial of service where the customer will install the rogue binary and it will crash in the area that the information was changed. MD5 based Tripwire will not catch this happening if this is done on the distribution machine.
2. If the rogue binary is a driver that gets into the kernel, it could cause a crash and that crash -could- cause a kernel exploit.
3. A modification to an seldom used section of code that can be invoked in some non-typical way can cause a machine to crash on command.
4. Offline, the attacker could automate a trial and error scheme to create random collisions and testing each until one produces an effect to their advantage and then substitute it for the real one.
Again, anything that gives the legitimate user a feeling of security (because the hashes match) and allows the attacker to do anything to their advantage is a failure of the MD5 algorithm.
Maybe these are low probabilities... Are you willing to step up and say there is nothing that the attacker can ever do using these collisions? I'm not.
My suggestion is that all distributions publish the MD5 and SHA-256 hashes for a while and then drop the MD5 based ones in a year or so.

@_date: 2004-10-04 21:07:15
@_author: james hughes 
@_subject: IBM's original S-Boxes for DES? 
In a personal interview with Walt Tuchman (IBM at the time, worked for StorageTek when I met him, now retired) he described the process for creating the s-boxes. A set of mathematical requirements were created and candidate s-boxes meeting these requirements would be printed out on a regular basis. The process ran over a weekend on a 360/195 and the results were given to the ASIC developers to determine which would result in the smallest ASIC size. One was selected by them. I was told that after the requirements were set, NSA did not have a hand in selecting the final S-Boxes.

@_date: 2004-09-09 09:21:01
@_author: james hughes 
@_subject: references on traffic analysis? 
In looking through my library, I came across two references (I would not say 'must read' though).
"Code Breakers" (David Kahn) has several short real world examples. It is not a treatise per-se, but is interesting.
"The Hut Six Story, Breaking the Enigma Codes" (Gordon Welchman) describes many of the aspects of traffic analysis as a precursor to an actual cryptanalysis attack.
I don't know any study texts on this subject.

@_date: 2004-09-28 13:29:29
@_author: james hughes 
@_subject: Illustrated DES Spreadsheet 
For anyone wondering, or teaching about DES, or interested in perverse uses for spreadsheets, I have created an "Illustrated DES spreadsheet". This is a vanilla spreadsheet (no VB or anything exotic) and should work on many excel and excel workalikes.
Acknowledgements for this work goes to J. Orlin Grabbe for his The DES Algorithm Illustrated which can be found at
I have also stole many tricks including how to do XOR in generic excel which I can not find again to give proper credit. I haven't run this through the FIPS 140-2 level 1 test suite (but this could easily be certified :-)
Anyone interested in making one for MD-5?

@_date: 2005-08-12 15:31:47
@_author: James Hughes 
@_subject: webcast of crypto rumpsession this year? 
At this time I believe the answer is no. I set it up last year and  have not this year. I take it that there is interest?
I will send an email to the group if this changes.

@_date: 2005-08-14 20:42:01
@_author: james hughes 
@_subject: Webcast of crypto rump session this year! 
I now have new and good news.
There _WILL_ be a webcast of this year's Crypto which will commence  at 7pm this Tuesday (Aug 16th).
Please watch  which will be posted as soon as  further information is known!
Please feel free to cross post this message to other cryptography  related lists!

@_date: 2005-08-16 11:34:11
@_author: james hughes 
@_subject: Webcast of crypto rump session this year! 
For those interested, testing will begin at 3:30pm PDT. More  information see
     The program is now available
     Please feel free to forward this to other security list. I believe it  will be an interesting rump session.

@_date: 2005-08-29 10:49:21
@_author: james hughes 
@_subject: Another entry in the internet security hall of shame.... 
In listening to this thread hearing all the hyperbole on both sides,  I would suggest that we may need more fuel to the fire.
There was a rump presentation at the recent Crypto on the use of  "Ceremonies" (which, pardon my misstatement in advance, is claimed to  be computer protocols with the humans included). The presentation  states, "Design a great protocol, prove it secure; add a user, it?s  insecure". This specifically discusses SSL.
The entire rump session is at
    scroll down to
    Ceremonies by Carl Ellison
The presentation and video
        The video is about 50MB.

@_date: 2005-12-15 08:24:23
@_author: james hughes 
@_subject: Looking for fast KASUMI implementation 
Hello list:
I have research project that is looking for a fast -software-  implementation of the KASUMI block cipher.  I have found many papers  on doing this in hardware, but nothing in software. While free is  better (as is beer), I will consider a purchase.
FYI, KASUMI is the cryptographic engine of the 3GPP.

@_date: 2005-02-02 06:19:33
@_author: james hughes 
@_subject: Is 3DES Broken? 
I would also like to reinforce Prof. Bellovin's comment that the 3DES block size is too small.
In bulk storage system encryption, 3DES will require rekey every ~~65GBytes. Most PC's have more than this.
With AES the number is ~250 Exabytes (which is 250 billion gigabytes).

@_date: 2005-02-04 10:09:20
@_author: james hughes 
@_subject: Is 3DES Broken? 
I don't want to take this down a rat-hole, but I respectfully disagree. The small block size of 3DES is also an issue with "more secure modes".
CCM states that only 128 but ciphers are to be used. The NIST document states "For CCM, the block size of the block cipher algorithm shall be 128 bits; currently, the AES algorithm is the only approved block cipher algorithm with this block size."
Ferguson points out that in OCB there is a birthday at the number of packets. From the paper, "Collision attacks are much easier when 64-bit block ciphers are used. Therefore, we most strongly advise never to use OCB with a 64-bit block cipher."
These basis of this is that the mode loses packet security at a birthday of the number of blocks. In communications, this is 2^32 blocks, and if we assume 1k blocks, this is 4TBytes, which occurs after transferring less than 2 full DVDs. As network performance grows, this will be a very common transfer size.
While 3DES is not "broken", it is my opinion that the 64 bit blocksize of 3DES is not adequate for today's requirements. In this sense, it is not broken, but obsolete.

@_date: 2005-02-08 19:28:36
@_author: james hughes 
@_subject: link-layer encryptors for Ethernet? 
The following device is a layer 2 tunneling device that has 256 bit AES at up to 400Mb/s.
Hope this helps

@_date: 2005-02-09 08:16:18
@_author: james hughes 
@_subject: link-layer encryptors for Ethernet? 
This is not _just_ an IPsec box. They also have a protocol that  predates IPsec. This was presented to the "then forming" IPsec group in  1994 but it was passed over.
 From the admin guide at:
I believe they do use protocol 50 (even though they are not IPSEC).

@_date: 2005-07-08 18:07:25
@_author: james hughes 
@_subject: SISW05, the 3rd International IEEE Security in Storage Workshop 
3rd International IEEE Security in Storage Workshop
December 13, 2005
Golden Gate Holiday Inn, San Francisco, California USA
Sponsored by the IEEE Computer Society
  Task Force on Information Assurance (TFIA)
  Part of the IEEE Information Assurance Activities (IEEEIA)
Held In Cooperation and Co-Located With the
4th USENIX Conference on File and Storage Technologies (FAST05)
  December 14-16, 2005, San Francisco, CA, USA
In Cooperation with the
  IEEE Mass Storage Systems Technical Committee (MSSTC)
Meeting the challenge to protect stored information critical to  individuals, corporations, and governments is made more difficult by  the continually changing uses of storage and the exposure of storage  media to adverse conditions.
Example uses include employment of large shared storage systems for  cost reduction and, for convenience, wide use of transiently- connected storage devices offering significant capacities and  manifested in many forms, often embedded in mobile devices.
Protecting intellectual property, privacy, health records, and  military secrets when media or devices are lost, stolen, or captured  is critical to information owners.
A comprehensive, systems approach to storage security is required for  the activities that rely on storage technology to remain or become  This workshop serves as an open forum to discuss storage threats,  technologies, methodologies and deployment.
The workshop seeks submissions from academia and industry presenting  novel research on all theoretical and practical aspects of designing,  building and managing secure storage systems; possible topics  include, but are not limited to the following:
- Cryptographic Algorithms for Storage
- Cryptanalysis of Systems and Protocols
- Key Management for Sector and File based Storage Systems
- Balancing Usability, Performance and Security concerns
- Unintended Data Recovery
- Attacks on Storage Area Networks and Storage
- Insider Attack Countermeasures
- Security for Mobile Storage
- Defining and Defending Trust Boundaries in Storage
- Relating Storage Security to Network Security
- Database Encryption
- Search on Encrypted Information
The goal of the workshop is to disseminate new research, and to bring  together researchers and practitioners from both governmental and  civilian areas. Accepted papers will be published by the IEEE  Computer Society Press in the workshop proceedings and become part of  the IEEE Digital Library.
Workshop Sponsor
- Jack Cole (US Army Research Laboratory, USA)
Program Chair
- James Hughes (StorageTek, USA)
Program Committee
- Don Beaver (USA)
- John Black (University of Colorado, USA)
- Randal Burns (Johns Hopkins University, USA)
- Ronald Dodge (United States Military Academy, USA)
- Kevin Fu (University of Massachusetts Amherst, USA)
- Russ Housley (Vigil Security, USA)
- Yongdae Kim (University of Minnesota, USA)
- Ben Kobler (NASA, USA)
- Noboru Kunihiro (University of Electro-Communications, Japan)
- Arjen Lenstra (Lucent Technologies' Bell Laboratories and
      Technische Universiteit Eindhoven, Netherlands)
- Fabio Maino (Cisco Systems, USA)
- Ethan Miller (University of California, Santa Cruz, USA)
- Reagan Moore (University of California, San Diego, USA)
- Dalit Naor (IBM Haifa, Israel)
- Andrew Odlyzko (University of Minnesota, USA)
- Rod Van Meter (Keio University, Japan)
- Tom Shrimpton (Portland State, USA)
- John Viega (Secure Software, USA)
- Erez Zadok (Stony Brook University, USA)
- Yuliang Zheng (University of North Carolina, USA)
Papers must begin with the title, authors, affiliations, a short  abstract, a list of key words, and an introduction. The introduction  should summarize the contributions of the paper at a level  appropriate for a non-specialist reader. Papers must be submitted in  PDF format less than 4MB in size (final paper has no limit). Email  submissions must attach the paper, specify if this is a duplicate  work, and be sent to James_Hughes at StorageTek.com
Papers should be at most 12 pages in length including the  bibliography, figures, and appendices (using 10pt body text and two- column layout). Authors are responsible for obtaining appropriate  clearances. Authors of accepted papers will be asked to sign IEEE  copyright release forms. Final submissions must be in camera-ready  PostScript or PDF. Authors of accepted papers must guarantee that  their paper will be presented at the conference.
Papers that duplicate work that any of the authors have or will  publish elsewhere are acceptable for presentation at the workshop.  However, only original papers will be considered for publication in  the proceedings.
Although full papers are preferred, submissions of extended abstracts  describing the final paper will be considered based on merit and  assessing the author's ability to complete the paper within the  allotted time.
Important Dates
Paper due: September 1, 2005
Notification of acceptance: October 1, 2005
Workshop paper due: December 1, 2005
Workshop: December 13, 2005
Proceedings paper due: December 20, 2005
Questions should be sent electronically to James_Hughes at StorageTek.com
The Call For Papers is also available on the web at

@_date: 2005-06-07 21:02:21
@_author: james hughes 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
There are large institution with 1000s of tape drives and 1,000,000  or more cartridges. Even simple solutions are huge to implement. This  is a non-trivial matter. The technical solutions are possible, there  are vendors out there that are already doing this. Getting from here  it there, even if the solutions were available for free is still a  very expensive challenge.
One proof point is that the standards are not in place for this yet.
Bottom line, this issue is here to stay and will take years to solve.  It is not a cryptographic problem, it's harder than that.

@_date: 2005-03-06 14:30:24
@_author: james hughes 
@_subject: [IP] One cryptographer's perspective on the SHA-1 result 
On this we respectfuly disagree.
You make it sound trivial. Wang has been working on these results for over 10 years. She received the largest applause at Crypto 2004 session from her peers I have ever seen.
on this I agree.
My recommendation for anyone that listens to (nobody) me is to abandon the MD series and SHA algorithms below SHA-256 for everything including certificates, pgp and even HMAC. But these are my inclinations. I would rather migrate to stronger crypto than have to continually justify why I continue to use algorithms that have known weaknesses.
What software do you use for this? Is it ECC or RSA?

@_date: 2006-08-10 17:06:54
@_author: james hughes 
@_subject: compressing randomly-generated numbers 
This is neither correct nor a good idea.
Taking almost random information and compressing it will lead to less  random results.
Specifically, I will give the general LZW case and then go to the BZ2  1) For LZW (even ignoring the magic numbers), if a byte does not  match anything in the dictionary (it starts with a dictionary of all  0s, so the probability of a match on the first byte is low) then that  byte will get a header of a 0 bit. That byte now becomes 9 bits. The  next byte will have a dictionary of the previous byte and all zeros.  The chance of a match will still be small and putting that into the  dictionary will be a 9 bit field with 0s. So in the first 2 bytes, I  can almost guarantee I know where 2 zero bits are.
2) BZ2 transforms the data and then uses LZW. See 1)....
The correct way to "improve" almost random data is to process it with  a hash like function with compression.

@_date: 2006-08-21 15:06:38
@_author: james hughes 
@_subject: Crypto rump session to be webcast.  
The Rump session of this year's Crypto conference will be webcast  Aug. 22 (tomorrow) starting at 19:30 pacific. Other timezones here:
and the webcast will be broadcast in Quicktime and will be available  For more information (including the agenda to be posted) will be  available here:
Please feel free to forward this message to other lists.

@_date: 2006-12-21 14:53:46
@_author: james hughes 
@_subject: news story - Jailed ID thieves thwart cops with crypto 
What algorithm was that? Seems like a really small time, especially  if you have a 4000 or larger CPU cluster...

@_date: 2007-01-22 11:10:46
@_author: james hughes 
@_subject: It's a Presidential Mandate, Feds use it. How come you are  not using FDE? 
I always find these arguments particularly frustrating.
By slowly raising the bar for the lower-skilled criminals, you get  the effect in Steven's firewall book cover (I forget the version,  where you must be a certain height to attack the castle.)
For me, the bottom line is that if you protect against the former,  then you get the latter, and it is only a small matter of time when  the lower-skilled people will get a script to do the higher quality  attacks. Remember WEP?
I really have to question continuing a snail's pace information  protection arms war when we have all the tools we need to properly  defend ourselves.

@_date: 2007-01-22 11:40:27
@_author: james hughes 
@_subject: It's a Presidential Mandate, Feds use it. How come you are not using FDE? 
30 seconds ago.
What mode is it using? How much information is encrypted under a  single key. Was the implementation FIPS certified. And the list goes on.
These are important issues.

@_date: 2007-01-22 10:56:06
@_author: james hughes 
@_subject: analysis and implementation of LRW 
The IEEE P1619 standard group has dropped LRW mode. It has a  vulnerability that that are collisions that will divulge the mixing  key which will reduce the mode to ECB.
There are new mode, XTS-AES being drafted. At this time no one has  claimed that XTS-AES is patented encumbered. There is a reference  implementation of XES-AES by Brian Gladman (although he calls it XEX).
Additionally, there are three modes for wide block encryption  (treating an entire sector as a single permutation) called
at this time no one has claimed that TET is patented encumbered.
More information about this work group, and their email archive can  be found at
Standard caveat applies to implementing non-ratified standards that  things will change.

@_date: 2007-05-31 16:19:26
@_author: James Hughes 
@_subject: Security In Storage Workshop- Extended Deadline 
============================== START ==============================
Call for papers, submission deadline now June 15th.
The 4th International Security In Storage Workshop will be held  September 27, 2007 (Thursday) at Paradise Point Resort and Spa in San  Diego, California, USA. The workshop is co-located with the 24th IEEE  Conference on Mass Storage Systems and Technologies (MSST2007).
Papers related to securing storage and circumventing secure storage  are welcome. This includes databases, file systems, removable media,  embedded systems, cell phones, etc.
For more information, and submissions, see
and the call for papers is at
Please feel free to forward this email to other lists.
Jim Hughes
Program Chair
SISW 2007

@_date: 2007-10-08 19:42:53
@_author: james hughes 
@_subject: Full Disk Encryption solutions selected for US Government use 
Out of curiousity, Vista (BitLocker) was not mentioned?

@_date: 2007-10-11 22:19:18
@_author: james hughes 
@_subject: Password hashing  
A proposal for a new password hashing based on SHA-256 or SHA-512 has  been proposed by RedHat but to my knowledge has not had any rigorous  analysis. The motivation for this is to replace MD-5 based password  hashing at banks where MD-5 is on the list of "do not use" algorithms.  I would prefer not to have the discussion "MD-5 is good enough for  this algorithm" since it is not an argument that the customers  requesting these changes are going to accept.

@_date: 2007-09-02 21:10:14
@_author: james hughes 
@_subject: debunking snake oil 
I am all for humor... Can you give us a hand with how to find this

@_date: 2008-04-21 18:04:29
@_author: james hughes 
@_subject: Snake oil crypto of the day: BabelSecure Samurai 
The company and all it's assets are for sale. Starting price $20M.

@_date: 2008-08-16 17:04:14
@_author: james hughes 
@_subject: Crypto2008 rump session to be webcast 
The Crypto2008 rump session will be webcast live starting at 19:30 on  August 19th. The details will be posted on the iacr website at
Local times are
PS. Please feel free to forward this as widely as possible.

@_date: 2008-05-14 09:42:26
@_author: james hughes 
@_subject: Call for papers for the Security in Storage Workshop 2008, due May 30th 
The 5th international Security in Storage Workshop (SISW)
will be held on Sept 25th, 2008 in conjunction with MSST 2008
  and the 	Key Management Summit 2008.
Prospective participants should submit either a full paper (not to  exceed 12 single-spaced pages) for a paper presentation to be  published in the proceedings or can submit a short abstract suggesting  alternative presentation forms, discussion items, or panel topics.  Please submit papers to James Hughes (James.Hughes at Sun.Com) by May 30th.
Jim Hughes

@_date: 2008-05-19 09:26:03
@_author: james hughes 
@_subject: Call for presentations: Cryptographic e-voting systems for the IACR 
The International Association for Cryptologic Research ( ) is seeking presentations and demos of e-voting systems. For its next  meeting in August-17, 2008 (in Santa-Barbara, CA, USA), the IACR board  would like to invite presentations and demos of cryptographic e-voting  systems that are open source and freely available for all.
For more information see

@_date: 2009-05-01 07:53:34
@_author: james hughes 
@_subject: Destroying confidential information from database 
If the material is that sensitive, and you only want to selectively  delete the information, the only way is to:
1) delete the information from the database using the commercial means,
2) export the database
3) Inspect the exported data to ensure all the sensitive information  is deleted
4) import the database to another storage system.
5) destroy (degauss, wipe) the original storage system.
6) the truly paranoid would destroy the raid controllers also (since  it contains NVRAM)
Not trivial...

@_date: 2009-08-02 23:29:25
@_author: james hughes 
@_subject: Unattended reboots 
I penned a recent blog about this fact at
It discusses this fact and how it can be mitigated. Specifically, how  wrapped keys can be escrowed, and used to boot a machine in, what I  consider, a significantly more secure manner. Given that you can never  guarantee a cloud provider can not tamper with you machine while  running, this post describes the problem, a set of goals and one  possible solution.
Encrypted Kernels are requirement. Geoff Arnold
suggested that an AMI that can boot an encrypted AMI may solve the  issue. A harder, but possible solution would be to change the AMI's  Grub loader without changing AWS's infrastructure. Anyone interested  on working on a prototype :-)

@_date: 2009-08-06 16:08:16
@_author: james hughes 
@_subject: cleversafe says: 3 Reasons Why Encryption is Overrated 
Until you reach the threshold, you do not have the information to  attack. It becomes information theoretic secure.
They are correct, if you lose a "slice, or two, or three" that's fine,  but once you have the threshold number, then you have it all. This  means that you must still defend the site from attackers, protect your  media from loss, ensure your admins are trusted. As such, you have  accomplished nothing to make the management of the data easier.
Assume your threshold is 5. You lost 5 disks... Whose information was  lost? Anyone? Do you know? What if the 5 drives were lost over 5  years, what then? CleverSafe can not provide any security guarantees  unless these questions can be answered. Without answers, CleverSafe is  neither Clever nor Safe.

@_date: 2009-08-14 11:56:37
@_author: james hughes 
@_subject: Crypto'09 Rump session to be webcast 
The first Crypto rump session took place in 1981 and was immediately  heralded as the most important meeting in cryptography. Each  subsequent Crypto rump session has reached a new level of historical  significance, outstripped only by the Crypto rump sessions that  followed it.
The Crypto2009[1] Rump Session[2] will be broadcast live[3] starting  at Tuesday, August 18th at 7:30pm to 11pm (PST/UTC-7)[4]. Download  calendar event (ics)[5]
[1] [2] [3] [4] [5]

@_date: 2009-08-18 17:47:06
@_author: james hughes 
@_subject: Crypto'09 Rump session to be webcast 
This year's rump session will include
The full agenda has been posted at

@_date: 2009-08-19 19:10:22
@_author: james hughes 
@_subject: Certainty 
Caution, the following contains a rant.
This is being done. What Perry said.
There was an invited talk at Crypto about "Alice and Bob Go To  Washington: A Cryptographic Theory of Politics and Policy". This was  interesting in that it explained that facts are not what politicians  and that politicians form blocks to create shared power.
It seems that your comment about "certainty" is not a technical one,  but a political one. The block of people that have implemented MD-5  believe that this algorithm is good enough and that the facts that the  hash function contains no science of how it works, can not be proven  to be resistant to pre-image, nor even reduced to any known hard  problem, are not "certain". Maybe this particular block just wants it  to be secure? If MD-5 is secure to pre-image attacks, the  cryptographic community does not know why. It seems that the only  proof that can be accepted as "certainty" is an existence proof that  the bad deed _has_ be done.
Maybe this is not really an MD-5 block, but an HMAC implementer's  block. This block does have some results to hang their hats on. The  paper "New Proofs for NMAC and HMAC: Security without Collision- Resistance" was publushed in 2006
that states that as long as the "compression function is a PRF" HMAC  is secure. This is mostly because the algorithm is keyed. This places  HMAC into the class of ciphers as PRF and out of the class of hash  I find this "interesting". Cryptographers knew in 2004 that the wheels  just came off MD-5, and it's future was going to be grim. The "common  sense" was that a collision by itself was not relevant. Then there was  the ?Colliding X.509 Certi?cates?
and still the "common sense" was that it could still be used. So then  there was "Chosen-Pre?x Collisions for MD5 and Colliding X.509  Certi?cates for Di?erent Identities"
but that was still not enough. This Crypto, the paper "Short Chosen- Prefix Collisions for MD5 and the Creation of a Rogue CA Certificate"
seems to have put a nail in this issue, but not the issue of the  "certainty" of pre-image attacks.
Some believe that the Best Paper award was given for the persistence  that the authors showed to continue to spend time and effort on what  the cryptographic community knows is an cart with no wheels on it to  counter the "common sense" implementing block that do not believe it  until they see it.
Effort placed on replacing MD-5 is more important now than taunting  the cryptographers to prove that MD-5 pre-images are feasible when  there is literally nothing proving that pre-images of MD-5 are  difficult. (Again, this is for bare MD-5, not HMAC.)
I am curious if you mean Immediately as in now, or immediately when a  pre-image attack is found?

@_date: 2009-02-17 07:27:27
@_author: James Hughes 
@_subject: Crypto Craft Knowledge 
[snip specific discussion]
I find this conversation off the point. Consider other trades like  woodworking. There is no FAQ that can be created that would be  applicable to building a picture frame, dining room table or a covered  bridge. A FAQ for creating a picture frame would be possible, but this  is not the FAQ that is being discussed.
Crypto protocol failures are not trivial. The recent CBC attack on SSH  shows that this is the case.
What FAQ would prescribe how not to make this mistake?
There are PhD programs related to this subject. I would argue (and  actually dovetailing with another thread) that, if one creates a FAQ,  that it point to well vetted implementations of information delivery  protocols like SSL and SSH, and that any FAQ regarding the use of  crypto libraries be that this is dangerous and should only be  attempted with proper oversight and/or training.
Crypto protocols are not trivial, and suggesting a FAQ would be able  to take the uninitiated to secure coding is more dangerous than good.

@_date: 2009-02-24 10:53:31
@_author: james hughes 
@_subject: SHA-3 Round 1: Buffer Overflows 
Two aspects of this conversation.
1) This algorithm is designed to be parallelized. This is a  significant feat. C is a language where parallelization is possible,  but fraught with peril. We have to look past the buffer overflow to  the motivation of the complexity.
2) This algorithm -can- be implemented with a small footprint -if-  parallelization is not intended. If this algorithm could not be  minimized then this would be a significant issue, but this is not the  I would love this algorithm to be implemented in an implicitly  parallel language like Fortress.

@_date: 2009-02-25 00:19:50
@_author: james hughes 
@_subject: Fwd: SMS 4 algorithm implemented as a spreasheet. 
Building a reference implementation of a cipher  can be an invaluable  aid to writing code. Building a cipher in a spreadsheet, while some  may suggest is strange, is a valid way to effectively describe a  cipher in a visual sense. This has been done before with The  Illustrated DES Spreadsheet, it has been done again. With the help of  a Chinese document and an english translation by Whit?eld Di?e and  George Ledin, I was able to create a spreadsheet that demonstrates the  SMS4 algorithm.
This algorithm is rather infamous for being proposed as a WiFi  standard. It is has raised it?s head again as being implemented  inside a derivative of the TCG standard TPM module for China under the  name TCM module.
The SMS4 spreadsheet is available in Mac OS X numbers format and .xls  format at

@_date: 2009-07-17 08:51:09
@_author: james hughes 
@_subject: 112-bit prime ECDLP solved 
"Prediction is very difficult, especially about the future".
I have researched the possibility of 50 or 100 year key sizes. All we  have to do is look back 50 years to the (unbreakable) Enigma, and 30  years to the famous Sci.Am article by Rivest that said it would take  40 quadrillion years to break the challenge, which actually took 25,  or more recently, or FEAL, or RC-4 (WEP), or MD-5, or SHA-1, or, or  need I say more?
If we assume that all knowledge to be discovered has been discovered,  and all mathematical insight humanity is capable of has been achieved,  you are correct that 144 bit EC keys are "good all the way to the  singularity" (which actually depends on the Hubble constant, but I  digress) and that "everything that could be invented has been invented."
I believe it is folly to suggest that 144 bit keys will never be  broken. Frankly, I hope to see the day.

@_date: 2009-07-24 07:33:58
@_author: james hughes 
@_subject: Fast MAC algorithms? 
Note for Moderator. This is not crypto but TOE being the solution to  networking performance problems is a perception that is dangerous to  leave in the crypto community.
IPSEC offload can have value. TOE are far more controversial.
TOEs that are implemented in a slow processor in a NIC card have been  shown many times to be ineffective compared to keeping TCP in the  fastest CPU (where it is now). For vendors that can't optimize their  TCP implementation (because it is just too complicated for then?) TOE  is a siren call that detracts them from their real problem. Look at  Van Jacobson post of May 2000 entitled "TCP in 30 instructions".
There was a paper about this, but I am at a loss to find it. One can  go even farther back to "An Analysis of TCP Processing Overhead",   Clark, Jacobson, Romkey and Salwen in 1989 which states "The protocol  itself is a small fraction of the problem".
Back to crypto please.

@_date: 2009-07-24 16:21:33
@_author: james hughes 
@_subject: Fast MAC algorithms? 
As long as we think this is interesting, (although I respectfully  disagree that there are any inherent security problems with TOE. Maybe  there are insecure implementation...).
There were a dozen or so protocol offload research projects that the  US government funded in the 90s. All failed. Is the people who say  "TOE's are bad"  because of zealotry or standing on the shoulders of  the people that ran those projects. At Network Systems, we partnered  with HT Kung of CMU at the time to move TCP out of a really slow  Decstation. Result? A accelerator that cost as much as the workstation  that was faster until the next processor version was available. Yes,  we could have reduced it to a chip but it wasn't. The take away was  that improving the software is the gift that keeps on giving. Moore's  law means you get a faster TCP every time the clock ticks.
BTW, I am not a Linux bigot, just someone that got caught up in this  issue more than a decade ago. I do not agree with your assertion or  the Wikipedia page that this is linux bigotry. I find that page  horribly inaccurate and self serving to the TOE manufacturing community.
What I learned from participating in a project that spent $5M of tax  payer money was that "The protocol itself is a small fraction of the  Offloading features like checksumming, fragmentation/reassembly (aka  Large Segment Offload), packet categorization, slitting flows to  different threads, etc. is not TOE.
TOE is offloading of the TCP stack. The thin line that is crossed is  "where is the TCP state kept". If the state is kept in the card, then  the protocol to get the data reliably to the application is has more  corner cases (hence complexity) since the IP layer can be lossy and  the socket layer can not. In all the research, this has always been  the case.
If there is something windows has not learned could be that processing  TCP should be simple and quick. Since the source code is not  available, I don't know if their software falls into the "too  complicated" camp or not... In the case of Chimney partial stack  offload, the state is in both places. Sounds simple straight forward,  The case of iSCSI where a complete protocol conversion is done (the  card looks like a SCSI card, but the data goes out over TCP/IP) it is  a different story (which is also arguably still about solving the OS  vendor's lack of software agility with hardware), but that is not the  intent of this discussion.
I fully agree that offloading features that makes the TCP processing  easier is a good thing.
Back to crypto?

@_date: 2009-07-26 12:11:02
@_author: james hughes 
@_subject: cleversafe says: 3 Reasons Why Encryption is Overrated 
....and probably patent pending regardless of there being significant  amounts of prior art for their work. One reference is ?POTSHARDS:  Secure Long-Term Storage Without Encryption? by Storer, Greenan, and  Miller at  Maybe  they did include this in their application. I certainly do not know.  They seem to have one patent
and 7 pending.
The trick is cute, but I argue largely irrelevant. Follows is a  response to this web page that can probably be broadened to be a  criticism of any system that claims security and also claims that key  management of some sort is not a necessary evil.
I agree with many of your points. I would like to make a few of my own.
1) If you are already paying the large penalty to Reed Solomon the  encrypted data, the cost of your secret sharing scheme is a small  additional cost to bear, agreed. Using the hash to ?prove? you have  all the pieces is cute and does turn Reed Solomon into an AONT. I will  argue that if you were to do a Blakley key split of a random key, and  append each portion to each portion of the encrypted file you would  get similar performance results. I will give you that your scheme is  simpler to describe.
2) In my opinion, key management is more about process than  cryptography. The whole premise of Shamir and Blakley is that each  share is independently managed. In your case, they are not. All of the  pieces are managed by the same people, possibly in the same data  center, etc. Because of this, some could argue that the encryption has  little value, not because it is bad crypto, but because the shares are  not independently controlled. I agree that if someone steals one  piece, they have nothing. They will argue, that if someone can steal  one piece, it is feasible to steal the rest.
3) Unless broken drives are degaussed, if they are discarded, they can  be considered lost. Because of this, there will be drive loss all the  time (3% per year according to several papers). As long as all N  pieces are not on the same media, you can actually lose the media, no  problem. This can be expanded to a loss of a server, raid controllers,  NAS box, etc. without problem as long as there is only N-1 pieces, no  problem. What if you lose N chunks (drives, systems, etc.) over time,  are you sure you have not lost control of someone?s data? Have you  tracked what was on each and every lost drive? What is your process  when you do a technology refresh and retire a complete configuration?  If media destruction is still necessary, will resulting operational  process really any easier or safer than if the data were just split?
4) What do you do if you believe your system has been compromised by a  hacker? Could they have read N pieces? Could they have erased the logs?
5) I also suggest that there is other prior art out there for this  kind of storage system. I suggest the paper ?POTSHARDS: Secure Long- Term Storage Without Encryption? by Storer, Greenan, and Miller at    because it covers the same space, and has a good set of references  to other systems.
My final comment is that you raised the bar, yes. I will argue that  you did not make the case that key management is not needed. Secrets  are still needed to get past the residual problems described in these  comments. Keys are small secrets that can be secured at lower cost  that securing the entire bulk of the data. Your system requires the  bulk of the data to to be protected, and thus in the long run, does  not offer operational efficiency that simple bulk encryption with a  traditional key management provides.

@_date: 2009-07-27 08:47:53
@_author: james hughes 
@_subject: Fast MAC algorithms? 
RC-4 is broken when used as intended. The output has a statistical  bias and can be distinguished.
and there is exceptional bias in the second byte
The latter is the basis for breaking WEP
These are not attacks on a reduced algorithm, it is on the full  If you take these into consideration, can it be used "correctly"? I  guess tossing the first few words gets rid of the exceptional bias,  and maybe change the key often to get rid of the statistical bias? Is  this what you mean by used "correctly"?

@_date: 2009-06-12 10:12:20
@_author: james hughes 
@_subject: Seagate announces hardware FDE for laptop and desktop machines 
On Jun 10, 2009, at 4:19 PM, travis+ml-cryptography at subspacefield.org  All of these statements may be true. The standardization of the  command set for encrypting disk drive does has a "set master key"  command. If this command does exist, and if the user had software that  resets this master password, then the backdoor would have been closed.  (I know, there area  lot of "ifs" in that sentence.)
and from universities you can access

@_date: 2009-05-22 08:45:15
@_author: james hughes 
@_subject: Warning!  New cryptographic modes! 
I believe that mode has been renamed EME2 because people were having a  fit over the *.

@_date: 2009-11-12 22:32:12
@_author: james hughes 
@_subject: hedging our bets -- in case SHA-256 turns out to be insecure 
I agree.
The logic of a "unknown flaw" being fixed flies in the face of prudent cryptanalysis. If you don't know the flaw, how can do you know you can or have fixed it. What if there is an unknown flaw in the fix? Wrap that again? Turtles all the way down. Putting multiple insecure algorithms together does guarantee a secure one.
The only solution that works is a new hash algorithm that is secure against this (and all other) vulnerabilities. It may include SHA 256 as a primitive, but a true fix is fundamentally a new hash algorithm. This process is being worked on by a large number of smart people. I can guarantee you that this kind of construction has been looked at. It is my opinion that putting a bandaid around SHA 256 "just in case" is not cryptanalysis, it's marketing.
P.S. once Sha-3 comes out, your bandaid will look silly.

@_date: 2009-11-18 23:33:58
@_author: james hughes 
@_subject: hedging our bets -- in case SHA-256 turns out to be insecure 
I guess I need a slight correction... I missed a 'not'. Putting multiple insecure algorithms together does NOT guarantee a secure one.

@_date: 2009-09-24 18:22:44
@_author: james hughes 
@_subject: FileVault on other than home directories on MacOS? 
Unauthenticated CBC is indeed a problem
I don't think that Jacob Appelbaum or Ralf-Philipp Weinmann work for  the NSA (but having "crypto.nsa.org" is cool :-)
Technically, you do not get integrity. With XTS (P1619, narrow block  tweaked cipher) you are not notified of data integrity failures, but  these data integrity failures have a much reduced usability than CBC.  With XTS:
1) You can return 16 byte chunks to previous values (ciphertext  replay) as long as it is to the same place (offset) as it was before.
2) If you change a bit, you will randomize a 16 byte chunk of  With the P1619.2 mode, I believe, is called TET (IEEE 1619.2, wide  block tweaked cipher) there are different characteristics. Usually the  wide block is a sector so it can be 512 or some other value. In this  case, you do not get complete integrity either. In this case
1) You can return a sector to a previous value (sector reply) as long  as it is to the same place (offset) as it was before.
2) If you change a bit, you will randomize a complete sector of  If you change this to ZFS Crypto
You get complete integrity detection with the only remaining  vulnerability that
1) you can return the entire disk to a previous state.
While I may have put you all asleep, the basic premise holds... XTS is  better than unauthenticated CBC.

@_date: 2010-07-14 10:33:59
@_author: james hughes 
@_subject: Encryption and authentication modes 
If there is no room for or an integrity field, you can look at XTS-AES.
CCM is a "counter mode cipher", so don't reuse the count (with any reasonable probability).

@_date: 2010-03-25 10:23:00
@_author: james hughes 
@_subject: "[Not] Against Rekeying" 
The paper on Cebolla[4] states that "Trust in symmetric keys diminishes the longer they are used in the wild. Key rotation, or re-keying, must be done at regular interfals to lessen the success attackers can have at crypt-anayzing the keys". This is exactly the kind of justification that the Dkr post and most of the comments agree is flawed.
It goes on to state what was said about new keys being derived from old keys.
[4] Hmm. Interesting. Learn one key, have them all for the future. Wow. Yes, that is Ross' definition of backward security, and clearly does not meet Ross' definition of forward security. In reading the paper, it seems like this system is: Crack one key, you're in forever. A government's dream for an anonymity service. Ross' definitions for, backwards, forwards makes sense from a terminology point of view, but IMHO without both, it is not secure.
Sure one can talk about attack scenarios, and that just proves the tautology that we don't know what we don't know (or don't know what has not been invented yet). There is no excuse to bad crypto hygiene. I don't know why someone would build a system with K_i+1 = h(K_i) when there are so many good algorithms out there.
I agree with the Ekr posting, but not the characterization above. The Ekr posting says "[rekey for] Damage limitation If a key is disclosed" and "This isn't totally crazy". The statement of fact is that if a key is compromised, a rekey limits the scope of the compromise.
The Ekr posting said nothing about how the key was disclosed. Yes, if you have root on the machine an have mounted an active attack, all bets are off, but there are other ways for key disclosure to happen (as was discussed in the Ekr posting).
For example a cold boot attack [3] can be used to recover a communications session key (instead of a disk key). If that key has been used for a particularly long time, and if one assumed that the attacker had the opportunity to  record all the ciphertext, then one must expect that all of that information can now be read.
[3] [The comments about breaking keys is deleted. I agree with the original posting and everyone else that changing a key OF A MODERN CIPHER to eliminate algorithm weaknesses is not a valuable reason to rekey.]
I agree (but as a nit, we can reverse the order. Create a completely new session and just move the traffic to the new connection.)
Limiting the scope of a key compromise is the only justification I can see for rekey. That said, limiting the scope of the information available because of a key compromise is still a very important consideration.

@_date: 2013-12-19 11:56:57
@_author: james hughes 
@_subject: [Cryptography] Chinese Cryptography 
No, it?s not "all stolen from America", they have universities, Chinese crypto conferences, their own crypto standards. Quite a long history independent of the US. Cryptography is quite an open subject. There are quite a few good conferences where these things are shared publicly and the Chinese do attend. Cryptographers of Chinese origin have made significant advances in western Cryptography. One algorithm that I know of, SMS4 has had public cryptanalysis. They offered it for a standard and was rejected even there has been no flaw found. Every country that has the means, has their own cryptography and cryptanalysis capability.

@_date: 2013-11-10 11:37:19
@_author: james hughes 
@_subject: [Cryptography] NIST Randomness Beacon 
Not enough entropy, Two possibilities? Take the closing of the entire DJI (the list, not the average) in some canonical order on a certain day, the result will both have enough entropy and certainly be far harder to ?influence?. The chain that you get back from Guardtime is interesting in that it is influenced by many around the world (potentially including the requester) -and- is backwards verifiable. (I am not affiliated with Guardtime, but find it an interesting technology.)

@_date: 2013-10-04 23:40:59
@_author: james hughes 
@_subject: [Cryptography] Sha3 
I agree: Sponge Good, Merkle?Damg?rd Bad. Simple enough. I believe this thread is not about the choice of Keccak for SHA3, it is about NIST's changes of Keccak for SHA3. [Instead of pontificating at length based on conjecture and conspiracy theories and smearing reputations based on nothing other than hot air] Someone on this list must know the authors of Keccak. Why not ask them. They are the ones that know the most about the algorithm, why the parameters are what they are and what the changes mean for their vision. Here is my question for them: "Given the light of the current situation, what is your current opinion of NIST's changes of Keccak as you specified it to SHA-3 as NIST standardized it?" If the Keccak authors are OK with the changes, who are we to argue about these chances? If the Keccak authors don't like the changes, given the situation NIST is in, I bet NIST will have no recourse but to re-open the SHA3 discussion.

@_date: 2013-10-05 12:18:28
@_author: james hughes 
@_subject: [Cryptography] Sha3 
From the authors: "NIST's current proposal for SHA-3 is a subset of the Keccak family", "one can generate the test vectors for that proposal using the Kecca kreference code." and this "shows that the [SHA-3] cannot contain internal changes to the algorithm."
The process of setting the parameters is an important step in standardization. NIST has done this and the authors state that this has not crippled the algorithm. I bet this revelation does not make it to Slashdot? Can we put this to bed now?

@_date: 2013-10-05 12:47:10
@_author: james hughes 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
In the case we are now, I don't think that it is actually "crypto failures" (RSA is still secure, but 1024 bit is not. 2048 DHE is still secure, but no one uses it, AES is secure, but not with an insecure key exchange) but standards failures. These protocol and/or implementation failures are either because the standards committee said to the cryptographers "prove it" (the case of WEP) and even when an algorithm is dead, they refuse to deprecate it (MD5 certificate mess) or just use bad RND (too many examples to cite). The antibodies in the standards committees need to read this and think about it really hard. and "(4) Assume algorithms fall faster than Moore's law and, in the standard, provide a sunset date."
I completely agree. The insane thing is that it is NOT the cryppies that are complaining about moving to RSA 2048 and 2048 bit DHE, it is the standards wonks that complain that a 3ms key exchange is "excessive". Who is the CSO of the Internet? We have Vince Cerf,  Bob Kahn or Sir Tim, but what about security? Who is responsible for the security of eCommerce? Who will VISA turn to? It was NIST (effectively). Thank you NSA, because of you NIST now has lost most of its credibility. (Secrets are necessary, but many come to light over time. Was the probability of throwing NIST under the bus [ part of the "challenge in finesse"? Did NSA consider backing down when the Shumow, Ferguson presentation (which Schneier blogged about) came to light in 2007?).  We have a mess. Who is going to lead? Can the current IETF Security Area step into the void? They have cryptographers on the Directorate list, but history has shown that they are not incredibly effective at implementing a cryptographic vision. One can easily argue that vision is rarely provided by a committee oversight committee. John: Thank you. These are absolutely the right criteria. Now what?

@_date: 2013-09-03 13:06:20
@_author: james hughes 
@_subject: [Cryptography] FIPS, NIST and ITAR questions 
"Hashes aren't ITAR covered" is a fact?.  from "Revised U.S. Encryption Export Control Regulations, January 2000" at
further, ECCN 5A992 is separated from the "high-functioning encryption" as follows. From Clear (as mud)?

@_date: 2013-09-05 22:11:40
@_author: james hughes 
@_subject: [Cryptography] Fwd: NYTimes.com: N.S.A. Foils Much Internet 
The following is from a similar list in Europe. Think this echoes much on this list but has an interesting twist about PFS cipher suites.
Begin forwarded message:

@_date: 2013-09-07 15:57:45
@_author: james hughes 
@_subject: [Cryptography] In the face of "cooperative" end-points, 
+1. Cooperative endpoints offer no protection to any cryptography because they have all the plaintext. One can argue that the subpoenas are just as effective as cooperative endpoints. The reductio ad absurdum argument is that PFS is not good enough in the face of subpoenas? I don't think cooperative endpoints is a relevant point. Passive monitoring and accumulation of cyphertext is a good SIGINT strategy. Read about the VENONA project. Clearly, the traffic was accumulated during which time there was no known attack.
While reusing OTP is not the fault here, PFS makes recovering information with future key recovery harder, since a single key being recovered with whatever means, does not make old traffic more vulnerable. This is not a new idea. The separation of key exchange from authentication allows this. A router I did the cryptography for (first produced by Network Systems Corporation in the 1994) was very careful not to allow any old (i.e. recorded) traffic to be vulnerable even if one or both end points were stolen and all the key material extracted. The router used DH (both sides ephemeral) for the key exchange and RSA for authentication and integrity. This work actually predates IPSEC and is still being used.
I am getting from the list that there have been or are arguments that doing two public key operations is too much. Is it really? PFS may not be a panacea but does help.

@_date: 2013-09-08 10:02:34
@_author: james hughes 
@_subject: [Cryptography] Bruce Schneier has gotten seriously spooked 
That will not work....
When the community questioned the source of the DES S boxes, Don Coppersmith and Walt Tuchman if IBM at the time openly discussed the how they were generated and it still did not quell the suspicion. I bet there are many that still believe DES has an yet to be determined backdoor. There is no way to prove the absence of a back door, only to prove or argue that a backdoor exists with (at least) a demonstration or evidence one is being used. Was there any hint in the purloined material to this point? There seems to be the opposite. TLS using ECC is not common on the Internet (See "Ron was wrong, Whit is right"). If there is a vulnerability in ECC it is not the source of today's consternation. (ECC is common on ssh, see "Mining Your Ps and Qs: Detection of Widespread Weak Keys in Network Devices")
I will be looking forward to Bruce's next paper.

@_date: 2013-09-08 16:16:57
@_author: james hughes 
@_subject: [Cryptography] In the face of "cooperative" end-points, 
I think we are growing closer to agreement, PFS does help, the question is how much in the face of cooperation. Let me suggest the following. With RSA, a single quiet "donation" by the site and it's done. The situation becomes totally passive and there is no possibility knowing what has been read.  The system administrator could even do this without the executives knowing. With PFS there is a significantly higher profile interaction with the site. Either the session keys need to be transmitted  in bulk, or the RNG cribbed. Both of these have a significantly higher profile,  higher possibility of detection and increased difficulty to execute properly. Certainly a more risky think for a cooperating site to do. PFS does improve the situation even if cooperation is suspect. IMHO it is just better cryptography. Why not? It's better. It's already in the suites. All we have to do is use it... I am honestly curious about the motivation not to choose more secure modes that are already in the suites?

@_date: 2013-09-08 20:45:32
@_author: james hughes 
@_subject: [Cryptography] In the face of "cooperative" end-points, 
Could cloud computing be a red herring? Banks and phone companies all give up personal information to governments (Verizon?) and have been doing this long before and long after cloud computing was a fad. Transport encryption (regardless of its security) is no solution either. The fact is, to do business, education, health care, you need to share sensitive information. There is no technical solution to this problem. Shared data is shared data. This is arguably the same as the analogue gap between content protected media and your eyes and ears. Encryption is not a solution when the data needs to be shared with the other party in the clear. I knew a guy one that quipped "link encryptors are iron pipes rats run through". If compromised end points are your threat model, cloud computing is not your problem. The only solution is the Ted Kazinski technology rejection principal (as long as you also kill your brother).

@_date: 2013-09-09 14:32:17
@_author: james hughes 
@_subject: [Cryptography] What TLS ciphersuites are still OK? 
+1 I have read the document and it does not mention key lengths. I would suggest that 2048 bit is large enough for the next ~5? years or so. 2048 bit for both D-H and RSA. How are the key lengths specified?

@_date: 2013-09-09 19:59:09
@_author: james hughes 
@_subject: [Cryptography] What TLS ciphersuites are still OK? 
I retract my previous "+1" for this ciphersuite. This is hard coded 1024 DHE and 1024bit RSA. From 80 bit strength. Hard coded key sizes. Nice. AES 128 with a key exchange of 80 bits. What's a factor of 2^48 among friends?. additionally, as predicted in 2003? They were off by 3 years.
What now?

@_date: 2013-09-10 07:58:06
@_author: james hughes 
@_subject: [Cryptography] What TLS ciphersuites are still OK? 
Yes, GCM does have implementation sensitivities particularly around the IV generation. That being said, the algorithm is better than most and the implementation sensitivity obvious (don't ever reuse an IV).

@_date: 2013-09-10 09:01:37
@_author: james hughes 
@_subject: [Cryptography] [TLS] New Version Notification for 
We are arguing about a key exchange that goes from ~1ms to ~3ms (where the cracking goes from "yes" to "no"). Yes, this is more but this is absolutely not a problem for PCs or even phones or tablets especially in the light of session keep alive and other techniques that allow a key exchange to last a while. Is the complaint that the server load is too high? Lastly, going a partial step seems strange also. Why do we what to put ourselves through this again so soon? The French government suggests 2048 now (for both RSA and DHE), and will only last 6 years. From The minimum size of the modulus is 2048 bits for use not to exceed 2020.
For use beyond a 2020, the minimum module size is 4096 bits
Pardon the bad cut/paste and google translate, but I believe you get the point.

@_date: 2013-09-24 23:52:54
@_author: james hughes 
@_subject: [Cryptography] Gilmore response to NSA mathematician's 
Je n'ai fait celle-ci plus longue que parce que je n?ai pas eu le loisir de la faire plus courte.
Many, if not all, service providers can provide the government valuable information regarding their customers. This is not limited to internet service providers. It includes banks, health care providers, insurance companies, airline companies, hotels, local coffee shops, book sellers, etc. where providing a service results in personal information being exchanged. The US has no corner on the ability to get information from almost any type of service provider. This is the system that the entire world uses, and should not be our focus.
This conversation should be on the ability for honest companies to communicate securely to their customers. Stated differently, it is valuable that these service providers know the information they have given to the government. Google is taking steps to be transparent. What Google can not say is anything about the traffic that was possibly decrypted without Google's knowledge.
Many years ago (1995?), I personally went to a Swiss bank very well known for their high levels of security and their requirement that -all- data leaving their datacenter, in any form (including storage), must be encrypted. I asked the chief information security officer of the bank if he would consider using Clipper enabled devices -if- the keys were escrowed by the Swiss government. His answer was both unexpected and still echoes with me today. He said "We have auditors crawling all over the place. All the government has to do is to [legally] ask and they will be given what they ask for. There is absolutely no reason for the government to access our network traffic without our knowledge." We ultimately declined to implement Clipper.
Service providers are, and will always be, required to respond to legal warrants. A company complying with a warrant knows what they provided. They can fight the warrants, they can lobby their government, they can participate in the discussion (even if that participation takes place behind closed doors). The real challenge facing us at the moment is to restore confidence in the ability of customers to privately communicate with their service providers and for service providers to know the full extent of the information they are providing governments.

@_date: 2013-09-26 21:30:07
@_author: james hughes 
@_subject: [Cryptography] Gilmore response to NSA mathematician's 
I think we are in agreement, but I am focused on what this list -can- do and -can-not- do.
All the large banks have huge systems and processes that protect the privacy of their customers. It works most of the time, but no large bank can say they will never have an employee go bad. My point is that this thread was moving towards the statement that citizens of country X should use service providers that "eliminate the need for trust". Because of subpoenas and collaboration this statement is true in whatever the country the service provider is in and who the 3rd parties are. In essence, this is a tautology that has nothing to do with Cryptography. Even if a service provider could "convince you that they _can't_ betray you", it would either be naivet? or simply be marketing. The only real way to "eliminate the need for trust" from any service provider of any kind, or any country (your's or some other country), is to not use them. The one problem that this list (cryptography at metzdowd.com) -can- focus on is that the bar has been set too low for the governments to be able to break a few keys and gain access to a lot of information. This is the violation of trust in the internet that, in part, has been enabled by weak cryptographic standards (short keys, non-ephemeral keys, subverted algorithms, etc.). I am not certain that Google could have done anything differently. Stated differently, Google (and all the world's internet service providers) are collateral damage.
The thing that this list can effect is the creation of standards with a valuable respect for Moore's law and increases of mathematical understanding. Stated differently, "just enough security" is the problem. This past attitude did not respect the very probably future that became a reality. Are we going to continue this behavior? IMHO, based on what I have been seeing on the TLS list, probably.

@_date: 2014-08-15 18:52:25
@_author: james hughes 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
My definition of "big-name algorithm got broken? is: Algorithms that were broadly deployed and deprecated because they do not longer provide the security expectation any more? On ?the web? in my living memory... 56 bit DES
512 bit RSA
and my favorite, any non-PFS protocol.
BTW, all are still being used.
Nice!!! ?Zombie algorithms?? I think you have coined a great new term for these ?undead algorithms?! Yes, Designing (or modifying) cryptographic algorithms should be accomplished by those skilled in that art (which I am not one). Taking an algorithm and doing random stuff to it claiming to "strengthen it", in many cases, has the opposite effect. For instance, randomizing the s-boxes in DES or changing the constants in AES. Sometimes simple things like lengthening the key, increasing the rounds, etc. can make an algorithm weaker. Yes, buffer overflows and RND snafu are the gift that keeps on giving for many reasons? I also agree that the majority of ?you? (me included) should focus on roadkill.

@_date: 2014-08-16 13:21:03
@_author: james hughes 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
This is a little semantics. RC4, MD5 and SHA1 were ?end of life" because people discovered flaws in (broke) the algorithm, not because of design life limits (DES and 512 RSA).
?Fortifying algorithms? has been discussed before. When DES was teetering, the reluctance of the govies to replace DES or relax the key length led to DESX which showed to be an improvement, but not a panacea. The real result was the AES competition.  NIST does for banks and govies, (and anyone else with a clue), but IETF seems more stubborn (IMHO). Your grandmother has a windows 3.1 machine running Netscape. Should she be banned from accessing her bank over the internet?  Hard question. What is the liability of the bank? I doubt anything? Choosing security over communications (secure or nothing) is a tough choice for many businesses. Standards organizations like the IETF seem to favor insecure communications over no communications at all (secure with insecure fallback)). Implementors seem to say "caveat crypto" and push information security awareness on your grandmother. I seem to remember some hack that secure encrypting radios of some government agency can be made to default to ?in the clear? with some smart interference. Is that a vulnerability or a feature?  That we know of ;-) ?because there is a literal army of extremely well trained cryptographers watching over these algorithms.
