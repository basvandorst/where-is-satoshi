
@_date: 2010-07-27 16:54:40
@_author: Ralph Holz 
@_subject: A mighty fortress is our PKI 
Have these results already been published somewhere, or do you maybe
even have a URL?

@_date: 2010-06-02 18:36:06
@_author: Ralph Holz 
@_subject: Question w.r.t. AES-CBC IV 
Dear all,
A colleague dropped in yesterday and confronted me with the following.
He wanted to scrape off some additional bits when using AES-CBC because
the messages in his concept are very short (a few hundred bit). So he
was thinking about a variant of AES-CBC, where he uses just 32 (random)
bits as a source for the IV. These are encrypted with AES and then used
as the actual IV to feed into the CBC. As a result, he does not need to
send a 128 bit IV to the receiver but just the 32 bit.
His argument was that AES basically is used as an expansion function for
the IV here, with the added benefit of encryption. On the whole, this
should not weaken AES-CBC. Although he was not sure if it actually would
strengthen it.
While I am prepared to buy this argument (I am not a cryptographer...),
I still felt that the argument might not be complete. After all, 32 bits
don't provide much randomness, and I wasn't sure if this, overall, would
not lead to more structure in the ciphercode - which might in turn give
an attacker more clues with respect to the key.
Are there any opinions on this?

@_date: 2013-08-25 21:33:42
@_author: Ralph Holz 
@_subject: [Cryptography] Implementations, attacks on DHTs, Mix Nets? 
Can you rephrase whether you want info about DHT systems that are
related to some kind of mix system (e.g. GNUnet), or whether you simply
want to know about common DHT systems. If the latter, what kind of
attacks are you after? Eclipse?

@_date: 2013-08-26 10:56:19
@_author: Ralph Holz 
@_subject: [Cryptography] Implementations, attacks on DHTs, Mix Nets? 
OK, so I'll just add to what's been written so far.
* Most DHTs are indeed intended for a non-hostile environment and allow
users to freely place information in the DHT. This means that data items
can be easily eclipsed from the network by abusing the DHT's principle
of storing an item on the node with the ID that is closest to the item's
own ID. Most concepts support replica.
* The only DHT type that really has seen wide deployment seems to be
Kademlia, most notably in aMule/eMule and some bot networks. Steiner et
al. showed by example that Eclipse attacks against data items are easy
("Conducting and optimizing Eclipse attacks in the Kad P2P network").
* The aMule developers reacted to that attack by restricting routing
tables. Kohen/Leske et al. showed that this can be easily circumvented
by introducing chains of attackers that cooperate in a particular
fashion to redirect queries and let Kad run into a timeout.
* We have been active in Kad research for a little while, too. We found
that while Eclipse attacks against data items are easy, they are much
much harder against active nodes. I.e. Kad is designed to keep
long-running nodes as long in the routing tables as possible, and to
spread this knowledge widely in the network. This makes it very hard for
an attacker to reroute traffic intended for a victim. However, given a
very strong attacker (1000s of nodes), this should become possible
again. It is one of the disruptive DoS methods.
* The most interesting work that I know of is GNUnet: They employ a DHT called R5N that combines recursive Kad-style routing
with an initial random walk to evade the above attacker. GNUnet's
problem is that there are not enough developers to get the network to a
reasonable size, but the underlying technology is interesting. GNUnet
also has a SDSI/SPKI-style DNS replacement called GADS. Christian
Grothoff is the main developer and also at TUM (that's how I know him) -
he recently gave a talk on PRISM and GNUnet:
There is a host of older literature, too - P2P research, however, has
become a cold topic. Although I expect that it will see a revival in the
face of surveillance.

@_date: 2013-08-27 11:22:54
@_author: Ralph Holz 
@_subject: [Cryptography] Implementations, attacks on DHTs, Mix Nets? 
I would like to add the following:
R5n: Randomized recursive routing for restricted-route networks
NS Evans, C Grothoff
Network and System Security (NSS) 2011
Routing in the dark: Pitch black
NS Evans, C GauthierDickey, C Grothoff
Computer Security Applications Conference, 2007. ACSAC 2007
Exploiting KAD: possible uses and misuses
M Steiner, T En-Najjary, EW Biersack
ACM SIGCOMM Computer Communication Review 37 (5), 65-70
A global view of kad
M Steiner, T En-Najjary, EW Biersack
Proceedings of the 7th ACM SIGCOMM IMC, 2007
Measurements and mitigation of peer-to-peer-based botnets: a case study
on storm worm
T Holz, M Steiner, F Dahl, E Biersack, F Freiling
Proceedings of 1st Usenix Workshop LEET

@_date: 2013-12-11 20:24:57
@_author: Ralph Holz 
@_subject: [Cryptography] Size of the PGP userbase? 
I know of two publications over the past 10 years that analysed the Web
of Trust. Disclaimer: I am a co-author on one of them.
Capkun, S., Butty ?n, L., Hubaux, J.P.: Small Worlds in security systems:
an analysis of the PGP certificate graph. In: NSPW ?02: Proc. 2002
Workshop on New Security Paradigms, ACM (2002) 28?35
Ulrich, A., Holz, R., Hauck, P., Carle, G.: Investigating the OpenPGP
Web of Trust. ESORICS 2011.
There are also two Web sites, which may or may not be so accurate:
Web of Trust statistics and pathfinder.
Analysis of the strong set in the PGP web of trust.
How were these obtained? I thought about gathering some stats on these,
too, but did not like the idea of scanning Web sites for links to certs.
The alternative, using a traffic monitor and counting the S/MIME-typical
parts of mails, is no longer a good alternative as there is too much use
of Web mailers and SMTP connections are generally secured with SSL now.

@_date: 2013-12-14 12:11:35
@_author: Ralph Holz 
@_subject: [Cryptography] Size of the PGP userbase? 
Very hard to get unless you have a tap into a major MX. Much SMTP
traffic is SSL-secured today (which is a good thing), and thus the
standard monitoring setups, where the monitors sits near the border
router, cannot determine if there is a signature inside the traffic.
The significance of doing local measurements is very disputable.

@_date: 2013-11-04 19:08:14
@_author: Ralph Holz 
@_subject: [Cryptography] DNSSEC = completely unnecessary? 
First, DNSSEC != DANE-TLSA != CAA. The latter is about pointing to the
intended CA (as an identifier string), although TLSA can also do the
same (as the hash value of a root's public key). DNSSEC is about the
integrity of any RR.
Second, what seems to be often missing in the discussion is the
consideration of synchronising TLSA records and the certificate-in-use.
I don't subscribe to the view that this is very easy -- if scans of the
HTTPS and SSH ecosystems have shown anything, then it is that poor
deployment practices are to be blamed for a huge part of our problems,
and none of DNSSEC/DANE/CAA solve those.
E.g. I recently did a scan of SSHFP records for all known hosts serving
SSH (this is a very short description of the process). 94% were
accurate. Fine for SSH -- but 6% failed resolutions would be a huge
problem for the Web. So what I'm saying is: the consideration of how to
fail on TLSA errors (soft? hard?) is a hard one for browser vendors. (I
think the RFC for TLSA defines it, actually, but that's the RFC only)
You may find your browsers rejecting it - because I don't see movement
among browsers anywhere to move away from the CA model towards a
TLSA-only model.
Not buying this in its entirety, either. You're forgetting about
governments being able to take control of zones, and issue rogue RRs.

@_date: 2013-11-24 11:33:50
@_author: Ralph Holz 
@_subject: [Cryptography] Dark Mail Alliance specs? 
Could this be the rebirth of SPKI? I'd like to see that standard live again.

@_date: 2013-10-30 18:07:18
@_author: Ralph Holz 
@_subject: [Cryptography] Standard exponents in RSA 
the two most common exponents that one finds in X.509 RSA certs are
65537 and 17 -- in my data, they account for near 100%. Have these been
chosen as the result of some standardisation and was there some
cryptographic reasoning behind it, or is it simply that any exponent
will do? Any performance issues?

@_date: 2013-09-06 16:12:14
@_author: Ralph Holz 
@_subject: [Cryptography] Suite B after today's news 
But for right now, what options do we have that are actually implemented
Take SSL. CBC mode has come under pressure for SSL (CRIME, BEAST, etc.),
and I don't see any move towards TLS > 1.0.
RC4 was good enough for a while, but with djb's new work - it's just
waiting to be improved and made practical by someone. FWIW, we still use
RC4 on our servers, but I'd be happy to see something else that is
Of course, the above attacks are probably not one of your worries when
you're up against the NSA - your own system is probably much more

@_date: 2013-09-06 20:00:01
@_author: Ralph Holz 
@_subject: [Cryptography] People should turn on PFS in TLS 
Firefox has added TLS 1.2 two or three weeks ago, and TLS 1.2 does
indeed protect against BEAST, CRIME, Lucky 13 (but not against BREACH, I
However, my guess would be that too many Apaches out there are linked to
older openssl versions that do not yet support TLS 1.1 or TLS 1.2.
I have found this a good write-up:

@_date: 2013-09-07 19:47:12
@_author: Ralph Holz 
@_subject: [Cryptography] Suite B after today's news 
Exactly, precious little movement on that front. Sadly.
BTW, I do not really agree with your argument it should be done via TLS
extension. I think faster progress could be made by simply introducing
new allowed cipher suites and letting the servers advertise them and
client accept them - this possibly means bypassing IETF entirely. Or, to
keep them in, do it in TLS 1.3. But do it fast, before people start
using TLS 1.2.
I don't really see the explosion of cipher suite sets you give as a
motivation - e.g. in SSH, where really no-one seems to use the
standards, we have a total of 144 or so cipher suites found in our
scans. Yet the thing works, because clients will just ignore the weird
ones. It should be possible in SSL, too, unless openssl/gnutls/nss barfs
at an unexpected suite name - but I don't think so.

@_date: 2013-09-08 13:17:47
@_author: Ralph Holz 
@_subject: [Cryptography] Suite B after today's news 
I've followed that list for a while. What I find weird is that there
should be much dissent at all. This is about increasing security based
on adding quite well-understood mechanisms. What's to be so opposed to
Does adding some ciphersuites really require an extension, maybe even on
the Standards Track? I shouldn't think so, looking at the RFCs that
already do this, e.g. RFC 5289 for AES-GCM. Just go for an
Informational. FWIW, even HTTPS is Informational.
It really boils down to this: how fast do we want to have it? I spoke to
one of the TACK devs a little while ago, and he told me they'd go for
the IETF, too, but their focus was really on getting the code out and
see an effect before that. The same seems to be true for CT - judging by
their commit frequency in the past weeks, they have similar goals.
I don't think it hurts to let users and operators vote with their feet here.

@_date: 2013-09-24 16:51:28
@_author: Ralph Holz 
@_subject: [Cryptography] RSA equivalent key length/strength 
Are you talking about the BCP? Then what you say is not true either.
1) General consensus seems to be that recommending DHE-2048 is not a
good idea in the BCP, because it will not be available now, nor in short
to mid-range time. Voices that utter different opinions are currently a
minority; the BCP authors are not among them.
2) Consequently, the BCP effort is currently on deciding whether a ECC
variant of DHE or DHE-1024 should be the recommendation. The factions
seem to be split about equally:
Pro DHE-1024:
* Some say not enough systems provide ECDHE to recommend it, and thus
DHE1024 should be the primary recommendation.
* Some say ECDHE is not trustworthy yet due to implementation
difficulties and/or NSA involvement.
Pro ECDHE:
* Others say Chrome and Firefox will soon, or already do, support ECDHE
it. That would leave only the Windows users on IE, and we know that
Windows 8.1 will support it.
* The same people acknowledge the "trustworthy" argument. The question
is whether it weighs heavily enough.
That seems to be a more accurate description as I understand it from
reading the list. Myself, I am currently still undecided on the issue
but tend slightly towards ECDHE for now -- with any luck, the BCP won't
be ready until we have some more data on the issue.

@_date: 2014-05-15 12:21:23
@_author: Ralph Holz 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
Except that we now have a multitude of gods and demi-gods. Name
constraints will help a lot, but the plentitude of CAs continues to a
devastating weakness.
FWIW, the number of organisations (!= certs) in browsers continues to
grow happily.

@_date: 2014-11-21 17:40:03
@_author: Ralph Holz 
@_subject: [Cryptography] New free TLS CA coming 
I'm also wondering what it will be used for. It seems to be a kind of CA
that is simply meant to enable HTTPS everywhere - Domain Validation at
its purest, without claims going beyond what the BR rule.
Reading what Ian Grigg had to say on that topic, and knowing someone who
was involved with them for a while, I feel hesitant about CACert, too,

@_date: 2014-11-21 17:41:18
@_author: Ralph Holz 
@_subject: [Cryptography] New free TLS CA coming 
Read the CABF's BR. It's a good way to while away an afternoon and
you'll be surprised. Have a look at the section that says liability.

@_date: 2014-11-21 17:47:18
@_author: Ralph Holz 
@_subject: [Cryptography] New free TLS CA coming 
Hi Ian,
Can you elaborate on that 'spooks' thing a bit? It certainly goes beyond
what you've written up on the Web so far.
I thought the CABF was founded in 2005 and the BR took effect in 2012?
The practice is frowned upon, anyway - wouldn't it be covered by Moz's
Problematic Practices?

@_date: 2014-09-25 22:41:49
@_author: Ralph Holz 
@_subject: [Cryptography] The Trouble with Certificate Transparency 
CT was designed to make attacks like DigiNotar near impossible - by
detecting and containing the attack fast, but post-fact. That was before
the NSA became the attacker everyone is concerned about. CT is about
The crucial point for CT to work is the existence of monitors and
auditors, and a certain number of logs.
CT solves attack detection quite neatly - even against a strong global
CT also protects quite some level of protection for clients - if
multiple SCTs are used, and monitoring, auditing and finally gossiping
is in place.
You may argue that is not much, but I think it's better than many other
concepts. Personally, I like key pinning as an extremely strong way to
prevent further attacks.

@_date: 2014-09-27 13:38:56
@_author: Ralph Holz 
@_subject: [Cryptography] The Trouble with Certificate Transparency 
I know I probably should not do this, but it bothers me no end, and hey,
the Internet is for fun:
This is "rouge": This is "rogue": A former Rogue Squadron fan,

@_date: 2015-02-08 12:52:59
@_author: Ralph Holz 
@_subject: [Cryptography] best practices considered bad term 
Hi Ian,
I find your argument intriguing, but it is not supported yet by
evidence, and there may be different interpretations to the same
statistics. I am not saying you are wrong, but having actual data to
support that would be great - this is probably an area for qualitative
analysis followed by quantitative:
This may also be an artefact of inaccurate reporting of bugs, e.g. due
to a security team that was too small. It may also be an artefact of not
enough people outside poking the software.
I cannot recall any colleague who gave security as the first argument
for a switch to OS X. It was almost always the convenience of the OS,
plus the coupling to other devices. Sure, this is not a representative
sample - but it makes me ask for some real data.
Given that Microsoft has a good development lifecycle in place, this
makes me ask for data even more so. :)
