
@_date: 2001-08-15 16:31:42
@_author: Rick Smith@Secure Computing 
@_subject: If we had key escrow, Scarfo wouldn't be a problem 
Declan McCullagh quoted the Post article:
I suppose it's true that "privacy groups," or perhaps our culturally and legally acknowledged right to privacy, drove the police to use keystroke monitoring by eliminating key escrow. But it seems obvious to me, a non-lawyer, that keystroke monitoring is very similar in behavior and result to planting a tape recorder, and I assume that requires a wiretap order, too. Besides, I don't think we would have eliminated a court case by using key escrow: at best, we'd exchange one case for a different one.
While people only mentioned it occasionally, this alternative to key escrow always seemed blindingly obvious. When documents like the NRC's CRISIS report recommended that police and intel organizations rely on something other than key escrow or weak crypto, the only other way to go was to compromise the privacy of the endpoints. As they say: "First, look for the plaintext." And that's where the plaintext shows up.
I hope Baker wasn't thinking that this troublesome (for some folks) court battle could have been avoided by using key escrow. It's likely that the first use of evidence collected through a key escrow activated wiretap would have also led to some sort of courtroom test.
Personally, I think things are moving in the right direction for two reasons. First, the technical reason: despite flashy demos and partial deployments, I seriously doubt that elaborate systems like the Escrowed Encryption Standard are viable. The engineering is costly and tricky, and it's piggy-backed atop technology that we barely understand anyway. (For those of you who think cryptographic protocols are a solved problem in practice, check out the current discussions on the IPSEC mailing list about replacing IKE, or check out well-known attacks on DVDs, GSM, PPTP, etc.). So we've saved ourselves a world of engineering hurt by dodging the key escrow bullet. We have enough trouble making the simpler things work well.
The second reason this is a good direction is because it's best for society at large to have strong crypto.  Yes, it increases the risks and costs of legally accepted data interception by driving it to the endpoints. It makes police work harder, the same way the Miranda warnings and various other civil liberties actions have done so. On the other hand, it makes the information superhighway safer overall, by reducing the opportunities for sniffing and fraud, which will lower costs and free up resources for other things. Maybe it's just a philosophical quirk of mine, but I generally think it's better for everyone when things cost less.
smith at securecomputing.com
Pre-order "Authentication" at Amazon, see

@_date: 2001-07-09 14:55:55
@_author: Rick Smith@Secure Computing 
@_subject: Your password must be at least 18,770 char... 
One of those recently posted lists of quotations included a reference to Microsoft Knowledge Base article Q276304, from late June, which described the following problem:
If you log on to an MIT realm, press CTRL+ALT+DELETE, click Change Password, type your existing MIT password, and then type a new, simple password that does not pass the dictionary check in Kadmind, you may receive the following error message:
   Your password must be at least 18770 characters and cannot repeat any of your previous 30689 passwords. Please type a different password. Type a password that meets these requirements in both text boxes.
   Note that the number of required characters changes from 17,145 to 18,770 with the installation of SP1."
Here's the KB URL: There's no April 1 in the KB article. Does anyone know the story behind this? I'm in the copyediting stages of my "Authentication" book (already listed on Amazon) but I might want to slip in something about this incident, if I can get the real story.
smith at securecomputing.com

@_date: 2001-07-31 11:31:37
@_author: Rick Smith@Secure Computing 
@_subject: Criminalizing crypto criticism  
Hmmm. What would happen if every "legitimate" cryptography researcher routinely transmitted an announcement to every vendor of copy protection telling them that the researcher was going to be 'researching' the vendor's Research is such a wonderful term. I suppose I'm doing some sort of "cryptography research" just by looking at the bits that encode some sort of protected content. I must guiltily confess that I've been doing security long enough that I look with a skeptical eye at every "security implementation" I see, even if it's just a security camera or a string of barbed wire.
There are probably enough "cryptography researchers" out there that even a large vendor won't feel tempted to harass them all proactively.

@_date: 2001-11-02 10:35:12
@_author: Rick Smith@Secure Computing 
@_subject: Rubber hose attack 
Hmmm. I'm able to walk into a bank in semi-rural Italy and pull hundreds of dollars out of my credit card account. I'm able to buy subscriptions to Russian news sites. This seems pretty world-wide and Internet-wide to me. Existing systems work pretty well even if they don't achieve some cosmic notion of "Trust" or "Identity."
Of course, the process isn't 100% foolproof, and I'd be less likely to take advantage of it if fraud recovery fell more heavily on me as a consumer. Even so, there are generally enough valid transactions to cover the costs of the invalid ones to Web site proprietors and remote bank branches. Even if computer based mechanisms have shortcomings, the overall system is pretty robust.
If Microsoft's system is too brittle, then they'll pay for it through fraud expenses. If people find it unreliable or untrustworthy, they'll use other mechanisms for buying things. While I would feel compassion for consumers who are hurt or inconvenienced by some huge scam that exploited a poor Microsoft security implementation, such a scenario would be entertaining to Regardless of .Net's expected convenience, most people will probably still patronize non-.Net vendors when they offer better prices, regardless of the inconvenience. It's not that hard to re-enter billing information, especially when compared to driving across town to the discount store instead of using the higher-cost mini-mart down the street.
smith at securecomputing.com            roseville, minnesota
"Authentication" in bookstores

@_date: 2001-11-02 13:58:36
@_author: Rick Smith@Secure Computing 
@_subject: Rubber hose attack 
Of course. But this hasn't prevented people from acquiring and using credit cards. More to the point, it hasn't prevented the merchants, banks, and credit card issuers from maintaining and promoting this imperfect system. This would suggest that the losses from fraud (which customers don't pay, at least not here in the US) are amply covered by the income they bring in.
This sounds to me like a system that "works" in a practical sense.
An example of an authentication regime that did *not* work would be the password-based mechanism Citibank used on the cash management accounts for their large corporate customers, until they got hacked in the early '90s.
smith at securecomputing.com            roseville, minnesota
"Authentication" in bookstores

@_date: 2001-11-02 14:13:08
@_author: Rick Smith@Secure Computing 
@_subject: Rubber hose attack 
I might be. And maybe I'll have a terrific story to tell when it's all over. But I'm not daunted by the inconvenience of providing my contact information when I order something on-line, so I'm less likely to be drawn to one of their 'wallet' initiatives. Besides, this isn't the first time Microsoft has proposed a leaky wallet for use by the surfing public. So I'm more likely to suffer a more conventional (and less interesting) type of card fraud. It's happened before.
As I've said before, I think the security community plays an essential role when picking apart commercial security technologies, especially when they turn out to be as flawed as Microsoft's latest balloon.
smith at securecomputing.com            roseville, minnesota
"Authentication" in bookstores

@_date: 2001-11-02 16:00:18
@_author: Rick Smith@Secure Computing 
@_subject: Proving security protocols 
Okay, I'll grab this hot potato.
There are a few cases where a commercial development organization performs formal verification, which would seem to indicate that it can in fact provide benefits that outweigh the costs. In particular, I know that DEC/Compaq and Motorola have used formal verification, at least to some degree, to detect flaws in integrated circuit designs. This makes sense because it's so expensive to recover from an IC design flaw, so it's cheaper to spend money up front on eliminating possible flaws. If you're building your crypto protocols into hardware, particularly silicon, then you might see similar benefits to formal protocol assurance.
On the other hand, the software industry has marketed itself into a situation in which vendors are penalized if they spend too much effort on quality assurance, whether it be formal methods or even just testing. I've heard that NSA put a lot of effort into crypto protocol verification, but they weren't constrained by the same economic forces as others. A colleague who does formal verification of ICs looked at formal verification of non-crypto networking protocols a few years back, and concluded that the developers achieved sufficient quality through conventional testing.
The most recent issue of ACM's Transactions on Info Security carries my article on LOCK TCB assurance costs, and I made a number of observations on the cost/benefit of formal assurance in secure OS development. There's no either/or regarding testing and formal assurance: the question is whether you can afford to take money from your testing budget to pay for formal Bottom line: the LOCK experience suggests that you find more flaws through testing (per unit of resources spent) than you find through formal assurance. You can generally find a way to test (or at least exercise) just about every requirement and capability in a typical software product, but formal assurance can't come anywhere close to that degree of coverage in real world systems. Testing might not detect all flaws, but neither will formal assurance.
Ultimately, the decision to use formal assurance is driven by the types of flaws you need to detect, and the risks to the product caused by such flaws. In LOCK, there was a huge desire to detect covert channels before the system was deployed, and the assurance effort was deemed to be successful at pursuing that goal.
smith at securecomputing.com            roseville, minnesota
"Authentication" in bookstores

@_date: 2001-11-02 16:03:14
@_author: Rick Smith@Secure Computing 
@_subject: Rubber hose attack 
The thread started with an op-ed piece by Diffie and Landau about MS .Net, briefly noting vulnerability reports about Microsoft's latest 'wallet' (called "Passport" and produced as part of .Net). Evidently the early version was storing passwords in a format that made them trivial to recover.
I think we can all agree that this is a Bad Idea, and that MS might have faced a good deal of liability and negative press if the system had been on-line and their .Net partners had been offering anything worth stealing.
While I prefer to see enterprises deploy strong security measures (especially ones they buy from us :->) it's important to acknowledge how much risk we routinely take, both personally and when operating businesses. We all settle for less than cosmically perfect automobiles, and they pose far more serious risks to us than credit card fraud.
smith at securecomputing.com            roseville, minnesota
"Authentication" in bookstores

@_date: 2001-11-05 11:24:06
@_author: Rick Smith@Secure Computing 
@_subject: when a fraud is a sale, Re: Rubber hose attack 
I'm not sure what this means.
If we get really specific, then a transaction between me and
a small used-book seller consists of a transaction between
individual humans, but my transactions with Amazon involve
an abstract entity represented by teams of humans. Presumably
my latest transaction still proceeds even if the first person
to process it at Amazon quits before the package is shipped.
That's not so clear if the bookseller drops dead.
If we look at authentication as an engineering problem, then
you can only 'authenticate' between entities that share some
fairly complex secret information. Anything else can be spoofed
pretty easily. I don't think it's practical to speak of strong,
network based authentication between 'users' unless we tie them
to physical devices that store those secrets (private keys, etc.).
Of course, this distinction simply illustrates the gap between
our policy objectives (authenticate particular roles and/or
entities) versus the available tools (verify ownership of hard
to forge credentials).
smith at securecomputing.com            roseville, minnesota
"Authentication" in bookstores

@_date: 2001-11-05 11:35:36
@_author: Rick Smith@Secure Computing 
@_subject: when a fraud is a sale, Re: Rubber hose attack 
Perhaps this is why I'm expecting PKI to flourish primarily within enterprises that run their own CAs as opposed to third parties, at least in the near term.
Although a few third party credit card vendors got things started decades ago, credit cards didn't really blossom until after a period in the '60s and '70s during which many/most individual enterprises issued their own cards. This allowed the enterprises to learn by themselves what the costs, risks, and rewards were. They had the opportunity to decide for themselves what risks to take and directly experience the results. Only after the enterprises developed this internal awareness of the real implications of such cards could they understand the system well enough to know what it meant to sign up with Visa, MC, or one of the other big names. At least, that's my reading of the history, and how it might apply to PKI or other authentication technologies.
It seems to me that the concept of identity is application specific (and thus enterprise specific in a sense), which makes it tricky for an 'authentication vendor' to try to provide a general 'identity' solution except maybe through 'AAA' products.
smith at securecomputing.com            roseville, minnesota
"Authentication" in bookstores

@_date: 2001-11-06 11:29:44
@_author: Rick Smith@Secure Computing 
@_subject: when a fraud is a sale, Re: Rubber hose attack 
Definitely true. It would be great to see that technology replace the relatively vulnerable challenge response hashes used by Microsoft and others. In general I'm skeptical of protocols that rely entirely on a memorized secret for remote access security, but the [SP]EKE stuff is supposed to use the weak secret to bootstrap a strong one without opening a crack that might allow a dictionary attack on the weak secret. A slick idea.
smith at securecomputing.com            roseville, minnesota
"Authentication" in bookstores

@_date: 2001-10-01 10:38:30
@_author: Rick Smith@Secure Computing 
@_subject: New encryption technology closes WLAN security loopholes 
What you have is a perimeter that shrinks to that of the individual devices. And you have to slice and dice your security policy so it considers types of risk incurred by penetrating different types of perimeters, instead of simply saying one set of things is 'trusted' and the rest 'untrusted.'
In the world of crypto, you want to treat different perimeters differently simply because some risks are more profound -- the perimeter that protects long lived keying material is more important than the one to protect session keys, etc.
I'd argue that this has always been true. The problem has always been that designers often ignore security issues and rely on features of existing products or infrastructure to provide the right protection. The results are often heavy handed and tend to leave gaps that are hard to monitor and/or The only way an application will get built-in security is if the design team includes someone who has looked at the security threats and developed application design requirements to address them. This doesn't seem to be a well-established process, partly because the requirements vary from one application to the next.
Financial systems represent one of the few application areas that have a set of reasonably well known security objectives, and they even have well known mechanisms to address them: the traditions of separation of duty and of double entry bookkeeping. These help detect unintentional errors as well as some types of intentional fraud.
And I'll bet you can trace the flaws back to an absence of design requirements to address the associated security risks. This is depressing when a requirement might be blindingly obvious, like individual accountability.
smith at securecomputing.com          roseville, minnesota
"Authentication" coming in October

@_date: 2001-10-03 09:59:07
@_author: Rick Smith@Secure Computing 
@_subject: Best practices/HOWTO for key storage in small office/home  
Or the iKey which is pretty much exactly what you're describing -- smart card-like crypto functions and key storage that plugs right into a USB socket. Given that just about everything has USB now, these seem a lot more practical than smart cards, which require the purchase of an interface device.
Either you have to be compulsive about carrying the key-holder, whether it be smart card, iButton, iKey, or whatever, or you have to incorporate memorized safe combinations, er, passwords. It's like the way that every key in a typical office is usually on a ring in the secretary's unlocked desk drawer, which is why safes have combination locks instead of keys. Of course, then you have to contend with the compulsion to write the secret down so you don't forget it, and you're back where you started...
smith at securecomputing.com          roseville, minnesota
"Authentication" coming in October

@_date: 2001-10-15 17:52:12
@_author: Rick Smith@Secure Computing 
@_subject: Scarfo "keylogger", PGP 
Stripping off the precise legal language, this looks like a software keystroke logger that was carefully crafted to collect a PGP passphrase while collecting as little other data as possible. Collecting evidence is tricky business. You have to collect exactly the information you need, but you mustn't collect any information you aren't authorized to collect. If you do, then you can't use the information you have. Moreover, you need to be able to show that the evidence is 'clean' and hasn't been tampered with. This makes it very tricky when you're trying to collect computer information that's intended to be used as evidence in legal proceedings.
Without actually seeing the warrant used to authorize the keystroke capture, it's hard to tell what was really going on. But it seems reasonable to speculate that the keystroke monitor was carefully configured to comply with the letter of the warrant issued to the FBI to implant the keystroke logger. If they collect too much data under the warrant, the defense attorney might be able to block the use of the logs as evidence by arguing that the FBI didn't comply with the warrant.
I suspect that the "components" of the logger are software modules that are included and/or configured according to the types of data that the FBI has a warrant to collect.
I found Scarfo's choice of password rather amusing, since it shows that a personally tailored dictionary attack would have worked as well as the keystroke logging, and probably wouldn't have taken as long (14 days).
If my speculations about the warrant are correct, the logger may have shut itself down just to reduce the risk of intercepting anything that might have violated the letter of the warrant.
If someone manages to install Back Orifice (or its latest incarnation) on the victim's computer, then it's possible to remotely command Back Orifice to install keystroke logging software. However, the remote approach isn't 100% guaranteed to work, and Scarfo might have detected the installation activity or the presence of Back Orifice.
No, it's a problem with any programmable computer. If you can install new programs, you can install changes to existing programs. Since the FBI snuck into Scarfo's house and had physical access to his computer, they could install or patch the Windows OS, or PGP, or anything else on the computer however they wanted. The only limitation on their actions was that they didn't want to change anything Scarfo might detect.
smith at securecomputing.com            roseville, minnesota
"Authentication" in bookstores

@_date: 2001-10-17 10:02:26
@_author: Rick Smith@Secure Computing 
@_subject: Scarfo "keylogger", PGP 
If you have physical access to a commercial computing device, be it Unix or Microsoft or anything else, and you have the right tools, you can reprogram the OS, the applications or both, to do whatever you want. The tools aren't that expensive or that hard to acquire, especially for an intelligence/law enforcement organization. Physical access always trumps the software access controls which we must rely on to protect the plaintext and passphrases handled by PGP.
smith at securecomputing.com            roseville, minnesota
"Authentication" in bookstores

@_date: 2001-10-17 10:09:57
@_author: Rick Smith@Secure Computing 
@_subject: Scarfo "keylogger", PGP 
I spoke to someone a couple of years ago who had tried to establish a set of technical standards for handling host security logs so that they could be used as legal evidence, and ran into a stone wall at the Justice Department. Evidently they feared that defendants could manipulate any such standards to ensure that *no* electronic evidence could ever stand up in court.
I suspect the affidavit is badly written so that it meets the minimum standard for the court while providing as little useful information as smith at securecomputing.com            roseville, minnesota
"Authentication" in bookstores

@_date: 2001-09-10 11:46:25
@_author: Rick Smith@Secure Computing 
@_subject: Sen. Hollings plans to introduce DMCA sequel: The SSSCA 
There are obviously a number of arguments that even Senators might listen to.
1) This Act actually creates two types of computers: those that comply with the Act and those that don't comply.
2) This Act artificially inflates the cost of a basic PC, making it much harder to install them in schools, or use them in other ways to educate disadvantaged American citizens.
3)  If this Act forces all U.S. vendors to comply with the Act, then it eliminates U.S. vendors from the international personal computer market. Overseas vendors will continue to build the powerful products we use today, which provide far greater capabilities than most user can harness. U.S. vendors will have to build more costly products that won't be able to compete against cheaper foreign products.
4) This Act prevents "garage shop" innovation in information technology by placing it entirely in the hands of established vendors. This kills the wellspring of innovation that was responsible for the PC revolution in the first place. Innovation doesn't happen if it has to ask permission first.
smith at securecomputing.com          roseville, minnesota
"Authentication" coming in October

@_date: 2001-09-10 11:46:25
@_author: Rick Smith@Secure Computing 
@_subject: Sen. Hollings plans to introduce DMCA sequel: The SSSCA 
There are obviously a number of arguments that even Senators might listen to.
1) This Act actually creates two types of computers: those that comply with the Act and those that don't comply.
2) This Act artificially inflates the cost of a basic PC, making it much harder to install them in schools, or use them in other ways to educate disadvantaged American citizens.
3)  If this Act forces all U.S. vendors to comply with the Act, then it eliminates U.S. vendors from the international personal computer market. Overseas vendors will continue to build the powerful products we use today, which provide far greater capabilities than most user can harness. U.S. vendors will have to build more costly products that won't be able to compete against cheaper foreign products.
4) This Act prevents "garage shop" innovation in information technology by placing it entirely in the hands of established vendors. This kills the wellspring of innovation that was responsible for the PC revolution in the first place. Innovation doesn't happen if it has to ask permission first.
smith at securecomputing.com          roseville, minnesota
"Authentication" coming in October

@_date: 2001-09-26 14:51:13
@_author: Rick Smith@Secure Computing 
@_subject: New encryption technology closes WLAN security loopholes 
If anything, the concept of 'perimeter' becomes more important as you look at distributed firewall architectures, since it becomes a lot trickier to discern what it is you've really managed to protect. I've been trying to craft a clear explanation of how/why it's hard to subvert the card-based distributed firewalls we developed with 3Com, and the perimeter concept is crucial to the argument.
In my own experience, the security perimeter(s) play an essential role whenever I try to explain real-world weaknesses in systems. I find I'm always drawing boxes (perimeters) around things in security architecture diagrams I draw.
smith at securecomputing.com          roseville, minnesota
"Authentication" coming in October

@_date: 2002-02-06 16:37:28
@_author: Rick Smith@Secure Computing 
@_subject: Welome to the Internet, here's your private key 
Here are some manufacturer claims for the DataKey 330 smart card: average of 23 seconds to generate a 1,024-bit RSA key, average of 3 minutes to generate a 2,048-bit RSA key.
In practice this becomes one of those "installing something new" delays on your computer. You stick the smart card into the reader and watch the watch dial spin or the hourglass or whatever. Once it's done, the thing is "installed" and you're ready to go. Unsophisticated users may worry that they'll face the same delay the next time it's plugged in, but presumably people will learn from experience.
Of course, you don't want to use such a key to protect a set of closely held encryption keys that protect critical data, since you'll lose the data if the smart card gets damaged or breaks down.
smith at securecomputing.com            roseville, minnesota
"Authentication" in bookstores

@_date: 2002-01-28 17:00:17
@_author: Rick Smith@Secure Computing 
@_subject: biometrics 
The essential problem I've always seen with biometrics (and one that Dorothy Denning acknowledged in her recent op ed piece without seriously examining) is the question of whether it's as efficient to deploy and manage biometrics safely as it is to deploy and manage some keyed alternative like smart cards or other tokens.
Once you start embedding crypto secrets into your biometric reader, you are no longer managing biometrics. You're now managing BOTH biometrics AND a bunch of crypto keys. Why not just save yourself the administrative headache, deploy tokens, and use that crypto key for authentication?
I'm sure there are applications where biometrics make sense (ATMs, door security, and other closed systems like that) but I just don't see them working in an open system where your main problem is to associate the endpoint with a person. If you also need to separately authenticate the endpoint, and that's what everyone recommends, then the system costs go up even more.
My favorite biometric implementation is the "fingerprint as PIN" token, which several vendors make. There's the Sony Puppy, a credit card calculator sized token with a USB cord and an embedded public key pair. There are also various PCMCIA readers that (apparently) you can plug in to your laptop to provide a biometric lock.
My impression, however, is that these readers provide a PIN-like resistance to attack. Once you've cranked the false rejections down to the point that it's convenient, the false positives are approaching PIN levels (2^13 guesses on average).
A nice feature of the "fingerprint as PIN" tokens is, of course, that the print never leaves the card. You still have to worry about images of fingerprints or rubber fingers, of course. The print is a back-up for physical possession.
smith at securecomputing.com            roseville, minnesota
"Authentication" in bookstores

@_date: 2002-01-28 17:07:06
@_author: Rick Smith@Secure Computing 
@_subject: Fingerprints (was: Re: biometrics) 
Have you been fingerprinted before? Did it take that long in that case? In my own experience, it only takes a few minutes to be fingerprinted on a standard card and, in theory, they should be able to build a database from high-res fingerprint card images. Some small percentage of the population has prints that are unusually hard to read. It might be time consuming to put such a person's prints onto a card.
Or perhaps it takes 20 minutes of ablutions and purifications to copy a fingerprint card, so they figure they might as well make the subject wait, smith at securecomputing.com            roseville, minnesota
"Authentication" in bookstores

@_date: 2002-06-10 17:01:16
@_author: Rick Smith@Secure Computing 
@_subject: NCR's Bombe 
I looked through the articles and found myself personally intrigued by comments about "Navy cryptographers at MIT" during WWII. This is the first I've heard of such a thing, though I've read a half-dozen books about crypto history. Either that, or I dismissed the comments as allusions to academics who dabbled in it but weren't really keyed into the war effort.
Does anyone have a thumbnail history of WWII crypto at MIT? The article points at the book "Information and Secrecy" by Colin B. Burke which I hadn't heard about before this. Anyone familiar with that book?
My father had been a naval officer at MIT during the war and stayed with the Navy (computer applications to 'business problems' and shipyards, not codebreaking) afterwards. The story he told was that he worked on radar, and the similarities between microwave circuits and evolving computer circuits had led him to go into computer R&D as the war wound down. After all these years I'm beginning to wonder if there was more to it than that.
smith at securecomputing.com            roseville, minnesota
"Authentication" in bookstores
