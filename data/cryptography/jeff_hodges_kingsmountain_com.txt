
@_date: 2001-04-03 17:00:10
@_author: Jeff.Hodges@kingsmountain.com 
@_subject: fyi: Content Protection for Recordable Media -- Jeffrey B. Lotspiech 
This talk is perhaps of interest. Note the second paragraph of the announcement below. The talk should be available live and archived for a while   ------- Forwarded Message
Reply-To: ee380            4:15PM, Wednesday, April 4, 2001
     NEC Auditorium, Gates Computer Science Building B03
Title:		Content Protection for Recordable Media
Speaker:	Jeffrey B. Lotspiech
About the talk:
Content Protection for Recordable Media, or CPRM, is a technology
developed by IBM, Intel, Matsushita, and Toshiba to provide copy
protection on portable media. The technology allows a recorder to
record encrypted content, and a player to play it back, without
having any keys in common. The media acts as a passive oracle to
allow the different boxes to come to the same cryptographic key.
In contrast, previous copy protection technologies like the one
used for DVD video, depended on shared keys between the mastering
studio and the players, with predictable results. As soon as a
16-year-old in Norway found one shared key, the system was
effectively broken: there was no way to exclude the broken key
from the system without hurting too many innocent consumers. In
contrast, CPRM can survive thousands of independent attacks, and
exclude millions of circumvention devices, without any chance of
innocent consumers being affected.
Recently, articles have appeared in the press that CPRM will be
standardized on all PC hard drives. This has fueled Orwellian
mages of a Big Brother chip on your PC that will decide whether
your files are worthy of being copied. This is complete nonsense.
CPRM would never be standardized, nor have we ever proposed such
a thing. CPRM strength is portability and interchangeability and
it is mismatch for fixed hard drive. It is completely passive,
requires no hardware, and can only be exploited by newly-designed
applications. It cannot possibly affect existing files or
applications. How these myths came about, and persist, was an
object lesson for a media-naive researcher.
About the speaker:
Jeff Lotspiech is the manager of the Content Protection
Technology Group at the IBM Almaden Research Center. He has a BS
and MS in Computer Science from MIT, 1972. He has been working on
content protection technologies, both the Internet and media, for
the last six years.
Contact information:
Jeffrey B. Lotspiech
IBM Almaden Research Center DPEM/B3
650 Harry Road
San Jose, CA 95120
lotspiech at almaden.ibm.com
------- End of Forwarded Message

@_date: 2001-04-06 09:09:49
@_author: Jeff.Hodges@kingsmountain.com 
@_subject: [from Risks] Book: Security Engineering, Ross Anderson 
[risks] Risks Digest 21.31
Ross Anderson
Security Engineering: A Guide to Building Dependable Distributed Systems
John Wiley & Sons
March 2001
xxviii+612 pp.
ISBN 0-471-38922-6
This book is an enormous undertaking.  The chapter titles suggest the
breadth of coverage.
Part 1 (basic concepts)
 1. What is security engineering
 2. Protocols
 3. Passwords
 4. Access controls
 5. Cryptography
 6. Distributed systems
Part 2 (important applications)
 7, Multilevel security
 8. Multilateral security
 9. Banking and bookkeeping
10. Monitoring systems
11. Nuclear command and control
12. Security printing and seals
13. Biometrics
14. Physical tamper resistance
15. Emission security
16. Electronic and information warfare
17. Telcom system security
18. Network attack and defense
19. Protecting e-commerce systems
20. Copyright and privacy protection
Part 3 (organizational and policy issues)
21. E-policy
22. Management issues
23. System evaluation and assurance
24. Conclusions
Although there are other books that delve into greater detail on specific
topics, this book should be extremely useful to many people who need the
overall system perspective that Ross provides.
Ross's preface concludes with this sentence:
  "I believe that building systems that continue to perform robustly
  in the face of malice is one of the most important, interesting,   and difficult tasks facing engineers in the twenty-first century."
I could not agree more, although I would add that building systems to
perform robustly in the face of arbitrary adversities (accommodating power
and communication losses, rodents, bad software engineering, user errors,
etc. -- that is, not merely accounting for malice) is even more challenging.
Many systems in common use tend to fall apart all by themselves -- without
any malice!

@_date: 2003-04-24 14:00:38
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: fyi: draft XML Key Agreement Specification 
Phill sent the below msg around to a few lists back in Aug-2001. The draft spec he's referring to is available here..
It defines an ostensibly new cryptographic key agreement algorithm. Have cryptographer & security protocol designer denizens of these lists evaluated this stuff? Is anyone willing & able to post their evaluation/thoughts if so?
------- Forwarded Message
describes a key agreement algorithm that establishes a shared secret between
two parties if an only if the two parties hold the private keys identified
in their credentials and requires only a single request/response round trip.
although the diagrams loose a considerable amount when converted.
the XML syntax. However the same algorithm could be implemented in another
syntax to support other applications (IPSEC, 802.11b security, TLS).
the near future. Phillip Hallam-Baker FBCS C.Eng.
Principal Scientist
VeriSign Inc.
pbaker at verisign.com
781 245 6996 x227
[attachment deleted]

@_date: 2004-06-24 16:47:47
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: cryptograph(y|er) jokes?  (Superpolynomial subexponential  
it's kinda long, but I was at the Cryptorights party (as were many others on this list) where Eric did this and it was really very funny.
available at..
    How to Give a Math Lecture at a Party.
1. Pick the right party. I would suggest the RSA patent expiration
    to benefit the CryptoRights Foundation, but that party has already
    happened. (See  )
      1a. Ensure that there are a bunch of people at the party who've had
           to learn more about modular rings than they ever thought they           would.
      1b. Ensure that these people have also had to think about
          analysis of runtimes.
      1c. In short, ensure that there are a bunch of cypherpunks and their
           fellow-travellers hanging around.
2. Have the MC give away the punch line by announcing that you're going to
    sing a funny song.
3. Begin by insisting that the MC was mistaken. Announce that you're
    going to give a math lecture instead, and turn on the overhead
    projector. (Props are important signals of intent here.)
4. Put up, in sequence, the following four slides. Prepare the slides to
    be unnecessarily notational.
4-1. A description of the RSA algorithm. Include the statement N=pq and
      make sure to include the notation for the Euler totient function.
4-2. A description of the algorithmic runtime of the Number Field
      Sieve. It's really messy. Write it all out and go through it
      in loving detail. Talk about the best known constants. Be sure
      to drop Don Copperfield's name, because many good mathematical
      cryptography lectures do so. Point out that the logarithm of a
      logarithm is uncommon.
4-3. The assertion that the runtime of the NFS is slower than every
      polynomial function in the limit of large inputs. Use first
      order logic notation to avoid as many understandable words as
      possible.
4-4. The assertion that the runtime of the NFS is faster than every
      exponential function with arbitrary constant base in the limit
      of large inputs. Again, use first order logic notation.
5. Say the words, "So the NFS has ..." and proceed without pause to the
    next step.
6. Break into song. Sing the following lyrics to the obvious Mary Poppins
    tune.
6a. Pause during each round of applause so the audience can hear all
    the words.

@_date: 2005-02-10 15:50:19
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: fyi: Fingerprinting CPUs 
of possible interest to denizens hereabouts...
Maybe a software manufacturer could lock software (say an OS :-) ) to a
spefic machine djf

@_date: 2005-02-11 13:17:35
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: fyi: Fingerprinting CPUs  
dan at doxpara.com said:
ah, yes, in various forms. The refs in that paper lead to this, fwiw..

@_date: 2006-04-19 09:20:42
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: fyi: Deniable File System - Rubberhose 
Reply-To: ukcrypto at chiark.greenend.org.uk
Some years ago I did some design work on something I called a Deniable File System. The basic idea was the fact that the existence of ciphertext can in itself be incriminating, regardless of whether or not anyone can decrypt it. I wanted to create a file system that was deniable: where encrypted files looked like random noise, and where it was impossible to prove either the existence or non-existence of encrypted files.
This turns out to be a very hard problem for a whole lot of reasons, and I never pursued the project. But I just discovered a file system that seems to meet all of my design criteria -- Rubberhose  :
    Rubberhose transparently and deniably encrypts disk data, minimising
    the effectiveness of warrants, coersive interrogations and other
    compulsive mechanims, such as U.K RIP legislation. Rubberhose differs
    from conventional disk encryption systems in that it has an advanced
    modular architecture, self-test suite, is more secure, portable,
    utilises information hiding (steganography / deniable cryptography),
    works with any file system and has source freely available.
The devil really is in the details with something like this, and I would hesitate to use this in places where it really matters without some extensive review. But I'm pleased to see that someone is working on this Next request: A deniable file system that fits on a USB token, and leaves no trace on the machine it's plugged into.

@_date: 2006-08-28 07:44:52
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: fyi: Ross' Book now online 
I finally managed to persuade Wiley to let me put "Security Engineering"
online for free download:
  Some of the chapters in it are on-topic for this list, such as ch 6 (naming), ch 8 (medical privacy), 13 (biometrics), 20 (copyright)
and 21 (crypto policy).
Enjoy ...

@_date: 2006-12-21 09:41:27
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: Skype reverse-engineering details]  
Yes, that's a very interesting slide deck. An alternative URL to the talk is in this blog posting..
 Skype.exe innards revealed...

@_date: 2006-06-09 08:27:03
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: mailer certificate retrieval via LDAP?  
You should consider also posting your query to ldap at umich.edu

@_date: 2006-05-12 09:19:56
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: fyi: Workshop on the Economics of Information Security ("WEIS" June  
We now have online the program for next month's WEIS 2006 workshop in
   There are many papers of interest to habitues of this list, with
topics ranging from liability and the economics of trust, through the
interaction of networks with crime and conflict; the dependability of
open source and free software; reputation, privacy and risk
perception; the economics of DRM and trusted computing; the return on
security investment; and economic perspectives on spam. WEIS is
co-located with the Sixth Workshop on Privacy Enhancing Technologies -
see  for more on that event.
Register by the end of the week for an early registration discount -
at

@_date: 2006-09-20 14:45:44
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: fyi: On-card displays 
Via Bruce Schneier's blog, flexible displays that can sit on smartcards.
So we finally have an output mechanism that means you don't have to
trust smartcard terminal displays:
So, when do we see the combined chip/fingerprint reader/display on a
payment card :) Doesn't of course address the requirement that we want
evidence (such as a signed paper receipt) that can later be adjudicated
by a court with higher evidential standards than a bank statement that
their systems work perfectly...

@_date: 2007-04-26 15:18:15
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: Public key encrypt-then-sign or sign-then-encrypt?  
There's also this paper..
Donald T. Davis, "Defective Sign & Encrypt in S/MIME, PKCS MOSS, PEM, PGP, and XML.", Proc. Usenix Tech. Conf. 2001 (Boston, Mass., June 25-30, 2001), pp. 65-78
..which addresses some of the questions, in a certain context, that Travis

@_date: 2007-04-30 09:17:23
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: Cryptome cut off by NTT/Verio  
Note that JohnY offers a DVD of the entire site's current state, plus bonus extra DVD, for a mere $25 donation. I've got mine, get yers now.

@_date: 2007-08-31 11:23:54
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: World's most powerful supercomputer goes online  
Dark Reading Keywords : Attacks / Exploits / Threats : Botnets Dark Reading News Analysis: Storm Hits Blogger
  	August 30, 2007 : The ubiquitous Storm Trojan has found a new home                              on spam blog sites in Google's Blogger network
Storm Botnet sends spoofed YouTube spam
Author: Phil Cogar
Published: 29th Aug 2007
 Storm Botnet Is Behind Two New Attacks
 Posted by kdawson on Sunday August 26, from the do-not-click-here dept. Storm Botnet Puts Up Defenses And Starts Attacking Back
Researchers are warning universities that they're at risk of being hit with massive distributed denial-of-service attacks when they scan their own By Sharon Gaudin
August 16, 2007 04:23 PM lots more...

@_date: 2007-12-23 17:24:47
@_author: ' =JeffH ' 
@_subject: 2008: The year of hack the vote? 
2008: The year of hack the vote?
December 17th, 2007
Posted by Larry Dignan @ 2:12 am
The state of Ohio has released a comprehensive study of voting machine
security and the report will have you longing for paper.
A 334-page PDF report from the Ohio Secretary of State reveals insufficient
security, poor implementation of security technology, lax auditing and shoddy
software maintenance. The report, which covers voting systems from Election
Systems and Software (ES&S), Hart InterCivic and Premier Election Solutions
formerly known as Diebold, was conducted by Ohio\u2019s EVEREST (Evaluation and
Validation of Election-Related Equipment, Standards and Testing) initiative in
conjunction with research teams from Penn State, University of Pennsylvania
and WebWise Security.
The EVEREST report was released Dec. 7 and I found it via Slashdot. Overall,
the report really raises questions about election systems. Buffer overflows, encryption, audit problems and firmware issues abound. One machine, the
M100, from ES&S accepts counterfeit ballots. The Premier AV-TSX allows an
unauthenticated user to read or tamper with its memory. The Hart EMS has audit
logs that can be erased.
In fact, the first 17 pages of the report\u2013essentially the table of contents\u2013is an
indictment of these systems. To make matters worse, these machines don\u2019t constantly. That means malicious software could be planted and not turn up election time. These machines aren\u2019t patched regularly either.
The report is too massive to detail completely here, but at a high level here the takeaways from the EVEREST report:
    * Systems uniformly stunk at security and \u201cfailed to adequately address important threats against election data and processes.\u201d
    * A root cause of these security failures was \u201cpervasive mis-application of security technology.\u201d Standard practices for cryptography, key and password management and security hardware go ignored.
    * Auditing capabilities are a no show. \u201cIn all systems, the logs of election practices were commonly forgeable or erasable by the principals who they were intended to be monitoring.\u201d Translation: If there\u2019s an attack the lack of auditing means you can\u2019t isolate or recover from the     * Software maintenance practices \u201cof the studied systems are deeply flawed.\u201d The EVEREST report calls the election software Why would these machines be so enticing as a target? You could swing an
entire election, produce incorrect results, block groups of voters, cast doubt on an election or delay results. And it may not take a brain surgeon to alter these systems. The EVEREST teams reported that they were able to subvert every voting system and not be detected \u201cwithin a few weeks.\u201d Meanwhile, the EVEREST teams found the issues with only limited access since vendors weren\u2019t exactly cooperative (Section 2.4 of the PDF has the details).
The researchers say:
    Any argument that suggests that the attacker will somehow be less capable     knowledgeable than the reviewer teams, or that they will not be able to reverse engineer the systems to expose security flaws is not grounded in fact.
As for the attackers, EVEREST ranks the following folks in ascending order of     * Outsiders have no special access to voting equipment, but could affect equipment to an extent that it is connected to the Internet. All of the systems reviewed run Microsoft Windows and occasionally connect to the Internet. In addition, an attacker could create a counterfeit upgrade disk and mail it to install malware.
    * Voters have limited and partially supervised access to voting systems while casting a vote.
    * Poll workers have extensive access to polling place equipment, management terminals before, during and after voting. They can authorize who votes and who doesn\u2019t and opportunities to tamper with equipment abound.
    * Election officials have extensive access to back-end election systems and voting equipment. Access is only loosely supervised if at all. One possibility: Bad software prompts election officials to \u201ccorrect\u201d     * Vendor employees have access to the hardware and source code of system during development. Employees may also be on site to assist workers and election officials. \u201cSome vendors use third-party maintenance and election day support whose employees are not tightly regulated,\u201d according to EVEREST.
Add it up and any hack the vote opportunities will most likely be an inside job of some sort. The attacks may or may not be detectable.

@_date: 2007-12-28 09:06:44
@_author: ' =JeffH ' 
@_subject: Storm, Nugache lead dangerous new botnet barrage 
Storm, Nugache lead dangerous new botnet barrage
By Dennis Fisher, Executive Editor
19 Dec 2007 | SearchSecurity.com
In early 2006, Dave Dittrich, a senior security engineer and researcher at the University of Washington in Seattle, got a sample of a new strain of malware from a colleague, and began monitoring its activity. The Trojan was a bit lazy at first, making just a few outbound connections. But it quickly became obvious that this was no ordinary piece of malware, because each of the connections was to a peer and not a central command and control server.
This was strange behavior for PCs that have been compromised by this type of malware. The members of a distributed network like this typically communicate only with one central machine, called the command and control server. It's a top-down structure; the C&C server gives the commands and the compromised PCs carry them out. However, this new network didn't seem to have one C&C server that was running the show, and the malware itself couldn't really even be classified as a bot as it didn't make its first IRC connection for more than a month. IRC, or Internet Relay Chat, is the preferred method of communication for botnet controllers.
But with this network, in lieu of one C&C server, there were a number of peers around the network that were sending out commands and serving as download sites for various pieces of the network. So if one of the peers in the network that the attacker is using to issue commands to the rest of the network is shut down, the attacker could simply begin sending orders through another peer. This made the entire network of compromised PCs equal partners and made the prospect of disabling the network incredibly daunting.
As troubling as this new development was, more troubling was the fact that the peers sending out the commands changed on the fly and, as Dittrich watched, various members of the network would drop off botnet, only to reappear days or weeks later. So the shape and size of the botnet was changing almost constantly, with entire branches going dark for extended periods of time and peers jumping from one portion of the network to another seemingly on a whim. And, to add to the pile of bad news, the bots were communicating with each other over an encrypted channel, making it all but impossible to listen in on their conversations.
Dittrich, one of the top botnet researchers in the world, has been tracking botnets for close to a decade and has seen it all. But this new piece of malware, which came to be known as Nugache, was a game-changer. With no C&C server to target, bots capable of sending encrypted packets and the possibility of any peer on the network suddenly becoming the de facto leader of the botnet, Nugache, Dittrich knew, would be virtually impossible to stop.
"The authors are making these subtle little changes to keep it under the radar, and they're succeeding," said Dittrich.
This is the future of malware and it's not a pretty picture. What it is, is a nightmare: a new breed of malicious software developed, tested and sold by professionals and engineered to change on the fly, adapt to its environment and evade traditional defenses.
Nugache, and its more famous cousin, the Storm Trojan, are not simply the next step in the evolution of malware. They represent a major step forward in both the quality of software that malware authors are producing and in the sophistication of their tactics. Although they're often referred to as worms, Storm and Nugache are actually Trojans. The Storm creator, for example, sends out millions of spam messages on a semi-regular basis, each containing a link to content on some remote server, normally disguised in a fake pitch for a penny stock, Viagra or relief for victims of a recent natural disaster. When a user clicks on the link, the attacker's server installs the Storm Trojan on the user's PC and it's off and running.
Various worms, viruses, bots and Trojans over the years have had one or two of the features that Storm, Nugache, Rbot and other such programs possess, but none has approached the breadth and depth of their feature sets. Rbot, for example, has more than 100 features that users can choose from when compiling the bot. This means that two different bots compiled from an identical source could have nearly identical feature sets, yet look completely different to an antivirus engine.
The creators of these Trojans and bots not only have very strong software development and testing skills, but also clearly know how security vendors operate and how to outmaneuver defenses such as antivirus software, IDS and firewalls, experts say. They know that they simply need to alter their code and the messages carrying it in small ways in order to evade signature-based defenses. Dittrich and other researchers say that when they analyze the code these malware authors are putting out, what emerges is a picture of a group of skilled, professional software developers learning from their mistakes, improving their code on a weekly basis and making a lot of money in the "If you look at the way [Storm] is used, it's clear that money is changing hands and that the software has gone through a testing and revision process," said Phillip Porras, a program director at SRI International in Menlo Park, Calif., who has studied Storm's behavior. "The botnet is out there to help some group of people make money. This kind of malware is an economy now. Storm is not meant to spread across the entire Internet. It's meant to compromise specific targets. It's a network that is very good at producing money."

@_date: 2007-07-02 13:40:48
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: fyi: UK National Information Assurance Strategy Launched 
"News
National Information Assurance Strategy launched On 27th June, a National Information Assurance Strategy was launched at the IA07 event in Brighton. The annual event is hosted by CESG and brings together key players in industry and government to work in partnership to address the UK?s needs in safeguarding information and ICT."
The document is available at:  . I haven't read it yet, and so cannot comment, but in a related area I'm puzzled: having heard that Cabinet Office will be supporting Cabinet, I wonder what will happen to all the technical stuff such as Govt Gateway and even CSIA.

@_date: 2007-07-20 14:10:40
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: Enigma for sale on eBay  
perry at piermont.com said:
ebay now says (as of when this messge is sent):

@_date: 2007-06-20 15:42:29
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: fyi: Ross Anderson on UK ATM fraud 
see also: "Reliability of security systems"
           We helped make a piece on ATM fraud a few weeks ago for Newsnight, pointing
out that law enforcement on bank fraud is now deeply corrupt. The Home Office did a deal with the banks so that fraud victims must report the crime to the
bank, not the police; the City force's card squad is a tied cottage (as Nick
put it) as the banks pays its bills; ditto the Met's e-crime squad; ditto the Financial services ombudsman. This is jolly nice for the banks when the
fraud is done by a bent insider they don't want exposed, and jolly nasty for
the poor customer. It's also jolly nice for terrorists such as the Tamil       Tigers who use ATM fraud to raise money to finance murder and mayhem. It's
really wonderful for government spin doctors as fraud figures have fallen to
near zero.
I'm now told that the programme will run tonight. Unfortunately a lot of its
teeth have been drawn (below)
Just to let you know. The piece will run tonight. Sadly  we could only
include a small part of your magnificent contribution, so the angle
about the tamil tigers was dropped,  against my wishes. The banks spokesman is coming on afterwards. The Home Office  and ACPO
both refused to appear.

@_date: 2007-06-20 16:59:19
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: wrt "Network Endpoint Assessment" (was: Re: Free Rootkit with Every  
of potential related interest is..
Network Endpoint Assessment (NEA): Overview and Requirements         note term "remediate/remediation".
relevant snippage below. see also..
1. Introduction     Today, most network providers can leverage existing standards-
    based technologies to restrict access to their network based     upon criteria such as the requesting system's user or host-based     identity, source IP address or physical access point.  However     these approaches still leave the network resident systems     vulnerable to malware-based attack, when an authorized but     infected system is admitted and the malware is able to spread     throughout the internal network.     As a result, network operators need a proactive mechanism to     assess the state of systems joining or present on the network to     determine their status relative to network compliance policies.      For example, if a system is determined to be out of compliance     because it is lacking proper defensive mechanisms such as     firewalls, anti-virus software or the absence of critical     security patches, there needs to be a way to safely repair     (remediate) the system so that it can be subsequently trusted to     join and operate on the network.  The NEA technology strives to     provide a mechanism to report the configuration of an endpoint     for evaluation against network compliance policy.  Such a     mechanism could offer a useful tool for the network operators'
    arsenal but should be recognized as not being a complete     endpoint compliance solution in and of itself.      NEA typically involves the use of special client software     running on the requesting system that observes and reports on     the configuration of the system to the network infrastructure.      The infrastructure has corresponding validation software that is     capable of comparing the system configuration information with     network compliance policy and providing the result to     appropriate authorization entities that make decisions about     network and application access.  Some systems may be incapable     of running the NEA client software (e.g. printer) or be     unwilling to share information about its configuration.  In     these cases the network infrastructure might decide to disallow     or limit access to the network.     In many cases, the admission decision is provisioned to the     enforcement mechanisms on the network and/or system requesting     access.  The decision might allow for no access, limited or     quarantined access (possibly to allow for remediation), or full     access to the network.  While the NEA Working Group recognizes     there is a link between an assessment and the enforcement of the     assessment decision, the mechanisms and protocols for     enforcement are not in scope for this specification.     Architectures, similar to NEA, have existed in the industry for     some time and are present in shipping products, but do not offer     interoperability.  Some examples of such architectures include:     Trusted Computing Group's Trusted Network Connect [TNC],     Microsoft's Network Access Protection [NAP], Cisco's Network     Admission Control [CNAC]).  These technologies assess the     software or hardware configuration of endpoint devices for the     purposes of monitoring or enforcing compliance to an     organization's policy.  These architectures are not     interoperable because they are implemented using primarily non-
    standards based technologies.     The NEA working group is working on defining standard protocols     so as to enable interoperability between devices from different     vendors allowing network owners to deploy truly heterogeneous     solutions. This document describes the requirements for NEA     candidate technologies and protocols.   4. Problem Statement     NEA technology may be used for several purposes.  One use is to     facilitate endpoint compliance checking against an     organization's security policy when an endpoint connects to the     network.  Organizations often require endpoints to run an IT-
    specified OS configuration and have certain security     applications enabled, e.g. anti-virus software, host intrusion     detection/prevention systems, personal firewalls, and patch     management software.  An endpoint that is not compliant with IT     policy may be vulnerable to a number of known threats that might     exist on the network.     Without NEA technology, ensuring compliance of endpoints to     corporate policy is a time-consuming and difficult task.  Not     all endpoints are managed by a corporation's IT organization,     e.g. lab assets and guest machines.  Even for assets that are     managed, they may not receive updates in a timely fashion     because they are not permanently attached to the corporate     network, e.g. laptops.  With NEA technology, the network is able     to assess an endpoint as soon as it requests access to the     network or at any time after joining the network.  This provides     the corporation an opportunity to check compliance of all NEA-
    capable endpoints in a timely fashion and facilitate endpoint     remediation potentially while quarantined where needed.      Endpoint that are not NEA-capable or choose not to share     sufficient posture to evaluate compliance may be subject to     different access policies.     The decision of how to handle non-compliant or non-participating     endpoints can be made by the network administrator possibly     based on information from other security mechanisms on the     network (e.g. authentication).  For example, remediation     instructions or warnings may be sent to the non-compliant     endpoint with a properly authorized user while allowing limited     access to the network.  Also, network access technologies can     use the NEA results to restrict or deny access to an endpoint,     while allowing vulnerabilities to be addressed before an     endpoint is exposed to attack.  The communication and     representation of NEA assessment results to network access     technologies on the network is out-of-scope for this document.      Re-assessment is an important part of NEA technology as it     allows for additional assessments of previously considered     compliant endpoints to be performed.  This might become     necessary because network compliance policies and/or endpoint     posture can change over time.  A system initially assessed as     being compliant when it joined the network may no longer be in     compliance after changes occur.  For example, re-assessment     might be necessary if a user disables a security protection     (e.g. host firewall) required by policy or when the firewall     becomes non-compliant after a firewall patch is issued and     network policy is changed to require the patch.      Another use of NEA technology may be to verify or supplement     organization asset information stored in inventory databases.       NEA technology can be used to provide posture assessment for a     range of ways of connecting to the network including (but not     limited to) wired and wireless LAN access, remote access via     IPsec[IPSEC] or SSL[TLS] VPN, or gateway access.   NEA     technology can also be used to check and report compliance for     endpoints when they try to access certain mission critical     applications within an enterprise by employing service     (application) triggered assessment.

@_date: 2007-06-21 17:43:47
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: Free Rootkit with Every New Intel Machine  
pgut001 at cs.auckland.ac.nz said:
in case you're referring to the TCPA (trusted computing platform alliance) my understanding from a person active in the NEA working group (IETF) is that TPMs these days "come along for free" because they're included on-die in at least one of said chips. I don't recall whether he said it was the network interface (NIC) and/or one of the others. So anyway, he said "...enterprise-class systems (eg Dell Latitudes) mostly all already contain, TPMs and various network gear manufacturers have boxes that speak to them already, and NEA is just trying to standardize the protocols..."
I've noticed my latitude systems do in fact have a bios option for enabling/disabling their TPMs. (mine are disabled)
the way in that IT depts ensure that vic...er...employees don't turn 'em off (as I understand it) is they set the BIOS admin password on their "assets" (computers) before their give them out.

@_date: 2007-06-25 10:49:16
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: fyi: SHA-2 patent status 
of possible interest...
-------- Original Message --------
Of possible interest (but hopefully no concern) to this list: a new IPR statement from the NSA to the IETF. --Paul Hoffman, Director
--VPN Consortium
saag mailing list
saag at mit.edu

@_date: 2007-06-26 15:03:41
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: Free Rootkit with Every New Intel Machine (aka TPM, AMT) 
i'd also scrawled:
pgut001 at cs.auckland.ac.nz said:
my bad. I'd neglected to add "on enterprise-class systems" after "come along for free" (a qualification he did indeed express). WRT to Dell notebooks, that'd be the Latitude models.
In fact, with a little searching, i found the Dell pages below [2] that indicate TPM is installed on Dell's D-series enterprise class notebooks.
david_koontz at xtra.co.nz said:
Of course. And that's the driving force behind the IETF NEA ("Network Endpoint Assessment") working group AFAIK [1].
[1] Trusted Platform Module (TPM 1.1)
The TPM, or Trusted Platform Module ships standard on D410, D610 & D810. TPM is a security hardware device on the system board that will hold computer generated keys for encryption. It is a hardware-based solution that can help avoid attacks by hackers looking to capture passwords and encryption keys to sensitive data.
"What is TPM?
system board that will hold computer generated keys for encryption. It is a hardware based solution that can help avoid attacks by hackers looking to capture passwords and encryption keys to sensitive data.
When deploying advanced security features like TPM in your environment, the archive and recovery of keys protected by the TPM is critical to avoiding the risk of data loss or inaccessibility in the event of a system failure.
The security features provided by the TPM are internally supported by the following cryptographic capabilities of each TPM: hashing, random number generation, asymmetric key generation, and asymmetric encryption/decryption. Each individual TPM on each individual computer system has a unique signature initialized during the silicon manufacturing process that further enhances its trust/security effectiveness. Each individual TPM must have an Owner before it is useful as a security device.
TPM Applications
layer of security to the computer system. The TPM, when bundled with an optional security software package, can provide overall system security, file protection capabilities and protect against email /privacy concerns. TPM helps provide security that can be stronger than that contained in the system BIOS, operating system, or any non-TPM application.
Which Dell systems support TPM?	
The TPM 1.2 security hardware device comes standard on the following LatitudeTM  notebook systems: Latitude D420, D620, D820, OptiPlexTM  desktop systems: Optiplex 745, 740 and Dell PrecisionTM  Mobile Workstations M65, M90. Dell recommends the use of Microsoft? Windows?  XP Professional XP Professional operating system with TPM which includes advanced security, mobility and networking features. TPM is currently not supported by Dell on Red Hat? Linux?  operating systems. Customers who deploy TPM should also purchase Wave Systems Embassy Trust Suite from Dell Software & Peripherals to enable full TPM features including key archival and migration."

@_date: 2007-03-02 10:58:56
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: fyi: NSA Releases UK Crypto Docs 
NSA has released under FOIA nine crypto docs in response to a request
for information on "Non-Secret Encryption" and JH Ellis. One is a formerly
secret paper by Ellis written in 1977. Another is a formerly confidential
paper by Clifford Cocks written in 1998. Ellis and Cocks were long
associated with CESG.
Three of the nine papers were formerly classified as Top Secret Codeword.

@_date: 2007-05-07 09:59:05
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: fyi: A5 Cracking Project 
we are inviting people to design and build a A5/1 cracking machine.
We are security enthusiasts. We started in January 2007 and built a
GSM Receiver for 700 USD ( The first alpha
version of the GSM receiver is available from our webpage.
We are now looking for the next challenge: Cracking A5/1 for real.
We put up a public wiki at  for anyone
to edit and to add information.
If you are interested please also subscribe to our mailinglist by sending
an email to a5-subscribe at lists.segfault.net
Spread the word & happy hacking,

@_date: 2007-11-15 09:49:45
@_author: ' =JeffH ' 
@_subject: fyi: Colossus in action 
"BP" being Bletchley Park of course.
Just in case anyone is as ill informed as me, I was delighted to read at  that the rebuilt Colossus is or is about to be used on a message enciphered on a Lorenz machine and transmitted from Germany.
Following the links from that page I see that the message will be "intercepted" using the radio equipment of the time before the Colossus tries to work out the settings. There will be  parallel effort with modern radio equipment and computers. It should be an interesting race.
I admire those who had the enthusiasm to rebuild the machine and who had the contacts to get hold of the plans and notes that remained. The last time I took a particular interest in the project, some years ago, they were trying to find the right sort of valve and had just completed the framework for hurtling paper tape round and round.

@_date: 2007-11-17 11:25:08
@_author: ' =JeffH ' 
@_subject: fyi: Adi Shamir's microprocessor bug attack 
Adi Shamir's note on a microprocessor bug attack on public key cryptography featured in the NY Times today:
The NYT report:
Research Announcement: Microprocessor Bugs Can Be Security Disasters
[reformatted to 64 cols single-spaced]
Adi Shamir
Computer Science Department
The Weizmann Institute of Science
With the increasing word size and sophisticated optimizations of
multiplication units in modern microprocessors, it becomes
increasingly likely that they contain some undetected bugs. This
was demonstrated by the accidental discovery of the obscure
Pentium division bug in the mid 1990's, and by the recent
discovery of a multiplication bug in the Microsoft Excel
program. In this note we show that if some intelligence
organization discovers (or secretly plants) even one pair of
integers a and b whose product is computed incorrectly (even in
a single low order bit) by a popular microprocessor, then ANY
key in ANY RSA-based security program running on ANY one of the
millions of PC's that contain this microprocessor can be
trivially broken with a single chosen message. A similar attack
can be applied to any security scheme based on discrete logs
modulo a prime, and to any security scheme based on elliptic
curves (in which we can also exploit division bugs), and thus
almost all the presently deployed public key schemes will become
vulnerable to such an attack.  The new attack (which we call a "Bug Attack") is related to the
notion of fault attacks discovered by Boneh, Demillo and Lipton
in 1996, but seems to be much more dangerous in its
implications. The original fault attack required physical
possession of the computing device by the attacker, and the
deliberate injection of a transient fault by operating this
device in an unusual way (in a microwave oven, at high
temperature, with high frequency clock, or with a sudden spike
in the power supply). Such attacks are feasible against smart
cards, but are much harder to carry out against PC's. In the new
bug attack, the target PC can be located at a secure location
half a world away, and the attacker has no way of influencing
its operating environment in order to trigger a fault. In
addition, millions of PC's can be attacked simultaneously,
without having to manipulate the operating environment of each
one of them individually.
We now describe the basic idea of the new attack. We assume that
the RSA decryption (or signature generation) is using the
Chinese Remainder Theorem (CRT) which speeds up the operation by
a factor of 4 compared to naive implementations, that each
multiplication of big  operation by a factor of 4 compared to
naive implementations, that each multiplication of big numbers
proceeds by breaking them into the largest words which can be
handled by the native multiplier in that microprocessor
(typically 32 or 64 bits), and that all pairs of such words from
the two numbers will be multiplied in some order. Knowing the
target's public key n, the attacker can easily compute a half
size number c which is guaranteed to be between the two secret
factors p and q of n. For example, a number c which is the
square root of n (rounded to the nearest integer) always
satisfies p<c<q, and any number close to c is also likely to
satisfy this condition. The attacker now chooses a message m
which is equal to c, except that two low order words in it are
replaced by a and b, and submits this "poisoned input" to the
target PC.  The first step in the CRT computation is to reduce the input m
modulo p and q. Due to its choice, m will be randomized mod the
smaller p, but remain unchanged mod the larger q. The next step
in RSA-CRT is always to square the reduced inputs mod p and q,
respectively. Since a and b are unlikely to remain in the
randomized value of m (mod p), the computation mod p is likely
to be correct. However, mod q the squaring operation will
contain a step in which the word a is multiplied by the word b,
and by our assumption the result will be incorrect in at least
one bit. Assuming that the rest of the two computations mod p
and q will be correct, the final result of the two
exponentiations will be combined into a single output y which is
likely to be correct mod p, but incorrect mod q. The attacker
can then finish off his attack in the same way as the original
fault attack, by computing the gcd of n with y^e-m, where e is
the public exponent of the attacked RSA key. With very high
probability, this gcd will be the secret factor p of n. This
completely breaks the security of this key.  How easy is it to verify that such a single multiplication bug
does not exist in a modern  microprocessor, when its exact
design is kept as a trade secret? There are 2^128 pairs of inputs in a 64x64 bit multiplier, so we cannot try them all in
an exhaustive search. Even if we assume that Intel had learned
its lesson and meticulously verified the correctness of its
multipliers, there are many smaller manufacturers of
microprocessors who may be less careful with their design. In
addition, the problem is not limited to microprocessors: Many
cellular telephones are running RSA or elliptic curve
computations on signal processors made by TI and others, FPGA or
ASIC devices can embed in their design flawed multipliers from
popular libraries of standard cell designs, and many security
programs use optimized "bignum packages" written by others
without being able to fully verify their correctness. As we have
demonstrated in this note, even a single (innocent or
intentional) bug in any one of these multipliers can lead to a
huge security disaster, which can be secretly exploited in an
essentially undetectable way by a sophisticated intelligence
organization.

@_date: 2007-11-21 15:41:34
@_author: ' =JeffH ' 
@_subject: Ross Anderson: Searching For Evil 
Of possible interest...
Ross Anderson: Searching For Evil
Google Tech Talks
August 23, 2007
Computer security has recently imported a lot of ideas from economics, psychology and sociology, leading to fresh insights and new tools. I will describe one thread of research that draws together techniques from fields as diverse as signals intelligence and sociology to search for artificial Evildoers online divide roughly into two categories - those who don't want their websites to be found, such as phishermen, and those who do. The latter category runs from fake escrow sites through dodgy stores to postmodern Ponzi schemes. A few of them buy ads, but many set up fake communities in the hope of having victims driven to their sites for...

@_date: 2007-10-15 16:02:54
@_author: ' =JeffH ' 
@_subject: fyi: Storm Worm botnet numbers, via Microsoft  
pgut001 at cs.auckland.ac.nz said:
thanks for commenting on it. I pointed to it in order to see what denizens of this list might have to say about it. I'm simply curious.
Also, as I'd noted, I haven't really seen any estimates of Storm's extent -- other than that article [0] -- that actually go into any details about how the number is arrived at (however bogus or not the approach might be).
Also, I'm personally mostly just curious and have done only modest searches for info. And based on that, I've only come across the (typically) unsubstantiated "one or two million zombies" [1] to "(breathless) maybe /50 million/ out there" [2] articles/posts.
I haven't come across any detailed Storm extent analysis, even with having Google search specific security company sites (e.g. using "site:sec-corp.com"). So if anyone has pointers to pages (other than the MSFT blog article pointed to in an earlier post) that present a sane and substantiated analysis of Storm extent, please post 'em. Maybe folks don't want to (post 'em or point to 'em)? Are there papers in submission? ;-)
..as compared to the overall population of windows machines, on the Internet, a resonably-substantiated worst-case estimate? Because it's only twice as many as the 50M number thrown around in the likes of [2].
But yes, it'd be alarming if there's really 1B windows machines on the Internet (one way or another) and there's a reasonable probability of 10% being Storm zombies.
Yeah, one hopes so.
So, it'd seem to me (tho I'm not a statistician) that if one could get a set of articles wrt Storm extent that say at least something to substantiate how they arrived at the numbers, and then do some back-of-the-envelope calcs, we'd have  a better idea of what's going on, at least here in the public domain. I have to believe that there's folks working hard on this given the down-the-road risks, and are just keeping the info close to their collective [0] [1] [2]

@_date: 2007-10-22 17:55:39
@_author: ' =JeffH ' 
@_subject: fyi: Storm Worm botnet numbers, via Microsoft  
bmenrigh at ucsd.edu said:
Good.  excellent, how'd it go? Anyone else present on Storm?
are your slides now available?

@_date: 2007-10-24 18:38:53
@_author: ' =JeffH ' 
@_subject: fyi: Report on Workshop on Next Steps for XML Signature and XML 	 
of possible interest to some...
Scott Cantor and I represented the perspective of "xmldsig is broken/mess/complex from some non-trivial number of implementors' perspective, we spec'd 'just sign the blob' in a SAML binding spec recently because of this, perhaps if xmldsig is rev'd these sorts of concerns/approaches should be taken into account, to promote interoperability", and didn't get ignored, interestingly enough. Also, a few other participants explicitly mentioned the "streaming" use case, which is a key concern in Peter Gutmann's xmldsig critique: .
As the report described below indicates, there's an effort emerging to charter a W3C working group to rev the xmldsig spec, which might be of interest to various folk.
-------- Original Message --------
On 25 and 26 September 2007, W3C held a Workshop on Next Steps for
XML Signature and XML Encryption [1] in Mountain View, CA, USA,
hosted by VeriSign. The group has published its summary report [2].
The Workshop report indicates strong interest in additional work on
XML security and interest in a Working Group. Attendees identified
the areas of highest interest:
   - Create a basic profile of XML Signature
   - Review and possibly update the referencing
     model using xml:id and other mechanisms
   - Update cryptographic algorithms
   - Revisit XML canonicalization
   - Update the transform model.
Areas of ongoing and medium interest that were identified are scalable
profiling, implementation guidance, key management issues, XKMS, XML 1.1, EXI,
and interaction with other security organizations.
The Workshop report will serve as input for the deliverable of the XML
Security Specification Maintenance Working Group to propose a draft charter
for possible follow-up work.
To enable discussion among Workshop attendees, Working Group
participants, and the broader community, this mailing list,
public-xmlsec-discuss at w3.org (public archive [3]), has been created.
Participation in the mailing list is open to all interested parties.
Current list subscribers include the members of the XML Security
Specifications Maintenance Working Group, and workshop participants.
If you want to be removed from the list, please let me know.
[1] [2] [3]

@_date: 2007-09-01 18:27:15
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: Neal Koblitz critiques modern cryptography.  
[fwiw, Pascal Junod had sent this to this list under the subject "provable security" on 9-Aug]
well, in my reading it's not a "critique of modern cryptography" -- rather, it's (1) a comparison of the cultural differences between mathematical research and crypto research, and (2) a critique of the crypto research subfield known as "provable security", done using examples culled from Mr. Koblitz's personal experience.
and yes, it's worth reading.

@_date: 2007-09-01 18:54:12
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: debunking snake oil  
to some degree, Schneier is already doing this with his "doghouse" section of the Crypto-gram newsletter. Although it sounds like you're being more ambitious in terms of desiring to publish cracks/hacks or whatever.
Perhaps thumbing through the various Doghouses would provide some reasonable targets for a more thorough inspection than what Bruce has often done? They are typically companies and that would certainly fit with the view of some of the other comentors on this thread.

@_date: 2007-09-30 13:48:01
@_author: Jeff.Hodges@KingsMountain.com 
@_subject: fyi: Storm Worm botnet numbers, via Microsoft 
============================== START ==============================
food for consideration. yes,  are from MSFT as he notes, but are the only ones we have presently wrt actual Storm extent, yes? If not, pls post Storm Worm botnet numbers, via Microsoft
Posted by Ryan Naraine @ 7:40 am Categories: Patch Watch, Hackers, Microsoft, Browsers, Rootkits, Vulnerability research, Spam and Phishing, Spyware and Adware, Botnets, Exploit code, Viruses and Worms, Data theft, Pen testing, Passwords Tags: Microsoft Corp., Worm, Machine, MSRT, Productivity, Microsoft Windows, Cyberthreats, Spyware, Adware & Malware, Viruses And Worms, Security, Operating Systems, Software, Ryan Naraine
16 votes Worthwhile?
If the statistics from Microsoft\u2019s MSRT (malicious software removal tool) are anything to go by, the Storm Worm botnet is not quite the world\u2019s most powerful supercomputer.
The tool \u2014 which is updated and shipped once a month on Patch Tuesday \u2014 removed malware associated with Storm Worm from 274,372 machines in the first week after September 11. In all the tool scanned more about 2.6 million Windows machines.
These numbers, released by Microsoft anti-virus guru Jimmy Kuo, puts the size of the botnet on the low end of speculation that Storm Worm has commandeered between 1 million and 10 million Windows machines around the world.
[ SEE: Storm Worm botnet could be world\u2019s most powerful supercomputer ]
The MSRT numbers, though helpful, shouldn\u2019t be relied on as gospel. For starters, the tool targets a very specific known malware (it only finds exactly what it\u2019s looking for) and attackers constantly tweak malware files to get around detection. In addition, it is only delivered to Windows machines that have automatic updates turned on, which means there are liely tons and tons of hijacked machines that never gets a copy of the MSRT.
Still, Kuo claims that the September version of MSRT made a dent in the botnet.
    Another antimalware researcher who has been tracking these recent attacks has presented us with data that shows we knocked out approximately one-fifth of Storm\u2019s Denial of Service (DoS) capability on September 11th. Unfortunately, that data does not show a continued decrease since the first day. We know that immediately following the release of MSRT, the criminals behind the deployment of the Storm botnet immediately released a newer version to update their software. To compare, one day from the release of MSRT, we cleaned approximately 91,000 machines that had been infected with any of the number of Nuwar components. Thus, the 180,000+ additional machines that have been cleaned by MSRT since the first day are likely to be home user machines that were not notably incorporated into the daily operation of the Storm botnet. Machines that will be cleaned by MSRT in the subsequent days will be of similar nature.
    The September release of the MSRT probably cleaned up approximately one hundred thousand machines from the active Storm botnet. Such numbers might project that the strength of that botnet possibly stood at almost half a million machines with an additional few hundred thousand infected machines that the Storm botnet perhaps were not actively incorporating.
Kuo also confirmed fears that the botnet will slowly regain its strength once those cleaned machines become reinfected because those machines are likely unpatched and not equipped with any security software.

@_date: 2008-02-01 13:53:46
@_author: ' =JeffH ' 
@_subject: questions on RFC2631 and DH key agreement 
So AFAICT from perusal of RFC2631 "Diffie-Hellman Key Agreement Method" and RFC2630 CMS, when one executes a simple DH static profile between two parties, the only things that really need to go over the wire are each party's public keys (ya and yb) if { p, q, g, j } are known to both parties. And thus, "Generation of Keying Material" is done by each party separately, using the value of ZZ that each independently calculates, yes?  Thus keying material doesn't cross the wire and risk exposure (among various things).
So if p, q, g are not static, then a simplistic, nominally valid, DH profile would be to..
      a                                         b
  ----------                               ----------
  g, p, ya ------------------------------------>
      <--------------------------------------- yb
 [calculates ZZ]                         [calculates ZZ]  [calculates keying material]            [calculates keying material]
      .                                         .
      .                                         .
      .                                         .
..yes? Other than for b perhaps wanting to verify the correctness of { p, q, g, j } ("group parameter validation"), is there any reason to send q ?

@_date: 2008-02-01 20:03:23
@_author: ' =JeffH ' 
@_subject: questions on RFC2631 and DH key agreement  
docbook.xml at gmail.com said:
thanks, but that doesn't actually answer my first question. It only documents that a and b (alice and bob) arrive at the ZZ value independently. My question is actually concerning section 2.1.2 "Generation of Keying Material" in RFC2631. My interpretation is that both parties are expected to generate "keying material" per that section on their own, rather than one party doing so and transmitting the results to the other party. This may seem obvious to those steeped in the tradition here, but it perhaps isn't to outsiders (i have a foot in both worlds).

@_date: 2008-02-01 20:06:00
@_author: ' =JeffH ' 
@_subject: questions on RFC2631 and DH key agreement  
Oh, yeah, sorry, your diagram (or whoever drew it) does in fact answer my second question wrt what one needs to send over the wire wrt a simplistic DH profile. Just g, p, and a public key (y).
thanks again,

@_date: 2008-02-02 12:56:10
@_author: ' =JeffH ' 
@_subject: questions on RFC2631 and DH key agreement  
I'd scrawled..
ashwood at msn.com replied:
That's what I thought. BTW, I'm not myself working on something employing a DH exchange -- I'm analyzing/reviewing something that does.
Indeed, b could be any entity because it isn't proving possession of any known-only-to-it information.
Yep, I suspected that too. Thanks.
So, another question or two: If a purportedly "secure" protocol employing a nominal DH exchange in order to establish a shared secret key between a requester and responder, employs widely known published (on the web) fixed values for g ("2") and p (a purportedly prime 1040 bit number) for many of it's implementations and runtime invocations, what are the risks its designers are assuming with respect to the resultant properties of ZZ?
I suspect that many implementations will simply use the equivalent of whatever rand() function is available to get the bits for their private keys directly, and will likely not reallocate private keys unless the implementation or machine are restarted. So if the random number generator has known flaws, then there may be some predictability in both the public keys and in ZZ, yes? Additionally there's the previously noted issue with the values of static private keys slowly leaking.
thanks again,

@_date: 2008-02-04 10:02:34
@_author: ' =JeffH ' 
@_subject: questions on RFC2631 and DH key agreement  
Ok thanks, I'm going to risk pedanticism in order to nail things down a bit more rigorously..
pgut001 at cs.auckland.ac.nz said:
Are you referring to the above mentioned mechanism of arriving at the ZZ value independently, which is implied in RFC2631?
(btw, I am not myself designing anything at this time that uses DH, I'm reviewing/analyzing. I am _not_ reviewing RFC2630/2631 themselves, rather it's a (non-IETF) spec that references 2631)
So by "the spec" you're referring to RFC2631 here?
Or are you referring to X9.42?
Or something else?
So here, and in the snippage, are you referring to X9.42 itself, or CMS (Cryptographic Message Syntax) ?
Exactly which "standard" ?  From grepping all RFCs, it seems you're referring to CMS when you say "the standard", which has indeed been revised a few times since RFC2630.

@_date: 2008-02-04 17:18:55
@_author: ' =JeffH ' 
@_subject: questions on RFC2631 and DH key agreement  
I'd scrawled:
Joseph Ashwood graciously replied:
what about just using bytes from /dev/urandom on *nix?
             ^^
             so?
Ok, I can see that from ya = g ^ xa mod p  ...  ya doesn't change if g, xa, and p don't change.
Are you saying here that if p (and g) are static, then one has some opportunity to brute-force guess the private key that some long-running instance is using, if it doesn't otherwise re-allocate said private key from time to time?
thanks again,

@_date: 2008-02-05 11:02:29
@_author: ' =JeffH ' 
@_subject: questions on RFC2631 and DH key agreement  
ashwood at msn.com said:
yeah, that's the way it sounds from the man page (on linux). thx. ok, gotcha.
thanks again,

@_date: 2008-02-06 11:22:26
@_author: ' =JeffH ' 
@_subject: questions on RFC2631 and DH key agreement  
Thanks Hal. It turns out the supplied default for p is 1024 bit -- I'd previously goofed when using wc on it..

@_date: 2008-02-07 12:08:18
@_author: ' =JeffH ' 
@_subject: questions on RFC2631 and DH key agreement  
Thanks for your thoughts on this Hal. Quite educational. Ok, so what tools did you use to ascertain that? I'm curious. Same here.
It is hashed, but isn't used to gen further keys. I'm crafting a review of the full DH exchange in the target spec that I'll post to the list today.
That's what I thought. Yep. thanks again,

@_date: 2008-02-07 14:17:31
@_author: ' =JeffH ' 
@_subject: questions on RFC2631 and DH key agreement  
I think I already know the answer to this question, but I just want to test my How wise (in a real-world sense) is it, in a protocol specification, to specify that one simply obtain an ostensibly random value, and then use that value directly as the signature key in, say, an HMAC-based signature, without any further stipulated checking and/or massaging of the value before such use?
E.g., here's such a specfication excerpt and is absolutely everything said in the spec wrt obtaining said signature keys:
  When generating MAC keys, the recommendations in [RFC1750] SHOULD be   ...
  The quality of the protection provided by the MAC depends on the randomness   the shared MAC key, so it is important that an unguessable value be used.
How (un)wise is this, in a real-world sense? [yes, I'm aware that using a only a SHOULD here leaves a huge door open compliance-wise, but that's a different issue]

@_date: 2008-02-08 09:34:37
@_author: ' =JeffH ' 
@_subject: fyi: Encrypted laptop poses legal dilemma 
[Note:  This item comes from reader Randall.  DLH]
Encrypted laptop poses legal dilemma
By JOHN CURRAN, Associated Press Writer
When Sebastien Boucher stopped at the U.S.-Canadian border, agents who  inspected his laptop said they found files containing child pornography.
But when they tried to examine the images after his arrest,  authorities were stymied by a password-protected encryption program.
Now Boucher is caught in a cyber-age quandary: The government wants  him to give up the password, but doing so could violate his Fifth  Amendment right against self-incrimination by revealing the contents  of the files.
Experts say the case could have broad computer privacy implications  for people who cross borders with computers, PDAs and other devices  that are subject to inspection.
"It's a very, very interesting and novel question, and the courts have  never really dealt with it," said Lee Tien, an attorney with the  Electronic Frontier Foundation, a San Francisco-based group focused on  civil liberties in the digital world.
For now, the law's on Boucher's side: A federal magistrate here has  ruled that forcing Boucher to surrender the password would be  The case began Dec. 17, 2006, when Boucher and his father were stopped  at a Derby Line, Vt., checkpoint as they entered the U.S.
Boucher, a 30-year-old drywall installer in Derry, N.H., waived his  Miranda rights and cooperated with agents, telling them he downloads  pornography from news groups and sometimes unknowingly acquires images  that contain child pornography.
Boucher said he deletes those images when he realizes it, according to  an affidavit filed by Immigration and Customs Enforcement.
At the border, he helped an agent access the computer for an initial  inspection, which revealed files with names such as "Two year old  being raped during diaper change" and "pre teen bondage," according to  the affidavit.
Boucher, a Canadian with U.S. residency, was accused of transporting  child pornography in interstate or foreign commerce, which carries up  to 20 years in prison. He is free on his own recognizance.
The laptop was seized, but when an investigator later tried to access  a particular drive, he was thwarted by encryption software from a  company called Pretty Good Privacy, or PGP.
A grand jury subpoena to force Boucher to reveal the password was  quashed by federal Magistrate Jerome Niedermeier on Nov. 29.
"Producing the password, as if it were a key to a locked container,  forces Boucher to produce the contents of his laptop," Niedermeier  wrote. "The password is not a physical thing. If Boucher knows the  password, it only exists in his mind."
Niedermeier said a Secret Service computer expert testified that the  only way to access Boucher's computer without knowing the password  would be to use an automated system that guesses passwords, but that  process could take years.
The government has appealed the ruling.
Neither defense attorney James Budreau nor Vermont U.S. Attorney  Thomas Anderson would discuss the charge.
"This has been the case we've all been expecting," said Michael  Froomkin, a professor at the University of Miami School of Law. "As  encryption grows, it was inevitable there'd be a case where the  government wants someone's keys."

@_date: 2008-02-08 10:02:46
@_author: ' =JeffH ' 
@_subject: questions on RFC2631 and DH key agreement  
I'll point that out, thanks.
agreed (thx for the ptr to RFC4880) after doing some further reading and such. RFC4086 covers the notion of "mixing functions" etc, so the above-quoted SHOULD statement covers those bases.
agreed, thanks again.

@_date: 2008-02-21 15:01:22
@_author: ' =JeffH ' 
@_subject: wrt Cold Boot Attacks on Disk Encryption 
Begin forwarded message:
The paper published today makes some pretty strong claims about the  vulnerabilities of Microsoft's BitLocker, Apple's FileVault,  TrueCrypt, Linux's dm-crypt subsystem, and similar products.
So I put the folks behind it to a test. I gave them my MacBook laptop  with FileVault turned on, powered up, encrypted swap enabled, and the  screen saver locked.
They were in fact able to extract the 128-bit AES key; I've put screen  snapshots of their FileVault bypass process here:
And my article with responses from Microsoft, Apple, and PGP is here:
Bottom line? This is a very nicely done attack. It's going to make us  rethink how we handle laptops in sleep mode and servers that use  encrypted filesystems (a mail server, for instance).
- -Declan

@_date: 2008-01-11 15:29:30
@_author: ' =JeffH ' 
@_subject: Foibles of user "security" questions  
of possible relevance...
Mike Just. "Designing and Evaluating Challenge-Question Systems". IEEE SECURITY & PRIVACY, 1540-7993/04, SEPTEMBER/OCTOBER 2004.

@_date: 2008-01-28 11:04:25
@_author: ' =JeffH ' 
@_subject: fyi: independent contactless card e-money scheme called sQuid (UK) 
independent contactless card e-money scheme called sQuid (UK)
Here is another one: Barclays use the Visa method, which was initially called Visa Wave, and early news of it came out of Singapore.
Both methods (Mastercard and Visa) should work through the same terminal, but I don't yet have proof.
Then there is a independent contactless card e-money scheme called sQuid just launching (squidcard.com), and they will want to use the same

@_date: 2008-06-03 08:22:14
@_author: ' =JeffH ' 
@_subject: fyi: Traitor Tracing for Anonymous Attack in AACS Content Protection 
Title: Traitor Tracing for Anonymous Attack in AACS Content Protection
Speaker: Hongxia Jin, IBM Almaden
Broadcast encryption and traitor tracing are two active problems in
cryptography community.  In this talk I will give an overview on how
broadcast encryption and traitor tracing can be used for content
protection.  My focus of the talk is on tracing traitors for anonymous
attack. It is a way to trace the source of unauthorized copies of the
content or content encrypting keys when the system is broadcasted.  I
will give the talk in the context of AACS, the new industry content
protection standards for next generation high definition DVDs, It is
the first large-scale commercial deployment of  a traitor tracing
technology.  Along the way we have had to solve both practical and
theoretical problems that had not been apparent in the literature to
date.  In this talk I will focus on addressing some of those problems
in our process of bringing a theoretical solution to practice.
3 Jun (Tuesday) at 1630 hrs
Gates 4B (opposite 490)
Stanford Security Seminar on Google Calendar:

@_date: 2009-02-18 10:04:58
@_author: ' =JeffH ' 
@_subject: fyi: Researchers Hack Biometric Faces 
apropos to the biometrics essay in the Jan 2009 crypto-gram:
Researchers Hack Biometric Faces
from the face-off dept. posted by kdawson on 2009-02-18 01:35:00
yahoi sends in news from a week or so back: "Vietnamese researchers have cracked the facial recognition technology [1] used for authentication in Lenovo, Asus, and Toshiba laptops in lieu of the standard logon/password. The researchers were able to easily bypass the biometric authentication system built into the laptops by using photos of an authorized user, as well as by presenting multiple phony facial images in brute-force attacks. One of the researchers will demonstrate the hack at Black Hat DC this week. He says the laptop makers should remove the facial biometrics feature from their products because the vulnerability of this technology can't be fixed."
[1]

@_date: 2009-05-07 15:29:41
@_author: =JeffH 
@_subject: fyi: Accelerating computation with FPGAs 
of possible (topical) interest...
              Stanford EE Computer Systems Colloquium
                  4:15PM, Wednesday, May 13, 2009
         HP Auditorium, Gates Computer Science Building B01
                    Topic:    Accelerating computation with FPGAs
           with a seismic computation example
Speaker:  Michael Flynn
           Maxeler Technologies (and Stanford)
About the talk:
For many high performance applications the alternative to the
multicore rack is to use an accelerator assist to each multicore
node. There are a number of instances of these accelerators:
GPGPU, Specialized processors (E.G.IBM's Cell) and FPGAs.
At Maxeler we've found that the FPGA array technology wins out on
performance for most relevant applications. Given the initial
area-time-power disadvantage of the FPGA in (say) a custom
designed adder this is a surprising result. The sheer magnitude
of the available FPGA parallelism overcomes the initial
Using Maxeler's FPGA compiler toolkit, it is now feasible to
transform a software application into a data flow graph mapped to
an "unconstrained" systolic array. The array structure can be
matched to the applications structure and is not constrained to
nearest neighbor communications as the FPGA provides a
generalized interconnect.
As an example we consider modeling problems in seismic data
processing.  In a typical problem we realize a 2000 node systolic
array on 2 FPGA's, each node performing an operation each 4 ns.
About the speaker:
Michael Flynn is now Professor Emeritus of EE at Stanford. He
directed the Architecture and Arithmetic group in CSL for many
He is now Senior Adviser and Board Chairman at Maxeler.
Contact information:
Michael J Flynn
flynn at maxeler.com[2]
Embedded Links:
[ 1 ]    [ 2 ]    mailto:flynn at maxeler.com
ABOUT THE COLLOQUIUM:
See the Colloquium website,  for scheduled
speakers, FAQ, and additional information.  Stanford and SCPD students
can enroll in EE380 for one unit of credit.  Anyone is welcome to attend;
talks are webcast live and archived for on-demand viewing over the web.
MAILING LIST INFORMATION:
This announcement is sent to multiple mailing lists. If you are signed
up on our private EE380 list you can remove yourself using the widget
at the upper left hand corner of the Colloquium web page. Other lists
have other management protocols.

@_date: 2010-08-03 18:15:46
@_author: =JeffH 
@_subject: TLS/SSL Survey (Ristic/Qualsys) (was: Re: A mighty fortress is our 
Internet SSL Survey 2010 is here!  (blog post)
Actual report:
Qualys Internet SSL Survey 2010 v1.6 (PDF, 3.2 MB)

@_date: 2010-08-20 11:58:52
@_author: =JeffH 
@_subject: towards https everywhere and strict transport security (was: Has 
fyi, this is a heads-up about on-going work in this area...
pgut001 at cs.auckland.ac.nz wondered:
 > I noticed that Bank of America ... now finally use HTTPS on their home
 > page, and redirect HTTP to HTTPS ...  Wachovia now do it too.  And Citibank
 > at least redirect you to an HTTPS page.  And so does US Bank ...
 > What on earth happened?  Was there a change in banking regulations in
 > the last few months?
fungi at yuggoth.org replied:
 >
 > On Fri, Aug 13, 2010 at 09:32:57AM -0700, Jeff Simmons noted:
 >
 >> It wouldn't surprise me if there's been some blowback from the
 >> adoption of PCI-DSS (Payment Card Industry Data Security
 >> Standards). As someone who has had to help several small to medium
 >> size businesses comply with these 'voluntary' standards, the irony
 >> of the fact that the big banks that require them often aren't in
 >> compliance themselves hasn't escaped my notice.
 >
 > In the past month, we've had several customers at work suddenly insist that
 > we make modifications to their firewalls and/or load balancers to redirect
 > *all* incoming HTTP traffic to HTTPS (which of course isn't always entirely
 > sane to do on proxying devices, but they apparently don't trust their server
 > admins to maintain an HTTP redirect). Most of them cited requirements from
 > their PCI-DSS auditors. ...
Coincidentally with this apparent PCI-induced trickle-down of default employment of HTTPS, there's been relatively recent work on enabling web sites' declaration of security policies, and browser-side enforment of such.
Presently there's a patchwork quilt of approaches, two in particular are directly on-topic with the spirit of the above questions and observations...
   EFF's HTTPS Everywhere (Peter Eckersley et al)
      HTTP Strict Transport Security (HSTS)
   WRT the latter, we're forming a new IETF WG where it'll be finalized, along with a couple of other I-Ds (which are related patches in the quilt)..
   HASMAT Charter Proposal
   Not-coincidentally, the W3C is working towards establishing a web app security working group, where the related Mozilla "content security policy" work (as well as present W3C work on secure Cross-Origin Resource Sharing) is slated to    W3C Web App Security WG charter strawman
      Content Security Policy
   pgut001 at cs.auckland.ac.nz also observed:
 >
 >  Given the million [0] easier attack vectors against
 > web sites, which typically range from "trivial" all the way up to "relatively
 > easy" ...
 > [0] Figure exaggerated slightly for effect.
Indeed. WRT to this plethora of web attack vectors, the present patchwork quilt of remedies, and thoughts on how to go about more holistically approaching the issues, please see..
   The Need for Coherent Web Security Policy Framework(s)
   Internet Standards and Governance Team
PayPal Information Risk Management

@_date: 2010-08-24 16:00:24
@_author: =JeffH 
@_subject: towards https everywhere and strict transport security (was: 
Hi Jerry, thanks for your review and thoughts on the HSTS spec.
first, overall context-setting -- HSTS is spec'd and implemented as it (presently) is in order to get near-term traction -- i.e. browser implementation. (it is likely to be in Firefox 4, and is already in Chrome, and was first implemented in the NoScript extension). If we can get the other major browsers to also support it, we believe that it will be a Good Thing for Internet users.
It is possible that the present HSTS site policy signaling will be supplanted down the road in a further refinement cycle leveraging other emerging technologies, e.g. DNSSEC, as Jakob indicated.
Also, note that HSTS is presently specific to HTTP. One could imagine expressing a more generic "STS" policy for an entire site -- i.e. "connect to me only using 'secure means' no matter which protocol you're using, and here's metadata explaining what that means on a per-supported protocol/service basis."   But that is for a future design cycle.
 > Consider the scenarios:
 >
 > 1.  "First connection" to an HSTS server.  Here "first connection"
 > means that client is approaching this server with no pre-existing
 > security context - either it's never connected before, or it's stored
 > no information that will influence security decisions on its part,
 > from the previous connections, if any.  ...  >
 > Because HSTS forbids self-signed certs, no legitimate server will
 > present a combination of a self-signed cert and an HSTS header.  No
 > intelligent MITM will do so either.  It will either suppress the HSTS
 > header - or, more likely, use a perfectly legitimate cert signed by a
 > perfectly legitimate CA that the client will accept. ...
Yes, the spec notes this as the "Bootstrap MITM Vulnerability" in security considerations. UA Implementation advice is notes that a means to address this is through some sort of pre-loaded list, a la the cert root store. Chrome has implemented such, for example.
This is where DNSSEC-assured DNS-based functionality may play a key role.
 > 2.  A "second connection":  [ client MUST connect via TLS with no
 >     cert errors/warnings, otherwise fail connection with no user recourse ]
 >
 > So what does HSTS actually add?  I'd say three things:
 >
 > 	- Standardization of, and requirement for, HTTP to HTTPS redirects;
 > 	- Standardization of, and requirement for, clients to retain security
 > 	  information about previous connections, per server/URI/domain;
 > 	- The notion of a "time to live" for that security information, set
 > 	  by the server.
We feel it additionally adds:
   * Standardization of, and facilitation of, expression of
     (a particular) site security policy -- at the direction
     of the site administrators
   * protection of users from "click-through insecurity" (i.e. active MITM
     attackers)
 > But having gone this far ... the proposal then goes off into the weeds
 > by regulating self-signed certs for no really good reason,
We feel that absent additional assurance, e.g. via DNSSEC, it is better overall given our target audience (sites wishing to enable users to safely conduct ecommerce with them) to disallow self-signed certs.
If some site wishes both to employ HSTS (they don't have to, its an opt-in sort of thing), and have a very cheap cert, they can get a domain-validated (DV) cert for less than $10/yr from any number of CAs that are also (for better or worse) embedded in the major browsers' trust anchor cert store. That's the trade-off we've explicitly made (for now).
 > But why not let the server say "On subsequent connections, only accept
 > a cert with this fingerprint" (SSH connection) or "only accept a cert
 > signed by this CA" (sites don't change CA's often, and the time limit
 > means they can do so as long as they plan ahead) or "only accept certs
 > signed by CA's based in the following country" (like  Soghoian and
 > Stamm's work).
Note that this is a draft spec. There's various bells'n'whistles we've thought about adding -- eg declaration to "lock" the policy to a cert from a given CA, declaration to accept EV cert(s) only -- which we're going to discuss in the working group as the spec matures. However, there /is/ the concern of getting the spec finalized and working towards uniform implementation across browsers.
 > Why a "require this CA" together with a self-signed
 > cert shouldn't be allowed is beyond me - I can't see any situation in
 > which it's weaker than the current HSTS proposal.
because absent some form of 3d party assurance, the HSTS policy is simply declaring the site itself as the CA, the site has signed the cert, and the entity wielding such a cert could be simply be a MITM, e.g the pwn'd router at your coffeeshop or hotel. And then the user clicks thru the cert warnings, and is pwn'd.  unless there's something I'm missing.
As you intimated, this is AFAWK possible today via nefarious CAs that are browser-recognized trust anchors. That's why we've thought about the additional policy declarations noted above.
 > By being able to say:  For any connections to this server/URI/set of
 > domains for the next n seconds, accept only HTTPS connections with
 > certs signed by the following CA's (*including* self-signed certs!) -
 > now, that would be useful.
We think the self-signed cert thing isn't really useful until the scenario is something like, e.g...
For any connections to this server/URI/set of
domains for the next n seconds, make only error-free HTTPS connections with
certs signed by the following CA's, or EV certs, or self-signed certs that map to DNSSEC-secured domain(s).
Internet Standards and Governance Team
PayPal Information Risk Management

@_date: 2010-08-25 13:47:12
@_author: =JeffH 
@_subject: towards https everywhere and strict transport security (was: 
> A really knowledgeable net-head told me the other day that the problem
 > with SSL/TLS is that it has too many round-trips.  In fact, the RTT costs
 > are now more prohibitive than the crypto costs.
Yes, although that's a different class of issue from the ones we're trying to address in hasmat and keyassure.
these two drafts comprise the approach Adam Langley (of google) is presently pursuing wrt both fast TLS startup (snapstart) and support for NextProtocolNegotiation (during TLS handshake)..
Note that the motivation for draft-agl-tls-nextprotoneg is so-called websockets, which are being worked on in the IETF HYBI (hypertext bidirectional) WG

@_date: 2010-08-26 11:01:28
@_author: =JeffH 
@_subject: Overclocking TLS/SSL (was: towards https everywhere and strict transport 
Peter Gutmann  asked..
 >
 > Has anyone published any figures for this, CPU speed vs. network latency vs.
 > delay for crypto and the network?
there's this (by Adam Langley)..
Overclocking SSL
..but it doesn't appear to have (yet) the experimental results you're curious

@_date: 2010-09-13 14:34:10
@_author: =JeffH 
@_subject: 'Padding Oracle' Crypto Attack Affects Millions of ASP.NET Apps 
practical "Padding Oracle Attacks" (cf travis' msg "padding attack vs. PKCS7" of Thu, 11 Jun 2009 11:37:16 -0500)...
'Padding Oracle' Crypto Attack Affects Millions of ASP.NET Apps
by Dennis Fisher
September 13, 2010, 7:58AM
A pair of security researchers have implemented an attack that exploits the way that ASP.NET Web applications handle encrypted session cookies, a weakness that could enable an attacker to hijack users' online banking sessions and cause other severe problems in vulnerable applications. Experts say that the bug, which will be discussed in detail at the Ekoparty conference in Argentina this week [0], affects millions of Web applications.
The problem lies in the way that ASP.NET, Microsoft's popular Web framework, implements the AES encryption algorithm to protect the integrity of the cookies these applications generate to store information during user sessions. A common mistake is to assume that encryption protects the cookies from tampering so that if any data in the cookie is modified, the cookie will not decrypt correctly. However, there are a lot of ways to make mistakes in crypto implementations, and when crypto breaks, it usually breaks badly.
"We knew ASP.NET was vulnerable to our attack several months ago, but we didn't know how serious it is until a couple of weeks ago. It turns out that the vulnerability in ASP.NET is the most critical amongst other frameworks. In short, it totally destroys ASP.NET security," said Thai Duong, who along with Juliano Rizzo, developed the attack against ASP.NET.
The pair have developed a tool specifically for use in this attack, called the Padding Oracle Exploit Tool [1]. Their attack is an application of a technique that's been known since at least 2002, when Serge Vaudenay presented a paper at on the topic at Eurocrypt [2].
In this case, ASP.NET's implementation of AES has a bug in the way that it deals with errors when the encrypted data in a cookie has been modified. If the ciphertext has been changed, the vulnerable application will generate an error, which will give an attacker some information about the way that the application's decryption process works. More errors means more data. And looking at enough of those errors can give the attacker enough data to make the number of bytes that he needs to guess to find the encryption key small enough that it's actually possible.
The attack allows someone to decrypt sniffed cookies, which could contain valuable data such as bank balances, Social Security numbers or crypto keys. The attacker may also be able to create authentication tickets for a vulnerable Web app and abuse other processes that use the application's crypto API.
Rizzo and Duong did similar work earlier this year on JavaServer Faces and other Web frameworks that was presented at Black Hat Europe [3]. They continued their research and found that ASP.NET was vulnerable to the same kind of attack. The type of attack is known as a padding oracle attack and it relies on the Web application using cipher-block chaining mode for its encryption, which many apps do.
[0] [1] Practical Padding Oracle Attacks
     [2] [3]

@_date: 2010-09-30 15:30:11
@_author: =JeffH 
@_subject: Wrong Direction on Privacy - using NSLs to obtain communication transactional 
another facet of The Administration's "We Hear You" efforts..
Wrong Direction on Privacy
Susan Landau
The White House wants to make it easier for the FBI to get at your email and web browsing records; the plan is to make transactional information surrounding your Internet communications --- the to/from information and the times and dates of those communications --- subject to National Security Letters (NSLs), meaning the FBI could get these records without going through a judge.
NSLs were created in 1978 to give FBI investigators an easy way to obtain various business records, including the transactional information of phone records (not the content, which is subject to more stringent protections). The "easy" part of NSLs is that no courts are involved in issuing an NSL; the bureau does so itself. FBI guidelines require NSLs to be issued only on a written request of an FBI Special Agent in Charge (or other specially delegated senior FBI official), and there are four approval steps in the process.
Originally NSLs were to be used against foreign powers and people believed to be their agents. But proving someone was an agent of a foreign power was not all that easy, and NSLs were rarely used. That situation changed with the PATRIOT Act, which allowed NSLs to be used to gather information relevant to international terrorism cases. In an Orwellian touch, under the PATRIOT Act the bureau could require that the recipient of an NSL keep the order secret. NSL numbers shot up; between 2003-2006, the FBI issued 192,000 NSLs. Many were to phone companies. Why is clear; knowing who the bad guys are communicating with leads to untangling plots, often before law enforcement understands exactly what the plot might be. Such appears to be what happened, for example, in the case of Najibullah Zazi, who recently pled guilty to a plot to bomb the New York City subways.
At first in the initial aftermath of September 11th, telephone company workers were sharing offices with the FBI Communications Assistance Unit, and many times the required procedures went by the wayside. And instead of NSLs, the FBI begun using "exigent letters'' requesting immediate access to telephone records with claims to the phone companies that the appropriate subpoenas were in process. Many times that wasn't true. Sometimes there wasn't even a paper trail for the requests; they were just issued verbally. Dates and other specifics were often missing from the requests, which meant law enforcement got many more months of data than there was need for.
Why does this matter? It turns out that communications transactional information is remarkably revelatory. When NSLs were created in 1978, phones were fixed devices, and the information of who was calling whom provided a useful past history of behavior. The information is much richer with mobile devices; knowing who is calling whom, or whose cellphone is repeatedly located in the same cellphone sector as whose, provides invaluable information --- information that is simultaneously remarkably invasive. Transactional data reveals who spends time together, what an organization's structure is, what business or political deals might be occurring. ...

@_date: 2013-11-01 17:00:04
@_author: =JeffH 
@_subject: [Cryptography] A Bibliography of Pseudorandom Number Generation, 
possibly of some modest background interest..
A Bibliography of Pseudorandom Number Generation, Sampling, Selection, Distribution, and Testing
NHF Beebe - 2013 - math.utah.edu

@_date: 2013-10-25 13:30:07
@_author: =JeffH 
@_subject: [Cryptography] programable computers inside our computers 
Quoting Viktor Dukhovni (2013-10-22 06:50:38)
Dragos Ruiu: More on my ongoing chase of  malware.
 on G+
 (AND LOTSA PARANOIA, PLUS FIREWORKS)
Persistent BIOS malware with hypervisor and SDR found

@_date: 2014-08-08 15:41:05
@_author: =JeffH 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
John Kelsey wondered..
 >
 > Does it make sense to have a small set of curves that everyone uses?  Or
 > would it be better to have every application or even every user generate
 > their own curve, using some process that would convince skeptics that the
 > curves had been generated randomly?
These questions appear to be aspects of an overall curve-selection property DJB & Lange term "rigidity"..
   SaveCurves: Curves: choosing safe curves for elliptic-curve cryptography
   "Rigidity"
   It appears that the SafeCurves introduction  argues that the answer to your questions are nominally 'yes' and 'no' respectively, and appear to be proposing 11 specific curves (by my count of curves meeting all requirements as listed in the last table on the SafeCurves introducion page).
Others may offer (somewhat) different answers, e.g. Microsoft Ressearch..
   Selecting Elliptic Curves for Cryptography:
   An E
ciency and Security Analysis
   [ Microsoft's 'NUMS Curves' proposal. Bos, Costello, Longa, Naehrig]
   ..who are proposing 13 curves.
Another wrinkle is "target security level". Safecurves does not appear to attempt to group their curves according to bit-equivalent security levels, but Microsoft's 'NUMS Curves' proposal does.
See also the slides from the CFRG meeting at IETF-90 Toronto (a couple weeks ago now) which featured presos on the above by their authors/proponents..

@_date: 2014-12-13 17:08:24
@_author: =JeffH 
@_subject: [Cryptography] When did zero day attacks become commonplace? 
> Back in the mid 1990s, the idea an attacker would hit you with a
 > 'zero-day' was considered a mythical possibility. Anyone who suggested it
 > might happen was scaremongering. Then within the space of a few weeks
 > they went from being unheard of to routine.
 >
 > I am trying to track down when the transition occurred for a talk. I
 > remember there being a sharp transition but I can't place when.
Wasn't it around 1995 when the below appeared on some mailing list(s) and/or

@_date: 2014-05-02 18:26:17
@_author: =JeffH 
@_subject: [Cryptography]  "Covert Redirect" vulnerability in OAuth, OpenID 
> Anyone looked at the details on this one?
 >
 > It's sorta difficult to tell precisely, because the above-cited article lacks specifics, but I'm thinking it's likely that flaw is one that's discussed (among several others) in this paper (possibly among others [1])..
Sun, San-Tsai, Kirstie Hawkey, and Konstantin Beznosov. "Systematically breaking and fixing OpenID security: Formal analysis, semi-automated empirical evaluation, and practical countermeasures." Computers & Security 31, no. 4 (2012): 465-483.
see for example attack A4 in S 6.1.
[1]

@_date: 2014-10-01 06:57:29
@_author: =JeffH 
@_subject: [Cryptography] "Spy Agencies Urge Caution on Phone Deal" 
>> a special network/database - oddly never named in the article - that "rout[es] millions of phone calls and text messages in the United States". Apparently this was a system created back in the late 1990's to implement number portability.
 >
 > It's the "Number Portability Administration Center":
 > see also...
North American Numbering Plan
North American Numbering Plan Administration: NANPA

@_date: 2015-04-17 15:02:44
@_author: =JeffH 
@_subject: [Cryptography] stanford talk: Juan Garay: The Bitcoin Backbone 
this seems to be their associated paper..
  and Applications
   The Bitcoin Backbone Protocol: Analysis and Applications
                          Juan Garay
                   Tuesday, April 21, 2015
                        Talk at 4:15pm
                          Gates 498
Bitcoin is the first and most popular decentralized cryptocurrency to date.
In this work, we extract and analyze the core of the Bitcoin protocol,
which we term the Bitcoin "backbone," and prove two of its fundamental
properties which we call "common prefix" and "chain quality" in the static
setting where the number of players remains fixed. Our proofs hinge on
appropriate and novel assumptions on the "hashing power" of the
adversary relative to network synchronicity; we show our results to be
tight under high synchronization.
Next, we propose and analyze applications that can be built "on top'' of the
backbone protocol, specifically focusing on Byzantine agreement (BA)
and on the notion of a  public transaction ledger. Regarding BA, we observe
hat Nakamoto's suggestion falls short of solving it, and present a simple
alternative which works assuming that the adversary's hashing power is
bounded by 1/3. The public transaction ledger captures the essence of
Bitcoin's operation as a cryptocurrency, in the sense that it guarantees the
liveness and  persistence of committed  transactions. Based on this  notion
we describe and analyze the Bitcoin system as well as a more elaborate BA
protocol, proving them secure assuming high network synchronicity and that
the adversary's hashing power is strictly less than 1/2, while the
bound needed for security decreases  as the network desynchronizes.
This is joint work with Aggelos Kiayias (U. of Athens) and Nikos Leonardos
(U. Paris Diderot -- Paris 7).

@_date: 2015-07-07 10:47:54
@_author: =JeffH 
@_subject: [Cryptography] Crypto Wars 
> Would anyone be able to recommend me good literature on the Crypto Wars?
 > Both historical and theoretical accounts (if they exist) would be great,
 > digital or printed. Either stuff on the Crypto Wars I or  II would be fab.
see also:

@_date: 2016-12-02 16:26:56
@_author: =JeffH 
@_subject: [Cryptography] Cryptech's work on randomness (was: Is Ron right 
Ah, I found this message which may be of interest: in it, Joachim gives an overview of the cryptech HWRNG..
[Cryptech Tech] HWRNG specs

@_date: 2016-12-02 16:20:30
@_author: =JeffH 
@_subject: [Cryptography] Cryptech's work on randomness (was: Is Ron right on 
fyi, in case it is helpful, the cryptech project ( has done quite a bit of work on hardware RNGs and processing their output.
Their email archives are here: And a search for "random" in there yields 300 some hits..
..which one can narrow down as necessary.
[to view their wiki which has coalesced info on their RNG work, go here to get the appropriate root cert: the wiki is here: [ tho appears down right now :( ]

@_date: 2016-03-22 13:06:04
@_author: =JeffH 
@_subject: [Cryptography] Susan Landau's written testimony before the 
> Susan Landau's written testimony for the March 1 hearing before the
 > House Judiciary Committee:
 >
 > the above 404'd for me. I poked around and located this other URL to it..
..on the below page, which also links to testimony of the other witnesses..

@_date: 2017-08-21 08:34:09
@_author: =JeffH 
@_subject: [Cryptography] fyi: crypto Framework for Cloud File Systems and 
a possibly interesting data point...
[ePrint Report] A Novel Cryptographic Framework for Cloud File Systems and CryFS, a Provably-Secure Construction
Sebastian Messmer, Jochen Rill, Dirk Achenbach, Jorn Muller-Quade
Using the cloud to store data offers many advantages for businesses and individuals alike. The cloud storage provider, however, has to be trusted not to inspect or even modify the data they are entrusted with. Encrypting the data offers a remedy, but current solutions have various drawbacks. Providers which offer encrypted storage themselves cannot necessarily be trusted, since they have no open implementation. Existing encrypted file systems are not designed for usage in the cloud and do not hide metadata like file sizes or directory structure, do not provide integrity, or are prohibitively inefficient. Most have no formal proof of security. Our contribution is twofold. We first introduce a comprehensive formal model for the security and integrity of cloud file systems. Second, we present CryFS, a novel encrypted file system specifically designed for usage in the cloud. Our file system protects confidentiality and integrity (including metadata), even in presence of an actively malicious cloud provider. We give a proof of security for these properties. Our implementation is easy and transparent to use and offers performance comparable to other state-of-the-art file systems.

@_date: 2017-11-21 16:04:45
@_author: =JeffH 
@_subject: [Cryptography] Intel Management Engine pwnd (was: How to find 
Oh joy...
Intel finds critical holes in secret Management Engine hidden in tons of desktop, server chipsets
  By Thomas Claburn in San Francisco 20 Nov 2017 at 23:53
Intel today admitted its Management Engine (ME), Server Platform Services (SPS), and Trusted Execution Engine (TXE) are vulnerable to multiple worrying security flaws, based on the findings of external security experts.
The firmware-level bugs allow logged-in administrators, and malicious or hijacked high-privilege processes, to run code beneath the operating system to spy on or meddle with the computer completely out of sight of other users and admins. The holes can also be exploited by network administrators, or people masquerading as admins, to remotely infect machines with spyware and invisible rootkits, potentially.
Meanwhile, logged-in users, or malicious or commandeered applications, can leverage the security weaknesses to extract confidential and protected information from the computer's memory, potentially giving miscreants sensitive data  such as passwords or cryptographic keys  to kick off other attacks. This is especially bad news on servers and other shared machines.
In short, a huge amount of Intel silicon is secretly running code that is buggy and exploitable by attackers and malware to fully and silently compromise computers. The processor chipsets affected by the flaws are as follows:
     6th, 7th and 8th Generation Intel Core processors
     Intel Xeon E3-1200 v5 and v6 processors
     Intel Xeon Scalable processors
     Intel Xeon W processors
     Intel Atom C3000 processors
     Apollo Lake Intel Atom E3900 series
     Apollo Lake Intel Pentiums
     Celeron N and J series processors
Intel's Management Engine, at the heart of today's disclosures, is a computer within your computer. It is Chipzilla's much maligned coprocessor at the center of its vPro suite of features, and it is present in various chip families. It has been assailed as a "backdoor"  a term Intel emphatically rejects  and it is a mechanism targeted by researchers at UK-based Positive Technologies, who are set to reveal in detail new ways to exploit the ME next month.
The Management Engine is a barely documented black box. it has its own CPU and its own operating system  recently, an x86 Quark core and MINIX  that has complete control over the machine, and it functions below and out of sight of the installed operating system and any hypervisors or antivirus tools present.
It is designed to allow network administrators to remotely or locally log into a server or workstation, and fix up any errors, reinstall the OS, take over the desktop, and so on, which is handy if the box is so messed up it can't even boot properly.
The ME runs closed-source remote-administration software to do this, and this code contains bugs  like all programs  except these bugs allow hackers to wield incredible power over a machine. The ME can be potentially abused to install rootkits and other forms of spyware that silently snoop on users, steal information, or tamper with files.
SPS is based on ME, and allows you to remotely configure Intel-powered servers over the network. TXE is Intel's hardware authenticity technology. Previously, the AMT suite of tools, again running on ME, could be bypassed with an empty credential string.
Today, Intel has gone public with more issues in its firmware. It revealed it "has identified several security vulnerabilities that could potentially place impacted platforms at risk" following an audit of its internal source code:
In response to issues identified by external researchers, Intel has performed an in-depth comprehensive security review of our Intel Management Engine (ME), Intel Server Platform Services (SPS), and Intel Trusted Execution Engine (TXE) with the objective of enhancing firmware The flaws, according to Intel, could allow an attacker to impersonate the ME, SPS or TXE mechanisms, thereby invalidating local security features; "load and execute arbitrary code outside the visibility of the user and operating system"; and crash affected systems. The severity of the vulnerabilities is mitigated by the fact that most of them require local access, either as an administrator or less privileged user; the rest require you to access the management features as an authenticated
