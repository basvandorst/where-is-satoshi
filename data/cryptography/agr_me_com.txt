
@_date: 2013-12-01 18:14:26
@_author: Arnold Reinhold 
@_subject: [Cryptography] Email is unsecurable 
There are other ways to get trusted hardware besides bootstrapping from CMOS logic. Old PC and Macs with non-reprogrammable firmware, booting off of CD-Rs, are unlikely to be backdoored. Maybe with some research we could develop tools for auditing firmware in some more modern PCs. Old Blackberries might be modified and reprogrammed to serve as portable secure e-mail/IM devices, exchanging encrypted data with modern smartphones or PC via bluetooth.  Arduino class CPUs have little room for back doors; one could make secure e-mail/IM devices using them that one could carry in a pocket. I suggested one way to randomize FPGA CPUs in a previous post: scrambling the instruction op codes. Many open source tools are already available that could simplify the task. For example the LatticeMico32 is an open 32-bit microprocessor soft core that runs on FPGAs from several manufacturers.  All its instructions have a 6-bit op code ( so inserting a 6-bit look up table into the instruction decode might not be that hard. Note that (2^6)! ~= 2^296, so a secret scrambling of the op codes is unlikely to be brute forced. There are only a few classes of instructions in the Mico32 architecture, so it might also be possible and sufficient to just scramble the op codes within each class, without adding any new circuitry. Full open tool chains are available for this CPU, including GCC and several OSs. Adafruit sells a Mojo FPGA Development Board for $80 that includes a Spartan 6 XC6SLX9 FPGA that is capable of running LatticeMico32.  The XC6SLX9 FPGA itself sells for $18 quantity 1.  I'm not a hardware expert, but the bits and pieces for creating open trusted crypto hardware seem to be there.  Arnold Reinhold

@_date: 2013-12-12 06:44:43
@_author: Arnold Reinhold 
@_subject: [Cryptography] An alternative electro-mechanical entropy source 
My problem with the Intel design is that there is no way to audit it.  The paper Bill cited points out that there is no access to the Intel entropy generator from software in production parts. All collected entropy is processed on-chip by a complex testing and whitening circuit that includes an AES-based RNG. There is plenty of room here to hide a way to restrict the entropy of generated bits in chips made for selected customers, or via some hidden command. Such a cooked chip would produce output indistinguishable from true random bits.
The Intel design completely misses the mark, in my opinion. For cryptographic security we don't need gigabits/second, we just need a couple of hundred bits of entropy we can trust to seed a strong deterministic RNG. And more than one source of entropy, preferably of different design, should be required for any system generating cryptographic keys. Here is an idea I have been playing with to provide a slow but auditable source of entropy.
I propose combining an accelerometer chip to collect entropy with a vibration motor of the type used in cell phones. For those not familiar with the later, they consist of a small motor with an unbalanced weight on the armature. Here is a drawing of one   Sealed coin types are also available, e.g.  Accelerometer chips are available with a two-wire I2S bus for reporting data and are easy to interface to simple microprocessors. Both the accelerometer chips and the vibration motors are made in huge quantities and cost under a dollar in quantity.  They can be audited separately. The items could be mounted on the mother board, daughter board or a USB dongle.  In operation, a few seconds of accelerometer readings would be collected with the motor cycling on and off. The readings would be analyzed in software for acceptable statistical properties and then hashed to provide the the random bits. The process could be repeated at intervals to stir the RNG state.
There may well be enough mechanical uncertainty and measurement noise just in combining these two elements, but for extra credit, one could attach to either item or to the circuit board on which they are mounted a "rattle" consisting of one or two loose objects in a small box, perhaps made of clear plastic or with a clear window for visual inspection. The objects might be a ball bearing or a small pebble of gravel, say, quartz, or one of each. A pebble would provide a physically un-cloneable element. The rattle would be completely mechanical, but could be designed with solderable leads for automatic part placement machines, or it could be epoxied in place. It would be possible to immobilize the rattle with a magnet if ferrous ball bearings are used, or in a centrifuge. This could be useful for testing and it should be possible for software to distinguish the proper operation of the rattle statistically.
This entropy generator would be cheap, simple and low-tech, with little room to hide back doors. Arnold Reinhold

@_date: 2013-12-13 16:26:58
@_author: Arnold Reinhold 
@_subject: [Cryptography] An alternative electro-mechanical entropy source 
The threat I am responding to is devices with no source of randomness other than a RNG buried in the CPU,  presenting a single point of attack that is impossible to detect or prevent. It is an emerging threat as hard drives, which served as a dependable source of entropy, are phased out in favor of solid state devices, and as the internet is increasingly used to control infrastructure with diskless devices. I am suggesting that good engineering practice requires more than one source of entropy in any system that generates mission critical keys and nonces.  The problem is what to use for a second source of entropy that is independent of the CPU. Obviously at some point a system user who is not a cryptographer is going to have to trust one or more (preferably) other actors, the system vendor, consultants, in house experts, standards certifications etc. But allowing the security of a system to be completely in the hands of a CPU or SoC vendor or the vendor's fabricator is foolhardy and should be unacceptable.
So what to use as a second source of entropy in diskless systems? It has to be inexpensive if it is to have a chance of being incorporated into systems and standards and, I submit, it has to be auditable. That is the problem I am addressing with my proposal. The CPU only has to authenticate that it is talking to a working accelerometer.  That can be done in a variety of ways such as looking for orientation, measuring ambient vibration, or reporting when someone bangs on the box.  The CPU reads the accelerometer outputs, applies statistical tests as appropriate, and hashes the data to get an RNG seed. It would be very difficult for an attacker to anticipate all the ways an accelerometer can be tested and somehow put firmware inside to anticipate and spoof all attempts at verification.
I  don't see how vibration from the CPU fans can do anything but improve this entropy collection scheme. And manipulating fan speed provides yet another way to audit the accelerometer chip.  Furthermore, the kind of devices I am most worried about, the internet of things, will mostly be fan-less.
What I am proposing is the exact opposite of a TPM. TPMs are opaque devices. The components I am using are simple, mass produced and widely available. Vibration motors are easy to verify. Accelerometer chips can be tested in many ways. There is no need for persistent storage, nor should any be allowed. Crypto functionality should be provided in the CPU, or, if necessary in a separate microprocessor loaded by the CPU with open software.   I'm not sure you'd be able to hear the vibrator buzz over the noise in a typical server room. There are plenty of ways to keep the noise inside the rack and the vibrator is only need for a few second when a system is initialized and periodically thereafter. Indeed in typical server installation an accelerometer may be all that is needed. That would going in a completely wrong direction, IMHO. The problem is not a lack of ways to generate entropy, it is the difficulty of verifying that cryptographic applications are getting true entropy and not subverted bit streams. If you want entropy attributable to laws of physics, Johnson noise is more than adequate. Any quantum device will be expensive and complex, with plenty of places to hide a backdoor.  And auditing it require not just cryptographic and electrical engineering skills, but a deep understanding of quantum physics.  Meanwhile thousands of critical systems are being deployed with a single, untrustworthy RNG.  We need simple, verifiable solutions,  not advanced research projects.
Arnold Reinhold

@_date: 2013-12-16 13:04:21
@_author: Arnold Reinhold 
@_subject: [Cryptography] An alternative electro-mechanical entropy source 
I like Zener/avalanche noise as a source and have links to several designs on my web page   Here is another nice wide band design from some RF guys that I recently came across.: But I am looking for something more off-the-shelf and as inexpensive as possible. Everyone here seems to agree that the right approach is to use more than one source of entropy, but this is becoming difficult as devices are fielded with only one source, the CPU or SoC.  I'm not aware of any commercial system that has two purpose-built noise sources now. Getting vendors to include a second source will be challenging and every penny counts. Ultimately, I'd like to see a requirement for dual independent sources incorporated into industry standards like SP-800-90, but any such proposal will get a lot of industry pushback if the added expense is significant.
So I am trying to come up with several robust solutions, generators that could fit on a thumbnail-sized DigiSpark, shield for example. Someone else on this thread suggested feeding an analog output into an analog input and relying on the low-order-bit measurement noise.  I'm not sure how much to trust that approach, but it's should easy to try. I'm thinking of including a thermistor in the voltage divider to get some physical variability that can be tested.  But accelerometers seem the best so far. Thy are cheap, widely available from several manufacturers and are easy to interface. They could also serve a second purpose as an tamper attempt detector. One could even afford to include two accelerometers from different manufacturers and compare their outputs as a check.  Arnold Reinhold

@_date: 2013-12-19 07:56:36
@_author: Arnold Reinhold 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and 
How do we safely initialize Yarrow or a another software RNG if the CPU's hardware RNG is compromised and there is no other source of entropy? This is a situation that is increasingly common in all solid-state black box devices, and is especially tricky at first startup, when keys used to manage such units are often generated.
The model I envision for a cooked CPU RNG seeds a deterministic RNG at power up with a secret constant and a small amount of entropy, maybe 30-40 bits.  This gives an attacker with knowledge of the scheme a relatively small search space to find any random bits generated, especially during initialization, while preventing easy detection. One approach to ameliorating the situation would be to apply a key stretching algorithm on the random bits obtained from the CPU RNG prior to using them to seed or stir Yarrow. A variant would be to use Yarrow itself as the key stretcher by seeding from the CPU RNG and then generating and discarding a large number of random bits (several seconds worth, say) from Yarrow before any production use. This would be about as effective as PBKDF2 and easier to implement, but it would be much better to use an algorithm like scrypt that employs more memory and CPU resources.  Such an algorithm would increase the cost of key recovery when using massively parallel hardware to search the restricted entropy space.  Note that unlike traditional key stretching, where it is important to have reproducible outcomes among different machines, here we can use expensive CPU capabilities like 80-bit floating point arithmetic that may available only in certain architectures or have minor variations between CPU implementations.  While key stretchers cannot make a cryptographically weak system impregnable, they can greatly reduce the practicality of attacks. A key stretcher that increases the time to test a trial key from, say, a millisecond to ten seconds increases the work by a factor of 10,000 or ~13 bits. That could be enough to prevent en-mass probing and might force the attacker to reduce the real entropy used in the cooked RNG, making it more detectable.  While I still think a second, auditable source of randomness should be required in any secure system, that is not happening in the near future and an all software solution like key stretching can be implemented today.
Arnold Reinhold

@_date: 2013-12-19 08:19:13
@_author: Arnold Reinhold 
@_subject: [Cryptography] The next generation secure email solution 
2-D bar codes like the QR-code might be a good solutions for this.  Consumers are already used to seeing them and (to a lesser extent) scanning them. They can be printed on business cards, stationary, bank statements, direct mail advertising and even billboards.  Banks and other institutions could display their QR-code in their lobby. A long time ago i suggested that PGP users post a copy of their key fingerprint on their front door or in a window nearby. A QR-code of the fingerprint would work even better.  A lock or key logo next to a key verification or distribution QR-code would be enough to identify it and any next generation security software should have an option to scan one.  Arnold Reinhold

@_date: 2013-12-20 15:46:32
@_author: Arnold Reinhold 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
Sandy Harris mentioned one more: ftp://ftp.cs.sjtu.edu.cn:990/sandy/maxwell/
I am not able to evaluate these RNG schemes that rely on uncertainties in CPU timing. They may well provide a solution, but I am not prepared to bet that people who can modify CPU innards, can't find a way to defeat them. In section 2.1 of your  you point out some possibilities, but say that if an attacker has that much access to the CPU, they have other means of compromising your system. That may not be true. The beauty of a compromised CPU RNG is that it does not require any covert communication from the compromised system back to the attacker.  As others have said, a billion chip CPU is impossible to audit. Adding what I proposed, a key stretcher in the initialization chain of an OS RNG like Yarrow or /dev/random, is very simple to do and need only add a few seconds to first boot up.  The best solution however is still an auditable source of randomness independent of the CPU.
Arnold Reinhold

@_date: 2013-12-24 00:21:29
@_author: Arnold Reinhold 
@_subject: [Cryptography] Why don't we protect passwords properly? 
I must respectfully take exception to the above. The possibility of mass off-line attacks was anticipated from the very beginning. The original Cyrpt (3) circa 1978 was designed to prevent such attacks. I proposed a KDF called HEKS that was designed to use large amounts of memory and computing resources in order to defeat hardware attacks on hashed passwords way back in 1999. It's referenced in the scrypt paper. And rainbow tables are indeed available and in use against password hash systems that do not employ salt, including Microsoft's notorious LM hash, straight MD5 and SHA1, etc. A quick Google search got me to freerainbowtables.com which offers some 9 terabytes of rainbow tables via torrents for your favorite low sodium hashes.
While the best hash attack methods in use today do involve novel ideas, the possibility of using mass parallel attacks in hardware and software has been on the minds of cryptographers since the early controversies about DES in the mid-1970s. That underling threat is what drives most innovations in cryptography. We are always looking ahead to be ready for processing breakthroughs. So why the lack of attention to KDFs? If one tenth the effort to replace SHA-2 had been devoted to improving password storage, the benefits to industry and the public would be far greater than anything we can expect from SHA-3.  While I'm glad the hear that there is at last a password-hashing competition (password-hashing.net), scrypt is available now. As long as an algorithm identifier is included in a password database, it's easy to substitute a better algorithm when it comes along. And is there any cryptographer out there who knows the algorithm and believes that scrypt could be weaker than PBKDF2? Seriously?
Arnold Reinhold

@_date: 2013-12-24 16:36:58
@_author: Arnold Reinhold 
@_subject: [Cryptography] Passwords are dying - get over it 
A 10 word Diceware? password has 129 bits of entropy.  Not trivial to memorize, but easier than your 13 words, and the individual words are shorter on average as well, e.g.:
   field mint flue elk hock paris 1990 ax quake sutton
Key stretching is not just about rounds. It can also engage more of the transistors on a typical client or server computer, forcing an hardware attacker to use more silicon area for each attack pipeline.  A factor of a million in transistor count over a simple SHA hash is not unreasonable, and coupled with a million iterations, one could get to a 40-bit increase in attack cost. But even 30 bits of key stretching gain gives a 4 word Diceware passphrase 81-bit strength and  5 words 94 bits. You get 120-bits with 7 Diceware words and 30 bits of stretching, close enough to full 128-bit strength, and three words fewer than are needed without any key stretching, e.g.:
   hamlin jig cub naiad frey allyn pig
Those three fewer words can make the difference between a passphrase that an ordinary person can remember and an burden most will shun. The vital role key stretching plays can be thought of as impedance matching crypto security systems to human memory capabilities.
Arnold Reinhold

@_date: 2013-12-25 14:29:20
@_author: Arnold Reinhold 
@_subject: [Cryptography] Why don't we protect passwords properly? 
You forgot the most important criteria, parameterizable to not execute in parallel on modern GPUs. That is the threat today, not side channel attacks.  Scrypt can claim that, PBKDF2 can't.  I'm not aware of any side channel attacks on even individual stored passwords, much less mass attacks, but mass breaches of poorly protected stored passwords have become common, resulting large economic losses.
If you are really concerned about side channels, note that scrypt begins with a PBKDF2 call, so increasing its net iteration count to the typical levels in use today would provide the same protection against side channel attacks we now enjoy while thwarting the active problem of GPU attacks on purloined password hashes, including those by bots using thousands of consumer GPUs. I hope the current KDF competition comes up with better solutions, but that is no excuse for failing to provide strong protection for user passwords today using the technology we already have. As always, the best is the enemy of the good.
Arnold Reinhold

@_date: 2013-12-25 15:40:54
@_author: Arnold Reinhold 
@_subject: [Cryptography] Passwords are dying - get over it 
Hamlin and Allyn are proper names. Many short name are included in the Diceware(tm) list to keep the average word length low. Other word lists are possible of course. And looking up an unfamiliar word can be an aid to memorization. The "never write down your password" stricture has been widely debunked. Most people need dozens of passwords. It is unreasonable to expect users to memorize more than a few high strength passwords, perhaps just their hard drive and password manager master passwords. Strong KDFs would help for both uses.
24 Dec 2013 Jonathan Thornburg asked:
There is no theoretical basis for the "at least 4 bits of entropy per character" you claim. People following the advice you cite are likely to use phrases in published works, such as books or songs, with predictable variations. The most common examples, such as popular quotes and lyrics, are likely already in password cracking tables. Also the initial characters in English words are even less uniformly distributed than English text in general. The entropy in Diceware word selection is a demonstrable 12.9 bits per word assuming a strong source of randomness, such as dice, is used. Diceware users are not asked to think up something unpredictable. It is well established that people are lousy at that.
I just posted to my blog (diceware.blogspot.com) a different approach, which is to generate a string of 10 random letters and then make up a mnemonic sentence that has those letters as its initial letters, using a simple table.  The sentences can be a bit wonky, and 10 random letters have only 47 bits of entropy, but with a good key stretcher that could be enough for may uses. Here is an example:
For more entropy, one can insert random numbers before the noun clauses, e.g.
Four random digits adds 13 bits of entropy, bringing us up to 60 bits. If still more is needed, one can use two sentences, or come up with a scheme for something longer, like a random poem or haiku.
Again, the random string is the password, the sentence is just an aid for memorization. Password formats are a matter of taste. It's good to have more than one strong option.
Arnold Reinhold

@_date: 2013-12-26 15:29:24
@_author: Arnold Reinhold 
@_subject: [Cryptography] A modification to scrypt to reduce side channel risk 
On Dec 25, 2013, at 2:51 PM, Kriszti?n Pint?r  wrote (responding to me):
But PBKDF2 is not transistor hungry. That's the problem. Rather than continuing to argue that side channel attacks are not as serious as the primary attack, I'd like to propose a way to minimize that risk. Here is the summary of scrypt from the draft RFC (tools.ietf.org/html/draft-josefsson-scrypt-kdf-00):
The B[i] are large memory blocks, one for each of p parallel processes, each 128*r octets. N is the cost parameter. DK is the derived key.
I propose replacing P, the password or passphrase, in step 1 with the null string. In other words all the memory banging in step 2 would be determined solely by the salt, S. The password would only affect the algorithm output in step 3, which should be sufficient for security. Since the salt is not a secret, there is nothing in memory, timing, acoustic signatures, power consumption, etc.  during steps 1 and 2 that provides any clue to the password. Step 3 is just PBKDF2 which is apparently considered safe from side channel attack and is, in any case, the standard of comparison here. This is in contrast to scrypt where knowledge of a few bytes of one of the B[i] captured at some stage could serve as an oracle for the password.
An attacker who can capture the entire state of the algorithm at some point during step 2 can bypass the work done up to that point, but still has to reverse step 3. Note that running PBKDF2 with a very large amount of pseudo-random data, even if known, is transistor-expensive. If desired, the iteration count in step 3 can be increased from 1 to a value that balances the work budgets in steps 2 and 3 against the different threats. Unless an attacker who captures the entire state of the algorithm during step 2 can then carry out their attack on the same machine, they also would have to communicate the entire state information to a machine they control. Anything less that all the B[i]'s minus a few bytes is useless. Carrying out the attack on the same machine would also require contemperanious knowledge of the output of step 3, which is not available to side channels due to the presumed strength of PBKDF2. Transmitting the state information in anticipation of a future leak of the password database would consume vast amounts of bandwidth that is hard to conceal. The salt would have to be big enough prevent precomputed exploded salt tables, but given the large amount of information that would have to be stored for each salt, 64 bits is likely enough. 80 bits would have a good margin of safety.
It seems like this simple modification to scrypt would address the concerns expressed by many on the cryptography list and is clearly at least as strong as PBKDF2 at the work level expended in step 3. Am I missing something?
Arnold Reinhold

@_date: 2013-12-26 20:33:53
@_author: Arnold Reinhold 
@_subject: [Cryptography] A modification to scrypt to reduce side channel 
You are correct. I should have known better.  We are back to discussing use cases where the benefits outweigh the risks.
Using a hash with a larger memory footprint could help. Also the SHA3 finalists were ranked by hardware execution speed, with Kaccek being fastest. Perhaps the candidates with the worst hardware implementation performance relative to software should be reconsidered for use with PBKDF2.
Arnold Reinhold

@_date: 2013-12-31 17:54:44
@_author: Arnold Reinhold 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
============================== START ==============================
The NSA's horse has already left the barn. The U.S. Government's first priority should be to do this at our nuclear weapons labs, before our bomb design codes (in Fortran, no doubt) wind up on BitTorrent. I would thought they'd have learned their lesson after Private Manning. How many times does the bell have to ring?
(And if anyone from NSA is reading these pearls of wisdom, a Happier New Year and pass this on to your boss.)
Arnold Reinhold

@_date: 2013-11-08 07:06:02
@_author: Arnold Reinhold 
@_subject: [Cryptography] suggestions for very very early initialization 
CD-ROMs have a big advantage over USB flash drives: they are physically unmodifiable. The read-only partition on your USB drive is enforced by system software that can be compromised.
On the other hand, the systems on which live CDs are typically used provides plenty of sources of entropy: sound input, keyboard interaction, often a webcam. Indeed, the process of booting off of a CD should provide plenty of mechanical timing entropy.
The real problem is the large number of cheap disk-less devices appearing on networks that will control everything from toasters to power plants. For security, these need a source of randomness unique and undiscoverable for each device. The hooks need to be built into the OS, but this problem can't be solved by software alone. Unless there is some standards setting body (FIPS, UL, EPA, OU, etc.) that requires such systems include a built in RNG or some other seed provisioning mechanism, it won't happen. Worse, we are likely to see reliance on supposed TRNGs built into CPUs or SoCs that are easily back-doored in undetectable ways. A major problem in this regard is the inclusion of hardware whiteners in such designs that cannot be bypassed. This makes it impossible to apply tests to the TRNG noise source.  NIST SP-800-90 currently allows such designs and then suggests statistical tests on the output, which a good whitener (e.g. AES in the x86 design) can easily pass regardless of the underlying entropy.  This standard is under review and needs to be fixed in this regard.
Arnold Reinhold

@_date: 2013-11-08 11:24:32
@_author: Arnold Reinhold 
@_subject: [Cryptography] suggestions for very very early initialization 
It seems you can get CD-ROMs pressed in quantities of several hundred for about a buck apiece. So this option is not out of the question. And an SHA256 hash of each disk would insure faithful reproduction. But I agree with your next sentence.
The attack I am concerned with is a remote attack that, say, exploits a zero-day and roots or inserts a backdoor in your operating system. Maybe you opened an e-mail or visited a web site you shouldn't have. The attacker does not want to lose access when you reboot. That attack seems feasible with a USB flash drive or CD-RW, but not with write-once CD or DVD media. I don't claim to be an expert in CD/DVD technology, but my understanding is that the optical drive firmware prevents overwriting and cannot be re-flashed from the computer the drive is attached to. The RIAA/MPAA would never allow that as it could be used to bypass DRM. One also has the option of using an optical drive no write capability on one's secure machine.  Such drives can still be purchased, and there are many old PCs gathering dust in basements with this "feature."
A large organization that gained physical access to your CD or DVD and wanted to modify it would more likely replace it with a different disc and use their forging skills to make the outside look identical, rather than trying to alter bits on your original. (Even more likely they would just modify your computer in some nefarious way.)
Building secure systems requires some starting point of trust and read only media is very attractive in this regard. It is certainly worth some effort to get authoritative answers to the questions you rase.
Arnold Reinhold

@_date: 2013-11-08 16:19:39
@_author: Arnold Reinhold 
@_subject: [Cryptography] suggestions for very very early initialization 
Right, but that is a user option. Presumably someone writing a LiveCD/DVD would not elect to make it multi-session.
Arnold Reinhold

@_date: 2013-11-11 15:30:02
@_author: Arnold Reinhold 
@_subject: [Cryptography] randomness +- entropy 
Per the above, it seems to me that some thought should be given about the advisability of logging instances where a PRNG is seeded before sufficient entropy is collected. It's at least conceivable that the logs will not be protected as tightly as the PRNG state (logs might be collected and sent to a compromised central server, for example), so an attacker might be able to examine the logs of many nodes on a network to find the few whose PRNGs are poorly seeded and focus his resources on breaking them.
Arnold Reinhold

@_date: 2013-11-20 12:39:22
@_author: Arnold Reinhold 
@_subject: [Cryptography] Moving forward on improving HTTP's 	security 
One of my pet ideas is to build an FPGA-based CPU with an S-box in its instruction decoder, so each instance of the CPU would have a different instruction set. Ideally the CPU would have a fixed size op code field in the instruction format, so any instruction could have any op code, and an op code space bigger than the number of operations implemented.  The S-box would be populated when each FPGA was loaded and the binary image of its software would be translated for that instance's S-box at the same time. A remote attacker would have no way to create a binary module that the CPU would execute. Of course one would have to make sure the software did not include a high level interpreter that could execute malware. Software upgrades would involve reprogramming the FPGA with a new random S-box, unless records were kept of the fielded S-boxes. If peripheral support circuits were implemented on the FPGA as well, there would be few places for malicious silicon to hide. Arnold Reinhold

@_date: 2013-11-27 13:18:33
@_author: Arnold Reinhold 
@_subject: [Cryptography] Email is unsecurable 
With the maker movement, open hardware, Adruino, et al, the barrier to entry for hardware has dropped dramatically. Crowd source funding is available. Someone mentioned DigiSpark on this list and I bought the full development package for $15 at my local computer store. The boards themselves cost $9 each and have a built in USB connector.  The Attiny85 SoC chip it uses costs $1.29 quantity one. It has 6kb of available flash memory, plenty for RC4 with improved key scheduling and a small key book. One could possibly shoe horn in AES (see eg   Other tiny, inexpensive single board systems (eg RFduino) are available with low power Bluetooth 4.0 that could sit in one's pocket and talk to a smartphone, encrypting e-mail or, what people really care about, IM. Other inexpensive SoC chips are available with more memory that could potentially do public key. Simple hardware systems have less space to hide backdoors.  I don't want to dump on the people trying to improve existing e-mail protocols and infrastructure, but maybe we should explore different, simpler paths at the same time. Arnold Reinhold

@_date: 2013-11-27 20:00:31
@_author: Arnold Reinhold 
@_subject: [Cryptography] Email is unsecurable 
I always assumed the NSA/FIPS bias favoring fast hardware implementation of crypto algorithms over software efficiency was to give cryptanalysis, presumably using massively parallel cracking arrays, an edge. But regardless, a lot has has changed since early software crypto systems for personal computers, such as PGP, were introduced. Back then CPU chips were much simpler: the Intel 8086 in early PCs had fewer than 30,000 transistors vs 1.4 billion in the latest Haswell processors. PC firmware was provided in unalterable ROM chips, not rewritable flash memory. E-mail was text-only and the idea it could be used to transmit viruses was considered ludicrous.  Operating systems were much smaller and did not feature a plethora of hooks, like scripting languages and plug-ins, for malware writers to exploit. All this has changed in the name of progress, giving us more versatile and useful devices. But  that progress has created a security disaster, with vastly more places to inject and hide malware and backdoors. It has become extremely difficult for experts to have confidence they control their own computers nearly hopeless for the general public. John Kelsey asked:
The answer, of course, is that we must have trustworthy software running on trustworthy hardware.  Either without the other is worse than useless, by creating a false sense of security. My current nightmare is the future "Internet of Things" where major pieces of infrastructure are run with minimalist devices that have no source of entropy other than the RNG instruction in a billion transistor CPU.  As has been shown, whoever controls the masks in the CPU fabrication process can to silently subvert the RNG process, allowing them take control of anything they want. The best software imaginable cannot prevent that.
We need to ask the question: what will it take today and in the future to get hardware we can trust?
Arnold Reinhold

@_date: 2013-10-02 13:42:17
@_author: Arnold Reinhold 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - 
This would be the 1981 Kansas City Hyatt Regency walkway collapse ( where 114 people died, a bit more than several. And the "take-away" included the fact there there were no architectural codes covering that particular structural design. I believe they now exist and include a significant safety margin.  The Wikipedia article includes a link to a NIST technical report on the disaster, but NIST and its web site are now closed due to the government shutdown. The concept of safety margin is a meta-design principle that is basic to engineering.  It's really the only way to answer the questions, vital in retrospect, we don't yet know to ask.  That nist.gov is down also keeps me from reading the slide sets there on the proposal to change to SHA-3 from the design that won the competition.  I'll reserve judgment on the technical arguments until I can see them, but there is a separate question of how much time the cryptographic community should be given to analyze a major change like that (think years). I would also note that the opinions of the designers of Keccak, while valuable, should not be considered dispositive any more than they were in the original competition.  Arnold Reinhold

@_date: 2013-10-07 11:45:56
@_author: Arnold Reinhold 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
If we are going to always use a construction like AES(KDF(key)), as Nico suggests, why not go further and use a KDF with variable length output like Keccak to replace the AES key schedule? And instead of making provisions to drop in a different cipher should a weakness be discovered in AES,  make the number of AES (and maybe KDF) rounds a negotiated parameter.  Given that x86 and ARM now have AES round instructions, other cipher algorithms are unlikely to catch up in performance in the foreseeable future, even with an higher AES round count. Increasing round count is effortless compared to deploying a new cipher algorithm, even if provision is made the protocol. Dropping such provisions (at least in new designs) simplifies everything and simplicity is good for security.
Arnold Reinhold

@_date: 2013-10-08 18:10:42
@_author: Arnold Reinhold 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
At least in the Intel AES instruction set, the encode and decode instruction have access to each round key except the first. So they could leak that data, and it's at least conceivable that one can recover the first round key from later ones (perhaps this has been analyzed?).  Knowing all the round keys of course enables one to decode the data.  Still, this greatly increases the volume o data that must be leaked and if any instructions are currently "spiked," it is most likely the round key generation assist instruction. One could include an IV in the initial hash, so no information could be gained about the key itself.  This would work with AES(KDF(key+IV)) as well, however. The fact that the round keys are simply xor'd with the AES state at the start of each round suggest this likely secure. One would have to examine the KDF to make sure the there is nothing comparable to the related key attacks on the AES key set up. The comparison would be to AES(KDF(key)). And in how many applications is key agility critical?
Given multi-billion transistor CPU chips with no means to audit them, It's hard to see how they can be fully trusted.
Arnold Reinhold

@_date: 2013-10-18 10:15:14
@_author: Arnold Reinhold 
@_subject: [Cryptography] please dont weaken pre-image resistance of 
Let's think for a moment about users who design to 256-bit security. There is nothing currently that comes close to compromising 128-bit systems. A trillion processors each testing a trillion keys a second would take 6 million years on average to recover or forge just one 128-bit key. Any rational choice for 256-bit security is seeking to protect data far into the future, against threats currently unknown or only imagined, like quantum computing, DNA processing, super algebra systems or other mathematical breakthroughs. How important is performance to such users? I submit such users want primitives with large margins of safety. Does larger internal state delay any quantum attack? Certainly. Does larger internal state complicate attacks on entropy collectors? Indeed. We've only had a few years to look at Keccak-like systems. Weaknesses that revealed less-than-nominal strength in other primitives have emerged after longer intervals. Those who express conservative instincts are being not foolish here.
Arnold Reinhold

@_date: 2013-10-31 09:05:15
@_author: Arnold Reinhold 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
The beauty of an RNG attack is that it does not require any communications back to the attacker, unlike the other attacks you mention.  Such back communications can arouse suspicion.  And done right, an RNG attack does not introduce any insecurity in the attacked system that others can exploit. NSA may want to monitor Angela Merkle's traffic without making it easier for Russia or China to do so, for example. Arnold Reinhold

@_date: 2014-04-02 13:39:23
@_author: Arnold Reinhold 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve 	numbers 
Sounds like what you want is not a classic "nothing up my sleeves" number, which may indeed have polynomially recognizable underlying structure, but something truly random. Doing a "Hail Mary pass" of combining everything under the sun leaves open the possibility of selecting a set of choices with some hidden property.  I would suggest digesting random bits from one of the many large, publicly available astronomical data sets. The NASA Solar Dynamics Observatory (SDO) images of the Sun seem like a good candidate, particularly the AIA 304 images which show a rotating boiling mass. Maybe select one image per day  going back in time from April 1, 2014 and feed them into Keccak/SHA3-512. Then take the output after each image as your final bits until you have enough.
Arnold Reinhold

@_date: 2014-04-20 16:19:03
@_author: Arnold Reinhold 
@_subject: [Cryptography] GCC bug 30475 (was Re:  bounded pointers in C) 
The events described in the above linked Bugzilla thread regarding Bug 30475, as I read it, is that the GCC team was informed that the GCC complier in its common mode of operation is, without any warning, removing safety checks that have inserted in wide variety of existing programs; that the safety checks were inserted by competent programmers who were unaware of any potential problem with their use; that the safety checks, if left in place, would be functional and could avert serious security lapses; and that it is not feasible to find all the instances of these checks and apply proposed workarounds in a reasonable amount of time and effort. The GCC team does not challenge these assertions but instead claims that the C language specifications say that the behavior of the language in these circumstances (signed arithmetic overflow) is undefined, and that they are therefore permitted (not required) to remove the safety checks. They make it clear they have no intention to do anything to eliminate or mitigate the resulting safety issues. While the report stems from 2007, the bug was closed and marked ?Resolved as fixed.? on February 16, 2014.
I am not a lawyer, but I remember well the briefing I got before serving on a automation system safety review board.  As I recall the law, everyone has a basic duty of care when made aware of a hazardous situation. Engineers in particular are expected to use reasonable diligence and good judgment in exercising their professional skill. The test for negligence is what a ?a reasonable person? would be expected to do in the same circumstance. Here, for example, is how New York State Penal Law ?15.05 ( defines things:
3. "Recklessly." A person acts recklessly with respect to a result or to a circumstance described by a statute defining an offense when he is aware of and consciously disregards a substantial and unjustifiable risk that such result will occur or that such circumstance exists. The risk must be of such nature and degree that disregard thereof constitutes a gross deviation from the standard of conduct that a reasonable person would observe in the situation.  ?
4. "Criminal negligence." A person acts with criminal negligence with respect to a result or to a circumstance described by a statute defining an offense when he fails to perceive a substantial and unjustifiable risk that such result will occur or that such circumstance exists. The risk must be of such nature and degree that the failure to perceive it constitutes a gross deviation from the standard of care that a reasonable person would observe in the situation. Again I am not a lawyer, but it is hard for me to imagine anything more reckless than allowing a compiler to silently remove safety checks in generated code for any reason, much less the modest performance gains being cited. And if anyone were killed or injured as a result of these checks being removed, this Bugzilla thread alone would be enough to convince me that a substantial and unjustifiable risk had be consciously disregarded. I doubt a lay jury would have more sympathy.
In my opinion, the GNU Project and the developers of GCC would be well advised to get legal advice on their responsibilities and liabilities in this matter. Arnold Reinhold

@_date: 2014-04-23 10:06:05
@_author: Arnold Reinhold 
@_subject: [Cryptography] GCC bug 30475 (was Re:  bounded pointers 	in C) 
The IBM 360 is 50 years old, the PDP-11 is 44. As you suggest, due to their influence twos-complement has become the de facto standard for binary signed integer arithmetic. The C standards have been revised several times since.
The leap from "overflows may or may not be trapped" to "the compiler can generate any evil code it want whenever it thinks it sees a possible signed integer overflow" is simply astounding. Criminal in my opinion.
As you say, C is a close-to-the-metal language. That is a major reason for its popularity, particularly with embedded systems. If translated into machine language in the naive way, the assert statement's test will do just what its author intended on the vast majority of computers out there.  If, on some architecture I am not familiar with, it generates a false abort, the assert will still have done its job in alerting the programmer to a potential problem. The only bad case I can see is if, on our outlier architecture, it allows a bad operation to pass. But that is exactly what removing the assert allows on ALL architectures. That of course is a common problem with no universal solution, but one should err on the side of safety, not blissful ignorance. Maybe your management, if your are writing mission critical, or life safety code, might see it differently. Most of us write code expecting to catch all bad conditions.  Assertions are backups in case we missed something. They are helpful when the concern is accidental oversights. They are vital when the concern is active attacks, where a clever attacker might find a way to generate a condition that the compiler considers impossible. That can be pretty subtile. See e.g.  The revelation that NSA has been working to weaken publically-available computer security has generated a hunt for possible instances where this has happened. I prefer stupidity and arrogance to conspiracy as an explanation for failures, but it is hard to imaging a a more productive win for the state security snoops than compilers that remove safety tests. Most programming is done by mere mortals.  And program maintenance is rarely assigned to top tier coders. Source code, publicly available or purloined, for targeted programs can be fed through an instrumented compiler to find instances where safety tests are removed and these can then be analyzed further for exploitable weaknesses. This bug/feature of C compilers not just an exploit, its an exploit generator, a gift that keeps on giving. Know your tools, indeed.
Arnold Reinhold

@_date: 2014-04-27 06:55:14
@_author: Arnold Reinhold 
@_subject: [Cryptography] GCC bug 30475 (was Re:  bounded pointers 	in C) 
In the situation I was positing, someone killed or seriously injured because GCC removed a safety test, it is my understanding that commercial wavers like that are no defense against criminal prosecution. Even in civil litigation, their enforceability is limited and the situation in the U.S. varies by state. Disclaimers are generally enforceable as part of a conscious contract between knowledgable parties of comparable bargaining power, but most states do not allow a party to limit their liability for gross negligence. Members of the general public, who depend on numerous pieces of software written in C but have never heard of GCC nor seen their disclaimers, may not be bound by them.
Again I am not a lawyer; my only advice is to talk to one before you assume disclaimers like the one you quoted will shield you against any consequences of your software development activities, particularly in the security area.
Arnold Reinhold

@_date: 2014-04-29 19:56:25
@_author: Arnold Reinhold 
@_subject: [Cryptography] GCC bug 30475 (was Re: bounded pointers in 	C) 
We don?t have to speculate about what the developers of GCC were told and how they reacted. It is documented in the GCC Bugzilla thread for 30475. The C standards committee, as far as I know, only said that signed integer overflow was undefined. They never required that code which could create such an overflow, and any code dependent on such an operation, be removed. Saying an operation is undefined is not a license to replace it with any evil code the compiler developers wish. Do you think it would be alright to replace such code with a RDrand instruction and abort the program if the random value was 17 modulo 100,000? or have all such code crash at noon on April 1 next year? or call a routine that searches memory for private keys and sends them to some url? It is every bit as irresponsible to silently remove safety checks placed in software by programmers who may not have understood all the implications of ?undefined? or did not fully grasp the arcane techniques needed to write ?legal? safety checks. (Other postings in this thread show just how difficult that is and how even very competent programmers can fail the test.)  In no context is it acceptable to knowingly remove or defeat someone else?s safety mechanisms without permission, much less without warning. The fact that the work was performed for free does not alter that. If you open a free food kitchen to feed the poor, your local health department will still insist on your following safe food preparation guidelines. If you give kids from the homeless shelter a free ride on your yacht, you better have enough life preservers. If you decide to provide free WiFi in a housing project, you can?t remove the smoke detectors and use their wiring to hook up your access points.
Books and patent documents don?t remove safety checks by themselves. Nor is it an excuse to say no one has to use C. Billions of people now rely on software written in that language. They have no say or knowledge as to which language the software was written in, what compiler was used or what optimization options were enabled. And yes, there are situations were software bugs can kill or cause great injury. Product liability lawsuits regularly target the supplier of defective tools alleged to have contributed to the injury, especially if they were previously warned of the problem and ignored it. And what is the benefit that supposedly justifies the removal of these safety checks? A speed improvement of a couple of nanoseconds per check removed?  Arnold Reinhold

@_date: 2014-04-30 08:41:11
@_author: Arnold Reinhold 
@_subject: [Cryptography] GCC bug 30475 (was Re: bounded pointers in 	C) 
As I pointed out there are a lot of evil thing the compiler could do with undefined code that are not prohibited by the C standards. That does not mean it is ok to do them. There are other standards for human behavior besides those written by programming language committees.
Why would you want the compiler to silently remove that code? If it wasn't an overflow check, maybe you made a typo--when you wrote that line you had something in mind.  Wouldn't it be better for the compiler to flag that statement so you could review it and either correct it or remove if from your source file? Why would you want a nonsense statement to remain in your program? That horse has left the barn long ago. Thousands of security sensitive programs written in C are in use today. And programmers don't often have the luxury of deciding what language to use. Often security is only a small part of a larger project and the choice of which language to use for a project has been decided for other reasons. Isn't it more reasonable to expect the developers of C to respond to the safety needs of their users and the public?
Well, to begin with Bug 30475 was about an assert statement and these are pretty much always safety checks. I realize that assert is a macro, but in principle there is no reason the compiler can't be aware of this. But not all safety checks are written as asserts. A program abort may not be the safe response to an error. So yes, I am saying  that compilers should never silently optimize undefined behavior. An optimizer is supposed to generate code that produces the same output as the original program, but more efficiently. If a behavior is undefined, how can the optimizer conclude that the output will be the same? I would think undefined behavior is the last thing I would want an optimizer to touch. Do you have nay evidence for this claim? How many statements with undefined behavior do you think are in a typical program? How many nanoseconds would we waste if the compiler just left them alone?
Arnold Reinhold

@_date: 2014-08-08 16:55:59
@_author: Arnold Reinhold 
@_subject: [Cryptography] All dice are loaded? 
The fact that inexpensive consumer dice are biased towards higher numbers is well known. Casinos employ special "Casino dice" that have the pips painted on instead of dimpled in. They are manufactured to higher tolerances and are transparent to make it easier to detect any "loading". Experimental evidence suggest they are fair, e.g.:   However biases that might be important to a casino have little effect on using dice to generate random quantities for cryptography.
The entropy of a single dice roll is the sum from k=1 to 6 of -pk log2 (pk), where pk is the probability of k being the top face in a roll. For a perfect die, pk = 1/6 = 0.1666666... for all k, so that sum reduces to -log2 (1/6) = log2 (6) = 2.5849625...
Lets say we have dice where side 6 comes up with a probability of 1/4 or 0.25, and the other five sides have equal probability of 0.15. That is a pretty big bias, much bigger than the biases reported for cheap consumer dice.
For our 1/4 biased dice, the entropy of each roll is -0.25*log2 (0.25) - 5*0.15*log2 (0.15) = 2.552724..., a difference of about 0.03224 bits per roll. For a six word Diceware passphrase, which requires 30 dice roll to generate, the difference is just under one bit. Thus a 6-word passphrase generated using the loaded dice would have an entropy of about 76.4 bits instead of 77.4 bits with perfect dice.
If this effect still concerns you, I list several sources of casino dice on my Diceware FAQ at  Feel free to pass this on the the Australian newspaper.
Arnold Reinhold

@_date: 2014-08-11 14:10:51
@_author: Arnold Reinhold 
@_subject: [Cryptography] All dice are loaded? 
My bad. Here is the correct link to my list of Casino Dice suppliers: Arnold Reinhold

@_date: 2014-08-24 13:52:58
@_author: Arnold Reinhold 
@_subject: [Cryptography] On 40-bit encryption 
The Bernstein case only affected the publication of source code as a form of speech, not the sale complete encryption products like browsers and operating systems. I doubt it was the only factor. The Clipper Chip gambit had failed, banks were starting to demand "128-bit browsers" and there were many voices calling for liberalization. The Cato Institute asked me to write a position paper, "Strong Cryptography: The Global Tide of Change," but it came out just days before the Clinton administration announced its new policy. Maybe the full story will come out some day in one of the NSA history series. My guess is that wiser heads at NSA, knowing from vast experience that strong ciphers were not in themselves enough to guarantee secure communications, began to realize what a bonanza the Internet could become for them and shifted gears to more subtle ways of insuring access to content while beefing up their collection, storage and traffic analysis capabilities.
Arnold Reinhold

@_date: 2014-02-02 21:34:05
@_author: Arnold Reinhold 
@_subject: [Cryptography]  Mac OS 10.7.5 Random Numbers 
Based on the Darwin source code posted at the xnu project, Apple uses the SHA1 version of Yarrow with the 1999 source code from Counterpane essentially unchanged. This give them a 160-bit secret state. An obvious improvement would be to switch to SHA2 or SHA3 with a 256 or 512 bit state, but the Apple source contains this warning:
"THIS FILE IS NEEDED TO PASS FIPS ACCEPTANCE FOR THE RANDOM NUMBER GENERATOR.
IF YOU ALTER IT IN ANY WAY, WE WILL NEED TO GO THOUGH FIPS ACCEPTANCE AGAIN,
AN OPERATION THAT IS VERY EXPENSIVE AND TIME CONSUMING. IN OTHER WORDS,
DON'T MESS WITH THIS FILE."
Arnold Reinhold

@_date: 2014-02-04 16:30:37
@_author: Arnold Reinhold 
@_subject: [Cryptography] Mac OS 10.7.5 Random Numbers 
NSA requires 256-bit AES keys for top secret. Assuming they have some good reason, a user who believes their data deserves the same level of protection and naively generates such a key using the 160-bit state RNG will not get the security they expect. There is a long history of crypto primitives becoming obsolete as technology and crypto analysis progresses. So new primitives are developed from time to time and old ones depreciated. Ideally the depreciated primitives should be replaced well before there is any exploitable weakness. The validation process should not be an obstacle to such deployment. Perhaps there should be an expedited validation for recommended upgrades or maybe validations should have a limited life time to force periodic review. In this case it seems that Apple picked wisely back in 1999 when it selected Yarrow and got a solution that still offers good security after 15 years, but I think their users would be better served if they made a simple upgrade. Yes, I know they have tons of cash, but there is always competition for budget. I wouldn't doubt their technical people have suggested such a change and some manager vetoed it on the grounds they are still FIPS-140 certified. That's a problem. Arnold Reinhold

@_date: 2014-02-09 12:36:26
@_author: Arnold Reinhold 
@_subject: [Cryptography] Entropy Attacks! 
D. J. Bernstein has proposed an attack on RNG schemes that use a hash algorithm to combine output from several entropy sources. (*2014.02.05: Entropy Attacks!,* on The cr.yp.to blog) I believe his attack can be thwarted by using a key-stretching hash to combine the outputs. Bernstein showed that, under certain circumstances, one specially crafted rouge entropy source can control, to a limited extent, the final output of the combining RNG. To successfully mount the Bernstein attack, the rogue entropy source must be the last entropy source polled and must know the system under attack?s combining scheme and all outputs the system obtained from the other entropy sources. The rogue RNG then internally repeats the combining hash function many times until it finds a random bit stream to output that, when hashed with the previous entropy outputs, produces a final RNG output value with the desired characteristics. If the combining hash is a key stretcher with a work parameter set high enough, it will not be possible for the rouge RNG to perform the many tries needed to find an acceptable output without imposing a large, easily detected delay. For simple hashes, especial ones crafted for fast hardware performance like the SHA families, it may be possible for the rouge RNG to make the necessary trials fast enough to avoid notice. But if a key-stretching hash is employed that uses enough CPU and memory resources, such as scrypt or HEKS, it will be infeasible, both physically and economically, to hide enough computation power inside the rouge RNG to do the repeated hash trials many times faster than system under attack, even if the rouge RNG is part of the system?s CPU chip. Assuming the system under attack has a trusted timer or real-time clock, the system software can set the work level parameter of the combining hash to be on the order of the maximum time that should be needed to interrogate the final entropy source (the potential rogue) after output has been received from all other entropy sources. If the measured time exceeds a reasonable margin above what is expected, the system should fail safe by refusing to accept the RNG outputs. Alternatively, the system can measure the time interval between obtaining the first and last RNG?s data and set its key stretching work factor based on this time interval.  In particular, Intel?s RDRAND is implemented as a CPU instruction, so almost no time should elapse between the penultimate entropy source's data being obtained and the needed RDRAND instructions being completed. Thus a fairly small key stretching work parameter, even a few milliseconds, should suffice to detect a CPU with an RDRAND that attempts to implement Bernstein's attack.
Arguably a CPU chip with a cooked RNG instruction could suspend its timers while possible RNG outputs are being tested, though it would have to know when the critical output was being requested, otherwise the timer interference might be detected. (At some point we must recognize that an all-knowing CPU with lots of covert firmware designed around our software can defeat any security scheme.) Other timing sources are possible. Common, inexpensive external real time clock ICs, such as the DS1302, provide timing to one second resolution. An available GPIO pin could be used to charge a small capacitor with a resistor in parallel and then measure its voltage decay for a rough time check. While placing a key stretcher in the RNG chain may seem overkill to just thwart the Bernstein entropy combiner attack, an attack which demands somewhat far fetched preconditions, using a key stretching hash can also mitigate the type of attack proposed by Georg T. Becker. et. al. (*Stealthy Dopant-Level Hardware Trojan*s  ) An attacker using the Becker technique will reduce the effective state space of the RNG being cooked. The cooked state space must be small enough to allow a brute force search of possible RNG states, but not so small as to risk detection. Applying a high work factor key stretcher to the output of a Beckerized RNG would make the state space search far more expensive and possibly force the attacker to shrink the effective state space enough to allow detection.
The key stretching defense is particularly well suited to the difficult case of a black box device needing random data at first start up, when, typically, its secret keys are generated. A tens of seconds delay during first boot would seem a reasonable trade off for enhanced security. The Bernstein attack would then take several minutes, a delay that would be easy to notice. A similar argument applies to systems that generate one time a high quality seed for a deterministic RNG which will then be used to meet all future randomness requirements.
Arnold Reinhold

@_date: 2014-02-14 17:31:30
@_author: Arnold Reinhold 
@_subject: [Cryptography] Unified resource on Random Number Generation 
I view RFC 4086 as from an different era, when we were mostly talking about PC type systems, with hard drives, sound cards, etc. Some topics that have come up here that I couldn't find covered in a quick scan of the RFC include:
Different threat/trust models
State actors as a threat (Snowden, etc)
How to audit RNGs
Internet of things and diskless nodes
Certification issues as a drag (FIPS-140, e.g.)
Seed once vs periodic refresh vs TRNG for everything
Risks of combining multiple entropy sources (Bernstein & responses)
CPU TRNGs, in particular Intel's RDrand
The Dual_EC_RNG issue
Yarrow, Fortuna
Inexpensive entropy sources e.g. accelerometer chips
Characterizing video cameras
Hard entropy characterization vs lower bounds
Design reviews
Documentation issues (e.g. what should a man page include)
Updating the RFC would be welcome of course, but I'm thinking of something a little less formal as a starting point, maybe a Wiki, to find consensus and clarify points of disagreements.
Arnold Reinhold

@_date: 2014-02-15 23:22:48
@_author: Arnold Reinhold 
@_subject: [Cryptography] The ultimate random source 
Put aside the question of camera digitization noise and just consider the random placement of discrete objects in the camera's scene. For simplicity replace the flask with a flat bottom box, such as a shoe box, and maybe rectangular candies, small compared to the box. Standard machine vision algorithms can measure the edges, and hence position and orientation of such objects to sub-pixel precision, easily one part per thousand in X, Y, and Theta with a megapixel camera. We don't have to make the measurements, just know the information is in the image. That's 30 bits per object, assuming the objects are all visually distinct.  (If the objects are identical visually, subtract log2(10!), or about 22 bits.) And then there is the position and orientation of the camera, which can also be calculated from the image. So a photo of ten different objects, randomly placed should easily produce at least 256-bits of entropy before even considering other information in the image.  Arnold Reinhold

@_date: 2014-02-16 13:03:32
@_author: Arnold Reinhold 
@_subject: [Cryptography] The ultimate random source 
There have been other such papers on the limitations of dice randomness, but all deal with the case where dice are given an initial throw and then bounce along until they come to rest, with no additional energy input. Adding in enough mechanical energy as the dice tumbles eliminates the bias. This is a problem that has been recognized since at least the 4th century CE, see  My approach is to put the dice in a shoe box and shake them up vigorously, at least ten hard shakes. There is also a small bias in inexpensive dice where the pips are dimpled causing a slight mass imbalance. Casino dice don't have this problem and also have sharp corners, which improve tumbling, but are subject to wear.
However none of this matters for cryptographic purposes. Small biases in dice have very little impact on the entropy generated. For an ideal cubic die, each face has probability 1/6=1.66666... and a random throw has entropy -log2(6)=2.5849625.  Consider a crooked die with one face having a higher probability and the other five equal. We can compute its entropy by summing -p*log2(p) for each face. Below are the results of the favored face having p=1/5, 1/4, and 1/3, all biases that would make a gambler who could command them drool:
Face p	-p*log2(p)	        p	-p*log2(p)	p	-p*log2(p)
1	0.2	0.464385619	0.25	0.5			0.3333	0.528320834
2	0.16	0.42301699	0.15	0.410544839	0.1333	0.387585413
3	0.16	0.42301699	0.15	0.410544839	0.1333	0.387585413
4	0.16	0.42301699	0.15	0.410544839	0.1333	0.387585413
5	0.16	0.42301699	0.15	0.410544839	0.1333	0.387585413
6	0.16	0.42301699	0.15	0.410544839	0.1333	0.387585413
Entropy	2.5794706		2.552724		2.466248
Delta entropy	 from ideal 0.00549		        0.03224		0.11871
As you can see, the difference from the entropy of an ideal die throw grows from negligible (0.00549 bits per throw) to insignificant (0.11871 bits per throw).
Arnold Reinhold

@_date: 2014-02-19 14:04:55
@_author: Arnold Reinhold 
@_subject: [Cryptography] The ultimate random source 
The boundary between adjacent gray scale/color levels will be affected by pixel noise in ways that filtering cannot completely eliminate.  Having a lens cap on prevents any gradients from appearing in the image. So even a completely predictable static scene would seem preferable to lens cap on.
While we are on the topic of building an auditable RNG, another possible element to include in the scene that the camera captures is a television screen tuned to a live channel, perhaps a 24-hour news station such as CNN, Fox or Bloomberg. This would allow verification that an image was taken no earlier than when the TV content was first aired.  A time stamp service could be used to certify a "no later than" date.
Arnold Reinhold

@_date: 2014-02-20 22:58:16
@_author: Arnold Reinhold 
@_subject: [Cryptography] e:  The ultimate random source 
Umm, it's more complicated than that. Here is a resource that explains some of what is going on: The primary purpose of cooling is to reduce dark current, not noise.  Subtracting a dark image removes the dark current bias, but it *increases* noise.  Thermal noise is reduced by lower temperature, but classic thermal noise power is 4kTdeltaF, where T is absolute temperature, = Celcius + 273. So going from 0C to 80C is only a 30% difference.
Note that there are other sources of noise, including shot noise and noise associated with any amplification. Shot noise is higher for higher light levels, another reason id question the lens-cap-on approach.
The question is, can one presume at least 256 bits of noise in a megapixel camera image of a complex scene for any camera with at least, say 10 bits of gray scale. Removing "as much noise as is physically possible" is the key phrase here. I'd bet yes, but it deserves more careful study with input for camera and image processing experts. Arnold Reinhold

@_date: 2014-02-21 16:30:47
@_author: Arnold Reinhold 
@_subject: [Cryptography] Entropy Attacks! 
Sent from my iPhone
There are a couple responses. First an engineering trade off, e.g. nine seconds of entropy gathering and one second of key stretching would get most of the benefits of both (72 bits from entropy gathering, 22 bits from key stretching).
Second, it's hard for me to see a use case where an added 10 or 20 seconds of processing when a box is powered up for the very first time is unacceptable.  Third, at some point we have to be willing to just say no, the constraints you put on your system in terms of available entropy sources and limits on start up time are incompatible with security requirements. Again there are engineering trade offs here. You could start key stretching immediately on your fast source, e.g. RDrand as you suggested in you second post, and finally mix in the slower source(s) at second 9. Arnold Reinhold

@_date: 2014-01-03 06:45:35
@_author: Arnold Reinhold 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
Sent from my iPhone
There is a huge difference between a general purpose particle physics simulation program like GEANT and US   bomb design codes that (presumably) deal with all aspects of bomb design, including high explosive lensing, neutron generation initiators, detonation chain timing, manufacturing and assembly tolerances, etc., and, most importantly, have been calibrated and verified through decades of actual testing. Coupled with computer aided design, CNC machining and 3-D printing, none of which was available in the 1940's and '50s, nuclear design code would greatly facilitate manufacture of a bomb, and possibly reduce the quantity highly enriched uranium or Pu-239 required. As for delivery, have you seen the submarines that drug smugglers have been building? Arnold Reinhold

@_date: 2014-01-03 15:40:31
@_author: Arnold Reinhold 
@_subject: [Cryptography] I posted a memory-hard key stretching algorithm 
Sounds promising. A couple of suggestions:
1. Consider replacing your SHA-256 rounds with the SIMD-hash from the SHA-3 competition. SIMD made it to round 2, but was eliminated from the finals because it was the most expensive to implement in hardware. Small hardware area was a positive criteria in the SHA-3 competition, but it should be considered a negative for KDFs. Why not benefit from all the evaluation work done for SHA-3 candidates? See, e.g. "Comparing Hardware Performance of Fourteen Round Two SHA-3 Candidates Using FPGAs" Ekawat Homsirikamol, Marcin Rogawski, Kris Gaj.  ?In particular Table 4.15 which compares the 14 Round Two candidates throughput-to-area ratio with SHA-512  shows SIMD being the lowest of the 14 hashes, which is the best outcome when we are designing a KBF.  The gain in software vs hardware difficulty is around a factor of 11 (3.5 bits), not insignificant for a simple software change.  I do not know how the 64-bit SIMD calculations play out in GPGPUs, but there should be some gain there as well. The ECHO hash was a close second. SIMD-256 is a little faster than SHA-256 in software, so your 4096 round count could b e increased if desired (perhaps the count should be a parameter). I also suggest going to a 512 bit hash since it requires more area. Why not? Finally, you might start with one round of SHA-2 or SHA-3, just to be able to say that any pre-image attack is blocked by a generally accepted standard hash.  (I'd been thinking of offering PBKDF2-SIMD-512 in the KDF competition.)
2. I also thought of an intermediate approach to the question of scrypt revealing password information via memory side channels. It?s not an obvious big win, but it might be useful in some situations: Instead of using P, the password, in step 1 of scrypt, use:
   Q = Hash (P, Salt) modulo m
(If m=2^k, this just means taking the k low order bits of the hash.)
The parameter m limits how much information about P is recovered if some of the scrypt memory is compromised. If m is low, and Hash is fast, an attacker can devote one processor to each possible value of Q, having each processor calculate Q for every trial password and only proceeding if the Q is its assigned value. Each processor will only have to fill memory once. If Hash is resource consuming, e.g. PBKDF2-4096 as you propose, then the processors will have to expend m-times the work of the defender to screen trial passwords. The attacker can get around this by screening trial passwords centrally and only passing the ones with a given Q to the processor handling that Q, but this entails a lot more interprocessor communication. As m gets larger, each processor will have to handle more than one Q, though they can do so one at a time. If m is big enough, the attacker is forced to do a lot of memory exercise. So there is a tradeoff here between password information inserted in step 1 and resulting difficulty for the brute-force attacker. 3. I think it is helpful to distinguish two use cases for KDFs: protecting login credentials and protecting cryptographic keys. In the case of a cryptographic key, the attacker who can glean KDF memory information most likely has easier access to some ciphertext generated with the key. Examples include whole disk encryption of a seized laptop or attacking WPA2 wireless encryption. The ciphertext itself is usually sufficient information to mount an off-line attack. So the gain from getting hold of the KDF memory, particularly after the algorithm is complete or half complete, is low.  On the other hand, login credentials can be further protected by limiting the number or rate of failed attempts. So any leaked information that allows an off-line attack is a big improvement for an attacker. For login credentials, it may be worth throttling the amount of information an attacker get from a memory compromise as described above, even if it means less KDF gain. Of course with common weak passwords, even 20 bits of leakage can be bad, but I see no reason to put the full entropy of a strong  login password into step 1. For cryptographic keys, any reduction in KDF gain may not be not worth it.
4. The SIMD-hash needs SIMD CPU instructions for full throughput, but these are available on modern x86 and ARM architectures. In the case of protecting cryptographic keys, there is less need for interoperability. If I am protecting my laptop's hard drive, I may as well use all the silicon in my CPU and not worry if the same algorithm will work efficiently on another architecture. We needn't design KDFs to the lowest common denominator.
Arnold Reinhold

@_date: 2014-01-17 13:28:14
@_author: Arnold Reinhold 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
Some thoughts:
1. The notion that one can buy security software from a large American company that will reliably thwart the NSA is silly. 2. The notion that one can buy security software from any vendor and use the default settings indefinitely without access to independent cryptographic expertise to advise on new threats is silly. 3. If I were working for NSA tasked with with disrupting the independent cryptographic community's response to the Snowdon revelations, I'd be hard pressed to come up with a better idea than a boycott of the RSA conference. Arnold Reinhold
Sent from my iPhone

@_date: 2014-01-18 18:22:05
@_author: Arnold Reinhold 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference 	boycott 
So Jon and Daniel have it all taken care of? We can just relax, and their admirable work, which solves all known and still undiscovered problems, will make its way into every security product by the sheer weight of its superiority?  No protocol issues to resolve? No need for people looking for weaknesses to brainstorm possible attacks? No need for people who are working on similar problems to meet and share issues and solutions? No need for those who still don't get it to hear from those who do? I can't think of any discipline where the advocacy, relationship-building and cross fertilization that takes place in a conference is needed more than in cryptography, especially in light of the recent disclosures.  Arnold Reinhold

@_date: 2014-01-19 17:47:29
@_author: Arnold Reinhold 
@_subject: [Cryptography] Conferences, committees, compliance 
If new cryptography is going to have any chance roll back the mass surveillance state, it will have to make its way into commercial use. Getting broad acceptance require dealing with committees and compliance processes.  Boutique cryptography solutions only give the security agencies smaller haystacks in which to search for troublemaker needles.  Single modes and single algorithms and single architects is a terrific goal, but CAESAR (a committee, by the way) won't have a final portfolio until the end of 2107 (tentative). And how long after that to gain broad acceptance? And what critical vulnerability in AES-GCM will they solve that we should wait that long to even start agreeing on a single algorithm suite? I seem to recall at least some issues that have arisen with the engineer-led solutions you admire.  There is a whole world of crypto stuff out there, and I certainly can't vouch for which benefited from contacts and information exchanged at conferences; I doubt anyone can.  Getting the people who are angry over Dual_EC_DRBG to stay away from the RSA conference, hardly hurts RSA. If you don't want to deal with all the process baloney involved and would rather stay home and write code, good for you. But don't demonize others who want to engage by demanding they boycott the conference.  Arnold Reinhold

@_date: 2014-01-20 06:23:27
@_author: Arnold Reinhold 
@_subject: [Cryptography] Conferences, committees, compliance 
Sent from my iPhone
You may be right, but perhaps others see things differently. Shouldn't they be encouraged to try? Restoring a proper privacy balance between the state and the people won't be easy and it is too soon to  surrender on the committee front. Arnold Reinhold

@_date: 2014-01-20 13:10:38
@_author: Arnold Reinhold 
@_subject: [Cryptography] cheap sources of entropy (Bill Frantz) 
Search for this thread on this list: "An alternative electro-mechanical entropy source" 12 Dec 2013. I suggested using a cheap accelerometer chip with a buzzer to insure some vibration and a small Arduino class microprocessor with USB interface.  Putting a low power microprocessor, an accelerometer and a small battery in a diskless black box device could allow it to collect entropy during shipment and have plenty available for initial startup key generation. It could also serve to detect excessive shocks during shipment, helping to justify the cost, which could be under a dollar or two in volume. For one off, you can get inexpensive modules from Digispark and Adafruit. As for cameras looking at a grey card in low light, it does depend on the camera, but it would be easy enough to test a specific model and cheap cameras are probably more suitable. Nonetheless, there is a lot to be said for pointing the camera at a real world chaotic image There will still be plenty of pixels whose low order bit is influence by Johnson noise (think about the line of transition in a gradient between grey scale n and greyscale n+1) and the images can be viewed periodically for verification. Some colored streamers in a server rack's fan exhaust should do, but I have always liked the aquarium approach. An air bubbler alone can provide tons of entropy (see this link  Arnold Reinhold
PS: In an earlier post I said CAESER's final portfolio was due at the end of 2107. That was a typo, I meant 2017, the date they give on their web site.

@_date: 2014-01-28 13:31:08
@_author: Arnold Reinhold 
@_subject: [Cryptography] cheap sources of entropy 
Nice analogy, but you both have it backwards. SHA2 vs SHA3, AES vs Salsa20 and RSA vs ECC are the bike shed/refreshment committee.  RNG is the $10 billion nuclear reactor waiting to blow up. At the present time there is no practical attack on the standard crypto algorithms, but RNG is a single point of failure that has shattered crypto security in practice many times ( and we have every reason to believe it is being actively exploited by large security organizations. If the Intel chip that you are using has its RNG cooked in ways that have been proposed, what you are suggesting will do you no good. The cook will still be able to recover the internal state of the faux-RNG and all your crypto will be worthless.
You are right that every RNG solution has limitations (for the record I suggested including a cellphone vibration motor to excite an accelerometer used on a server) and there is no one right answer. The best we can do is to use two or more independent sources, characterize each source well, implement them properly, and design our systems for maximum auditability consistent with security.
There are (at least) four reasons we keep having endless discussions about RNG: there are many possible solutions, there is no one solution that fits every situation, there are no good standards available (NIST SP800-90 ducks the hard questions), and people forget what was said before. Maybe it is time to build a Wiki that summarizes cryptography mailing list discussions on this and other topics.
Arnold Reinhold

@_date: 2014-01-29 15:36:16
@_author: Arnold Reinhold 
@_subject: [Cryptography] cryptography Digest, Vol 9, Issue 29 
Yesterday's thread on this topic demonstrates what I have been trying to say. Everyone thinks they know how to generate random bits for cryptography. Everyone thinks the other guy is doing it wrong and everyone is looking at the problem through a different lens and therefore missing something important. (And i don't exclude my self.) I agree, find me a standard that says that.
Mobile phones are easy. The hard case is the large number of "internet of things" devices being sold or about to be introduced. (Google did not pay $3.2 billion for Nest just to conquer the digital thermostat business.) Most of these devices lack a hard drive and many have no other obvious source of randomness, especial when they first start up. Some will be used in places where they can do real damage.
As I understand it, FreeBSD currently uses Yarrow for both random an urandom. See  for a discussion of possible startup problems.
Intel hid their entropy source behind a AES-based whitener, a design that is ideal for back-dooring. There is no technical or economic reason for doing that and it should be considered as suspicious as Dual_EC_RBG was.  If they had a more transparent design, I suspect they would have earned broad community support. As for economic incentives, the only one I can think of is to earn a certification stamp. FIPS-140 is both overkill and underkill for such devices.  We need something better.
An astute observation. I submit this happens because there is no standard or guideline nor a process to get one that has any acceptance.  I suggested a Wiki as a start. Any other ideas?
Arnold Reinhold

@_date: 2014-01-31 10:39:01
@_author: Arnold Reinhold 
@_subject: [Cryptography] Unified resource on Random Number Generation 
Is there any interest in developing a unified resource on RNG for Cryptography that summarizes  the various viewpoints expressed here recently? It wouldn't have to resolve debates like general purpose computer vs engineered crypto hardware, or one good entropy source vs hash together everything, but instead present the arguments and engineering trade offs involved.  As I envision it, it would also summarize and point to existing standards and implementations where they exist.
In particular, are any posters willing to release what they have written under a Creative Commons license, with or without attribution?
Arnold Reinhold

@_date: 2014-07-08 14:10:14
@_author: Arnold Reinhold 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
Ian commented:
# There aren't specific restrictions as such with security clearances [1]
# but there are conflicts of interest.  If a person has a security
# clearance, then they have a master or power.  If they are devoted to
# your project, then this means they serve two masters, the best you can
# hope for is that the other master is dormant.
# That power can be used at will.  There are a range of pressures that can
# be put on a person to assist the power.
Anyone who is employed by some government or a company that does business with one or more governments has a conflict of interest and is subject to job pressure. So is anyone whose tax returns have been less than pristine or is here on a temporary visa.  I would be particularly suspicious of anyone with teenage children. Most of them do illegal drugs, and who would believe a kid that doesn't if accused by a cop or dealer who's been offered a plea bargain? That makes their parents subject to irresistible pressure.  And if someone has absolutely no apparent conflicts, isn't that kind of suspicious in itself?
In short, you can't trust anyone. So this kind of thinking is unhelpful. Not only encryption systems but the process by which they are created and maintained must be auditable; security can't be based on "trusted" personalities. Arnold Reinhold

@_date: 2014-07-17 21:56:55
@_author: Arnold Reinhold 
@_subject: [Cryptography] [cryptography] hashes based on 
Much of the worlds security still depends on passwords (or pass phrases) and there are use cases where deriving cryptographic keys from a memorized secret is inescapable. How does an individual do meaningful disk encryption without deriving a key from a password? How does a reporter or aid worker bring a key across a border at which they expect to be throughly searched? There is an arms race between people who wish to keep secrets and people who want to pry them lose. The latter got a huge boost with general purpose GPUs, to the point where the longest passwords that most people feel comfortable remembering can be easily if secured by standard cryptographic hashes. These were all designed to be fast in software and even faster in hardware, exactly the wrong criteria for a password hash. Users typically have a substantial amount of processing power and memory at their disposal, whether on a PC or on a smart phone. That processing power may not match the power available to many attackers, but if used properly it can restore a balance to a large extent. (I started preaching this concept 15 years ago For those of us in a world where even the most sophisticated organizations are limping on broken security legs, memory-bound functions designed to be unimplementable on graphics processors are more like an modern air-cast than a band-aid: they can put us back on our feet and help heal the breaks.  Arnold Reinhold

@_date: 2014-06-05 23:47:04
@_author: Arnold Reinhold 
@_subject: [Cryptography] Crippling Javascript for safer browsing 
I think we are being a little too defeatist here. This is mostly a question of good marketing, not technology. A set of "security extensions" to Javascript that users could opt for might well be adopted if it had wide support from the security community. There are only a few major browser vendors and most have some interest in getting things right. The extensions would need to be well-thought out beforehand, and there is still the cat-herding problem, but getting organized is long overdue. And while we are on marketing, might I point out "FalseCrypt" is not an optimal choice of name for a TrueCrypt fork?  All for the fork, but really... How about PhoenixCrypt?
Arnold Reinhold

@_date: 2014-06-06 11:17:52
@_author: Arnold Reinhold 
@_subject: [Cryptography] Crippling Javascript for safer browsing 
It might take a few iterations to get it right, but I think something like that could get mass adoption, opt-in at first, opt-out eventually, much like pop-up blockers. The proof of concept work has to be done first and I believe we need some organized effort to push such ideas. While we may disagree on a lot of things, I think there are possible consensuses on many issues.  Can we get organized to at least push the obvious solutions?
Arnold Reinhold

@_date: 2014-06-09 07:30:51
@_author: Arnold Reinhold 
@_subject: [Cryptography] Swift and cryptography 
Apple's new programming language, Swift, seems to be a clean, modern and well thought out design, with a number of improvements that remove possible sources of programming errors.  In particular, Swift faces the integer overflow problem head on, perhaps the first modern language to do so (see  Integer overflows in Swift cause a runtime error, but programmers can choose to allow overflows by using the special arithmetical operators &+, &-, &*, &/ and &%. Swift also allows types to have properties, and the properties 'min' and 'max' are defined in Swift for all integer types, e.g. UInt8.max, which is 255.  They can be used to safely check for potential overflows, as contrasted with relying on--and remembering--the names of min/max constants defined in some external library.
Swift is still in beta and there still might be time for the cryptography community to influence its design. Yes, I realize Swift is an Apple-only language at the moment, but this could change and getting things right on one platform is at least a start.  One area that comes to mind is some way to erase data that is guaranteed not be removed by the optimizer. Swift has destructors ("deinitializers") for classes. A guarantee that assignments in deinits would always be preserved might be a start. Swift also has "optional" variables that can either have a value or be nil. A zeroize generic function or protocol that erased all storage associated with an optional variable before setting it to nil could also be helpful. Another possibly helpful improvement would be a circular shift operator.  Swift allow operator overloading, but it would be nice if circular shift were built in. Other thoughts on desirable cryptography and security features for Swift?
Arnold Reinhold

@_date: 2014-06-10 08:20:29
@_author: Arnold Reinhold 
@_subject: [Cryptography] Swift and cryptography 
I think you may like Swift then. "Protected" is for access control. The version of Swift that was just released does not have access controls but there are reports that Apple intends to add them ( Are you thinking of .net's SecureString class? ( I'm not aware of anything similar that Apple provides. It might be possible to write an equivalent class in Swift, assuming deinit can relied on to erase data. If not, it should be added to the crypto wish list. FWIW, I also found this in my searching: Arnold Reinhold

@_date: 2014-06-10 14:25:47
@_author: Arnold Reinhold 
@_subject: [Cryptography] Subject: Re:  Swift and cryptography 
Apple has 9 million registered developers (who pay $99/yr for the privilege), many over 30, and the general reaction to Swift has been quite positive, if not ecstatic. Unlike most new languages, even ones that get glowing reviews, Swift is guaranteed get a lot use from the get go and a vast amount of software, used daily by hundreds of millions of people, will be written in it over the coming decade. That is exactly the syntax bear was complaining about. Not very easy to remember compared to: n = UInt8.max
Checks that are hard to implement are more likely to get left out.  And what happens if I define a new type whose underlying data is an int? And how do I write a generic function that checks for potential overflow? It seems straightforward the way Swift implements type properties. I'm not enough of an expert on C++ to say it can't be done, but I'd be curious to know how.
That doesn't make it any less important for security. And Apple controls its platforms.
Arnold Reinhold

@_date: 2014-06-11 11:06:46
@_author: Arnold Reinhold 
@_subject: [Cryptography] Subject: Re: Swift and cryptography 
Thank you. David Kahn in The Codebreakers pointed out that security was not possible as long as code clerks were the lowest paid members of an embassy's staff. There is a parallel in modern times where software maintenance is done by the least expensive programmers available. Human factors are as important for cryptography as mathematics. A clean, safe language will result in fewer exploits. I particularly find loathsome the term "semantic sugar" which implies that understandable constructs are an unhealthy and morally dubious concession to human frailty. Assembly language is just semantic sugar for hexadecimal machine language.
That likely isn't Apple's priority, but I'll bet other people are already hard at work figuring out how to convert Swift to Android's flavor of Java.
I think eliminating pointers and null-terminated strings and getting overflow right are also wins. I'm not sure what you mean by "drop in". Apple is betting the company that Swift will serve as a full replacement for Objective-C, which is a strict superset of C.
I started this thread asking what other features Swift should have for security. The two I came up with are a circular shift operator and class destructor semantics that do not equate x=0 with a no-op.  People tell me that the latter is platform dependent, though I am not clear why.  Like it or not, Apple is a major player in computing. Swift is about to turn 9 days old and is still in beta. There is time to influence its final form. What else should Swift have?
Arnold Reinhold

@_date: 2014-06-27 15:19:02
@_author: Arnold Reinhold 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
I like your dual encryption argument, but I think it leads to a better solution to the fallback algorithm question:
Choose two independent algorithm suites we currently trust, A and B. The primary protocol will use A. The fallback algorithm will be A super-encrypted with B. If a flaw is found in A, the fallback will have at least the strength of B. But if an attacker knows a weakness in B, there is no incentive for them to force a switch to the fallback.  The relative inefficiency of the fallback is only a problem in the unlikely circumstance that A is compromised. If that ever happens, we develop a revised protocol with B as the primary, A super-encrypted with B as a transition, and choose a new suite C and make B super-encrypted with C as the new fallback.
(And if there is a weakness discovered in both in A and B, pick a new C and D. One might still allow  A super-encrypted with B as a temporary transition, as  their combination may yet be strong.) Outside of some added complexity, I see no security downside with this approach, it's efficient as long as we trust A and it covers the algorithm flaw risk, without the "trust my favorite cryptographers not to make mistakes" argument.
Arnold Reinhold

@_date: 2014-03-16 11:41:16
@_author: Arnold Reinhold 
@_subject: [Cryptography] Apple's Early Random PRNG 
I want to call the list's attention to this white paper on the weaknesses in Apple's "Early Random" PRNG: "Revisiting iOS Kernel (In)Security: Attacking the early random() PRNG"  byTarjei Mandt   . I find it hard to understand why they even considered using an linear congruential PRNG here. I realize this PRNG is being used to supply unpredictable bits needed at a very early stage in the boot process, but they apparently have a SHA-1 function available at this stage in the boot, which they use to get the seed. Why not use one of the standard SHA-1 PRNGs here? Am I missing something?
Arnold Reinhold

@_date: 2014-03-29 21:03:13
@_author: Arnold Reinhold 
@_subject: [Cryptography] Subject:  OpenPGP and trust 
First, as I understand things, the prohibition on encryption in Amateur Radio under International Radio Regulations and US FCC regulations does not apply to authentication. They both say  "Transmissions between amateur stations ... shall not be encoded for the purpose of obscuring their meaning ..." The US Amateur Radio Relay League (ARRL) filed a lengthly brief with the FCC opposing a petition to allow encryption, on the grounds it was harmful and unnecessary.  Section IV of the brief specifically covers authentication, and says: "ARRL has previously advised members, following discussions with Commission Enforcement Bureau and Wireless Bureau staff, that encoding exclusively for authentication purposes does not violate Section 97.113(a)(4). The use of encryption to authenticate the identity of participants who are entitled to use Amateur Radio data networks, for example, is not intended
to obscure the meaning of a transmission. Rather, it is for the purpose of insuring control and prohibiting unauthorized access to Amateur stations and networks of stations. Therefore, encryption for purposes of authentication of a user and prevention of access by unlicensed or unauthorized persons is arguably the same as the goal of the encryption prohibition in Section 97.113(a)(4) in the first place: It allows Amateurs to police their own allocations and prevent intruders, as indeed they must do in order to maintain control of their licensed facilities."
I have no idea if the Australian authorities take the same view, but I imagine you would have a strong argument for such an interpretation.
Second, I would ask what kind of system you would set up in the good old days before electronic signatures? I imagine you might make a list of people authorized to use the facilities, a separate list of those authorized to make changes and finally a list of people authorized to issue authorization, the last perhaps the governing board or owner of the organization.  You might have reciprocal use privileges with another organization, but would expect members with change privileges in the other organization to apply for the same privileges in your group, perhaps with expedited approval. You would also want to know who in the other organization is authorized to issue credentials. It seems to me a similar system model could be set up using OpenPGP signatures, rather than use the more generic trust levels. Arnold Reinhold, K2PNK

@_date: 2014-05-01 17:36:41
@_author: Arnold Reinhold 
@_subject: [Cryptography] GCC bug 30475 (was Re: bounded pointers in 	C) 
After the developers have been told that a specific optimization is potentially causing widespread security and safety problems, I think it is evil. Same issue applies. If you know what foo does, why would you write "foo(x, x+1);" ? If you weren't checking for overflow, it makes no sense. Maybe you really meant to write "foo(x, y+1);" The compiler should flag this, not optimize it away. If you were running a code review that included this fragment and the programmer wasn't present, would you simply replace foo(x, x+1); with printf("Lesser\n"); or would you get hold of the programmer and ask him what the heck were you thinking?
And I'm trying to in my own feeble to address it. Which course of action makes more sense, rewriting all the security sensitive software written in C or getting the compiler to stop removing signed integer overflow checks? If the undefined behavior is a bug, the compiler should treat it as a compile time error, not allow it during ordinary compiles and silently optimize it away later. I pointed out that asserts may not be appropriate in production code, but if they are only there for debug, then there is even less of a case for optimizing them out. We don't need maximum execution speed during testing and they might catch other problems caused by misunderstandings between the programmer and the optimizer. Assert style checks are also useful in hardening programs against deliberate attacks, say where the attacker figures out a way to create an overflow with malformed inputs that should have been checked but weren't.  If anything, we need programmers to put more sanity checks in their programs, not put hurdles in their way. OK, you got me here. "...produces the same output as the original program" overstates my intent. A simpler example (in lazy pseudocode) would be
print (currenttime() - starttime);
Obviously using the optimizer should produce a different output here.  Does what you said above "Writing programs with undefined behavior is writing programs with bugs" apply equally to "accessing arrays, dereferencing pointers, passing arguments to functions, etc."?
In your example:
char c[] = "abcdefghij";
int z = 55;
printf("%s %d\n", c+20, z);
return 0;
Would your optimizer be justified in silently deleting everything but the return, since whatever c+20 points to is clearly undefined? Can it remove all pointer dereferences whenever it can't verify that proper bounds checks have taken place?  The "undefined behavior" rubric is getting a little squishy. Maybe we should just focus on signed integer overflow.
If your bank account gets raided, will you be comforted in the knowledge that the bank got by with fewer servers? Again let's stick to signed integer overflow. How much of this performance gain would be lost if signed integer overflow operations and their sequela were not optimized away? Arnold Reinhold

@_date: 2014-05-02 18:38:34
@_author: Arnold Reinhold 
@_subject: [Cryptography] GCC bug 30475 (was Re: bounded pointers in 	C) 
Excellent example. I completely agree, it shouldn?t be.  But it is: 1. The gets() problem was a real one affecting most, if not all platforms, not obscure hypothetical platforms or code generators of which no one seems to be able to give a real-world example.
2. The gets() problem was a security issue that was taken seriously, not brushed of with some variant of ?that?s how C works, if you don?t like it, don?t use C.?
3. A clear, workable alternative, fgets(), was thought out and provided first.
4. The transition was encouraged by first depreciating gets() and then removing it from the C library, NOT by having the optimizer silently mung your code if you use gets().
Knowing what I know, absolutely. If I could find a bank like that, I?d give them all my business.
I think we?re done with this thread. Thank you for a courteous exchange. You?re welcome to have the last word.
Arnold Reinhold

@_date: 2014-05-08 16:05:46
@_author: Arnold Reinhold 
@_subject: [Cryptography] Heartbleed and malloc 
In April, I edited the Wikipedia article on Heartbleed to reflect Theo de Raadt?s critique of OpenSSL?s memory management ( Other editors questioned this and a few of us have attempted to read the OpenSSL code. It looks like the Heartbeat extension did not used the free-list tools that Theo complained about. Heartbleed uses the OPENSSL_malloc wrapper, but from what I can determine, as long as debug modes are off, OPENSSL_malloc gets redefined in a series of steps to be the operating system malloc. Maybe Theo has a better understanding of what is going on, the OpenSSL source is very complex, but if our reading is correct, Theo?s critique may have been too harsh. Perhaps more importantly, to the extent that Theo is correct about OpenBSD?s malloc and free being able to catch the bad behavior of Heartbleed, this may put a bound on how widely the bug was exploited before it became public. Exploiting Heartbleed requires lots of probing because the bug returns a maximum of 64K bytes of memory per probe. The occasional crash might have been ignored, and maybe some exploiters could have been sophisticated enough to avoid OSs that have such safety measures. Still it should be possible to check old error logs for signs of Heartbleed exploitation. Am I missing something here?
Arnold Reinhold

@_date: 2014-05-27 14:07:34
@_author: Arnold Reinhold 
@_subject: [Cryptography] [PHC] Re: The proper way to hash password files 
While for low volume applications, key stretching with sufficient work factor, along with some enforcement of password standards, e.g. a minimum acceptable size and not on standard cracking lists, may be reasonably adequate, for high volume or high security applications we have to recognize that there is no solution that just involves software running on the enterprise servers. Some specialized hardware will be required. My vision of a password security server module would use a keyed MAC of the password and salt to form the hash to be stored as the password authenticator in the user?s record on the enterprise database. The key or keys would be stored in a hardware module in a way such that the keys would not be accessible to the network. The password security server would accept a fixed size message consisting of return address for the application requesting authentication, request number, the plaintext password, stored hash, salt, key number, and perhaps a time stamp for auditing. The password security server would calculate the keyed MAC of the password and salt using the secret key identified by the key number. The calculated hash would be compared to the one in the message. The server would then return the request number and a match/no match response.  The actual user ID corresponding to the password would not be transmitted, so intercepting the request on the network would have limited value, though an attacker who penetrates the network might also be able to see who is currently logging in. To reduce that risk, the authentication request message might be encrypted using a public key belonging to the password security server. A separate message type, possibly to a separate server, would be used to calculate the hash when a new password is created, accepting the plaintext password and returning the hash value, salt and key number. These would then be stored in the enterprise database. Since the password server also generates the salt, the response would be of limited value to an attacker.
The key number selects one of several keys. The password security server could store a large number of keys, perhaps even an SSD full, making it difficult for the secret hashing keys to be leaked, say via side channel attacks. The password security server would need features to prevent a rogue program inserted into the network from using the password security server to do key cracking. These might include limiting the number of responses per minute, perhaps based on expected usage patterns, and maintaining an audit trail. Note that the password security server would not see user IDs, just a request number generated by the application, which would be responsible for keeping track of the requests. Off the shelf HSMs might be adaptable to this application, but the underlying hardware could be much less expensive, perhaps consisting of an FPGA  with a microprocessor front end that dealt with the network and only passed fixed size messages to the FPGA. The microprocessor could check that the request number on the response message matched the request number of an incoming message, so that there would be minimum opportunity for leakage. The FPGA board would be designed so that firmware and key loads would require physical access, perhaps using unique connectors. The biggest expense might be getting full FIPS-140 certification for the module.
Arnold Reinhold

@_date: 2014-05-27 18:00:36
@_author: Arnold Reinhold 
@_subject: [Cryptography] [PHC] Re: The proper way to hash password files 
Right, but I am concerned with packaging and key management. I envision classic NSA style key management with the MAC key separated from any device that can communicate with the network and keys loaded by a mechanism that requires physical access to the module, and preferably prevents easy readout of the key(s) even if physical access is obtained. The MAC key becomes a very valuable secret in this model, on a par with the private keys used for enterprise financial transactions. The good news is that the protocol and proof of concept software can indeed be developed on a hobby-class processor. Packaging and key management are mostly orthogonal issues. The Netdulino may even be too powerful. It apparently has an operating system, I don?t want no stinken operating system, just a simple loop that looks for a message and issues a response. I was thinking of the $80 Mojo FPGA Development Board  It has a Xilinx XC6SLX9 and a ATmega32U4 micro controller that can handle the USB communication. The Xilinx XC6SLX9 is way overkill for a MAC but the board has 84 digital I/O pins. I was thinking of having a secondary hardware key consisting of a daughter board randomly connecting 40 output pins to 40 input pins to produce a permutation function. log2 (40!)= 159.1 bits.  (A 32 bit permeation might be easer to use in an algorithm, log2 (32!) = 117.6 bits.) Or just have an 84-bit key programmed with dip switches. Ultimately, I?d like an ASIC to do the MAC and accept key-fill. The ATmega also loads the Xilinx firmware, so that is a potential attack path. It might be better to use a separate ATmega to handle the USB communications, but for starters, the Mojo board looks good to me. Power dissipation is low enough to lock the whole thing in a safe, maybe several modules, maybe with dual combination locks for two person integrity. The physical security stuff will dwarf the cost of the electronics.
The big advantage of having the new password hash mode generate the salt is that it then cannot be used as an oracle to crack passwords.  If the hash generation mode accepts the salt, then it can be used as a cracking tool. Creating the salt is easy enough and does not require a very sophisticated random number generator.
Ultimately applications will want to access this service over the network, but I agree the security module itself should not contain the complexity of a network stack. It should talk via the simple messages I described, maybe over USB, to a server or whatever that has the full network stack.  Not sure what you mean above.
I want a little data as possible coming out of the security module, maybe just an alarm signal if it sees too high a rate of queries. The server it is connected to can do any logging and more sophisticated attack detection and mitigation.
Perhaps, but this issue is orthogonal to the basic design and more than one solution could be allowed.
I completely agree that an inexpensive solution should be possible and is worth building now. But the people who really need this, the Targets and e-Bays of the world, probably will insist on having all the CYA certifications imaginable, so it is worth designing with eventual certification in mind.
Arnold Reinhold

@_date: 2014-05-27 18:33:50
@_author: Arnold Reinhold 
@_subject: [Cryptography] [PHC] Re: The proper way to hash password files 
I would load the 1 TB ROM from a plugin hard drive module with a non standard connector that is locked in a two-combination safe with the password authentication server, and never allow access to it from the enterprise network. I would encrypt the ROM on the hard drive with a key stored in password authentication server, so anyone who just steals the module would have nothing. (If I understand things correctly, this is what NSA calls benign key fill.) For maintenance, I?d keep spares in a vault.
Per previous conversations on the cryptography list, there are lots of security tools that could be built with small microprocessors. I think keeping them each focused and simple is the best approach for now.
Arnold Reinhold

@_date: 2014-10-07 10:21:54
@_author: Arnold Reinhold 
@_subject: [Cryptography] Best internet crypto clock 
This conundrum suggests a need for a camera that cryptographically signs its images. It could be packaged and certified as a FIPS-140 level 4 HSM. The camera would have a built-in asymmetric key pair with the public key available from the manufacturer by camera serial number. It might also accept additional keys via Bluetooth or USB and sign images using those keys as well. As with any HSM, secret keys would be erased upon detection of tampering. The camera could communicate via Bluetooth or USB or an optical link and be controlled by a cell phone app, perhaps clipping onto the cell phone or phone case. It might use inductive charging to minimize electrical connections.
I would envision including a good quality internal clock, set at time of manufacture and non alterable. (When the clock battery dies, the camera is toast.) The camera would periodically or on command output a signed certificate containing the current reading of its internal click and maybe an external nonce like the NIST beacon, which might then be sent to a time stamping service, creating a record of internal clock drift over time.. The camera might store a correction factor, so it could output a UTC time, but the internal clock would be included in any certificate as well.
It would seem that a camera like this would be useful in a variety of applications (besides kidnapping) to create legally provable documents. Assuming it had a video mode, it could be used as a notary, recording a person's spoken acceptance of contract, or witnessing his handwritten signature on a document. Of course one would still have to trust the manufacturer.
A signing camera isn?t a new idea, a quick Google search came up with this 1992 paper  , but camera technology developed for cell phones makes something like this much more affordable. Has anyone attempted this? How close could we get with an iPhone 6, given Apple's improved security scheme?
Arnold Reinhold

@_date: 2014-10-07 19:41:37
@_author: Arnold Reinhold 
@_subject: [Cryptography] Best internet crypto clock 
Having the camera and a cock inside the module cuts out all video editing techniques. The camera can attest when the entire optical image was captured. I?d go further and include a gyro/accelerometer package so a panorama could be captured with attestation that the camera was actually turned, rather than presented with a moving image. INAPL, but my understanding is that the US is going to a first-to-file patent priority system, a has much of the rest of the world already, so what ideas you had when only matters if you publish them. An online, timestamped, well indexed patent disclosure journal where people could post all their clever ideals as fast as they get them would be easy enough to implement. A mechanism for others to add comments that clarified and extended your ideas (much like these threads) would help prevent trolls patenting the gaps in your ideas.  A small fee could keep down spam. Existing crypto and time-stamping services should suffice for authentication. Arnold Reinhold

@_date: 2014-10-08 08:09:21
@_author: Arnold Reinhold 
@_subject: [Cryptography] Best internet crypto clock 
Those are interesting examples, but intended for fixed mounting in nuclear surveillance situations and presumably very costly. I'm thinking of a much more portable device based on cell phone technology, with video, audio and additional sensors such as motion, compass, GPS (ok, that one's a deal breaker for the kidnapping market). And as a separate question, what can be done with, say, the newer iPhones, given their stronger security model.  For example, two adversarial parties could video and sign the same event with separate phones and then sign each others videos after inspecting them and concluding that each captured the intended information. One could bound the time of the videos by starting with a Nist Beacon value and ending with a time stamp from a service. Arnold Reinhold

@_date: 2014-10-08 16:57:43
@_author: Arnold Reinhold 
@_subject: [Cryptography] The world's most secure TRNG 
Good call. The Lattice ICE40 FPGA product page  says it has a hard I2C core. Is that only for configuring the FPGA or can it be used to output your random bits? If not, how hard would it be to make a second I2C controller from the excess FPGA logic? It sounds like the ICE40 costs about the same as the USB-to-FIFO, so an alternative FPGA-only I2C model should hit the same price point. I2C would open a different market -- embedded -- that might be much larger and really needs a good cheap random source.
There might well be a market for hobbyists, e.g. via AdaFruit. FPGA dev boards I've seen start in the $100 range, admittedly much more powerful arrays but how many hobbyists can begin to use that power? You'll need to package up the design software with some documentation and a few simple examples.
Arnold Reinhold

@_date: 2014-10-12 10:18:21
@_author: Arnold Reinhold 
@_subject: [Cryptography] Best internet crypto clock 
Sent from my iPhone
A simple counter with no overflow would work, of course, but Inexpensive cpu clock chips, like the DS-1307 family, provide a 99 year range with one second resolution and have all the circuitry for dual supply (5 VDC and battery) with very low power (500 na) operation on battery.  Another possible advantage over a straight counter: yy-mm-dd-hh-ss in a time stamp is a lot easier to explain to a judge and jury than a long hexadecimal constant. Here's a data point. I installed a cheap digital video recorder for a surveillance system just over four years ago. It's not connected to the Internet and I never adjusted the clock since installing it. I had to pull a clip off of it last week and the clock was 44 minutes fast. That's about a minute a month.  So if the device grabbed the current NIST beacon signed it with its internal clock and had the resulting certificate time stamped by an external authority once a month, that should be enough to establish minute accuracy. Arnold Reinhold

@_date: 2014-10-14 19:24:25
@_author: Arnold Reinhold 
@_subject: [Cryptography] [messaging] Gossip doesn't save Certificate 
I agree that the pin distribution problem seems quite solvable. But how do browser manufacturers get valid pin data for 100,000 sites, not to mention regular updates? If they want to get the information independently, they will have to set up the kind of rigorous verification infrastructure that we would want CAs to employ. (The fact that most CAs fall short does not suggest the problem is an easy one.) And if I trust my browser manufacturer?s signature on the browser software distribution that includes the initial pin list, as well as on subsequent pin updates, why not also trust the same signature key to sign individual web site credentials and use the existing TLS infrastructure, with the browser manufacturer serving as a super-CA for those 100,000 sites?
If the browser manufacturers choose instead to subcontract getting the pin data to one or a few high quality CAs, expect those CAs to charge a very steep price since it undermines their business model. The other CAs will no doubt raise a ruckus, perhaps invoking local antitrust laws. And if the browser manufacturers accept most CA data, what is the point? I?d still like QR codes in my bank?s lobby.
Arnold Reinhold

@_date: 2014-10-18 22:40:48
@_author: Arnold Reinhold 
@_subject: [Cryptography] Best internet crypto clock 
No, you misunderstood me. My conception is that the clock can never be reset or adjusted once the device is FIPS-140 sealed during the manufacturing process. For example, a module that contains the clock chip, crystal and battery, as described in the Maxim application note above, might first be plugged into a station that starts up the clock and set its time. If the NVRAM on the clock chip were used, it would be initialized as well. The clock module would then be plugged into the camera board. The camera board would include a hardware interface that did not permit writing to the clock module. A parallel interface might wire the write line off. An I2C serial interface might use a special state machine that never asserts the write bit. My "grab NIST beacon" step is part of creating an electronic document that might be called a Clock Calibration Certificate (CCC). The camera gets the latest NIST beacon, appends its current clock reading, signs it with its secret key, and then send the resulting document to a time stamping authority. The timestamped document is our CCC (or maybe we have the camera add a second internal clock time stamp and sig).  Each CCC bounds the actual time that corresponds to the internal clock reading. It is no sooner that the time of the NIST beacon value and no later that the time stamp authority's time stamp. CCCs can be generated periodically or at the start of a picture-taking session. The CCCs can be stored on the camera, as they are small compared to even a single photo, and/or backed up to the cloud, a server or a registrar. Each image file might have the latest CCC attached and perhaps enough older CCCs to allow the camera clock's drift rate to be calculated, allowing more precise time measurements. There are a variety of ways of using the CCC, but the point is that each one is an irrefutable comparison of internal clock time with time traceable to national standards.
That is a possibility, but I like the simplicity of a device that cannot have its clock and security parameters altered.
If the camera has Internet access, it might be able to use the same approach of getting a NIST beacon, appending it to an image or set of images, then hashing everything. Or maybe use the NIST beacon number as the key for a keyed hash. Then time stamp the hash
One fewer wheel to re-invent. Arnold Reinhold

@_date: 2014-10-22 17:36:16
@_author: Arnold Reinhold 
@_subject: [Cryptography] Best internet crypto clock 
Presumably this would mainly apply to video or audio files. I see no reason a still image would contain a significant hum signal, as long as the exposure time is << 1/60 sec. I wonder how hard it would be to forge one of these power line hum time signals? Per previous posts, the only case of interest is ?no earlier than?, i.e. pretending a file was recorded at a notional date and time which is later than when it actually was recorded. (Time stamp authorities solve the ?no later than? case.) The simplest situation would be if the file is created in conditions where hum is minimal, say in a well shielded room or way out in the country, far from power lines. The power line hum could then be recorded at the notional time and place and added into the file. A more interesting case would be a file that did have a hum signal. It seems to me that with some clever signal processing, the hum signal could be modified to match the desired hum. One would first adjust the notional time of the image a few milliseconds so that the recorded hum and the notional hum were close to being in phase for as long as possible (this might place a limit on how long the video file can be). One would then compute a difference signal to be added to the recording that would make it match the hum at the notional time. It might also be possible to observe the hum on the notional day for long enough to let the forger select a time interval during the day when the frequency is close to what was recorded. Doing this would minimize the amount of alteration needed and/or maximize the length of the recording. Given limited applicability to still images, venues where archived hum might be absent (wilderness, air planes, cruse ships, war zones, etc.) and the possibility of forgery, I don?t think the hum method obviates the need for a camera with a secure clock.
Arnold Reinhold

@_date: 2014-10-29 12:36:02
@_author: Arnold Reinhold 
@_subject: [Cryptography] Randomness for Cryptography wiki 
We seem to be in the midst of another rearguing of the issues surrounding randomness generation for cryptography applications. The last time this happened, earlier this year, I suggested creating a wiki so these debates can at least be recorded once and for all, if not resolved. I started building one, but the discussion died down and I got busy with other things. I?d like to resurrect my proposal. The ?Randomness for Cryptography? wiki I started is at  I attempted to outline the subject, not to provide a definitive resource. I?d need a lot of help to accomplish the later, of course, and I?m not up for a solo effort. I spent some time researching wiki farms and shoutwiki seemed the best match. The Randomness for Cryptography wiki is ad supported at the moment, but ads can be removed for ~$50 a year, which I?d spring for if enough interest develops.
Right now this wiki is open for read, but private for write. I?m happy to give editing privileges to anyone here who wants them. You will need to open a (free) shoutwiki.com account first, then email me your user ID and I?ll do the necessary hocus-pocus. Note that all contributions must be released under a Creative Commons CC-BY 3.0 license. Please take a look.
Arnold Reinhold

@_date: 2014-09-04 16:12:03
@_author: Arnold Reinhold 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
There is no reason to think P!=NP is a prerequisite for crypto. In fact AES-256 is in complexity class C, as it can be solved in constant time, namely no more than 2^256 steps. The same is true for all block ciphers in use today. Of course, testing 2^256 possibilities not practical to do, but it is still constant time.
Even if an algorithm is in P, it is not necessarily tractable. A cipher whose difficulty grows as L^100, where L is the key length, could be perfectly suitable. Here is a link to a couple of sci.crypt posts on the subject from 1995: Arnold Reinhold

@_date: 2014-09-07 17:06:39
@_author: Arnold Reinhold 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
An interesting point, but it is worth plugging in some numbers. Say you are using a 128-bit key with your n^50 algorithm family and want to know the improvement adding one key bit brings. (129/128)^50 = 1.47566?, a little more than the square root of two. So you will need to add two bits to double the strength, compared to one bit for a 2^n strength algorithm, not a big problem. For an n^20 algorithm, you?ll need to add 5 bits to a 128-bit key to double strength, again not so unreasonable.
It is also worth computing the break-even point between a family of polynomial-time algorithms whose strength grows as n^k vs a 2^n exponential-time algorithm family, assuming the same constant multiplier, say one. So do so, set
    2^n = n^k and solve for k. Taking log2 of both sides:
   n = k log2(n)
   k = n/log2(n)
So if n=128,then  k = 18.3.
In other words, an algorithm family whose difficulty grows as n^19  will be stronger than one that grows as 2^n when n<=128, again assuming the same constant multipliers. So the notion that P algorithms are necessarily tractable while exponential ones are not really falls apart at values relevant to cryptography. Arnold Reinhold
Sent from my iPhone

@_date: 2014-09-27 21:47:16
@_author: Arnold Reinhold 
@_subject: [Cryptography] Billboard Defense and CT 
Back in 1997, I proposed defending against man in the middle attacks by encouraging people to display hashes of PGP public keys in public places, where anyone could easily copy them without using electronic channels that a MITM can control. I called it the Billboard Defense.( The same method could help shore up schemes like certificate transparency. The ?Mossad Level? MITM attacks being posited depend on keeping the person under attack in an electronic bubble, in which all communications are controlled by the attacker. Any out of band communication that transmits a valid key can puncture that bubble. Worse for the attacker, the person under attack is given incriminating evidence in the form of key certificates cryptographically signed by a supposedly thrusted third party that attest to a phony key. Once alerted, the person under attack can save such ?smoking gun? certificates and turn them in, perhaps in person, as a public service or to claim a reward. A Web site that then posted such smoking gun certificates would quickly reveal untrustworthy CAs. (Has anyone so far exhibited even one forged certificate signed by a browser-accepted CA?)
High capacity bar codes, such as QR-codes, would further simplify this process. Institutions like banks and retailers (including Apple) could post a public key verification QR-code in their lobbies or stores. They could also print it on business cards, brochures and monthly statements (for those still offering hard copy). We would still need software apps to accumulate signed keys that a user?s browser has accepted and compare them with publicly displayed fingerprints obtained from QR-codes and the like. The app would then mail or print out any smoking gun certificates it finds. Companies that lack significant retail presence, like Google, might purchase space on physical billboards or use some other non-Internet communications scheme. One simple approach would be to put a light on a building that flashes the key fingerprint in Morse code (in hex or base-32). Google in particular owns a massive 18-story building in Manhattan, 111 Eighth Avenue, that is near the Hudson River and easily visible from New Jersey. (It is the long brown building at the far left in this image of the lower Manhattan skyline  Google Hq inManhattan visible). I would propose that Google place a visible beacon on the roof of 111 Eighth Avenue that blinks out Google's current public key fingerprint in slow Morse code. Of course only a few people would bother to use such a system at first, but as more companies follow the example, life will become increasingly hard for men in the middle.
Arnold Reinhold
(I hereby release to the public domain any patentable ideas of mine contained in this e-mail.)

@_date: 2014-09-30 09:16:45
@_author: Arnold Reinhold 
@_subject: [Cryptography] Billboard Defense and CT 
The BBC in WWII was mostly using one time codes, otherwise, I suspect that, at least with modern tech, the broadcasts could have been MITM'd. A local transmitter could overwhelm the distant broadcast and a half hearted attempt at jamming could cover up any artifacts. Matching voices would have been the tricky part, easier to do today.
The advantage of a flashing light beacon on a landmark building, like Google's 111 8th Ave., or the Citibank building, or the Bank of America tower in San Francisco, is that both the owner's identity and the message content can be perceived and trusted directly without any electronic intervention. While knowledge of Morse code is not as widespread as it used to be (the Boy Scouts used to teach it), if the transmission speed is slow enough it can be copied down as dots and dashes and looked up in a table. There is no place for a man in the middle to operate. The totally-ignored fate of legal notices could be avoided by having the beacon transmit other messages that would keep the public interested, such as consumer discount codes.  Trademarks are directly relevant to domain names and there are many other examples besides the Trademark Office of governments providing useful managed identity information services, deed registries, passports, driver's licenses, etc. Unfortunately few of us are willing to trust the government with verifying our keys. Hence the need for an absolutely trustworthy, independent way to communicate key verification data.
Arnold Reinhold

@_date: 2014-09-30 18:06:23
@_author: Arnold Reinhold 
@_subject: [Cryptography] The world's most secure TRNG 
I disagree here. It?s time we stopped second guessing people and assume other engineers will do their job. Your device should come with a spec sheet characterizing how it deviates from perfectly random, and if possible include some test software intended to distinguish it from a fake device with a PRNG. As i see it, the real need for an inexpensive, high quality TRNG is for disk-less embedded systems, e.g. the Internet of Things. There even $2 can be prohibitively expensive. On the other hand an I2S bus interface is all that is needed, which could let you use a cheaper part. The major IoT use case is generating a unique public/private key pair at first startup, when other entropy sources are not available. But the software for generating public/private key pairs had better be written by someone who knows what they are doing and expecting them to whiten your entropy first is not unreasonable.  Arnold Reinhold

@_date: 2015-04-14 19:29:17
@_author: Arnold Reinhold 
@_subject: [Cryptography] upgrade mechanisms and policies 
My approach to providing alternative cypher suites would be to use superencryption for the alternates. (Here  superencryption is meant broadly for each primitive, eg dual sigs & hashes.)
So if one has a primary ciphersuite A and backup suite B, the protocol would offer a choice of A or A*B, where * denotes superencryption. If A is ever compromised we kill A, switch to A*B as an interim, select a new backup, C, perhaps based on lessons learned from the break in A. We then upgrade as many systems as possible to a new protocol version that supports B, B*C and, for backwards compatibility, A*B.  If B is compromised but not A, we pick C and upgrade to A and A*C. In the unlikely event both A and B are compromised simultaneously there is still a good chance A*B will remain strong. The switch from primary to backup is always safe for end users and of little use to attackers. It imposes a performance hit, but mainly on servers which can be beefed up temporarily until most clients upgrade. Switching ciphersuites then becomes more of an "in case of emergency break glass" situation. Arnold Reinhold

@_date: 2015-08-18 15:02:21
@_author: Arnold Reinhold 
@_subject: [Cryptography] Speculation on the origin of Speck and Simon 
I have been playing with NSAs Speck cipher on ATtiny85 microprocessors and I happened upon Bruce Schneiers 2013 blog on Speck and Simons introduction. He asks Why was the work done, and why is it being made public? I'm curious. This question provoked a long discussion thread. The comments fell pretty much into two schools of thought, the sneaky NSA must have some backdoor or the noble NSA is acting in its communication security role protecting the future Internet of Things. Here is a third possible explanation, based on the Snowden leak of NSAs Tailored Access Organization catalog of implantable devices for compromising communications. Jacob Appelbaum revealed that NSA is using RC6 in those implants. Initial reactions to that tidbit included claims that NSA must not trust AES, but it turns out the leaked documents were written before the AES selection process was completed. RC6 was a finalist in that competition. Since the implants, once deployed, are out of NSAs physical control, it is inevitable that some will be discovered by their targets and reverse engineered. So it makes sense to use a publicly available algorithm rather than a classified one. But RC6 (and AES) have relatively large code footprints. Presumably NSA wants those implants to be as small and inexpensive as possible. Small commercially available microprocessors like the Atmel AVR line have limited program space and even more limited RAM. So I can easily imagine that NSA would develop a suite of lighter weight ciphers for use with its implants. Publishing those ciphers eliminates any need to treat devices carrying the ciphers as sensitive material. It also provides some deniability if a device is captured.  Note that NSA not only published the algorithms themselves ( but several ways to implement them in AVR assembly code:  If my reasoning is correct, it suggests that these ciphers are not just curiosities from the research lab but important production tools that NSA would have put put considerable effort into validating, and that these ciphers deserve to be taken seriously.
BTW, for what its worth, the Speck 128/128 source code in the Wikipedia article works without modification on an ATtiny85. The published 128/128 test vector validates once one realizes that the 128-bit constants must stored with the low order word as the zero element of the long long word arrays. Decryption is a little trickier since one has to run the round keys in reverse. As far as I can see, the NSA has not published decryption pseudocode. Perhaps their implants have no need to decrypt, so decryption is left as an (easy enough) exercise for the reader..
Arnold Reinhold

@_date: 2015-12-17 08:25:10
@_author: Arnold Reinhold 
@_subject: [Cryptography] My response to White House re strong encryption 
[Here are my comments to:  ]
Response to the White House request to Share Your Thoughts on Strong Encryption
Comments by Arnold G. Reinhold
December 13, 2015
In September 1999, I wrote a briefing paper for the Cato Institute titled Strong Cryptography The Global Tide of Change. Its available on-line at:    Cryptographic technology is so widespread that it is impossible to stop. If any major governments, terrorist organizations, or drug cartels are not now using strong cryptography, it is not because of lack of availability or lack of reliable suppliers. There are many firms overseas that are willing to provide cryptographic software, and, for better or for worse, some of the cryptographic products most widely available on the international market were originally made in the United States.
key recovery will create new targets for miscreants to attack. Given the enormous value that the data in key repositories represents, it is only a matter of time before they will be compromised. Even the best security arrangements are vulnerable to bribes, blackmail, and threats of bodily harm. Over time, commitment to security will wither under cost pressures and boredom.
We saw an example of the latter point this year at the Office of Personnel Management when the security clearance forms and data of millions of cleared workers, including all our intelligence agents, were electronically stolen by China.
Tools for surveillance have multiplied since 1999
Since my briefing paper appeared, there have been many changes in technology and legislation that have enhanced the ability of law enforcement and the intelligence community to track terrorists and gather evidence:
o The dramatic drop in the cost of mass storage (by a factor of over 300) has allowed the indefinite retention of almost every detail of each Americans lives. Lower storage and processing costs have enabled the big data movement, which stores and analyzes every financial transaction we make as well as all our interactions with the Internet. As business records, such data is available to the government without search warrants.
o The Patriot Act was passed giving the FBI broader power to demand data through secret National Security Letters, hundreds of thousands of which have been issued. The act was also interpreted by the Bush administration to allow wholesale collection of metadata on every U.S. citizens telephone and electronic communications, creating a database that reveals each person and organization with whom we communicate. While recent legislation has moved this database from government data centers to those of the private telecommunication carriers, it is still available for government search.
o The growth of cell phone usage to near ubiquity has, as a by product, allowed the movements of every individual who carries one to be tracked at all times. Newer phones with built-in GPS must, by law, allow tracking to the nearest 50 meters for most calls. While this data is only needed temporarily to route calls and pass on location data to emergency responders, it is being stored indefinitely. Again, as business records, this data is available to the government without search warrants.
o License plate readers have become cheap and reliable, and are being used on traffic signals and roving police patrol cars, providing another means to track our movements. o Surveillance video cameras have become common and are being linked in many jurisdictions. Combined with rapidly improving face recognition software, they provide yet a third way to track individuals, even those who avoid cell phones and private automobiles. o The rise in social media has placed a vast array of information about individuals on line. Accounts associated with terrorist organizations designed to recruit new terrorists can and no doubt do provide a wealth of intelligence about potential threats.
o We now know that the NSA has actively worked to weaken security standards intended to protect electronic communication systems, many of which are essential to civil safety. o We also learned that the NSA has developed an extensive catalog of technologies that can infiltrate computer network systems and circumvent their encryption.
These new technologies have greatly expanded the arsenal of our law enforcement and intelligence agencies, but they also threaten to entrench despotic regimes around the world by creating a totalitarian infrastructure far beyond what George Orwell imagined in 1984. Use of strong encryption to protect our personal records and communications from government snooping is one of the last lines of defense for individuals here and abroad who wish to resist oppressive governments. We need stronger security systems, not weaker
Since 1999, the dangers of weak electronic security have become all too clear.
o The have been a long series of massive data breaches affecting even companies in the security industry. Tens of millions of U.S. citizens have been affected.
o Cyber criminals have developed ransomware systems so effective that the FBIs best advice to victims is to pay the ransom. Even police departments have paid.
o Current government officials have warned of the dangers of cyber attack from China, Russia, North Korea, Iran, and even ISIL.
o In particular there is evidence that computers that control critical infrastructure, such as our water supplies and the electric power grid have already been infiltrated by malware controlled by foreign actors.
Weakening the security of our electronic networks is the last thing we should be considering in light of these threats.
We dont want the terrorists to go silent
The recent attacks in Paris and San Bernardino demonstrate that small, self-sufficient terror cells need not communicate electronically in ways that would reveal their intentions. It does not take much imagination to see how others can do this in the future. U.S. government action to require backdoors in encryption products would only alert terrorists to shun any electronic communication whatsoever in planning their operations. Even if backdoored encryption exposes a few terrorist plots, others intent on evil will soon learn the lesson. But a great deal of valuable information can be gleaned from patterns of electronic communication, even if the messages themselves cannot be read. Requiring backdoors could shut off this valuable intelligence and truly blind us.
Please dont weaken our security
Weakening the encryption on the computers we use has damaged and will continue to damage the security of our infrastructure, but it wont stop the terrorists. As I wrote in 1999:  the simple reality that strong encryption is widely available around the globe can rescue us from endless debate.
Respectfully submitted,
Arnold G. Reinhold

@_date: 2015-12-17 13:50:50
@_author: Arnold Reinhold 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
It seems to me that from the perspective of cryptography there are two classes of concerns with a true random number generator:
Class A. The unit is defective in some way. This includes:
A.1 The underling physics is wrong or improperly analyzed
A.2 The design is not reliable (a chain of op amps amplifying thermal noise  might pick up non-random signals, for example)
A.3 The unit as manufactured is defective (a bad solder joint rectifying radio signals, for example)
A.4 The unit fails in the field
Class B. The unit is deliberately modified in some way to reduce security, either by the manufacturer, or by a third party, perhaps during shipment.
The class A problems can be dealt with with some effort, including careful review of the design, good manufacturing practice and statistical health tests.  Class B, I would argue is almost impossible to detect. There are just too many ways to hide a modification and the situation is only getting worse. There was a recent report, for example, that someone has built a microprocessor that fits in a one millimeter cube. The good news is that most cryptographic systems in practice are not sensitive to modest deviations for perfect randomness, i.e. Class A problems. The physics does not have to be perfect. The bad news is that malicious modifications to a RNG (Class B) can be devastating.  Substituting a DRNG bit stream with a 32-bit state space can be very hard to detect, but renders encryption easy to break. Tricking a DH signature scheme into occasionally reusing the same nonce reveals the secret key by simple math.
The perverse conclusion I come to is that any device sold to produce random numbers should not be relied on alone for cryptographic purposes. OneRNG is a nice design, I bought one during the kickstarter campaign, but it is now made in China. If it gained significant traction in cybersecurity, it would be an irresistible target for Chinese signals intelligence, and it is also subject to tampering by others during shipment from China. The only remedy I see is to obtain random numbers for cryptography from more than one source and to have at least one of those sources built from general purpose hardware devices that are not intended for cryptographic use. It is much less feasible for even a state actor to subvert devices built in large quantity for a mass market, especially if they do not know in advance what software will be used. One example is a Raspberry Pi with its optional camera module. In this regard, I think a well documented open-source software/firmware TRNG package that runs on an off-the-shelf FPGA board would be a valuable addition to what is now a limited tool chest. Perhaps the simplest option is a high quality deterministic RNG seeded with an external source of entropy, such as dice. Each dice roll provides 2.58 bit of entropy, so 99 rolls (say 11 rolls of 9 dice) provides 255.9 bits of entropy. The 99 rolls can be converted directly into a 256 bit integer using bignum arithmetic. Xoring that with the output of OneRNG or the Intel x86 instruction would be a minimal level of defense. Arnold Reinhold

@_date: 2015-12-20 23:15:03
@_author: Arnold Reinhold 
@_subject: [Cryptography] Questions about crypto that lay people want 
Here are my attempts to answer your questions. But I would strongly recommend reading A History of U.S. Communications Security; the David G. Boak Lectures, National Security Agency (NSA), Volumes I, 1973, Volumes II 1981, partially released 2008, with additional portions declassified October 14, 2015 ( It was recently mentioned on this list. It is from the horses mouth, informative and very well written. There is crypto in the Bible: Several words in Jeremiah are coded in the AtBash cipher, the Hebrew version of the A->Z, B->Y,  substitution cipher. See  for more on ancient uses. Crypto was a secret art reserved for the powerful in most places.
There was really very little crypto technology available to the public before the introduction of DES in 1977. The Boak lectures discuss this at length, see Volume II  p. 27 ff. Telephones were the only electronic communication system in common use until the last two decades. Now much of our lives are carried out electronically and our cell phones are filled with private information.  I believe the forces on the restrictive side of the crypto wars were never reconciled to their loss back then. Any terrorist crisis becomes an excuse to reopen the question. In addition, improved security by Apple and to a lesser extent Google, has made strong encryption a default behavior. Im skeptical about quantum cryptography and I certainly dont see it filtering down to the general public by 2035 or ever. And I would not trust it in any case because it is too hard to audit.
Ciphers do not create secrets, instead they allow a small secret (a key) to protect a much larger amount of information. That key does not have to be random, a good passphrase can be an effective key, for example, but random quantities are the most efficient secrets, since they lack any structure. There are also situations in public key cryptography where it is important that a number used in an algorithm, while not secret, never be used twice. By using long, truly random numbers, the likelihood of a repetition is very small.
The word code" in English has many loosely related meanings, most of which do not involve any secrecy. One meaning is just a reversible correspondence between two sets of symbols, e.g. ASCII converting letters and numbers into binary patterns or a legal code that matches citation strings like 18USC703 with paragraphs of text. Typical keys to mechanical locks have a series of bumps and depressions on them. When inserted into the lock, these push up tiny pins to different heights. Those pins are split into two parts. When all the splits line up at the right level, the lock cylinder can turn. The heights for each pin can be (and are) assigned a number and the sequence of those numbers works just like a cryptographic key.
Encryption must be reversible to be useful. An operation that transforms a set of symbols in a reversible way is an example of a mathematical group, and cryptography makes use of different groups to create very complicated transformations. Cyclic operations are simple examples of groups, but they are not the only types of groups used. Information theory and crypto are closely tied to a different branch of physics, thermodynamics.  The notion of entropy, first discovered in the theoretical analysis of 19th century steam engines, is directly applicable. The formula on the headstone of the great thermodynamicist Ludwig Boltzmann is in everyday use in understanding cryptographic security.
Its a good question. The difficulty of certain problems is accepted in cryptography based on long failure to make progress against those problems, not any mathematical proof. There is a nice section on NSAs 1981 opinion of then relatively new public key cryptography in Volume II of the Boak Lectures, p. 33: "We did not leap to its adoption for a variety of a reason. Foremost, we were uncertain of its security potential. The fact that mathematicians bad not yet found a way to factor large numbers did not mean that there was no way.  It was an interesting mathematical puzzle, first put forward centuries ago. but with no great incentives for its solution beyond the satisfaction of intellectual curiosity, no perceived commercial applications, and so on. So there was no evidence of a great many brains having worked the problem over the years; Since then commercial applications abound and many more great brains have worked on the problem and NSA now uses public key crypto but not RSA, but there are still no proofs. Unfortunately (in my opinion) too many mathematicians direct their efforts to a theory of complexity that uses the language of cryptography, has little or nothing to do with practical cryptography (e.g. the P vs NP problem).
Some here would question your premise that HTTPS is good enough for banking. There are additional protections for your bank account, including the ability to trace and reverse fraudulent transactions and legal limits (in the U.S. at least) to your losses in case of provable theft. The security track record of electronic voting has not been good and the desire to have a system that is both audit-able and preserves the secret ballot is a big technological challenge. Finally not every one in the political world wants voting to be easy. Ill leave this one to others.
I would define strong crypto as systems that encrypt using algorithms for which there is no know way to to decrypt without knowing the key or at least a large portion of it. For systems that use completely random strings of bits as keys, such as symmetric ciphers, adding one bit doubles the systems security against ordinary decryption means. Two bits are thought to be needed to double strength against theoretical quantum computers. When 1024-bit keys 2048-bit keys are mentioned they are usually for public key cryptography and thus are not random quantities, but have considerable structure, e.g. a 1024 bit RSA key is typically the product of two 512-bit prime numbers. Doubling the length of such keys does much more that double their strength. In 2003 RSA Security claimed that 1024-bit RSA keys are equivalent in strength to 80-bit symmetric keys, and 2048-bit RSA keys to 112-bit symmetric keys. If that analysis still holds true, then 2048-bit RSA keys are 2^32 time stronger that 1024-bit RSA keys, or about 4 billion times harder to crack.
Hope this helps,
Arnold Reinhold

@_date: 2015-12-25 09:12:33
@_author: Arnold Reinhold 
@_subject: [Cryptography] Questions about crypto that lay people want to 
It may just be that prosecution of voter fraud in absentee ballot situations is easier to prove because there is more of a paper trail. One has to fill our a form requesting an absentee ballot, usually signing under penalty of perjury, and give a valid mailing address. The in-person system we use here in Cambridge, Massachusetts, is a paper ballot where the voter fills in ovals that are then scanned electronically as the ballot is fed into the ballot box. Vote totals are readable immediately after the election, but the paper ballots are still available for a manual recount if needed. There are poll checkers from the neighborhood paid for the day by the Citys Elections Commission who check voters in and cross of their name on a voter list. A second team does the same check before the voter can turn in their ballot. This gives two counts that should match the number of ballots in the box at the end of the day. Political parties can have their own poll watchers as well. This seems far superior to computer voting machines that either cannot be audited or produce some kind of cryptographic electronic receipt that few voters will ever check. If it aint broken...
Arnold Reinhold

@_date: 2015-12-25 16:01:52
@_author: Arnold Reinhold 
@_subject: [Cryptography] Photon beam splitters for "true" random 
I agree that ordinary dice are good enough for cryptographic applications, but in formal engineering good enough arguments dont always convince, so it is nice to know that high quality dice with very low bias are readily available from the casino industry.
True, doing a manual base conversion on a 256-bit number is non-trivial by hand, but it is trivial in Python or any other language with big number support. Assume rolls is an array with 99 entries, each integers from 1 to 6. Then:
seed = 0
for i in range (1,99):
   seed= 6*seed + rolls[i] -1
print hex(seed)
will produce a number with a maximum value 6^99, which is just under 2^256. (Note that log2(6^99) = 255.91). And  the program is easy to check by hand for smaller cases.
Theater and visibility can be an antidote to subversion. See, for example, the ongoing thread on "Juniper & Dual_EC_DRBG,  My original post which brought up the dice issue suggested combining the output of any commercial True RNG with the output of a good quality PRNG seeded, as per above, with 99 dice throws. Not quite. As others have pointed out, if the image is stored in SSD, it may be hard to erase. One could write a program that accessed the camera directly and never stored the image in memory. That would not be too hard on, say a Raspberry Pi with a camera module, but that is another program to write and verify and the Pi is not open source hardware. As some else said, Its turtles all the way down. Well dice are turtle free. You could well argue that the computer we do our encryption on are not turtle free either, and I agree that is a big problem. I think the best solution is to move crypto to much small computers with single CPUs, no OS, and memory systems that can be erased, but that is another discussion. In the meantime, I would argue that removing one stack of turtles is not silly.
Arnold Reinhold

@_date: 2015-12-26 19:33:50
@_author: Arnold Reinhold 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
I dont distrust SHA2 algorithmically, Im just pointing out it isnt needed to convert dice rolls into a 256 bit random key.  And I just used python as an illustration. All you really need is 256-bit addition to do the base conversion (you can multiply by 6 with 6 additions, obviously) and that is easy to implement on even the smallest microprocessor. The relevance here is that people thought they knew the mechanism by which the keys are generated. Turns out they didnt understand it fully.  I tried destroying an SD card with a hammer once. Its tricky to prevent chip fragments from flying all over the room. And how small do the have to be before data recover is impossible? Or what temperature do you have to raise them to? The best solution is to use a microprocessor that does not need to store any sensitive data in flash. I like those solutions too, but they all take some work. I think the Raspberry Pi with its camera module is a very promising solution, Ive been playing with one, but it takes work to get something anyone can easily duplicate with off the shelf components and easily understood software.
I could not agree more. In my view, the right approach it to create a toolbox of simple solutions with minimum room for turtles. Dice are one tool in the tool box. A way to produce a small, but cryptographically useful, amount of high quality randomness that anyone can understand and use. Again, I was suggesting a dice-seeded DRNG be combined with other sources for cryptographic puposes. Simple and makes attacks on the RNG process more difficult. There is no need to do anything to remove bias for ordinary dice for cryptographic purposes. The biases are too small to exploit. Higher bandwidth sources are indeed available, but not readily so, as numerous discussions on this list demonstrate. They all  have issues and require some level of engineering to be trustworthy. Every layer of complexity offers new possibilities for attacks.  Finished, easily usable projects in this area are certainly possible and I think very worthwhile, but I am not aware of any that dont still require significant effort to get working. Id love to be proven wrong.
Arnold Reinhold

@_date: 2015-12-29 00:49:52
@_author: Arnold Reinhold 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
[Interesting stuff about melting points, torch temperatures and belt sanders trimmed.]
Here is an alternative technology that might serve for key storage: Ferroelectric RAM (FRAM)  . Its a niche product but they are commercially available and I believe they meet all the above specifications, except they currently have much lower capacity than flash. "Unlike Flash or EEPROM there's no pages to worry about. Each byte can be read/written 10,000,000,000,000 times so you don't have to worry too much about wear leveling." Adafruit,  , sells a 256K-bit (32K-byte) FRAM module with an I2C bus interface for $10, single quantity. 32K is more than enough for key storage, except for one time pads. It should be easy to interface one to a Raspberry Pi, Beagle Bone, or Arduino board. Maybe someone could design a 3D-printed holder for one of these with an I2C connector and keychain attachment point.
Arnold Reinhold

@_date: 2015-12-29 16:31:00
@_author: Arnold Reinhold 
@_subject: [Cryptography] Understanding state can be important. 
I agree. Lack of write-protect is a bedrock problem in developing secure systems. One solution is to use live CD-ROMs. There are a number of live distros out there that can be compiled from source. Once burned, the checksums of the CD-ROMs can be verified on a variety of different platforms. The U.S. Air Force distributes such a system called Lightweight Portable Security ( But the flexibility of such systems is limited.
Here is a more general approach that I think can be made practical with relatively straightforward development. They fit with my view is that modern desktop and portable computer are too complex to be trusted and we should be looking at simpler computing models based on inexpensive, widely available off-the-shelf hardware, such as the Arduino and Raspberry Pi boards. I consider the Arduino devices more secure than the Pi because they are simpler and do not require an operating system, but the Pi is more useful and is at least stateless. My solution to the write-protect problem involves two single purpose devices. One is a board that would serve as a write-protect shield for an SD card. All SDcards implement a Serial Peripheral Interface Bus (SPI), which is supported on the Arduino and R Pi platforms. The SDcard specs, e.g.  include a set of commands. There are two for writing blocks: CMD24 and CMD 25. The wedge board would send commands and data to and from the SDcard transparently except that it would block the Write commands. This feature might be controlled by a switch or a jumper on the board. A light or sound alert could warn of a write attempt.
The write-protect board with an SDcard installed might be made to act like a USB flash drive.  This should be easy to do on a Raspberry Pi Zero ($5 retail), using the Pi Zeros USB port. One might have to load a special, verified OS distro beforehand. (Here are instruction to make the Pi read only, with a RAM disk.   I havent tried them.) It should also be possible to make a write-protect board using an Arduino platform, but with more software effort. The second component is a unit that would verify the checksums of SDcards. This could be built quickly on a Pi, and with more work, on an Arduino.  A variety of hashes and output formats (e.g. hex and base-32) could be supported, though a common standard would be desirable (SHA2 maybe). A Beagle Bone might also be suitable, though it is not stateless.  The Pi has a built in display generator for HDMI and analog video. The later could be used with an old TV or an analog display designed for automotive use to display the check sum. Alternatively a 7-segment display could blink out the checksum one hex digit at a time. (Ive used Morse code on a built in LED for debugging, but its not a general solution.)
The two devices, ideally on distinct platforms, would work together as follows. A suitable distro would be built and loaded onto an SDcard using an ordinary PC. That SDcard would then be verified using the checksum board. After it is verified, it could be used to load software onto to a stateless device such as another Raspberry Pi, through the write-protect board. With a trusted OS installed, other SD cards or flash drives can then be used with disk encryption, which if done properly deal with the secure delete problem
Once the necessary software is built and configured, setting up and using these devices would be relatively simple. Arnold Reinhold

@_date: 2015-12-31 18:06:18
@_author: Arnold Reinhold 
@_subject: [Cryptography] Understanding state can be important. 
What devices have you looked at? There are several out that have no firmware that should be up to the task, e.g. the Beagle Bone or the Teensy 3.2. Its  just a matter of programming.  What am I missing?
This seems like a perfect application for a small machines like the Raspberry Pi. Build minimalist distro that listens for log packets on Ethernet, or a USB port, say, and writes them to the Pis SDcard (with the distro on it). When the card is close to full, the Pi could signal an alarm via a GPIO pin to request a new pre-configured SDcard. Meanwhile it could keep overflow log messages in its RAM. Modifying server OSs to send copies of all log messages to the Pi should should be easy enough. Of course once the malware takes over, log messages will disappear or be phony, but there is no way to prevent that. The Pi software would have to be hardened to prevent malware-generated log messages from infecting the Pi or modifying what was already recorded. That doesnt seem insurmountable. The Pi software could also allow network read-only access to the log files, maybe depending on a jumper on the Pi board. This seems like a most worthy open source project. Once such a logging distro is available, hooking up a $5 Pi Zero, say, to any security sensitive server to save log files should be cheap and easy. Peter Gutmann wrote Agreed. Anyway, there are many microprocessors that include a few K of on board eeprom. The goal I thought was to have portable memory that is also selectively erasable. And inexpensive FRAM modules are available to today retail.
The need to erase is obviously an issue with any memory. The remanence properties of flash drives are known to be horrible as several posts have pointed out.  One alternative is to use SRAM with a small backup battery that can be removed, but my understanding is that this would not be a reliable erasure. FRAM is small, fast and allows large numbers of writes, so overwriting each byte randomly many times should not take long. The number of passes could be conservative enough (32?) to make recovery unlikely.
Happy 2016,
Arnold Reinhold

@_date: 2015-02-02 19:05:50
@_author: Arnold Reinhold 
@_subject: [Cryptography] best practices considered bad term 
Decades of enforcement of the National Electrical Code, along with other building codes, and UL certification for appliances, has led to a dramatic reduction in fires and fatalities therefrom in the US ( ), to the extent that many local fire departments are trying to find other functions, such as EMT and Hazmat services to justify their budgets.
Yes, the NEC analogy is not perfect: fire isn?t an intelligent enemy, finding cleverer ways to start and spread. But what is the alternative to best practice recommendations for cybersecurity? Telling every business to hire a consultant? Leaving the field to marketing departments with breathless claims of 5000-bit security or trade magazine articles written by writer who know little about the subject? There is a lot of knowledge and experience in this group, but it is poorly disseminated. Developing a knowledge base of basic things to do or avoid, with a clear statement that following it is not a guarantee of security, would advance practical security enormously, even if it were followed by people who only want to cover their backside. There are plenty of areas where I think consensus is possible, many mentioned in this thread. Maybe call it ?Minimum Best Practices? to avoid the implication that nothing more then following them needs to be done, or some other name, but lets do it.
Arnold Reinhold

@_date: 2015-02-05 11:09:29
@_author: Arnold Reinhold 
@_subject: [Cryptography] =?utf-8?q?The_World=E2=80=99s_Email_Encryption_Sof?= 
ProPublica is running a story about Werner Koch, author and maintainer of GPG, who is "is running out of money and patience with being underfunded.? He received money from the German government in the past to do a Windows port, but lately has relied on various ad hoc fund raising efforts that have only raised about $21,000, not enough to support his wife and 8-year old daughter. Props to Julia Angwin for a well-written piece.
We are really skating on thin ice. Arnold Reinhold

@_date: 2015-02-06 16:51:14
@_author: Arnold Reinhold 
@_subject: [Cryptography] best practices considered bad term 
Sound like the start of a best practices guide. Yes security is hard an yes we know more about why it?s hard than we do about how to do it right. But is that a reason not to collect what we do know in a form that implementers can avoid the grossest mistakes?
In the construction industry an early question when bidding a new project is how far down do we have to dig to find competent bed rock. Where is bedrock in computer security? How does one build the simplest system that we can be sure will not be compromised? There is no point to encryption if we don?t have a safe platform to encrypt on. Arnold Reinhold

@_date: 2015-02-09 08:02:50
@_author: Arnold Reinhold 
@_subject: [Cryptography] What do we mean by Secure? 
There is a company called Knox Box that sells small safes (~$200) that you can bolt to the outside of your house to store a key for the Fire Department. Only your local FD has keys to the safes (not all FDs participate), so you can make your door as secure as you want. You can buy the safes with a tamper switch that you wire to your alarm system. Yes, this introduce a failure mode where a firefighter loses or sells his key, but the risk is lower than weakly secured doors that any skilled thief can open. The analogy to computer security is that there are engineering solutions that can solve most security conundrums, once they are clearly stated. There has been a lot of defeatism expressed on this list recently that I think is overblown. I suggest that most security breaches do not occur because ?Security Is Sooooooo Hard? but because knowledge that is common in the cryptographic community is not well disseminated. Here is a quote from the Slashdot "How To Hack a BMW? thread:
   "If there were a single best book to read on cyber security, then perhaps we'd have fewer problems like what BMW had. But in reality, to get good at it, you have to have a vast familiarity with the literature and tools. You do that much reading, you might as well get a PhD. And my friends with PhDs focusing on security are in academia, not industry, so we get more security papers but not more secure devices."
Indeed, BMW had all the security components it needed in its remote car unlock system, it just didn?t put them together properly. Sneer all you want at ?best practices? documents but they are a proven tool to help change happen in large organizations. Arnold Reinhold

@_date: 2015-02-10 14:16:07
@_author: Arnold Reinhold 
@_subject: [Cryptography] What do we mean by Secure? 
I don?t think it is as hard as you suggest. The policy questions you rase about who has access are ones that auto makers have dealt with for a century. Cars have never been as secure as technology allowed. Owners who lock their keys inside want solutions other than breaking a window. Finance companies need to be able to repossess cars with minimal damage or they must charge higher interest rates. (I suspect the ability to turn on remote access after the owner disabled it was a feature, not a bug.) And the communications model is one central server to many clients. That makes checking the signature of messages to the cars relatively straightforward.
Remember the threat here, car thieves. Even if the underground hacking community finds an exploit and offers it for sale, there are only so many thieves who can employ it. And BMW apparently has the ability to remotely upgrade car firmware, so a spike in break-ins could be detected and remedied before excessive damage is done. (Even without remote upgrades, it has a well tested dealer recall mechanism that do them.)
The Internet of Things is just getting started. Thousands of programming projects are not doubt already underway, some involving critical infrastructure. Most will be designed and coded by security muggles; there are simply not enough wizards to go around. A binder labeled "best practices" will not solve all the IoT problems, but it can avoid a lot of them. And, of course, among the best practices can be a requirement for expert security design reviews at several stages of each project, including an analysis of unprotected threats, so no one has a false sense of security. But you don?t want those reviews to get bogged down in fixing the most basic mistakes.
Arnold Reinhold

@_date: 2015-02-20 06:02:04
@_author: Arnold Reinhold 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
The Bradley (Chelsea) Manning incident in 2010 should have been more than enough warning to the NSA. Manning siphoned off vast swaths of SIPRNET content with little effort, not that different from what Snowden did more than 3 years later. How much of a heads up does the world?s largest information security agency need?
The reliability equation is in play here. Say a system has N components any one of which failing will cause an overall system failure. It could be links in an anchor chain or in this case trusted employees. If each employee is 99% reliable how big does N have to be for a 50% probability of failure? Answer 69 employees. If the vetting process is 99.9% reliable, N goes up to 692 employees. With thousands of people having access to highly classified information and easy ability to copy it, a leak is inevitable. Maybe it was only sysadmins at NSA who had access and the tools to copy without being noticed, but how many of them did NSA have? Did NSA conduct a lessons learned security review after Pvt. Manning? If so, I?d love to know what what actions were recommended and which were carried out. It?s great fun to read all the juicy details of NSAs activities, but the government's inability to keep anything secret should be troubling too. Think about nuclear weapon design software. Arnold Reinhold

@_date: 2015-02-23 13:15:08
@_author: Arnold Reinhold 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
I love the US Constitution too, but let us not make too much of a morality play about this. There are always reasons for someone to turn on an organization he or she was once loyal to (or appeared to be).  Communism was the biggie in the mid 20th century, but religion, political ideology, money troubles, blackmail over an illicit affair, mental illness, or just a new significant other can all do it. I heard Daniel Ellsberg, the guy who leaked the Pentagon Papers, speak years ago and he said a divorce and new girl friend helped alter his view of the Vietnam War. The United States went from Perl Harbor to the Invasion of Normandy in just over 2-1/2 years. I suspect NSA put a lot of changes in place within months after Snowden. The point is you cannot have vast quantities of secrets circulating freely on digital networks accessible to thousands of people, no matter how well vetted, even if your goals and methods are pure as can be, and not expect major leaks from time to time. Securing US secrets is one of NSA two missions. They should have gotten the message with Pvt. Manning. Obviously they did not. Arnold Reinhold

@_date: 2015-01-15 19:29:29
@_author: Arnold Reinhold 
@_subject: [Cryptography] The Crypto Pi 
One can purchase a 5 megapixel HD camera module for the Raspberry Pi for $25.  An extensive Python support package is available, including the ability to read an image unencoded.    Using a camera as a source for entropy was discussed at length in a thread here last year. A camera might be cheaper than a plug-in HRNG, doesn?t take up a USB port, and could be easier to audit.
Arnold Reinhold

@_date: 2015-01-16 10:09:15
@_author: Arnold Reinhold 
@_subject: [Cryptography] The Crypto Pi 
A bit more detail.  The picamera Python package provides a ?bayer? mode which gives the raw bits from the camera pixels. (Bayer is the inventor of the RGGB color filter mosaic that overlays the camera chip in this and many other single chip color cameras.) According to the documentation:
While unpacking all this data and processing it could be a pain for imaging, hence other modes like jpg, rgb and yuv are available, for RNG purposes bayer is ideal. The whole mishmash or chunks of it can be fed directly to a whitener such as a cryptographic hash. If speed is a concern, it might even make sense to just hash every fifth byte containing the low order bits, though careful experimentation, e.g. to look for correlation between nearby low order bits, is in order first. Arnold Reinhold

@_date: 2015-01-16 11:12:54
@_author: Arnold Reinhold 
@_subject: [Cryptography] The Crypto Pi 
There are two potential sources of randomness in a digital camera image, randomness in the scene being imaged and randomness in the picture taking process, e.g. from quantization noise, thermal noise, etc. We discussed last year various ways to insure the former (M&Ms or other small objects on a vibrating plate, a bubble wall in a water tank, tropical fish, Sun?s patented Lava Lamp, ...) The later needs some experimental testing. How close to identical are two successive image captures of ostensibly the same scene, or alternatively taken with the lens cap on? How much entropy is in the difference? No point in speculating, the Raspberry Pi seems an ideal platform to do such tests.
Arnold Reinhold

@_date: 2015-07-08 20:16:57
@_author: Arnold Reinhold 
@_subject: [Cryptography] Crypto Wars 
For what its worth, here is a link to a white paper, "Strong Cryptography: The Global Tide of Change, that I wrote for the Cato Institute  on the topic in 1999:
 My CipherSaber page might also be of  interest:   (Yes, I know, its RC4 based...)
Arnold Reinhold

@_date: 2015-06-01 10:27:42
@_author: Arnold Reinhold 
@_subject: [Cryptography] Uniform Data Fingerprint 
A small suggestion: instead of slavishly following RFC4648 Base-32 format, I would suggest a modified Base-32 where the letters I, O, and L  are excluded from the encoding alphabet as they are too easily confused with the digits 0 and 1 (L being confusing in lower case). Maybe exclude S as well for potential confusion with 5. Only 32 of the possible 36 characters are needed, so excluding three or four characters poses no technical problem. Here is a worst-case example I concocted: 0lSS0-l1151-I5S1O-I05S1 Of course the potential for confusion is font dependent but I dont think it is practical to specify fonts in such a standard.
Since the fingerprint strings are intended for human recognition, we should do everything possible to minimize confusion.  At the standards writing stage, this is a cheap improvement.
Arnold Reinhold

@_date: 2015-06-01 14:29:16
@_author: Arnold Reinhold 
@_subject: [Cryptography] Uniform Data Fingerprint 
I should have read the RFC more carefully. There is a second "Base 32 Encoding with Extended Hex Alphabet that confused me (section 7).  Base-32 Encoding (section 6) as you say discards the digits 0 and 1, but not 5. Close enough. Its good to know that they and you got it right the first time.
Arnold Reinhold

@_date: 2015-06-07 13:54:19
@_author: Arnold Reinhold 
@_subject: [Cryptography] Farewall, Alan Turing 
There is a third possibility, it could have just been an accident.  From the Wikipedia bio of Turing:
"Philosophy professor Jack Copeland has questioned various aspects of the coroner's historical verdict, suggesting the alternative explanation of the accidental inhalation of cyanide fumes from an apparatus for gold electroplating spoons, using potassium cyanide to dissolve the gold, which Turing had set up in his tiny spare room. Copeland notes that the autopsy findings were more consistent with inhalation than with ingestion of the poison. Turing also habitually ate an apple before bed, and it was not unusual for it to be discarded half-eaten.[115] In addition, Turing had reportedly borne his legal setbacks and hormone treatment (which had been discontinued a year previously) "with good humour" and had shown no sign of despondency prior to his death, setting down, in fact, a list of tasks he intended to complete upon return to his office after the holiday weekend.[115] At the time, Turing's mother believed that the ingestion was accidental, resulting from her son's careless storage of laboratory chemicals.[116] Biographer Andrew Hodges suggests that Turing may have arranged the cyanide experiment deliberately, to give his mother some plausible deniability.[117] The half-eaten apple was never tested for cyanide.
Regardless, his death was tragic and a loss for the world.
Arnold Reinhold

@_date: 2015-06-12 12:45:40
@_author: Arnold Reinhold 
@_subject: [Cryptography] Farewall, Alan Turing 
I think its worth remembering that in the mid-1950s chemistry was the main option available for a person interested in experimenting with science at home. Electronics was all vacuum tubes and relays.  No computers were available to hobbyists. Amateur radio was another option, but Turing had spent the war dealing with the most advance radio networks available, so one can easily imagine his lacking interest in that hobby. Overarching concerns about occupational safety in chemistry were far in the future. Working with dangerous chemicals just added excitement. I was in elementary school at the time and did things that would have brought in the SWAT teams and Hazmat trucks today, though I think I knew enough not to mess with cyanide.
The fact that Turing had been working with cyanide solutions, in a confined space no less, and died of cyanide poisoning passes the Duck Test for me. Arnold Reinhold

@_date: 2015-03-02 22:29:23
@_author: Arnold Reinhold 
@_subject: [Cryptography] Cheap forensic recorder 
More generally, I think the way to approach ?nothing up my sleeves? hardware is to move down in complexity, not up. I?d like to see a series of small security devices based on minimalist processors. We?ve talked about HRNGs in the past. How about a small device that did nothing but compute the hash of the contents of an SD card? If you are using a Raspberry Pi as your forensics tool, you could use a Beagle Bone with a different distro to check the SD card hash. Even better would be an Arduino class machine with the smallest AT processor that can do the job. For minimal cost, one could blink out the hash in Morse code on a single LED. In base-32, you?d need ten 5-character groups for a 250-bit hash, this would take a minute and a half at a relatively slow 7 groups per minute. The target hash could be printed with the dots and dashes next to the characters so there would be no need for a user to know Morse, e.g.:
R   .-.
X  -..-
6   -?.
K   -.-
Of course Arduino LED and LCD display shields are available at more cost for easier hash output reading. Yes you?d have to trust the firmware in the checker microprocessor. But this would be open source and simple enough to read. And one could checkthe device against different SD cards whose hash is tested elsewhere. The possibility that the checker firmware has a hidden  ?if you see hash X report hash Y? backdoor could be ruled out by accounting for all long non-instruction strings in the object code. Another useful device might be an SPI bus repeater wedge that would block all write commands unless a jumper was in place. This could act as trustable a write protect switch for SD cards and PC BIOS chips. Arnold Reinhold

@_date: 2015-03-04 11:03:46
@_author: Arnold Reinhold 
@_subject: [Cryptography] Cheap forensic recorder 
Perhaps in terms of functionality, but not in terms of trust model. TPMs, as I understand them, are opaque black boxes. We have to trust the manufacturers for their content. That works fine in a wide swath of corporate security applications, where policy must be asserted and maintained over many machines and users. But I don?t see how TPMs help individual practitioners who seek complete control of their computing environment. Some might say the Thompson ?Trusting Trust? paper makes that goal unattainable, but I?m not convinced. Thompson assumed a fixed target that code hidden in the compiler attacks. In large systems it might even be possible to hide a large code blob that figures out what is going on and devises an attack. But by moving down in the complexity chain instead of up, it becomes harder to hide a smart evil code blob. Using a variety of microprocessor architectures and software sources, makes the Thompson attack even more difficult.
My approach is an ecosystem of simple devices made from inexpensive off the shelf components with wide availability and limited capabilities that solve specific problems, in this case just verifying the hash on an SD card. Each problem in the trust chain, e.g where does one get the known good hash value, can be dealt with separately. Maybe there is a different approach to ?nothing up my sleeves" control of computing using TPMs. If so I?d be interested in hearing how it might work.
Arnold Reinhold

@_date: 2015-03-21 22:28:24
@_author: Arnold Reinhold 
@_subject: [Cryptography] Kali Linux security is a joke! 
Use of MD5 is indeed a big ? and totally unnecessary --- risk. While you are correct that a collision attack requires some ability to modify the original file, that is hardly an insurmountable obstacle. All an attacker has to do is inject some random bits in the target, say by modifying an included icon. A member of the team could be a mole or suborned by bribery or blackmail. Or malware could modify the tool chain in a way that injects the required bits at the last minute. While such ability would allow other attacks, there are many reasons why an attacker might want hacked and clean versions of the same program.  MD5?s vulnerability to collision attack has been known for over a decade.  Here is a link to a sci.crypt discussion of this question from 2004:   Given that there are hash functions considered safe against collision attacks, there is no excuse for continuing to use one that isn?t after all this time. Arnold Reinhold

@_date: 2015-05-21 16:25:37
@_author: Arnold Reinhold 
@_subject: [Cryptography] NIST Workshop on Elliptic Curve 
I believe that understates the situation. I was crypto-curious when I was a grad student in math at MIT in the late 1960s and there was almost nothing in the MIT libraries on the topic except Shannons paper and pre-WWII hobby stuff. David Kahns 1967 Codebreakers was groundbreaking, but basically covered up to the 1950s and did not have much technical detail on even WW-II systems. I remember one textbook on shift register sequences that looked relevant, but had no crypto discussion. The buzz I remember was that working on cryptography could get you a visit from the government and your work could be classified with you denied clearance to see it.
I attended a  briefing in 1971 by BB&N on the ARPA net where I asked about encryption. I was told that they specifically did not incorporate encryption into their work because if they did the entire project would be classified and they did not want that. Instead they intended to rely on link encryption when it was fielded for the military.
The publication of DES in 1975 was highly anticipated. I think I still have the copy I made from the Federal Register as soon as it became available. It wasnt the first block cipher. Horst Feistel had an article on Lucifer in Scientific American in May 1973, but DES would tell us what the NSA considered strong. RSA appeared two years later in 1977.
The significance of a 56-bit key was understood almost immediately. NSA was giving us security but only so strong. But it was a benchmark and work on what could be done to do better began immediately. The publication of RSA in 1977 added another dimension to public cryptography, but DES was the big starting point.
There have been comments on the list that this was the last time NSA helped public cryptography. I dont see how to square that with SHA-1, which was a major improvement over MD% and SHA-2 which still seems secure. Note that both used nothing up my sleeve numbers, unlike the NIST elliptic curves, to reduce suspicion that they had backdoors. Arnold Reinhold
PS: True story: A friend of mine was working on having PCs communicate across air gaps using ultrasound. He was testing out his implementation of the concept when his kids came home and said Dad, thats really loud. Lends a whole new purpose to take your kids to work day.

@_date: 2015-11-14 17:38:57
@_author: Arnold Reinhold 
@_subject: [Cryptography] Bear Bonds - a new crytpocurrency 
Why not include a well-tested algorithm, such as SHA-512, in the hash chain? And assuming one would trust a novel hash algorithm before years of analysis have taken place, why is "a modest memory requirement of 85 KB per input a good thing? It seems to me such a tiny memory retirement quickly leads to control of mining by the custom chip crowd. Arnold Reinhold

@_date: 2015-11-15 12:46:10
@_author: Arnold Reinhold 
@_subject: [Cryptography]  Fwd: Re:  Post Quantum Crypto 
More to the point, a great deal of effort has gone into finding experimental evidence of string theory, using the most powerful particle accelerators and cosmic ray detectors, with no success so far. Even if string theory effects are eventually found, they are expected to show up at the extremely high temperatures particle accelerators and cosmic phenomena generate, while quantum computing operates at extremely low temperatures. A q-bit system or quantum computer that failed to perform as predicted by standard 3-D quantum mechanics due to the effects of extra dimensions would be a revolutionary breakthrough in physics and instant Nobel Prize material.

@_date: 2015-11-20 16:14:29
@_author: Arnold Reinhold 
@_subject: [Cryptography]  Re: Burner phone == One Time (i)Pad 
Searching for potential terrorists is a needle in a hay stack problem. Anything that narrows that search potentially helps the good guys. Presumably they can get lists of the IMSIs for all prepaid phones from the manufacturers and give their traffic special attention. Its not hard to imagine patterns of suspicious behavior that could be easily detected, phones that go dark after a short interval of use, for example. It two such phones called each other, that would be a huge red flag and gain priority handling. Remember all cell phone that are powered on (and maybe some that arent) are tracked by the cell towers and that location data is saved. If a terrorist cell switches phones all at the same time, that pattern should be easy to pick up. But if they dont, new phones can be linked to older ones in the network. And if anyone in the group foolishly carries a burner and real phone together, it would not be hard to link them. Given the NSA  thoroughness revealed in the Snowden document, I would be very surprised if they dont have a whole department devoted to tracking burner phones. Yes, some will turnout to be innocent uses like battered women shelters or people with limited income, but those will not be hard to separate out. Arnold Reinhold

@_date: 2015-11-22 10:04:48
@_author: Arnold Reinhold 
@_subject: [Cryptography] =?utf-8?q?_Re=3A_Pearl_Harbor_and_Crypto_=28was_Re?= 
=?utf-8?q?terrorists_with_encryption=29?=
The Codebreakers chapter is an essential read for anyone interested in cryptography, but doesnt materially change the conclusion. The US was generally aware that conflict was immanent and sent a message to all its bases in the Pacific in late November 1941 saying war could break out at any time.  The US did intercept and decode diplomatic traffic between Japan and its consulate in Honolulu requesting very detailed information on which warships were in the harbor and where each was located. This alarmed some lower level intelligence officers who finally got a message off to Pearl Harbor at the last minute, but it did not arrive in time due to communication screw-ups. The bottom line is that the Japanese fleets strict adherence to radio silence (orders for the attack were delivered by hand before the fleet left Japan) succeeded in preserving surprise. By contrast, the attack on Midway required radio traffic for coordination of an already dispersed fleet. The target identity was super-encrypted using a map code that took considerable ingenuity to break convincingly. That intelligence plus a fair amount of luck resulted in a devastating defeat for the Japanese Navy, from which they never recovered, and is widely considered the turning point in the Pacific war. Kahns chapter on Midway is also worth reading.
The lesson here is that even with strong encryption, electronic communication provides a wealth of opportunities for intelligence gathering. Good communication security requires a great deal of discipline, with even the best trained operatives making mistakes that can be exploited. The last thing we should want is for terrorists to learn to completely avoid electronic communication. Arnold Reinhold

@_date: 2015-11-23 15:46:12
@_author: Arnold Reinhold 
@_subject: [Cryptography] Bin Laden satphones 
The 9/11 Commission did not say which paper it was, but the White House and members of the commission said it was the *Washington Times*,  not the Washington Post or the New York Times. And no, the Washington Times article in question did not say anything about the U.S. ability to track bin Laden. All it said, (in the 22nd paragraph) was He keeps in touch with the world via computers and satellite phones and has given occasional interviews to international news organizations.   Of course, as others have pointed out, one does not need a leak to explain why bin Laden ditched his satellite phone right after he was almost killed by a cruse missile. But the government rarely lets facts get in the way of an alarming story that bolsters their agenda, as we are seeing today in the post-Paris battle over encryption.
Arnold Reinhold

@_date: 2015-11-25 12:11:46
@_author: Arnold Reinhold 
@_subject: [Cryptography] Pearl Harbor and Crypto 
That may be true of the satellite phone system itself, but I believe that the U.S. military has other equipment for locating emitters more precisely, especially when they are given a rough position to 10 km accuracy. Some missiles are designed to home in on emitters.
Im surprised no one has proposed this simple system for dealing with the security implications of the European refugee crisis: when the refugees are being processed initially, give them all cheap cell phones or special SIM cards for the ones they already have. Then get a voice sample along with ID photo and other biometric data, say be asking them to test the phone. Give the refugees a modest monthly voice and text budget they can add to, more if they supply their own phone. The phones would serve a humanitarian purpose, helping the refugees get settled, find lost loved ones and look for work. But the phones would also allow them all to be tracked 7/24, just like the rest of us. Any phone that falls off the network or stops being used by the same voice would add its owner to a suspect list. The phones could even serve as a credential: Cell phone please instead of Papers please at checkpoints.
Arnold Reinhold

@_date: 2015-10-01 14:00:32
@_author: Arnold Reinhold 
@_subject: [Cryptography] Paper check security 
There has been a lot of discussion here about credit card security and the strengths and weaknesses of the newly required in the U.S. EMV chip. (e.g the thread "Insecure Chip 'n' PIN starts tomorrow"). But I havent heard anything at all about the security of paper checks. Banks now allow such checks to be deposited by scanning them or even photographing them with a smart phone. This defeats more than a century of inventions designed to make paper checks unforgeable.  It also increases the likelihood that recipients of your checks will keep them on file for long periods and/or dispose of them insecurely, allowing others to get at your bank account numbers and other information needed to Photoshop a phony deposit.
It would seem easy to create a cryptographic authenticator that makes each blank check unique. It could be a hash of the account number, check number and a secret held by the bank or check printing company. A public key signature would be better, of course, since it could be verified by anyone. The authenticator could be printed on the check as text or as a 1-D or 2-D bar code. The best case would be for the bank processing a deposit to verify the authenticator, but just having it big enough to be readable in a scan would allow the account holder to challenge a phony deposit by showing the authenticator was used twice. Of course getting the banking community to agree on anything is a long, difficult process, but a similar feature could be included in accounting packages that print checks, in which case the authenticator could verify the payee and amount as well. Arnold Reinhold

@_date: 2015-10-01 20:05:51
@_author: Arnold Reinhold 
@_subject: [Cryptography] Paper check security 
I think most consumers in the U.S. get their checks from check printing firms and I believe they generally include standard security features as described here    The most recent batch of checks I got a couple of weeks ago has the lock and MP logo and security features listed on the back as that page mentions, including a signature line that is microprinted. (I never noticed that before.) The Check 21 Act says (Sec. 4 b) "A substitute check shall be the legal equivalent of the original check for all purposes, including any provision of any Federal or State law, and for all persons if the substitute check
(1) accurately represents all of the information on the front and back of the original check as of the time the original check was truncated; 
That arguably includes any exposed security features. In several places (e.g. SE 7aiD) act talks about the banks "production of the original check or a better copy of the original check to verify a claim.
If the original check is scanned by the bank, more gross features like brown stains and colored spots that indicate chemical tampering could show up on a high quality scan. On the other hand if a criminal can simply scan his Photoshopped forgery at relatively low resolution, the check security features indeed become useless.
And even if the banks dont keep the original, at least if the original check is physically deposited in a bank, it will be safely destroyed instead of staying in someone file cabinet forever after being scanned by the payee.
That assumes consumers review their bank statements regularly. I suspect few do. Id at least like to see a system where I get an e-mail every time one of my checks clear.
Arnold Reinhold

@_date: 2015-10-02 14:44:36
@_author: Arnold Reinhold 
@_subject: [Cryptography] Paper check security 
Right, but I still have the option of using professionally printed checks with all the cool paper security features. They are available in sheets designed for laser printing too. And I think most people and business do use pre-printed checks. [crypto-relavance on]
But if the software that people use to print their own checks added a scan-surviving cryptographic signature that included the core information (bank routing, account number, check number, date, payee, amount and whatever Ive left out), then I argue a plain paper inkjet check would be more secure than one printed on a fancy form but without the cryptographic signature.  Note that unlike a lot of cryptographic proposals, this would be easy to implement and would not require any time-consuming standards making process to get started. One software vendor, e.g. Quicken, could pick a format and start using it. Banks would ignore of course, at least until sufficient customer demand emerged. A simple app could be used to verify the sig.  If a different standard emerges later, it can be used on new checks without need for backward compatibility. Arnold Reinhold

@_date: 2015-10-02 17:21:19
@_author: Arnold Reinhold 
@_subject: [Cryptography] Paper check security 
In the use case where banks or check printers authenticate each check as unique, there wouldnt seem to be much of a PKI needed. One could get the public key to verify checks from the banks web site perhaps verifying the banks electronic signature via a CA. Individuals printing checks would need to register their public key, a service the company that makes the check writing software might offer. I dont expect the banks to verify the signatures at first. A few banks might make arrangements with the check writing software company to offer the full service as a way to capture new customers.. The end game is just strong electronic fund transfer with data transmitted by machine readable paper or images instead of a digital electronic communication network. Isnt that is what checks have become anyway, but with weak security?
Arnold Reinhold

@_date: 2015-10-15 14:28:38
@_author: Arnold Reinhold 
@_subject: [Cryptography] Fwd: freedom-to-tinker.com: How is NSA breaking so 
This article suggests that the widespread use of a common prime modulus in Diffie-Hellman may be the weakness NSA is exploiting to break much Internet traffic.

@_date: 2015-10-15 14:39:58
@_author: Arnold Reinhold 
@_subject: [Cryptography] Fwd: freedom-to-tinker.com: How is NSA breaking 
Here is a link to the research paper: Imperfect Forward Secrecy: How Diffie-Hellman Fails in Practice
David Adrian Karthikeyan Bhargavan Zakir Durumeric Pierrick Gaudry Matthew Green J. Alex Halderman Nadia Heninger Drew Springall Emmanuel Thome Luke Valenta Benjamin VanderSloot Eric Wustrow Santiago Zanella-Beguelin Paul Zimmermann
Arnold Reinhold

@_date: 2015-10-20 14:48:03
@_author: Arnold Reinhold 
@_subject: [Cryptography]  Other obvious issues being ignored? 
Excellent question. Heere is my list of issues are the cryptographic community knows about, but keeps ignoring:
1. Compilers that break security code in the name of optimization. The C community in particular insists that it is ok to delete potential overflows, even deleting assert checks,  and to ignore zeroization of data that will not be used again. Rather than demanding change, security programmers resort to subterfuge to fool the compiler, but there is no guarantee that the tricks employed will still work in the next release of the compiler or when ta newt build is done with a different optimization setting. 2. Modern computers are too complex to be secure. Its turtles (processors) all the way down. Few people even know how many processors are in their computer and peripherals they use, much less have any visibility into the code that executes on those processors. And the universal presence of user accessible I/O ports creates huge security holes.
3. Embedded systems often have no way to receive security updates.
4. There are no published standards (at least that I am aware of) that specify how to manage passwords. As a result organizations still do bonehead things like failing to use salt or keeping old password files on their network after they have upgraded security. 5. Designing cryptographic hashes for best performance on hardware.
6. Failure to explain the provenance constants used in cryptographic algorithms, preferably using nothing up my sleeve numbers.
7. Fuzzy goals for cryptography. We want to access our data any time we want, anywhere we want, on any platform we want and to share it with anyone we want but we want all of our data to be secure against the bad guys, who we cant clearly define and dont all agree on. And we dont want to be inconvenienced in any way.
8. Ignoring physical security. The list here is endless, but my favorite at the moment is searching people when they leave a secure facility, but not when they enter. Arnold Reinhold

@_date: 2015-10-21 08:55:00
@_author: Arnold Reinhold 
@_subject: [Cryptography] Other obvious issues being ignored? 
The fact that the C standards ALLOW certain optimizations does not mean that compiler writers are REQUIRED to do the most evil things imaginable in certain situations, e.g. removing assert tests that could involve an overflow without even generating a warning (we had a long thread on this a while back). So I think compiler writers share the blame with the standards committees. Exactly, but the C-family languages still dominate the software industry. At least Apples new Swift language includes arithmetic operators that explicitly allow overflows, but I have found no info about zeroization in Swift. Ideally I would like a language whose output object code is canonical for any given platform, so we could predict exactly what our code will do. It should be possible to expect two separately written compilers to produce identical object code for a given platform. This might be done with an intermediate language that specified the resulting object code for each major platform. We dont need a lot of fancy language features, a subset of C might do.
Indeed. NSA could help rebuild is damaged reputation by sharing its approach.
The original question was what problems are we ignoring and lack of crypto-safe programming languages is a biggie. Arnold Reinhold

@_date: 2015-10-26 15:06:25
@_author: Arnold Reinhold 
@_subject: [Cryptography] Other obvious issues being ignored? 
The opinions on this thread range from the C can never be safely used for cryptographic programs to C can be safely used if you really know what you are doing and are really, really careful. But C and C++ are very popular languages with a huge corpus of code written in them and not going away any time soon. Its as if we are in some fraternity that throws members into the lake if they mess up the secret handshake sequence and we all think this can never change or that this is ok because those are the rules or even that getting the secret handshake right builds good character.
Perhaps we should shift the focus of this discussion from the problems we perceive (or dont perceive) with C-family compilers  and language specifications, to what steps the cryptographic community might take to get better cooperation from the developers of GCC and LLVM. Others have pointed out that commercial C compilers tend to behave better, possibly because the companies that supply them motivate their (paid) developers to feel more responsible to their customers. So what motivates the developers of free compilers like GCC? I speculate it is some combination of the following factors:
1. The intellectual challenge of developing compilers that produce highly efficient code.
2. The prestige of working on a project that is so widely used and influential.
3. A spirit of competition between compiler teams to produce the fastest object code.
4. Normal human defensiveness in the face of criticism.
5. Maybe some impish pleasure in creating a puzzle world that others have to navigate. (Peter Gutmans recollection that gcc 1.17 tried to load the games Rogue and NetHack as an form of undefined behavior is suggestive in this regard.)
Developing compiler features that support safe cryptography could be as intellectually challenging and interesting as squeeing the last fraction of a percent of code efficiency. I know many people have tried to get cooperation from compiler developers and been stonewalled. A more organized approach might stand a better chance. Here are some possibilities that occur to me:
1. Drafting an open letter/petition that hopefully many members of the cryptographic community would sign. It might calmly state the problems many of us are seeing and seek the developers cooperation. At the least, it might be sent to the Free Software Foundation, which nominally is the home for GCC, and Richard Stallman, FSFs founder. I think both care about their reputations, Im less familiar with the LLVM community and whom to approach. Maybe Chris Lattner at Apple, a key player in LLVM and Swift. Without being too heavy handed we might point out the Snowden revelation that NSA has a program to influence standards in order to create vulnerabilities for NSA cryptanalysts, and that intentionally or not, their compilers are de facto aiding this effort. I would think FSF and the individuals mentioned would be horrified at that possibility.
2. Organizing a professional meeting where cryptographers and compiler developers can discuss the impact of language tool chains on security. It might be under FSF or neutral auspices, such as the ACM, perhaps with a professional facilitator or mediator. 3.Developing a cryptographic community standard for crypto-safe compiler behavior. Ray Dillinger proposed a Crypto C, but a crypto-safe standard might even be language neutral, e.g. requiring a way to guarantee data is erased after it is no longer needed, a way to prevent code optimizations that introduce side channels, not changing code execution sequence without warning, pragmas to control or test for optimization level (so compiler behavior is under version control), etc. Sample test programs might be provided in pseudo code and/or in the major languages.
While there are people who have strong feelings of frustration, it seems to me that improving tool chains is a heck of a lot easier from an organizational perspective than getting fixes to bad Internet protocols adopted. Maybe treating this as a human relations problem rather than a technical one is the best way to make progress.
Arnold Reinhold

@_date: 2015-10-30 17:48:09
@_author: Arnold Reinhold 
@_subject: [Cryptography] letter versus spirit of the law ... 
You make a compelling case that how errors are to be handled is a design tradeoff that requires careful consideration based on the nature of the application.   But I can see no argument that this tradeoff should be decided by the compiler, much less covertly and with no warning, with the decision possibly changing depending on what compiler optimization level is selected. And suggesting that people just choose another language, as others have, is not helpful. Many software projects have long been committed to C or C++ and changing languages can be prohibitively expensive. Even for new starts, the large pool of programmers reasonably proficient in C or C++ (tho maybe not expert), the extensive libraries available in those languages and the languages' reputation for efficiency, means those languages are still attractive for new starts, particularly in the embedded world.
Have your ever had a loved one in the intensive care unit of some hospital, hooked up to a bunch of beeping monitors and pumps and thought about what compiler was use to generate code for those boxes?
Arnold Reinhold

@_date: 2016-08-08 13:14:17
@_author: Arnold Reinhold 
@_subject: [Cryptography] Generating random values in a particular 	range 
The 2007 Supreme Court ruling in KSR Int'l Co, v. Teleflex inc., which held that combing art from two previous patents in a straightforward way was not patentable, might be relevant here. Arnold Reinhold

@_date: 2016-08-17 14:43:40
@_author: Arnold Reinhold 
@_subject: [Cryptography] Generating random values in a particular 	range 
... In general, there ought to be a standard
patent-examiner practice to immediately reject anything like
In a series of cases, most recently Alice Corp. v. CLS Bank (2014), the U.S Supreme Court has said pretty much exactly that, and Im told that the U.S. Patent Office will now reject such patent applications. 'Simply appending conventional steps, specified at a high level of generality, to a method already well known in the art is not enough to supply the  inventive concept  needed to make this transformation. Mayo, supra. The introduction of a computer into the claims does not alter the analysis. Neither stating an abstract idea while adding the words apply it,  Mayo, supra, , nor limiting the use of an abstract idea  to a particular technological environment,  Bilski, supra, at 610611, is enough for patent eligibility. Stating an abstract idea while adding the words apply it with a computer simply combines those two steps, with the same deficient result. Wholly generic computer implementation is not generally the sort of additional featur[e] that provides any practical assurance that the process is more than a drafting effort designed to monopolize the [abstract idea] itself. Mayo, supra, Pp. 1114.'   Alice Corp,Syllabus
Challenging existing patents can still be expensive. Any serious patent litigation is costly and the lawyers defending such a patent will usually attempt show there was an additional get up and walk around the desk step that supplies an "inventive concept needed for patent eligibility. However in this thread's Blackberry patent case, I think an obviousness argument is more likely to prevail.  Arnold Reinhold

@_date: 2016-12-04 15:28:26
@_author: Arnold Reinhold 
@_subject: [Cryptography] TV set power correlates to TV channel? 
I think it is worth remembering that in the old days of analog TV, it was quite simple to determine what channel someones TV was tuned to. One need only point a directional antenna at their residence and measure the frequency of their TVs local oscillator. Throughout the last half of the 20th century, vans with yagis on top would prowl the streets gathering such information for rating services. In the UK such vans were used to find TV watchers who hadnt paid the BBC's licensing fee. Im sure someone must have written an article bemoaning the loss of privacy back then, but no one really cared.
On the other hand real-time-metering can make a big difference in the shift to renewable sources of power. A major issue is the so called Duck Curve, which measures the difference between renewables and overall power demand. At sundown in places like California with lots of solar generation, conventional generating capacity has to come up quickly, which usually means keeping generators spinning before they are needed. Intelligent appliances that can defer demand for short periods, like refrigerators and air conditioning, can help flatten the curve and make these transitions much more energy efficient. TVs would have a harder time adjusting power consumption dynamically, though overall brightness could be reduced by 25% or so without most viewers noticing. If the price incentives are right, it may become economical to equip TVs with their own batteries and operate them on stored power during peak demand. That could eliminate power correlation to TV channel. I think we have more pressing privacy issues, such as the new UK law requiring retention of user browsing data for one year and making it accessible to dozens of government agencies (with the exception of Members of Parliament, whose browsing is only to be seen with the approval of the Prime Minister.) Id rather focus on things we can fix, like TLS and IoT RNG, and not create a privacy scare about a technology that has a lot of promise and only marginal privacy impact.
Arnold Reinhold

@_date: 2016-12-23 16:08:09
@_author: Arnold Reinhold 
@_subject: [Cryptography]  USB hardware token for $2?? 
The units in the link you give look similar to ones I purchased about 8 months ago. They are knockoffs of the Digispark design and I could never get them to work. There may have been something wonky about the boot loader (if there was one).  The real Digispark units work great and dont cost that much more. (Yes a PI zero costs less, but you need accessories, and good luck auditing the softwarean operating system is not always a plus.) The Arduino development environment is a pleasure to use and programming the AVR chips is fun.  The 512 bytes of RAM prevents some crypto algorithms from being used. RC4 works, but takes up more than half of RAM. I was able to get Speck to work on the chip. An analog accelerometer chip could make a good random bit source, and ferroelectric RAM boards ($10 from Adafruit) can store 32K bytes that can be reliably erased (unlike flash), if 512B EEPROM isn't enough.
You can also buy raw ATTiny 85 chips and get them working with minimal components.  Peter Gutmann > added:
Packaging a processor in a compact but usable form factor is not a trivial task. Having a well thought out design available for small money saves a lot of time and aggravation. Add in a good development environment, lots of libraries, accessory boards and a strong developer community and you are way ahead using boards like this. For simple uses, the software may not be that bad.
Arnold Reinhold

@_date: 2016-02-26 17:46:45
@_author: Arnold Reinhold 
@_subject: [Cryptography] A possible way into an iPhone? 
I found this link while working on the "FBI v. Apple" Wikipedia article:
   Here is the most relevant part:
...you are allowed five guesses for free, meaning that the first five guesses do not incur any timeout penalty. But as soon as you enter your 6th guess at your passcode, if its wrong, your device is disabled for one minute. Then you get one more guess, and if that one is wrong, your device is disabled for five minutes. Then another guess, and if its wrong, you are locked out of your device for 15 minutes. After that each wrong guess at your passcode incurs a full 60 minute disabled timeout.
"The way to avoid the disabled timeout when guessing at your passcode is this: as soon as you get your first message that your device is disabled try again in X minutes, connect it to your computer, with iTunes running. As soon as your device starts syncing disconnect it from the computer (yes, while it is starting to sync  we told you this was not approved by Apple!)
This method has worked for us every time  as soon as you disconnect the device from your computer, for some reason, you will once again have 5 free guesses to guess at your passcode.
If this really works, it should be fairly easy to construct a rig to automate the process, with 10 solenoids to do the key presses and a relay to connect and disconnect the iPhone cable.  One could test it on another iPhone 5C to make sure it was safe. Even if it takes 3 minutes to try each set of 5 guesses, testing all 10,000 four digit PINs would take 100 hours. Seems worth a try to avoid an expensive law suit with potentially unpleasant outcomes for both sides.
Arnold Reinhold

@_date: 2016-02-27 19:48:14
@_author: Arnold Reinhold 
@_subject: [Cryptography] A possible way into an iPhone? 
Sent from my iPhone
This blog is creating a straw man and knocking it down. The FBI is not demanding Apple construct a certified forensic instrument, read the order.  I doubt any court could order that anyway.  If the reported vulnerability is real the FBI likely has labs and contractors that can develop it into a reliable method, testing it on enough sample iPhones to have confidence. The NSA certainly does and this case has international ties (ISIS) so they can be involved. The FBI could also ask for/compel technical assistance from Apple without raising the hard constitutional issues in this case. Seems like a win for everyone. Arnold Reinhold

@_date: 2016-01-16 19:18:46
@_author: Arnold Reinhold 
@_subject: [Cryptography] Verisimilitrust 
Top of my list would be a standard way to get or verify certificates via QR-codes. Consumers are already familiar with them. Coupled with certificate pinning, this would allow the whole CA mess to be bypassed in many important cases, such as banking, health care and email. Most people have periodic out-of-band contact with their banks, visiting offices, ATM machine kiosks, or getting written statements. Health care usually entails in-person contact. Scanning a QRcode on the wall or in the printed statement letterhead would allow a direct establishment of trust. Email trust could be established when exchanging business cards at first contact, and so on.  Banks and others might even get into the business of verifying certificates for business and individuals that have accounts with them, perhaps for a fee. Arnold Reinhold

@_date: 2016-01-18 08:45:47
@_author: Arnold Reinhold 
@_subject: [Cryptography] TRNG review: Arduino based TRNGs 
Several of the Arduino chips have internal temperature sensorsno external components needed. I have been playing the one included in the ATtiny85. The analog reading of the sensor has a resolution of one Celsius degree pr LSB. There are a number of instructions on the web for using the internal temperature sensor on this part, with suggestions on how to minimize noise in the measurements. I, naturally, have been doing the exact opposite. I get somewhat random behavior, but havent had time to set up real tests. Id be happy to share my code with any who wants to try this. One thought I had was for situations where a string of random bits is required at first startup, say to initialize a key pair: One could measure the internal temperature and use the timers to record when the LSB changes as the chip warms up. Core temperature rises by about 3.5 deg C in the first minute of operation.  The timers are free running and have quite high resolution available, so the recorded times should have lots of entropy. Of course someone with physical possession of the device could try to game this, say be keeping the chip cold, but I think there are many applications where one might not worry about this threat. Also the thermal resistance between the chip and package could pose difficulty. Arnold Reinhold

@_date: 2016-01-18 09:14:35
@_author: Arnold Reinhold 
@_subject: [Cryptography] Verisimilitrust 
Its far more expensive than a flood of phishing e-mails and it requires physical presence and activity in country that can lead to prosecution. And if QRcodes are on every mailing, the likelihood that a victim will select the phishers document to scan is small. Banks could use locked frames to display the QRcode in unattended locations. Also remember that ATMs are often under video surveillance and banks have an interest in prosecuting fraudsters. It might even be possible to display the QRcode on the ATM screen itself. I think there is enough screen resolution on newer ones  for a verification code at least. There will be some need for care in doing this, but direct verification of certificates from material supplied by the owner makes a lot more sense that indirect verification by any one of several hundred trusted third parties scattered across the globe.
Arnold Reinhold

@_date: 2016-01-22 13:19:04
@_author: Arnold Reinhold 
@_subject: [Cryptography] How to counter the security risks of shared 
I think the simplest backdoor is to constrain the random number generated to start the search for one of the one of the primes to have a small enough state space so that searching all possible primes is feasible. The simplest scheme might be to use a randomly seeded 32-bit PRNG. More sophisticated schemes might seed a larger PRNG in part with data that could partially be guessed, such as the date and hour of key generation or the user name to avoid birthday paradox collisions between different key pairs. The PRNG might also contain some secret parameters known only to the attacker. Of course this sort of attack applies to other public key systems as well.
Arnold Reinhold

@_date: 2016-01-24 20:01:52
@_author: Arnold Reinhold 
@_subject: [Cryptography] USS Pueblo and crypto 
There is quite a bit of material about this on the web:
The NSA's damage report:
   Comments by the USS Pueblo crew:
   And in particular, a rebuttal by the Pueblos commander, Captain Bucher:
   He makes a strong case that the Soviets already knew what they needed to make use of the key lists Walker ring supplied them. His suggestion that KL-7s had been captured during the Vietnam war is buttressed by the fact that the KL-7 on display in the NSAs National Cryptologic Museum was donated by North Vietnam. According to the Wikipedia KG-13 article: "When the USS Pueblo, with a KG-13 aboard, was captured by the North Koreans in 1968, the personnel didn't have time to destroy it. As a result, a working model of the KG-13 fell into enemy hands. NSA quickly designed a modification to the koken stage board to alter its operation in order that the enemy didn't have an identical working model. Therefore its quite possible the Soviets were unhappy with the PRKs seizure of the USS Pueblo since it may have caused them some extra trouble. Arnold Reinhold

@_date: 2016-07-04 15:38:53
@_author: Arnold Reinhold 
@_subject: [Cryptography] London Review of Books on the Craig Wright affair 
The June 30 London Review of Books has a long article, "The Satoshi Affair by Andrew OHaganon, about the incidents culminating in Craig Wright coming out (or not) as Satoshi Nakamoto back in May. A good read. OHaganon had extensive access to most of the participants. I wonder who owns the movie rights.
Arnold Reinhold

@_date: 2016-07-11 14:11:05
@_author: Arnold Reinhold 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
The lack of mathematical proofs for the security of cryptographic primitives is a reality with which the cryptographic community is perhaps too comfortable. We all know, of course that a mathematical proof of security would be traceable to the axioms of logic, and, at least for the existence of random quantitates, the laws of physics.  And that would be great. But there is something else a mathematical proof supplies: the precise statement of a theorem. Its not just that we lack a proof that factoring is hard or that discrete logarithms in finite groups are difficult to compute, we dont even know exactly what those words mean. A recent example of this is the Logjam attack for which there was no new mathematics developed, merely a realization that most the work needed in the best attack on D-H in the group of integers modulo a prime p was only dependent on p, not on the group element whose logarithm was to be computed. Since many protocols use the same p for all D-H encryption, attacks became more feasible than previously thought. The assertion "D-H is hard" omitted that bit of fine print.  What other overlooked technicalities are out there behind statements that "X is assumed to be hard"?
Arnold Reinhold

@_date: 2016-07-12 07:37:36
@_author: Arnold Reinhold 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
Thanks for the interesting reference. Ive added it to the relevant Wikipedia articles. But I think this case still supports my argument. A valid theorem on the security of D-H would have to include a limitation on the reuse of p, as well as other limitations, such as choosing p to avoid PohligHellman attacks. Limitations would be all in one place instead of being scattered throughout the literature. Even the mention of the prime reuse vulnerability in Section 5.2 of the Station-To-Station Protocol publication seems to be an afterthought. Later in Section 5.2, the importance of including p and the group generator in public key certificates is mentioned, but the example given for why is downright silly, when avoiding prime reuse would have been a much more convincing argument.
Perhaps we need a body of pseudo-theorems: clear statements of what exactly we are assuming about each of our cryptographic primitives that can be updated when new limitations are found.
P vs. NP has nothing to do with cryptography. Nothing. I wrote a rant on this over two decades ago ( Give me an encryption algorithm whose difficulty grows as (key size)^20 (i.e. polynomial time) and I can pick a reasonable key size that will not be crackable in the age of the universe.  And for that matter AES-256 is in P because it is solvable in constant time, its just a very big constant.
If that isnt enough to convince you, consider this. The whole theory of NP-completeness is based on finding problems that are "NP-hard. NP-hard just means that there exist examples of the problem into which you can embed a Turing machine and its program. The best example is Boolean-satisfiability. You can unwind any Turing machine into a giant Boolean expression in polynomial time. NP-competenss of NP-hard problems follows because for a problem, X, to be in NP there must exist a Turing machine program, testX, that can check any proposed solution of X in polynomial time.  If you can solve any NP-hard problem, Y, in polynomial time, then you can solve X in polynomial time by embedding its testX program in Y.  All very pretty and all dependent on the fact that polynomial-time is such an elastic equivalence relation: the product of two polynomials is another polynomial, but the degrees keep getting bigger and problems whose solution time is s high degree polynomial are intractable in practice.
Note that this theory is all about existence proofs: we find some example of the problem that can contains a Turing machine (or an arbitrary Boolean expression, which is usually much easier to show) and that problem gets the NP-hard stamp of approval. But that is no proof that all or even most examples of the problem are in any way difficult. And for cryptography we want assurance that our algorithms are strong in all cases, not just a sparse subset. We dont know if factoring primes or finding discrete logs are NP-hard because no one has found enough structure in those problems to embed Turing machines. Suppose someone did. Then the problem would be officially NP-hard, but the newly discovered structure might nonetheless enable new attacks on practical sized problems. So the whole P vs NP theory has no bearing on cryptography. Arnold Reinhold

@_date: 2016-07-15 17:57:18
@_author: Arnold Reinhold 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
Thank you.
While It is possible that new mathematical techniques developed to answer the P vs. NP question might, as a byproduct, show the way to proving some cipher algorithm is strong in the sense you mentioned, it is just as possible that some clever new approach to building strong ciphers could lead to a proof that P!=NP. Or our salvation might come from some other mathematical breakthrough. This relevance of P vs. NP to cryptography is tenuous at best.
I finally made it through that paper. Thanks for the reference, it is an interesting read. I found this Aaronson quote pertinent: But what about the oft-repeated claim that asymptotic statements have no relevance for physical reality? This claim has never impressed me. For me, the statement Max Clique requires exponential time is simply shorthand for a large class of statements involving reasonable instance sizes (say 10^8) but astronomical lengths of time (say 10^80 seconds). If the complexity of the maximum clique problem turned out implausibly to be 1.000000001^n or n^10000, then so much the worse for the shorthand; the finite statements are what we actually cared about anyway. With this in mind, we can formulate the NP Hardness Assumption concretely as follows: Given an undirected graph G with 10^8 vertices, there is no physical procedure by which you can decide in general whether G has a clique of size 10^7, with probability at least 2/3 and after at most 10^80 seconds as experienced by you.
It seems to me that what Aaronson is asking for is not that different from what cryptographers would like to have, a proof that some cipher algorithm with key length n cannot be computed absent the key in, say, less than 2^(n/2 - epsilon) steps by any physical procedure . But we dont need a proof that P!=NP to get there nor would the discovery that P=NP bar the existence of such a proof.
Note that Godel is asking for "a linear or quadratic-time procedure. That is very different from being satisfied with a procedure in P. He is not endorsing the notion that P means tractable" which so many computer science majors have been brainwashed into believing.
Arnold Reinhold

@_date: 2016-07-21 13:22:32
@_author: Arnold Reinhold 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
Another interesting article by Scott Aaronson, thank you, but like P vs. NP it is dealing with the theory of Turing machines. While cryptographic algorithms are executed on Turing machines, there is not reason to think that they contain embedded Touring machines. Why would we allow that? And it is not likely to happen by accident. The paper has a link to a discussion of what is the smallest universal Touring machine, but I wonder if that has ever been translated into bits, i.e. how many bits are needed to define the smallest UTM? I think it is unlikely from a purely information content argument that some cleverly chosen key could turn AES-256 into a Turing machine, much less one executing a program that is impossible to decide in ZF. A similar argument might apply to Elliptic Curve groups of the size currently used. And as I pointed out previously, we like EC groups because of their lack of internal structure. Some as yet undiscovered structure that enabled them to act as Turing machines might also lead to a faster solutions of discrete logs.
It is certainly possible that a proof of a lower bound for a cryptographic algorithm might be impossibly large, but it hardly follows from this paper. Peano arithmetic is a much bigger system (infinite of course) than what is needed to construct common cryptographic algorithms, which all operate in finite systems that are too small for Goedel's encoding.
Complexity theory has led to some beautiful math, but it also seems to have discouraged some practical research because it is mistakenly taken to suggest things are unsolvable even in situations when the underling assumptions for undecidability are not met.
Arnold Reinhold

@_date: 2016-07-22 13:00:13
@_author: Arnold Reinhold 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
It is not intractable. We know that there exists one highly efficient, if impractical, Touring machine for breaking a block cipher given a known plaintext-ciphertext pair. Equip the Touring machine with a table of all plaintext ciphertext pairs for each key (a codebook) sorted by plaintext-ciphertext string. Then do a binary lookup in the table for the known plaintext-ciphertext pair to recover the key. The table will be very large (to big for the known universe no doubt) but finite. Binary search time would be on the order block-size times key-length, which is short enough to be considered an efficient break. Let G be the size of this table-lookup Turing machine including its precomputed table. Then we can as you suggest "exhaustively enumerate all possible algorithms and eliminate each one as a candidate attack by actually running them on all possible keys and record the most efficient, but stop when the length of the candidate programs exceeds G. Stopping then is justified since the table look up approach is good enough. So our algorithm-searching Touring machine is guaranteed to halt. If you like we can stop the search at a smaller program length, maybe the number of atoms in the Solar system, or a maybe one exabyte, so we only get solutions of practical size (there will be plenty of still-too-large partial table solutions found before we get to length G).  Note that this search is guaranteed to find at least one solution since the brute-force attack of trying all keys is sure to be found.
Given that I have just proven that searching for efficient solutions is finite, I dont see how Turing equivalence comes into play at all. The only question is whether there are more efficient search methods and will they find anything better than brute force.
Arnold Reinhold

@_date: 2016-07-22 15:01:05
@_author: Arnold Reinhold 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
John Tromp via e-mail has kindly called my attention to the following:
29 bytes is 232 bits, more than the key size of AES-128 and a little less than for AES-256. Of course this is not a lower bound, but it does support the notion that block ciphers with keys in that range are unlikely to contain a hidden Turing machine.
Also in my most recent post where I tried to show that a search for the most efficient algorithm to break a given block cipher is finite, I neglected to mention that the running time for each candidate algorithm can be limited to the maximum time for a brute force search, since we dont care about algorithms which take longer.
Arnold Reinhold

@_date: 2016-07-26 13:01:44
@_author: Arnold Reinhold 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
In a previous post, you suggested that one answer to 2 might be that the question is undecidable. I attempted to show that for a fixed size block cipher there exists a finite (tho entirely impractical) algorithm for finding all possible attacks and that therefore the the question is not undecidable. Im glad you now agree that this is obvious. (I believe my argument can be extended to discrete log public key systems by replacing the code book with a table of logarithms as a finite upper bound for the size of Turing machines to search.)
I agree, and have said so before, that machinery developed to answer P=?NP could perhaps lead to insights on how to verify the security of cryptographic algorithms. However the central technique currently used in the study of Turing machines, the diagonal method, relies on the infinite nature of the problems. But no one can say with confidence what a yet-to-be-discovered mathematical method could or could not do. What I strongly disagree with is the widely stated belief that a proof that P=NP would, by itself, invalidate current cryptographic methods.
Arnold Reinhold

@_date: 2016-07-28 12:40:40
@_author: Arnold Reinhold 
@_subject: [Cryptography] State of sin (was Re: What to put in a new 
You were the one who suggested that the security of cryptographic algorithms might be undecidable in the formal sense, pointing us to Aaronson's blog with an example of a formally undecidable Turing machine with just a few thousand states.  The fate of nations has depended on the security of cryptography in the past and maybe depends on it even more so today. We currently spend scarce resources on far less practical questions than finding cryptographic systems that are provably secure.
I never said you did. I said it was a "widely stated belief. Here are quotes from the Clay Mathematics Institutes Official Description of the P Versus NP Problem  by Stephen Cook:
"The security of the Internet, including most financial transactions, depends on complexity-theoretic assumptions such as the difficulty of integer factoring or of breaking DES (the Data Encryption Standard). If P = NP, these assumptions are all false. ... "Even if P != NP it may still happen that every NP problem is susceptible to a polynomial-time algorithm that works on most inputs. This could render cryptography impossible
Cook, who with Leonid Levin, was the first to state the P vs NP problem, and the Clay Mathematics Institute are hardly straw men.
Arnold Reinhold

@_date: 2016-03-04 15:38:44
@_author: Arnold Reinhold 
@_subject: [Cryptography] More Apple news 
Take the San Bernardino County District Attorney at his word. This opens whole new line of defense for Apple and could give the court an easy out. Remember the FBI has been demanding that Apple connect the suspect iPhone to Apple's computers to upload a modified iOS operating system with the FBI then accessing that iPhone remotely (presumably via a connection to the Apple network) to try all the 4-digit key combinations. If there is even a small chance the iPhone contains a dormant cyber pathogen then the procedure the FBI has asked for carries the risk that the phone could infect Apples infrastructure and from there hundreds of millions of computers that connect to Apple services. The potential liability for Apple is unbounded and the danger to the public enormous. Apple is not in the cyber-warfare business and cannot be expected to have the kind of high-grade software containment facility and expertise needed to safely deal with weaponized software at this level. Furthermore the government neglected to even warn Apple of the danger until yesterday. The court could throw out the FBI order on that basis alone, without having to reach the trickier and precedential issues of undue burden and compelled speech this case presents.
Arnold Reinhold

@_date: 2016-03-09 13:55:58
@_author: Arnold Reinhold 
@_subject: [Cryptography] EFF amicus brief in support of Apple 
Apple does make assertions of this sort in writing to it customers. For example, in its "iOS Security Guide"  Apple says iOS includes a Device Firmware Upgrade (DFU) mode, and that Restoring a device after it enters DFU mode returns it to a known good state with the certainty that only unmodified Apple-signed code is present.
Arnold Reinhold

@_date: 2016-03-09 16:29:13
@_author: Arnold Reinhold 
@_subject: [Cryptography] Michael Hayden supports Apple 
In a March 7 interview with Maria Bartiromo on the Fox Business Network, General Michael Hayden, former director of the NSA and CIA, said that while he thought the government does have a legal right to compel Apple to unlock the iPhone, he nonetheless supports Apples position, in what he called "a close but clear call on just security grounds  He noted that the CIA currently considers cyber-attacks the number one threat to U.S. security and said ...this may be a case where weve got to give up some things in law enforcement and even counter terrorism in order to preserve this aspect, our cybersecurity.     Also see the image link there to the interview itself. It doesnt get more authoritative than that.
Arnold Reinhold

@_date: 2016-03-17 18:00:03
@_author: Arnold Reinhold 
@_subject: [Cryptography] Trust & randomness in computer systems 
Here is a link to a paper I submitted back in 1989 that analyzes one section of the biological immune system from a cryptographic point of view:
    It discuss why humans reject organ transplants from other humans so aggressivelyafter all invasion by other human organs is not a threat found nature. It turns out cells in humans (and other animals) take apart used proteins on a regular basis, and present a subset of all possible protein fragments on the cell surface for inspection by a certain type of white blood cell. Cells presenting fragments that appear foreign are attacked by white cells. The subset of protein fragments that are presented varies between individuals, effectively creating a password for that individual. The genes that control this password, the Major Histocompatiblity Complex (MHC), are randomized during sexual reproduction. So a virus that evolves to evade this system in one individual is not likely to successfully evade it in a different individual. As a side effect, tissue transplanted from another individual is unlikely to have the right password. Tissue matching looks for donors who have similar passwords. My analysis suggests that the password space is limited by a trade off between the stringency of the checking system and the process, early in life, of training white blood cells to ignore self-protein fragments. Failures in this training can result in auto-immune disease.
The possible lesson for computer security might be increased diversity. For example, anti-virus companies might ship different detection vectors to different subsets of customers, so virus authors would not necessarily know what vectors are out there (though collecting them all might not be that hard). Another idea inspired by this is processors with different (permuted) opcodes. The compiler or loader would have to know the opcode table for the target computer.  A virus that gets into one machine wont run on most others. The opcode space might be larger than needed to express all instructions, so an attacker could not just create a bunch of viruses with different combinations of opcodes to see which one worked. Arnold Reinhold

@_date: 2016-03-17 18:07:28
@_author: Arnold Reinhold 
@_subject: [Cryptography] Time Magazine on Apple vs. FBI 
The March 28 issue of Time Magazine has Tim Cook on the cover and a quite good (in my opinion) summary of the case to date, "Inside Apple CEO Tim Cooks Fight With the FBI by Lev Grossman:
 There is also a link to their interview with Cook.
Arnold Reinhold

@_date: 2016-05-04 12:25:34
@_author: Arnold Reinhold 
@_subject: [Cryptography] USB 3.0 authentication: market power and 	DRM? 
Authentication chips in every computer cable is disaster in the making. Its an opportunity for cyber-sabotage on a grand scale too good for major powers to pass up. A small fraction of cables in service containing a chip that can be triggered remotely to fail or to load malware could wreak havoc on a modern economy. Where will the cable chips be made and who will control the final masks? The master private signing key owned by the USB tImplementers Forum will be an incredibly valuable target, on the order of an NSA core secret. There is no way a trade association will spend the kind of money needed to secure this asset, not will they have the layers of legal and other protections that the NSA enjoys (security clearances, long prison terms for leaks, threat of covert action, etc). What are the penalties these days for leaking a corporate trade secret? Thats assuming the leaker is caught; a few thousand bits passed to a contact or overnight access to an HSM in exchange for a suitcase of cash or freedom for a relative in the old country and no one is the wiser. And if the USB-IF does discover a leak, what can they do about it?
It may be time to stock up on computers that can be powered by a pair of wires and can talk over chip-free copper cables. They are the only ones that should be used for critical infrastructure.
Madness, irresponsible madness.
Arnold Reinhold

@_date: 2016-11-04 16:28:09
@_author: Arnold Reinhold 
@_subject: [Cryptography] "we need to protect [our dox] by at least encrypting 
For her fiftieth birthday, Hillary Clinton received a copy of "Internet Email for Dummies" from her staff to help her communicate with Chelsea who was going off to college. Both Time and Newsweek reported this at the time, with some amusement. I was co-author of that book (along with John Levine, Margey Levine and Carol Baroudi) and I wrote the chapters on E-Mail Security and PGP. So maybe its all my fault. Back in 1996 we warned her and other readers (first edition, pp. 255-256):
the general lack of security for e-mail is a real threat to personal privacy. 
When e-mail was a toy for computer geeks, its lack of privacy may not have mattered. With e-mail poised to become a nearly universal means of communication in the 21st century, privacy matters now.
There is a policy question that has not been adequately discussed in all the commotion about Hillarys email: to what extent does a high government official require and deserve a confidential way to communicate unclassified messages with others? Yes, there is a need to maintain a historical record, but while in office, governmental leaders need privacy to carry out their duties. But there is no evidence the e-mail system then provided by the State Department had any provision to shield Secretary Clintons email from the small army of technicians that would have had administration access to her email account. Civil servants frequently leak sensitive data, and we know the State Department email was hacked by foreign powers at least once.  A Secretary of State is especially in need of private communications. International negotiations are often about economic issues affect multiple parties within the US as well as in other nations--there are always winners and losers. Information on what the Secretary of State is thinking or even with whom he or she is talking can help marshal forces against an agreement under negotiation or, more likely, to apply pressure to modify its terms. Having a private server provided a modicum of privacy that likely helped Mrs. Clinton do her job. And if it is true that Russia is orchestrating the DNC leaks, its hard to think of a reason why they wouldnt also  leak her private servers data if they have it. Neither her servers nor the State Departments were suitable for classified material, but most cables at the Secret level and below from that era were leaked by Pvt, Manning via Wikileaks. If there were messages on her server at that level, they may be among the few that are not already on Wikileaks.
Eschewing email altogether would likely have impaired her effectiveness, so given the limited tools she had available at the time, her use of a private server may have been in the countrys best interest.
Arnold Reinhold

@_date: 2016-11-06 07:21:54
@_author: Arnold Reinhold 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
What you espouse is essentially the first of Woodrow Wilsons 14 Points framework for peace after World War I (
     I. Open covenants of peace, openly arrived at, after which there shall be no private international understandings of any kind but diplomacy shall proceed always frankly and in the public view.
Wilsons health failed before the Paris Peace Conference and point 1 was not observed in practice then nor since.  (Some of the other points address issues that are still in the headlines.)
I took a labor relations course years ago and our professor, an experienced labor mediator, explained that every negotiation between two large organizations devolves into at least three separate negotiations, the one between the two negotiating teams and the two between each team and its constituency, and that of three the first is often the easiest. The two negotiating teams sometimes find themselves cooperating in helping their opposite team outmaneuver more extreme elements opposing a settlement, while feigning hostility between the teams in public. Like it or not, difficult negotiations frequently demand such secret maneuvers. Note that the hot line between Washington and Moscow established during the height of the cold war used strong encryption (one time tapes initially) and I never heard anyone suggest it be open to the public.
The idea of using cryptography to facilitate negotiation is a very interesting research topicId like to hear more of your thoughts on this--but it doesnt alter the reality that effective diplomacy currently depends on confidentiality of discussions between the various parties, domestic as well as foreign. I think Secretary Clinton diid what she needed to do to maintain that confidentiality.
Arnold Reinhold

@_date: 2016-11-07 12:57:22
@_author: Arnold Reinhold 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
This claim is based on a Fox News story   that has since been revised to say:
   "Authorities are operating under the working assumption there is a high chance Hillary Clintons private server was breached, one source with intimate knowledge of the FBI investigation told Fox News  though there still are no digital fingerprints proving a breach.
   The source said the server may have been hacked by up to five foreign intelligence agencies. While other sources believe this is probable, evidence has not emerged to confirm this.
   When FBI Director James Comey publicly discussed the Clinton email case back in July, he also said that while there was no evidence hostile actors breached the server, it was possible' they had gained access."
That is a big difference; it now appears to be all speculation. I still ask why, if her servers were hacked, havent at least some of the deleted emails been release through Wikileaks or some other source? If the other leaks have indeed been directed by a state actor, likely Russia, as several intelligence agencies have concluded, why hold back before the election convincing evidence she was hacked?  And if you believe the other leaks were from insiders, not state actors, all the more reason that Hillary was wise to use a private server with a few hand-picked admins she trusted. We have been deluged with Secret and Top Secret documents purloined by Manning and Snowden. The handful of emails on Hillarys server that the FBI says were or should have been classified seem to be among the few U.S. state secrets that the public has yet to see.
Arnold Reinhold

@_date: 2016-11-08 09:10:02
@_author: Arnold Reinhold 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
Was it a leak from someone in the know, or was it embellished (e.g. "98% chance) along the way? The fact the Fox News retracted its original story strongly suggests the latter. Except that there have been numerous reports, from multiple sources, that Russia is indeed futzing with the American elections. People close to Putin have said on video that electing Clinton means war. Bluster no doubt, but hardly neutral. And Russia doesnt have to admit anything, the can just anonymously leak the deleted Clinton email through Wikileaks or some other channel. The fact that they have not appeared as of 9 am on election day, suggests Russia doesnt have them. Im not suggesting that a private server in each official's home is the right answer going forward, but a separate email server in each top-level officials office safe with encrypted back up to the department servers might be a good solution for unclassified email privacy. The servers would be inside the departments firewall perimeter defenses and could have additional protection, such as a stripped down operating system loaded from ROM, to minimize attack surface. Admin access would be limited to a few staff vetted by the official. The backups' encryption key might be escrowed in the national archives for future historical records. The old model of all email stored en clar on department servers is unworkable.
Arnold Reinhold

@_date: 2016-11-10 09:59:44
@_author: Arnold Reinhold 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
The nice thing about leaks is we get to pick and choose the ones we like. But positing that Fox News suddenly became afraid of Hillary Clinton is a bit over the top. The threat the private server was an attempt to deal with was employees who have access to her emails leaking them to political opponents. The security officers you suggest she should have gone to are likely part of the threat, not the solution. Hacking was a risk, but we know that the State Department unclassified email system was hacked, while there in apparently no evidence her servers were.  Of course a more secure approach would be preferable.
As for voting systems, where I live we have paper ballots that are optically scanned. Results are available immediately after the election but the paper ballots can be manually counted as a check. A hack attack that targeted only a few machines would be noticeable statistically if it was large enough to matter. An attack that made small increments in many voting machines could be caught be hand counting a few precincts. Best of all, a paper system is understandable by the retirees hired to staff the voting places. What is the point of going to an all electronic system that only a few specialists can audit?

@_date: 2016-11-11 13:32:26
@_author: Arnold Reinhold 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
[Thanks, Ill address the second in the election security thread.]
She would still need trusted devices (e.g. office, mobile, home) that can hold her decryption key and retrieve and read the encrypted mail. There also needs to be a device or process that receives her unclassified emails, much of which are plain text, encrypts them for archiving in the department email database and makes them available to her trusted devices. Even knowledge of which emails are read when has potential value to political opponents, so a dedicated device with physical security controlled by her trusted aides is preferable to a virtual machine in some department or GSA server farm. When all is said and done, it seems to me that in meeting these requirements what you create is hardly distinguishable from a private email server.
Arnold Reinhold

@_date: 2016-11-11 14:12:41
@_author: Arnold Reinhold 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
Optical scanners for paper ballots can detect over-votes at the time of submission and return the paper ballot to the voter so they can get it voided and try again. ( See   Faq  So any over-voted ballot detected during later hand counting should arouse suspicion, more than one alarm.  I assume that when you were a child there were no scanners. But this rather clever scheme was non the less detected. The skills needed to figure out mischief with paper ballots are widely held, the skills needed to discover problems in electronic systems are far more scarce. Is that even possible today? Weve had discussions on this list of subverting processors by changing the dopant level at certain transistors. Isnt hand counting ballots from a sample of scanners, as you suggest below, just as effective, cheaper, and doable by people without (very) advanced training in computer security?
Great link, thanks. I wasnt all that disturbed by the bug that caused one batch (their term deck warms my heart) of ballots to be occasionally deleted. Vastly more serious was the fact that the audit log produced by their software failed to record all such deletions, even permitted ones. Was that ever fixed? That the later problem was apparently not recognized as a show-stopper is a strong indicator that all-electronic voting systems can never be trusted, because the builders and maintainers of such systems will never take security seriously enough long term.

@_date: 2016-11-17 07:29:18
@_author: Arnold Reinhold 
@_subject: [Cryptography] securing the ballot scanners 
Thanks for the link to the interesting Rivest & Smith proposal. Putting aside the difficulty of getting voters to understand it and the challenge of building the complex voting machines needed sans any computer elements, I see a couple of problems with the 3-ballot approach:
1. What happens when a voting receipt is produced that doesn't match a posted ballot? Ill buy for the moment that this proves some fraud has occurred. Now what? Re-run the election? 2. What prevents people from forging ballot receipts? There are no computers to create an electronic signature and anyway electronic signatures require trusting someone to hold and protect the signing key. If a single forged receipt can cast doubt on the legitimacy of an election, there is a huge incentive for the losing part to do so. This past election is a perfect example. Imagine if Hillary could challenge the vote in a few key states. One requirement for a voting system that is not often mentioned is finality. I don't see the advantage of this system over the scanned paper ballots we have here in Cambridge, Massachusetts. Poll watchers from the political parties can supervise the handling of the paper ballots. It is a process people understand and trust. Privacy? There are many threats to the privacy of voting besides jiggered voting machines. Our cell phones already track us and the data can tell when we voted. Video cameras are almost undetectable and can watch voters make their selection. Cameras can be put in the ceiling overlooking the voting booths, or in the booths themselves. (The complex mechanical voting machines that Rivest & Smith posit would have many places to insert a camera. The camera could be removed at the end of election day.) And a sufficiently coercive group can influence the vote merely by spreading rumors it knows how you voted. Whack a few citizens for voting the wrong way; it doesnt matter if they did or not. Arnold Reinhold

@_date: 2016-11-25 15:55:43
@_author: Arnold Reinhold 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
In addition the need for a proper published audit that bear suggested, the most glaring defect in the Intel design is the lack of access to the un-whitened random bits. Adding a mode that bypassed the whitener would have been simple. Statistical analysis of the raw bit stream can provide ongoing assurance that the RNG is doing what it says. Likely there will be correlations between raw bit statistics and external parameters such as chip temperature and supply voltage. Of course it is possible for a deterministic generator to mimic such variations, but it would have to have a relatively large footprint on the die compared to simply using the whitener in a feedback mode or similar mischief. Subversion of the RNG is one of the best methods for compromising a cryptographic system. It offers an attack for any algorithm that relies on that RNG and requires no data channel back to the attacker. While there are lots of ways to create good random sources, there are very few ways to audit the results. I cant see any good reason why a hardware RNG should include a whitener. The benefit of avoiding the need to whiten in software is far outweighed by the impenetrable barrier a good whitener presents to any statistical audit of RNG behavior. Separation of responsibilities is a basic security concept. A well run accounting department does not let the person who signs checks have access to the blank forms. Similarly, whitening should always be separate from random bit creation. A RNG that only offers whitened output must always be suspect. Arnold Reinhold

@_date: 2016-11-28 10:01:21
@_author: Arnold Reinhold 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
First let me say that I believe the people at Intel who designed RdRand were working sincerely and produced a design of top quality. I also believe it received a reasonable amount of peer review, at least internally. My concern is the possible existence of a secret mode that substitutes a deterministic or partially deterministic bitstream for the output RdRand would have produced under the advertised design. The value of such a mode to a cryptanalyst who knew how to use it is beyond dispute. It is inconceivable to me, and I suspect most people on this list, that the NSA would not at least attempt to have such a mode included in some or all of these chips. Doing so could have been approved by a patriotic senior management, required under a National Security Letter (with gag order), or, less likely, implemented by a mole in the Intel organization. The knowledge that such a mode exists would be closely guarded, known by a minimal number of people. If such a mode does not exist, that fact could be asserted by only the even tinier handful of people who have access to all of NSA's exceptionally guarded compartments (I hope there are only a handful).
So asking for access to un-whitened entropy is not a question of curiosity, nor is this an attack on the integrity of the people who designed RdRand. It is a request for a mechanism that could restore trust in the on-board RNG. Your argument does convince me that a mode to simply turn off whitening would not be wise, but it does not rule out a separate RdRandRaw or RdSeedRaw instruction. I have commented at length on the -90C revision (tho not on this point) and was told explicitly that NIST specs and FIPS standards reflect the interests of the U.S. government and not necessarily the public at large. And I see nothing in those specs that prevent a separate access to raw entropy.
So my answer to this thread's original question is that if your threat model does not include the NSA and other state actors who may have gained access to a possible secret DRNG mode, use of RdRand by itself is likely ok. If not, its output should be mixed with other sources of entropy.
Arnold Reinhold

@_date: 2016-09-02 08:54:39
@_author: Arnold Reinhold 
@_subject: [Cryptography] Negative energy pricing and Bitcoin 
Here is an interesting presentation from the California ISO, the people who manage the energy grid, on time of use pricing: Because of large amounts of solar power on the grid and a mismatch with demand patterns, there are times during the day, 9 am to 5 pm, when spot wholesale pricing for electricity often goes negative (see slide 14). This is especially true on weekends and holidays and in the months of March and April, what the report call Super-off-peak." About 2700 megawatt hours of capacity is not being used each day. Some of that excess power will no doubt be diverted to storage as battery technology improves, but it wont be economical to have storage capacity to absorb the weekend and seasonal variations
This suggests an interesting possibility for mining cryptocurrencies like Bitcoin. Ship older, less efficient mining hardware, whose capital costs have been paid for, to California or other locations with a similar situation and operate them when electricity market conditions are favorable. Maybe co-locate the mining equipment with some of  the large solar farms in the desert to eliminate transmission costs. The farms would divert their power to mining when spot prices make it economical to do so. Such a system could respond very quickly to short term demand fluctuations, which has added economic value as dispatchable power." With some of the newer cryptocurrencies that are designed to disfavor custom silicon, one might be able to use old PCs for mining, perhaps setting them up in the shade of the solar panels.  Arnold Reinhold

@_date: 2016-09-08 20:19:28
@_author: Arnold Reinhold 
@_subject: [Cryptography] Secure erasure 
Imagine for a moment what might be possible if the toolmakers were actually trying to help instead of sticking to a language spec largely from the PDP-11 era. One might add a "zeroizable keyword, for example. All that code generation ingenuity could be applied to not only insuring the code that erases zeroizable data was not elided, but also to keeping track of copies, and even minimizing their creation. The compiler might be able to recognize when the data in question was no longer needed it and erase each copy of it at the earliest instant. Object destructors could reliably destroy. Pragmas could prevent optimization of critical code. Leaky constructs, such as storing zeroizable data in flash, could be flagged. And so on.
This thread, and so many like it, have concentrated on the problems facing the programmer. But what about software maintenance, management and quality assurance? The sin qua non of the last is strict configuration management. This thread and others over the years say that it is not possible to insure that object code which has passed tests in the past will not change in the future, despite unchanged source files. There are relatively simple things that could be added to the tool chain to fix that, if we were all cooperating. Yes there are many security dragons to slay, but they must be attacked individually. Fixing the tool chain seems like one of the easiest problem technically. The barrier seems to be in human relations, not technology. The Free Software Foundation says it is opposed to mass surveillance and has even joined in a law suit against the NSA. ( ) Do they realize that the tools they develop and promote stymie efforts to develop secure software and no doubt make the NSAs hacking job much easier? How do we start what FBI Director Comey calls an adult conversation" with the computer language and tool community?
Arnold Reinhold

@_date: 2016-09-12 14:36:57
@_author: Arnold Reinhold 
@_subject: [Cryptography] [Crypto-practicum] An historical document 
Alan Turing's Turing Machine paper, "On Computable Numbers, With an Application to the Entscheidungsproblem," was delivered in November 1936, so he was already thinking deeply about the possibilities of machine computation. His biography on Wikipedia says "From September 1936 to July 1938, Turing spent most of his time studying under Church at Princeton University. In addition to his purely mathematical work, he studied cryptology and also built three of four stages of an electro-mechanical binary multiplier. (citing Andrew Hodges biography of Turing) Also the Polish cryptographer Marian Rejewski invented the Bomba in 1938 and told the British about it in 1939. Truing was working part time for the British GC&CS from September 1938 so even in the extremely unlikely event that he would not have thought of using machines to break codes, news of the Polish success would have gotten him going.
The IBM punched card equipment you talk about was being actively sold in England and Germany in the 1930s. What you call printers were sophisticated tabulating machines, which could be programed much like FPGAs today, i.e. they had a variety of relay logic components and electro-mechanical adder-counters that could be wired up to perform calculations. See the Wikipedia article on Plugboard, which describes how they work in some detail.  As far as I know, the US cryptologic groups used them to tabulate statistics about intercepted ciphertext and codes, not for testing large numbers of possible key settings at high speed, which is what the Turing Bomb and Colossus did.
Finally IBM when I knew them was very carful about getting permission from customers before touting their use of IBM equipment in ads. I suspect they were as careful in the 1930s. I have to agree with bear that this very interesting memo is more about insuring that the use of tabulating equipment for cryptanalysis stays secret than a response to a leak that Turing would have picked up and by doing so changed history. Arnold Reinhold

@_date: 2016-09-16 14:59:19
@_author: Arnold Reinhold 
@_subject: [Cryptography] Physical security risks of onetime pads just changed 
The reported technology is terahertz radiation. A sheet of aluminum foil each under the top and bottom covers should completely block terahertz scanning signals. In the olden days, naval code books had lead sheets in the covers to insure they would sink if thrown overboard. Wrapping all sensitive documents sent through the mail in foil may be the next thing, Arnold Reinhold

@_date: 2016-09-16 16:31:04
@_author: Arnold Reinhold 
@_subject: [Cryptography] Ada vs Rust vs safer C 
In the recent thread on safe erasure in C,  much was made of better languages including Ada and Rust. But there is a vast mount of code already written in C. Converting all of it or even a large fraction seems hopeless. For comparison what would it take to make a safer C?
To begin with, many of the problems with unsafe code generation have to do with the large number of undefined behaviors in C.  Since the dogma is that undefined means the compiler can do anything its developers want, what would it take to develop a supplemental specification that defines the most concerning undefined behaviors? What would it then take to develop  compiler that meets those specifications? If the Free Software Foundation might be convinced to help. If not, GCC, or parts of it, could be forked. There must be some programmers out there with compiler chops that would find this kind of project interesting. Perhaps a Kickstarter campaign might be helpful. Defining undefined behavior shouldnt affect most existing programs.
Building a safer C seems more doable than converting massive amounts of C code, and programers, to new languages.
Arnold Reinhold

@_date: 2016-09-19 14:25:00
@_author: Arnold Reinhold 
@_subject: [Cryptography] Ada vs Rust vs safer C 
That is not an unreasonable approach, one that Apple seems to be taking with Swift, but I dont think it is likely to work in the majority of cases. There are too many software systems committed to C and C++. Many projects these days depend on multiple libraries over which they have little control. The often expect to benefit from ongoing development of those libraries. And C has almost a lock on embedded systems development. Rust seems to be largely ignoring embedded systems. There is an experimental project, zink.rs, trying to make it work for ARM that has a blinking LED program working, the embedded equivalent of Hello world.. Efforts to make Rust work for the AVR architecture seem stalled. ADA was designed for embedded systems, but it is unloved and doesnt seem to be used much outside of situations where its use in mandated. Embedded system, especially Internet of Things, has arguably the most critical need for improved security.
So I think an effort to make C more security friendly is warranted. I think this is primarily an organizational problem, not a technical one. As is obvious from this thread there have been many past attempt to solve Cs insecurity, with very limited success. C11 at least has memset_s. Whats missing is some form of governance, starting with a clearer definition of the project. Here is what I had in mind: Since the target is the vast amount of existing C code, the proposed compiler must be fully compatible with the C specs (C99? C11?), with the possible exception of one or a few new reserved words, e.g. for a zeroizable type. Ab initio safety improvements would consist of clarification of undefined C behaviors to make unsafe code generation less likely. For example assuming twos complement arithmetic. For data marked zeroizable, the compiler would minimize copies and keep track to insure they are erased after they are no longer needed in all situations. (That might be the linear type mentioned, I dont want to prejudge technical solutions at this point.)
In addition memset_s support, maybe statements of the form x=0; should never be optimized out. Neither would asserts. Invalid memory access is a great cause of concern in C and a vast amount of work has already been done. I would expect to adopt the best-of-breed approaches on a opt in basis, with detect & warn, suggest corrections and automatic remediation as options.
A major focus would be designing for strict configuration management. It should be possible for source code version management tools to completely control how the compiler is invoked for each source module. The goal should be that once a code module is tested and approved, the object code should never change without a new approval process. If that means compiling to assembler code and then storing the assembler file as the primary source document with the C source as a reference, so be it. Maybe add electronic signatures to source code modules. There also needs to be some way of avoiding the Hobson's choice of turning off warnings or being flooded by them. Maybe all the hooks to do this are already present and all that is needed is a best practices document. Whatever. Effective change control is a necessity in any sane engineering discipline.
To me the question is what kind or organizational; effort is needed to make this work and get compiler people thinking about security as something that they wish to assist with?
Arnold Reinhold

@_date: 2016-09-20 15:40:24
@_author: Arnold Reinhold 
@_subject: [Cryptography] Ada vs Rust vs safer C 
Consider what I said shorthand for what you said. This issue and its consequences, such as deleting integer overflow checks, has been discussed many times on this list over many years. So has the problem of secure erasure, and others. My question is what is the best way to get things changed? The literature on organizational behavior and even psychological abuse may be more relevant here than formal language design or type theory. (You didnt perform your overflow check in exactly the approved way, so you must be punished. We cant change our standard because of computer designs from the 1950s that were obsolete before work on C began. Other people will suffer if we make the changes you request. An abuse counselor would have no difficulty recognizing the constructs.)
Possible ways forward include:
* Holding a meeting
* Engaging some compiler developers in a non-hostile conversation
* Writing a group letter to FSF
* Talking to Richard Stallman (I remember when he used to wear an Impeach God button. That is what we seem to be asking.)
* Publicizing the issue
* Going to the standards bodies
* Starting our own standards organization
* Forking GCC
* Looking for funding (grants, Kickstarter, sugar parent, )
If not now, when?
Arnold Reinhold

@_date: 2016-09-21 21:55:03
@_author: Arnold Reinhold 
@_subject: [Cryptography] Ada vs Rust vs safer C 
Disagree. Cyber security is a major problem, a major threat to national security, even arguably to civilization as we know it. The head of the CIA, John Brennen, says it "really is the thing that keeps me up at night. We also know that unreported security flaws, "zero days, are the underpinning of the most dangerous cyber attacks and they are avidly stockpiled by cyber warfare groups and, likely, criminal enterprises. There is a thriving black market for such zero days. We are not going to see most of them until it is too late. Computer security must be proactive, not just reactive. We can make reasonable judgements about compiler behaviors that weaken security without waiting for a disaster post mortem.
But the article also says that the effect is larger for specific issues, such as memory bugs and security; see Table 8 and Result 4. Security bugs were only 2% of the total bugs studied, however.
Not doubt, but the issues we are discussing affect them as well, see below.
I wouldn't be surprised if NSA had similarly restrictive rules.  I agree, but thats something of an over simplification. There have been many efforts to get programmers to follow better security practices. The issues with C that we are discussing sabotage those efforts. Its one thing to try to get programmers to take extra effort to check for overflows or zero out sensitive data after it is no longer needed. Quite another thing to teach them the exacting incantations that C requires be used to accomplish those tasks, if they are even possible. Consider Rule 16 of the JPL guidelines cited above:
"Rule 16 (use of assertions) Assertions shall be used to perform basic sanity checks throughout the code. All functions of more than 10 lines should have at least one assertion.
The C optimizer can and does remove those sanity checks if the behavior is considered undefined.
Large companies like Sony are more likely to get their act together than the smaller firms in the IoT space. Imagine how much more progress we could make it the tool makers were on our side. Arnold Reinhold

@_date: 2017-04-02 00:39:33
@_author: Arnold Reinhold 
@_subject: [Cryptography] Removal of spaces in NIST Draft SP-800-63B 
Sent from my iPhone
What you suggest may make sense in an historical context, it's the best explanation I've heard so far, but sending clients their password requires that the computer service provider store the plaintext password, which is bad practice and is totally prohibited by the NIST 63B draft. So it's not a justification for space elimination being allowed in 63B. Arnold Reinhold

@_date: 2017-04-02 02:24:31
@_author: Arnold Reinhold 
@_subject: [Cryptography] Removal of spaces in NIST Draft SP-800-63B 
Sent from my iPhone
Sorry, I misunderstood. I thought you were describing a situation where the help desk sent a forgetful user the password they had originally established.  In the use case you describe, which I agree has other problems, there is no reason the system generated temporary password would contain spaces in the first place. I've never seen any that did. And I'm not aware of any OS where cutting and pasting into a form field adds leading or trailing spaces, but if that's the problem, only those should be permitted to be removed and perhaps only in the time window during which the temporary password is valid. Arnold Reinhold

@_date: 2017-04-03 11:10:11
@_author: Arnold Reinhold 
@_subject: [Cryptography] Removal of spaces in NIST Draft SP-800-63B 
I took a look at NIST 800-63 A and B and did not find anything suggesting password resets via security questions was OK. They mention knowledge based verification (KBV) but that seems to mean using info that the proper user might know, e.g. what was your last credit card transaction, and NIST seems to discourage its use. The NIST specs deal with three levels of verification and re-establishment of lost authenticators at the upper two levels is spelled out and requires re-verification of the means of identity verification used in initial establishment to some degree.
Password resets via security questions are a huge security hole since many answers can be guessed, found by researching a user's online presence or phishing techniques ("free dating service, just fill out this questionnaire). Im not sure hashing answers helps much. If an attacker gets a hold of the hash value, the universe of possible answers to test could be quite small by cryptographic standards. (All movie, book and song titles, for example, all city names, all valid address in the U.S., all names in a phone book -- testing these against a known hash output would be quite easy.) Chained encryption with random initial padding might make more sense, since the data would be accessed infrequently. The account creation software could encrypt answers with a public key and the decryption could take place in a special server used only for answer verification. Having the decrypted plaintext answers available would let a human intervene if needed. But I agree that canonicalization of the security answers could be acceptable. It might include removing multiple spaces, converting other whitespace characters to space, even removing punctuation and converting to single case. These steps would aid in automatic verification. But I dont see a justification for removing single space characters even then.
Arnold Reinhold

@_date: 2017-04-04 12:42:33
@_author: Arnold Reinhold 
@_subject: [Cryptography] Removal of spaces in NIST Draft SP-800-63B 
Getting a voice sample at account creation and using voice prints, asking for a high resolution version of a low resolution jpg file already posted, asking a long time friend on the site to verify, asking for a short selfie video of them responding to a random question; there are lots of possibilities. Lets agree that security Q/A isnt going away, whether I like it or not.
Hashing does no good for simple answers and it isnt suitable for complex answers.
   Initial answer "He made me run up and down our driveway, naked.
   Challenge answer: "He made me run naked up and down our driveway.
Hashes would not match. Encryption would allow easy human intervention and I suspect current language understanding software could match up the two answers. Even a simple algorithm such as sorting the words alphabetically and calculating a correlation might work well enough in many cases. The goal would be to avoid human intervention in most reset requests. You cant rely on humans remember the exact way they answered a complex question.  I had to exercise in front of the house with no clothes on might still take a human to verify.
In every case, it seems to me, hashing is NEVER right for security Q/A. Maybe you could update your cheat sheet?
Im sorry if I wasnt clear, but I agree with condensing multiple space to a single space, as well as other canonicalization measures (though if encryption is used to store the answers, there is no need to canonicalize until the answer is decrypted and ready to be tested). What I was trying to say is there is no justification in removing single space characters. ""Hemademerunupanddownourdriveway,naked. helps neither human nor machine.
Arnold Reinhold

@_date: 2017-04-10 18:00:51
@_author: Arnold Reinhold 
@_subject: [Cryptography] Removal of spaces in NIST Draft SP-800-63B 
Thanks for the detailed response. I think there is a very big problem here. As far as I can tell there are no published standards for storing responses to security questions, nor for storing user generated questions. As we discussed, the methods used for passwords are not necessarily applicable because the space of likely answers to most canned questions is far smaller than the space of passwords. From what you say about there being few problems with hashing user-generated questions, my guess is that people mostly choose questions with short answers. Perhaps they have been trained by unforgiving password reset systems that demand exactly identical answers. It seems to me there are several ways to improve the system. There are two problem areas: security, and user friendliness. For security, it is clear that a simple salted hash is worthless for protecting most security answers, no matter what your lawyers believe.
1. One improvement is to use a resource-consuming key derivation function to slow down attacks. NIST draft SP-800-63B recommends such an approach suggesting PBKDF2 with a minimum of 10,000 repetitions. More would be needed to protect security answers I would prefer a memory intensive algorithm, such as scrypt or, better, argon2, the winner of the recent password hashing competition. Account creation and reset are much less frequent events that logins and so consuming, say, a second of server time to protect security answers does not seem unreasonable.
2. Incorporating a corporate-wide secret key, along with individual salt, to hash the security answers. Protecting that secret is still a problem, but at least it would add some security.
3. Encrypt the hashed answer, along with a prepended random pad, using a public key and then only store the encrypted hash value. Employ a hardware security module to decrypt the hash when needed, ideally attached to a dedicated serve that just does password re-sets, with limited connections to the corporate intranet.  This method might be strong enough to usefully encrypt answers to multiple choice questions.
4. Use a similar approach as 3 to protect user-supplied questions. The questions are about as sensitive as the answers, since knowing the question is likely to limit the space of answers to be searched.
5. Canonicalization, as we discussed, could help a little with complex answers.
6. Use a similar approach as 3 to protect user-supplied answers. I understand your lawyers do not want to take responsibility for protection user answers, but other organizations lawyers might see the risk in discouraging more secure (i.e. lengthy) answers.
Arnold Reinhold

@_date: 2017-08-10 11:50:45
@_author: Arnold Reinhold 
@_subject: [Cryptography] NIST SP 800-63-3 
The final version of NIST SP 800-63-3 "Digital Identity Guidelines" was finally published on June 22, in four volumes. Volume B, Authentication and Lifecycle Management, Section 5.1.1.2 has a lot of interesting things to say about passwords and pass phrases, though it calls them memorized secrets which perpetuates the quaint idea that people will memorize all or even most of the passwords they use. Like many standards 63B has both mandates, recommendations and allowances (SHALLs and SHOULDs and MAYs). On the topic of allowing the removal of spaces, which was discussed earlier on this list, the final version says:
   To make allowances for likely mistyping, verifiers MAY replace multiple consecutive space characters with a single space character prior to verification, provided that the result is at least 8 characters in length.
Thats a lot better than the original text, which allowed total removal of space characters, though I would like to see evidence that adding extra spaces is a likely mistyping. Recommendations include
o Allowing passwords up to 64 characters, o Allowing all printable ASCII characters including space, oallowing UNICODE characters with normalization, o Not requiring passwords to be changed periodically and o Not using composition rules (e.g., requiring mixtures of different character types or prohibiting consecutively repeated characters).
Among the mandates (SHALLS and SHALL NOTs):
o User-selected passwords must be 8 characters or longer o No truncation of passwords
o No password hints
o New passwords must be compared against a list of commonly-used, expected, or compromised passwords
o Limiting the rate and number of failed attempts (100 max) before a successful authentication.
o Use of encrypted channels for entering and changing passwords
At least 32-bit salt
Perhaps the most important mandate is the following:
   Verifiers SHALL store memorized secrets in a form that is resistant to offline attacks. Memorized secrets SHALL be salted and hashed using a suitable one-way key derivation function. Key derivation functions take a password, a salt, and a cost factor as inputs then generate a password hash. Their purpose is to make each password guessing trial by an attacker who has obtained a password hash file expensive and therefore the cost of a guessing attack high or prohibitive. Examples of suitable key derivation functions include Password-based Key Derivation Function 2 (PBKDF2) [SP 800-132] and Balloon [BALLOON]. A memory-hard function SHOULD be used because it increases the cost of an attack. The key derivation function SHALL use an approved one-way function such as Keyed Hash Message Authentication Code (HMAC) [FIPS 198-1], any approved hash function in SP 800-107, Secure Hash Algorithm 3 (SHA-3) [FIPS 202], CMAC [SP 800-38B] or Keccak Message Authentication Code (KMAC), Customizable SHAKE (cSHAKE), or ParallelHash [SP 800-185].
NIST also recommends another layer of protection using a keyed hash with a secret key:
   In addition, verifiers SHOULD perform an additional iteration of a key derivation function using a salt value that is secret and known only to the verifier. This salt value, if used, SHALL be generated by an approved random bit generator [SP 800-90Ar1] and provide at least the minimum security strength specified in the latest revision of SP 800-131A (112 bits as of the date of this publication). The secret salt value SHALL be stored separately from the hashed memorized secrets (e.g., in a specialized device like a hardware security module). With this additional iteration, brute-force attacks on the hashed memorized secrets are impractical as long as the secret salt value remains secret.
These are big and long needed improvements. NIST is effectively saying that the use of a standard, fast cryptographic hash function, such as MD5 or the SHA series, by itself and even with salt is no long acceptable for storing password validation data. A KDF that slows the hashing process must be used.
Of course NIST publications are only mandatory for the U.S. Federal Government. But other enterprises are now on notice that simply hashing passwords is no longer considered a safe way to protect the data. Its about time we shifted the responsibility for password security from asking users to pick ever more complex passwords to requiring the organizations that request passwords to take reasonable precautions in storing validation data.
Arnold Reinhold

@_date: 2017-08-12 23:06:59
@_author: Arnold Reinhold 
@_subject: [Cryptography] NIST SP 800-63-3 
First of all, with millions of password hashes released in data breaches, "linear in the number of passwords to be cracked is no small thing. A factor of a million reduction in attack rate due to the simple expedient of using salt is equivalent to increasing each passwords entropy by about 20 bits, which can be the difference between a password complexity that people might employ and requirements that are too onerous for most users. But as you point out, salt also makes rainbow tables infeasible and rainbow tables offer a speedup much larger than a factor of a million for vast numbers common passwords, allowing passwords hashed into the table to be recovered in seconds.
I dont know NISTs reasons for suggesting memory hard hashing be used along with a secret hashing key, but it could make sense as a belt-and-suspeners layered defense.  For HSMs to be used for password hashing, there must be some way to clone the HSM and/or backup the secret key.  That secret then becomes a potential, and extremely valuable, target for attack. How much confidence should we place in ordinary enterprises protecting such secrets?  I gave a talk on this problem a year ago at BsidesLV16:  Arnold Reinhold

@_date: 2017-08-14 22:05:13
@_author: Arnold Reinhold 
@_subject: [Cryptography] NIST SP 800-63-3 
Here is what NIST SP800-63B says re your first point:
"5.2.2 Rate Limiting (Throttling)
When required by the authenticator type descriptions in Section 5.1, the verifier SHALL implement controls to protect against online guessing attacks. Unless otherwise specified in the description of a given authenticator, the verifier SHALL limit consecutive failed authentication attempts on a single account to no more than 100.
Additional techniques MAY be used to reduce the likelihood that an attacker will lock the legitimate claimant out as a result of rate limiting. These include:
When the subscriber successfully authenticates, the verifier SHOULD disregard any previous failed attempts for that user from the same IP address.
That would seem to include the VMS approach, unless VMS was silent about it's lockout, which I think users would find extremely obnoxious. Presumably the NIST-recommended throttling would start long before the 100 limit was approached. Other methods should, imho, include sending email to the account holder after a few fails and allowing a help desk to temporarily reset the lockout time, allowing a locked out user to enter the correct password immediately. Another related measure I would include is not counting a blank password or an entry the same as the previous entry as failed attempts, since they are of no benefit to an attacker and represent common user errors (e.g. caps lock on). As for your second point, I think the dangers posed by inadequate security in storing password validation data have reached such a level that they overwhelm any concern about requiring some new tools for password reset.
Arnold Reinhold

@_date: 2017-08-17 05:35:47
@_author: Arnold Reinhold 
@_subject: [Cryptography] [FORGED]  NIST SP 800-63-3 
NIST deals with this problem via the following recommendation in 800-63B Section 5.1.1.2:
In order to assist the claimant in successfully entering a memorized secret, the verifier SHOULD offer an option to display the secret  rather than a series of dots or asterisks  until it is entered. This allows the claimant to verify their entry if they are in a location where their screen is unlikely to be observed.
If the password is displayed in a monospace font, the extra spaces would be clear, except maybe at the end. A font that had a glyph for space, a faint blue dot perhaps, would be even better. In addition, if extra erroneous spaces are a significant problem, there is nothing to prevent the password verifier from displaying a message when rejecting a password saying it contains spaces at the beginning and end, or multiple consecutive spaces in the middle. Some systems i have encountered warn you that the caps lock is on. (They may have a way to detect this directly in JavaScript, but detecting that all alphabetic characters in the bad password are  uppercase would work too.) I would much rather have a clear principle that passwords should be evaluated as entered without any changes. If there is a perceived need to protect the user from common typing errors, issuing a warning when possible errors are detected is a better solution. Any kind of password mangling, especially if its silent, weakens security for a few users and simplifies the task for attackers by reducing the number of combinations they have to try. By contrast, warnings have no security impact, as any attacker can be presumed to know what characters they entered. As a separate matter, I think systems using passwords should be required to disclose the security measures they are using to protect those passwords. Otherwise users must always select passwords strong enough for the worst case.
Arnold Reinhold

@_date: 2017-08-20 08:32:13
@_author: Arnold Reinhold 
@_subject: [Cryptography] [FORGED]  NIST SP 800-63-3 
NIST says "All printing ASCII [RFC 20] characters as well as the space character SHOULD be acceptable in memorized secrets.  So nonprinting characters can and should be ignored.  For Unicode, NIST advises:
"If Unicode characters are accepted in memorized secrets, the verifier SHOULD apply the Normalization Process for Stabilized Strings using either the NFKC or NFKD normalization defined in Section 12.1 of Unicode Standard Annex 15 [UAX 15]. This process is applied before hashing the byte string representing the memorized secret. Subscribers choosing memorized secrets containing Unicode characters SHOULD be advised that some characters may be represented differently by some endpoints, which can affect their ability to authenticate successfully.
Unicode normalization is complex issue, see e.g.  and I dont know remotely enough to say the normalization modes suggested cover all potential password-related problems in Unicode, but they clearly represent a carefully considered attempt by people with much expertise on the topic.
Overall I strongly agree with your statement that the system is there for its users, not the other way around.  If the new NIST recommendations were widely adopted, users would benefit by not have to chose super-strong passwords. Where I differ is in the best way to help the user. My big concern is giving the developers of password authentication systems leeway in improving passwords, beyond what NIST allows. I can see developers removing all sorts of text patterns that do not normally have semantic meanings, such as a letter repeated more than twice, capitalizations in the middle of a word, special characters in the middle of a word, words that are close enough to dictionary word to be likely misspellings, and so on. On the other hand, one of the few strategies available to users who want strong yet memorable passwords is to introduce just such non-semantic transformations. So I would not allowing password transformations beyond the ones NIST permits and instead encourage developers to include messages along with password failure notices that call attention semantically dubious patterns, such as those mentioned above and others, such as entering the same incorrect password a second time. Then let the user reenter the password with corrections if they were unintentional errors, but dont degrade the security of those users who deliberately chose such variants.
Arnold Reinhold

@_date: 2017-12-22 15:08:42
@_author: Arnold Reinhold 
@_subject: [Cryptography] Rubber-hose resistance? 
Umm, no, you can't get rid of random noise with a filter. You can remove the annoying high frequency portion, but you remove the high frequency component of the signal as well. If you could reduce in-band noise with a passive filter, you could create a node with a lower noise temperature than the source and hence build a perpetual motion machine, violating the second law of thermodynamics. Writing zeros to a disk may be good enough to prevent data recovery, but writing random data is no worse. Of course this is mostly moot with the rise of SSDs, which, as others have pointed out, are hard to erase with any certainty. Recent Apples iOS devices solve this by encrypting the entire volume with a key stored in a special effaceable memory that they erase when you reset the device. SSH clients are available for iOS, so one can travel with a clean device and download sensitive data after crossing a border and then reset the device before leaving. A reset takes only a few minutes. The device reboots to factory settings once you get through several are you sure you really want to do this? screens. The procedure simple enough for someone without computing skills to do.
Arnold Reinhold

@_date: 2017-01-02 14:50:42
@_author: Arnold Reinhold 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
How much would it add to the cost of an electric meter to employ an SoC with enough power to implement the security standards? A dollar? Two? That cost would presumably be recovered from the rate payer over the life of the meter, a few cents a month. The problem is a broader one in my view, the lack of a body that sets security standards with enough clout to say, no, really, you have to do this.
Arnold Reinhold

@_date: 2017-01-02 23:01:39
@_author: Arnold Reinhold 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
Im afraid I dont find that a satisfactory explanation. There is no unregulated commons here. The power company owns the meter and the upstream infrastructure and is responsible for reliable power delivery. If anything power companies are ideal customers for good security solutions. They are used to creating and following detailed specifications for everything they build, and they generally procure high quality equipment that is designed to work for decades. Much of their management has an engineering background. The utilities are typically regulated monopolies that get paid a reasonable return on invested capital, so if anything they have an incentive to buy premium products. The meter company will produce what their major (only?) customer demands and can earn extra profits from a more complex product. The crypto authentication code could be incorporated in the radio modem, which could be a purchased black box from the smart meter manufacturers perspective. I think there is a puzzle piece missing here.
Its entirely possible the smart meter PKI spec is too new for compliant meters to be available yet, in which case there is no story here. Its just too early. But we know from the links you supplied previously that companies with serious technical competence, like BT, are eager to supply this market. Im sure SoC and radio modem vendors are interested too. Getting product to market shouldnt take ten years.
Arnold Reinhold

@_date: 2017-01-03 18:05:25
@_author: Arnold Reinhold 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
Of course they are eager to make money. Thats not evil per-say. I presume large power utilities know how to run competitive procurements for sophisticated technology. Whether they have the skills on board or on tap to evaluate proposed security solutions is another question. I give them a better shot than large retailers, but its a concern. That said, its not clear to me that the PKI proposals are necessarily bad. This is a much simpler problem than the Internet PKI with its hundreds of independent CAs that are each trusted by default. The utility can be its own root CA and can exercise complete control of any second level CAs. Maybe there are simpler solutions. If so Id expect some firm to bid one. I was not familiar with LoRa, but I found this white paper:
  Putting aside long term problems with the fixed symmetric keys it uses, I notice that the LoRa physical layer depends on gateway transceivers "able to transmit over several kilometers. That means the utility will have to acquire spectrum, assuming it doesnt want to share the ISM band, and would have to place, connect and maintain dozens of Gateways throughout a city to get total coverage. A solution that uses the existing cellphone network might be more attractive to the utility as the only devices needed to be installed and maintained in the field would be the smart meters themselves. I expect a large utility is competent to make these tradeoffs, if they have access to people with good cryptographic judgement. Theres the rub, in my opinion. Cryptographic standards are set in too many places, with government security agencies having mixed agendas. Perhaps the world needs a Cryptographers' Cabal to provide independent judgement.
Arnold Reinhold

@_date: 2017-01-04 10:41:46
@_author: Arnold Reinhold 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
[Nice description of mechanical power line recloser breakers elided]
Late last century I had a gig at Foxboro Corporation, a well-respected manufacturer of industrial instrumentation and control systems. The stuff I was working on was electronic and computer based, but they had been in business since 1908 and had developed many intricate, mechanical and pneumatic sensors and control systems, which they still supported. After the Iran-Iraq war, Foxboro got a flood or orders for the older, non-electronic devices from refineries and chemical plants in the Middle East because they had proved more resilient in war time than the newer electronic systems. We need to heed that lesson in the context of cyberwar.
More to the point, however:
One skill essential to any good engineer is knowing when you are out of your depth and finding engineering resources with the necessary skills. So in this regard the power guys are likely in better shape than the vast majority of commercial, government and non-profit enterprises, who have few if any traditional engineers on the payroll, and probably none in management. (Im not counting the IT department, which every enterprise has.) The problem I see is where does our power engineer go to to get reliable advice on a security design? There is so much snake-oil security out there. Vendors have their products to sell, right or wrong, government security agencies want to protect their offensive capabilities, standards bodies are not specialized in computer security and treat it as one more chapter in stove-piped specs. Who would you recommend?
Arnold Reinhold

@_date: 2017-01-16 08:06:54
@_author: Arnold Reinhold 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
As far as I know there was no cybersecurity in industrial controls back then, hence the effectiveness of Stuxnet. And it wasnt just electrical failures, it was overall robustness in situations where refineries and the like came under conventional attack (bombs, shelling). The mechanical stuff was more likely to keet working and could be more easily repaired after an attack. Ive seen too many bad security products designed by people who thought they knew enough, but didnt, to accept telling engineers to "do it yourself as the answer to the question I posed. Your right that industry standards tend to be too late and too bloated, (and possibly sabotaged by state actors). I think we need some sort of professional organization to develop lighter weight standards  that can become components in engineered systems. NaCl is a start. But why not develop a few simple use cases, e.g. simple machine to machine message passing, and designing solutions around them? Arnold Reinhold

@_date: 2017-01-17 14:16:46
@_author: Arnold Reinhold 
@_subject: [Cryptography] Cryptocurrency Exchange without a trusted third 
The implication is that BitCoin, unlike fiat currency, is immune from inflation. That is only true if you ignore the risk of deflation, which is just inflation with a negative sign in the exponent. There is a cap on the amount of BitCoin that can be generated (21,000,000 vs. ~16,000,000 now in circulation), so were BitCoin to become much more popular, the value of each BitCoin would increase relative other measures of value, such as market baskets of fiat currencies, consumer goods, or stocks. If you consider BitCoin a currency, that constitutes deflation.
Inflation of a currency takes from holders of assets denominated in that currency; deflation takes from people who owe debts denominated in that currency.  Economies depend on the ability to borrow money long term and make other arrangements for future payments, such as employment contracts. Unexpected inflation or deflation could be considered theft from the losing party. Neither the debtor nor the creditor is more worthy of protection. Would you take out a mortgage with payments denominated in BitCoin? Yes, there are adjustable rate mortgages, but they rely on some index created by a trusted third party. If you hired someone with a salary in BitCoin, how happy would they be when you lowered their pay check periodically to account for deflation? (No one complains about cost-of-iving raises, not is their absence as galling as a periodic pay cut.)
Arnold Reinhold

@_date: 2017-01-23 13:35:49
@_author: Arnold Reinhold 
@_subject: [Cryptography] Oracle discovers the 1990s in crypto 
Its true that tampering with existing signed objects requires a second pre-image attack, but creating two versions of software with the same sig could be accomplished by an entity who has the ability alter the final version of the legitimate object being signed, say by modifying a random nonce, bitmap or seed, or messing with white space in comments. This attack mode would only require a collision attack. The entity could be a mole working for loyalty to a cause, financial gain, or under duress. It might be done remotely if configuration management security is breached or the government could order cooperation e.g. in the U.S. by National Security Letter. Arnold Reinhold

@_date: 2017-07-11 08:04:13
@_author: Arnold Reinhold 
@_subject: [Cryptography] [FORGED]  Attackers will always win, 
This is a classic engineering pattern: we ignore b because a is a much more likely. Effort is successfully directed at a and eventually b becomes a major problem. A lot of effort is going into getting software that does produces the correct answer: new, safer languages such as Rust, Swift and Haskell, formal verification systems that Perry, at least, thinks are becoming real, better testing tools, etc. We may not there yet, or even close, but progress is being made. So I think it is appropriate to pay attention to b. Newer symmetric ciphers are already designed to minimize side channel leakage. Hardware security modules are a potential solution in some cases, but they have limitations, including size, cost, and the potential for hidden flaws, accidental or deliberate. It that power supply filter capacitor inside the protective mesh? If not it may be easy to disconnect. Can I update my HSMs firmware? Is yes or no the better answer? One possible area for improvement is the tool chain. The C/C++ folks insist that they only make promises about final outputs of programs, not what happens to data while it is being processed.  Does anyone doubt that compilers and linkers could keep track of sensitive data and insure that it was handled in a way that minimized leakage to the extent possible? Do any of the newer languages do better? Do we need special languages for crypto, perhaps with fewer features but more predictable relationships between source and object code? Or maybe just configuration management tools that insure compiler optimization levels are never turned on? No doubt there are other areas worth looking at as well. Arnold Reinhold

@_date: 2017-07-12 11:43:36
@_author: Arnold Reinhold 
@_subject: [Cryptography] [FORGED] Attackers will always win, 
It may be of interest to know how much faster a maximally optimized version of my code runs or what optimization might do to key space, but I always have the option of doing those experiments during development and testing. (And any speed difference is likely dwarfed by the fact that an attacker can be running 100,000 pipeiined instances on 5GHz custom hardware cooled by liquid nitrogen, while my production code is running on an 8 MHz Atmel processor.) The code I ship only needs to be fast enough to keep up with the traffic I expect while allowing the processor to do its other work. I might prefer code 10X slower than optimized if it reduces side channel risks.
Almost all processors today have similar integer instructions: twos complement arithmetic, 8, 16, 32 and 64 bit words, shifts at bit resolution, and, or, xor, not, etc. A crypto designer could write code with expectations of what instructions will be executed and in what order. The compiler community, as I understand things, only is concerned about the final output and feels free to rearrange things or eliminate steps that don't affect the final result. As a minimum, I want configuration control over whether that happens or not.
Even better would be having the compiler suggest reordering and maybe additional computations that produce the same result with reduced possibility of leakage. And also make sure sensitive data is zeroed out as soon as it is no longer needed, and keep track of intermediate results and zero them out too; and make sure sensitive data and intermediate results are always stored in memory that is never swapped to mass storage. And Im sure there are possibilities I havent thought of if the compiler communitys creativity were applied to our problems.
Regardless, from a security engineering perspective, we should have total control over what crypto object code we ship.  I dont want my code altered just because someone elsewhere in the organization decides to up the optimization level to make the user interface snappier. Arnold Reinhold

@_date: 2017-07-13 21:54:29
@_author: Arnold Reinhold 
@_subject: [Cryptography] [FORGED] Attackers will always win, 
Its possible to measure and characterize the instruction timings of specific processors. A compiler could take advantage of that data, which might be more trustworthy than manufacturers specifications anyway. At the least its worth finding out if there is, in practice, significant variation based on "particular exact mask and microcode level.  But if it really is a problem that affects side channel security, then yes, it may be necessary to test incoming batches of microprocessors and customize code for them. I doubt it would come to that, but this sort of tight product control is not uncommon is product engineering. More likely security product manufacturers would select processor vendors who did not allow such extreme variation.  For software intended to work on whatever is out there, including a timing test at startup that selects one of several implementations might cover most cases. For timings that are outside some acceptable range one might issue a warning issue a warning or refuse a secure connection, if thats what it takes. There may also be other tricks that the compiler could do, such as inserting no-ops to make instruction timing more consistent. There are other problem areas where crypto-friendly compilers might help, such as insuring sensitive data is zeroed as soon as possible and never written to shared memories.  But at the very least, having some way to insure that compilers do not rearrange security critical code is just engineering common sense.
Arnold Reinhold

@_date: 2017-07-16 10:55:57
@_author: Arnold Reinhold 
@_subject: [Cryptography] Defeating timing attacks 
That is an excellent approach, but it cant be the only answer. First, as you say: ... this still
requires tremendous attention to detail. And not just in design and manufacture. A careless repair technician can defeat a Faraday cage by failing to replace one screw (that tiny one that fell behind the desk). A malicious tech can turn an RF gasketed seam into a slot antenna with a few drops of clear nail polish. Not all applications of cryptography can afford the extra costs or long term maintenance requirements. So addressing timing attacks through software design is part of a sensible belt and suspenders approach.
You are correct that:
Another way of saying that is randomizing the timing is equivalent to X decibels of shielding. How big an X can be achieved is a valid subject for research. There is no reason why one couldnt put a crypto module in a test cell with appropriate sensors (RF, power line, and acoustic, with shields and filters removed) and allow the compiler to download different instructions configurations and make adjustments based on computed correlations between recorded test data and the signals we are trying to hide. If this process takes hours or weeks, so what?
Again as you say: "There is a rather long list of ways bad guys could exfiltrate information about your message, and you have to stop them all. I dont have to come up with every possible approach to argue that cooperative compilers could reduce the risk of many of those exfiltration paths. and certainly not to insist that crypto designers must have end-to-end control of the software that is put into production, lest the task become insurmountable.
Arnold Reinhold

@_date: 2017-06-22 06:52:23
@_author: Arnold Reinhold 
@_subject: [Cryptography] Brainstorming for encrypted text messaging 
Here is my take on a non-electronic system for securing text messages.
As described in the Boak lectures,   the NSA developed several formats to simplify the use of one time pads. One of these, code named ORION,  , consists of a sheet printed on both sides. The front side had 100 sets of complete mono-spaced, ordered alphabets (ABCDEFGHIJKLMNOPQRSTUVWXYZ). The back had 100 randomly permuted alphabets, one behind each ordered alphabet on the front, and aligned so that the the letter positions on both sides matched to a fraction of a letters dimensions. To encode with such a sheet, one placed it on a sheet of carbon paper with the carbon side up. The user circles  one letter of plain text in each alphabet. After the letters in the message (up to 100 characters) were circled, one flipped the page over and read off the ciphertext from the randomized alphabets. To decode, the procedure was reversed.
A naive way to use the ORION system would be to follow the encoding procedure above and then take a picture of the reverse side and send it to recipient (who is assumed to have a matching pad). She could print it out the image, place it over the matching pad sheet and follow the decode procedure above. One could include fiducial marks on the OTP sheets that the computer could use to correct for any distortions, insuring the printed-out image would be in proper registration.
The problem, of course, it that the position of the circled letter in each line of the ciphertext side reveals the plaintext letter.  I propose to overcome this as follows. Instead of ordered alphabets on the  front side, have randomly scrambled alphabets. In other words, the front side of my OTP would look like the back of an ORION sheet. The reverse side would be blank except for the calibration marks and an identifying number or bar code for the OTP sheet. Encoding would be slower, since the user would have to hunt for the right letter in each scrambled alphabet, but our eyes are pretty good at that. After encoding, the reverse with its carbon paper circles and fiducial marks, would be photographed, and the image sent to the recipient. There would be no need for alphabets on the reverse, The position of the marks in the image would encode the ciphertext. As above, the recipient could print it out the image, place it over the matching pad sheet and follow the decode procedure above.
On variation would be to have the calibration marks printed on the front as open bubbles and require the user to fill them in when encoding, or have a combination of pre-prided marks on the back plus a few user supplied ones to correct registration. This would eliminate the need to print the OTP sheets in exact registration, the difficulty of which is the reason Boak gave for NSA is no longer using ORION.
Arnold Reinhold

@_date: 2017-06-29 19:06:30
@_author: Arnold Reinhold 
@_subject: [Cryptography] Brainstorming for encrypted text messaging 
You are correct, I am just using ORION backwards. According to Boak, NSA abandoned ORION due to the difficulty of getting front and back in alinement, I was trying to avoid this problem by not printing the other side, but I have since done a test on an Inexpensive B/W laser printer, a Brother HL2240D, and it seems to have enough repeatability to make two sided ORION sheets, at least at 12 point type. Such a printer connected to a computer with no or little writable persistent storage, such as an old laptop sans hard drive that boots from a live CD-Rom should be able to print ORION sheets with full security, assuming a good source of random bits. (and, no, I don't want to restart that discussion).  The only need for persistent memory from batch to batch would be giving each set of sheets a unique edition ID. This could be done by having a paper list of IDs and entering one manually at the start of each batch's production. A Raspberry Pi has too much writable storage to be ideal for pad generation. Arduinos dont seem suited for driving a laser printer. Since encryption and decryption are symmetrical there is no loss of security, only loss of some, but not all, of ORION's convenience. You do have to find letters in scrambled alphabets, but you do not have to perform mod 26 arithmetic. I tried encrypting a short piece of text backwards using a sample of a related system called MEDEA ( It took a bit under 4 seconds per character. I think it would go faster with crisper text.  In exchange, the ciphertext side of the sheet is safe to photograph (assuming no detectable bleed through from the scrambled alphabet side). It also means that used carbon paper is harmless and can be safely reused. Back when Boak was lecturing NSA recruits, carbon paper was cheap and plentiful in every supply cabinet. It's far less common today. Staples sell a package of 100 sheets for $20. The bleed through issue could be addressed in a number of other ways, including using paper of at least a minimum weight and using grey printing for the scrambled alphabets.  Placing the sheet to be photographed on a dark backing when photographing the ciphertext, perhaps the carbon paper itself, should help. There was a discussion here a while back about dots in color copies being used to trace the machine used for the printing. To avoid such a technique leaking sensitive data, the unscrambled alphabet side could be printed separately in quantity before the rng is initialized to produce the scrambled alphabets. After printing scrambled alphabets on the other side of the unscrambled alphabet sheets, the printer's memory could be flushed by printing some number of not-to-be-used scrambled sheets that are then discarded. Ideally the computer and printer would locked up and never used for another purpose. Laser printers are cheap. One use case for photo-transmitted backwards ORION, without the image processing I originally proposed, might be someone in the field sending reports back to a headquarters, where someone could then do the more tedious decoding manually. Of course ORION sheets produced as above could be used in their original way and the ciphertext typed into a text message. Software for producing ORION style pads might be a useful addition to security-oriented distros.
Arnold Reinhold

@_date: 2017-03-10 08:44:21
@_author: Arnold Reinhold 
@_subject: [Cryptography] Crypto best practices 
They do say elsewhere:  "Authentication must be provided using HMAC, asymmetric cryptography, or by operating the chosen block cipher in Gaolis/Counter Mode (GCM).
The document appears to be from 2012, judging by the declassification date, so it is not surprising that it some of it seems out of date. As for RC4, it is only discussed in connection with their "Weak Suite which they say "shall be retired on 31 December 2013 and shall not be used for deliveries after that time. This date complies with NIST Special Publication 800-131A regarding protection of unclassified data for US Government systems. Interestingly, they say in one paragraph, marked S//NF, that the first 1024 bytes of the RC4 cryptostream should be discarded before use. In the next paragraph, marked TS//SI, they up that to 3072 bytes.
Another interesting recommendation: "Tools should perform key exchange exactly once per connection. Many algorithms have weaknesses during key exchange and the volume of data expected during a given connection does not meet the threshold where a re-key is required.xiii To reiterate, re-keying is not recommended. Footnote xiii adds "The exact nature of which algorithms are weak at this stage is highly classified. In the absence of those facts this guidance is still relevant; the utility inherent in re-keying derives from minimizing key exposure when performing bulk encryption of large amounts of data. Even the most data-intensive NOD operations involve several fewer orders of magnitude of data per session key. Consequently, re-keying introduces unnecessary complexity (and therefore opportunities for bugs or other unexpected behavior) without delivering value in return."
I also looked at the Wikileaks Apple iOS specific stuff. My two favorite tidbits: 1. The signup sheet for the Sublime Text editor--they have a ten user license and someone suggested getting a 15 user license next time. Not a massive team there. Maybe the NSA has a bigger shop. 2. An instruction in their DRBOOM user guide, complete with a screen shot, that in hooking up an iPhone to the CIA cracking server, when the target phone asks whether or not it should trust this new computer, "make sure you hit the trust option on the main screen".
Arnold Reinhold

@_date: 2017-03-12 11:09:16
@_author: Arnold Reinhold 
@_subject: [Cryptography] encrypting bcrypt hashes 
Here is a very different approach based on the Rock Salt talk I gave at BSidesLV last August:
  Build a table of random (or strong pseudo-random, see below) numbers with an entry for each PIN and salt combination. So based on your using 16-bit salt and 6-digit PINS there would be  10^6 x 2^16 = 65,636,000,000 entries. The table entries would serve the same function as a password hash, so say they are 64 bits or 8 byte numbers. The table size would then be 524.3 gigabytes, small enough to store in a terabyte SSD drive. Instead of calculating a hash algorithm, the login system would look up the proper value for each PIN + salt combination in the table. Security would come from restricting the rate at which data could be retrieved from the table, say by putting the SSD in its own isolated server with a limited bandwidth connection to the login server and employing software to detect unusual levels of requests. All messages would be fixed size to avoid overflow attacks.
To prevent attackers from just exfiltrating the smaller number of values associated with the more common 4-bit PINs, you might create a random permutation of [0,,999999] on the isolated server and pass the PINs through that first, thereby scattering the 4-digit PIN entries throughout the table. Multiple copies of the table and permutation array should be made, for backup and in case more than one server is needed for availability. But this data is static, so periodic backups would not be necessary and the table and permutation need not and should not be stored in the enterprise backup system. Physical security for the server and backup copies would be needed to prevent insider attacks.
Generating 524 gigabytes of random data isnt trivial, but the Intel RdRand/RDseed instructions should be good enough for this use. An alternative would be to generate the table using a strong pseudorandom number generator, say AES-256 in counter mode, on a machine isolated from the Internet. The seed key might be then destroyed or recorded on paper as an ultimate backup.
Again the security depends on the difficulty of exfiltrating such a large data set, not on a short key that that is relatively easy to steal.
Arnold Reinhold

@_date: 2017-03-15 23:14:41
@_author: Arnold Reinhold 
@_subject: [Cryptography] encrypting bcrypt hashes 
I do suggest additional randomization in my Rock Salt talk (  ), but I was trying to keep this simple for a transitional application. I envision the large file of pseudo-hash values to be kept in an isolated server, with only a restricted connection to the log in servers, a security appliance much like a HSM. Thus there would be no path from the internal or external network to the large file other than fixed format requests from the login server on a dedicated channel. I dont see how this offers less security than conventional access controls.
Arnold Reinhold

@_date: 2017-03-19 08:43:27
@_author: Arnold Reinhold 
@_subject: [Cryptography] Crypto best practices 
The requirement for a unique IV for each encryption when employing stream ciphers is central to their security, but not as stringent as the requirements on the nonce needed for each signature using DSA or ECDSA or the need to use unique primes in generating RSA public keys. Detecting a duplicate IV with two stream cipher encryptions compromises just the two messages. The IVs can be predictable, as long as they are never duplicated. By contrast, a duplicate or even slightly predictable DSA signature nonce compromises the signing key itself (q.v. the Sony PS3 hack). With RSA, using the same prime in two different public keys allows them both to be broken and efficient methods have been demonstrated (by Lenstra, et al) for detecting such lapses in the universe of RSA public keys. The only cure is to use a strong random starting point when searching for primes. Thus most modern crypto systems require strong sources unpredictable bits and are extremely demanding of correct programming to insure their proper use. The Wikileaked CIA programming guide requires use of the crypto libraries and strong RNGs supplied with modern operating systems, in part to help maintain cover for their implants, but also to avoid programming errors. Excellent advice to J.Random coder. The big problems we face are in embedded applications, including IoT, where vetted crypto libraries are less commonly available and where strong randomness is harder to come by. It seems to me that eschewing stream ciphers, especially those with desirable properties that can prevent other JRC programming lapses, is not the best way to solve such problems.
Arnold Reinhold

@_date: 2017-03-30 09:52:41
@_author: Arnold Reinhold 
@_subject: [Cryptography] Removal of spaces in NIST Draft SP-800-63B 
I filed a comment ( on NIST Draft SP-800-63B Digital Identity Guidelines urging removal of the provision in Section 5.1.1.2:  Verifiers MAY remove multiple consecutive space characters, or all space characters, prior to verification provided that the result is at least 8 characters in length. since it can reduce password entropy for no good reason. Id be curious to know if anyone can figure out how that got in there in the first place. My comment is here:
   Public comment period ends March 31 (tomorrow).
Arnold Reinhold

@_date: 2017-05-21 07:31:55
@_author: Arnold Reinhold 
@_subject: [Cryptography] Password rules and salt 
Bloom filters are a really cool way to enforce a global no-reuse password policy, but such a policy is neither necessary nor sufficient for password security. Its not necessary because a random password with enough entropy is unlikely to be reused often and if adequate salt is used, the likelihood of detecting any repetition from stored hashes is very low. And such a policy is insufficient because it won't detect closely related passwords. In current systems that insist on passwords changes and make sure that the new password doesnt match ones previously used, users learn how to make small changes to the older password and these changes often follow predictable patterns. Password cracking programs can mimic these patterns. A policy blocking reuse on a global basis will no doubt be evaded in the same way, so cracking programs that look for variations on common passwords will still succeed too often.
Then there is the problem of getting the password information to the Bloom filter. A cryptographic hash of the password is not protective, since knowledge of the hash allows rapid guessing. Worse, the hash used for the Bloom filter cannot include salt, since salt would defeat the function of the filter (the same password hashed with different salts will set different bits in the filter). Unsalted hashes are exceptionally dangerous as they are subject to precomputed database attacks, such as Rainbow tables. One could try to use an encrypted channel to send the hashes to the filter, but that is subject to all sorts of problems, The system maintaining the Bloom filter is yet another potential point of failure, one with access to password hashes from many systems. Even assuming the hashes are sent to the filter unaccompanied by any account-identifying information, knowledge of when an account was created or its password was changed would greatly narrow down the possible hashes and underlying passwords. One thing that might be of use would be a Bloom filter with entries for all the passwords generated by known cracking programs. That would not have any issues with forming hashes of actual user passwords and the filter could be updated with new common passwords and their variations periodically. There might be versions of the filter for different amounts of cracking program running time. Organizations could download a filter and use it to test user-created passwords locally. Bolting a Bloom filter to several open source password crackers and letting them run seems easy enough. This might be a good student project. The real problem with password based authentication is the difficulty of securely storing information needed to validate passwords. Forcing users to jump through more password hoops is not the best way to solve the safe storage issue, but thats another discussion.
Arnold Reinhold

@_date: 2017-11-14 10:24:35
@_author: Arnold Reinhold 
@_subject: [Cryptography] One Bitcoin Transaction Now Uses as Much Energy 
I mentioned this a while back. The problem is that the times of negative energy cost are sporadic and depend on the day of the week (weekends) and the time of year (early spring, when the days are longer and air conditioning is not yet needed). I suggested that  a good use of obsolescent cyber-coin mining boxes might be to ship them to wind and solar farms. There mining could take place whenever the wholesale price of electricity falls below what the mining gear could earn on for the same energy. The only real cost would be transporting the mining equipment (perhaps just mount the old boxes in an ISO shipping container and ship it to wherever). But this kind of energy scavenging is not going to materially alter the overall energy cost of mining. At some point it might make sense to build solar-powered cyber-mines in remote desert sites, where sunlight is plentiful but energy transmission costs are prohibitive. One question: does satellite internet service have enough bandwidth and low enough latency to support mining in locations far from landline Internet connections? I assume most of the traffic would be downlink; the occasional success message could be sent via low earth-orbit satellites (e.g. Iridium), or even HF radio, it the geosynchronous orbit time delay is too long..
Arnold Reinhold

@_date: 2017-09-25 13:44:54
@_author: Arnold Reinhold 
@_subject: [Cryptography] Crypto basic income 
I think one needs to consider what is meant by a "compromise of biometric details. It should be possible to build devices that scan biometric features in ways that assure an actual person is present and being properly scanned. The iPhone X face scanner seems like a step in that direction. I can envision an authentication booth at a bank, say, that scanned multiple biometrics, with trusted human supervision to minimize possible fakery, and maybe record a signed video of the process to be kept for future reference. People who are unable to come to the bank, due to illness, etc., could be visited by notaries with portable scanners. The problem is still safe storage of any private key or other secret generated from the biometric data.  It maybe be good enough for the authentication booth to sign ones public key, with the ability to use another authentication booth to generate a new key and revoke the old one if there is reason to suspect compromise, i.e. payments dont show up. It the guaranteed income is paid weekly, say, only a few weeks' loss might be at risk and that might be acceptable.  Arnold Reinhold

@_date: 2018-08-14 23:33:51
@_author: Arnold Reinhold 
@_subject: [Cryptography] Throwing dice for "random" numbers 
*) Another possibility: Dave mentioned reducing the sum modulo N, suggesting he is trying to get uniformly distributed numbers in the range 0 to N-1. If N is small compared to the expected mean of the sum, the result is likely to be close to uniform, but not exactly so. Here are some simple tricks for generating uniform integers using dice for common ranges, such as random decimal digits or random characters. Each of these methods produce perfectly random symbols, assuming the dice are perfect. To generate a string of random decimal digits, for example, one can roll many dice and record their values, discarding any that come up 6, until you have enough digits. Then roll that number of dice again, noting whether each outcome is odd or even. If odd, add 5 to the previous value, with 5+5 recorded as zero. For random characters from the set {a-z, 0-9}, one can make a six by six table with the 36 possible characters and use pairs fo dice rolls to select each character. For just letters, one can just discard pairs of rolls that produce digits from that table. To include upper and lower case letters and some special characters (the ten above the digits on standard keyboards), one can use a third die and press the shift key if the roll is odd. In the last example there are 72 possible characters, giving 6.17 bits of entropy per character, compared to 6.57 bits per character selected randomly from the 95 printing ASCII characters.
There are more examples in the diceware.com FAQ.
Arnold Reinhold

@_date: 2018-08-20 07:09:58
@_author: Arnold Reinhold 
@_subject: [Cryptography] Throwing dice for "random" numbers 
Here is the advice I give on my Diceware page:
...get a shoe box or a food storage box about the same size. Put [the] dice in the box, shake them up vigorously -- at least ten hard shakes -- and then tip the box to let all the dice slide down to one edge. Now open the box, read the dice from left to right, or front to back if a few line up. There will be occasional ambiguous situations, such as when a die ends up in a position were two faces are equally visible (a tap on the box will usually resolve this), but this method is fast and easy to use and eliminates almost all selection bias.
Arnold Reinhold

@_date: 2018-08-20 07:55:37
@_author: Arnold Reinhold 
@_subject: [Cryptography] Quantum hardness of key stretching 
I was recently asked how to pick passphrases for quantum computing resistance. The questioner noted that standard advice is to use twice the key length one would normally use pre-QC (based on Grover's algorithm). To get a passphrase with 256-bit security would require 50 random characters from {a-z, 0-9} or 20 Diceware words, well beyond most peoples ability to memorize. I replied suggesting the real solution is to only use security software that incorporates key stretching for passphrases. Algorithms that use a lot of memory as well as processing power, such as scrypt or argon2, should be much more resistant to quantum attack, though I have not seen a formal analysis. Assuming there isnt a time-memory trade off for a key stretcher that requires lots of memory, the number of coherent q-bits needed would seem to be at least the number of required memory bits, by design potentially millions or billions. That should be orders of magnitude more q-bits than needed for cracking RSA, say. That could make QC attacks infeasible or at the very least give plenty of warning, since practical QC attacks would have appeared for current public key systems long before attacks on large memory key stretchers would be possible. Am I missing something? Is argon2-level key stretching enough to allow passphrases of current strength to be secure in a post quantum world? Is there any literature on quantum hardness of key stretching? Arnold Reinhold

@_date: 2018-12-12 16:10:13
@_author: Arnold Reinhold 
@_subject: [Cryptography] Decrypting the Encryption Debate 
The stated reason for U.S. government opposition to public access to strong cryptography is to preserve the governments ability to gain access to criminal communications through wiretaps and computer data files seized as evidence. Such claims usually invoke a troika of evilsdrug dealers, terrorists, and child pornographers though decades of wiretapping have not halted those crimes.
In some ways, not much has changed, the same troika of evils are still invoked. But in other ways there have been vast changes. Here are some of the tools and resources now available to law enforcement that did not exist, and mostly were not even imagined, back in 1999:
o Ubiquitous use of cell phones, with GPS that track owners movements whenever they carry them o Ubiquitous use of text messaging and email, with metadata easily available even if text is encrypted o Numerous forms of insecurity in commercial hardware and software, enabling penetration of all but the most secure systems
o Password cracking tools that are capable of trillions of hashes per second
o Widespread social media accounts where users volunteer copious personal data
o Dramatic drop in cost of mass storage, enabling full take storage of metadata and intercepted content (about a factor of 1000, o Big data warehousing, holding every credit card purchase, other financial transactions and anything else they can get
o Widespread use of cloud storage, coupled with third party business records exclusion from Fourth Amendment guarantees U.S v. Miller, Smith v Maryland
o Widespread video surveillance, interconnected and linked with face recognition and AI filtering
o Inexpensive license plate readers
o Electronic transit fare collection that tracks riders
o Voice recognition and voice to text programs in multiple languages
o Quality automatic language translation
o Electronic medical records
o Passports with RFID
o Federal drivers license standards that are effectively creating a national ID card
o Automated DNA sequencing
o DNA databases large enough to zero in on a suspect through their relatives, even if the suspects DNA isnt in the database
o Millimeter radar looking through walls
o Indias attempt to create a cashless society and Chinas efforts towards total citizen control (1984 Rev 2.0), creating model for the future that others will soon clamor to employ
Im sure Ive missed a few. In short, law enforcement has never had it so good. Theyre hardly going dark.. Arnold Reinhold

@_date: 2018-02-05 13:20:49
@_author: Arnold Reinhold 
@_subject: [Cryptography] RISC-V branch predicting 
I was looking through the RISC-V User-Level ISA Specification (Volume I, V2.2  and found the following in the description of the conditional branch instructions (p.17 ff):
Software should be optimized such that the sequential code path is the most common path, with less-frequently taken code  paths  placed out of line.  Software should also assume that backward branches will be predicted taken and forward branches as not taken, at least the first time they are encountered.  Dynamic predictors should quickly learn any predictable branch behavior. and
We  considered but did not include static branch  hints in the instruction encoding. These can reduce the pressure on dynamic predictors, but require more instruction encoding space and software profiling for best results, and can result in poor performance if production runs do not match profiling runs.
Given the apparently unbounded Spectre security risks presented by current branch predictors, maybe it is time to reconsider the division of labor between clever CPU hardware and the software development tool chain. In most cases the programmer knows which way branches are likely to go, optimizing compilers can make good guesses and profiling tools should be very effective if given realistic data. I suspect that most execution time in software people use these days is spent in libraries that can be highly tuned. Is there any research that says the additional gain from sophisticated hardware branch prediction is significant in practice? Is it significant enough to justify the security problems? Given that the RISC-V community has not gone very far in deploying advanced risk prediction logic, maybe simple rubrics such as those suggested in the first paragraph quoted above, along with improvements in the software tool chain, such as maybe branch prediction pragmas and better integrated profiling tools, could result in good performance while avoiding Specte type problems.
Arnold Reinhold

@_date: 2018-02-07 18:54:27
@_author: Arnold Reinhold 
@_subject: [Cryptography] RISC-V branch predicting 
On Feb 6, 2018, at 6:53 PM, John Levine  replied to my post:
I have no doubt that people would rather let the hardware do the work if given the option. But if the goal is strong security, there may not be a choice.
As I understand it, test cases for correctness are hard because of the need to exercise all paths in the code to look for edge cases. Performance tests should be easier to acquire by sampling real world usage. Indeed I dont see how performance critical libraries can be developed in the first place without good test cases. Certainly problems like face recognition or speech to text or machine learning are driven by standard corpuses of test files. There is undoubtedly a Pareto distribution of libraries: only a fraction of them matter for noticeable performance, so libraries can be optimized over time, hopefully starting with the most impactful  ones. Crypto libraries in particular are (or should) be carefully analyzed, not just for optimal performance but to minimize the risk of side channel timing attacks.
I read the stackoverflow thread and, with all due respect, I think it supports what I am saying. The 6x performance delta example involves summing some elements out of a large array of random numbers based on a conditional test. The sum runs 6x faster if the array is sorted beforehand.  The explanation given is that with the sorted array, the sums conditional test branches more predictably.  Its a very simple example, but if anything the unsorted case is more representative of real world situations. The stackoverflow thread then goes on to discuss how several programming techniques, such as conditional moves and table lookup, can make the unsorted sum almost as fast as the sorted sum, and suggests several compliers already make such optimizations. I think this supports my notion that most of the benefits of highly complex branch prediction hardware can be realized by careful programming along with an improved tool chain.
I dont pretend to understand the many pathologies possible in C++ code generation, but I do not see why instrumented tests on sampled user data cant suss out most of them during program development.  I am by no means suggesting the elimination of speculative execution. Im proposing that the RISC-V community skip creating the elaborate hardware mechanisms for predicting which way branches go and push that burden upstream to programmers and the development tool chain. The Spectre paper suggests that there is no magic bullet to eliminate the problem of exploitable side effects from the complex branch prediction mechanisms in modern processors. And even if a brilliant solution emerges, it could well be patented and unavailable to open hardware. Given the risk of state actors building in vulnerabilities, open source software on predictable open hardware seems an essential platform for cryptography. Im just suggesting an alternative approach that lowers hardware development costs and makes program execution behavior much more predictable, while potentially getting most of the performance benefits. Arnold Reinhold

@_date: 2018-02-15 05:16:53
@_author: Arnold Reinhold 
@_subject: [Cryptography] Spectre again (was Re: RISC-V branch predicting) 
This gets back to the original topic of this thread. Why have a branch target buffer in the first place if most of the benefit of the hardware trying to determine likely branch direction can be accomplished in software development through the efforts of the programmer, compiler and profiler? Market pressures and, many suspect, state actors, are pushing commercial CPU designs to ever greater complexity that makes successful security analysis nearly impossible.The cleverest cryptographic code can be rendered useless if one cannot trust the hardware it runs on. Open hardware may be the last best hope of restoring a rigorous basis for cryptographic security. Traditionally an instruction set architecture was the sole contract between hardware and software. Any hardware that met an ISA spec would run any software generated for that spec. Evidently we need more for cryptography to work. Given the relatively early state of the RISC-V project, it would seem dialog between the crypto community and the RISC-V designers is appropriate and needed, to negotiate one or more hardware design profiles that would make demonstrable security possible again. Arnold Reinhold

@_date: 2018-01-23 16:24:36
@_author: Arnold Reinhold 
@_subject: [Cryptography] Speculation considered harmful? 
Most  discussions of the Spectre vulnerabilities have centered on how to keep speculative execution from revealing contents of kernel memory without paying a big cost in performance. It might be worth exploring other directions including safer storage of secrets now kept in kernel memory and adding hardware to detect such attacks.
An ideal solution might be storing secrets in special hardware that would not be subject to Spectre attacks, perhaps in a cryptographic coprocessor or even a memory module off the I2C or SPI bus. Short of that, it might be worth considering storage of the relatively small amount of secret kernel data in ways that are harder to recover via Spectre probing, perhaps by secret splitting or encryption. This could be done entirely in software. One might xor the secret data with one or more blocks of random bits, perhaps selected pseudorandomly from a larger array of random bits. if this storage arrangement is refreshed with new random bits periodically, at intervals less than the time needed to recover enough kernel bits via Spectre attacks, the likelihood of successful secret recovery might be significantly reduced. The kernel could refrain from ceding control to user processes durning the brief intervals when secret data is decoded for use. An attacker who knew all the details of the kernel program and its layout could concentrate on recovering just the bits needed to unwind the process, but some randomization of memory during boot could make this difficult in practice
An attack detection solution in hardware might use a counter that is incremented every time a speculative branch attempts access beyond memory bounds. A high enough count would trigger an interrupt, based on a programmable threshold, that could allow the offending process to be identified and suspended, or some other way of mitigating the attack, say by turning off speculative execution for that process.  Sending a bit to a counter at a bounds violation shouldnt slow down speculative execution at all, unlike other mitigations. There would have to be some allowance for occasional bound violations due to normal execution, say by measuring the interval between such interrupts, so a very slow attack could still be possible,. But combined with dynamic secret splitting in software, as above, a detection scheme might render recovering secrets infeasible. And a speculative execution memory violation counter could be implemented fairly easily in most architectures. Arnold Reinhold

@_date: 2018-05-02 13:03:35
@_author: Arnold Reinhold 
@_subject: [Cryptography] Data erasure by erasure of a salt 
If I understand you correctly, you want to insure that anyone with all the details of how a data element was encrypted, except for the salt, will be unable to read the data element. So in effect your are creating a derived symmetric encryption algorithm with the salt as the key. The general take on symmetric encryption and quantum computers is that an encryption algorithm with work factor N on conventional computers would take sqrt(N) on an ideal quantum computer (Grover). See e.g. NSA guidance suggests using 256-bit keys for quantum resistance. It may be that the totality of what you are doing will be harder to implement on a quantum computer than, say, AES, but given how speculative QC is, I cant imagine anyone giving you reliable assurance that 128-bits is enough if you want long term security at the highest levels. Adding 16 bytes per file does not seem a high price to protect against an uncertain future.
Arnold Reinhold

@_date: 2018-11-02 12:29:42
@_author: Arnold Reinhold 
@_subject: [Cryptography] Massive CIA communications compromise starting in 
This article describes a massive CIA communication system failure from 2008 to 2013, with continuing issues. It seems the CIA used a series of phony web sites to communicate with agents in hostile countries.  The Iranians apparently found some by tracking down moles based on who knew about information that had leaked, e.g. their underground enrichment facility. They then analyzed the sites they knew about and developed signatures that could be used to successfully find similar sites using Google searches. They likely shared the information with other countries including China. Large portions of CIA networks in many countries were compromised and dozens of sources executed. Some sources were likely turned, creating ongoing problems as to who is still trustworthy. An interesting quote form the article:
'Within some corners of the intelligence world, there was widely held belief that technology was the solution to all communications problems, according to one of the former officials. Proponents of older methods  such as chalk marks, burst communications, brush passes and one-time pads  were seen as troglodytes, said this official.
A defense contractor, John Reidy, detected and reported problems in 2008 but was then reassigned and later fired. Apparently no one has been held accountable.
Arnold reinhold

@_date: 2018-11-04 00:05:24
@_author: Arnold Reinhold 
@_subject: [Cryptography] Massive CIA communications compromise starting 
Well, that is indeed the question. Its not uncommon in bureaucracies, where it is much easier to fire someone than it is to admit you are completely screwed. One of my favorite books, Leo Marks "Between Silk and Cyanide," deals with a similar situation in WW II, though the author manages not to get fired by knowing when to shut up. Many OSS agents sent into the Netherlands were captured and executed because managers refused to believe their network was compromised even though prearranged signals, in the form of specific message mistakes, were detected by Marks.
We spend a lot of time worrying about how many bits to use for this and that, important stuff to be sure, but nowhere near enough time worrying about the dangers of security monocultures and the need for effective monitoring, with backup plans that are usable and get used promptly. Arnold Reinhold

@_date: 2018-11-29 16:23:36
@_author: Arnold Reinhold 
@_subject: [Cryptography] Buffer Overflows & Spectre 
We had a lengthy discussion on this list in April 2014 regaring assert statement removal,   , which is one example where "produce a negative result is what the programmer sanely wants. They are trying to enforce dont do that. Yes there are other ways to do that, but how much performance would be lost if compilers never removed an assert? Its not necessarily a question of disallowing some optimizations. Its an engineering management/configuration control problem as much as a compiler issue. There seems to be compiler options for GCC that avoid the more egregious behaviors.  The problem is who selects those options and when, and how does a medium to large project control that? If a software module that has been designed and tested under one set of optimization options is later incorporated into a system that is recompiled with different options, how does QA or security management know? One solution might be a way for an object module to limit what optimizations are used or a library method that returns at run time the compiler options used to compile it. If the limits on optimizations could be incorporated in the modules source code, then at least any changes in optimization level would receive the same attention and review as a change in the program logic itself. Do such facilities exist in any of the major tool chains?
The suggestion to use a different language ignores the enormous amount of C an C++ code already in use. Its economically infeasible to rewrite it all. If the compiler writer and computer security communities could have a reasonable dialog on this, continued use of C and C++ code, including code written and maintained by less than top-notch programmers, could be made significantly safer. Arnold Reinhold

@_date: 2019-04-29 14:09:06
@_author: Arnold Reinhold 
@_subject: [Cryptography] An observation on the Japanese PURPLE machine 
The breaking of the Japanese PURPLE cipher in 1940 was a legendary moment in U.S. cryptographic history. A complex cipher machine was completely reconstructed based solely on the cryptanalytic analysis and ingenuity of a team led by William Friedman, without any information whatsoever about the design of the machine under attack, other than intercepted ciphertext. In particular, Leo Rosen deduced that the machine was implemented using 25-pole stepping switches, commonly used in telephone switching, unlike the rotor machines like Enigma and SIGABA that were in vogue at the time. After some brilliant work by the team and a breakthrough by Genevieve Grotjan, they were able to build an equivalent machine using 13 six-layer stepping switches, one for the sixes cipher and four each for the three stages of the twenties cipher. (The PURPLE machine followed the design of the earlier RED cipher machine, both based on the English alphabet, of having the ability to encode 6 vowels and 20 consonants separately. This was  apparently to save money on telegram charges. Pronounceable words were charged less and foreign ministry budgets are always tight.)
At the end of the war, all the Japanese machines were throughly destroyed, with the exception of a few fragments found at the Japanese Embassy in Berlin. Lo and behold, the largest fragment contained three stepping switches.  Many sources report, in amazement, that the stepping switches in the captured fragment are exactly identical to the ones selected by Rosen.
Except they arent. The Japanese stepping switches have seven layers, not six. The big fragment is on display at the NSAs National Cryptologic Museum in Baltimore and there are several high resolution photos of the device available on Wikimedia Commons in the category PURPLE cipher machine. Here is a good one:
                                           If you look closely at the middle unit you will see the the stepping switch in the fragment have seven sets of contacts. This can be verified in other photos as well. Be sure to select the original image on the Commons page for full resolution.
Seven layers makes complete sense and helps explain what the fragment is. To implement the twenties cipher one needs effectively a twenty layer switch at each stage to select a permutation of the 20 inputs. Three synchronized 7-layer switches provide 21 layers, one more than was needed to implement a Purple cipher stage. The extra layer may have been used to control stepping of the stages, the order of which was a cryptovariable in the keying system. Rosen used four 6-layer switches that he found readily available, but there is nothing special about 6 layers. The switches themselves are modular in design and the manufacturers could easily make them with whatever number of layers a customer wants, within reason. The U.S. captured a Japanese JADE cipher machine during the war. Its on display at NCM as well and here is a good photo:
It contains five blocks of four stepping switches, each with seven layers. Thats 28 layers per stage , enough for the 25 character alphabet the machine used (50 katakana characters with a shift function). Interestingly, there is one 2-layer switch just to the right of the plugboard, perhaps for stepping.  (My guess for why there are five blocks of steppers is to allow both encryption and decryption with only one typewriter. To accomplish this without extremely elaborate switching, they could have added blocks with the inverses of the first and last stage permutations. The middle stage could have been wired to work both ways.)
The mechanical design of the JADE four-switch module matches the Purple fragment. This explains the two bushing mounts on the PURPLE fragment: they would have held a shaft with gears that engaged the ratchet gears on the steppers, keeping the switches synchronized. The shaft would have also held a wheel with the stepper positions marked on its circumference to allow setting the initial position of each stage at the start of a message, another part of the message key. The moving contact wiper on semicircular stepping switches has two arms so that when one arm leaves the 25th contact the other engages the first contact. Thus a 2 to 1 gear ratio provides one revolution per sweep of the 25 contacts. None of this diminishes the brilliance of what the SIS team accomplished, but it does shed light on the fragments function. Based on the numbers stamped on the switches, I would guess it is the middle stage, but similar numbers on the Jade machine make no sense to me. An ohmmeter or telephone signal tracer could verify that the fragments wiring matched one of the Purple permutation sets. I cant believe Im the first person to notice that the Japanese PURPLE stepping switches have seven layers, not six. Does anyone know another reference?
Arnold Reinhold

@_date: 2019-04-30 23:57:03
@_author: Arnold Reinhold 
@_subject: [Cryptography] Review of "Code Girls" by Liza Mundy 
============================== START ==============================
The Final Countdown was a 1980 alternate-history film about a modern nuclear aircraft carrier that travels through time to the day before the 1941 Japanese attack on Pearl Harbor. Filmed on board the USS Nimitz, The Final Countdown was a moderate success at the box office. (Wikipedia)
So here is a concept for another World War II fantasy film: the U.S. War Department, short on manpower, staffs an entire battleship with women. The clever females on this Top Secret mission track down and sink the majority of Japans transport ships. Their success enables General Douglas McArthur to conquer island after island defended by poorly equipped and weak-from-hunger Japanese troops. For an added kicker, the women recover Japanese plans for defending their home islands, influencing President Truman to use the atomic bomb to end the war without the planned bloody invasion of Kysh. After the war, most of the women, sworn to secrecy on pain of execution, return to traditional roles and take their stories to the grave. This plot may seem even more far-fetched than The Final Countdown, except that it is very close to what really happened. The women werent on a battleship, they were in a commandeered girls school in Arlington, Virginia, just across the Potomac from Washington D.C. And they werent firing 16-inch guns at the transport ships, they were breaking Japanese Army codes and unscrambling position reports transmitted regularly by the ships. Those decoded reports kept the U.S. submarine fleet busy sinking two-thirds of the Japanese merchant marine by the end of the war. But for those women, the fanatic and well dug in Japanese soldiers might have held on much longer, or even thrown back some of McArthurs assaults.
All this is recounted in Liza Mundys Code Girls, (Hachette Books, 2017, ISBN|978-0-316-35253-6) an account of the thousands of women recruited for U.S. cryptologic work before and during World War II, including top analysts such as Elizebeth Friedman and Agnes Driscoll, lesser known but outstanding contributors like Genevieve Grotjan Feinstein and Ann Zeilinger Caracristi, and many others. Mundy has a chapter titled Pencil-Pushing Mamas Sink the Shipping of Japan. Its a tad misleading. Most of the women were single and any discovered pregnancy would get you fired. But other than that, the title is accurate. By 1944, the job of breaking Japanese Army codes, especially the 2468 code used for transport ships, was woman staffed and woman led. The results were devastating. Black humor in the Japanese military was that one could walk from Singapore to Tokyo on the periscopes of U.S. submarines. In fact there were not enough subs to exploit all the position reports the women decrypted. The womens efforts left Japanese Army troops without reinforcements, supplies, medicine and food. More Japanese soldiers died of starvation and illness that were killed by bullets and bombs.
There is a scene in another movie, 2014s The Imitation Game, where Joanne Clarke shows up late at a qualifying examination for cryptanalysts and is told the openings are not for women and that the secretarial exams are on a different floor, but Alan Turing tells her to sit down and she bests the men. The reality, at least in the U.S., was that smart women were very much wanted from the beginning.
In 1939 young Genevieve Grotjan (I am posting this on her 106th birthday) took a math test for an unrelated civil service job and did very well. William Freedman, our top cryptanalyst, apparently had feelers out for such things, and hired her. Perhaps the most dramatic moment in the book takes place at 2 pm, September 20, 1940 with Grotjan patiently trying to get the the attention of Frank Rowlett, who was engrossed in conversation with male colleagues. She has found evidence of cyclicality in the Japanese cipher the U.S. codenamed Purple. Cheers erupt, toasts of Coca Cola are poured and Friedman, under severe pressure to break Purple, slumps against a table. A decryption machine for Japans main diplomatic cipher is constructed a couple of weeks later. It wasnt a solo effort on Grotjans part of course, other women and men had helped assemble several long enciphered texts with the same indicator and laboriously transformed them to matching plugboard settings. By November 1941, a month before Pearl Harbor, the U.S. Navy was asking top womens colleges for names of their best seniors who were then sent letters inviting them to secret meetings. There they were asked if they were planning marriage (a disqualification) and if they liked crossword puzzles. The women who passed were invited to take correspondence courses in cryptography. Those that did well were offered jobs in Washington.  The Army soon followed, but had to find their own schools, a no poaching agreement prevailed, and soon focused on teachers colleges. Once the Army and Navy began accepting women, the smartest woman enlistees were assigned to code breaking.
Code Girls is not just another story of women playing their part in the war; we all knew about Rosie the Riveter. This is different. The decision to recruit large numbers of smart women to the code breaking effort was a strategic move that could rank with Einsteins letter to Roosevelt and the Tizard mission that brought the cavity magnetron to the U.S. as decisive in WW II.
Mundy points (p. 80) out that Germany and Japan were more traditional societies and discouraged women from war work. The Nazis emphasized the traditional German Kinder, Kche, Kirche dogma that women should concern themselves with domestic duties and even created a Cross of Honor of the German Mother medal for pure German women with four or more children. Their failure to utilize their female intellectual talent put them at a major disadvantage, thank goodness.
This book is written for a broader audience than cryptography geeks. There is plenty of material about the womens personal lives: finding living accommodations in overcrowded war-time Washington, corresponding with soldiers overseas (a major pastime apparently), dating and finding husbands. One of my favorite bits is Mundys detailed description of the spiffy new Navy WAVE uniforms, filled with fashion jargon as impenetrable to me as cryptographic terminology must be to lay people.
But there is plenty of interest for the geek crowd, Mundy does an excellent job explaining concepts like depth, modular addition and checksums, and paints a convincing picture of the day-to-day operation of the code breaking effort. Even after a code is cracked there is still a lot of work to do, much of it rote, but much creative too, as Japan kept changing and improving its encryption.
College graduates in 1942 would be in their late 90s today and only a few were still alive for Ms. Mundys research. It sad that it took so long for these womans stories to be told, the womens strict obedience to the war time secrecy oaths was a factor, but it's wonderful that it has finally been written.
Happy birthday Genevieve!
Arnold Reinhold

@_date: 2019-08-13 13:49:45
@_author: Arnold Reinhold 
@_subject: [Cryptography] generated passphrases 
Please see my Diceware(tm) page (diceware.com) which has word lists in 27 languages and a simple random generation scheme using dice. The English word list emphasizes short words and abbreviations to keep generated passphrases as short as is reasonably possible. The Diceware word lists have 7776 words (6^5) for easy dice selection.  So each word adds 12.9 bits of entropy (log2(7776)) and 10 words are enough to provide 128 bits of entropy. Whether that many is needed for acceptable security depends, in part, on how much key stretching is used by the program that accepts the passphrase.  In general, the practice of taking a passphrase, putting it through a fast hash, such as the SHA series, and using the hash output as a cryptographic key is a very bad idea. Only resource intensive hashes should be used. For  crypto currency several seconds of hashing should be acceptable to unlock wallets, but others can comment on what is actually done in the various currencies.
A Diceware passphrase will not be grammatically correct in general. Passphrases that are grammatically correct will have significantly lower entropy, the sum of the binary logarithms of the number of words for each part of speech. A "randomly generated grammatically correct twelve word nonsense phrase? is unlikely to come close to 128 bit entropy. However, for randomly generated passphrases it is often possible to invent a story or thought image that makes them easier to remember. See the famous xkcd.com/936. I also have made a table that generates a grammatically correct sentence for any random string of 10 English letters:      Two sentences worth (twenty random characters) provides 94 bits of entropy. A third shorter sentence with 7 letter can be added for 128 bit entropy. Just to be clear, the random letters are the password, the sentences are mnemonics for the user.  I do recommend writing passphrases down and keeping them somewhere safe.
Arnold Reinhold

@_date: 2019-08-14 14:00:44
@_author: Arnold Reinhold 
@_subject: [Cryptography] generated passphrases 
Yours is a reasonable design, but going from 11 words to 14 words just to get a passphrase that is roughly grammatically correct is a significant penalty.  It would be worth building such a generator and seeing what the generated passphrases look like. The passwords in my scheme are completely random strings of English letters. You can?t get higher density than that with the same alphabet. The sentences I generate are just mnemonics, the passwords are comprised of the initial letter of each word. The sentences need never, indeed should never, be entered into the keyboard. As long as the entropies are correctly calculated and are about the same, passphrase format issue is largely a matter of taste.  I could see a system that generates passphrases offering several equivalent versions in different format. Of course this assumes the system accepting the passphrases accepts long enough strings. And if threat models include acoustic or video spying, then passphrases are less desirable than random letters since the former have significant redundancy that can aid those attacks.
But the reality is that almost no one is willing and able to memorize a 128-bit passphrase, indeed few will memorize an 80-bit passphrase. Any crypto currency or other security scheme that depends on people memorizing and entering very long passphrases is not going to gain wide consumer acceptance. That leads to the necessity of using effective key stretching. The only widely accepted standard is PBKDF2, which uses a single processor and very little memory. (i proposed memory intensive hashing way back in 1999.  There was a community-sponsored Password Hashing Competition in 2015 that selected Argon2, but it seems to get very little use. NIST has not adopted it, perhaps because it uses the Blake2b hash, which is not NIST-approved. NIST SP800-63B briefly mentions the Balloon hash approvingly, but that is not the same as recognizing it as a standard. If other standards bodies have a better key stretching hash standardized or in the works I?d like to hear about it.  There is also a need for standards on how key stretching is used and how much is needed for various cases. And I believe there is a need for a key stretcher that intensively uses modern graphics processors. Let?s use all the horse power a user has to protect their secrets.
Key stretching addresses a critical mismatch in security technology, how much entropy a typical user can be expected to remember and enter accurately, vs how much entropy is needed for system security. So here is a big question: why doesn?t key stretching get more respect?
Arnold Reinhold

@_date: 2019-08-16 18:18:42
@_author: Arnold Reinhold 
@_subject: [Cryptography] generated passphrases 
Fort wide adoption of technologies like cyber currencies, I submit there is no choice but to depend on key stretching. The size of secrets needed are too big for most people to memorize. (People on this list, motivated crypto nerds, are hardly typical users.) There are key stretching hash algorithms with proofs of memory hardness (e.g. Balloon, Argon2) that can increase required processing time and silicon area each by a factor of at least 100,000 on ordinary portable devices with negligible battery draw. Combined, that's a total gain of about 35 bits over SHA256(passphrase).  Using the GPUs on these devices might gain another 8 bits or so. That is a big reduction in the the amount of entropy a user needs to memorize and enter to achieve high security. Arnold Reinhold

@_date: 2019-12-30 15:51:56
@_author: Arnold Reinhold 
@_subject: [Cryptography] how to detect breakage -- lures etc.?? 
What is your organization going to do if you do detect breakage? In ?Between Silk and Cyanide,? Leo Marks describes teaching OSS radio operators to make certain mistakes in Morse code transmissions if the operator were compromised. After he noticed those mistakes being made by agents parachuted into Holland, his management refused to believe him and many more agents were sent to that country, only to be caught and eventually executed by the Germans. There are many other stories of warnings about possible failures of communications security being ignored, Allied convoy codes were broken by the Germans, despite warnings, until an intercepted Enigma message made reference to the breakage, providing incontrovertible proof. US communications were exploited by the North Vietnamese, yet commanders refused to believe that was possible until a complete North Vietnamese COMINT unit was captured (Boak Lectures. There are neat examples of captured NVA intelligence reports in the National Cryptologic Museum.)
If your organization has an active security team, they might find time to investigate and patch specific weaknesses, but if the failure is system wide, is senior management  prepared to make massive and expensive changes? More broadly, don?t we already have enough evidence that most computer-based products have exploitable weakness that are not publicly known and could be used for coordinated attacks by hostile nations or large terror groups? How many organization have backup plans for dealing with a situation where all their computer-based systems are crashed or rendered untrustworthy? Arnold Reinhold
P.S. Minor historical note: Since you brought up Enigma security, I?ve often wondered why the Germans didn?t give their Enigma operators better guidance on picking random indicators. The U.S. ECM-2 manual suggests using ordinary playing cards as a random character source. I recently found out, while visiting the U-505 exhibit at the Chicago Museum of Science, that the by-far most popular German card game was Skat, which is played with a 32 card deck. That may have made the use of playing cards a less obvious possibility than with the 52 = 2*26 card decks used by the Allies.

@_date: 2019-02-19 17:16:21
@_author: Arnold Reinhold 
@_subject: [Cryptography] Questions of taste on UDF presentation 
This seems to me to be part of a broader issue: as computation power increases, we need longer cryptographic primitives and as a result are stressing the ability of most people to inspect, memorize or type correctly. I like the mixed length scheme as it seems more likely to make small variations stand out. Id even go for a more dramatic variation that mimics the word length variation in natural language sentences. For example 3-5-4-3-5-3-5:
My instincts say the greater variation would aid users in comparing the individual elements. The best thing, of course, would be to do some actual testing of various schemes. Perhaps some local university might take it on as a student project. I wouldnt worry about coding difficulty.The needs of the general public far out weigh programmer issues. Providing sample code and some test cases should minimize that problem.  Given the ongoing noise about quantum cryptanalysis, Id go with the longer version (isnt it 140 bits?). I would also use a resource intensive hash if possible, at least PBKDF2, preferably Argon2 or balloon. This would make it much more difficult for an attacker to forge a fingerprint that is even close.
Id just mention that one of the better recent Super Bowl ads was for a company called Expensify that is addressing the expense report market.
Arnold Reinhold

@_date: 2019-02-24 12:25:29
@_author: Arnold Reinhold 
@_subject: [Cryptography] Best way to create a MAC from SHA3 
NIST SP 800-185 defines a set of MAC functions (KMAC) derived from SHA3. Arnold Reinhold

@_date: 2019-01-06 21:51:30
@_author: Arnold Reinhold 
@_subject: [Cryptography] Came up with a weird use case, got questions 
I remember a similar request being discussed many years ago, after some politician or the like was required to turn over his private diary to some legal proceeding. The idea was to come up with a public key that could be used to encrypt a diary, where the corresponding private key would only be released some time well in the future. One suggested approach was to generate keys for, say, ten-year intervals and secure the private keys with an m out of n secret sharing system. The key shards would then be entrusted to a dozen or so reputable agencies, such as university libraries, in different legal jurisdictions, who would agree to hold the keys in secret until the appointed times. There were also discussions of ceremonies to create the keys in a way that would be trusted. I remember the suggestion being made that any such ceremony protocol be reviewed be a good professional magician before being declared foolproof.
Another approach suggested, if I remember right, was to store the private keys in satellites placed in orbits that only return to earth infrequently.  With several  large satellite constellations in development or being proposed, all with much more computer capability than was common decades ago, adding a delayed key service might be quite feasible.
Arnold Reinhold

@_date: 2019-03-19 12:44:16
@_author: Arnold Reinhold 
@_subject: [Cryptography] BBC on crypto-currencies in Venezuela 
The BBC is running an interesting story on crypto-currencies in Venezuela:
What ever else one might say about Bitcoin, it does seem to be performing a useful role there, with trading volumes in February of $8.76M per week. One interesting observation from the article is that usage went down when there was a widespread power failure. This suggest a narrow niche where society has broken down to the point where regular money is useless, but basic utilities such as power and Internet connectivity are still functioning. I wonder how Venezuelans are paying for these utilities. People who are fleeing intolerable conditions will try anything to get some money out. When my father left Nazi Germany in 1937, he was only allowed to take out a small amount of money (200 DM iirc) and two suitcases. To maximize the value of the stuff he could bring with him, he purchased a large supply of razorblades which he then used for many years after. Arnold Reinhold

@_date: 2019-03-22 12:59:52
@_author: Arnold Reinhold 
@_subject: [Cryptography] Clinton email issues 
There are a number of security issues related to the Hillary Clinton email story that seem to have gotten lost. The Secretary of States job involves sensitive negotiations where complete confidentiality is essential. The unclassified State Department email system was widely reported as having been hacked by the Chinese, to the point where it took a long time to purge the system. The classified email system was, of course, run by the NSA, which is part of the Department of Defense. Rivalries between DOD and State are legion and there was no reason for Secretary Clinton to trust that her email would be kept confidential from the Secretary of Defense, not to mention dozens of system administrators and other civil servants. She was reportedly advised by outgoing Secretary of State Colin Powell to get a private email account. Tens of thousands of her email messages were reviewed by the Justice Department, which determined that a small number contained information that the Justice Department believed to be classified. As far as I know, no similar review of the entire corpus of email messages sent over unclassified systems by any other major government official has ever been made or, at least, revealed. Whether her actions in deleting what she considered her private email was justified or not, it was evidently done to high security standards. None have been recovered from her servers and if they were intercepted by Russia, they have not been leaked, despite a public request from President Trump to do so.
For what its worth, both Time and Newsweek reported that for her 50th birthday, Secretary Clinton was given a copy of Internet E-Mail For Dummies so she could keep in touch with her daughter, who was going off to college. I was co-author of that book, along with Carol Baroudi, as part of a writing group led by John Levine. I wrote the chapter on email security, basically describing PGP. Running ones own email server was not discussed.  So basically everything that has transpired since is my fault.
Arnold Reinhold

@_date: 2019-05-15 16:00:56
@_author: Arnold Reinhold 
@_subject: [Cryptography]  Computing Artifacts 
Christies story that the initial Apple Is were built in the kitchen and garage does not match the account I heard from Steve Wozniak at a talk at the Computer Museum many years ago when it was still in Boston. The Woz story is more interesting and has some relevance to cryptography since it involves complex trust management. According to Woz, the Apple I boards were to be professionally made and assembled. The problem was that the Byte Shop did not want to buy all the 50 boards they ordered at once, but would only take them and pay in smaller batches. Steve Jobs found a vendor who would build them in batches, but the parts supplier would only kit the full order. The parts supplier would not give the assembly shop all the parts without paymentif the assembly shop went under, the parts supplier would not be able to recover the parts. Jobs had no credit and didn't have enough money to to buy all the parts, and the assembly shop would not advance the cash. So Jobs came up with a very clever solution. There was a closet at the assembly shop that got designated as temporarily property of the parts supplier and all the parts for 50 boards were stored there under lock, except for the initial batch. When done, Jobs took the first batch of completed boards to the computer store, got a check in payment, brought it back to the assembly shop, who got paid for their work and paid the parts supplier who then released the next batch of parts. This process repeated until it was clear that the Apple kids had a going business.  There were four parties to this transaction, Apple, the Byte Shop, the assembly shop and the parts supplier, with little trust between any of them.   By figuring out how to chop the risk into pieces small enough that trust became possible, Jobs was able to start a $1T business with almost no capital. By the time Apple finally did need venture capital they had a thriving business and could command very favorable terms, giving up only 25% ownership, if I remember right. Perhaps this is the sort of thing blockchain advocates are dreaming about.
I also recommend reading Ada Lovelaces translation and notes on the first item, a journal article, being sold. Its amazing how much she foresaw about computing back then.
Arnold Reinhold

@_date: 2019-11-08 04:54:44
@_author: Arnold Reinhold 
@_subject: [Cryptography] Very best practice for RSA key generation 
Diceware has been around since 1995, long before xkcd 936, and there are currently word lists in 28 languages* linked from diceware.com. The lists are each 7776 words long, 6^5, for easy random selection using ordinary dice. That works out to 12.9 bit of entropy per word. There is a point of diminishing returns on word list size. A 32K world list (15 bits/word) will have longer words on average and the entropy per entered character is likely lower. I also argue that passwords with random capitalization should be judged on entropy per keystroke, counting the shift key presses as separate keystrokes, not entropy per character. If you evaluate that way, adding a few random characters to a single case password turns out to be better than fully random capitalization, and almost certainly easier to remember and type. I have not seen any formal usability studies, but we all have lots of passwords we use. Try changing to a random word passphrase on one account and a random character password of equal strength on another and see which one you find works better. If it were up to me, account creation/change programs would offer randomly generated passwords of equal strength in a few different formats, and let the users pick whichever they prefer.
Arnold Reinhold
* Basque, Bulgarian, Catalan, Chinese, Czech, Danish, Dutch, English, Esperanto, Estonian, Finnish, French, German, Hungarian, Italian, Japanese, Latin, Maori, Norwegian, Polish, Portuguese, Romanian, Russian, Slovak, Slovenian, Spanish, Swedish and Turkish

@_date: 2019-10-25 09:46:20
@_author: Arnold Reinhold 
@_subject: [Cryptography] Jim Baker explains encryption to us 
Hi John, I think you need to read that blog more carefully. Mr. Baker is NOT reiterating his past positions on the encryption debate; he is coming darn close to changing sides. He now says that cybersecurity concerns are extremely grave and should be given higher priority and and that strong encryption is needed for that battle. The quote you cited is immediately followed by a but?
"But a solution that focuses solely on law enforcement?s concerns will have profound negative implications for the nation across many dimensions. I am unaware of a technical solution that will effectively and simultaneously reconcile all of the societal interests at stake in the encryption debate, such as public safety, cybersecurity and privacy as well as simultaneously fostering innovation and the economic competitiveness of American companies in a global marketplace.?
The article spends a lot of time explaining the threat from China and that the inevitable market penetration by Huawei switching equipment means we will have to live with ?zero-trust networks? and that strong encryption is essential to counter that threat. He also suggests that law enforcement concentrate more on exploiting metadata. He concludes:
"All that said, I still found it painful to write this piece, especially since I worked for so many years in the Justice Department and the FBI on the going dark problem without ever finding a viable solution. I have no choice but to admit that I failed in that regard.
But we all need to deal with reality. And in my experience, that?s what the people who have dedicated their lives to protecting all of us?such as the employees of the FBI?usually do best. How else do you stop the bad guys but by living in reality and aggressively taking the fight to them based on an accurate assessment of the facts? I am most certainly not advocating surrender, but public safety officials need to take a different approach to encryption as a way to more effectively thwart our adversaries, protect the American people and uphold the Constitution in light of the existential cybersecurity threat that society faces. If law enforcement doesn?t want to embrace encryption as I have suggested here, then it needs to find other ways to protect the nation from existential cyber threats because, so far, it has failed to do so effectively."
This is an important reconsideration of strong encryption by a previous opponent.
Arnold Reinhold

@_date: 2019-09-18 16:34:04
@_author: Arnold Reinhold 
@_subject: [Cryptography] TRNGs as open source design semiconductors 
An attack that weakens the RNG has unique advantages to the attacker. Once in place it is almost impossible to detect. It requires no back channel to reveal sensitive information. It is immune to software updates, as long as they do not change the source of random numbers. It?s a very economical attack. It potentially opens all of the victims encryption to the attacker. A gift that keeps on giving. Here is a link that discusses current US DoD efforts to address the supply chain problem:
 Arnold Reinhold

@_date: 2020-04-27 21:24:40
@_author: Arnold Reinhold 
@_subject: [Cryptography] Cryptography of vertebrate virus defense 
Back last century, I wrote a short paper analyzing one of the body?s defense mechanisms against viruses from a cryptographic standpoint. If you've ever wondered why humans reject organ transplants from other humans, something never encountered in nature, it?s because each of our cells has a mechanism that authenticates it to our immune system. The cells do this by sampling protein fragments that are created as the cell recycles old proteins, and presenting the selected fragments on the outside of the cell for inspection by immune cells. Only a small fraction of possible fragments are chosen, and the determination of which fragments is based on genes that are highly variable from individual to individual, effectively serving as a password. If a cell becomes infected by a virus, the virus protein fragments it presents can trigger an attack on it by immune system killer cells. (See  for background.)
However the immune system needs to be trained early in life to avoid recognizing normal cell proteins as foreign. The body has limited ability to perform this training and failures can result in auto immune diseases. My paper analyzed this trade off between fragment selection fraction, effectively password entropy, and the risk of training failures. Here is a link for anyone interested:
    Arnold Reinhold

@_date: 2020-08-07 15:06:17
@_author: Arnold Reinhold 
@_subject: [Cryptography] Terakey, 
Principles
What you are proposing are active attacks. The security model I used in my paper[1] for what I claim is a first-principles proof of confidentially is based on a passive attack. The attacker know all aspects of the basic Terakey system, including all past traffic, except the Terakey itself.  In particular, that means he cannot generate Terakey encrypted messages. I separately consider confidentiality for groups of key holders, but I do not claim a first-principles proof. An obvious problem with security among key holders is that they have some level of access to the Terakey. I propose using a processor interposed between the Terakey storage module and the rest of the system that monitors and limits the rate at which key material is extracted from the Terakey, so as to prevent users from recovering large portions the Terakey by generating large quantities of messages. Your suggestion of messages tailored to extract specific bytes would not be caught by this mechanism. A possible solution would be incorporating a secret as part of the seed that is not known to all users. I propose something like that to minimize damage if a Terakey is stolen, but you have provided another good reason for doing so. The proof of security is simply that if a byte in the randomly generated Terakey is only used once, the resulting encryption is secure, as with an OTP. If it is used more than once, then security of the resulting encryption using that byte is not guaranteed  I am only using the uniform random sampling properties of the PRNG to calculate the probability that a given message will contain bytes encrypted by re-used bytes and, if so, how many. By keeping the size of the Terakey large compared to the volume of traffic that probability can be kept arbitrarily low. I?m not making any security assumptions about the selection process, only that the outputs, and hence selected key bytes, are distributed close to uniformly. There is a vast literature on the design of PRNGs that have this property on theoretical grounds, plus extensive efforts to verify those properties experimentally with a range of tests. Here is a 2006 paper that describes one family of such PRNGs with strong uniformity properties and very large period, with a review of the literature.
  Panneton, et al,  "Improved long-period generators based on linear recurrences modulo 2, ACM Transactions on Mathematical Software. 32 (1): 1?16
There are many others. Can you clarify what your objections are?
Arnold Reinhold
[1]

@_date: 2020-08-28 18:30:16
@_author: Arnold Reinhold 
@_subject: [Cryptography] Terakey, 
Principles
The security model for Terakey distinguishes between people who have access to the current Terakey and those who do not. This should not be surprising since Terakey is basically a shared secret system, and thus intended for relatively small networks. In my paper[1], I suggest methods of providing privacy between Terakey users sharing a key, but these methods depend, in part, on physical measures such as safes for storing the Terakey and special processors for metering access to the Terakey. I don?t claim a proof.
The first-principle security proof I do claim is for confidentiality from people who do not have access to the current Terakey. In particular they would not be able to mount the active attacks we have been discussing.  The security proof does not guarantee that there will never be data compromise, only that any potential compromise under the proof can be made rare by keeping the Terakey large compared to the volume of traffic. I then propose a variety of ways to secure the data that is not covered by the proof by using conventional cryptography. Crudely speaking, under reasonable usage parameters 99.9% of data bytes are provably secured by first principles, the remaining 0.1% are protected conventionally. Some would argue that even 0.1% of data not being covered by the proof is ?broken," but I would respond compared to what?  Conventional symmetric and asymmetric ciphers have no first-principle security proof at all. The right question, I believe, is whether Terakey could be useful. Right now a large part of the world?s economy is secured by mathematical conjecture, a single point of failure. I would think an independent alternative could be desirable, at least as a backup.  One alternative that is being actively pursued is quantum key distribution.  In my paper, I compare QKD with Terakey used exclusively for symmetric key distribution and attempt to show that Terakey offers comparable security with less complexity, easier audit, and without the restrictions on communication channels QKD demands. I appreciate your thoughts on Terakey, but at this point we don?t seem to disagree so much about what Terakey does, but rather whether what it does has any value. I?ll leave that for others to decide.
Arnold Reinhold
[1]

@_date: 2020-08-31 16:12:24
@_author: Arnold Reinhold 
@_subject: [Cryptography] Terakey, 
============================== START ==============================
 Principles
What you are describing is a variant of standard attacks on stream ciphers (which Terakey is). And yes, they are well known in the literature and there are standard defenses that have been known since before World War II. I describe them in my paper[1] as an essential part of Terakey, as follows:
"Terakey consists of three major elements: ...
These measures can prevent an attacker from getting "someone who does know the terakey to encrypt a known message with his chosen key.?  In particular, including an automatically generated random nonce, which is standard practice with modern stream ciphers, completely foils such an attack.
If you want to posit an insider with the necessary permissions to modify the Terakey software to inject a chosen PRNG seed, they can just as well read out the Terakey directly. No encryption system can survive an attack by someone who can modify the encryption software at will.
Arnold Reinhold
[1]

@_date: 2020-01-03 15:21:28
@_author: Arnold Reinhold 
@_subject: [Cryptography] how to detect breakage -- lures etc.?? 
You may be right about a "layer of modest super-encryption? rendering Enigma completely secure, but I would like to see a more thorough analysis. Desperation might have driven the allies to make more of an effort to steal machines and give more attention to all electronic solutions rather than electromechanical ones. Necessity is the mother of invention.
We know what William Friedman, who rejected Enigma for U.S. use, thought was needed in a rotor machine. ( The SIGABA/ ECM-II sets, which by all accounts was never broken during the war, included: * five rotors in the alphabet maze, chosen from a set of ten
* pseudo random stepping of the five rotors, at first controlled by an additional set of rotors
* no reflector, eliminating the ?no letter can encode to itself? weakness, at the expense of a bulky 26+ position encrypt/decrypt switch Also, using a cipher disk for super encryption would have doubled the labor needed to encrypt and decrypt messages and probably would have increased the number of garbles and request for repeats that can be gold for the cryptanalyst. To me the more interesting question is what would have been the impact on allied Enigma breaking if the German had adopted the same basic security steps that the U.S. employed with SIGABA:
* guidance for operators in selecting random message keys. (Dice would work, of course, but playing cards seem more practical. Just shuffle and deal three cards. The reduction in key space for a Skat deck, 26*25*24 vs 26^3, would not be significant.)
* message padding to eliminate easy cribs at the beginning of messages (The German Navy did take some measures to reduce cribs, but not enough)
* use of separate daily keys and indicators for messages at different security levels
Another simple fix would be to supply users more rotors to choose from.  Two more rotors would increase difficulty by a factor of 56; three more by a factor of 504.
Note that the German Navy did use a more complex indicator system, and had a special procedure for ?Offizier?s" messages that double encrypted with Enigma. Both created considerable difficulty, but were broken to varying degrees. (See Budiansky?s Battle of Wits, esp. Appendix B.)
What the U.S. lost by its more complex machine was portability. The German Enigma was modest in size and weight compared to the radios it was used with, and it was mostly mechanical, only requiring a battery and using less power than a flashlight. One simple answer is size, cost and complexity. Each bombe was massive, weighting a ton and containing some ten miles of wiring, While a bombe had the equivalent of 36 Enigma machines, dividing those numbers by 36 still produces a bulky device. And there were many more moving electrical contacts involved, which reduce reliability. The bombe were notoriously finicky, even though there were operated in fixed indoor facilities. Stacked rotor machines with enough rotors and complex stepping were reasonable secure (unless the key lists were stolen as happened with the Walker spy ring) and were in use by the U.S. through the 1970s (e.g. NSA?s KL-7 and Soviet Fialka). Arnold Reinhold

@_date: 2020-01-05 11:48:10
@_author: Arnold Reinhold 
@_subject: [Cryptography] how to detect breakage -- lures etc.?? 
We all admire how the Poles and later the British were able to recover rotor wiring cryptographically.  The Soviets apparently had a different approach. The Cryptomuseum page on the U.S. KL-7,  , a successor to SIGABA, has a section on the Soviet ?Rotor Reader,? with a photo and diagram.  This was a small, folding device, pocket sized, with 36 contacts and lights designed to quickly scan and readout the wiring of a KL-7 rotor. It was recovered from John Anthony Walker when he was arrested in 1985. He had been given it when he started spying in 1967. An identical device was recovered from U.S. Army Sergeant Joseph Helmich in the mid 70s. The devices are well designed, obviously not one-offs. Someone with crypto access could smuggle one into a code room and quickly write down the 36 numbers for a rotor while his two-man-rule colleague was napping or otherwise not paying attention. No doubt the Soviets had a supply of these devices for the KL-7 and other machines and systematically looked for cleared people at remote installations whom they could bribe or blackmail into recovering rotor wiring. I expect the intelligence agencies of the world today all have USB dongles designed to plug into computers to inject malware and grab passwords and private keys, just waiting for a compromised person with access to plug them in.
Arnold Reinhold

@_date: 2020-01-06 07:21:52
@_author: Arnold Reinhold 
@_subject: [Cryptography] how to detect breakage -- lures etc.?? 
If I understand your concept correctly, you are proposing a cipher based on a pseudo-random sequence generator made up of rotors of different periods with incommensurate lengths to produce a very long overall period, where the connections between the individual rotors can be varied according to a key so as to produces a very large set of different sequences. It?s my understanding that is pretty much what the NSA ended up doing in the 1950s, except they used binary electronic shift registers with feedback instead of alphabetic rotors. Your rotor idea may well have worked, but major military communications circuits were migrating to radio and landline teletype from Morse code, so binary made sense and electronics were the future. Transistors were not ready for prime time in the early '50s, so NSA used bi-stable transformer logic, along with vacuum tubes, in its first systems, e.g. the KW-26.(  ) I?m not aware of the actual algorithms NSA used being publicly available (and I have no inside knowledge), but there has been cryptanalytic attacks on commercial shift register ciphers that might suggest things to avoid in your rotor design. Arnold Reinhold

@_date: 2020-01-07 16:44:50
@_author: Arnold Reinhold 
@_subject: [Cryptography] how to detect breakage -- lures etc.?? 
Those are the official classification markings and yes, they only refers to "more critical" stuff contained in the removable subassembly, which if you look carefully is marked KLK-7/TESC, with a ?Reg. No.? The base of the KL-7 does not have the Confidential label and its serial number is marked ?Non-Reg No.? The U.S. practice of having removable rotor assemblies with a higher classification goes back to to at least the SIGABA, which was designed in the 1930s. Security is primarily provided by the keys, per Kirckhoffs.
One hint as to why the KL-7 rotors had such a low classification is in the NSA 1969 Cryptologic Damage Assessment from the North Korean?s capture of the USS Pueblo (  ).  One of the crypto machines on board was a KL-47, a variant of the KL-7. The NSA concluded that it was highly likely that KL-47s had been compromised in the past since: "many thousands of equipments have been in use world-wide for more than a decade, including about 10,000 functionally identical KL-7 equipments held by NATO countries. The equipment, associated maintenance manuals, and operating instructions have been exposed to possible compromise a number of times over the years. In recent intelligence operations, the USSR has made efforts to obtain key lists and rotors, but have shown no interest in the equipment itself or its supporting documents. Damage to SIGINT interests as a result of loss of the KL-47 to the USSR is also judged to be negligible.? Given that the KL-7 was intended for widespread field use, it's low classification would seem to make sense.
The Cryptologic Damage Assessment has lots of interesting stuff, including accounting of the various key lists on the Pueblo, along with many that were offloaded before it left port, that give a sense of how complex the U.S. military communications networks were. Arnold Reinhold

@_date: 2020-07-02 16:46:51
@_author: Arnold Reinhold 
@_subject: [Cryptography] Statement from Attorney General William 	P., 
This concern led to perhaps the greatest failure in cryptographic history. To minimize lost revenue, telegraph companies charged a higher price for code words. At some point they got tired of arguing about what constituted a code word vs a foreign word and simply charged more for words that were unpronounceable. The pre-WW II Japanese Foreign Office wanted to save money on the many encrypted cables it sent and had their cipher machines (which worked on the English alphabet for compatibility with the international telegraph network) designed so they could encrypt vowels into vowels and consonants into consonants, thereby preserving pronounceability. When the more secure Type B system, code named PURPLE by the U.S., was introduced, the Army Signal Intelligence Service quickly noticed that the ciphertext showed the same 6 vs 20 letter grouping that they knew from the earlier broken Type A RED code. They were soon able to break the 6s code which gave them some cribs that eventually led to breaking the much stronger 20s cipher, after some 18 months of hard work. The initial break was reported by 27 year old Genevieve Grotjan at about 2 pm on September 20, 1940. The 80th anniversary of that break is coming up and deserves some commemoration, both to honor a woman whose technical achievements helped change history (she also made a major contribution to breaking Soviet ciphers in the VENONA project) and as a warning about the dangers of non-cryptographer bureaucrats insisting on changes that weaken encryption systems.  Arnold Reinhold

@_date: 2020-07-07 14:04:15
@_author: Arnold Reinhold 
@_subject: [Cryptography] Statement from Attorney General William 	P., 
The best argument against AG Barr's efforts to weaken end-to-end encryption in popular communications services is that the bad guys will simply stop using them.  The recent new reports about the successful law enforcement take down of the Encrochat secure mobile phone network seems relevant here:
    According to the article: "Police monitored a hundred million encrypted messages sent through Encrochat, a network used by career criminals to discuss drug deals, murders, and extortion plots.? Extensive arrests followed and several major criminal gangs have gone to ground. The authorities are not saying how they cracked the encryption, though they apparently found a way to install software on the Encrochat devices. There are many ways for law enforcement to exploit communications on end-to-end networks. Call metadata provides valuable information on who is talking to whom. Traffic analysis can suggest timing of big events, like major drug shipments of terrorist attacks.  Infiltrating an undercover agent or turning a gang member can gain access to message traffic, and there are always zero day flaw and physical implant possibilities. And of course we have a vast array of surveillance tools that have become practical since the last time the U.S. Government tried to restrict cryptograph,  in the 1990s: cell phone location tracking, ubiquitous CCTV with license plate readers and face recognition, DNA databases, microphones in TV sets and voice assistants, etc. etc. That?s hardly going dark.
The last thing we want is for the bad guys to develop more effective communication discipline. When Osama bin Laden learned that the phones he was using were tracking him, he switched to communicating via thumb drive and trusted courier.  The most dangerous of all is radio silence, used in the most devastating attacks from Pearl Harbor to 9/11. We want to keep the bad guys talking as much as possible.
Arnold Reinhold

@_date: 2020-07-07 14:47:48
@_author: Arnold Reinhold 
@_subject: [Cryptography] ham radio and encryption 
When I was first licensed as a kid in the late 1950s, I was told that the ban on secret codes was to stop spies. Note that amateur radio was completely shut down during World War II. (There was a small War Emergency Radio Service staffed by hams, on VHF bands.) Today, the amateur community would likely oppose lifting the restriction on encryption, lest their valuable spectrum be choked with illegal but hard to detect commercial use. Note, by the way, that paragraph (5) is carefully worded to allow commercial communication in an emergency. Arnold Reinhold, K2PNK

@_date: 2020-07-13 11:43:05
@_author: Arnold Reinhold 
@_subject: [Cryptography] Terakey, 
Principles
Terakey(tm) is a cipher that offers confidentiality properties provable from first principles. It employs a shared secret key substantially larger than the anticipated volume of message traffic. Key bytes are extracted pseudo-randomly from the large key, using a message indicator as the seed. The extracted bytes can then be used one-for-one to encrypt message bytes. Terakey allows multiple stations that share the large key to communicate amongst each other without the elaborate bookkeeping required for one time pads. Two messages can occasionally use the same key byte, violating the one-time use restriction. That risk can be quantified and various ways are proposed to deal with it. Terakey can also provide a mechanism for key distribution that can serve an alternative to quantum cryptography, arguably providing comparable security, with lower cost, easier audit, and compatibility with any communication channel.  Arnold Reinhold

@_date: 2020-07-15 21:22:44
@_author: Arnold Reinhold 
@_subject: [Cryptography] Terakey, 
Principles
Thanks. An architect proposing a skyscraper with a structural design based on an unproven mathematical theory, would ridiculed. But a large part of the world economy is secured based on mathematical conjectures and no one seems to mind.
It?s a good point. Choosing v in a minimalist way may be better, perhaps just station of origin and date and time, with a log of message v?s to detect any duplication. More generally, it?s not sound to use widely assumed, but unproven, properties of a hash function in a first-principles security assertion. I?m not sure I see the problem here. An attacker who does not know the Terakey would have no way to accompany a chosen message indicator with a message body that decrypted into anything sensible or even predictable. So almost any sort of checksum would detect a failed decryption. What information would the attacker gain from the inevitable message rejection?
Right. I included the CSPRNG key generation method for completeness, with the warning. It?s at least conceivable that someone might want a key that could be regenerated in an emergency and be willing to give up first-principle security to have that. A petabyte key would presumably have a cryptoperiod much greater than a month, so taking that long to generate the key is not a deal breaker. But realistically, a petabyte (Petakey) array would be constructed of smaller memory modules and these could be loaded with random key in parallel using multiple ADC modules.  For example, Samsung currently offers a 30.72 TB solid state module. If 32 of these are used to build a petabyte storage, then 16 ADC rigs could generate a Terakey in a couple of days.
Sorry, I don?t understand the connection to what I am proposing. A photo of the Moon or pretty much anything else is highly non-random, except perhaps for low order bits which might encode digitization noise, and even a 10,000 by 10,000 pixel image would be much smaller than secret keys I am suggesting. Am I missing something? If there was an earlier proposal similar to mine, I?d like to include a reference.
Arnold Reinhold

@_date: 2020-07-17 14:38:17
@_author: Arnold Reinhold 
@_subject: [Cryptography] Terakey, 
Principles
On Jul 16, 2020, at 10:29 AM, Jerry Leichter  replied to me:
This is not the forum to argue about structural engineering, but I remember when the finite element method, extensively used nowadays in analyzing structures, was in its infancy and since then a large amount of theoretical research has gone into establishing its mathematical validity. The full story, about the Citicorp Center in Manhattan, illustrates a critical use of accurate mathematical models, the caveat being the importance of using the model correctly.  But an analogy is just a rhetorical tool and if it did not work for you, my apologies. I only made it to suggest there is some need for encryption tools that do not depend on the correctness of unproven mathematical statements.  Quantum cryptography also attempts to meet that need, but it has its own problems, including very finicky communications requirements. In my paper    I argue that Terakey offers comparable security, without many of the drawbacks of QKD.  If nothing else, a conjecture-free tool for secure communication would be valuable in rebuilding, if some mathematical breakthrough or other broad security beach undermines the existing public key world. The recent breach of VIP Twitter accounts, is only the latest in a very long series of warnings about cracked foundations in our security structures. Arnold Reinhold

@_date: 2020-07-20 14:21:57
@_author: Arnold Reinhold 
@_subject: [Cryptography] Terakey, 
Principles
Not quite. One-timeness is the issue we are dealing with. See below.
   [moving your footnote in-line:]
The purpose of the PRNG is to generate a pad for encrypting each message. Ideally, one would want a one-time pad, but Terakey does not guarantee zero-reuse, instead it generates pads that promise individual byte reuse will be rare, if you like, a mostly one time pad (MOTP). My paper proposes various ways of dealing with the rare reuse. ( )
Terakey is intended to provide confidentiality in two situations, key holders vs. non-key holders, and between pairs or groups of key holders (for privacy or compartmentalization). For confidentiality between key holders and non-key holders (arguably the more important case), confidentiality is based on first principles, without any reliance on mathematical conjectures. For compartmentalization, security also depends on the PRNG, as you point out. My paper suggests using public key methods to agree on a seed for compartmentalization purposes, and for forward security, but, as you say, any method to agree on a shared secret will do.
However, even with a cryptographically weak PRNG, say one whose state is recoverable from N known outputs, it is not easy to recover the state as used in Terakey. Suppose Alice sends Bob a new password and Mel gets hold of the cyphertext. And suppose further that the standard form for transmitting a password has lots of boilerplate, so Mel knows most of the plaintext and hence the cypherbytes used to encrypt a large part of the message. With a one terabyte Terakey, there are about 4 billion possible PRNG outputs that could produce each 8-bit cipherbyte. If N > 3, trying all combinations in order to find the state that produces the known cipherbyte sequence has a work factor of least 2^128. There are two cases where a stronger assertion for compartmentalized security among key holders is possible.
1. When Terakey is used for distribution of symmetric keys. This would be the use case where Terakey is compared with Quantum Key Distribution. In both instances there is a presumption that the symmetric cipher can be relied on, at least with frequent key changes. Then that same cipher's use as a PRNG would at least as secure, since each seed is a new key.
2. Using Terakey itself as the PRNG (an idea stimulated by this discussion), After selecting a Terakey byte to use as the cipher byte, use the subsequent 6 bytes to address the next cipher byte. (6 bytes can address up to 280 terabytes).  Since most mass storage does block reads, this would be fast, The only downside is that under the most conservative assumption that any Terakey byte previously accessed is compromised, this PRNG uses up Terakey seven times faster than using a conventional PRNG, but mass storage is cheap and getting cheaper.
You are correct that a shared secret is a single point of failure. There are many other single points of failure in our current security arrangements, conjectures on the difficulty of factoring and discrete logarithms, signing keys used by software vendors, CA root keys, private keys of government officials, etc. Because of its large size, logically and physically, a Terakey is more easily protected than conventional symmetric and asymmetric keys. I would envision each Terakey would be housed in a secure container with tamper detection and alarms, with one copy and a backup at each location. An interface processor could monitor the rate at which key bytes are extracted and sound an alarm if the rate exceed a threshold. Terakey is not a PKI replacement. I don?t envision it being used to protect very large networks.  More like different offices of an international bank, a large software organization with multiple locations, or coordinating security between different ISPs and browser makers. Multiple users at each site could use the same Terakey module. If nothing else, Terakey could serve as an independent secure communication method to facilitate rebuilding in the the event of a major system collapse or cyberattack.
Yes, probability of collisions. There is extensive literature on the statistical properties of PRNGs. A PRNG with good statistical properties need not be secure and there are many examples. In addition to theoretical analysis, many PRNGs have been subjected to exhaustive empirical tests, some of which could be incorporated as self-tests in a Terakey system. Here are a few Terakey advantages I see:
1. Elimination of each station's need to track pad usage with absolute reliability (not so trivial, systems do crash, malware could move back the pointer). 2. The ability to easily create private channels between any pair of key holders, again without bookkeeping, 3. New stations can be added without coordination with all the other key holders, and, for the same reason, a backup Terakey can be brought on line without requiring a transfer of state from the previous Terakey system.
4. For straight OTP, one has to provision enough pad material for each station. In a crisis where one staton sees very heavy traffic, there would be no easy way to re-provision if they ran out of key. Terakey degrades gracefully and all the key material is effectively available to the stations that need it most. 5. As an alternative to QKD. One has to admire QKD from an esthetic standpoint, finding a practical use for one of the most bizarre predictions of Quantum Mechanics, but an alternative that does not require precision optics and cryogenic refrigeration is at least worth considering.
Arnold Reinhold

@_date: 2020-07-22 22:12:22
@_author: Arnold Reinhold 
@_subject: [Cryptography] Terakey, 
Principles
Hi Whit,
Thanks for your comments. What I am doing is quite similar to the additive books you describe, with the important difference that because mass storage has gotten so cheap, we can afford to make the ?book? tens of millions of times bigger than what was possible in WW II. Most of the security of Terakey is based on this fact alone.
The analysis of Terakey ( consists of a series of levels. For the basic level, the attacker is assumed to know the PRNG algorithm and the message indicators, ciphertext and plaintext of all past traffic. Under these assumptions, the attacker would therefore know the locations and contents of all the Terakey bytes used for past traffic. The only thing relied on from the PRNG is providing a reasonable approximation of a uniform random sampling of the Terakey. It is well established that PRNGs can do that. The security analysis then consists of estimating the likelihood of a cypherbyte already known to the attacker being used in a new message of a given size. As long as the volume of traffic is kept low compared to the size of the Terakey, only a small number of bytes in a message will potentially be compromised. Various ways to deal with this potential leakage are presented, including classical methods like message padding and folding. The use of a cryptographic PRNG is later introduced as one way to deal with the occasional byte leakage. In addition, it offers a subset of key holders privacy from the other key holders, a secondary security objective. In terms of protecting Terakey-encrypted traffic from non-key holders, the information theoretic approach is doing the heavy lifting, with the computational approach only dealing with rare collisions. There is also synergy between the two, as I pointed out in my previous post, in that an attacker who even knows most of the plaintext of a new message gets very little to work with in finding the PRNG seed.
Back in WW II, it would have been unreasonable to ask a code clerk to go to a different page and line for each code group, and so adjacent strings of additives were used, a weakness Allied cryptanalysts took advantage of.  Liza Mundy, in her excellent book Code Girls, describes interviewing one of the elderly veterans of Arlington Hall who is at long last able to say out loud her top secret wartime job title: she was an overlapper.
Thanks for clarifying that. I am convinced that World War III will be started by a spell checker autocorrecting a diplomatic note.
Arnold Reinhold

@_date: 2020-07-29 19:06:42
@_author: Arnold Reinhold 
@_subject: [Cryptography] Terakey, 
Principles
I am not proposing Terakey as a replacement for all current encryption. You wouldn?t use it to directly encrypt a 4K video stream. I envision it being used for high value communications on relatively small networks, an international bank, say, or as a backup system for coordinating response to a cyber attack where the PKI infrastructure is in doubt. A one terabyte key can comfortably protect a gigabyte of data. That is a lot of text messages. Terakey can also be used to exchange symmetric keys, similarly to Quantum Key Distribution. in that mode a one terabyte key can protect one AES key exchanged per second for a year, a speed comparable to current QKD systems over long distances. And even that limitation is based on the very conservative assumption that an attacker has acquired the key used to encrypt most previous messages, not just the plaintext.
Current public key systems rely on mathematical problems whose difficulty is conjectured, not proven. This is a single point of failure with a probability that is not quantifiable. Progress in mathematics is extremely sporadic. Problems that have stymied great mathematicians for centuries have been solved in recent memory. I?m not saying a breakthrough is likely any time soon, just that having some backup might be a good idea.
If the NSA can tweak my algorithm, they can make it emit rot13, or an apparently strong PRNG stream where they know most of the seed. That is true of any crypto system.  If I have somehow created the impression that I am proposing the PRNG be something that is negotiated between the parties, my fault for not being clear. I did not specify a specific PRNG because I wanted to consider ones that were reversible vs ones that were based on security primitives.  A Terakey system fielded would have a fixed PRNG.  If you like, the one specified in NIST SP 800-90A Rev 1, section 10.2, using a block cipher such as AES-256 or whichever block cipher is being used with key exchange mode.
Arnold Reinhold

@_date: 2020-11-13 13:38:37
@_author: Arnold Reinhold 
@_subject: [Cryptography] Swiss helped with CIA spying 
Dave Horsfall wrote on 12/11/20 4:12 pm:
The Wikipedia article on Crypto AG was created in April 2004. On February 10, 2005 the following paragraph was added:
"Crypto AG has been accused of rigging their machines in collusion with intelligence agencies such as the United States National Security Agency (NSA), enabling such organisations to read the encrypted traffic produced by the machines. Crypto AG has dismissed these accusations as groundless.?
The article's discussion of the allegations have expanded over time and were quite elaborate well before the recent story about the Swiss government investigation. This includes information corroborated by NSA history documents that were declassified in 2015 and reported at that time by major outlets, including BBC News. The documents describe a 1955 deal between the U.S. and Crypto AG.
This is indeed old news.
Arnold Reinhold

@_date: 2020-11-16 16:16:28
@_author: Arnold Reinhold 
@_subject: [Cryptography] Possible reason why password usage rules are 
Another historical source is the now obsolete FIPS PUB 112, from May 30, 1985:    Its Appendix E.3, Determining Password Length (p. 44 ff), includes formulas for calculating needed password size based on allowed life, and acceptable probability of penetration. It assumes the threat is repeated login attempts and that the rate at which passwords can be tried is fixed. In their worked example they assume "8.5 guesses per minute (guess rate possible with 300-baud service)? They calculate a password length of 8 or 9 characters or 3 dictionary words, depending on assumptions, including 1 year and 6 month password life.
RFC 1750, December 1994, also obsolete, contains a similar calculation in section 8. They assume a fixed delay (e.g. 6 seconds) is imposed between successive attempts and that passwords are changed yearly and they then derive a minimum password size. They mention the possibility of detecting large numbers of failed attempts, but assume conservatively that this will be done infrequently by inspecting log files. Its replacement RFC 4086 section 8 has a similar but lengthier analysis, but still assumes a fixed rate. Interestingly, the later document assumes an acceptable probability of getting access as 1 in 1000, whereas RFC 1750 assumes 1 in 1,000,000. What is missing in both models is the notion of limiting the rate of attempts by progressively increasing the delay after each failure, i.e. throttling. (NIST SP 800-63B Section 5.2.2) It would be interesting to know when throttling was introduced. Also is there any standard or other text that does the same calculations assuming a throttling profile?
More broadly, there seems to be a lack of accepted best practices for password security. NIST Special Publication 800-63B ?Digital Identity Guidelines Authentication and Lifecycle Management,? is a recent attempt, but this large document crams the issues we are talking about into just one section, 5.1.1.1. The document is relatively recent (June 2017) and not widely followed, as far as I can tell. It does not include many topics I believe are needed. Here is my list:
o Larger minimum password length (SP 800 63B requires 8 characters, but this is to few. 10 or 12 should be the minimum)
o Only using computationally and memory hard hash or hardware security for verification data
o Special treatment required for password reset answers (e.g. segregated server with separate backup and restricted connectivity)
o including algorithm versioning to allow password verification storage upgrades
o Disclosure to consumers by public-facing, password-protected systems of the protective measures in use , including, salt size, hash function and repetition count
o Offering system generated password or passphrases, preferably in several formats, e.g.
  Random pass phrase with different word lists
  Random letters with mnemonic sentences
  Random pronounceable syllable groups o Smart throttling     Higher limit for longer passwords     No dings for blank password or repeat of previous try
    Non bricking ? no extreme lockout (>6 hours)
    Notification of possible caps lock
o Encouraging people to use password managers, at least for most passwords
o Encouraging people to write down non-managed passwords, with suggestions for safe places. It?s no longer reasonable to expect ordinary users to memorize all the passwords or passphrases users need, if they are to be strong enough. Other suggestions welcome.
Arnold Reinhold

@_date: 2020-11-18 19:49:17
@_author: Arnold Reinhold 
@_subject: [Cryptography] Possible reason why password usage rules are 
. . . [out of order]
This is very important work and I wish you great success, but realistically even if it is a technical triumph, it will take many years, maybe decades, to see widespread adoption. In the meantime the world will continue to depend on password based systems and my focus has been on ways to make passwords work. Because of the increased power of cracking rigs you describe, since 2014 my Diceware.com recommends 6 word passphrases for strong security. If generated per instructions given there, 6 words offers about 77.4 bits of randomness. At a trillion  (2^40) attempts per second, that requires ~2^36 seconds or ~2,800 years for a 50% chance of cracking, a reasonable margin. Many people find such passphrases memorable if used frequently (happy users have contributed Diceware(tm) word lists in 28 languages). For very long term passwords (e.g. crypto currency wallets) I suggest 7 words or more.
But I agree that many users find passwords that long unacceptable. That is why I have to challenge with your rejection of slower digest functions. Resource intensive hashes are a way to restore some balance between users and crackers.   Crackers have to a large extent taken advantage of progress in consumer electronics, namely the development of powerful but inexpensive GPUs for computer gaming. Ordinary cryptographic hashes such as the SHA family, are designed to be fast. If they are used to secure passwords, the crackers advantage only grows with time. Back in 1999, in proposing that password hashes be designed to also use large amounts of memory, I wrote (  ):
''A key stretcher can also try to level the playing field against an attacker with the resources to build massively parallel search engines by utilizing as much as possible of the computing resources that the user already possesses. In the ideal limit, the best such an attacker could do would be to replicate the user?s computer many times.?'
To do that today would require key stretchers to employ the GPUs consumers already have, but that should be possible. And over time the number of iterations and amount of RAM required should increase to match the increase in available computing power. That?s why I suggest password systems include a version field with each password hash so passwords can be rehashed as more intensive hashes are introduced. Yes, resource intensive hashing will impose a burden on organizations that offer password-protected services, but that may be the price for security. On Nov 17, 2020, Kent Borg added:
Unfortunately, experience show that files containing password hashes are stolen all too frequently. I?ve also come up with a very different concept for storing password validation data securely using a very large (e.g. terabyte) secret that doesn?t require resource intensive hashing, which I call Rock Salt:( )
If a user?s machine is infected, the malware can harvest passwords directly as the user logs in to protected services. There are several password managers out there with good reputations. A reasonable compromise might be to use a password manager for the vast number of passwords whose compromise would cause limited damage and use strong passwords, backed up on paper, for ones most valuable accounts. Arnold Reinhold

@_date: 2020-11-20 14:57:55
@_author: Arnold Reinhold 
@_subject: [Cryptography] Possible reason why password usage rules are 
These two comments do not reflect the current state of the art. For example by 2012 it was possible to try all Windows NTLM eight-character password containing upper- and lower-case letters, digits, and symbols in 5.5 hours using 25 AMD Radeon graphics cards of that era (GPUs have gotten a lot faster since). Modern cracking software, such as John the Ripper use a variety of modes, including dictionaries, straight brute force and pattern-based searches using word mangling rules. Almost all the passwords in recovered corpuses were from stolen file of hashedd passwords. It?s rare to hear of a hacked firm storing plaintext passwords. Recovery rates from stolen hash files are typically 70 to 80%.
Here is a nice presentation from 2009 And a more recent story Not only have GPUs gotten faster, but you can now rent clusters from AWS and the like. Salt helps, of course, by eliminating the possibility of cracking a group of hashes at the same time, but cracking has gotten fast enough that it is still possible to attack them one at a time. And to penetrate an organization, you often only need one recovered password. Resource intensive hashes such as bcrypt, scrypt and  pbkdf2 with high iteration count make a bigger difference. Here is a collection of benchmarks using hashcat and modern GPUs:
There are a large number of hashes presented. Note that the hash rate units range from GH/s to MH/s, kH/s and just H/s. A huge range of speeds.
Several people have commented on the distinction between passswords used to access a resource and those used to derive a cryptographic key.
As Phillip Hallam-Baker wrote (Thu, 19 Nov 2020 05:46):
That is why we need some requirement that services that use password authentication disclose the measures they use to store password validation data, or maybe some seal of approval that guarantees the meet some minimum standards, in particular something better than fast hashes, which are no almost useless.
In one way the situation with passwords used to generate cryptographic keys can be better in that a long resource intensive password can be used to process the password. A user can tolerate a delay of a secong or more when opening an encrypted disk drive, for example. Arnold Reinhold

@_date: 2020-11-20 16:03:35
@_author: Arnold Reinhold 
@_subject: [Cryptography] Possible reason why password usage rules are 
One has to begin somewhere. And poor storage of password validation data is a major vulnerability. For starters, I am suggesting transparency and self-certification, not regulation. We already have regulation in many jurisdictions requiring prompt notification when data is compromised and the associated costs to enterprises can be quite high. So there is an incentive to do things right, which needn't cost that much anyway. What is lacking is clear standards. Most developers seem to know they should use a hash and many know about salt, but most think SHA-256 is perfectly adequate, when it isn?t.
And yes there are other vulnerable areas to address. Password reset answers are orders of magnitude easier to guess or recover from hashes, for example. But that is another discussion (see  e.g.).
Arnold Reinhold

@_date: 2020-11-21 19:43:30
@_author: Arnold Reinhold 
@_subject: [Cryptography] Possible reason why password usage rules are 
The password cracking community has had lots of experience and their tools are quite refined. They sensibly try the easy stuff first, common passwords, dictionary words, mangled words in various patterns. They save brute force for the passwords that still don?t yield. But brute force is quite feasible for password sizes people commonly use if hashed by a fast hash.
Microsoft still stores passwords using NTLM, I believe. NTLM hash is based on MD4, which is throughly broken for collision attacks, but last I heard is still reasonably resistant to preimage attacks. And the attack described above didn?t exploit any weakness except fast speed of execution, which allows many guesses to be tried per second. SHA-256 takes about the same time to evaluate as MD4, so it is no stronger against brute force attacks. Worse, there is a lot of hardware designed to evaluate SHA-256 for Bitcoin mining on the used market, so SHA-256 might even be more vulnerable than MD4.  We do have NIST SP-800-63B, especially section 5.1.1.2, By my count, this one paragraph has about 28 separate provisions which addresses many of these issues, including a negative opinion on requiring frequent password changes, absent a breach:  "Verifiers SHOULD NOT require memorized secrets to be changed arbitrarily (e.g., periodically). However, verifiers SHALL force a change if there is evidence of compromise of the authenticator.?
I have some ideas for a set of certification levels, which I will try to write up.
Arnold Reinhold
