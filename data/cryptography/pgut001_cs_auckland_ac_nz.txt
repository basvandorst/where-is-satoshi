
@_date: 2001-08-26 11:08:52
@_author: Peter Gutmann 
@_subject: New Microsoft Security Server* (*runs Solaris) 
Microsoft is currently running a series of double-page ads in various security
magazines advertising the new Microsoft Internet Security and Acceleration
Server (see for example Info Security Magazine, August 2001, p.38-39, or
Information Security magazine, August 2001, p.2-3).  The picture in the ad
shows a sysadmin sitting in a network room in front of the server.  The server is a Sun machine.
(Nice to see even MS admit that if you want a secure server, you shouldn't be
 running Windows on it :-).

@_date: 2001-12-16 06:36:09
@_author: Peter Gutmann 
@_subject: The MS DRM Patent and Freedom to Speak and Think 
I wonder how long it'll take for someone to locate the undocumented _MPAAKEY
which exists alongside MS's documented one?

@_date: 2001-12-19 00:51:02
@_author: Peter Gutmann 
@_subject: Cut and paste defeats Excel's password protection 
Spreadsheet passwords can be foiled by simply copying and pasting.
By John Fontana
Network World, 12/17/01
REDMOND, WASH. - Microsoft Excel, the predominant spreadsheet in use today,
contains a feature that could expose sensitive corporate data once the document
is distributed within a company or among trading partners.
That feature is drawing an increased level of attention from researchers and
Excel users alike as its implications become more fully understood. One expert
calls it "as potentially damaging" as many of the most recent viruses.
Excel has features that allow spreadsheet creators to hide, lock and/or
password-protect data and mathematical calculations used in original documents.
These features seemingly provide a measure of data security to conceal
specified data from prying eyes.
In reality, that data can be exposed by any end user who can execute a simple
copy-and-paste procedure. It takes fewer steps to reverse the security than it
does to set it up.

@_date: 2001-12-28 10:41:28
@_author: Peter Gutmann 
@_subject: CFP: PKI research workshop 
Naah, it's the monorail/videophone/SST of security.  Looks great at the World
Fair, but a bit difficult to turn into a reality outside the fairgrounds.
Peter (who would like to say that observation was original, but it was actually
       stolen from Scott Guthery).

@_date: 2001-12-28 11:50:17
@_author: Peter Gutmann 
@_subject: CFP: PKI research workshop 
The reason for this is that as we work on PKI deployment, we discover more and
more (previously unknown) problems which need to be solved.  If you look at PKI
in 1978 it was pretty simple (certificates in a "public file"), then in the
1980's it got more complex with directories and CRLs and whatnot, and after
that an ongoing stream of further issues which need to be addressed were
discovered as systems were finally deployed.  It's just getting harder and
harder as we discover more and more problems when we try and actually implement
the thing.  Even what we know now is only the tip of the iceberg compared to
what we're going to discover further down the track, and that's only
identifying the *problems* to be solved, not providing solutions.
PKI is like an erection: The more you think about it, the harder it gets.

@_date: 2001-12-30 13:03:29
@_author: Peter Gutmann 
@_subject: Neat security quote found on slashdot 
Re:Nondisclosure (Score:1) by FauxPasIII (fauxpas at cc.gatech.dont.spam.me.edu) on Saturday December 29,  (   Businesses are not going to expend money fixing any problem, no matter how
  severly it affects me as a customer, until it starts to affect their
  profitability. I wouldn't expect them to; they are a construct created with
  the express purpose of optimizing profitability. My goal as a security-
  conscious consumer is to -make- it the corporation's best interest to fix any
  problems that would have a detrimental effect on me as quickly as possible.
(Please, not another full-disclosure flamewar, I just wanted to post this
 because it seems to summarise the situation nicely).

@_date: 2001-12-31 12:12:21
@_author: Peter Gutmann 
@_subject: CFP: PKI research workshop 
Thus making it a perfect analogy for PKI [0].
[0] Before people flame me for this, what's currently widely-used is what's in
    X.509v1 modulo CRL support.  Anything else, you're on your own.

@_date: 2001-06-05 05:38:14
@_author: Peter Gutmann 
@_subject: Starium (was Re: article: german secure phone) 
They went belly-up some time ago, I don't know who owns the IP rights.

@_date: 2001-06-07 05:23:00
@_author: Peter Gutmann 
@_subject: secure phone (was Re: Starium...) 
I hate to sound like a pessimist, but I can't imagine this ever happening. This sort of thing has been around since (at least) brew-a-stu in the early
'90s, and seems to come up again every few years (and then fade out after 6-12
months).  Every time the people involved eventually find that while the general
details are easy enough to sort out, getting a finished implementation done
take an awful lot of effort (the first 90% of the work takes 90% of the time
and money, the remaining 9% takes the other 90%, and the final 1% takes the
third 90%).
Given that most people are going to be sitting at PCs or have laptops with
them, what's wrong with Nautilus (or whatever) and a headset plugged into their
sound card (in the worst case you can tunnel $generic_internet_talk_software
over ssh or SSL with CPU power to spare)?  Most users have already got most of
the hardware at hand, is it worth the effort to design a special-case secure
phone for a market which seems to be fairly limited?

@_date: 2001-06-24 11:55:40
@_author: Peter Gutmann 
@_subject: crypto flaw in secure mail standards 
It's a standard part of the S/MIME signature.  Optional features include
countersignatures from external timestamping authorities.

@_date: 2001-11-03 18:15:47
@_author: Peter Gutmann 
@_subject: Proving security protocols 
I may as well speak up as well then... I spent most of a chapter of my thesis
looking at formal security verification in fairly exhaustive detail (if I
missed anything I'm sure I'll hear about it soon :-).  You can get it as
  The conclusion is
that there are more effective ways to spend your time and money, but for the
full story I'd recommend you read the above document.

@_date: 2001-11-11 07:04:06
@_author: Peter Gutmann 
@_subject: when a fraud is a sale, Re: Rubber hose attack 
... contained within a minefield of patents and IP restrictions, which is
killing its use.  What would be necessary is either for someone (presumably
with any army of lawyers to back them up) to state that a particular (sound)
scheme was free of any IP restrictions, or for one or more of the groups with
patents to state they'd allow everyone royalty-free use.  As it is at the
moment, it's just too risky to do anything.  Even if someone has a technology
which they claim is unencumbered, others may claim that they have some patent
which covers it, or the situation is unclear enough to scare off companies who
are afraid of lawsuits.  As a result, no-one can do anything.

@_date: 2001-11-14 03:59:37
@_author: Peter Gutmann 
@_subject: Thai Pirates Crack Microsoft's New Windows System 
BANGKOK (Reuters) - Thai computer users are buying thousands of pirated copies
of Microsoft's new Windows XP (news - web sites) operating system a week ahead
of its official launch in Thailand, vendors said on Monday.
Shops at Bangkok's Pantip Plaza -- a multi-story rabbit's warren of computer
goods outlets -- said pirates had found ways of getting around the new
operating system's security features.
``We've had XP Professional for three weeks and it's selling very well. We sell
around 200 copies a day,'' one shop owner, who identified himself only as Nop,
told Reuters.

@_date: 2001-10-07 06:45:16
@_author: Peter Gutmann 
@_subject: bin Laden's hidden messages revealed 
While browsing through an expansion of pi I have discovered various messages
describing the attack on the WTC.  Further corroboration of this was found in
e, the square root of 2, 3, and other values.
The next step is obvious: The government must either ban entirely, or at least
introduce strict licensing of, irrational numbers.  God knows there's enough
irrationality around already after the attack.

@_date: 2001-10-31 03:28:29
@_author: Peter Gutmann 
@_subject: Thawte Protects The World From Crypto (was Re: [ Slashdot Message ] Daily Stories) 
"R. A. Hettinga"  forwarded:
As was mentioned on the Slashdot debate, this has nothing to do with crypto but
is for AuthentiCode signing certs.  Blaming this move in terrorists therefore
makes it even more bizarre.  According to Thawte (via Slashdot), they were just
following orders from Verisign.  The only explanation I can think of is that
it's some attempt by MS to further lock small developers out of XP/.NET
(alongside charging $1K/year for developers and similar things), but that's
pretty far-fetched.  On the whole this move makes no sense, is anyone from
Verisign able to exlain it?  (Is anyone from *anywhere* able to explain it?).

@_date: 2001-09-18 20:11:00
@_author: Peter Gutmann 
@_subject: How to ban crypto? 
There is strong empirical evidence to support the fact that we can't get this
right.  Let's say a GAK infrastructure is two orders of magnitude more
difficult to establish than a PKI (it may be even worse than that, but let's
take that as an estimate - to get a GAK infrastructure going you need, as a
minimum, a fully functional PKI to build on top of).
After 10 years of effort we haven't even managed to get a basic PKI going yet
(what's being practiced today could best be described as "certificate
manufacturing").  I can't see how a GAK infrastructure will ever be practical.
(I once heard a story about a someone in the military who suggested that
 security researchers develop a program which could analyse another program to
 see if it would do something malicious.  The response was that the military
 should fund the research and they'd let them know when they had a solution.
 Perhaps this is a way to get funding for further PKI/GAK research).

@_date: 2001-09-20 05:19:22
@_author: Peter Gutmann 
@_subject: Field slide attacks and how to avoid them. 
I can feel this sliding into a specification language debate, but I have to put
in a word to defend ASN.1 here.  When used by a skilled practitioner, ASN.1 can
be truly elegant.  The problem is that, like BASIC, it looks deceptively
simple, so that everyone thinks they can write a spec in ASN.1 after five
minutes study of an ASN.1 introductory guide, and they usually do.  The result
is a great confused muddle which noone can figure out and everyone implements
slightly differently, leading to ASN.1's reputation of being a pain to work
with (to paraphrase the famous FORTRAN comment, "The determined hack can write
crap in any language").  Having had experience working with ASN.1, XDR, the SSL
specification notation, and PGP, I definitely prefer ASN.1 for its ability
(when used correctly) to provide a clear, unambiguous definition of a data
exchange format.

@_date: 2001-09-20 05:55:36
@_author: Peter Gutmann 
@_subject: Field slide attacks and how to avoid them. 
And those who enjoy pain shouldn't forget X.12, EDIFACT, and HL7.

@_date: 2001-09-25 16:24:34
@_author: Peter Gutmann 
@_subject: Stego spy terrorist scare 
Looking at some of the recent (unsubstantiated) reports of people who would
abhor porn for religious reasons using porn to communicate (!!), I wonder if
these stories can be traced back to the LA Times nonsense of a few years ago
where unnamed spies were doing the same thing?  Given that these rumours stick
around more or less forever once started, and that the stego-porn story seems
to be no more than a rumour, could it just be a mutation of the same story?
(Why couldn't bin Laden just record his messages backwards in heavy metal like
satan does?).

@_date: 2001-09-26 16:39:04
@_author: Peter Gutmann 
@_subject: [FYI] Did Encryption Empower These Terrorists? 
I've run into the same issue with various companies (including some big ones)
who eventually run into the following situation:
  "We need to encrypt our customer database because of security concerns over
  credit card numbers being stolen.  Oh yes, we use the CC# as the primary key
  for all our accounts".
This practice seems to be fairly widespread.  Workarounds are very difficult
(*everything* is keyed off the CC# as a unique customer ID, something like that
is very hard to fix in practice).
(And before someone jumps in with the obvious "It's easy, just replace the CC#
 with ", consider the following
 scenario: You have a company with gear distributed over 300 sites worldwide,
 using software from 120 vendors running on 18 different platforms, of which 3
 provide source code.  8 have gone out of business (the software is still being
 used because it does the job), and all but the 3 which have source code
 available use an undocumented, proprietary format for their data.  Your job is
 to provide a time-and-materials estimate on what it'd take to fix this.  You're
 allowed a maximum of 90 days and $50K (+ 3 programmers) to get the problem  solved).

@_date: 2002-04-04 22:27:43
@_author: Peter Gutmann 
@_subject: what is GPG's #1 objective: security or anti-patent stance ( Re: on the state of PGP compatibility (2nd try)) 
I've been in a similar situation.  Back when I was fighting our government over
crypto export controls, it was sometimes necessary to talk to journalists in a
manner which didn't give the spooks a week's advance notice about something
which they shouldn't have known about until they opened the morning paper.
This was in the days of PGP 5.x.  Some of the people I was talking with were
pretty patient, and often put up with multiple iterations of neither side being
able to decrypt the other's messages, but eventually the choice came down to
given the opposition advance notice or not having the story published at all,
and there's really not much choice there.
Now substitute "human rights group" for "journalist" and "secret police" for
"spooks" and you can see why this is a problem.  Non-commercial PGP has always
been by geeks, for geeks, with features more important than minor
considerations like usability.  Who cares if there are 146 semi-documented,
vaguely-defined command-line options, look at the algorithm choices!  If you
want to use some obscure hash algorithm which was fasionable for 2 months in
1997, you can, and who cares if it takes you half an hour, the FAQ, the
manpage, and an online search to figure out how to encrypt a file?
That's why non-commercial crypto will always struggle to find mainstream
acceptance.  Doing the crypto engine is (relatively) easy, and fun, and there
are lots of people willing to help.  Doing the UI components is dreary and
boring, and no-one is interested because they've just spotted a hash algorithm
published in the Journal of the Bratislavian Philological Society in 1978 which
they urgently need to add support for.
(Although I don't use Windows mailers, I've heard nice things about The Bat,
  which has built-in PGP support.
 Apparently at some point Pegasus Mail,  will have built-
 in PGP and S/MIME support as well).

@_date: 2002-08-30 17:35:07
@_author: Peter Gutmann 
@_subject: Cryptographic privacy protection in TCPA 
I designed something along these lines some years ago as a way of building a
fault-tolerant key management system.  The idea is that you create a pile of
keys, and these vote on key updates.  If a key is compromised, you sign its
replacement with a quorum of non-compromised keys, and replace the bad key.
You also periodically roll over keys as a preventive measure, limiting
exposure due to compromises.  No need for a PKI or anything else complex like
that, it's all automatic and transparent.
There can be slight problems if a device stays offline long enough that enough
keys have been rolled over to make reaching a quorum impossible, which was an
issue when I designed the thing but rather unlikely now.  I can dig up the
exact details in case anyone's interested.

@_date: 2002-12-09 18:07:16
@_author: Peter Gutmann 
@_subject: PGPfreeware 8.0: Not so good news for crypto newcomers 
There are other PGP libraries available.  The Veridis Filecrypt SDK,
 is a commercial
offering which uses the OpenPGP format, and my own cryptlib,
 is available under
the Sleepycat license (GPL or commercial, your choice).  You can modify it in
any way you like, although if you want to do things with it long-term, you may
want to wait until the next release, I rewrote a lot of the lower-level PGP
code recently.

@_date: 2002-12-13 22:20:09
@_author: Peter Gutmann 
@_subject: TCPA-defeating BIOS switcher 
There's a neat device called the BIOS Saviour which I first saw on the
eksitdata site,  but which has now had a few reviews
on English-language sites, e.g.
 and
  It
consists of a mezzanine board that fits underneath your existing BIOS chip and
contains a second BIOS chip that can replace the existing one.  You can switch
between the two via a switch mounted on a blank plate in an expansion slot.
The intended use is to protect against bad flashes or virii (other uses are
for hot-chipping motherboards with hacked overclocking-enabled BIOSes if the
existing BIOS doesn't support it), but it could also be quite useful to swap
out a TCPA-crippled BIOS for an unencumbered one .  Really adventurous modders
could set it up to boot into the TCPA BIOS as far as is necessary, halt the
CPU via a small processor sitting on the SMB, swap in the non-TCPA BIOS, and

@_date: 2002-02-04 03:09:57
@_author: Peter Gutmann 
@_subject: Welome to the Internet, here's your private key 
It is accepted practice among security people that you generate your own
private key.  It is also, unfortunately, accepted practice among non-security
people that your CA generates your private key for you and then mails it to you
as a PKCS  file (for bonus points the password is often included in the same
or another email).  Requests to have the client generate the key themselves and
submit the public portion for certification are met with bafflement, outright
refusal, or at best grudging acceptance if they're big enough to have some
clout.  This isn't a one-off exception, this is more or less the norm for
private industry working with established (rather than internal, roll-your-own)
CAs.  This isn't the outcome of pressure from shadowy government agencies, this
is just how things are done.  Be afraid.
(I have a paper in the works which covers things like this in some detail, but
 the number of times this has come up recently is sufficiently alarming that I
 thought I'd post a heads-up here to let others who aren't exposed to this sort
 of stuff know about it.  This also doesn't begin to go into the number of CAs
 who are re-certifying the same user key over and over again, year after year
 ("We haven't been informed that it's been compromised, so it's safe to keep
 using it for another year")).

@_date: 2002-02-07 04:41:11
@_author: Peter Gutmann 
@_subject: Welome to the Internet, here's your private key 
I've seen similar things.  The CAs are so worried about key security that they
insist on generating the things themselves, but then hand them over in PKCS format from which they're shared by, and easily accessible to, every single app
running on the machine, are copied across other machines (because it's valuable
enough that you don't want to have to get a new one for each machine), etc etc
(again, I go into this in some detail in my paper in the section titled
"Private keys aren't").
Some of the, uh, logic applied by CAs for cert management can lead to really
bizarre situations.  For example there's a public CA which password-protects
access to its CRLs, using the reasoning that anyone who can get access to a CRL
can determine which keys have been compromised, and that's a bad thing (isn't
that what a CRL is for?).  As a result, anyone can get access to the certs the
CA issues, but only a tiny, select few can check whether they've been revoked
or not (given that most apps just ignore revocation checking, this probably
isn't as serious as it sounds).  There's a list of silliness as long as a very
long thing when it comes to working with certs...

@_date: 2002-02-07 05:15:18
@_author: Peter Gutmann 
@_subject: Welome to the Internet, here's your private key 
General-purpose data compressors (which make rather nice entropy estimators)
also have problems with counting events.  The Calgary compression corpus (the
Dhrystone of the compression world) includes a file geo in which every fourth
byte is a zero.  No standard compressor will pick this up, so that while they
all realise that zeroes occur with ~25% probability, they don't realise that
they always occur at every fourth byte (alongside a few others in between).
There will always be data patterns which appear obvious to a human but aren't
easily picked up by automated tests, so I don't know how far it's worth chasing
this thing.

@_date: 2002-02-07 04:54:00
@_author: Peter Gutmann 
@_subject: Welome to the Internet, here's your private key 
This also causes problems, because it's really, really hard to spread the key
around if the only copy is on the card.  Solutions I've seen are to multiplex a
single card + reader across multiple machines, or (more commonly) to generate
the key in software and then load it onto the card, with copies kept active on
the host PC.  This combines the benefits of smart card security and the
flexibility of software crypto keys which can be copied and distributed as

@_date: 2002-02-07 05:55:27
@_author: Peter Gutmann 
@_subject: Welome to the Internet, here's your private key 
A generic order-0 entropy estimator (think Huffman coder) will pass this,
because each symbol occurs with equal probability.  The reason this is a
problem is because any introductory information theory text will give the
standard formula for entropy estimation (H = -sum(prob(x) * log( prob(x)))) and
users will either stop reading there or the text won't go any further.  I've
seen a (fielded) crypto RNG which uses this sort of estimator, which won't
catch a whole pile of failure modes which the FIPS tests would get.

@_date: 2002-01-12 20:25:57
@_author: Peter Gutmann 
@_subject: Liability issues in computer security 
For those who don't normally read it, the December issue of ;login (which
you'll eventually be able to get at if you're not a member) has a nice legal analysis of the issue of liability for
negligent computer security.  It's probably the best (and certainly the most
detailed) discussion I've seen on this topic (unless there's something hidden
in an obscure law journal somewhere which I'm not aware of).

@_date: 2002-01-18 15:31:25
@_author: Peter Gutmann 
@_subject: Bill's Bull, pt. 2... 
"R. A. Hettinga"  quotes:
  Microsoft Issues Press Release to Say it Will No Longer Treat Security as
  Just a PR Problem

@_date: 2002-01-21 16:02:07
@_author: Peter Gutmann 
@_subject: PGP & GPG compatibility 
There are already a number of S/MIME gateways which do exactly this.  The most
typical mode of operation is org-to-org, where all mail from an organisation is
routed through their corporate gateway anyway so it's a natural place to
perform this operation.  It works reasonably well, and is completely
transparent to the end user (although org-to-org is rather easier to get going
than end-user-to-end-user).  The S/MIME WG has been working on a whole string
of add-ons to basic S/MIME for handling this type of messaging, encrypted
mailing lists, and assorted other useful stuff.

@_date: 2002-01-25 23:07:50
@_author: Peter Gutmann 
@_subject: Amusing note on real-world PKI deployment 
Found while submitting a paper to a PKI conference:

@_date: 2002-07-05 17:02:04
@_author: Peter Gutmann 
@_subject: New Chips Can Keep a Tight Rein on Consumers 
"R. A. Hettinga"  quotes:
Actually I'm amazed no printer vendor has ever gone after companies who produce
third-party Smartchips for remanufactured printer cartridges.  This sounds like
the perfect thing to hit with the DMCA universal hammer.  I wonder if there's a
good reason for this?  Why is this particular field immune?

@_date: 2002-07-06 17:47:16
@_author: Peter Gutmann 
@_subject: New Chips Can Keep a Tight Rein on Consumers 
If the vendor required it, how long do you think it would take their lawyers to
figure out a way in which some sort of copyright was involved somewhere, and it
could therefore be hit with the DMCA hammer?  Thus the "universal hammer"
comment, you can define almost anything you want to be a copyright violation if
it suits your purposes.  My guess on this one (and IANAL) is that reading the
instruction codes sent from the host would be the user-definable copyright
violation for third-party Smartchips.

@_date: 2002-07-11 15:17:52
@_author: Peter Gutmann 
@_subject: IP: SSL Certificate "Monopoly" Bears Financial Fruit 
Both Netscape 6 and MSIE 5 contain ~100 built-in, automatically-trusted CA
 * Certs with 512-bit keys.
 * Certs with 40-year lifetimes.
 * Certs from organisations you've never heard of before ("Honest Joe's Used
   Cars and Certificates").
 * Certs from CAs with unmaintained/moribund websites ("404.notfound.com").
These certs are what controls access to your machine (ActiveX, Java, install-
on-demand, etc etc).
  * It takes 600-700 mouse clicks to disable these certs to leave only CAs you
    really trust.
(The above information was taken from "A rant about SSL, oder: die grosse
 Sicherheitsillusion" by Matthias Bruestle, presented at the KNF-Kongress
 2002).
How many more do you need?

@_date: 2002-07-12 03:18:09
@_author: Peter Gutmann 
@_subject: IP: SSL Certificate "Monopoly" Bears Financial Fruit 
I'd heard stories of collapsed dot-coms' keys being auctioned off, that being
the only thing of value the company had left.  It makes the title of Matthias'
paper even more appropriate.
(However, I do think that anyone wanting to compromise your security will use
 this morning's MSIE hole to do it rather than buying a CA key.  OTOH it'd be a
 great universal skeleton key for government agencies charged with protecting
 the world from equestrians).

@_date: 2002-07-21 19:47:39
@_author: Peter Gutmann 
@_subject: Photos of Bad Aibling RSOC posted 
I've finally got around to posting the photos of Bad Aibling RSOC which I took
two years ago.  From the web page:
  There are very few photos of the Bad Aibling RSOC around, only one or two
  rather old, grainy illustrations which presumably came from newspaper
  articles.  When I was in the area in the summer of 2000 I took the
  opportunity to wander round and take a few photos, which I've posted below
  as scanned.  These were taken with a borrowed camera and a fixed 50mm lens,
  so don't expect too much (I'm a cryptographer, not a photographer).  Note
  that this page can take awhile to load because of the number and size of the
  images.
Available at

@_date: 2002-07-31 17:07:13
@_author: Peter Gutmann 
@_subject: building a true RNG 
There's another way to avoid this problem, which is to separate the nonce RNG
and crypto RNG, so that an attacker seeing the nonce RNG output can't use it
to attack the crypto RNG.  This is done in PGP 5.x and the cryptlib RNG.  OTOH
some RNGs are used in exactly the opposite manner, generating alternate public
and private random quantities, which make it possible to use one to infer
information about the other.  Examples are generators used with SSL and ssh,
which both alternate from public nonces to private session keys and back.

@_date: 2002-06-01 20:18:20
@_author: Peter Gutmann 
@_subject: PKI: Only Mostly Dead 
Thankyou Nobody.  You should have found the e-gold in your acount by now :-).
  "Opinion is divided on the subject" -- Captain Rum, Blackadder, "Potato".
The use with SSL is what Anne|Lynn Wheeler refer to as "certificate
manufacturing" (marvellous term).  You send the CA (and lets face it, that's
going to be Verisign) your name and credit card number, and get back a cert.
It's just an expensive way of doing authenticated DNS lookups with a ttl of one
year.  Plenty of PK, precious little I.
You can play with semantics here and claim the exact opposite.  All of the
cases you've cited are actually examples of global distinguisher + locally
unique name.  For example the value 1234567890 taken in isolation could be
anything from my ICQ number to my shoe size in kilo-angstroms, but if you view
it as the pair { ,  } then it makes sense
(disclaimer: I have no idea whether that's either a valid ICQ number or my shoe
size in kilo-angstroms).
(This is very much a philosophical issue.  Someone on ietf-pkix a year or two
 back tried to claim that X.500 DNs must be a Good Thing because RFC 822 email
 address and DNS names and whatnot are hierarchical like DNs and therefore
 can't be bad.  I would suspect that most people view them as just dumb text
 strings rather than a hierarchically structured set of attributes like a DN.
 The debate sort of fizzled out when no-one could agree on a particular view).
I think the unified view is that what you need for a cert is a global
distinguisher and a locally meaningful name, rather than some complex
hierarchical thing which tries to be universally meaningful.  Frequently the
distinguisher is implied (eg with DNS names, email addresses, "for use within
XYZ Copy only", etc), and the definition of "local" really means "local to the
domain specified in the global distinguisher".  I'm not sure whether I can
easily fit all that into the paper without getting too philosophical - it was
really meant as a guide for users of PKI technology.

@_date: 2002-06-18 15:15:06
@_author: Peter Gutmann 
@_subject: Hiding (and Seeking) Messages on the Web 
Does anyone know what sort of hidden terrorist messages Microsoft are
communicating?  Their web pages appear and disappear, and contain nonsensical
phrases and quotations from the Windows documentation.  A Windows icon can
appear in one location one day, and another location the next.  Colours of
icons can change as well.  Messages can be hidden on pages inside Microsoft
sites with no links to them, or placed openly in .HLP files in the Windows
system directory.  The messages and patterns of symbols are given to sysadmins
and programmers to decipher.

@_date: 2002-06-20 21:12:12
@_author: Peter Gutmann 
@_subject: Good quote on biometric ID 
I was reading a late-70's paper on computer security recently when I saw that
it contains a nice quote about the futility of trying to use biometrics to
prevent Sept.11-type attacks, I thought I'd share it with people:
  When a highway patrolman is sent to his duty, he has to be given the
  authority to cite traffic violators.  This cannot be done explicitly for each
  violator because at the time that the patrolman is sent to his duty, the
  traffic violator does not exist, and the identity of the future violators is
  not known, so that it is impossible to construct individual access rights for
  the violators at that time.  The point is that the patrolman's authority has
  to do with the behaviour of motorists, not their identity.
  - Naftaly Minsky, "An Operation-Control Scheme for Authorisation in Computer
    Systems", International Journal of Computer and Information Sciences,
    Vol.2, No.2, June 1978, p.157.

@_date: 2002-06-25 16:56:21
@_author: Peter Gutmann 
@_subject: Steven Levy buys Microsoft's bullshit hook, line, and sinker 
I think a major contributing factor is TCPA's history.  It's the product of a
bunch of failed security initiatives by a collection of hardware vendors,
dating back to HP's ICF from 1996 (can anyone else even remember what ICF was,
without looking it up)?  Then there's CDSA, and IBM's experiement with smart
cards embedded in motherboards... a ton of vendors with completely different
objectives and a pile of leftover projects which failed to take off when they
weren't called TCPA yet.  Is it even worth wasting cycles on speculating where
TCPA will end up?

@_date: 2002-06-28 02:15:57
@_author: Peter Gutmann 
@_subject: Revenge of the WAVEoids: Palladium Clues May Lie In AMD Motherboard Design 
Think of it as DIVX for PCs, with a similar chance of success (see my earlier
post about TCPA being a dumping ground for failed crypto hardware initiatives
from various vendors).  Its only real contribution is that the WAVEoid board on
Ragingbull (alongside the Rambus one) is occasionally amusing to read, mostly
because it shows that the dot-com sharemarket situation would be better
investigated by the DEA than the FTC.

@_date: 2002-05-29 18:05:17
@_author: Peter Gutmann 
@_subject: PKI: Only Mostly Dead 
"R. A. Hettinga"  quotes:
Actually it's not quite that bad.  I have a paper "PKI: It's Not Dead, Just
Resting" (no relation to the article, despite the name) which takes a
(hopefully) somewhat detached look at PKI issues and how they can be addressed,
covering (as far as possible within the 15-page limit) the X.509 and PGP
approaches, as well as the other usual suspects like AADS, XML/SAML, SPKI, and
so on, as well as some areas which nothing seems to be doing at the moment -
it's an attempt to do a grand unified view of PKI without ending up with a
whole book.  I've also tried to throw in a reasonable amount of historical
perspective to explain why some (mostly X.509) things are done the way they
are.  It may or may not appear in ;login, the Usenix journal, at some point,
although I haven't heard anything for awhile.  It's available from
 (zipped PDF) for anyone
who's interested.  I wouldn't link to it at the moment because of its current
in-limbo status, once it's officially published somewhere I'll add a link from
my home page.

@_date: 2002-11-07 15:07:21
@_author: Peter Gutmann 
@_subject: Did you *really* zeroize that key? 
No it isn't.  This was done to death on vuln-dev, see the list archives for
the discussion.
[Moderator's note: I'd be curious to hear a summary -- it appears to
work fine on the compilers I've tested. --Perry]

@_date: 2002-11-08 17:40:02
@_author: Peter Gutmann 
@_subject: Did you *really* zeroize that key? 
You can't, in general, assume the compiler won't optimise this away
(it's just been zeroised, there's no need to check for zero).  You could make it volatile *and* do the check, which should be safe from being optimised.
It's worth reading the full thread on vuln-dev, which starts at
This discusses lots of fool-the-compiler tricks, along with rebuttals
on why they could fail.

@_date: 2002-11-09 16:55:49
@_author: Peter Gutmann 
@_subject: did you really expunge that key? 
The majority of them.
Win32 via ReadProcessMemory.  Most Linux systems which set up the user as root
when they install the OS.  The combined total would be what, 97%? 98%? 99%? of
the market?
Watson under Win32, any Unix system with poor file permissions (which means a
great many of them).  Again, that's most of the market.
This *is* a serious issue, which is why any security software worth its salt
takes care to zeroise memory after use.

@_date: 2002-11-09 18:48:14
@_author: Peter Gutmann 
@_subject: did you really expunge that key? 
Almost all Win32 systems (except for a few Citrix-style systems) are single-
user, so the check is irrelevant.  Even if it's running in a different user
context, for Win9x systems that's meaningless, and for NT systems it's pretty
safe to assume the user is Admin so you can get to anything anyway.
The problem is someone running a program 3 days later and finding keys in
memory, not active attacks.
It can do a pretty good job of it.  Zeroising a key after use on a system
which isn't currently thrashing gives you a pretty good chance of getting rid
of it.
(Yes, you can hypothesise all sorts of weird places where data could end up if
 you're not careful, but to date multiple demonstrated attacks have pulled
 plaintext keys from memory where they were left by programs, and not from
 keyboard device driver buffers or whatever).
Right, so we'll just given up even trying then, and wait for the day when
secure systems are readily available.
Nope.  NT (not Win9x) has VirtualLock(), but there are special issues
surrounding this which are too complex to go into here, and Unix doesn't have
anything (mlock() won't cut it).
BTW I misattributed the previous message in my reply (I'm posting from another
system and had to manually edit the reply), apologies for any confusion this

@_date: 2002-11-21 20:34:10
@_author: Peter Gutmann 
@_subject: Digital signature legislation tutorial posted 
I've recently revamped part 2 of my Godzilla security tutorial, splitting off
the coverage of digital signature legislation and related issues into its own
section.  Part2a, consisting of a total of 79 slides, covers the question of
why we need digital signature legislation, what is a signature, paper
vs.electronic signatures, non-repudiation, trust, and liability, existing
approaches, examples of existing legislation of various types including
advantages and drawbacks, and the Digital Signature Law litmus test (and it
also explains why having a techie comment on legal issues isn't as silly as it
sounds :-).  It's available as part 2a of the Godzilla tutorial at
  Comments welcome

@_date: 2002-11-26 18:22:56
@_author: Peter Gutmann 
@_subject: Biometrics "isn't for security--it's for convenience" 
There's a good retrospective on biometrics by Jim Wayman (former director of
the U.S. Biometrics Center) in this month's Info Security Magazine, available
at   "It's going to be hard to know how these technologies can be applied to
  increase national security. They might be an added tool, but it will require
  a lot more human intervention. We're not just going to turn these machines
  on and start catching terrorists," he says.
  Because he works primarily for the government, Wayman says he was irritated
  with the way some biometrics vendors tried to capitalize on Sept. 11 by
  suggesting their technology could have prevented the terrorist attacks.
  "No, the government didn't have this stuff in place, precisely because it
  had been working on it and knew its limitations and didn't find any value
  for the costs involved. The government has been on top of this; the
  government's position hasn't changed," he says.
It's nice to see an honest, commonsense assessment like this instead of the
usual biometrics-will-solve-everything stuff (after Sept.11 I was contacted by
a number of biometrics-peddlers, including one who got quite nasty when I
suggested that his wares weren't the panacea he seemed to think they were).

@_date: 2002-10-06 17:33:39
@_author: Peter Gutmann 
@_subject: Interesting KPMG report on DRM 
KPMG have a report "The Digital Challenge: Are You Prepared?" available at
 in which they surveyed execs at
media companies and conclude that they're focusing too much on (trying to)
lock up content using encryption rather than how to do something useful with
  Digital content is getting a lot of attention - but not at the board level,
  where it is urgently needed. As a recent KPMG survey of top executives
  shows, media companies are focusing too much on encryption and other
  defensive technologies while failing to develop proactive strategies that
  recognize and leverage their online intellectual property assets.
  But the industry.s efforts to grapple with losses on this scale by locking
  away content behind multiple layers of protection - whether encryption,
  copyright protection, or authentication - have tended to detract from the
  user experience while failing to deliver the hoped-for revenue streams.
  Indeed, for all the publicity, expert attention, and corporate ingenuity
  devoted to digital piracy, it is striking that global content companies have
  not yet been able to find a working solution.
  This white paper, organized around a survey conducted for KPMG by The
  Economist Intelligence Unit, takes the industry.s pulse on The bottom line
  is that media companies need to shift their focus from a circle-the-wagons
  defense of digital intellectual property to innovative strategies for
  managing online content as a core revenue source. To achieve this shift,
  digital intellectual property needs to be valued properly, just like other
  assets on the balance sheet. Also, its protection needs to be treated as a
  key issue of corporate governance and given sustained and dedicated board-
  level attention.
  It is clear from the survey that media executives are trying to remain
  optimistic about the potential of digital content - but securing
  intellectual property rights is an uphill battle. In the quest for the right
  mix of measures to fight piracy, executives are relying heavily on
  encryption as well as reactive steps to police and punish violators. At the
  same time, however, many companies fail to conduct systematic accounting for
  their digital assets, or to pursue more proactive strategies to build new
  revenue streams from their online content.
  Media companies have so far failed to pioneer new business models that would
  rob piracy of its appeal. Preoccupied with defending the barricades against
  pirates, the industry has shown a deficit of creativity and innovation in
  rolling out products and services that can compete with the pirates. This
  was clear in KPMG.s survey, where only a handful of respondents saw offering
  potential abusers the chance to distribute content legally as a way of
  protecting digital intellectual property.
  In addition, the content industry remains hostage to its own strict
  interpretations of copyright laws and definitions of intellectual property.
  Most leading media organizations have their roots in traditional media
  formats - they still consider every bit of content they produce to be
  subject to copyright and they defend it - tooth and nail. However, today.s
  Internet world conflicts with this business model, as consumers expect more
  fluid boundaries and demand a free flow of information.
Good stuff, read the whole thing at

@_date: 2002-10-31 20:41:45
@_author: Peter Gutmann 
@_subject: Windows 2000 declared secure 
Microsoft Windows 2000 Awarded Common Criteria Certification
Tuesday October 29, 2:00 pm ET
Achieves Highest Level of Security Evaluation for the Broadest Set of Real-
  World Scenarios
Microsoft Corp. (Nasdaq: MSFT - News) today announced that its Windows 2000
platform has been awarded the Common Criteria certification for the broadest
set of real-world scenarios yet achieved by any operating system as defined by
the Common Criteria for Information Technology Security Evaluation (CCITSE).
The Common Criteria (CC) certification is a globally accepted standard for
evaluating the security features and capabilities of information technology
Microsoft Says Windows 2000 Passes Security Check
Tue Oct 29, 8:23 PM ET
By Elinor Mills Abreu
SAN FRANCISCO (Reuters) - Microsoft Corp. (NasdaqNM:MSFT - news) said on
Tuesday that Windows 2000 (news - web sites) has received the highest level of
security evaluation of any commercial operating system, an important benchmark
for government and other contracts.
 NOT TESTING FOR FLAWS
"This type of testing isn't testing for flaws," said John Pescatore, an
analyst at Gartner Inc. "It's more testing whether we can believe the claims
the operating system is making for the security functions it provides."
Alan Paller, research director at the System Administration, Networking and
Security Institute, agreed.
"It doesn't mean anything for the users. Right now, it's a relatively pure
marketing program for the vendors," Paller said. "They still deliver the
software misconfigured and with flaws."

@_date: 2002-09-17 16:00:41
@_author: Peter Gutmann 
@_subject: Cryptogram: Palladium Only for DRM 
There was a rather nice paper at Usenix Security 2000 on this [pause]
available from

@_date: 2002-09-21 14:47:51
@_author: Peter Gutmann 
@_subject: Sun donates elliptic curve code to OpenSSL? 
Could someone with legal know-how translate whatever it is this is saying into

@_date: 2002-09-23 17:04:09
@_author: Peter Gutmann 
@_subject: Sun donates elliptic curve code to OpenSSL? 
Doesn't this exclude it from being used in OpenSSL, since it violates the
 * The licence and distribution terms for any publically available version or
 * derivative of this code cannot be changed.  i.e. this code cannot simply be
 * copied and put under another distribution licence
 * [including the GNU Public Licence.]

@_date: 2002-09-25 18:56:04
@_author: Peter Gutmann 
@_subject: FIB workstation photos 
As part of its tour of Nvidia, Anandtech got to look at an FIB workstation of
the kind used for (among other things) reverse-engineering and modifying
semiconductors.  For those who have never seen one of these things, there are
photos at

@_date: 2002-10-01 13:54:30
@_author: Peter Gutmann 
@_subject: Real-world steganography 
I recently came across a real-world use of steganography which hides extra
data in the LSB of CD audio tracks to allow (according to the vendor) the
equivalent of 20-bit samples instead of 16-bit and assorted other features.
According to the vendors, "HDCD has been used in the recording of more than
5,000 CD titles, which include more than 250 Billboard Top 200 recordings and
more than 175 GRAMMY nominations", so it's already fairly widely deployed.
Hidden Code Addition/Output Dither/Quantization
The final step in the reduction to 16 bits is to add high-frequency weighted
dither and round the signal to 16-bit precision. The dither increases in
amplitude in the frequency range of 16 to 22.05 kHz, leaving the noise floor
flat below 16 kHz where the critical bands of hearing associated with tonality
occur. As part of the final quantization, a pseudo-random noise hidden code is
inserted as needed into the least significant bit (LSB) of the audio data. The
hidden code carries the decimation filter selection and Peak Extend and Low
Level Range Extend parameters. Inserted only 2?5 percent of the time, the
hidden code is completely inaudible-effectively producing full 16-bit
undecoded playback resolution. The result is an industry-standard 44.1-kHz,
16-bit recording compatible with all CD replication equipment and consumer CD
The paper describing the process is available under the somewhat misleading
name   The description of
the stego en/decoding process is on p.15 (it's a rather long excerpt, but it's
interesting stuff):
As part of the final quantization, a hidden code side channel is inserted into
the LSB when it is necessary for the encoder to inform the decoder of any
change in the encoding algorithm. It takes the form of a pseudo-random noise
encoded bit stream which occupies the least significant bit temporarily,
leaving the full 16 bits for the program material most of the time. Normally,
the LSB is used for the command function less than five percent of the time,
typically only one to two percent for most music. Because the hidden code is
present for a small fraction of the time and because it is used as dither for
the remaining 15 bits when it is inserted, it is inaudible. This was confirmed
experimentally with insertion at several times the normal fraction of time.
The mechanism which allows insertion of commands only when needed consists of
encapsulating the command word and parameter data in a "packet". A
synchronizing pattern is prepended to the data and a checksum is appended. The
resulting packet is then scrambled using a feedback shift register with a
maximal length sequence and inserted serially, one bit per sample, into the
LSB of the audio data. The decoder sends the LSB's of the audio data to a
complementary shift register to unscramble the command data. A pattern
matching circuit looks for the synchronizing pattern in the output of the
descrambler, and when it finds it, it attempts to recover a command. If the
command has a legal format and the checksum matches, it is registered as a
valid packet for that channel. The arrival of a valid packet for a channel
resets a code detect timer for that channel. If both channels have active
timers, then code is deemed to be present and the filter select data is
considered valid immediately. However, any command data which would effect the
level of the signal must match between the two channels in order to take
effect. The primary reason for this is to handle the case where an error on
one channel destroys the code. In such a case, the decoder will mistrack for a
short time until the next command comes along, which is much less audible than
a change in gain on only one channel, causing a shift in balance and lateral
image movement. If either of the code detect timers times out, then code is
deemed not to be present, and all commands are canceled, returning the decode
system to its default state. If the conditions on the encoder side are not
changing, then command packets are inserted on a regular basis to keep the
code detect timers in the decoder active and to update the decoder if one
starts playing a selection in the middle of a continuous recording.
Since the decoder is constantly scanning the output of the de-scrambler shift
register for valid command packets even when none are present, the possibility
exists that there may be a false trigger. For audio generated by the encoder,
this possibility is eliminated in the absence of storage and transmission
errors by having the encoder scan the LSB of the audio data looking for a
match. If a match to the synchronizing pattern is found, the encoder inverts
one LSB to destroy it.
Modern digital storage and transmission media incorporate fairly sophisticated
error detection and correction systems. Therefore, we felt that only moderate
precautions were necessary in this system. The most likely result of an error
in the signal is a missed command, which can result in a temporary mis-
tracking of the decoding, as mentioned above. Given the low density of command
data, and the small changes to the signal which the process uses, these errors
are seldom more audible than the error would be in the absence of the process.
The chances of a storage error being falsely interpreted as a command are
extremely small.
For material not recorded using the encoder, a small probability for a false
trigger does exist. Given a moderate length for the scrambling shift register
so that its mapping behaves in a noise-like fashion and a choice of
synchronizing pattern which avoids patterns likely to appear in audio data
with a higher than average probability, susceptibility to false triggers can
be made arbitrarily small by increasing the length of the part of the packet
requiring a match. In the case of the current system, the combination of the
synchronizing pattern with the bit equivalence for all valid commands plus
check sum results in a required match equivalent to 39 sequential bits. For a
stereo signal, in which a match must occur in both channels within a one
second interval and the commands in both channels must specify the same gains,
this amounts to an expectation of one event in approximately 150 million years
of audio.
The scrambling operation uses a feedback shift register designed for a maximal
length sequence in which data taken from taps in the register are added using
modulo two arithmetic, equivalent to an 'exclusive or' operation, and fed back
to the input of the register. For a given register length there are certain
configurations of taps which will produce a sequence of one and zero values at
the output that does not repeat until 2N-1 values have emerged, where N is the
length of the shift register. This corresponds to the number of possible
states of the shift register minus one illegal state, and is called a maximal
length sequence. Such an output sequence has very noise-like properties and,
in fact, is the basis of some noise generators. We use the noise-like behavior
of the generator to scramble the command signals by adding them modulo two to
the input of the shift register, as for example in Figure 7a. This has the
advantage that a second similar shift register with taps in the same places
but with only feed forward addition modulo two (Figure 7b) will reproduce the
original input sequence when fed with the output of the first one. The fact
that the decode side has no feedback means that the initialization
requirements are limited to having N input samples prior to the beginning of
decoding, which means that the decoder will "lock up" very quickly. In this
scheme, the presence of a bit error anywhere in the length of a packet plus
initialization sequence will completely scramble the data, preventing
recovery. However, in practice, this has not been a problem for reasons
described above.

@_date: 2003-04-03 13:58:44
@_author: Peter Gutmann 
@_subject: TPM coming to Canada 
A better suggestion for killing it: Write letters strongly encouraging the
PKI-based method.

@_date: 2003-04-09 04:29:47
@_author: Peter Gutmann 
@_subject: aural cryptography 
There's an RFID technique that relies on having two fairly narrow RF beams and
you have to place the token at the intersection point of the two in order to
get anything out of it.  I don't know whether it uses interference of the
signals or the fact that you have to be receiving boths signals
simultaneously, it's been awhile since I read about it.  The technique goes
back to at least WWII and the German Knickebein target-location system,
although that one was definitely something based on processing two discrete
signals in a modification of the Lorentz landing aid system, and not any
property of interference between the two signals.

@_date: 2003-04-09 04:39:18
@_author: Peter Gutmann 
@_subject: Via puts RNGs on new processors 
It's not really "including a hardware RNG", that appears to be the sum total
of the crypto support, at least as far as anyone's been able to find out.  The
exact details of using the hardware RNG are only available under NDA from Via,
although anyone who can somehow get their hands on an appropriate Nehemiah-
core C3 to test with and the Cryptography Research tech report should be able
to put something together.
(Anyone have a Nehemiah?)

@_date: 2003-04-09 11:57:19
@_author: Peter Gutmann 
@_subject: Via puts RNGs on new processors 
Presumably we'd see this as a standard part of the POST (power-on self-test)
option in Nehemiah-aware BIOSes, just as various other CPU-specific features
are managed by specific BIOSes.  There'd also be a "Continue anyway if TRNG
test fails" option, enabled by default so as not to inconvenience users.

@_date: 2003-04-11 17:28:20
@_author: Peter Gutmann 
@_subject: Via puts RNGs on new processors 
t.c.jones at att.net top-quotes:
My main point wasn't where it's done, it was that users don't care whether it
passes some test or not, they just want to use it, so whether a failure is
ignored in the POST or later on doesn't really matter.  From the vendors'
point of view, would you want to present your users with a message "You can't
log on to your corporate network/read mail/pay a bill/$DO_YOUR_JOB because
murfle burble tech-jargon mumble mutter"?  Of course not, you record a
suitably vague warning in the syslog somewhere and continue anyway, falling
back to a software-only RNG, or possibly just rand().
A more succint way of putting that is: If you have something whose failure
will prevent users from using their machine, you'd better either make sure it
never fails or you have a very good returns policy.  If it's something whose
failure appears to the users as an otherwise functional unit that refuses to
cooperate, you'd better make damn sure it never fails.

@_date: 2003-12-05 14:15:36
@_author: Peter Gutmann 
@_subject: Open Source Embedded SSL - (License and Memory) 
How common is it to find an embedded system sophisticated enough to have a TCP
stack and ethernet interface (and running SSL), but not sophisticated enough
to have a malloc() implementation?  I've always assumed that anything with the
former will also have the latter (I know there are some highly constrained
embedded platforms used in some web-enabled widgets, but they usually don't
run SSL).  My code will run without malloc() in general, but assumes that if
you have a TCP stack and ethernet then you're also going to have malloc()
support.  For the non-malloc platforms, it's written to grab memory in a FIFO
manner, so you can get away with a simple brk-style allocator.

@_date: 2003-12-07 19:00:44
@_author: Peter Gutmann 
@_subject: Additional Proposed Hash Function (Forwarded) 
There'd already been a similar discussion on this topic on another list, every
security standard and protocol already provides for generating 3DES keys via
its PRF of choice, so why create yet another new hash variant for it?  All it
does is create extra complexity for implementors (let's see, was that
SSL_RSA_WITH_3DES_EDE_CBC_SHA, SSL_RSA_WITH_3DES_EDE_CBC_SHA224,
SSL_RSA_WITH_3DES_EDE_CBC_SHA256_WITH_BITS_MISSING, or
SSL_RSA_WITH_3DES_EDE_CBC_SHA384_FOLDED_IN_HALF?).  As a result, it's in
danger of ending up with a de facto profile of "ignore it".  In terms of
implementations of standard protocols (TLS, SSH, S/MIME, PGP, IPsec), I can't
see any use for it that'd justify the complexity and overhead of adding it to
an implementation.

@_date: 2003-12-15 23:46:07
@_author: Peter Gutmann 
@_subject: PKI root signing ceremony, etc. 
Not necessarily.  I looked at this in an ACSAC'2000 paper (available from
  This uses a TP-capable database
as its underlying engine, providing the necessary auditing capabilities for
all CA operations.  This was desgined to meet the security/auditing
requirements in a number of PKI standards (see the paper for full details,
I've still got about 30cm of paper stacked up somewhere from this).  The paper
is based on implementation experience with cryptlib, you can't do anything
without generating an audit trail provided you have proper security on the TP
system (that is, a user can't inject arbitrary transactions into the system or
directly access the database files).  I tested the setup by running it inside
a debugger and resetting/halting the program at every point in a transaction,
and it recovered from each one.  It can be done, it's just a lot of work to
get right.
I should mention after having done all that work that most CAs rely on
physical and personnel security more than any automatic logging/auditing.
Take a PC and an HSM, lock it in a back room somewhere, and declare it a
secure CA.

@_date: 2003-12-19 15:34:28
@_author: Peter Gutmann 
@_subject: Difference between TCPA-Hardware and a smart card (was: example: secure computing kernel needed) 
Conversation from a few years ago, about multifunction smart cards:
 -> Multifunction smart cards are great, because they'll reduce the number of
    [smart] cards we'll have to carry around.
 <- I'm carrying zero smart cards, so it's working already!

@_date: 2003-12-19 15:48:34
@_author: Peter Gutmann 
@_subject: Difference between TCPA-Hardware and other forms of trust 
That gobbledigook sounds like Common Criteria-speak.  So it's not deliberate,
it's a side-effect of making it CC-friendly.
Yup, that's definitely Common Criteria.

@_date: 2003-12-25 14:29:39
@_author: Peter Gutmann 
@_subject: Ousourced Trust (was Re: Difference between TCPA-Hardware and a smart card and something else before 
X.509 certs were designed to solve the problem of authenticating users to the
global X.500 directory.  So they're good at what they were designed for
(solving a problem that doesn't exist [0]), and bad at everything else
(solving any other sort of problem).
[0] Actually they're adequate at what they were designed for.  The original
    directory authentication work was really just a bunch of suggestions as to
    how you'd do it, ranging from passwords through to certs, and a lot of the
    cert stuff was more a set of suggestions than any firm guideline.

@_date: 2003-12-28 02:07:29
@_author: Peter Gutmann 
@_subject: Ousourced Trust (was Re: Difference between TCPA-Hardware and a smart card and something else before 
That's my big gripe with OCSP, it's compromised in almost every way in order
to make it completely bug-compatible with CRLs.  It's really mostly an online
CRL query protocol rather than any kind of status protocol (in other words a
responder can give you an, uhh, "live" response from a week-old CRL via OCSP).
A recent update to the protocol even removes the use of nonces, to make replay
attacks possible.

@_date: 2003-12-29 16:05:02
@_author: Peter Gutmann 
@_subject: stego in the wild: bomb-making CDs 
The story could apply to any one of hundreds (thousands?) of hacker/warez CDs
available off-the-shelf in the US.  Heck, it could apply to the Encyclopedia Britannica CD edition.  So I'd pick:
sounds a lot better than:

@_date: 2003-12-29 16:19:57
@_author: Peter Gutmann 
@_subject: Non-repudiation (was RE: The PAIN mnemonic) 
This refers to the second (and IMHO more sensible) use of the X.509
nonRepudiation bit, which uses digitalSignature for short-term signing (e.g.
user authentication) and nonRepudiation for long-term signing (e.g. signing
a document).  The other definition uses digitalSignature for everything,
and nonRepudiation as an additional service on top of digitalSignature.  The
problem with that definition is that no two people in the X.509 world can
agree on what nonRepudiation actually signifies.  The best suggestion I've
seen for the nonRepudiation bit is that CAs should set it to random values
to disabuse users of the notion that it has any meaning.  For the
"additional-service" definition of nonRepudiation, the X.509 Style Guide   Although everyone has their own interpretation, a good practical definition   is "Nonrepudiation is anything which fails to go away when you stop   believing in it".  Put another way, if you can convince a user that it isn't   worth trying to repudiate a signature then you have nonrepudiation.  This   can take the form of having them sign a legal agreement saying they won't   try to repudiate any of their signatures, giving them a smart card and   convincing them that it's so secure that any attempt to repudiate a   signature generated with it would be futile, threatening to kill their kids,   or any other method which has the desired effect.  One advantage (for   vendors) is that you can advertise just about anything as providing   nonrepudiation, since there's sure to be some definition which matches   whatever it is you're doing (there are "nonrepudiation" schemes in use today   which employ a MAC using a secret shared between the signer and the verifier,   which must be relying on a particularly creative definition of   nonrepudiation).

@_date: 2003-02-05 17:08:17
@_author: Peter Gutmann 
@_subject: The Crypto Gardening Guide and Planting Tips 
After much procrastination I recently put the Crypto Gardening Guide and
Planting Tips online at
 this may be of
interest to readers.  From the introduction:
  There has been a great deal of difficulty experienced in getting research
  performed by cryptographers in the last decade or so (beyond basic
  algorithms such as SHA and AES) applied in practice.  The reason for this is
  that cryptographers don't work on things that implementors need because it's
  not cool, and implementors don't use what cryptographers design because it's
  not useful or sufficiently aligned with real-world considerations to be
  practical. As a result, security standards are being created with mechanisms
  that have had little or no security analysis, often homebrew mechanisms or
  the standards editor's pet scheme.  The problem is a lack of communication:
  Cryptographers often don't seem aware of the real-world constraints that
  their design will need to work within in order to be successfully deployed.
  The intent of this document is to cover some of those real-world constraints
  for cryptographers, to point out problems that their designs will run into
  when attempts are made to deploy them.  Also included is a motivational list
  of extremely uncool problems that implementors have been building ad-hoc
  solutions for since no formal ones exist.

@_date: 2003-02-22 18:19:34
@_author: Peter Gutmann 
@_subject: [Bodo Moeller <bodo@openssl.org>] OpenSSL Security Advisory: Timing-based attacks on SSL/TLS with CBC encryption 
An extremely trivial observation, but may be useful to some:
There's been a discussion about how this affects POP over SSL on a private
list.  My suggestion was:

@_date: 2003-01-22 15:18:34
@_author: Peter Gutmann 
@_subject: deadbeef attack was choose low order RSA bits (Re: Key Pair Agreement?) 
That way's trivially detectable by inspection of the private key (which
admittedly isn't a problem in this case because you're not trying to hide its
presence).  More challenging though are ways of embedding a fixed pattern that
isn't (easily) detectable, a la various ways of leaking information in the
public key such as SETUP attacks.

@_date: 2003-01-25 21:54:13
@_author: Peter Gutmann 
@_subject: Verizon must comply with RIAA's DMCA subpoena 
Lucky Green a while back reported that some European ISPs charge customers
less if they use IPsec because then there's less cost involved in complying
with surveillance requirements.

@_date: 2003-01-31 15:18:44
@_author: Peter Gutmann 
@_subject: Sovereignty issues and Palladium/TCPA 
It looks like Palladium (or whatever it's called this week) is of concern not
just to individuals but to governments as well (the following text forwarded
from elsewhere):

@_date: 2003-07-01 21:01:40
@_author: Peter Gutmann 
@_subject: Attacking networks using DHCP, DNS - probably kills DNSSEC 
Given that their goal is zero-configuration networking, I can see that being
required to provide a shared secret would mess things up a bit for them.  It'd
be a bit like PKIX being asked to make ease-of-use a consideration in their
work, or OpenPGP to take X.509 compatibility into account.

@_date: 2003-07-11 00:55:48
@_author: Peter Gutmann 
@_subject: SSL 
There are two good books on SSL, which complement each other:
  "SSL & TLS Essentials", which is more or less an extended-precision version
  of the RFC with a more detailed explanation of everything, lots of TCP-RFC-
  style packet diagrams that aren't in the original RFC (that alone's worth
  the cost of the book), etc etc.
  "SSL and TLS: Designing and Building Secure Systems", which is all the
  background info for SSL that you can't get anywhere else, along with
  information about protocol quirks, bugs, implementation issues, etc etc.
If you want to use SSL (rather than implement it yourself from scratch) I'd
get "SSL and TLS", but if you can afford it I'd recommend getting both,
they're both very worthwhile books.  If only there were books like this
(targeted at crypto people, with all the nuts-and-bolts details) for IPsec or
If you don't specifically need OpenSSL, you could always look at cryptlib,
 which is Windows-
friendly if that's what you're after, and has an SSL implementation among a
ton of other stuff.  Alternatively, if you *really* want that Windows
functionality, there's always CryptoAPI :-).

@_date: 2003-07-13 17:16:44
@_author: Peter Gutmann 
@_subject: traffic analysis of phone calls? 
A friend of mine who used to work for a large telco ended up being delegated
to attend some of the CALEA meetings.  He reports that the FBI were totally
unable to comprehend that if they built a system full of easily-accessible
backdoors (pushbutton access to anything anytime), anyone with the necessary
know-how could also use those backdoors, and since the CALEA monitoring system
didn't appear to have been designed with security in mind (and as Cringely's
article points out, that obviously got carried through to the final design),
it would be possible to watch the watchers.  Sort of like assuming that when
you shoot at the bad guys they go down, but when they shoot back the bullets
bounce off.
(I think this was a manifestation of a generic problem with nontechnical
 decision-makers, the FBI has very clueful technical people, but the ones who
 got sent to the CALEA meetings were nontechnical people armed with wish-
 lists rather than techies armed with clues).

@_date: 2003-07-25 20:03:29
@_author: Peter Gutmann 
@_subject: S.E.E. PKI Paper 14 - International and New Zealand PKI experiences across government 
I thought the following might interest readers, it's an examination of PKI
experiences in various parts of the world (Australia, Finland, Germany, Hong
Kong, New Zealand, US).  It reads best to the sound of an Alka-Seltzer fizzing
in a glass :-).

@_date: 2003-06-03 03:54:53
@_author: Peter Gutmann 
@_subject: Maybe It's Snake Oil All the Way Down 
Actually SSHv2 is just SSL with a different packet format (when I did my SSHv2
implementation I recycled the code from the SSL engine, it was that close
[0]).  That's probably a good indication that SSL/SSHv2 is a fairly optimal
(security/functionality/implementability/etc) design for an application-level
security protocol if two groups independently came up with the same design,
which brings us back the original question of why on earth Nullsoft tried to
roll their own.
[0] Note that my SSL implementation follows the standard SSL ladder diagram
    rather than the state-machine that SSL implementations are usually
    described as, which made it trivial to switch over for SSHv2 use.  I've
    never understood why every explanation of the SSL protocol I've ever seen
    uses ladder diagrams but once they talk about implementation details they
    assume you're doing it as a state machine, which makes it vastly harder to
    implement.  For example all the stuff about pending cipher suites and
    whatnot follows automatically (and transparently) from the ladder diagram,
    but is a real pain to sort out in a state machine.

@_date: 2003-06-04 01:04:39
@_author: Peter Gutmann 
@_subject: Maybe It's Snake Oil All the Way Down 
That's a red herring.  It happens to use X.509 as its preferred bit-bagging
format for public keys, but that's about it.  People use self-signed certs,
certs from unknown CAs [0], etc etc, and you don't need certs at all if you
don't need them, I've just done an RFC draft that uses
shared secret keys for mutual authentication of client and server, with no
need for certificates of any kind, so the use of
certs, and in particular a hierarchical PKI, is merely an optional extra.
It's no more required in SSL than it is in SSHv2.
They spend a nontrivial portion of the book reinventing SSL/SSHv2.  I guess
they lean towards the roll-your-own side of the argument :-).  I'm firmly in
the opposite camp (see "Lessons Learned in Implementing and Deploying Crypto
Software", links off my home page at I think that providing an abstract description of a fairly complex security
protocol *in a book targeted at security novices* and then hoping that they
manage to implement it correctly is asking for trouble.  OTOH it's fun going
through the thought processes involved in designing the protocol.  I just wish
they'd applied the process to SSL or SSHv2 instead, so that at the end of it
they could tell the reader to go out and grab an implementation that someone
else has got right for them.
[0] The vendor of one widely-used MTA once told me that 90% of the certs they
    saw used in STARTTLS applications were non-big name CA-issued ones (self-
    signed, etc etc).

@_date: 2003-06-04 01:11:51
@_author: Peter Gutmann 
@_subject: Maybe It's Snake Oil All the Way Down 
Naah, that third sentence is wrong.  It's:
  The overwhelming majority of [SSL users] wouldn't know SSL from HTTP with a
  padlock GIF in the corner.
I think the assertion was that SSH is used in places where it matters, while
SSL is used where no-one really cares (or even knows) about it.  Joe Sixpack
will trust any site with a padlock GIF on the page.  Most techies won't access
a Unix box without SSH.  Quantity != quality.
If you could wave a magic wand and make one of the two protocols vanish, I'd
notice the loss of SSH immediately (I couldn't send this message for
starters), but it would take days or weeks before I noticed the loss of SSL.

@_date: 2003-06-04 16:32:23
@_author: Peter Gutmann 
@_subject: Maybe It's Snake Oil All the Way Down 
There's a two-level answer to this problem.  At an abstract level, doing
client certs isn't hard, there are various HOWTOs around for Apache, Microsoft
have Technet/MSDN papers on it for IIS, etc etc.  At a practical level, it's
almost never used because it's just Too Hard.  That's not the SSL client-cert
part, it's the using-X.509 part.  To save having to type in a long
explanation, I'll lift a representative paragraph from a (not-yet-published,
don't ask :-) paper on PKI usability:
  There is considerable evidence from mailing lists, Usenet newsgroups and web
  forums, and directly from the users themselves, that acquiring a certificate
  is the single biggest hurdle faced by users [1].  For example various user
  comments indicate that it takes a skilled technical user between 30 minutes
  and 4 hours work to obtain a certificate from a public CA that performs
  little to no verification, depending on the CA and the procedure being
  followed.  Obtaining one from non-public CAs that carry out various levels
  of verification before issuing the certificate can take as long as a month.
  A representative non-technical user who tried to obtain an (unverified)
  certificate from a public CA took well over an hour for the process, which
  involved [...] eventually the user gave up.
and that doesn't even get into the mess of managing private keys, handling
revocation, etc etc etc ad nauseum:
  The problems that this creates are demonstrated by what happens when
  technically skilled users are required to work with certificates.  The
  OpenSSL toolkit [2][3] includes a Perl script CA.pl that allows users to
  quickly generate so-called clown suit certificates (ones that 'have all the
  validity of a clown suit' when used for identification purposes [4]), which
  is widely-used in practice.  The cryptlib toolkit [5][6] contains a similar
  feature in the form of Xyzzy certificates (added with some resistance and
  only after the author grew tired of endless requests for it), ones with
  dummy X.500 names, an effectively infinite lifetime, and no restrictions on
  usage.  Most commercial toolkits include similar capabilities, usually
  disguised as 'test certificates' for development purposes only, which end up
  being deployed in live environments because it.s too difficult to do it the
  way X.509 says it should be done.  Certificates used with mailers that
  support the STARTTLS option consist of ones that are 'self-signed, signed-by
  the default Snake Oil CA, signed by an unknown test CA, expired, or have the
  wrong DN' [7].  The producer of one widely-used Windows MUA reports that in
  their experience 90% of the STARTTLS-enabled servers that they encounter use
  self-signed certificates [8].  This reduces the overall security of the
  system to that of unauthenticated Diffie-Hellman key exchange, circa 1976.
  In all of these cases, the entire purpose of certificates has been
  completely short-circuited by users because it.s just too difficult to do
  the job properly.  The problematic nature of X.509 is echoed in publications
  both technical and non-technical, with conference papers and product
  descriptions making a feature of the fact that their design or product works
  without requiring a PKI.  For example, one recent review of email security
  gateways made a requirement for consideration in the review that the product
  'have no reliance on PKI' [9].  As an extreme example of this, the inaugural
  PKI Research Workshop, attended by expert PKI users, required that
  submitters authenticate themselves with plaintext passwords because of the
  lack of a PKI to handle the task [10][11].
The assumption of the protocol's creators was that someone would figure out
how to make X.509 PKI work by the time SSL took off, and everyone would have
their own certificates and whatnot.
At least they got *most* of the design right :-).

@_date: 2003-06-05 03:24:44
@_author: Peter Gutmann 
@_subject: Maybe It's Snake Oil All the Way Down 
A debate topic I've thought of occasionally in the last year or two: If
digital signatures had never been invented, would we now be happily using
passwords, SecurIDs, challenge-response tokens, etc etc to do whatever we need
rather than having spent the last 20-odd years fruitlessly chasing the PKI
dream?  There was some interesting work being done on non-PKI solutions to
problems in the 1970s before it all got drowned out by PKI, but most of it
seems to have stagnated since then outside a few niche areas like wholesale
banking, where it seems to work reasonably well.
(Hmm, now *that* would make an interesting panel session for the next RSA
 conference).

@_date: 2003-06-05 22:11:45
@_author: Peter Gutmann 
@_subject: Maybe It's Snake Oil All the Way Down 
Is there some specific advantage here, or is it an academic exercise?  Some
quirk of supporting certain types of hardware like nCipher boxes that do async
crypto/scatter-gather?  I have a vague idea from discussions with some
OpenSSL-engine developers that they had some requirement for supporting async
hardware in non-threaded environments, but from hearing the complaints about
how hard this ended up being I had the impression that this was a major
rewrite rather than something the state-machine implementation had been
specifically designed for (sorry, I don't have that much technical info, the
discussions tended to devolve into griping sessions about how hard async
crypto hardware was to work with, not helped by comments like "That's because
you're taking the path of most resistance, just use threads" :-).
I also don't know if that explains why, years before this was an issue,
everyone was already treating SSL as a state machine problem.

@_date: 2003-06-07 18:21:52
@_author: Peter Gutmann 
@_subject: M-209 for sale on ebay 
Currently at GBP 280 (~US$500).  Looks like it's in pretty good condition.

@_date: 2003-06-07 18:42:06
@_author: Peter Gutmann 
@_subject: Maybe It's Snake Oil All the Way Down 
That doesn't really have anything to do with ASN.1 though.  You can make just
as big a mess with XML (actually even bigger, in my experience), or EDIFACT,
or whatever.  The problem isn't the bit-bagging format, it's that it's
accumulated such a mass of cruft that no two people can agree on what to put
in there.  Whether the resulting mess is wrapped in ASN.1 or XML or EDIFACT or
plastic pooper scooper bags doesn't really make any difference.

@_date: 2003-06-10 14:21:57
@_author: Peter Gutmann 
@_subject: An attack on paypal --> secure UI for browsers 
Ka-Ping Yee has a web page at  and a lot of interesting
things to say about secure HCI (and HCI in general), e.g. a characterisation
of safe systems vs. general-purpose systems:
  In order for Alice to use her computer usefully, she has to be able to
  instruct programs to do things for her.  In order for those programs to
  carry out tasks, she has to trust those programs with some authority.  So
  every useful operation involves making the system a little bit less safe.
  In order to keep the system from becoming unboundedly unsafe, Alice must
  also be able to make her system more safe.
  A system in an ultimately safe state is one that can't do anything other
  than what was planned ahead of time.  General-purpose computing is useful to
  Alice only because she can make unpredictable inputs into the system, asking
  it to do new things.

@_date: 2003-06-11 01:16:06
@_author: Peter Gutmann 
@_subject: An attack on paypal --> secure UI for browsers 
Maybe MS will implement something like the secure attention key in the old VAX
A1 VMM (Ctrl-Alt-Del already serves this purpose for logins) which gives you a
guaranteed non-spoofed interface to the kernel (see for example "A
Retrospective on the VAX VMM Security Kernel" by Karger et al for more
information on this).  They certainly have the VMS knowhow :-).

@_date: 2003-06-11 14:44:17
@_author: Peter Gutmann 
@_subject: An attack on paypal 
There's a wonderful story at  by someone who
tried to see how b0rken he could make his signature before anyone complained.
He tried at various times a random scribble, a crosshatched scribble, a
regular grid, an 'X', a drawing of a stick-figure person, his name in Egyptian
hieroglyphics, various famous people's names, and even "I stole this card".
No-one ever complained.

@_date: 2003-06-12 16:35:11
@_author: Peter Gutmann 
@_subject: An attack on paypal 
Just to clarify this, so you need a multivalued CN, with one containing the
expression "(a|b|c)" and the remaining containing each of "a", "b", and "c"?
Is it multiple AVAs in an RDN, or multiple RDNs?   (Either of these could be
hard to generate with a lot of software, which can't handle multiple AVAs in
an RDN or multiple same-type RDNs).  Which hack is for MSIE and which is for

@_date: 2003-06-27 14:02:30
@_author: Peter Gutmann 
@_subject: Draft Edition of LibTomMath book 
You have to differentiate between bignum maths libraries and bignum crypto
libraries.  Bignum libraries specifically targeted at crypto use (e.g. bnlib,
the OpenSSL bignum lib) do sanitise memory, it's only general-purpose bignum
libraries that don't (since there's no need to).  I'm not familiar enough with
the GMP source code to know what it does (GMP is a special case, being a
general bignum library but with an implicit acknowledgement that it's going to
end up used for crypto as well, although there are some missing primitives
such as a double-exp mod that would be useful for DSA, hint hint :-).

@_date: 2003-06-28 15:54:51
@_author: Peter Gutmann 
@_subject: Draft Edition of LibTomMath book 
I've seen it used in a couple of lesser-known apps that I played with for
interop testing, nothing that counts as a major app though.  Maybe it's being
used by people who prefer the LGPL to the more widely-used OpenSSL bignum
lib's BSD license (or perhaps it's the fact that GMP has documentation :-).
Ouch!  This is a pity, because GMP seems to have the most active development
in terms of both algorithm optimisation and machine-specific optimisations -
if you want to find a version that runs well on $obscure_embedded_platform,
it's pretty much GMP or nothing.

@_date: 2003-03-18 01:09:44
@_author: Peter Gutmann 
@_subject: Brumley & Boneh timing attack on OpenSSL 
I had blinding code included in my crypto code for about 3 years,
when not a single person used it in all that time I removed it
again (actually I think it's probably still there, but disconnected).
I'm leaning strongly towards "general ignorance" here...

@_date: 2003-05-18 17:50:45
@_author: Peter Gutmann 
@_subject: Payments as an answer to spam (addenda) 
The problem with OCSP is that it's designed to be 100% bug-compatible with
CRLs.  All it is is an online CRL-query mechanism, rather than a true realtime
status mechanism.  In other words if you go to your CA and ask for a status
check, it looks at the 8-hour-old CRL lying around on disk and gives you the
status from that, rather than from a live database.  To do otherwise would be
disrespectful to the memory of OSI.
Now there are some responders that query a live database, but there are
concerns that this will lead to responses that differ from those obtained when
the relying party queries a CRL (you're back to the "bug-compatible with CRLs"
issue again).  In addition there is a larger problem, which is the almost
religous belief in X.509 that you can only return a negative response to a
query, never a positive one.  In other words when fed a freshly-issued
certificate and asked "Is this a valid certificate", OCSP can't say "Yes" (a
CRL can only answer "revoked"), and when fed an Excel spreadsheet it can't say
"No" (the spreadsheet won't be present in any CRL).  By extension if you feed
it a fake certificate it can't tell you it's invalid (it isn't on any CRL),
and many clients will interpret this as an indication that it's currently
valid.  In other words when you feed a fake cert to an OCSP responder (say a
Verisign cert actually issued by Joe Hacker), the result of the OCSP process
is that the client thinks it's valid cert.  The design is a triumph of X.509
theology over practicality.
So I agree with the statement that "OCSP is actually a more timely version of
the paper booklets that were distributed in the 50s & 60s".  A real solution
to the problem would follow the online authorisation model used for financial
transactions, just a straight "Accepted/Declined" response, rather than the
"Maybe/Maybe not" silly-walk that OCSP does.

@_date: 2003-05-19 13:24:31
@_author: Peter Gutmann 
@_subject: Payments as an answer to spam (addenda) 
Anne & Lynn Wheeler  write:
Well, it depends on what it is you're asking.  In abstract terms the relying
party is querying an authority about the validity of some predicate p().  For
a credit card this might be p( can_debit_$1000_from ) (with an implied
expansion to is_account_in_good_standing, account_contains_$1000,
relying_party_is_allowed_to_debit, etc etc).  For an access-control-mechanism
this might be p( is_allowed_access_to ) (with a similar background expansion).
SPKI was an extreme form of this, making the predicates and predicate-
evaluation process very explicit, which from a techie's point of view made it
rather nice to analyse and work with.
In contrast, the only predicate an X.509 certificate can assert is the
tautological p( is_an_X509_certificate ).  So while the CC and access-control
systems are set up to handle dynamic information and thus can answer useful
queries like ones related to current account balances and an ability to pay a
given amount, the only thing you can do with a cert is query whether the
static information that was originally dumped in there is currently still
valid.  So all you can ask is (as you've said above) whether the credential is
still valid, because the design doesn't allow you to do anything else.
Now there does exist a red herring in the form of attribute certificates, but
they're merely an attempt to plaster over the cracks of X.509, and in any case
in their current form are an unproven hypothesis rather than a workable
It wasn't even that, it was originally designed solely for use for user
authentication to the worldwide X.500 directory (something which is very
obvious in the structure of an X.509v1 cert), a problem that never eventuated.
It is quite literally a solution in search of a problem.  The difficulty in
applying it to any pressing real-world problem arises directly from its X.500
Unfortunately any attempts to fix this by switching to practical, widely-used
technology (e.g. dump X.500 DNs as identifiers, use online whitelist checking
instead of offline blacklists, move them around using HTTP instead of
X.500/LDAP, etc etc) so you can actually do something useful with the things,
is met with extraordinary resistance by the people writing the standards.  As
the quote on my home page says: "[PKI designs are based on] digital ancestor-
worship of closed-door-generated standards going back to at least the mid
80's. [...] The result seems to be protocols so convoluted and obtuse that
vendor implementation is difficult/impossible and costly".

@_date: 2003-11-16 20:00:03
@_author: Peter Gutmann 
@_subject: Clipper for luggage 
When true locks are banned, that's actually a rather good protection
mechanism, constituting a type of hashcash for luggage.  Someone who's looking
for targets of opportunity and has a choice between a Clipper-locked container
they can get into almost unnoticed in 5 seconds or something where it'll take
a minute or two of obvious fiddling will presumably go for the Clipper-lock.
Just don't go overboard with those custom foot-long screw machined "locks".

@_date: 2003-11-18 13:43:13
@_author: Peter Gutmann 
@_subject: Partition Encryptor 
Since this was last discussed (without resolution) in alt.security.scramdisk
about a week ago, I'd say the answer is "Probably not".  A better question
would be "Can someone who knows about NT device drivers make the necessary
changes to the code (it's GPL'd and freely available)?".

@_date: 2003-10-01 16:21:33
@_author: Peter Gutmann 
@_subject: Reliance on Microsoft called risk to U.S. security 
This doens't really work.  Consider the simple case where you run Outlook with
'nobody' privs rather than the current user privs.  You need to be able to
send and receive mail, so a worm that mails itself to others won't be slowed
down much.  In addition everyone's sending you HTML-formatted mail, so you
need access to (in effect) MSIE via the various HTML controls.  Further, you
need Word and Excel and Powerpoint for all the attachments that people send
you.  They need access to various subsystems like ODBC and who knows what else
as an extension of the above.  As you follow these dependencies further and
further out, you eventually end up running what's more or less an MLS system
where you do normal work at one privilege level, read mail at another, and
browse the web at a third.  This was tried in the 1970s and 1980s and it
didn't work very well even if you were prepared to accept a (sizeable) loss of
functionality in exchange for having an MLS OS, and would be totally
unacceptable for someone today who expects to be able to click on anything in
sight and have it automatically processed by whatever app is assigned to it.
Even if you could somehow enforce the MLS-style restrictions and convince
people to run an OS with this level of security enabled, the outcome when this
was tried with MLS OSes was that users would do everything possible to bypass
it because it was seen as an impediment to getting any work done: SIGMA
eventually allowed users to violate the *-property to avoid them having to re-
type messages at lower security levels (i.e. it recognised that they were
going to violate security anyway, so it made it somewhat less awkward to do),
Multics and GEMSOS allowed users to be logged in at multiple security levels
to get work done (now add the 1,001 ways that Windows can move data from A to
B to see how much harder this is to control than on a 1970s system where the
only data-transfer mechanism was "copy a file"), KSOS used non-kernel
security-related functions ("kludges") to allow users to violate security
properties and get their work done, etc etc.
One thing that I noticed in the responses to "CyberInsecurity: The Cost of
Monopoly" was that of the people who criticised it as recommending the wrong
solution, no two could agree on any alternative remedy.  This indicates just
how hard a problem this really is...

@_date: 2003-10-02 13:49:00
@_author: Peter Gutmann 
@_subject: Monoculture 
Hmm, I think the size argument is a bit of a red herring - you can strip SSL
and SSH down and run it in remarkably little space (3DES, RSA, SHA-1 and a
static server cert will get you talking to any non-crippled SSL client, for
example).  I've got users running SSL and SSH servers on little 16-bit
embedded systems (alongside the existing app that the SSL or SSH is securing),
and AFAIK their main problem is that doing RSA or DH on the 16-bit CPU isn't
exactly quick.
Peter (still backlogged, if you're waiting for mail please be patient).

@_date: 2003-10-02 14:26:32
@_author: Peter Gutmann 
@_subject: Monoculture 
Let me guess, your background is in software rather than hardware? :-).  Not
all computers are PCs, where you can just drop in another SIMM and the problem
is fixed.  Depending on how you measure it, there are at least as many/many
more embedded systems out there than PCs, where you have X system resources
and can't add any more even if you wanted to because (a) the system is already
deployed and can't be altered, (b) it's cheaper to rewrite the crypto from
scratch than spend even 5 cents (not $1.50) on more memory, or (c) the
hardware can't address any more than the 128K or 512K (64K and 256K 8-bit
SRAMs x 2, the bread and butter of many embedded systems) that it already has.
See above.  This is exactly the situation that embedded-systems vendors find
themselves in (insert tales of phone exchanges built from clustered Z80s
because it's easier to keep adding more of those than to move the existing
firmware to new hardware without the Z80's restrictions, or people being paid
outrageous amounts of money to hand-code firmware for 4-bit CPUs because it's
cheaper than moving everything to 8-bit ones, or ...).
Yup.  I once had a user discuss with me the use of my SSL code in an embedded
application that controlled X.  I was a bit curious as to why they'd bother,
until they explained the scale of the X they were controlling.  If anything
were to go wrong there, it'd be a lot more serious than a few stolen credit
Once you have a general-purpose security tool available, it's going to be used
in ways that the original designers and implementors never dreamed of.  That's
why you need to build it as securely as you possibly can, and once it's done
go back over it half a dozen times and see if you can build it even more
securely than that.

@_date: 2003-10-02 14:37:31
@_author: Peter Gutmann 
@_subject: anonymous DH & MITM 
Uhh, I think that implementations don't support DH because the de facto
standard is RSA, not because of any concern about MITM (see below).  You can
talk to everything using RSA, you can talk to virtually nothing using DH,
RSA is already used as anon-DH (via self-signed, snake-oil CA, expired,
invalid, etc etc certs), indicating that MITM isn't much of a concern for most

@_date: 2003-10-04 17:58:49
@_author: Peter Gutmann 
@_subject: Protocol implementation errors 
I would say the exact opposite: ASN.1 data, because of its TLV encoding, is
self-describing (c.f. RPC with XDR), which means that it can be submitted to a
static checker that will guarantee that the ASN.1 is well-formed.  In other
words it's possible to employ a simple firewall for ASN.1 that isn't possible
for many other formats (PGP, SSL, ssh, etc etc).  This is exactly what
cryptlib does, I'd be extremely surprised if anything could get past that.
Conversely, of all the PDU-parsing code I've written, the stuff that I worry
about most is that which handles the ad-hoc (a byte here, a unit32 there, a
string there, ...) formats of PGP, SSH, and SSL.  We've already seen half the
SSH implementations in existence taken out by the SSH malformed-packet
vulnerabilities, I can trivially crash programs like pgpdump (my standard PGP
analysis tool) with malformed PGP packets (I've also crashed quite a number of
SSH clients with malformed packets while fiddling with my SSH server code),
and I'm just waiting for someone to do the same thing with SSL packets.  In
terms of safe PDU formats, ASN.1 is the best one to work with in terms of
spotting problems.

@_date: 2003-10-06 21:38:45
@_author: Peter Gutmann 
@_subject: Protocol implementation errors 
Yup, and that's exactly what makes me so nervous about the ad hoc formats.
Having had the experience of writing parsers for every major format used in
crypto (oh, except IPsec, but that falls into the same class as PGP/SSH/SSL)
and ranking things in the order in which I'd expect problems to occur, I get:
  ASN.1: Pretty much bulletproof, you should be able to throw anything at that
  code and it'll just bounce off (I should be able to run the recent
  malformed-ASN.1 attacks that hit OpenSSL on it within the next few days, but
  I'd expect all of those to be caught by the firewall and not even get to the
  actual ASN.1 code).
  Ad hoc formats (PGP, SSH, SSL): Should be OK because I apply anal-retentive
  amounts of checking everywhere and have done probably a dozen full audits of
  the code, but I'm still not fully confident about it (it did withstand the
  malformed-SSH-message weaknesses from a few months ago though).
  Text formats (XML): Full of arbitrarily-complex messages, variable-length
  encodings, special-case escape sequences, and other horrors, and that's
  without even thinking about the nightmare added by XSLT, XPath, DTDs, style
  sheets, XML namespace problems, and who knows what else.  If anything goes
  wrong, it'll be in there.  I can't even imagine how you could produce a
  truly safe parser for that stuff.
While I'm not madly in love with ASN.1, in terms of safe data formats it's the
one I feel most comfortable with.
ASN.1 has a *reputation* of being notoriously hard to parse, gained chiefly
from some early bad experiences with OSI work (which would give anything a
reputation of being hard to work with :-).  I've implemented, and I know of
others who have implemented, extremely compact and portable ASN.1 libraries.
My ASN.1 library is about the same level of complexity as the PGP and SSH
libraries, so the bit-bagging scheme being used has little to do with it.

@_date: 2003-10-07 21:15:21
@_author: Peter Gutmann 
@_subject: NCipher Takes Hardware Security To Network Level 
If you think that's scary, look at Microsoft's CryptoAPI for Windows XP FIPS
140 certification.  As with physical security certifications like BS 7799, you
start by defining your security perimeter, defining everything inside it to be
SECURE, and ignoring everything outside it.  Microsoft defined their perimeter
as "the case of the PC".  Everything inside the PC is defined to be SECURE.
Everything outside is ignored.
FIPS 140 requires role-based access control (RBAC).  Microsoft enforces this.
There's a single role, "everyone that uses the machine".
After that, it gets a bit dodgy, and the credibility of the certification
process might be called into question by some of the more sceptical readers.
OTOH it does show that Microsoft has a good grasp of the value of the
certification system.
Note that you could probably get a system running MSDOS FIPS 140 certified
following this methodology, provided that you enable the BIOS password to meet
the access control requirements.
Peter ("I define myself to be A BIT CYNICAL about all this").

@_date: 2003-10-07 21:20:18
@_author: Peter Gutmann 
@_subject: Other OpenSSL-based crypto modules FIPS 140 validated? 
Ditto.  The problem is that when vendors have spent $100K+ on the
certification, they're very reluctant to give anyone else (and specifically
their competitors) the benefits of their expenditure, so you end up getting
the same thing re-certified over and over for private use, rather than a
single generally-usable version being certified.

@_date: 2003-10-07 22:56:18
@_author: Peter Gutmann 
@_subject: [e-lang] Re: Protocol implementation errors 
Sorry, I meant BER/DER data, not the ASN.1 source text.

@_date: 2003-10-08 04:07:21
@_author: Peter Gutmann 
@_subject: NCipher Takes Hardware Security To Network Level 
Uhh, so you're avoiding privilege escalation attacks by having everyone run as
root, from which you couldn't escalate if you wanted to.  This doesn't strike
me as a very secure way to do things (and it would still get MSDOS certified,
because you've now turned your machine into a DOS box protection-wise).
That's the "Define the bits that we can easily get away with to be secure and
ignore the rest" approach that I commented on.  It was actually part of a
posting to another list where I was poking fun at BS 7799:

@_date: 2003-10-08 04:42:21
@_author: Peter Gutmann 
@_subject: Protocol implementation errors 
According to the CERT advisory, roughly half of all known SSH implementations
are vulnerable (some of the vendor statements are a bit ambiguous), and the
number would have been higher if it weren't for the fact that several of the
non-vulnerable implementations share the OpenSSH code base (there are a number
of implementations not in the advisory, but we can take it as being a
representative sample).
The reason I appear to be defending ASN.1 here is that there seems to be an
irrational opposition to it from some quarters (I've had people who wouldn't
recognise ASN.1 if they fell over it tell me with complete conviction that
it's evil and has to be eradicated because... well just because).  I don't
really care about the religious debate one way or the other, I'm just stating
that from having used almost all of the bit-bagging formats (starting with PGP
1.0) for quite a number of years, ASN.1 is the one I feel the most comfortable
with in terms of being able to process it safely.
Incidentally, if anyone wants to look for holes in ASN.1 data in the future,
I'd be really interested in seeing what you can do with malformed X.509 and
S/MIME data.
Peter (who's going to look really embarrassed if the NISCC test suite finds
       problems in his ASN.1 code :-).

@_date: 2003-10-09 01:56:47
@_author: Peter Gutmann 
@_subject: Open Source (was Simple SSL/TLS - Some Questions) 
I would add to this the observation that rather than writing yet another SSL
library to join the eight hundred or so already out there, it might be more
useful to create a user-friendly management interface to IPsec implementations
to join the zero or so already out there.  The difficulty in setting up any
IPsec tunnel is what's been motivating the creation of (often insecure) non-
IPsec VPN software, so what'd be a lot more helpful than (no offense, but) yet
another SSL implementation is some means of making IPsec easier to use
(although that may not be possible... OK, let's say "less painful to use" :-).

@_date: 2003-10-09 02:00:31
@_author: Peter Gutmann 
@_subject: NCipher Takes Hardware Security To Network Level 
Since it could appear that I'm gratuitously bashing FIPS 140 (or certification
processes in general) here, I should clarify: As with all attempts at one-
size-fits-all solutions, one size doesn't quite fit all.  You can break the
people getting the certification down into three classes:
  Group 1: Vendors who really care about security, and go well beyond the FIPS
    140 requirements anyway.
  Group 2: Vendors who are generally interested in security, and will polish
    up their product to meet the FIPS 140 requirements.
  Group 3: Vendors who want government contracts and see getting to their goal
    as being a penetration exercise on the certification process.
Over time, the certification has been moving from being a value-add performed
only by vendors who really care to being a "You must be at least this high to
ride the government-contract gravy train" ticket check.  During this
progression, group 1 membership has remained more or less constant (they've
been building secure products for years, with or without the certification),
group 2 has grown slowly (mostly for hardware vendors doing level 2-3 stuff),
and everything else sort of ends up in group 3 (no-one wants to miss the gravy
Of the three groups, only group 2 really benefit from the certification
requirements.  Group 1 is frequently hindered by them because the vendors'
security systems and models are far more sophisticated than the FIPS 140 ones,
but to get your certification you have to show that it's only at the FIPS 140
level (this situation is a bit like the short story that's been circulating
for some years in which systems engineers lobotomise a HAL 9000 so that it can
run COBOL and JCL as the market requires).  Group 3 just sees it as a
paperwork-production exercise, shipping exactly the same product as before,
only now they're allowed to sell it to government departments.  The problem is
that what we really need to be able to evaluate is how committed a vendor is
to creating a truly secure product.  Saying "You won't get government
contracts until you can fill in the checkboxes" seems to be providing entirely
the wrong motivation.

@_date: 2003-10-09 22:48:42
@_author: Peter Gutmann 
@_subject: Open Source (was Simple SSL/TLS - Some Questions) 
IP-over-TCP has some potential performance problems, see
 although having used SSH and
SSL tunnels quite a lot, I wonder how serious this really is - the author of
the above analysis mentions performance problems on a link with a high level
of packet loss, but on a typical link I haven't found any real problems.  If
you specifically want a pure TCP tunnel though, there's a pile of solutions
available, of which the easiest to set up is SSH (point it at the target,
indicate that you want port forwarding, and you're done).
Some guy called Ylonen already did this in 1995 :-).

@_date: 2003-10-13 22:03:58
@_author: Peter Gutmann 
@_subject: Security APIs (was Open Source, was Simple SSL/TLS - Some Questions) 
Yup.  And the real measure of demand for something isn't how many people line
up to eat the lunch you've made but how many volunteer to help you prepare it.
That's the exact problem with most security functionality.  Look at the NT ACL
API for example.  Microsoft have been rewriting that in every OS release in an
attempt to make it more accessible, but have mostly just ended up with a pile
of equally awkward ways of doing things.  This isn't because of bad design,
but because there's no easy way to handle this kind of complex security
functionality in a manner that works for all people.  When working with ACLs I
actually prefer the original NT 3.1 API to the awful sendmail.cf-style ACL
minilanguage they invented for NT 4.0, because although the original was
awkward to use, you at least knew what you were getting.  In more recent
attempts to make it usable they've added a pile of DoSomeHighlySpecificThing-
ToThisParticularFileInThisParticularContext() type functions, but unless you
want to perform the one or two predefined operations they give you, you still
need to use one of the more complex APIs.
Coming a bit closer to home, let's look at what's necessary for a TLS API.
I'm going to use the cryptlib API, both because that's what I'm most familiar
with and because the design is strongly driven by user requirements, so I can
say with a reasonable level of confidence that the stuff I mention below is
based on what users actually want and use.  At the simplest level, you can
establish a connection with three calls (taken straight from p.87 of the
manual in this case):
  create session;
  add server name/URL;
  activate session;
You could actually condense this down to a single function:
  session = open_session( server_name );
except that almost no-one would ever use this for the same reason that almost
no-one would ever use the three-simple-function-call version above: It does no
checking of anything, and doesn't handle most of the special cases that users
First of all, you need to authenticate the connection.  Typically this is done
via certs, so you need to fetch the server cert, check the signature, check
the server URL against the one in the cert, perhaps check its validity/
revocation status, check the issuing cert, the whole X.509 circus.  Since
everyone wants different levels and types of checking, your API needs to be
able to handle all the different cases.  Then some poor misguided souls will
inevitably want to use client certs (typically because they read about them in
some whitepaper and have no idea how much pain they're in for) so you have to
support those as well.  Of course the server side needs to check these certs,
so you need the whole PKI mess on the server as well.
Next, no-one can agree on how to handle the network connection.  Some want you
to handle it for them, some want to handle it themselves, some want to set it
up themselves but pass it on to you (e.g. STARTTLS/STLS use), and some want
one of the above but want to handle network events themselves.  Then there are
variants of the above: Of the DIY-connection crowd, some will handle network
events via select() or poll() (or (shudder) WSAAsyncSelect(), which has some
really cute features of its own), but others are running odd embedded OSes
where the event-handling functionality is done by various event_wait() style
functions that act as general-purpose event demultiplexers and require all
sorts of special-case handling.
Now I haven't even got to any of the real complexity, just the basic
establish-a-link bit, and already you're starting to get an awfully complex
API.  It's not just a case of designing an interface that keeps things as
simple as possible for those who want a simple API while still allowing
everyone else to apply their preferred network interfacing mechanism, you then
need to actually code and test all of the various ways of applying the
Let's skip this low-level networking stuff for now and look at what else is
needed.  Well, since you're more or less requiremed to use certs if you use
SSL (TLS-SRP and TLS-sharedkeys will fix this in the future), you need to be
able to create the things, which means that you also need to handle key
generation and storage.  Some people will want to use smart cards (storage)
and crypto accelerators (server-side) for this, so you need a crypto device
interface.  Then if you're not using self-signed certs (some people want them
from a CA) you need to handle CA cert enrolment and issue.  OTOH if the people
want to issue their own certs, you need CA functionality, and if you do that
in anything more complex than a simple ad hoc manner you need a whole pile of
awkward PKI protocols (CMP, SCEP, OCSP, etc etc) to handle all of that.
All of a sudden your original elegant Venus de Milo is looking more like Durga
(Hindu goddess) with an assortment of arms holding a trident, a lotus, an
elephant goad, a can-opener, an annoyed-looking wombat, and a small bent thing
with a knob at one end, riding on a tiger, and with an entire religion devoted
to interpreting the different nuances and mystic symbolism (hmm, that sounds
like OpenSSL :-).
So it's not just a case of saying "I'm going to create something with a simple
API", because you end up either creating a simple, straightforward, do-
exactly-what-I-want implementation that no-one else can use because it doesn't
do what *they* want, or if it does become popular with users you'll be forced
to add a huge pile of functionality to cater for different user requirements.
Getting back to cryptlib as an example, if you only require very minimal,
fixed functionality, it'll run in very little (the smallest implementation I'm
aware of consists of an ASIC, an external RAM chip, and an Ethernet socket,
running an SSL web server and control software, there's also one running in a
USB token but I think that has a bit more hardware to play with), but that's a
do-exactly-what-I-want application that suits the needs of exactly one user.
For general-purpose use, you need a huge pile of support functionality for
which, even if an individual user only requires 10% or 20% of it, the overall
user community all require different 10-20% bits, so that in the end you have
to support the whole lot to keep all the users happy.
I guess that's always a good reason for writing something, but for any project
of this size you'll either need extreme amounts of perseverance or an active
user community to encourage you to keep going, and (more important from a
security perspective) to stress-test the code in unusual applications to allow
you to identify and fix any problems that might arise.  My original message
wasn't to try to discourage you from writing code, but pointing out that
you're competing in an extremely crowded market, which makes it somewhat
unlikely that the latter will occur, so that you're counting entirely on the
former (your own perseverance) to keep things going.  This probably isn't the
most optimal way to motivate a large coding project.

@_date: 2003-10-13 22:22:16
@_author: Peter Gutmann 
@_subject: NCipher Takes Hardware Security To Network Level 
Yeah, it's largely a case of looking where the light is.  An extreme example
of this is the use of formal methods for high-assurance systems, as required
by FIPS 140-2 level 4.  Why is it in there?  Because FIPS 140-1 had it there
at the highest levels.  Why was it in there?  Because the CC has it in there
at the highest levels.  Why was it in there?  Because the ITSEC had it in
there at the highest levels.  Why was it in there?  Because the Orange Book
('85) had it in there at the highest levels.  Why was it in there?  Because
the proto-Orange Book ('83) had it in there at the highest levels.  Why was it
in there?  Because in the 1970s some mathematicians hypothesised that it might
be possible to prove properties of complex programs/systems in the same way
that they proved basic mathematical theorems.
(Aside: This is starting to sound like that apocryphal "Why are railway tracks
 spaced X units apart" saga).
To continue: At what point in that progression did people realise that this
wasn't a very practical way to build a secure system?  Some time in the late
1970s to early 1980s, when they actually tried to reduce the theory into
practice.  There were quite a number of papers being published even before the
first proto-Orange Book appeared which indicated that this approach was going
to be extremely problematic, with problems... well, insert the standard
shopping list here.
So why is this stuff still present in the very latest certification
requirements?  Because we're measuring what we know how to measure, whether it
makes sense to evaluate security in that way or not.  This is probably why
penetrate-and-patch is still the most widely-used approach to securing
systems.  Maybe the solution to the problem is to figure out how to make
penetrate-and-patch more rigorous and effective...

@_date: 2003-10-16 05:24:03
@_author: Peter Gutmann 
@_subject: NCipher Takes Hardware Security To Network Level 
The Viper.  Because it needed to be formally verifiable, they had to leave out
most of the things that people are used to in modern CPUs and that make
writing an OS easy, leading to a vaguely early-60s level of CPU architecture
that probably would have been unpleasant to program for for anyone used to
modern CPUs, and requiring expensive custom development of almost everything
from scratch (you can't run Linux on that one).  Eventually the project went
into a meltdown over what was actually done (for example is verifying a set of
4-bit slices the same as verifying a 32-bit CPU?) and the legal battles lead
to the demise of the company that was to exploit it commercially (there's a
lot more to it than that including a fair bit of politics, that's a cut-down
version to save space).
There were actually quite a few efforts, starting in the 1970s, some of which
went on much longer than the 9-year VAX VMM effort.  PSOS -> SAT -> LOCK ->
SMG (it may be called something else again now) has been going for about 25
years.  However, this is a really complex topic (way too much to cover here),
so I'll cheat a bit and refer anyone who's really that interested in the
problems that people ran into to Chapter 4 of "Cryptographic Security
Architecture Design and Verification" to save me having to paraphrase 40 pages
of text here.
The point of my post wasn't to start yet another round of formal-methods
bashing, but to point to an example of measuring what we know how to measure
even if there are strong indicators that this isn't the best way to do it.

@_date: 2003-10-18 19:16:25
@_author: Peter Gutmann 
@_subject: WYTM? 
One of the reason why many implementations may not support it is that the spec
is completely ambiguous as to the data formats being used.  For example it
specifies the signature blob format as "an X.509 signature", which could be
about half a dozen different things.  Same with PGP signatures, for which
there's even more possibilities.  In addition since almost nothing implements
them, it's not possible to get test data from someone else's server to see
what they're doing (hmm, and even if there was there's no way to tell whether
their interpretation would match someone else's).

@_date: 2003-10-19 19:47:56
@_author: Peter Gutmann 
@_subject: WYTM? 
Actually I think the main reason was that there's virtually no interest in this.
So that it's possible to claim PGP and X.509 support if anyone's interested in
it.  It's (I guess) something driven mostly by marketing so you can answer
"Yes" to any question of "Do you support ".  You can find quite a number of
these things present in various security specs, it's not just an SSH thing.
To give an example from the home court (and avoid picking on other people's
designs :-), I've been advertising ECC support in my code for years.  After
three years of the code being present and a total of zero requests for its
use, I removed it because it was a pain to maintain (I also changed the text
at that point to say that it was optional/available on request).  It's now
been another three years and I'm still waiting for someone to say they
actually want to use it.  There has been the odd inquiry about potential
availability where I was able to say that it's available as an option, at that
point the user can fill in the appropriate checkbox in the RFP and forget
about it.
(Just to add a note here before people leap in with "But XYZ uses ECC
 crypto!", it's only really used in vertical-market apps.  To use it in
 general you need to know how to get it into a cert (data formats, parameters,
 and so on), find a CA to issue you the cert, figure out how to use it with
 SSL or PGP or whatever, find some other implementation that agrees with what
 your implementation is doing, etc etc etc.  This is why there's so little
 interest, not because of some conspiracy to supress ECCs.  For a more general
 discussion of this problem, see "Final Thoughts" in the Crypto Gardening
 Guide).

@_date: 2003-10-21 15:02:14
@_author: Peter Gutmann 
@_subject: WYTM? 
Are there any known servers online that offer X.509 (or PGP) mechanisms in
their handshake?  Both ssh.com and VanDyke are commercial offerings so it's
not possible to look at the source code to see what they do, and I'm not sure
that I want to run the gauntlet of getting some sample copy of a commercial
app (if they're available) and figuring out how to set it up to work with
certs just to see what the data format is supposed to be...

@_date: 2003-10-22 19:58:10
@_author: Peter Gutmann 
@_subject: PKI Research Workshop '04, CFP 
I note that it's still not possible to use PKI to authenticate submissions to
the PKI workshop :-).
(To those people who missed the original comment a year or two back, the first
 PKI workshop required that people use plain passwords for the web-based
 submission system due to the lack of a PKI to handle the task).

@_date: 2003-10-23 15:26:03
@_author: Peter Gutmann 
@_subject: SSL, client certs, and MITM (was WYTM?) 
Actually there's no need to even extend TLS, there's a standard and very
simple technique which is probably best-known from its use in SSH but has been
in use in various other places as well:
1. The first time your server fires up, generate a self-signed cert.
2. When the user connects, have them verify the cert out-of-band via its
   fingerprint.  Even a lower-security simple phrase or something derived from
   the fingerprint is better than nothing.
3. For subsequent connections, warn if the cert fingerprint has changed.
That's currently being used by a number of TLS-using apps, and works at least
as well as any other mechanism.  At a pinch, you can even omit (2) and just
warn if a key that doesn't match the one first encountered is used, that'll
catch everything but an extremely consistent MITM.  Using something like SSH
keys isn't going to give you any magical security that X.509 certs doesn't,
you'll just get something equivalent to the above mechanism.

@_date: 2003-10-24 03:01:56
@_author: Peter Gutmann 
@_subject: SSL, client certs, and MITM (was WYTM?) 
In theory, yes.  See "SET" :-).  It runs into a lot of the problems that SET
ran into as well, e.g. that half the merchants use the CC# (technically the
PAN) as the primary key for all their accounts so they want to process
everything themselves (the SET specs were changed at one point to make the PAN
visible to the merchant so they could continue this practice, completely
defeating one of the main benefits of the scheme), that no-one wants to pay to
build that sort of infrastructure, that [insert standard SET lament with
backing violins].
So in theory, yes, it would work.

@_date: 2003-10-23 15:52:51
@_author: Peter Gutmann 
@_subject: Intel announces DRM-enabled motherboard 
Intel has just announced a desktop motherboard with Wave's Embassy chip built
in at   Embassy is a DRM
chip that was more recently re-targeted slightly for, uhh, non-DRM
TCPA/TPM/whatever when they realised that DRM hardware was a bit of a hard

@_date: 2003-09-03 13:41:08
@_author: Peter Gutmann 
@_subject: PRNG design document? 
Yes it does, you just have to interpret it correctly.
  The post-processed pool output [from the cryptlib generator] is not sent
  directly to the caller but is first passed through an X9.17 PRNG that is
  rekeyed every time a certain number of output blocks have been produced with
  it, with the currently active key being destroyed.  Since the X9.17
  generator produces a 1:1 mapping, it can never make the output any worse,
  and it provides an extra level of protection for the generator output (as
  well as making it easier to obtain FIPS 140 certification).  Using the
  generator in this manner is valid since X9.17 requires the use of DT, "a
  date/time vector which is updated on each key generation", and cryptlib
  chooses to represent this value as a complex hash of assorted incidental
  data and the date and time.  The fact that 99.9999% of the value of the
  X9.17 generator is coming from the "timestamp" is as coincidental as the
  side effect of the engine-cooling fan in the Brabham ground-effect cars
  [Reference].

@_date: 2003-09-03 15:43:29
@_author: Peter Gutmann 
@_subject: invoicing with PKI 
It was definitely a "must hear" talk.  If you haven't at least read the slides
(were the invited talks recorded this year?  Any MPEGs available?), do so now.
I'll wait here.
The main point he made was that designers are resorting to "fixing" mostly
irrelevant theoretical problems in protocols because they've run out of other
things to do, while ignoring addressing how to make the stuff they're building
usable, or do what customers want.  My favourite example of this (coming from
the PKI world, not in the talk) is an RFC on adding animations and theme music
to certificates, because that's obviously what's holding PKI deployment back.
I did a talk last year at Usenix Security where I said that all SSL really
needed was anon-DH, because in most deployments that's how certificates are
being used (self-signed, expired, snake-oil CAs, even Verisign's handed-out-
like-confetti certs).  It's no less secure than what's being done now, and
since you can make it completely invisible to the user at least it'll get
used.  If all new MTA releases automatically generated a self-signed cert and
enabled STARTTLS, we'd see opportunistic email encryption adopted at a rate
that tracks MTA software upgrades.
(BTW I was seeing about 15% of mail handled via STARTTLS a year ago, a quick
 check on my current mail shows about 20%).

@_date: 2003-09-04 02:00:34
@_author: Peter Gutmann 
@_subject: Is cryptography where security took the wrong branch? 
You forgot the most important one:
    f.  value added elsewhere
SSL's real strength is that it's convinced 100 million Joe Sixpacks that it's
safe to make purchases online.  This has nothing to do with security (you
could do the same with padlock GIFs stuck on your web page), but does count as
some sort of measure of "success", although it's marketing success rather than
security success.  Although they provide about the same level of real
security, it seems that SSH is the tool of choice for people who care about
providing real security while SSL is the tool of choice for people who care
about providing their customers warm fuzzies.

@_date: 2003-09-04 02:17:36
@_author: Peter Gutmann 
@_subject: invoicing with PKI 
Right, that's what I meant - RSA is being used as anon-DH anyway, so we may as
well stop pretending and just make it fully automated and transparent to the
user.  If people do want to go to the effort of checking that everything is
OK, do it by checking the cert fingerprint in the same way that PGP and SSH
have been doing for years.
It's a real RFC (currently still a draft), "Logotypes in X.509 certificates".
I originally suggested this as a joke a couple of years ago ("Can you imagine
KPMGCoopersPriceLybrandWaterhouseAnderson distributing a single cert without
the Official Corporate Logo(tm) with Official Corporate Animation(tm) and
Official Corporate Song(tm) playing in the background?").  Given PKIX's
propensity for standardising anything that comes along ("PKIX was formed to do
one thing and has become a standing committee that will do anything, provided
it is in ASN.1 syntax" - Phil Hallam-Baker), it was only a matter of time
before a draft appeared.  I made the following suggestion for a further
addition to the RFC, but it hasn't been adopted (yet):

@_date: 2003-09-09 17:48:05
@_author: Peter Gutmann 
@_subject: OpenSSL *source* to get FIPS 140-2 Level 1 certification 
I think this uniquely broad certification, if permitted, would be mostly a
sign that the politicians have finally won out over the certification purists.
Let me explain... it's been known for a long time (at least from talking to
evaluators, I don't know if NIST will admit to it) that there's large-scale
use of unevaluated crypto going on, with the FIPS eval requirement being
ignored by USG agencies, contractors, etc etc whenever it gets in the way of
them getting their job done.  If NIST allow this extremely broad
certification, it'd be a sign that they're following the Calvin and Hobbes
recipe for success: "The secret to [success] is to lower your expectations to
the point where they're already met".  In other words the unevaluated crypto
problem (or a major part of it) suddenly goes away, and it's possible to
report that the certification effort has been wonderfully successful, because
a large portion of the noncompliant usage is (at least on paper) magically
made compliant overnight.
The only potential downside to this is that a pile of vendors who previously
got a very narrowly-interpreted certification will presumably be queueing up
to do the "I'll have what she's having" thing as soon as an open-ended
certification is issued.
As with others who have commented on this, I'm going to believe this when I
see it.

@_date: 2003-09-12 00:53:36
@_author: Peter Gutmann 
@_subject: fyi: bear/enforcer open-source TCPA project 
Only for some definitions of "stolen".  A key held in a smart card that does
absolutely everything the untrusted PC it's connected to tells it to is only
marginally more secure than a key held in software on said PC, even though you
can only steal one of the two without physical access.  To put it another way,
a lot of the time you don't need to actually steal a key to cause damage - it
doesn't matter whether a fraudulent withdrawal is signed on my PC with a
stolen key or on your PC with a smart card controlled by a trojan horse, all
that matters is that the transaction is signed somewhere.

@_date: 2003-09-18 02:06:47
@_author: Peter Gutmann 
@_subject: PGP makes email encryption easier 
With all due respect to Ian's work, I think this approach has been
independently reinvented many times by many people.  Here's a message I just
posted to a thread in another discussion list where this topic has come up:

@_date: 2003-09-23 06:24:55
@_author: Peter Gutmann 
@_subject: Linux's answer to MS-PPTP 
A friend of mine recently pointed me at CIPE, a Linux VPN tool that he claimed
was widely used but that no-one else I know seems to have heard of.  Anyway, I
had a quick look at it while I was at his place.  It has some problems.
CIPE lead me to another program, vtun, which is even worse.  Someone else then
told me about another one, tinc, which apparently was just as bad as CIPE and
vtun, but has been partially fixed after flaws were pointed out (it still has
some problems, see below).  The following writeup covers some of the problems,
with conclusions drawn at the end.  As you'll note from reading this, as I
went down the list of VPN software I got less and less interested in writing
up lengthy analyses, so CIPE (the first one I looked at) has the most detail.
The following comments on CIPE apply to the protocol described at
 (the CIPE home
Section 2, Packet Encryption:
- CIPE uses a CRC-32 for integrity-protection.  The use of a weak checksum
  under CFB or CBC has been known to be insecure for quite some time,
  providing no (or at least inadequate) integrity protection for the encrypted
  data.  This first gained widespread attention in 1998 with the Core SDI
  paper "An attack on CRC-32 integrity checks of encrypted channels using CBC
  and CFB modes" by Ariel Futoransky, Emiliano Kargieman, and Ariel M. Pacetti
  of CORE SDI S.A, which lead to the SSHv1 insertion attacks and the more or
  less complete abandonment of SSHv1.  To quote the Core SDI work:
   The use of these algorithms in CBC (Cipher Block Chaining) or CFB (Cipher
   Feedback 64 bits) modes with the CRC-32 integrity check allows to perform a
   known plaintext attack (with as few as 16 bytes of known plaintext) that
   permits the insertion of encrypted packets with any choosen plaintext in
   the client to server stream that will subvert the integrity checks on the
   server and decrypt to the given plaintext, thus allowing an attacker to
   execute arbitrary commands on the server.
  In other words this provides no real integrity protection for data.
  Although it first gained widespread publicity in the SSHv1 attacks, the fact
  that this mechanism is insecure and shouldn't be used goes back to (at
  least) Kerberos v4 dating from the 1980s (the des-cbc-crc mechanism was
  deprecated in Kerberos v5, the now ten-year-old RFC 1510).
- The padding length is limited to 3 bits, making it unusable with any recent
  128-bit block cipher.  In particular, AES can't be used.  In addition, the
  inability to pad to more than one (64-bit) cipher block length makes it
  impossible to disguise message lengths by padding messages to a fixed size
  (there are further SSHv1 attacks that arose from similar problems there).
  This weakness is particularly problematic when applied to section 3.
- There is no protection against message insertion or deletion.  In particular
  an attacker can delete or replay any message, and in combination with the
  weak checksum problem can replay modified messages.  Consider for example
  what would happen if an attacker can replay database transactions where
  money is involved.  This issue is also particularly problematic when applied
  to section 3.
Recommendation to fix:
This portion of the protocol has a number of flaws, but can be fixed with a
more or less complete overhaul of the message format:
- Replace the weak checksum with a MAC like HMAC-SHA1.  If size is a concern,
  use a truncated HMAC (but not to a mere 32 bits, which is unconvincing).
  IPsec cargo-cult protocol design practice would seem to require a MAC size
  of at least 96 bits (IPsec truncated the MAC to 96 bits because that makes
  the AH header a nice size, with a payload length of exactly 128 bits (4 32-
  bit words); everyone else assumed that the number 96 had some magic
  significance and copied it into their own designs).
- Replace the padding with standard PKCS  padding, allowing both the use of
  ciphers with block sizes > 64 bits and padding of messages to hide plaintext
  size.
- Either provide protection against insertion/deletion via message sequence
  numbers, or make it very explicit in the documentation that CIPE should not
  be used where insertion/deletion can be a problem, i.e. in situations where
  the higher-level protocol being tunneled doesn't provide its own mechanism
  for detecting missing, inserted, or out-of-sequence messages.
A quick fix would be to take the SSL (or SSH) format, strip out the SSL
headers and encapsulation, and use what's left (the padding, MAC'ing, etc
etc).  SSL/SSH also provides message-sequence handling if you want this.
Section 3, Key exchange:
- The lack of integrity protection means that it's possible to modify keys in
  transit.  As an extreme example, if 3DES keys were used it'd be possible to
  flip the necessary bits to force the use of weak keys, so that both sides
  would end up sending plaintext.  CIPE doesn't appear to use 3DES, but does
  use IDEA, which also has weak keys.  In any case having an attacker able to
  set key bits in arbitrary ways is never a good thing (they could presumably
  force the use of an all-zero or otherwise known key, which is only
  marginally better than sending in plaintext).
- The lack of replay protection means messages containing instructions to
  switch to old keys can be replayed.  In particular, re-use of a compromised
  key can be forced in this way.
- Since packets are ignored if the checksum is bad, it's possible to no-op out
  key-change messages (forcing continued use of a compromised key) by flipping
  a bit or two.
- The lack of ability to mask the content length allows key management packets
  to be quickly identified by an attacker, and the above attacks applied.  For
  example it looks like keyex packets will always be 24 bytes long, while
  tunneled TCP packets will never be that short.  In any case keyex packets
  are ID'd by the (plaintext) flag that indicates which key type is being
  used.
I'm sure I could find more problems if I thought about it a bit more, but I
think that's about enough - see below for more.
Recommendation to fix:
Basically, this part of the protocol is beyond repair.  Any active attacker
can cause about the same level of havoc that Schneier et al managed for
Microsoft's original PPTP implementation.  The fix for this is to scrap the
key exchange portion completely and replace it with an SSH or SSL management
tunnel, which provides the necessary confidentiality, integrity protection,
authentication, message insertion/deletion protection, etc etc.  Since control
messages are rarely sent, there's no performance penalty to using a TCP
channel for this portion of the protocol.  The alternative would be to more or
less reinvent SSH/SSL using CIPE, which doesn't seem like a useful exercise.
(Author contacted, no response).
While googling for more CIPE info, I found another Linux encrypted tunnel
package, vtun.  Ugh, this makes CIPE look like a paragon of good crypto design
in comparison.  Although the format is undocumented, from a quick look at the
source code it appears that vtun uses Blowfish in ECB mode with no integrity
protection, and as for CIPE no protection against message insertion or
deletion, replay attacks, etc etc.  Eeek!  Then there are code fragments like:
void encrypt_chal(char *chal, char *pwd)
   char * xor_msk = pwd;
   register int i, xor_len = strlen(xor_msk);
   for(i=0; i < VTUN_CHAL_SIZE; i++)
      chal[i] ^= xor_msk[i%xor_len];
void gen_chal(char *buf)
   register int i;
   srand(time(NULL));
   for(i=0; i < VTUN_CHAL_SIZE; i++)
      buf[i] = (unsigned int)(255.0 * rand()/RAND_MAX);
which sort of speak for themselves.  Furthermore, the key is just a straight
hash of the password (no salt, iterated hashing, or other standard precautions
are used).  I feel somewhat bad for the author(s) of vtun because it looks
like there's been a fair bit of work put into it, but honestly my best
recommendation for this in terms of security is to scrap it and start again.
(Some more googling for vtun info found a post by Jerome Etienne from January
 2002 pointing out these exact same problems, so the fact that it has horrible
 security problems has been known for quite some time.  Nothing appears to
 have been fixed in the nearly two years since the problems were pointed out).
So I sent a rough set of notes out for comment and someone pointed me to tinc.
tinc uses a completely predictable 32-bit IV in combination with CBC
encryption, which makes the first encrypted block vulnerable, but isn't quite
as bad as the ECB used in vtun (actually it isn't an IV in the true sense, it
prepends a 32-bit sequence number to the encrypted data, but it works the same
way).  Packets are protected with a 32-bit (very) truncated HMAC using
tinc's real problem though is the handshake protocol, in which the client and
server exchange random RSA-encrypted strings.  That's raw bit strings, there's
no PKCS  or OAEP padding, and the server is happy to act as an oracle for
you too.  This is a terrible way to use RSA, and usually compromises the key.
There is a significant body of literature on this (too much to cite) going
back to the early 1980s and continuing through to the current day, with
probably the most recent publication in this area being the attack published
at Usenix Security '03 only a few weeks ago (in that case it was a timing
Beyond that, the protocol writeup
( points out that:
  the server sends exactly the same kind of messages over the wire as the
  client
In case the problem isn't obvious yet, note that what's being exchanged is
purely a random bit string, with no binding of client or server roles or
identities into any part of the exchange.  Again, there are way too many
references to cite on these issues, although my favourite coverage of a lot of
the things you need to think about is in "Network Security: Private
Communication in a Public World" by Kaufman, Perlman, and Speciner.  As an
example, here's a simple attack.  The documentation (section 6.3.1) is a bit
vague about the message flow, but assuming I've understood it correctly, the
message flow is:
  client                                   server
               rsa( random_key ) -->
             random_key( challenge ) -->
         <-- random_key( sha1( challenge ) )
Simplifying things a bit so that the entire exchange can be abstracted down to
"challenge" and "response" (with implied RSA decryption, etc etc taking place
as part of that), let's say Mallet wants to mess with Alice and Bob.  So
Mallet sends a challenge to Bob (claming to be Alice) and gets back a
response.  Mallet gets Bob's encrypted key and challenge back and forwards it
to Alice, who returns a response, which Mallet in turn forwards to Bob, a
classic chess grandmaster attack.  Bob now thinks he's talking to Alice, when
in fact Mallet controls one of the two channels.
If the exchange is something like rcp (where all traffic is one-way, you write
the entire file and close the connection), that's all that's needed, Mallet
just sits there sucking down the data that Bob thinks is going to Alice.  If
not, Mallet does the same thing to Alice, who thinks she's talking to Bob.
Again, Mallet now has a one-way channel to Alice.  Mallet can then splice the
two channels that he controls, so he has a channel in both directions.  Again,
depending on the protocol being tunneled, it may be possible for Mallet to use
the write channel he controls to get data sent on the read channel he
As an extension of the handshake problem, tinc relies heavily on an
administrator at both ends of the link configuring the software identically
for the handling of the handshake phase, replacing the authenticated parameter
negotiation performed by protocols like SSL/TLS, SSH, and IPsec (during the
data transfer phase there are ways of negotiating parameters, I haven't looked
at this too much).
Overall, tinc isn't anywhere near as bad as CIPE or vtun, but like CIPE it
should have the handshake/control channel replaced with an SSH or SSL tunnel,
and the data-transfer portion could be improved as well.  The tinc handshake
protocol would also make a nice homework exercise for students in computer
security courses, since it's not trivially broken but does exhibits various
textbook handshake-protocol flaws, violating a great many of the general
principles of secure protocol design:
  - Don't use the same key for encryption and authentication.
  - Don't have the key chosen entirely by one side (even if it's only Alice
    and Bob on the line rather than Mallet, if one of the two has a poor RNG,
    all communications from them are compromised).
  - Provide message freshness guarantees (and, if possible, some form of PFS).
  - Bind the identity of the principals to the authentication.
  - Don't use the same messages in both directions.
  - Don't act as an oracle for an attacker.
As it stands the handshake protocol only narrowly avoids a variety of basic
attacks, making it quite brittle in that the most trivial change (or someone
thinking about possible attacks a bit further :-) would probably kill it.
(Following on from the footnote at the end of the vtun bit, the same Jerome
 Etienne who pointed out problems in vtun also picked apart an earlier version
 of tinc, which looks to have been at about the same level as CIPE when he
 looked at it.  A lot of it appears to have been fixed since then.
 Authors contacted, their response was that the problems were not that
 serious, and that a main goal of tinc was to get the opportunity to
 experiment with secure tunneling protocols and mechanisms (other design goals
 are at Several points arise from this writeup:
- These programs have been around for years (CIPE goes back to 1996 and vtun
  to 1998) and (apparently) have quite sizeable user communities without
  anyone having noticed (or caring, after flaws were pointed out) that they
  have security problems.  I only heard of CIPE when a friend of mine
  mentioned it to me in passing, and came across vtun by coincidence when I
  was looking for more info on CIPE.  Who knows how many more insecure Linux
  crypto-tunnel products there may be floating around out there.
- It's possible to create insecure "security" products just as readily with
  open-source as with closed-source software.  CIPE and vtun must be the OSS
  community's answer to Microsoft's PPTP implementation.  What's even worse is
  that some of the flaws were pointed out nearly two years ago, but despite
  the hype about open-source products being quicker with security fixes, some
  of the protocols still haven't been fixed.  At least Microsoft eventually
  tries to fix their stuff, given sufficient public embarrassment and the odd
  hundred thousand or so computers being taken out by attackers.
- For all of these VPN apps, the authors state that they were motivated to
  create them as a reaction to the perceived complexity of protocols like SSL,
  SSH, and IPsec.  The means of reducing the complexity was to strip out all
  those nasty security features that made the protocols complex (and secure).
  Now if you're Bruce Schneier or Niels Ferguson, you're allowed to reinvent
  SSL ("Practical Cryptography", John Wiley & Sons, 2003).  Unfortunately the
  people who created these programs are no Bruce or Niels.  The results are
  predictable.
- Whenever someone thinks that they can replace SSL/SSH with something much
  better that they designed this morning over coffee, their computer speakers
  should generate some sort of penis-shaped sound wave and plunge it
  repeatedly into their skulls until they achieve enlightenment.  Replacing
  the SSL/SSH data channel is marginally justifiable, although usually just
  running SSL/SSH over UDP would be sufficient.  Replacing the SSL/SSH control
  channel is never justifiable - even the WAP guys, with strong non-SSL/SSH
  requirements, simply adapted SSL rather than trying to invent their own
  protocol.
Thanks to Greg Rose for checking a draft copy, and Matt Robinson for inspiring
the sound-wave comment (I wonder how many mail filters that'll get caught
Peter (sigh).

@_date: 2003-09-24 10:38:18
@_author: Peter Gutmann 
@_subject: End of the line for Ireland's dotcom star 
Is it really that big a deal though?  You're only ever as secure as the *least
secure* of the 100+ CAs automatically trusted by MSIE/CryptoAPI and Mozilla,
and I suspect that a number of those (ones with 512-bit keys or moribund web
sites indicating that the owner has disappeared) are much more of a risk than
the GTE/Baltimore/beTRUSTed/whoever-will-follow-them succession.
The real lesson of this, I think, is the observation that "The company would
have done better to concentrate on making its core PKI technology easier to
deploy", which applies to most other PKI vendors and products as well.
Baltimore had the bizarre business strategy of using revenue from its PKI
products as a means of driving/funding work in its other product branches,
which is a bit like a drowning man going for a boat anchor as his most likely
flotation device.
Peter (curently flooded with Linux VPN mail, please be patient).

@_date: 2003-09-24 17:36:46
@_author: Peter Gutmann 
@_subject: Coda to "Linux's answer to MS-PPTP" 
My recent posting about problems in some open-source VPN products generated
quite a bit of mail, particularly after it turned up on slashdot (for the
first time in years I got more real mail than spam).  This is a followup to
address some points arising from the original.  In case people are
archiving/re-posting the original article, please attached this as a coda to
that.  I'll also be putting up a long-term copy at
 (linked off my home
page) in the next few days.
The main purpose of the writeup was to point out that it's just as easy to
create insecure "security" software with open source as with closed source,
thus the title of the article.  Microsoft took a fair bit of flak over their
insecure PPTP implementation, but there are open-source alternatives that are
just as bad.  The article wasn't meant to be an exhaustive survey of Linux (or
any OS in general) VPN software, since there's way too much of this around to
cover it all.  As with X window managers a few years ago, there are probably
enough VPN packages around for every user to have their own personal one
without much overlap.  Just as with WMs, there will probably be a general
shakeout leading to a smaller number of widely-used ones, driven by a
particular environment (kwm driven by KDE, Sawfish driven by Gnome) and a
large number of lesser-used alternatives driven by user preference.  The
problematic ones will still continue to be used for some time, for example Ed
Gerck pointed out a Linux Journal article published only last month (Linux
Journal, August 2003, p.84, that recommends the use of vtun, the least secure of the three that I looked
at, but hopefully the better ones will come to predominate.
As with window managers, the same move towards a small number of standardised
VPN apps will probably occur over time in the open-source community.  Although
I deliberately omitted mentioning them in the original writeup because
specifically pointing to one particular implementation would be seen to
implicitly exclude anything else and because predicting the future is always a
dicey proposition, I'm guessing that the two major players will probably be
Free S/WAN ( and OpenVPN ( or something very much like it, with Free
S/WAN representing the IPsec approach and OpenVPN representing the SSL-based
approach that I described in the article (to paraphrase Winston Churchill,
"SSL is the worst way to build a VPN, except for all the others").
Free S/WAN, being an IPsec implementation, inherits all of the IPsec
complexity that seems to have been the inspiration for the creation of several
of the non-IPsec VPN applications.  There are also quite a number of
complaints from users that Free S/WAN is excessively difficult to use if you
want anything more sophisticated than a straightforward setup using pre-shared
static keys, more so than most normal IPsec implementations (please, no
flamewars over this, I'm just reporting user comments and experience).  There
also exists an IKE-less IPsec implementation called ipsec_tunnel ( that uses the Linux (not Microsoft)
CryptoAPI and that was also inspired by the difficulty in using Free S/WAN
("it was hard to understand and hard to configure").
OpenVPN is the "IPsec is too complex" alternative, running a TLS-protected
dual control/data channel session over either TCP or UDP, with a reliability
layer added for the control chanel if it's running over UDP.  The manpage
indicates that the data channel doesn't have this since tunnelling TCP will
provide the necessary facility, but that makes tunnelled protocols without
built-in reliability facilities vulnerable to message insertion or deletion.
This could cause problems in cases where users assume that the use of a secure
tunnel gives them this additional protection.  A note in the manpage to
indicate that OpenVPN provides only the same facilities as the protocol being
tunnelled would be useful in clearing up any possible confusion.  Overall, the
choice of how to handle this is a tradeoff: You can either have protection
against message insertion (strictly speaking, message replay), deletion, and
reordering, but the first occurence of UDP unreliability will be detected as
an attack by the security layer and the connection terminated, or you can have
the ability to live with UDP's unreliability, at the cost of not detecting
insertion/deletion/reordering at the VPN level.
In terms of security, Free S/WAN is an implementation of the IPsec design and
so should be as secure as any other IPsec implementation.  OpenVPN uses
OpenSSL's SSL/TLS for its control channel, and so should be as secure as
SSL/TLS in general.  For the data channel it uses encrypt-then-MAC with
explicit IVs (these will be added to TLS 1.1), which is good.  The key
management step (that is, how to get from the SSL control channel to the data
channel) is documented only in the source code, which I don't feel like
reverse-engineering, but a quick look through it indicates that the author
knows what he's doing.  One thing that would be nice here is an RFC
documenting a standard way to do this, to allow compatible implementations to
be created by others doing SSL-based VPN work.  Something based on experience
with OpenVPN and other tunnelling implementations that may be around would be
useful here (I'm currently bugging various people about this).
My focusing on those two imlementations should not be taken to mean that
everything else not mentioned above is insecure.  I'm was aware of a number of
other VPN implementations, but had no idea just how many there really were
until people pointed a great many of them out to me in email, and I really
don't have time to go through them all.

@_date: 2003-09-25 09:58:33
@_author: Peter Gutmann 
@_subject: End of the line for Ireland's dotcom star 
No-one ever got fired for buying Verisign.  Unfortunately in order to
understand that buying your certs from anything but the cheapest CA present is
a waste of money, you need a certain amount of understanding of how PKI (or at
least certificate manufacturing, as currently practiced) works.  Verisign have
invested an enormous amount of time and money into communicating the message
that it ain't secure if it doesn't say Verisign, and that's been very
effective.  I have, very occasionally, run into people who've told me how they
managed to locate a CA that sold them their certs for $29.95/year instead of
$495/year, but this is very much the exception to the rule.

@_date: 2003-09-26 09:24:48
@_author: Peter Gutmann 
@_subject: why are CAs charging so much for certs anyway? (Re: End of the line for Ireland's dotcom star) 
Actually there's a second aspect to this as well: Verisign's managed PKI
services.  The idea here is that since PKI (specifically, the X.509 PKI model)
is too hard for any normal person or organisation to handle, you charge people
an enormous amount of money to run their PKI for them.  You end up talking to
a Verisign cloud that acts as an authorisation oracle ("Is this thing OK?" -
"Yep, go ahead"), although exactly why you need a PKI for this rather than
(say) a basic challenge-response protocol to query the cloud is unclear (maybe
it's a fashion thing, or an in-joke that no-one's let me in on).  As a
moneymaking racket, it's second only to the "make the browser warning dialogs
go away" one: First you create an unworkable PKI design (although Verisign
didn't do that, they're just taking advantage of it), then you charge people
buckets of money to run it for them (and in terms of money-earners, it leaves
the $495 server certs in the dust - it's sort of like a PKI-DNS service,
except that you pay 5-6 figure sums for your name/key registration).

@_date: 2003-09-26 17:52:26
@_author: Peter Gutmann 
@_subject: Reliance on Microsoft called risk to U.S. security 
"R. A. Hettinga"  forwarded:
There was an example of a point raised in the paper the same day it was
published, when two anti-spam services (monkeys.com and compu.net) were both
DDOSed out of existence by (as the monkeys.com admin put it) "thousands of
separate zombie machines".  Although the anti-spam services were (I would
assume) not running Microsoft software, Windows provided the "convenient and
susceptible reservoir of platforms from which to launch attacks".  In addition
the users of the anti-spam services would be mostly Unix boxen (e.g. Postfix
users pulling in the monkeys.org hash-lists or sendmail users using the
compu.net RBL), so ironically the only systems that wouldn't be adversely
affected by this are the ones that are causing the problem.

@_date: 2003-09-27 15:20:13
@_author: Peter Gutmann 
@_subject: A different Business Model for PKI (was two other subjects related to the demise of Baltimore) 
Actually it's even better than that, the companies using the managed service
are still expected to act as the RA (registration authority) for certs, so
they do all the hard work (verifying users, etc etc).  Verisign just give them
access to a cert-management engine and collect the fees (OK, there's a bit
more work to it than that, but still, the Verisign beancounters must be like
pigs in clover over it).
That's my perpetual gripe with PKIX, they're frantically busy distracting
themselves with interesting (to them) but ultimately pointless and irrelevant
additional standardisation of features so obscure that you need 15 pages of
diagrams just to explain to users why they might be useful, while ignoring the
fact that most people can't use even the most rudimentary parts of what's
already there.  This is presumably why the IETF finally shut them down, they
realised they'd just keep endlessly churning out RFCs until the sun goes out
(I'm not sure whether just leaving them alone to do that in perpetuity
wouldn't have been the better option).
Can't be done.  SPKI tried it, designing an eminently workable PKI (for the
task they were trying to solve) and no-one was interested because it wasn't
X.509.  Certainly if you throw out all the X.500 baggage that we know doesn't
work (X.500 DNs, directories, CRLs, etc etc, which is what SPKI did) you can
build a very workable, usable, scalable PKI, but OSI digital ancestor-worship
requirements say that you're not allowed to do that.
Please, go ahead.  I'll stand over here and watch.
It's a vicious circle, X.509 doesn't work/doesn't do what people want, but
anything that does work isn't X.509 and therefore won't be accepted by the
market.  SPKI was a heroic effort to break out of the cycle, but despite a lot
of work and input from some very clever people, it ultimately failed for that
reason.  And unlike the OSI experience in the 1980s (which X.509 is a direct
repeat of), for PKI there isn't any DARPA-spawned white knight to come in and
fix things when it fails.
To some extent this is computer darwinism in action.  With networking
protocols, some alternative to OSI (that is, a relatively functional set of
networking protocols) had to evolve at some point, because computers had to
communicate somehow.  There was intense market pressure to get something that
worked, and eventually it was the IP protocol suite.  With PKI on the other
hand no such pressure exists, since it's pretty much irrelevant for most
people.  Sure, your marketing folks can come up with all sorts of neat
hypothetical scenarios where PKI would make things so much better, but in
reality people and companies can do their banking, tax filing, buying and
selling, share trading, and every other use that might justify a PKI,
perfectly well without it.  As a result there's no pressure on the people
involved in PKI standardisation to create anything that meets any real-world
requirement, allowing them instead to spend their time building great gothic
cathedrals of infinite complexity whose sole purpose seems to be to strike awe
and terror into the masses.
What's left to vendors is a few niche markets, generally the same groups who
are still trying to make a go of using X.400 (government departments/the
military, a few large corporates, a few banks) who will keep struggling with
X.509 for a number of years.  That's a pretty small market to peddle your
wares into, as companies like Baltimore, Entrust, and a number of other PKI
vendors have found out.
Peter (who probably now officially qualifies as a PKI curmudgeon).

@_date: 2004-04-18 10:21:28
@_author: Peter Gutmann 
@_subject: Cryptonomicon.Net - Key Splitting : First (and Second) Person Key Escrow 
"R. A. Hettinga"  quotes:
It's not surprising because there's no demand for it.  A number of commercial
(crypto hardware) products do it, but only as a backup mechanism / to allow
key migration into new hardware units.  Every vendor has their own techniques
for this, which fit their existing key management mechanisms.  I talked to
some people about doing a standard for this a while back, but given the vast
number of implementation details you'd have to accomodate and the absence of
demand for it, it never went any further than that.

@_date: 2004-08-02 16:44:27
@_author: Peter Gutmann 
@_subject: Microsoft .NET PRNG (fwd) 
This is cross-posted back to the original list (with snippets from various
postings) to try and tie up the loose ends:
That's based on what was known of the CAPI PRNG at the time, there's a more
up-to-date version of that in "Cryptographic Security Architecture Design and
Verification", but that also predates the most recent information on the
generator, which is the second edition (not the first) of "Writing Secure
Code" by Michael Howard and David LeBlanc.  It also appears that the generator
itself has changed somewhat over time, with more recent versions being rather
better than the earlier ones.

@_date: 2004-08-02 16:44:27
@_author: Peter Gutmann 
@_subject: Microsoft .NET PRNG (fwd) 
This is cross-posted back to the original list (with snippets from various
postings) to try and tie up the loose ends:
That's based on what was known of the CAPI PRNG at the time, there's a more
up-to-date version of that in "Cryptographic Security Architecture Design and
Verification", but that also predates the most recent information on the
generator, which is the second edition (not the first) of "Writing Secure
Code" by Michael Howard and David LeBlanc.  It also appears that the generator
itself has changed somewhat over time, with more recent versions being rather
better than the earlier ones.

@_date: 2004-08-16 17:10:00
@_author: Peter Gutmann 
@_subject: Microsoft .NET PRNG (fwd) 
As I've said in a previous post, the best documentation for the RNG is in
"Writing Secure Code (2nd ed)".  The main purpose of the CryptoAPI FIPS 140
documentation is to document an active penetration attack on the FIPS 140
certification process (I could get an 8086 MSDOS machine FIPS 140 certified
[0] using their methodology).
[0] If anyone would like to fund, please get in touch :-).

@_date: 2004-08-18 18:43:55
@_author: Peter Gutmann 
@_subject: HMAC? 
Perhaps they've managed to reduce RFC 3607 to practice :-).

@_date: 2004-08-21 18:04:34
@_author: Peter Gutmann 
@_subject: Good quote about the futility of ID-checking 
Yeterday I watched Gillo Pontecorvo's 1966 film "The Battle of Algiers", a
dramatisation of real events that looks at France's own "war on terror" in
Algeria in the 1950s.  The police attempt to control things by only allowing
people who can show valid ID into the european quarter of Algiers via a few
checkpoints.  When this proves completely ineffective, the French army, led by
a Colonel Mathieu, is called in.  The first thing he does is show his troops
film footage of the checkpoints and the ID checking, pointing out that this
footage is useful because it illustrates how not to do things:
  Checking identity papers is a complete waste of time.  If anyone can be
  counted on to have valid papers, it will be the terrorists.
That's actually a rather astute observation: Joe Sixpack will be lucky to
remember to bring their passport, let alone check whether it's currently valid
and every little detail is correct, but any terrorist will triple-check every
bit of it to make sure that they don't get picked out.  The best that the ID-
checking can hope to do is stop opportunists (as well as any number of
innocent Joe Sixpacks).

@_date: 2004-12-01 20:09:24
@_author: Peter Gutmann 
@_subject: SSL/TLS passive sniffing 
Jack Lloyd  writes"
I was just about to point that out myself.  I'd expect for more usual TLS
usage (web browser/server) it'd be 99+% RSA-*.

@_date: 2004-12-03 22:21:35
@_author: Peter Gutmann 
@_subject: Anti-RFID outfit deflates Mexican VeriChip hype 
"R.A. Hettinga"  forwarded:
Since kidnapping is sort of an unofficial national sport in Mexico (or at
least Mexico City), this is particularly apropos.  An implanted RFID seems to
be just asking for an "express kidnap", something more traditionally used to
get money from ATMs.

@_date: 2004-01-05 15:56:01
@_author: Peter Gutmann 
@_subject: Difference between TCPA-Hardware and a smart card (was: example: secure computing kernel needed) 
Do you trust the PC manufacturer though?  Let's assume:
  - The security target is DRM.
  - The users of the security are content providers.
  - The threat model/attackers are consumers.
In other words the content providers have to trust the PC manufacturer to
provide security to restrict what the consumer can do.  The PC manufacturer's
motivation though is to enhance their own revenue stream by selling as much
hardware as they can, not to enhance the content provider's revenue stream by
making it as crippled as they can.  If you look at the DVD market, every DVD
player made outside the US has some form of vulcan nerve pinch that you can
apply that'll remove the enforcement of region coding.  Depending on your
country's view of differential pricing enforcement mechanisms, this may be
enabled by default (I don't know if you can buy a DVD player in NZ that
enforces region coding, if I was more of an enterpreneur I could start a good
business exporting universal-region, universal-format players to the US :-),
or at least will be a widely-known secret that everyone applies ten minutes
after they first plug the device in.  Given that the goals of the hardware
vendor and the user (= content provider) are mutually exclusive, I wouldn't
trust the hardware vendor.  Conversely, as a consumer, I would trust the
hardware vendor to provide backdoors to bypass the DRM in order to sell more
units after the first year or two, when it's become a commodity and they need
some other way of competing for sales than the novelty value.

@_date: 2004-01-10 15:07:12
@_author: Peter Gutmann 
@_subject: fun with CRLs! 
The CryptoAPI list has been lit up end to end with mail about this.  The
summary from one poster (Tim Anderson ) is:
  IE5.x's digital signature expired yesterday. Every computer that uses
  WinVerifyTrust now has to have the "verify publisher certificate" dealy
  unchecked or the WinVerifyTrust call takes upwards of 5 minutes to complete.
The fix, as for the "We're from Microsoft, give us a certificate" fiasco of
two years ago, is an OS update from Microsoft to replace the certs.  Further
patches will be in Win2K SP5 and WinXP SP2.
ObSnideComment: It's a good thing 99.99% of PKI use is just window dressing,
  imagine if people were basing things like electronic funds transfers on
  technology as brittle as this: "Please wait 5 minutes for the server to time
  out so your funds can become available".

@_date: 2004-07-05 20:31:58
@_author: Peter Gutmann 
@_subject: Question on the state of the security industry 
I never considered phishing to be much of an issue until about a month ago,
when I had a long discussion with someone at a security conference about a
scale and type of phishing you never really hear about much.  Not small-scale
script-kiddie stuff but large-scale phishing run as a standard commercial
business, with (literally) everything but 24-hour helpdesks (if you can read
Portuguese you may be able to find more info at  Some of this I've already covered in the "Why isn't the Internet secure yet"
tutorial I mentioned a while back: Trojans that control your DNS to direct you
to fake web sites, trojans that grab copies of legit web sites from your
browser cache and render them asking for your to re-validate yourself since
your session has expired, trojans that intercept data from inside your browser
before it gets to the SSL channel, etc etc.  This isn't stuff that only
newbies will fall for, these are exact copies of the real site that look and
act exactly like the real site.
This stuff is the scariest security threat I've heard of in (at least) the
last couple of years because it's almost impossible to defend against.  There
is simply no way to protect a user on a standard Windows PC from this type of
attack - even if you can afford to give each user a SecurID or crypto
challenge-response calculator, that doesn't help you much because the attacker
controls the PC. It's like having users stick their bank cards into and give
their PIN to a MafiaBank branded ATM, the only way to safely use it is to not
use it at all.
The only solution I can think of is to use the PC only as a proxy/router and
force users to do their online banking via a small terminal (not running
Windows) that talks to the PC via the USB port, but it's not really
economically viable.

@_date: 2004-07-25 12:25:28
@_author: Peter Gutmann 
@_subject: Using crypto against Phishing, Spoofing and Spamming... 
Some (a lot of?) large-scale phishing is done by or with the aid of
international organised crime, who have a great deal of experience in making
money disappear (or reappear in cleaned form).  Even without that expertise,
there are many, many ways to launder the money.  One that I heard of recently
is to go to small in-debt businesses and offer to pay off their debt, with the
business then paying back half that in cash to the phisher.  The business has
half their debt paid off, and the phisher has converted their wire transfer
into untraceable cash.

@_date: 2004-07-25 13:07:17
@_author: Peter Gutmann 
@_subject: dual-use digital signature vulnerabilityastiglic@okiok.com 
A depressing number of CAs generate the private key themselves and mail out to
the client.  This is another type of PoP, the CA knows the client has the
private key because they've generated it for them.

@_date: 2004-07-27 11:40:25
@_author: Peter Gutmann 
@_subject: dual-use digital signature vulnerabilityastiglic@okiok.com 
Anne & Lynn Wheeler  write:
I don't think there's any confusion about the threat model, which is "Users
find it too difficult to generate keys/obtain certs, so if the CA doesn't do
it for them the users will complain, or not become users at all".  Having the
CA generate the key addresses this threat model.

@_date: 2004-07-27 11:53:37
@_author: Peter Gutmann 
@_subject: dual-use digital signature vulnerabilityastiglic@okiok.com 
Both.  Typically what happens is that the CA generates the key and cert and
mails it to the user as a PKCS  file, either in plaintext, with the
password in the same email, or occasionally with the password in separate
email.  See "How to build a PKI that works" on my home page (direct link at
 Challenge  starting on
p.25) for details, including a few sample quotes from users.
Actually they both seem to have the same need, it's the least effort to do it
this way.  Occasionally you see it dressed up as something else, e.g. "We
don't trust our users to generate the keys properly themselves" (one of those
was from a CA that's distinguished itself through the bugginess of its
software, which makes the comment rather amusing coming from them), but it
almost always boils down to the same thing.

@_date: 2004-07-29 16:01:31
@_author: Peter Gutmann 
@_subject: dual-use digital signature vulnerabilityastiglic@okiok.com 
Yes it does, if you needed this you could add an extension (say)
additionalRecipients with a SEQUENCE of GeneralName naming the additional
parties listening in.

@_date: 2004-07-31 17:49:13
@_author: Peter Gutmann 
@_subject: dual-use digital signature vulnerabilityastiglic@okiok.com 
What I meant was that if there was any demand for this, someone would define a
standard place to store the info, which apps would (eventually) display.  At
the moment there's neither a "additionalRecipients", a "additionalSubjects", a
"coKeyOwners", or anything else, because no-one's ever asked for it.
Given the complete lack of demand for this to date I suspect that even if you
did do an RFC for it it'd be relegated to Experimental status and everyone
would ignore it... what exactly is the intent of adding this information?
Under what circumstances would it be used?  What's the UI for it?  Do you
throw up a warning?  Warning of what?  If it's "Others are listening in" then
the alternative is to not use the cert at all, in which case the choice given
to the users will be "Allow one or two others to listen in" vs. "Allow anyone
to listen in", since everyone will choose the former there's not much point in
putting it there in the first place.  etc etc etc.
(There have been similar suggestions made about other warn-the-user type
 features on the S/MIME list, which tend to get shot down with some variant of
 "I wouldn't even know how to begin to do a UI for this", with a backup of
 "This amounts to giving the user a choice of communicate or don't
 communicate, guess which one they'll choose?").

@_date: 2004-07-31 18:19:29
@_author: Peter Gutmann 
@_subject: should you trust CAs? (Re: dual-use digital signature vulnerability) 
Trusting them to safely communicate the key pair to you once they've generated
it is left as an exercise for the reader :-).

@_date: 2004-06-03 20:14:39
@_author: Peter Gutmann 
@_subject: Article on passwords in Wired News 
One-time passwords (TANs) was another thing I covered in the "Why isn't the
Internet secure yet, dammit!" talk I mentioned here a few days ago.  From
talking to assorted (non-European) banks, I haven't been able to find any that
are planning to introduce these in the foreseeable future.  I've also been
unable to get any credible explanation as to why not, as far as I can tell
it's "We're not hurting enough yet".  Maybe it's just a cultural thing,
certainly among European banks it seems to be a normal part of allowing
customers online access to banking facilities.
(If anyone from the outside-Europe banking industry can provide me with an
 explanation for non-use of TANs that goes beyond "We're looking into it", I'd
 be interested in hearing from them).

@_date: 2004-06-05 18:24:09
@_author: Peter Gutmann 
@_subject: Chalabi Reportedly Told Iran That U.S. Had Code 
Isn't this just a corollary from the Hans Buehler affair?  (Buehler was a
salesman for Crypto AG who was imprisoned by the Iranians when they suspected
Crypto AG of selling them gear that had been doctored at the request of the
NSA.  This would seem to confirm their suspicions).

@_date: 2004-06-08 18:07:06
@_author: Peter Gutmann 
@_subject: Passwords can sit on disk for years 
Hmm, one part of the article isn't quite accurate:
  Operating systems such as Windows and Linux have no facility for stopping
  data being written to the hard drive.
Both Unix and Windows support this.  Under Unix only the superuser can use it
and it isn't supported by all Unix variants, but under Windows (at least the
NT branch), anyone can call VirtualLock(), and (at least under Win2K and
later) as far as anyone can tell it'll prevent data from being swapped (note
all the caveats and weasel-words there :-).  There's a bit of confusion
surrounding this issue, but it should be OK at least with Win2K and XP).  It's
not too hard to do an implementation that manages cryptovariables
appropriately, for example both GPG and my own cryptlib store keys in
pagelocked memory and never let them leave that memory (although I don't know
how well GPG's mlock() gets mapped to Win32 calls under Cygwin).  I think the
problem with the general-purpose apps mentioned (Mozilla, Apache, Emacs) was
that they were never designed to manage cryptovariables, so the data ends up
all over the place.  Anything designed from the outset to do it properly
shouldn't be vulnerable to this sort of problem.

@_date: 2004-06-12 19:11:28
@_author: Peter Gutmann 
@_subject: Chalabi Reportedly Told Iran That U.S. Had Code 
On a semi-related note, there's ex-Iraqi crypto gear for sale on e-bay at
Only used once by a slightly gullible/careless owner...
It'd be interesting for someone with too much spare time on their hands to buy
one of these and try and figure out what customised-for-Iraq supplemental
functionality it contains.

@_date: 2004-06-16 03:37:54
@_author: Peter Gutmann 
@_subject: Breaking Iranian Codes (Re: CRYPTO-GRAM, June 15, 2003) 
"R. A. Hettinga"  forwarded:
Someone (half-)remembered reading the Crypto AG story in the Baltimore Sun
several years ago, bragged to Chalabi that the US had compromised Iranian
crypto, and the story snowballed from there.  The story could have started out
with a loquacious (Sun-reading) cab driver for all we know.  Some reports have
suggested the source was drunk, so maybe it was a drunk in a bar.  Maybe
Chalabi read the story himself and invented the snitch to make it seem more
important than it was, or to drive the US security community nuts with an orgy
of internal witch-hunting.  Given the lack of further information, it could
have been just about anything.

@_date: 2004-06-30 01:49:21
@_author: Peter Gutmann 
@_subject: recommendations/evaluations of free / low-cost crypto libraries 
Hmm, that list is somewhat out of date (several years in some cases).

@_date: 2004-05-28 16:57:15
@_author: Peter Gutmann 
@_subject: CAs for spies? 
Minor nitpick: That should really be phrased as "Have you ever wondered what
CA a spy agency would select to make the browser warning dialogs go away?".

@_date: 2004-05-28 16:48:51
@_author: Peter Gutmann 
@_subject: [ijccr] Re: DELL ships "CryptoPDA" 
*when implemented* is the key phrase.  This thing seems to be vapourware,
after two months of poking around via various channels I have yet to find
anyone at Intel who can tell me anything about this device.  The only info on
it is in a whitepaper, the XScale developer network, Intel security folks,
Intel tech support, no-one knows anything about it.  OTOH what's described in
the whitepaper is pretty cool, this'll be wonderful for building small secure
embedded devices, if it ever materialises.

@_date: 2004-05-29 01:20:30
@_author: Peter Gutmann 
@_subject: Examining the Encryption Threat 
To save people downloading the PDF, it's an 11-page article that reinvents
the 'file' command.

@_date: 2004-05-29 03:27:26
@_author: Peter Gutmann 
@_subject: The future of security 
No they won't.  All the ones I've seen are some variant on the "build a big
wall around the Internet and only let the good guys in", which will never work
because the Internet doesn't contain any definable inside and outside, only
800 million Manchurian candidates waiting to activate.  For example
MessageLabs recently reported that *two thirds* of all the spam it blocks is
from infected PCs, with much of it coming from ADSL/cable modem IP pools.
Given that these "spammers" are legitimate users, no amount of crypto will
solve the problem.  I did a talk on this recently where I claimed that various
protocols designed to enforce this (Designated Mailers Protocol, Reverse Mail
Exchanger, Sender Permitted From, etc etc) will buy at most 6-12 months, and
the only dissent was from an anti-virus researcher who said it'd buy weeks and
not months.  The alternative proof-of-resource-consumption is little better,
since it's not the spammers' resources that are being consumed.
There is one technological solution which would help things a bit, which is
Microsoft implementing virus throttling in the Windows TCP stack.  Like a
firebreak, you can never prevent fires, but you can at least limit the damage
when they do occur.  Unfortunately I don't see this happening too soon, both
because MS aren't exactly at the forefront of implementing security features
(it took them how many years to add the most basic popup-blocking?), and
because of liability issues - adding virus throttling would be an admission
that Windows is a petri dish.
The problem we're facing is social, not technological, so no there's no
technological fix.  The problem is that neither users nor vendors have any
natural incentive to fix things.  In the long run, only legislation will help:
penalise vendors for selling spam-enabling software (MS Outlook, via
viruses/worms), and penalise users for running software in a spam-enabling
manner (open relays).  This is equivalent to standard corporate-governance
legislation that sets auditing/environmental/due diligence/etc requirements.
Unfortunately this is unlikely to pass in the US (where it matters most) due
to software industry lobbying, it'd require an Enron-style debacle to pass
over there, perhaps a virus-induced reactor meltdown or something similar.
(Much of the above was lifted from "Why isn't the Internet secure yet,
 dammit?",  with the
 section on spam starting at page 5.  Apologies for the PDF link, but there
 are some diagrams in there that don't translate well to text).

@_date: 2004-05-31 16:14:12
@_author: Peter Gutmann 
@_subject: Yahoo releases internet standard draft for using DNS as public key server 
It *is* happening, only it's now called STARTTLS (and if certain vendors
(Micromumblemumble) didn't make it such a pain to set up certs for their MTAs
but simply generated self-signed certs on install and turned it on by default,
it'd be happening even more).
The S/MIME list debated this some time ago, and decided (pretty much
unanimously) against it, for two reasosn.  Firstly, because it adds huge ugly
blobs of base64 crap to each message (and before the ECC fans leap in here,
that still adds small ugly blobs of base64 crap to each message).  Secondly,
because if you get a message from someone you know you'll be able to get a
pretty good idea of its authenticity from the content (for example an SSH
developer would be unlikely to be advocating SSL in a list posting), and if
you get a message from someone you don't know then it's pretty much irrelevant
whether it's signed or not.  So the consensus was not to sign messages.

@_date: 2004-06-01 06:50:14
@_author: Peter Gutmann 
@_subject: Yahoo releases internet standard draft for using DNS as public key server 
Since none of Uma, Wendy, or Violet implement DomainKeys or even know what
they are, DomainKeys accomplishes nothing.  OTOH if their { ISP, company,
whatever } has STARTTLS enabled, they're getting their email encrypted without
even knowing about it and are having better-than-average security applied to
their POP/IMAP mail account, again without even knowing about it (I suspect
the latter is far more of a selling point to users than encryption.  No-one
would want to read their mail anyway so they're not worried about that, but if
it stops those nasty hackers from breaking into their account, it's a good
He'd instead need to spend even more time mucking around with keyrings and
updating keys and writing scripts to handle all the checking and wondering why
it all has to be so complicated, and maybe he should just ask people to fax in

@_date: 2004-11-02 23:30:36
@_author: Peter Gutmann 
@_subject: "Scan design called portal for hackers" 
A link ( would
have been useful...
The JTAG interface is your (that is, the reverse engineer's) friend.  This is
why some security devices let you disconnect it using a security-fuse type
mechanism before you ship your product.  Of course that only works if (a) the
device allows it, (b) you remember to activate it, and (c) your attacker isn't
sufficiently motivated/funded to use something like microprobing or a FIB
workstation to bypass the disconnect.

@_date: 2004-10-12 05:34:38
@_author: Peter Gutmann 
@_subject: Certificate serial number generation algorithms 
It's just a SHA-1 hash.  Many CAs use this to make traffic analysis of how
many (or few) certificates they're issuing impossible.  An additional
motivation for use by Verisign was to avoid certs with low serial numbers
having special significance.  While there are a few CA's that follow the
monotonically-increasing-integers scheme that certs were originally intended
to have (and all manner of other weirdness, 32-bit integer IDs of unknown
origin seem to be popular in the "other" category), most seem to use a binary
blob of varying length.

@_date: 2004-10-22 17:23:07
@_author: Peter Gutmann 
@_subject: New IBM Thinkpad includes biometrics 
Even that may not be a valid market target.  If your threat model is script
kiddies/hackers in eastern Europe (that is, generic un-targeted attacks, the
most common type) then writing your password on a post-it note is a perfectly
acceptable response, because the one thing a script kiddie can't do is get
physical access to your desk.  So the primary market for biometrics would
really be people for whom passwords are too much effort (as the former US
biometrics czar said, they're for convenience, not security).

@_date: 2004-10-24 15:13:38
@_author: Peter Gutmann 
@_subject: Financial identity is *dangerous*? (was re: Fake companies, real money) 
The answer to that one is actually "To provide a development environment for
Windows CE (and later XP Embedded)" (the emulator that's used for development
in those environments is VirtualPC).  Thank you for playing.

@_date: 2004-10-26 17:27:03
@_author: Peter Gutmann 
@_subject: Financial identity is *dangerous*? (was re: Fake companies, real money) 
They're not "using it in their development shop", that's their standard
development environment that they ship to all Windows CE, Pocket PC,
SmartPhone, and XP Embedded developers (and include free with every copy of
MSDN).  If an entire branch of my OS development was centered around a
particular technology, I'd want to make sure I owned both the technology and
the developers who created it and will be maintaining/updating it in the
future.  This isn't an optional add-on that MS uses internally, it's a core
component of their embedded OS effort that they push out to anyone who'll take
it in an attempt to dissuade them from going with QNX, embedded Linux,
VxWorks, etc etc.

@_date: 2004-09-01 16:02:02
@_author: Peter Gutmann 
@_subject: Compression theory reference? 
comp.compression FAQ, probably question  given the number of times this
comes up in the newsgroup.
(I've just checked, it's question  in part 1.  Question  in part 2 may
 also be useful).

@_date: 2004-09-12 05:43:44
@_author: Peter Gutmann 
@_subject: [anonsec] Re: potential new IETF WG on anonymous IPSec (fwd from hal@finney.org) (fwd from touch@ISI.EDU) 
So in other words it's the same baby-duck security model that's been quite
successfully used by SSH for about a decade, is also used in some SSL
implementations that don't just blindly trust anything with a certificate
(particularly popular with STARTTLS-enabled MTAs/MUAs where you don't want to
bother with CA-issued certs), and is even used in various X.509 applications
(via "certificate fingerprints"), although the X.509 folks don't like to admit
that because it implies that a known-good cert fingerprint is more reliable
than a CA :-).
Maybe it's worth doing some sort of generic RFC for this security model to
avoid scattering the same thing over a pile of IETF WGs, things like the
general operational principles (store a hash of the server key, compare it on
subsequent connects), how to present the value to the user (a format that's
consistent across protocols would be nice), maybe a simple /etc/passwd-type
file format listing servers and their matching hashes, etc etc etc.

@_date: 2004-09-14 05:09:13
@_author: Peter Gutmann 
@_subject: [anonsec] Re: potential new IETF WG on anonymous IPSec (fwd from hal@finney.org) (fwd from touch@ISI.EDU) 
Since there seems to be at least some interest in this, I'll make a start on
it.  If anyone else wants to add their $0.03 to it [0], let me know.
[0] Or $0.04 if you're paying in Euros.

@_date: 2004-09-25 17:03:03
@_author: Peter Gutmann 
@_subject: An interesting "new" computer security problem 
A few days ago I was chatting with some people working on a government IT
project who had a rather complex security problem that they needed help with.
They have a large number of users with Windows dumb terminals (think Xterms
but for Windows) connected to a central ASP server, which runs various
mutually untrusted apps from different vendors.  Their problem was that they
needed a means of securing the individual apps from each other.
I told them that they were in luck, and this exact problem had already been
addressed before.  I'd drop off the detailed technical specs for the solution
when I next saw them, they could recognise it by its bright orange cover.
(Actually it wasn't quite that simple and easily solveable: The ASP server is
 untrusted as well, it just acts as a middleman for back-ends located at
 various locations, and only the back-ends are trusted.  I figured giving them
 the Orange Book would be easier than trying to explain that they had an
 unsolveable problem on their hands).

@_date: 2004-09-30 16:58:13
@_author: Peter Gutmann 
@_subject: Customs and Excise Electronic Returns 
I've seen this one before, this happens when the CA generates your private key
and emails it to you, a relatively common practice.  Since the Mac can't
import the PFX/PKCS  files used by MSIE/CryptoAPI, you can't use them on
the Macintosh.
(This was pre-OS X, I don't know what the status is now, it may still be
 something that requires MS CryptoAPI to work, or maybe the web page is just
 out of date).

@_date: 2004-09-30 17:05:15
@_author: Peter Gutmann 
@_subject: Linux-based wireless mesh suite adds crypto engine support 
VIA AES support is included in Brian Gladman's AES implementation, which is
pretty much the de facto standard AES implementation.  The RNG code is pretty
easy to do, I did an implementation that worked out of the box without ever
having access to the hardware.
Tinfoil hat stuff - why trust any crypto hardware then?.  The only thing they
could fiddle is the RNG, and I just can't see them risking their reputation
over something as silly as this.  Besides, they're also going for government
markets, so I would imagine third parties have gone over the design in much
more detail than in any published analysis.

@_date: 2005-08-09 00:49:33
@_author: Peter Gutmann 
@_subject: solving the wrong problem 
Yes.  The intent was that forging the fingerprint on a warhead should cost as
much or more than the warhead itself.
Then the Soviet Union collapsed, and the unforgeable fingerprints were
replaced by magic markers, which were cheaper to manage.

@_date: 2005-08-09 01:04:10
@_author: Peter Gutmann 
@_subject: solving the wrong problem 
That sounds a bit like "unicorn insurance" ("We've taken out insurance against
unicorns, and we know that it's effective because we haven't been attacked by
any unicorns yet"), which is used for silly threat models.  However, this is
slightly different from what Perry was suggesting.  There seem to be at least
four subclasses of problem here:
1. "???" : A solution based on a misunderstanding of what the real problem is.
2. "Unicorn insurance": A solution to a nonexistent problem.
3. "???": A solution to a problem created artificially in order to justify its
   solution (or at least to justify publication of an academic paper
   containing a solution).
4. "PKI": A solution in search of a problem.

@_date: 2005-08-09 21:46:06
@_author: Peter Gutmann 
@_subject: solving the wrong problem 
The cost was US$12M per warhead.  I think that's sufficient.

@_date: 2005-08-11 23:42:02
@_author: Peter Gutmann 
@_subject: How much for a DoD X.509 certificate? 
$25 and a bit of marijuana, apparently.  See:
    Although the story doesn't mention this, the "ID" in question was the DoD
Common Access Card, a smart card containing a DoD-issued certificate.  To get
a CAC, you normally have to provide two forms of verification... in this case
I guess the two were photo ID of dead presidents and empirical proof that you
know how to buy weed.
The cards were issued by Yusuf Khalil Jackson, a man with a long criminal
history (including, ironically, identity fraud):
  John Pike, Global Security.org:  "The notion that we're going to let
  somebody with this type of criminal record, with no background check on him
  and give him the ID card machine defies understanding."
Jackson admitted to making about 30 of the ID cards:
  John Pike:  "The good news is that it looks like some of these people were
  just doing it so they could go to a bar and claim to be over 21. The bad
  news is that you don't know what else some of these other people might have
  done."
One of the cards was later "seized from a Pakistani national" by the police.
  Bowens:  "That's the nightmare of it. The cards themselves are not
  counterfeit. They're authentically made but they've been issued in an
  unauthorized manner for profit or ideology or a little of both."
This sort of thing doesn't bode well for Real ID either.  These cards were
real ID too.

@_date: 2005-08-13 17:35:08
@_author: Peter Gutmann 
@_subject: The summer of PKI love 
The same place we were standing on OSI deployment 15 years ago.

@_date: 2005-08-17 22:07:16
@_author: Peter Gutmann 
@_subject: How many wrongs do you need to make a right? 
In the 1950s we had cheque blacklists, which were used in an attempt to manage
bad cheques.
  They didn't work well, and were abandoned as soon as better mechanisms
  became available.
In the 1960s and 70s we had credit card blacklists, which were used in an
attempt to manage bad credit cards.
  They didn't work well, and were abandoned as soon as better mechanisms
  became available.
In the 1980s, the fine folks who gave us OSI also brought us CRLs in an
attempt to manage bad certs.
  They didn't work well, but twenty years later the X.509 folks are still
  hanging in there in the hope that one day they'll spontaneously start
  working.
In the eight years since the U.S. Department of Defense started using the PKI
certificate management system it bought from Netscape Communications, it has
issued more than 16 million digital certificates. Most of them are stored on
the department's common access smartcard, which is the main ID card used by
the Army, Navy, Air Force and Marines.
Along the way, the military also has revoked 10 million certificates as
personnel and network needs change. That huge certificate revocation list
(CRL) - which has bloated to over 50M bytes in file size - is the crux of the
problem facing the Defense Department, because the entire CRL is supposed to
be downloaded daily to every PKI user's desktop at the department from servers
acting as distribution points.
Gosh, I wonder why no-one saw that coming.
(I guess they have to revoke all those certs that were issued in exchange for
a few dollars and some weed :-).

@_date: 2005-08-18 02:08:27
@_author: Peter Gutmann 
@_subject: How many wrongs do you need to make a right? 
You mean something like  ?
They don't work, no matter how much polish you apply.  See e.g.

@_date: 2005-08-18 19:37:37
@_author: Peter Gutmann 
@_subject: When people ask for security holes as features 
Raymond Chen's blog has an interesting look at companies trying to bypass
Windows XP's checks that a driver has been WHQL-certified:
  My favorite stunt was related to my by a colleague who was installing a
  video card driver whose setup program displayed a dialog that read, roughly,
  "After clicking OK, do not touch your keyboard or mouse while we prepare
  your system." After you click OK, the setup program proceeds to move the
  mouse programmatically all over the screen, opening the Display control
  panel, clicking on the Advanced button, clicking through various other
  configuration dialogs, a flurry of activity for what seems like a half a
  minute. When faced with a setup program that does this, your natural
  reaction is to scream, "Aaaiiiiigh!"
There are many more examples (in followup comments and links) of vendors
cheating in the certification and install process:
  my new Dell laptop came with an usigned bluetooth driver whose setup
  automatically clicks on the Continue button of the dialogs while installing
  the driver
  the driver for a USB memory key [...] would install and auto-push the button
  on that warning dialog. XP SP2 added a new check for kernel memory pool
  corruption and guess what? This driver would blue-screen every time the
  memory key was plugged in.
  I work on a wifi product that sometimes is bundled with wifi cards. When
  packaged like that our installer also installs the wifi card dirver. Guess
  what. The suits are all upset about the "unsigned driver" warning, and they
  are sure that a programmer more clever than me could make them go away. Of
  course actually getting the drivers certified is too expensive. Excuse me
  while I get back to work on my TPS report.
  I still remember one of Linksys's Wireless B PCMCIA cards. I went to install
  the driver, the instructions actually said something to the tune of "Ignore
  this warning box, it doesn't mean anything important. Continue clicking OK
  on every screen until the driver finishes installing." Hell I could have put
  a box in that said "Click here to format your hard drive" and I'm sure some
  end users would have clicked OK. Cisco is a huge company, surely the WHQL
  payment isn't much to them.
  At a company I used to work for they had found away around that dialog box.
  They would silently launch the System Properties / Driver Signing Options
  dialog, send windows messages to select "Ignore" and then click ok,
  effectively turning off the dialog box (BTW, the code to re-enable the
  setting was commented out, so the installer made your machine less secure
  forever -- great stuff coming from a security company).
More details at The best suggestion is that the warning be changed to:
  Warning! Your hardware manufacturer hasn't bothered to test this driver!
  Do you feel lucky?
  [Yes] [No]

@_date: 2005-08-27 01:56:32
@_author: Peter Gutmann 
@_subject: Another entry in the internet security hall of shame.... 
This is now the third time in the last few months in which invalid/expired SSL
server certs have totally failed to have any effect, at least until a security
person noticed that there was a problem.  Maybe one of the HCI people reading
the list could be persuaded to investigate whether SSL server certs have any
real security value and/or what changes to the UI need to be made to make them
useful.  Alternatively, how long can you get away with a $19.95 cert from
Honest Joe's Used Cars and Certificates that expired several years ago?

@_date: 2005-08-29 22:23:55
@_author: Peter Gutmann 
@_subject: Another entry in the internet security hall of shame.... 
TLS-PSK fixes this problem by providing mutual authentication of client and
server as part of the key exchange.  Both sides demonstrate proof-of-
possession of the password (without actually communicating the password), if
either side fails to do this then the TLS handshake fails.  Its only downside
is that it isn't widely supported yet, it's only just been added to OpenSSL,
and who knows when it'll appear in Windows/MSIE, Mozilla, Konqueror, Safari,

@_date: 2005-08-30 17:05:11
@_author: Peter Gutmann 
@_subject: Another entry in the internet security hall of shame.... 
And that's it's killer feature: Although you can still be duped into handing
out your password to a fake site, you simply cannot connect securely without
prior mutual authentication of client and server if TLS-PSK is used.
What'd be necessary in conjunction with this is two small changes to the
browser UI:
- Another type of secure-connect indicator (maybe light blue or light green in
  the URL bar instead of the current yellow) to show that it's a mutually
  authenticated connection, along with a "Why is this green?" tooltip for it.
- A non-spoofable means of password entry that only applies for TLS-PSK
  passwords.  In other words, something where a fake site can't trick the user
  into revealing a TLS-PSK key.
Anyone know how to communicate this to the Mozilla guys?  The only mechanism
I'm aware of is bugzilla, which doesn't seem very useful for this kind of

@_date: 2005-12-08 17:03:29
@_author: Peter Gutmann 
@_subject: Countries that ban the use of crypto? 
I've had a similar debate with banking users who only wanted integrity
protection and didn't care about confidentiality (or at least they didn't want
to invest the CPU time to provide confidentiality, since for their application
it wasn't warranted).

@_date: 2005-12-08 17:07:51
@_author: Peter Gutmann 
@_subject: Countries that ban the use of crypto? 
You also have to remember that the severity of Russian law is compensated for
by its non-mandatoryness.  Those who care enough about complying (banks and so
on) shouldn't have much trouble getting the necessary license, everyone else
just goes ahead and uses crypto and no-one cares.

@_date: 2005-12-15 02:42:55
@_author: Peter Gutmann 
@_subject: crypto for the average programmer 
I should point out that what's in the tutorial isn't an exhaustive list of
potential pitfalls, it simply contains examples of some of the easiest-to-
explain ones.  The reason for adding that section was that I've seen a number
of cases of people using raw PKC ops (e.g. raw, unpadded RSA) because their
boss told them "Use RSA encryption" and their crypto toolkit provides an
rsaEncrypt() function, the result being that they encrypt a 10MB file with RSA
in ECB mode.  Java is the main offender here, they make it pretty trivial to
do this even though it makes no sense, so people who are told to "encrypt this
with RSA" end up using the RSA-ECB that their tools give them.
I don't know if there's any site tracking this, but (as the tutorial says) you
can either go with PKCS  (the de facto standard, easy to implement and
widely used) or if you want to put in the effort of tracking things through
the literature to see which one is currently in fashion, take your pick of
OAEP, RSA-PSS, Simple RSA, and so on ad nauseum.  The P1363 work tracks
progress in this area pretty closely, although you'll need some sort of P1363-
to-english phrasebook to figure out what they're saying.
Uhh, do you want a non-off-the-shelf product or an off-the-shelf product?  If
off-the-shelf is OK, grab any crypto toolkit that handles this for you and use
that, you know that if it's used in any standard protocol and interoperates
with a pile of other software then there's a good chance they've got it right.

@_date: 2005-12-22 01:10:49
@_author: Peter Gutmann 
@_subject: browser vendors and CAs agreeing on high-assurance certificates 
That's a somewhat cynical view :-) of what I'd seen it as, a case of looking for the dropped contact lens where the light is.  The CAs and auditors are in the
business of auditing and checking, so they try and address the phishing problem by adding more auditing and checking to the cert issue process because that's the only thing they can do.  To grab a few chunks from an article on security usability that'll be published Real Soon Now (note that this is a summary of much preceding text containing examples of each of the cases mentioned below):
  The security model used with SSL server certificates might be called honesty-
  box security: In some countries newspapers are sold on the street by having a   box full of newspapers next to a coin box (the honesty box) into which people   are trusted to put the correct coins before taking out a paper. Of course they   can also put in a coin and take out all the papers, or put in a washer and   take out a paper, but most people are honest and so most of the time it works.   SSL.s certificate usage is similar. If you use a $495 certificate, people will   come to your site. If you use a $9.95 certificate, people will come to your   site. If you use a $0 self-signed certificate, people will come to your site.   If you use an expired or invalid certificate, people will come to your site.   If you.re a US financial institution and use no certificate at all but put up   a message reassuring users that everything is OK, people will come to your   site. In medical terms, the effects of this .security. are indistinguishable   from placebo.
  In fact the real situation is even worse than this. Although there has been   plenty of anecdotal evidence of the ineffectiveness of SSL certificates over   the years, it wasn.t until mid-2005 (ten years after their introduction) that   a rigorous study of their actual effectiveness was performed. This study,   carried out with computer-literate senioryear computer science students (who    one would expect would be more aware of the issues than the typical user)   confirmed the anecdotal evidence that invalid SSL certificates had no effect   whatsoever on users visiting a site.
  [...]
  A contributing factor in the SSL certificate problem is the fact that security   warnings presented to the user often come with no supporting context. Since   web browsers implicitly and invisibly trust a large number of CAs, and by   extension a vast number of certificates, users have no idea what a certificate   is when an error message mentioning one appears. One user survey found that   many users assumed that it represented some form of notice on the wall of the   establishment, like a health inspection notice in a restaurant or a Better   Business Bureau certificate, a piece of paper that indicates nothing more than   that the owner has paid for it (which is indeed the case for most SSL   certificates). Users were therefore dismissive of .trusted. certificates, and   as an extension cared equally little about .untrusted. ones.
  This user conditioning presents a somewhat difficult problem. Psychologists   have performed numerous studies over the years that examine people.s behaviour   once they.ve become habituated into a particular type of behaviour and found   that, once acquired, an (incorrect) whirr, click response is extremely   difficult to change, with users resisting attempts to change their behaviour   even in the face of overwhelming evidence that what they.re doing is wrong. So, as you say, high-assurance certs are solving a CA and regulation-settor problem, not a phishing problem.
It'll be interesting to see how things look in a years' time.

@_date: 2005-12-23 13:31:11
@_author: Peter Gutmann 
@_subject: RNG quality verification 
This question may be solveable, but to do it you need to step back and look at
who's set this requirement and why.  I've run into the random-number-obsession
a couple of times, generally when government agencies or government
departments following policy set by government agencies are involved (example:
The CA generates the keys for the user and emails them their private key,
because we can't trust the quality of the user's RNG).  The reason I label it
an obsession is because the entire remainder of the keygen process can be
arbitrarily insecure as long as the RNG is some sort of officially approved
one (in fact in at least two cases the officially approved RNGs were quite
dubious).  So if this is merely a checkbox requirement then it can be met by
including an attribute in the request saying that the RNG was FIPS-certified,
and recording during the issue process that the request stated that it used an
approved RNG.
Easily solveable bureaucratic problems are much simpler than unsolveable
mathematical ones.

@_date: 2005-12-23 21:30:06
@_author: Peter Gutmann 
@_subject: browser vendors and CAs agreeing on high-assurance certificates 
The users?  No, not really, in that given the extensive conditioning that
they've been subject to, they're doing the logical thing, which is not paying
any attention to certificates.  That's why I've been taking the (apparently
somewhat radical) view that PKI in browsers is a lost cause - apart from a
minute segment of hardcore geeks, neither users nor web site admins either
understand it or care about it, and no amount of frantic turd polishing will
save us any more because it's about ten years too late for that - this
approach has been about as effective as "Just say no" has for STD's and drugs.
That's why I've been advocating alternative measures like mutual challenge-
response authentication, it's definitely still got its problems but it's
nothing like the mess we're in at the moment.  PKI in browsers has had 10
years to start working and has failed completely, how many more years are we
going to keep diligently polishing away before we start looking at alternative

@_date: 2005-12-24 01:10:06
@_author: Peter Gutmann 
@_subject: "2005 in review - The Year I lost my Identity" 
Ian Grigg's blog has a neat tongue-in-cheek review of the year in security.
Here's a sample:
  Browser manufacturers have moved slightly faster than your average glacier.
  Microsoft moved forward by announcing that phishing was a browser problem
  (Mozilla and KDE followed 8 months later), and again by putting some tools
  into the IE7 release. Another big step forward was announcing the switch-off
  of SSL v2.
  But Microsoft also moved backwards one step IMO by going for the "shared
  database of phishing alerts" idea pioneered by Netcraft. Computer scientists
  and security gurus are still scratching their heads over how that is ever
  going to work, given that it never worked the other several hundred times we
  tried it.
There's more there, definitely worth a read:

@_date: 2005-12-24 04:59:58
@_author: Peter Gutmann 
@_subject: RNG quality verification 
How would this differentiate between keygen for which the PRNG is
SHA1( get_thermal_noise() ) and one where it's SHA1( counter++ )?  Or
one where it's get_constant_value() and you take the counter++ -th primes as p
and q?  Or one where ...?  In addition the PRNG input to the keygen process
has no bearing on the form of the primes generated, look at the IPsec DH
primes with their long strings of 1 bits for an example, they'd fail a
statistical test because they've been specially constructed to have a certain
form, but that makes them stronger, not weaker.  Thus both David Wagner's and
my comments that the people asking this question/setting this requirement
don't understand the problem.  So if you want a solution to something
originating at the bureaucratic layer, you need to provide the solution at the
bureaucratic layer.

@_date: 2005-12-24 14:13:46
@_author: Peter Gutmann 
@_subject: Standard ways of PKCS #8 encryption without PKCS #5? 
If you're using PKCS  then you'd want to use PKCS  with CMS password-
based encryption, which, although it's called "password-based encryption", is
as you've pointed out a general-purpose mechanism that can be used to wrap
data using a key from any source, not just a PKCS  password.
(PKCS  is the logical successor to PKCS

@_date: 2005-12-31 13:49:21
@_author: Peter Gutmann 
@_subject: ADMIN: end of latest SSL discussion 
It's been a good start though.  The first step towards recovery is admitting
that you have a problem...
Hi.  My name is Peter and I have an X.509 problem.  Initially it was just
small things, a little PKI after lunch, maybe a digital ID after dinner and a
small CRL as a nightcap.  Then I discovered OCSP, and started combining low-
and high-assurance certificates.  It just got worse and worse.  In the end I
was experimenting with cross-certifying CAs and even freebasing trust
anchors.  One morning I woke up in bed next to a giant lizard wearing a Mozilla t-shirt and knew I had a problem.
It's now been six weeks since my last PKI...

@_date: 2005-02-03 17:53:22
@_author: Peter Gutmann 
@_subject: Dell to Add Security Chip to PCs 
Neither.  Currently they've typically been smart-card cores glued to the MB and accessed via I2C/SMB.

@_date: 2005-02-05 00:47:58
@_author: Peter Gutmann 
@_subject: Dell to Add Security Chip to PCs 
A simple crypto device controlled by the same software is only slightly less
nonsensical.  That is, the difference between software-controlled keys and a
device controlling the keys that does anything the software tells it to is
negligible.  To get any real security you need to add a trusted display, I/O
system, clock, and complete crypto message-processing capability (not just
"generate a signature" like the current generation of smart cards do), and
that's a long way removed from what TCPA gives you.
Yes he will.  That is, he may not really need to do it, but he really, really
wants to do it.  Look at the almost-universal use of PKCS  to allow people
to spread their keys around all over the place - any product aimed at a mass-
market audience that prevents key moving is pretty much dead in the water.
The only effective thing a TCPA chip gives you is a built-in dongle on every
PC.  Whether having a ready-made dongle hardwired into every PC is a good or
bad thing depends on the user (that is, the software vendor using the TCPA
device, not the PC user).

@_date: 2005-02-11 14:16:56
@_author: Peter Gutmann 
@_subject: A cool demo of how to spoof sites (also shows how TrustBar preventsthis...) 
Absolutely certainly.  Even before Baltimore, CA's private keys had been
bought and sold from/to third parties, usually as a result of bandruptcies or
takeovers.  You can also occasionally find lesser CA's keys left in crypto
gear sold on ebay or similar surplus-disposal channels.

@_date: 2005-02-16 18:38:20
@_author: Peter Gutmann 
@_subject: How to Stop Junk E-Mail: Charge for the Stamp 
And the spammers will be using everyone else's PC's to send out their spam, so
the spam problem will still be as bad as ever but now Joe Sixpack will be
paying to send it.
Hmmm, and maybe *that* will finally motivate software companies, end users,
ISPs, etc etc, to fix up software, systems, and usage habits to prevent this.

@_date: 2005-02-17 17:23:06
@_author: Peter Gutmann 
@_subject: That's gratitude for ya... 
This ties in to one of my favourite articles on security usability, "Good-
Enough Security: Toward a Pragmatic Business-Driven Discipline", Ravi Sandhu,
IEEE Internet Computing, Vol.5, No.3 (January/February 2003), p.66, or
 if you don't get the
print version.  This contains observations like:
  How many security engineers would it take to design a system for ATM
  security today? I don't think it could be done. We would be debating
  biometric-enabled smartcards, assurance, protection profiles, denial of
  service, non-repudiation, viruses and buffer-overflow attacks till we were
  blue in the face. There is no way that such a system with "good enough"
  security could be designed and built today on the basis of conventional
  security wisdom. Yet it happened. And it works.
The author offers three design principles for good-enough security:
  1. Good enough is good enough.
  2. Good enough always beats perfect.
  3. The really hard part is determining what is good enough.
I think Trustbar does a pretty good job of getting (3) right.

@_date: 2005-02-24 02:29:46
@_author: Peter Gutmann 
@_subject: I'll show you mine if you show me, er, mine 
"R.A. Hettinga"  forwarded:
Isn't this a Crypto 101 mutual authentication mechanism (or at least a
somewhat broken reinvention of such)?  If the exchange to prove knowledge of
the PW has already been performed, why does A need to send the PW to B in the
last step?  You either use timestamps to prove freshness or add an extra
message to exchange a nonce and then there's no need to send the PW.  Also in
the above B is acting as an oracle for password-guessing attacks, so you don't
send back the decrypted text but a recognisable-by-A encrypted response, or
garbage if you can't decrypt it, taking care to take the same time whether you
get a valid or invalid message to avoid timing attacks.  Blah blah Kerberos
blah blah done twenty years ago blah blah a'om bomb blah blah.
(Either this is a really bad idea or the details have been mangled by the

@_date: 2005-01-05 16:08:05
@_author: Peter Gutmann 
@_subject: Cryptography Research wants piracy speed bump on HD DVDs 
I heard from a friend of mine who works for an organisation that deals with China a fair bit that the DVD licensing costs make up the majority of the cost of cheapie DVD players.  They have people who fly over there, buy a NZ$35 (= ~US$20) player to use for a week while they're working there, and then throw it away when they leave.  These are made so cheap because the manufacturers there aren't paying the DVD licensing costs.

@_date: 2005-01-06 21:04:19
@_author: Peter Gutmann 
@_subject: FreeBSD's urandom versus random 
The "improved" homebrew RNG covers all 5.x versions AFAIK.  The OS X guys did
the same thing BTW, both OSes use a weird Yarrow-derived implementation and
have /dev/urandom pretending to be /dev/random (i.e. /random lies about

@_date: 2005-01-12 05:00:29
@_author: Peter Gutmann 
@_subject: Simson Garfinkel analyses Skype - Open Society Institute 
There have been other posts about this in the past, even though they use known
algorithms the way they use them is completely homebrew and horribly insecure:
Raw, unpadded RSA, no message authentication, no key verification, no replay
protection, etc etc etc.  It's pretty much a textbook example of the problems
covered in the writeup I did on security issues in homebrew VPNs last year.
(Having said that, the P2P portion of Skype is quite nice, it's just the
 security area that's lacking.  Since the developers are P2P people, that's
 somewhat understandable).

@_date: 2005-07-10 23:29:08
@_author: Peter Gutmann 
@_subject: the limits of crypto and authentication 
Banks here have been using it to authenticate higher-value electronic
transactions as well.  The way it works is that for transactions with a
combined value over the default floor limit of NZ$2.5K you have to use an
additional PIN sent via SMS to a pre-configured number to authenticate the
session.  The PIN authenticates that particular session (not just one
transaction), with a fee of NZ$0.25.  It's not perfect, obviously, but that
was seen as the best tradeoff between cost, user convenience, and security.
A few years ago I wanted to do this out-of-band authentication as a
research project, and at the time couldn't find anyone interested in it; now
they've paid an arm and a leg for it themselves, sigh.

@_date: 2005-07-14 18:36:53
@_author: Peter Gutmann 
@_subject: mother's maiden names... 
I don't know about photos specifically, but I know that signature imprints are
often still moved around by laborious manual means because the background
infrastructure to handle images doesn't exist.  Most banks are still using
3270-style interfaces, even if they have a screen-scraped GUI front-end.
There simply isn't any provision for handling anything other than basic text
records - the data-centre back-ends are text-record based (and in some cases
the text is EBCDIC), the communications channels send and receive text records
(often at a few kbps over leased lines, X.25, or PSTN dialup), and the branch
software processes text records.
So using images (of any kind) isn't just a case of making an executive
decision to do so, it would involve a massive, end-to-end infrastructure
upgrade to implement.

@_date: 2005-07-15 22:42:57
@_author: Peter Gutmann 
@_subject: mother's maiden names... 
For those who haven't seen this study, it's an important read (it's also been
re-published in a somewhat more accessible journal, perhaps it was CACM?).
What they did was send students into a supermarket with card photos of either
them, someone who looked vaguely like them, or someone who looked nothing like
them.  Both the FRR and FAR were found to be such that the photo IDs were more
or less worthless for fraud prevention.  Some banks over here started to
introduce photos on cards, but dropped the scheme based on this study and
other research which showed that it wasn't worth it: The photos were too small
to be useful, only customs & immigration staff and to a lesser extent police
have any formal training in matching faces to images, and your typical
minimum-wage checkout operator couldn't care less if the image matched or not,
their incentive was to move shoppers through quickly, not to check IDs.

@_date: 2005-07-18 03:06:42
@_author: Peter Gutmann 
@_subject: ID "theft" -- so what? 
I've seen this happen on many occasions, one example being the posting I made
to this list a few months ago where an organisation had spent so much money
setting up a PKI that they then had to use it (even though it was totally
unnecesary for what they were doing) simply because it was there.
Been there, seen that.  You're well into layers 8 and 9 whenever anything
related to PKI is involved.  I think the fact that PKI is so strong at
enabling layers 8+9 is its great appeal to the inhabitants of said layers.

@_date: 2005-07-18 04:01:17
@_author: Peter Gutmann 
@_subject: ID "theft" -- so what? 
What makes you so sure of that?  When I looked at this ("Plug-and-play PKI: A
PKI your Mother can Use", available from my home page), I found that by the
time you'd hidden enough of the PKI complexity to make it user-friendly, you
had something that was indistinguishable from a username-and-password
interface.  Conversely, as soon as you start surfacing any of the PKI arcana,
it becomes unusable by the majority of users.
Currently the best way that I know of securing an SSL link is through the use
of TLS-PSK, which provides mutual authentication of client and server as part
of the TLS handshake without requiring any public-key technology at all.  This
also happens to be the most usable security technology around - even your
mother can use it, and since the TLS handshake will fail in a very obvious
manner if she connects to a spoofed site, there's no need to rely on users
mastering PKI/PKC arcana for the security to work.

@_date: 2005-06-02 22:29:58
@_author: Peter Gutmann 
@_subject: Citibank discloses private information to improve security 
Oh, I see.  So we were actually in violent agreement :-).
It's not just passive false-positive acceptance, users are actively encouraged
by software vendors to accept verification-failed content.  For example every
other hardware device you install, as part of it's connect-the-dots sequence
of screen shots in the install guide, shows a shot of some sort of signature-
warning dialog, along with an arrow pointing to the "Ignore this warning"
button to click and text telling users to, well, do what the button says. Even
things like WHQL-certified drivers, which should have all the correct
credentials, end up being installed in non-certified ways because the vendors
submit a safe-but-slow configuration to get certified and then ship the
unsafe-but-fast one to be installed (this is standard practice for any
hardware where performance is the main selling point, i.e. video drivers, RAID
drivers, network drivers, etc etc).  Alternatively, the latest bugfix drivers
are several steps ahead of the certified WHQL'd ones, so you get the same
For non-Windows users who haven't seen this sort of user-conditioning in
documentation, here's the first half-dozen online examples of this (to save me
having to scan install guides to demonstrate it):
            Note also that the warnings for valid and invalid signed content are extremely
similar, and both contain lots of text, jargon, and incomprehensible (to the
average user) information.  Both in effect state "Mumble mutter fnord fnord,
do you want to go ahead", with the fnord-level for the valid-signature dialog
being at least as high as the invalid-signature one.  It'd be interesting to
see if users can tell the difference between the two.
This one is particularly cool: "Don't get worried by the JPilot Security
Warning! Just click "YES" to install & run applet. If you don't, you can't
  (Don't worry about those nasty warnings, just close your eyes and click your
heels tog^H^H^H^Hclick OK).
Just to show that it's not just ActiveX signing under Windows that's training
users in this manner, here's a Unix one too:

@_date: 2005-06-03 20:27:47
@_author: Peter Gutmann 
@_subject: Digital signatures have a big problem with meaning 
That cuts both ways though.  Since so many systems *do* screw with data (in
insignificant ways, e.g. stripping trailing blanks), anyone who does massage
data in such a way that any trivial change will be detected is going to be
inundated with false positives.  Just ask any OpenPGP implementor about
handling text canonicalisation.

@_date: 2005-06-03 20:27:47
@_author: Peter Gutmann 
@_subject: Digital signatures have a big problem with meaning 
That cuts both ways though.  Since so many systems *do* screw with data (in
insignificant ways, e.g. stripping trailing blanks), anyone who does massage
data in such a way that any trivial change will be detected is going to be
inundated with false positives.  Just ask any OpenPGP implementor about
handling text canonicalisation.

@_date: 2005-06-04 02:18:10
@_author: Peter Gutmann 
@_subject: Digital signatures have a big problem with meaning 
Yup, see "Why XML Security is Broken",
 for more on this.  Mind
you ASN.1 is little better, there are rules for deterministic encoding, but so
many things get them wrong that experience has shown the only safe way to
handle it is to do an exact bit-for-bit copy from A to B, rather than trying
to re-code at any point.  I've frequently commented that there is only one
workable rule for encoding objects like X.500 DNs, and that's memcpy().

@_date: 2005-06-09 03:42:46
@_author: Peter Gutmann 
@_subject: Digital signatures have a big problem with meaning 
Right, but that's lead to a de-facto encoding rule of "The originator encodes
it however they like, and everyone else re-encodes it (if required) using
memcpy()".  The advantage of the format is that it's never tried to be
anything other than a pure binary-only format, so moving it over text-only
channels is handled at the next layer down (usually base64), rather than
trying to make the encoding itself text-only-capable and then finding yourself
in a world of pain when half the systems the stuff passes through mangle the

@_date: 2005-06-09 17:06:03
@_author: Peter Gutmann 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
"Must use 128-bit RSA encryption" has to be the all-time favourite.
One I saw recently was a requirement for using X9.17 key management... in SSL.

@_date: 2005-06-09 17:17:06
@_author: Peter Gutmann 
@_subject: AmEx unprotected login site 
.> home page do not log in, and hence do not (to Amex) require
I was just going to mention this myself because I've noticed local banks doing
it, you click on some "log in for online banking" link and get to an HTTPS
login page that's distinct from the HTTP main page.  For Mozilla/Firefox
users, grab a copy of the TargetAlert extension and you'll see this on the
originating page, TargetAlert will tag the login link with the "opens in new
window" indicator and the "HTTPS" indicator (the usual yellow padlock).  When
you've got TargetAlert installed, go to e.g.  to see

@_date: 2005-06-10 21:34:28
@_author: Peter Gutmann 
@_subject: Digital signatures have a big problem with meaning 
I don't want to have to re-implement Apache in order to do
         an SSL implementation.
         I don't want to have to re-implement MS Exchange in order to
         do a PGP implementation.
         I don't want to have to re-implement ext2fs in order to encrypt
         a file.
Makes sense to me.  The other problem with XML sigs (also pointed out in the
writeup) is the fact that it gives you 10 ways to do everything, of which only
1 is actually correct/secure/usable, but is indistinguishable from the other
9.  Since ease of use/secure-by-default is a major goal of my work, I'm rather
reluctant to implement something that lets users blow their feet off in a
dozen different ways without even knowing it.

@_date: 2005-06-14 13:11:58
@_author: Peter Gutmann 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
That wasn't quite the case for the Ethernet encryption.  What happened there
was that they had a complete product ready to ship and quite a bit of interest
when it was killed by marketing.  The problem was that Ethernet at the time
wasn't the forgone conclusion it is now, it was just one of a number of
potential candidates for the foregone-conclusion role.  By shipping an
encrypting Ethernet adapter, marketing felt that DEC were saying that standard
Ethernet wasn't safe.  In contrast token ring didn't have an encryption
adapter, so obviously token ring must be secure by default, whereas Ethernet
clearly wasn't.  As a result, the encryption adapter was never shipped.
"Strategy is not letting the enemy know you're out of bullets by continuing to
 fire".

@_date: 2005-06-17 23:57:29
@_author: Peter Gutmann 
@_subject: AES cache timing attack 
It is?  Recovering a key from a server custom-written to act as an oracle for
the attacker?  By this I don't even mean the timing-related stuff, but just
one that just acts as a basic encryption oracle.  Try doing that with TLS or
SSH, you'll get exactly one unrelated packet back, which is the connection
shutdown message.  So while it's a nice attack, section 15 should really be
simplified to:
  Don't do that, then.

@_date: 2005-06-20 21:44:14
@_author: Peter Gutmann 
@_subject: AES cache timing attack 
Definitely.  Maybe time for a BCP, not just for AES but for general block
Well, it depends on what your design assumptions were.  I assume AES went with
a basic Newtonian physics-level approximation of the world that's good enough
for most cases without going into so much specialised detail that it becomes
unworkable.  In fact I'd say it's actually not possible to certify resistance
to timing attacks across all possible CPUs, because it'll always be possible
to find some oddball CPU for which an AES-critical instruction somewhere has
some weird characteristic that helps in an attack.
Lets say you want constant timing for at least the most common CPU family,
x86.  But that's way too broad, so you restrict it to Intel x86.  That still
covers far too many architectures to be useful, so you say Intel P4 x86.  But
there are multiple P4 cores that all perform differently, so you decide on the
Northwood core. Now, which stepping of that core do you want to use?
So in the end you've got an algorithm design that happens to be resistant to
timing attacks on the D0 stepping of a Northwood-core Intel P4.  Anything else
and all bets are off.  This doesn't seem very useful to me.
I think the standardisation process went about as well as can be expected,
given Newtonian physics-level assumptions about how CPUs work.  Once you start
getting into special relativity-level analysis, the model gets a bit more
precise, but you also need a small book of explanatory notes for each
situation, and it becomes impossible to ship any normal product if you require
per-core-stepping tuning.
So I'll stick by my comment that the only really practical way to address this
is with "Don't do that, then".  Now, as you point out, we need a BCP on what
not to do.  I'd suggest not just a basic "don't do this" but also a "do do
this", along with a list of common solutions that get it right: SSL/TLS, SSH,
IPsec, etc etc.

@_date: 2005-06-22 00:45:21
@_author: Peter Gutmann 
@_subject: AES cache timing attack 
Best Current Practice, a special-case type of RFC.  Based on recent experience
with this style of collaborative document editing, I've set up a wiki at
 blank username, password 'sbox', for anyone
who wants to add their $0.02 about what to do/what not to do to protect block
ciphers from side-channel attacks.  If it works out, this could turn into a

@_date: 2005-06-22 01:12:50
@_author: Peter Gutmann 
@_subject: massive data theft at MasterCard processor 
Not only that, you can mess up the transaction without even wanting to do it
fraudulently.  With PIN-based authentication (at least every one I've ever
seen), you insert your card, enter your PIN to authorise the transaction, and
then it prints your receipt.  As you point out, there's no link between the
paper trail and the authorisation, and by the time you get to see the paper
trail it's too late to do anything about it.  Running a two-phase commit to
fix this is unworkable (it'd double the number of transactions and require
holding state at the acquirer gateway), and even then it doesn't tie the
authorisation to the paper trail.
Consider a recent example, in which a hotel inadvertently charged me twice for
one stay.  The first time they ran the transaction on the handheld card
terminal the built-in printer ran out of paper, so they reversed the charge
and charged me a second time with a new roll of paper in the printer.  Since I
didn't trust them to get this right, I asked for both printouts, wrote "VOID"
on the first one, and signed the second one.  As it turned out, they didn't
get it right, and I have a pretty clear paper trail to prove that the first
transaction wasn't authorised.  If I'd done this with a PIN, both would have
been authorised, because I can only take the merchant's word for it that
they've cleared up the first transaction for me - the client has to go to some
lengths to prove their credentials, but the merchant only has to claim that
they've sorted it out. In fact I don't think there's any way for them to prove
to a client that they've reversed a transaction short of phoning their bank
and getting them to fax out a statement.
So I'll stick with printouts and signatures for the foreseeable future.

@_date: 2005-06-22 01:33:57
@_author: Peter Gutmann 
@_subject: AES cache timing attack 
Uhh, that wasn't really what I was after, that's pretty much textbook stuff,
what I wanted was specifically advice on how to use block ciphers in a way
that avoids possibilities for side-channel (and similar) attacks.  I have some
initial notes that can be summarised as "Don't let yourself be used as an
oracle" that I was planning to add after I've fleshed them out a bit.

@_date: 2005-06-22 14:09:07
@_author: Peter Gutmann 
@_subject: AES cache timing attack 
Moves you from being an encryption oracle to a related-key oracle, and makes
the protocol non-idempotent.

@_date: 2005-03-07 18:11:43
@_author: Peter Gutmann 
@_subject: MD5 collision in X509 certificates 
I ran into an interesting example of the conflict between PKI's almost
completely offline design vs.the almost completely online world recently.
Someone showed me a full-page diagram of their PKI/certificate management
process containing several dozen boxes, a maze of connecting arrows, labels,
references to pages and pages of further explanation, etc etc etc.  After
reverse-engineering the process displayed in the diagram over a period of
about quarter of an hour, I simplified the whole thing by drawing a single
line from the top left ("I have someone's public key") to the bottom right
("Ask an online service who it belongs to and whether it's OK to use"),
completely bypassing the morass of PKI in the middle.  (This is a bit like the
financial industry use of PKI that Lynn mentioned a while back in which you
throw away everything but the public key and check that directly, because all
the PKI does is get in the way).
At that point the conversation went something like this:
"Why not do it that way then, since that's the end effect anyway?".
  "We can't do that.  $LARGE_ORGANISATION have spent millions of dollars
   setting up their PKI, and they won't allow something that sidesteps it".
"So the only reason the PKI is there is because not having it there would be
an admission of its uselessness?"
  "Uhh, yeah".
This leads to the following PKI business model:
1. Spend millions of dollars setting up a PKI.
2. Everyone is forced to use it because not to do so would be a waste of the
   setup costs.
3. Profit!

@_date: 2005-03-15 19:07:05
@_author: Peter Gutmann 
@_subject: Security is the bits you disable before you ship 
Key open-source programming tool due for overhaul
  Published: March 14, 2005, 10:46 AM PST
  By Stephen Shankland
  Staff Writer, CNET News.com
  [...]
  GCC 4.0 also introduces a security feature called Mudflap, which adds extra
  features to the compiled program that check for a class of vulnerabilities
  called buffer overruns, Mitchell said. Mudflap slows a program's
  performance, so it's expected to be used chiefly in test versions, then
  switched off for finished products.
So you have an interesting definition of a security feature as "the bit you
disable before the product goes into the environment where it'll be subject to

@_date: 2005-03-15 20:46:15
@_author: Peter Gutmann 
@_subject: $90 for high assurance _versus_ $349 for low assurance 
Given the universal implicit cross-certification model used in browsers,
mailers, etc etc, the only things that "Low" and "High" apply to are price,
not assurance.
(UIXC means that all certs are implicitly trusted equally, which is the same
as having all CAs cross-certify all other CAs.  The effect of either
implicitly or explicitly doing this is that all CAs are only as secure as the
least secure CA, and the only certificate that it makes any sense to buy is
the cheapest one).
You are assured that your credit card will be charged before the certificate
is issued.

@_date: 2005-03-16 02:23:49
@_author: Peter Gutmann 
@_subject: $90 for high assurance _versus_ $349 for low assurance 
It's a distinction in adspace only, in the same way that you're expected to
think that a $200 DVD play from Sony Corp is better than a $40 player from Foo
Yuk Corp (obviously enough people think that way that the $200 ones still
sell, even if a feature-by-feature comparison shows the $40 one is better). In other words the charge-more-for-the-name model seems to work as well here
as it does elsewhere.
(Note that Verisign do perform more extensive checking for the more expensive
grades of cert, but whether that's worth several hundred dollars is an open
question.  Certainly with UIXC it's not worth anything).

@_date: 2005-03-22 16:14:24
@_author: Peter Gutmann 
@_subject: how to phase in new hash algorithms? 
Kick it upstairs to the political layer.  Someone else's problem, we've already
shown them what the solution is, our job is done.
Peter :-).

@_date: 2005-03-29 13:28:43
@_author: Peter Gutmann 
@_subject: and constrained subordinate CA costs? 
Not necessarily, some implementations also ignore the critical flag, so the
cert is treated as valid, although the entire extension is ignored.  However a
corollary of this is that because of the hit-and-miss nature of support for
the extension, you can't rely on it unless you carefully control all of the
software that's used to process certs and make sure that it handles everything
(Even if your app supports name constraints, there are some rather arcane
matching rules in the spec that a number of apps get wrong, so there's a whole
range of behaviours that you can encounter when you put a nameConstraints
extension in a cert).

@_date: 2005-05-18 22:24:18
@_author: Peter Gutmann 
@_subject: Invalid banking cert spooks only one user in 300 
Invalid banking cert spooks only one user in 300
  Stephen Bell, Computerworld
  16/05/2005 09:19:10
  Up to 300 New Zealand BankDirect customers were presented with a security
  alert when they visited the bank's website earlier this month - and all but
  one dismissed the warning and carried on with their banking.
The rest of the story is at
 or
(PC World Australia or ComputerWorld NZ).  To provide a little more background
information, BankDirect is an online-only offshoot of another bank (ASB)
that's targeted at computer-savvy users who don't need (or want) the expense
of a standard bricks-and-mortar account.  There are no branches, and payment
is done electronically at the point of sale (EFTPOS) and managed via the
Internet or a cellphone, thus the (apparently) low number of accesses - you'd
generally rarely need to access it over the net.
So in other words the number of computer-savvy users who were stopped by an
invalid server cert at a banking site was essentially zero.  To quote the
article again:
  Peter Benson, chief executive of Auckland-based Security-Assessment.com,
  says he is "not at all surprised" at the statistics. "In my experience, the
  single weakest point in the chain of [computer] security is the space
  between the keyboard and the floor."
  A lot more education of users in responding appropriately to security alerts
  is needed, he says.
Looks like we have a long way to go in making effective security usable.  Note
that if the same site had used TLS-PSK
( instead of
straight passwords over TLS, and had this been malicious spoofing instead of
just an accident, none of this would have been possible (TLS-PSK provides
mutual authentication of both parties before any sensitive information is
exchanged, so even if the user ignores the warning, they won't be able to
communicate with a spoofed site).

@_date: 2005-05-31 20:03:53
@_author: Peter Gutmann 
@_subject: Citibank discloses private information to improve security 
James (and others): I really wouldn't cite the BankDirect figure as a hard
value, since it represents just a single user, who may in turn have clicked on
the wrong button (i.e. the real figure could have been 0%).  It'd be better to
say "statistically insignificant" or "negligible" or some other close-to-or-
equal-to-zero synonym.

@_date: 2005-06-01 05:28:56
@_author: Peter Gutmann 
@_subject: Citibank discloses private information to improve security 
Probably not.  This issue was discussed at some length on the hcisec list,
(security usability,  e.g:

@_date: 2005-11-14 03:56:51
@_author: Peter Gutmann 
@_subject: [smb@cs.columbia.edu: Skype security evaluation] 
Well, the above text mentions the recommended protocols.  You can get YASSL
from  and the IPsec ESP implementation from
 (although it looks like it hasn't been
updated for awhile, Freshmeat,
 seems to have newer info).
My article on problems I found in homebrew VPN implementations is at
  If you want to save yourself the
effort of building your own TLS + ESP combination, you can use OpenVPN,
 (and if you've ever had to struggle with IPsec, you should
also consider OpenVPN - unlike IPsec, you can just point it at your target
system and that's it, you don't have to start a new career in network and
server reconfiguration :-).
(actually To be precise OpenVPN doesn't use the ESP format directly (which is
 rather IPsec-specific), only the general protocol design:
    OpenVPN's security model can be summarized as such: Use the IPSec ESP
    protocol for tunnel packet security, but then drop IKE in favor of SSL/TLS
    for session authentication. This allows for a lightweight, portable VPN
    implementation that draws on IPSec's strengths, without introducing the
    complexity of IKE.

@_date: 2005-11-18 04:06:43
@_author: Peter Gutmann 
@_subject: "ISAKMP" flaws? 
I feel a need to comment on statements like this... at several times in the
past I've seen people make sweeping generalisation like this, "Everyone knows
about this security weakness, this { paper | article | security alert } isn't
{ novel | interesting | worth publishing }", or some variant thereof (in this
case "these trivial errors are easily avoided").
What makes these statements rather unconvincing is that the majority of all
implementations out there all make these trivial easily-avoided errors (or the
majority of users aren't aware that the well-known problem in the security
alert exists, or whatever).  The nicest example I've seen of this was where
the head of a standards working group explained that some obscure feature that
implementors had been getting wrong was so obvious that they'd consciously
omitted putting it in the standard because everyone just magically knew about
In this particular case if the problem is so trivial and easily avoided, why
does almost every implementation (according to the security advisory) get it

@_date: 2005-11-19 13:44:44
@_author: Peter Gutmann 
@_subject: "ISAKMP" flaws? 
The problem is that these are extraordinarily labour-intensive to write.
Admittedly they're incredibly effective in finding problems (every time
someone's gone to the effort of creating one, it seems like 90% of all
implementations in the target area have proven vulnerable), but that still
leaves the problem of creating the things in the first place.
Another issue is that all of the current ones (that I know of) test for random
rather than Byzantine failures, i.e. they create large numbers of random
packets and hope that one of them triggers a bug, rather than carefully
crafting malicious payloads designed to cause faults.  Once we get Byzantine
test-case generators, I predict there'll be another round of security alerts
as 90% of the products out there fail yet again.

@_date: 2005-11-19 16:31:58
@_author: Peter Gutmann 
@_subject: "ISAKMP" flaws? 
Already happened, unfortunately it's diverged into three different branches:
- VPN hardware vendors replaced it with "management tunnels", typically things
  like single-DES-encrypted backdoors with no message integrity or message
  flow integrity protection and 8-character uppercase-only passwords.
- Open source folks replaced it with OpenVPN.
- The remaining user base replaced it with on-demand access to network
  engineers who come in and set up their hardware and/or software for them and
  hand-carry the keys from one endpoint to the other.
  I guess that's one key management model that the designers never
  anticipated... I wonder what a good name for this would be, something better
  than the obvious "sneakernet keying"?

@_date: 2005-11-20 23:15:49
@_author: Peter Gutmann 
@_subject: "ISAKMP" flaws? 
Unless you're the one paying someone $200/hour for it.
Somehow I suspect that this (making it so unworkable that you have to hand-
carry configuration data from A to B) wasn't the intention of the IKE
designers :-).  It's not just the keying data though, it's all configuration
information.  One networking guy spent some time over dinner recently
describing how, when he has to set up an IPsec tunnel where the endpoints
aren't using completely identical hardware, he uses a hacked version of
OpenSWAN with extra diagnostics enabled to see what side A is sending in the
IKE handshake, then configures side B to match what A wants.  Once that's
done, he calls A and has a password/key read out over the phone to set up for

@_date: 2005-11-22 01:13:23
@_author: Peter Gutmann 
@_subject: "ISAKMP" flaws? 
The two that I'm aware of (the X.509 cert data generator that found ASN.1
parser faults and the SSH hello-packet generator) both just created vaguely
correct-looking PDUs that contained garbage data, so that a simple firewall
check would reject 99% of the packets before they even got to the real
processing.  The SSH generator only sent the first packet, so it never got
past the first step of the SSH handshake.  I'm not sure what the ISAKMP data
generator did.

@_date: 2005-10-02 00:50:58
@_author: Peter Gutmann 
@_subject: Nice use of opportunistic encryption with SIP 
In order to use encryption with SIP, you're stuck with using certificates
(there's no way to do authenticated DH like a number of other secure-phone
devices allow you to do).  However, one vendor has found a nice way around
this: You go to their web page, enter your device IP address and SIP user ID,
and they generate a pre-packaged certificate for you that your browser posts
to the VoIP device once you click the submit button.  See
 for the interface.
(I don't know if they use key continuity management, but they've certainly
reduced the PKI-based entry barrier for voice encryption to a minimum.  The
only way to make it even easier would be to have the device automatically
contact the server for a cert when it's set up, but then that might be
difficult due to firewalling).

@_date: 2005-10-12 21:36:58
@_author: Peter Gutmann 
@_subject: US Banks: Training the next generation of phishing victims 
Banks like Bank of America have taken some flak in the past for their awful
online banking security practices.  I was poking around their home page today
because I wanted some screenshots to use as examples of how not to do it and I
noticed the following incredible message, which appears when you click on the
tiny padlock icon next to the login dialog:
  Browser security indicators
  You may notice when you are on our home page that some familiar indicators
  do not appear in your browser to confirm the entire page is secure. Those
  indicators include the small "lock" icon in the bottom right corner of the
  browser frame and the "s" in the Web address bar (for example, "https").
  To provide the fastest access to our home page for all of our millions of
  customers and other visitors, we have made signing in to Online Banking
  secure without making the entire page secure. Again, please be assured that
  your ID and passcode are secure and that only Bank of America has access to
  them.
Yep, no need to worry about those silly browser security indicators, just hand
over your banking logon details to anything capable of displaying a Bank of
America logo on a web page.
(Another thing I noticed is that if you indicate that your logon state is WA
or ID, you get sent to an HTTPS page which asks for your SSN alongside your
name and password.  Anyone know what legal requirement is behind that?)
Amex is another example of this type of user training:
  Security is important to everyone!
  Please be assured that, although the home page itself does not have an
  "https" URL, the login component of this page is secure. When you enter your
  User ID and password, your information is transmitted via a secure
  environment, and once the login is complete, you will be redirected to our
  secure area.
Wachovia has:
  Browser security indicators
  You may notice when you are on our home page that some familiar indicators
  do not appear in your browser to confirm the entire page is secure. Those
  indicators include the small "lock" icon in the bottom right corner of the
  browser frame and the "s" in the Web address bar (for example, "https").
  To provide the fastest access to our home page, we have made signing in to
  Online Services secure without making the entire page secure. Again, please
  be assured that your ID and password are secure.
(hmm, their admins must have gone to the same security night school as the BoA
ones :-).
Can anyone who knows Javascript better than I do figure out what the mess of
script on those pages is doing?  It looks like it's taking the username and
password and posting it to an HTTPS URL, but it's rather spaghetti-ish code so
it's a bit hard to follow what's going where.

@_date: 2005-10-13 19:32:07
@_author: Peter Gutmann 
@_subject: US Banks: Training the next generation of phishing victims 
My original comment on that was "Looks like they got their security
certification from the same cornflakes packet" :-).  An anonymous contributor
sent in the following comment:

@_date: 2005-10-26 19:51:21
@_author: Peter Gutmann 
@_subject: Godzilla crypto and security tutorial updated 
I've finally got around to finishing a major update of my Godzilla crypto and
security tutorial to cover newer material like WEP, WPA, and WPA2.  It's
available from  and
comprises 784 slides in 10 parts.
The tutorial covers security threats and requirements, services and
mechanisms, and sercurity data format templates, historical ciphers, cipher
machines, stream ciphers, RC4, block ciphers, DES, breaking DES, brute-force
attacks, other block ciphers (AES, Blowfish, CAST-128, GOST, IDEA, RC2,
Skipjack, triple DES), block cipher encryption modes (ECB, CBC, CFB,
encrypt+MAC modes), public-key encryption (RSA, DH, Elgamal, DSA), using PKCs,
elliptic curve algorithms, hash and MAC algorithms (MD2, MD4, MD5, SHA-1,
SHA-2, RIPEMD-160, the HMAC's), pseudorandom functions, key management, key
distribution, the certification process, X.500 and X.500 naming, certification
heirarchies, X.500 directories and LDAP, the PGP web of trust, certificate
revocation, X.509 certificate structure and extensions, certificate profiles,
setting up and running a CA, CA policies, RA's, timestamping, PGP
certificates, SPKI, why do we need digital signature legislation, what is a
signature, paper vs.electronic signatures, non- repudiation, trust, and
liability, existing approaches, examples of existing legislation of various
types including advantages and drawbacks, the Digital Signature Law litmus
test, user authentication, Unix password encryption, LANMAN and NT domain
authentication and how to break it, GSM security, S/Key, OPIE, TANs, PPP
PAP/CHAP, PAP variants (SPAP, ARAP, MSCHAP), RADIUS, DIAMETER,
TACACS/XTACACS/TACACS+, EAP and variants (EAP-TTLS, EAP-TLS, LEAP, PEAP)
Kerberos 4 and 5, Kerberos-like systems (KryptoKnight, SESAME, DCE),
authentication tokens, SecurID, X9.26, FIPS 196, Netware 3.x and 4.x
authentication, biometrics, PAM, SSL, TLS, TLS-PSK, SGC, SSH, TLS vs.SSH,
IPsec, AH, ESP, IPsec key management (Photuris, SKIP, ISAKMP, Oakley, SKEME),
IKE, IPsec problems, OpenVPN, WEP, WEP problems, WPA, TKIP, AES-CCM, DNSSEC,
S-HTTP, SNMP, email security mechanisms, PEM, the PEM CA model, PGP, PGP keys
and the PGP trust model, MOSS, PGP/MIME, S/MIME and CMS, MSP, opportunistic
email encryption (STARTTLS/STLS/AUTH TLS), electronic payment mechanisms,
Internet transactions, payment systems, Netcash, First Virtual, Cybercash,
book entry systems, Paypal, Digicash, e- cheques, SET, the SET CA model, SET
problems, prEN 1546, TeleQuick, Geldkarte, EMV, micropayments, smart cards,
smart card file structures, card commands, PKCS  PC/SC, JavaCard/OCF,
multiapplication cards, iButtons, contactless cards, vicinity cards, attacks
on smart cards, traffic analysis, anonymity, mixes, onion routing, mixmaster,
crowds, LPWA, steganography, watermarking, misc. crypto applications
(hashcash, PGP Moose), TEMPEST, snake oil crypto, selling security.
TCSEC/Orange Book, crypto politics, digital telephony, Clipper, Fortezza and
Skipjack, US export controls, effects of export controls, legal challenges,
French and Russian controls, non-US controls (Wassenaar), Menwith Hill,
Echelon, blind signal demodulation, Echelon and export controls, Cloud Cover,
UK DTI proposals, and various GAK issues.

@_date: 2005-10-27 13:11:13
@_author: Peter Gutmann 
@_subject: [smb@cs.columbia.edu: Skype security evaluation] 
This would match my experience with homebrew VPN protocols when I looked at a
pile of OSS VPN implementations a year or so back.  Evrey single one of them
had flaws (some quite serious) not in getting the basic crypto right, but in
the way that the crypto was used.  I don't see any reason why Skype should
break this mould.
I can't understand why they didn't just use TLS for the handshake (maybe
YASSL) and IPsec sliding-window + ESP for the transport (there's a free
minimal implementation of this whose name escapes me for use by people who
want to avoid the IKE nightmare).  Established, proven protocols and
implementations are there for the taking, but instead they had to go out and
try and assemble something with their own three hands (sigh).
(Having said that, I don't consider it a big deal.  I've always treated Skype
as a neat way of doing VoIP rather than a super-secure encrypted comms link.
The security (for whatever it's worth) is just icing on the basic Skype
service - I'd use it with or without encryption.  The killer app is the cheap
phonecalls, not the crypto).

@_date: 2005-11-01 02:38:34
@_author: Peter Gutmann 
@_subject: Some thoughts on high-assurance certificates 
A number of CAs have started offering high-assurance certificates in an
attempt to... well, probably to make more money from them, given that the
bottom has pretty much fallen out of the market when you can get a standard
certificate for as little as $9.95.  The problem with these certificates is
that, apart from the fact that the distinction is meaningless to users (see
work by HCI people in this area), they also don't fit the standard CA business
processes.  CAs employ people whose job role, and job expertise, lie in
shifting as much product as possible as quickly as possible (as has already
been demonstrated in the race to the bottom for supplying standard
certificates), not in enforcing PKI theology on their clients.
There are only a very small number of people who understand the theology
behind certificates sufficiently to be able to explain the motivation behind
the various steps in the process of issuing them, and none of them are going
to be employed in doing certificate checking for CAs.  Instead, the task will
be managed by, and performed by, the same people who spam everything in the US
that has a pulse with pre-approved credit card applications, loans, and
similar items.
Here's a real-world example of this process in action.  A user approached a
large public CA for a high-assurance certificate and specifically requested
that his identity be checked thoroughly via his hard-to-forge paper documents.
The CA did the usual standard-assurance checking (whois lookup, email to the
whois contact address, caller ID check on the calling number, all easily
spoofed), and then announced that the user had been pre-approved for the high-
assurance certificate, *before* the user had supplied his authenticating
documents.  Made perfect sense, they'd done the equivalent of running a credit
check before pre-approving a credit card or loan or whatever. Their proactive
service and rapid attendance to the customer's needs put them ahead of the
... except that this isn't something like a standard credit-check business.
The user tried explaining this to the CA employees doing the checking, but
they just didn't understand what the problem was.  They'd done everything
right and provided outstanding service to the user hadn't they?
And therein lies the problem.  The companies providing the certificates are in
the business of customer service, not of running FBI-style special background
investigations that provide a high degree of assurance but cost $50K each and
take six months to complete.  The same race to the bottom that's given us
unencrypted banking site logons and $9.95 certificates is also going to hit
"high-assurance" certificates, with companies improving customer service and
cutting customer costs by eliminating the (to them and to the customer)
pointless steps that only result in extra overhead and costs.  How long before
users can get $9.95 pre-approved high-assurance certificates, and the race
starts all over again?

@_date: 2005-09-07 19:33:01
@_author: Peter Gutmann 
@_subject: Another entry in the internet security hall of shame.... 
Gosh, I don't know.  How about the way we've already been doing it for the
past decade or so on every single passworded web site in existence, and for
another decade before that with ATM PINs.
Exactly, PSK's are infeasible, and all those thousands of web sites that have
successfully employed them for a decade or more are all just figments of our
imagination.  By extension, ATMs are also infeasible.
Sarcasm aside for a minute, several people have responded to the PSK thread
with the standard "passwords don't work, whine moan complain" response that
security people are expected to give whenever passwords are mentioned.  It's
all the user's fault, they should learn how to use PKI.  Well we've had ten
years of that and it seems to be making bugger-all difference in protecting
users, based on the universal success of phishing attacks.
What's happened is that security people have said "Here's our perfect
solution, PKI, and we're not budging an inch on that", the masses have said
"We can't manage anything beyond usernames and passwords and we're not budging
an inch on that", and the phishers have leaped in and filled the gap between
the two while both sides are sitting there holding their breath to see whose
face goes blue first.
The failing is in the security community.  Security practitioners (by which I
mean people who build secure systems, not ones who merely go out and
pontificate about them) have 30 years of research in authentication mechanisms
to fall back on, and yet the state-of-the-art in use today is to hand out the
plaintext password to anyone who asks for it: "Hi, I'm your bank, or Paypal,
or something, please give me your password".
Instead of using a decent (and trivial to implement) challenge-response
mutual-authentication mechanism, we're using the worst possible one there is,
the one that every security textbook warns against, while sitting back and
waiting for PKI to start working.
My mother (to use my favourite canonical non-technical user) has figured out
how to use a username and password to authenticate herself.  She hasn't, and
never will, figure out PKI, and nor will most of the rest of humanity.  The
users have amply demonstrated to us what they're capable of handling.  It is
the failing of the security community to not use that effectively - password-
based authentication is bad because the security community (or at least
security application developers) have made it bad, not because it's inherently
Here's my proposal for an unmistakable TLS-PSK based authentication mechanism
for a browser:
  When connecting to a TLS-PSK protected site, the URL bar (or something else
  very obvious, say the top border of the page) is set to a colour like blue,
  matching what Mozilla currently does with its yellow for SSL sites.  The
  blue bar then zooms out into a blue-marked dialog box asking for the
  username and password (I'm assuming here that you can't spoof this sort of
  thing in Javascript).  Once the mutual auth of client and server has
  occurred, the blue-marked dialog box zooms back to the blue-marked web page,
  providing a clear connection between all stages of authentication and secure
  display.  All that users have to learn is to never enter their password on a
  non-blue-marked site.
It doesn't solve *all* phishing problems, but it's a darn sight better than
the mess we're in now.

@_date: 2005-09-11 03:52:27
@_author: Peter Gutmann 
@_subject: Another entry in the internet security hall of shame.... 
Oh no, I wasn't focusing on any one person, it was a characterisation of the
general response from security people when this sort of thing is mentioned.
Long before the discussion on this list, there were already missionaries
coming to the ietf-tls list to enlighten the heathens who dared to mention PSK
and remind them of their duty to support PKI in all its infinite perfection,
and not to take any false gods before it.
Sure, but those issues have already been addressed by pretty much every site
that needs to use passwords or user authentication for any reason.  That's the
point I was trying to make, that the standard response to use of passwords (or
PSKs) is they don't work, they don't scale, you can't handle revocation,
distribution is a problem, etc etc etc.  However, despite all of these issues,
all the sites that need to authenticate users are using passwords, and they
seem to be doing OK with that.
Sure, that will be an issue.  I think it depends on how much pain banks and
merchants are willing to endure due to phishing attacks.  The problem with
current authentication methods is that the authenticated is in completely the
wrong direction to prevent phishing, and the phishing industry has developed
in response to the fact that TLS server cert-based auth is useless against it.
TLS-PSK addresses this problem.  Not only does it authenticate the server, but
it authenticates it in a manner that proves the server has direct knowledge of
the client, not merely that they've shelled out all of $7 for a server cert.
So in other words I could be directed by phishers to dozens of sites all
claiming to be XYZ Bank, some even with TLS certs proving this, but only one
will be able to authenticate itself to me as my bank.
If you look at this in terms of attack surface reduction, it's gone from:
  The software I use will hand my password over (in plaintext) to anything
  claiming to be my bank.
  An attacker will have to get to me during the enrolment phase, or compromise
  the bank's server to steal the passwords.
This is an *enormous* reduction in attack surface.  Compromising the enrolment
phase would typically require a huge spoofing effort or powerful MITM, much
more than the "spam out a plausible password-renewal request" type of attack
that's been so successful against the current way of doing things.
Yeah, that's still a possibility, although I think you can probably train most
users to not do this.  I only know about NZ banking practices here, but every
one of them provides a best-practices way to do things (don't do banking from
an Internet cafe, check for the padlock, when logging on check that the last-
logon time display was when you actually logged on, etc etc).  Drilling it
into people that if they don't see the blue flashy bits there's a problem
shouldn't be that hard.  Sure, there'll be some margin-of-error group who
still won't get the message, but these same people would also hand out their
credit card details over the phone to someone claiming to be from their bank.
The thing is, TLS-PSK provides major attack surface reduction, and that's a
big win in the fight against phishing.

@_date: 2005-09-11 22:47:58
@_author: Peter Gutmann 
@_subject: Is there any future for smartcards? 
Smart cards were cool in the 1970s because back then it was almost science-
fiction technology - imagine a standard plastic card with a built-in computer!
So the initial effort wasn't "What can we do with them" but "Can we even
create them".  Then once they were created, it became "OK, now we've got them
what do we do with them?".
There were some things that they're inherently very good for (stored-value
micropayments, phone cards, fare payment, photocopying, that sort of thing)
and as portable embedded CPUs (SIM cards), and a whole pile of other solution-
in-search-of-a-problem things that they're awful at.
The main downside in expanding out of the basic stored-value micropayments
field is that the thing that made the cards so cool in 1975 - the fact that
they're in the same form-factor as a credit card - makes them almost useless
for any other application.  No onboard clock, no onboard power, no display, no
keypad, no network connectivity, and no ability to ever add any of them
because of the form factor limits, that's a serious killer for usability and
Much like Java rings ("Look what I've got, a Java processor in a ring!  Isn't
it cool!" - "So what's it good for?" - "No, you don't understand, it's a Java
processor in a ring!"), the thing that made them take off in the first place
is also what's locking them into a particular niche market.

@_date: 2005-09-11 22:53:34
@_author: Peter Gutmann 
@_subject: Is there any future for smartcards? 
The problem with this is that in 99.99% of cases the insecure networked
machine *is* the reader, rendering the smart card pretty much pointless.  I've
only ever seen a handful of card readers that have keypads and displays, and
none that have succeeded commercially.  Everyone just gets the cheap reader-
only devices.

@_date: 2005-09-19 23:37:45
@_author: Peter Gutmann 
@_subject: Java: Helping the world build bigger idiots 
Found on the Daily WTF,   try {     int idx = 0;     while (true) {       displayProductInfo(prodnums[idx]);
      idx++;       }     }   catch (IndexOutOfBoundException ex) {     // nil
    }
The editor also comments that when he got the first few of these submitted he
rejected them as being faked, but ended up with so many that he realised this
usage must be relatively common.

@_date: 2006-04-04 18:43:55
@_author: Peter Gutmann 
@_subject: Using Bluetooth to locate stealable items 
It's a bit like the idea of putting RFID tags in cash to let muggers know who
to target:
    MOBILE phone technology is being used by thieves to seek out and steal
  laptops locked in cars in Cambridgeshire.
  [..]
  But thieves in Cambridge have cottoned on to an alternative use for the
  function, using it as a scanner which will let them know if another
  Bluetooth device is locked in a car boot.
  Det Sgt Al Funge, from Cambridge's crime investigation unit, said: "There
  have been a number of instances of this new technology being used to
  identify cars which have valuable electronics, including laptops, inside.
  [...]

@_date: 2006-12-04 15:35:26
@_author: Peter Gutmann 
@_subject: cellphones as room bugs 
You make use of the undocumented remote management interface [0].
[0] Buffer overflow bug in the packet header parsing code.

@_date: 2006-12-06 00:14:44
@_author: Peter Gutmann 
@_subject: "Verified by VISA" looks phishy 
"Verified by VISA" was something that Visa came up with after being burned by
SET.  Instead of Visa having to go through the pain of coming up with and
deploying a secure system, they outsourced it.  The idea is that third-party
payment processors come up with whatever Rube Goldberg security scheme they
like, produce enough paperwork to overwhelm Visa's auditors, and then it gets
the "Verified by Visa" stamp of approval (and no, I'm not kidding about that
Yep, that's about the level of some of the "Verified by Visa" stuff I've seen.
(I'm sure they're not all that bad, but the few I've seen have been security
by handwaving and excessive production of paperwork.  The scary thing is that
there are probably quite good ones that didn't make the cut because they
couldn't produce enough paperwork).

@_date: 2006-12-22 03:17:38
@_author: Peter Gutmann 
@_subject: A Cost Analysis of Windows Vista Content Protection 
A Cost Analysis of Windows Vista Content Protection
           ===================================================
                Peter Gutmann, pgut001 at cs.auckland.ac.nz
                              Last updated 22 December 2006
Executive Summary
The Vista Content Protection specification could very well constitute the
longest suicide note in history.
This document looks purely at the cost of the technical portions of Vista's
content protection.  The political issues (under the heading of DRM) have been
examined in exhaustive detail elsewhere and won't be commented on further,
unless it's relevant to the cost analysis.  However, one important point that
must be kept in mind when reading this document is that in order to work,
Vista's content protection must be able to violate the laws of physics,
something that's unlikely to happen no matter how much the content industry
wishes it were possible.  This conundrum is displayed over and over again in
the Windows content-protection specs, with manufacturers being given no hard-
and-fast guidelines but instead being instructed that they need to display as
much dedication as possible to the party line.  The documentation is peppered
with sentences like:
  "It is recommended that a graphics manufacturer go beyond the strict letter
  of the specification and provide additional content-protection features,
  because this demonstrates their strong intent to protect premium content".
This is an exceedingly strange way to write technical specifications, but is
dictated by the fact that what the spec is trying to achieve is fundamentally
impossible.  Readers should keep this requirement to display appropriate
levels of dedication in mind when reading the following analysis [Note A].
Disabling of Functionality
Alongside the all-or-nothing approach of disabling output, Vista requires that
any interface that provides high-quality output degrade the signal quality
that passes through it.  This is done through a "constrictor" that downgrades
the signal to a much lower-quality one, then up-scales it again back to the
original spec, but with a significant loss in quality.  So if you're using an
expensive new LCD display fed from a high-quality DVI signal on your video
card and there's protected content present, the picture you're going to see
will be, as the spec puts it, "slightly fuzzy", a bit like a 10-year-old CRT
monitor that you picked up for $2 at a yard sale.  In fact the spec
specifically still allows for old VGA analog outputs, but even that's only
because disallowing them would upset too many existing owners of analog
monitors.  In the future even analog VGA output will probably have to be
disabled.  The only thing that seems to be explicitly allowed is the extremely
low-quality TV-out, provided that Macrovision is applied to it.
The same deliberate degrading of playback quality applies to audio, with the
audio being downgraded to sound (from the spec) "fuzzy with less detail".
Amusingly, the Vista content protection docs say that it'll be left to
graphics chip manufacturers to differentiate their product based on
(deliberately degraded) video quality.  This seems a bit like breaking the
legs of Olympic athletes and then rating them based on how fast they can
hobble on crutches.
Beyond the obvious playback-quality implications of deliberately degraded
output, this measure can have serious repercussions in applications where
high-quality reproduction of content is vital.  For example the field of
medical imaging either bans outright or strongly frowns on any form of lossy
compression because artifacts introduced by the compression process can cause
mis-diagnoses and in extreme cases even become life-threatening.  Consider a
medical IT worker who's using a medical imaging PC while listening to
audio/video played back by the computer (the CDROM drives installed in
workplace PCs inevitably spend most of their working lives playing music or
MP3 CDs to drown out workplace noise).  If there's any premium content present
in there, the image will be subtly altered by Vista's content protection,
potentially creating exactly the life-threatening situation that the medical
industry has worked so hard to avoid.  The scary thing is that there's no easy
way around this - Vista will silently modify displayed content under certain
(almost impossible-to-predict in advance) situations discernable only to
Vista's built-in content-protection subsystem.
Elimination of Open-source Hardware Support
Vista includes various requirements for "robustness" in which the content
industry, through "hardware robustness rules", dictates design requirements to
hardware manufacturers.  For example, only certain layouts of a board are
allowed in order to make it harder for outsiders to access parts of the board.
Possibly for the first time ever, computer design is being dictated not by
electronic design rules, physical layout requirements, and thermal issues, but
by the wishes of the content industry. Apart from the massive headache that
this poses to device manufacturers, it also imposes additional increased costs
beyond the ones incurred simply by having to lay out board designs in a
suboptimal manner.  Video card manufacturers typically produce a one-size-
fits-all design (often a minimally-altered copy of the chipset vendor's
reference design), and then populate different classes and price levels of
cards in different ways.  For example a low-end card will have low-cost,
minimal or absent TV-out encoders, DVI circuitry, RAMDACs, and various other
add-ons used to differentiate budget from premium video cards. You can see
this on the cheaper cards by observing the unpopulated bond pads on circuit
boards, and gamers and the like will be familiar with cut-a-trace/resolder-a-
resistor sidegrades of video cards. Vista's content-protection requirements
eliminate this one-size-fits-all design, banning the use of separate TV-out
encoders, DVI circuitry, RAMDACs, and other discretionary add-ons.  Everything
has to be custom-designed and laid out so that there are no unnecessary
accessible signal links on the board.  This means that a low-cost card isn't
just a high-cost card with components omitted, and conversely a high-cost card
isn't just a low-cost card with additional discretionary components added,
each one has to be a completely custom design created to ensure that no signal
on the board is accessible.
This extends beyond simple board design all the way down to chip design.
Instead of adding an external DVI chip, it now has to be integrated into the
graphics chip, along with any other functionality normally supplied by an
external chip.  So instead of varying video card cost based on optional
components, the chipset vendor now has to integrate everything into a one-
size-fits-all premium-featured graphics chip, even if all the user wants is a
budget card for their kids' PC.
Increased Cost due to Requirement to License Unnecessary Third-party IP

@_date: 2006-12-23 02:51:41
@_author: Peter Gutmann 
@_subject: gang uses crypto to hide identity theft databases 
I don't think you can even apply that much analysis to it.  How exactly did
they come up with such a figure in the first place?  400 *what* computers?
TRS-80's?  Cray XT4's?  Does the encryption software come with a disclaimer
saying "if you forget your password, it'll take 400 computers 12 years to
recover your data"?  With that level of CPU power it sounds like it'd
something at the level of brute-forcing a 56-bit DES key (using a software-
only approach), which sounds like an odd algorithm to use if it's current
crypto software.  It sounds more like a quote for the media (or, more likely,
misreporting) than any real estimate of the effort involved.

@_date: 2006-12-23 21:04:09
@_author: Peter Gutmann 
@_subject: A Cost Analysis of Windows Vista Content Protection 
Some people have asked for references for the information in the writeup,
these weren't so easy to provide because some of the content was from non-
public sources, but after a fair bit of searching I've managed to find public
locations for much of the information.  It's in the updated online version at
  The comments by an
ATI product manager are particularly illuminating, the phrase "increased costs
will be passed on to consumers" seems to appear on every second slide of his

@_date: 2006-12-26 17:36:42
@_author: Peter Gutmann 
@_subject: How important is FIPS 140-2 Level 1 cert? 
That was because for a good RNG the only thing you'd ever get are false
positives, so it acted as a "make your generator fail randomly" test.
There are already a pile of huge mental leaps you have to make to apply some
of the more hardware-oriented bits of FIPS 140 to software implementations, if
you read Microsoft's CryptoAPI FIPS docs you'll see some examples of these in
there.  For example the physical module boundary is the case of the PC that
Windows is running on, the role-based access control covers one single user,
whoever's using the PC at the moment, and so on - it's compliance by creating
paperwork rather than by engineering.  (I'm not trying to bash Microsoft here,
other vendors have to resort to the same thing, see e.g. the Crypto++ docs for
another public example of this).  The problem here is that FIPS 140 was
designed for military-security-model crypto hardware, has been stretched to
cover embedded device crypto, and has been seriously over-stretched to try and
cover software crypto (that is, instead of creating a distinct profile to
cover software implementations, the hardware implementation was taken into
places it was never meant to go).  The only way to reconcile the two is
through increasingly tortuous interpretations of various hardware-derived
requirements that don't really work for software.
Depends entirely on who's doing the eval.  For example if you read the OpenSSL
FIPS docs ( they have an entire appendix
dedicated to a discussion of the absence of fork protection, which they
weren't allowed to add:
  The fact that this 'fork protection' is absent in the FIPS object module
  PRNG concerns many in the OpenSSL developer and user community, and will be
  the primary obstacle to making the fips configuration option a default.
In addition I've heard of evaluations where the generator is required to use a
monotonically increasing counter (clock value) as the seed, so you can't just
use the PRNG as a postprocessor for an entropy polling mechanism.  Then again
I know of some that have used it as exactly that without any problems.
(Maybe we could come up with a cross-reference of who will allow what, and
then people could choose the most sensible evaluator to submit things to).

@_date: 2006-12-29 20:49:35
@_author: Peter Gutmann 
@_subject: How important is FIPS 140-2 Level 1 cert? 
I don't know if it's the only permitted form, the KAT simply feeds in known
input and checks that the output is as required.  You can feed in anything you
want, there's no need for it to be a counter.  The known input just happens to
be in the form of a monotonically increasing counter (for the Variable Seed
Test (VST), these are from test vectors that NIST has published), the other
test, the Monte Carlo Test (MCT) is just a single random seed value which
isn't a counter.  The values created by the NIST tool are actually rather odd
and consist of a one bit shifted down from the MSB, so you get a successively
longer string of one bits as input to the VST until all 64 bits are ones.  I
have no idea why they chose these particular values.
I know of at least one and possibly two (I'd have to go back through old email
to see who did what), certified at the same time that others couldn't get
certified when doing more or less the same thing.
I'm not sure, I just read through the certification docs on their web site,
but they don't go into this.

@_date: 2006-12-30 01:03:24
@_author: Peter Gutmann 
@_subject: How important is FIPS 140-2 Level 1 cert? 
Did you also notice that the MCT test vectors published in "The Random Number
Generator Validation System (RNGVS)" are wrong?  Or is that what you meant by
"independent of the input"?

@_date: 2006-02-06 01:23:59
@_author: Peter Gutmann 
@_subject: serious threat models 
Just in case people think the answer is "The MIB", it's actually "Any kid with
a bit of technical knowledge".  Susan Dreyfus' book "Underground", for
example, documents hackers playing around inside cellular phone switches in
Europe.  So although the target list looks like a typical intelligence agency
hitlist, it could also have been done by a joyriding teenager interested in
listening in on what politicians, the military, and journalists were saying
and hearing.
(Yes, I know the evidence points at the MIB, but that doesn't automatically
mean it was them).

@_date: 2006-02-09 16:37:19
@_author: Peter Gutmann 
@_subject: Hiding data on 3.5" using "40 track mode" 
I recently had to check out the low-level capabilities of a range of CD/DVD
writers for reasons too tedious to mention, CD+G read support is at best about
50:50.  I used cdrinfo.com to get the data, they use Nero's InfoTool to ask
the drive for capabilities rather than relying on manufacturer specs, since
they can change between printing the data sheets and shipping the drive, and
across different firmware versions.  CD+G read support is treated as a drive
firmware/hardware characteristic since the drives that don't support it simply
won't send the streams to the host system even if they can read them.  Beyond
that, your software driver also has to support it.
You can get assorted software to write CD+G, but it's mostly targeted at the
DIY Karaoke market and not for use as subliminal channels (I believe Nero can
write CD+G, but haven't tried it yet - if it's like the other CD+G progs,
it'll only allow insertion of song text and graphics).  In addition you only
get 4 lots of 16 bytes per 2352-byte sector, and can then use 6 bits of each
byte for data.
The cdrinfo folks really thrash the drives they test, including running
through a pile of special disks with artificial errors and other problems.
It's a good source of info on which drives handle errors best... of course
then you could end up creating CDs that are only readable on one drive type,
or even one drive.

@_date: 2006-02-09 17:01:05
@_author: Peter Gutmann 
@_subject: general defensive crypto coding principles 
Here's a trivial way in which it can be weaker: Let's say you MAC the
ciphertext and if it checks out OK you decrypt it and use it.  If you're using
any mode other than ECB (which you'd better be doing) an attacker can
arbitrarily modify the start of the message by fiddling with the IV.  CBC (by
far the most widely-used mode) is particularly nasty because you can make the
decrypted data anything you want, as the IV is xored directly into the
plaintext.  So you can use encrypt-then-MAC, but you'd better be *very*
careful how you apply it, and MAC at least some of the additional non-message-
data components as well.
Another problem with encrypt-then-MAC is that it creates a nice timing channel
if you bail out on a MAC failure before doing the decryption step.  So while
EtM may be theoretically better in some (somewhat artificial) cases, it's much
more brittle in terms of implementation problems.  Since implementors are
rarely expert cryptographers, I'd prefer the safer MtE rather than EtM for
protocol designs.

@_date: 2006-02-09 17:52:11
@_author: Peter Gutmann 
@_subject: general defensive crypto coding principles 
Just after I sent my previous message (sigh) I finally remembered the name of
the paper that contains a more realistic analysis of encrypt/MAC modes and
looks at existing implementations, it's "Building Secure Cryptographic
Transforms, or How to Encrypt and MAC" by Kohno, Palacio, and Black.  Google
tells me it's available from the IACR ePrint archive,

@_date: 2006-02-09 19:55:35
@_author: Peter Gutmann 
@_subject: Unforgeable dialog. 
I think a more general extension of this is "HTML allows the use of
arbitrarily sophisticated presentation attacks".  This definitely isn't a
capability you want to give to a malicious party, although it's way too late
to shut the barn door any more.

@_date: 2006-02-10 19:21:05
@_author: Peter Gutmann 
@_subject: general defensive crypto coding principles 
Well, that's the exact problem that I pointed out in my previous message - in
order to get this right, people have to read the mind of the paper author to
divine their intent.  Since the consumers of the material in the paper
generally won't be expert cryptographers (or even inexpert cryptographers,
they'll be programmers), the result is a disaster waiting to happen.

@_date: 2006-02-14 03:24:09
@_author: Peter Gutmann 
@_subject: general defensive crypto coding principles 
Unfortunately that doesn't work in the real world for two reasons:
1. There are a great many special-case situations where no published protocol
   fits.  As the author of a crypto toolkit, I could give you a list as long
   as your arm of user situations where no existing protocol can be applied
   (I'd prefer not to, because it's a lot of typing).  The reason why existing
   protocols don't fit isn't because they're deficient in some way, but
   because there are so many specialised applications where security is needed
   that it isn't really possible to accomodate all of them in a single, or
   small set of, protocols - sometimes you have to do a custom design.
2. Published standards, at least IETF and ISO ones, don't include any
   rationale, and usually don't include implementation guidance either (this
   is a pet peeve of mine).  Sometimes they don't even include critical
   security-related information.  There was one wonderful example in IPsec
   where a major implementation got some feature that the spec never bothered
   to specify wrong.  In the absence of any guidance in the spec, there was a
   50:50 chance of doing it wrong, and this particular implementation happened
   to have the coin fall the wrong way when they were deciding on what to do.
   When asked about why the spec never specified how to handle this, one of
   its authors replied that this was a well known problem and everyone should
   know what to do.  So implementors were expected to read the authors' minds
   to get it right.
So saying it's a simple case of waving an expert cryptographer over the
problem doesn't really work.  There's a wonderful quote about this approach by
Marv Schaefer, "to get a truly secure system, you must ensure that it's
designed and built by geniuses.  Unfortunately, geniuses are in short supply".
It's better to design a system that can be used by the average user than one
that's brittle enough that only geniuses can safely employ it.

@_date: 2006-02-15 19:06:56
@_author: Peter Gutmann 
@_subject: HDCP support in PCs is nonexistent now? 
The extra item was just a little serial EEPROM with per-device
keying/entitlement info.  All the HDCP stuff (the challenge-response handshake
over the DVI link and the actual content en/decryption and MAC'ing) are
performed by the DVI controller chip using data from the external EEPROM.
Since adding the EEPROM would have added a few tens of cents to the cost of
the hardware (the hardware itself, the manufacturing cost, the cost of
personalising and testing each EEPROM, and the fact all portions of the
manufacturing process that come into contact with keys require special
security measures), there was no clear idea when anyone could actually use it,
and there would no doubt be interop problems once devices did hit the market,
leading to product returns from dissatisfied customers, manufacturers skipped
the cost and overhead of adding it.  It makes sense really, why add something
that's both completely useless to users and a potential liability to
manufacturers?  If I were creating the devices, I'd have done the same thing.
It's like region coding in DVD players, you reduce cost and add value by *not*
including it.

@_date: 2006-02-16 18:54:21
@_author: Peter Gutmann 
@_subject: the return of key escrow? 
That's one way of looking at it.  It's not really a backdoor, it's a way of
spiking DRM.  If the UK government can be scared into requiring that Windows
Vista not be fully DRM-enabled (by whatever means necessary), then that's a
good thing.  Waving the four horsemen at them is a good way of achieving this
- the horsemen have been used for years to justify restrictive computer laws,
now (for once) they're being used to try and combat restrictions.  So we
hould be supporting this, not condemning it.  Maybe someone with a
congresscritters ear in the US could get the same thing adopted over there.
The horsemen are bigger than Hollywood.

@_date: 2006-02-27 19:23:36
@_author: Peter Gutmann 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
Something's getting lost in this description.  What does the value in the
"From" field have to do with you decrypting a message?  OTOH the mention of an
"attachment" indicates a detached S/MIME signature, which doesn't have
anything to do with encryption.  If it is a signature, then the software
should verify it with the included cert and display that as the signer.
Please correct and resubmit.

@_date: 2006-03-01 04:20:43
@_author: Peter Gutmann 
@_subject: What's the easiest way to crack an RSA key? 
Answer: Use google.
 yields just under *four thousand* OpenSSL private key files.  Admittedly some
of these are test keys, but it looks like many of them aren't.
(I doubt this is restricted to OpenSSL.  If there was a way to search for
 registry keys via Google, I'm sure we'd find a vast mass of IIS and whatnot
 keys as well).
Peter (thanks to anon for the info).

@_date: 2006-01-15 19:04:46
@_author: Peter Gutmann 
@_subject: Echelon papers leaked 
In 1996, New Zealander Nicky Hager wrote a book "Secret Power" containing a
great deal of information on Echelon, with a particular NZ perspective.  A few
days ago, papers held by the Prime Minister of the time were accidentally
released and appeared in the Sunday Star Times.  Some quotes from the story at
  The top-secret intelligence report found among David Lange's papers shows
  New Zealand had been spying on friendly countries throughout the region.
  Targets included Japanese and Philippines diplomatic cables and the
  government communications of Fiji, the Solomons, Tonga and "international
  organisations operating in the Pacific".
  The Government Communications Security Bureau's 1985/86 annual report also
  reveals that one of New Zealand's main targets was "UN diplomatic" cables,
  but which agencies of the United Nations were targeted is not stated.
  [...]
  "A total of 171 reports were published, covering the Solomons, Fiji, Tonga
  and international organisations operating in the Pacific. The raw traffic
  for this reporting provided by NSA the US National Security Agency)."
  The GCSB also produced 238 intelligence reports on Japanese diplomatic
  cables, using "raw traffic from GCHQ/NSA sources". This was down from the
  previous year: "The Japanese government implementation of a new high grade
  cypher system seriously reduced the bureau's output." For French government
  communications, the GCSB "relied heavily on (British) GCHQ acquisition and
  forwarding of French Pacific satellite intercept".
  [...]
  Each page of the 31-page report that mentioned eavesdropping operations was
  headed "TOP SECRET UMBRA HANDLE VIA COMINT CHANNELS ONLY". COMINT stands for
  "communications intelligence".
There's also a second story at
covering US pressure on NZ over its anti-nuclear policy.

@_date: 2006-01-26 12:34:33
@_author: Peter Gutmann 
@_subject: Kama Sutra Spoofs Digital Certificates 
If you track down the original Fortinet advisory you'll see that the Information-
Week text is slightly misleading, all it does is set the "this control is all right" flags in the registry to make Windows think it's passed a signature check
at some point in the past.

@_date: 2006-01-28 20:24:49
@_author: Peter Gutmann 
@_subject: thoughts on one time pads 
For no adequately explored reason I've tried various ways of physically
destroying CDs:
- Hammer on hard surface: Leaves lots of little fragments, generally still stuck
  together by the protective coating.
- Roasting over an open fire: Produces a Salvador Dali effect until they catch
  fire, then huge amounts of toxic smoke ("fulfilling our carbon tax quota"
  was one comment) and equally toxic-looking residue.
- Propane torch: Melts them without producing combustion products.
- Skilsaw: Melts them together at the cutting point, rest undamaged.
- Axe: Like skilsaw but without the melting effect.
- Using the propane torch and hammer to try and drop-forge a crude double-
  density CD: Messy.

@_date: 2006-07-03 14:31:10
@_author: Peter Gutmann 
@_subject: Use of TPM chip for RNG? 
You have to be pretty careful here.  Most of the TPM chips are just rebadged
smart cards, and the RNGs on those are often rather dubious.  A standard
technique is to repeatedly encrypt some stored seed with an onboard block
cipher (e.g. DES) as your "RNG".  Beyond the obvious attacks (DES as a PRNG
isn't particularly strong) there are the usual paranoia concerns (how do we
know the manufacturer doesn't keep a log of the seed and key?) and stupidity
concerns (all devices use the same hardwired key, which some manufacturers
have done in the past).  There are also active attacks possible, e.g. request
values from the device until the EEPROM locks up, after which you get constant
"random" values.  Finally, some devices have badly-designed challenge-response
protocols that give you an infinite amount of RNG output to analyse, as well
as helping cycle the RNG to lockup.
So the only hardware RNG I'd trust is one of the noise-based ones on full-
scale crypto processors like the Broadcom or HiFn devices, or the Via x86's.
There are some smart-card vendors who've tried to replicate this type of
generator in a card form-factor device, but from what little technical info is
available about generators on smart cards it seems to be mostly smoke and
(As an extension of this, the lack of access to a TPM's RNG isn't really any
great loss.  If it's there, you can mix it opportunistically into your own
RNG, but I wouldn't rely on it).

@_date: 2006-07-05 22:50:11
@_author: Peter Gutmann 
@_subject: Use of TPM chip for RNG? 
Exactly.  The FIPS 140 (strictly speaking X9.17/X9.31 PRNG) tests test a
generator's determinism, not its nondeterminism.  In other word they generate
a set of input/output pairs from a known-good generator and then make sure
that the generator being certified produces the same output.  Actually getting
nondeterminism into the process is quite tricky, and involves extremely
careful and creative reinterpretation of the "DT vector" (date-and-time)
input.  The non-creatively-interpreted generator depends for its strength
entirely on the key chosen for the PRNG.  If it's constant across all devices,
it'll pass the certification but its strength will be close to zero.

@_date: 2006-07-06 01:56:16
@_author: Peter Gutmann 
@_subject: Dirty Secrets of "noise based" RNGs 
Someone from HiFn discussed an older HiFn design based on ring oscillators
with postprocessing at the NIST RNG workshop in 2004,
  Newer designs are apparently
more sophisticated than this, but the details aren't easily available.  I feel
reasonably confident in their design, they know what they're doing.
Broadcom makes no documentation of any kind available.  Nothing to see here,
move along.
Via's stuff is currently the best-documented and best-analysed, and you know
what you're getting in the CPUs (you can read all the status info out of
Unfortunately the security techies are very much in the minority here, for
99.99% of customers "trust us, really" is fine.  For the vendors it's just too
much work to prepare and clear technical documentation for release when only a
handful of guys in an ivory tower somewhere will ever read it.  I've seen
documentation for one crypto device where it was obvious that it was an
internal doc that had been hastily cleaned up for publication because someone
somewhere had demanded it (some bits of the document had been passed over in
the clearing process, their lawyers would have had a fit).
Asking for these sorts of docs reminds me of the situation with the kernel
hackers who bug vendors for hardware technical data ("why on earth do you want
this information, we provide you with the drivers don't we?"), but with an
even harder case to make to the crypto hardware vendors.
That's why I'd never trust a single source of entropy for anything, but mix as
many sources as possible into a PRNG (safety through redundancy).  If you look
at the Skipjack RNG, the NSA seem to do the same thing, there are multiple
sources and even if one fails completely it won't destroy the usefulness of
the generator as a whole.

@_date: 2006-07-06 02:24:45
@_author: Peter Gutmann 
@_subject: Use of TPM chip for RNG? 
System integrators usually.  The way it works is that the company that fabs
the devices (typically Atmel, STMicroelectronics, or Infineon) create the
silicon.  Then a second-level vendor (say, Gemplus) load their firmware into
the basic device and bond out the serial lines (ISO 7816) or USB lines (USB
key) and then it's a GemSAFE card or a USB token (OK, Gemplus don't do USB
tokens, but you know what I mean).  Some companies (e.g. Infineon) do both
steps themselves.
For the TPM, you bond out the LPC lines instead of the USB or serial ones, and
load TPM firmware instead of smart-card firmware.
I'm simplifying that somewhat in that there isn't one single device into which
you load one set of firmware and it's a TPM and another set of firmware and
it's a smart card.  Smart cards and TPMs are part of the same family of
devices, where you might have 20 variants on the same basic device with 18 of
the variants targeted for smart-card use and 2 targeted for TPM use.  Look at
Atmel's SecureAVRs for an example, there's a whole shopping-list of variations
on that (ROM/RAM/EEPROM/with or without bignum accelerator/etc), and some of
the shopping-list entries are targeted at TPM.  But under the hood the
97SCwhatever TPM is a 90SC-family SecureAVR with different firmware.  Same
with STM's ST19something smart card vs. ST19something-else TPM, and Infineon's
SLE66CX smart card vs. SLE66CX TPM - they're just smart cards with clever

@_date: 2006-07-26 16:45:45
@_author: Peter Gutmann 
@_subject: Crypto to defend chip IP: snake oil or good idea? 
implementing software-controlled antifuses.
I think this goes a step beyond DRM in that while DRM is putting something
valuable in a safe and giving it to an opponent to hold onto, this concept
allows the opponent to furnish the safe as well.  Good luck with that.

@_date: 2006-06-06 18:56:19
@_author: Peter Gutmann 
@_subject: Status of opportunistic encryption 
OK, it looks like there are several different views of what opportunistic
encryption actually is.  My definition was "I'd like to talk to X, with
encryption if available", which is what the STARTTLS/STLS/AUTH TLS upgrade
mechanisms do for POP/IMAP/SMTP/FTP.  In that sense no tunnel mechanism (at
least that I know of) can really do that, you'd need something like a STARTTLS
mechanism for L2TP (the non-opportunistic way of doing this is to run L2TP
over IPsec).  I don't know why anyone'd want to implement that, since it's
easier to just drop in your VPN app or device of choice.
The opportunistic encryption that OpenVPN gives you is manual rather than
automatic, since there's no way to upgrade "any protocol at all" to "any
protocol at all, but with encryption".  The reason it's opportunistic is
because it allows you to use the equivalent of unauthenticated DH (self-
signed/arbitrary-CA certificates) rather than putting you through the torture
test of obtaining and configuring a cert from a recognised CA (that's non-
opportunistic, and because it's so difficult, frequently just non-encryption).

@_date: 2006-03-06 03:13:39
@_author: Peter Gutmann 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
Hmm, that sounds like broken software then, since the (probabilistically)
unique keyID to locate the appropriate decryption or signature verification
key is included in the message/signature - you never have to look at the From:
address, and indeed trying to use it for key lookups would be a recipe for
disaster because of the problems you pointed out.

@_date: 2006-03-06 14:03:04
@_author: Peter Gutmann 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
Uhh, what does RFC 3280 have to do with PGP, which is what you said you were
using?  In any case if you are using X.509 certs, you match by subject DN (or
issuerAndSerialNumber for S/MIME), all of which serve the same function as the
PGP key ID.
You use the PGP keyID or X.509 issuerAndSerialNumber to look up the key or
certificate, then display as the signer the identity associated with the key
or certificate.  What's in the "From:" address never enters into it, although
your software may choose to warn if the From: address doesn't match the email
address associated with the key.

@_date: 2006-05-05 02:42:15
@_author: Peter Gutmann 
@_subject: fyi: Deniable File System - Rubberhose 
TrueCrypt is definitely deep tinfoil-hat crypto (I have an upcoming article on
disk-encryption software that goes into this in more detail).  That's rather
unfortunate, because technically it's probably the nicest OSS disk encryption
program around.

@_date: 2006-06-01 13:47:06
@_author: Peter Gutmann 
@_subject: Status of opportunistic encryption 
============================== START ==============================
Grab OpenVPN (which is what OpenSWAN should be), install, point it at the
target system, and you have opportunistic encryption.
Why do you need expensive certs?  It's opportunistic encryption, you generate
a self-signed cert on install and you're done.

@_date: 2006-11-04 13:06:54
@_author: Peter Gutmann 
@_subject: Can you keep a secret? This encrypted drive can... 
I agree that in most cases the access-time argument is a red herring.  Back
when I wrote SFS (DOS-based FDE that ran on 386's), I got plenty of feedback
from users that the slowdown was barely or not noticeable.  The only
time I've really noticed it (using current FDE software, not on a 25 MHz 386)
is when copying large amounts of data onto an encrypted partition, but that's
(a) a very rare event and (b) somewhat slow anyway even for an unencrypted

@_date: 2006-11-07 17:22:08
@_author: Peter Gutmann 
@_subject: Can you keep a secret? This encrypted drive can... 
That's because you're doing something that produces worst-case behaviour.  The
(obvious) solution is the standard "don't do that, then".  My main development
machine builds to a RAM drive, and for some odd reason I don't notice any disk
access latency at all.
"Realising the importance of the case, my men are applying twice the usual
 amount of tinfoil".

@_date: 2006-11-08 02:41:14
@_author: Peter Gutmann 
@_subject: Can you keep a secret? This encrypted drive can... 
HKLM\System\CurrentControlSet\Control\FileSystem\NtfsDisableLastAccessUpdate =
1, but this probably shouldn't be necessary because for temp files Windows
will try to avoid creating the file on disk unless it runs out of file buffer

@_date: 2006-11-13 19:39:44
@_author: Peter Gutmann 
@_subject: Citibank e-mail looks phishy 
I think "Citibank aims at foot and lets loose with both barrels, then reloads
and shoots a second time" would be a better title.  This is a really scary
example of what Perry once referred to as banks actively training users to
become future victims of phishing attacks.  What's even worse is that Citibank
uses such a profusion of marketing-driven vaguely bank-related domain names
(e.g. accountonline.com, although this now seems to have been shut down) that
the email could just as easily have directed users to .com without raising too much suspicion.  Any half-awake phisher will
immediately send out an identical email sending people to some other vaguely
correct-looking URL and asking for the same information.

@_date: 2006-11-14 22:07:28
@_author: Peter Gutmann 
@_subject: Citibank e-mail looks phishy 
Here's another one... United Airlines send out email to UA flyers directing
them to 2006elitechoice.com for frequent flyer benefits.
2006elitechoice.com is registered to Srirangapatna Chandrashekar of Grey
Direct, Chicago.  There are no indications on united.com of any connection to
Querying United about this yields an emailed reply from usa.net telling you
that it's legit and not to worry.
Apparently this is legit, but the whole thing is less credible than many
phishing scams.

@_date: 2006-11-17 16:54:18
@_author: Peter Gutmann 
@_subject: Citibank e-mail looks phishy 
That's because it's really, really hard to get papers that document negative
experiences accepted for publication.  My guess is that this is because if you
write a paper documenting (say) negative experience with cryptoFoo, it's going
to be sent to cryptoFoo-savvy reviewers for evaluation (obviously), which
generally means ones involved in cryptoFoo development.  They aren't going to
take too well to a paper describing how hard their baby is to implement (I've
seen this effect several times while serving on program committees).  The
Oakland paper was on a topic where everyone was having problems with the
technology (so it was acceptable to say "this stuff doesn't work"), and came
at the tail end of the trusted-windowing-system period where it was
permissible to publish non-positive retrospectives.

@_date: 2006-11-20 15:40:27
@_author: Peter Gutmann 
@_subject: A New Vulnerability In RSA Cryptography 
That's not quite accurate.  What it did was succeed against a an old version
of OpenSSL that (a) didn't have the protections present yet and (b) had been
specially modified to make it vulnerable to the attack.  It's a nice attack,
but based on what's been published so far the claims of RSA's demise are
considerably exaggerated.
What it does is rely on the fact that on a HT P4, if you saturate the branch
target buffer (BTB) from a second thread running in the same pipeline (i.e. on
the same HT CPU), you can see when BTB misses occur in the RSA thread and
therefore observe whether it's branching on a one or zero bit.
To do this, they had to use (as mentioned above) a rather old version of
OpenSSL that doesn't employ any protection against this type of attack.  In
addition they reduced the modexp window size from 5 to 1 (to make sure you get
a branch for each bit, with the standard window size 5 the branches are
replaced by a table lookup), and they disabled the CRT code (to force use of
the textbook-mode RSA operation that, in practice, no software implementation
ever uses).
This isn't to say that the paper doesn't point out a potential vulnerability.
However, saying "we broke RSA" or "we broke OpenSSL" is pushing things a bit.

@_date: 2006-09-11 11:14:20
@_author: Peter Gutmann 
@_subject: Exponent 3 damage spreads... 
Ben Laurie  quotes:
What you really meant to say was "most of the vanishingly small proportion of
the world that bothers with DNSSEC", right?  So the real vulnerability level
is down somewhere lost in the noise level.

@_date: 2006-09-14 17:27:48
@_author: Peter Gutmann 
@_subject: Exponent 3 damage spreads... 
Can you make a guess at what it is?  Is it the fact that you can have NULL
parameters for algorithms or optionally non-NULL parameters?  Changing this
could be tricky because there are all sorts of inconsistencies both in
standards and implementations, the standard practice has been to skip the
parameters field because if you don't, things break.

@_date: 2006-09-14 23:40:49
@_author: Peter Gutmann 
@_subject: Exponent 3 damage spreads... 
In that case (and because of something else I thought of after I posted, I was
just heading out for dinner at the time) I think it's game over for RSA e=3
(see below).
It may or may not be used, depending on which standard you follow.  First of
all, even for the simple case of SHA-1, the parameters can be present or not.
See the note in RFC 3274:
  There are two possible encodings for the [...] parameters field which arise
  from the fact that when the 1988 syntax for AlgorithmIdentifier was
  translated into the 1997 syntax, the OPTIONAL associated with the
  AlgorithmIdentifier parameters got lost.  Later it was recovered via a
  defect report, but by then, everyone thought that algorithm parameters were
  mandatory.  Because of this, some implementations will encode null
  parameters as an ASN.1 NULL element and some will omit them entirely (see
  for example section 12 of CMS [RFC2630]).
So for both standards and implementations it's pretty much a coin-toss (crap-
shoot if you're in the US) as to what you'll find there.  Because of this,
standard practice in implementations has been to skip any parameters.
But wait, there's more!  Some AlgorithmIdentifiers have parameters that you
can set to either a large range of fixed values or even arbitrary values.  For
example the Ascom Tech IDEA AlgoID (which admittedly isn't a hash algorithm,
but bear with me) has three optional parameters for CFB mode which can take
various values and may or may not be present (AES almost took this path as
well, luckily NIST decided against it at the last minute and went with simple,
streamlined parameters).
OK, so you don't sign IDEA values so this isn't a problem.  However, a related
AlgoID is the DES one, which for CBC mode has as parameter an IV, which is 64-
bits of arbitrary user-chosen data.  I've seen a banking security system from
either Belgium or the Netherlands that signs CBC-MACs (I'm using a borrowed
machine to send this so I can't provide names and numbers at the moment, it's
from memory), and knowing the organisation that produced it it wouldn't
surprise me if they also used keys with e=3.
So lets extend this further.  There have been a pile of designs for hash
algorithms with user-definable parameters.  If these ever get used in
standards then no doubt there'll be AlgoIDs defined that allow an attacker to
set arbitrary values in the AlgoID through them.  So the arms-race of trying
to track invalid data now becomes a problem of proving a negative, i.e.
proving that there isn't some AlgoID out there somewhere that allows you to
set one or two parameter bytes to arbitrary values.
But wait, there's more!  From what I understand of the attack, all you need
for it to work is for the sig.value to be a perfect cube.  To do this, all you
need to do is vary a few of the bytes of the hash value, which you can do via
a simple brute-force search.  So even with a perfect implementation that does
a memcmp() of a fixed binary string for all the data present but the hash, the
attack still works.
In either of these cases, RSA e=3 is dead.  Obesa cantavit.
So the fix isn't an ongoing penetrate-and-patch arms race to try and filter
out more and more hard-to-find possibilities, it's to immediately deprecate e=
3.  Grab a pile of IETF boilerplate, add a single sentence "Don't use RSA with
e <= 3" (I'd actually say "<= 17" since there's no good reason not to), and
apply it as a BCP by reference to SSL/TLS, IPsec, S/MIME, PGP, DNSSEC, and so
on.  There'll always be broken standards out there that require e=3 (I know of
at least one that uses e=2, and another that uses raw, unpadded RSA, and
another that... well you get the idea), but the only quick, sure fix is to
kill e=3, not to try and anticipate every potential way of trying to use it,
because you'll never be secure that way.

@_date: 2006-09-15 11:18:57
@_author: Peter Gutmann 
@_subject: Exponent 3 damage spreads... 
Right, but it's been pure luck that that particular implementation (and most
likely a number of others) happen to have implemented only a small number of
hash algorithms that allow only absent or NULL parameters.  Anything out there
that implements a wider range of algorithms, including any that allow
parameters, is most likely toast.
What's more scary is that if anyone introduces a parameterised hash (it's
quite possible that this has already happened in some fields, and with the
current interest in randomised hashes it's only a matter of time before we see
these anyway), then changing a simple AlgoID definition in one part of the
code is going to suddently break signatures 23 code modules away in a
different directory.  Will whoever's responsible for maintaining the hash
AlgoID table in some far-removed code module in several years' time know that
any change they make could cause a security breach via a non-obvious mechanism
in a far-distant piece of code?  This is just going to keep coming back and
biting us again and again unless we default to deny-all.
The e=3 issue is like the old war movies where someone steps on a mine and
hears the click, but isn't quite sure yet how long it'll be before they spread
themselves decoratively around the countryside.  We've heard the click, it's
time to get out of the minefield.

@_date: 2006-09-15 11:32:50
@_author: Peter Gutmann 
@_subject: Why the exponent 3 error happened: 
It's not just this, sometimes the acceptance of not-quite-correct data is
necessary, because if you don't do it, your implementation breaks.  To take
one well-known example, Microsoft have consistently encoded the version info
in their SSL/TLS handshake wrong, which in theory allows rollback attacks.  So
as a developer you have the following options:
 1. Reject the encoding and be incompatible with 95(?)% of *all* deployed SSL
    clients (that's *several hundred million* users).
 2. Turn a blind eye and interoperate with said several hundred million users,
    at the expense of being vulnerable to rollback attacks.
I'm not aware of a single implementation that takes option 1, simply because
it would be pure marketplace suicide to do so.
The same goes for pretty much every other security protocol I know of.  SSH is
the most explicit since clients and servers exchange ASCII strings before they
do anything else, all SSH implementations have a built-in database of known
bugs that they adjust their behaviour for when they detect a certain client or
server.  Obviously no-one will implement this bug-compatibility for a security
hole, but it's not impossible that some of this extended flexibility may at
some point lead to a problem.
For other protocols, it works in reverse, you recognise the peer implemention
by the bugs, not the other way round.  Beyond straight implementation bugs
though are standards that require insecure or ambiguous behaviour, either by
accident (which eventually gets fixed), or because of design-by-committee
politics, about which enough has probably been said in the past
(cough*IPsec*cough :-).

@_date: 2006-09-15 20:49:31
@_author: Peter Gutmann 
@_subject: A note on vendor reaction speed to the e=3 problem 
When I fired up Firefox a few minutes ago it told me that there was a new
update available to fix security problems.  I thought, "Hmm, I wonder what
that would be...".  It's interesting to note that we now have fixes for many
of the OSS crypto apps (OpenSSL, gpg, Firefox (via NSS, so probably
Thunderbird as well), my own cryptlib), but nothing from any of the commercial
vendors.  Maybe someone should convert this into a DRM attack so Microsoft
will fix it before 2007 :-).
(The real  for me is that I wanted to turn off e=3 years ago, but when
I did it in a snapshot release some squawk piped up to say that they were
using e=3 and the standard said it was OK and I was being non-standards
compliant and so on and so forth, so in the end I had to leave it enabled.  I
did make it very easy to turn off with a single-character code change, but
that may explain why commercial vendors are going to be reluctant to rush out
a fix without a lot of prior impact assessment).

@_date: 2006-09-16 00:44:47
@_author: Peter Gutmann 
@_subject: Exponent 3 damage spreads... 
To make this easier to work with, I've combined them into a PKCS  cert chain
(attached).  Just load/click on the chain and see what your app says.
(As an aside, this chain is invalid for an entirely unrelated reason, so no
standards-compliant PKI application should validate this chain even if the
signature did check out.  I wonder how many current apps will detect this?
See, you don't even need PKCS  padding tricks to fool a PKI app... :-).
[2. application/octet-stream; bad_chain.der]...

@_date: 2006-09-16 01:15:52
@_author: Peter Gutmann 
@_subject: Why the exponent 3 error happened: 
Might I refer the reader to   I've even
received mail from address-harvesting bots that begin "Dear Paranoid, ...".

@_date: 2006-09-16 05:35:27
@_author: Peter Gutmann 
@_subject: A note on vendor reaction speed to the e=3 problem 
How does it handle the NULL vs.optional parameters ambiguity?

@_date: 2006-09-16 14:03:32
@_author: Peter Gutmann 
@_subject: Exponent 3 damage spreads... 
Apparently the included cert chain didn't make it (it looks like smaller stuff
like the certs in the original message is OK, but the larger chain got
stripped by the list software).  I've therefore it it online at
 for anyone
who wants to play with it.

@_date: 2006-09-16 14:08:34
@_author: Peter Gutmann 
@_subject: A note on vendor reaction speed to the e=3 problem 
Ah, OK, and it uses the NULL-parameters interpretation (section 5.2.2), which
would actually be incorrect according to the current standards but at least
it's unambiguous.

@_date: 2006-09-21 07:12:56
@_author: Peter Gutmann 
@_subject: Exponent 3 damage spreads... 
No, it's a bug in the spec:
Nothing in there about trailing garbage.
Someone else said that the spec requires creating an encoded form of the ASN.1
data and then comparing the encoded form.  Let's look at the spec again:
Again, there's nothing in there that requires comparing the encoded form.
You may be wondering why this differs somewhat from RFC 3447.  That's because
this is the original "gold standard" PKCS  spec, the one that everyone was
using when PGP, PKCS  SSL, and possibly DNSSEC were created.  True,
since then other variants of the spec have imposed slightly different
requirements, but the gold standard version that's contemporaneous with the
security protocols in which it's caused problems doesn't require the measures
that people have been claiming.
(And I really, *really* don't want to get into some pointless debate over
whose bits are more standards-compliant.  All I'm pointing out is that a
blanket assertion that "doing/not doing X is a violation of the spec" is an
invalid claim.  At best you can say "doing X is a violation of one variant of
the spec that didn't actually exist when the affected protocols were
No, you've implemented PKCS  exactly as the specification says.  Look up
the definition of AlgorithmIdentifier.
Because everyone else on the planet is doing it, and (with a few caveats) it's
worked so far.  If you're quite happy to not be able to talk to anything else
in existence, you're welcome to use PSS or whatever's in fashion at the
moment.  Since my users expect to be able to, you know, talk to one another,
I'll stick with PKCS It's not at all misguided.  This whole debate about trying to hang on to e=3
seems like the argument about epicycles, you modify the theory to handle
anomalies, then you modify it again to handle further anomalies, then you
modify it again, and again, ...  Alternatively, you say that the earth
revolves around the sun, and all of the kludges upon kludges go away.
Similarly, the thousands of words of nitpicking standards, bashing ASN.1, and
so on ad nauseum, can be eliminated entirely by following one simple rule:
  Don't use e=3
This is never going to be reliably fixed if the "fix" is to assume that every
implementor and implementation everywhere can get every miniscule detail right
every time.  The fix is to stop using e=3 and be done with it.  Even Microsoft
finally got this message after 14 years of getting RC4 wrong, they simply
banned its use because they realised that they'd never get it fixed even if,
in theory, it's easy to get right on paper.

@_date: 2006-09-21 20:52:09
@_author: Peter Gutmann 
@_subject: Exponent 3 damage spreads... 
Uhh, did you actually read the rest of my post?  *One variant of the PKCS spec, that didn't exist at the time the the affected other standards were
created*, talks about ..., not "the PKCS  spec" as a whole.  I even quoted
the original text of the spec in my message.

@_date: 2006-09-22 04:28:14
@_author: Peter Gutmann 
@_subject: Exponent 3 damage spreads... 
I don't think it's a problem, you just take the ASN.1 DigestInfo value, since
the trailing garbage isn't part of the DigestInfo, you ignore it.
Specifically, the ASN.1 object is entirely self-contained, so you can tell
exactly where it ends and what it contains.  Anything outside it is beyond the
scope of this specification :-).
(When the spec was written, I think the thought that someone would append
trailing garbage never cropped up, so it's never explicitly addressed).
Yup :-).

@_date: 2006-09-22 20:20:28
@_author: Peter Gutmann 
@_subject: fyi: On-card displays 
Oh, so you were the Mondex user!  I've always wondered who that was.

@_date: 2006-09-23 01:43:11
@_author: Peter Gutmann 
@_subject: Exponent 3 damage spreads... 
Almost no-one does this anyway, but I don't think that's much help.
I can't think of any other way to get people to move away from e=3.  The
problem isn't major implementations who use e=F4 and check signatures properly
(at least as of a week or so back :-), it's the hundreds (or thousands?) of
random obscure implementations and deployments that'll never even hear about
this and will never be fixed, and so will remain vulnerable in perpetuity
without even knowing it.  Unless things break obviously, there's no incentive
(or even apparent need) to fix it.

@_date: 2006-09-23 01:50:19
@_author: Peter Gutmann 
@_subject: Exponent 3 damage spreads... 
No, because the compiler doesn't use a TLV encoding as ASN.1 does.  Take
something like an MPEG, JPEG, MP3, or some other piece of data that uses a TLV
encoding with a clearly defined (by the data) end.  Cat some extra garbage
onto the end of it, and try and view/play it.  If it's a proper TLV encoding
(rather than a TV encoding, i.e. no explicit length info included) then the
decoder/player will ignore the extra garbage at the end*.  That's the whole
point of TLV encodings, you know a priori when to stop, and stop exactly at
that point.
* There are some that might keep reading in the hope that something else crops
  up, I don't know, but the ones I've looked at don't care if there's extra
  junk at the end.  This goes back at least as far as CP/M, where you could
  have extra ^Z's and other junk at the end of files that apps had to avoid
  ploughing into.

@_date: 2006-09-23 02:25:31
@_author: Peter Gutmann 
@_subject: Exponent 3 damage spreads... 
Yet another e=3 attack, although this one is a bit special-case.  As Burt
Kaliski points out in his paper on hash function firewalls,
if you can control the AlgorithmIdentifier (specifically the object identifier
or OID), you can also inject arbitrary bits into the signature.  This works as
1. Create your forged e=3 signature using extra chosen garbage data.
2. Register an object identifier for the hash algorithm that contains the
   extra data, thus allowing you to retro-create the forged signature using
   "legitimate" data.
3. Profit!
The use of multiple OIDs to identify a single algorithm is relatively common
(see the OID table for dumpasn1, there are something like a dozen overlapping
OIDs for DSA alone), all you need to do is get one registered and adopted.
Sure, it's a bit of work, but if implemented no amount of checking will catch
it, since it's a perfectly valid, legitimate OID and encoding.
(I know of at least one registered OID that was back-engineered to contain an
 particular interesting bit pattern, and I've seen it used in several
 implementations, so this isn't that far-fetched an attack).
Oh yes, and before the ASN.1-bashing starts again, this affects any encoding
scheme, it's not some "ASN.1 problem".

@_date: 2006-09-23 02:40:03
@_author: Peter Gutmann 
@_subject: Exponent 3 damage spreads... 
Damn, beat me to it :-).
The problem is that no amount of checking can catch this.  If you register the
OID or otherwise get it into some standard somewhere, then it's kosher as far
as anyone's concerned.  There's no "check" that can catch it if you're
required (by a standard, by a client, by bilateral agreement, etc) to accept
that OID.
(There's been at least one case where random OIDs have been used in the past.
 Since it's a pain to register them, a large vendor generated them randomly
 beneath an arc registered to them.  Although this is kind of weird and I'm
 sure was never meant to be done this way, there's nothing inherently invalid
 about this).

@_date: 2007-04-06 02:36:12
@_author: Peter Gutmann 
@_subject: DNSSEC to be strangled at birth. 
I realise this is a bit of a cheap shot, but:
How will this be any different from the current situation?

@_date: 2007-08-19 19:44:28
@_author: pgut001@cs.auckland.ac.nz 
@_subject: =?UTF-8?B?QU1EcyBuZXcgaW5zdHJ1Y3Rpb25zIGZvciBwYXJhbGxlbGlzbSBhbmQ=?=?UTF-8?B?IHN1cHBvcnQgZsO2ciBzaWRlLWNoYW5uZWwgYXR0YWNrcz8=?= 
I think it's exactly the opposite, we're already having enough problems with
microarchitectural (MA) attacks without explicit diagnostic facilities built
into the CPU.  If you look at the AMD specs these extra ring3-accessible
facilities are only going to make it worse.  These attacks are essentially
impossible to defend against merely by modifying the victim code, the only
possible defences at the moment are:
1. "Don't do that then" (i.e. don't allow arbitrary untrusted code to run in
   parallel with your crypto ops).
2. With future hardware support, some mechanism for partitioning the CPU so
   that critical regions of code can run without leaving externally observable
   traces, ending with some sort of super-INVD/INVLPG instruction to clear all
   caches and buffers.  So the code would be something like:
    enter_secure_region
    [[[crypto code]]]
    INV_everything
    exit_secure_region
   Of course something like this would have to be accessible from ring 3,
   which makes it a built-in DoS mechanism.
So "don't do that then" seems to be the only fix for this (not including the
usual blue-sky response of everyone having built into their system).

@_date: 2007-08-18 21:26:46
@_author: pgut001 
@_subject: =?UTF-8?B?QU1EcyBuZXcgaW5zdHJ1Y3Rpb25zIGZvciBwYXJhbGxlbGlzbSBhbmQ=?= =?UTF-8?B?IHN1cHBvcnQgZsO2ciBzaWRlLWNoYW5uZWwgYXR0YWNrcz8=?= 
I think it's exactly the opposite, we're already having enough problems with
microarchitectural (MA) attacks without explicit diagnostic facilities built
into the CPU.  If you look at the AMD specs these extra ring3-accessible
facilities are only going to make it worse.  These attacks are essentially
impossible to defend against merely by modifying the victim code, the only
possible defences at the moment are:
1. "Don't do that then" (i.e. don't allow arbitrary untrusted code to run in
   parallel with your crypto ops).
2. With future hardware support, some mechanism for partitioning the CPU so
   that critical regions of code can run without leaving externally observable
   traces, ending with some sort of super-INVD/INVLPG instruction to clear all
   caches and buffers.  So the code would be something like:
    enter_secure_region
    [[[crypto code]]]
    INV_everything
    exit_secure_region
   Of course something like this would have to be accessible from ring 3,
   which makes it a built-in DoS mechanism.
So "don't do that then" seems to be the only fix for this (not including the
usual blue-sky response of everyone having built into their system).

@_date: 2007-08-23 17:00:44
@_author: pgut001 
@_subject: interesting paper on the economics of security 
A topic very similar to this came up recently on the hcisec list.  My comments
there were:
We already have really, really good metrics for this.  It's called the
commercial malware industry (blatant ad: see my Defcon talk from last week for
examples of exploit sales and pricing models).  To find out how secure
something is, look at how much exploits for it are selling for on the black
market.  I've been thinking of doing a maverick paper for next years MetriCon
about this [0], for example although OS X is veritable smorgasbord of 0days
the market value of these is close to zero because everyone's targetting
Windows instead.  A prime example of this is Safari, it was 0dayed within two
hours of the Windows version appearing, yet the same flaws had lain dormant in
the OS X version (presumably) for years because there's little to no
commercial interest in exploiting Macs.  So it could be argued that the best
real-world metric that we have for security comes from the attackers, not the
(Incidentally, this powerful real-world metric is telling us that the
existing browser security model is indistinguishable from placebo :-).
[0] This should not be construed as a promise of a paper appearing.  I'm not
    sure whether I could get enough material to make an interesting paper.

@_date: 2007-08-31 18:23:57
@_author: Peter Gutmann 
@_subject: World's most powerful supercomputer goes online 
This doesn't seem to have received much attention, but the world's most
powerful supercomputer entered operation recently.  Comprising between 1 and
10 million CPUs (depending on whose estimates you believe), the Storm botnet
easily outperforms the currently top-ranked system, BlueGene/L, with a mere
128K CPU cores.  Using the figures from Valve's online survey,
 for which the typical machine
has a 2.3 - 3.3 GHz single core CPU with about 1GB of RAM, the Storm cluster
has the equivalent of 1-10M (approximately) 2.8 GHz P4s with 1-10 petabytes of
RAM (BlueGene/L has a paltry 32 terabytes).  In fact this composite system has
better hardware resources than what's listed at  for the
entire world's top 10 supercomputers:
  BlueGene/L: 128K CPUs, 32TB
  Jaguar: 22K CPUs, 46TB
  Red Storm: 26K CPUs, 40TB
  BGW: 40K CPUs, 10TB
  New York Blue: 37K CPUs, 18TB
  ASC Purple: 12K CPUs, 49TB
  eServer Blue Gene: ?
  Abe: 10K CPUs, 10TB
  MareNostrum: 10K CPUs, 20GB
  HLRB-II: 10K CPUs, 39GB
This may be the first time that a top 10 supercomputer has been controlled not
by a government or megacorporation but by criminals.  The question remains,
now that they have the world's most powerful supercomputer system at their
disposal, what are they going to do with it?  And I wonder what the LINPACK rating for Storm is?

@_date: 2007-09-01 14:41:13
@_author: Peter Gutmann 
@_subject: World's most powerful supercomputer goes online 
Sure, absolutely, that's why I made the LINPACK comment at the end.  However
for the sorts of usages that malware authors might put it to (e.g. A5 rainbow
tables, as Daniel Schroeder suggested) it's probably going to be hard to beat.
The message was intended to draw attention to the frightening amount of raw
computing power that's now in the hands of people whose identities and motives
we're not even certain of.

@_date: 2007-09-01 15:46:45
@_author: Peter Gutmann 
@_subject: World's most powerful supercomputer goes online 
============================== START ==============================
I feel I should add a followup to the earlier post, this was implied by the
rhetorical question about what the LINPACK performance of a botnet is, but
I'll make it explicit here:
The standard benchmark for supercomputers is the LINPACK linear-algebra
mathematical benchmark.  Now in practice the LINPACK performance of a botnet
is likely to be nowhere near that of a specially-designed supercomputer, since
it's more a distributed grid than a monolithic system.  On the other hand bot-
herders are unlikely to care much about the linear algebra performance of
their botnet since it doesn't represent the workload of any of the tasks that
such a system would be used for.
Where Storm leaves every conventional supercomputer in the dust is in terms of
the sheer hardware resources (number of CPUs, amount of memory, and network
bandwidth) at its disposal.

@_date: 2007-12-04 21:53:52
@_author: Peter Gutmann 
@_subject: Flaws in OpenSSL FIPS Object Module 
That's a good problem statement for the dark side of "many eyes make bugs

@_date: 2007-12-06 16:09:23
@_author: Peter Gutmann 
@_subject: Flaws in OpenSSL FIPS Object Module 
You're misunderstanding the threat model.  The problem here is that commercial
vendors are in a panic because the certification of free OSS security tools is
allowing all sorts of riff-raff onto the previously exclusive US government
purchasing gravy train.  In order to keep the gravy train free of said riff-
raff, they've kept up a steady stream of objections to the certification based
on various nitpicks.  While it's possible to say "There's something we noticed
here in the source code that requires the software to be ejected from the
train", it's a bit harder to say "We spent three months reverse-engineering
someone else's proprietary protected intellectual property and think we may
have found something".
[1] Not my reference.
[2] Not my reference.

@_date: 2007-12-09 19:16:22
@_author: Peter Gutmann 
@_subject: More on in-memory zeroisation 
There was a discussion on this list a year or two back about problems in using
memset() to zeroise in-memory data, specifically the fact that optimising
compilers would remove a memset() on (apparently) dead data in the belief that
it wasn't serving any purpose.
Reading through "Secure Programming with Static Analysis", I noticed an
observation in the text that newer versions of gcc such as 3.4.4 and 4.1.2
treat the pattern:
  "memset(?, 0, ?)"
differently from any other memset in that it's not optimised out.  I couldn't
find any reference to this behaviour anywhere and asked the authors about
this.  They replied:
  We did experiments with a number of different compilers and optimization
  levels in order to determine the conditions under which calls to memset
  would be optimized out.  We stumbled on the fact that zero is treated
  differently pretty much by accident, and we don't have any particular
  insight into why it behaves the way it does.
I've done a bit of poking around in gcc discussion threads and there's a lot
of muttering about support for bcopy() and bzero(), and in particular its use
in places where it can't be replaced by memcpy()/memset() (e.g. assorted OS
kernels).  Speculating wildly, my guess is that the peculiar behaviour with
(?, 0, ?) parameters is a compatibility hack for bzero() support, so it may be
purely an implementation artefact.  For example if you look at
 it recommends:
   bzero(b,len) (memset((b), '\0', (len)), (void) 0)
so it's possible the gcc folks added the special memset() semantics because of
So it seems that at least with recent versions of gcc you *can* zeroise memory
provided you use the "memset(?, 0, ?)" pattern.  Unfortunately since it's
undocumented behaviour it's not certain whether this will be persistent in
future compiler versions.  Doing it this way does seem a nice compromise
between "I want maximum optimisation" and "I want maximum security".
Can anyone who knows more about gcc development provide more insight on this?
Could it be made an official, supported feature of the compiler?

@_date: 2007-12-14 18:52:11
@_author: Peter Gutmann 
@_subject: Intercepting Microsoft wireless keyboard communications 
... the battery runs down in a fraction of the time that it does for any other
keyboard on the market.
Would it be possible to use load modulation (e.g. ISO 14443, for which
transponders are readily available) to get around this, where the power for
the communication is supplied by the PC?  That way you could have a protocol
that's as chatty as you like without your keyboard ending up as an advertising
device to make your competitors' products look good.

@_date: 2007-12-18 00:52:13
@_author: Peter Gutmann 
@_subject: More on in-memory zeroisation 
When I said "a year or two" I meant for large values of two.

@_date: 2007-02-05 21:59:35
@_author: Peter Gutmann 
@_subject: OT: SSL certificate chain problems 
Like a lot of PKI, it's total pot-luck ("crapshoot" in the US I guess) as to
what a particular implementation does when it encounters this situation.  It
may work, it may not work, it may work under some circumstances, or it may do
anything in between.
(I've seen some implementations that require a "system rebuild" (meaning
reinstall all your PKI software with the new roots) to roll over roots, all
the way through to ones that handle the situation automatically.  There really
is no way to tell what a particular implemenation will do, apart from trying
it out and seeing what happens).

@_date: 2007-02-09 18:00:22
@_author: Peter Gutmann 
@_subject: One Laptop per Child security 
Just a general thought, it seems like the OLPC security design is a real-world
implementation of Bill Cheswick's "Windows OK" proposal.  See for example
 for more on this (modulo
the comments on "feature starvation", which don't apply to the OLPC design).

@_date: 2007-01-05 19:52:21
@_author: Peter Gutmann 
@_subject: Tamperproof, yet playing Tetris. 
there was no online credit-card based Internet payment system.  This was
before STT and SEPP and SET and all the others.  There were things like
Cybercash, but they were too complex to make much headway.
There was however one company that could set up anyone to do live credit card
processing over the Internet (they had a travelling dog & pony show where they
could demonstrate this to potential customers).  This was (for the time)
pretty amazing, something that no major CC vendor could offer.
What they had done was set up an Internet front-end to hacked "tamperproof"
POS terminals that effectively turned them into Internet-controlled remote
payment devices, so as far as the acquirer was concerned the purchaser had
swiped their card at the terminal and entered their PIN when in fact it was
someone sitting at a laptop on the other side of the world.

@_date: 2007-01-20 22:10:47
@_author: Peter Gutmann 
@_subject: It's a Presidential Mandate, Feds use it. How come you are not using FDE? 
Just a word in OpenSSL's defence, see the X.509 Style Guide for the reasoning
behind this.  I don't think any ASN.1-using security toolkit since TIPEM has
done character-set checking, it would fail to verify a large chunk of the
certs out there (I once had a TIPEM user complain to me that they had to stop
using it specifically because it would reject invalid character strings, which
encompassed a nontrivial portion of their user base).

@_date: 2007-01-23 14:26:36
@_author: Peter Gutmann 
@_subject: analysis and implementation of LRW 
Is there any more information on this anywhere?  I haven't been able to find
anything in the P1619 archives (or at least not under an obvious heading).

@_date: 2007-01-23 19:09:25
@_author: Peter Gutmann 
@_subject: MS responds to Gutmann's Vista paper 
Their response is a mixture of technical content and PR handwaving, I've
responded to the latter as part of the Vista writeup at
 and will
be integrating the technical clarifications into the body of the writeup when
I get time

@_date: 2007-01-24 14:44:59
@_author: Peter Gutmann 
@_subject: analysis and implementation of LRW 
David Wagner Actually there's a lot more to it than that, the original analysis was posted
by Quantum crypto guy Matt Ball (that's the drive manufacturer Quantum, not
quantum crypto) in late 2005:
  with a followup in early 2006:
  So it's not a case of "google is your friend", it's "'knowing which magic
incantation to type into google to find what you're looking for' is your
Anyway, it's a pretty detailed analysis, well worth reading.

@_date: 2007-01-24 14:51:54
@_author: Peter Gutmann 
@_subject: analysis and implementation of LRW 
to not standardise something that's very brittle (think RC4).  For example in
a later followup the same person who pointed out the LRW issues thought that
one widely-deployed implementation, TrueCrypt, might have fallen into this
trap.  Luckily it didn't, but it was a sign that LRW may be just a bit too
brittle to safely deploy, particularly when the intended audience is embedded
systems and ASIC engineers and not cryptographers.  So the current
recommendation is to go to XTS (sometimes, confusingly, referred to as XEX),
which can be implemented using existing IP blocks developed for AES-GCM.
There are already several vendors shipping IP for AES-XTS.

@_date: 2007-01-25 21:30:02
@_author: Peter Gutmann 
@_subject: more on NIST hash competition 
The AES competition was already a severe resource drain, running another one
for an AHS would have been prohibitive, until the clear signs that SHA was in
real trouble made it more palatable.

@_date: 2007-01-26 19:06:00
@_author: Peter Gutmann 
@_subject: OT: SSL certificate chain problems 
In some cases it may be useful to send the entire chain, one such being when a
CA re-issues its root with a new expiry date, as Verisign did when its roots
expired in December 1999.  The old root can be used to verify the new root.

@_date: 2007-01-27 14:12:34
@_author: Peter Gutmann 
@_subject: OT: SSL certificate chain problems 
Because the client may not have the new root yet, and when they try and verify
using the expired root the verification will fail.
(There's a lot of potential further complications in there that I'm going to
 spare people the exposure to, but that's the basic idea).

@_date: 2007-01-31 13:57:04
@_author: Peter Gutmann 
@_subject: OT: SSL certificate chain problems 
You use the key in the old root to validate the self-signature in the new
root.  Since they're the same key, you know that the new root supersedes the
expired one.

@_date: 2007-07-01 21:55:58
@_author: Peter Gutmann 
@_subject: Quantum Cryptography 
One threat model (or at least failure mode) that's always concerned me deeply
about QC is that you have absolutely no way of checking whether it's working
as required.  With any other mechanism you can run test vectors through it,
run ongoing/continuous self-checks, and (in the case of some Type I crypto)
run dual units in parallel with one checking the other.  With QC you've just
got to hope that everything's working as intended.  That alone would be enough
to rule out its use as far as I'm concerned, I can't trust something that I
can't verify.

@_date: 2007-07-01 22:38:03
@_author: Peter Gutmann 
@_subject: TPM, part 2 
I think it's more like "There must be some business case for these things
somewhere, surely.  Let's try a breadth-first search...".
I have a friend who implemented a basic trusted-boot mechanism for a student
project, so we have evidence of at least one use of a TPM for TC, and I know
some folks at IBM Research were playing with one a few years ago, so that's at
least two users so far.  Anyone else?

@_date: 2007-07-01 23:02:28
@_author: Peter Gutmann 
@_subject: The bank fraud blame game 
I've said roughly the same in a talk on the commercial malware industry that
I'll be giving at Defcon next month (normally I'd have the slides online to
point people to, but since I haven't given the talk yet you'll have to wait a
bit, sorry).  The malware industry is several years (at least) ahead of
anything that the defenders can produce at the moment.  So while US banks
still haven't (after years of criticism) taken even the most basic step of
using SSL on their login pages, the malware industry has things like the grams
eGold siphoner, which defeats any currently known browser security mechanism,
all ready to pull out and deploy.  While the defenders are struggling to keep
up with the latest malware (including some which are effectively undetectable
using current technology), the malware authors are getting their UI designers
to design flashy-looking skins for their botnet controllers and providing
video demos of their products in action.  The only countermeasure seems to be
to relegate PCs to being untrusted network middleboxes and run the financial
portions of all transactions on single-function external devices with built-in
pin-pads and displays.
(The usage model is that you do the UI portion on the PC, but perform the
actual transaction on the external device, which has a two-line LCD display
for source and destination of transaction, amount, and purpose of the
transaction.  All communications enter and leave the device encrypted, with
the PC acting only as a proxy.  Bill of materials shouldn't be more than about

@_date: 2007-07-02 01:08:12
@_author: Peter Gutmann 
@_subject: The bank fraud blame game 
Such a device was actually manufactured in Europe in the late 1990s,
unfortunately they couldn't find any bank willing to pay the cost, and it was
discontinued.  Similar devices are still being made for some vertical-market
applications, but they're sold at astronomical prices.
Given that all you need for this is a glorified pocket calculator, you could
(in large enough quantities) probably get it made for < $10, provided you shot
anyone who tried to introduce product-deployment DoS mechanisms like smart
cards and EMV into the picture.  Now all we need to do is figure out how to
get there from here.

@_date: 2007-07-02 20:15:58
@_author: Peter Gutmann 
@_subject: The bank fraud blame game 
Banks here have been using SMS-based transaction verification for a few years
now (although not done very well, sigh) without, apparently, any real problems
(I've been trying to get figures for per-transaction costs out of them for a
while now, so far without any success).  Since the SMS-based system is just a
labour-intensive way of doing what I was proposing but using a cellphone
instead of a dedicated device, my guess is that the overhead won't be that
bad.  If it was, the banks wouldn't be doing it now :-).
(Using smart cards as a yardstick isn't terribly useful, as I mentioned in my
previous message they're really a deployment DoS mechanism, not a solution).
Smart cards are part of the problem set, not the solution set - they're just
an expensive and awkward distraction from solving the real problem.  What I
was suggesting (and have been for at least ten years :-) is a small external
single-function device (no need for an OS) that can't be compromised by
malware because there's no attack vector for the malware to get at it.

@_date: 2007-07-05 17:59:32
@_author: Peter Gutmann 
@_subject: The bank fraud blame game 
The portability aspect was one contributing factor, but the other one was more
philosophical.  As Dan Geer put it recently, "If you're losing at a game that
you can't afford to lose, change the rules".  We've been trying since at least
the mid-1960s to move the insecurity away from the computer using an entire
industry's worth of gadgets and tricks, and yet we're falling further and
further behind the attackers.  The external-authorisation-box approach changes
the rules and instead moves the computer away from the insecurity.  Since the
only interface to the computer is "feed in blob" and "retrieve blob", it
doesn't matter how insecure the surrounding environment is, there's not much
that it can do to the auth-box.
I had the feeling it sort of collapsed under its own complexity, the smart
card/EMV/etc problem that I referred to earlier.
The external device emulates a standard USB memory key, to send data to it you
write a file, to get data back you read a file (think "/dev").  There's no
device driver to install, and no particularly tricky programming on the PC

@_date: 2007-07-06 17:10:03
@_author: Peter Gutmann 
@_subject: How the Greek cellphone network was tapped. 
Some years ago I talked to an ex-GTE person about law enforcement requiring
intercept capabilities to be built into phone switches.  His comments about
their approach to security (which he was responsible for) was: "They were
absolutely clueless, they assumed you could put 'Police line do not cross'
tape on the intercept portions and everyone would dutifully keep out".  He'd
left by the time it was implemented, but since there was never any significant
budget allocated to securing the intercept capabilities the impression I got
was that it only had whatever the developers could bolt on with the least cost
and effort.

@_date: 2007-07-17 16:45:49
@_author: Peter Gutmann 
@_subject: How the Greek cellphone network was tapped. 
I think you're looking at this a bit wrong.  I rememeber the same opinion as
the above being expressed on the brew-a-stu list about fifteen years ago, and
no doubt some other list will carry it in another fifteen years time, with
nothing else having changed.  Anyone who wants secure voice connections
(governments/military and a vanishingly small number of hardcore geeks)
already have them, and have had them for years.  Everyone else just doesn't
care, and probably never will.  This is why every single encrypted-phones-for-
the-masses project has failed in the market.  People don't see phone
eavesdropping as a threat, and therefore any product that has a nonzero price
difference or nonzero usability difference over an unencrypted one will fail.
This is why the only successful encrypted phone to date has been Skype,
because the crypto comes for free.
I once had a chat with someone who was responsible for indoctrinating the
newbies that turn up in government after each election into things like phone
security practices.  He told me that after a full day of drilling it into them
(well, alongside a lot of other stuff from other departments) it sometimes
took them as long as a week before they were back to loudly discussing
sensitive information on a cellphone in the middle of a crowded restaurant.
So in terms of secure voice communications, the military and geeks are already
well served, and everyone else doesn't care.  Next, please.

@_date: 2007-07-17 17:00:42
@_author: Peter Gutmann 
@_subject: improving ssh 
That would probably make things much worse.  A study of SSH attacks a few
years ago showed that nearly two thirds of all SSH private keys were stored on
disk with no protection at all, so that simply being able to read a hard drive
will get you access to any number of systems without having to trojan the SSH
client or plant a keyboard logger as you'd need for an SSH password.  So
turning off password auth would make things less secure, not more.
I started work on an paper that looked at doing exactly this based on traces
from SSH scanning attacks a year or two back, and realised that this is an
arms race that you can't win.  No matter what heuristics you use, all an
attacker has to do is change their scanning pattern to avoid them and all your
work is rendered useless.
The reason I never finished the paper (well, apart from that fact that this
type of defence is a lost cause) is because there's a much easier way to do
this than at the firewall or network level.  There's a paper by Pinkas and
Sander, "Securing Passwords Against Dictionary Attacks", later updated by van
Oorschot in a TISSEC paper (sorry, don't have the ref.handy) that contains a
very nice, elegant way to defeat SSH (and, in general, any password-based
protocol) scanning attacks.  I have an RFC draft to add this to SSH on the
back burner, I just haven't finished it yet because (a) too many other things
to do and (b) I'm not sure how it'll be received by the SSH community, who
seem to see public-key auth as the answer to any problem.

@_date: 2007-07-20 02:45:34
@_author: pgut001@cs.auckland.ac.nz 
@_subject: New article on root certificate problems with Windows 
The executive summary, so I've got something to reply to:
   In the default configuration for Windows XP with Service Pack 2 (SP2), if a
   user removes one of the trusted root certificates, and the certifier who
   issued that root certificate is trusted by Microsoft, Windows will silently
   add the root certificate back into the user's store and use the original
   trust settings.
While I don't agree with this behaviour, I can see why Microsoft would do
this, and I can't see them changing it at any time in the future.  It's the
same reason why they ignore key usage restrictions and allow (for example) an
encryption-only key to be used for signatures, and a thousand other breaches
of PKI etiquette: There'd be too many user complaints if they didn't.
The people designing this stuff aren't the ones who have to man the tech
helpdesk when users find that things break because of some action that they
don't even understand (see e.g. the Xerox PARC study where a bunch of people
with PhDs in computer science, after following paint-by-numbers instructions
to install certs on their machines, had absolutely no idea what they'd just
done to their computers).
 From a security point of view, this is really bad.  From a usability point of
view, it's necessary.  The solution is to let the HCI people into the design
process, something that's very rarely, if ever, done in the security field [0].
[0] Before people jump up and down about this: Yes, HCISec has become a very
     active and productive field in the last few years.  Unfortunately far too
     little of the work that's being done is making it into products though.
     We have lots of data saying "X is unusable in practice" and "The best way
     to handle this is Y", but developers keep on pushing X and avoiding (or
     don't even know about) Y.

@_date: 2007-07-20 19:58:32
@_author: pgut001@cs.auckland.ac.nz 
@_subject: New article on root certificate problems with Windows 
It depends on what you mean by "user".  You're assuming that direct action by
the wetware behind the keyboard resulted in its removal.  However given how
obscure and well-hidden this capability is, it's more likely that a user agent
acting with the user's rights caused the problem.  So the message you end up
communicating to the user is:
   "Something you've never heard of before has changed a setting you've never
   heard of before that affects the operation of something you've never heard
   of before and probably wouldn't understand no matter how patiently we
   explain it".
(those things are, in order "some application or script", "the cert trust
setting", "certificates", and "PKI").
I guess we'd need word from MS on whether this is by design or by accident,
but I can well see that quietly unbreaking something that's broken for some
reason would be seen as desirable behaviour.

@_date: 2007-06-22 01:49:25
@_author: Peter Gutmann 
@_subject: Free Rootkit with Every New Intel Machine 
With TPMs it's a bit different, they're absent from the hardware by default

@_date: 2007-06-22 01:57:18
@_author: Peter Gutmann 
@_subject: question re practical use of secret sharing 
It's available as part of other products (e.g. nCipher do it for keying their
HSMs), but I don't know of any product that just does... secret sharing.  What
would be the user interface for such an application?  What would be the target
audience?  (I mean a real target audience, not some hypothesised scenario).
(This is actually a serious question.  I talked with some crypto guys a few
years ago about doing a standard for secret sharing, but to do that we had to
come up with some general usage model for it rather than just one particular
application-specific solution, and couldn't).
Besides that, user demand for it was practically nonexistent... no, it was
completely nonexistent, apart from a few highly specialised custom uses we
couldn't even find someone to use as a guinea pig for testing, and the
existing specialised users already had specialised solutions of their own
for handling it.

@_date: 2007-06-22 15:10:23
@_author: Peter Gutmann 
@_subject: question re practical use of secret sharing 
I think that's the perfect summary of the problem with threshold schemes.
The processes they involve is simply too complex both to model mentally for
users and to build an interface to.  If you look at the nCipher manuals (for
 you
can see that they're really struggling to communicate the operational details
to users, and I've heard from users that it's so hard to use that few bother.
This isn't due to any failing of nCipher, the cognitive load imposed is just
so high that most users can't cope with it, particularly since they're already
walking on eggshells because they're working on hardware designed to fail
closed (i.e. lock everything out) if you as much as look at it funny.
When we were mulling it over to see whether it was worth standardising, we
tried to come up with a general-purpose programming API for it.  We were
working at the very geeky crypto toolkit API level (where you're allowed to be
somewhat non-user-friendly), not at the UI level.  Now if you compare a
standard crypto-op:
  encrypt( data, length, key );
  sign( message, length, key );
with what's needed to manage a threshold scheme:
  "add share 7 of a total of 12, of which at least 8 are needed, returning an
   error indicating that more shares are required"
with a side order of:
  "using 3 existing valid shares, vote out a rogue share and regenerate a
   fresh share to replace it"
then you start coming up with an API with abstract data-access capabilities at
a complexity level of something like ODBC.
(ODBC is the most appropriate API model I could come up with without thinking
about it for too long, obviously you don't need all of ODBC but the data
representation and access model was a reasonably good fit).
So that lead to two questions:
1. Who would want to implement and use an ODBC-complexity-level API just to
   protect a key?
2. How are you going to fit a UI to that?  (This is the real killer, even if
   you come up with some API to do (1), there's probably no effectively usable
   way to do (2)).
At that de facto QED the discussion more or less ended.

@_date: 2007-06-24 00:48:40
@_author: Peter Gutmann 
@_subject: Free Rootkit with Every New Intel Machine 
Check again.  A few months ago I was chatting with someone who works for a
large US computer hardware distributor and he located one single motherboard
(an Intel one, based on an old, possibly discontinued chipset) in their entire
inventory that contained a TPM (they also had all the ex-IBM/Lenovo laptops,
and a handful of HP laptops, that were reported as having TPMs).  He also said
that there were a handful of others (e.g. a few Dell laptops, which they don't
carry) with TPMs.
I've seen all sorts of *claims* of TPM support, but try going out and buying a
PC with one (aside from IBM/Lenovo and the handful of others) - you have to
look really, *really* hard to find anything, and if you do decide you
specifically want a TPM-enabled MB or laptop you're severely restricting your
options (unless it's a Lenovo).
Unless something truly miraculous happens, TPMs are destined to end their
lives as optional theft-discouragement gadgets for laptops (assuming they're
running Windows XP, or possibly Vista if you can find the drivers).  They've
certainly failed to make any impression on the desktop market.

@_date: 2007-06-24 18:33:11
@_author: Peter Gutmann 
@_subject: Free Rootkit with Every New Intel Machine 
Those are actually misleading, since there's no certainty that you'll be able
to find anything that'll actually plug into them.  That is, not only are the
TPM whatever-they-are-that-goes-there's almost impossible to find, but if you
do find one there's no guarantee that it'll actually work when plugged into
the header. In practice this is just a way of adding the "TPM" keyword to your
marketing without having to actually do anything except include a dummy header
on the MB.
(For people who don't work with the innards of PCs much, most motherboards
have assorted unused headers, sites for non-installed ICs, and so on, as a
standard part of the MB.  The TPM header is just another one).

@_date: 2007-06-25 18:59:07
@_author: Peter Gutmann 
@_subject: Free Rootkit with Every New Intel Machine 
As I said in my previous message, just because they exist doesn't mean they'll
do anything if you plug them into a MB with the necessary header (assuming you
have a MB with the header, and it's physically compatible, and electrically
compatible, and the BIOS is compatible, and ...).
Which MBs have you plugged one of these TPMs into and had it work?
Smart cards may well end up being present ubiquitously.
Hardware RNGs may well end up being present ubiquitously.
NIC-based crypto may well end up being present ubiquitously.
Biometric readers may well end up being present ubiquitously.
Home taping is killing mus... oops, wrong list.
Been there, done that, got the tchotchkes to prove it.
I've seen zero evidence that TPMs are going to be anything other than a repeat
of hardware RNGs, NIC-based crypto, biometric readers, and the pile of other
failed hardware silver bullets that crop up every few years.  Wait a  year or
two and there'll be some other magic gadget along to fix all our problems.

@_date: 2007-06-26 15:47:11
@_author: Peter Gutmann 
@_subject: Free Rootkit with Every New Intel Machine 
It's not just questionable, it's a really, really bad idea.  TPMs are
fundamentally just severely feature-crippled smart cards.  That is, they're
optimised for doing DRM/secure boot/whatever-you-want-to-call-it, but in
practice not much good for doing anything else (even if there are paper and
Powerpoint-slide claims to the contrary).  So you have something with all the
drawbacks of a smart card (external widget that needs to be bought at extra
cost and plugged in) and none of the advantages.
BitLocker just uses the TPM as a glorified USB key (sealing a key in a TPM is
functionally equivalent to encrypting it on a USB key).  Since BitLocker isn't
tied to a TPM in any way (I'm sure Microsoft's managers could see which way
the wind was blowing when they designed it), it's not going to be TPM's killer

@_date: 2007-03-15 03:08:22
@_author: Peter Gutmann 
@_subject: PKI: The terrorists' secret weapon 
Excerpts from a recent (non-public) PKI discussion paper:

@_date: 2007-05-21 13:44:23
@_author: Peter Gutmann 
@_subject: kernel-level key management subsystem 
Yes, but first you'd have to tell me what you're trying to do.

@_date: 2007-05-21 13:54:09
@_author: Peter Gutmann 
@_subject: Russian cyberwar against Estonia? 
Given that there are large numbers of disaffected re-settled Russians living
in Estonia, combined with the usual collection of hooligans who'll jump at any
opportunity for a fight, why would Russia need to get involved?  It makes for
some nice posturing, but why would the Russian government bother when they can
just sit back and let the local script kiddies cause havoc?
(I was in the centre of Tallinn when the reported riots over this were
 happening and didn't even notice a disturbance.  This whole thing seems more
 an excuse for media hype and political posturing than anything else.  Ignore
 it and it'll go away.  Something else will be along presently).

@_date: 2007-05-23 14:45:49
@_author: Peter Gutmann 
@_subject: 307 digit number factored 
I would go further and say that for most applications of PKCs/PKI today, 1024-
bit RSA keys are not a risk at all, or more specifically that on a scale of
risk they're so far down the list that they're close to negligible.  As
numerous security HCI studies have shown, user comprehension of PKI is close
to zero percent, which means that the security effectiveness of the same is
also close to zero.  As the multi-billion dollar phishing industry has ably
demonstrated, the bad guys are more than aware of this too.  So going from x-
bit RSA to y-bit RSA on a component with close to zero-percent effectiveness
is... well, I'll let you do the maths.  Until the hundred other constituent
parts required to secure something like web browsing are fixed, changing the
key size is just pointless posturing, since it's not fixing anything that
anyone is attacking.  Once all the other bits are fixed and working as
intended, then we can go back to debating whether length is more important
than width in key sizes.

@_date: 2007-05-28 14:08:45
@_author: Peter Gutmann 
@_subject: Free Rootkit with Every New Intel Machine 
(Forwarded with permission from a NZ security mailing list, some portions
 anonymised)

@_date: 2007-11-09 15:28:13
@_author: Peter Gutmann 
@_subject: ITU-T recommendations for X.509v3 certificates 
There is very little correspondence between PKI specs and reality.  As I put
it in one analysis on X.509 implementation flaws, "You cannot build an
implementation so broken that it can't claim to be X.509" (I know of at least
one instance in which deeply-buried PGP was sold as X.509).  See part 2a of
the Godzilla crypto tutorial,
 for a few examples.
Having said that, if all you want is interoperability then you should be fine.
The brokenness in X.509 implementations creates a self-sustaining cycle in
which applications that accept certs are more or less oblivious to anything
that's in the cert (beyond basic stuff like correct formatting and encoding,
and so on), so you can get away with almost anything (and then in turn because
apps will accept anything, cert creators can create arbitrarily broken certs
without being caught out by it, so the cycle is self-sustaining).
So if you want to do the minimum amount of work, all you need is some
approximation to a DN, maybe a basicConstraints, and if you're feeling really
enthusiastic, keyUsage (although this is often ignored by implementations, see
the part 2a slides for examples.  Even basicConstraints, the single most
fundamental extension in a certificate, and in most cases just a single
boolean value, was widely ignored until not too long ago).
If you want to be really lazy, use an X.509v1 cert where you don't even need
to bother with extensions.  A downside (?) of this is that some applications
will treat it as a CA root cert.

@_date: 2007-10-02 15:38:56
@_author: Peter Gutmann 
@_subject: Linus: Security is "people wanking around with their opinions" 
For people who don't read LKML (or get interesting bits forwarded to them),
there's a wonderful quote by Linus Torvalds about the difference between OS
scheduler design and security design:
  "Schedulers can be objectively tested. There's this thing called
  'performance', that can generally be quantified on a load basis.
  "Yes, you can have crazy ideas in both schedulers and security. Yes, you can
  simplify both for a particular load. Yes, you can make mistakes in both. But
  the *discussion* on security seems to never get down to real numbers. So the
  difference between them is simple: one is 'hard science'. The other one is
  'people wanking around with their opinions'."
  Peter :-).

@_date: 2007-10-04 23:27:28
@_author: Peter Gutmann 
@_subject: fyi: Storm Worm botnet numbers, via Microsoft 
I have two problems with this report.  Firstly, I don't think this is a very
representative sampling technique compared to the estimates from security
companies.  If you look at the sample that's being used, "Windows machines
that have automatic updates turned on", then the typical machine is going to
be configured with something like Windows XP SP2 with all available hotfixes
and updates applied, in other words the very systems that are (one would hope
:-) the *least* likely to be affected by malware.  If you take the rule-of-
thumb estimate that's sometimes used on MSDN blogs of 1B Windows machines out
there then 2.6M machines is < 0.3% of that total.  Now this in itself wouldn't
be so bad if it was an unbiased sample, but in fact it's probably a rather
non-representative 0.3%.  Although some of the numbers from security companies
for infections may be just guesswork, they also use broad sampling across all
Windows machines (not just ones with Windows Defender), honeypots, monitoring
of botnet traffic patterns, and other methods as well.  So while it's valid to
say that this provides data for Storm on fully patched, up-to-date machines
running Windows Defender, I don't think this generalises for all Windows
Secondly, the text completely contradicts the figures given.  If the figures
really are accurate and not a typo, then 274K machines infected out of 2.6M
puts Storm on 10% of Windows PCs, which would make the worldwide infection
rate 100M systems, or ten times larger than the previous worst-possible case
estimate.  Storm may be big, but it's not *that* big.  I think there's
something wrong with the figures.

@_date: 2007-10-06 18:58:02
@_author: Peter Gutmann 
@_subject: Undocumented Bypass in PGP Whole Disk Encryption 
Specifically, the capability exists to allow a one-off unattended reboot of
servers, i.e. you tell the program that at the next reboot, it should
automount the drive without stopping to ask for the password so that the
reboot can continue.  Without this, it would be impossible to run servers with
encrypted drives.  The mysterious "unnamed customers" was a misrepresented
reference to sysadmins who needed the capability to run their machines.
Nothing to see here, move along, move along.

@_date: 2007-10-09 01:17:06
@_author: Peter Gutmann 
@_subject: Trillian Secure IM 
Or they could be using static/ephemeral DH with fixed shared DH key values,
which isn't much better.  (This is just speculation, it's hard to tell without
knowing what the exchanged quantities are).

@_date: 2007-10-09 01:25:14
@_author: Peter Gutmann 
@_subject: Full Disk Encryption solutions selected for US Government use 
Given that it's for USG use, I imagine the FIPS 140 entry barrier for the
government gravy train would be fairly effective in keeping any OSS products

@_date: 2007-10-09 02:11:54
@_author: Peter Gutmann 
@_subject: Full Disk Encryption solutions selected for US Government use 
But if you build a FDE product with it you've got to get the entire product
certified, not just the crypto component.
(Actually given the vagueness of what's being certified you might be able to
get away with getting just one corner certified, but then if you have to use a
SISWG mode you'd need to modify OpenSSL, which in turn means getting another
certification.  Or the changes you'd need to make to get it to work as a
kernel driver would require recertification, because you can't just link in
libssl for that.  Or...).

@_date: 2007-10-09 15:21:27
@_author: Peter Gutmann 
@_subject: Trillian Secure IM 
Opportunistic cryptography designed as opportunistic cryptography (with key
continuity measures and so on) is fun.
Opportunistic cryptography that exists because the developers have screwed up
something better (and are under the delusion that what they've implemented is
something better) is less fun.

@_date: 2007-10-09 15:41:47
@_author: Peter Gutmann 
@_subject: Full Disk Encryption solutions selected for US Government use 
Not necessarily.  It's up to the vendor to draw the boundary around what they
want evaluated.  In some cases it's purely a single crypto module, in others
it's significant portions of the application using the crypto (and I guess
this would be the case for FDE, where pretty much the entire application does
nothing but crypto).  The advantage of the former is that there's less to
evaluate, the advantage of the latter is that there's less paperwork to handle
things like data crossing cryptomodule boundaries, which can happen in cases
where most of your key management is done outside the core crypto code.
However with the latter you can also declare large chunks of your code non-
security-relevant and therefore out of scope, so you get the benefits of a
fairly broad perimeter but not too much actual code to get checked.  It's
give-and-take with the evaluators, generally you juggle things to get the most
benefit for the least amount of work (which doesn't necessarily correspond to
the most thorough evaluation).
Only if you know what FIPS 140 is.  For procurement people, FIPS 140 is a
capitalised word and a small integer value printed next to a checklist item.
If the software meets their requirements and pricing expectations then they'll
look for something somewhere in the vicinity of the product that's close
enough to saying "FIPS 140", check the box, and they're done.

@_date: 2007-10-09 15:52:12
@_author: Peter Gutmann 
@_subject: Trillian Secure IM 
One of them contains a link to an older thread:
with this gem:
  Speaking as a software developer, if I ever had someone on my forums who's
  posts consisted solely of questions about how my private encryption systems
  work [an earlier post by a crypto-savvy person had asked about use of DH
  primes, the block cipher encryption mode used, and so on], I would ban them
  and report them to their ISP without a second thought.
  If you're interested in creating good encryption, learn the principles:
  encryption schemes are weakened when they are based on someone else's work.
  So if you're honestly wanting to make good encryption for Trillian, its a
  bad idea to try to use CS's work as a base.
Bruce, one for your next Cryptogram?

@_date: 2007-10-09 18:08:44
@_author: Peter Gutmann 
@_subject: kernel-level key management subsystem 
OK, those are all pretty trivial in terms of having an identified problem to
Right, and that's what I wanted a definition for.  95% of the what you're
asking for is defining the problem, and that's what I was after.  For example
how do you want access to the keys controlled?  ACLs?  Who sets the ACLs?  Who
can manage them?  How are permissions managed?  What's the UI for this?  Under
what conditions is sharing allowed?  If sharing is allowed, how do you handle
the fact that different apps (with different levels of security) could have
access to the same keys?  Do you derive keys from a master key?  Do you
migrate portions of the app functionality into the kernel to mitigate the
problems with untrusted apps?  How is key backup handled?  What about
[Another 5 pages of questions]
Once you've got a clear statement of exactly what you want to do (which in its
most abstract form is "solve an arbitrarily complex key management problem"),
implementation is almost trivial in comparison.

@_date: 2007-10-14 18:48:53
@_author: Peter Gutmann 
@_subject: Password hashing 
Or just use PBKDF2, RFC 2898.  It does what's required, has been vetted by
cryptographers, is an IETF standard, has free implementations available, ...

@_date: 2007-10-23 00:10:25
@_author: Peter Gutmann 
@_subject: Commercial CAPTCHA-breakers for sale 
is a site that sells commercial CAPTCHA-breaking
software.  It also shows success rates for different types of CAPTCHAs.  While
the ratings are merely representative of this particular site's software and
not a universal measure, it's interesting to note that there's already a
commercial market for anti-CAPTCHA tools.

@_date: 2007-09-07 19:55:41
@_author: Peter Gutmann 
@_subject: World's most powerful supercomputer goes online 
Another potential use for the Storm worm... I can't imagine this would be why
it's being assembled since there's no money in it, but consider the prospect
of x million machines cycling from idle to full load once a minute.  If the
power swing in doing this is (for example) 100 watts per PC then even at 1M
machines that's switching a 100 megawatt load in and out of circuit, which
could cause some fun in power distribution systems.  You could easily double
(or more) this load by putting the PC and monitor to sleep and then running it
up to full load and back down again.
Obviously it's highly unlikely that that's what the Storm botherders are
planning to do with it, apart from the lack of financial motive you'd need to
carefully synchronise the timing, and the PCs would be distributed all over
the world rather than affecting one grid.  Probably the most you'd get is
localised problems, overloading, maybe a few fires from wiring.
OTOH the shock effect of someone being able to do this worldwide would be
something to behold.  Talk about a "light blue touch paper and stand clear".
(If they *do* do this with Storm, remember that you read it here first :-).
A much simpler attack if all you want to do is cause panic would be to just
brick the machines and watch the fun when 1M+ RMAs hit the service channels.

@_date: 2007-09-16 20:39:27
@_author: Peter Gutmann 
@_subject: using SRAM state as a source of randomness 
The paper actually covers two (related) things, fingerprint extraction and
using SRAM power-up state as a random number source/seed, I assume your
question is about the latter so I'll only address that.  In addition I'm not
sure if you're looking only at RAM state in limited embedded devices or are
wondering whether it's a good source in general, e.g. in desktop PCs, so I'll
assume both.
The problem with using device state in this manner is that it's *completely*
dependent on the device technology and environment in which it's used.  Any
change in the manufacturing process can change the results.  Any change in the
environment (temperature, supply voltage, etc) can change the results.  A
deliberate attack, for example irradiating the device to change the RAM cell
threshold, can change the results.  Even without deliberate modification, SRAM
cells that store a constant value tend to acquire a set over time in which
they'll come up in that state when powered on (this has happened with crypto
hardware storing keys over long periods of time in SRAM).  In a PC, the +5VSB
may trickle enough power through the RAM to retain state for some time after
the machine is powered off, or possibly more or less indefinitely unless the
power is physically removed (I've measure 3-5W power consumption on several
PCs in the suppsedly turned-off state, which indicates that the +5VSB is
powering parts of the system).  ECC memory scrubbing may set the RAM into a
predetermined state.  A slow memory test during POST (which writes all cells
rather than doing a quick test with a large stride) would have the same
effect.  The list goes on and on...
The worst case is a change in the environment or manufacturing process, which
typically occurs without the end user even knowing about it.  You simply can't
guarantee anything about RAM state as an RNG source, you'd have to prove a
negative (no change in manufacturing technology or the environment will affect
the quality of the source) in order to succeed.  It's like the thread-timing-
based RNGs, you can never prove that some current variation of or future
change to the scheduler won't result in totally predictable "random" numbers.
So RAM state is entropy chicken soup, you may as well use it because it can't
make things any worse, but I wouldn't trust it as the sole source of entropy.
What I'd use is a device fingerprint (which is never revealed) seeding a PRNG
with a counter in nonvolatile memory to ensure the PRNG state never repeats.
However the publication of the fingerprint for device ID purposes tends to
negate this usage.

@_date: 2007-09-19 01:09:56
@_author: Peter Gutmann 
@_subject: DRM Shoots Itself in the Foot, episode... oh, many 
Lets say you've been tasked with implementing a DRM system.  So you go to the
Digital Content Protection LLC site and download the HDCP spec, which contains
all the test vectors, sample keys, and whatnot, that you need for HDCP.
However, since it's from Digital Content Protection LLC, the docs are DRM'd
(PDF-protected from copying).  So you can eyeball the key tables and test
vectors that you need to implement the DRM, you just can't use them.
DRM, helping prevent... DRM.

@_date: 2007-09-20 16:38:24
@_author: Peter Gutmann 
@_subject: Scare tactic? 
It's quite possible that many implementations do this.  When the Mozilla folks
changed their code a year or two back to reject RSA keys with an exponent of
one (which in itself means that they'd been accepting those keys for years), a
number of certs broke because CAs were issuing exponent-one keys, which in
turn means that many other implementations that never complained about these
certs were freely accepting them.  Windows CryptoAPI, for example, still
allows exponent-one keys as a by-design feature to allow the export of
"wrapped" keys in plaintext form.  So it's quite believable that a number of
DH implementations allow bad key parameter values, and that this has been
going on for years.
(Even the level of validation discussed on the web page doesn't help entirely,
FIPS 186 provides extra parameters that you can use for checking the key
(p,q,g) while the still widely-used PKCS  doesn't (p,g), so even just using
PKCS  rather than FIPS 186 is a problem).

@_date: 2007-09-21 13:47:18
@_author: Peter Gutmann 
@_subject: Scare tactic? 
You've forgotten Hanlon's razor, "Never attribute to malice that which can be
adequately explained by stupidity".  So the comment should really be:
  All this attack allows is for one side of a DH exchange to inadvertently
  downgrade the security,
This sort of thing has happened several times in the past (with RSA, not DH in
this case), one example being the CA-issued exponent-one certs that I
mentioned previously, the other being an implementation that shall go unnamed
that sent out plaintext because the developers didn't do key paramter
validation.  So the problem isn't a deliberate attack, it's screwups by people
implementing or deploying the crypto.

@_date: 2008-04-04 00:08:14
@_author: Peter Gutmann 
@_subject: how to read information from RFID equipped credit cards 
Lock it down completely.  What really panicked the mgt. wasn't so much the
thought of their data appearing on other organisations' networks but cases
where other organisations' data had appeared on *their* network (due to, in
some cases, overzealous employees, in another case an outside contractor, and
in another someone who wanted to sell them "commercially useful information").
You want to explain that to management terrified of criminal prosecution?  I
got the feeling from talking to the IT security guy in the case of the
suspected commercial espionage that the management really wanted to pour
quick-setting concrete into the USB ports just to be absolutely sure.

@_date: 2008-04-17 20:21:28
@_author: Peter Gutmann 
@_subject: New results against the Mifare cipher 
Algebraic Attacks on the Crypto-1 Stream Cipher in MiFare Classic and Oyster
  Cards
  Nicolas T. Courtois and Karsten Nohl and Sean O'Neil
  MiFare Crypto 1 is a lightweight stream cipher used in London's Oyster card,
  Netherland's OV-Chipcard, US Boston's CharlieCard, and in numerous wireless
  access control and ticketing systems worldwide. Recently, researchers have
  been able to recover this algorithm by reverse engineering.
  We have examined MiFare from the point of view of the so called "algebraic
  attacks". We can recover the full 48-bit key of MiFare algorithm in 200
  seconds on a PC, given 1 known IV (from one single encryption).
  The security of this cipher is therefore close to zero. This is particularly
  shocking, given the fact that, according to the Dutch press, 1 billion of
  MiFare Classic chips are used worldwide, including in many governmental
  security systems.

@_date: 2008-04-24 23:00:30
@_author: Peter Gutmann 
@_subject: [Fwd: Secure Server e-Cert & Developer e-Cert. Comerica TM Connect Web Bank] 
These have been around for awhile, I'm not on my home machine at the moment or
I'd post a link to a blog analysis of this sort of thing.  Although it's impossible to tell due to the lack of figures from either side (PKI phishing vs. client cert use) it may well be that there's more use of PKI to attack bank clients than to defend them.

@_date: 2008-04-27 17:36:51
@_author: Peter Gutmann 
@_subject: more on malicious hardware 
"Perry E. Metzger" If you look at the linked article you'll see the example they give of
counterfeit chips is:
which has been going on for about 15 years or so [0], back then they'd grind
the tops off the ceramic heat spreader on P5s and print on a new speed rating,
later when distributors got wise to this they went to laser-etched labels that
were indistinguishable from the originals.
The other example given in the article was chips for avionics/milspec use,
which just means that they rebranded standard non-milspec parts as being for
milspec use, and that scam predates CPU speed re-gradings by some time (in my
device zoo I have some interesting not-really-milspec 7400s dating back to the
early 80s, as well as other oddities like Apple-branded TTL and who knows what
else).  Neither of these are really counterfeits, they're genuine chips
remarked for use outside their intended use parameters.  In fact given the
overclockability of many CPU binnings and the fact that manufacturers have in
the past sold CPUs at lower speed grades than they tested for at manufacture
in order to meet price points (in other words a CPU was tested for x MHz,
branded for x - y MHz to meet a price point, and then had the branding ground
off and was re-branded for its original binned speed by dodgy re-
distributors), the result may be no loss at all [1].
It'd be interesting to see some figures for genuine faked-from-whole-cloth
devices vs. basic re-brandings/re-binnings/whatever of original products, I
would imagine there's vastly more re-branding and re-binning going on than
someone actually cloning (say) a CMI8788 and selling it as the real thing.
[0] And probably for a long time before that, AFAIK it first became a major
    issue when the high price differential between different speed binnings     of P5s made it a lucrative business.  Making a non-milspec 7404 into a     milspec part was a lot less profitable.
[1] That one should keep the lawyers busy: If I sell a device de-rated purely
    to meet a price point and someone else on-sells it at its original     designed rating, with what would you prosecute them?

@_date: 2008-08-01 17:38:28
@_author: Peter Gutmann 
@_subject: On "randomness" 
I would never rely *exclusively* on any source because then a failure in your exclusive source, no matter how magical it is, will bring down your entire system.  Use a hardware RNG if you want to, but also XOR in the output from a PRNG, and a block cipher in counter mode, and a MAC of the time.  And apply the NIST tests on the data you're using, and on the generator output.  And don't forget to do [...].
A good randomness/key generator is more an engineering problem than an algorithmic one.

@_date: 2008-08-09 05:49:42
@_author: Peter Gutmann 
@_subject: OpenID/Debian PRNG/DNS Cache poisoning advisory 
You'd also end up with a rather large list for the client to carry around, which would be especially problematic for lightweight clients.  You'd need to represent it as something like a Bloom filter to avoid this (given that most users will just click OK on invalid certs, the small false positive rate shouldn't have much effect either :-).

@_date: 2008-08-16 11:27:17
@_author: Peter Gutmann 
@_subject: Is snake oil cryptography trans-fat free? 
It's a (formerly) good product badly sold.  DriveCrypt used to be the open- source ScramDisk until it was taken over by a commercial operator whose handling of the product since then has been a bit erratic (see endless threads in alt.security.scramdisk from a couple of years ago for details).

@_date: 2008-08-18 15:58:25
@_author: Peter Gutmann 
@_subject: Kiwi expert cracks chip passport 
The original story was actually the coverage in the UK Times last week,
  It was a
three-person effort, Adam Laurie did the RFID part (via RFIDIOt), Jeroen van
Beek did the passport software implementation and tying the whole thing
together, all I did was the signing.  We never touched the passport chip, what
we showed was that it's possible to create your own fictitious e-passport
that's accepted as valid by the reference Golden Reader Tool.  In other words
we showed that what security researchers had been warning about ever since e-
passports were first proposed was actually possible, following the l0pht's
motto "Making the theoretical practical".  Jeroen presented the work at Black
Ugh, no, make it go away.
(Alert readers may notice the anomaly with the carefully-placed monitor right
behind my head, which is displaying something slightly different from the
surrounding sea of Vista desktops :-).  It's actually a file photo from a news
story from the start of last year about Vista).

@_date: 2008-08-19 02:25:59
@_author: Peter Gutmann 
@_subject: Kiwi expert cracks chip passport 
Adam and I used the Omnikey Cardman 5321 (I'm not sure what Jeroen used,
probably the same), which is cheap, well-supported with drivers, and cheap.
Oh, and it's cheap too.  The card was a standard NXP JCOP 41, one country's
passport implementation didn't change the ATR so when you ping the passport it
returns the product ID in the response :-).  Having said that, going with the
JCOP 41 was more a case of "OK, we'll use that too then" rather than "now we
know the secret" so having the product ID returned in the ATR isn't really a
security problem.  In practice anything programmable with a 13.56MHz RFID
interface should do it, you don't have to specifically use a JCOP 41 card.  As
with the reader, the card just happened to be available and cheap.  Given that
people have built their own prox card emulators it wouldn't surprise me if
someone did the same for a 13.56MHz card (e.g. using the freely-available
OpenPICC design) so you can return "foo'; DROP TABLE passports; --" as your
passport MRZ when the "card" is read :-).
One thing that wasn't mentioned in the news coverage is that, as with any
SCADA-type software, there are bound to be all manner of bugs and holes in the
various reader implementations just waiting to be exploited.  For example when
I was initially playing with creating signatures I just memcpy()d some fixed
data together to create something to sign and was surprised when the Golden
Reader software accepted invalid signed data that should have been rejected as
valid.  I also managed to crash it at one point, quickly fixed the problem,
and then spent the next day kicking myself for not recording what data I'd fed
in to cause this (all your readers are belong to buffer overflows).  I'm sure
there's going to be many more Black Hat/Defcon talks on this in the future.
Has there ever been any third-party analysis of passport reader software as
there has for voting-machine software?  By "analysis" I don't mean the usual
Common Criteria rubber-stamping, I mean actual independent scrutiny of the

@_date: 2008-08-19 16:34:31
@_author: Peter Gutmann 
@_subject: Extended certificate error 
What's the expiry date for the CA certificate that signed it, and its CA
certificate?  What's the clock on your PC set to?  And why aren't you just
clicking "Continue anyway" like everyone else does? :-).

@_date: 2008-08-20 13:23:28
@_author: Peter Gutmann 
@_subject: Kiwi expert cracks chip passport 
[Not sure if this is still of general list interest, let's take the followups
 off-list.  If anyone else wants to be included in the off-list discussion,
 let me know].
I sense Vista running on your machine :-).  To get it to work I had to fire up
XP and explicitly install the Omnikey drivers from their web site rather than
using Windows auto-install to get them.  It also runs well in Parallels on a
Mac, although I haven't been able to get it to work under Vista.

@_date: 2008-08-23 23:03:40
@_author: Peter Gutmann 
@_subject: Good writeup on UI spoofing attacks 
The Codinghorror blog has a good writeup on the level of sophistication of UI
spoofing being used in phishing attacks, specifically how a web search for
lilies leads to a pretty convincing social-engineering attack designed to get
users to install their malware:
    What I'm more concerned about here is how well the user interface was
  spoofed. The browser FUI [fake UI] was convincing enough to even make me --
  possibly the world's most jaded and cynical Windows user -- do a bit of a
  double- take. How do you protect naive users from cleverly designed FUI
  exploits like this one? Can you imagine your mother doing a web search on
  flowers -- flowers, for God's sake -- clicking on the search results to a
  totally legitimate website, and correctly navigating the resulting maze of
  fake UI, spurious javascript alerts, and download dialogs?
To pre-empt the inevitable discussions of Noscript and similar measures,
they're all well and good but the very people who need them the most are the
ones who're least likely to have them installed.

@_date: 2008-08-23 23:03:40
@_author: Peter Gutmann 
@_subject: Good writeup on UI spoofing attacks 
The Codinghorror blog has a good writeup on the level of sophistication of UI
spoofing being used in phishing attacks, specifically how a web search for
lilies leads to a pretty convincing social-engineering attack designed to get
users to install their malware:
    What I'm more concerned about here is how well the user interface was
  spoofed. The browser FUI [fake UI] was convincing enough to even make me --
  possibly the world's most jaded and cynical Windows user -- do a bit of a
  double- take. How do you protect naive users from cleverly designed FUI
  exploits like this one? Can you imagine your mother doing a web search on
  flowers -- flowers, for God's sake -- clicking on the search results to a
  totally legitimate website, and correctly navigating the resulting maze of
  fake UI, spurious javascript alerts, and download dialogs?
To pre-empt the inevitable discussions of Noscript and similar measures,
they're all well and good but the very people who need them the most are the
ones who're least likely to have them installed.

@_date: 2008-08-25 00:20:50
@_author: Peter Gutmann 
@_subject: [cryptography] 5x speedup for AES using SSE5? 
Speaking of CPU-specific optimisations, I've seen a few algorithm proposals
from the last few years that assume that an algorithm can be scaled linearly
in the number of CPU cores, treating a multicore CPU as some kind of SIMD
engine with all cores operating in lock-step, or at least engaging in some
kind of rendezvous every couple of cycles (for example the recently-discussed
MD6 uses a round of 16 steps, if I read the description correctly) to exchange
data.  This abstraction seems to be particularly convenient when dealing with
things like hash trees.  However I'm not aware of any multicore CPU that
actually works this way, you'd need to have exclusive use of each core by one
thread and use incredibly expensive (compared to the other primitive CPU
operations used in hashing) barriers or something similar to ensure
Is there some feature of multicore CPUs that I'm missing, or is it a case of
cryptographers abstracting a bit too much away?  And if it's the latter,
should someone tell them that multicore CPUs don't actually work that way?

@_date: 2008-08-28 03:35:43
@_author: Peter Gutmann 
@_subject: Decimal encryption 
I posted a description of how to perform encryption in limited subranges to
sci.crypt about ten years ago,
the code and a cleaned-up description is also in "Building Secure
Software" by John Viega and Gary McGraw.

@_date: 2008-08-21 15:40:42
@_author: Peter Gutmann 
@_subject: Good writeup on UI spoofing attacks 
The Codinghorror blog has a good writeup on the level of sophistication of UI
spoofing being used in phishing attacks, specifically how a web search for
lilies leads to a pretty convincing social-engineering attack designed to get
users to install their malware:
    What I'm more concerned about here is how well the user interface was
  spoofed. The browser FUI [fake UI] was convincing enough to even make me --
  possibly the world's most jaded and cynical Windows user -- do a bit of a
  double- take. How do you protect naive users from cleverly designed FUI
  exploits like this one? Can you imagine your mother doing a web search on
  flowers -- flowers, for God's sake -- clicking on the search results to a
  totally legitimate website, and correctly navigating the resulting maze of
  fake UI, spurious javascript alerts, and download dialogs?
To pre-empt the inevitable discussions of Noscript and similar measures,
they're all well and good but the very people who need them the most are the
ones who're least likely to have them installed.

@_date: 2008-08-28 17:32:10
@_author: Peter Gutmann 
@_subject: Decimal encryption 
... and most of them seem to be excessively complicated for what they end up
achieving.  Just for reference the mechanism from the sci.crypt thread of more
than a decade ago was:
    KSG_RANGE = ( 256 / RANGE ) * RANGE;
    do
        val = ksg();
    while( val >= KSG_RANGE );
  The worst-case scenario is when RANGE = 129, when nearly 50% of the ksg()
  output will be discarded.  A more typical case when RANGE = 96 (ASCII text)
  loses 25% of the output, and RANGE = 10 (digits) loses 2% of the output. The
  full process then becomes:
  encrypt:
    do
        val = ksg();
    while( val >= KSG_RANGE );
    cipher = ( ( ( plain - BASE ) + val ) % RANGE ) + BASE;
  decrypt:
    do
        val = ksg();
    while( val >= KSG_RANGE );
    plain = ( ( ( cipher - BASE ) - val ) % RANGE );
    while( plain < 0 )
        plain += RANGE;
    plain += BASE; This takes any cipher (block or stream) and, by using it as a KSG, allows
encryption of arbitrary (including discontinuous) data ranges.
Another advantage of the KSG use is that you can precalculate the key stream
offline, the implementation I used at the time pre-generated 4K of keystream
and then used it to encrypt bursty text messages with real-time constraints
that didn't allow for pauses to run the cipher.
(The thread contains lots of tweaks and variations of this).

@_date: 2008-08-29 20:25:36
@_author: Peter Gutmann 
@_subject: Decimal encryption 
Sure, thus the thread on sci.crypt about all the little situation-specific
tweaks you can apply.  In this case it was being used to encrypt ongoing ASCII
streams (computer terminal traffic, SMS, and other stuff) so there weren't
multiple independent values (the terminal-traffic one was particularly
interesting because it needed to encrypt discontinuous ranges so that control
characters went through without encryption).  As you say, if you're processing
independent values you'd need to tweak it somehow, for example by using a
public value like the account number as an IV if you're using this to encrypt
the credit card number stored next to the account number (with standard
caveats about IV reuse, birthday attacks, and so on, pedants please assume a
two-page enumeration of requirements here :-).

@_date: 2008-08-29 20:41:47
@_author: Peter Gutmann 
@_subject: privacy in public places 
Painting the camera lenses with laser pointers is quite effective, at least as
a short-term civil-disobedience measure.  Since there's no long-term damage
caused (unless you use a really impressive laser pointer) it's a bit tricky to
charge you with anything, at least under current law.
Or you could follow the lead of Captain Gatso in the UK...

@_date: 2008-08-30 15:36:17
@_author: Peter Gutmann 
@_subject: Generating AES key by hashing login password? 
... and specifically PBKDF2, not the original PKCS   See also the
discussion at

@_date: 2008-12-10 02:21:15
@_author: Peter Gutmann 
@_subject: The next time someone tells you "no one" would do something... 
They got this a little wrong -- he's actually removing the stacked die
  NOR/PSRAM, erasing and reprogramming to a version that is vulnerable to the
  SIM proxy attack.  It's not really any big deal.

@_date: 2008-12-17 16:54:14
@_author: Peter Gutmann 
@_subject: Why the poor uptake of encrypted email? 
... to a statistically irrelevant bunch of geeks.  Watch Skype deploy a not-
terribly-anonymous (to the people running the Skype servers) communications
system.  Watch the entire world not care, and flock to it in droves.  Heck,
the entire business model for social networking, one of the biggest Internet
phenomena in the last few years, is built around users being as non-anonymous
as possible.
So Alec's argument still stands.  It's pretty hard selling anonymity and
privacy to people who think nothing of sending Twitter updates of everything
they do all day long to anyone prepared to listen and posting videos of their
drunken antics to MyFace.

@_date: 2008-12-17 17:07:22
@_author: Peter Gutmann 
@_subject: CPRNGs are still an issue. 
The ARM family are particularly problematic for entropy gathering because
anything that'd help you is (a) locked away so it can't be accessed and (b)
not easily ascertainable at runtime.  For the x86 equivalent you just do a
CPUID (and you can in turn check whether the architecture supports that before
you use it if you're on an old-enough system) and from there can execute
further instructions to determine hardware capabilities and read/use them via
MSRs as required.
For the ARM the equivalent is a CP15 read (and then further accesses to the
ARM equivalent of x86 MSRs), e.g.:
  asm volatile (
      "mrc p15, 0, r0, c0, c0, 0\n\t"
      "str r0, %0\n"
      : "=m"(processorID)
      :
      : "cc", "r0");
but this is only accessible in supervisor mode so for any normal app it's an
instant illegal instruction fault.  Furthermore, even in supervisor mode
there's no way to bootstrap your way up as you can for x86 and tap the amazing
store of hard-to-predict information that most ARM cores make available
because each ARM implementation adds its own oddball registers and the only
way to know whether they're present and usable is if you know in advance which
specific core and stepping you're working with.  In other words you can't get
there from here and even if you could you wouldn't know what to do when you
got there.  So everything you need is there, you just can't use it.

@_date: 2008-12-17 17:14:49
@_author: Peter Gutmann 
@_subject: CPRNGs are still an issue. 
This is only going to be a problem if your RNG is... well, to be blunt, stupid
enough to rely entirely on HDD timings as an entropy source.  I would hope
that any well-designed entropy polling system would use as many sources as
possible for the simple reason that otherwise a single failure can destroy the
security of your entire system.  In other words an entropy polling mechanism
should see the change from HDD to SSD as nothing more than a small glitch for
its fault-tolerant front-end to accomodate and continue as before.

@_date: 2008-12-22 01:18:31
@_author: Peter Gutmann 
@_subject: Security by asking the drunk whether he's drunk 
In recently had an opportunity to talk to someone who had had a family member
become a victim of identity fraud, not in the usual manner to target them
directly but as a springboard to target others by registering a phishing site
in their name.  Variations on this theme include using stolen identities to
buy code-signing certificates for malware and a variety of other end-runs
around identity-based accountability mechanisms.  The problem here is the fact
that the market is so awash with stolen identities that vendors have to sell
them in bulk lots just to turn a profit.  In other words a system designed to
defeat the problem of identity theft relies on the flawless functioning of a
global identity-based accountability infrastructure in order to work, a
classic catch22-situation.  If it's possible to buy stolen identities with
almost arbitrary amounts of accompanying verification data to authenticate
them for purposes of financial fraud:
  We sell all you need to hack, shop & cashout.
  CardTipe / * CC Name / * CC Number / * CC Expiry / * CVV2 / * CC PIN
  First & Last Names / * Address & City / * State & Zip/Postal code / *
  Country (US) / * Phone #
  MMN [Mother's maiden name] / * SSN [Social security number] / * DOB
    [Date of birth]
  Bank Acc No / * Bank Routine [Routing] No
  On our forum you can buy:
  Active eBay accounts with as many positive feedbacks as you need
  Active and wealthy PayPal accounts
  PINs for prepaided AT&T and Sprint phone cards
  Carded Western Union accounts for safe and quick money transfers
  Carded UPS and FedEx accounts for quick and free worldwide shipping of
    your stuff
  Full info including Social Security Info, Driver Licence  Mother'
    Maiden Name and much more
  Come and register today and get a bonus by your choice:
  One Citybank account with online access with 3k on board, or 5
    COB' cards with 5k credit line
    10 eBay active eBay accounts with 100+ positive feedbacks
    25 Credit Cards with PINs for online carding
then it's just as easy to turn those identities towards facilitating further
identity fraud, and indeed it's become pretty much standard practice to
register fraudulent domains and buy fraudulent X.509 certificates with stolen
credentials paid for with stolen financial information.  As a result, if the
putative owner of an AuthentiCode certificate used to sign a piece of malware
is ever tracked down then it's invariably some innocent victim somewhere,
possibly someone who doesn't even use a computer.  Even the argument that at
least the signed malware allows for the use of CRLs to disable it falls flat
when you consider the difference in speed between having the malware
identified and blocked by anti-virus software and the ponderous delays of the
CRL issue process, assuming that the end-user software even checks them.
Another online fraud technique that's seen use in some countries, although
it's not widespread because it's still much easier to do the same thing via
less labour-intensive means, is to use stolen credentials to establish an
online presence for an existing business with a good credit history, use it
for whatever fraud you want to perpetrate, and then vanish before anyone's the
wiser, for example before the end of the monthly billing cycle when the real
business either gets sent paperwork that it isn't expecting or doesn't get
sent paperwork that it is.  Since this is borrowing the identity of a bona
fide business rather than an individual, there's almost no way to catch such
problems because any (rational) amount of checking will simply confirm that
it's a long-established legitimate business.  This type of fraud could
probably even defeat the verification used for EV certificates (at least as
set out in the guidelines published by some CAs), although at the moment it's
entirely unnecessary since it's possible to achieve the same ends through far
less laborious means.
This is a classic case of asking the drunk whether he's drunk - a system
rampant with identity fraud is expected to function as the basis for an
identity-based accountability mechanism.  Or to put it another way, on the
remote chance that someone does finally figure out what it'll take to make PKI
work, it still won't actually work.

@_date: 2008-12-22 23:18:44
@_author: Peter Gutmann 
@_subject: Security by asking the drunk whether he's drunk 
The sentence actually referred to the use of stolen credentials to facilitate
multiple crimes, the use to register fraudulent domains is widespread and
documented in blogs and cybercrime reports, e.g. the APWG's phishing surveys
and... well, lots of places, a Google search on something like "phishing
domain stolen credit card" should turn up a pile of hits.  The code-signing
and business impersonation is a lot rarer because it's mostly unnecessary to
do it, it's been done a number of times but barely reported in public so you
hear about it talking to security researchers at conferences as interesting
anecdotes but there's little public reporting on it ("Estonia nuked by DDoS"
gets rather more readers than "Signed rootkit discovered").  One of the most
notorious cases was Gromozon (a particularly nasty blended attack toolkit, see
e.g.  which was signed with a
Verithawte certificate issued to bogus company supposedly in Panama, for which
a quick Google:
  turns up a pile of hits (note that the issuing CA just happened to be
Verithawte in this case, malware authors will use just about any CA since all
they care about is turning off the warnings, and since it's not their money
they're using they don't care whether they're using cheap or expensive
certs... it's a nasty corollary to Ben Laurie's "proof-of-work proves not to
work", alongside effectively infinite CPU power the attackers also have access
to effectively infinite financial resources so they don't mind spending
whatever it takes of someone else's money to facilitate their attacks, e.g. in
registering large numbers of bogus accounts or domains for phishing purposes.
I've actually seen one of these live while monitoring a social networking site
that had paid premium accounts, they were using a script to register a user in
every locality in the countries served by the site for a total cost of
thousands of dollars of someone else's money, and the site admins then had to
manually go through and weed them all out again since they were set up to shut
down individual accounts based on user complaints but had never anticipated
anyone going in at this level).
The Sunbelt folks have blogged about signed malware they've discovered several
times as part of their investigation of malware, for this particular one:
  they tracked down the signer details (shown in the screen shots in the blog).
Again, a Google search like:
  should turn up about all that's been publicly written about this, but there's
a lot of further info that's circulated anecdotally.  All that sounds horribly
imprecise but I it's because it's something that's mostly a curiosity for now,
if you can get X zillion victims with unsigned/un-"authenticated" malware then
there's not much need to even try to bypass certificates, so only a bunch of
malware researchers and a few PKI geeks care about it.  In other words the
attackers haven't even bothered with the (supposed) defences yet, although the
small number of trial runs done so far indicate that if they ever need to
attack them they won't have any problems.
(Incidentally, there's a nice illustration of the closing-the-gate-after-the-
horse-has-bolted nature of CRLs in the Register:
    Within an hour of the reported incident we had attempted to examine the
  executable.  However, the site was no longer live.  After an unsuccessful
  attempt to contact the company by telephone we decided the best course of
  action in the short term would be to revoke the certificate.
So they revoked the cert after discovering that the hosting site had already
been taken down).
That one I can't give any references for, sorry, it was something that was
discussed at a cybercrime conference a few years ago as an example of the
impedance mismatch between bricks-and-mortar security mechanisms and the
Internet, but I don't know whether it's been formally documented.
(If anyone knows of anywhere where this is publicly documented, please let me
I doubt you'll be able to get this, for the reasons given above (and mentioned
in the original post), there's no need to go to this length yet so most
attackers don't bother.  As a result what we're seeing is proof-of-concept
stuff demonstrating that the defences don't work, but not enough to get
serious statistics for.  In mathematical terms we've got a there-exists rather
than a for-all.
This leads to a scary rule of thumb for defenders:
1. The attackers have more CPU power than any legitimate user will ever have,
   and it costs them nothing to apply it.  Any defence based on resource
   consumption is in trouble.
2. The attackers have more money than any legitimate user will ever have, and
   it costs them nothing to apply it.  Any defence built around financial
   outlay as a limiting factor is in trouble.
   Corollary: Systems that can't defend themselves against a situation where
   the financial cost of any operation (for example registering a new account)
   is effectively zero is in trouble.
3. The attackers have as much identity information as any legitimate user, but
   they have (effectively) infinite quantities of it.  Any defence based on
   identity-based accountability mechanisms is in trouble.
Anyone wanna try fighting that with cryptography?

@_date: 2008-12-22 23:38:00
@_author: Peter Gutmann 
@_subject: Security by asking the drunk whether he's drunk 
This always happens right after you hit ^D... it turns out that Microsoft
actually has published figures for this, although it's fairly recent so I
hadn't seen it before now:
    ... approximately 135,000 validly signed malware files were reported to
  Microsoft [there were 173K files in total, but 38K were
  expired/revoked/whatever].  Of signed detected files, severity of the
  threats tended to be high or severe, with low and moderate threats
  comprising a much smaller number of files.
Going directly to the source gets you much better stats than talking to
malware researchers at conferences :-).
"High" and "severe" typically means 0day rootkit-type exploits, so that's
scary stuff, particularly since that's only malware reported to MS and not all
the malware that's out there.  Hmm, I wonder if it's just coincidence that the
malware authors only bother signing the most effective/vicious malware to
ensure a good success rate and for the less effective ones they just leave
them as is?
Another interesting figure:
  valid code signing certificates were reported on over 1.78 million distinct
  non-malicious files to the MMPC
So from Microsoft's figures it looks like roughly every tenth signed file is
active (i.e. non-revoked/expired/whatever) malware.
Peter (so what we need now is EV certs for code-signing. Yeah, that'll fix
       it).

@_date: 2008-12-25 00:34:53
@_author: Peter Gutmann 
@_subject: Security by asking the drunk whether he's drunk 
One note of caution with the statistics given on that page, those figures are
apparently as reported by the Malicious Software Removal Tool (MSRT) (see
 so they'll represent the
output of a basic malware removal tool (not a full-blown malware/AV scanner),
and since it's only run on up-to-date Windows systems with auto-updates (and
therefore security hotfixes and whatnot) actively applied (MSRT is itself
supplied via auto-updates) it's likely that the real situation is a lot worse
than that, i.e. a full-blown AV program might find even more malware, and any
system that's regularly running the MSRT and applying security updates is
going to be less malware-infested than a general random sample of systems.  So
while they're a (really scary, much, much worse than I thought) indicator of
how bad it is, it's likely that things are even worse than that.  I've written
to the person who wrote the blog entry to try and get clarification on some
issues raised there.
(Oh, and I assume people have seen Eddy Nigg's article on how easy it is to
get a certificate for a site belonging to someone else from a commercial CA,
 which also made Slashspot earlier today).

@_date: 2008-12-26 20:39:50
@_author: Peter Gutmann 
@_subject: Security by asking the drunk whether he's drunk 
Given the more or less complete failure of commercial PKI for both SSL web browsing and code-signing (as evidenced by the multibillion-dollar cybercrime industry freely doing all the things that SSL certs and code-signing were supposed to prevent them from doing), it's not so much "cleaned up" as "replaced with something that may actually work".  Adding support for a service like Perspectives (discussed here a month or two back) would be a good start since it provides some of the assurance that a commercial PKI can't (and as an additional benefit it also works for SSH servers, since it's not built around certificates).
So, when will Google add Perspectives support to their search database? :-).

@_date: 2008-12-29 23:10:49
@_author: Peter Gutmann 
@_subject: Security by asking the drunk whether he's drunk 
I think this is missing the real contribution of Perspectives, which (like
almost any security paper) has to include a certain quota of crypto rube-
golbergism in order to satisfy conference reviewers.  The real value isn't the
multi-path verification and crypto signing facilities and whatnot but simply
the fact that you now have something to deal with leap-of-faith
authentication, whether it's for self-generated SSH or SSL keys or for rent-a-
CA certificates.  Currently none of these provide any real assurance since a
phisher can create one on the fly as and when required.  What Perspectives
does is guarantee (or at least provide some level of confidence) that a given
key has been in use for a set amount of time rather than being a here-this-
morning, gone-in-the-afternoon affair like most phishing sites are.  In other
words a phisher would have to maintain their site for a week, a month, a year,
of continuous operation, not just set it up an hour after the phishing email
goes out and take it down again a few hours later.
For this function just a single source is sufficient, thus my suggestion of
Google incorporating it into their existing web crawling.  You can add the
crypto rube goldberg extras as required, but a basic "this site has been in
operation at the same location with the same key for the past eight months" is
a powerful bar to standard phishing approaches, it's exactly what you get in
the bricks-and-mortar world, "Serving the industry since 1962" goes a lot
further than "Serving the industry since just before lunchtime".

@_date: 2008-12-30 17:25:55
@_author: Peter Gutmann 
@_subject: Security by asking the drunk whether he's drunk 
I'm not aware of any absolute figures for this but there's a lot of anecdotal
evidence that many cert renewals just re-certify the same key year in, year
out (there was even a lawsuit over the definition of the term "renewal" in
certificates a few years ago).  So you could in theory handle this by making a
statement about the key rather than the whole cert it's in.  OTOH this then
requires the crawler to dig down into the data structure (SSH, X.509,
whatever) to pick out the bits corresponding to the key.  Other alternatives
are to use a key-rollover mechanism that signs the new key with old one
(something that I've proposed for SSH, since their key-continuity model kinda
breaks at that point), and all the other crypto rube-goldbergisms you can
dream up.
In any case though at the moment we have basically no assurance at all of
key/cert information so even a less-than-perfect mechanism like trusting
Google and having problems during cert rollover is way, way better than what
we've got now.  In any case if Google decides to go bad then redirecting
everyone's searches to  is a bigger worry than whether
they're sending out inaccurate Perspectives responses.

@_date: 2008-12-31 13:57:47
@_author: Peter Gutmann 
@_subject: Security by asking the drunk whether he's drunk 
... or the fact that one in ten signed Windows binaries are comercial CA-
certified signed malware, or that we have a multibillion dollar global
phishing industry built around the failure of SSL certs to do what they were
supposed to?
On this, the final day of 2008, the 30th anniversary of certificates and the
20th anniversary of X.509, I declare commercial PKI...
... failed [0][1].
It's had thirty years, let's get over it and move on to something that
actually has a hope of working.
Peter (who doesn't see much chance of that happening, unfortunately).
[0] Except to people holding stock in certificate manufacturers, who aren't
    doing so badly.
[1] Or at least "obviously failed", as opposed to the earlier "failed but we
    can pretend there isn't a problem".

@_date: 2008-02-04 22:36:39
@_author: Peter Gutmann 
@_subject: Gutmann Soundwave Therapy 
I should provide some background for the writeup, it started when someone sent
me a link to some VPN software they were using and asked whether it was
actually secure.  I looked at it, found that it was, well, pretty awful, and
told them so.
So they sent me a link to another VPN app.  I had a look at it and it was just
as bad.
By this time it'd turned into an ongoing discussion/attempt to track down some
sort of decent easy-to-use secure-VPN app.  The more we found, the more
discouraged I became.  Initially we'd tried to contact developers but didn't
get much (if any) response, so that towards the end (after getting to the n-th
broken VPN app), to quote the VAX assembler manual, "little sympathy was
extended".  After the initial writeup ended up on Slashdot I did a bit more
googling and found out that some of the problems had been pointed out by
others years before I noted them with no action from the application authors
to fix anything.  This, again, didn't inspire much confidence.
In terms of problems, it wasn't just the homebrew crypto mechanisms, there
were also numerous problems with careless implementations.  One thing that was
very common was to find very little error- or sanity-checking.  Function
return calls weren't checked, critical errors like crypto failures were logged
but the app continued anyway (!!), operations were assumed to have succeeded
at all times, even minor things like checking for an error return with a check
for '== -1' when the function could also fail with a return status of zero (so
only some failures were caught and the code could continue with ininitialised
crypto), the list just went on and on.
I think OpenVPN took the right approach here, they took the part of IPsec that
works well (the ESP transport mechanism) and bolted on the TLS handshake to
replace IKE (DTLS has only appeared quite recently).  They didn't have to
invent their own mechanisms for anything, but took tried-and-tested crypto
mechanisms and code and just went with that.

@_date: 2008-02-04 22:38:31
@_author: Peter Gutmann 
@_subject: Gutmann Soundwave Therapy 
This is where the OpenVPN developers got it right: Use TLS for the handshake
and IPsec's ESP for the transport.  It's been a solved problem for some years

@_date: 2008-02-04 23:03:08
@_author: Peter Gutmann 
@_subject: Dutch Transport Card Broken 
HCI people have been studying this for quite some time, and there's been a lot
of good work done in this area.  Because of the amount of information, I'll
answer indirectly via a link (warning, it's a partial book draft and is
currently ~140 pages long):
Even without this detailed analysis, one of the Mac browsers (Safari?) already
has a quite distinctive password prompt that rolls down out of the menu bar at
the top.  Sure, you can spoof that if you own the browser, but if malware owns
your browser then you're toast anyway.
That's the response I got from a browser developer when I talked about this
about a year ago, "Sufficiently sophisticated malware can spoof any piece of
browser UI, so let's just give up and admit that the phishers have won".  At
the moment, after 15-odd years of work, the state of the art for both major
secure-channel protocols is to connect to anything listening on port 22 or 443
and then hand over the user's password in plaintext form (although inside a
secure tunnel, as if that made any difference) [0].  This is only just barely
better than the 1970s-era telnet in that the authenticator is still handed
over in plaintext, but at least you can't capture it with a packet sniffer.
Moving to a challenge-response mechanism (which PSK and SRP aren't really,
it's more a bit-commitment since there's no real challenge or response process
[1]) would at least move the security into the late 1980s.
As a side-note, I was talking to a security person from a large (multi-
national) bank recently and they mentioned that they were slowing down on the
push to move to two-factor auth (real two-factor auth with SecurIDs and the
like, not the gimmicks that US banks are using :-) because the problem isn't
authenticating the user, it's authenticating the server and/or the
transaction, and most two-factor auth tokens can't do that.  As a result
they're not going to commit to sinking much more money into something that
doesn't actually solve the problem.  So mutual client/server auth is something
that's of concern to more than just some geeks on security mailing lists, it's
coming onto the radar of large financial institutions as well.
[0] By "443" I mean HTTP over SSL/TLS, obviously.
[1] Actually this is neither challenge-response nor bit-commitment so in the
    absence of anything better I'll propose "failsafe authentication" because
    the other side doesn't get your authenticator unless they can prove they
    already possess it.  In other words if the authentication process fails,
    it fails safe.

@_date: 2008-02-04 23:21:09
@_author: Peter Gutmann 
@_subject: questions on RFC2631 and DH key agreement 
I'm going to approach the answer somewhat differently: Why are you using this
mechanism?  The only reason that it's present in the spec is politics, it
being an attempt to avoid the RSA patent.  Its adoption was severely hampered
by the fact that US vendors already had RSA licenses, non-US vendors didn't
care (and in any case the patent has now expired, so they care even less), no
CA's of note will issue X9.42 certificates, and even if they did almost no
S/MIME implementations support it.  Although X9.42 was at one point listed as
mandatory to implement for S/MIME v3, the approach that was taken by most
vendors was to vaguely pretend to support X9.42 while actually concentrating
on RSA, knowing that noone else supported it either (AFAIK only two vendors
ever really supported it, Microsoft had a receive-only implementation so that
no-one could accuse them of not being compliant with the spec, and the S/MIME
Freeware Library (which was the reference implementation and therefore had no
choice in supporting it) supported it because it had to).  A few years after
the expiry of the RSA patent, the matter was corrected by changing the
standard so that vendors were no longer required to even pretend to support
X9.42.  My comments at the time were:

@_date: 2008-02-04 23:33:07
@_author: Peter Gutmann 
@_subject: Gutmann Soundwave Therapy 
Hmm, given this X-to-key-Y pattern (your DTLS-for-SRTP example, as well as
OpenVPN using ESP with TLS keying), I wonder if it's worth unbundling the key
exchange from the transport?  At the moment there's (at least):
  TLS-keying --+-- TLS transport
               +-- DTLS transport
               +-- IPsec (ESP) transport
               +-- SRTP transport
               +-- Heck, SSH transport if you really want
Is the TLS handshake the universal impedance-matcher of secure-session

@_date: 2008-02-05 02:46:09
@_author: Peter Gutmann 
@_subject: TLS-SRP & TLS-PSK support in browsers (Re: Dutch Transport Card Broken) 
I know of a number of organisations (mostly governmental, but also some
financial) in various countries who are really, really keen to get support for
(as James Donald pointed out) cryptographically secured relationships (not
requiring PKI would be a big feature) into browsers, but no-one knows who to
beat over the head about it.  The last group I talked to (banks) were hoping
to use commercial pressure to get MS to add support for it in IE7^H^H8 at
which point Firefox would be forced to follow, but it's a slow process.
I think it's a combination of two factors:
1. Everyone knows that passwords are insecure, so it's not worth trying to do
   anything with them.
   (My counter-argument to this is that passwords are only insecure because
   protocol designers have chosen to make them insecure, see my previous post
   about the quaint 1970s-vintage hand-over-the-password model used by SSH and
   SSL/TLS).
2. If you add failsafe authentication to browsers, CAs become redundant.
   (My counter-argument to this is to ask whether browser security exists in
   order to provide a business model for CAs or to protect users.  Currently
   it seems to be the former, with EV certs being a prime example).
There are probably other contributory reasons as well.

@_date: 2008-02-05 16:42:46
@_author: Peter Gutmann 
@_subject: questions on RFC2631 and DH key agreement 
' =JeffH ' I'm referring to the "X9.42" mechanism (as used in CMS) as a whole (see below
for the reason why this is in quotes).
Oh.  In that case you have my sympathy :-).
I'm referring to the (old) CMS RFCs.  Even the RFCs themselves don't use
proper X9.42, they were based on an old draft that floated around for awhile
and was subsequently changed and updated.  You can see this if you look at the
order of the DLP key parameters, everything else (e.g. FIPS 186) uses { p, q,
g }, while the old CMS RFCs flip the second two values to use { p, g, q }.
I think the definitive comment on this (which also talks about differences
between FIPS 186, various X9.42 drafts, and the CMS use of those drafts) is by
the former editor of X9.42, and is archived at
Specifically CMS, since X9.42 isn't necessarily what's used in CMS.

@_date: 2008-02-07 17:37:02
@_author: Peter Gutmann 
@_subject: Dutch Transport Card Broken 
I've discussed this with (so far) a small sample of assorted corporate TLS
users to get at least a general idea of what'd be involved.  At a very
abstract level all they see is "username + password + TLS" ->
"permitted/denied", the only change is that by moving the verification into
TLS this process happens a bit earlier than when it's done in HTML (and
obviously the failsafe nature means the other side never gets the password if
the auth fails).
At an implementation level it's also fairly simple, it's maybe 2-3 pages of
code added to my SSL implementation, and I spoke to another SSL developer who
gave similar figures.  All you're doing is mixing a little extra keying
material into the premaster secret, it's not a major piece of programming.
The real issues occur in two locations:
1. In the browser UI.
2. In the server processing, which no longer gets the password via an HTTP
   POST but as a side-effect of the TLS connect.
(1) is a one-off cost for the browser developers, (2) is a bit more complex to
estimate because it's on a per-site basis, but in general since the raw data
(username+pw) is already present it's mostly a case of redoing the data flow a
bit, and not necessarily rebuilding the whole system from scratch.  To give
one example, a healthcare provider, they currently trigger an SQL query from
an HTTP POST that looks up the password with the username as key, and the
change would be to do the same thing at the TLS stage rather than the post-TLS
HTTP stage.
With the folks I've discussed this with their concern has been far more "We
want this yesterday, why isn't it here yet" rather than "We can't integrate
this with our existing back-ends".

@_date: 2008-02-07 17:39:39
@_author: Peter Gutmann 
@_subject: TLS-SRP & TLS-PSK support in browsers (Re: Dutch Transport Card Broken) 
There's always the problem of politics.  You'd think that support for a free
CA like CAcert would also provide fantastic marketing opportunities for free
browser like Firefox, but this seems to be stalled pretty much idefinitely
because since CAcert doesn't charge for certificates, including it in Firefox
would upset the commercial CAs that do (there's actually a lot more to it than
this, see the interminable flamewars on this topic on blogs and whatnot for
more information).
Here's a suggestion to list members:
- If you know a Firefox developer, go to them and tell them that TLS-PSK and
  TLS-SRP support would be a fantastic selling point and would allow Firefox
  to trump IE in terms of resisting phishing, which might encourage banks to
  recommend it to users in place of IE.
- If you know anyone with some clout at Microsoft, tell them that your
  organisation is thinking of mandating a switch to Firefox because IE doesn't
  support phish-resistant authentication like TLS-PSK/TLS-SRP, and since you
  have x million paying customers this won't look good for MS.
- If you work for any banking regulators (for example the FFIEC), require
  failsafe authentication (in which the remote site doesn't get a copy of your
  credentials if the authentication fails) rather than the current two-factor
  auth (which has lead to farcical "two-factor" mechanisms like SiteKey).
Oh, and don't tell them I put you up to this :-).

@_date: 2008-02-07 20:44:23
@_author: Peter Gutmann 
@_subject: Dutch Transport Card Broken 
It really depends on the value of the account, for high-value ones I would
hope it's done out-of-band (so you can't just sign up for online banking by
going to a bank's purported web page and saying "Hi, I'm Bob, give me access
to my account"), and for low-value stuff like Facebook I'm not sure how much
effort your password is worth to an attacker when they can get a million
others from the same site.  I agree that it's still a problem, but switching
to failsafe auth is a major attack surface reduction since now an attacker has
to be there at the initial signup rather than at any arbitrary time of their
choosing.  It's turning an open channel into a time- and location-limited

@_date: 2008-02-07 20:47:20
@_author: Peter Gutmann 
@_subject: TLS-SRP & TLS-PSK support in browsers (Re: Dutch Transport Card Broken) 
Is that the FF devlopers' reason for holding back?  Just wondering... why not
release it with TLS-PSK/SRP anyway (particularly with 3.0 being in the beta
stage, it'd be the perfect time to test new features), tested against existing
implementations, then at least it's ready for when server support appears.  At
the moment we seem to be in a catch-22, servers don't support it because
browsers don't, and browsers don't support it because servers don't.

@_date: 2008-02-07 22:00:00
@_author: Peter Gutmann 
@_subject: Want to drive a Jaguar? 
Physical Cryptanalysis of KeeLoq Code Hopping Applications
  Recently, some mathematical weaknesses of the KeeLoq algorithm have been
  reported. All of the proposed attacks need at least 2^16 known or chosen
  plaintexts. In real-world applications of KeeLoq, especially in remote
  keyless entry systems using a so-called code hopping mechanism, obtaining
  this amount of plaintext-ciphertext pairs is rather impractical. We present
  the first successful DPA attacks on numerous commercially available products
  employing KeeLoq code hopping. Using our proposed techniques we are able to
  reveal not only the secret key of remote transmitters in less that one hour,
  but also the manufacturer key of receivers in less than one day. Knowing the
  manufacturer key allows for creating an arbitrary number of valid
  transmitter keys.
KeeLoq is used in large numbers of car keyless-entry systems.  Ouch.

@_date: 2008-02-08 20:59:46
@_author: Peter Gutmann 
@_subject: Gutmann Soundwave Therapy 
Is there much work to be done here?  If you view the keyex mechanism as a
producer of an authenticated blob of shared secrecy and the post-keyex
portions (data transfer or whatever you're doing) as a consumer of said blob,
with a PRF as impedance-matcher (as is done by SSL/TLS, SSH, IPsec, ..., with
varying degrees of aplomb, and in a more limited store-and-forward context
PGP, S/MIME, ...), is there much more to consider?

@_date: 2008-02-10 15:54:45
@_author: Peter Gutmann 
@_subject: Fixing SSL (was Re: Dutch Transport Card Broken) 
Because they're essentially unworkable.  At the risk of spamming this
reference a bit too often here:
There's detailed discussion there of results of user studies, conference
papers, references, (hopefully) all the information you need.

@_date: 2008-02-10 16:02:08
@_author: Peter Gutmann 
@_subject: Toshiba shows 2Mbps hardware RNG 
I've always wondered why RNG speed is such a big deal for anything but a few
highly specialised applications.  For security use you've got two options:
1. Use it with standard security protocols, in which case you need all of 128
   or so bits every now and then (and very rarely a few thousand bits for
   asymmetric keygen).
2. Use it at its full data rate, in which case you can only communicate with
   someone else who has the same device, but more importantly you need to
   design and build your own custom security infrastructure to take advantage
   of the high-data-rate randomness, which is much harder than simply
   designing an RNG device and declaring victory.
   (In any case if you really need high-data-rate randomness, you just take
   your initial 128 bits and use it to seed AES in CTR mode).
So your potential market for this is people running Monte Carlo simulations
who don't like PRNGs.  Seems a bit of a limited market...

@_date: 2008-02-11 17:47:27
@_author: Peter Gutmann 
@_subject: Toshiba shows 2Mbps hardware RNG 
I think a much bigger reason for its non-acceptance was that it wasn't there
(either present or available or accessible) in most cases.  The PRNG in VIA's
C7 series hasn't had any of these problems, and is supported out of the box by
a pile of software and even some distros (typically via /dev/random).

@_date: 2008-02-11 18:00:14
@_author: Peter Gutmann 
@_subject: Toshiba shows 2Mbps hardware RNG 
The security target for the gambling industry is to pass (incredibly
stringent) auditing requirements.  A simple PRNG seeded from a factory-set
initial value is fine as long as it's been certified to death.  The impression
I got from this some time ago from people who work in this area was that they
really wanted deterministic PRNGs rather than nondeterministic hardware ones,
although at the time I didn't ask whether it was because it made certification
easier or because they just didn't trust unpredictable RNGs ("unpredictable"
meaning that an infinite number of environmental variations can cause it to
potentially do things that you don't want).

@_date: 2008-02-11 19:01:07
@_author: Peter Gutmann 
@_subject: Gutmann Soundwave Therapy 
This is an example of what psychologists call own-side bias ("everyone thinks
like us"), in this case the assumption that after a peer has authenticated
itself it'd never dream of sending a malformed packet and so we don't need to
do any checking after the handshake has completed.  Why would you trust data
coming from a remote system just because they've jumped through a few hoops
before sending it?  I can steal the remote system's credentials or hijack the
session and then send you anything I want, it's no safer to blindly accept the
data if there's a MAC attached or not.
More scarily, and specifically for the case of VoIP, the security of many SIP
devices is absolutely appalling (for German speakers there's a paper on this
at the DFN-Cert workshop in a few days,
  So the obvious attack
vector is to 0wn the peer's SIP device and ensure that it creates malformed
data packets well before the security layer ever takes effect.  As a result
your secured tunnel is pouring out carefully authenticated attack packets as
fast as it can send them.  The bad guys have been exploiting this for years,
spamming their malware out to "trusted" friends on contact lists, and it's
proven quite successful.
Hostile data inside a secure tunnel or wrapper is still hostile data.  As the
OP said, as long as you can deterministically detect attacks (which a 1-of-n
packet MAC will do) you're not giving up much security by not MAC'ing all

@_date: 2008-02-11 19:17:43
@_author: Peter Gutmann 
@_subject: Please steal my personal data [OK] 
Jan Miksovsky (UI designer) has an interesting post on his blog about the
phishing-friendly nature of Facebook apps. Consider the following scenario:
  You get a message from someone you know (well, someone on your Facebook
  friends list, which means a complete stranger you've never met before but
  who you added because whoever dies with the most entries on their list wins)
  saying "Hey, click on/run this!".  "This" is an unknown app that (by
  default) has access to your information and embeds itself in your Facebook
  experience.
Sound like a phishing attack?  Nope, it's SOP for Facebook:
Facebook (and who knows how may other sites): Hard at work training up the
next generation of phishing victims.

@_date: 2008-02-14 17:08:04
@_author: Peter Gutmann 
@_subject: Toshiba shows 2Mbps hardware RNG 
That's only a part of it.  Military silicon has a hardware RNG on chip
alongside a range of other things because they know full well that you can't
trust only a hardware/noise-based RNG, there are too many variables and too
many things that can go wrong with that single source.  That's why I was
sceptical of the "we've solved the RNG problem with our custom hardware"
claim, they've created one possible source of input but not a universal

@_date: 2008-02-15 00:00:16
@_author: Peter Gutmann 
@_subject: Fixing SSL (was Re: Dutch Transport Card Broken) 
While there's an obvious interpretation of that ("Microsoft want to lock
everyone into CardSpace"), there's a second interpretation that's equally
likely: After > 10 years of effort and getting almost exactly nowhere with
client certs, Microsoft are moving on to something more likely to succeed
(actually I have no idea how workable CardSpace is since I don't think
anyone's done any usability studies on it, but I doubt it's more unworkable
than client certs.  Is anyone aware of any third-party usability studies on
CardSpace, OpenID, ...?).
Does anyone care that you can't sign your emails with CardSpace?
(I could post my standard reference on this here again :-).  The unwashed
masses don't even know what signed email is, let alone care about using it.  I
know that there are assorted corporates and so on that are still keen on email
signing, but they can keep playing with PKI for that.  CardSpace/Liberty/
OpenID/SAML/whatever[0] should handle the rest.
[0] I'm not sure whether putting "CardSpace" and "Liberty" in such close
    proximity in the above line was a good idea.  If your monitor starts     smoking due to the friction generated, please cut&paste one of the two     elsewhere.

@_date: 2008-02-15 19:21:44
@_author: Peter Gutmann 
@_subject: House o' Shame: Amtrak 
The next problem, though, is that the message asks people to log in by
  clicking a link in the message:
  Go to Amtrak.com now and update your profile
  It's not just Amtrak that do that, CapitalOne also send out phishing email
directing users to bfi0.com.
Lesson for phishers: If you want your phish to seem more legit, outsource it
to Bigfoot Interactive, which seems to lead back to Epsilon Agency Services,
who specialise in... well, phishing, but for the good guys.  I bet the Russian
Business Network could do it for less though :-).

@_date: 2008-02-16 16:32:15
@_author: Peter Gutmann 
@_subject: Toshiba shows 2Mbps hardware RNG 
Clipper (or more specifically Capstone, via the Fortezza card) is a great
example of the NSA's sound engineering approach to generating random data [0].
They used a physical randomness source of an unpublished type, presumably a
standard noise-based source.  And a Skipjack-based ANSI X9.17 PRNG with a pre-
set random seed.  *And* a 48-bit counter.  All were mixed up with SHA-1.  The
design was such that there was no single point of failure in the sources
themselves, so that as long as SHA-1 was at least vaguely useful as a mixing
function the sources themselves didn't have to be totally failure-proof.  For
example if some environmental condition reduced the utility of the noise-based
source, the PRNG would still provide strong input.  If the PRNG locked up for
some reason, the secret seed for that combined with the counter would still
provide varying input to the SHA-1 step.  I assume they also sampled the
output as per FIPS 140 to detect a lockup of the SHA-1 step.  The result was a
failure-tolerant design that didn't rely on the totally failure-proof
operation of a single component in order to function.
[0] I'm calling it a "sound engineering approach" because I did the same thing
    in my PRNG design (independent of Fortezza).  Others may wish to call it
    "excessively paranoid" and other things :-).

@_date: 2008-02-16 17:26:36
@_author: Peter Gutmann 
@_subject: kit to prevent computers from losing power during seizure. 
For those who don't want to plough through the docs, it looks like a static
transfer switch that requires you to take a tap from a mains line and feed it
to a UPS.  The "tap" relies on either having the PC on a power strip or
stripping the mains flex and attaching jumpers.  So the setup before is:
  Mains -------------------+------ PC
  UPS   ------- STS -------+
When you unplug the resulting setup from the wall, the HotPlug device detects
the voltage loss (in other words it contains a portion of a UPS switchover
circuit, the static transfer switch, which is just some SCRs operating as a
zero-crossing switch and a controller IC) and switches over to the UPS:
                           +------ PC
  UPS   ------- STS -------+
and you can remove the PC.
I was going to suggest that given the usual LE-targeted device pricing (five
figures and up) it'd probably be cheaper to buy a commercial STS, but at only
$500 it's quite reasonably priced.
And successively more Rube-Goldberg :-).

@_date: 2008-02-22 14:39:02
@_author: Peter Gutmann 
@_subject: Fixing SSL (was Re: Dutch Transport Card Broken) 
Correction, "exactly like phishing criminals are actively doing right now"
(hat tip to Don Jackson of SecureWorks who's investigated and documented this
practice).  Given the almost complete failure of client certs in the
marketplace, I found it most amusing that the current active users of "client
certs" are phishers.  It reminded me of spammers and SPF.
Thus postdating Microsoft's CertEnroll/Certenr3/Xenroll ActiveX control by
several years.  The only difference here is that the user generates the cert
directly rather than involving a CA.

@_date: 2008-02-22 14:56:22
@_author: Peter Gutmann 
@_subject: USB drive manufacturer encrypts data with XOR 
The fault here isn't with Nova (or any of the other vendors that use the
Innmax device), it's squarely with Innmax, whose advertising and data sheets
imply that the disk is encrypted with AES without ever mentioning that it's
only ever used in conjunction with the RFID portion.
(By "imply" I mean the specs state "Uses AES encryption", technically you
could argue that since they never specifically say "All disk data is encrypted
using AES encryption" they'd be justified in storing it in plaintext, but
that's not what someone looking at the spec will interpret it as).
To make it even more confusing, "Easy Nova" is a near-namespace collision with
eNova, who do AES-encrypt the disk contents (well, as far as anyone knows...
anyone here have a eNova X-Wall drive controller and feel like seeing what's
on the disk?).

@_date: 2008-02-23 05:09:29
@_author: Peter Gutmann 
@_subject: cold boot attacks on disk encryption 
There were commercial products that did this available some years ago, they
hooked into the Windows auth using a custom GINA DLL (GINA = the Windows
extensible login/authentication mechanism, think PAM for Windows) and locked
the machine when you moved away from it.  They failed in the marketplace,
there was no interest in them from users (or at least several of them failed,
some may still be around).  I was given a bunch of the tags some years ago
when one vendor discontinued them, but from memory the drivers were from the
NT4 era and there was no chance they were going to be updated further so I
never did anything with them.  I wasn't able to find out any more details
about their failure in the marketplace beyond "no-one bought them".
There have even been DIY articles on this published, e.g.
 from ExtremeTech.

@_date: 2008-02-24 22:35:39
@_author: Peter Gutmann 
@_subject: Toshiba shows 2Mbps hardware RNG 
PRNG design starts getting into the philosophy of paranoia fairly quickly.
How much safety is enough?  When do you stop?  For example I designed my PRNG
so that any one critical-path component in it can fail completely without it
affecting the security of the generator (barring pathological failures like
every byte of code spontaneously transmuting itself into a NOP).  In other
words you can replace (say) the SHA-1 operation with a memcpy() and it won't
affect overall security.  OTOH other designers have assumed that their mixing
operation has the desired properties and therefore adding this level of
redundancy is complete overkill.  Who's right?  (You are not expected to
answer this :-).
I think the answer to this generalises to "why does weak security keep showing
up"?  The answer, to quote Bruce Schneier, is that people will deploy
technology that's as insecure as they can possibly get away with.  Ages ago
when DPA attacks were first publicised I talked to some folks who had designed
a particular piece of crypto gear that happened to be completely immune to any
attempts to mount DPA attacks (and various other side-channel attacks) on it.
They hadn't designed it to be specifically immune to DPA attacks, all they'd
done was apply sound engineering design with good power-supply decoupling,
filtering of all signal lines, and so on.  The result was that the NRE cost
was higher, the manufacturing cost was higher, and it took a bit longer to get
to market, but they had a device that severely annoyed the pen-testers because
everything they tried simply bounced off.  The competitors' engineers probably
got a bonus for bringing their product to market faster and at lower cost by
being less conservative with their design. The folks in the competitors'
manufacturing department who figured out that you could omit several of the
security features in the product once it had been evaluated in order to save
money almost certainly got a bonus for their clever cost-cutting.  Ask
hardcode geeks and they'll say that the former product is better.  Ask almost
anyone else and they'll say the latter product is better - it's much cheaper,
and who cares about a few theoretical attacks that never happen in the real
I did warn you that it'd get philosophical...

@_date: 2008-01-04 16:38:07
@_author: Peter Gutmann 
@_subject: DRM for batteries 
At $1.40 each (at least in sub-1K quantities) you wonder whether it's costing
them more to add the DRM (spread over all battery sales) than any marginal
gain in preventing use of third-party batteries by a small subset of users.

@_date: 2008-01-04 16:47:33
@_author: Peter Gutmann 
@_subject: Question on export issues 
That's because there's nothing much to publish:
In the US, notify the BIS via email.
Outside the US, just export away.

@_date: 2008-01-04 20:53:27
@_author: Peter Gutmann 
@_subject: Death of antivirus software imminent 
Actually VMMs do provide some security, but not in the way you think.  Since
malware researchers typically run malware they're analysing inside a VM, quite
a bit of malware will silently exit (or at least not exercise the "mal" part
of its name) if it detects that it's running inside a VM.  So you can
inoculate yourself against at least some malware by running your OS inside a

@_date: 2008-01-05 02:57:27
@_author: Peter Gutmann 
@_subject: Death of antivirus software imminent 
It's been done for awhile by various rootkits.  The first AFAIK was
ShadowWalker, which marked pages to be hidden as non-present and used a custom
page fault handler to allow it to slip in whatever it wanted the victim to
There's a large number of variants of this sort of thing.  Some of the most
deviously elegant rootkits use anti-anti-virus scanners to detect antivirus
software from underneath before the AV software detects it.  They then either
de-fang the AV software in some way, or unhook themselves until the AV scan
has passed.  One neat trick used by... ah, forgotton the particular malware,
but it swaps the handle of the process the AV software is trying to terminate
and the AV software itself, so the AV program ends up terminating itself.
There's lots more like this.  You name it, they've done it.

@_date: 2008-01-05 22:46:13
@_author: Peter Gutmann 
@_subject: "From the Russian translation of one of your service manuals" 
There's a great exchange in "Never Say Never Again" that pokes fun at
unnecessary secrecy preventing access by legitimate users:
Bond: "Commander Peterson are you equipped with the new XT-7B's?"
Peterson: "That's top secret.  How do you know about them?"
Bond: "From the Russian translation of one of your service manuals"
Reading through a recent malware paper I think I've found the computer
equivalent: "An Inquiry into the Nature and Causes of the Wealth of Internet
Miscreants" contains a section (section 3.1.1) in which the researchers try to
determine compromised card information by country:
  The official BIN number database is not available to the public.  We use a
  BIN list containing information for 52,492 banks [...] which we acquired as
  part of the source code of a channel service bot.
("From the Russian source code of the people the security measure is designed
 to protect against").

@_date: 2008-01-07 19:07:37
@_author: Peter Gutmann 
@_subject: DRM for batteries 
The device is also sold for use in other high-profit-margin items like laser
printer toner cartridges and ink tanks.  While I can certainly see the point
of your argument as applied to batteries, I haven't heard of too many toner
cartridges exploding recently, and given the extensive bad press given to
dubious tactics like "starter cartridges" I don't think the printer
manufacturers are that worried about publicity issues (the printer ink/toner
market is definitely a racket, and has been documented in some detail

@_date: 2008-01-08 21:17:30
@_author: Peter Gutmann 
@_subject: "I am ashamed of you. They couldn't hit an elephant at this distance" 
TV presenter Jeremy Clarkson has lost money after publishing his bank
  details in his newspaper column.
  The Top Gear host revealed his account numbers after rubbishing the furore
  over the loss of 25 million people's personal details on two computer discs.
  He wanted to prove the story was a fuss about nothing.
  But Clarkson admitted he was "wrong" after he discovered a reader had used
  the details to create a ?500 direct debit to the charity Diabetes UK.
  [...]

@_date: 2008-01-12 16:20:53
@_author: Peter Gutmann 
@_subject: Foibles of user "security" questions 
It's not just older people, it's manual workers, children, and (as a
generalisation for all biometrics) "goats", the percentage of the overall
population who don't produce useful results for whatever biometric is being
employed.  The population of goats (for a reasonable FAR/FRR) is usually in
the low single digits.  The standard response to goats is to wind down the FRR
until the problem is no longer noticeable.  More on this in
(FAR = false acceptance rate, FRR = false rejection rate).

@_date: 2008-01-20 20:28:33
@_author: Peter Gutmann 
@_subject: HOWTO: Avoid common problems when writing a security conference paper 
Having served on 1, 2, 3... many security conference program committees, there
are some common mistakes that I've seen come up again and again in submitted
papers.  Every time this happens I feel I should write a HOWTO discussing
these, but somehow I never get around to finishing it.  Anyway, I've finally
been motivated to (mostly) finish it.  Feedback welcome, once it's done I'll
put it online at the URL given in the text.

@_date: 2008-01-22 13:02:54
@_author: Peter Gutmann 
@_subject: Changes in Russian licensing of cryptraghical tools 
Someone who works for a Russian bank told me that it was designed to ensure
that FAPSI got paid [0].  In other words the government recognises that trying
to regulate encryption use is pointless, but since corporates can't afford to
flout the law this measure guarantees a steady cashflow.
Never attribute to malice what can be adequately explained by baksheesh.
[0] It's not FAPSI any more now, this was a few years ago.  I assume the FSB
    now get the money.

@_date: 2008-01-22 17:05:43
@_author: Peter Gutmann 
@_subject: Unusual threat models 
Threat modelling doesn't just apply to computer security.  What happens if
someone decides to pen-test paid parking?
It's an interesting read because I'm sure they never anticipated anyone
attacking it at that level.

@_date: 2008-01-24 02:19:05
@_author: Peter Gutmann 
@_subject: patent of the day 
It does seem a bit puzzling... could it be a defensive patent?  There were
MSDOS OTFE programs doing exactly this more than 15 years ago.

@_date: 2008-01-28 12:50:00
@_author: Peter Gutmann 
@_subject: malware in digital photo frames infects users computers 
"And if thine eye offend thee, pluck it out".
It's not that easy.  Windows relies on autoplay for software installs (that
is, it's intended use is to automatically run the installer when you insert a
software CD).  Turning this off is probably going to cause an avalanche of
user support calls when their software "stops working".
It is possible to turn off autoplay just for USB devices through an obscure
registry hack, but this may turn off automatic handling of your digital camera
(and scanner, and ...) as well.  In other words when you plug in your digital
camera to copy photos across, nothing happens, and the camera isn't recognised
by Windows (I've seen this happen when you turn off the Still Image Service,
there's no way to access your camera any more).

@_date: 2008-01-28 15:12:06
@_author: Peter Gutmann 
@_subject: VaultID 
they're implementing is one-time CC numbers on various devices.  Banks have
been using one-time CC numbers for awhile now, all this is doing is garnishing
them with an extra layer of biometric magic.  The important thing isn't the
biometrics, it's the one-use-only CC number that provides the security, and
that's not really new.

@_date: 2008-02-01 13:08:02
@_author: Peter Gutmann 
@_subject: Fixing SSL (was Re: Dutch Transport Card Broken) 
To paraphrase Winston Churchill, "SSL is the worst secure-pipe protocol,
except for all the others".  Like most people here, I can find assorted nits
to pick with it (mostly message-formatting stuff and the like, which is
actually relatively trivial), but every time I look at its competitors I
realise that they're all much, much worse.  Conversely, it's amazing how many
other protocols are just SSL reinvented badly (or in several cases, really
really badly).

@_date: 2008-02-01 13:15:09
@_author: Peter Gutmann 
@_subject: Dutch Transport Card Broken 
If anyone's interested, I did an analysis of this sort of thing in an
unpublished draft "Performance Characteristics of Application-level Security
Protocols",   It
compares (among other things) the cost in RTT of several variations of SSL and
SSH.  It's not the TCP RTTs that hurt, it's all the handshaking that takes
place during the crypto connect.  SSH is particularly bad in this regard.

@_date: 2008-02-01 13:29:52
@_author: Peter Gutmann 
@_subject: Dutch Transport Card Broken 
Actually it doesn't even require X.509 certs.  TLS-SRP and TLS-PSK provide
mutual authentication of client and server without any use of X.509.  The only
problem has been getting vendors to support it, several smaller
implementations support it, it's in the (still unreleased) OpenSSL 0.99, and
the browser vendors don't seem to be interested at all, which is a pity
because the mutual auth (the server has to prove possession of the shared
secret before the client can connect) would significantly raise the bar for
phishing attacks.
(Anyone have any clout with Firefox or MS?  Without significant browser
support it's hard to get any traction, but the browser vendors are too busy
chasing phantoms like EV certs).
There's actually no requirement that you use certs at all.  In fact if
everyone dropped them (i.e. stopped pretending that they work and moved
towards something that does) we might all be a whole lot better off.

@_date: 2008-07-02 00:01:51
@_author: Peter Gutmann 
@_subject: The wisdom of the ill informed 
It's a pity that Kjell Hole et al didn't know this was impossible when they
mounted exactly this attack against the Norwegian banking system :-).

@_date: 2008-07-02 12:05:42
@_author: Peter Gutmann 
@_subject: Strength in Complexity? 
It's true to some extent.  For most crypto protocols, usability is job right after "did we get the punctuation right in the footnotes for the third

@_date: 2008-07-02 12:08:18
@_author: Peter Gutmann 
@_subject: Strength in Complexity? 
IPsec.  Anything to do with PKI.  XMLdsig.  Gimme a few minutes and I can
provide a list as long as your arm.  Protocol designers *love* complexity.
The more complex and awkward they can make a protocol, the better it has to

@_date: 2008-07-02 12:42:40
@_author: Peter Gutmann 
@_subject: Strength in Complexity? 
What, me, sarcastic?  Never!
Sure, any rational designer, working by themselves, will (hopefully) create a
simple, easy-to-analyse protocol.  The problem seems to occur once you get
committees involved (although I've seen some one-person-designed protocols
that can match the output of any standards committee :-).  So there's a
difference between what should happen in an ideal world and what happens in
practice.  People will quite easily build monstrous ziggurats one mud-brick at
a time, as any number of security protocols aptly demonstrate.  They're not
built because someone thinks they'll be more secure that way, but because the
delegate from IBM suggested that we need this, and the delegate from MS
insisted on having that, and the delegate from Verisign required the other.
(Actually even that doesn't really explain something like IKE... :-).

@_date: 2008-07-03 10:45:26
@_author: Peter Gutmann 
@_subject: Strength in Complexity? 
The reason why I was using IKE as an example is that it's a lot better-known than PKI.  That is, most people on the list know in general terms that PKI is a mess, but probably only a few who have had to read and implement some of the RFCs ( know just how incredibly *bad* it really is - it's a perpetual motion machine [0] of incomprehensible, contradictory, unimplementable, and often entirely nonsensical requirements [1] for which, once you get beyond the simplest mechanisms, the behaviour of any one implementation is more or less arbitrary as authors have to take guesses at what the authors of the spec (and the 15 other interfering specs in the same field) might have been thinking when they wrote it.  And unlike the IKE bakeoffs there's no interop testing, so there's no way to tell whether any two implementations will ever treat the same (nontrivial) cert in the same manner.  Unless you've had to implement PKI stuff it's difficult to convey the true horror of trying to make sense of those specs, which is why I've been using IKE as a better-known example.  If you're an IKE fan then mentally replace all occurrences with "PKI" :-).
[0] Don't like some way of doing things?  Wait six months, three more RFCs
    will be along to provide different (generally incompatible)
    interpretations.
[1] Show of hands, how many people here not directly involved with X.509 work
    knew that the spec required that all extensions in CA root certificates
    ("trust anchors" in recent X.509 jargon) be ignored by an implementation?
    So if you put in name constraints, key usage constraints, a policy
    identifier, etc, then a conforming implementation is supposed to look at
    them, throw them away, and proceed as if they weren't there?  More     amusingly, if you mark a non-CA cert as trusted then the requirement to
    ignore the extensions means that it can act as a trusted CA root     certificate (with the same rights as, say, Verisign's root certs)
    since the "not-a-CA" flags has to be ignored.

@_date: 2008-07-03 11:03:56
@_author: Peter Gutmann 
@_subject: Strength in Complexity? 
I think that's phrasing it a bit badly, it'd be better put as "without
usability, you won't have users" (see the Tor paper "Challenges in deploying
low-latency anonymity" for more thoughts on this).  This is why Skype is the
dominant internet phone protocol and every attempt at building encrypted VoIP
phones in the past ten years or so (anyone else here remember brew-a-stu?) has
languished among a small set of crypto geeks.  As Ian Grigg put it on his blog
a few years ago, "It took longer to do the setting up of some security options
[in other software] than it takes to download, install, and initiate an
encrypted VoIP call over Skype with someone who has never used Skype before".

@_date: 2008-07-06 00:37:49
@_author: Peter Gutmann 
@_subject: ITU-T recommendations for X.509v3 certificates 
That's the best way to use them.  For one thing it doesn't create any mistaken impression that setting a particular extension will have any useful effect when the software at the other end sees it :-).

@_date: 2008-07-06 00:48:36
@_author: Peter Gutmann 
@_subject: Strength in Complexity? 
Yup.  The app is supposed to read the cert, parse and process the extensions,
and then throw them away.  The text from the spec is:
  3.3.60 trust anchor: A trust anchor is a set of the following information in
  addition to the public key: algorithm identifier, public key parameters (if
  applicable), distinguished name of the holder of the associated private key
  (i.e., the subject CA) and optionally a validity period. The trust anchor
  may be provided in the form of a self-signed certificate. A trust anchor is
  trusted by a certificate using system and used for validating certificates
  in certification paths.
Rendered into English, that says "take the pubic key, owner name, and validity period, and ignore everything else in the cert".
To pre-empt the inevitable "yes, but it doesn't explicitly say you can't process the extensions if you want to": It also doesn't say you can't reformat the user's hard drive when you see a cert, but the intent is that you don't do anything not explicitly listed in the text above.  One of the known problems with this is that any cert that's marked as trusted now becomes a root CA cert because of the requirement to ignore the fact that it isn't a root CA cert.
Luckily no sane implementation pays any attention to this nonsense :-).

@_date: 2008-07-07 20:26:09
@_author: Peter Gutmann 
@_subject: Strength in Complexity? 
Conceptually yes, in the same way that the Soviet constitition was conceptually quite liberal and protective of individual rights.
In practice, no.  Look at your browser, email app, ... to see how it's reaally I think people might be getting a bit tired of this discussion of PKI quirks so how about the following: you go to the X.509 standards folks and convince them that your interpretation of the spec as given above is the correct one.  Once that's done, we can continue.

@_date: 2008-07-07 21:07:15
@_author: Peter Gutmann 
@_subject: Strength in Complexity? 
Paul Hoffman  end:
I suspect we're in violent agreement over this, just from two different persepectives.  From a security threat-modelling view I have to look at what the worst is that can happen if I deploy a cert (or whatever else it is I'm deplying).  Since the spec says that an implementation is free to ignore every single extension in a trust anchor/root cert then I have to assume that no extension I put in a root cert will ever be enforced (it *might* be, but it's not safe to rely on it).  From an optimist's point of view the spec is guaranteeing that extensions will be enforced.  From a paranoid security person's point of view the spec is guaranteeing that no extensions will be enforced.  I'm in the latter camp.

@_date: 2008-07-09 21:35:51
@_author: Peter Gutmann 
@_subject: disks with hardware FDE 
One thing about drive-based encryption is that we're been proised this since about 2000 or 2001, and it's always just another year or two away for various reasons: standardisation, host controller support, OS support, phase of the moon, ... .  The current reason seems to be FIPS 140: the turnaround time for a FIPS 140 eval is significantly longer than the mean lifetime of any particular hardware/firmware config, and the cost of the constant re-evals doesn't help much either.  So drive-based FDE is currently awaiting the loading of a compliment of small FIPS 140-soaked paper napkins.  Until then there will be a short delay.  Please return to your seats.

@_date: 2008-07-27 09:44:13
@_author: Peter Gutmann 
@_subject: cleartext SSH, Truecrypt, etc passwords in memory 
What the abstract doesn't make at all clear is that the process used
seems to have been (from section 2 of the paper):
Start application;
Enter password;
Take snapshot of running application's memory;
(although some passwords were apparently found in non-application-specific
memory, see section 3.7 of the paper).
In other words what's apparently being demonstrated for most of the apps
isn't an ability to recover keys still hanging around in memory at some
arbitrary later point but to recover keys from the active process memory
image.  The reason why I keep using "apparently" is that paragraphs 2 and
3 of section 2 don't make at all clear whether the application is still
active or not, although "after all programs had been launched process
memory was captured live" seems to imply it was a snapshot of a running
process.  Since many crypto applications zeroise keys after they've
been used, it seems a bit surprising that it'd be possibly to recover key
data after the app has exited, as the paper implies.
So was this a case of "recover data from an active app's memory image"
(not surprising) or "recover data after the app has exited" (surprising,
at least for the crypto apps)?

@_date: 2008-07-28 06:59:57
@_author: Peter Gutmann 
@_subject: cleartext SSH, Truecrypt, etc passwords in memory 
I think it'd be good to distinguish between cases where keeping
cryptovariables around is a bug and where it's by design.  For example SSL
caches the shared secret information for later use in session resumption
so finding a copy of that in memory while an SSL client or server is
running isn't a bug.  Finding it after it's exited is.  Even then though,
some apps include daemons that cache credentials and whatnot for ongoing
use by the app (e.g. the assorted 'xyz-agent' helpers for things like
various SSH clients or GPG) so finding the information in memory when the
app has exited but the cacheing daemon hasn't isn't necessarily a bug.
That'd be the interesting one, because keys left lying around in memory
afterwards is definitely a sign of a problem (but be careful about the
cacheing issue mentioned above).

@_date: 2008-06-04 18:50:50
@_author: Peter Gutmann 
@_subject: Can we copy trust? 
[Moderator's note: I'm letting just this one through, because I think
Peter's point bears repeating. --Perry]
They don't copy any trust at all, they copy a (usually expensive)
cryptographic cookie that turns off browser warning messages.
(They do copy the cookies with high fidelity though, if one bit is corrupted
then the cookie becomes invalid).

@_date: 2008-06-05 19:20:26
@_author: Peter Gutmann 
@_subject: the joy of "enhanced" certs 
There's another data source that's examined the effect of EV certs and browser
blacklists on a much larger scale, namely the APWG statistics.  They show an
essentially flat distribution for phishing from January 2007 to January 2008,
the period of phase-in of EV certs and the browser anti-phishing filters.  In
other words the attack stats show that the effect of EV certs was exactly as
(Hat tip to an APWG member who made this point during a conference talk
So you could have EV certs, EEV certs, EEEV certs, EEEEV certs, a PKI
equivalent of the 'aptitude -v[v[v[v[v[v...]]]]] moo' trick.  Every couple of
years when people realise that the current level of (E^n)V certs work no
better than the (E^n-1)V certs that preceded them did, you add another 'E' and
everyone gets to pay again for a new set of certs.  The only potential problem
is that all the CAs would have to agree to add more E's in lock-step,
otherwise you'd get a tragedy-of-the-commons effect where the CA who adds the
most E's the quickest wins.

@_date: 2008-06-10 16:58:51
@_author: Peter Gutmann 
@_subject: Ransomware 
It's not speculation, encryption virii have been around for at least ten
years, although the encryption used was pretty crude and easily broken.  Even
this particular variant (public-key encryption) is hardly new, if it's a
PGPCoder derivative then it'd be at least two years old.

@_date: 2008-06-14 15:21:07
@_author: Peter Gutmann 
@_subject: Why doesn't Sun release the crypto module of the OpenSPARC? 
That's a thought I'd had as well, using the Dank Defence ("these nasty people
are holding a gun to our head and forcing us to do this even though we don't
want to") is the easy way out when your real reason is "we don't want to
release this because it's giving too much away to potential competitors".

@_date: 2008-03-03 23:43:35
@_author: Peter Gutmann 
@_subject: Politics 1, security 0 
Microsoft recently published the specs for a pile of previously undocumented
or semi-documented protocols and data formats.  One of them covers the
atrociously-named Health Certificates, which have nothing to do with
healthcare but are used to indicate compliance of systems with security
policies.  A system obtains a health certificate from a server which
encapsulates its compliance with the security policy, and other systems can
then verify the compliance by checking the cert rather than having to perform
the check of the system itself.  The object identifier associated with this
usage is 1 3 6 1 4 1 311 47 1 1.
This leads to a small problem: Since health certificates are now desirable,
everyone wants one.  The problem is that non-compliant systems can't get one
because the very purpose of a health cert is to prove compliance with a
security policy and non-compliant systems are, well, non-compliant.
To work around this, there's a second identifier 1 3 6 1 4 1 311 47 1 3 which
is used to provide health certs for systems that don't qualify for health
certs, so they can have their health certs too.
(I've encountered many examples of business and politics messing up security,
 but I think this one must qualify as some kind of poster boy).

@_date: 2008-03-04 23:25:21
@_author: Peter Gutmann 
@_subject: Safari falls afoul of the security fashionistas 
Various browsers (e.g. Firefox and IE) recently implemented the latest fashion
in "security", EV certs (already discussed on this list in the past) and
blacklists, neither of which have much effect on phishing but both of which
make great security fashion statements.
Unfortunately, it looks like Safari, which doesn't follow the trend, has
fallen victim to the fashionistas:
      PayPal warns: Steer clear of Apple's Safari browser
   If you're using Apple's Safari browser, PayPal has some advice for you:
   Drop it, at least if you want to avoid online fraud.
   [...]
   Unlike its competitors, Safari has no built-in phishing filter to warn
   users when they are visiting suspicious Web sites, Barrett said. Another
   problem is Safari's lack of support for another anti-phishing technology,
   called Extended Validation (EV) certificates. This is a secure Web browsing
   technology that turns the address bar green when the browser is visiting a
   legitimate Web site.
Looks like Safari just isn't the look to be sporting this year... maybe Apple
could invent their own security fashion and then criticise Paypal for being so
five minutes ago in its look.

@_date: 2008-03-16 23:26:07
@_author: Peter Gutmann 
@_subject: delegating SSL certificates 
The desire to do it isn't uncommon, but it runs into problems with PKI
religious dogma that only a CA can ever issue a certificate.  For example I
proposed this on the PKIX working group nearly a decade ago, specifically the
ability for end entities with signing certs to issue their own encryption
certs, since there's absolutely no need to involve a CA in this.  I've still
got the draft online at
  The WG chair's
response was "we don't want to turn X.509 into PGP", and that was the end of
it.  The grid computing folks eventually got something through in the form of
proxy certificates for the Globus GSI, but that probably isn't what you're
looking for.

@_date: 2008-04-01 00:47:45
@_author: Peter Gutmann 
@_subject: how to read information from RFID equipped credit cards 
Actually there are already companies doing something like this, but they've
run into a problem that no-one has ever considered so far: The GTCYM needs a
(relatively) high-bandwidth connection to a remote server, and there's no easy
way to do this.
(Hint: You can't use anything involving USB because many corporates lock down
USB ports to prevent data leaking onto other corporates' networks, or
conversely to prevent other corporates' data leaking onto their networks. Same
for Ethernet, Firewire, ...).

@_date: 2008-04-01 15:30:42
@_author: Peter Gutmann 
@_subject: how to read information from RFID equipped credit cards 
============================== START ==============================
That's what one company ended up using, not specifically 2D barcodes but a
visual channel via the PC monitor (actually nothing like 2D barcodes in this
particular case :-).  The problem is, as you point out, that the channel is
unidirectional and not very high-bandwidth.  This makes the crypto very hard
because you have to roll your own protocols and mechanisms and everything ends
up custom and nonstandard.
Here's an interesting crypto design problem, how do you do something like
S/MIME or PGP (to submit or receive an authenticated request/purchase order/
whatever) with a relatively low-bandwidth channel in one direction and almost
no channel (perhaps humans typing in a 4-digit number) in the other, and
without requiring huge amounts of oddball custom crypto mechanisms.

@_date: 2008-05-03 17:06:30
@_author: Peter Gutmann 
@_subject: User interface, security, and "simplicity" 
I think you mean:
]... if you're willing to avoid a huge pile of pointless PKI cruft.
which is a major feature of OpenVPN.
They're "easier to configure and use" because most users don't want to have to
rebuild their entire world around PKI just to set up a tunnel from A to B.

@_date: 2008-05-06 19:06:15
@_author: Peter Gutmann 
@_subject: Comments on SP800-108 
Somewhat more strongly worded than my comments :-), but I had the same
feeling: Why yet another bunch of arbitrary PRF/KDFs to implement?  We now
have ones for SSL, for TLS, for SSH, for IKE, for PGP, for S/MIME, for... well
I don't know every crypto protocol in existence but I'm sure there's plenty
more.  What's wrong with PBKDF2, which seems to do the job quite nicely?
Whoever dies with the most KDFs wins?
There just doesn't seem to be any reason for this document to exist except
NIH.  PBKDF2 is a well-specified KDF, is relatively easy to implement (and
implement in an interoperable manner), has been around for years, and has
numerous interoperable implementations, including OSS ones if you don't want
to implement it yourself.  What's the point of SP800-108?  What
requirement/demand is this meeting?

@_date: 2008-05-07 17:01:35
@_author: Peter Gutmann 
@_subject: User interface, security, and "simplicity" 
Precisely.  An example of where dynamic strings can lead you is what happens
to old (very old) versions of Netscape when you feed them a cert with, say, an
MPEG of a cat in the X.500 DN.  Netscape happily accepts the cert but you then
have to reinstall the browser because while it'll quite readily accept
ridiculously long values it doesn't actually cope with them very well.  A
security component that's trivially taken out by a DoS isn't a security
component, it's a vulnerability.

@_date: 2008-05-14 17:29:28
@_author: Peter Gutmann 
@_subject: [ROS] The perils of security tools 
Debian seem to be particularly bad for not reporting changes to maintainers,
although other distros do it as well.  I've got a few packages that are
contained in a number of distros and I notice via occasional Google searches
for semi-related items that I'm getting hits to CVS change logs for my code
where someone is repeatedly re-applying some patch to every new version I
release.  All they'd have to do is send me email to say they've made the
change and I can apply it to the master copy, but instead they re-patch every
new release.  In addition because I have no idea where it's ending up, I can't
even send out a notification to say there's a new version out.  It's a very
strange way to "maintain" code.

@_date: 2008-05-14 17:41:06
@_author: Peter Gutmann 
@_subject: [ROS] The perils of security tools 
So just to clarify, does the Debian patch only remove the ability to add
uninitialised memory (which will be all-zeroes anyway on an OS with proper
resource controls) or does it remove the ability to add any entropy at all?
The advisory makes it sound like it's the latter.

@_date: 2008-11-03 03:11:10
@_author: Peter Gutmann 
@_subject: Who cares about side-channel attacks? 
One of the XBox attacks, allowing rollback to a vulnerable kernel, was a
timing attack.  I'd heard it was also tried in some form (unsuccessfully)
against the Wii as part of the breadth-first attack approach.
You can see this with the games-console hacking, the attackers try and release
the least amount of information possible so they've got something in reserve
when the countermeasures appear.  In some cases they use attack method A to
find a weakness and then exploit it using unrelated method B, allowing reuse
of method A once B is patched by the vendor.
Sorry, I was referring to two different attacks in the same sentence, and on
re-reading managed to make the result quite unclear :-).  The timing attack
didn't directly recover the authentication key directly but avoided the need
to know it, thus allowing unauthorised vulnerable kernels to be loaded.
Often the simplest tricks are the most effective, e.g. stick a PGP header on
the data to be protected and the attackers spend forever trying to decrypt it
when in fact the processing function is (in pseudocode):
  seek( file, 16 );    // Skip red-herring junk at start
  processData( file );
(the problem with this one was that they memcpy()'d the fixed header on and
the lengths were wrong, but apart from that it would probably have distracted
attackers for some time).

@_date: 2008-11-08 21:38:05
@_author: Peter Gutmann 
@_subject: This is a test. This is only a test... 
In my previous alert, I included the text of a phishing email as an example
  [of phishing emails that people shouldn't reply to]. Some students
  misunderstood that I was asking for user name and password, and replied with
  that information. Please be aware that you shouldn.t provide this
  information to anyone.
Rest at

@_date: 2008-11-25 17:54:11
@_author: Peter Gutmann 
@_subject: Certificates turn 30, X.509 turns 20, no-one notices 
This doesn't seem to have garnered much attention, but this year marks two
milestones in PKI: Loren Kohnfelder's thesis was published 30 years ago, and
X.509v1 was published 20 years ago.
As a sign of PKI's successful penetration of the marketplace, the premier get-
together for PKI folks, the IDtrust Symposium (formerly the PKI Workshop and
now in its eighth year) authenticates participants with... username and
password, for lack of a working PKI.
(OK, it's a bit of a cheap shot and it's been done before, but I thought it
was especially significant this year :-).

@_date: 2008-10-06 17:51:50
@_author: Peter Gutmann 
@_subject: Who cares about side-channel attacks? 
For the past several years I've been making a point of asking users of crypto on embedded systems (which would be particularly good targets for side-channel attacks, particularly ones that provide content-protection capabilities) whether they'd consider enabling side-channel attack (SCA - no, not that SCA) protection in their use of crypto.  So far I've never found anyone who's made an informed decision to trade off performance for SCA protection.  By "informed decision" I mean the following:
- SCA protection isn't enabled by default, i.e. they don't just get it whether   they want it or not.
- The SCA protection is more than just a token throw-some-blinding-at-the-RSA,   it extends to things like pubic/private key validation on load (for example   via MACs) to detect key-manipulation attacks, checksumming of key data after   each crypto op to detect memory-disturb attacks, and so on.
- There is a tangible cost/tradeoff in enabling SCA protection, i.e. it's not   just chicken-soup protection, "turn it on, it's a 2GHz multicore CPU it   can't hurt".
In other words the user has to make a conscious decision that SCA protection is important enough that performance/power consumption can be sacrificed for it.  Can anyone provide any data on users making this tradeoff?  And since negative results are also results, a response of "I've never found anyone who cares either" is also useful.  Since the information may be commercially sensitive, respond in private email if you'd rather not discuss it in public and I'll summarise if there's any interest.
(Pre-emptive response to the inevitable "OpenSSL/NSS/xyz smart card/... have had RSA blinding enabled by default since ...": Yes, I know, and now go back and re-read points 1 and 2 above).

@_date: 2008-10-16 19:46:42
@_author: Peter Gutmann 
@_subject: Snatching defeat from the jaws of victory 
The DailyWTF has an entertainnig writeup on how not to use strong crypto to protect an embedded device, in this case a Wii, at   The signature-verification function was particularly entertaining:
  decrypt_rsa(signature, public_key, decrypted_signature);
  if(strncmp(content_sha1, decrypted_signature + 236, 20) == 0)
  [...]
(And before you burst out laughing, Apple did something only slightly less bad
in the iPhone).

@_date: 2008-10-25 15:25:25
@_author: Peter Gutmann 
@_subject: Who cares about side-channel attacks? 
It could if there was a large enough repondent base to draw samples from :-). This is one of those surveys that can never be done because no vendor will publicly talk to you about security measures in their embedded systems.
In fact none of the people/organisations I queried about this fitted into any of the proposed categories, it was all embedded devices, typically SCADA systems, home automation, consumer electronics, that sort of thing, so it was really a single category which was "Embedded systems".  Given the string of attacks on crypto in embedded devices (XBox, iPhone, iOpener, Wii, some not-yet-published ones on HDCP devices :-), etc) this is by far the most at-risk category because there's a huge incentive to attack them, the result affects tens/hundreds of millions of devices, and the attacks are immediately and widely actively exploited (modchips/device unlocking/etc, an important difference between this and academic proof-of-concept attacks), so this is the one where I'd expect the vendors to care most.
Actually that's a special case, or more generally having certification/ auditing requirements (which a private-email responder also mentioned) is a special case in that the risk analysis is now "if I don't do this I don't get sign-off" rather than "it makes good security sense to do this so we'll do it".  In the immortal words of the Bastard Operator from Hell, when you have the audit/certification gun pointed at someone's head you can pretty much "[get them to] to run naked across campus with a power-cord rammed up [their] backside" and they'd do it not because they thought it was a terribly good idea but because they had a gun pointed at their head.
An associated problem with this is that if vendors are motivated solely by checkbox requirements then they'll often ship the product in a non-approved mode (coughFIPS140cough) to reduce manufacturing or support costs/increase performance/increase ease of use/whatever.  It's a nasty catch-22, hold a gun to someone's head and they'll only do what you tell them for as long as the gun is applied.
Getting back to the SCADA/home automation/consumer electronics embedded market, the only certification that applies is the likes of FCC Class B, ROHS, CE, and UL.  This is why I was interested in finding cases (or counterexamples) of informed-consent use of SCA countermeasures, because in the general embedded-systems case vendor cost/benefit analysis is the only deciding factor on whether it gets used or not, and vendors seem to be deciding (from my own experience and some private-email replies) that it's not worth it.

@_date: 2008-10-30 16:32:23
@_author: Peter Gutmann 
@_subject: Who cares about side-channel attacks? 
The published ones seem to be the (relatively) easy ones, but the ones that
have been tried (and either not published or just had the easy outcome
published) have been pretty amazing.  This is another one of these things
where real figures are going to be near-impossible to come by, even harder
than my hypothetical public vendor survey of who uses SCA protection.  You'd
have to read about 20 blogs and a hundred mailing lists, many private, just to
keep up, but from various informal contacts with people working in this area
it seems you're not looking at anything like the conventional "identify the
weakest point, then attack there" approach.  Instead what's being done is more
like the Iraqi weapons program prior to 1991 where they were using every
imaginable type of approach, including ones like calutrons that had been
abandoned decades earlier by everyone else, for a first-past-the-post finish,
they'd try anything and everything and whatever got them there first would be
declared the winner.  It's the same with these attacks, whenever I've asked
"have you tried X" the answer is invariably "yes, we have".
This style of attack is quite different from the usual academic model, it
neatly illustrates Bruce Schneier's comment that a defender has to defend
every single point along the line while an attacker only has to find a single
weakness.  In this case it seems to be literally true, and the weakness won't
necessarily be the actual weakest point but merely the point where an attacker
with sufficient skill and access to the right tools got in.  Look at the XBox
attacks for example, there's everything from security 101 lack of
checking/validation and 1980s MSDOS-era A20# issues through to Bunnie Huang's
FPGA-based homebrew logic analyser and use of timing attacks to recover device
keys (oh, and there's an example of a real-world side-channel attack for you),
there's no rhyme or reason to them, it's just "hammer away at everything with
anything you've got and exploit the first bit that fails".

@_date: 2008-10-31 21:25:37
@_author: Peter Gutmann 
@_subject: the skein hash function 
I don't know about ASICs but for FPGAs you can pay in the thousands of dollars
for a single high-end device (forget Xeons, that's the market to be in), so
you don't want to set your sights too high.  My guess is they were designing
down to a price rather than up to a performance figure.

@_date: 2008-09-07 19:59:12
@_author: Peter Gutmann 
@_subject: Quiet in the list... 
The UI still leaves quite a lot to be desired.  Try sitting a non-geek user in
front of a fresh Skype install and see how long it takes them to figure out
how to make a phonecall to (say) a Skype user name supplied via email.  I've
seen times of 15+ minutes to make the first call (OK, so I treat neighbours
and family as UI guinea pigs :-).  Skype still has a lot of fundamental
usability flaws like the inability to remember a password (requiring it to be
manually re-entered each time it's run unless you choose to start Skype on
system boot) that make it a less-than-perfect example of usable security.
The scary thing though is that even with all its flaws, it's still more usable
than virtually all other crypto-using apps around.

@_date: 2008-09-08 01:29:34
@_author: Peter Gutmann 
@_subject: More US bank silliness 
In the ongoing comedy of errors that is US online banking "security" I've just
run into another one that's good for a giggle: Go to  and,
without entering any credentials, click 'Login' on their unsecured logon page.
You get taken to an authenticated, SSL-secured... error message page.  The
error message page gives you a chance to retry your logon, carefully
redirecting you back to the insecure logon page.  So displaying a glorified
401 requires SSL, but obtaining user credentials doesn't.
(Insert standard moan about US banks here).
On a semi-related topic, it'd be interesting to get some discussion about FF3 removing the FF2 SSL indicators of the padlock and (more visibly) the background colour-change for the URL bar when SSL is active and replacing it with a spoof-friendly indicator that's part of the favicon, i.e. part of the attacker-controlled content.  The URL bar colouring was by far the most visible security indicator that any web browser had, the giant leap backwards of moving to a near-invisible blue border around the favicon does nothing to indicate security and is trivially spoofed by putting a blue border around the favicon.  There's a bugzilla bug filed against it,  (with inevitable dups, e.g.  but there's no indication that the FF developers are interested in fixing it.  From the discussion thread on bugzilla it seems the reason is that only EV certs matter so there's no point in paying much attention to non-EV certs.
(Again, roll standard music about EV certs benefitting no-one but the CAs
selling them).

@_date: 2008-09-09 18:49:55
@_author: Peter Gutmann 
@_subject: More US bank silliness 
That's an artefact of the SSL MITM that Akamai performs for sites that are
hosted via their service (and up until Kevin Fu pointed out that this was a
problem, any random site you want as well).  You usually don't see this
though, so it's indicative of a configuration error somewhere...

@_date: 2008-09-09 23:24:49
@_author: Peter Gutmann 
@_subject: once more, with feeling. 
Unfortunately I think the only way it (and a pile of other things as well) may get stamped out is through a multi-pronged approach that includes legislation, and specifically properly thought-out requirements rather than big-business- bought legislation like UCITA/UCC or easily-circumvented recommendations like the FFIEC ones (the banks quickly discovered that by redefining "two-factor auth" to mean "twice as much one-factor auth" they could meet the requirements without having to do anything to improve security).
I'm saying that under the influence of "Zero Day Threat" by Byron Acohido and
Jon Swartz, which looks at some of the financial and credit-reporting industry
practices that make identity fraud possible.  If you haven't read this
already, go and get it now, apart from the annoyingly frequent context-
switching between threads (one every few pages instead of the more usual one
per chapter) it's a very scary read.  Given what it reveals about how the US
financial/credit reporting industry works it should really be subtitled "We're
all going to die", since there's no obvious handbrake mechanism present in the
system to slow down identity theft - the rate-limiting step is the fact that
the crooks simply can't use all the stolen identities they have, not any
security measures that may be present.  If you don't believe me, visit any of
the hits from the following search:
  (that's the easiest way to demo the problem to the masses without requiring
people to learn to read cyrillic first :-).
Yup, we're all going to die.
The "financial impact" point is the key word, at the moment it's cheaper and
easier for the banks/credit reporting companies to be non-compliant/insecure
than it is for them to be secure.  I'm not sure that the browser is the most
effective way to hit them over this though.
Discuss :-).

@_date: 2008-09-11 17:57:56
@_author: Peter Gutmann 
@_subject: once more, with feeling. 
You're think about this from the wrong angle.  We don't need to legislate
network security because, as you say, we'll never get a workable law, and even
if we did we really have no idea how to build secure systems that users would
actually want to use (although there are some good hypotheses out there).
What we need is real-world controls (that have nothing to do with computers)
to rein in the free hand that computerisation has given to attackers.  Credit
freezes are the first step, although even then it's been a massive battle and
most likely Congress will eventually pass a law that neutralises the various
state laws, as it has for numerous other laws in the past (and even some of
the state laws have been watered down with "thaw" provisions that take you
right back to square one).
Some examples that come to mind immediately for fighting phishing:
- Credit freezes that are real freezes, and require a physical bank visit with
ID to thaw.
- COB and credit-limit-increase freezes that require physical presence to
change (the first thing phishers do when they get your CC info is to wind the
credit limit up to max and change the billing address).  The once a blue moon
that you might want to change these details it's really not to hard to drop by
a bank for a minute or two to authorise things.
- Ability to specify floor limits for spending independent of the credit
limit, e.g. with a credit limit of $10K you can't spend more than $2K
domestically and $1K internationally.
I think that should give you a general idea of where this is going.  At the
moment the banks' fraud-guessing systems are really just that, guessing
systems, and from numerous reports and assorted anecdotal evidence they're not
very effective.  The user holds the "position of the interior", they know
better than any guessing system what's appropriate and what isn't for their
financial transactions.  The rampant exploitation of the banking system by
crooks works because all of the above are totally uncontrolled, and banks have
no interest in controlling them.  That's what we need legislation for, not to
require two-factor-authentication-that-isn't and other gimmicks but to get the
banks and credit-reporting agencies to install effective internal controls.

@_date: 2008-09-11 18:19:10
@_author: Peter Gutmann 
@_subject: street prices for digital goods? 
I've been (very informally) tracking it for awhile, and for generic data (non-
Platinum credit cards, PPal accounts, and so on) it's essentially too cheap to
meter, you often have to buy the stuff in blocks (10, 20, 50 at a time) to
make it worth the sellers while.  I haven't tracked the big-ticket items like
PPal accounts with guaranteed minimum balances (rather than just any generic
PPal account) because the offerings are too ephemeral, you might get "PPal
with minimum $5K balance" advertised for a few weeks, then "Platinum Visa" for
a few weeks, and then something else again.
I'm not aware of anyone having done this, mostly because the data doesn't seem
to be available.  The phishers don't sell (e.g.) BofA accounts specifically,
they sell whatever's available - you get a block of X accounts or cards from
various banks, whatever's at hand when you buy.  The only way to see whether a
measure was effective would be to keep buying blocks over time and see what
the mix of banks was, and even then it'd be pretty unscientific because you'd
be getting lots from random phishing sources or data thefts which might
(coincidentally) be targetting one particular bank and not another.  Given the
diverse sources for this stuff, it's likely that even the vendors only have a
vague idea of what the statistics are.

@_date: 2008-09-12 22:32:16
@_author: Peter Gutmann 
@_subject: street prices for digital goods? 
The difference is that you're paying for service with the higher-priced
vendors (and this is something new that's only really come in in the last
couple of years).  Cheap ones are just a dump of some looted merchant database
or whatever where you may or may not get the data after paying some fly-by-
night operator and when it arrives half the cards will be invalid.  The
premium-priced ones are established vendors charging for the level of service
they provide: You get a guaranteed-good card (typically with 48- or 72-hour
replacement guarantee), you can use escrow services to guarantee delivery of
goods, you may get a tech support hotline (assuming you speak Russian), and so
on (it varies from seller to seller, obviously).  But what you're paying for
isn't really the card but the level of service that comes with it.

@_date: 2008-09-18 17:18:00
@_author: Peter Gutmann 
@_subject: once more, with feeling. 
The mechanisms for this actually already exist, they're just not used.  First
of all, you need to admit that you have a problem: SSL certs by themselves are
more or less useless in providing assurance, the phishers are buying genuine
CA-issued certs for soundalike domains using stolen credit cards just as
easily as legit sites are buying certs for genuine domains, and no amount of
signature checking and CRLs and other PKI paraphernalia will fix that.  As
Matt Blaze once said, "A commercial CA will protect you from fraud by anyone
whose money it refuses to accept", which was meant tongue-in-cheek at the time
but given current practice by phishers that's exactly what a commercial CA
will do, and no more.
It's only once developers have accepted this that you can start looking for a
real solution:
- Use TLS-PSK, which performs mutual auth of client and server without ever
  communicating the password.  This vastly complicated phishing since the
  phisher has to prove advance knowledge of your credentials in order to
  obtain your credentials (there are a pile of nitpicks that people will come
  up with for this, I can send you a link to a longer writeup that addresses
  them if you insist, I just don't want to type in pages of stuff here).
- Implement key-continuity at the client, i.e. warn if submitting a password
  or credit card number to a site you've never visited before; display the
  password in cleartext if submitting over a non-SSL connection (this is
  better than any explicit warning); (insert standard list, I have a whole
  pile of these, including responses to objections).
- Use a service like Perspectives,
   to catch here-today-gone-
  tomorrow phishing sites (this is actually a service that someone like Google
  could provide, they crawl the web anyway so keeping track of how long a
  server key's been in use should be a relatively minor change to the existing
  process.  Anyone from Google security reading this list?).
- ... and many more, I won't list them all here.
So it's a two-step process, the first of which is to admit that you have a
problem.  As EV certs have shown, we haven't really even reached that first
step yet.

@_date: 2008-09-19 19:53:33
@_author: Peter Gutmann 
@_subject: =?iso-8859-1?Q?First_sighting_of_real-life_AFM_data_retrieval=3F__=5B=A1P?= =?iso-8859-1?Q?ING!_Peter_G...=5D?= 
Whatever it is it seems to have been garbled beyond recognition, if they were
just looking for straight evidence they'd be using Encase, if they were
looking for data in erase bands (assuming the drive was old enough to still
feature them, they'd be hard to find in any modern drive) they'd be using
something like a magnetic force microscope which is a scanning probe
microscope, or a magnetic force scanning tunneling microscope, and in either
case I doubt you could read perpendicularly-recorded EPRML data just by
staring at it.  So I don't think it's possible to draw any conclusions from
this data.

@_date: 2008-09-19 20:29:11
@_author: Peter Gutmann 
@_subject: once more, with feeling. 
I don't know if anyone tracks the exact count (apart from the 2005 figure of
(at least) 450 recorded incidents of secure phishing) but every now and then
you get reports of particular ones that stand out, e.g. the Sunbelt discussion
of the Gromozon distributors signing their malware,
 (the cert
purchase was eventually traced back to a victim in Florida)... sorry, I don't
catalogue them all, just watch them drift by every now and then and that one
came to mind.  In fact didn't our esteemed moderator mention running into one
of these via spammed email just a few months back?
In any case with the assistance of services like  for the
ID and any one of the incorporation brokers operating in the US (I
particularly like the appropriately-named
 you can do it for far less via US
brokers but then if you're paying with soemone else's credit card cost isn't
really an issue) how long do you think it'd take to create an official US
corporation that qualifies for an EV cert?  I think the main reason we haven't
seen more of this is:
- There's no need for it, usability evaluation tests have shown that EV certs
  have no effect on security so there's no point in going to the trouble
  (cybercrooks are cheapskates).
- It's much easier to use an exploit toolkit like MPACK or whatever to hit an
  EV-certified site for a genuine organisation than it is to bother with your
  own EV cert.  Since an EV cert only says that the CA did a little bit more
  than almost no checking at all of credentials but doesn't tell you anything
  about the site, the easiest way to get an EV cert is to use someone else's
  (AV vendors report peaks of up to 10,000 fresh malware infection sites via
  legit compromised servers a day, which is bad news for the "only enter your
  credit card details on sites you trust" education effort).

@_date: 2008-09-23 15:03:54
@_author: Peter Gutmann 
@_subject: The You are Now in France attack, still with us after all these years 
I was browsing through the Windows download centre for reasons not relevant
here and came across KB955417, dated 22 August 2008:
  Install this update to resolve an issue in which protected storage (PStore)
  uses a lower quality cryptographic function when the system locale is set to
  French (France).

@_date: 2008-09-23 15:24:29
@_author: Peter Gutmann 
@_subject: once more, with feeling. 
Yeah, I knew someone would say that which is why I included the note at the
end saying that there was a (much) longer discussion of the issues available.
The problem is that the default has always been to be insecure, and there's no
effective way to get people to move to the secure non-default, or at least
none that isn't relatively easily circumvented by a bit of creative thinking
and/or social engineering.  The problem is really a multi-part one, some parts
of which are easily solveable, others which aren't.
The easy part: New developments.  SSL seems to be pretty much the de facto
substrate for securing network applications (even more so now that we have
DTLS), there's no need to repeat the same mistakes over and over again when
rolling out new SSL-enabled apps - build in TLS-PSK (or TLS-SRP, or whatever),
make it the default, and mark anything else as insecure with big red flags. To
quote Ian Grigg, "there is only one mode and it's secure".  Heck, even getting
away from the current "Connect to remote system, hand over username+password"
would be a massive step forward into the 1980s.
That leaves the harder part, existing apps:
For existing apps with habituated users, so am I.  So how about the following
strawman: Take an existing browser (say Firefox), brand it as some special-
case secure online banking browser, and use the "new developments" solution
above, i.e. it only talks mutual-auth challenge-response crypto and nothing
else.  At that point you've reduced "Reformat user and reinstall browsing
habits" to "Train users to only use safe-browser when they do their banking,
i.e. 'Never enter banking details using anything other than safe-browser'".
Even if you only get a subset of users doing this, it's still a massive attack
surface reduction because you've raised the bar from any idiot who buys a
phishing kit to having to perform a man-in-the-browser attack.

@_date: 2008-09-23 17:56:23
@_author: Peter Gutmann 
@_subject: EV certs: Doing more of what we already know doesn't work 
Inspired by Ian Grigg's comment (in the subject line) and various remarks made
in a recent thread, I had a look at the Verisign 1.0 CPS from 1996 and the
very latest Verisign CPS from June 2008, twelve years later.  Here's the
authentication requirements for businesses.  One is from the 1.0 CPS, which
was going to Solve the Problem twelve years ago.  The other is from the 2008
EV-cert CPS, which is going to Solve the Problem today.  Without going out to
check, see if you can tell which is which.  I've harmonised the style and
wording a bit since it's otherwise possible to make a pretty good guess based
on the form of the text, the current CPS has way more legalese:
  Where required, the third party confirms the business entity's name,
  address, and other registration information through comparison with third-
  party databases and through inquiry to the appropriate government entities.
  Confirmation of information of companies, banks, and their agents requires
  certain customized (and possibly localized) procedures focusing on specific
  business-related criteria (such as proper business registration). The third
  party also provides telephone numbers that are used for out-of-band
  communications with the business entity to confirm certain information (for
  example, to confirm an agent's position within the business entity or to
  confirm that the particular individual listed in the application is in fact
  the applicant). If its databases do not contain all the information
  required, the third party may undertake an investigation, if requested by
  the IA, or the certificate applicant may be required to provide additional
  information and proof.
  The third party must be a legally recognized entity whose formation included
  the filing of certain forms with the Registration Agency in its
  Jurisdiction, the issuance or approval by a Registration Agency of a
  charter, certificate, or license, and whose existence can be verified with
  that Registration Agency, and must have a verifiable physical existence and
  business presence.  If the third party represents itself under an assumed
  name, VeriSign verifies the third party's use of the assumed name.
The only difference I can see is that one CPS exhaustively itemises the
various bits and pieces that are checked whereas the other gives it in general
terms, I've left out the list itself because it'd make this a five-page
posting with not a lot more content.  If you want to see the verification
requirements for yourself, they're in section 5.1.3 (old CPS) and Appendix B1
5(c) with an almost-identical copy at 14(C) with more detail in sections that
follow (new CPS).

@_date: 2008-09-23 18:25:06
@_author: Peter Gutmann 
@_subject: once more, with feeling. 
It's actually not that bad, we have some really good password managers that
can take care of this for us, alongside quite a bit of research from HCI
people that examine their effectiveness.  By "password manager" I mean one
that generates a strong password for you and supplies it as required, not the
noddy "managers" built into things like web browsers, look at something like
Roboform for an example of what I mean.  The problem is that I don't know of
any application that natively uses them, there are between half a dozen and a
dozen Firefox plugins and third-party apps (it varies over time) that all
provide enhanced password-handling capabilities but the browser itself still
has the incredibly clunky 1.0 "manager" that it's always had (not specifically
picking on FF here, all the others are just as bad, the difference is that FF
has a pile of functioning plugins and usability research that demonstrate how
to get it right).
The problem isn't with passwords, it's with developers: Passwords are insecure
because developers have chosen to make them insecure.  We have mechanisms to
address a lot of the problems with passwords but no-one ever uses them.  Even
suggesting some of these things is hard ehough, the response to "What about
using security measure X which addresses problem Y" isn't to use measure X but
to find some obscure corner case where X won't work, declare the problem
unsolveable, and keep on doing the same thing that already didn't work the
last 100 times we tried it.

@_date: 2008-09-24 18:03:07
@_author: Peter Gutmann 
@_subject: once more, with feeling. 
Combining several replies into one...
In my experience that's the brontosaurus in the room.  There are vendors out
there that'll do cellphone auth (basic SMS-based out-of-band transaction
authorisation), the technology's in place, the problem is that once everyone
has taken their cut it's no longer economical.  To some extent the technology
still sucks quite a bit (e.g. RSA's SMS-based system takes the bank-side
information "Request authorisation for transfer of $10,000 from your bank
account to the bank account of J.Random Retailer" and turns it into "Enter the
following PIN to unlock all further debits from your account until it's
empty"), but we're getting there.
The killer is the cost involved.  Access to the mobile networks is expensive
enough that I've seen solutions in some countries like buying SMS capacity in
bulk from foreign providers (it's cheaper to send the texts from a provider on
the other side of the world than to do it locally) to the extreme step of
setting up (or perhaps buying up) your own cellular network.
Can you describe the WoW interface?  It sounds like they've taken advantage of
the greenfields approach and built something different that's secure from the
start, but I'm not familiar with how it works.
It's already been done, in situation-specific ways:
Marcus Ranum's Six Dumbest Ideas in Computer Security,
Microsoft/Scott Culp's Ten Immutable Laws of Security,
My own Ten Inescapable Truths of Security UI,
 (last three slides)
... if you can stand the clickfest that's required to get there with FF3

@_date: 2009-04-10 22:26:10
@_author: Peter Gutmann 
@_subject: Is PGP X.509's secret weapon? 
I was just reading through the WiMAX PKI documentation [0]... this uses PGP to issue device and server X.509 certificates for use in WiMAX networks:
  "Name" is an identifying name for the recipient that will be used as an
  authenticated identity by the CA signing system.  This is the identifier by
  which the CA system identifies which PGP key is used to encrypt the
  deliverable certificates and keys.
  The WIMAX Forum will notify Authorized Users of the completion of their
  order via email. Certificates will be delivered as an encrypted PGP archive.
  The private key of the Authorized User's PGP key is required to decrypt it.
There's nothing technically wrong with this, it just kinda says it all for X.509 that you need to use PGP to get it going.  I can just see PGP Corp's new marketing slogan: "PGP: Making X.509 possible".
(Jon, if you guys print shirts with something like this on it, I want one :-).
[0] "WiMAX Certificate Authority Users Overview", WiMAX Forum, 2008(?).

@_date: 2009-04-21 21:56:56
@_author: Peter Gutmann 
@_subject: Brazilians hijack US military satellites 
The whole story's at:
it appears that Brazilians wanting to communicate on the cheap are using US
FLTSATCOM links to talk to each other.  This works because "the communication
channel was open, not encrypted, lots of people used it to talk to each other"
(later they talk about hearing encrypted military voice comms, so I assume
they mean that there's no access control, not necessarily encryption).
Truckers use it because you get better quality voice than with CB. Drug
dealers use it to coordinate operations, rogue loggers use it to warn of raids
by the authorities.  As the story says "Until [they get upgraded to use
crypto], the military is still using aging FLTSAT and UFO satellites - and so
are a lot of Brazilians".

@_date: 2009-08-04 07:38:43
@_author: Peter Gutmann 
@_subject: Unattended reboots (was Re: The clouds are not random enough) 
I talked to an auditor about this a while back, here's my summary of this:
  For auditing purposes the only thing that.s required for unattended restart
  is a mechanism to prevent an attacker from copying unprotected keying
  material from the machine.  For example storing the key in a token plugged
  into the machine is generally considered sufficient because it gives you the
  ability to point to a physical security procedure that.s used to prevent the
  key (meaning the token that it.s held in) from being removed.  This
  functions as an audit mechanism because it.ll be noticed if someone removes
  the token, which isn.t the case if someone copies a file containing the
  unprotected key from the machine.  Hardware security modules (HSMs, a
  special-purpose crypto computing device capable of storing thousands of
  keys and performing encryption, signing, certificate management, and many
  other operations) are often used for this purpose, storing a single
  symmetric key in the HSM to meet audit requirements.  If the HSM vendor has
  particularly good salespeople then they.ll sell the client at least two
  $20,000 HSMs (each storing a single key) for disaster recovery purposes.
In other word's the target isn't necessarily what's good enough for security
people, but what's good enough for the auditors, and the above was deemed good
enough for the auditors.

@_date: 2009-08-07 00:49:17
@_author: Peter Gutmann 
@_subject: Client Certificate UI for Chrome? 
This is predicated on the assumption that it's possible to make certificates usable for general users.  All the empirical evidence we have to date seems to point to this not being the case.  Wouldn't it be better to say "What can we
do to replace certificates with something that works?", for example TLS-SRP
or TLS-PSK?

@_date: 2009-08-07 00:49:17
@_author: Peter Gutmann 
@_subject: Client Certificate UI for Chrome? 
This is predicated on the assumption that it's possible to make certificates usable for general users.  All the empirical evidence we have to date seems to point to this not being the case.  Wouldn't it be better to say "What can we
do to replace certificates with something that works?", for example TLS-SRP
or TLS-PSK?

@_date: 2009-08-11 05:35:38
@_author: Peter Gutmann 
@_subject: Client Certificate UI for Chrome? 
Sure, but that's a relatively tractable UI problem (and see the comment below on Camino).  Certificates on the other hand are an apparently intractable business, commercial, user education, programming, social, and technical problem.  I'd much rather try and solve the former than the latter.
The problem with password auth is that no browser (with the exception of Camino) has made even the most basic attempt to do the UI for this properly.  In all cases the browser pops up a dialog box, unconnected to the underlying operation or web page, that says "Gimme your password" in one way or another. This could be coming from anywhere, the browser, Javascript on the web page, another web page, who knows where, but since everyone knows that passwords are insecure there's no point in expending any effort to try and make them secure, and that's been the status quo for fifteen years.
What Camino does (and it's been awhile since I played with it, so I'll qualify that with "what I hope it still does") is roll the password-entry box down out of the browser menu bar in a circular motion that's both hard to spoof and that unmistakably ties the credential-entry request both to the web page that it's associated with and to the browser rather than being some floating popup coming from who knows where or what.  This can no doubt be nitpicked, but it's better than any other browser (that I've seen) does.
More generally, I can't see that implementing client-side certs gives you much of anything in return for the massive amount of effort required because the problem is a lack of server auth, not of client auth.  If I'm a phisher then I set up my bogus web site, get the user's certificate-based client auth message, throw it away, and report successful auth to the client.  The browser then displays some sort of indicator that the high-security certificate auth was successful, and the user can feel more confident than usual in entering their credit card details.  All you're doing is building even more substrate for phishing attacks.
Without simultaneous mutual auth, which -SRP/-PSK provide but PKI doesn't, you're not getting any improvement, and potentially just making things worse by giving users a false sense of security.

@_date: 2009-08-12 01:21:23
@_author: Peter Gutmann 
@_subject: Client Certificate UI for Chrome? 
Ah, that is a good point, you now need the credential information present at
the TLS level rather than the tunneled-protocol level (and a variation of
this, although I think one that way overcomplicates things because it starts
diverting into protocol redesign, is the channel binding problem (see RFC 5056
and, specific to TLS, draft-altman-tls-channel-bindings-05.txt)).  On the
other hand is this really such an insurmountable obstable?  For client-cert
usage you already need to perform a lookup based on a given cert (well, unless
you blindly trust anyone displaying a cert coming from a particular CA or set
of CAs, which I know some sites do), so now all you'd be doing is looking up a
shared-secret value instead of a cert based on a client ID.  I don't really
see why you'd need complex scripting interfaces though, just "return the
shared-secret value associated with this ID" in response to a request from the
TLS layer.  The only problem I can see is if you have an auth system built
around "is this authenticator valid" rather than "return the authenticator for
this ID".

@_date: 2009-08-17 12:57:54
@_author: Peter Gutmann 
@_subject: Client Certificate UI for Chrome? 
We seem to be talking about competely different things here.  For a typical
application, say online banking, I connect to my bank at  or
whatever, the browser requests my credential information, and the TLS-SRP or
TLS-PSK channel is established. That's all.  There's no web application
framework and PHP and scripting and other stuff at all, in fact I can't even
see how you'd inject this into the process.
You only do it once when the TLS session is set up, it's exactly as for
standard TLS except that instead of PKI-based non-authentication you use
cryptographic mutual authentication.  How do you get an SRP exchange for every
web page?

@_date: 2009-08-19 11:22:13
@_author: Peter Gutmann 
@_subject: Client Certificate UI for Chrome? 
We really are talking about completely different things here.  The scripting
and whatnot has nothing to do with TLS or TLS auth mechanisms.  The only thing
you need to care about (if you really want to go this way) is channel binding.
Yeah, and that's what channel binding is for.
Only if you deliberately choose to make it so.  Read the RFCs I mentioned
But TLS doesn't work like that.  If it did, you'd get a cert popup every time
you clicked on a link on a TLS-protected web site.  Unless you somehow manage
to flush the TLS session cache on the server (which seems unlikely unless
you're DoS'ing it) there's no additional round-trip(s), or additional anything
for that matter.

@_date: 2009-08-27 11:30:08
@_author: Peter Gutmann 
@_subject: SHA-1 and Git (was Re: [tahoe-dev] Tahoe-LAFS key management, part 2: Tahoe-LAFS is like encrypted git) 
It's not just that, while pluggability (for transparent crypto upgrade) may
sound like a fun theoretical exercise for geeks it's really a special case of
the (unsolveable) secure-initialisation problem.
Consider for example a system that uses two authentication algorithms in case
one fails, or that has an algorithm-upgrade/rollover capability, perhaps via
downloadable plugins.  At some point a device receives a message authenticated
with algorithm A saying "Algorithm B has been broken, don't use it any more"
(with an optional side-order of "install and run this plugin that implements a
new algorithm instead").  It also receives a message authenticated with
algorithm B saying "Algorithm A has been broken, don't use it any more", with
optional extras as before.  Although you could then apply fault-tolerant
design concepts to try and make this less problematic, this adds a huge amount
of design complexity, and therefore new attack surface.  Adding to the
problems is the fact that this capability will only be exercised in extremely
rare circumstances.  So you have a piece of complex, error-prone code that's
never really exercised and that has to sit there unused (but resisting all
attacks) for years until it's needed, at which point it has to work perfectly
the first time.  In addition you have some nice catch-22's such as the
question of how you safely load a replacement algorithm into a remote device
when the existing algorithm that's required to secure the load has been
Compounding this even further is the innate tendency of security geeks to want
to replace half the security infrastructure that you're relying on as a side-
effect of any algorithm upgrade.  After all, if you're replacing one of the
hash algorithms then why not take the opportunity to replace the key
derivation that it's used in, and the signature mechanisms, and the key
management as well?  This results in huge amounts of turmoil as a
theoretically minor algorithm change carries over into a requirement to
reimplement half the security mechanisms being used.  One example of this is
TLS 1.2, for which the (theoretically minor) step from TLS 1.1 to TLS 1.2 was
much, much bigger than the change from SSL to TLS, because the developers
redesigned significant portions of the security mechanisms as a side-effect of
introducing a few new hash algorithms.  As a result, TLS 1.2 adoption has
lagged for years after the first specifications became available.
What new horrible problems in SHA1 (as it's used in SSL/TLS)?  What old
horrible problems, for that matter?  The only place I can think of offhand
where it's used in a manner where it might be vulnerable is for DSA sigs, and
how many of those have you seen in the wild?

@_date: 2009-02-02 20:29:20
@_author: Peter Gutmann 
@_subject: full-disk subversion standards released 
In which way, and for what sorts of "protection"?  And I mean that as a serious inquiry, not just a "Did you spill my pint?" question.  At the moment the sole significant use of TPMs is Bitlocker, which uses it as little more than a PIN-protected USB memory key and even then functions just as well without it.  To take a really simple usage case, how would you:
- Generate a public/private key pair and use it to sign email (PGP, S/MIME,
  take your pick)?
- As above, but send the public portion of the key to someone and use the
  private portion to decrypt incoming email?
(for extra points, prove that it's workable by implementing it using an actual
TPM to send and receive email with it, which given the hit-and-miss
functionality and implementation quality of TPMs is more or less a required
second step).  I've implemented PGP email using a Fortezza card (which is
surely the very last thing it was ever intended for), but not using a TPM...
This use is like the joke about the dancing bear, the amazing thing isn't the quality of the "dancing" but the fact that the bear can "dance" at all :-).  It's an impressive piece of lateral thinking, but I can't see people rushing out to buy TPM-enabled PCs for this.

@_date: 2009-02-15 18:55:39
@_author: Peter Gutmann 
@_subject: Property RIghts in Keys 
[Moderator's note: my forwarding this is not an indication that I want
 to continue the "are certs IP" discussion. --Perry]
You obviously haven't seen some of the certificates out there.  I've seen
interpretations of the X.509 spec floating around that deserve a Hugo award.

@_date: 2009-02-17 18:16:33
@_author: Peter Gutmann 
@_subject: how to properly secure non-ssl logins (php + ajax) 
Go out and get a copy of "Network Security" by Kaufman, Perlman and Speciner,
this has an entire chapter that discusses this issue, including the pros and
cons of different approaches and all the ways you can get it wrong (oh, and
it's written for a non-security-geek audience).   I think any discussion here
will end up being mostly a rehash of bits of the chapter, their version goes
into much more detail and has diagrams to boot.

@_date: 2009-02-20 02:36:17
@_author: Peter Gutmann 
@_subject: The password-reset paradox 
There are a variety of password cost-estimation surveys floating around that
put the cost of password resets at $100-200 per user per year, depending on
which survey you use (Gartner says so, it must be true).
You can get OTP tokens as little as $5.  Barely anyone uses them.
Can anyone explain why, if the cost of password resets is so high, banks and
the like don't want to spend $5 (plus one-off background infrastructure costs
and whatnot) on a token like this?
(My guess is that the password-reset cost estimates are coming from the same
place as software and music piracy figures, but I'd still be interested in any
information anyone can provide).

@_date: 2009-02-21 23:37:09
@_author: Peter Gutmann 
@_subject: Crypto Craft Knowledge 
If we're allowed to do self-promotion I'll have to mention cryptlib, which had
as one of its principal design goals what was later stated by Ian Grigg as
"there should only be one mode and that is secure".  With cryptlib you have to
work very, very hard to do things insecurely (generally by resorting to
calling very low-level functions that the docs contain all sorts of dire
warnings about), and some things just can't be done at all, plaintext key
export being one really major sticking point that I get no end of complaints
about (if you really want the gory details you can get them at either
 or at
 for a newer, cleaned-up version).
This points out an awkward problem though, that if you're a commercial vendor
and you have a customer who wants to do something stupid, you can't afford not
to allow this.  While my usual response to requests to do things insecurely is
"If you want to shoot yourself in the foot then use CryptoAPI", I can only do
this because I care more about security than money.  For any commercial vendor
who has to put the money first, this isn't an option.

@_date: 2009-02-22 03:34:08
@_author: Peter Gutmann 
@_subject: stripping https from pages 
My analysis of this (part of a much longer writeup):

@_date: 2009-02-25 23:37:22
@_author: Peter Gutmann 
@_subject: Security through kittens, was Solving password problems 
You don't even need a MITM, just replace the site image on your phishing site with either a broken- image picture or a message that your award-winning site-image software is being upgraded and will be back soon and it's rendered totally ineffective. Ref: "The Emperor's New Security Indicators", Stuart Schechter, Rachna Dhamija, Andy Ozment, and Ian Fischer.  These things are as worthless as most of the other wish-it-was-two-factor authentication methods that US banks have deployed in reaction to the FFIEC guidance (in the case of Sitekey, it's the top-rated URL for the Prg malware, indicating that it presents no problem at all for the phishers).  The best "two-factor" I've seen to date is the New Horizons Community Credit Union, whose idea of two-factor auth is "Oh, we got both kinds.  We got user name *and* password".

@_date: 2009-02-26 19:34:35
@_author: Peter Gutmann 
@_subject: Security through kittens, was Solving password problems 
You'd really need to perform a controlled experiment to see which factors
actually affect this.  For example another factor could be that the gamer
demographic is more aware of phishing than Joe Sixpack and therefore less
likely to become a target.  Or that they're more interested in gaming than
account management and just ignore the message.  It'd be interesting to see
what the contributing factors are (although if it's "more interested in gaming
than account management" then it doesn't translate to other areas much).

@_date: 2009-01-11 00:42:08
@_author: Peter Gutmann 
@_subject: On the topic of "Asking the drunk"... 
Sigh, you wait awhile to make sure it's not an intermittent thing and then as
soon as you post it it stops working (or maybe someone from Visa is reading
this list and took it down quickly :-).  What it was doing until now was a
really convincing simulation of a phishing attack, the Firefox error message
  visa.com uses an invalid security certificate.
  The certificate is not trusted because it is self-signed.
  The certificate is only valid for MIA21793WWW002.managed.cln.
  (Error code: sec_error_ca_cert_invalid)
Unfortunately posting that bit to the list kinda lessens the effect of seeing
it live.  Good thing I saved a screenshot while it was still active :-).

@_date: 2009-01-12 16:05:08
@_author: Peter Gutmann 
@_subject: MD5 considered harmful today, SHA-1 considered harmful tomorrow 
I have a general outline of a timeline for adoption of new crypto mechanisms
(e.g. OAEP, PSS, that sort of thing, and not specifically algorithms) in my
Crypto Gardening Guide and Planting Tips,
 see "Question J"
about 2/3 of the way down.  It's not meant to be definitively accurate for all
cases but was created as a rough guideline for people proposing to introduce
new crypto mechanisms to give an idea of how long they should expect to wait to see them adopted.

@_date: 2009-01-20 17:57:09
@_author: Peter Gutmann 
@_subject: MD5 considered harmful today, SHA-1 considered harmful tomorrow 
Not a lot, I think.  The problem with 1.2 is that it introduces a pile of
totally gratuitous incompatible changes to the protocol that require quite a
bit of effort to implement (TLS 1.1 -> 1.2 is at least as big a step, if not a
bigger step, than the change from SSL to TLS), complicate an implementation,
are difficult to test because of the general lack of implementations
supporting it, and provide no visible benefit.  Why would anyone rush to
implement this when what we've got now works[0] just fine?
[0] For whatever level of "works" applies to SSL/TLS, in the sense that 1.2
    won't "work" any better than 1.1 does.

@_date: 2009-01-21 19:07:50
@_author: Peter Gutmann 
@_subject: MD5 considered harmful today, SHA-1 considered harmful tomorrow 
That wasn't really meant as a compliment :-).  The problem is that by leaping
on things the instant they appear you end up having to support a menagerie of
wierdo algorithms and mechanisms that the industry as a whole never adopts.
How many crypto libraries that could be used to implement the OpenPGP spec
actually support Haval, or Tiger, or Twofish, or SAFER-SK128?  The result of
this too-quick adoption has been a bunch of gaps in newer versions of the spec
(look for a range of algorithm IDs marked as "reserved") where algorithms
adopted too quickly were removed again when they failed to gain general (or
any) acceptance.
Another concern with too-quick adoption is the potential for moving to an
algorithm that's later found to be flawed.  This hasn't happened yet for
cryptographer-designed algorithms and mechanisms (as opposed to WEP et al) but
it's quite possible that some new algorithm introduced at Crypto n is shown to
reduce to rot-13 in a paper published in Crypto n+1.  I use an informal five-
year rule that I won't make an algorithm a default (i.e. enabled without
explicit user action) until it's had active attention paid to it for at least
five years, where "active attention" means not so much published in an obscure
conference somewhere but required in an industry spec or something similar
that results in active scrutiny of its security.  (Actually it's not quite
that simple, it's more of a balancing act and the pace depends on whether
there are credible threats to the current default algorithm or not).
In crypto, "new" doesn't necessarily mean "better", it can also mean

@_date: 2009-01-24 14:55:15
@_author: Peter Gutmann 
@_subject: MD5 considered harmful today, SHA-1 considered harmful tomorrow 
I'm aware of why the changes were made, but the point of my comment was to
explain their consequences in the lack of support for TLS 1.2.  I had (AFAIK)
one of the first implementations of TLS 1.1, an incremental upgrade of TLS 1.0
(and then had some fun trying to find things to interop against) but for TLS
1.2 I haven't implemented it and have no plans to implement it because it
provides absolutely no benefit over TLS 1.1 and a great many downsides.
It may have been a nicely principled job but what actual problem is the switch
in hash algorithms actually solving?  Making changes of such magnitude to a
very, very widely-deployed protocol is always a tradeoff between the necessity
of the change and the pain of doing so.  In TLS 1.2 the pain is proportionate
to the scale of the existing deployed base (i.e. very large) and the necessity
of doing so appears to be zero.  I don't know of any attack or threat to the
existing dual-hash mechanism that TLS 1.2 addresses, and it may even make
things worse by switching from the redundant dual-hash (a testament to the
original SSL designers) to a single algorithm.  This is why I called the
changes "gratuitous", there is no threat that they address - it can even be
argued (no doubt endlessly) that they make the PRF weaker rather than stronger
- but they come at considerable cost.
SSL/TLS is (and has been for many years) part of the Internet infrastructure.
You don't make significant, totally incompatible changes to the infrastructure
without very carefully weighing the advantages and disadvantages.  Maybe
there'll be a sudden flood of unexpected TLS 1.2 implementations right after I
post this, but at the moment it seems that implementors have weighed up the
cost and said "no thanks".

@_date: 2009-01-28 17:52:29
@_author: Peter Gutmann 
@_subject: Obama's secure PDA 
I wonder what a "classified USB cable" is.  Perhaps it's an unclassified USB
cable with the little three-prong USB logo blacked out by the censors.

@_date: 2009-01-29 18:44:03
@_author: Peter Gutmann 
@_subject: full-disk encryption standards released 
( it doesn't actually
tell you anything about how to do disk encryption, it's just... well I'll have
to quote the doc itself because I'm not quite sure what its purpose is, but
the document claims it's an "architecture for putting Storage Devices under
policy control as determined by the trusted platform host".  Reading through
the Opal spec ("minimum requirements for storage devices used in PCs and
laptops") is like reading a SCSI CDB reference, it outlines a means of
connecting something over here with something else over there with no
indication of what either of the two something's are.  It seems to be mostly
intended to be a means of tying a hard drive into the TPM framework, with the
entire crypto-related portions of the Opal spec being:
  2.4  Cryptographic Features
  An Opal SSC compliant SD SHALL implement Full Disk Encryption for all host
  accessible user data stored on media. AES-128 or AES-256 SHALL be supported
  (see [FIPS 197]).
  2.5  Authentication
  An Opal SSC compliant SD SHALL support password authorities and
  authentication.
There's an older draft from 2007 covering storage architecture which is...
um... 266 pages of the sort of thing you'd expect to emerge if the TCG tried
to define a standard for dealing with hard drives.
So I wouldn't call these "full-disk encryption standards", it's more like "TPM
glue for hard drives".  The P1619/SISWG work is completely different, you can
actually take this and implement drive encryption from it, and it specifies
(in some detail) how to do it right.

@_date: 2009-01-30 17:46:27
@_author: Peter Gutmann 
@_subject: "Attack of the Wireless Worms" 
Does anyone know whether anything like this actually exists?  I've seen earlier work in this area that was either man-in-the-router proof-of-concept stuff or simulation (as this work appears to be), but I don't know of any in-the-wild mesh-network malware.

@_date: 2009-01-31 23:19:14
@_author: Peter Gutmann 
@_subject: full-disk subversion standards released 
There's another problem with this theory and that's the practical
implementation issue.  I've read through... well, at least skimmed through the
elephantine bulk of the TCG specs, and also read related papers and
publications and talked to people who've worked with the technology, to see
how I could use it as a crypto plugin for my software (which already supports
some pretty diverse stuff, smart cards, HSMs, the VIA Padlock engine, ARM
security cores, Fortezza cards (I even have my own USG-allocated Fortezza ID
:-), and in general pretty much anything out there that does crypto in any
way, shape, or form).  However after detailed study of the TCG specs and
discussions with users I found that the only thing you can really do with
this, or at least the bits likely to be implemented and supported and not full
of bugs and incompatibilities, is DRM.
In all the time I've worked with crypto devices I've never seen something so
totally unsuited to general-purpose crypto use as a TPM.  There really is only
one thing it can reliably be used for and that's DRM.  Now admittedly if you
look really hard you may find a particular vendor who has a hit-and-miss
attempt at implementing some bits of the spec that, if you cross your eyes and
squint, is almost usable for general-purpose crypto use, but that's it.  Even
with the best intentions in the world, the only thing you can really usefully
do with a TPM is DRM.
(NB: This was a few years ago, maybe things have improved since then but I
haven't seen any real indication of this.  Oh, and I'm not going to get into
the rathole of whether the whole "attestation" thing is DRM or not, if you
think it isn't then please replace all occurrences of "DRM" in the above text
with "attestation").

@_date: 2009-07-13 17:58:29
@_author: Peter Gutmann 
@_subject: HSM outage causes root CA key loss 
I haven't been able to find an English version of this, but the following news
item from Germany:
reports that the PKI for their electronic health card has just run into
trouble: they were storing the root CA key in an HSM, which failed.  They now
have a PKI with no CA key for signing new certs or revoking existing ones.
(When I talk about PKI I always title the root CA as "the Single Point of
Failure", but I think this is the first time in a non-private CA where it's
actually become this in practice.  For private-label PKIs it's a lot more
common because of the "lesser-known public key" phenomenon).

@_date: 2009-07-15 16:56:12
@_author: Peter Gutmann 
@_subject: HSM outage causes root CA key loss 
I thought the Safekeypers had a cloning mechanism (as do things like Chrysalis
cards, although that proved to be not very secure when it was reverse-
engineered), and the idea was that you cloned one into the other as a backup?
Mind you at $x0,000 per device that's a good business for the HSM vendor.
The original article doesn't make this clear but what's involved here isn't
really a PKI in the conventional sense but more something like a master-keyed
system in the style of ATM networks.  In the same marvellous repurposing of
terminology that often occurs elsewhere in smart cards where, for example, a
checksum becomes a "signature", in this case the "certificates" are just a
jumble of parameters, some stuffed inside the signature itself (via a sign-
with-message-recovery mechanism instead of the usual sign-with-appendix) and
the rest bound to it through a hash.  The "CA" key is more an attestation key,
there are no CRLs or certificate-checking in the conventional sense (you can
get away with these name games by calling the stored data a "card verifiable
certificate", and if you have a "certificate" then what signs it is obviously
a "CA", so you get something that seems to be a PKI but isn't).  So when you
lose your master key as they did in this case and there isn't really a PKI
there at all, it really is game over.
Even if it was a real PKI, rolling over a root is an incredibly traumatic
experience, which one trial found could only be done via a "system rebuild"
(in plain english a reformat and reinstall of the whole PKI).  This is why CA
root certs have a 20-40 year lifetime, so you never end up in a position where
you have to roll them over.

@_date: 2009-07-15 17:05:05
@_author: Peter Gutmann 
@_subject: HSM outage causes root CA key loss 
... and now you have two (probably unsolveable) problems instead of one.  In addition because the second problem virtually never occurs, it'll receive little or no evaluation in the real world, and will either not work when it's needed or will break when it's not needed, allowing your main PKI to be compromised through it.

@_date: 2009-07-16 01:32:17
@_author: Peter Gutmann 
@_subject: HSM outage causes root CA key loss 
Thus the universal CA root cert lifetime policy, "the lifetime of a CA root certificate is the time till retirement of the person in charge at its creation, plus five years" :-).

@_date: 2009-07-18 15:39:17
@_author: Peter Gutmann 
@_subject: XML signature HMAC truncation authentication bypass 
Leandro Meiners  quotes:
This excessive generality is a serious problem in way too many crypto specs, and implementations of security protocols.  For example PKCS  allows you to leak keys out of cryptographic hardware by specifying the use of a 1-bit key derivation function, allowing you to guess an entire key one bit at a time (fortunately few, if any, implementations actually support this).  In the PKI world, virtually no spec requires sensible limits on anything (some years ago I brought a CA to a halt by specifying a huge salt and large iteration count for the challenge-response protocol they used to authenticate certificate issues.  Then I repeated it to make sure it wasn't just a coincidence the first time :-).  PGP Desktop 9 uses as its default an iteration count of four million (!!) for its password hashing, which looks like a DoS to anything that does sanity-checking of input.  Netscape (and obviously this test was done some years ago) will happily accept a certificate with a 1MB MPEG of a cat in the X.500 DN, but then become what could mildly be described as "unresponsive" once it's stored it in its cert.database.  The SSH oracle attack of a few months ago was helped by the fact that the length field wasn't constrained to sensible values.  This list goes on and on, it's scary just how vulnerable a lot of security software is to anything that can drop a slightly unexpected value into a data field.

@_date: 2009-07-23 17:34:13
@_author: Peter Gutmann 
@_subject: Fast MAC algorithms? 
Could the lack of support for TCP offload in Linux have skewed these figures
somewhat?  It could be that the caveat for the results isn't so much "this was
done ten years ago" as "this was done with a TCP stack that ignores the
hardware's advanced capabilities".

@_date: 2009-07-24 17:30:24
@_author: Peter Gutmann 
@_subject: Fast MAC algorithms? 
[I realise this isn't crypto, but it's arguably security-relevant and arguably
 interesting :-)].
The problem with statements like this is that they smack of the Linux
religious zealotry against TCP offload support in the kernel, "TOE's are bad
because we say they are, and we'll keep asserting this until you go away".  A
decade ago, during the Win2K development, Microsoft were measuring a 1/3
reduction in CPU usage just from TCP checksum offload.  Given the time frame
this was probably on 300MHz PII's, but then again it'd be with late-90s
vintage NICs.  On the other hand I've seen even more impressive figures with
their more recent TCP chimney offload (which just moves more of the NDIS stack
onto the NIC, I think it came out around Server 2003).
Does this mean that MS have figured out (a decade or so ago) how to make TOE
work while the OSS community has been too occupied telling everyone it doesn't
to do anything about it?  There must be some reason for the difference between
the two camps.

@_date: 2009-07-27 01:24:58
@_author: Peter Gutmann 
@_subject: XML signature HMAC truncation authentication bypass 
In that case why not use a billion iterations (or at least bytes of output),
that would really slow down attackers.
Where this falls apart completely is when there are asymmetric capabilities
across sender and receiver.  Having an embedded device suspend (near) real-
time processing while it iterates away at something generated on a multicore
3GHz desktop PC isn't really an option in a production environment (the actual
diagnosis was "messages generated by PGP Desktop cause our devices to crash"
because they were triggering a deadman timer that soft-restarted them, it
wasn't until they used an implementation that sanity-checked input values that
they realised what the problem was).

@_date: 2009-07-27 16:45:55
@_author: Peter Gutmann 
@_subject: The latest Flash vulnerability and monoculture 
There are quite a number of third-party video players that will render Flash
video, are these using Adobe codecs or third-party H.263/264/VP6 ones?  In
theory you don't actually need to run Adobe code to view FLV's, but given the
freewheeling nature of video players which often, um, borrow codecs from all
over the place, it's hard to tell what you're actually getting.

@_date: 2009-07-27 17:08:10
@_author: Peter Gutmann 
@_subject: The latest Flash vulnerability and monoculture 
That's because crypto is cool, and it's so simple that absolutely anyone who's
read the first two chapters of Applied Cryptography can do it.  Writing,
tuning, and debugging video codecs on the other hand is only slightly more
interesting than developing accounts receivable software, only five people on
earth really understand how they work, and at least two of them aren't allowed
near sharp objects because of what they might do with them.

@_date: 2009-07-27 17:31:31
@_author: Peter Gutmann 
@_subject: XML signature HMAC truncation authentication bypass 
Well, I think it's necessary to consider the tradeoffs, if you don't know the
other side's capabilities then it's a bit risky to assume that they're the
same as yours.
The data was encrypted using pre-shared secrets (i.e. packet type 3) which
does have this property.
(Don't ask me, I didn't create the requirements, I just got called in to help
diagnose the problem, which was that at some point S2K's coming from PGP
Desktop were killing their embedded units.  Maybe they were even using
externally-generated private keys or who knows what rather than pre-shared
secrets for messages, but whatever it was it was the S2K step that was causing
This doesn't work in a heterogeneous environment where the requirements will
be something like messages having to comply with certain parts of the OpenPGP
spec, and no more.  Adding riders telling users how to manually configure
individual applications doesn't work because end-users will never read the
technical spec, or even know that it exists.
I guess we could argue this point endlessly, but I really just brought it up
to mention the unintended consequences of a particular design decision, and
more generally the dangers of allowing unbounded integer and general data
ranges in specs.  Some implementations will enforce sensible limits, many
won't (and will fail against fairly trivial attacks because of this), and
without any guidance in the spec the ones that do take care to bound values
are deemed non-compliant while the vulnerable ones that don't do any checking
are deemed compliant.  This is completely backwards for a security spec.

@_date: 2009-07-29 03:58:48
@_author: Peter Gutmann 
@_subject: XML signature HMAC truncation authentication bypass 
They're not technically password-protected files but pre-shared key (PSK)
protected files, where the keys have a high level of entropy (presumably 128
bits, but I don't know the exact figure).  So in this case the S2K isn't
actually necessary because of the choice of password/PSK used.
(Sorry, for non-OpenPGP folks "S2K" = "string to key", a parameterised way of
processing a password, for example by iterated hashing with a salt, into a
The answer depends on what sort of user base you expect to have to support.  In my case I disable things that I don't think get used much in betas and see if anyone complains.  If no-one does, it remains disabled in the final release.  Now if only I could rearrange this process so I didn't have to implement support for assorted practically-unused mechanisms in the first This is another interesting philosophical debate: What do other people do in terms of deprecating obsolete/insecure/little-used mechanisms?  Deprecate by stealth?  Flag day?  Support it forever?

@_date: 2009-06-02 01:57:06
@_author: Peter Gutmann 
@_subject: Banks phishes its own customers 
Imagine if you got the following email:
  You may have noticed that we've created a new tool in FastNet Classic called
  the Online Vault. Hopefully you'll find it pretty handy - it allows you to
  securely store important personal information such as:
  - IRD number [equivalent to the SSN in the US]
  - medical details
  - passport number
  - anniversaries, birthdays etc.
  This means that all your important information is in one secure place,
  allowing you to get hold of it easily, anytime you need it.  No one else can
  access this information - only you!
  To see how the Online Vault works, click on the link below to play the 10-
  second challenge
  [Link to the Online vault site].
When you sign in, you're invited to enter (copied from the web page):
  Passport details, drivers licence, accounts, credit cards, next of kin,
  accountant, solicitor [lawyer in the US], medical conditions, allergies,
  vaccination history, doctor details, insurance contacts and insurance
  policies, and [a general-purpose field for anything else you may want to
  store].
Surprisingly, this isn't phishing email, it's genuine email from a bank.  While the bank's security page warns about attacks where:
  A fraudster sends an email to a large number of email addresses.  The email
  may appear to be from the email recipient's bank.  The message urges the
  recipient to click on a link to update their personal profile or carry out
  some transaction.
in this case the users can tell that the email is from the real bank because
they also ask for your credit card numbers, medical history, and drivers
licence, and no phisher would be that blatant.

@_date: 2009-03-04 19:14:54
@_author: Peter Gutmann 
@_subject: Judge orders defendant to decrypt PGP-protected laptop 
Very nice explanation.  The name I've used for this (attempted) defence is the
Rumpelstiltskin defence, for reasons that should be obvious (and at some point
I'll get around to finishing the writeup on this, which I get motivated to do
every time I see someone advocate the Rumpelstiltskin defence as a strategy to
use in court).

@_date: 2009-03-05 04:28:53
@_author: Peter Gutmann 
@_subject: Judge orders defendant to decrypt PGP-protected laptop 
If you have an appropriate netbook (about 50% support this, check your
manufacturer and model type), unplug the SD card containing the OS image and
replace it with the SD card containing the clean install of XP along with
"Letter to mom.doc" and "Aunty Edna's 90th birthday.jpg".  Once you're at your
destination, pull the real SD card from the collection in your camera bag and
reinsert it.  Takes next to no time at all, and it guarantees there really
isn't anything there to be found (including large collections of random noise)
in any normal search.

@_date: 2009-03-07 07:36:25
@_author: Peter Gutmann 
@_subject: full-disk subversion standards released 
Kent would be the one to answer this definitively, but the docs on the web
page talk about using OpenSSL to change the password on the stored keys,
without (apparently) needing the TPM, which seems a bit odd.
In any case though, how big a deal is private-key theft from web servers?
What examples of real-world attacks are there where an attacker stole a
private key file from a web server, brute-forced the password for it, and then
did... well, what with it?  I don't mean what you could in theory do with it,
I mean which currently-being-exploited attack vector is this helping with?
This does seem like rather a halfway point to be in though, if you're not
worried about private-key theft from the server then do it in software, and if
you are then do the whole thing in hardware (there's quite a bit of this
around for SSL offload) rather than just one small corner of it.  If your
target market is "people who are worried about theft of private key files (but
not in-memory keys) from web servers and who don't want to use hardware to
protect them and who are running a server that actually has a TPM installed"
then I suspect you've limited your applicability somewhat...

@_date: 2009-03-15 00:26:39
@_author: Peter Gutmann 
@_subject: full-disk subversion standards released 
Ah, that kinda makes sense, it would parallel the experience with client-side
keys (SSH in this case since client-side PKI is virtually nonexistent) were
nearly 2/3 of all private keys were found to be stored in plaintext form on
shared machines.  This is why a security developer some years ago started
referring to the private key as "the lesser-known public key" :-).
(Does anyone know of any studies that have been done to find out how prevalent
this is for servers?  I can see why you'd need to do it for software-only
implementations in order to survive restarts, but what about hardware-assisted
TLS?  Is there anything like a study showing that for a random sampling of x
web servers, y stored the keys unprotected?  Are you counting things like
Windows' DPAPI, which any IIS setup should use, as "protected" or
I was hoping someone else would leap in about now and question this, but I
guess I'll have to do it... maybe we have a different definition of what's
required here, but AFAIK there's an awful lot of this kind of hardware
floating around out there, admittedly it's all built around older crypto
devices like Broadcom 582x's and Cavium's Nitrox (because there hasn't been
any real need to come up with replacements) but I didn't think there'd be much
problem with finding the necessary hardware, unless you've got some particular
requirement that rules a lot of it out.

@_date: 2009-05-02 21:28:09
@_author: Peter Gutmann 
@_subject: Destroying confidential information from database 
It's nearly fifteen years old (it was written in 1995, when the very first
PRML drives were just starting to appear, there's a reference in there to a
Quantum whitepaper published the same year) and refers to technology from the
early 1990s (and leftover stuff from the late 1980s, which was still around at
the time).  I've had an epilogue attached to the paper for, oh, at least ten
of those fifteen years saying:
  In the time since this paper was published, some people have treated the 35-
  pass overwrite technique described in it more as a kind of voodoo
  incantation to banish evil spirits than the result of a technical analysis
  of drive encoding techniques.  As a result, they advocate applying the
  voodoo to PRML and EPRML drives even though it will have no more effect than
  a simple scrubbing with random data.  In fact performing the full 35-pass
  overwrite is pointless for any drive since it targets a blend of scenarios
  involving all types of (normally-used) encoding technology, which covers
  everything back to 30+-year-old MFM methods (if you don't understand that
  statement, re-read the paper).  If you're using a drive which uses encoding
  technology X, you only need to perform the passes specific to X, and you
  never need to perform all 35 passes.  For any modern PRML/EPRML drive, a few
  passes of random scrubbing is the best you can do.  As the paper says, "A
  good scrubbing with random data will do about as well as can be expected".
  This was true in 1996, and is still true now.
  Looking at this from the other point of view, with the ever-increasing data
  density on disk platters and a corresponding reduction in feature size and
  use of exotic techniques to record data on the medium, it's unlikely that
  anything can be recovered from any recent drive except perhaps a single
  level via basic error-cancelling techniques.  In particular the drives in
  use at the time that this paper was originally written have mostly fallen
  out of use, so the methods that applied specifically to the older, lower-
  density technology don't apply any more.  Conversely, with modern high-
  density drives, even if you've got 10KB of sensitive data on a drive and
  can't erase it with 100% certainty, the chances of an adversary being able
  to find the erased traces of that 10KB in 80GB of other erased traces are
  close to zero.
(the second paragraph is slightly newer than the first one).  The reason why I
haven't updated the paper is that there really isn't much more to say than
what's in those two paragraphs, EPRML and perpendicular recording are nothing
like the technology that the paper discusses, for these more modern techniques
a good scrubbing is about the best you can do, and you have to balance the
amount of effort you're prepared to expend with the likelihood of anyone even
trying to pull 10kB of data from a (well, at the time 80GB was the largest
drive, today 1TB) drive.  I made the paper as forward-looking as I could with
the information available at the time (i.e. projection to PRML/EPRML read
channels and so on in the original paper), but didn't realise that people
would skip that bit and just religiously quote the same old stuff fifteen
years later.
(I've been working on a talk on "Defending where the Attacker Isn't" where I
look at this sort of thing, in some areas like password "best practices" this
phenomenon is even more pronounced because organisations are religiously
following "best practices" designed to defend shared mainframes connected to
029 keypunches and model 33 teletypes, I hope the data erasure thing doesn't
follow the same lifecycle :-).

@_date: 2009-05-02 21:53:40
@_author: Peter Gutmann 
@_subject: SHA-1 collisions now at 2^{52}? 
Seriously, what threat does this pose to TLS 1.1 (which uses HMAC-SHA1 and
SHA-1/MD5 dual hashes)?  Do you think the phishers will even notice this as
they sort their multi-gigabyte databases of stolen credentials?
The problem with TLS 1.2 is that it completely breaks backwards compatibility
with existing versions, it's an even bigger break than the SSL -> TLS
changeover was.  If you want something to incentivise vendors to break
compatibility with the entire deployed infrastructure of TLS devices, the
attack had better be something pretty close to O( 1 ), preferably with
deployed malware already exploiting it.
Ten years ago you may have been able to do this sort of thing because it was
cool and the geeks were in charge, but today with a deployed base of several
billion devices (computers, cellphones, routers, printers, you name it) the
economists are in charge, not the cryptographers, and if you do the sums TLS
1.2 doesn't make business sense.  It may be geeky-cool to make the change, but
geeky-cool isn't going to persuade (say) Linksys to implement TLS 1.2 on their
home routers.
(I can't believe I just said that :-).

@_date: 2009-05-03 01:07:45
@_author: Peter Gutmann 
@_subject: Has any public CA ever had their certificate revoked? 
Subject says it all, does anyone know of a public, commercial CA (meaning one
baked into a browser or the OS, including any sub-CA's hanging off the roots)
ever having their certificate revoked?  An ongoing private poll hasn't turned
up anything, but perhaps others know of instances where this occurred.

@_date: 2009-05-05 16:11:40
@_author: Peter Gutmann 
@_subject: Has any public CA ever had their certificate revoked? 
Yes, several times, see e.g. the recent mozilla.org fiasco, as a result of
which nothing happened because it would have been politically inexpedient to
revoke the CA's cert.
Not explicitly lost, but sold on eBay (depending on what your definition of
"public CA" is, probably more "large private-label CA", once the PKI project
is scrapped no-one really cares what happens to the hardware, so just as you
can buy hard drives full of financial records on eBay you can also buy HSMs
loaded with CA keys.  Unfortunately I'm still waiting for a browser root CA
key to turn up in one :-).
Yes, see above.
Again, what's "mismanagement"?  Would "CA went bankrupt and ex-employees
issued themselves certs in lieu of severance pay" count?  Or "CA went bankrupt
and there was no-one left to manage the keys, including issuing CRLs for
revoked certs" count?  Or ...

@_date: 2009-05-07 01:02:41
@_author: Peter Gutmann 
@_subject: Has any public CA ever had their certificate revoked? 
What I meant was that there were no repercussions due to the CA acting
negligently.  This is "nothing happened" as far as motivating CAs to exercise
diligence is concerned, you can be as negligent as you like but as long as you
look suitably embarassed afterwards there are no repercussions (that is,
there's no evidence that there was any exodus of customers from the CA, or any
other CA that's done similar things in the past).
Imagine if a surgeon used rusty scalpels and randomly killed patients, or a
bank handed out money to anyone walking in the door and claiming to have an
account there, or a restaurant served spoiled food, or ... .  The
repercussions in all of these cases would be quite severe.  However when
several CAs exhibited the same level of carelessness, they looked a bit
embarassed and then went back to business as usual.  The CA-as-a-certificate-
vending-machine problem (or "rogue CA" if you want to call it that) had been
known for years (Verisign's "Microsoft" certificates of 2001 were the first
case that got widespread publicity) but since there are no repercussions for
CAs doing this there's no incentive for anything to change.
If a CA in a trust anchor pile does something terribly wrong and there are no
repercussions, why would any CA care about doing things right?  All that does
is drive up costs.  The perverse incentive that this creates is for CAs to
ship as many certificates as possible while applying as little effort as
possible.  And thus we have the current state of commercial PKI.

@_date: 2009-05-07 01:16:49
@_author: Peter Gutmann 
@_subject: CSPRNG algorithms 
I did a (hopefully) reasonably comprehensive analysis of what was around in
the late 90s in my thesis, available via
 (there's an updated
version available as "Cryptographic security architecture: design and
verification", published by Springer), specifically chapter 6, "Random number
generation".  This covers PRNGs from AC2, X9.17, PGP 5.x, /dev/random, Skip,
ssh (that is, the ssh.com implementation), SSLeay/OpenSSL, CryptoAPI,
Capstone/Fortezza, the Intel PIII generator, and some other bits.

@_date: 2009-05-07 01:23:52
@_author: Peter Gutmann 
@_subject: Solving password problems one at a time, Re: The password-reset paradox 
That's not the reason, TLS-SRP isn't that annoyingly encumbered, and even the totally unencumbered TLS-PSK doesn't get used by anyone.  I was told a reason for the lack of use of strong password protocols from one browser vendor that was so stunningly stupid that I had trouble beliving that it was for real, ask me in private mail if you want the details.  In any case though it's not patent issues that are leading to non-use.

@_date: 2009-05-07 03:00:50
@_author: Peter Gutmann 
@_subject: SHA-1 collisions now at 2^{52}? 
I'm not really sure if it works that way.  From my experience with SSH in
routers [0] I'd say it's more like:
  Binary images in routers last years.  If we deploy first-cut, buggy
  implementations of new protocols now, we'll have to support the bugs in a
  backwards-compatible manner for the rest of eternity.
That is, in the absence of widely-deployed, mature implementations to test
against, router vendors will (if they were to ship with this right now) deploy
pre-alpha quality code that would then be frozen for the rest of eternity.  I
have to maintain support for ten-year-old SSH bugs in my code because of ports
to... well, unnamed vendors' systems done a decade or so back that never get
touched again once the initial version got to the point where it would respond
to a packet.  So if vendors are going to bake things into firmware (which
includes firmware images that never get updated, more or less the same thing)
then I'd prefer they hold on a bit until it's certain they've got somewhat
more mature code.
[0] Implementations of this are easier to date than SSL, and also a lot
    buggier so there's more to watch out for.

@_date: 2009-05-07 14:42:15
@_author: Peter Gutmann 
@_subject: Has any public CA ever had their certificate revoked? 
The problem with this is that recent changes in browser UI (particularly in
FF3) make it really, really hard to work with anything but cert-vending-
machine certificates.  It could be argued that of all the (public) CAs out
there, CACert is the most trustworthy because they're the only one not
motivated by money to crank out as many certs as possible as cheaply as
possible (although the last time I checked they also do email-verification-
only certs, so it may be more a theoretical advantage than a real one).
Of course with the universal implicit cross-certification present in browsers
this is all a moot point because the whole thing is only as secure as the
least reliable, least digilent sub-sub-sub-CA in the whole dogpile (insert
Matt Blaze PKI quote here).
Uhh, how is that meant to work?
In any case even if it did, every time you went to a site using a cert vending
machine not on your list the browser wouldn't let you connect (or at least not
without serious amounts of messing around, which means that eventually you'd
add it to your list just to get rid of the nuisance).
This is unfixably broken.  We've been trying the same broken thing for fifteen
years now and it still hasn't started to work.  The solution is to look at
alternatives like mechanisms that protect relationships (challenge-response
mutual auth like TLS-SRP and TLS-PSK), not a nonfunctional mechanism which,
even if it worked perfectly, could only protect mostly-meaningless names.

@_date: 2009-05-11 22:06:48
@_author: Peter Gutmann 
@_subject: What happened to X9.59? 
I was looking for information on this recently to update an old reference to the DSTU version but it seems to have vanished, there's no information on it online that I could find after about 2001 or so (apart from a reference to a 2006 version in a conference paper).  The ANSI web site claims that it doesn't exist, stopping the series at X9.58.

@_date: 2009-05-30 00:48:57
@_author: Peter Gutmann 
@_subject: consulting question.... (DRM) 
I think a far more important consideration for license-management software isn't "how secure is it" but "how obnoxious is it for legitimate users"?  I know a number of people who have either themselves broken or downloaded tools to break FlexLM and similar schemes, and in every single case they were legitimate users who were prevented from using their legally purchased product by the license-mismanagement tools, or who after spending hours or even days fighting with the license-mismanagement software found it easier to break the protection than to try and figure out what contortions were required to keep the license-checking code happy.  I've experienced this myself with a software tool I use, there are some (as I found out after several hours of searching support forums) well-known problems with it that the vendor doesn't seem interested in fixing, and that you can eventually resolve either with some registry hacks and other low-level changes or by downloading haxor tools that'll achieve the same result with a few minutes work (just for the record, I took the multi-hour route).  So if your license-management software is sufficiently obnoxious that it turns legitimate users into DMCA-violators, you have a problem.

@_date: 2009-11-20 20:12:13
@_author: Peter Gutmann 
@_subject: Phone company phishes its own users 
There have been numerous posts to this list about banks phishing their own
users so I figured I'd start a new thread about other companies who are
potential phishing-targets doing this as well, in this case a phone company.
  User:
  Not sure if this is genuine or not - just had a call from a blocked number
  caller saying they were from Vodafone doing a 'courtesy' call checking I'm
  happy on my plan and updating contact details. First thing they ask for is
  my Date of Birth and Vodafone pin number.  Is this genuine?
  Vodafone staff:
  I would not be giving out my date of Birth
  User:
  So thats a no to it being official then?
  Vofafone staff:
  Call was all good nothing to worry about
So next time you get a call from a caller-ID-blocked number asking for your
PIN, it's OK, it's only your phone company.

@_date: 2009-11-20 20:12:48
@_author: Peter Gutmann 
@_subject: Why the onus should be on banks to improve online banking security 
There's been a near-neverending debate about who should be responsible for
improving online banking security measures: the users, the banks, the
government, the OS vendor, ... .  Here's an interesting perspective from Peter
Benson , reposted with permission, on why the onus
should be on banks to provide appropriate security measures:
  One of the main reasons to target the banks with accountability is "because
  you can". There is a lot of historical regulation and controls around
  banking, which makes it *relatively* easy to hold them to account. The
  bigger problem, and the next logical step, is how the banks hold suppliers /
  vendors of software accountable for flaws in their systems and software that
  enable the problems to occur in the first place.
  Anyone recognise the following?
  "This software is provided as is, and any expressed or implied warranties,
  including, but not limited to, the implied warranties of merchantability and
  fitness for a particular purpose are disclaimed. In no event shall the
  contributors be liable for any direct, indirect, incidental, special,
  exemplary, or consequential damages (including, but not limited to,
  procurement of substitute goods or services; loss of use, data, or profits;
  or business interruption) however caused and on any theory of liability,
  whether in contract, strict liability , or tort (including negligence or
  otherwise) arising in any way out of the use of this software, even if
  advised of the possibility of such damage."
  Accountability is great, and I fully support it, and would like to somehow
  find the way to push a level of accountability back to various software
  developers / manufacturers. Unfortunately in the current state of Contract
  and Tort law, there is so much protection(ism) of the software industry,
  that its still going to be time consuming and expensive to get a couple of
  decent case studies out there or to change anything. So from a public good
  perspective, unfortunately (realistically), it is the banks that should
  carry the onus.

@_date: 2009-11-20 20:13:52
@_author: Peter Gutmann 
@_subject: Crypto dongles to secure online transactions 
Some general thoughts on this, there have been attempts going back at least
ten years to bring devices like this to market (for example I have a nice
device that does exactly this built in the late 90s sitting in a drawer
somewhere), but they always die for the same reason, lack of interest and, for
the few who are interested, lack of interest in paying the cost.
Because (apart from the reasons given above) with business use specifically
you run into insurmountable PC <-> device communications problems.  Many
companies who handle large financial transactions are also ones who, due to
concern over legal liability, block all access to USB ports to prevent
external data from finding its way onto their corporate networks (they are
really, *really* concerned about this).  If you wanted this to work, you'd
need to build a device with a small CMOS video sensor to read data from the
browser via QR codes and return little more than a 4-6 digit code that the
user can type in (a MAC of the transaction details or something).  It's
feasible, but not quite what you were thinking of.

@_date: 2009-09-03 16:26:30
@_author: Peter Gutmann 
@_subject: Client Certificate UI for Chrome? 
Good enough to satisfy security geeks, no, because no measure you take will
ever be good enough.  However if you want something that's good enough for
most purposes then Camino has been doing something pretty close to this since
it was first released (I'm not aware of any other browser that's even tried).
When you're asked for credentials, the dialog rolls down out of the browser
title bar in a hard-to-describe scrolling motion a bit like a supermarket till
printout.  In other words instead of a random popup appearing in front of you
from who knows what source and asking for a password, you've got a direct
visual link to the thing that the credentials are being requested for.  You
can obviously pepper and salt this as required (and I wouldn't dream of
deploying something like this without getting UI folks to comment and test it
on real users first), but doing this is a tractable UI design issue and not an
intractable business-model/political/social/etc problem.

@_date: 2009-09-06 19:34:24
@_author: Peter Gutmann 
@_subject: Client Certificate UI for Chrome? 
There's a variant of this, the site-specific browser (SSB), that takes you to
(for example) your bank in a strongly sandboxed, hardened environment.  This
reduces the cognitive load on the user from a more or less impossible-to-
follow set of instructions to "only ever do your banking by clicking on this
desktop icon".  This isn't by any means a general solution, but by solving for
the most common cases (your bank, Paypal, eBay, Amazon) you'd address a fairly
large chunk of the problem.  See "Breaking out of the Browser to Defend
Against Phishing Attacks" by Smetters and Stewart for more details on this.
Actually that does point out one problem, which I alluded to in my previous post: we have lots and lots of good ideas, but little hard data to indicate which ones will work and which won't, or which ones work better than others (although the cynical response to this might be that almost anything would work better than what we've got now).  Specifically, there are a pile of papers along the lines of "here's an experiment showing that what we're doing now doesn't work, here's a completely new security mechanism we've invented that involves redesigning the browser and server authentication back-end, and as a side-effect here are some UI ideas to go with it".  What we don't have however is "here's a real-world evaluation of various ideas that have been proposed for fixing what we already have built into browsers and servers". Unfortunately without this data we (including myself) are to some extent just "people wanking around with their opinions" [0].
It's also not certain how such data would be published.  Which journal or
conference would accept a paper with no "new ideas" in it, just a
straightforward evaluation of existing material?
[0] A Linus quote, brought about by a discussion on the difference between OS
secheduler design and security design: "the *discussion* on security seems to
never get down to real numbers. So the difference between them is simple: one
is 'hard science'. The other one is 'people wanking around with their

@_date: 2009-09-08 00:41:24
@_author: Peter Gutmann 
@_subject: Client Certificate UI for Chrome? 
Here's a real-world example of the problem I was referring to, from a
discussion with some browser developers.  We were going over the above issue -
how to use the results of usability studies to improve the browser security UI
- and I asked why, if they were aware of this work, no-one had done anything
with it.  The response was that "if we implement this then attackers will use
XUL to spoof it, and because of that it's not even worth trying".  In other
words because a hypoethetical weakness existed, it wasn't even worth
attempting to improve anything.  Instead of trying to help at least some of
the people some of the time, it was better to leave everyone unprotected all
of the time.
But the problem with these approaches is that they're pretty much all just
random tweaking of the same thing, aimlessly rearranging the UI elements in
the hope that eventually something will work after (as you point out) the
twenty more or less identical previous attempts have failed.  For example the
Firefox password-entry mechanism (a generic popup dialog with a gibberish
title) hasn't changed in at least ten years (possibly even longer, I can't
remember what the Netscape 2.0 one looked like any more), all that's changed
is that every major release a few new bits of flair get added to the browser
chrome.  The "previous attempts" aren't lots of different approaches, it's the
same failed approach tried over and over and over again, with slight
variations over time in the hope that one of them might work.
What I was advocating was trying new approaches based on ideas from UI
research, not just fiddling with the chrome in the hope that this time it'll
finally start working.

@_date: 2009-09-08 01:02:43
@_author: Peter Gutmann 
@_subject: SHA-1 and Git (was Re: [tahoe-dev] Tahoe-LAFS key management, part 2: Tahoe-LAFS is like encrypted git) 
Well, let's move the TLS 1.2 aspect out of the discussion and look at the
underlying issues.  If you're looking at this purely from a theoretical point
of view then it's possible that the ability to use SHA-2 in the PRF is an
improvement (it may also be a step backwards since you're now relying on a
single hash rather than the dual hash used in the original design).  Since no-
one knows of any attacks, we don't know whether it's a step backwards, a step
forwards, or (most likely) a no-op.
However there's more to it than this.  Once you've got the crypto sorted out,
you need to implement it, and then deploy it.  So looking at the two options
you have:
Old: No known crypto weaknesses.
     Interoperable with all deployed implementations.
     Only one option, so not much to get wrong.
New: No known crypto weaknesses.
     Interoperable with no deployed implementations.
     Lots of flexibioity and options to get wrong.
Removing the common factors (the crypto portion) and the no-op terms
("interoperable with existing implementations") we're left with:
Old: -
New: Non-interoperable.
     Complex -> Likely to exhibit security flaws (from the maxim that
       complexity is the enemy of security).
That's a rather high cost to pay just for the ability to make a crypto fashion
statement.  Even if the ability to negotiate hash algorithms had been built in
from the start, this only removes the non-interoperability but doesn't remove
the complexity issue.
You keep asserting that this is a mistake, but in the absence of any
cryptographic argument in support, and with practical arguments against it, it
looks like a feature to me.
A properly-designed security protocol is one that's both cryptographically
sound and simple enough that it's hard to get wrong (or at least relatively
easy to get right, admittedly not necessarily the same thing).  Adding a pile
of complexity simply so you can make a crypto fashion statement doesn't seem
to be helping here.
... like that model of security protocol design IKEv1 was [0], then we'd have
all kinds of interop problems and quite probably security issues based on
exploitation of the unnecessary complexity of the protocol, for a net loss in
security and interoperability, and nothing gained.
[0] Apologies to the IPsec folks on the list, just trying to illustrate the
    point.

@_date: 2009-09-09 18:43:38
@_author: Peter Gutmann 
@_subject: RNG using AES CTR as encryption algorithm 
I was just going to reply with a variation of this, if you're implementing a
full protocol that uses AES-CTR (or any algorithm/mode for that matter), find
other implementations that do it too and make sure that you can talk to them.
In theory everyone could end up implementing it wrong, but that's somewhat
(This has already caught AES-CTR implementation bugs in the past, for example
one particular version of OpenSSL 0.9.8 got AES-CTR keying wrong and it was
noticed when SSH users couldn't connect to OpenSSH servers using this mode).

@_date: 2009-09-14 17:34:03
@_author: Peter Gutmann 
@_subject: RNG using AES CTR as encryption algorithm 
I first saw it reported on the Putty bugs list [0], a good place to track
interop problems with implementations since it's so widely used, which in turn
points to   Connections from "OpenSSH_4.5p1, OpenSSL 0.9.8d 28 Sep 2006" to
  "OpenSSH_4.5p1, OpenSSL 0.9.8e 23 Feb 2007" using "aes256-ctr" fail with
  "Bad packet length".  The same problem occurs when using PuTTY 0.59 against
  the newer server.
  PuTTY users have reported this problem too, with servers on both FreeBSD and
  Linux, and with OpenSSH versions back to 4.0.
In fact it was listed as closed and resolved by, uh, one Damien Miller :-).
[0] Meaing "bugs encountered while using Putty", not necessarily "bugs in
    Putty".

@_date: 2009-09-17 13:20:45
@_author: Peter Gutmann 
@_subject: Detecting attempts to decrypt with incorrect secret key in OWASP ESAPI 
The answer to that depends on whether you need to support an existing base of
crypto software and hardware.  Even though (in this case) it's a new standard,
it still requires support from the underlying crypto libraries.  If little or
none of those do AES-CMAC yet (I don't think Windows CryptoAPI does, only very
recent versions of OpenSSL do... it's not looking good) then you'd want to
stick with HMAC-SHA1.
(Forestalling the inevitable "but developers can implement AES-CMAC themselves from raw AES" that I'm sure someone will follow up with, the target audience for this is web application developers, not cryptographers, so you need to give them something that works as required out of the box).

@_date: 2009-09-17 17:20:33
@_author: Peter Gutmann 
@_subject: Detecting attempts to decrypt with incorrect secret key in OWASP ESAPI 
You'd be surprised at what JCE developers will implement just because they
can, and what therefore gets used by application developers.  I've seen RSA-CBC used on more than one occasion.
(No, that's not a typo, RSA in CBC mode.  The app developers wondered why it
was so slow).

@_date: 2009-09-18 16:27:04
@_author: Peter Gutmann 
@_subject: Bringing Tahoe ideas to HTTP 
Although the draft has expired, the concept lives on in various tools.  For
example DownThemAll for Firefox supports this.  There was some discussion
about including it into FF3, but then the draft was dropped and the FF support
never appeared, does anyone know what happened?
(The cynic in me would say "it's such a simple, straightforward, easy-to-
implement idea, of course it'll never be adopted when there are things like EV
certs to be pushed instead", but who knows...).

@_date: 2009-09-18 16:36:41
@_author: Peter Gutmann 
@_subject: [tahoe-dev] Bringing Tahoe ideas to HTTP 
You can extend this further to make it tolerant of key loss by embedding
multiple public keys and allowing a quorum of them to replace an existing key.
So say you have five keys, you can decide that any three of them can vote out
an existing key, allowing compromised keys to be replaced and existing keys to
be rolled over.  This creates a kind of fault-tolerant PKI which does away
with the need to have a CA vouch for key replacements, once you've got the
initial keys established (for example on first install) you can recover from
anything short of a total compromise, upgrade to larger keys sizes and hashes,
and so on.
See my previous post, there was an attempt made to do this in the past but it
never got anywhere.  It'd be interesting to hear the reasons why.

@_date: 2009-09-27 15:03:00
@_author: Peter Gutmann 
@_subject: Interesting way of protecting credit card data on untrusted hosts 
A Canadian company called SmartSwipe has come up with an interesting way to
protect credit card numbers from most man-in-the-browser attacks.  What they
do is install a Windows CSP (cryptographic service provider) that acts as a
proxy to an external mag-stripe reader with built-in crypto processing, so the
CSP on the host PC does nothing more than forward data to be encrypted out to
the external device.  There's also a browser plug-in that pre-populates the
credit-card field in web forms with a cookie.  When the page is sent to the
CSP for encryption for SSL, the software running on the reader recognises the
cookie in the web-form content, reads the card data via the mag-stripe reader,
inserts it into the web-form field, and returns the encrypted result to the
host PC to forward to the remote server.  As a result, the CC data is never
present on the host PC.
The downsides are obvious: not secure against phishing (which is a killer),
only works with MSIE because of the requirement for use of a CSP (although you
could do it with Firefox as well by creating a PKCS  soft-token), and not
secure against page-rewrite trojans which have the web page show one thing and
do another, but it's an interesting concept.  You can find a description of
the technology under the name Dynamic SSL(tm)(c)(p), a start point is:

@_date: 2010-08-01 23:08:23
@_author: Peter Gutmann 
@_subject: Five Theses on Security Protocols 
A number of CAs provide (very limited) warranty cover, but as you say it's
unclear that this provides any value because it's so locked down that it's
almost impossible to claim on it.  Does anyone know of someone actually
collecting on this?  Could an affected third party sue the cert owner who can
then claim against the CA to recover the loss?  Is there any way that a
relying party can actually make this work, or is the warranty cover more or
less just for show?

@_date: 2010-08-01 23:10:55
@_author: Peter Gutmann 
@_subject: Is this the first ever practically-deployed use of a threshold scheme? 
Thanks to all the folks who pointed out uses of m-of-n threshold schemes,
however all of them have been for the protection of one-off, very high-value
keys under highly controlled circumstances by trained personnel, does anyone
know of any significant use by J.Random luser?  I'm interested in this from a
usability point of view.
As a corollary, has anyone gone through the process of recovering a key from
shares held by different parties at some time after the point at which they
were created (e.g. six months later), the equivalent of running a restore from
your backups at some point to make sure that you can actually recover your
sysem from it?  How did it go?

@_date: 2010-08-01 23:20:51
@_author: Peter Gutmann 
@_subject: Five Theses on Security Protocols 
What's your threat model?  At the moment if I get a key from a PGP keyserver
for a random contact I have no way of authenticating it (it may be signed, but
I have no idea who the signers are), I just hope the key's the right one.  The
ability to receive email at the given address helps prove it's them, and the
ability to reply indicates proof of possession of the private key.
In this case if I want to know whether (say) a Verisign-issued cert is valid I
go to  and ask.  Sure, you can defeat this with a fair bit of
effort, but doing a live MITM on a random TCP connection from an arbitrary
user just doesn't scale as well as a spamming out a zillion phishing emails
and waiting for users to come to my botnet.  The point isn't to create a
perfect defence but to raise the bar sufficiently that attackers can no longer
profitably use it as an attack vector.
In any case for this specific case DNSSEC will be along any minute to save us
all, so we don't have to worry about it any more.

@_date: 2010-08-02 03:50:59
@_author: Peter Gutmann 
@_subject: Is this the first ever practically-deployed use of a threshold scheme? 
What they do will really depend on what their threat model is.  I suspect that
in this case their single biggest threat was "lack of display of sufficient
due diligence", thus all the security calisthenics (remember the 1990s Clipper
key escrow procedures, which involved things like having keys generated on a
laptop in a vault with the laptop optionally being destroyed afterwards, just
another type of security theatre to reassure users).  Compare that with the
former mechanism for backing up the Thawte root key, which was to keep it on a
floppy disk in Mark Shuttleworth's sock drawer because no-one would ever look
for it there.  Another example of this is the transport of an 1894-S dime
(worth just under 2 million dollars) across the US, which was achieved by
having someone dress in somewhat grubby clothes and fly across the country in
cattle class with the slabbed coin in his pocket, because no-one would imagine
that some random passenger on a random flight would be carrying a ~$2M coin.
So as this becomes more and more routine I suspect the accompanying
calisthenics will become less impressive.
(What would you do with the DNSSEC root key if you had it?  There are many vastly easier attack vectors to exploit than trying to use it, and even if you did go to the effort of employing it, it'd be obvious what was going on as soon as you used it and your fake signed data started appearing, c.f. the recent Realtek and JMicron key issues.  So the only real threat from its loss seems to be acute embarassment for the people involved, thus the due-diligence

@_date: 2010-08-02 18:30:10
@_author: Peter Gutmann 
@_subject: Is this the first ever practically-deployed use of a threshold scheme? 
There's a *huge* difference, see my previous posting on this the last time the topic came up,  at metzdowd.com/msg07671.html:
  the cognitive load imposed is just so high that most users can't cope with   it, particularly since they're already walking on eggshells because they're   working on hardware designed to fail closed (i.e. lock everything out) if   you as much as look at it funny.
The last time I went through this exercise for a high-value key, after quite some time going through the various implications, by unanimous agreement we went with "lock an encrypted copy in two different safes" (this was for an organisation with a lot of experience with physical security, and their threat assessment was that anyone who could compromise their physical security would do far more interesting things with the capability than stealing a key).
For the case of DNSSEC, what would happen if the key was lost?  There'd be a bit of turmoil as a new key appeared and maybe some egg-on-face at ICANN, but it's not like commercial PKI with certs with 40-year lifetimes hardcoded into every browser on the planet is it?  Presumably there's some mechanism for getting the root (pubic) key distributed to existing implementations, could this be used to roll over the root or is it still a manual config process for each server/resolver?  How *is* the bootstrap actually done, presumably you need to go from "no certs in resolvers" to "certs in resolvers" through some

@_date: 2010-08-03 04:21:12
@_author: Peter Gutmann 
@_subject: Is this the first ever practically-deployed use of a threshold scheme? 
That's a good start, but it gets a bit more complicated than that in practice
because you've got multiple components, and a basic red light/green light
system doesn't really provide enough feedback on what's going on.  What you'd
need in practice is (at least) some sort of counter to indicate how many
shares are still outstanding to recreate the secret ("We still need two more
shares, I guess we'll have to call Bob in from Bratislava after all").  Also
the UI for recreating shares if one gets lost gets tricky, depending on how
much metadata you can assume if a share is lost (e.g. "We've lost share 5 of
7" vs. "We've lost one of the seven shares"), and suddenly you get a bit
beyond what the UI of an HSM is capable of dealing with.
With a two-share XOR it's much simpler, two red LEDs that turn green when the
share is added, and you're done.  One share is denoted 'A' and the other is
denoted 'B', that should be enough for the shareholder to remember.
If you really wanted to be rigorous about this you could apply the same sort
of analysis that was used for weak/stronglinks and unique signal generators to
see where your possible failure points lie.  I'm not sure if anyone's ever
done this [0], or whether it's just "build in enough redundancy that we should
be OK".
[0] OK, I can imagine scenarios where it's quite probably been done, but
    anyone involved in the work is unlikely to be allowed to talk about it.

@_date: 2010-08-04 01:42:15
@_author: Peter Gutmann 
@_subject: Using file-hiding rootkits for good 
I recently came across an example of a file-hiding rootkit for Windows that's
used for good instead of evil: It's a minifilter that hides (or at least
blocks, the files are still visible) access to executables on removable media,
with user-configurable options to block autorun.inf and/or all executables, as
well as making files on the media non-executable (although you could still map
them into memory and then execute them from there if you really wanted to).
This is a neat idea, since it stops a pile of exploits that take advantage of
the autorun capability.  More at

@_date: 2010-08-05 17:37:54
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI, Part II 
It's a bogus argument, the text says:
  He took a legitimate software package and removed the signature of the
  digital certificate it contained, then installed the package on his
  computer. The Installer application didn't indicate that the certificate had
  been modified.
The certificate wasn't modified, they just stripped the signature from the
  "Only an expert will be able to detect a problem," Schouwenberg said. "And
  all Microsoft will tell you is that the file is not signed."
And what else should Windows say?  "We put this through our time machine and
noticed that at some time in the past it was signed and now it isn't"?
The rest of the story isn't much better:
  The Stuxnet worm, which surfaced last month, used fake Verisign digital
  certificates
No, they were genuine certs, just in the wrong hands.

@_date: 2010-08-05 18:29:49
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI, Part II 
Uhh. minor nitpick, it was Jerome K.Jerome who wrote "Three Shares in a Boat". He followed it up with "Three Certificates on the Bummel", a reference to the sharing of commercial vendors' code-signing keys with malware authors.

@_date: 2010-08-05 21:24:07
@_author: Peter Gutmann 
@_subject: Preventing a recurrence of the Realtek/JMicron fiasco 
I've been having an off-list discussion with someone about how you'd prevent
the recent Realtek/JMicron certificate fiasco.  My thoughts on this:
  Since many development shops see the signing process as nothing more than an
  annoying speed-bump that stands in the way of application deployment, not
  helped by the fact that code-signing tools like Windows SignTool and Unix'
  GPG are hard to use and poorly integrated into the development process,
  developers have generally used the most expedient means possible to sign
  their code, with signing keys left unprotected or with easy-to-guess
  passwords (trivial variations of "password" are a favourite in web advice
  columns that give examples of how to do code signing [0]), or passwords
  hard-coded into the scripts that are needed in order to integrate the
  signing into the build process.  Combine this with the existence of entire
  families of malware such as Adrenalin, Nuklus/Apophis, Ursnif, and Zeus that
  integrate key-stealing functionality and it's inevitable that legitimate
  code-signing keys will end up in the hands of malware authors.
  [0] "p at ssw0rd" is the "password1" of code signing.
So my advice would be to keep the signing key on a dedicated, non-network-
connected machine that takes to-be-signed input from a USB drive with autorun
turned off (or, better, Didier Stevens' USB-protection driver installed,
 and sign that.  For test
purposes during development you can always sign with test keys, and then only
sign the final release once it's passed QA.  Even if you don't want to go that
far, just getting rid of the current worst practice would be a start, where
code-signing keys are just random data to be copied onto every developer's
machine with no password or a fixed password coded into batch files.
Potential issues/discussion topics:
- The signing tools should include a test key along the lines of the EICAR
  test virus sig. that's included by default and recognised everywhere as
  being purely a test key, to create a zero-overhead way of leaping the
  signing hurdle during development.
- As an extension of the above, the development environment should have some
  checkbox option to test-sign debug builds of binaries so developers don't
  have to google + cut&paste obscure command-line strings and batch files into
  equally obscure config dialogs in their IDE.
- Developers may need to repeatedly sign test releases and beta releases.  How
  do you distinguish "signature for testing purposes" from "signature for live
  release"?  Pretty much anything you do, e.g. throw up a warning every time a
  test-signed version is run, is going to cause enough discomfort eventually
  that developers will go back to using the release key.
Any other issues that anyone can think of?

@_date: 2010-08-12 05:02:06
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI, Part II 
Only "in the making"?
Actually it's all relative, in Japan the Docomo folks turned off CRLs because
they found that even a relatively modest CRL (not just the DoD monsters)
presented a nice DoS when sent over cellular data links.  What happened was
that as the CRLs grew, performance got worse and worse as the phone downloaded
the CRL.  It took them quite some time to diagnose that they were being DoS'd
by their own PKI.

@_date: 2010-08-13 23:59:18
@_author: Peter Gutmann 
@_subject: Has there been a change in US banking regulations recently? 
As part of a thread on another list, I noticed that Bank of America, who until
recently didn't bother protecting the page where users are expected to enter
their credentials with anything more substantial than a GIF of a padlock, now
finally use HTTPS on their home page, and redirect HTTP to HTTPS (this only
took them, what, about ten years to get right?  Or is it fifteen?  When did
BofA first get a web presence?).  Wachovia now do it too.  And Citibank at
least redirect you to an HTTPS page.  And so does US Bank, after asking for
your ID.
What on earth happened?  Was there a change in banking regulations in the last
few months?

@_date: 2010-08-16 05:17:30
@_author: Peter Gutmann 
@_subject: Has there been a change in US banking regulations recently? 
Insecure against what?  Given the million [0] easier attack vectors against
web sites, which typically range from "trivial" all the way up to "relatively
easy", why would any rational attacker bother with factoring even a 1024-bit
key, with a difficulty level of "quite hard"?  It's not as if these keys have
to remain secure for decades, since the 12-month CA billing cycle means that
you have to refresh them every year anyway.  Given both the state of PKI and
the practical nonexistence of attacks on crypto of any strength because it's
not worth the bother, would the attackers even notice if you used a 32-bit RSA
key?  How would an adversary effectively scale and monetise an attack based on
being able to break an RSA key, even if it was at close to zero cost?
The unfortunate effect of such fashion-statement crypto recommendations as
"you must use 2K bit keys, regardless of the threat environment" is that what
it actually says is "you must not use SSL on your web site".  "Le mieux est
l'ennemi du bien" strikes again.
[0] Figure exaggerated slightly for effect.

@_date: 2010-08-17 20:20:52
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI, Part II 
A quick followup note on this, I was reading Microsoft's code-signing best
practices document and one comment caught my eye:
  If code is signed automatically as part of a build process, it is highly
  recommended that any code that is submitted to that build process be
  strongly authenticated.
Given that Realtek produce huge numbers of products and therefore even larger
numbers of drivers for different environments (and JMicron probably not much
less so), it's likely that they use a highly automated driver-signing process
to deal with this.  Even if they use "strong authentication" as per the
guidelines, for example making the network share on the company-wide signing
server non-public (i.e. requiring Windows domain authentication), this means
that anyone who compromises any PC on the network can now use the code-signing
server as an oracle.  So there may have been no need to steal the key, just
compromise one developer PC and you can get your malware signed.  So perhaps a
corollary requirement might be:
  Your code-signing system should create a tamper-resistant audit trail [0] of
  every signature applied and what it's applied to.
[0] By this I don't mean the usual cryptographic Rube-Goldbergery, just log
    the details to a separate server with a two-phase commit protocol to     minimise the chances of creation of phantom non-logged signatures.

@_date: 2010-08-18 03:46:50
@_author: Peter Gutmann 
@_subject: About that "Mighty Fortress"...  What's it look like? 
What about them?
(Have you ever seen a PKI or similar key-using design where anyone involved in
speccing or deploying it genuinely cares about privacy implications?  Not only
have I never seen one, I've even been to a talk at a conference where someone
was criticised for wasting time on privacy concerns).
In any case if it really is a concern, there are any number of ways of
blinding or masking what's going on.

@_date: 2010-08-24 03:11:13
@_author: Peter Gutmann 
@_subject: Fw: [IP] Malware kills 154 
"Perry E. Metzger"  forwards:
Sigh, yet another attempt to use the "dog ate my homework" of computer
problems, if their fly-by-wire was Windows XP then they had bigger things to
worry about than malware.

@_date: 2010-08-27 19:20:06
@_author: Peter Gutmann 
@_subject: questions about RNGs and FIPS 140 
No.  If you choose your eval lab carefully you can sneak in a TRNG somewhere
as input to your PRNG, but you can't get a TRNG certified, and if you're
unlucky you won't be allowed to use a TRNG at all.
That's the sensible way of doing it, but will probably be disallowed by the
FIPS lab.  In my case I slipped one in through (a) careful choice of lab and
(b) defining the date-time vector DT to be "a hash of the date and time and
miscellaneous other information" where "hash" was "PRF" and "other
information" was the actual entropy input.  YMMV based on lab, evaluator,
phase of the moon, and hash of the date and time.

@_date: 2010-08-28 18:29:12
@_author: Peter Gutmann 
@_subject: questions about RNGs and FIPS 140 
As a general rule for FIPS 140, *anything* can be a problem at *any* lab. This case seems to be particularly ambiguous, with labs interepreting it in a variety of different ways (this is both from evals I've been part of and from talking to other people who've had stuff evaluated).  For example the OpenSSL guys had to remove fork-protection from their RNG at the request of the lab.  I didn't, but that's because I didn't document it as being present, and if they don't read about it they can't object to it.
(It's kind of depressing that engineering a properly secure system requires
gaming the arbitrary requirements in the certification process).

@_date: 2010-08-28 18:39:42
@_author: Peter Gutmann 
@_subject: questions about RNGs and FIPS 140 
Interesting that you should mention this, I was having a debate earlier today on the use of DLP/ECDLP-based cryptosystems vs. RSA in embedded devices.  My argument was that DLP, and particularly ECDLP, looked good on paper but in practice were quite dangerous because the lack of entropy on the very limited systems that they're being pushed for makes it risky to use them there.  So far of the three ECDLP-using embedded devices I've been able to look at in detail, all three failed to use proper entropy where required and one definitely and the other two probably didn't check the key parameters as required either.
(Cue debate on ECC vs. RSA :-).

@_date: 2010-08-28 19:01:18
@_author: Peter Gutmann 
@_subject: questions about RNGs and FIPS 140 
What matters to someone getting something evaluated isn't what NIST thinks or
what one person's interpretation of the standard says, but what the lab does
and doesn't allow.  Since what I reported is based on actual evaluations
(rather than what NIST thinks), how can it be "factually incorrect"?
Yup, and if you look at some of the generators you'll see things like the use
of a date-and-time vector DT in the X9.17/X9.30 generator, which was the
specific example I gave earlier of sneaking in seeding via the date-and-time.
Unfortunately one lab caught that and required that the DT vector really be a
date and time, specifically the 64-bit big-endian output of time(), the
Security 101 counterexample for how to seed an RNG.
In summary it doesn't matter what the standard says, it matters what the labs
require, and that can be (a) often arbitrary and (b) contrary to what would
generally be regarded as good security practice.

@_date: 2010-08-29 18:40:46
@_author: Peter Gutmann 
@_subject: questions about RNGs and FIPS 140 
Well, at least in your opinion it is :-).  And this illustrates the problem
here, just from the small number of contributors to this thread (including
some off-list ones) we've already had a whole pile of different opinions on
how to apply the PRNGs, and as with the labs there's quite some leeway in the
interpretations.  The problem is that the labs take the most conservative,
restrictive interpretation possible for CYA purposes while the people on here
take the best security-engineering interpretation.  The CYA approach may be
safe in terms of making it hard to challenge a ruling afterwards, but it's not
the best way to engineer a secure device or system.

@_date: 2010-07-10 19:06:32
@_author: Peter Gutmann 
@_subject: Question w.r.t. AES-CBC IV 
Unfortunately CTR mode, being a stream cipher, fails completely if the
IV's/keys aren't fresh (as you could force them to be for SRTP under SIP by
attacking the crypto handshake that preceded it, a nice example of attacking
across a protocol boundary, taking advantage of a weakness in one protocol to
break a second), while CBC only becomes a bit less secure.  In addition CTR
mode fails trivially to integrity attacks, while with CBC it's often more
obvious (you get at least some total corruption before the self-healing takes
The problem with CTR is that, like RC4, it's very brittle, make a tiny mistake
anywhere and you're toast.

@_date: 2010-07-13 03:58:51
@_author: Peter Gutmann 
@_subject: Intel to also add RNG 
You mean rampant paranoia from a small group of people... if you are genuinely
worried about this, just use it as another input to mix into your entropy pool
(which you should be doing anyway, never trust a single source of entropy).
I'd be quite happy to use the RNG on a Loongson CPU (if there was one) in this
manner, let alone an Intel CPU.
What killed it wasn't paranoia about Intel but their almost total lack of
interest in supporting it once the initial media attention waned.  This
doesn't look any different, note that it's not saying "This will be in Core2's
starting August" but "We've done this in the lab".

@_date: 2010-07-13 17:46:36
@_author: Peter Gutmann 
@_subject: Intel to also add RNG 
Do you have any more details on this?  Was it a hardware problem, software
problem, ...?  How was it caught?

@_date: 2010-07-13 17:53:31
@_author: Peter Gutmann 
@_subject: Fwd: Anyone make any sense out of this skype hack announcement? 
No need to apologise, it's a damn good read.  For people not familiar with it, the title is a bit misleading (it sounds like a book about malware), it's actually a book on software obfuscation and tamperproofing, IMHO it's the definitive reference on the topic.

@_date: 2010-07-22 19:48:24
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI 
Readers are cordially invited to go to  and have a look at the subjectAltName extension in the certificate that it presents.  An extract is shown at the end of this message, this is just one example of many like it.  I'm not picking on Edgecast specifically, I just used this one because it's the most Sybilly certificate I've ever seen.  You'll find that this one Sybil certificate, among its hundred-and-seven hostnames, includes everything from Mozilla, Experian, the French postal service, TRUSTe, and the Information Systems Audit and Control Association (ISACA), through to Chainlove, Bonktown, and Dickies Girl (which aren't nearly as titillating as they sound, and QuiteSFW).  Still, who needs to compromise a CA when you have these things floating around on multihomed hosts and CDNs.
Ian Grigg pointed out that this is also an EV certificate, I'm guessing that CDNs and multihomed hosts run into the same system-high problem that dogged MLS systems in the 1980s, they have to use the certificate at the highest level of any of the constituent domains.  So if you compromise (say) inpath-static.iseatz.com (which consists of a page that says "We're sorry, but something went wrong") or images.vrbo.com ("Directory Listing Denied") then you have an EV-validated site.  So the overall EV security becomes that of the least secure co-hosted domain.
I've tried connecting to the above site with HTTPS and get a normal non-EV Sybil certificate even though it's rooted in an EV CA... well, pseudo-rooted, the "root" is then signed by an old Entrust certificate, and the certificate itself is another multi-domain one, for Delta, Amtrak, Air France, KLM, Alaska Air, and others.  I wonder if they have some context-specific way to override EV on a per-site basis when it's used with Sybil certificates?  At the moment it's rather hard to test because depending on where you are in the world you get different views of servers and certificates (for example when I connect to ISACA, which is an EV site, I get a standard non-Sybil certificate that's only valid for ISACA), and finding a particular hostname in a Sybil certificate doesn't mean that you'll see that particular certificate when you connect to the server.
(Again, not wanting to pick on ISACA here, but finding a security audit organisation sharing a certificate with Dickies Girl is kinda funny.  You'd think there'd be a security audit process to catch this :-).
What a mess!  A single XSS/XSRF/XS* attack, or just a plain config problem,
and the whole house of cards comes down.
(For the TLS folks, SNI (a client-supplied Server Name Indication when it  connects) won't fix this because (a) it's not widely-enough supported yet and  (b) the server admin would have to buy 107 separate certificates to do the  job that's currently done by one Sybil certificate, and then repeat this for  every other Sybil certificate they use).
 666 2633:         SEQUENCE {
 670    3:           OBJECT IDENTIFIER subjectAltName (2 5 29 17)
 675 2624:           OCTET STRING, encapsulates {
 679 2620:             SEQUENCE {
 683   15:               [2] 'edgecastcdn.net'
 700   18:               [2] 'ne.edgecastcdn.net'
 720   21:               [2] 'minitab.fileburst.com'
 743   30:               [2] 'cdn.montimbrenligne.laposte.fr'
 775   27:               [2] 'zeroknowledge.fileburst.com'
 804   23:               [2] 'images.goldstarbeta.com'
 829   25:               [2] 'radialpoint.fileburst.com'
 856   19:               [2] 'wac.edgecastcdn.net'
 877   22:               [2] 'ne.wac.edgecastcdn.net'
 901   19:               [2] 'images.goldstar.com'
 922   15:               [2] 'images.vrbo.com'
 939   12:               [2] 'cdn.vrbo.com'
 953   18:               [2] 'content.truste.com'
 973   13:               [2] 'e1.boxcdn.net'
 988   13:               [2] 'e2.boxcdn.net'
1003   13:               [2] 'e3.boxcdn.net'
1018   25:               [2] 'privacy-policy.truste.com'
1045   13:               [2] '
1060   19:               [2] '
1081   26:               [2] 'static-cache.tp-global.net'
1109   29:               [2] 'images.homeawayrealestate.com'
1140   14:               [2] 'cdn.verint.com'
1156   13:               [2] 'swf.mixpo.com'
1171   21:               [2] 'cdn.traceregister.com'
1194   14:               [2] 's.tmocache.com'
1210   17:               [2] 's.my.tmocache.com'
1229   23:               [2] 'ne1.wpc.edgecastcdn.net'
1254   23:               [2] 'gp1.wpc.edgecastcdn.net'
1279   23:               [2] 'gs1.wpc.edgecastcdn.net'
1304   23:               [2] 'ne1.wac.edgecastcdn.net'
1329   23:               [2] 'gp1.wac.edgecastcdn.net'
1354   23:               [2] 'gs1.wac.edgecastcdn.net'
1379   24:               [2] 'c1.socialcastcontent.com'
1405   21:               [2] '
1428   22:               [2] '
1452   17:               [2] '
1471   16:               [2] '
1489   16:               [2] '
1507   16:               [2] '
1525   15:               [2] '
1542   22:               [2] 'resources.homeaway.com'
1566   21:               [2] 'ssl-cdn.sometrics.com'
1589   35:               [2] 'cache.vehicleassets.captivelead.com'
1626   17:               [2] 'static.woopra.com'
1645   20:               [2] 'images.cardstore.com'
1667   15:               [2] 'images.ink2.com'
1684   32:               [2] 'resources.homeawayrealestate.com'
1718   18:               [2] 'cdn1.adadvisor.net'
1738   24:               [2] '
1764   26:               [2] 'images.vacationrentals.com'
1792   34:               [2] 'serviceportal.carestreamhealth.com'
1828   23:               [2] 'assets-secure.razoo.com'
1853   29:               [2] 'resources.vacationrentals.com'
1884   23:               [2] 'download.entraction.com'
1909   12:               [2] 'ec.pond5.com'
1923   21:               [2] 'images.esellerpro.com'
1946   15:               [2] 'use.typekit.com'

@_date: 2010-07-23 23:25:26
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI 
handle certs provide a brief summary for the list?  From looking at Sybil
certs grabbed from a few CDN sites there doesn't seem to be any rhyme or
reason to them.  Also, how and under what conditions can you get access to the
CDN as an insider?  I'd found ads like
 "Resell the Edgecast
CDN and make a killing!", which seem to imply that anyone with a chequebook
can play.

@_date: 2010-07-24 01:36:55
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI 
Looks like the CDN certificate is already causing security problems, although
not the kind that I was expecting:
  While trying to import a server certificate for a CDN service, a segv bug
  was found in [PKI app].  It is likely that this bug is exploitable by
  sending a special crafted signed message and having a user verify the
  signature.
Hmm, I wonder if this particular certificate happened to be one with 107
subjectAltName entries?
  Description
  Importing a certificate with more than 98 Subject Alternate Names via import
  command or implicitly while verifying a signature causes [...].
Yup :-).  So if nothing else it's a good stress test for your certificate-
parsing code...

@_date: 2010-07-25 05:55:22
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI, Part II 
Have you ever wondered what would happen if malware started appearing that was
authenticated by signing keys belonging to major hardware or software vendors?
Over the last week or two we've had a chance to find out:
One of the scariest scenarios for code signing is when the malware authors
manage to get their hands on a legitimate developer's code-signing key.  Since
many development shops see the signing process as nothing more than an
annoying speedbump that stands in the way of application deployment, not
helped by the fact that code-signing tools like Windows SignTool and Unix' GPG
are hard to use and poorly integrated into the development process, developers
have generally used the most expedient means possible to sign their code, with
signing keys left unprotected or with easy-to-guess passwords ("password" is a
favourite in web advice columns that give examples of how to do code signing),
or passwords hard-coded into the scripts that are needed in order to integrate
the signing into the build process.  Combine this with the existence of entire
families of malware such as Adrenalin, Nuklus/Apophis, Ursnif, and Zeus, with
key-stealing functionality and it's inevitable that legitimate code-signing
keys will end up in the hands of malware authors.
The most serious case of this occurred in mid-2010 when malware signed with a
key belonging to the major semiconductor manufacturer Realtek started to
appear ["Rootkit.TmpHider", discussion thread, 12 July 2010,
 Malware Used
Valid Realtek Certificate", Lucian Constantin, 16 July 2010,
Certificate-147942.shtml.].  Although PKI dogma states that a certificate
belonging to such a key should be immediately revoked, the fact that vast
numbers of Realtek drivers had already been signed by it and could now no
longer be installed without unsigned-driver warnings or, in the case of 64-bit
Windows, used at all, would no doubt have given both Realtek and the issuing
CA cause for concern.  After several days the certificate was revoked
["VeriSign working to mitigate Stuxnet digital signature theft", Steve Ragan,
21 July 2010,
mitigate-Stuxnet-digital-signature-theft.], although the CA had to wait until
the story started to appear in news reports before they became aware of the
need for the revocation.  The decision to revoke the certificate was probably
influenced by a combination of the fact that the majority of users will simply
click through a driver install warning and that the hit-and-miss nature of
revocation checking meant that many systems would keep on using the
certificate regardless (if the certificate had only been used to sign 32-bit
code so that the worst that could happen was a warning dialog on install for
users to click past then this would have made the decision to revoke even
easier).  In any case since antivirus vendors had added the malware signature
to their scanners as soon as it was discovered, the revocation likely had
little actual effect in protecting users from harm.
A few days later a new version of the malware appeared, this time signed with
a key from another semiconductor manufacturer, JMicron ["Win32/Stuxnet Signed
Binaries", Pierre-Marc Bureau, 19 July 2010,
 Signed
Stuxnet Binary", Sean Sullivan, 20 July 2010,
 Stuxnet-Related
Malware Signed Using Certificate from JMicron", Lucian Constantin, 20 July
Certificate-from-JMicron-148213.shtml.].  Making the debacle even more
entertaining was the fact that one of the principal systems targeted by the
malware is a Siemens SCADA (industrial control) system that uses a hardcoded
password "2WSXcder" that can't be changed because doing so causes the system
to stop working ["Siemens warns users: Don't change passwords after worm
attack", Robert McMillan, 20 July 2010,
passwords-after-worm-attack-915.] and that had been circulating on the
Internet for years, including being posted to a Siemens online forum in Russia
["SCADA System.s Hard-Coded Password Circulated Online for Years", Kim Zetter,
19 July 2010,  and
online lists of default passwords ["default password list",
 (the reason for this poor
level of security is that SCADA systems rate availability above everything
else, so that anything that affects, or potentially affects, security is
strongly avoided.  In addition SCADA systems often use thoroughly out-of-date
hardware and software that no-one wants to change for fear of breaking things
and for which there's no way to schedule downtime even if someone did decide
to take the momentous step of upgrading them, and that are administered by
control engineers rather than computer geeks, none of which create an
environment that's conducive to strong security, or often any, security).
General questions:
- How many 64-bit systems would the revocation have potentially bricked?
  (JMicron make drive controllers, being unable to load the driver for your
  storage device could be... fatal).
- If the sig-check fails for a critical system component, what does Windows
  do?  That is, the driver itself is OK (the hash verifies) but the signature
  can't be verified, do you boot with the unverified driver or brick the
  machine?
- Were the keys only used to sign 32-bit drivers, or 64-bit ones as well?
- PKI dogma doesn't even consider availability issues but expects the
  straightforward execution of the condition "problem -> revoke cert".  For a
  situation like this, particularly if the cert was used to sign 64-bit
  drivers, I wouldn't have revoked because the global damage caused by that is
  potentially much larger than the relatively small-scale damage caused by the
  malware.  So alongside "too big to fail" we now have "too widely-used to
  revoke".  Is anyone running x64 Windows with revocation checking enabled and
  drivers signed by the Realtek or JMicron certs?

@_date: 2010-07-28 02:11:15
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI 
This is kind of a long message to reply to so I'll just post a meta-reply to
avoid getting bogged down in nitpicking, the message, as the subject line
indicated, was intended to start a discussion on some of the weaknesses
inherent in the SSL and commercial PKI model.  I consciously worded it to
avoid mentioning any CA names, and only mentioned Edgecast because it was
impossible not to (I had to provide a URL for the cert), and even then
included a disclaimer that it wasn't a criticism of Edgecast.  I actually
agree with a lot of the points made in the response, since this wasn't a
failing of Edgecast or a CA but a problem in the way SSL's PKI (or more
generally just PKI as a whole) works.  Because it was designed for the
purposes of authenticating a single user to the global X.500 directory it
really doesn't have any provision for Sybil certs (I'm going to keep calling
them that because we need some sort of label for them :-).
The intent with posting it to the list was to get input from a collection of
crypto-savvy people on what could be done.  The issue had previously been
discussed on a (very small) private list, and one of the members suggested I
post it to the cryptography list to get more input from people.  The follow-up
message (the "Part II" one) is in a similar vein, a summary of a problem and
then some starters for a discussion on what the issues might be.
So a general response to the several "well, what would you do?" questions is
"I'm not sure, that's why I posted this to the list".  For example should an
SSL cert be held to higher standards than the server it's hosted on?  In other
words if it's easier to compromise a CDN host or (far more likely) a web app
on it, does it matter if you're using a Sybil cert?  I have no idea, and I'm
open to arguments for and against.
I'm not actually sure what the "fix" would be for this, or even if there is a
fix that needs to be made.  Thus the hope to get it discussed on the list.
(Oh, and a comment on the XS* bit, that was based on an earlier off-list
discussion on messing with Firefox' same-origin policy protection mechanism
and isn't relevant here, the real issue is the more obvious one of a single
cert acting for large numbers of totally unrelated domains with very different
security requirements).

@_date: 2010-07-28 02:23:07
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI 
... or talking to PKI standards groups about adding a CRL reason code for
"certificate issued in error" (e.g. to an imposter).  This was turned down
because CA's never make mistakes, so there's no need to have such a reason

@_date: 2010-07-28 20:57:49
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI, Part II 
This wouldn't make any difference, except for the special case of x64, signatures are only verified on install, so existing installed code isn't affected and anything new that's being installed is, with either form of In any case though the whole thing is really a moot point given the sucking void that is revocation-handling, the Realtek certificate was revoked on the 16th but one of my spies has informed me that as of yesterday it was still regarded as valid by Windows.  Previous experience with revoked certs has been that they remain valid more or less indefinitely (which would be really great if CAs offered something like domain-tasting for certs, you could get as many free certs as you wanted).
The way to revoke a cert for signed malware is to wait 0-12 hours for the malware signature to be added to AV databases, not to actually expect PKI to

@_date: 2010-07-29 00:18:24
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI, Part II 
It's not just that the world doesn't work that way now, it's quite likely that it'll never work that way (for the case of PKI/revocations mentioned in the message, not the original SNI).  We've been waiting for between 20 and 30 years (depending on what you define as the start date) for PKI to start working, and your reponse seems to indicate that we should wait even harder.  If I look at the mechanisms we've got now, I can identify that commercial PKI isn't helping, and revocations aren't helping, and work around that.  I'm after effective practical solutions, not just "a solution exists, QED"

@_date: 2010-07-29 01:22:29
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI, Part II 
I think the problems go a bit further than just Realtek's motivation, if you look at the way it's supposed to work in all the PKI textbooks it's:
  Time t: Malware appears signed with a stolen key.
  Shortly after t: Realtek requests that the issuing CA revoke the cert.
  Shortly after t': CA revokes the cert.
  Shortly after t'': Signature is no longer regarded as valid.
What actually happened was:
  Time t: Malware appears signed with a stolen key.
  Shortly after t: Widespread (well, relatively) news coverage of the issue.
  Time t + 2-3 days: The issuing CA reads about the cert problem in the news.
  Time t + 4-5 days: The certificate is revoked by the CA.
  Time t + 2 weeks and counting: The certificate is regarded as still valid by
    the sig-checking software.
That's pretty much what you'd expect if you're familiar with the realities of PKI, but definitely not PKI's finest hour.  In addition you have:
  Time t - lots: Stuxnet malware appears (i.e. is noticed by people other than
    the victims)
  Shortly after t - lots: AV vendors add it to their AV databases and push out
    updates
(I don't know what "lots" is here, it seems to be anything from weeks to
months depending on which news reports you go with).
So if I'm looking for a defence against signed malware, it's not going to be PKI.  That was the point of my previous exchange with Ben, assume that PKI doesn't work and you won't be disappointed, and more importantly, you now have the freedom to design around it to try and find mechanisms that do work.

@_date: 2010-07-29 01:38:22
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI 
That's the key-continuity model, which has been proposed a number of times for Firefox, for example here's a discussion by a FF developer from over two years ago on this,  (that's specific to FF, I don't know what the IE, Opera, Safari, ... guys talk about).  There's no sign of it gaining any traction.
I hate to be the perpetual wet blanket here but the problem isn't a lack of ideas (many backed by extensive real-world research) but a lack of motivation in browsers to change the security mechanisms and UI, most of which have remained essentially unchanged (except for cosmetic rearrangement of the chrome every release or so) since the debut of SSL in 1995.  That's the mastodon in the room, we can debate ideas pretty much forever but if no browser vendor is interested in adopting any of them it isn't going to help secure users.
(Having said that, it's fun to throw around ideas, so I'm not complaining about that bit).

@_date: 2010-07-29 02:18:19
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI, Part II 
It depends on what you mean by revocation, traditional revocation in the PKI sense isn't needed because (well apart from the fact that it doesn't work, you can't un-say something after you've already said it) if you look at what a PK or a cert is, it's just a capability, and the way to revoke (in the capability sense) a capability is to do something like rename the object that the capability refers to or use a level of indirection and break the link when you want to revoke (in the capability sense) the access.  This means that no matter how many copies of the capability are floating around out there and whether the relying party checks CRLs or not, they're not going to be able to get in.
Which purpose?  If you mean securing the distribution channel for binaries, here's a very simple one that doesn't need PK at all, it's called a self-authenticating URL.  To use it you go to your software site, and here's a real-world example that uses it, the Python Package Index, and click on a download link, something like  (yeah, I know, it uses MD5...).  This link can point anywhere, because the trusted source of the link includes a hash of the binary (and in this case it's a non-HTTPS source, you can salt and pepper it as required, for exammple make it an HTTPS link and use key continuity to manage the cert).  In this form the concept is called link fingerprints, it was actually implemented for Firefox as a Google Summer of Code project, but then removed again over concerns that if it was present people might actually use it (!!).  It's still available in DL managers like Another option is to cryptographically bind the key to the URL, so you again have a trusted link source for your download and the link is to  where  is a fingerprint of the cert you get when you connect to the site.  This does away with the need for a CA,
because the link itself authenticates the cert that's used.
Then there are other variations, cryptographically generated addresses, ... all sorts of things have been proposed.
The killer, again, is the refusal of any browser vendor to adopt any of it.  In the case of FF someone actually wrote the code for them, and it was rejected.  Without support from browser vendors, it doesn't matter what cool
ideas people come up with, it's never going to get any better.

@_date: 2010-07-29 03:51:33
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI, Part II 
OCSP only appears to work in that manner.  Since OCSP was designed to be 100% bug-compatible with CRLs, it's really an OCQP (online CRL query protocol) and not an OCSP.  Specifically, if I submit a freshly-issued, valid certificate to an OCSP responder and ask "is this a valid certificate" then it can't say yes, and if I submit an Excel spreadsheet to an OCSP responder and ask "is this a valid certificate" then it can't say no.  It takes quite some effort to design an online certificate status protocol that's that broken.
(For people not familiar with OCSP, it can't say "yes" because a CRL can't say "yes" either, all it can say is "not on the CRL", and it can't say "no" for the same reason, all it can say is "not on the CRL".  The ability to say "vslid certificate" or "not valid certificate" was explicitly excluded from OCSP because that's not how things are supposed to be done).

@_date: 2010-07-29 04:23:52
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI, Part II 
It's not an online certificate status protocol because it can provide neither
a yes or a no response to a query about the validity of a certificate.
(For an online status protocol I want to be able to submit a cert and get back
a straight valid/not valid response, exactly as I can for credit cards with
their authorised/declined response.  Banks were doing this twenty years ago
with creaky mainframes over X.25 and (quite probably) wet bits of string, but
we still can't do this today with multicore CPUs and gigabit links if we're
using OCSP).
They may be, or they may not be, but you as a relying party have no way of telling.  OCSP covers not only the three incompatible business models of the different authors' employers but, for good measure, an extra "anything else you may care to do" option if the first three aren't enough.  A decade after it was published, PKI experts are still arguing over what various bits of the OCSP spec actually mean (the PKIX list has only just gone through yet another round of this... *ten years* later and domain experts still can't agree on how it's supposed to work).  So given the schizophrenic nature of the RFC you can easily claim "but you can do X" because chances are if you read it just right you probably can.  Unfortunately this doesn't give a relying party much to rely on, because they have absolutely no idea what they're getting, it could be anything from a live database query to a value from a month-old CRL to (thanks to the removal of nonces from the protocol a few years ago) an attacker replaying an old "not-revoked" value (although I don't know why they'd even bother with that, given the state of revocation checking in client In any event though since OCSP can't say yes or no, it doesn't matter whether the response is coming from a live database or a month-old CRL, since it's still a fully CRL-bug-compatible blacklist I can trivially avoid it with a manufactured-cert attack.

@_date: 2010-07-30 19:34:32
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI 
The problem is that neither the browser vendor nor the users will see it like
this.  For the user its:
  "Do you want to have lots of sites that you normally use break?"
For the browser vendor its:
  "Do you want lots of your users to become frustrated when things stop
  working for them so that they switch to another browser?"
None of the non-IE browsers can afford to do this because people will just
switch back to IE, and this has been observed in usability testing of proposed
browser security features by HCI researchers, as soon as anything goes wrong
the users switch back to IE, which allows pretty much anything through.  You
can even get IE as a plugin for other browsers (shudder) in order to "make
things work".
So you'd need to get the change made in IE (or at least get it made in such a
manner that fallback-to-IE is no longer an option).  I don't know what size
hammer you'd need to wield in order to get that done.
It can't say "yes" because the only thing OCSP can say is "not revoked" (and
in more general terms the only thing a blacklist can say is "not on the
blacklist").  "Not revoked" doesn't mean "valid", it just means "not in the
"Unknown" is generally treated by client apps as "good", because if "revoked"
maps to "bad" then anything else must map to "good" (OCSP's muddle of non-
orthogonal response types is yet another perpetual motion-machine debate topic
among PKI people).
How can you do this?  Note that the various timestamps in OCSP responses are
as big a mess as the rest of OCSP, and can't be relied upon for any decision-
More importantly, how can you possibly make any meaningful decisions in time-
critical protocols based on a system for which your responses can have come
from any time in the past?  As one security architect commented some years
ago, "learning in 80ms that the certificate was good as of a week ago and to
not hope for fresher information for another week seems of limited, if any,
utility to us or our customers".
The problem here is best seen by looking at certificates as capabilities.
1. You have an abitrary and unknown number of capabilities floating around out
   there.
2. Some of those capabilities (CA certs) have the ability to mint new
   capabilities.
2a. These capabilities can impersonate existing capabilities, and because of
    (1) the real issuer of the capabilities has no idea that they exist.
And the means of dealing with these unknown numbers of arbitraily-identified
capabilities is... a blacklist.
There's no way this can possibly, ever work.  It's the 1960s credit-card model
that Perry mentioned with the added twist that there are an unknown number of
cards and issuers involved, and some of the cards can invent new cards
whenever they feel like it.

@_date: 2010-07-30 23:58:45
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI, Part II 
"Oh no my Lord, I assure you that parts of it are excellent!" :-).
That was the whole point, that the whole system doesn't work, and it's the system as a whole that has to work, not just some parts of it.  Here's another description, without the possibly confusing 't +/- x' stuff:
Shortly after the malware appeared (or at least got noticed) it was added to
anti-virus vendor databases and pushed out to users via updates.  Some time
later when it made headlines because of its use of the Realtek certificate,
the CA that had issued it read about it in the news, contacted the certificate
owner, and revoked it.  However due to the dysfunctional nature of revocation
handling, the certificate was still regarded as valid by Windows systems after
it had been revoked, and of a range of machines running Windows 7, Vista, and
XP, all with the latest updates and hotfixes applied and with automatic
updates turned on, the first machine to notice the revocation and treat the
signature as invalid didn't do so until a full week after the revocation had
occurred, and some machines still regard the signature as valid even now (I've
heard this before a number of times in software developer forums and mailing
lists, plaintive complaints from users to the effect that "I know this
certificate is revoked, but no matter what I do I can't get the software to
stop using it!").
So while PKI and code-signing promise the user a fairly deterministic series
of events in which A occurs, the B occurs, then C occurs, and then the user is
safe, what actually happens in practice is that A occurs, then a comedy of
errors ensues [0], and then the user is still unsafe while possibly being
under the mistaken impression that they're actually safe.
[0] I've never understood why this is a comedy of errors, it seems more like
    a tragedy of errors to me.
A real-world demonstration of the relative effectiveness of various protection
mechanisms occurred when I wanted to evaluate the ability of code-signing to
protect users.  A researcher sent me a copy of the signed malware (thanks!),
and because of its nature encrypted it with AES using the RAR archiver.
Because RAR (and indeed most other archivers) don't protect file metadata, the
message was blocked by email scanners that identified the overall contents
from the metadata even though the file contents themselves were encrypted.
After some discussion with the IT people ("yes, I am certain what the file is,
it's a rather nasty piece of Windows malware, and I trust the sender to have
sent me malware") they forwarded the email to the PowerPC Linux machine on
which I read email, and which is rather unlikely to be affected by x86 Windows
Unfortunately I never could check it on the Windows system that I wanted to
test it on because the instant it appeared on there the resident malware
protection activated and deleted it again, despite various attempts to bypass
the protection.  Eventually I got it onto a specially-configured Windows
system, which reported that both the signature and its accompanying
certificate were valid (this is now two weeks after the CA had declared the
certificate revoked).  So it actually proved quite difficult to see just how
ineffective PKI and code-signing actually was in protecting users from malware
because the real protection mechanisms were so effective at doing their job.
(It's also rather an eye-opener about the effectiveness, at least in some
cases, of malware-protection software, no matter what I did I couldn't get the
malware files onto a Windows PC in order to have the code-signing declare them
valid and, by implication, perfectly safe).
I'd say it's more than just a claim, the malware was first detected around 1
1/2 months ago and added to AV vendor databases, a full month later the
certificate was declared revoked by the CA, and currently the majority of
Windows systems still regard the signature as valid (I've had a report from
someone else of one machine that records it as revoked, so at least one
machine has been belatedly protected by the code signing, assuming the user
doesn't just click past the warning as pretty much all of them will).
So yes, I'd say the AV companies respond a helluva lot faster, and a helluva
lot more effectively. The bigger lesson, for people who ever believed this to
be the case, is "don't rely on code signing to protect you from malware".

@_date: 2010-07-31 18:44:12
@_author: Peter Gutmann 
@_subject: Is this the first ever practically-deployed use of a threshold scheme? 
Apparently the DNS root key is protected by what sounds like a five-of-seven
threshold scheme, but the description is a bit unclear.  Does anyone know
(Oh, and for people who want to quibble over "practically-deployed", I'm not
 aware of any real usage of threshold schemes for anything, at best you have
 combine-two-key-components (usually via XOR), but no serious use of real n-
 of-m that I've heard of.  Mind you, one single use doesn't necessarily count
 as "practically deployed" either).
Peter (who has two more Perry-DoS-ing conversation-starter posts to make, but
       will leave them for awhile now :-).

@_date: 2010-08-01 06:05:38
@_author: Peter Gutmann 
@_subject: Five Theses on Security Protocols 
Are we allowed to play peanut gallery for this?
Based on the ongoing discussion I've now had, both on-list and off, about
blacklist-based key validity checking [0], I would like to propose an
  The checking should follow the credit-card authorised/declined model, and
  not be based on blacklists (a.k.a. "the second dumbest idea in computer
  security", see
  (Oh yes, for a laugh, have a look at the X.509 approach to doing this.  It's
eighty-seven pages long, and that's not including the large number of other
RFCs that it includes by reference: This is, I suspect, the reason for the vehement opposition to any kind of
credit-card style validity checking of keys, if you were to introduce it, it
would make both certificates and the entities that issue them superfluous.
[0] It's kinda scary that it's taking this much debate to try and convince
    people that blacklists are not a valid means of dealing with arbitrarily
    delegatable capabilities.

@_date: 2010-10-03 14:05:11
@_author: Peter Gutmann 
@_subject: 'Padding Oracle' Crypto Attack Affects Millions of ASP.NET Apps 
You got there before I did - real-world studies of users have shown that a
common failure mode for this is that when users get their user name wrong they
then try every password they can think of under the assumption that they've
remembered the wrong password for the site.  So not only does not
distinguishing between incorrect username and incorrect password not help [0],
it actually makes things much, much worse by training users to enter every
password for every site they know.
[0] Well, it helps the attackers I guess...

@_date: 2010-10-06 16:52:46
@_author: Peter Gutmann 
@_subject: Formal notice given of rearrangement of deck chairs on RMS PKItanic 
December 31, 2010 - CAs should stop issuing intermediate and end-entity
  certificates from roots with RSA key sizes smaller than 2048 bits [0]. All
  CAs should stop issuing intermediate and end-entity certificates with RSA
  key size smaller than 2048 bits under any root.
  Under no circumstances should any party expect continued support for RSA key
  size smaller than 2048 bits past December 31, 2013. This date could get
  moved up substantially if necessary to keep our users safe. We recommend all
  parties involved in secure transactions on the web move away from 1024-bit
  moduli as soon as possible.
Right, because the problem with commercial PKI is all those attackers who are
factoring 1024-bit moduli, and apart from that every other bit of it works
[0] This is ambiguously worded, but it's talking about key sizes in EE certs.

@_date: 2010-10-07 16:29:41
@_author: Peter Gutmann 
@_subject: Formal notice given of rearrangement of deck chairs on RMS PKItanic 
End-entity certs, i.e. non-CA certs.  This means that potentially after the end of this year and definitely after 2013 it will not be possible to use any key shorted than 2048 bits with Firefox.  Anyone using, for example, an embedded device adminstered via SSL will have to use another browser.
move has been given pretty much zero thought beyond "we need to do what NIST

@_date: 2010-10-07 16:58:50
@_author: Peter Gutmann 
@_subject: What if you had a very good entropy source, but only practical at crypto engine installation time? 
Hmm, they're somewhat expensive... a cheaper alternative, even if they require a bit more manual effort, are these:
  (16-sided dice numbered 0...F, $1 each, although shipment outside the US is damn expensive).

@_date: 2010-10-07 17:05:16
@_author: Peter Gutmann 
@_subject: Formal notice given of rearrangement of deck chairs on RMS	PKItanic 
As I mentioned in my previous followup, it's badly worded, but the intent is to ban any keys < 2K bits of any kind (currently with evolving weasel-words about letting CAs certify them up to 2013 or so if the user begs really hard).

@_date: 2010-10-08 16:25:17
@_author: Peter Gutmann 
@_subject: Computer "health certificate" plan: Charney of DoJ/MS 
Before people get too far into conspiracy theories with this, I should point
out that health certificates have been part of corporate Windows environments
for years (I don't know how many exactly, I think it's been since at least
Server 2003).  The intent of health certs is that it allows the IT department
to manage PCs by allowing checks that they have the latest AV updates
installed, the corporate desktop background and Windows theme, the corporate
mail client in an up-to-date version, and so on.  In other words it's a
configuration management solution.  Think cfengine with certs.
In this case it looks like a MS spokesperson has decided that the existing
cfengine-with-certs approach used in corporate environments would work on an
ISP-wide or even nation-wide level.  It's no conspiracy theory, just a case of
either cluelessness about scaling issues or misreporting of a blue-sky, what-
if proposal.  I'd guess it's the latter.

@_date: 2010-09-14 23:14:36
@_author: Peter Gutmann 
@_subject: 'Padding Oracle' Crypto Attack Affects Millions of ASP.NET Apps 
=JeffH  quotes:
The earlier work is also pretty devastating against CAPTCHAs (as well as being
a damn good read, "Sudo make me a CAPTCHA" :-).  A great many CAPTCHAs work by
using a hidden form field containing the encrypted solution to the CAPTCHA,
which is then POSTed back to the server along with the client's solution (this
is needed to make the operation stateless).  If the decrypted version matches
what the client provides, they've solved the CAPTCHA.  So all an attacker has
to do is solve one CAPTCHA manually and then replay the encrypted version back
along with the solution as often as they like, you don't need to hire a
Pakistani Internet cafe any more for your CAPTCHA-breaking.  This destroys an
awful lot of CAPTCHAs, and isn't at all easy to fix because of the requirement
to keep it stateless.

@_date: 2010-09-15 00:47:09
@_author: Peter Gutmann 
@_subject: Debian encouraging use of 4096 bit RSA keys 
The one that says that if you wind things up past 11 (4096 bits), various
things break.
(D'you really think they applied any kind of security analysis to the choice
of key size?  They just wound it up until they got to 11, then declared that
the new key size).

@_date: 2010-09-15 04:03:36
@_author: Peter Gutmann 
@_subject: Intel plans crypto-walled-garden for x86 
Naah, we're perfectly safe, like every other similar attempt after 5-10 years
of effort and several hundred million dollars down the drain it'll come to
nothing.  I guess that's one silver lining of the corollary to "We can't
secure PCs against the bad guys", which is "We can't 'secure' them against
their owners either" (with the rider "... although we can cause a lot of cost
and inconvenience in trying").

@_date: 2010-09-15 16:07:24
@_author: Peter Gutmann 
@_subject: 'Padding Oracle' Crypto Attack Affects Millions of ASP.NET Apps 
I still haven't seen the paper/slides from the talk so it's a bit hard to
comment on the specifics, but if you're using .NET's FormsAuthenticationTicket
(for cookie-based auth, not viewstate protection) then you get MAC protection
built-in, along with other nice features like sliding cookie expiration (the
cookie expires relative to the last active use of the site rather than an
absolute time after it was set).  I've used it in the past as an example of
how to do cookie-based auth right

@_date: 2010-09-16 03:39:47
@_author: Peter Gutmann 
@_subject: A mighty fortress is our PKI, Part III 
Some more amusing anecdotes from the world of PKI:
- A standard type of fraud that's been around for awhile is for scammers to
  set up an online presence for a legit offline business, which appears to
  check out when someone tries to verify it.  A more recent variation on this
  is to buy certs for legit businesses.  One of these certs was traced back by
  a security researcher who found that the scammers had obtained it through
  the incredibly devious trick of shopping round commercial CAs until they
  found one that was prepared to sell them a certificate.
- In a repeat of the original race to the bottom with non-EV certs, CA's have
  issued EV certs for RFC 1918 addresses (!!!).  What makes this particularly
  entertaining is that in combination with a router warkitting attack and
  Moxie Marlinspike's OCSP faking it allows an attacker to spoof any EV-cert
  site.
- The list of people who have bought certificates for Apple from commercial
  CAs keeps on growing (I guess Microsoft is just so five minutes ago :-).
  For example one SMTP admin needed a cert for his server and wondered what
  would happen if he asked for one for *.apple.com instead of his actual
  domain name.  $100 and a cursory check later he had a wildcard cert for
  Apple.  At least two more users have reported buying certificates for Apple,
  and there are probably even more lurking out there - if you too have a
  certificate from a certificate vending machine saying that you're Apple, do
  get in touch
- There's malware out there that pokes fake Verisign certificates into the
  Windows trusted cert store, allowing the malware authors to be their own
  Verisign.
- CAs have issued certs to cybercrime web sites like
   (an affiliate program for malware
  installers), because hey, the Russian mafia's money is as good as anyone
  else's.
- One of the most important things a CA needs to manage is certificate serial
  numbers, because the combination { CA name, cert serial number } is a unique
  identifier used in lots of security protocols to identify certs.  Without
  this uniqueness, you can't tell who signed something, you can't revoke a
  cert, you can't... well, you get the idea.  Not only have commercial CAs
  issued certs with duplicate serial numbers, they've issued *CA certs* with
  duplicate serial numbers.  Ouch!
  (When this was pointed out to the CA who did this - "oops, my bad, we'll get
  those re-issued for you" - someone else pointed out that their OCSP
  responder certs had expired, which none of the CA's clients appeared to have
  noticed until then.  "Yeah, we'll look into fixing those too.  Anything else
  while we're at it?").
If anyone has any further amusing PKI stories, please get in touch, I'd love
to add a Part IV to this series.

@_date: 2010-09-17 02:29:50
@_author: Peter Gutmann 
@_subject: More on padding oracles 
Brian Holyfield has created another implementation of the padding oracle
exploitation tool first described by Juliano Rizzo and Thai Duong, as well as
providing a step-by-step, easy-to-understand explanation of how the attack
works, you can find it at:

@_date: 2010-09-17 20:53:51
@_author: Peter Gutmann 
@_subject: Something you have, something else you have, and, uh, something else you have 
Just had a new Lloyds credit card delivered, it had a sticker saying I have
  to call a number to activate it. I call, it's an automated system.
  It asks for the card number, fair enough. It asks for the expiry date, well
  maybe, It asks for my DOB, the only information that isn't actually on the
  card, but no big secret. And then it asks for the three-digit-security-code-
  on-the-back, well wtf?
  AIUI, and I may be wrong, the purpose of activation is to prevent lost-in-
  the-post theft/fraud - so what do they need details which a thief who has
  the card in his hot sweaty hand already knows for?
Looks like it's not just US banks whose interpretation of n-factor auth is "n
times as much 1-factor auth".

@_date: 2013-08-27 00:28:13
@_author: Peter Gutmann 
@_subject: [Cryptography] Using Raspberry Pis 
In that case why not just get an Alix embedded system,
 and drop pfSense, on it?  Someone else has already done all the work, all you need to do is
configure it however you want it.

@_date: 2013-08-27 00:28:54
@_author: Peter Gutmann 
@_subject: [Cryptography] Implementations, attacks on DHTs, Mix Nets? 
For people who are interested, the list I have (for a year or two back) is:
"Security Considerations for Peer-to-Peer Distributed Hash Tables", Emil Sit
and Robert Morris, Proceedings of the 1st International Workshop on Peer-to-
Peer Systems (IPTPS'01), Springer-Verlag LNCS No.2429, March 2002, p.261.
"A Survey of Peer-to-Peer Security Issues", Dan Wallach, Proceedings of the
2002 International Symposium on Software Security (ISSS'02), Springer-Verlag
LNCS No.2609, November 2002, p.42.
"Eclipse Attacks on Overlay Networks: Threats and Defenses", Atul Singh,
Tsuen-Wan Ngan, Peter Druschel and Dan Wallach, Proceedings of the 25th
International Conference on Computer Communications (INFOCOM'06), April 2006,
"The Index Poisoning Attack in P2P File Sharing Systems", Jian Liang, Naoum
Naoumov and Keith Ross, Proceedings of the 25th Conference on Computer
Communications (INFOCOM'06), April 2006,
"Conducting and Optimizing Eclipse Attacks in the Kad Peer-to-Peer Network",
Michael Kohnen, Mike Leske and Erwin Rathgeb, Proceedings of the 8th IFIP-TC 6
Networking Conference (Networking'09), Springer-Verlag LNCS No.5550, May 2009,
"Combating Index Poisoning in P2P File Sharing", Lingli Deng, Yeping He and
Ziyao Xu, Proceedings of the 3rd Conference and Workshops on Advances in
Information Security and Assurance (ISA'09), Springer-Verlag LNCS No.5576,
June 2009, p.358.
"Hashing it out in public: Common failure modes of DHT-based anonymity
schemes", Andrew Tran, Nicholas Hopper and Yongdae Kim, Proceedings of the 8th
Workshop on Privacy in the Electronic Society (WPES'09), November 2009, p.71.
"Poisoning the Kad Network", Thomas Locher, David Mysicka, Stefan Schmid and
Roger Wattenhofer, Proceedings of the 11th International Conference on
Distributed Computing and Networking (ICDCN'10), Springer-Verlag LNCS No.5935,
January 2010, p.195.
If there's anything significant I've missed, feel free to fill in the gaps.

@_date: 2013-12-03 20:56:03
@_author: Peter Gutmann 
@_subject: [Cryptography] Explaining PK to grandma 
They're not being used as examples of clueless users, they're representative
personas.  Geeks have a really bad problem of design-for-the-self, creating
software that's designed for people like themselves.  The best way to combat
this is through usability testing, except that few developers will ever do
that.  The next-best thing is to provide them with personas, a mental image of
someone not like themselves that they can identify with and imagine using
their software, which will in turn point out that it's unusable by any normal
human.  So using "your mother" or "granny" isn't stereotyping, it's taking a
user that they can readily identify with and applying them as a persona to see
if the software actually is usable.  The alternative is to go back to design-
for-the-self, which leads to fundamentally unusable software.

@_date: 2013-12-22 21:45:47
@_author: Peter Gutmann 
@_subject: [Cryptography] Why don't we protect passwords properly? 
It's pretty simple really.  Everyone knows that passwords are no good, so
there's no point in trying to use/apply/implement them properly.  Instead, we
need to move everyone to PKI^H^H^Hbiometrics^H^H^HOpenID^H^H^H^Hsmart
cards^H^H^H^HOAuth^H^H^H^HPassport^H^H^H^HCardSpace^H^H^H... and then all our
problems will be solved.  Any minute now.

@_date: 2013-12-23 10:38:13
@_author: Peter Gutmann 
@_subject: [Cryptography] RSA is dead. 
That won't help things much: Any sufficiently capable developer of crypto
software should be competent enought to backdoor their own source code in such
a way that it can't be detected by an audit.  If you're capable of dealing
with exotic side-channel and timing attacks, countering weird obscure
mathemtatical properties of cryptosystems to avoid leaking keys, and all
manner of other tricks, then you had better be capable of backdooring your
code as well.
Availability of source code is not soy sauce for security.

@_date: 2013-12-28 13:45:00
@_author: Peter Gutmann 
@_subject: [Cryptography] how reliably do audits spot backdoors? 
That's why you use   Someone else gets to manage to
dependency hell for you.
Linux in general seems to suffer from this.  It was always amusing seeing
Linux users make fun of Windows because of DLL hell (circa Windows 3.1), but
then go on to create their own depdency tartarus, malbolge, xibalba, naraka,
and sheol.  Open-source security tools seem to be particularly bad for this,
the amount of hacking you need to do to get something up and running often
isn't worth the effort.
Peter (yeah, I know, way off-topic).

@_date: 2013-12-28 22:03:45
@_author: Peter Gutmann 
@_subject: [Cryptography] Fwd: [IP] RSA Response to Media Claims Regarding 
The old COCOM restrictions were quaint anachronisms almost as soon as they
were issued.  I remember going through the catalogue of a large PC dealership
and realising that at least 50% of their entire stock, if not more, was
export-controlled, due to things like using chips with 208+ pins, graphics
capabilities above the permitted minimum (an S3 Virge was too powerful to
export), and so on and so forth.  Within a few years, 802.11 with its
DSSS/OFDM would have rendered anything with wireless capabilities
unexportable, and the presence of a basic Riva 128/3DFX Voodoo would have done
for the rest.

@_date: 2013-12-31 12:09:16
@_author: Peter Gutmann 
@_subject: [Cryptography] TAO, NSA crypto backdoor program 
A real-life supply-chain attack.  Very cool.  While I don't agree with what
they're doing, I'm damn impressed at the level they're doing it at.  The US
taxpayer is really getting their money's worth.

@_date: 2013-11-02 01:12:21
@_author: Peter Gutmann 
@_subject: [Cryptography] FIPS 140 testing hurting secure random bit 
+1 (by "CMVP people" I assume you mean "the labs").  What the labs apply is
what they interpret the requirements to be.  Interpretations (it might be more
appropriate to label them "guesses" in some cases) vary between labs, so that
something that's OK'd by one lab is rejected by another.  In the worst case,
two labs can set mutually exclusive requirements.  As an implemeter, your
options are either (a) do the appropriate silly-walk or (b) escalate the issue
to NIST.  The latter is sufficiently painful that you'd have to be facing a
serious showstopper before trying it.
It's difficult to get people directly involved in this to talk about it in
public (although many are happy to complain at length in private).  Vendors
are reluctant to publicly criticise the organisation that they depend on for
access to government markets.

@_date: 2013-11-12 12:28:04
@_author: Peter Gutmann 
@_subject: [Cryptography] [cryptography] NIST Randomness Beacon 
You could do it with a physical one-way function.  Take a photo of the victim
on top of the WTC and you know that it can't have been occurred after 9/11. To
generalise it, photograph the victim in front of some documented object and
then destroy the object.  I'm assuming in the movie-plot scenario that someone
who's kidnapped a victim won't worry about blowing up a statue in a park or
performing whatever the physical one-way operation is.  Depending on how evil
your movie-plot villain is (and how convoluted the plot will get), he/she
could kill random strangers after photographing them with the victim, in order
to fix a point in time.

@_date: 2013-11-27 12:41:51
@_author: Peter Gutmann 
@_subject: [Cryptography] Dark Mail Alliance specs? 
My version of this (stolen from a comment by Vesselin Bontchev about user
education for security): If mass-market secure email was going to work it
would have worked by now.

@_date: 2013-11-27 12:44:59
@_author: Peter Gutmann 
@_subject: [Cryptography] Explaining PK to grandma 
+1 to all the above.  I've been a bit behind on email recently so you got
there first with a good summary of the problem, but if your secure email
scheme has a step 0 of "explain to granny how secure email works" then you may
as well not bother going to step 1, whatever your step 1 is meant to be.
Secure email has to work like Skype, you install it, you click on "Call
", and it works.

@_date: 2013-11-27 20:28:11
@_author: Peter Gutmann 
@_subject: [Cryptography] Microsoft announces new email encryption 
It also sounds a lot like what a range of vendors have been doing since maybe
the late 1990s to secure-email-enable users who can't handle secure email as a
desktop app (decrypt on the server, read via an SSL'd web page).  Didn't PGP
Anywhere do something similar to this?  Is it cool if it's PGP but epic fail
if it's Windows?

@_date: 2013-11-29 00:51:21
@_author: Peter Gutmann 
@_subject: [Cryptography] Explaining PK to grandma 
You forgot to mention one of the most important factors, "Hollywood and TV".
Everyone's seen who-knows-how-many car chases, explosions, and car wrecks, in
movies, and gory car accidents, on the TV new or on America's Funniest
Automotive Decapitations or whatever.  Telling them "if you do/don't do X,
you'll end up like Y" is pretty easy.  OTOH for email security the best you
can do is say "if you do/don't do X", where X is invariably some geeky silly-
walk, "then some vague and undefined bad thing may or may not happen".

@_date: 2013-10-03 20:49:05
@_author: Peter Gutmann 
@_subject: [Cryptography] encoding formats should not be committee'ized 
For those not familiar with TL1, "supposed to be readable" here means "encoded
in ASCII rather than binary".  It's about as readable as EDIFACT and HL7.

@_date: 2013-10-04 21:17:47
@_author: Peter Gutmann 
@_subject: [Cryptography] encoding formats should not be committee'ized 
Things like HL7 and EDIFACT/X12 (and ASN.1 in DER/BER form) were never meant
to be human-readable, they're meant to be easily machine readable and
processable.  This is why you have viewers to turn them into human-readable
form in any format you want.  The problem with formats like XML is that it's
never been quite sure what it wants to be, so that the result is neither
easily human-readable nor easily machine-readable.
Trying to get back on track, I think any attempt at TLS 2 is doomed.  We've
already gone through, what, about a million messages bikeshedding over the
encoding format and have barely started on the crypto.  Can you imagine any
two people on this list agreeing on what crypto mechanism to use?  Or whether
identity-hiding (at the expense of complexity/security) should trump
simplicity/security 9at the expense of exposing identity information)?

@_date: 2013-10-07 18:43:27
@_author: Peter Gutmann 
@_subject: [Cryptography] Universal security measures for crypto primitives 
Given the recent debate about security levels for different key sizes, the
following paper by Lenstra, Kleinjung, and Thome may be of interest:
  "Universal security from bits and mips to pools, lakes and beyond"
    should be required to rate their pet scheme in terms of
neerslagverdampingsenergiebehoeftezekerheid (although I'm tempted to suggest
the alternative tausendliterbierverdampfungssicherheit, it'd be too easy to
cheat on that one).

@_date: 2013-10-11 03:11:07
@_author: Peter Gutmann 
@_subject: [Cryptography] Crypto Standards v.s. Engineering habits - Was: 
And how do you know that you're doing it right?  PGP in 1992 adopted a
bleeding-edge cipher (IDEA) and was incredibly lucky that it's stayed secure
since then.  What new cipher introduced up until 1992 has had that
distinction?  "Doing it right the first time" is a bit like the concept of
stopping rules in heuristic decision-making, if they were that easy then
people wouldn't be reading this list but would be in Las Vegas applying the
stopping rule "stop playing just before you start losing".
This is particularly hard in standards-based work because any decision about
security design tends to rapidly degenerate into an argument about whose
fashion statement takes priority.  To get back to an earlier example that I
gave on the list, the trivial and obvious fix to TLS of switching from MAC-
then-encrypt to encrypt-then-MAC is still being blocked by the WG chairs after
nearly a year, despite the fact that a straw poll on the list indicated
general support for it (rough consensus) and implementations supporting it are
already deployed (running code).  So "do it right the first time" is a lot
easier said than done.

@_date: 2013-10-11 20:05:58
@_author: Peter Gutmann 
@_subject: [Cryptography] PGP Key Signing parties 
I've very rarely used that (would you recognise a fake European ID card, or NZ
passport, if you saw one?), I've always used either direct personal knowledge
or personal WoT, i.e. an introduction from someone I know, in person.  This is
exactly how organised crime does it (see "Codes of the Underworld: How
Criminals Communicate" by Diego Gambetta, damn good read), and it's extremely
effective, if you think your generic APT requires effort then look at what it
takes for law enforcement to get someone inside an organised crime ring.

@_date: 2013-10-12 13:41:10
@_author: Peter Gutmann 
@_subject: [Cryptography] SSH small RSA public exponent 
both a suboptimal exponent (it's less efficient that a safer value like 257
   or F4) and non-prime.  The reason for this was that the original SSH used
   an e relatively prime to (p-1)(q-1), choosing odd (in both senses of the
   word) numbers > 31.  33 or 35 probably ended up being chosen frequently so
   it was hardcoded into OpenSSH for cargo-cult reasons, finally being fixed
   after more than a decade to use F4.  In order to use pre-5.4 OpenSSH keys
   that use this odd value we make a special-case exception for SSH use */

@_date: 2013-10-17 18:49:01
@_author: Peter Gutmann 
@_subject: [Cryptography] PGP Key Signing parties 
So the CA would be stating 'I now declare this text string and this bit string "name" and "key" so then they are no more twain, but one certificate.  What therefore RSA hath joined together let not man put asunder'.

@_date: 2013-10-23 16:00:01
@_author: Peter Gutmann 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
Well, you can get easy-to-install interfaces like this one:
that anyone can plug in in a few seconds, but it's a bit of a one-shot affair
in terms of sampling the 50Hz signal.

@_date: 2013-10-30 17:14:55
@_author: Peter Gutmann 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
Yes, they won't let you feed in additional entropy.  In my case I managed to
get around it through some code subterfuge (they think they got what they
asked for and I know it was actually done right, not the way they wanted), but
it wouldn't surprise me if other implementers just threw up their hands and
did what the labs wanted.

@_date: 2013-10-31 16:32:16
@_author: Peter Gutmann 
@_subject: [Cryptography] Standard exponents in RSA 
Until not too long ago, many implementations would accept e=1.  Windows CryptoAPI still does, as a means of allowing plaintext key export while still being FIPS 140 compliant.

@_date: 2013-09-06 13:18:19
@_author: Peter Gutmann 
@_subject: [Cryptography] tamper-evident crypto?    (was: BULLRUN) 
Cognitive dissonance.  "We have been...", sorry "Ve haff been reassured zat
our cipher is unbreakable, so it must be traitors, bad luck, technical issues,

@_date: 2013-09-06 13:28:10
@_author: Peter Gutmann 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Not informed since I don't work for them, but a connect-the-dots:
1. ECDSA/ECDH (and DLP algorithms in general) are incredibly brittle unless
   you get everything absolutely perfectly right.
2. The NSA has been pushing awfully hard to get everyone to switch to
   ECDSA/ECDH.
Wasn't Suite B promulgated in the 2005-2006 period?
Peter (who choses RSA over ECC any time, follow a few basic rules and you're
       safe with RSA while ECC is vulnerable to all manner of attacks,
       including many yet to be discovered).

@_date: 2013-09-06 13:42:47
@_author: Peter Gutmann 
@_subject: [Cryptography] Implementations, attacks on DHTs, Mix Nets? 
[Apparently a pile of my mail got dropped, the following few messages are re-sends]
And that's the problem with DNS, it's the only global distributed database
that we've got, so everyone wants to use it as the universal substrate for,
well, anything.  We'd just need to get draft-ietf-dnsind-kitchen-sink-02.txt
adopted and people could cram anything they liked into the DNS.

@_date: 2013-09-06 13:43:46
@_author: Peter Gutmann 
@_subject: [Cryptography] Why human-readable IDs (was Re: Email and IM are 
"Our direct competitor has asked us to recommend a technology for .  What should we recommend to (Bit of an artificial example, but between that and Corba you can really mess
up someone's business).

@_date: 2013-09-06 13:44:11
@_author: Peter Gutmann 
@_subject: [Cryptography] NSA and cryptanalysis 
If I had to bet, I'd bet on anything but the crypto.  Why attack when you can
bypass [1].
[1] From Shamir's Law [2], "crypto is bypassed, not penetrated".
[2] Well I'm going to call it a law, because it deserves to be.
[3] This is a recursive footnote [3].

@_date: 2013-09-06 13:44:47
@_author: Peter Gutmann 
@_subject: [Cryptography] Keeping backups (was Re: Separating concerns 
You read my mind :-).  I suggested more or less this to a commercial provider
a month or so back when they were trying to solve the same problem.
Specifically it was "if you lose your key/password/whatever, you can't call
the helpdesk to get your data back, it's really gone", which was causing them
significant headaches because users just weren't expecting this sort of thing.
My suggestion was to generate a web page in printable format with the key
shares in standard software-serial-number form (XXXXX-XXXXX-XXXXX etc) and
tell people to keep one part at home and one at work, or something similar,
and to treat it like they'd treat their passport or insurance documentation.

@_date: 2013-09-06 13:50:54
@_author: Peter Gutmann 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
I think you're ascribing way too much of the usual standards committee
crapification effect to enemy action.  For example I've had an RFC draft for a
trivial (half a dozen lines of code) fix for a decade of oracle attacks and
whatnot on TLS sitting there for ages now and can't get the TLS WG chairs to
move on it (it's already present in several implementations because it's so
simple, but without a published RFC no-one wants to come out and commit to
it).  Does that make them NSA plants?  There's drafts for one or two more
fairly basic fixes to significant problems from other people that get stalled
forever, while the draft for adding sound effects to the TLS key exchange gets
fast-tracked.  It's just what standards committees do.
(If anyone knows of a way of breaking the logjam with TLS, let me know).

@_date: 2013-09-06 14:01:31
@_author: Peter Gutmann 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
It's not just randomness, it's problems with DLP-based crypto in general.  For
example there's the scary tendency of DLP-based ops to leak the private key
(or at least key bits) if you get even the tiniest thing wrong.  For example
if you follow DSA's:
  k = G(t,KKEY) mod q
then you've leaked your x after a series of signatures, so you need to know that you generate a large-than-required value before reducing mod q.  The whole DLP family is just incredibly brittle.
That's assuming that the threat is cryptanalysis rather than bypass.  Why
bother breaking even 1024-bit RSA when you can bypass?

@_date: 2013-09-06 14:15:28
@_author: Peter Gutmann 
@_subject: [Cryptography] Suite B after today's news 
Same here.  AES is, as far as we know, pretty secure, so any problems are
going to arise in how AES is used.  AES-CBC wrapped in HMAC is about as solid
as you can get.  AES-GCM is a design or coding accident waiting to happen.
This isn't the 1990s, we don't need to worry about whether DES or FEAL or IDEA
or Blowfish really are secure or not, we can just take a known-good system off
the shelf and use it.  What we need to worry about now is deployability.  AES-
CTR and AES-GCM are RC4 all over again, it's as if we've learned nothing from
the last time round.

@_date: 2013-09-06 14:35:14
@_author: Peter Gutmann 
@_subject: [Cryptography] Suite B after today's news 
If it's not a stream cipher and doesn't fail catastrophically with IV reuse
then it's probably as good as any other mode.  Problem is that at the moment
modes like AES-CTR are being promulgated as fashion statements without any
consideration about operational deployment, when what we should be promoting
is something that's safely and effectively deployable.  Someblockcipher-CBC +
HMAC is a nice safe bet, run your HMAC, do a constant-time compare of the
result, toss the encrypted data if you get a verify failure, otherwise
decrypt, it's pretty straightforward.

@_date: 2013-09-07 10:50:04
@_author: Peter Gutmann 
@_subject: [Cryptography] Suite B after today's news 
fixes all of
these, I just can't get any traction on it from the TLS WG chairs.  Maybe
they're following
 :-).

@_date: 2013-09-07 10:51:11
@_author: Peter Gutmann 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
How does '(a) Organizations and Conferences' differ from SOP for these sorts
of things?

@_date: 2013-09-08 20:40:53
@_author: Peter Gutmann 
@_subject: [Cryptography] Protecting Private Keys 
Just buy second-hand HSMs off eBay, they often haven't been wiped, and the
PINs are conveniently taped to the case.  I have a collection of interesting
keys (or at least keys from interesting places, including government
departments) obtained in this way.

@_date: 2013-09-08 22:45:27
@_author: Peter Gutmann 
@_subject: [Cryptography] Suite B after today's news 
It's done that way based on discussions on (and mostly off) the TLS list by
various implementers, that was the one that caused the least dissent.

@_date: 2013-09-09 14:35:31
@_author: Peter Gutmann 
@_subject: [Cryptography] Suite B after today's news 
There wasn't really much dissent (there was some discussion, both on and off-
list, which I've tried to address in updates of the draft), it's just that the
WG chairs don't seem to want to move on it.
I've heard from implementers at Large Organisations that having it non-
standards-track makes it hard to get it adopted there.  I guess I could go for
Informational if all else fails.
That's what's already happened/happening, problem is that without an RFC to
nail down at least the extension ID it's a bit hard for commercial vendors to
commit to it.

@_date: 2013-09-09 16:31:56
@_author: Peter Gutmann 
@_subject: [Cryptography] Market demands for security (was Re: Opening 
Some years ago NZ abolished its offensive (fighter) air force (the choice was either to buy all-new, meaning refurbished, jets at a huge cost or abolish the capacity).  Lots of people got very upset about this, because it was leaving us defenceless.
(For people who are wondering why this position is silly, have a look at the
position of New Zealand on a world map.  The closest country with direct
access to us (in other words that wouldn't have to go through other countries
on the way here) is Peru, and they don't have any aircraft carriers).

@_date: 2013-09-10 22:29:11
@_author: Peter Gutmann 
@_subject: [Cryptography] Suite B after today's news 
It does?  draft-ietf-tls-applayerprotoneg-01 doesn't mention ID 0x10 anywhere.
(In any case -encrypt-then-MAC got there first, these Johnny-come-lately's can
find their own ID to squat on :-).

@_date: 2013-09-11 17:54:02
@_author: Peter Gutmann 
@_subject: [Cryptography] Suite B after today's news 
Hmm, I talked to  earlier this year to let him know that 0x10 was already being used, I'd assumed it'd be marked as RFU, but I guess not.  Maybe we could use 36, which follows on from the SessionTicket one at 35 and is far enough up that it won't be overrun for awhile.
Annoying though, there are already deployed implementations that use 0x10.

@_date: 2013-09-12 19:03:58
@_author: Peter Gutmann 
@_subject: [Cryptography] Why prefer symmetric crypto over public key 
Was that the change that was required by FIPS 140, or a different vuln?

@_date: 2013-09-12 19:14:15
@_author: Peter Gutmann 
@_subject: [Cryptography] Radioactive random numbers 
If you're in Australia you don't need to use smoke detectors, you've got direct access to the real stuff.  I've used a lump of Australian uranium ore with my geiger counter in the past.  Problem is that this is hardly scalable.

@_date: 2013-09-19 10:23:03
@_author: Peter Gutmann 
@_subject: [Cryptography] Gilmore response to NSA mathematician's "make 
For people unfamiliar with this one, it's the bit that reads:
  Congress shall make no law respecting the wearing of hosiery, or prohibiting
  the free exercise thereof; or abridging the freedom of colour selection, or
  of the material used; or the right of the people peaceably to assemble, and
  to petition the manufacturers for a redress of manufacturing defects.

@_date: 2013-09-19 21:52:08
@_author: Peter Gutmann 
@_subject: [Cryptography] PRISM-Proofing and PRISM-Hardening 
Precisely.  I made the same point recently in an interview about PRISM, that a
well-designed, well-engineered protocol will be NSA-proof (or at least as NSA-
proof as you can get within reason).  It'll also be Russian mafia-proof,
Chinese-government-proof, and your-mother-proof.  There isn't some exotic
class of protocol or mechanism that's needed to resist the NSA, anything well-
designed and implemented can do it.

@_date: 2013-09-23 20:47:45
@_author: Peter Gutmann 
@_subject: [Cryptography] RSA equivalent key length/strength 
That's rather misrepresenting the situation.  It's a debate between two
groups, the security practitioners, "we'd like a PFS solution as soon as we
can, and given currently-deployed infrastructure DH-1024 seems to be the best
bet", and the theoreticians, "only a theoretically perfect solution is
acceptable, even if it takes us forever to get it".
(You can guess from that which side I'm on).

@_date: 2013-09-24 16:27:58
@_author: Peter Gutmann 
@_subject: [Cryptography] RSA equivalent key length/strength 
... and I guess that puts you firmly in the theoretical/impractical camp.
Would you care to explain how this is going to work within the TLS protocol?
It's easy enough to throw out these hypothetical what-if's (gimme ten minutes
and I'll dream up half a dozen more, all of them theoretically OK, none of
them feasible), but they need to actually be deployable in the real world, and
that's what's constraining the current debate.

@_date: 2013-09-25 23:59:50
@_author: Peter Gutmann 
@_subject: [Cryptography] RSA equivalent key length/strength 
Something that can "sign a new RSA-2048 sub-certificate" is called a CA.  For a browser, it'll have to be a trusted CA.  What I was asking you to explain is how the browsers are going to deal with over half a billion (source: Netcraft web server survey) new CAs in the ecosystem when "websites sign a new RSA-2048

@_date: 2013-09-26 11:18:08
@_author: Peter Gutmann 
@_subject: [Cryptography] RSA recommends against use of its own products. 
+1.  It's the Vinny Gambini effect (from the film My Cousin Vinny):
  Judge Haller: Mr. Gambini, didn't I tell you that the next time you appear
  Vinny: You were serious about dat? And it's not just Dual-EC-DRBG that triggers the "You were serious about dat?" response, there are a number of bits of security protocols where I've been... distinctly surprised that anyone would actually do what the spec said.
(Having said that, I've also occasionally been pleasantly surprised when, by unanimous unspoken consensus among implementers, everyone ignored the spec and did the right thing).

@_date: 2013-09-26 11:32:50
@_author: Peter Gutmann 
@_subject: [Cryptography] RSA recommends against use of its own products. 
???Software Defaults as De Facto Regulation: The Case of Wireless APs???, Rajiv
Shah and Christian Sandvig, Proceedings of the 33rd Research Conference on
Communication, Information and Internet Policy (TPRC???07), September 2005,
reprinted in Information, Communication, and Society, Vol.11, No.1 (February
2008), p.25.

@_date: 2013-09-26 11:40:33
@_author: Peter Gutmann 
@_subject: [Cryptography] forward-secrecy >=2048-bit in legacy 
Yes, but not in the way you want.  This is what the 1990s-vintage RSA export
ciphersuites did, but they were designed so you couldn't use them to provide
strong security.
That'd be the other problem :-).

@_date: 2013-09-29 21:06:57
@_author: Peter Gutmann 
@_subject: [Cryptography] RSA recommends against use of its own products. 
At least some X.500/LDAP folks thought they could do it.  Mind you, we're
talking about people who believe in X.500/LDAP here...

@_date: 2014-04-02 02:30:13
@_author: Peter Gutmann 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve numbers 
The fact that the random digits are slightly biased:
There are also persistent rumours that a representative from the Signal
Security Agency / Armed Forces Security Agency that existed at the time the
numbers were created, a certain H.Celine, somehow influenced the creation of
the list, thus the lack of information about how they were actually generated.
These concerns go back even further though, to the sneaky introduction of the
digit zero, which appears in nearly 10% of the "Random Digits" entries, by
Arab infiltrators into Iberia in the 11th century.  So overall a highly
suspect work whose origins are far too murky to make it suitable for
cryptographic use.

@_date: 2014-04-02 02:39:09
@_author: Peter Gutmann 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve numbers 
This is pretty close to how the numbers racket worked, except that instead of
stocks it used the least significant digits of amounts bet at racetracks,
which was taken as a similarly publicly verifiable source of non-manipulable
randomness.  This didn't stop Otto "Abba Dabba" Berman, Dutch Shultz'
accountant, from fixing the numbers racket by a mechanism that's still the
subject of speculation today.

@_date: 2014-04-06 20:18:00
@_author: Peter Gutmann 
@_subject: [Cryptography] TLS/DTLS Use Cases 
For people following this thread, I'm working on an RFC draft for the UTA (Using TLS in Applications) working group (or as one person put it, the TLS fixup crew) on use cases.  I'm a bit overwhelmed with work at the moment so it'll take some weeks, but there should be a use-case RFC (draft) out at some

@_date: 2014-04-06 20:59:17
@_author: Peter Gutmann 
@_subject: [Cryptography] OpenPGP and trust 
And the neat thing is that any bad guy can buy a cert from the same CA you
bought your one from (or any other commercial CA of their choice), set up a
dummy server, and all your friends will connect thinking it's the real thing.
The false sense of security created by the cert will make things much easier
for them.

@_date: 2014-04-07 13:52:54
@_author: Peter Gutmann 
@_subject: [Cryptography] Verifying X.509 Verification - how about an 
Yup.  The goal for any certificate use is "make it work, dammit!", since
they're already hard enough to deal with as it is.  After fighting this for
many years in my code I eventually introduced the concept of compliance
levels, ranging from "full PKIX" through to "oblivious".  There are a number
of public CAs that require the compliance level to be taken all the way down
to "oblivious" in order for their cert chains to work.  Conversely, some years
ago when I tested this, setting the compliance level to "full PKIX" rejected
ten out of eleven certs chains from public CAs.
It's a somewhat dubious set of tests that range from reasonable through to
just plain stoopid.  I've commented on these before, he's one summary:
  (The US National Institute of Standards (NIST) has created a suite of test
  certificates that exercise various features of standards, but these function
  more as an exhaustive enumeration of standards-document features than a
  rigorous compliance-test suite.  As a result many of the problems discussed
  above are never checked, but on the other hand obscure aspects of standards
  present only for historical or political reasons are.  Since a number of
  these design artefacts are never used in real life, implementations have to
  add special support for them just to pass the tests, to the extent that some
  have added a special NIST-test mode that allows them to clear the tests,
  after which normal operation can be resumed at the flip of a compile option.
  The most extreme case of this reportedly detects the presence of the NIST
  test certificates and returns the expected results from an internal lookup
  table, which saves the developers the trouble of having to implement the
  complex certificate-processing functionality that would be required to pass
  the tests. These sorts of developer tricks are another example of gaming
  onerous qualification tests that???s discussed in ???Asking the Drunk Whether
  He???s Drunk??? on page 54.  What makes this even more entertaining is the fact
  that some versions of the test suite contained errors and yet
  implementations still successfully processed it, indicating that the
  implementation target was ???whatever???s required to pass the test??? rather than
  ???what the standard requires???).

@_date: 2014-04-14 16:28:28
@_author: Peter Gutmann 
@_subject: [Cryptography] Preliminary review of the other Applied 
It's not because I find DNSSEC personally offensive or anything, but because
(and this is a very, very cut-down version of the longer reasoning in the
book) it's a huge amount of effort that achieves almost nothing.  As I point
out in the book, if you could wave a magic wand right now and get DNSSEC
deployed globally, only the good guys would notice (lots of breakage,
slowdowns, attack amplification, etc).  The bad guys (phishing, spam, 419'ers)
wouldn't even notice, because it doesn't counter anything they're doing.  If
you're going to expend that much effort then put it into something that will
at least be a speedbump for the phishers and whatnot.

@_date: 2014-04-14 16:34:26
@_author: Peter Gutmann 
@_subject: [Cryptography] Preliminary review of the other 
Really?  How?
(And by "how?" I mean what actual, measurable effect will it have.  By what
measurable percentage will phishing decrease with DNSSEC in effect?).

@_date: 2014-04-18 13:13:25
@_author: Peter Gutmann 
@_subject: [Cryptography] Simpler programs? 
I counted twelve.  For cat.  I think this is another sign that it's time to
tear down civilisation and let the ants have a go.

@_date: 2014-04-18 14:14:03
@_author: Peter Gutmann 
@_subject: [Cryptography] I don't get it. 
Note that the version of PREfast used inside Microsoft is a lot more powerful
than the general-relase one, so the fact that the internal one would find it
doesn't necessarily mean that the one that everyone else uses would.
(The internal-only analysis tools require a lot more expertise to drive, the
released ones are training-wheels versions that won't result in MS getting
flooded in support calls for error messages that developers don't understand).

@_date: 2014-04-18 15:53:29
@_author: Peter Gutmann 
@_subject: [Cryptography] I  don't get it. 
You're forgetting clang.  gcc is the compiler of last resort, the tool you use
if there's no other alternative.  Its endless code-generation bugs are an
ongoing headache.  The workarounds for compiler bugs in my code for gcc are
about an order of magnitude larger than *all other compilers combined*, and
the bug-rate seems to be constant over a twenty-year period.  The devs ignore
requests for bug-fixes.  There's behaviour in gcc that's utterly braindamaged
(see e.g.  but the devs will
spend more time arguing that it's OK to do this than it would take to fix it.
Another example is the totally braindamaged behaviour of -Wshadow, which took
a complaint from no less then Linus Torvalds to get fixed (see
 and even then it took them six years to
fix the problem).  Out of many issues I've tried (unsuccessfully) to get
fixed, my pet peeve would be:
   /*
   but STDC_NONNULL_ARG (=> '__attribute__(( nonnull argIndex ))') is
   downright dangerous since it'll break correctly functioning code.
   [...]
   STDC_NONNULL_ARG on the other hand is far more broken since the warnings
   are issued by the front-end before data flow analysis occurs (so many
   cases of NULL pointer use are missed) but then the optimiser takes the
   annotation to mean that that value can never be NULL and *removes any
   code that might check for a NULL pointer*!  This is made even worse by
   the awkward way that the annotation works, requiring hand-counting the
   parameters and providing an index into the parameter list instead of
   placing it next to the parameter as for STDC_UNUSED.
   */
That's got a code warning that not only doesn't do what it's supposed to, it
breaks existing, working code.
So while my general pacifist leanings would prevent me from helping you shoot
the developers, I'd be happy to hand you the ammunition and do the reloading.
Then we can all switch to clang and spend time finding bugs in our code rather
than bugs (or braindamage-by-design) in the compiler.

@_date: 2014-04-19 13:45:04
@_author: Peter Gutmann 
@_subject: [Cryptography] It's all K&R's fault 
This was first stated (in variant form) by Ed Post in "Real Programmers Don't
Use PASCAL":
  The determined Real Programmer can write FORTRAN programs in any language

@_date: 2014-04-20 16:26:18
@_author: Peter Gutmann 
@_subject: [Cryptography] bounded pointers in C 
It also includes a policy of constant audit (which I also do in my code),
which is a necessary addition to the above.  This yields a more realistic
rendition of ESR's dictum, "at least one pair of eyes that's actively looking
makes bugs shallow".

@_date: 2014-04-21 01:05:16
@_author: Peter Gutmann 
@_subject: [Cryptography] bounded pointers in C 
Sorry, nonstandard terminology, I meant constantly auditing your own code,
which in both Wietse's case and mine means taking printouts of code modules
home to read offline at your own pace to check for problems.  That's not just
code you've recently been working on, but selections of older code modules
that you're re-checking in case you see something that didn't pop out the last
time you checked.

@_date: 2014-04-21 14:05:57
@_author: Peter Gutmann 
@_subject: [Cryptography] GCC bug 30475 (was Re:  bounded pointers in C) 
They have no responsibilities to anyone, and that's the problem.  Being
completely disconnected from any responsibility to their users, they have the
liberty to sit there pontificating about hair-splitting interpretations of the
standard rather than Doing the Right Thing by users.  This is a serious
problem with many OSS projects which are driven by the whims of the owners
rather than real-world considerations (it's also a benefit in some cases, so
it cuts both ways).
[Long philosophising discussion about requirements-driven development and
customer responsiveness snipped since it's really only tangentially security-

@_date: 2014-04-21 18:45:34
@_author: Peter Gutmann 
@_subject: [Cryptography] Apple and OpenSSL 
I use a slightly different approach: Don't assume a C API/ABI.  If your API
can deal with C, Java, Perl, Tcl, Delphi, and Visual Basic, all via the single
API, then there's a pretty good chance that you're not going to break things
via some change in a C-only API artefact.

@_date: 2014-04-24 04:31:20
@_author: Peter Gutmann 
@_subject: [Cryptography] It's all K&R's fault 
That's obviously a Unix-specific thing, because Windows will run with zero
swap quite happily.  I don't think I've ever used a swapfile under any
NT-kernel version of Windows.

@_date: 2014-04-24 21:09:36
@_author: Peter Gutmann 
@_subject: [Cryptography] Apple and OpenSSL 
See Dennis' followup message, use access methods (get/set/delete) rather than
direct access to internal members.  One side-effect of this is that it
requires a reasonably well thought-out API, so that you don't end up having to
call a dozen set-value methods for each operation you want to perform.

@_date: 2014-04-25 05:50:23
@_author: Peter Gutmann 
@_subject: [Cryptography] Swap space (Re:  It's all K&R's fault) 
Aren't many fork()s now pretty close to vfork(), via COW?
(Feel free to take this off-list if people feel this is too far off topic).

@_date: 2014-04-25 20:42:33
@_author: Peter Gutmann 
@_subject: [Cryptography] [cryptography] Is it time for a revolution to 
And it is a dream.  This is another one of the "let's replace TLS with My Pet
Secure Protocol and then we'll be safe", ignoring the fact that an
implementation of MPSP can and will be just as buggy as an implementation of
TLS.  As with "let's replace C with My Pet Programming Language", you can
write crap in any language you want.  The problem isn't the language, or
protocol, or OS, or whether you use a Qwerty or Dvorak keyboard, but the
culture that creates the implementation.

@_date: 2014-04-25 20:44:24
@_author: Peter Gutmann 
@_subject: [Cryptography] Apple and OpenSSL 
Is that terribly useful though?  You can marshal and unmarshal almost anything
(arbitrarily nested variant records and the like), so I'm not sure whether
that's a good measure of good vs. bad.

@_date: 2014-04-25 20:58:32
@_author: Peter Gutmann 
@_subject: [Cryptography] GCC bug 30475 (was Re:  bounded pointers  in C) 
It's not even functional (unfortunately) since sizeof() isn't evaluated by the

@_date: 2014-08-03 19:55:55
@_author: Peter Gutmann 
@_subject: [Cryptography] ADMIN: Periodic reminder about top posting 
Hopefully this won't hijack the conversation too much, but I had a
coincidental talk about this yesterday with a non-geek friend who mixes with
lots of geeks.  She mentioned that the style of conversation used here, quoted
excerpts and short passages as replies, are very much the exception for email
exchanges.  The only place she's ever seen it used is by geeks.  The norm for
all other people (that she's aware of, obviously it's not a sample of the
entire world) is to include the entire message at the bottom (nested
arbitrarily deep), and have the reply at the top.  Your mail reader then
formats it as required.
This came as something of a surprise to me.

@_date: 2014-08-15 06:39:20
@_author: Peter Gutmann 
@_subject: [Cryptography] Encryption opinion 
Yep, it should be fine for what you're using it for.  The problem that you're
running into here is one of marketing, not security.  People will happily use
a platform crawling with random third-party apps, plugins, and (frequently)
malware for their "secure" messaging, but tell them you're using keys that
don't make the appropriate fashion statement and they'll scream bloody murder.
See "The Curse of Cryptography Numerology",
Unfortunately, the opinion of the (technical) crypto community doesn't count
for much when you're going up against a fashion statement.  Announce a new
release, with the crypto designed by Oscar Zoroaster Digg, and mention either
a key size of 2048 bits or greater or the use of ECC (key size doesn't matter,
just "ECC" is fine).
(This then runs into a second problem, that ECC is incredibly brittle and easy
to get wrong, so you're probably going to be less secure with ECC than RSA.
Depends on whether you're more concerned about actual security or marketing
Another option is to challenge anyone who questions a 1024-bit key to describe
how they'd attack it (with a breakdown of resources needed, computing times,
etc).  Unfortunately I think this is a battle you can't win, you can't fight
fashion/numerology with logic.

@_date: 2014-08-17 11:55:17
@_author: Peter Gutmann 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
I realise this is hyperbole, but the fact that this is being put up as a
strawman (strawgranny?) shows how unreal the debate over key sizes (and
algorithms) has become.  In this case we have some stereotypical 80-year-old
granny who spams her grandchildren with email about poor Craig Shergold while
running Netscape 2.0 on Windows 3.1, and the security concern is over her use
of RC4/40.  Talk about losing sight of the forest for the trees.  It wouldn't
matter if she used rot13, she's going to get phished or pwned a thousand times
over before anyone bothers going after the crypto she's using.

@_date: 2014-08-17 12:29:04
@_author: Peter Gutmann 
@_subject: [Cryptography] Encryption opinion 
It's the old "what's your threat model" again.  RSA-512 is still in active
use, and in many situations it's perfectly secure.  Consider one real-life
example that I ran into a few years back, a company was using embedded print
servers with access secured via 512-bit RSA keys (the hardware couldn't really
manage much more than that) across a number of their business locations.  Now
if you look at what that entails in terms of security, it means that an
attacker who has already penetrated their internal network (full of machines
containing business-sensitive data, payroll information, trade secrets, and so
on) who ignores all of this and instead decides to attack the RSA key on one
of the print servers, now has the ability to turn off toner savings on the
printers, and change the page size from US Letter to A4.  RSA-512 was
perfectly adequate for what they were using it for, which was to keep random
gefingerpokers from messing up the printer settings.
If they'd followed the NIST guidelines, they'd have had to scrap all of their
print servers and replace them, at considerable cost in terms of time and
money, with new ones that provided no benefit over the existing ones.
Another example of this occurs with online commerce.  Turn off every cipher in
your browser except single DES (I'm not sure if you can still enable RC4/40)
and go to your bank and transfer some funds, or go to eBay and buy something.
Watch the complete lack of anything that arises from this.

@_date: 2014-08-17 15:56:56
@_author: Peter Gutmann 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
Matthew Green actually did a good writeup on this recently in "What's the
matter with PGP?",
Quotable quotes:
  Poking through a modern OpenPGP implementation is like visiting a museum of
  1990s crypto.
I think this applies mostly to GPG though, which still uses crazy defaults
like CAST5 as its standard cipher.
  It's one thing to provide optional backwards compatibility for that one
  friend who runs PGP on his Amiga. [...] Even if these archaic ciphers and
  formats aren't exploitable today, the current trajectory guarantees we'll
  still be using them a decade from now.
And this is exactly the problem.  I don't know what optional new ciphers we
might have available in 2020 (possibly even that newfangled thing called AES),
but I'm pretty sure the default will still be CAST5 from 1996.

@_date: 2014-08-17 19:49:09
@_author: Peter Gutmann 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
There's a lot of people using GPG for file encryption, which means CAST5.  I
found this out the hard way a few years ago when I removed CAST5 support (I
was unable to identify anything other than GPG that still used it), and then
had to quickly back out the change when people complained that they couldn't
decrypt GPG-encrypted files any more.

@_date: 2014-08-19 05:55:00
@_author: Peter Gutmann 
@_subject: [Cryptography] Encryption opinion 
Uhh, you've misunderstood the point I was trying to make: If you do your
online banking/eBay buying/whatever and use weak crypto, nothing bad will
happen...  Corollary: ...because there's no need to attack the crypto, there
are a thousand [1] easier ways to get credit card numbers and whatnot than via
the crypto.  For example [1] Well, some sort of non-small integer anyway.

@_date: 2014-08-25 05:22:16
@_author: Peter Gutmann 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
Yeah, that's an example of the problem that several people pointed out in the
"Encryption opinion" thread, people obsess over whatever value has the biggest
magnitude (in this case public key sizes) and ignore everything else.  The EU
has moved to ban vacuum cleaners over 1600W because of a numbers race (people
equate larger power draw -> better clean, vendors oblige by shipping power-
guzzling cleaners whose main noticeable difference is that they make more
noise than more economical ones), so that now cleaners will be rated on
(shock, horror) actual cleaning performance rather than how much power they
burn.  Perhaps the EU could do the same for crypto as well, you can't enable
ridiculous key sizes until you can demonstrate that the rest of your crypto
and surrounding software provides equivalent strength.
(Oh, and I filed a request to move to AES as the default in 2011, subject "Why
does GPG still default to the 15-year-old CAST5 for everything?", so people
have asked for this to be fixed).

@_date: 2014-08-26 19:14:12
@_author: Peter Gutmann 
@_subject: [Cryptography] Open Source Sandboxes to Enforce Security on 
I assume you mean that this is the conceptual model for sandboxing rather than
that you actually do this.  If you do trust your sandbox, I have some Java
code that I'd like you to run...
Peter :-).

@_date: 2014-08-26 23:24:29
@_author: Peter Gutmann 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
There's nothing obviously wrong with CAST, but it is nearly twenty years old
and hasn't had anywhere near the analysis of AES (or 3DES), particularly
against recent cryptanalysis techniques.  Anything new that turns up will
pretty much automatically get thrown at AES, and we know that it's resistant
to it.  Do we know that anyone's tried the same with CAST?  From this page:
the last analysis published was in 1997 (a Google search turns up one or two
newer ones, but mostly in regard to CAST5 being the ancestor of CAST-256).
David Wagner's boomerang attack breaks CAST-256 with 16 rounds, and CAST5 is
the predecessor of CAST-256 with...16 rounds (presumably it can't be extended
back to CAST5 or someone would have announced this, but how hard has anyone
looked?).  CAST5 also has lots of lovely large S-boxes and S-box lookups,
which would seem to make it vulnerable to assorted timing/cache/whatever side-
channel attacks, but there's no indication that anyone's looked at them
because they're all too busy focusing on AES instead.
So we've got an algorithm that hasn't had any significant cryptanalytic
attention since the late 1990s, and that could well be vulnerable to newer
techniques (and in particular a whole pile of side-channel attacks), but we'll
never know because as far as we know no-one's ever looked.
Show me evidence that it's immune to cryptanalytic techniques developed in the
last 15 years, and to the smorgasbord of side-channel attacks that have been
thrown at AES, and I'll agree with you.  As Bruce Schneier likes to say,
"attacks always get better, they never get worse".  CAST5 has been standing
still for about fifteen years while the attackers moved ahead.  How do we know
it's still safe?

@_date: 2014-08-29 00:46:59
@_author: Peter Gutmann 
@_subject: [Cryptography] Which big-name ciphers have been broken in 
Crypto isn't really maths though.  PKCs are *based* on maths, but block
ciphers... oh dear, this is going to quickly go down a rathole on what
something like a proof in the standard model actually proves so I won't make
any statements on that, but in any case you're not proving a mathematical
theorem for which either your proof succeeds or fails (with a third case, it
fails but you think it's succeeded due to error) but starting with a
mathematical model and then quickly moving from maths to engineering.
In your mathematical model, computing C = M^e mod n is an O( 1 ) atomic
operation, but in your engineering model it's a non-atomic Montgomery modexp
that's vulnerable to timing attacks, and if you fix that you're still
vulnerable to memory-access-pattern (e.g. cache) attacks, and if you fix that
then you're vulnerable to EM side-channels, and so on.
So maths doesn't go stale, but it's also a very non-representative
representation of what the crypto is doing.

@_date: 2014-12-03 20:34:20
@_author: Peter Gutmann 
@_subject: [Cryptography] Toxic Combination 
If by "UAs" you mean "browsers" then the answer is "something on the order of
divine intervention".  The browser vendors have to date shown themselves to be
totally resistant to implementing anything that would threaten the CA business
model, so it's unlikely that something like TLS-SRP or TLS-PSK will ever be

@_date: 2014-12-04 20:21:39
@_author: Peter Gutmann 
@_subject: [Cryptography] Toxic Combination 
You forgot the rest of the list that gets trotted out:
It won't scale, there's no user demand, there's insufficient industry support, I ran out of gas, I had a flat tire, I didn't have enough money for cab fare, my tux didn't come back from the cleaners, an old friend came in from out of town, someone stole my car, there was an earthquake, a terrible flood, There have been endless studies done and papers published on how to do
perfectly usable shared secret-based authentication.  Heck, I devote
significant chunks of my book (draft) to them, I'd be surprised if there were
less than a hundred references to published work on how to do it.
Oh, that's a new one: Set a requirement that can't possibly be met (except
perhaps through the use of magic) and then claim you can't meet that
requirement, therefore it's not worth doing.
Looking past all the excuses, there is one, and only one, reason why no
browser supports proper shared secret-based mutual auth: The browser vendors
don't want to do it.  Meanwhile they're busy implementing more mission-
critical stuff like live in-browser video chat via WebRTC, because that's
functionality that everyone has been crying out for for a web browser.

@_date: 2014-12-07 16:14:15
@_author: Peter Gutmann 
@_subject: [Cryptography] Toxic Combination 
I don't want anyone to design a new globally scalable system, in fact we don't
need any new "system" at all.  I just want to see the current browser strategy
of "hand over the password in plaintext to whoever asks for it" replaced with
"perform password-based mutual challenge/response auth", which short-circuits
the whole phishing equation.  We already have standard mechanisms defined for
this (TLS-PSK, TLS-SRP), they're just not implemented by any browser.

@_date: 2014-12-09 00:06:15
@_author: Peter Gutmann 
@_subject: [Cryptography] Toxic Combination 
Thanks, that was pretty much what I was going to say.  Just one minor nit:
Actually not quite, the CAB Forum isn't a secret cartel, they're quite public
about what they do, and even have their own web site.
Apart from that I agree with everything you've said.

@_date: 2014-12-10 10:46:44
@_author: Peter Gutmann 
@_subject: [Cryptography] A TRNG review per day (week?): ATSHA204A has low 
Does this problem affect the ATECC108 as well?  The data sheet (well, extended
brochure since you can't get the data sheet publicly) for this is from October
2013, which predates the fix introduced in the ATSHA204A (July 2014) by quite
some time.

@_date: 2014-12-10 11:05:50
@_author: Peter Gutmann 
@_subject: [Cryptography] Toxic Combination 
Ian has already responded to this:
  As you yourself show below, asking for references is a setup for a
  knockdown.
However, let's give this a go.  I've collected them all in one place for
convenience, see:
  I count (well, Word counts) just under five hundred references for the
"Passwords" chapter.  Is that enough?
Now you have to come back and say that that's way too many, and you want just
one.  So I go through them and find some representative paper and forward it
to you.  You glance through it and find some reason why it won't work ("it
suggests using a 24-pixel menu bar but we only have 23 pixels available",
something like that).  So I go through and find another paper.  You come back
to me with some reason why it won't work.  We continue this dance until I get
tired of it and find something better to occupy my time.
As Ian has said, this is a setup for a knockdown.  It's like a religious
fundamenatalist asking "Send me a reference proving to me that God doesn't
exist", the outcome is a foregone conclusion, so the only winning move is not
to play.
In any case you have just under 500 references there, so you can't claim
"cannot point to a single example" any more.
(Incidentally, how many references did you require for certificates being effective in protecting browsers from phishing?).

@_date: 2014-12-13 11:39:05
@_author: Peter Gutmann 
@_subject: [Cryptography] Sony finding SHA1 collisions? 
"Dennis E. Hamilton"  quotes:
So an organisation that doesn't notice attackers stealing 100TB of their
corporate files knows how to create hash collisions in SHA-1?  This looks like
another candidate for a Tui (NZ brewery) ad, along the lines of these ones:

@_date: 2014-12-15 13:12:56
@_author: Peter Gutmann 
@_subject: [Cryptography] Windows certificate update breaks/bricks PCs 
Microsoft has somehow managed to release a certificate update that (among
other things) disables graphic driver updates, Windows Defender, USB 3.0,
further windows updates, and messes up UAC:
Unfortunately I haven't been able to find any more details on causes, does
anyone know how a certificate update could cause this much trouble?  I'm after
real details here rather than just speculation ("maybe a code-signing cert was

@_date: 2014-12-15 22:23:48
@_author: Peter Gutmann 
@_subject: [Cryptography] Sony finding SHA1 collisions? 
I'd be far more interested in the \infty BTC reward outstanding for anyone who
can find collisions for RIPEMD-160(SHA-256(x)).

@_date: 2014-12-16 10:59:53
@_author: Peter Gutmann 
@_subject: [Cryptography] Sony "root" certificates exposed 
Henry Baker  quotes:
The Ars story confuses certificates and keys, for example in the second picture they show (and highlight the names of) certificates for GTE Cybertrust Global Root and JP Morgan, US, which it's unlikely that Sony have the private keys for.  I can do something similar to what's shown in the story by going to (for example)  and clicking on the padlock icon.

@_date: 2014-12-19 23:24:53
@_author: Peter Gutmann 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
There's a more empirical argument for why this is a bad thing: What the OP is
describing is what's known as a fortress economy.  A notable current example
of a fortress economy is North Korea.  Past examples are Romania under
Ceausescu and, further back, the Central Powers during WWI.

@_date: 2014-12-24 18:15:03
@_author: Peter Gutmann 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
For anyone interested in hands-on experience, we have one downstairs:
If you're in the area...

@_date: 2014-12-27 16:30:04
@_author: Peter Gutmann 
@_subject: [Cryptography] GHCQ Penetration of Belgacom 
It'll be a pun on "procession", since it was part of the (capping?)

@_date: 2014-02-15 21:35:20
@_author: Peter Gutmann 
@_subject: [Cryptography] RNG exploits are stealthy 
This may be news for Android devices, but interrupt (and timer) coalescing has
been done for several decades in general-purpose computers, originally because
CPUs weren't powerful enough to handle a high interrupt load, and then more
recently for power-saving purposes.  For example Windows has had timer and
interrupt coalescing for some years now, see e.g:
The effects have also been heavily studied, mostly in terms of performance
impact, see e.g. "Estimating the Impact of Interrupt Co alescing Delays on
Steady State TCP Throughput" by Zec et al from SoftCom 2002:

@_date: 2014-02-16 16:02:44
@_author: Peter Gutmann 
@_subject: [Cryptography] Another Bitcoin issue (maybe) (was: BitCoin bug 
Interesting!  The first-gen stuff seems to be mostly repurposed BTC miner
hardware, and it's still at the prototype stage, but in the long term it'll
confirm something I wrote a while back:
  So how do Bitcoin-mining ASICs affect general security?  Passwords and
  encryption keys are often protected using the same hash algorithms that the
  mining ASICs (and FPGAs and GPUs) are designed to calculate at great speed.
  By repurposing the hardware that was designed for Bitcoin mining it would be
  possible to attack hashed passwords with an efficiency that wasn???t feasible
  before Bitcoin appeared (having said that, the Bitcoin ASICs for which
  details have been published are specifically designed for high-speed mining
  rather than password-cracking and would require changes to their control
  circuitry to make them suitable for password cracking ??? it???s not for nothing
  that they???re called application-specific ICs).
  Two variations of Bitcoin called Litecoin and Novacoin turn another
  cryptographic mechanism into collateral damage.  In this case it???s scrypt,
  which was specifically designed to be expensive to implement in custom
  hardware by accessing data spread across a large amount of memory in a
  pseudorandom manner, a so-called memory-hard algorithm [ ].  Unfortunately
  while this makes scrypt extremely expensive to implement in FPGAs and ASICs,
  it???s well suited to GPUs, so mining isn???t nearly as hard as it should be.  A
  side-effect of this Lite/Novacoin mining is that, again, a mechanism
  designed to protect one type of resource, passwords, is weakened when it???s
  also used to protect another type of resource, coin scarcity.
The PHC (password hashing competition) folks are working hard on this, stand
by.  For people interested in the technical details, the PHC list archives,
available at  make for very
interesting, if voluminous, reading.

@_date: 2014-02-18 14:56:10
@_author: Peter Gutmann 
@_subject: [Cryptography] BitCoin bug reported 
This is the big problem with ASN.1.  It's actually a relatively simple, clean
encoding, but it's applied by... well, by the sort of people who gave us
X.509.  So you get monstrosities like:
  For example consider the use of a DN in an IssuingDistributionPoint
  extension, which begins:
  IssuingDistributionPoint ::= SEQUENCE {
    distributionPoint [0] DistributionPointName OPTIONAL,
    ...
    }
  DistributionPointName ::= CHOICE {
    fullName [0] GeneralNames,
    ...
    }
  GeneralNames ::= SEQUENCE OF GeneralName
  GeneralName ::= CHOICE {
    ...
    directoryName [4] Name,
    ...
    }
  Name ::= CHOICE {
    rdnSequence RDNSequence
    }
  RDNSequence ::= SEQUENCE OF RelativeDistinguishedName
  RelativeDistinguishedName ::= SET OF AttributeTypeAndValue
                    [It] was of a baroque monstrosity not often seen outside
                    the Maximegalon Museum of Diseased Imaginings.
                        -- Douglas Adams, "The Restaurant at the End of the
                           Universe"
  Once we reach AttributeTypeAndValue we finally get to something which
  contains actual data - everything before that point is just wrapping.
  Now consider a typical use of this extension, in which you encode the URL at
  which CA information is to be found.  This is encoded as:
  SEQUENCE { [0] { [0] { SEQUENCE { [6] " } } } }
  All this just to specify a URL!
                    It looks like they were trying to stress-test their ASN.1
                    compilers.
                        -- Roger Schlafly on stds-p1363
                    It smelled like slow death in there, malaria, nightmares.
                    This was the end of the river alright.
                        -- Captain Willard, "Apocalypse Now"
The problem isn't the encoding, it's how it's used.  Give the folks who
dreamed up the above JSON, or anything else for that matter, and they'll make
a similar mess.  So I'm not sure if the Sapir-Whorf hypothesis entirely
applies to ASN.1, it's powerful enough to give you all you need to build the
most godawful mess, and it's usually applied that way, but when used correctly
you end up with a simple, clean encoding.
Exactly.  If you look at some of the extensions used with TLS you can
immediately see which ones were created by X.509ers because they've emulated
the sort of mess you'd usually see done in ASN.1 in TLS' ad-hoc encoding. This
makes the whole thing twice as bad, you now have an ASN.1-style mess combined
with the need to hand-code a TLS ad-hoc parser for it.
The downside with ASN.1, admittedly, is that it makes it easier than most
notations to create such a mess :-(.

@_date: 2014-02-18 18:35:46
@_author: Peter Gutmann 
@_subject: [Cryptography] The ultimate random source 
Or point the webcam at a snow globe.
(Another neat source is to take something with a fairly complex structure, say
a couple of scale model trees, glue them onto a base that you put on top of a
solar-powered rotating display stand, and point a webcam at it).

@_date: 2014-02-19 21:23:47
@_author: Peter Gutmann 
@_subject: [Cryptography] Encodings for crypto 
The same can be said of ASN.1.  This currently consists of:
EOC				
BOOLEAN			
INTEGER			
BITSTRING		
OCTETSTRING		
NULLTAG			
OID				
OBJDESCRIPTOR	
EXTERNAL		
REAL			
ENUMERATED		
EMBEDDED_PDV	
UTF8STRING		
SEQUENCE		
SET				
NUMERICSTRING	
PRINTABLESTRING	
T61STRING		
VIDEOTEXSTRING	
IA5STRING		
UTCTIME			
GENERALIZEDTIME	
GRAPHICSTRING	
VISIBLESTRING	
GENERALSTRING	
UNIVERSALSTRING	
BMPSTRING		
Removing the unnecessary stuff, what's left is:
EOC				
BOOLEAN			
INTEGER			
OCTETSTRING		
OID				
ENUMERATED		
UTF8STRING		
SEQUENCE		
GENERALIZEDTIME	
In everything crypto-related that uses ASN.1, that's what the elements being
used actually are.  Everything else is just unnecessary complexity.

@_date: 2014-02-19 21:50:35
@_author: Peter Gutmann 
@_subject: [Cryptography] The ultimate random source 
I would certainly hope they transmit as close to a black image as possible,
since they're designed to do that.  For example when shooting dark scenes some
(still-image) cameras will take a second image with the shutter closed and use
that to get an idea of any noise present, then use that information to cancel
noise in the primary image.  This will remove a lot of the hoped-for noise. In
addition since a lot of what you're hoping for is thermal noise, taking the
images infrequently (so the sensor is relatively cool, therefore with low
noise), or with the camera in a cold room, is going to reduce noise
significantly.  This is why devices like telescopes and medical imaging
devices use externally-cooled CCD sensors, I've seen serious (or perhaps
slightly crazy) people pull apart thousand-dollar DSLRs to fit TEC (Peltier)
elements to them.
I've already said this before, but I'll say it here again: The most you can
say about sensor noise (optical, audio) is that any value you get for your
particular setup applies only for that specific combination of hardware,
software, and environmental conditions.  These devices are specifically
designed to get rid of as much noise as is physically possible, so can't be
relied on, in isolation, as noise sources.

@_date: 2014-02-21 14:54:40
@_author: Peter Gutmann 
@_subject: [Cryptography] Random numbers only once 
If some obscure system component the user has never heard of before doesn't
  have enough zarglemorf, it has the stop the system from functioning.
It's fair enough to make this a theoretical requirement, but it'll never fly
in practice.

@_date: 2014-02-21 23:35:19
@_author: Peter Gutmann 
@_subject: [Cryptography] RNG exploits are stealthy 
Conversely, advances over time can make your security better rather than
worse.  For example my entropy-polling code, when it was first written, was
limited to checking a few truly physical-randomness sources, generally timing
jitter on network packets and the like (alongside a great mass of generally-
unpredictable events related to the running of the OS).  Now it checks things
like fan speeds, GPU core temps, SATA error rates, CPU core voltage
variations, and a whole smorgasbord of other physical measurements that would
have been impossible to get when the code was first written.  The whole thing
is getting better over time as support for monitoring of more and more system
parameters is added.  So while in some cases we may be moving backwards, in
others we're moving quite a bit forwards.

@_date: 2014-02-22 01:25:50
@_author: Peter Gutmann 
@_subject: [Cryptography] RNG exploits are stealthy 
Not really.  The means of collecting environmental data just returns a memory
blob of whatever's available on the system.  One day/month/year it might be 5K
of data, the next it could be 20K as more hardware-monitoring sources are
supported by system hardware and software.  So it is silently getting better,
but not because of any changes in my code (once the initial get-environment-
noise code has been written).

@_date: 2014-03-01 14:33:14
@_author: Peter Gutmann 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
I was just about to say the same thing.  Even if you rewrote the entire think
in Haskell (the newspeak of programming languages in which it's impossible to
write incorrect code[0]), you can still produce something where some crypto
check is missed.  This isn't an issue of coding style, it's one of software
engineering practice (or lack thereof).
This could be, and should have been, caught with automated testing.  The
problem is that most (all?) testing of this type of code is along the lines of
"do the things that should happen, happen?", with very little testing of "do
the things that shouldn't happen, not happen?".  What happens if a bit in the
SSL handshake is flipped?  What if a bit in the payload data is flipped?  What
if the server presents cert A and signs with cert B?  What if a bit in the
signed DH parameters is flipped?  What if the hash has a valid signature but
it's the wrong hash for the data?  (Those are all self-checks that my code
performs, if there's anything else obvious that I've missed I'd love to hear
about it so I can add checks for that too).  These are trivial, automated
checks that you can run before you ship, and several of them would have caught
Apple's bug (the signature wasn't checked at all, so wrong-key, DH-parameter-
manipulation, and valid-sig-on-wrong hash would all have caught the problem).
[0] Some advocates of Haskell actually seem to believe this, which always
    provides for much entertainment during discussions.

@_date: 2014-01-05 18:39:47
@_author: Peter Gutmann 
@_subject: [Cryptography] nuclear arming codes 
This was developed over a number of years in the 1980s, read using an embedded
x86 system programming in Pascal (an impressive piece of image processing for
the time).  It was just getting ready for deployment when the Soviet Union
collapsed, and was replaced by a mechanism that was described to me as "magic
marker on the side of the missile".  There were some (hard-to-find) papers
published on it after the project was canned, it was basically PUFs two
decades before they were called PUFs.

@_date: 2014-01-06 18:48:28
@_author: Peter Gutmann 
@_subject: [Cryptography] Using Raspberry Pis 
Or an Alix PC running pfSense.  The advantage of this is the very mature software, you get the same out-of-box experience as an off-the-shelf product, except that its much more sophisticated than most home-market commercial products.  The upcoming Alix APU platform will offer even more power for those who need it.

@_date: 2014-01-07 14:16:04
@_author: Peter Gutmann 
@_subject: [Cryptography] defaults, black boxes, APIs, 
(warning: Some of those links take quite awhile to load).

@_date: 2014-01-17 01:22:40
@_author: Peter Gutmann 
@_subject: [Cryptography] Boing Boing pushing an RSA Conference boycott 
Since when was 3DES a broken construct?  In fact in the early-mid 2000's there
were several papers published that made AES look a bit shaky (none of the
attacks were developed much further, but we didn't know that at the time), so
sticking to 3DES, with its extra quarter century of provenance, was a
perfectly sensible move.  Even now, it's unlikely that any algorithm has
received as much attention and analysis as 3DES.

@_date: 2014-01-19 23:12:52
@_author: Peter Gutmann 
@_subject: [Cryptography] cheap sources of entropy 
... unless your camera, or the driver software, is doing postprocessing to remove noise.  If you're using a DSLR for this then you can just force it to return data in raw mode, but with anything else you have no idea how much noise is being removed by aggressive postprocessing.  Or, more importantly, whether a future software update won't change things so that your currently nicely noisy image is de-noised.
So by all means point your camera out the window at the leaves of a tree or
something similar, but don't rely on noise from a static image like a test

@_date: 2014-01-21 20:57:21
@_author: Peter Gutmann 
@_subject: [Cryptography] cheap sources of entropy 
If you're going to do that, you may as well go with relays:
Or if you need something a bit faster, try the MT15:
All you need is about 3,000 BC547/557s.  Working at that level, you *can*
actually check everything yourself.

@_date: 2014-07-16 14:34:29
@_author: Peter Gutmann 
@_subject: [Cryptography] VCAT report on NIST's process review 
It's the ISO 9000 of security measures, keep doing what we've always done but
now there's a Documented Procedure in the Quality Manual for it.

@_date: 2014-07-17 16:21:25
@_author: Peter Gutmann 
@_subject: [Cryptography] VCAT report on NIST's process review 
That hasn't stopped commercial CAs from doing this in the past (and, no doubt, in the future as well).  In terms of effectiveness, the CABF is rather less useful than the League of Nations.
That's why you declare yourself to be compliant before you get your root included, and only once you're in do you give any certificate to anyone who

@_date: 2014-07-17 16:25:02
@_author: Peter Gutmann 
@_subject: [Cryptography] multi-key encryption of "meta" data 
I can't speak for MS, but I know that the implementation folks at IBM weren't thrilled with the mess that it turned into either.  Hypothesising evil vendor conspiracies is always popular, but in this case I think it was just a standard result of design-by-committee [0].
[0] A sufficiently determined single author can come up with a     design-by-committee standard, so it's more a state of mind than a physical     entity.

@_date: 2014-07-25 16:13:57
@_author: Peter Gutmann 
@_subject: [Cryptography] hard to trust all those root CAs 
Yeah, and the judge and prosecutor who get your case will be helpless before
  your clever skills at evading them, because they've never had to deal with
  literal-minded people trying transparent dodges to get around the law
  before. You will doubtless enjoy the same success as tax protesters do when
  they end up in court.  And shortly thereafter, you'll enjoy an all-expenses-
  paid vacation with free room and board, courtesy of the US government.
This should be preserved somewhere as the standard response to the
Rumpelstiltskin Defence ("you can't prove I'm using crypto/know the keys so
you'll have to let me go").  This perfectly sums up what will happen to anyone
who wants to try the Rumpelstiltskin Defence in court.

@_date: 2014-07-26 01:31:24
@_author: Peter Gutmann 
@_subject: [Cryptography] "How to manipulate curve standards: a white paper 
How to manipulate curve standards: a white paper for the black hat
  Daniel J. Bernstein and Tung Chou and Chitchanok Chuengsatiansup and Andreas
  H\"ulsing and Tanja Lange and Ruben Niederhagen and Christine van Vredendaal
  Abstract: This paper analyzes the cost of breaking ECC under the following
  assumptions: (1) ECC is using a standardized elliptic curve that was
  actually chosen by an attacker; (2) the attacker is aware of a vulnerability
  in some curves that are not publicly known to be vulnerable.
  This cost includes the cost of exploiting the vulnerability, but also the
  initial cost of computing a curve suitable for sabotaging the standard. This
  initial cost depends upon the acceptability criteria used by the public to
  decide whether to allow a curve as a standard, and (in most cases) also upon
  the chance of a curve being vulnerable.
  This paper shows the importance of accurately modeling the actual
  acceptability criteria: i.e., figuring out what the public can be fooled
  into accepting. For example, this paper shows that plausible models of the
  ???Brainpool acceptability criteria??? allow the attacker to target a one-in-a-
  million vulnerability.

@_date: 2014-07-28 16:02:53
@_author: Peter Gutmann 
@_subject: [Cryptography] hard to trust all those root CAs 
This is somewhat difficult to follow, it's a discussion of legal minutiae around a set of amendments to a law? bill?, could you perhaps provide a brief interpretation for us?  Has the result been tested in court?

@_date: 2014-06-04 19:52:32
@_author: Peter Gutmann 
@_subject: [Cryptography] Java: The 1990s called, they want their keys back! 
This is for DH keys:
  java.security.InvalidAlgorithmParameterException: Prime size must be
  multiple of 64, and can only range from 512 to 1024 (inclusive)
  java.lang.RuntimeException: Could not generate DH keypair I know that you should never attribute to malice what is adequately explained
by stupidity but man, 512-bit keys in 2014 is a lot of stupid.

@_date: 2014-06-04 21:57:13
@_author: Peter Gutmann 
@_subject: [Cryptography] To what is Anderson referring here? 
It's not the patents, it's because you use certificates for situations where
EKE would be appropriate.  No other options (for example EKE) exist.  Look at
browsers, TLS-SRP and TLS-PSK have been standardised, and freely usable, for
years but no browser (or web server) vendor supports them, or is interested in
supporting them.  SSH is no better, you fire up a tunnel and hand over the
password in plaintext over it, or use public-key auth (OK, not certificates
but the SSH equivalent), there's no attempt at any EKE-like mechanism.  It's
the same for many other protocols, it's either certificates or passwords, and
that's it.

@_date: 2014-06-05 21:29:27
@_author: Peter Gutmann 
@_subject: [Cryptography] To what is Anderson referring here? 
I asked David Jablon, author of US Patent 6,226,383 (the only remaining one of
EKE and SRP that isn't freely usable) about this some time ago and his
response was a somewhat ambiguous mix between "I've earned a little off it but
not much" and "I'd rather have it not used at all than let it be used for
In any case I don't think it's the patents that have prevented use, it's the
perception that if you want to do authenticated key exchange you have to use a
PKI, anything else is unacceptable.  Heck, the IPsec folks more or less made
this explicit:
  all password-based authentication is insecure; IPsec is designed to be
  secure; therefore, you have to deploy a PKI for it
See my earlier message about this, TLS-PSK and TLS-SRP are freely usable and
have been for years; no-one supports them.

@_date: 2014-06-06 13:53:50
@_author: Peter Gutmann 
@_subject: [Cryptography] To what is Anderson referring here? 
Yup, and most of that came about because people realised that if you forced
users to deploy PKI as a precondition to deploying IPsec, IPsec would never
get deployed.  It's certainly a lot better now, but in the early days when
keying was supposed to be PKI-or-nothing, vendors got around the problem by
adding homebrew "management tunnels" to do the PSK (things like single-DES in
ECB mode, or only encrypting data in 8-byte blocks and leaving the rest in
plaintext because (a) they didn't know how to encrypt less than 8 bytes and
(b) "the little leftover bit won't be interesting anyway", or using a
hardwired key with an IPsec SA to communicate the PSK, or ...).

@_date: 2014-06-09 07:47:20
@_author: Peter Gutmann 
@_subject: [Cryptography] Help investigate cell phone snooping by police 
People can use the material that was left over from when they made the hat.

@_date: 2014-06-09 19:30:20
@_author: Peter Gutmann 
@_subject: [Cryptography] Licensing OCB (RFC 7253) 
This doesn't help though.  It's a fax-machine-effect problem, whether I have a
license to use is irrelevant, it's only useful if everyone else has a license
to use.

@_date: 2014-06-10 03:00:36
@_author: Peter Gutmann 
@_subject: [Cryptography] Licensing OCB (RFC 7253) 
Same here, I'm not trying to create some OSS-vs-whatever argument, just point
out what the real problem is:
That's not the issue though.  Lets say there's some super-duper new algorithm
(to avoid picking on OCB) called FooX that's available under RAND terms, and
that I license to put in my code, which gets used in SSL servers.  Unless
every single vendor of SSL software in the whole world also licenses FooX,
there's not much point in me deploying it because it's going to lock out
users.  The chances of everyone else in the world also licensing FooX are,
well, about zero, for the same reason why I don't license it.
That's what I meant by the fax machine effect, if you're the only person in
the world who owns a fax machine (or SSL server running FooX) then not only
does it have little positive value, it actually has negative value because it
prevents others from communicating with you.

@_date: 2014-06-11 19:19:51
@_author: Peter Gutmann 
@_subject: [Cryptography] Yet more formal methods news: seL4 to go open 
Uhh, I think you meant to say:
  If you don't know how to use OpenSSL, looking at the source is a big
  hindrance.

@_date: 2014-06-21 09:58:31
@_author: Peter Gutmann 
@_subject: [Cryptography] "Is FIPS 140-2 Actively harmful to software?" 
Supporting the "actively harmful" argument could go either way, but the case
for "actively worthless" is easier to defend.  In terms of verifying a crypto
implementation, your $100K FIPS-140 certification does less for you than a TLS
connection to amazon.com.
I was joking recently with some other security people that a much more
entertaining way of getting what FIPS 140 gives you, namely an indication of
how desperate a vendor is to sell to USG customers, would be to post a Youtube
video of yourself setting fire to a $100K mound of US$20 notes.

@_date: 2014-03-02 03:06:24
@_author: Peter Gutmann 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
I doubt there are, because to do this kind of testing you need to poke around
deep inside the internals of the crypto library.  What you're doing is
generating incorrect or malformed output in a controlled manner, which isn't
generally something that's supported in standard code.  In fact you don't even
want the capability to do this present in your code (in my case you need to do
a custom build) because it's rather dangerous to have sitting in there.

@_date: 2014-03-03 21:17:26
@_author: Peter Gutmann 
@_subject: [Cryptography] Btcoin -- bubble or investment opportunity? 
tl;dw :-).
I've heard it described as a science experiment that got out of hand, which I
think is pretty accurate.

@_date: 2014-03-03 21:21:28
@_author: Peter Gutmann 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
It's a lot more than just fuzzing, you need to do things like "create a
message A, flip a bit in it to get A', MAC it, and then unflip the bit so A is
sent but with a MAC for a A'", or "send out a cert chain for one key but then
sign the DH exchange with a different key".  You don't get that with random
mutation, it requires custom code for each situation.

@_date: 2014-03-04 19:55:14
@_author: Peter Gutmann 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
That's because gcc's '-Wunreachable-code' does nothing.  That is, the compiler
accepts it as an option but doesn't do anything in response to it.  And don't
even get me started on the total braindamage of gcc's nonnull and ignored-
return-value handling, for which the latter is totally useless and the former
is so broken that it's too dangerous to ever use, making it literally worse
than useless.
(Yet another of the many reasons why the sooner clang/LLVM kills off gcc, the

@_date: 2014-03-05 01:20:28
@_author: Peter Gutmann 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
Looks like Apple don't have the only TLS implementation that went to fail,
GnuTLS did it too:
Details at

@_date: 2014-03-06 23:30:41
@_author: Peter Gutmann 
@_subject: [Cryptography] GnuTLS -- time to look at the diff. 
The fact that both of the publicised problems that have just been noticed is because of a 'goto fail' doesn't necessarily point to a goto crisis, since it's an arbitrary coding choice I doubt there's any more, or less, goto-ing in secure apps than in anything else (see the example Viktor Dukhovni's post from a few days ago showing the range, from thousands of gotos used in OpenSSL to zero in Postfix).  I've just grepped my cert-checking code, all 1.5MB of it [0], and there's a single location that uses gotos, to break out of the middle of a really complex state machine that handles the parsing of nested/overridden ASN.1 tags [1].
OTOH the goto mess in the Apple/GnuTLS code is really a sign of a much larger problem in the way that the coding is being done.  If you look through the posted GnuTLS code for example it seems pretty undisciplined, hardcoded explicit values, confusion over return values, and all sorts of other things.  Using an example I'm pretty familiar with, in my code for reporting status values I have a global set of fixed, well-defined status values, a universal macro cryptStatusError() that checks whether a function failed, and I use compiler-enforced checking (and three different static analyzers) to make sure that a function status check isn't skipped.
So it's not a simple "goto = bad", it's excessive use of gotos being one (of many) signs of what appear to be bad/unsafe coding practices.
[0] That's more than the code needed to implement SSL/TLS, SSH, PGP, and PKCS
     combined.
[1] If anyone thinks they can refactor this in a cleaner way with no gotos,
    I'll send them the code.  I'm sceptical...

@_date: 2014-03-07 21:04:52
@_author: Peter Gutmann 
@_subject: [Cryptography] GnuTLS -- time to look at the diff. 
There's the NIST test suite, but (a) it's about a decade old, (b) there are
errors in it, and (c) it tests, among other things, a bunch of crazy stuff
that not only no-one cares about but that no sane implementation should
actually do.  It is a reasnably thorough test of your cert-handling code
(I can't imagine the amount of therapy the poor person who created all the
certs had to go through afterwards to recover).

@_date: 2014-03-09 00:44:42
@_author: Peter Gutmann 
@_subject: [Cryptography] GnuTLS -- time to look at the diff. 
It's also a scary figure, because error-handling code is the code that almost
never gets tested.  A friend of mine once took a widely-used software security
tool and ran its test suite inside a wrapper that on successive runs made the
first malloc fail, the second malloc fail, and so on, a simple but effective
way of exercising quite a lot of error-handling code paths.  It was lucky that
he hard-limited the number of coredumps to 10,000 before he ran it...

@_date: 2014-03-13 17:14:18
@_author: Peter Gutmann 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
Unless it's the embedded world (which makes the PC and phone and whatnot
market seem trivial by comparison), in which performance stays constant and
cost goes down.  Ten years ago your code had to run on a Cortex-M.  Ten years
from now your code will need to run on more or less the same Cortex-M, only
it'll be cheaper and have more integrated peripherals.

@_date: 2014-03-14 17:58:06
@_author: Peter Gutmann 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
The problem this runs into is that in the embedded world security is job after reliability/availability, reliability/availability,
reliability/availability, reliability/availability, reliability/availability,
reliability/availability, reliability/availability, and
(A lesser problem is that the devices are mostly built by hardware guys for whom software development consists of "it compiles, ship it".  Obviously that's an over-generalisation, but I don't know how much embedded stuff I've looked at that has beautifully-engineered hardware and software that looks like someone's incomplete undergrad project.  That's an issue that you can try and legislate though, with the emphasis on "try").

@_date: 2014-03-16 21:34:51
@_author: Peter Gutmann 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
Yep, when they've adopted wholesale an existing standard done by a small
handful of talented people.  This is why I like the competition-style approach
to design, you short-circuit the design-by-committee approach and pick out the
best design from several small, hopefully-talented groups.

@_date: 2014-03-17 15:58:51
@_author: Peter Gutmann 
@_subject: [Cryptography] Focus 
ITYM "the Voynich Manuscript has been reported partially decoded, the latest
in an ongoing series of reported decodings".  Until there's academic consensus
on it, I'd rather leave this one as undecided.

@_date: 2014-03-17 16:28:08
@_author: Peter Gutmann 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
What we're lacking to guide us in making decisions is any proper empirical
data.  The arguments for speedups-ueber-alles all tend to be along the lines
of "we need faster X, here's an anecdote about performance issues at the ISP
my drinking buddy Dave works for, therefore we need faster X".  There's very
little empirical data out there about what is and isn't possible, and what is
and isn't needed.  The example I keep using of where this leads to is the
ridiculous requirements imposed on smart meters, in which regulators throw
everything in whatever textbook they read recently at a system with 8kB of RAM
and an 8051.  The result is something that often doesn't provide the security
that's required (or at least if someone sat down to write out a threat model,
which is rarely the case, then the "security" measures would do little to
address it), and is physically impossible to implement in the target device.
The result is fig-leaf security, doing just enough [0] to claim some sort of
compliance with the requirements without actually providing any security.
[0] "Just enough" doesn't necessarily mean doing the bare minimum, it's often
the case that the hardware can't do anything like what's required, so the
result is compliance engineering, doing whatever the hardware can manage while
still squeaking past the compliance requirements.

@_date: 2014-03-17 18:27:30
@_author: Peter Gutmann 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Only MD5 as a hash, not as a MAC, was faulty.  Also, the combination used in
SSL was MD5-and-SHA1, not either-MD5-or-SHA1, or something else where you
could exploit the weakest link.  So "moving painlessly to SHA1" actually
weakened the overall mechanism rather than strenghthening it.
... even if they weren't necessarily sane ones, like X9.42DH + DSA for S/MIME.
Yup, that's exactly what happened with S/MIME.  Luckily the standards were
adapted to match what everyone was doing anyway.

@_date: 2014-03-19 12:43:40
@_author: Peter Gutmann 
@_subject: [Cryptography] How to build trust in crypto (was:recommending 
The problem with Interlock is that it doubles the number of exchanges
required.  Fortunately there's a simple optimisation that addresses this, you
just need to bundle the two message halves into the same exchange.  If there's
any interest in this optimisation I can probably get an RFC for it out by
about April 1.

@_date: 2014-03-19 13:01:34
@_author: Peter Gutmann 
@_subject: [Cryptography] Client certificates as a defense against MITM 
I think this is the prose version of the famous XKCD cartoon:
The (elided) text above is the "A crypto nerd's imagination" part.  The "What would actually happen" part is that the two guys get about three or four hours into the calisthenics required (which would be about the second or third paragraph in the description) and then decide "Bugger this, I'm off to my favourite bar to see if I can pick someone up there, and damn the mutaween".
It's no good having a system so super-duper-secure that you can use it to
protect nuclear weapons launch codes if it's so hard to use that no-one can do
anything with it.

@_date: 2014-03-19 15:42:17
@_author: Peter Gutmann 
@_subject: [Cryptography] Apple's Early Random PRNG 
Some of them already do, but they're so badly done that they're effectively
useless: No way to check for their presence a la Intel's CPUID (you just have
to know that the SoC you're using supports it), no way to access it from user
space, weird restrictions on how it can be used (you need to set up various
hardware registers in specific ways, there can be access conflicts if
different tasks try and get at things at the same time, etc).  So it's there
(at least in some cases), but you can't have it.

@_date: 2014-03-20 00:15:36
@_author: Peter Gutmann 
@_subject: [Cryptography] Client certificates as a defense against MITM 
In that case it would be good to see the mapping from technical detail to UI
aspects, this is one of those things that historically has proven more or less
impossible to make very usable, so it'd be interesting to see how the UI
aspects are translated to the underlying technicalogy.

@_date: 2014-03-20 22:27:17
@_author: Peter Gutmann 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Given that { username+password * no_internet_users * no_sites_used } is
somewhere in the hundreds of billions, at what point does it stop scaling?
It's PKI that doesn't scale.  Like ethernet, passwords work in practice but
not in theory, and vice versa for PKI.
It's PKI that enables MITM CAs in the first place.  Since they can't occur for
PSK, you don't need to worry about trying to detect them.  The evidence of
MITM CAs is a sign of a fundamentally broken design, not a "feature" of PKI.

@_date: 2014-03-24 15:15:09
@_author: Peter Gutmann 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
+1.  I generally won't use a new algorithm until it passes the five-year test,
meaning it's been five years since its introduction without any flaws found.
Remember how, after Rijndael was chosen as the AES winner and everyone was
falling over themselves in their rush to make everything use the shiny new
algorithm, there were a series of papers published on various algebraic
aspects of AES that looked like they could lead to a break if the analysis was
moved forward another step or two.  That step was never found, but it would
have been a major catastrophe if it had, while anyone who stuck with 3DES for
awhile longer would have been perfectly safe.
The SHA-3 competition has actually lead to a somewhat odd situation in which,
due to the new analysis techniques introduced for that, we now know that SHA-2
is actually quite strong (not to mention faster, and already widely deployed).
This is limiting the rush to SHA-3 in a manner that wasn't present for AES.

@_date: 2014-03-26 14:19:05
@_author: Peter Gutmann 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
For those who missed his Hashdays talk, you can find the details of a banana
attack here:

@_date: 2014-03-31 12:51:38
@_author: Peter Gutmann 
@_subject: [Cryptography] ADMIN Re:  Amateur Radio Authentication 
Can I vote to have it reopened?  It's a fascinating perspective on a form of
communication that crypto folks more or less ignore (or, more likely, don't
know exists), so having some discussion of it will help balance the usual
"assume infinite bandwidth with possibly a bit of latency" assumptions applied
to anything involving encryption.  Case in point, from a current debate on the
TLS list:
  Documenting use cases is an unnecessary distraction from doing actual work.
  You'll note that our charter does not say "enumerate applications that want
  to use TLS".
(Yes, someone actually said that).

@_date: 2014-05-03 19:40:49
@_author: Peter Gutmann 
@_subject: [Cryptography] One third IT managers think homomorphic is 
You're looking in the mirror here and seeing people like yourself making the
decisions.  They're not, they're IT managers.  They're doing the crypto in the
cloud, with the keys in the cloud.  After all, the data's already there, so if
you trust the cloud with your data you can also trust it with your keys.

@_date: 2014-05-05 07:16:10
@_author: Peter Gutmann 
@_subject: [Cryptography] One third IT managers think homomorphic is 
It shouldn't really boggle the mind, the argument "if you trust the cloud with
your data you can also trust it with your keys" is one I've heard again and
again, it may sound strange to a security geek but to an IT manager it makes
perfect sense.  They may be opposed (at least on principle) to putting
sensitive data in the cloud, but once the data is there, the keys follow
naturally.  In any case it's not much different from having your data and keys
on a dedicated machine in a data centre, it's just a bit more... cloudy.
(In addition, for empirical data on this, look at some of the studies that
have been done on extracting keys from things like EC images).

@_date: 2014-05-13 17:03:25
@_author: Peter Gutmann 
@_subject: [Cryptography] What faults would you inject to test crypto 
As a number of recent research papers (and published vulnerabilties) have
highlighted, quite a lot of implementations of security protocols and
mechanisms don't really detect problems with everything from invalid
signatures through to the crypto verifying but whatever it is that's verified
being for the wrong web site.
One way of checking that your implementation doesn't have (some of) these
problems is through testing via fault injection, creating some failure like
the presence of corrupted data leading to an invalid signature and then making
sure that it's detected.  Problem is, what sort of faults do you inject?  The
reductio ad absurdum approach is that you need to test every bit of every byte
of any protocol, but what we're looking for here is high-level implementation
flaws in which crypto mechanisms aren't applied properly (and we're assuming
that something like a MAC failure will occur whether we corrupt bit 1 of byte
1 or bit 8 of byte n).  So, what sort of faults need to be injected to test
for typical flaws?  What I've been using is:
  SSH and SSL/TLS:
    Wrong certificate/key
    Handshake message corruption
    Payload data corruption
    Bad signature - wrong hash value
    Bad signature - data corrupted
  S/MIME and PGP:
    Wrong certificate/key
    Bad signature - wrong hash value
    Bad signature - data corrupted
    Bad signature - signed attributes corrupted
Can anyone think of anything else that needs to be checked?  I'm looking for
faults that check for specific failures, not something like "check every X.509
extension in every certificate in an SSL server's cert chain".

@_date: 2014-05-13 23:50:35
@_author: Peter Gutmann 
@_subject: [Cryptography] What faults would you inject to test crypto 
There's no specific check to catch it, or more specifically you don't need to
code in a special-case check for it, since it would be caught by any of:
all of which will lead to a sig-check failure for various reasons.

@_date: 2014-05-15 16:39:04
@_author: Peter Gutmann 
@_subject: [Cryptography] What faults would you inject to test crypto 
A way to do that is to change the sequence number in SSL/TLS/SSH, which is a
simple fault to inject.

@_date: 2014-05-21 00:01:47
@_author: Peter Gutmann 
@_subject: [Cryptography] Facebook on the state of STARTTLS 
You're using the wrong analogy there.  The PKI community (at least the small
bits of it that are grounded in the real world) has known for a decade or
more, and the rest of the world has discovered in the last year or so, that
beyond "make the browser warnings go away", the usefulness of PKI is pretty
limited, and in particular it doesn't protect you against any serious attack
unless you've had your PKI set up and evaluated and tested ad nauseam by
experts.  So a better analogy would be from the field of medicine:
Cost of a consult with Dr.Google and meds from an online pharmacy: $50.
Cost of a consult with a medical specialist = $2,000.
Cost of treatment = $200/hr.
Which of those two would you choose to deal with your skin/lung/stomach
(A quick Google indicates that you can actually buy homeopathic cancer cures,
see e.g.   It appears to top out at
about $25 though, not $50:   Maybe they
can dilute it another 50% for the $50 version).

@_date: 2014-05-23 17:24:52
@_author: Peter Gutmann 
@_subject: [Cryptography] How secure are hashed passwords? 
Troy Hunt has a long discussion about the stupidity of this at:

@_date: 2014-05-28 11:47:01
@_author: Peter Gutmann 
@_subject: [Cryptography] client certificates ... as opposed to password 
That's 192 pages of very dense slides, I'd see it as more of an argument
against using client certs than anything else.  Compare that to this article:
which runs to about a dozen pages when printed, which tells you how to write
your own Linux kernel module.  That's less than one-tenth the size, and it's a
guide on writing not some basic hello world program but a kernel module.
So I think the lesson from that would be "if it takes 192 pages of text to
explain how to do X then you probably shouldn't be doing X".

@_date: 2014-06-01 01:33:38
@_author: Peter Gutmann 
@_subject: [Cryptography] What is going on with TrueCrypt? 
While people are throwing out random scenarios, here's another: We know that
the TC developer (and I'm going to use the singular here because I'd be
surprised if there was more than one person working on it) sets great store by
the "plausible-deniability" features of TC.  The warning text "Using TrueCrypt
is not secure as it may contain unfixed security issues" could simply mean
that they've talked to a lawyer and found out that all of the plausible
deniability in TC isn't and doesn't.  So if the primary goal of TC was to
create a P-D data-at-rest encryption solution rather than just a pretty good
generic data-at-rest encryption solution then this may have motivated him (and
yes, I'm using that one consciously as well) to give up further work on it.
(Hey, I reckon that's at least as random as anyone else's speculation :-).

@_date: 2014-11-02 05:47:33
@_author: Peter Gutmann 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
It's... not entirely useful, it uses a rather abstract model of faults that
include things like register faults in which a value in a CPU register is
corrupted, but many modern CPUs (Intel, ARM, etc) have ECC on internal storage
and memory buses, and have had them for years, so that type of fault seems
unlikely.  That means that you're left with faults on external memory, from an
off-list discussion with someone who's experimented with this it's a fairly
remote possibility for affecting the crypto (alpha particles aren't
predictable and guidable, and even if they were you'd have to hit exactly the
right memory location at the right time to have an effect).

@_date: 2014-11-05 07:41:33
@_author: Peter Gutmann 
@_subject: [Cryptography] $750k Fine for exporting crypto 
That was my feeling too when this was first reported: Someone's being hit with
the crypto-controls universal hammer for something else, either something
they've done or something they're not doing ("we want you to do X for us, it'd
be a real shame if we suddenly found out that one of your products violated
some export control or other").  These things don't just suddenly pop up for
no reason...

@_date: 2014-11-08 02:27:51
@_author: Peter Gutmann 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
An attempt to summarise the discussion, both on- and off-list:
- Deliberate fault attacks are really hard, even under artificial conditions
(RSA run in a tight loop and not much else present) because the chances of
causing an appropriate fault, hitting a key bit, and getting it at exactly the
right moment are pretty remote.  From an off-list discussion, quoted with
permission from the author:
  Carrying out these sorts of attacks is actually very difficult. I used a
  microprobing station that a friend's failure analysis lab had to get an
  Am241 source right over the memory of a microcontroller (87C51, IIRC). That
  was actually tricky to do. This microcontroller just ran a loop to carry out
  RSA again and again. And even in those fairly optimal conditions, actually
  getting [bit-flip fault] that caused the right kind of error took a very,
  very long time. So although this is theoretically possible, and is
  potentially a good way to get funding to buy interesting toys for a lab, I'd
  say that it's really not even close to being practical.
- Accidental fault attacks caused by inadvertent bit-flips on data held in
memory over a period of time are scarily common, see e.g. the Bitsquatting
issue, discovered by Artem Dinaburg,
and investigated further by Duane Wessels
 and
Nick Freeman
So the most appropriate defence appears to be chained/overlapping checksumming
of data to detect memory corruption:
  checksum keying data;
  perform operation;
  verify checksum on keying data to make sure post- == pre-;
For private keys this requires multiple levels of chaining.  Assuming a
conventional process of read of a private key from backing store that decrypts
it and loads it from its now-decrypted but marshalled form into some internal
bignum format ready for use, there are three stages, encrypted data, decrypted
data, and data loaded into bignums (and, obviously, another check on the
bignum when it's actually used for crypto):
  checksum1 = check( encrypted key data );
  MAC-verify encrypted key data;
  decrypt encrypted key data;
  checksum2 = check( decrypted key data );
  load decrypted key data;
  checksum3 = check( loaded key data );
  if( checksum3 != check( re-load of decrypted key data ) )
    error; // Decrypted vs. bignum data differs
  if( checksum2 != check( decrypted key data ) )
    error; // Decrypted data was changed
  if( checksum1 != check( encrypted key data ) )
    error; // Encrypted data was changed
In other words if the key data is corrupted after the MAC check then checksum1
will detect it, if it's corrupted between decryption and load then checksum2
will detect it (with a very slight race condition between decrypt and check()
that would require storing a second MAC of the plaintext alongside the MAC of
the ciphertext), and finally if it's corrupted after it's loaded then the
second load attempt will detect it since the loaded values will differ from
the first load attempt.
This should catch the likely attacks.  If an attacker can flip a bit after a
checksum computation, wait for it to be used in the next stage, and then flip
it back before the verification checksum is computed then you can evade this,
but if an attacker has that much control over your system then you've got
bigger things to worry about.

@_date: 2014-11-08 08:43:15
@_author: Peter Gutmann 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
I saw that, but I'm not sure how much of a problem it'd really be: The MAC
check would catch stuck bits when the encrypted data is read into memory, and
the checksum3 check would catch stuck bits when the marshalled data is loaded
into bignums.  The only part with missing coverage would be when the data is
decrypted, but for that a pairwise consistency check (not shown in the
pseudocode, since it was for illustrating catching faults) should catch
Another way of checking for purely stuck bits (rather than more complex
pattern-based failures, which are arbitrarily hard to detect) would be to
write 0xAA and 0x55 patterns into memory before storing data into it.
OTOH I don't have any data for this failure mode in current systems, for the
bit-flip failures we have plenty of real-world measurements, but what's the
likelihood, and typical failure patterns, for stuck-bit failures?

@_date: 2014-11-09 10:49:36
@_author: Peter Gutmann 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
Another option is to perform a pairwise consistency test each time the private key is used.  In other words every time you generate a signature you then use the public-key components to verify it (which my code does anyway just as a general precaution).  Question: Will this catch all possible problems?

@_date: 2014-11-12 21:55:05
@_author: Peter Gutmann 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
What I meant was "all posible problems within the context of the discussion",
i.e. bit flips, memory corruption, stuck-at faults, etc.  Dealing with ebola,
global warming, and world hunger were implicitly excluded from the problem
So the question remains, would a pairwise consistency test detect any memory- based problems, or would you still need checksumming to detect specific changes in the public and private keys where the RSA/DLP operation succeeded on a modified form of the key?  And as a corollary, can you modify the key values so that you leak the private key but the pairwise check still succeeds (in other words you don't just replace key parameters wholesale to meet the "modified key" criteria).

@_date: 2014-11-20 20:27:37
@_author: Peter Gutmann 
@_subject: [Cryptography] New free TLS CA coming 
That was my immediate reaction as well.  CACert has been given the runaround
for more than just four years, it's been more than a decade, and yet as soon
as a Mozilla-sponsored CA turns up it's in.
Perhaps someone from Mozilla would be able to explain what the difference is
that gets Let's Encrypt immediate acceptance while CACert has been left out in
the cold for more than a decade.
[CC'd to the relevant Mozilla list]

@_date: 2014-11-20 20:30:51
@_author: Peter Gutmann 
@_subject: [Cryptography] New free TLS CA coming 
So buy a used FIPS 140 level 3 device off eBay for $20, and you're done (if
you're lucky you may even find someone else's CA keys in it).  If that's the
silly-walk requirement they've set in order to join the club, and given that
the marginal cost is close to zero, do the silly-walk for them and move on.

@_date: 2014-11-27 15:01:04
@_author: Peter Gutmann 
@_subject: [Cryptography] [cryptography] Underhanded Crypto 
ianG  quotes:
Since some of the organisers read this list and this question may be relevant for other contributors as well I'll ask it here: How much code are you expecting?  In theory a submission for "an encrypted chat protocol" could involve sending in a reimplementation of TLS from which it'll be pretty hard to dig out the backdoor due to the sheer mass of code involved.  Can contributors send in code snippets with assumptions like /* Both sides have a shared authentication key */ or /* Both sides have exchanged fresh nonces */ to save having to send in 1,000 lines of code to implement this?  This would allow the reviewers to focus on the code containing the backdoor rather than having to dig through vast masses of support code.
Even then it's going to be really hard to review, if I send in code containing something like /* 2048-bit MODP Group from RFC 3526 */ someone's going to have to do a byte-for-byte comparison ("is that an 'E' or an 'F'?") of the entire thing to see whether it really does match RFC 3526.  And that's an easy one, if I decide to use parameters from GOST 0177545, held in the Zheleznogorsk Academy of Sciences and currently buried under 3m of snow, who knows how the organisers will verify it.
I think the contest needs a few more rules :-).

@_date: 2014-11-28 17:31:03
@_author: Peter Gutmann 
@_subject: [Cryptography] [cryptography] Underhanded Crypto 
So sending in a copy of openssl-1.0.1j.tar.gz and saying "there's security
holes in this, see if you can find them" is out as a strategy? :-).
That's what I was planning to do, with lots of the detail abstracted away,
since a full implementation would probably make it impossible to spot the flaw
due to the huge amount of complexity that's required for a reasonably secure
crypto protocol.
That's actually a bit of an indictment of crypto protocol implementations,
that they have to be so complex that there's no easy way to verify them any
more.  I've noticed this with crypto in embedded systems, the entire RTOS and
control system is a minimal, fairly easily-assessed system all nicely done to
something like IEC 880, and then the security/crypto portion is five times the
size of all the rest combined and so complex you can never really be sure it's
doing what it's supposed to.

@_date: 2014-11-28 23:59:07
@_author: Peter Gutmann 
@_subject: [Cryptography] [cryptography] Underhanded Crypto 
It's not really "giving it a shot" in my case, it's taking crypto
implementation mistakes so old that people have forgotten about them and
adding them to recent code.  All you need to do in theory is plough through a
bunch of old CVEs and update the use from (say) SSH 1.2.09 to something
current, and you're done.

@_date: 2014-11-30 23:30:20
@_author: Peter Gutmann 
@_subject: [Cryptography] [cryptography] Underhanded Crypto 
I think a major problem with them is the design-by-committee that reached its
apex in IKEv1.  If you look at something like TLS or SSH there are a million
negotiable options, most of which will never be used but that an
implementation has to support in the name of standards-compliance or
flexibility or marketing or whatever (see Ian Grigg's "There is only one
cipher suite and that is   In practice all you need is for both sides to
exchange nonces and DH y values authenticated with RSA or a PSK, and after
that use AES and HMAC for further communications.  No negotiation, no options,
nothing.  This is what you're getting anyway most of the time with TLS (except
for Firefox which inexplicably puts Camellia before AES), but no protocol will
allow you to do that, you need to be able to specify a million options and
parameters, with both SSH and TLS having nondeterministic behaviour where you
have to retrace earlier protocol steps if a parameter or option that arrives
at a later date changes something that you've already done.
It's not that crypto protocols are inherently complex, it's that the process
that creates them yields horribly complex designs.
I don't know if you'd need a large variety, or even a small one, I think the
basic design above would cover the large majority of all cases where a secure
tunnel from A to B is required.  Even if you're using SSH or TLS, you're
probably not doing more than what's described above anyway, just in a really,
really complex roundabout manner.

@_date: 2014-10-02 16:03:51
@_author: Peter Gutmann 
@_subject: [Cryptography] Internet of Things and small cheap ASICs? 
That question would take a small essay to answer (even defining IoT would take
a small essay, I'm going to map it to SCADA-like systems rather than a Twitter
feed to the LCD panel on your fridge), so I'll just reply with a few bullet
points to cover the main issues:
* The infrastructure is stuck at about the Windows 95 level of security, and
isn't getting any better.
* There's no obvious driver for improvement.  With Win95 (and NT) it was
global worms and the fact that you had one of these things on every desktop,
but if your thermostat reboots itself every now and then because it's part of
a botnet no-one will notice or care much.
* Availability and safety trump security in every case.  Having a hundred-ton
hydraulic press take someone's fingers off because of an expired certificate
(although I'm not quite sure how that particular case could happen) is a no-
* After availability and safety comes cost.  Security comes in at about
position 100 in the feature priority list, with the first 80 slots being taken
up by "availability/safety".
* The security model for IoT (in the form of SCADA-like devices) has always
been not to hook them up to a WAN.  Unroutable serial protocols helped here.
For more recent devices, the security model is "block it at the firewall".
* Oh, and assume it's insecure by design.  You'll rarely be disappointed.
* To finally answer the question, see any work on securing things, the OWASP
guides, static source code analysis tools, a roomful of books on secure coding
and pen-testing, etc.

@_date: 2014-10-02 20:18:39
@_author: Peter Gutmann 
@_subject: [Cryptography] Best Internet crypto clock ? 
Look for work on secure/authenticated audit logs, e.g. google
"cryptographically secure audit logs" (there's an awful lot of material out
there, too much to post individual references to).

@_date: 2014-10-06 18:42:30
@_author: Peter Gutmann 
@_subject: [Cryptography] 1023 nails in the coffin of 1024 RSA... 
Having had to deal with waaay too many optimisation bugs in gcc (including one
just found in in 4.8.2 that produces totally incorrect code when optimisation
is enabled) this may not be too far-fetched.  It seems like every new release
of gcc has a new set of code-generation bugs, but unless you run extensive
test suites (which a crypto library typically will, or at least should) you
won't notice them.  The advantage of a crypto test suite is that slight errors
are much easier to detect when everything is cryptographically secured, while
they'd slip by unnoticed in other cases.
That's the scary thing about this, the the buggy code that gcc can generate
will appear to function just fine in 99.9% of cases [0], it's only if you
include lots of internal consistency checks that you'll catch these can-never-
occur cases (in my case I have both a huge test suite and large amounts of
internal self-checks that caught this problem, even if I haven't been able to
track down the exact location and after a day of poking through disassembled
code I'm tempted to just say "get a less buggy compiler").
So a quick question for the people behind this:
- What version of gcc was this present for, and what level of optimisation was
  used?
- Does it still occur with a different gcc version?
- Does it still occur if you disable optimisation?
- Does it still occur if you use a less buggy compiler like clang/LLVM or MSVC,
  or in fact anything but gcc?
If, and that's a big if, this is real, I'd say the probability that it's yet another gcc compiler bug is far, far higher than the probability that it's some fundamental flaw in RSA or the RSA implementation.
[0] Figure freely pulled out of thin air.

@_date: 2014-10-07 03:20:17
@_author: Peter Gutmann 
@_subject: [Cryptography] 1023 nails in the coffin of 1024 RSA... 
Oh, I didn't mean the problem was a mismatched p and q but that it could have
come about because of some other code-generation error.  I've seen gcc in the
past generate output code that bears no relation to the source code that's fed
to it, it could be that they discovered some combination of gcc release and
target platform that produces broken code.
Or at least that seems a less unlikely explanation than their mismatched-p-q

@_date: 2014-10-08 17:55:41
@_author: Peter Gutmann 
@_subject: [Cryptography] Best internet crypto clock 
These already exist, and have been in use for many years:

@_date: 2014-10-09 18:37:55
@_author: Peter Gutmann 
@_subject: [Cryptography] Best internet crypto clock 
You don't need a perfectly accurate clock, you just need to track the drift at
the receiving end.  In fact is you can bound the transmission latency (if it's
an online protocol rather than store-and-forward) you don't need a clock at
all on the sending device.

@_date: 2014-10-11 21:24:54
@_author: Peter Gutmann 
@_subject: [Cryptography] Sonic.net implements DNSSEC, 
So just to make sure I'm getting this right, Sonic are sending out DNSSEC-
authenticated but invalid/spoofed/however you want to label them DNS
responses?  As you say, the very thing that DNSSEC was designed to prevent?

@_date: 2014-10-14 00:48:20
@_author: Peter Gutmann 
@_subject: [Cryptography] [messaging] Gossip doesn't save Certificate 
Yup, and that's been proposed in the past (late 1990s) as a way of getting
away from X.509's 1970s origins in offline systems.  Instead of asking a
source for a certified copy from some self-appointed authority (certificate
from a CA) and then groping around for further information to check whether
the certified copy you've just fetched is actually valid (CRL), you just ask
the authority directly, "give me the currently-valid, known-good key for X"
(pin from Google).  This short-circuits all of PKI.
For some reason it hasn't proven too popular with CAs and browser vendors.
It's just a very roundabout way of implementing the "give me a known-good key
for X" described above without disintermediating the CAs.

@_date: 2014-10-17 00:21:07
@_author: Peter Gutmann 
@_subject: [Cryptography] HP accidentally signs malware, 
X.509 doesn't handle this situation by design.  More than a decade ago a a
neverValid revocation flag to handle this type of situation was proposed and
rejected because That's Not How PKI Is Supposed To Work:
  we cannot allow a status which implies 'please unwind all transactions using
  this certificate, the purchaser must return the goods and a refund will be
  issued' as this removes all the certainty which PKIX is trying to provide.
PKI provides absolute certainty, dammit, and to have any kind of facility that
even hints otherwise is heresy.

@_date: 2014-10-24 22:53:09
@_author: Peter Gutmann 
@_subject: [Cryptography] Samsung Knox 
While this again much confirms my opinion of the value of security
certification programs (although the fact that it was CC did surprise me
slightly, I would have expected it from FIPS 140 but I thought CC was a bit
better than that), I wonder what'll happen to the certification?  Will it be
withdrawn, or will the issue just be ignored.

@_date: 2014-10-25 05:27:38
@_author: Peter Gutmann 
@_subject: [Cryptography] Simon, Speck and ISO 
Does anyone know what this is?  It isn't the current incarnation of ISO 9979
is it?  That was created in the early 1990s when the ISO decided, all by
itself, without any external pressure applied at all, not to standardise any
crypto algorithms.  The register was created as a compromise where the ISO
appeared to do something but didn't really do anything, it's just a number-
allocation method and nothing more.  It's also been, after an initial burst of
registrations for algorithms like DES, IDEA, and RC2/4, a graveyard of
algorithms that no-one cares about (FWZ1, SPEAM1, CIPHERUNICORN, Triplo,
FSAngo), if you have something that you can't get standardised anywhere then
you dump it into 9979 (and possibly now the ISO JTC1/SC27 registry).

@_date: 2014-10-26 07:11:52
@_author: Peter Gutmann 
@_subject: [Cryptography] Arduino Enigma simulator 
For those who want a workalike Enigma without shelling out a fortune for the
real thing:

@_date: 2014-10-27 09:57:13
@_author: Peter Gutmann 
@_subject: [Cryptography] Best internet crypto clock: hmmmmm... 
There is, however, an ongoing battle between the device manufacturers and
people who use these artefacts, since the manufacturers see them as flaws and
try and eliminate them once they're pointed out.  This makes it really
annoying for people who use them for image-source authentication purposes.

@_date: 2014-10-28 07:19:38
@_author: Peter Gutmann 
@_subject: [Cryptography] Best internet crypto clock: hmmmmm... 
That only applies if the final consumer of the image is a human being.  In
this case it's not human eyeballs that the image is intended for, and the
sensor manufacturers aren't optimising for that target.

@_date: 2014-10-28 07:45:12
@_author: Peter Gutmann 
@_subject: [Cryptography] Paranoia for a Monday Morning 
Interesting that you should mention this, I was talking today to a PKI
practitioner (so not someone who charges you $50,000 to tell you how wonderful
your PKI will be when it's working, but someone who actually has to get it
working) and they mentioned that while the geeks are worrying about whether
they can roll over their SHA-1 certs and whatnot quickly enough and when
attackers will start forging certs, what's really hitting them is the fact
that it needs constant shepherding and tweaking and maintenance to keep it
running.  So the problem isn't one of security but one of availability, that
once you've tied your infrastructure to the inflexible rigidity of a
cryptographically-bound system your concerns will be running your
organisational processes within that straightjacket and not any actual attacks
that the straightjacket may or may not be preventing.
(And as you say, the attacks aren't against the crypto anyway, but against all
the other stuff, completely ignoring the presence of the crypto.  Insert
Shamir's Law quote here).

@_date: 2014-10-29 07:05:12
@_author: Peter Gutmann 
@_subject: [Cryptography] EMV as a fraud enabler 
Brian Krebs has an interesting writeup at:
on EMV as a fraud enabler.  The trick is that instead of spoofing non-EMV on
an EMV card, you spoof EMV on a non-EMV card because it's assumed to be secure
so there's less checking done by the back-end processing systems.

@_date: 2014-10-31 23:47:59
@_author: Peter Gutmann 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
Most, if not all, publications on the topic of fault attacks on RSA and DLP-
based algorithms (DSA, ECDSA) use a very abstract model of the fault, assuming
merely "a fault" or, for example, that an attacker can:
  modify any intermediate value by setting it to either a random value
  (randomizing fault) or zero (zeroing fault), such a fault can be either
  permanent or transient
  skip any number of consecutive instructions (skipping fault)
or at the individual-bit level:
  If an adversary has full control over the injected fault, it is possible to
  manipulate bits at will
with the optional ability to inject a fault with accurate timing control,
typically in the middle of a signature computation.  While I haven't been able
to track down every publication on the topic, there doesn't seem to be much
that specifically addresses the case of random single-bit faults, e.g. due to
alpha particles, and of a non-malicious nature, so your in-memory private-key
component x becomes x' at some point with the difference being a single bit.
Has any work been done on this?  Is RSA more robust against random single-bit
faults than the DLP-based algorithms?

@_date: 2014-09-05 18:32:07
@_author: Peter Gutmann 
@_subject: [Cryptography] What is the difference between a code and a 
At least it doesn't have a code for "Sentence of a Court Martial to be put
into execution" ("oh, misread that, it was actually 'Sail to the Northward the
first fair Wind'.  Terribly sorry old chap, we'll send flowers to your
Those Q-codes can (or at least were) at one time quite useful, not far from
here there's an informal air museum containing a handwritten note dropped by
Guy Menzies, the pilot of the first trans-Tasman Ocean flight that asks "Where
can I land?  Point to nearest town".  He ended up mistaking a swamp for flat
land and crashed, flipping his aircraft.  A Q-code or two would have helped
quite a bit there.

@_date: 2014-09-09 16:49:20
@_author: Peter Gutmann 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
Same here, but the reasoning is much simpler.  Let's say aliens land tomorrow
and announce "There was another gunman on the grassy knoll, we were
responsible for the Mary Celeste (sorry about that), oh, and also P = NP.  See
you in another billion years".
How does this affect practical usage of PKC in any conceivable way?  Every textbook on crypto *theory* will need to be updated (not to mention a bunch of other books), but what actual, *real-world* effect does it have on the security of RSA, DH, Elgamal, and others?

@_date: 2014-09-10 17:34:34
@_author: Peter Gutmann 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
OK, I'll change the scenario slightly:
  The aliens still land tomorrow and announce "There was another gunman on the
  grassy knoll, we were responsible for the Mary Celeste (sorry about that)",
  but this time they finish with "here's a way of efficiently solving SAT.
  See you in another billion years or whenever they stop re-running Cheers,
  whichever comes sooner".
Same question as last time.

@_date: 2014-09-11 01:13:33
@_author: Peter Gutmann 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
Yeah, and now you're back to the how-to-travel-to-Mars problem:
1. Build a big rocket.
2. Round up some astronauts.
3. Fly to Mars.  QED.
The fact that you can state a solution doesn't mean that you can actually do
it.  I specifically chose SAT because of its everything-maps-to-SAT status,
but was trying to point out that having a SAT solver doesn't automatically
break RSA or DH or whatever.

@_date: 2014-09-11 16:04:28
@_author: Peter Gutmann 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
Ooh, good to know, thanks!  The "running code" bit is the important point, "runs on a slide projector" is intriguing but not as practically useful. ToughSAT,  in particular looks interesting.

@_date: 2014-09-15 16:31:33
@_author: Peter Gutmann 
@_subject: [Cryptography] RFC possible changes for Linux random device 
+1.  The SoC vendors like to advertise all the cool hardware features they
have, but (at least for the ones that would be useful in a crypto context) you
can't use them for exactly the reason you describe.  So you end up in a
perverse situation where if you've got some deeply embedded system (ICS, bread
makers, washing machines) where the hardware never changes over a 10-15 year
period you can safely assume the presence of crypto-related features that
you'll never need or use, but if you're building something more flexible that
does need the crypto features then you can't use any of them because you don't
know whether they'll crash the system or not.  As a result, you have to treat
everything like a basic ARMv3 from the 1990s.
Is it a BOM issue thought?  A feature-test mechanism like CPUID essentially
comes free with the CPU, and in fact adding IP that can't be reliably used
would seem to come with a negative cost.  I've always assumed (for lack of a
more sensible reason) that it was a case of "only our SoC exists and no-one
else's" (with a side-order of "lalala I'm not listening"), "so we'll force the
creation of binaries that don't work on anything else".

@_date: 2014-09-19 16:47:55
@_author: Peter Gutmann 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Windows has has it since Win7, as part of EFS.  Details are a bit (OK, very)
sparse, but I get the feeling it's meant more for personal-PC users than
servers where the data-at-rest never actually rests.

@_date: 2014-09-19 17:09:00
@_author: Peter Gutmann 
@_subject: [Cryptography] CloudFlare reinvents crypto offload 
Someone recently pointed me to CloudFlare's Keyless SSL:
I can't see what the innovation is here.  They say that instead of doing the
SSL premaster secret processing directly on the web server, the magic is to do
it on a secure external system/device.  I guess they could call this external
device a Helper for SSL Mechanisms or "HSM".  I wonder why no-one's ever
thought of this before.
Oh, wait...
Where's the magic?

@_date: 2014-09-20 11:28:02
@_author: Peter Gutmann 
@_subject: [Cryptography] CloudFlare reinvents crypto offload 
They've posted an update:
which looks like it's taken straight from Rescorla's SSL and TLS from fifteen
years ago (or perhaps an nCipher brochure from nearly twenty years ago :-).
I'm sure they think they've invented something wonderful, but from everything
they've published there's nothing new there.  The odd thing is that they go
into all sorts of details about session tickets and other TLS technicalities,
and yet at the same time they're describing a mechanism that's not novel at
(Maybe it's just a clever marketing campaign, "CloudFlare invents new security
mechanism" sounds a lot better than "CloudFlare buys someone's SSL-offload

@_date: 2014-09-21 15:10:04
@_author: Peter Gutmann 
@_subject: [Cryptography] Email encryption for the wider public 
In response to that I was expecting a Sinatra impression, "I wanna wake up /
In that city that doesn't sleep / ...".

@_date: 2014-09-21 15:18:51
@_author: Peter Gutmann 
@_subject: [Cryptography] CloudFlare reinvents crypto offload 
they seem to be creating a logical HSM by removing physical control over the
crypto from the ISP.  I've seen that before in the past, don't bother with
HSMs but just locate a PC that does the job in a physically secure location
(e.g. inside a strongroom in an old fortress where it took two days to drill
through the walls to lay the cables).
Of course the keys that control access to the remote HSM are still held by the
ISP, and the premaster secret that protects the communications is also fed
back to the ISP, so you have to define your threat model pretty carefully to
see what they're defending against here.

@_date: 2014-09-22 15:41:54
@_author: Peter Gutmann 
@_subject: [Cryptography] Of writing down passwords 
Finally a large organisation providing sensible password advice.
(Since you seem to disagree with it, what threat exactly is introduced by the
above?  Walk us through a scenario...).

@_date: 2014-09-25 03:42:17
@_author: Peter Gutmann 
@_subject: [Cryptography] NSA versus DES etc.... was: iOS 8 
I don't know about a single algorithm but one attempt was to provide black
boxes under the CCEP that accepted a key and plaintext/ciphertext and spat out
ciphertext/plaintext.  The CCEP gear would be non-exportable, thus solving the
DES problem where either US banks/corporates could export it or you could buy
foreign gear built to the publicly-available design.  Since it never survived
into production several aspects of it were rather unclear, but it looked like
the NSA would be a middleperson in the key management process.

@_date: 2014-09-26 13:38:10
@_author: Peter Gutmann 
@_subject: [Cryptography] NSA versus DES etc.... was: iOS 8 
I'd always assumed that was an oversimplification/dumbing down of what we know
about Type 1 algorithms like BATON and JUNIPER, that most of the key is
actually "checksum" bits (given that the checksum is 160 bits long I'd guess
it's a SHA-1 MAC over the key data) and that an attempt to load a non-NSA-
generated key will fail the MAC check and load some form of fixed key (e.g.
one where only the low 16 bits are chosen randomly, making them easily brute-
forceable by someone who knows the remaining bits) instead.
The main reason was the banks' massive investment in DES and refusal to change
horses mid-stream.  The ABA's pressure to get the NSA to recertify DES was
much, much stronger than the NSA's pressure to get the ABA to switch to CCEP.
See above, you don't need any fancy special-properties cryptosystem, if you've
got tamper-resistant hardware you can bake in whatever behaviour you want,
giving an algorithm properties that go beyond the original design.

@_date: 2014-09-28 18:47:27
@_author: Peter Gutmann 
@_subject: [Cryptography] [messaging] Gossip doesn't save Certificate 
So in other words it'll help the organisations who are already more or less
covered by certificate pinning (except that CT does it in a really roundabout,
complex manner rather than directly at the source as pinning does).
Looking at what CT gives you, there seem to be three scenarios to cover:
1. Cert issued for Google or Paypal.
2. Cert issued for First Bank of Podunk.
3. Cert issued for Case  is already handled by pinning, and cases  and  won't be helped
through CT.  So CT will end up solving the browser PKI problem in the same way
that SPF solved the spam problem.
It is a lot of fun to theorise about and debate, as the ongoing discussions
have more than proven, but it's not going to be a lot of use if the attackers
don't even notice it's there.
I think trying to determine whether a purported crypto solution to a problem
will actually solve it is definitely on-topic, but just in case I've cross-
posted to the cryptography list, and people can edit followups as required.

@_date: 2014-09-29 16:27:04
@_author: Peter Gutmann 
@_subject: [Cryptography] The world's most secure TRNG 
Since 99.9% of consumers of crypto random numbers will max out at about 128
bits a minute (generating an AES session key every now and then), I'd go for
cost as the main driver.
(Some time ago a large, multibillion-dollar telco asked its general-public
(not commercial) users to rate in order of importance the qualities they
wanted in the service that they provided.  The results were:
1. Cost.
2. Cost.
3. Cost.
They didn't care about reliability [0], availability [0], value-added services, the quality of the helpdesk support, whether the ISP tried to filter out malware, or anything else, if they saw a competitor that was $5/month cheaper then they wanted to know why, and when their bill was going to be reduced.  This proved quite distressing to The Mgt., who had put a lot of effort into telling potential customers what a fine telco they were, even if they weren't necessarily the cheapest).
Having said that, it depends what your market is. These guys:
sell quantum RNGs (true quantum RNGs, they document the physics package in a
paper somewhere).  Their target market is things like casinos, so it doesn't
matter if they cost $1K each.
What's your target market?
[0] That is, they'd complain if there were issues, but they didn't see it as a
    factor in deciding on which ISP to go with.
    (This footnote has two references to it so please read it twice to make
    sure the garbage-collector can mark it as unused later).

@_date: 2015-04-06 19:51:22
@_author: Peter Gutmann 
@_subject: [Cryptography] Cipher death notes 
DNSSEC is interesting in that the further you get from the DNS core (the root
servers), the more it goes from being a highly-esteemed security measure to an
annoying PITA.  In other words the closer it gets to contact with IT reality,
the less well it functions.  Take your typical end user, who wants a buy one
of whatever it is you buy your kids this year, tries to connect to
 and gets told "The primary main processing cores
cross linked with a redundant melacortz ramistat and fourteen kiloquad
interface modules report that the core elements based on FTL nanoprocessor
units arranged into twenty-five bilateral kelilactirals with twenty of those
units being slaved to the central heisenfram terminal have said that you can't
visit this site, continue anyway Y/N?".  About, oh, ten out of ten users will
continue anyway.  Heck, I'm a security geek and I would continue past the
error, but for entirely different reasons, I know that about 99.9% of DNSSEC
failures are due to misconfigurations, software incompatibility, and other
FPs, so looking at the odds I know it's almost certainly just another FP and I
can ignore it.
Here's another one... this is a list full of security geeks, so lets do a
quick (virtual) show of hands:
- How many people have at some point received signed email (S/MIME, PGP,
  whatever)?
- Of the above, how many people have been warned about some sort of validation
  failure in said signed email (expired cert, couldn't find the key, signature
  didn't validate, couldn't find gpg for the validation, etc)?
- Of the above again, how many people immediately deleted the email without
  looking at it (it could be a drive-by download/infection)?
I would guess that by the time you've got to the third question, you'd be down
to zero people (I've been waiting for an excuse to do this poll in a roomful
of people at a security conference, just need to get the right talk to ask it
Designing a security mechanism that doubles as a self-inflicted DoS pretty
much guarantees it's going to be disabled by anyone who has a choice.

@_date: 2015-04-07 16:28:02
@_author: Peter Gutmann 
@_subject: [Cryptography] Fwd: OPENSSL FREAK 
More important is to remember the fact that IoT stands for "Internet of
Targets", not "Internet of Things".
That would make an interesting value-add, either build an Internet of Targets
DMZ into existing home routers, or sell an add-on (maybe an Alix PC running
pfSense) that does this for you.
Of course then you need to punch in holes to let your fridge tell your
cellphone that it's out of celery, and it's game over.

@_date: 2015-04-13 21:51:37
@_author: Peter Gutmann 
@_subject: [Cryptography] Go home PKI, you're drunk 
It was recently pointed out on the Mozilla security list [0] that a particular
large corporation's web site was failing cert validation in Firefox because
there were spaces embedded in the FQDNs in the cert (alongside other problems
with cert-holder identification).  So I grabbed a copy of the cert chain from
one of the sites, postofficeshop.de, and found, among other things...
1018   45: SEQUENCE {
1020   37:   OBJECT IDENTIFIER '1 3 6 1 4 1 311 21 8 3675690 6234259 10436751 12227305 62135 141 959321 10252252'
         :     Error: OID contains random garbage.
1059    1:   INTEGER 100
1062    1:   INTEGER 6
         :   }
(that's one of Microsoft's "encode random noise and call it an OID), and then:
1209   68: SEQUENCE {
1211    9:   OBJECT IDENTIFIER
         :     sMIMECapabilities (1 2 840 113549 1 9 15)
for what is explicitly a TLS server cert:
1074   20: SEQUENCE {
1076    8:   OBJECT IDENTIFIER
         :     clientAuth (1 3 6 1 5 5 7 3 2)
1086    8:   OBJECT IDENTIFIER
         :     serverAuth (1 3 6 1 5 5 7 3 1)
         :   }
         : }
Oh yeah, and the S/MIME implementation that their TLS server is supposed to run advertises:
1226   14: SEQUENCE {
1228    8:   OBJECT IDENTIFIER rc2CBC (1 2 840 113549 3 2)
1238    2:   INTEGER 128
         :   }
1242   14: SEQUENCE {
1244    8:   OBJECT IDENTIFIER rc4 (1 2 840 113549 3 4)
1254    2:   INTEGER 128
         :   }
1258    7: SEQUENCE {
1260    5:   OBJECT IDENTIFIER desCBC (1 3 14 3 2 7)
         :   }
because someone has to keep all those 1970s and 1980s ciphers alive somewhere.
Then the next cert up the chain (an intermediate CA) has:
 710 2683: SEQUENCE {
 714    3:   OBJECT IDENTIFIER nameConstraints (2 5 29 30)
 719 2674:   OCTET STRING, encapsulates {
 723 2670:     SEQUENCE {
 727 2616:       [0] {
 731   17:         SEQUENCE {
 733   15:           [2] 'adressdialog.de'
         :           }
 750   20:         SEQUENCE {
 752   18:           [2] 'adress-research.de'
         :           }
[on and on for hundreds of lines]
3347   48: [1] {
3349   10:   SEQUENCE {
3351    8:     [7] 00 00 00 00 00 00 00 00
         :     }
3361   34:   SEQUENCE {
3363   32:     [7]
         :       00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
         :       00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
         :     }
         :   }
         : }
The recent CNNIC discussion mentioned the fact that trusted CAs shouldn't be
allowed to issue unconstrained certs for intermediate CAs.  Perhaps we need to
introduce requirements for drug-testing intermediates as well.
(I should note here that there's nothing in the PKI specs that prohibits any of the above, so that putting an MPEG of a cat into a certificate is perfectly standards-compliant [1].  There's also no law specifically saying that you're not allowed to stagger around in public complaining that the sun is too loud and warning people about the ice weasels [2], but that doesn't mean that it's not a sign that something's gone seriously wrong somewhere).
[0] [1] [2]

@_date: 2015-08-17 15:09:25
@_author: Peter Gutmann 
@_subject: [Cryptography] Speculation about Baton Block Cipher 
That's not a tweak, it's just a way of making the crypto capture-proof, you
can only key it using an NSA-supplied fill device.  The Clipper/Capstone chip
did the same thing (although not very well, as Matt Blaze demonstrated).
So what you've got is... a block cipher.  Nothing magic about it.

@_date: 2015-08-19 07:44:57
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: SHA-3 FIPS-202: no SHAKE512 but 
It's what's more usually called crypto numerology, I'm not sure where "binary
RSA myopia" came from (it's an apt enough name, but doesn't cover other uses,
e.g. with DH or DSA).

@_date: 2015-08-19 07:49:59
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: Speculation about Baton Block Cipher 
The IV is a bit odd, it hints at a LEAF-like capability a la Clipper/Capstone.
However, it could be a completely ordinary composite nonce value as outlined
in e.g. RFC 5116, "An Interface and Algorithms for Authenticated Encryption".

@_date: 2015-12-02 03:14:59
@_author: Peter Gutmann 
@_subject: [Cryptography] "The Moral Character of Cryptographic Work" 
For those who missed his talk on the topic this morning, he's also doing a
public lecture next Wednesday:
Feel free to drop by, and then follow it up with KiwiCon the next day:

@_date: 2015-12-08 10:13:54
@_author: Peter Gutmann 
@_subject: [Cryptography] Elliptical Curveball 
Everything in cryptography depends upon high quality random numbers, and
  lots of them. People get into semi-informed flamewars about what entropy
  means, government agencies sneak backdoors into algorithms, performance
  matters, secrecy matters, and unpredictability matters. The standard which
  defines four randomness generators is NIST Special Publication 80090. One
  of the four raised suspicions because it (Dual_EC_DRBG) was three times
  slower than any of the others.
  Joe well, Joe sends us his own code, which fixed this. He made one of the
  others slower. Using MySQL stored procedures. Just bear in mind, with the
  below, that its still cleaner, more comprehensible, and generally saner
  than OpenSSL.
  -- prng_nistctr_sp
  -- Implements the NIST SP 800-90 CTR_DRBG cryptographic random number generator standard
  -- with MySQL AES_ENCRYPT() as the block cipher (256 bit seed, 128 bit output)
  [...]
A PRNG implemented as MySQL stored procedures.  Wow.

@_date: 2015-12-14 02:52:19
@_author: Peter Gutmann 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
Their intended market is gaming machines, about the only situation where the
cost is justified, so there's little point to backdooring them.  They're
reasonably nice devices, and easy to interface to, I've had support for them
in my code for awhile.

@_date: 2015-12-24 20:16:24
@_author: Peter Gutmann 
@_subject: [Cryptography] Photon beam splitters for "true" random 
A prestidigitator can make a coin come up any way they want (you want 100
heads in a row, you got it), so you asked the wrong question, you need to ask
whether whoever is flipping the coin is unbiased.
(Or you could take a photo of the crowd using an unbiased cellphone and run it
through SHA-2 using an unbiased software implementation and display the result
on an unbiased display.  The coin flip will be easier to get accepted though).

@_date: 2015-12-30 04:52:40
@_author: Peter Gutmann 
@_subject: [Cryptography] Understanding state can be important. 
There is still one vendor that makes USB devices with write-protect switches,
Kanguru.  OK, so you can still in theory get an infected host with USB-level
0day that 0wns the Kanguru USB firmware, but against anything less
sophisticated than that you're OK.  I keep several of them for taking slides
to conferences and the like, since I have no idea of the state of the machine
they'll be plugged into.
They're also useful for dealing with OS X devices, which insist on scribbling
all over any media that's plugged in as soon as you insert it.  At least it's
slightly less bad then it used to be, the system would refuse to mount a
device if it couldn't scribble to it.

@_date: 2015-12-30 05:00:05
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: Photon beam splitters for "true" 
The downside is that you then have to work with an MSP 430, one of the world's
truly awful CPUs.  You're better off tacking a FRAM on the side of something
Why would you want FRAM though?  You still need to erase it to prevent
capture, and its remanence properties are largely unknown.

@_date: 2015-02-02 03:44:42
@_author: Peter Gutmann 
@_subject: [Cryptography] best practices considered bad term 
This is particularly the case for the "cryptography" subset of "security", for
which "best practice" seems to be synonymous with, as Linus put it, "people
wanking around with their opinions".  In something like medicine we have
evidence-based best practice, "don't discontinue your antibiotics until you've
gone through the full course".  In agriculture we have "don't overuse one type
of fungicide or you'll end up with resistant strains".
In contrast in crypto it's "Use ECC!" / "No, use RSA with an 8K key!" / "No,
use AES-GCM!" / "No, use Poly1305-AES" / "No, use ECC but only with My Pet
Curve!" / "No, use Ed25519" / "Camellia! Gost! Twofish! SEED!  LIONs and
Tigers and BEARs, oh my!", ignoring the fact that an attacker won't care what
you do since they're exploiting a buffer overflow in some ancillary support
library that you don't even know exists.
In medicine and agriculture we know from real-world experience that if you
don't follow best practice (in the use of antibiotics, fungicides, whatever),
bad things will happen.  In the crypto world if you don't follow best practice
(pick someone's at random, it doesn't make much difference) chances are
nothing will happen, and even if you do follow best practice, you'll probably
get owned anyway because crypto won't stop anyone who wants to get in (see
Shamir's Law, what I mean here is that if there's a way in then it won't
involve breaking the crypto, an extended form of which is in this slightly
NSFW poster: So it's certainly a rain dance, but I wouldn't say it's for avoiding security,
it's for avoiding liability, a la "no-one ever got fired for buying IBM".

@_date: 2015-02-02 03:54:15
@_author: Peter Gutmann 
@_subject: [Cryptography] best practices considered bad term 
You don't just need a description, you need a rationale.  Taking the example
in my previous message of agricultural use of fungicide, "Don't use
myclobutanil more than three times in one growing season or you'll produce
resistant strains".  That's advice, rationale, and consequences of not
following the advice in a single statement.
Now contrast this with security advice like "Passwords are like underwear, you
should change them often".  Yeah, cute analogy, but so what?  I could invent a
new saying, "Passwords are like a spouse, pick a good one and stick with it",
and now you've got exactly the opposite advice.  I think the reason why so
much security "best practice" comes without any explanation or rationale is
because there isn't any available, it's just "we've always done it that way"
or "this is my pet idea, everyone else should believe in it too".

@_date: 2015-02-03 22:34:18
@_author: Peter Gutmann 
@_subject: [Cryptography] best practices considered bad term 
There's a large difference between *exploitable* holes and *exploited* holes.
Attackers aren't in this to count coup (well, most of them aren't), they're
there to maximise ROI.  How many credit cards or bank accounts were
compromised due to the attacks you mention?  No-one actually knows, but it's
likely to be zero, or close to it, because only an idiot attacker would bother
with CRIME or BEAST or POODLE when you can simply lift a vendor's credit card
database wholesale (imagine explaining to your boss in the Russian mafia "I've
just spent three days implementing BEAST, and then tomorrow I'm gonna steal me
some cookies, that'll show 'em").
In fact lets see what happens when you use really bad crypto.  I'll fire up my
Windows 95 PC and load up Netscape Navigator 3.0 (the first to do SSLv3) and
buy something from Amazon (assuming it still allowed SSLv3 connections) all
secured with the power of 56-bit single DES and MD5.  Absolutely nothing (bad)
will happen.  Well, my Windows 95 PC will get owned within about five minutes
of going online, but the crypto won't cause any problems.
Let's get even sillier and use the closest we can get to worst-practice
crypto, RC4-40 with MD5.  Watch the complete lack of anything bad that happens
(apart from my PC now being a member of twenty different botnets).
Apart from having my PC owned, if anything does go wrong it won't be due to me
using 56-bit single DES or RC4-40 or MD5 or even no encryption at all, it'll
be from someone using SQL injection to steal 20 million credit cards at once
from some online vendor, not a single credit card using BEAST.
(I specifically chose SQL injection there because it remains  in the OWASP
top ten.  Bad cryptography doesn't even feature in the top ten).
This isn't Formula 1 racing where you're in a competition to be the fastest,
it's a binary, or at least shades-of-grey, distinction: "works for me",
"doesn't work for me", and possibly "isn't so good but I can deal with it".
If you're on an embedded system then you may well go with RSA (not ECC)
because client-side it's much faster than ECC.  OTOH if you've got CPU
power to spare (which a great many devices have) it doesn't matter what you
use because, well, you've got CPU power to spare.

@_date: 2015-02-03 22:52:30
@_author: Peter Gutmann 
@_subject: [Cryptography] best practices considered bad term 
The value of an electrical code (and in fact any code) depends greatly on how
it's written.  For example the one I'm familiar with:
says (among other things) for earth electrodes:
  The type and embedded depth of the earth electrodes shall be such that soil
  drying and freezing will not increase the earth resistance of the earth
  electrodes above the required value. Where practicable, the earth electrodes
  shall be embedded below permanent moisture level, except for electrodes
  which are used for gradient control.
  The design of the electrode shall take into consideration the type,
  emperature and moisture content of the soil as well as the magnitude and
  duration of expected current flow so as to minimise soil dryness in the
  vicinity of the electrodes.
  The material and total cross -sectional area of the earth electrodes shall
  be such as to provide a conductance of not less than that of the earthing
  conductor.
  The design, selection of materials, and construction of the earth electrodes
  shall take into consideration the possible deterioration and increase of
  resistance due to corrosion over the expected period of use of the
  installation.
  In areas where corrosion is likely to be severe, the electrodes shall be of
  hard drawn copper, copper clad or stainless steel, or other metal of such a
  nature or so treated as to be not less resistant to corrosion than hard
  drawn copper, or copper clad or stainless steel.
  In areas where corrosion is not severe, galvanised or plain steel electrodes
  may be used.
Note the emphasis on function rather than form, it outlines a set of
parameters and says you need to operate within them rather than saying "you
need to bury a nickel-plated 20mm copper rod 2m into the soil".  There's no
explicit rationale, but it's made pretty clear why each requirement is the way
it is.
Or you end up perpetuating a bad decision made decades earlier, and having to
kludge around it for the rest of eternity.  I'm thinking of ring mains in the
UK (created to save copper after WWII, and a PITA ever since), and the analogy
to X.509 (created to secure OSI directories, ditto).

@_date: 2015-02-03 23:05:17
@_author: Peter Gutmann 
@_subject: [Cryptography] best practices considered bad term 
I was curious to see how it compared to our code, and went out to grab a copy.
Dear God, it's a 128MB document!  Supposedly (since I didn't download it) 924
pages long.
And it allows wire nuts for connecting electrical cables.

@_date: 2015-02-03 23:51:26
@_author: Peter Gutmann 
@_subject: [Cryptography] best practices considered bad term 
Frequently it's not even "the project" but "the person who runs it", and that
transcends any open vs. closed-source issues.  I know of plenty of closed-
source projects for which the authors/maintainers take great pride in their
work, and it doesn't matter that it's closed-source because you know you're
getting a good product.  So a good maintainer is a lot more important than
whether it's closed or open source, or indeed most other religious issues
surrounding software development.  That's why I said many years ago that "The
Venema Development Method trumps the Vienna Development Method", i.e. what
matters is who develops something, not how it's developed.
(This then runs into the unfortunate situation that talent doesn't scale well,
which is an argument for not over-complicating security systems).

@_date: 2015-02-06 16:09:43
@_author: Peter Gutmann 
@_subject: [Cryptography] best practices considered bad term 
It may be accurate in terms of comparing two numbers, but it doesn't say too
much about actual vulns present because it ignores externalities.  If no-one
cares about your product (even if it's riddled with vulns), you'll look good
on a compare-the-numbers scale because you'll have no (apparent) vulns while
your competition will have many.  The AV industry has a rule of thumb that a
product is uninteresting to attackers until it reaches about 5-10% market
share (there's no point in expending any effort to own a product with 2%
market share when you can go for Windows with 95% or whatever market share).
Once you cross this threshold you start drawing the attention of attackers,
and your apparent vuln. count explodes.  This is what happened to Apple around
about 2011.
That's because Apple's now a target (although still not nearly as big a target
as Microsoft).

@_date: 2015-02-08 14:30:10
@_author: Peter Gutmann 
@_subject: [Cryptography] What do we mean by Secure? 
Here's the military version of secure, as in "Secure the building":
* The Navy would turn out the lights and lock the doors.
* The Army would surround the building with defensive fortifications, tanks
  and concertina wire.
* The Marine Corps would assault the building, using overlapping fields of
  fire from all appropriate points on the perimeter.
* The Air Force would take out a three-year lease with an option to buy the
  building.
(An old joke, author unknown).
That sounds like another form of the Break-Glass Security Policy (lots of
references, Google for whatever you feel like reading).

@_date: 2015-02-11 19:15:38
@_author: Peter Gutmann 
@_subject: [Cryptography] Do capabilities work?  Do ACLs work? 
I don't think users demand ACLs (or capabilities), they demand some means of
doing things like "make sure the competition doesn't get hold of our business
plans" or "make sure no-one outside payroll and the employee concerned can see
pay details".  Whether you use ACLs, capabilities, or nasally-housed demons
doesn't really matter.
Having said that, ACLs are better-suited to expressing most of what users want
then capabilities.  The reason why both Unix and Windows use groups and
permissions the way they do isn't because of a grand anti-capability
conspiracy, it's because that's the most practical/real-world-applicable way
to do it.

@_date: 2015-02-13 15:27:05
@_author: Peter Gutmann 
@_subject: [Cryptography] Crypto Trick Makes Software Nearly Impossible to 
This looks like a modern update of an old trick from the late 1908s/early
1990s, you set the trap bit on the CPU and decrypt each instruction before
it's executed, then re-encrypt it afterwards.  Unfortunately this only worked
under DOS, as soon as protected-mode anything came along (e.g. a DOS box under
Windows 3.1), it didn't work any more (or at least I couldn't be bothered
trying to figure out what was required).  The downside was that it made
execution really, really slow, but if you ran just a few core bits of code
that way then it was manageable.

@_date: 2015-02-16 19:22:43
@_author: Peter Gutmann 
@_subject: [Cryptography] Capability Myths Demolished was: Do capabilities 
As a general reply to this (I was trying to avoid turning it into a long
thread since it's probably not worth going into lots of detail on), I read the
"Capability Myths Demolished" paper some years ago and compared it to a
(hypothetical) "X.509 Myths Demolished" [0], or from the non-IT world,
"Monorail Myths Demolised", "Communism Myths Demolished", "CDO Myths
Demolished" [1], and so on: if you pick your examples very carefully and
propose theoretical solutions that don't necessarily have to work in practice
(or, even worse, that have been shown not to work when deployed in the real
world) then you can "demolish" all sorts of "myths".
[0] I actually read a paper on this theme a few years ago, its premise was
    that if we all used smart cards for everything then X.509 could be made to
    work.  It was actually valid, if we could deploy smart cards everywhere     and get people to use them correctly at all times then we could make X.509
    work.  The only thing missing was the squadrons of flying pigs to     distribute the cards to users.
[1] Although that particular one hadn't happened at the time.

@_date: 2015-02-23 12:45:02
@_author: Peter Gutmann 
@_subject: [Cryptography] trojans in the firmware 
Almost.  Its sole use is for very fast "drive erasure", i.e. you change the
key and the data on it becomes inaccessible.  Have a look at this
which describes what Samsung (and others) are doing, in particular slide 18.
The decryption key (DEK) is stored in the drive, and is unlocked using a
password (and "authentication key", AK).  So to decrypt the drive you extract
the encrypted DEK, brute-force the password (AK), and you're in.
In any case though it doesn't protect against an attack that occurs when the
drive is mounted since it looks like an unencrypted drive at that point (and
presumably the AK is hardcoded into a startup script or something similar in
order to survive power outages, so you can grab that if you really need it).
It's actually hard to see what purpose this "encryption" is serving (the
vendors studiously avoid providing a threat model), it doesn't protect live
data, it barely protects data at rest (say if you decide to Fedex the contents
of your data centre across town), the only thing it really does is allow for
fast erasure of contents, and protect against casual snooping of the "buy a
batch of drives on ebay and see what's on them" kind.
So I guess if ebay is your threat, it's good enough.  OTOH a BIOS password set
for the drive will do the same thing.
As a more general response to "what's the point", regulatory compliance ("our
drives were encrypted so we don't have to disclose the 40M credit card breach
from last week"), buzzword-compliance, CYA, it's not a bad idea from a
marketing point of view.

@_date: 2015-02-24 18:30:26
@_author: Peter Gutmann 
@_subject: [Cryptography] trojans in the firmware 
There's another summary of the OLPC's use of flash at:
Warning: You probably don't want to read that if you're an OLPC fan...

@_date: 2015-02-25 21:25:50
@_author: Peter Gutmann 
@_subject: [Cryptography] My ignorance and drive firmware hacking 
It doesn't necessarily need to be rewritable to deal with a firmware
corruption issue, it could just be a case of swapping out the controller board
with one from another drive of the same model, which used to be a standard
means of dealing with failed controller issues.
(Glossing over a lot of details here, e.g. whether the controller loads its
firmware from the host protected area and lots of other technicalities).

@_date: 2015-02-28 21:08:50
@_author: Peter Gutmann 
@_subject: [Cryptography] My ignorance and drive firmware hacking 
Depends on the drive type, for example some IBM drives (which shows the
vintage) stored servo information in the controller, but that was only a
passing thing.  Most drives (caveat: last time I looked) use self-servo track
writing where the drive controller handles everything internally and writes
the metadata (AGC preamble, servo info, positioning info, etc) onto the drive
at manufacture time.  In addition to the servo data though there may be extra
information stored in flash on the controller, so if a controller swap doesn't
work then you may need to swap the flash as well.  Finally, that may also not
work, so any one of the above three options is a possibility (thus my
"glossing over the details" comment in the original post, it's a YMMV

@_date: 2015-01-04 18:54:57
@_author: Peter Gutmann 
@_subject: [Cryptography] 
TLS finally fixed this after a year-long battle to get the change accepted.  I
also suggested it to the SSH folks but they weren't interested, and after the
fight it took to get it into TLS I just didn't have the energy to go through
the same thing for SSH.

@_date: 2015-01-04 19:44:15
@_author: Peter Gutmann 
@_subject: [Cryptography] 
Unfortunately in terms of the protocol itself, IPsec is what you would design
if you had a palletload of paper, a standing committee of representatives from
multiple competing vendors, and about a decade of time you didn't want back.
More seriously, totally transparent security is a liability, not a feature,
since it's indistinguishable from security that's not present.  Arguably, the
mass failures of SSL/TLS in Android and iOS that researchers have reported on
in the last year or so have been due to attempts to make SSL/TLS fully
transparent: "Just gimme a secure link and don't bother me with details" (c.f.
"Shut up and take my money").
It's actually very, very difficult to turn SSH into a library.  Take for
example SFTP, which pretends to be a protocol but is actually an RPC mechanism
for the Posix filesystem API.  Do you know just how *hard* it is to turn that
into a library?  If you're the client and all you want to do is "put file" and
"get file" it's easy enough (for some values of "easy", consider what happens
when one side talks NTFS and the other ext3), but if you're the server you
have to be able to process arbitrarily complex operations, and sequences of
operations, from the client.  The SFTP spec is in fact so complex that most
implementations are still several versions behind the most recent unfinished-
draft version, and there's no sign that they'll ever be updated.
Having implemented quite a number of security protocols over the years (except
for IPsec, I'm not that crazy), SSH is by far the most complex and awkward to
do.  Even the baroque monstrosities dreamed up by PKIX aren't as complex as
SSH is.
The reason why SSH (as OpenSSH) is primarily a standalone app with a million
options and extensive fallback to user intervention is because the protocol is
so complex that you can't really do it any other way.  Many years ago I wrote
up a proposal called SimpleSSH that profiles SSH to do about 99% of what about
99% of users want (secure telnet, get file, put file), which removed... not
sure, maybe 75% of all of the complexity with no loss in functionality for
most users (and resulted in a considerable increase in security, it's still
way too easy to crash SSH servers by sending completely valid but unexpected
options, my code has all sorts of checks for various vendors' gear to make
sure it doesn't send something that'll hang or reboot it).  I never published
the SimpleSSH proposal because I couldn't see it getting much support...

@_date: 2015-01-05 13:37:28
@_author: Peter Gutmann 
@_subject: [Cryptography] 
Yup, IPsec is a whole minefield of problems.  For example:
  Getting into IPsec-specific peculiarities, the way that traffic is protected
  is that packets are pattern-matched to successive processing rules, with the
  type of processing that gets applied being determined by the order of the
  rules in the IPsec security policy database (SPD).  So if a packet matches
  two rules, one saying that it should be sent out encrypted (PROTECT in IPsec
  terminology, although strictly speaking PROTECT is an entire family of
  actions because nothing is ever simple in IPsec) and one saying that it
  should be sent out unencrypted (BYPASS in IPsec terminology) then the action
  that gets taken is based on whatever happens to appear first in the SPD.
  This, combined with the issues already mentioned above, makes creating and
  working with IPsec policies a very complex and error-prone task [348].
  A real-world example of how sensitive traffic can end up unprotected as a
  matter of course occurs when setting up a Generic Routing Encapsulation
  (GRE) tunnel from one router to another.  Since it???s sensitive data you
  define a security policy requiring that all data over the GRE tunnel must be
  encrypted.  Some time later the router discovers a lower-cost route that
  doesn???t involve the GRE tunnel and switches all the traffic across to that.
  The GRE tunnel still has encryption applied as required but all of the
  traffic is going via the lower-cost unencrypted link, with the encrypted GRE
  tunnel standing idle.  All of the routing and security mechanisms are
  working exactly as they should be, but the traffic isn???t being protected.
  This problem is particularly bad in the presence of VPN split tunnels in
  which traffic for the corporate LAN is sent over the VPN and traffic for
  everywhere else (for example general web browsing) is sent directly over the
  Internet in order to avoid the overhead and cost of tunnelling it back onto
  the corporate LAN and then straight back out again.  As you can probably
  imagine, split tunnels are an accident waiting to happen.
  It???s not just fancy features like split tunnels that cause issues.  Another
  problem case crops up with IPv4 and IPv6 dual-stacked systems.  If the VPN
  tunnel only supports IPv4 (as many do) then any DNS lookup that returns an
  IPv6 address will result in traffic being sent over the unencrypted IPv6
  link instead of the encrypted IPv4 one [349].
This illustrates what I mentioned in my previous message, that IPsec's
transparency means that "it's indistinguishable from security that's not
If you're willing to add at least a small number of new functions to update a
thirty-year-old API you can fix a fair few problems at the low-hanging fruit
level.  Look at, for example, Winsock's AcceptEx() (accept an incoming
connection, report peer identity information, and return the first data item
sent by the peer, all in a single function call), or WSAConnectByName() (whose
name pretty much explains what it does).

@_date: 2015-01-05 15:29:27
@_author: Peter Gutmann 
@_subject: [Cryptography] 
I've done the same thing, but the problem is that a bunch of (probably)
incompatible vendor-specific extensions doesn't profit the community as a
whole.  If anyone from OpenSSH would like to get in touch, we can (a) see if
what we're doing is interoperable and (b) document it in an RFC for general

@_date: 2015-01-06 02:29:08
@_author: Peter Gutmann 
@_subject: [Cryptography] 
So the necessary information isn't quite on display in the bottom of a locked
filing cabinet stuck in a disused lavatory with a sign on the door saying
'Beware of the Leopard', but close: The page points to usr/bin/ssh/PROTOCOL
which takes you to which is a CVS log with links to and then finally by grepping that for "etm" you get a rather terse two-
paragraph description of what OpenSSH does.  Specifically, it looks like
OpenSSH does two things that I've argued for in the past, sending the length
field in plaintext so you don't have to decrypt and process unauth'd data in
order to see what comes next (in other words so you don't act as an oracle for
attackers), and using EtM.

@_date: 2015-01-06 14:29:13
@_author: Peter Gutmann 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
There have been even more recent attempts to design (at least as gedanken
experiments) secure rotor machines, see Ross Anderson's "A Modern Rotor
Machine" from FES'93, available via

@_date: 2015-01-06 23:14:24
@_author: Peter Gutmann 
@_subject: [Cryptography] 
For other people who have asked as well, I've put it up at

@_date: 2015-01-10 17:42:12
@_author: Peter Gutmann 
@_subject: [Cryptography] Compression before encryption? 
This seems to be based on thinking dating back to hand ciphers and mechanical
cipher machines, for which this was indeed the case.  Any decent cipher today
has to be resistant not just to known-plaintext but to chosen-plaintext
attacks, so hiding plaintext patterns doesn't really get you much.  OTOH it
opens you up to a pile of oracle attacks based on compressability of
plaintext, so the answer to "should I compress before encrypting?" is no-
biased, it'll definitely hurt you but it's questionable whether it'll help

@_date: 2015-01-18 16:27:49
@_author: Peter Gutmann 
@_subject: [Cryptography] Summary: compression before encryption 
This doesn't work.  I looked at this in the early 1990s, the problem is that
any decent compressor adapts to the characteristics of the source that it's
processing, so you rapidly lose any unpredictability caused by the keying.
What I played with was creating a synthetic source based on a user-supplied
key and feeding that to the compressor so that the source model it built was
key-dependent.  Since the compressor adapts rapidly to changes in source
characteristics, after seeing data from the real rather than synthetic source
it had discarded all the synthetic-source information and was only using the
real source characteristics.  You can inject noise back into the source to
avoid this, but all you're doing there is screwing up the compression
In order to compress well, the compressor has to track the source as precisely
as possible.  What keyed compression does is mess up this capability, so that
the better the keying is, the worse the compression gets, until your ultimate
keyed compression is incompressible.  Redde Caesari quae sunt Caesaris, do
your compression with your compressor and your encryption with your encryptor,
don't try and mix the two.

@_date: 2015-01-24 02:26:14
@_author: Peter Gutmann 
@_subject: [Cryptography] Android's Secure ADB as a security hole 
In Android 4.2, Google introduced something called "Secure ADB".  It works
like this:
Device -> Host: 20-byte random value ADB_AUTH_TOKEN.
Host -> Device: signed( 20-byte random value ) using RSA with SHA-1
                ADB_AUTH_SIGNATURE.
So if you send in a SHA-1 hash of something (which happens to be 20 bytes), the host is required to sign it for you and send you back the signature.  What you're getting is a hash of a hash, but that just means you need to find a signature that uses this doubled hash, like S/MIME signed attributes.  From tracing through the source code:
it doesn't look like the 20-byte limit is enforced anywhere, so by the looks
of it you can send in something slightly longer (e.g. the S/MIME attributes)
and you'll get back a signature on them from the host.  Rewrite the signature
into S/MIME form and you're done.
Can someone who's more familiar with Android internals verify whether this
signing-oracle-by-design really is there?

@_date: 2015-01-26 23:25:47
@_author: Peter Gutmann 
@_subject: [Cryptography] Android's Secure ADB as a security hole 
When I was looking at this I did some googling to try and figure out what
controls there were on keys and found somewhere in the ADB docs:
  If needed, the ADB_KEYS_PATH env variable may be set to a :-separated (;
  under Windows) list of private keys, e.g. company-wide or vendor keys.
The key is a generic OpenSSL PEM file, so all you need to do is point at your
company-wide private key in PEM format (conveniently available, and it even
comes with a certificate so why not use it?) and your "secure" ADB is a
signing oracle for your corporate key.

@_date: 2015-01-29 04:34:31
@_author: Peter Gutmann 
@_subject: [Cryptography] Android's Secure ADB as a security hole 
Great :-).  A better long-term fix though would be to borrow something from
Abadi and Needham's "Prudent Engineering Practice for Cryptographic
Protocols", and specifically take Principle 1 which says:
  Every message should say what it means. The interpretation of the message
  should depend only on its content. If the identity of an agent is essential
  to the meaning of a message, then the message should mention that agent
  explicitly.
The ADB design violates this rule, since the message is totally opaque.  The
spec says it should be 20 random bytes, but that also makes it
indistinguishable from a SHA-1 hash of a message, which is a message type
that's used very widely in signatures.  So it's still a signing oracle, just a
somewhat more restricted one.
A better solution would be to add something to what's being signed that's
controlled solely by the signer, so that the client can no longer force the
signer to sign whatever it wants:
  sign( "ADB authentication" || nonce || ADB_AUTH_TOKEN )
(the nonce is to make sure that the signer never signs a predictable-to-the-
client value, possibly overkill but if "MD5 considered harmful today" is
updated at some point to become "SHA-1 considered harmful today" then it'll
save your bacon.  The downside is that you'll need to communicate the nonce
back to the client for when they verify the signature.  Also, if you really
wanted to do it properly, you could make it a Halevi-Krawczyk hash/RMX...).

@_date: 2015-07-30 05:07:28
@_author: Peter Gutmann 
@_subject: [Cryptography] How to solve the hen-and-egg problem 
Even if it's used widely, it may never get looked at. I probably don't need to
name names here for examples of OSS security apps where no-one ever bothered
looking at the code... come to think of it, the only well-audited apps [0] are
ones where the authors themselves take the time to do the auditing.
Occasionally someone will take a peek inside some code somewhere, and with
distressing frequency find security problems, but I'm not sure if anyone ever
sits down and says "I'm going to spend the next six weeks reviewing the XYZ
code base".
So you've (unfortunately) really only got two options:
1. Review it yourself (which includes using static source code analysers,
   Valgrind/ASAN, and every fuzzer you can run on it).
2. Pay someone else to review it.
[0] Excluding crisis-mode audits like the ones for Truecrypt and OpenSSL.

@_date: 2015-06-02 00:04:15
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Why is ECC secure? 
Just wondering, do you actually read the messages you're replying to, or do
you just trigger off certain keywords in them and spout out a response?  In
any case since this discussion isn't going anywhere useful and I don't want to
waste the rest of the list's time, I'll bow out now.  Good luck in pushing
your absolutely foolproof cryptosystem in all its flawless perfection and

@_date: 2015-06-08 09:08:55
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Why is ECC secure? 
Depending on what your overall goal is, I would argue that teaching public-key
cryptography at anything other than a very abstract level is detrimental
rather than helpful.  A year or two back I was doing some work with a company
who had a pretty bright intern in who'd learned the theory behind RSA, DH, ECC
(he had an enthusiastic teacher who was into all of these things), and was
pretty good at all of that.
This provided him with approximately zero help when he had to work with a SCEP
client and X.509 certificates.  Even if someone had asked him to write an RSA
implementation from scratch (why would anyone ever do that?), since there
wasn't time in the course to cover every nuance of every PKC algorithm (side-
channels, formatting issues, use of entropy/nonces, etc etc) the result would
have been an implementation that needed another five years of massaging and
upgrading before it was ready for real-world use.
So he would have been better served with a course that provided a fairly
abstract overview of the principles of PKCs, and then a lot of exposure to how
they're applied in the real world.  He was a bright, enthusiastic kid so I
just hope his first experience with real-world use of these systems didn't
scare him away...

@_date: 2015-06-08 09:12:24
@_author: Peter Gutmann 
@_subject: [Cryptography] let's kill md5sum! 
Yup.  There are a lot of cases where it's used as a kind of super-CRC32, to
detect data corruption on storage media rather than malicious alterations, and
it's perfectly adequate for that.  It's nearly universally available and
provides a convenient means of verifying that your transfer via FTP or USB
flash didn't flip a few bits or lose some blocks somewhere.

@_date: 2015-06-13 20:19:42
@_author: Peter Gutmann 
@_subject: [Cryptography] New attack on SHA-1 
The IACR ePrint archive has just published a new attack on SHA-1:
  Practical Free-Start Collision Attacks on 76-step SHA-1,
  Since it's a free-start attack on the compression function (i.e. you need
control of all input parameters to the function) it's not an attack on the
full SHA-1, but in any case they've implemented it on a GPU (GTX-970) and it
takes less than 5 days runtime to find a collision for 76 rounds.

@_date: 2015-03-01 19:20:27
@_author: Peter Gutmann 
@_subject: [Cryptography] The Crypto Bone's Threat Model 
On the occasions where I've run into it at a low enough level, the lock is an
opt-in rather than opt-out, so the reader has to specifically include code
  if R/O tab set -> return EACCES
On one occasion we got some rather baffled looks from an embedded-systems
vendor whose devices would happily write all over write-protected SD cards.
It was a *feature*, if the field engineers forgot to slide the tab into the
correct position the system wouldn't shut down due to lack of writeable

@_date: 2015-03-04 15:19:48
@_author: Peter Gutmann 
@_subject: [Cryptography] Air Traffic Control computers are maintained 
There's another view of that as well, which is that while lack of security
controls may threaten "the agency's ability to ensure the safe and
uninterrupted operation of the national airspace system (NAS)", enforcement of
standard security controls would virtually guarantee that "the agency's
ability to ensure the safe and uninterrupted operation of the national
airspace system (NAS)" would be impaired.  For example:
  In some situations the use of shared credentials is even enshrined in
  organisational policy, typically where the organisation has analysed the
  situation and decided that availability is more important than nominal
  security.  For example in air traffic control, where availability is
  paramount, organisations use group passwords for systems that control radar,
  navigation and communications gear, the instrument landing system (ILS), and
  similar equipment.  The fact that everyone in the group knows the password
  means that there???s little chance of anything ever being inaccessible for
  lack of the correct password, with 41% of Federal Aviation Administration
  (FAA) facilities in the US reporting the use of group logins for this
  purpose [86].
  [...]
  An extreme case of the shared login is the 24-hour login, in which the user
  is never logged off.  This is typically used in high-availability systems
  that are manned around the clock, thereby speeding up shift changes (there???s
  no need for one shift to log out and the next one to log straight back in
  again) and increasing availability since a system that???s never logged out
  can???t become inaccessible for lack of a password.  This is an example of the
  type of practical flexibility and resilience that theoretically less secure
  passwords add to a system, in which the use of a stricter, theoretically
  more secure access control mechanism would significantly impact the
  reliability and availability of the overall system, a social issue discussed
  in more detail in ???User Conditioning??? on page 16.

@_date: 2015-03-05 17:00:17
@_author: Peter Gutmann 
@_subject: [Cryptography] practical verifiable systems -- forensic and 
If SMM, ring -1 access to your system, is bad, LOM, ring -2 access, is a
nightmare, typically an ancient, buggy, vuln-riddled version of Linux running
a collection of equally buggy and insecure services that have complete
backdoor control over your system and that typically can't actually be
disabled even if you think you've disabled them.
I don't remember offhand seeing much about exploiting the various LOM systems
in the NSA's catalogues, but I'd be very surprised if they didn't have
implants for the various systems out there.

@_date: 2015-03-06 17:13:15
@_author: Peter Gutmann 
@_subject: [Cryptography] FREAK attack 
+1.  The only other requirement would be to go to EtM, which Hugo Krawczyk pointed out for SSL in "The Order of Encryption and Authentication" about fifteen years ago, so that falls within the "past 15 years" requirement.
Make that change and about 95% of all crypto-related atacks on SSL/TLS [0] would never have happened (NB: this is specifically crypto-related attacks, not protocol flaws like rehandshake issues which fall outside the scope of the [0] Anyone want to go through the complete catalogue and see whether anything
    would have succeeded?  The "95%" figure is freely pulled from thin air,
    but it could be closer to 100%.

@_date: 2015-03-07 21:44:08
@_author: Peter Gutmann 
@_subject: [Cryptography] FREAK attack 
Nope, not really.
EDH+RSA, 3DES, and HMAC-SHA1 (with EtM) would still work for the next 3-5
years, although they're getting pretty dated.  Since AES has been around for
15 years now I'd have switched to EDH+RSA, AES, and HMAC-SHA2 at some point,
but the original question was what suite would work for 15 years, and that's
the one with 3DES et al.
None if you use the original suggestion throughout, or at most a best-suite-
suported flag to allow upgrade to the AES/SHA2 version.
Perhaps, probably, go ahead and do it then.  We'll wait here.
(OK, don't go ahead and do it because we won't hear from you for a very long
time, and your input would be missed, but you know what I mean).

@_date: 2015-03-13 17:17:26
@_author: Peter Gutmann 
@_subject: [Cryptography] Securing cryptocurrencies 
Not necessarily:
  Mining Bitcoins requires finding a bit string that yields a SHA-256 hash
  value beginning with a certain number of zero bits.  In other words to mine
  a Bitcoin you need to hash data values until you find one whose hash begins
  with the required number of zero bits [82][83].  To do that you need a means
  of calculating SHA-256 hashes very quickly.
  [...]
  Passwords and encryption keys are often protected using the same hash
  algorithms that the mining ASICs (and FPGAs and GPUs) are designed to
  calculate at great speed.  By repurposing the hardware that was originally
  designed for Bitcoin mining it would be possible to attack hashed passwords
  with an efficiency that wasn???t feasible before Bitcoin appeared.  Having
  said that though, the Bitcoin ASICs for which details have been published
  are specifically designed for high-speed mining rather than password-
  cracking, so that they would require significant changes to their control
  circuitry in order to make them suitable for password cracking ??? it???s not
  for nothing that they???re called application-specific ICs)

@_date: 2015-03-15 18:10:27
@_author: Peter Gutmann 
@_subject: [Cryptography] Fun and games with international transaction 
Looking at the ever-useful  (which currently seems to be
out of action, you can find older screenshots via Google, e.g.
 if I'm
reading that right then US personal debt is at about the same level as
sovereign debt, with savings essentially nonexistent.  This means that the
assets aren't capital but fixed (and some intangible) ones, and what will the
PRC do with Mt.Rushmore and Golden Gate Bridge?
I think it's not so much that it's independent of the banking system, it's
that there's a perception that it's not tied to the house of cards that is the
current financial system.  Of course BTC is its own house of cards, but as
long as there are people around to believe in it...

@_date: 2015-03-20 00:13:42
@_author: Peter Gutmann 
@_subject: [Cryptography] TB2F CAs as (un)official browser policy 
An interesting discussion is currently occurring on the Mozilla security
policy list.  It seems a CA is late in filing an acceptable audit statement
(the sort in which Ernst and Young said DigiCert was OK, WebTrust said
TrustWave was OK, PWC said DigiNotar was OK, and so on).  The deciding factor
on pulling the CA's cert is:
  Richard Barnes has verified that there's minimal compatibility impact to
  removing this root certificate. Current telemetry shows that this root has
  been responsible for 9.57k out of 9.4B validations, or about one in a
  million.
OTOH if you're TB2F and get 0wned by Iranian hackers (Comodo, not DigiNotar,
who weren't TB2F) then nothing happens.  So alongside DigiNotar's cert count,
we now have another lower bound for the browser vendors' TB2F criteria, 10K
certs issued isn't enough.

@_date: 2015-03-20 13:45:15
@_author: Peter Gutmann 
@_subject: [Cryptography] TB2F CAs as (un)official browser policy 
I can't see that in the thread, there were a bunch of replies along the lines of (to quote one message) "If they can't follow the rules, they need to go", and then a longish discussion on removing trust bits vs. removing the CA cert.  The bugzilla entry ( says the same thing, "pull the cert", with replies indicating that it's been pulled (or at least that a code update to do so is in the pipeline).
Sure.  So there are a number of CAs all called Digicert, the one I was referring to was the Malaysian (bad) Digicert, not any of the other (apparently OK) Digicerts.
It's also been pointed out that the figure given was for validations of the cert by Mozilla clients rather than number of certs issued.  I assumed it was a glitch in reporting the figures since I couldn't imagine that Mozilla apps would report all cert validations back to Mozilla ("[Mozilla telemetry] may collect anonymized site visit information in some circumstances, such as when a secure browsing connection fails to connect, or for some experiments"), but apparently they do.

@_date: 2015-03-23 20:53:59
@_author: Peter Gutmann 
@_subject: [Cryptography] Kali Linux security is a joke! 
That doesn't really tell you much about what they're doing though.  They could
be taking advantage of the endless EasySocketFactories and related things that
don't validate connection security (there's bound to be 10 million Android and
iOS-app TLS transactions per day doing that), or given the year, take
advantage of goto fail and its GnuTLS equivalent, or steal the RSA key for a
site that doesn't use DH so they can later decrypt all TLS sessions, or
whatever.  If you use DH and validate parameters (and as a corollary use a TLS
implementation that's done right) then there's no reason to suspect TLS isn't

@_date: 2015-03-26 01:57:43
@_author: Peter Gutmann 
@_subject: [Cryptography] What if your CA's HSM was an Android tablet? 
One of the interesting points in this week's regularly-scheduled browser PKI
debacle was that the (intermediate) CA that issued the Google MITM certs was
using a Palo Alto firewall as an HSM.  This arises because of the CAB Forum
baseline requirements that:
  "The CA SHALL protect its Private Key in a system or device that has been
  validated as meeting at least FIPS 140 level 3 or an appropriate Common
  Criteria Protection Profile or Security Target, EAL 4 (or higher), which
  includes requirements to protect the Private Key and other assets against
  known threats."
Until now it had been implicitly assumed that CAs would be using an HSM, but
the requirements really just say "must have a FIPS sticker on it somewhere"
(technically the CA was non-compliant because while the firewall they used had
mostly level 3 specs, the physical protection wasn't there so it only got a
level 2 overall).
So lets look at:
to see what else a CA can use to store its root keys... alongside actual HSMs
there's the usual list of routers and firewalls, but also lots and lots of
disk drives (HDD and SSD), digital cinema projectors, franking machines, and a
whole array of USB keys, alongside more unusual items like satellite modems, a
medication monitor (like the firewall used by the CA, it's mostly level 3 but
only level 2 overall), a barcode reader (as above), Java cards, a handful of
tape drives, and, most interestingly, a family of ARM CPUs targeted at smart
phones, tablets, and thin clients, including among other things the OLPC XO-3
So, using the interpretation of the CAB Forum requirements pointed out by this
week's MITM CA, you can run your CA using a Dell thin client, a Panasonic
Android tablet, or an OLPC, as its security module.
Or, as has already been demonstrated, a Palo Alto firewall with issues like:
  A vulnerability exists whereby an unauthenticated user can execute arbitrary
  code as root on the device. (Ref   This vulnerability can result in arbitrary command execution, and can result
  in total compromise of the device.
farce (n): a comic dramatic work using buffoonery and horseplay and typically
including crude characterization and ludicrously improbable situations.
[0] The product certified was the PXA610, the OLPC uses the PXA618 which is a
    minor rev of the same thing, it's unclear whether the claim is for the     PXA610 platform or the specific PX610 device, and/or how far claims over
    what was certified will be stretched.

@_date: 2015-03-27 23:23:11
@_author: Peter Gutmann 
@_subject: [Cryptography] Certificate transparency on blockchains 
The spec was created by Google, the RFC was written by Google, all the authors
work for Google, I'd say that calling it "Google's CT" is fairly accurate.
Failing that, I think the people who are calling the IETF's MS-CHAP standard a
Microsoft standard also need correcting.

@_date: 2015-05-10 00:20:44
@_author: Peter Gutmann 
@_subject: [Cryptography] Is there a good algorithm providing both 
It's also safe if used in non-interactive environments (e.g. store-and-forward
messaging).  One of the major drivers for compression in CMS was EDI use,
where messages are (a) enormous, (b) highly compressible, and (c) chosen-
plaintext attacks aren't an issue.

@_date: 2015-05-11 01:15:54
@_author: Peter Gutmann 
@_subject: [Cryptography] Is there a good algorithm providing both 
Sure, you can come up with hypothetical examples to demonstrate almost
anything about any crypto or security issue.  As used in EDI though, it's safe
And in general you need to ask "safe against what?" rather than just saying "safe" or "unsafe".  Most users of crypto don't know or care about traffic analysis, it's only a tiny subset of security geeks who do, so any definition of "safe" doesn't need to include "safe against traffic analysis".  When I used the term "safe" I meant "safe against anything that EDI users care about", for example.

@_date: 2015-05-11 22:09:07
@_author: Peter Gutmann 
@_subject: [Cryptography] Is there a good algorithm providing both 
Sure, I'm aware of over two dozen papers on this topic (discussing the ability to identify spoken phrases, language used, video content for movie streaming, web site browsing patterns, e.g. which tax bracket you're in based on which pages on tax sites you visit, and so on), but as I said it depends on whether the user considers this a problem or not, and by and large the masses (which includes companies, banks, government organisations, and so on, not just Joe Sixpack at home) don't.  Skype is in a particularly bad spot here with their use of encryption because the vast majority of users don't care whether it's encrypted or not (they just want free calling over the Internet), and those who really care about encrypted phonecalls typically won't use Skype because they let governments listen in.

@_date: 2015-05-21 01:02:31
@_author: Peter Gutmann 
@_subject: [Cryptography] Intel SGX: Augean stables piled higher & deeper? 
Because all of the existing security mechanisms we use also have rigorous
mathematical proofs?
Can someone give the record player a bit of a thump, I think the needle's
stuck again.

@_date: 2015-05-28 21:26:15
@_author: Peter Gutmann 
@_subject: [Cryptography] open questions in secure protocol design? 
One generalisation I think is that Schneier and Ferguson's "security protocols
should not be designed by a committee" still holds (following on from the
implied "security protocols should not be designed by people who don't know
much about cryptography").  The every-algorithm-ever designs (TLS now has
what, 400 cipher suites?) seem to come as a byproduct of design-by-committee
specs, while having one or two people who know what they're doing do the work
leads to much cleaner designs.
(A possible rule for this would be that you're allowed two each of a PKC,
hash/MAC, and block cipher/mode.  Every time you want to introduce something
new, you have to throw an existing one out.  That'd make people think...).

@_date: 2015-05-28 21:33:06
@_author: Peter Gutmann 
@_subject: [Cryptography] I broke a cipher this week. 
The IETF is set of committees.  Committees work by throwing in every feature
anyone on the committee wants.  So the mess of algorithms is, as you say, a
byproduct of the way the IETF is structured, but is mostly an artefact of that
structure, not a great need or desire for agility (no matter how diverse your
environment, no-one needs four hundred cipher suites, or IPsec's Chinese-menu
equivalent thereof).

@_date: 2015-05-28 21:43:38
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: open questions in secure protocol 
See the discussion that precedes this one on the SAAG list, in the thread at
  This has
been covered in some detail there, the cross-reference is to try and avoid re-
running the thread on this list.
(Oh, and while you're at it and just for giggles, check out this thread:

@_date: 2015-06-01 00:52:08
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Why is ECC secure? 
That's mostly just bad implementation issues, at which point you may as well
call using fixed pre-generated keys shared across tens of thousands of systems
and publishing your server.pem in webroot as RSA weaknesses as well.  It's the
DLP-based cryptosystems that are the really brittle ones, starting with their
scary propensity to leak bits of, or in some cases all of, the private key if
you get the slightest thing wrong.  Heck, even in normal operation with
nothing done wrong, you can still leak bits of private data (I'm thinking of
the recent discussion about the appropriate choice of DH primes on the TLS
list, where you end up leaking a bit of the private data for each DH
(Another thing about this is that we've had 30 years of RSA use to try and
iron out the bugs, while ECC is just starting to take off.  Given an
intrinsically brittle algorithm and not too much experience so far in finding
all the ways you can get things wrong, I expect to see lots more ECC failures
in the upcoming years).
I would have said the same thing for DLP-based PKCs.

@_date: 2015-06-01 00:57:19
@_author: Peter Gutmann 
@_subject: [Cryptography] open questions in secure protocol design? 
Oh not this old saw again.  How many times has this happened with any
properly-designed algorithm (DES, RC2, RC4, IDEA, RC5, AES, MD5, SHA-1,
RIPEMD, SHA-2, RSA, DH, DSA, Elgamal, ECDSA, and so on)?
Actually, it's never happened.  Ever.
OTOH any mechanism deployed to deal with this is going to be something that
can't easily be tested beforehand but that has to work perfectly, and
perfectly securely, the first time it's used.  Sort of like SDI, but not as
simple and straightforward.

@_date: 2015-06-01 09:03:25
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Why is ECC secure? 
Sure, just give me about ten years or so until we've built up some real-world
experience with it (because currently we have basically none, which is what
makes it easy to claim there's no problems) and I'll be able to give you some
Which is precisely the point I was making in my previous message.

@_date: 2015-11-01 04:20:09
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
Not if the documentation doesn't tell them what the annotation will do.  As
I've already said twice now, the various people I've talked to about this
assumed that what the annotation did was warn about inadvertent null pointer
use.  It's not a large sample, but of that sample, 100% assumed from reading
the docs that it did more or less the opposite of what it actually does.  This
implies that either the compiler or the docs need to be fixed.  The gcc
developers have chosen to do neither.
I find them useful guidance, if you assume the compiler/compiler developers
are hostile, you can take steps to guard against them.  It's like telling
someone "don't walk through (the streets) south of Market St in San Francisco
late at night by yourself", you've been warned of danger and can take steps to
ameliorate it.

@_date: 2015-11-01 11:17:15
@_author: Peter Gutmann 
@_subject: [Cryptography] How programming language design can help us 
The compiler being referred to that does the analysis for non-NULL isn't gcc,
so it won't optimise the check away.  It's a bit of a tautology, gcc won't do
the analysis and will optimise away the null check, the other compiler does
the analysis and won't optimise away the check.
(I'm just playing with it at the moment, the analysis is incredibly powerful,
if a bit quirky at times when the analyser reasons itself down the wrong

@_date: 2015-11-02 00:09:38
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
Repeating, for the third time now, the same thing, 100% of the people I've
talked to about this (same qualifier as the other times, small sample size)
interpreted the docs to mean that the compiler does more or less the opposite
of what it actually does.  This indicates that either the compiler or the docs
have a problem that needs to be fixed.
We can keep going around and around and around in circles more or less forever
here, but I think other people will get bored.

@_date: 2015-11-02 00:15:54
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
The term was coined by psychology professor James Reason, whose specialty is
the breakdown of complex technological systems, it applies to "problems that
aren't discovered until the user has fallen victim to them" (see "Human
Error", James Reason).  I did choose the words with care.

@_date: 2015-11-02 00:33:46
@_author: Peter Gutmann 
@_subject: [Cryptography] How programming language design ca help us 
At the risk of sounding like a Microsoft commercial, there's one company
that's going to great lengths to deal with the "perfect programmer"
assumption.  For example here's ExAllocatePoolWithTag():
  __drv_allocatesMem(Mem)
  _When_((PoolType & PagedPool) != 0, _IRQL_requires_max_(APC_LEVEL))
  _When_((PoolType & PagedPool) == 0, _IRQL_requires_max_(DISPATCH_LEVEL))
  _When_((PoolType & NonPagedPoolMustSucceed) != 0,
       __drv_reportError("Must succeed pool allocations are forbidden. "
                         "Allocation failures cause a system crash"))
  _When_((PoolType & (NonPagedPoolMustSucceed |
                    POOL_RAISE_IF_ALLOCATION_FAILURE)) == 0,
       _Post_maybenull_ _Must_inspect_result_)
  _When_((PoolType & (NonPagedPoolMustSucceed |
                    POOL_RAISE_IF_ALLOCATION_FAILURE)) != 0,
       _Post_notnull_)
  _Post_writable_byte_size_(NumberOfBytes)
  NTKERNELAPI PVOID NTAPI
  ExAllocatePoolWithTag (
    _In_ __drv_strictTypeMatch(__drv_typeExpr) POOL_TYPE PoolType,
    _In_ SIZE_T NumberOfBytes,
    _In_ ULONG Tag
  );
This allows the compiler to check at compile time that you're using the
function correctly.  The above (which should be at least somewhat intuitive,
or maybe I've just spent too much time with the SAL annotations) specifies the
precise conditions under which you can call ExAllocatePoolWithTag().  Instead
of getting the dreaded IRQL_NOT_LESS_OR_EQUAL bluescreen at some point whem
the code is run, the _IRQL_requires_max_ annotation tells you at compile time
what you can and can't do.
The above example looks like quite a handful, but MS have done it all for you
so you don't normally see the annotations, just their product.
Oh, and when the annotation says something like _Post_notnull_, it's used to
guide the analyser to warn about problems.  NULL checks are not removed from
the code.
Now if only Windows 10 didn't suck so much, sigh.

@_date: 2015-11-02 03:27:36
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: How programming language design 
That is actually how a number of the other gcc attributes work.  For example
__attribute__ (( format ( printf, 2, 3 ) )) tells gcc to check the function as
a printf-style function, __attribute__ (( format_arg ( 2 ) )) tells gcc to
check the value as a printf-style format specifier, __attribute__ (( unused ))
tells gcc that a warning for an unused parameter isn't required for this
value, and __attribute__ (( warn_unused_result )) tells gcc to warn if you
don't use the return value of the function.
The latter is another gcc gem, it typically doesn't warn if you don't use the
result because the warning is generated at a different level than the data
flow analysis so it can't tell whether the result is used or not (assigning
the return value to a dead variable is enough), but then it also doesn't allow
a void cast to silence it in cases where you don't care about the return
value.  So something like:
  ( void ) fread( ... );
which has been valid at least since lint was introduced in 1979 will still
produce a warning about an unused result.  Vast avalanches of them in some
projects, so that you have to turn the check back off again.

@_date: 2015-11-03 13:08:29
@_author: Peter Gutmann 
@_subject: [Cryptography] How programming language design can help 
This only fixes some of the brokenness.  Even if you trawl through the huge
mass of docs and figure out the full (and ever-expanding) complement of flags
you need to pass to gcc to try and undo some of the mess the compiler creates
(this currently stands at "-fwrapv -fno-strict-overflow
-fno-delete-null-pointer-checks"), you still can't fix all the issues.  For
example on any known processor that gcc generates code for, taking the 32-bit
value 0x01 and shifting it left consistently gives you 0x80000000.  However,
the spec says this is UB, so gcc can do whatever it likes with that, even
though the behaviour is completely predictable and known.
The scariest example of how crazy things can get is with time handling.  Under
MSVC, to work with time values (e.g. "will an event that occurs in ten
minutes' time fall within a given window") you use "if( currentTime + 3600 >
threshold && currentTime + 3600 < other_threshold )".  While the spec says
that this is UB, the MSVC developers recognise that being able to actually
work with time values is kinda useful, and guarantee that the above will
always work [0].
With gcc the same thing involves jumping through all manner of hoops in order
to recast anything you do in terms of difftime(), the only operation that's
allowed on time_t's.  Anything but the most trivial operations then become a
form of mental gymnastics, like working with a one instruction set computer
(OISC) whose only instruction is difftime().  Since most people aren't geared
up to think in terms of programming OISCs, who knows how many errors will be
introduced by this...
Can you point me to an online repository of a significant body of code you've
written?  I'd love to run it through some analysers to see what they find.  I
mean, I'm sure you write absolutely perfect, fault-free C code, but it'd be an
interesting exercise nonetheless.
[0] OK, working with times far beyond the heat death of the universe isn't
    guaranteed.

@_date: 2015-11-08 02:50:28
@_author: Peter Gutmann 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
That "false sense of security" argument is one of the great bugbears of
security.  It's typically presented as "we can't use a less-than-perfect but
very effective security measure because it'll give people a false sense of
security" (with an implied "we'll keep using this theoretically perfect but
practically useless security measure instead").  The major real-world example
I like to use of this is browsers using TOFU/key continuity vs. using CA
certs.  TOFU isn't theoretically perfect ("it gives people a false sense of
security!"), but would make phishing vastly harder.  CA certs do virtually
nothing to prevent phishing, but in theory if they worked then they could, so
we'll keep using those rather than implementing TOFU/key continuity.
And that's the important point with less-than-theoretically-perfect crypto, if
you can get even a 10% reduction in attacks using some not-perfect mechanism
you've made a significant dent in what the attackers can do (to put this into
perspective, there's no real empirical evidence that CA certs have any effect
at all on attackers, so 10% is a huge improvement over the current state of
the art).

@_date: 2015-11-16 02:25:50
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  ratcheting DH strengths over time 
[Citation needed]
(Specifically, one that doesn't simply defer to numerology).

@_date: 2015-11-16 02:44:52
@_author: Peter Gutmann 
@_subject: [Cryptography] ratcheting DH strengths over time 
Very interesting idea.  One of the long-standing problems of crypto has always
been that in order to upgrade the security being offered you need to reinstall
the software.  This is one of the few situations where it can take care of
itself without needing constant reinstalls.

@_date: 2015-11-17 03:34:23
@_author: Peter Gutmann 
@_subject: [Cryptography] Long-term security (was Re: ratcheting 
That really depends on the manufacturer, some like Draytek keep up support
more or less indefinitely, they still issue firmware updates for decade-old
long-EOL'd products.  On the other hand you do pay quite a premium for that.
At the other end of the scale are vendors like Linksys, where it's typically
an unsupported legacy product the minute you walk out of the store with it.
Yeah, that's the killer, until they explode or melt they'll continue to be
used.  There's some upgrade pressure from ADSL1 -> ADSL2 -> fibre, but even
then what'll get put in is the cheapest, crappiest router the ISP can source
for you.
That won't happen, no-one's going to turn off HTTP 1.x.  At least one reason
for this is the HTTP 2 standards folks' attitude of "HTTP 2 is heavyweight to
implement, if you can't deal with that stick with HTTP 1".  This means they'll
have to continute to exist in parallel for perpetuity.

@_date: 2015-11-17 04:16:24
@_author: Peter Gutmann 
@_subject: [Cryptography] Long-term security (was Re: ratcheting 
Exactly.  There's a huge amount of SCADA out there with per-element symmetric
keys.  Works perfectly well, for the reasons you give.
You don't even need a "high-security" algorithm for it to be good enough.  For
example there's a security system that uses 16 bits of a truncated DES MAC to
protect a security perimeter.  That sounds terribly insecure, except that you
need to cut and splice a MITM into a fibre-optic link in under 50ms and then
generate a new MAC forgery every 50ms for an indefinite period of time (months
or years).
Paraphrasing Bruce Schneier, security is doing what's appropriate for the
situation, not a key size in bits.

@_date: 2015-11-17 04:22:34
@_author: Peter Gutmann 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
That's the "false sense of security" bugbear I mentioned in an earlier post.
What about just moving comms to a nonstandard port, or (better) requiring port
knocking, or at least doing something?  Relying on someone else to secure it
means you're going to end up with your totally unprotected device sitting on
the public Internet.  Even just using a nonstandard port will protect against
port-scanning attacks, and then you can add further defences as required, with
each one boxing the attack onto a smaller and smaller attack surface.  This
isn't a "false sense of security", this is actual security that's reducing
your attack surface, even if it's not theoretically perfect cryptographic

@_date: 2015-11-17 08:34:27
@_author: Peter Gutmann 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
... which means the attacker has to control a switch or router between the
victim and device and be actively listening at the time that comms take place.
That's a long, long way from being able to seize control of it via a random
port scan over the Internet.
You're still vulnerable to any number of attacks with full crypto, they're
just different attacks.
Crypto is not soy sauce for security [0].
[0] Patrick McKenzie, Kalzumeus Software.

@_date: 2015-11-18 04:40:49
@_author: Peter Gutmann 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
We (NZ) have a mix, the real cheapies are ionisation, the good ones are dual
(optical + ionisation).  The best-performing ones are dual, you want photo for
smouldering fires and ionisation for fast flaming ones.
Getting this at least somewhat on-topic, eventually we'll have IoT smoke
detectors like this:
which should (hopefully) be configurable enough that you can deal with false
positives and other annoyances of current detectors.
Next thing will be that you'll get pwned via your IoT smoke detector...

@_date: 2015-11-18 04:42:13
@_author: Peter Gutmann 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
It's also a fine way to break your entire customer base's operations in one
fell swoop.  The rule in embedded has always been "once it's installed and
configured, never ever touch it again".  Routers and the like (or more
generally Internet of Targets, IoT) count as embedded, not desktop PCs.
Autopatch/autoupdate for embedded is inconceeeevable.
You can't even present this as a business proposition.  If people cared about
having supported, updated devices then Linksys would be out of business and
everyone would buy Draytek (see my earlier post on support for these).  For
another example, no-one would ever buy Canon scanners again and Epson would
own the market (they're still releasing x64 driver updates for scanners dating
from Windows XP).
So you've got something that people won't take care of (and by "people" I mean
ones other than hardcore geeks), the market won't take care of, and you can't
legislate without comitting political suicide.  It's easy enough to
philosophise about it, but there's no obvious practical fix.

@_date: 2015-11-19 04:55:10
@_author: Peter Gutmann 
@_subject: [Cryptography] Long-term security (was Re: ratcheting 
Same with our government, which has to stockpile assorted  drugs for
emergencies.  They carried out ageing tests on them and found that the
effectiveness after ten years of storage was typically 90% of what it was when the item was new.  Saved them a fortune in costs in terms of throwing out a warehouse full of perfectly good medication every 18-24 months or Desperately trying to tie this back to security, it's another example of
believing what's on the label ("keep your AV up to date, don't visit
strange web sites") vs. asking practitioners what the real issues are
("use different passwords for each web site, make them random, and use
a password manager to deal with them").  Talk to a physician some time
about what the real dosages are for medication (not the max dosage on
the packages), what the real effective lifetime is, and what off-label
usages are common ("it's sold as X but it's terrible for that, everyone
uses it for Y instead even though it's not meant for that").

@_date: 2015-11-20 10:46:44
@_author: Peter Gutmann 
@_subject: [Cryptography] Burner phone == One Time (i)Pad 
Based on the recent terrorists-and-encryption media panic, I should point out
that burner phones aren't only used by terrorists.  A friend of mine works for
a women's shelter and they're a large consumer of burner phones, means of
communication that can't be turned into covert tracking/monitoring devices by
abusive exes.  A cellular carrier recently ran a special offer where you could
get a phone plus $20 credit for $20, they bought an entire store's stock in
one go.
There was some debate over whether they should send one of the guys with
middle-eastern looks down to pick up a case full of what were obviously going
to be used as burner phones...

@_date: 2015-11-20 23:16:12
@_author: Peter Gutmann 
@_subject: [Cryptography] ratcheting DH strengths over time 
The problem is that NIST make endless numbers of random recommendations, many
of which seem to be based on little more than numerology.  Cherry-picking one
of the many and saying "I toldja so" well after the event is easy enough when
you've got dozens of them to pick and choose from.
I could just as easily have said:
WHAT THE HELL IS WRONG WITH YOU PEOPLE? NIST^H^H^H^HHANS DOBBERTIN ***TOLD***
***YOU*** TO GET OFF THIS STUFF A DOZEN^H^H^H^H^H^H^HTWENTY YEARS AGO!
(I'm talking about RIPEMD-160 here, a drop-in replacement for SHA-1 that
hasn't succumbed to the SHA-1 attacks).
WHAT THE HELL IS WRONG WITH YOU PEOPLE? NIST^H^H^H^HFAPSI ***TOLD*** ***YOU***
TO GET OFF THIS STUFF A DOZEN^H^H^H^H^H^H^HMORE THAN TWENTY YEARS AGO!
(This time it's GOST GOST R 34.11-94, there's a 2^105 attack on it but that's
still a lot better than SHA-1's innate 2^80 collision security).
WHAT THE HELL IS WRONG WITH YOU PEOPLE?  THAT BALL WAS ON THE LINE.  CHALK
FLEW UP!  HOW CAN YOU POSSIBLY CALL THAT OUT?!  YOU CANNOT BE SERIOUS!
(Sorry, wrong thread).
Horses for courses.  A lot of the stuff out there doesn't have intelligence
agencies as attackers (and if they really want to get in, they'll get in no
matter what you do anyway).  For an awful lot of where my stuff gets used,
SHA-1 and 1024-bit keys are just fine for the foreseeable future (when I say
"a lot" I mean "probably the majority", but then I'd have to enumerate all the
uses to figure out what the real ratio is).  Heck, 512-bit keys are fine for
many applications, because it's only being used to keep out nosy people and
nuisance-level attacks.
Providing a gradual upgrade path for these situations has a far greater chance
of success than setting some unachievable (with the current hardware/software)
goal that signals to people that it's safe to ignore because it's so

@_date: 2015-10-02 01:32:02
@_author: Peter Gutmann 
@_subject: [Cryptography] Paper check security 
For non-US people, could someone explain how much of a problem this still is?
It's got to be at least twenty years since I last wrote a cheque, and even
then it was once or twice a year for special payments.

@_date: 2015-10-06 01:49:40
@_author: Peter Gutmann 
@_subject: [Cryptography] [openpgp] OpenPGP SEIP downgrade attack 
Nice work!
It's always been a quick hack though.  I didn't implement MDC for a long time because I was waiting for it to be done properly (encrypt-then-MAC),
but eventually I decided that a hack was better than nothing at all.  It's
really not hard to do properly, just take what CMS / S/MIME did and convert
the bit-bagging to PGP format [0].  Encrypting a non-keyed hash in CFB mode of all things is just asking for trouble.
Is the SEIP -> SE rewrite completely transparent, or are there implementation
quirks/peculiarities that make it work in some cases and not others?  It'd
be interesting to have a sample of a SEIP message with its SE rewrite to look
[0] It specifically protects against strip-the-MAC/rewrite-the-message     attacks, but if you *can* find an attack I'd be interested in hearing     about it.

@_date: 2015-10-06 01:51:58
@_author: Peter Gutmann 
@_subject: [Cryptography] [openpgp] OpenPGP SEIP downgrade attack 
We don't need to get it deployed, we need to get it replaced by encrypt-
then-MAC, with the whole handled in a manner where downgrade attacks aren't

@_date: 2015-10-07 13:50:52
@_author: Peter Gutmann 
@_subject: [Cryptography] [openpgp] OpenPGP SEIP downgrade attack 
Which has just as little support as a planned EtM mode?  The reason why I prefer EtM is that it can be pretty trivially retrofitted to
existing crypto (just add a SHA-256 MAC somewhere) and is compatible with any
existing cipher, while whatever AEAD mechanism is chosen (I'm guessing AES-
GCM, which seems to be fashionable) is purely for AES, there's no Twofish or
CAST or whatever AEAD mode defined.

@_date: 2015-10-08 14:59:15
@_author: Peter Gutmann 
@_subject: [Cryptography] [openpgp] OpenPGP SEIP downgrade attack 
AES-GCM is only fast on CPUs with dedicated hardware support for it (PCLMULQDQ
on x86), it's actually quite slow in pure software (on x86 the slowdown is
about an order of magnitude).  The figures are really all over the place
depending on what system it's running on, so it's a bit hard to generalise any
statement about it.
(It's also not clear whether someone encrypting a 10k email message with PGP
is going to notice it being processed at 100MB/s or 150MB/s).
It's also a lot more patented than GCM.
(I actually really like OCB and don't like GCM much, but the patent situation
makes it pretty problematic).

@_date: 2015-10-16 14:39:35
@_author: Peter Gutmann 
@_subject: [Cryptography] Fwd: freedom-to-tinker.com: How is NSA breaking 
Anything above 1024 bits is safe for some time yet.  For RSA (a less tough nut
to crack than the DLP):
  If you wanted to step beyond the 1024-bit key, you'd need to dedicate the
  entire Tianhe-2 supercomputer, the most powerful computer on earth at the
  time of writing, to breaking a single 1280-bit key (anything larger than
  that is out of reach, the Tianhe-2 only just has the resources to attempt a
  break on a 1280-bit key).
  Alternatively, you could just trojan or backdoor the server that you're
  interested in, which is what'll actually happen if someone wants to get in.
  Just the interest on the power bill from running the Tianhe-2 would be
  enough to bribe cleaning or maintenance staff to plug in a trojaned USB key
  for a minute or two.  And if you really are concerned about China secretly
  building a second Tianhe-2 and dedicating it to attacking your mail server,
  change your key once a year or so.  Or use a 1536-bit key.
As usual, XKCD says it better (and/or more succinctly) than I can:
As usual, XKCD says it better (and/or more succinctly) than I can:
In any case it's not the sooper-sikrit alien-technology NSA supercomputer you
need to worry about, it's the night janitor, the two dozen undisclosed 0days
in your VPN box, the fact that it was sent to you via standard Fedex, the fact
that it's controlled by Windows PCs, the fact the you allow BYOD devices to
connect to it, the fact that... well, you get the picture.

@_date: 2015-10-17 01:05:30
@_author: Peter Gutmann 
@_subject: [Cryptography] Fwd: freedom-to-tinker.com: How is NSA breaking 
For the same reason they're being reused everywhere in ECC: Complex to
generate correctly, and the known provenance of the published values.
(Well, in the case of the NIST primes the known provenance goes back to a
large spy agency with a history of putting backdoored crypto into standards,
but no-one seems too worried about that, or at least not worried enough to
stop using them).

@_date: 2015-10-17 01:28:48
@_author: Peter Gutmann 
@_subject: [Cryptography] Fwd: freedom-to-tinker.com: How is NSA breaking 
It almost certainly isn't.  If you read the Logjam paper it's actually three
different things:
1. A description of a downgrade attack on broken implementations.  This isn't
   a crypto weakness, it's just straight bad programming, like falling back to
   RC2/40 (hi, Microsoft!).
2. A discussion of the weakness of 768-bit keys and borderline nature of 1024-
   bit keys.  The same implementations that will fall back to 512-bit keys
   (see (1)) seem to be one of the few places left in mainstream crypto that
   still use keys already known to be weak 1-2 decades ago (hi, Sun/Oracle!).
3. Speculation about the NSA breaking 1024-bit DH to get into VPNs, mostly
   ignoring [0] the fact that almost any other (very effective) attack doesn't
   require any of this effort, and that all the mentions of specific
   successful attacks (rather than generalisations about techniques used) in
   the Snowden docs mention stealing keys, backdooring hardware, etc.
The third point seems to have now blown up into a general "ZOMG the NSAs can
break DH!", taking their direction from a general comment from an unnamed
source quoted by James Bamford about an "enormous breakthrough several years
ago in its ability to cryptanalyze, or break, unfathomably complex encryption
systems".  Clapper's comment is even less useful than this, using
"groundbreaking cryptanalytic capabilities to defeat adversarial cryptography
and exploit internet traffic" is exactly what the NSA was created to do, so
he's basically saying "we're doing our job".  He could have said the same
thing fifty years ago with "Russian communications" substituted for "the
Even if the anonymous-source comment is valid (I've occasionally had ex-
military/ex-spooks quote astounding things to me over the years, much of which
could never be confirmed or mapped to actual facts/events [1]), if you're
going to apply Delphic oracle-like post-hoc mapping of predictions onto events
then a far better fit for the "remarkable breakthrough" was "we figured out
how to design a PRNG that looks at first glance to be sound, and managed to
get it adopted into international crypto standards".
That's a perfect match for an "enormous breakthrough several years ago in its
ability to cryptanalyze, or break, unfathomably complex encryption systems".
Mind you so is just about anything else: We figured out that WEP wasn't
secure, we figured out how to hack Bluetooth pairing, we figured out that WPS
isn't secure, we figured out how to break A1/A2, we figured out how to bypass
[23 pages of further crypto weaknesses deleted]
If the comment is even valid and not just some guy shooting his mouth off, I'd
go for either EC-DRBG or "we figured out how to generate backdoored ECC curves
from a seed value (although they probably didn't name it BADA55) and get them
adopted into international crypto standards and widely used everywhere".  Finally, given that "several years ago" most SSL/TLS implementations (which
carries a lot more interesting traffic than IPsec does) were still using RSA
for key exchange and not DH (it's a relatively recent move to deprecate RSA
keyex and move to DH), telling your boss that you needed $x00,000,000 for a
DH-breaking supercomputer wouldn't have got you very far.
[0] Actually they do mention that "The attack system also seems to require
    knowledge of the PSK", which means that the DH is irrelevant because
    you've lost your auth key for the exchange.
[1] Lots of people do cool things in their jobs, and everyone embellishes a
    bit from time to time...

@_date: 2015-10-17 09:00:26
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: Fwd: freedom-to-tinker.com: How is 
Uhh, what's the value s1 when DHE is used, and how does the other side get a
copy?  TLS uses DHE to compute a shared secret and uses that as the premaster,
there's no s1 and s2.

@_date: 2015-10-17 12:33:03
@_author: Peter Gutmann 
@_subject: [Cryptography] Fwd: freedom-to-tinker.com: How is NSA breaking 
Actually that would be one caveat to apply for the paper, never underestimate
the brokenness of commercial/proprietary IPsec implementations (the Cisco
example you give is just the tip of the iceberg).  Things were particularly
bad with IKEv1, which was so hard to configure that vendors invented their own
management tunnels to bypass it, typically exquisitely homebrew protocols that
made every beginner mistake in the book (the worst I encountered was single
DES in ECB mode, with the "tail" (their words) of each message unencrypted
because it wasn't a multiple of 8 bytes, and the key being the shared password
used to set up the tunnel).  That could be another candidate for the NSA's
magic breakthrough, vendor homebrewed management tunnels.

@_date: 2015-10-20 03:00:01
@_author: Peter Gutmann 
@_subject: [Cryptography] Other obvious issues being ignored? 
Lots and lots and lots, and specifically questions so obvious that they
shouldn't even need to be asked.  Things like "will your implementation accept
RSA keys with exponent 1" (many did, until bad publicity forced a fix), "will
it accept keys known to be insecure twenty years ago?" (ditto), "will it
accept obviously non-prime primes for public keys?" (ditto), and so on and so
on.  To quote Bruce, many crypto-using applications are "as insecure as you
can possibly get away with", because they use crypto and are therefore secure
by executive fiat rather than actual practice.
You can't even come up with a checklist for this, because you'd have to ask so
many questions, and of such boneheaded obviousness, that you couldn't get
anyone to come up with them all.  In the meantime, people are so busy debating
whether they can use the Ed209 curve with the Blake7 hash function or not that
they're missing the fact that the app they want to use it with will happily
accept the number 15 as a DH prime (yes, there are browsers that did that).

@_date: 2015-10-20 03:23:56
@_author: Peter Gutmann 
@_subject: [Cryptography] Other obvious issues being ignored? 
Some other issues to take into account when trying to replace the parameters:
- Support for DH in MSIE was really poor for a long time, you needed a 1024-
  bit DSA server cert for it to work, and the only keys supported were in the
  range 512-1024 bits.
- Sun/Oracle (Java), ugh, also limited to 512-1024 bits for most of the
  lifetime of Java, finally fixed in Java 8 just short of the twenty year
  anniversary of the JDK.
- Because TLS sends the PKCS  form of the DH keys rather than the FIPS 186
  form, you can't verify the parameters you're being sent.  Using the RFC
  2409/3526 values makes parameter validation (via a memcmp()) functional, if
  not exactly elegant.  When you generate your own parameters, you're breaking
  the ability of the other side to verify your DH params.
- If you're worried about faults (embedded devices in harsh environments) then
  loading a fixed, pre-generated DH value from checksummed/ECC'd flash memory
  is safe, going through a lengthy keygen process with ephemeral data can be
  risky if there's a fault during the keygen process.
So it's not just a case of "scrap it and order a new one", you'd need to:
- Update TLS to allow transmission of the FIPS 186 form of the parameters
  (this is actually pretty trivial, you just need a signalling handshake
  extension in which the client indicates its readiness to accept the FIPS 186
  form), however getting it past the TLS WG could be nearly insurmountable.
- Figure out what the rollback implications are for the above.
- Figure out whether you want to risk keygen fault issues.
- Figure out how many clients will break when you move to keys over 1024 bits
  (if you're generating your own DH values then this is mitigated somewhat by
  staying with 1024 and doing a regen from time to time, even if the NSAs can
  break 1024-bit keys they probably won't expend the effort on a key that's
  only valid for a few hours).

@_date: 2015-10-20 10:38:09
@_author: Peter Gutmann 
@_subject: [Cryptography] Other obvious issues being ignored? 
Not being vulnerable to MITM attacks doesn't seem like a funny threat analysis
to me:
... or attacks.

@_date: 2015-10-22 02:42:30
@_author: Peter Gutmann 
@_subject: [Cryptography] Other obvious issues being ignored? 
Have you been reading the PHC list by any chance?  (There was a thread on this
on there recently).
gcc is by far the biggest offender in this regard (although LLVM seems to be
following hard in its footsteps).  For an example of how deep the braindamage
goes, consider the following (example provided by Alexander Cherepanov):

@_date: 2015-10-22 02:59:53
@_author: Peter Gutmann 
@_subject: [Cryptography] Other obvious issues being ignored? 
The problem is squarely with the compiler writers.  The C standard is (of
necessity, although in some cases it's really just unnecessary pedantry)
written in a rather generic manner.  One example is around undefined behaviour
(UB), where UB is an open-ended escape hatch under which anything is
permitted.  For example a compiler that, if it thought an integer overflow was
possible in a certain case, reformatted your hard drive, is fully C standards-
compliant (UB has been triggered, and the C standard doesn't specify what will
happen next).  gcc takes this approach.  That is, it doesn't reformat your
hard drive because the braindamage is so obvious that people would complain,
what it does is produce executable code that's nothing like what the source
code indicates.  MSVC takes the common-sense approach and leaves your code
intact (apparently icc does too, but I haven't been able to check that).  Both
compilers are fully compliant with the C standard, but one of them is so
dangerous that I can't believe it's being used for things like safety-critical
You can fix it in two ways, either the C standard or the compiler.  MSVC
doesn't have these issues (it doesn't break your code, or at least not as
freely as gcc does), it has a safe zeroise function SecureZeroMemory()
replacements for the traditional C unsafe ops (that's getting more into the
libraries than the compiler, but again on the gcc side it's something that the
glibc maintainer has resisted adding even for the simple strlcpy() function,
and Posix has rejected them as well).

@_date: 2015-10-22 03:20:25
@_author: Peter Gutmann 
@_subject: [Cryptography] Other obvious issues being ignored? 
Exactly.  Currently, the gcc developers think, and will argue till they've
blue in the face, that this behaviour is OK.  The MSVC developers don't.
Some compilers will make the same assumption.  The thing with the C standard
is that it's written in a manner where it doesn't exclude things like ones-
complement machines.  Now the last one I know of that did that was the CDC
6600 from 1965, but in theory you could be targetting a half-century-old
computer with your compiler and so the standard can't rule it out.  Some
compilers, knowing that they're generating code for a twos-complement
architecture, behave accordingly.  Other compilers also know that they're
generating code for a twos-complement architecture, but use the vague language
in the C standard to, as you put it, "the most evil things imaginable".

@_date: 2015-10-22 03:29:36
@_author: Peter Gutmann 
@_subject: [Cryptography] Other obvious issues being ignored? 
Beyond a certain point, you really can't use a checklist any more, you need
someone with actual security skills to write the code.  If you're having to
ask things like "does your code accept the value 15 as a DH cryptographic
prime" and "does your code allow RSA keys with exponent 1" then you're also
going to have to ask things like "does your code store its private keys in
Pastebin" and "does your code allow root access to the system if the user
enters xyzzy as their password" (what about "october"? What about "joshua"?
What about "joshua1"?), at which point there's a near-infinite amount of
further stupid that you need to check for.
If asked for a checklist, I'd assume at least some minimum level of security
competence, and so only have to cover the tricky stuff that does trip people
up from time to time.
Or just get the code written by someone who knows what they're doing.  Do you
really want someone who has to constantly refer to a Checklist of Boneheaded
Security Mistakes to be writing your crypto code?  If you walked into a
hospital for surgery and your surgeon started flipping through "Introduction
to Open-heart Surgery for Dummies", would you feel comfortable going under the

@_date: 2015-10-23 08:58:59
@_author: Peter Gutmann 
@_subject: [Cryptography] Other obvious issues being ignored? 
And how do you know you found and fixed all the problems?  Since compilers
(and by "compilers" I mean gcc mostly) quietly break your code behind your
back, you have no way of telling whether you really fixed things or not.
(Also, "portability" != UB, see further down).
Just to provide some data on this, the SOSP paper that inroduced the STACK UB
analyser found that UB code occurs in 40% of all Debian Wheezy packages (that
use C).  That means that gcc, the worst offender of the lot in terms of
breaking code (again from the SOSP paper) will break nearly half of all Debian
packages when it builds them (the figure of 40% is a lower bound since STACK
doesn't do the same level of analysis that gcc does).
I'll just repeat that again, gcc will quietly break nearly half of all the
packages that it compiles.
One of the examples they give is a pointer-overflow check.  Say you're
compiling code for x64, Sparc, Power, ARM, whatever.  You include code to
check for pointer overflows.  Because the Intel 8088, which shipped in 1979,
eight years before gcc existed and for which gcc has never been able to
generate code, had a segmented architecture for which the pointer-overflow
check doesn't work, gcc feels justified in removing the pointer-overflow
This is totally, utterly stupid behaviour for a compiler.
It's pretty much useless, it doesn't warn about situations where it'll break
code, and the small number of locations where it warns in my code are ones
where, no matter how hard I stare at the code, I can't see how any overflow
can take place.  So in the few places where it does produce a warning, it's
effectively telling me that it's going to break my code due to a false
positive.  Elsewhere it just silently breaks the code without notice.
Nonportable != UB.  I can write code that assumes twos-complement behaviour
and it'll work the same on any CPU architecture I can get it to.  It's
completely portable, but according to the C spec it's UB, so gcc feels
justified in breaking it.
Even if you look at a more architecture-specific level, if I'm compiling for
(say) x86-64 then gcc knows that there exists an instruction "multiply the 64-
bit value in RAX by another 64-bit value to get a result in the 128-bit
register pair RDX:RAX", but claims that it doesn't know that adding 10 to
INT_MAX on the same CPU that it knows every intimate detail of produces a
negative value.
This isn't "compliance to the C standard", this is just the gcc developers
being idiots.

@_date: 2015-10-24 05:12:45
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
No it's not.  Go back to my posting and read what it says, specfically the
fact that I point out that it uses argc to prevent the compiler from
optimising the contents of the code away (again, hat tip to Alexander
Cherepanov for providing the original).
If you still can't see what you've done wrong in your code based on my comment
above, use gcc -S.
This does however beautifully illustrate my point.  You've got an absolutely
trivial function with a total of ten lines of code that adds two values and
prints the result, and yet it does nothing like what you think it does.  If
you can't get a+b=c right, how are coders working on 1M LOC applications
expected to deal with every single case where the compiler can break their
Absolutely.  I'll note that compilers like MSVC, armcc, suncc, and IBM's xlc,
which follow exactly the same C standard that gcc does, don't break the code
in the same way that gcc does.  gcc is by far the worst compiler in terms of
code breakage.
Condensing replies to several emails to reduce bandwidth, Werner Koch
Oh, trust me, that wasn't harsh words :-).  I actually thought about what
description to use before I posted it, how would you describe someone who's
made a conscious decision that their product will act in a way that breaks
other people's products, in other words that it fails to function as expected.
You can't use "negligent" because they made a conscious decision to do so, and
then applying the maxim "never attribute to malice what is adequately
explained by stupidity"...

@_date: 2015-10-25 02:00:47
@_author: Peter Gutmann 
@_subject: [Cryptography] Other obvious issues being ignored? 
[Again combining several posts into one to save bandwidth]
Many of the commercial compilers, who as Mansour Moufid has pointed out have
obligations to their customers, do get it right.  As I mentioned in a previous
posting, based on data taken from the SOSP paper, MSVC, armcc, suncc, and
IBM's xlc (all of which happen to be commercial products) are pretty good.
gcc is the exact opposite.
Good point.  It gets scarier though when you look one step further downstream:
The customers of commercial-compiler customers are also customers.  The
customers of gcc's users are... victims.
It's sad to see that LLVM seems to be following gcc down the same rathole.
I'd always hoped that LLVM would gradually kill gcc and we'd finally be rid of
it, but in an attempt to supplant gcc they're also copying all of its
It's not a gcc bug, the standard is written in such a manner that any
behaviour, literally anything, for example reformatting your hard drive, can
be claimed to be standards-compliant.  gcc 1.17 actually did something like
that, although not quite as fatal:
  execl("/usr/games/hack", " 0); // try to run the game NetHack
  execl("/usr/games/rogue", " 0); // try to run the game Rogue
This is fully standards-compliant behaviour, since the magic UB has been
invoked and so the compiler can do anything it wants.
In the same situation, commercial-grade compilers issued a warning and

@_date: 2015-10-25 03:34:01
@_author: Peter Gutmann 
@_subject: [Cryptography] letter versus spirit of the law ... UB 
I would say it's actually miniscule, not just tiny.  Some data points:
* Measurements of deployed code
The SOSP'13 paper found that a minimum of 40% of all Debian packages have UB
issues (the figure could be much higher since they may not detect everything
that's there).  Taking this as representative of gcc-built Unixes in general,
that means that for any app you're running there's a nearly 50/50 chance that
it has gcc-inserted latent pathogens in it.
* Use of 'safe' libraries
OK, so we'll fix this by using carefully-written UB-safe libraries like David
LeBlanc's SafeInt or CERT's IntegerLib.  These libraries were written and
vetted not by J.Random coder but by security experts to safely handle integer
Only problem is that even with security experts creating the code, they still
trigger UB:
  If even experienced security conscious programmers can't write code that's
safe from gcc's breakage, how is J.Random programmer supposed to do it?
* Use of developers who write perfectly error-free code
John Gilmore pointed out in an earlier post that he addressed the problem by
writing perfect, UB-free code, and everyone else should just do the same.
Let's check some of that code (linked via  using a UB
analyser and see what we get...
  'In 1985 I wrote the "pdtar" program, which eventually became GNU Tar'
  I couldn't actually get to the point of running the analyser because of the
  avalanche of bugs like:
  error: non-void function 'fl_write' should return a value
  error: non-void function 'fl_read' should return a value
  error: too few arguments to function call, expected at least 2, have 1
    if (STDIN != open("/dev/null"))
Strike 1.
  'While at Cygnus I maintained the GNU Debugger (GDB) from 1990 through 1993'
  Unfortunately gdb 4.10 from 1993 was too old to build without rewriting way
  too much of it (e.g. use of varargs.h instead of stdarg.h), the closest I
  could get was gdb 4.18:
  Generated 4 warnings, see pstack.txt for details.
  A -Wall also produced lots of warnings, most weren't serious, variables set
  but never used, char * vs. unsigned char *, but some were certainly dubious:
  warning: cast to pointer from integer of different size [-Wint-to-pointer-cast].
Strike 2.
  'From 1996-2003 I co-created and sponsored FreeS/WAN'
  I grabbed FreeSWAN 2.0 from early 2003, but since it's patches to the Linux
  kernel and there's no easy way to build even the user-space apps I couldn't
  get any further on this.  However, attempts at building did turn up things
  like:
  # include "programs/pluto/constants.h" /* XXX this is crap */
  * XXX this is NOT the way to do things.
  * XXX should generate a diagnostic.
  XXX: Not very clean.  We manipulate the port of the ip_address [...]
  XXX: this is really ugly and only temporary until addrtot can [...]
  XXX This should be replaced by a call to the kernel [...]
  XXX the following hash is pretty pathetic   so I'll assume that the rest is going to be no better than pdtar or gdb, if
  I could get it to the point where the analyser would run on it.
Based on the above, I'd like to propose the gcc breakage hypothesis:
  With gcc as the adversary, it is not possible to write any nontrivial
  application that won't have latent pathogens inserted by the compiler.

@_date: 2015-10-28 03:08:14
@_author: Peter Gutmann 
@_subject: [Cryptography] Collisions w/SHA-1 ~$100,000 TODAY 
Don't worry, they're being kept safe by other issues:
    China Unable To Recruit Hackers Fast Enough To Keep Up With Vulnerabilities
  In U.S. Security Systems
What with the endless numbers of other readily-exploited security holes that
are available, it'll be a long time if ever before someone goes for a highly-
visible, easily-identified SHA-1 collision attack.

@_date: 2015-10-28 03:16:06
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] The Energy Budget for Wireless Security 
That's something that really needs saying.  The number of times I've had
people come to me agonising about which algorithm(s) they can pare down to in
embedded devices to save code space and CPU, when what they're using them with
is massively complex and heavyweight security stacks like TLS or SSH (and then
loading XML config files over their embedded web server).  For some reason
everyone seems to focus on the crypto algorithm and forget about the fact that
the crypto can run entirely in the RAM that their XML parser leaks every time
it loads a file.

@_date: 2015-10-29 04:25:54
@_author: Peter Gutmann 
@_subject: [Cryptography] Hiding parties identities 
It doesn't really reveal its identity (well, not unless it wants to), just some unique value identifying an account, like a 128-bit string.  So the
most an observer could do is determine that the same entity as before
connected, which could well be possible by other means like host system or
traffic fingerprinting.
When the opportunity has arisen, I've asked users of PSK whether this is a concern to them.  So far none have indicated that it is.  Another consideration is that since browser vendors have so far resolutely refused to support TLS-PSK (because it's not certificates, so you can't
have it), I'm not sure how many people it would actually affect.
It depends on what level of protection you want.  If it's not a concern (see above), then you're done.  If it's straight masking of the identity then using a 128-bit (or whatever) string is fine.  If you also need protection from an attacker being able to tell that whatever connected last week is the same thing that's connecting this week then that's a harder problem, but then again you're going to need to apply countermeasures up and down the stack to deal with fingerprinting issues.
So the question really is: What's your threat model?

@_date: 2015-10-30 03:22:31
@_author: Peter Gutmann 
@_subject: [Cryptography] composing EC & RSA encryption? 
Well, only of you're a dedicated NSA entrails-reader.  If you look at the
NSA's recent decision about Suite B, there are two things there, the move off
Suite B and the attempt to find a replacement.  The move off Suite B is no
surprise, the NSA are simply admitting that after a decade of fruitless
attempts to get anyone interested in it (outside of organisations with a
government gun pointed at their heads, who had no choice), no-one wanted it.
They were in the same position they were in before Suite B in terms of people
not being able to take advantage of COTS products, it still didn't solve the
Type-1-algorithm product problem.  So this is just admitting defeat after a
decade of not making any progress, not some admission of hitherto-unknown
security holes in Suite B.
The second issue is what to replace Suite B with.  They could have said "AES,
'25519, and SHA2, and we're done".  Instead, they're pushing yet another white
elephant to follow on from their previous herd.  After all this time they
still don't understand how COTS actually works.

@_date: 2015-10-30 03:44:20
@_author: Peter Gutmann 
@_subject: [Cryptography] letter versus spirit of the law ... Eventus 
Exactly.  There have been cases where the compiler/language policy was to
crash on numeric overflow, resulting in a rocket that cost $7 billion to
produce (with the rocket itself costing around $100M) exploding on launch.
Saying that destroying a $100M rocket is "not good policy" is an
Throwing an exception *during development* is perfectly OK, because you want
to detect problems like this.  However, this only works if you've got a test
suite that provides 100% coverage of every single obscure corner case and
condition, which is unlikely to be the case (the reason why they're "obscure
corner case and conditions" is because they almost never occur).
For a release build, you never want to deliberately crash.  If you continue,
things might be OK in the long run.  If you crash, things definitely won't be
OK 100% of the time.
There's a huge amount of code out there that works by coincidence.  For
example the code I posted earlier from pdtar worked purely by coincidence, an
appropriate value happened to be in the right register(s) and made the code
work.  It doesn't work by design, it works by coincidence, but the point is
that it works.
An example I ran into a while back, while auditing some code used in a
sensitive control application, was the use of a loop controlled by an exact FP
comparison.  Technically you can't do this because you can't guarantee an
exact match, you need to use something like 'abs( a - b ) < epsilon', but in
this case it worked fine because the FP hardware produced appropriate results.
Without wanting to spend hours poring through the C language spec, it's quite
possible that the gcc developers, taking their usual most-hostile
interpretation of no-exact-comparisons, would decide that since (technically)
you can never get an exact match, the result could be "optimised" to 'while( 0
)'.  The result would be an executable that, like one where you've replaced
all the mutexes with no-ops as an optimisation, worked fine most of the time,
except when it didn't.
So: Deliberately crashing in release code is pretty much always wrong.  OTOH
continuing anyway, even with slightly incorrect values, is often right, but in
any case still better than crashing.

@_date: 2015-10-30 03:58:03
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: How programming language design 
Do you mean PREfast?  That's an amazing checker, at least the newer versions
with less FPs and more meaningful diagnostics, and is getting better and
better over time:
C28020: The expression is not true at this call	
 The expression '0<=_Param_(1)&&_Param_(1)<=10' is not true at this call.	
 test.c	98
That's for a function taking as an arg and int 0...10 and passing an
unconstrained integer value to it.  And it's available even in the free
versions, which I hope was helped at least in part by my nagging about it :-).
You've also got things like a way of telling the compiler that a pointer
shouldn't be NULL, so it can warn about it, or that it may be NULL, so it can
warn about a deref without checking for NULL.
Even if they do, gcc will find a way to screw it up.  gcc has a similar
annotation to the PREfast one, __attribute__(( nonnull  )).  Purely from
reading the gcc docs (so you can't compile test code with it to see what
happens, and you can't answer this if you already know what'll happen), can
someone tell me what they think this will do if applied to the following code:
__attribute__(( nonnull 1 )) \
int double( int *ptr )
  {
  if( ptr == NULL )
    return( -1 );
  return( *ptr * 2 );
  }
  thing = double( thingPtr );

@_date: 2015-10-31 01:29:41
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
Oh, you're just assuming the gcc devs are following their usual "policy of
delivering the most useless possible conformance" :-).  Here's what the gcc
docs say for the annotation:
  The nonnull attribute specifies that some function parameters should be non-
  null pointers. For instance, the declaration:
          extern void *
          my_memcpy (void *dest, const void *src, size_t len)
          	__attribute__((nonnull (1, 2)));
  causes the compiler to check that, in calls to my_memcpy, arguments dest and
  src are non-null. If the compiler determines that a null pointer is passed
  in an argument slot marked as non-null, and the -Wnonnull option is enabled,
  a warning is issued. The compiler may also choose to make optimizations
  based on the knowledge that certain function arguments will not be null.
Here's what gcc actually does:
  The nonnull attribute specifies that some function parameters should be non-
  null pointers. For instance, the declaration [...] causes the compiler to
  silently remove any checks for null pointers in the code when compiling it.
  No warnings will be issued.
So it does pretty much the reverse of what the documentation says.  The reason
I asked for people's interpretations was because I wanted to see what the
average user would expect to happen based on what's documented.  So far my
data is mostly anecdotal, arising from discussions with people who had used
the annotation and thought it did what the docs implied, not what gcc actually
I had an offline discussion with someone playing devil's advocate to try and
justify this behaviour, by pointing to the added-as-an-afterthought "The
compiler may also choose to make optimizations based on the knowledge that
certain function arguments will not be null".  So the question for this is,
why would you write code that carefully checks for null pointers, and then
annotate it to tell the compiler to ignore the code that checks for null
(For people who are going to leap in with "but you've told the compiler that
the pointer is non-null", read the above paragraph again.  Why would you first
write code to carefully check for null pointers, then tell the compiler to
silently ignore that code?).

@_date: 2015-11-01 01:29:47
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: How programming language design can 
Not really, if you know how gcc works then it happens because the warnings are
issued by the front-end before data flow analysis occurs (so cases of
inadvertent NULL pointer use are missed) but then the optimiser later applies
the annotation to mean that the value can never be NULL.
This is about as useful a response as saying "well, there's a race condition
where, if the operator is entering data as the counter overflows, the beam
spreader plate isn't rotated into place and therefore you get hit with X-rays
at 25MeV, that's why your uncle died.  Quite straightforward really".
This is more or less the gcc developers' response, they explained in great
detail why gcc does what it does without making an attempt to fix it.  When I
asked them to at least update the docs to reflect what would actually happen
when you used the annotation, they ignored the request.
The reason why I asked for other peoples' interpretations of what the
annotation did was because I wanted to see what the typical developer would
think happened when they used it.  As I mentioned earlier, from the small
sample of people I've talked to about it, they were under the impression that
it would warn them about inadvertent use of NULL pointers.  At least one major
OSS project discontinued use of the annotation after finding out what it
really did.  I've defined it to a no-op in my code, it's literally worse than
useless, it has negative value.
(gcc is full of latent pathogens like this one).

@_date: 2015-09-01 07:47:55
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: NSA looking for quantum-computing 
I thought it was fifteen.  If it's now 21 then that's a tremendous amount of
progress in the last decade, you now need to train a dog to bark seven times
instead of five [0].
(In case it's not obvious, I'm not losing any sleep at all over quantum
anything, it's down at about position 9,000 in the list of practical attacks
against crypto-using applications.  I'll get back to it when the preceding
8,999 have been addressed).
[0] "The state of the art in quantum cryptanalysis is factoring the number 15.     The same can be achieved with a dog trained to bark five times.

@_date: 2015-09-02 19:00:27
@_author: Peter Gutmann 
@_subject: [Cryptography] Checking for the inadvertent use of test keys 
Let's say you've got some key-consuming code that's supposed to be fed random
keys and you want to catch inadvertent use of test keys and nonces, strings
like "012345678" and "\x01\x23\x45...", that sort of thing.  For argument's
sake these can be as short as 64 bits/8 bytes.  Can anyone think of any
reasonably simple tests that will catch keys like this?  Standard tests like
the FIPS ones won't catch them (through a combination of the sample size being
very small and the tests not being able to identify "\x01\x23\x45..." as non-
random).  I was looking at things like edit distances between bytes, but you
rapidly end up in a mess of epicycle-style hacks as you add handling for
special cases.
What I'm after is a simple safety check to warn about inadvertent use of
predictable sequences that might get used as test keys.  I'm after actual
algorithmic tests rather than general advice like "have you looked at
Kolmogorov complexity"...

@_date: 2015-09-03 05:26:41
@_author: Peter Gutmann 
@_subject: [Cryptography] NSA looking for quantum-computing resistant 
2-alt: NSA has a couple of guys who have a bee in their bonnet about quantum.
3-alt: One or more of these guys are in influential decision-making positions.
4-alt: 2-even-more-alt: The aliens the NSA keeps in the basement have told them that
                 the other aliens who got away have quantum computers.
3-even-more-alt: The truth is that we really have no idea.  Since the NSA never makes any of
its procedures or reasoning public, it could be absolutely anything.  The
principle of least surprise says it's 2-alt (based on a combination of the
unlikelihood of the NSA knowing something about QC that the entire rest of the
world doesn't, and having worked with government departments and seen how a
single manager with a fixation on something can skew how its treated), but in
practice who knows...
Too true, unfortunately.

@_date: 2015-09-03 05:53:58
@_author: Peter Gutmann 
@_subject: [Cryptography] Checking for the inadvertent use of test keys 
Several people have suggested, on-list and off, checking against common known-
bad strings and/or patterns.  Unfortunately this gets back to the epicycles
approach I mentioned in my original message, you need a priori knowledge of
likely values for test keys in order to be able to detect them and warn when a
test key may be being used in production.
One off-list contributor mentioned the following:
  Opening up a philosophical discussion here, but wouldn't that be like trying
  to find the smallest uninteresting number? Being the smallest, it would have
  an interesting property and thus be no longer eligible for the job.
which makes a good point, that it may be an unsolvable problem in general,
that identifying inadvertently-used test keys may require an algorithm
implementing the Potter Stewart "I'll know it when I see it" test.
So for now I've left things at the ad-hoc level, checking for ASCII strings,
strings where value n+1 differs from value n by a small amount, that sort of
thing.  It's not meant to be a bulletproof test, just something to ask the
user "are you sure this is what's meant to be used as a key".

@_date: 2015-09-04 00:43:37
@_author: Peter Gutmann 
@_subject: [Cryptography] Introducing the phone-directory certificate 
In recent years, certificates have appeared containing hundreds of (often
unrelated) domain names.  These have in the past been called Sybil
certificates, however for the one that Google is currently serving up (at
least that's visible in NZ) I'd like to propose a new name, the phone-
directory certificate.  It's reassuring to note that a single certificate is
capable of reliably vouching for, well, half the planet.

@_date: 2015-09-13 10:14:49
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: millions of Ashley Madison bcrypt 
dating site, the number of (obviously) fake profiles is actually quite small
because they very actively police the site for scammers.  "Obviously fake"
means people who sign up, create a fake profile, and then start contacting
other members to take them off-site.  The profiles typically have impossible
(or at least unlikely) combinations of attributes (location, profession, age,
and so on), or fit a certain profile (from memory firemen and... some similar
profession were popular for attracting female victims, for guys I think it was
just anything with a pulse :-), they're created at unusual times, and so on.
Not-so-obviously fake profiles were compromised accounts where the scammers
invested a lot of time spear-phishing individuals.  These were hard to catch,
but also yielded a low rate of return for the scammer.
In terms of the most significant demographic for non-active accounts (which
would be the closest equivalent to a fake profile on A-M), that was people who
signed up, set up a minimal profile to allow them to have a look around
(anecdotally, alongside general tire-kickers, many of them were people keeping
tabs on exes and things), and then never came back.  He wasn't terribly
forthcoming with figures for this, but reading between the lines it seemed the
actual figure for this type of account was "a lot".

@_date: 2015-09-19 03:51:12
@_author: Peter Gutmann 
@_subject: [Cryptography] The default password of '1234' 
I've run into that a fair bit in smaller motels where the Wifi has been set up
by the owner's nephew or a guy he knows from across the road.  I usually write
them a short note on how to disable admin page access from the WLAN and how to
change the default password, and get them to pass it on to whoever set the
thing up...

@_date: 2015-09-19 05:50:05
@_author: Peter Gutmann 
@_subject: [Cryptography] Microsoft's new, free, 
This is Microsoft Research, not Microsoft corporate.  Like other academic
groups, they often make their work publicly available rather than keeping it
proprietary.  At least one reason why they can do that is because, unlike corporate, open-
sourcing their code won't make them a target for every patent troll from
here to East Texas.

@_date: 2015-09-21 05:12:10
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Feedback welcome on 
No, it's a nice idea, but it's also been around since at least the 1980s.
Google S/Key for starters

@_date: 2015-09-25 13:35:59
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: VW/EPA tests as crypto protocols ? 
Uh, it's not the sensor that's the problem, it's the ECU (not EMU, that's a
large Australian bird) software.  In fact there is an industry that's already
dealt with this, Formula 1 racing.  The problem there is that the ideal F1
software suite would result in a driving experience that consists of pushing a
start button and then sitting back while the software takes the car around the
track, because it's far better at this than a human driver.  So the software
has to be deliberately constrained to not be very smart at all, so the driver
still has something to do.  The way this is done is that it goes through a
strict audit procedure to make sure that it only has the minimal functionality
required.  It's then digitally signed, and only the signed software can load
and run on the ECU.  (There may also be additional controls beyond that point,
but I wasn't told much about that, all I was worried about was the crypto
This works in the F1 environment, but is completely impractical for standard
road vehicles.
Overall, this is not a problem that can be solved by technology, you need
audits, proper testing, and legal measures to deal with it.

@_date: 2015-09-25 13:44:56
@_author: Peter Gutmann 
@_subject: [Cryptography] VW/EPA tests as crypto protocols ? 
There's an entire industry doing hot chips/superchips/upgrade chips for ECUs.
They're anything from dodgy-as-hell (e.g. advancing the ignition timing at the
risk of wrecking your engine) through to straighforward stuff like running the
fuel mixture richer (less fuel efficiency, more pollution, but more power),
and whatever else the modders can dream up.  I've been in a car with someone
who had an SH-4 controller set up to modify the ECU operations on the fly, it
was an interesting experience going down a (carefully chosen, empty) country
road with him messing with the engine operations as we were driving.

@_date: 2015-09-28 09:26:44
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Brainpool Curves Found to Be Suspicious 
It's not just those curves, it's hard to find other crypto parameters that
have been validated independently.  In March 2014 I asked whether there were
any independent evaluations of the widely-used DH primes from RFC 2409 and RFC
3526 (my code verifies the integrity of the values, but then I realised that I
didn't know whether the published values in the RFC were correct, or at least
matched the derivation parameters given there).  Cryptographer Henrick
Hellstrm was kind enough to check the values at that point, but I couldn't
find any evidence of any independent verification before then (and RFC 2409
was published seventeen years ago).
... nor do they audit crypto source code (unless they're paid to or ordered
to), because the assumption is always that someone else has done it for them,
the Bystander Effect taken to extremes since the diffusion of responsibility
has been scaled out to the population of the Internet.
Maybe a new design criterion for any crypto parameters should be that, before
a spec can be published, the parameters in it have to be independently derived
using the published derivation technique by three unrelated groups of people,
with their names listed in the final spec.  You don't necessarily need a full-
blown Byzantine agreement mechanism, but you do want a bit more assurance than
the authors saying "here are the values, they should be all right".

@_date: 2016-04-08 06:35:54
@_author: Peter Gutmann 
@_subject: [Cryptography] At what point should people not use TLS? 
All I found was a very informal discussion of it:
Trevor Perrin is someone who knows what he's doing, but still, that's an
incredibly informal description to have to evaluate the design by.  It's also
just another STS-style design, they're not hard to invent but pretty hard to
get all the details right, which the informal nature of the spec doesn't help

@_date: 2016-04-12 11:13:29
@_author: Peter Gutmann 
@_subject: [Cryptography] Is storing a hash of a private key a security risk? 
For private keys stored in a secure enclave in an embedded environment,
there's some concern that over long periods of time the key components may be
subject to bit rot/corruption.  One possible solution is to store a truncated
hash of the private key components outside the enclave and use that to verify
that nothing has changed, by asking the enclave whether the stored key
corresponds to the given hash value.  Without going into a huge amount of
detail on the specific application, it has to be a hash, it can't be a MAC or
something similar, and the hash can't be stored inside the enclave because it
only stores key components (it's a hardware constraint).
This is something that, on the one hand, seems to be not a good idea, but on
the other hand I can't see any obvious problem that it would cause (you can
use it to find out whether a given key is stored inside the enclave, but you
can do that anyway without the hash because you need to identify the key to
query the enclave whether the hash matches).
Can anyone see any problems with this?  Or suggest an alternative approach
that doesn't violate the constraints imposed by the hardware?

@_date: 2016-04-12 20:23:28
@_author: Peter Gutmann 
@_subject: [Cryptography] Is storing a hash of a private key a security 
That was the means of last resort, I was trying to avoid that because I'm not
sure whether there aren't some sort of controls on signing, i.e. whether you
can just perform a signing op without user interaction/authorisation.  Also
creating a capability to perform an arbitrary signature on something may not
be a good idea, an attacker may be able to leverage it to sign something of
value rather than just test data or an empty string (the hardware doesn't know
what it signs, it just gets a command "perform a sig.operation").

@_date: 2016-04-12 20:36:02
@_author: Peter Gutmann 
@_subject: [Cryptography] Is storing a hash of a private key a security 
That is the one concern with it, you can now tell whether the key stored in
HSM-like-device  is the same as the one stored in HSM-like-device However since you can probably do that anyway by comparing the corresponding
public keys, it may not be such a big deal.  It's just one of those things that seems unsound, although I can't quite
elucidate why.
I'll have to check the hardware for whether it's possible to mix a salt into
the hash before it's run over the key components, but I think it's not
possible, the enclave just gives a yes/no response for a given hash but no

@_date: 2016-08-07 09:30:39
@_author: Peter Gutmann 
@_subject: [Cryptography] Generating random values in a particular range 
That's the exact method I used, long before NIST made their recommendation
(just in case anyone needs prior art for a patent, this would have been about
1995).  In fact I've never understood why you'd use the alternative Rube-
Goldberg approach, at best you can save a mod q operation but then you've got
a bunch of hashing and nondeterministic behaviour, and doing it on a 160+delta
bit value is pretty cheap anyway.  So the easiest way to avoid the patent would
seem to be to do it the straightforward way.

@_date: 2016-08-09 09:00:05
@_author: Peter Gutmann 
@_subject: [Cryptography] BBC to deploy detection vans to snoop on 
It's true!  They're using modified cat detector vans from the Ministry of
Housinge.  A friend of mine was visited because of his WLAN, Eric the WiFi,
but he managed to fob them off with a dog licence with the word 'dog' crossed
out and 'WiFi' written in crayon.

@_date: 2016-08-10 11:06:57
@_author: Peter Gutmann 
@_subject: [Cryptography] Public-key auth as envisaged by first-year science 
I've been helping out with a student session where a group of first-year
science students get exposed to various interesting things outside their usual
field of study.  This year, one of the exercises we set was to give them an
overview of DH via the paint-mixing analogy, explain what a MITM was, and ask
them to come up with a means of dealing with it.  This is what they came up
Location-limited channels: Detect how long it takes for messages to go back
  and forth, the MITM will introduce delays.
OOB auth/Multipath messaging: This group was having some difficulties ("we're
  biologists", to which my response was something like "great, your minds
  haven't been polluted by 30 years of doing the wrong thing over and over",
  I was thinking of the "obvious" solution of suggesting PKI here), their
  solution was "Send an owl".  Perfect, now develop it.  So the end result was
  to send a block of 50 owls on the assumption that all of them couldn't be
  intercepted by an attacker.
OOB/Multipath  Send multiple pigeons trained to eat different colours of
  M&Ms (this was based on the explanation of DH using paint mixing).
TTPs: Use a Skype call in which you hold up your passport to prove your
  identity.  You have to trust that Skype isn't controlled by the attacker as
  well, but that's a pretty big step up from MITMing your Internet connection.
TTP  Use Instagram, with the authenticator, a photo, deleted after it's
  used.
Multipath over the Internet: This one was from the computer scientists, route
  the comms over multiple channels on the assumption that the attacker can't
  control every path (which is what things like Perspectives do).
Remember, all this was from students who had been in high school the previous
year, and most of whom weren't computer scientists.

@_date: 2016-08-24 01:57:27
@_author: Peter Gutmann 
@_subject: [Cryptography] "NSA-linked Cisco exploit poses bigger threat 
Ugh.  Do some reading on coding forums about what you're getting yourself
into, and see your doctor to make sure your shots for Amiga Persecution
Syndrome [0] are current.
[0] "We have this absolutely perfect piece of technology but no-one else will

@_date: 2016-08-24 02:08:48
@_author: Peter Gutmann 
@_subject: [Cryptography] "NSA-linked Cisco exploit poses bigger 
There's also the other problem, inspired by Ed Post's comment that "the
determined Real Programmer can write FORTRAN programs in any language".  You
can write insecure code in any language, it's just that C is everywhere, and
in particular in mission-critical areas, so the problems are more visible.
Look at Java for example, no buffer overflows and no pointers so it's got to
be totally secure.  No-one has ever found an exploit involving Java, have

@_date: 2016-08-24 08:02:39
@_author: Peter Gutmann 
@_subject: [Cryptography] "NSA-linked Cisco exploit poses bigger threat 
You mistyped "1997" (I've seen "hack like its 199x" in several talks on
embedded/SCADA/IoT insecurities), but apart from that you're spot on.

@_date: 2016-08-31 06:27:47
@_author: Peter Gutmann 
@_subject: [Cryptography] Strength of 3DES? 
For those not used to imperial units, a buttload is somewhere between an
(obsolete) arseload and a metric s**tload.  Not to be confused with a f**kton,
a unit used for dry goods, household items, and your wife/girlfriend's clothes

@_date: 2016-12-02 11:24:45
@_author: Peter Gutmann 
@_subject: [Cryptography] OpenSSL and random 
As others have pointed out, it's not the right thing, because it turns
getrandom() into make_application_crash_at_random() (if your application hangs
then to users it's crashed, no matter what you may want to call it).  Would
you want to ship a product to customers that calls
Why 128 bits?  What if it's less?  Lets say you never block, which means you
could run on, say, 32 bits of entropy early on.  Given the information
"somewhere on the Internet there may be a system that may be running with
lowered entropy in the RNG", how would an attacker exploit this?
(What I'm pointing out there is that it seems to be an unassailable article of
faith that your PRNG needs to have 128 bits of entropy as defined by
$something in order to work.  Why?  What's the threat if some random system
somewhere on earth has 127 bits?  80 bits?  56 bits?  32 bits?).

@_date: 2016-12-04 07:19:04
@_author: Peter Gutmann 
@_subject: [Cryptography] OpenSSL and random 
That's a good point, but it's conflating entropy with randomisation.  To get
per-device unique keys, you don't need strong entropy, just a per-device
unique value to make sure you don't get repeats.  In fact, here's a magic
trick: A secure SSH key without needing any entropy! [0]
  seed = HMAC( fixed_secret, time() || MAC address || IP address || kernel version || ... );
Yup, that's a good idea.  In fact I implement exactly that in my code, there's
a nonce RNG and a crypto RNG, and in most cases the nonce RNG gets called
because that's all that's needed.
Only downside is that you then need to educate users to only call the crypto
RNG when required.  This may be tricky, even crypto standards get this wrong.
For example every version of the SSL/TLS spec has required that the
client/server nonce be generated from a crypto RNG when what's needed is a
generic nonce RNG.  In fact using a crypto RNG is actively harmful since it
exposes your a considerable amount of your crypto RNG output (think EC-DRBG)
directly to an attacker.
[0] Since entropy is defined relative to a source model, you can in theory
    claim almost anything has an entropy level ranging from zero to infinity.
    I'll avoid semantic tricks like that and define "entropy source" to mean
    some sort of strong physical randomness source, not necessarily
    radioactive decay or equivalent but things like packet jitter, interrupt
    timing skew, that sort of thing.

@_date: 2016-12-04 07:32:00
@_author: Peter Gutmann 
@_subject: [Cryptography] TV set power correlates to TV channel? 
Having read papers in the past on power disaggregation, I would say it's going
to be quite tricky.  Sorting out whether you're running your kettle at the
same time as your microwave is one thing (very different power characteristics
overlaid), but determining precisely what your TV is doing is an entirely
differnt kettle (of fish, this time).  This stuff is blind signal demodulation, starting with a fluctuating power
line signal you need to sort out that this bit there is probably a microwave
and that bit over there is most likely a kettle (some are very easy, for
example I can tell just by looking at the power monitoring graph whether the
hot water, fridge, and/or heatpump are running, they have very distinctive
line signals), but for TV programs you'd probably need far more precise
knowledge of the TV model, firmware rev, and details of any other devices with
SMPS that'll be injecting noise and causing fluctuations at the same time.
It's a matter of what's easiest.  The MIB can hack your smart meter and
covertly replace it with a far more powerful piece of gear that has what it
takes to sort out which TV program you're watching based on OLED light-pattern
power consumption, or they can point a $5 webcam at your window and record it
straight from the OLED source.
Yup, exactly.

@_date: 2016-12-05 02:14:46
@_author: Peter Gutmann 
@_subject: [Cryptography] OpenSSL and random 
Oh, I wasn't intending it for that use, my interest was SCADA/embedded,
devices that are notoriously short of entropy.  So you have a per-device
unique value (MAC address) and varying value (IP address or time) to ensure
that you get unique keys per device, and if you recreate the keys you get
different ones each time.  That's also why it used very low-level information
rather than kernel stats, routing info, network statistics, and so on - there
won't be any.
Random number generation is very situation-specific.  In this case you know
that the attackers don't have physical access, are unlikely to get remote
access (the devices are typically running a custom RTOS, there's not much to
attack and even if you can find a vuln, it's quite hard to exploit since
there's no room for anything but the RTOS in memory), and the manufacturer
controls the fixed secret.

@_date: 2016-12-06 02:36:59
@_author: Peter Gutmann 
@_subject: [Cryptography] OpenSSL and random 
Combining replies to conserve bandwidth (incidentally, is it about time to
wind this up, and we'll reconvene in another six months as usual?):
Sure, lack of any kind of RTC was anticipated, which is why I used multiple
sources.  It's not really critical, as long as you have one per-device unique
value mixed in you'll get distinct keys for each device which is the important
bit, adding something that varies over time will give a different key on a
regen but it's not fatal if you don't get that.
Right, but it's not being used as a secret value, merely a diversifier.  As
long as you don't get multiple devices with the same MAC address you're fine.
I just see TCP over something, but not what it is.  Conveyor-belt static,
possibly (an actual event, turns out that 40kV of ESD and SCADA don't mix).
But they're either trained process control engineers or IT admins, not
security people.  They use phrases like "perfectly ordinary sodium iodide
gamma ray spectrometer" (another actual event), but not "perfectly ordinary DH
key exchange".  So successful rollout of a SCADA system doesn't mean secure
rollout of a SCADA system.
That's the killer with lots of these devices, you've got great sources of
entropy (see "gamma ray spectrometer" above, radioactive decay as the
definitive randomness source), but you can't get to any of it because it's
handled by a separate subsystem.  There's lots of cool parts in there, but
they don't talk to each other because they're all done by different groups. In
particular, the security part has a very low priority compared to the various
make-sure-things-don't-fail parts, so asking for access to the components
involved in making sure things don't fail invokes a no-by-default response.

@_date: 2016-12-08 02:12:49
@_author: Peter Gutmann 
@_subject: [Cryptography] Is glibc right on randomness 
John Gilmore  quotes:
I think that what he's saying there is that everyone has agreed that this new policy is really an excellent plan. But in view of some of the doubts being expressed, may he propose that he recall that after careful consideration, the considered view of the glibc maintainers was that, while they considered that the proposal met with broad approval in principle, that some of the principles were sufficiently fundamental in principle, and some of the considerations so complex and finely balanced in practice that in principle it was proposed that the sensible and prudent practice would be to submit the proposal for more detailed consideration, laying stress on the essential continuity of the new proposal with existing principles, the principle of the principal arguments which the proposal proposes and propounds for their approval. In principle.
(joseph at codesourcery is a rank amateur compared to Sir Humphrey).

@_date: 2016-12-08 02:26:59
@_author: Peter Gutmann 
@_subject: [Cryptography] Anyone else seeing an uptick in infected 
Ugh, as soon as I saw the title of that first one I thought "TR-069", and sure enough:
"incoming traffic destined for Port 7547 using communications protocols  known as TR-069"
If ever there was a protocol was designed to be insecure, it's that.  And
what's worse is that in many SOHO-level routers it's impossible to disable, since it's used by ISPs and vendors and everyone between Murmansk and Yakutsk with an Internet connection to provision the routers.
For anyone not familiar with this horror, see

@_date: 2016-12-14 04:26:27
@_author: Peter Gutmann 
@_subject: [Cryptography] 5 Questions to Ask your IoT Vendors; 
Forwarded by a friend:
Very apropos given the IoS mess, and ones that virtually no IoS vendor would
be able to answer.  In most cases the answers for 1-4 would be "whut?" and for 5 it'd be "military-strength AES" or something similar.

@_date: 2016-12-19 10:45:50
@_author: Peter Gutmann 
@_subject: [Cryptography] Photojournalists & filmmakers want cameras to 
These are the same vendors who believe that a single private key baked into
publicly-downloadable firmware images that are shared across all cameras is a
good idea, or that HMAC (with a similarly shared key) is a perfectly good
"signature" function?
They're probably still waiting for their rush order of Cryptography For
Dummies (Chey Cobb, 2004) to turn up from Amazon before they come up with a

@_date: 2016-12-19 11:13:43
@_author: Peter Gutmann 
@_subject: [Cryptography] Photojournalists & filmmakers want cameras to be 
This is just asking for:
If a camera vendor could produce a strongly-encrypted camera (which they
can't, see my previous message), it would make journalists considerably less
safe.  Previously, you seized the camera, perhaps roughed the journalist up a
bit, and then sent them on their way.  Now you have to seize the journalist
and torture them, even if the camera appears to have nothing on it, since they
may confess under torture that there is something hidden/encrypted there.

@_date: 2016-12-20 08:59:45
@_author: Peter Gutmann 
@_subject: [Cryptography] TR-069 & firewalls 
That depends on the manufacturer, model, and firmware revision.  In general
it's really hard to turn off TR-069 because ISPs want to use it to configure
"their" routers, so there's often an implied exception for TR-069 to make sure
that customers can't configure the device to block ISP updates/management.

@_date: 2016-12-23 00:24:13
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED]  USB hardware token for $2?? 
Anything (with a CPU) can be used as a low-cost HSM, the problem isn't the
hardware, it's the software. Take any random ARM-based device (or Atmel, or
MSP430 if you're a masochist) and turn it into an HSM, all the work is in the
software, not the hardware.
It always amuses and/or depresses me to see yet another ARM board on Tindie or
Kickstarter or Indiegogo or whatever, "the world's first/smallest/most
cromulent ARM XYZ" [0], which is exactly the same as every other ARM XYZ
except that it has no software for it.
To generalise this, the problem with almost any consumer-level hardware device
isn't the hardware, it's the software.  Hardware is easy, buy it from your
favourite crapvendor, get a locally-made product, clone the manufacturer's
reference design if you really want to DIY, it doesn't matter.  Software, OTOH, is hard.
[0] Everything on Kickstarter has to be at least one, possibly more of, the
    world's first, smallest, or thinnest.  No idea why, it just is.

@_date: 2016-02-03 02:41:55
@_author: Peter Gutmann 
@_subject: [Cryptography] DH non-prime kills "socat" command security 
For the years before that, the Linux command "socat" has been assuming that a
512-bit prime is secure; thus breaking its crypto security.
They also do things like tell you how to set up the SSL tunnel without any
mention of validating certs so it's unlikely they check those, and various
other signs that they're not doing crypto very well.

@_date: 2016-02-06 02:38:21
@_author: Peter Gutmann 
@_subject: [Cryptography] DH non-prime kills "socat" command security 
I doubt it.  I surmise that possibly Google was used to find it, and then you
cut & paste what's in the first search result on Stackexchange.
(As Leif Johansson added in commentary:
   It really is that bad, the amount of crappy code that gets propagated through
there is scary: people have a problem they can't easily solve, Google leads
them to Stackexchange where others have had the same problem, they grab the
code there, done.
Alternatively, it could have been 128 bytes from /dev/random.  "FIPS 140 needs
a random number of size X, this is a random number of size X":
(The response to the above from someone on another mailing list was an
  I think my soul just got a few more scars and dark spots).
In any case though people still seem to be missing the big picture, that
despite the non-prime being just that, it was still a lot more secure than the
512-bit value that it replaced.  The 1024-bit value has as its largest factor
a 1002-bit value that, while also being non-prime, isn't easily factorisable.
That's still better than the 512-bit (alleged) prime that it replaced:
(Has anyone checked the earlier value?  I don't have PARI/GP on here so I
can't check it right now).
So it's quite possible that moving to the 1024-bit non-prime was an increase
in security over the previous state of the code.

@_date: 2016-02-06 07:50:05
@_author: Peter Gutmann 
@_subject: [Cryptography] Basic auth a bit too basic 
Someone just pointed out an interesting problem with HTTP basic auth,
published in 1999 as RFC 2617 and updated 15 years later as RFC 7617: It's an
HTTP version of Hotel California, you can log in but you can never leave
(Stackoverflow has various hacks to deal with this,
most of them pretty hairy and not very portable).
Perhaps in 2030 when RFC 13,617 comes out, it could include some form of HTTP
extended auth that also allows you to log out.
And a general note to people designing auth protocols: You probably want to
include a mechanism to get the client to stop authenticating to the other side
as well.

@_date: 2016-02-08 09:21:09
@_author: Peter Gutmann 
@_subject: [Cryptography] Basic auth a bit too basic 
That doesn't really help though because it doesn't provide a means for the
site and the client to agree to end the authenticated session.  That's what a
lot of the hacks on Stackexchange try and do, but they remain just... hacks.

@_date: 2016-02-15 02:19:26
@_author: Peter Gutmann 
@_subject: [Cryptography] XOR linked list & crypto 
How would you use them in an encryption scheme?  Given that the pointers will
have very low entropy and in some cases be entirely predictable (malloc'd
entries in a linked list of fixed size), you'd get pretty poor security.
You'd have to have a random IV at the start, but then you're mostly reusing it
on each link pointer due to the near-zero entropy contributed by each link, so
it wouldn't offer much security.

@_date: 2016-02-18 04:28:06
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Hope Apple Fights This! 
You don't even need to do that, just use a glitch attack a la smart card
hackers 20+ years ago.  Unless the checking code is something like:
  result = constant-time-memcmp( entered-PIN, stored-PIN );
  decrement counter;
  if result = FALSE goto retry;
  increment counter;
which it probably won't be, it'll be more like:
  if( !memcmp( entered-PIN, stored-PIN ) )
    decrement counter;
    goto retry;
then all you need to do is reset the CPU before the counter is decremented, or
just walk down the memcmp() with a timing attack.  It helps if you can
underclock the CPU, which seems to be do-able on most CPUs in devices like

@_date: 2016-02-19 03:03:08
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Hope Apple Fights This! 
It depends on what you mean by "destructive", they need to gain physical
access to the circuitry, and possibly cut a trace to the crystal used for
clocking, but that's about it.  The inject-backdoored-firmware approach the
FBI have requested is far more destructive.

@_date: 2016-02-23 05:47:01
@_author: Peter Gutmann 
@_subject: [Cryptography] eliminating manufacturer's ability to 
According to the Wassenaar control lists, about half the phone, if you take it
across a border without an export license.
Also most laptops, desktop PCs, home video games, and other similar items.

@_date: 2016-02-24 00:51:21
@_author: Peter Gutmann 
@_subject: [Cryptography] eliminating manufacturer's ability to backdoor 
The crypto is irrelevant, it's all the other tech in there (hardware and
software) that's the problem, or at least that would be the problem if the
controls were enforced as written.
(They seem to have removed a large amount of the more obvious stupid that used
to be in there, frequency-hopping/spread-spectrum radio, SDR, DSPs, A/D and
D/A converters, it was a long list, but there's still quite a bit left).

@_date: 2016-02-29 15:25:10
@_author: Peter Gutmann 
@_subject: [Cryptography] Damned if you don't, even more damned if you do 
In the last few years there have been a number of papers pointing out that far
too many Android apps will blindly trust any cert they run into.  The solution
to this is to install your own trust anchor on the device so you can build a
proper trust chain that you control, and verify that the certs you encounter
chain up to a root that you trust.
That's the theory.  If you do do this then you find out that Google has
sabotaged your efforts by popping up a warning that "a third party is capable
of monitoring your network activity including emails, apps, and secure
(thanks to Jrgen Brauckmann for the link, and the one below).
So if your app blindly trusts any cert it encounters, including ones from
MITMs, Android/Google are quite happy with that.  If you want to run your own
private PKI with verification of trust chains to ensure proper security,
Google's software pops up scary warnings about your comms being intercepted.
This is even worse than the reverse security offered by browsers where
unencrypted web sites are treated as more secure than encrypted-via-a-non-
public-CA ones.  In this case the MITM-able connection is treated as secure
while the (hopefully) MITM-proof one is treated as insecure.
But wait, there's more!  Android also includes an API in Android 4.3+ for apps
to silently install their own CA certs (e.g. malware slipping in a rogue CA)
without triggering any warnings.  OTOH if the user manually installs a
carefully-selected CA cert of their own choice, they get eternal warnings
about it being unsafe:
I think one of the contributors to the thread sums it up best: "this is

@_date: 2016-01-01 06:03:43
@_author: Peter Gutmann 
@_subject: [Cryptography] Photon beam splitters for "true" random 
Alternatively, running with hacked firmware that reports whatever size they
want to you pay for and relying on the fact that USB keys are typically used
at vastly less than their (alleged) capacity.

@_date: 2016-01-01 06:10:02
@_author: Peter Gutmann 
@_subject: [Cryptography] Understanding state can be important. 
Already exists (at least in prototype form), one was described at Kiwicon this
year, "Fear and Loathing on your Desk: BadUSB, and what you should do about
it" by Robert Fisk.  Another one is in development by an embedded device
engineer, but hasn't been publicly announced yet.

@_date: 2016-01-03 02:31:58
@_author: Peter Gutmann 
@_subject: [Cryptography] Alice, Bob, Eve, Nessa, Twein ?? 
Another use I've seen is Anne, full name "Anne Essay".  I like Vanessa though,
it's just obvious enough who'd being referred to.
Unfortunately there doesn't seem to be a name with more than two i's in it.

@_date: 2016-01-03 02:34:38
@_author: Peter Gutmann 
@_subject: [Cryptography] Alice, Bob, Eve, Mallory, Maxwell ??? 
Vidkun, or is that too obscure?  OTOH there's little chance of it clashing
with something else, although there's already a Victor = verifier.

@_date: 2016-01-04 02:31:15
@_author: Peter Gutmann 
@_subject: [Cryptography] Understanding state can be important. 
That's their enterprise-features version, filled with junk they throw in for
goldilocks pricing purposes, if you don't want all that then don't buy the
premium version.

@_date: 2016-01-04 02:43:10
@_author: Peter Gutmann 
@_subject: [Cryptography] How can you enter a 256-bit key in 12 decimal 
That's because you're not the target market.  Vendors started adding AES
engines to the data paths of disk controllers some years ago, and in some
cases it's done by default (many Intel SSDs, for example).  The problem is
that pushing the data through an AES engine 128 bits at a time is a long way
removed from a full encryption solution, so what you typically get is AES in
ECB mode and the most basic key management you can implement, which means you
can advertise 256-bit AES (or whatever) without having to do much except
license the AES IP core at $0.001 per unit.
If you want to implement the TCG's OPAL standard for key management... well,
have a look at the spec, figure out what it'd take to implement, and then
figure out the chances of any vendor being able to do all that, as well as the
software on the host side to talk to it (which has to run at ring 0), without
leaving a whole string of 0days in their implementation.

@_date: 2016-01-08 04:01:16
@_author: Peter Gutmann 
@_subject: [Cryptography] Chaum Has a Plan to End the Crypto War 
Paul Ferguson  quotes:
The counterargument to that would be, somewhat unfortunately, "don't worry,
this guy has a multi-decade track record of proposing fancy theoretical stuff
that nobody can make work in practice.  This one is no different".
If Bernstein did it, I'd be more worried :-).

@_date: 2016-01-08 09:09:41
@_author: Peter Gutmann 
@_subject: [Cryptography] Verisimilitrust 
Kazakhstan requesting that their MITM certificate be added to the browser
trust lists:
  It would appear from this information, that this CA (and probably others
  like it) is deliberately serving a dual role:
  1. It is the legitimate trust anchor for some domains that browser
     users will need to access (in this case: Kazakh government sites
     under gov.kz).
  2. It is the trust anchor for fake MITM certificates used to harm
     browser users, and which should thus be regarded as invalid.
causing an immediate panicked response to try and find a reason to deny the
request, because the CA/Browser Forum policies don't actually say you can't
have an acknowledged MITM cert as a trusted root:
  Kazakhstan has submitted the request for root inclusion:
    So, we really do need to have this discussion now.
I think we need to formally give up on the use of the word "trust" in its
conventional sense in relation to PKI.  Browser PKI has done to the term
"trust" what the popular press has done to the word "hacker".
  Thus it would be prudent to extend the trust list format (and the NSS code
  using it) to be able to specify additional restrictions beyond those
  specified in the CA root itself.
  [...]
In other words certificates are going to be turned inside-out, instead of the
cert encoding policy-related information as per X.509, we've got a third party
(browser vendors) imposing its policy on the certificate from the outside.
We've already got the same third party overriding CAs on revocation via
hardcoded cert blacklists, and as has been shown over and over again, CAs do
only the bare minimum of checking for anything but EV certs.  So if this
change is made we can summarise the purpose of a CA as follows:
  Verify identity in certs - Not really (except to justify premium-priced EVs).
  Provide policy for certs - No, the browser vendor will.
  Provide revocation info for certs - No, the browser vendor will.
  Charge money to turn off the browser warnings - Yes.
So that's pretty much pared browser PKI down to its essence, a license to
print money for a select group of companies.

@_date: 2016-01-11 22:49:30
@_author: Peter Gutmann 
@_subject: [Cryptography] Verisimilitrust 
Another problem with browser PKI, or more specifically browser TLS, is that
they've made it impossible to use encryption without involving a CA.  TLS
doesn't require the use of certificates, and in fact offers a number of
encryption modes that are much, much more secure than the CA-based ones (e.g.
ones that provide true mutual auth of client and server), but the browser
vendors have chosen to disallow all of those modes.  The only modes that are
allowed are ones where you have to ask a CA for permission to encrypt.
This is now coming back to bite them with the Kazakh MITM CA ("Borat"), the
only option they have is to either allow the MITM CA, or turn off all
encryption to every site on the planet (note that this won't prevent
connections to TLS-only sites since Borat will just run an SSLstrip proxy).
There's no way to do non-CA-controlled encryption if you're using a web
browser, which is perfect for Borat.

@_date: 2016-01-11 22:51:56
@_author: Peter Gutmann 
@_subject: [Cryptography] Verisimilitrust 
In around 2000 during the peak of the PKI mania, a frequently-heard claim was:
  "e-commerce needs PKI in order to function"
Carl Ellison turned it around to express the real situation:
  "PKI needs e-commerce in order to function"

@_date: 2016-01-11 23:16:44
@_author: Peter Gutmann 
@_subject: [Cryptography] Verisimilitrust 
Hmm, the page you link to (which is one of the sources I used for my post)
  Online (i.e. OCSP and CRL) checks are not, generally, performed by Chrome.   CRLSets (background) are primarily a means by which Chrome can quickly block
  certificates in emergency situations. As a secondary function they can also
  contain some number of non-emergency revocations.
That does look an awful lot like it's the browser vendor providing revocation
info for certs, not the CA.  And it's the same with other browsers, the CA
publishes a CRL that's ignored by browsers, and if any critical revocation
(i.e. one that users actually care about, rogue CA certs, that sort of thing)
happens, it's handled by the browser vendors pushing out an
update/blacklist/whatever, not by a CA.

@_date: 2016-01-11 23:19:48
@_author: Peter Gutmann 
@_subject: [Cryptography] OpenSSL minimal "safe" configuration? 
MD5 is required for TLS before 1.2 (it uses a dual-hash, MD5 || SHA1), and DES
is required for 3DES.  Getting rid of those will disable a whole pile of
crypto functionality.
So let me get this straight, AES-128 isn't safe enough for you, but OpenSSL
Perhaps the first step in getting to your goal might be to buy a large roll of
Peter :-).

@_date: 2016-01-12 09:32:29
@_author: Peter Gutmann 
@_subject: [Cryptography] OpenSSL minimal "safe" configuration? 
What do you mean by "use multiple cores"?  I would assume all of them do by
default, in that they can be scheduled across any number of cores.  If you
mean "does the SHA-1 implementation of library X use all cores" then the
answer is that none of them do, because SHA-1, and indeed most crypto, isn't
parallelisable outside of some special-case modes like CTR/GCM.
Even then, what problem are you trying to solve that requires more than, say,
50Gbps throughput (Intel's figures for AES on a 16-core system)?

@_date: 2016-01-12 09:57:11
@_author: Peter Gutmann 
@_subject: [Cryptography] Verisimilitrust 
Which doesn't change in any way what I said:
  Provide revocation info for certs - No, the browser vendor will.
The browser vendor is providing revocation info via the CRLSet.  The CA's
revocation information, CRLs and OCSP, are ignored ("Online (i.e. OCSP and
CRL) checks are not, generally, performed by Chrome").  The browser is relying
on the browser vendor, not the CA, to obtain its revocation information.
I don't know how many more ways I can rephrase this, perhaps the following
will help:
  Browser downloads CRL from CA and checks it -> CA "provides revocation info
  for certs"
  Browser downloads CRLSet from browser vendor and checks it -> Browser vendor
  "provides revocation info for certs".
In this case it's clearly the browser vendor providing the revocation
information.  To put it another way, if the CA goes away, the revocation info
still gets to the browser.  If the browser vendor goes away, the revocation
info doesn't get to the browser any more ("CRL/OCSP checks are not

@_date: 2016-01-20 23:56:00
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: TRNG related review: rngd and 
Only if the attacker is pretty incompetent.
Once an attacker has had physical access and/or root, all bets are off.

@_date: 2016-07-04 10:09:34
@_author: Peter Gutmann 
@_subject: [Cryptography] Android Full Disk Encryption Broken - 
I think a more direct lesson here is that taking a security mechanism that
consists of a bit flag used to tag a block of memory, defining any such tagged
area as secure by executive fiat, and selling it as TrustZone, is no match
for, you know, actually doing real security.  It's not like this hasn't
happened before, in 2013 Motorola cellphones got 0wned via attackers targeting
the insecure TrustZone and attacking from inside that out to the (apparently)
quite secure non-TrustZone code.

@_date: 2016-07-04 10:13:40
@_author: Peter Gutmann 
@_subject: [Cryptography] Android Full Disk Encryption Broken - Extracting 
... which is exactly what was exploited in the 2013 attack, alongside a whole
boatload of other missing defensive features, no DEP, no ASLR, executable
stack, strcpy()s all over the place, it was described at the time as a "hack
like it's 1999" attack.  As I said in the previous post, security is more than
just a fancy name and a lot of marketing, you have to actually make an effort
to make it secure.
Oh, and given that this looks like a repeat of the same flaws from three years
ago, patching your insecure code also helps.

@_date: 2016-07-14 04:56:24
@_author: Peter Gutmann 
@_subject: [Cryptography] The Laws (was the principles) of secure 
Nice.  Some changes I'd make to make things a bit more succinct:
"A certificate vending machine".
"Someone else's computer".
?.  I'd use "a technology for treating the legitimate user as the bad guy".

@_date: 2016-07-17 09:57:14
@_author: Peter Gutmann 
@_subject: [Cryptography] The Laws (was the principles) of secure 
Most people, I think, don't even know what a rootkit is, so why should they
care about it?

@_date: 2016-07-20 03:10:09
@_author: Peter Gutmann 
@_subject: [Cryptography] The Laws (was the principles) of 
I think it definitely needs to stay in its original form, a huge amount of
stuff that's secure by executive fiat is sold as "trusted" (PKI, TPMs,
TrustZone, etc), so it's essential that there's a law pointing out what
"trust" really means in this case.
Oh, now you're just being pedantic... :-).

@_date: 2016-07-21 23:39:08
@_author: Peter Gutmann 
@_subject: [Cryptography] The Laws (was the principles) of secure 
A somewhat more cynical form is:
  Metadata is a term hijacked by governments to allow warrantless collection
  of data.  "But there's two different kinds. There's bad asbestos, and   there's nice asbestos. Anyway, it grows on you".

@_date: 2016-06-13 20:05:58
@_author: Peter Gutmann 
@_subject: [Cryptography] Determining TLS session keys from the hypervisor 
Actually I would have thought the defence was pretty trivial: Don't run your
sensitive crypto on hardware controlled by an attacker.  On the one hand the
work they've done is pretty neat, but the overall result is a "well, duh".

@_date: 2016-06-23 06:39:41
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] What to put in a new cryptography course 
* Cryptography is wonderful because it provides clear markers for which parts
of the system to attack.  Look for the crypto as a beacon, then look next to
it to find all the holes and flaws.

@_date: 2016-06-23 06:42:05
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  craig steven wright 
Nothing voids a US patent application, including the fact that what you're
trying to patent has already been patented by someone else.  Unless lots of
publicity forces the USPTO to reject the applications, I don't doubt that
at least of them will be rubberstam^H^H^Hissued.

@_date: 2016-06-29 14:46:13
@_author: Peter Gutmann 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
It's not really a comment either way, more of a food-for-thought article.  We
need usable, non-brittle crypto, and Dan et al are filling that gap while at
the same time few others seem to be doing so.  What's even worse is that some
of the currently most trendy algorithms and modes are also the most brittle,
failure-prone ones (AES-GCM springs immediately to mind).
One of the bits of off-list feedback I had helped illustrated this, someone
commented that they needed an efficient post-quantum-safe hash function.
After a bit of poking around they found one.  Guess who created it?
This also gives me a chance to comment on another bit of feedback I had,
apparently some people have felt that the writeup came across as an attack on
Dan.  It was never intended as such, given that I wrote it I may be a bit
biased but I can't see how it could be interpreted as that.  If it has been
then it wasn't intended that way.
Go not to the security geeks[0] for counsel, ...
[0] Or lawyers, or psychologists, or several other professions.

@_date: 2016-03-04 00:00:30
@_author: Peter Gutmann 
@_subject: [Cryptography] Side channel attack on OpenSSL ECDSA on iOS 
I would agree with them there.  If your threat model is an attacker who's
going to walk up to your hardware and attach sensors to it or stick an antenna
next to it then you need to deal with it via hardware measures (shielding,
decoupling, etc), not try and patch around it with software.  Although coders
like to think that any hardware problem should be fixable via software, in
this case it can't.  If you want EM and whatnot resistance then you need to
design the hardware to deal with this.
For software countermeasures you've got the countermeasure version of Zooko's
Triangle, { EM resistance, timing resistance, performance }, choose any two.
It's even more complex than that, often making something more timing-resistant
makes its EM resistance worse and vice versa, so sometimes it's "choose any
I've got some old crypto hardware lying around here that was designed before
Kocher et al's work was published, and was later subjected to every known
side-channel attack (EM/power/whatever).  Nothing worked.  When I asked one of
the hardware guys about this, his response was "we just designed it using
sound engineering practice".  It hadn't been specifically hardened to resist
EM and whatnot side-channel attacks, it just had a sound level of power supply
decoupling, filtering, and shielding built into the design.  It also probably
added $10-20 to the BOM, which is why no-one else designs like that, it's
cheaper to leave it all out.

@_date: 2016-03-04 01:13:45
@_author: Peter Gutmann 
@_subject: [Cryptography] Side channel attack on OpenSSL ECDSA on iOS and 
You still really need to address it in hardware to do it properly though.  The
risk with a software quick-fix is that any change in the architecture will
negate it or even make it worse, and then you get the false-sense-of-security
rathole where you get to argue endlessly over whether you really should be
advertising it as fixed when what you really mean is "probably mitigated for
this particular stepping of this exact CPU" (with a later side-order of "that
the vendor stopped making three years ago").

@_date: 2016-03-06 05:34:52
@_author: Peter Gutmann 
@_subject: [Cryptography] 
=?windows-1252?q?=22How_the_Feds_Could_Get_Into_iPhones_Without_Apple=92s?=
 =?windows-1252?q?_Help=22?=
That's because if you did, you'd have to figure out how to actually do
something useful with them (which includes "just making them work at all"), as
opposed to the current practice of using them as an excuse for hand-wringing
over who has the most cromulent allegedly-quantum-resistant crypto that
everyone needs to throw their currently-deployed crypto away for and start
It would make a fantastic red herring through, "The NSA has said 'quantum
cryptanalysis' and everyone dutifully jumped, obviously they can use that to
get into the phone.  Your move".

@_date: 2016-03-18 07:31:38
@_author: Peter Gutmann 
@_subject: [Cryptography] Formal Verification (was Re: Trust & 
SAL is probably the best thing we have right now in terms of analysis, in that
while analysers like Coverity and Fortify and Klocwork have more powerful
analysis engines than PREfast, they have to infer things about code behaviour
that are made explicit by SAL annotations.  A Coverity that understands SAL
would be a very powerful analyser indeed.
The one downside with SAL is that you're stuck with what Microsoft give you.
If there's something that your API does but the Windows one doesn't, there's
no way to express it in SAL.  In other words SAL is very non-orthogonal in
places, you can say "this by-value parameter can have the range [0...1000]"
(_In_range_ in SAL terms) but not "this by-reference parameter can have the
range [0...1000]" (_Deref_inout_range_ in SAL terms, it's documented but
doesn't actually exist).  There's also no obvious feedback channel to get
things fixed, you just have to wait for the next version of Visual Studio to
appear and hope something's changed.
SAL can detect some types of lock contention statically, which is impressive.
Or at least it would be if the annotation worked the way it was documented.

@_date: 2016-03-18 23:18:29
@_author: Peter Gutmann 
@_subject: [Cryptography] Formal Verification (was Re: Trust & randomness 
Which leads to a variant of the Gnu problem in which, to build Gnu-anything,
you first need Gnu-everything-else.  I started playing with the CompCert
rabbit hole a few days ago when you first mentioned it, but now I'm waiting
for the proverbial rainy weekend to keep going...

@_date: 2016-03-21 05:59:26
@_author: Peter Gutmann 
@_subject: [Cryptography] This is why we have Stuxnet 
I usually do embedded cross-development under Linux, typically with some
hacked-up ancient version of gcc and obtuse command-line utilities that fail
with cryptic error messages until you've spent several hours hacking around
with them.  This time though I had to use Windows because getting the drivers
going under Linux just wasn't working.  So I go to the web site of the $20B
global hardware vendor that makes this stuff and download their SDK tools.
  "We've detected that you've got A/V running.  You should disable this in
  order to run our tools.  Are you sure you want to continue?".
Yeah, I'm not doing that, so I click continue.
  "I said, WE'VE DETECTED THAT YOU'VE GOT A/V RUNNING AND YOU REALLY NEED TO
  DISABLE IT.  Waiting for A/V to be disabled".
OK, so I'll disable A/V.  At which point Windows goes to about Defcon 2 and
starts screaming about the imminent collapse of civilisation, but I don't have
any choice.
So the install starts, except it won't install in $Program_Files because that
has, you know, security applied to it.  It wants to create its own public
directory off $SystemRoot and install to that.
OK, so I'll allow it to do that.
Now Windows Firewall is throwing up warnings about tclsh groping around on the
Internet (they install a complete Cygwin environment, presumably because their
Windows SDK is all scripted in Tcl).  So I allow that, and various other
things that I get warnings about.
It then proceeds to download and install a 2-year-old version of Java, which
apparently is needed by their SDK.
After that, it reaches out to about a hundred-odd HTTP URLs, downloads binary
blobs from them, and installs them.  I tried setting up a tunnel to an HTTPS
equivalent but it only does HTTP.
Finally, it's finished.  The app starts up and requests elevation to
Administrator.  Then it starts grabbing more binary blobs from HTTP URLs and
installing them.
All that was just from watching what was happening, I didn't do any further
checking to see what other horrors lurked beneath the surface, but given what
I'd seen so far it was bound to be pretty bad.
I think we need to treat any embedded device developed via this vendor as pre-
compromised.  And that includes the aerospace and military ones.

@_date: 2016-03-22 03:26:25
@_author: Peter Gutmann 
@_subject: [Cryptography] This is why we have Stuxnet 
I didn't want To poInt the finger at any particular vendor because many of the
others probably aren't that much better, and in terms of usability the
experience wasn't too bad, the hairy collection of Tcl scripts that runs
everything in the background does actually work and the debug experience isn't
that much more painful than a native build of the code (which is pretty
impressive for embedded).  They'll also sell you eval kits for a few tens of
dollars rather than $1,500 like many vendors do.  So ten out of ten for style,
but minus several million for good thinking.
Another thing with this particular SDK is that the whole thing seems to have
been assembled by people who are primarily hardware engineers (again, very
common in embedded, nice hardware, hacked-together software), so the hardware
is outstanding, really really well thought-out and designed [0], while the
software is held together with duct tape.
[0] And I don't say that lightly.

@_date: 2016-03-23 22:37:47
@_author: Peter Gutmann 
@_subject: [Cryptography] Paris attackers used OTP's: One Time Phones 
I know of at least one legit (meaning beyond-reproach-by-media-hysteria)
reason for the crate-of-cellphones, women's shelters.  There's a group here
that collect old cellphones for use as burner phones for this purpose.

@_date: 2016-03-24 12:41:20
@_author: Peter Gutmann 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
On the Impending Crypto Monoculture
A number of IETF standards groups are currently in the process of applying the
second-system effect to redesigning their crypto protocols.  A major feature
of these changes includes the dropping of traditional encryption algorithms
and mechanisms like RSA, DH, ECDH/ECDSA, SHA-2, and AES, for a completely
different set of mechanisms, including Curve25519 (designed by Dan Bernstein
et al), EdDSA (Bernstein and colleagues), Poly1305 (Bernstein again) and
ChaCha20 (by, you guessed it, Bernstein).
What's more, the reference implementations of these algorithms also come from
Dan Bernstein (again with help from others), leading to a never-before-seen
crypto monoculture in which it's possible that the entire algorithm suite used
by a security protocol, and the entire implementation of that suite, all
originate from one person.
How on earth did it come to this?
The Underlying Problem
This essay came about as the result of a discussion at AsiaCrypt 2015, and was
then developed with significant input from Lucky Green.  Prior to publication,
further input was provided by some of the people whose work is mentioned in

@_date: 2016-03-25 09:58:02
@_author: Peter Gutmann 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
There's very little HW support for this in mobile, which is why Google wants
ChaCha20, because it's efficient in software.  And since Google wants it, it's
going to be mandatory in TLS 1.3.

@_date: 2016-03-25 10:14:48
@_author: Peter Gutmann 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
I'm not advocating anything, it was just some thoughts.  Although there is one
interesting thing that someone pointed out off-list, until now many standards
groups have slavishly followed anything NIST does, to the point where I'm sick
of hearing "what would NIST want?" in discussions on algorithms (I once
responded with "well, that non-backdoored PRNG will have to go for starters").
This, for the first time, is a wholesale rejection of the "anything that comes
from NIST" philosophy, which I haven't seen before.
(I'm not pro- or anti-NIST, but I think algorithms should be chosen based on
their merits, not "whatever NIST says").

@_date: 2016-03-27 10:06:43
@_author: Peter Gutmann 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
TLS 1.3 is heavily influenced by what Google wants, in the same way that HTTP
2.0 is really HTTP4Google.  So the design and algorithm choice is heavily
tilted towards serving up web pages quickly.
Whatever does the above most efficiently.  If you've got a ton of time to
kill, look at the debate around TLS 1.3 in the list archives.

@_date: 2016-03-27 10:14:22
@_author: Peter Gutmann 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
In security terms it's a red herring.  None of the major users of EtM (IPsec,
SSH, TLS, and S/MIME) are subject to this attack, because you can't substitute
a different MAC for an existing one.
No they can't.

@_date: 2016-03-27 10:18:30
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: On the Impending Crypto Monoculture 
That's pretty much what I've done with TLS-LTS, exactly two algorithm groups
chosen from different families (e.g. DH vs. ECDH, RSA vs. ECDSA), a fixed set
of algorithms and parameters, no variants and optionally negotiated parameters
and eight different types of sauces and so on.

@_date: 2016-05-01 20:07:58
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: USB 3.0 authentication: market 
One genuine reason, although it's not clear that the auth achieves it, is to
prevent problems due to cables that lie about their capabilities.  The typical
USB cable is 28 AWG, which can't carry anywhere near the power that USB 3
power delivery is rated for.  So you get a cheap Chinese cable that lies about
its capabilities, which then melts or catches fire when the device attached to
it tries to draw the advertised amount of power.  Or shorts out at full power,
or fries the power source when the device attached to it tries to draw 5x what
it's rated for based on what the cable told it.  Or a zillion other failure
modes induced by something lying about its capabilities.

@_date: 2016-05-25 02:14:31
@_author: Peter Gutmann 
@_subject: [Cryptography] Entropy Needed for SSH Keys? 
You don't need a usable (where I assume "usable" means "capable of generating
crypto keys") source of randomness, for ASLR and stack canaries and the like
you just need enough to make it hard for an attacker (meaning a dumb piece of
code, not an active, adaptive attack) to guess.  16 bits should be fine (see
various analyses of this topic, in practice it's anywhere from 12 to 24 bits,
based more on hardware limits than anything else).

@_date: 2016-11-08 10:10:07
@_author: Peter Gutmann 
@_subject: [Cryptography] "we need to protect [our dox] by at least 
But look at what they're doing, it's what some guy called Grigg once described
as "more of what we already know doesn't work".  So they're going to warn even
harder that you're not using a CA-supplied cert, because if there's one thing
that 20 years of experience with insecurity has told us, it's that that's all
you need to make web browsing secure.
Note that the article doesn't say they're actually going to take any measures
to improve security, just following the old "anything HTTP is unsafe, anything
HTTPS is safe" that hasn't done anything to stop phishing, malware, or any of
the other fun stuff on the web.
So I'd say it's mostly the Ugly of security thinking.  It shows that after 20
years of failure to make progress, nothing has changed.  Prognosis: 20 more
years of the same.

@_date: 2016-11-10 08:37:04
@_author: Peter Gutmann 
@_subject: [Cryptography] protecting information ... was: we need 
Leading to an old joke:
If you give the command "SECURE THE BUILDING", here is what the different
  services would do:
The NAVY would turn out the lights and lock the doors.
The ARMY would surround the building with defensive fortifications, tanks and
  concertina wire.
The MARINE CORPS would assault the building, using overlapping fields of fire
  from all appropriate points on the perimeter.
The AIR FORCE would take out a three-year lease with an option to buy.

@_date: 2016-11-19 03:38:34
@_author: Peter Gutmann 
@_subject: [Cryptography] Crypto and rustling 
What century are we talking about?  If it's now, inject a microchip.  If it's
19th-century Australia (or 19th-century anywhere), hang (i.e. publicly
execute) the rustlers.
Some problems are most easily solved by moving the goalposts to where the ball
is going, not the other way round.

@_date: 2016-11-19 11:28:52
@_author: Peter Gutmann 
@_subject: [Cryptography] Crypto and rustling 
I think your options are somewhat limited when your medium is burnt animal
hair and you've only got access to 19th-century technology, thus the
suggestion to look at alternative approaches.  At best you could design your
brand to have the maximum Hamming distance between it and any other registered
brand in order to make alteration difficult, but I assume that was being done
by the different ranches anyway.  You could use a variant of the Histiaeus
trick of shaving a patch of your cow, tattooing something in there, and
waiting for it to grow over.  If you saw any shaved cows wandering around,
you'd know that cattle rustlers were looking for your stego.  Then you'd need
a trusted referee who could verify the stego without revealing its details,
which probably wouldn't have worked in any 19th-century legal jurisdiction
because it'd be indistinguishable from the rich ranch owner bribing the
referee to rule in his favour.
Sure.  It's just stating common sense though...

@_date: 2016-11-20 23:56:01
@_author: Peter Gutmann 
@_subject: [Cryptography] On the deployment of client-side certs 
It's the security equivalent of Maxwell's Demon, the HSM needs to separate the
good commands from the bad commands.  That's the standard response to "we have
a EAL 6.023x10^23 certified HSM!", to which you add "...that does absolutely
everything the Windows PC it's plugged into tells it to".  So in the end you
fall back to justifying the HSM as being useful for auditing purposes because
an auditor can check off the physical artefact that The One Key is stored in.
In practice it's a bit more complex than that, I looked at it in my 1998
Usenix Security paper, see pages 4-5 of
 specifically the "Tier 1
... 5" distinction.  Most crypto devices are Tier 1 or 2...

@_date: 2016-11-24 02:27:34
@_author: Peter Gutmann 
@_subject: [Cryptography] On the deployment of client-side certs 
The problem is that no-one wants that.  Or at least everyone says they'd like
it as an abstract concept, but when you productise it no-one actually wants
it.  IBM tried this with their 4758, a fully user-programmable HSM (and rather
nice piece of engineering), and barely managed to sell any of them outside of
a few niche applications.

@_date: 2016-11-24 02:31:06
@_author: Peter Gutmann 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
That's for general fiduciary-style audits.  Remember that we're dealing with
crypto paranoia here, for which "a proper audit" is "an audit that's far more
comprehensive than what was applied in audit level X", for any value of X up
to infinity.
In other words no matter how much it's audited and by whom, there will always
be people for which it's not enough.

@_date: 2016-11-26 08:17:59
@_author: Peter Gutmann 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
Just a note on this for anyone who's planning to use RDSEED, the information
on how to detect this in the Intel docs is wrong, it's not bit 31 of EBX that
indicates whether RDSEED is present but bit 18.

@_date: 2016-11-27 04:38:11
@_author: Peter Gutmann 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
Right, because Many Eyes Make Bugs Shallow, which is why there's never been
any vulns discovered in open-source software, lots of people have gone through
and audited it and found all the bugs.
See above.  You've basically got the choice between "the code never gets
audited" (the Bystander Effect at Internet scale, I don't need to look at it
because someone else is bound to have already done so) or "the code gets
audited by someone paid or otherwise incentivised to do it", which means
you're taking someone else's word that it's OK.

@_date: 2016-11-29 09:34:54
@_author: Peter Gutmann 
@_subject: [Cryptography] Gaslighting ~= power droop == side channel attack 
Smart meters can already characterise pretty much everything that goes on in a
house to do with electricity use, fridge, TV, kettle, radio, computer, you name
it, it's astounding how much data you can recover from a noisy mains signal.
There's a whole (small) industry built around this, google "energy
disaggregation" in the lack of privacy of your own home...

@_date: 2016-11-29 09:44:25
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  OpenSSL and random 
That's taking a very narrow view of the world.  If you step back a bit from
the hardcore-crypto-geek perspective and adopt a more general view,
"A read from the /dev/urandom device will block waiting for more entropy. This
means your application will appear to hang/crash at random intervals, leading
to hard-to-diagnose faults, customer complaints, loss of business, and, if
you're really unlucky, lawsuits".
So you've got a table that looks roughly like this:
  Geek                                     Normal human
  ----                                     ------------
  App.blocks waiting for entropy           App doesn't work
  App doesn't block on entropy             App works

@_date: 2016-11-30 09:01:43
@_author: Peter Gutmann 
@_subject: [Cryptography] RNG design principles 
Not picking on you specifically here, but it's interesting that many times
when I point out some stereotypical security person's response being at odds
with how non-security-geeks see the world, someone comes along and confirms
the stereotype (it happened on CFRG recently, to a level where I was able to
incorporate some of the responses into a talk :-).
Anyway, to pull a scenario from my previous message: Large EDI-based trading
network that can't go down, ever.  A tiny, insignificant component of this is
the crypto.  The RNG there reports it's not getting enough entropy.  What do
you do?
This is a single representative example, substitute something like "factory-
floor SCADA network that can't guarantee entropy", etc.  Wishing away the
problem, e.g. "they need to add a hardware entropy source to each device",
isn't allowed.

@_date: 2016-11-30 22:38:53
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Is Ron right on randomness 
And that's the problem with getRandom(), it's the right theoretical solution
but not the right practical one, unless you want to ship source code for each
targeted system and get the user to build it themselves or use dlsym() to get
to it. Here's the hackery I use to get to it, which requires a build from
source on each system:
 defined( __linux__ ) && defined( GRND_NONBLOCK )
 /* getrandom() was defined in kernel 3.17 and above but is rarely
  supported in libc ("if we add support for it then people might use it
  and things won't work any more with older libc versions"). In some
  cases it's possible to access it via syscall() with SYS_getrandom,
  so the best that we can do is use that if it's available */
  SYS_getrandom
   noBytes = syscall( SYS_getrandom, buffer, DEVRANDOM_BYTES,       GRND_NONBLOCK );
  /* noBytes = getrandom( buffer, DEVRANDOM_BYTES, GRND_NONBLOCK ); */
  /* No guarantee of getrandom() support */
 /* Half-dozen other approaches to the same thing, all incompatible */
The thing with a /dev/urandom read is that you can solve the problem once,
rather than once per OS type, distribution, and kernel version.  One of the
lesser-mentioned software freedoms is, unfortunately, the freedom to make a
complete mess.

@_date: 2016-10-01 07:47:25
@_author: Peter Gutmann 
@_subject: [Cryptography] another security vulnerability / travesty 
Why?  Fax is a lot harder to get at than email.  Account breaches are so
routine and so vast in scope that they don't even make the news any more
unless it's 100 million plus accounts affected.  OTOH when was the last time
you heard about a single fax being intercepted?  (And I mean intercepted
remotely by a third party, not "someone walked out of the office with a

@_date: 2016-10-01 09:12:01
@_author: Peter Gutmann 
@_subject: [Cryptography] distrusted root CA: WoSign 
Because we have no choice.  What are you going to do in order to opt out, stop
using the web?  It's a totally captive market.
Note that things are run by the CA/Browser forum, not the CA/Browser/web site
operator/end user/customer forum.  The only people with a say in things are
the ones who are making money off the whole racket, and they aren't going to
do anything to change the status quo.

@_date: 2016-10-02 04:35:29
@_author: Peter Gutmann 
@_subject: [Cryptography] distrusted root CA: WoSign 
It depends on who you mean by "we".  When I said "we" I meant the end user,
you, me, everyone around you.  We have no choice.  We have to take what the
browser vendors give us, which in turn is what the CA/B Forum wants us to
have, and their response to the ongoing failure of PKI [0] is to give us more
That's just fixing a problem that was created by the browser vendors in the
first place, the fact that the browsers are set up so that you need to ask a
CA for permission to use encryption.  They could have done the same thing
years ago by allowing anon-DH.
Even then, as Jerry Leichter pointed out in a previous message, they're still
requiring you to ask a CA to use encryption, they've just acted, years too
late, to blunt the most obvious criticism.  Ever tried to set up TLS on a non-
public-Internet network (RFC 1918 or whatever)?  You basically can't, unless
you use your own software on both the client and server.  Browsers just don't
work there, because needing to ask a CA for permission to encrypt is hardwired
into them.
[0] Now I know some people are going to claim PKI isn't a failure, and to some
    extent that's correct, since there was never any mission statement for PKI
    beyond "you asked for PKI, here is some" it's not really possible to say     it's failed.  Or succeeded.  Or anything really.  However given that twenty     years of evidence indicates it has no effect on phishing, malware, or much     of anything else that you'd sort of expect it to deal with, I'm going to     say it's been a failure.

@_date: 2016-10-03 02:54:45
@_author: Peter Gutmann 
@_subject: [Cryptography] distrusted root CA: WoSign 
Given that browsers mostly don't do revocation either except for a token
subset of certs, I'd say it's 0 : 0 to both sides there.  It's also a bit of a
red herring, going through each of the revocation reason codes, we find that
"key compromise" is unlikely to be useful unless the attacker helpfully
informs the server administrator that theyve stolen their key, "affiliation
changed" is handled by obtaining a new certificate for the changed server URL,
"superseded" is handled in the same way, and "cessation of operation" is
handled by shutting down the server.  In none of these cases is revocation of
much use, which is why browsers mostly ignore it.
Now, let's talk about proper mutual authentication, and browser PKI's failure
to provide any of that...
Firstly, that means reconfiguring each install of each browser on each PC with
custom roots, which is more or less unmanageable on any kind of scale.
Secondly, re-read Jerry's message, a cert (as implemented in browsers) is OK
if what you care about is that you've apparently connected to the server or
device at FQDN/IP address X, but not that you've connected to the booster pump
controller for the south machine hall.  The IP address doesn't matter, you
want to know that you've connected to a particular device.
(And, by extension for browser PKI, whether I've connected to doesn't matter, I want to know that I've connected to Amazon the global
retailer, not some random FQDN.  This is why phishing works).
That's only because Chrome uses cert pinning.  In fact what you're talking
about there is a symptom of the *failure* of PKI, if PKI worked then Chrome
wouldn't need to use cert pinning.  When you hardcode in "only trust these
certs for Google" then you're bypassing CAs and PKI, all that's left is a
convenient bit-bagging mechanism.  Like SSH does, without any PKI, or need to
pay a CA to be allowed to encrypt.
Right, and that's the standard excuse for PKI, "it's not guaranteed to do
anything, and that's exactly what it does".  So why are we paying millions?
billions? of dollars a year for it then?  It's pure snake oil [0].
Oh gawd, how many times have we already gone round this loop before?  See
chapters 1 and 3-4 of my book (and in fact the rest of the book in general).
And I'm not claiming that's the one true solution, in addition see 20 years of
work by security researchers on ways of dealing with this, quite a bit of
which I reference in the book so you can start from there.
[0] Again, this may be a bit of a difficult claim to substantiate, because     snake oil at least claims to solve all manner of problems, while PKI     just is.  As Ben rightly points out, it doesn't address phishing, it
    doesn't address malware, it doesn't...  Perhaps we should paraphrase     Pauli to say that "it's not even snake oil".

@_date: 2016-10-03 11:02:03
@_author: Peter Gutmann 
@_subject: [Cryptography] another security vulnerability / travesty 
Drop a letter in the post?  Or is this a trick question?

@_date: 2016-10-03 23:20:41
@_author: Peter Gutmann 
@_subject: [Cryptography] Debunking the "SMTP TLS "s a mess" myth. 
And that is an important (if depressing) data point: If you want to encrypt
email, you use STARTTLS, not S/MIME or PGP.

@_date: 2016-10-03 23:28:46
@_author: Peter Gutmann 
@_subject: [Cryptography] distrusted root CA: WoSign 
I would say get the EFF or someone similar to run it.  Both of those groups,
while nominally open, are way too easy to render captive to vendor interests.
I'm not saying they do bad work, but that the structure is very vulnerable to
vendor stacking.

@_date: 2016-10-03 23:54:40
@_author: Peter Gutmann 
@_subject: [Cryptography] distrusted root CA: WoSign 
Yeah, the EFF was just the first thing that came to mind.  I realise it's a
bit of a gedanken experiment, but this is one situation where the "open"
process (anyone can join, so the vested interests are most strongly motivated
to do so) actually works against you.  You'd need something in the style of
the committee/board often set up by governments to discuss public-interest
issues with a fixed number of representatives of each segment on it.  I'm sure
this isn't the first time this problem has had to be solved in the field of

@_date: 2016-10-04 10:40:02
@_author: Peter Gutmann 
@_subject: [Cryptography] French credit card has time-varying PIN 
Half an hour on average, not a full hour.  In any case it pretty much kills
phishing, because the ecosystem requires a matter of some days between the
card being phished and cashed out.

@_date: 2016-10-05 03:58:16
@_author: Peter Gutmann 
@_subject: [Cryptography] French credit card has time-varying PIN 
It's not that easy.  At this point you've switched from the critical link
being technology to it being people, and you can't speed that up with
software.  The rate-limiting step in the ecosystem is how fast you can cash
out the cards, even if you can somehow reduce the time from phish to cashiers
to zero you can't cash them out that quickly because it involves humans doing
the work.  The trick in trying to make something secure is to find the
component that can't be solved/overrun with technology, cashing out cards is a
prime example of this.
In addition there's the outrunning-the-bear issue, you don't have to stop
every single possible attack, you just have to be more secure than everyone
else.  And as long as there are US banks, that's not going to be that hard

@_date: 2016-10-12 03:36:37
@_author: Peter Gutmann 
@_subject: [Cryptography] 
=?windows-1252?q?put_undetectable_=93trapdoors=94_in_millions_of_crypto_k?=
 =?windows-1252?q?eys=22?=
I'm not sure if it's really that bad.  IPsec has predefined groups,
CMS / SMIME uses the X9.42/DSA format but then no-one uses DH in S/MIME
anyway, SSH had predefined groups but switched to the server-specifies-the-
group, but many use e.g. the RFC 3526 parameters which you can check for
and fastpath, and TLS has the TLS-LTS draft which fixes the problem there.
So the problem areas are really SSH with unknown-provenance negotiated parameters and TLS without TLS-LTS with the same thing.  However even there the potential attack seems a bit unclear, someone would have to convince a server operator to adopt booby-trapped parameters.  Sure, you may be able to do that, but then you can probably also persuade them to install this little server-side plugin that optimises performance or something, don't bother
scanning it with your AV, it's perfectly legit.

@_date: 2016-10-13 09:02:04
@_author: Peter Gutmann 
@_subject: [Cryptography] 
=?windows-1252?q?put_undetectable_=93trapdoors=94_in_millions_of_crypto_k?=
 =?windows-1252?q?eys=22?=
Right, but then on what basis would the client reject a parameter set?  You'd
need some not-terribly-heavyweight non-spoofable test that you can apply to
verify that what the server has sent you isn't booby-trapped, and then a means
of negotiating the parameters that doesn't devolve into "no, you go first"
between client and server.  That rapidly gets messy.
You could in theory use the client and server random (which both SSH and TLS
do) to generate the DH params, but that's an awful lot of CPU burned by both
That's the killer really, how much CPU do you want to burn as a tradeoff for
feeling a bit safer?  I'm working with systems that in some cases can barely
do the DH exchange in a reasonable amount of time, so any kind of heavyweight
checks are out of the question.

@_date: 2016-10-13 09:59:20
@_author: Peter Gutmann 
@_subject: [Cryptography] Defending against weak/trapdoored keys 
See my other message, it's (theoretically) not too hard to do using SSH or TLS
client and server random, but the overhead of generating the parameters is
If you wanted to be really clever you could use the hash of entire client and
server hello to generate the DH parameters, so you also get integrity
protection of the initial messages.  You've still got the high overhead
problem though.  So overall I think this falls into the cute-but-impractical

@_date: 2016-10-14 22:33:44
@_author: Peter Gutmann 
@_subject: [Cryptography] Defending against weak/trapdoored keys 
The point I was making, in my usual indirect manner :-), was that the
important factor isn't how fast you can get it to go on the most bleeding-edge
CPU available, it's how it works on the trailing edge ones that are still in
use everywhere.

@_date: 2016-10-27 04:24:13
@_author: Peter Gutmann 
@_subject: [Cryptography] A PKI without CRLs or OCSP 
I get the feeling that if Procrustes were around today, and a geek, he'd be positively drooling over the blockchain.

@_date: 2016-10-28 01:51:21
@_author: Peter Gutmann 
@_subject: [Cryptography] A PKI without CRLs or OCSP 
Lots of people tried to address this, over a long period of time, but it never
made any difference.  Watching the PKIX standing committee operate was a weird
experience, it was more like a group trying to formalize religious dogma than
create a technical standard.  Almost anyone with any practical, hands-on
experience either left or went into read-only mode (I was surprised at some of
the off-list responses I got at times, people who I thought had bailed years
ago were still reading the list but had given up trying to contribute).  What
was left was the same sort of people, or in some cases literally the same
people, who had made OSI the success it was (as someone once observed, and
this is an approximate quote "it's a good thing they're all tarpitted in PKIX,
imagine how much damage they could be causing if they went elsewhere").
Unfortunately I don't think it'll ever change.  The CA business model is, as
Ian has pointed out, the problem.  It's based around selling as many
organisations as possible a bag-o-bits, billable yearly, and neither the CAs
nor browser vendors have any interest in changing that.  No matter how clean
or elegant or functional your newly-proposed technical solution is, you need
to get the CAs and browser vendors out of the loop, and I can't see how you'd
do that.

@_date: 2016-09-10 05:26:21
@_author: Peter Gutmann 
@_subject: [Cryptography] Secure erasure 
That's an excellent (I would say must-) read in any case, it documents what
happens when a technical security system designed in a vacuum comes into
contact with the real world.  And by that I'm describing not just the crypto
but also the process for obtaining and maintaining a clearance.  It's one of
the best security usability writeups I've ever read.

@_date: 2016-09-11 09:50:33
@_author: Peter Gutmann 
@_subject: [Cryptography] Secure erasure 
Thus the two quotes at the start of my IoT crypto post (to another list)
  The problem we have is not how to get stronger crypto in place, it's how to
  get more crypto in place.
    -- Ian Grigg, 28 August 2016.
  ... and to raise the level of security of the rest of the system so that
  attackers are actually forced to target the crypto rather than just
  strolling around it.
    -- Peter Gutmann, in corollary.
(the latter was just a re-stating in the context of Ian's quote of Shamir's
Law that crypto is bypassed, not attacked).
Which leads to a further corollary that anything more than maybe single DES
when your opponent is anything other than a nation-state is probably a waste
of time because there's always an easier way in.

@_date: 2016-09-12 06:53:29
@_author: Peter Gutmann 
@_subject: [Cryptography] Secure erasure 
I didn't say "use weak crypto", I said that using anything stronger than about
single DES isn't necessary because it's no longer the weakest point.  Barring
corner cases, can you give me an example of a widely-deployed system involving
crypto where single DES is the weakest point, in other words where attackers
are using DES-cracking to get in?  It's not SCADA, both because SCADA isn't
protecting anything worth applying a DES-breaker to and because there's
always, always a much easier way in.  It's not protecting bank accounts/credit
cards (TLS) because you can buy those in bulk from any carder forum for next
to nothing (heck, carders give away free samples to prove their wares are
good).  It's not Unix logons/server access (SSH) because you can buy
compromised machines for equally little.
So for which generally-used, widely-deployed system (where the opponent isn't
a nation-state) is DES the weakest point of attack?

@_date: 2016-09-12 09:01:12
@_author: Peter Gutmann 
@_subject: [Cryptography] Secure erasure 
Oh, it does have some use if you're doing pen-testing, it's a great indicator
for where to look for vulns.  What I do when I'm asked to audit code is to
look for the crypto, ignore it, and look right next to the crypto itself to
find all the mistakes being made in applying and managing it.  So it's a
beacon to use for finding vulnerabilities.
(I may have given away a pen-testing secret there :-).  If someone looks at
your code for five minutes or less and points out an exploitable flaw, it's
not because they have magic powers, it's because you're using crypto to guide
them to it).

@_date: 2016-09-14 01:07:44
@_author: Peter Gutmann 
@_subject: [Cryptography] Secure erasure in C. 
So now there's also a hardware equivalent of the compiler optimising away your
zeroise operation...
There was a talk by someone from Synopsis at an embedded Linux conf a few
years ago that gave a good overview of the implications of big.LITTLE under
Linux, let me see...

@_date: 2016-09-14 01:21:09
@_author: Peter Gutmann 
@_subject: [Cryptography] Secure erasure 
It was still used in specialised circumstances.  German snipers and, more
generally, troops in exposed positions, wore what's close to a medieval set of
armour known as a Sappeurpanzer (body armour) and Sappeurhelm (helmet).
There's not much online about it but one example is at:
Also the vent lugs on the original M16/M18 helmet could have additional armour
plates attached to them, the Stirnpanzer:
Then there's (non-official) stuff like this, showing 19 bullet impacts:
but that predates WWI by a bit.

@_date: 2016-09-17 02:29:42
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED]  Ada vs Rust vs safer C 
I wouldn't pin much hope on that, gcc is aggressively part of the problem
space, not the solution space.  If you look at "Towards Optimization-Safe
Systems: Analyzing the Impact of Undefined Behavior" from SOSP 2013, the
safest (meaning biggest chance of applying common sense) widely-used compilers
are MSVC and armcc.  The worst compiler is gcc, and the gcc developers have,
over a period of many years, argued endlessly for maintaining this behaviour.
So your immediate solution, if you're worried about this, is to compile for
Windows or Arm (using armcc, not gcc).  In terms of OSS compilers, perhaps the
clang folks would be more open to addressing the problem.

@_date: 2016-09-17 11:44:09
@_author: Peter Gutmann 
@_subject: [Cryptography] Ada vs Rust vs safer C 
For a first start, you don't even need to do that.  Just have the compiler
make some common-sense assumptions, e.g. that it's running on a two's-
complement machine, that if it's (say) x86-64 then integer ops will have the
semantics of the x86-64 architecture, and so on.  This would immediately get
rid of the majority of the problems covered in the paper I referenced in my
previous message.

@_date: 2016-09-18 06:31:35
@_author: Peter Gutmann 
@_subject: [Cryptography] Ada vs Rust vs safer C 
You've just described PREfast (for an annotation-guided analyser) [0] and
Coverity/Fortify/whatever (plus the somewhat more limited clang analyser) for
a non-guided analyser.  PREfast has the advantage that it's free, but also the
disadvantage that it's somewhat under-documented, so the annotations for more
complex things like pointer derefs are kind of trial-and-error (if anyone from
MS is reading this and can answer some questions on PREfast annotations or can
update the documentation, it'd be a great help, there's a pile of stuff around
DEREF that's so sparsely documented it's pure guesswork on how to apply it).
Having said that, its annotations make it incredibly powerful, you can turn C
into something close to Pascal in terms of its checking via PREfast analysis.
So that would be another answer to the question about new tools, to some
extent they already exist, you've got the PREfast analyser and a matching
compiler that's the least likely to do unexpected things to your code.
[0] And for the people who are going to say "but that's Windows only", you've
    got the source code for gcc/clang/whatever, what's the problem? :-).

@_date: 2016-09-18 14:44:05
@_author: Peter Gutmann 
@_subject: [Cryptography] Ada vs Rust vs safer C 
That's just saying that if you have a huge code base you may want to break it
up into smaller pieces to speed up analysis.  All of the static analysers take
a long time/lots of CPU to run, and re-analysing the entire code base every
time would be pretty slow.  Certainly for PREfast on my code it's case of
going away to do something else when it's running while I can do a full
rebuild in about 30 seconds.  That's not a big deal, I wouldn't really care if
it had to run overnight to work.
None of them are Windows-specific, it's just things like "this value can only
take ranges between 0 and 100" or "this value points to a buffer whose size is
defined by that other value".  There are composite annotations that say things
like "this is a handle to some Windows-specific thing", but they're just
convenience macros built up from the lower-level primitives.

@_date: 2016-09-19 06:06:49
@_author: Peter Gutmann 
@_subject: [Cryptography] Ada vs Rust vs safer C 
There have been several projects like this, e.g. Larch and LCLint/splint,
which required near-incomprehensible annotations and produced near-
incomprehensible diagnostics.  It requires experts to use, and another set of
experts to evaluate the results.
The problem with all of these is that the designers chose to create a
metalanguage/system for specifying the precise semantics of a program, which
means you need to create a second copy of your code using mathematical
notation, and the proof assistant then checks whether your mathematical
description matches what your code does.  The problems with this are that most
coders prefer to only write their code once, the notation is invariably near-
incomprehensible, it often can't express what the developers are trying to do
(by long-standing tradition you demonstrate it using toy examples like a
simple quicksort, or even just a memcpy()), and the developers are looking for
a tool that gives them something like what a Pascal compiler would do, not a
full-blown formal verification system.
PREfast does a pretty good job.  What made the PREfast work unique is that
they didn't start with some abstract mathematical goal (well, not once they
got the tool into production use beyond the research stage) but looked at what
real-world programmers were doing and created what was needed to deal with
that.  It's evolved continuously over the years to match real-world
experience, so the current SAL and PREfast is very different from what it was
five and ten years ago.

@_date: 2016-09-19 06:12:25
@_author: Peter Gutmann 
@_subject: [Cryptography] Ada vs Rust vs safer C 
Seems highly unlikely, it came out of MS Research, and in any case there are
already third-party code bases that use it (I can't remember where I saw it,
Wine perhaps, but someone's duplicated the annotations in an independent
header set) without anything happening to them.
SAL is the annotation language (source-code annotation language) and PREfast
is the checking tool.
In any case you wouldn't use the low-level attribute-based stuff, just the
high-level things, things like In_Range_(), _Ret_range_(), _Acquires_lock_(),
_Requires_lock_held_(), _Releases_lock_(), _Success_(), _Check_return_, and so

@_date: 2016-09-20 07:19:01
@_author: Peter Gutmann 
@_subject: [Cryptography] defending against common errors (was: 
It's been in Visual Studio/VC++ for several years now.
Again, under Windows besides memset_s() you've got the explicit
In any case since we're all living in an open-source utopia  :-), if you want the equivalent for gcc or clang all you need to do is modify the code...

@_date: 2016-09-21 15:16:58
@_author: Peter Gutmann 
@_subject: [Cryptography] Ada vs Rust vs safer C 
There's a hint there for anyone who knows any more to spill the beans :-).
You don't really need to do anything at the C standard level though, the
standard already says, in effect, "if you shift more than X bits the compiler
is allowed to reformat your hard drive" (or whatever it chooses to do), so all
a compiler vendor has to do is define their compiler's behaviour to be "the
code we generate will behave as per the native machine architecture".  For
example on x86 it'll do exactly what you'd expect from x86 (i.e. what
Intel/AMD say in their architecture reference), on x64 it'll do what x64 is
specified to do, etc.  Some compilers already do a pretty good job of this,
see the paper I previously referenced.

@_date: 2016-09-21 15:23:19
@_author: Peter Gutmann 
@_subject: [Cryptography] Ada vs Rust vs safer C 
That doesn't always work though.  One case where it's really hard is with
parsers that need to move a lot of state around.  And I'm not thinking so much
of programming languages but parsing of TLS and SSH client/server hellos and
PGP and CMS headers.  You can sort of work around it by putting all of the
state into a huge struct and passing it down through all the inner blocks, but
that generally makes things worse because now you've got a whole jumble of
mostly-unrelated values stuffed into a big blob that you pass around from one
function to the next.

@_date: 2016-09-21 15:33:09
@_author: Peter Gutmann 
@_subject: [Cryptography] Ada vs Rust vs safer C 
This is actually similar to a comment I made a few years ago on another list
about the computer-geek response to a problem where pilot over-use of rudder
controls could result in the vertical stabilizer becoming detached.  The FAA
response to this was that it was a one-off accident because the industry would
identify and correct the problem and make sure it never happened again.
Here's my sketch of the same issue if computer geeks were assigned to it:
Bugzilla Bug 123456: A300 vertical stabilizer falls off if rudder controls
  overused
April 2004: FIXME: Needs to be resolved
July 2005: I'm sorry, I can't reproduce this on my 737.
September 2005: Have you tried downloading the latest nightly build?
March 2006: OK, try it now.
October 2006: Resetting status due to new code release
May 2007: Are we sure this is a bug?  Isn't this pilot error?
May 2007 - June 2008: 57-page debate on whether it's pilot error.
November 2008: What about the following patch? December 2008: This doesn't comply with submission guidelines, correct and
  resubmit.
July 2009: Is anyone still working on getting this fixed?
February 2010: Could we get the UI guys to look at perhaps resolving this?
August 2010: This issue is clearly documented in the appendix to the apocrypha
  to the Howto, and therefore isn't a problem that needs addressing.
Resolved: Wontfix.
Although the above is intended as satire, it's scarily close to way too many
bugzilla threads I've followed.

@_date: 2016-09-23 00:09:09
@_author: Peter Gutmann 
@_subject: [Cryptography] Spooky quantum radar at a distance 
You don't need quantum entanglement to do that, any one of these:
would do it just as well.  Hiding from C-, X-, Ku- and S-band doesn't help you
much if the opposition isn't operating in those bands.

@_date: 2016-09-29 04:32:15
@_author: Peter Gutmann 
@_subject: [Cryptography] Posting the keys/certs for: Two distinct 
Isn't this a bit like saying that the fix for Linux kernel bugs is also
trivial: Wherever there's a kernel bug, add a line of code that fixes it.
(Evaluating multiplicative orders, from a quick look at Bach & Shallit's
 "Algorithmic Number Theory", isn't exactly a one-liner).

@_date: 2016-09-30 06:06:41
@_author: Peter Gutmann 
@_subject: [Cryptography] Posting the keys/certs for: Two distinct 
Given that the keys Ron posted were as follows:
P:  00:90:df:c4:88:8f:91:41:57:b9:b0:9d:9f:8d:53:
    ce:3b:ac:8e:f9:59:7a:47:08:c7:3d:6f:ab:45:e2:
    0b:3e:6f:da:a8:d0:08:7a:9f:f0:bb:19:9b:c8:60:
    d1:af:91:81:03:bf:2c:f2:dd:0e:09:fc:db:4a:1d:
    ab:a6:99:17:f5:a2:f4:0c:b1:2c:5e:f4:9d:21:2d:
    9c:0b:4f:b6:f0:b0:0c:a0:87:36:b3:f0:ff:cc:a1:
    d8:a3:32:8b:cb:b6:e0:3a:a5:a0:8f:ad:43:9f:fc:
    f6:de:28:18:da:af:86:80:c2:6e:63:95:0a:4e:0f:
    9b:00:09:1a:b6:74:34:ce:a9
Q:  00:d7:14:b8:0b:1d:52:ff:da:64:7b:ba:c7:20:00:
    98:f9:fc:4c:b2:4b
G:  1 (0x1)
I think a check for validity is pretty trivial.  Or at least detecting an
obviously-invalid key like this is pretty trivial.
Before everyone bashes OpenSSL, remember that until a year or two back Mozilla
would happily accept RSA keys with e = 1, and AFAIK Windows still does, it's a
by-design, documented means of bypassing FIPS 140.

@_date: 2017-04-04 09:09:00
@_author: Peter Gutmann 
@_subject: [Cryptography] Interesting new TLS RFC draft 
In case anyone missed it:
   The Transport Layer Security (TLS) Extension to Support Code Execution
   draft-tls-yolo-rce
      Historically arbitrary code execution has been a TLS feature.  We can
   look to the openssl-too-open extension to the Secure Sockets Layer
   first introduced in 2002 as precedent, however more recently code
   execution was provided via Microsoft's SChannel library as documented
   in the [MS14-066] specification.  Other vendors have implemented code
   execution as an X.509 extension such as the [TALOS-2017-0296]
   specification which augments standard X.509 name constraints with
   code execution features.
   With the rapid adoption of TLS-based applications and rich history of
   vendor-specific code execution features implemented as library-
   specific point-solutions, we feel the TLS ecosystem could benefit
   from a standardized method for accepting a client-specified octet
   string of otherwise unspecified architecture-specific native code.
   [...]

@_date: 2017-04-05 00:58:11
@_author: Peter Gutmann 
@_subject: [Cryptography] Tempest and limits on receiving 
That's only ones meant for the US market, or where the sales are small enough
that they don't do US vs. non-US versions, which is why many US users would
try and get the non-US models, except that US customs then started seizing
them at the border.  OTOH many of the US-targeted models suffer from a series
of strange design defects where they'll re-enable full coverage when you cut a
circuit trace or jumper (older models) or press the right combination of keys
(newer models).  For some odd reason the magic nerve pinch always seems to
leak in some manner soon after a particular model is released.

@_date: 2017-04-27 05:38:48
@_author: Peter Gutmann 
@_subject: [Cryptography] 
=?windows-1252?q?tor_for_AES_Counter_Mode=85?=
+1.  CTR mode, and anything built from it, is way too brittle to be used safely, especially when there are far more robust modes available.
Unless you're being rather oblique here and I'm missing something, I can't see how you could say the value doesn't matter.  If you're encrypting a bunch of files (and the OP did say "files", not just one file) and you reuse the counter value across any of them, you're back to the problems you described earlier in your post.  So:
is actually the exact opposite, it's an awful counter because you'll end up repeating the keystream, while it's merely a somewhat poor IV, you fail to hide the fact that two encrypted plaintext blocks start with the same data, which may or may not be an actual problem.
I'd use OCB, but then you've potentially got patent issues unless you're very
careful about how it's used.

@_date: 2017-05-01 01:56:32
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Escrowing keys 
============================== START ==============================
That's not necessarily a ringing endorsement, "for Jaguars (something where
Lucas Electric was involved), they are surprisingly reliable".  That's like
"this Windows server has an uptime of nearly two weeks" or "I actually got a
second day of battery life for my phone the other day".  I've got a bunch of
PLCs here that date back as far as the 1980s [0], and they're unsurprisingly
reliable.  They were designed to be that long-lived, and the only reason I
don't have older ones is that before that they tended to be hardwired.  So
getting back to the OP, there's hardware out there that not only can be
trusted beyond ten years, it'd be regarded as defective if it didn't last ten
years.  Or twenty.  Or thirty.
Or, in the case of relay ladder logic controllers, eighty or a hundred.
This sort of life cycle is more or less impossible for crypto people to
understand [1].  Conversely, SCADA/industrial control people understand the
life cycle but not crypto.  This is why we have so much SCADA gear that's an
OWASP top-ten antipattern.
[0] Using a somewhat loose definition of PLCs, the "embeded computer running
    control logic software" started more in the 90s.
[1] There are a few practitioners that get it, but they're so busy building
    reliable systems that they don't have much time to talk about them.

@_date: 2017-08-02 09:34:43
@_author: Peter Gutmann 
@_subject: [Cryptography] Another Enigma kit 
For those who want to play with one but can't afford an original:
  This is an electronic replica of the Enigma crypto machine used during WWII
  by the Germans to encrypt all the military communications. It is intended to
  be as close to the real one as economically possible.
Some^H^H^Hquite a bit of assembly required, although you can get a kit with the SMD parts pre-soldered.

@_date: 2017-08-15 03:20:43
@_author: Peter Gutmann 
@_subject: [Cryptography] NIST SP 800-63-3 
A variant of this extends the evasion across all accounts, which is an
effective way to try and address the standard retry-count-defeating attack of
trying one password across many accounts rather than many passwords against
one account.  Obviously this only works for a server that typically only has a
few logins per day for administrative purposes (dbas, sysadmins, etc), not
something with hundreds or thousands of users logging in and out all day long.

@_date: 2017-08-15 12:05:18
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED]  NIST SP 800-63-3 
If you've ever had to do friends-and-family support for Windows, particularly
for older people, you'll have noticed that random spaces added to the start
and end of filenames aren't uncommon.  It's most visible in that case because
it perturbs the natural sort order of filenames, but it's bound to be present
in other locations as well.  In the specific case of passwords I've caught
several occurrences of "my password doesn't work any more" that involve a
spurious space at the start or end.  So if the space-mangling also includes stripping spaces at the start and end,
it seems like sensible advice, because every time I've encountered those it's
been due to mistyping, and since they're invisible there's no way for non-
geeks to spot the problem.

@_date: 2017-12-09 02:20:31
@_author: Peter Gutmann 
@_subject: [Cryptography] When HTTP is outlawed, the outlaws will use HTTPS 
What happens when you try and force everyone off HTTP onto HTTPS?  Everyone,
including the bad guys, end up on HTTPS:
In particular, "the rate at which phishing sites are hosted on HTTPS pages is
rising significantly faster than overall HTTPS adoption".
What's even worse is that since we've been telling users for years that if
it's on HTTP it's safe, it's actually making the phishing more effective.
Presumably the browser vendors' response will be to derate HTTPS so that it's
more like the old HTTP, and make EV the only "true" HTTPS.  Then everyone will
have to move to EV, and the whole dance will begin again.  The term "da capo
al segno" [0] springs irresistibly to mind.
[1] That should probably be "firmare", but it's less of a bad pun that way.

@_date: 2017-12-19 12:41:35
@_author: Peter Gutmann 
@_subject: [Cryptography] Rubber-hose resistance? 
Yup.  Law enforcement can read manuals just like everyone else.  Using a
"plausible deniability" cryptosystem actually makes things worse because it's
waving a huge red flag at them telling them you're trying to hide something
(that's from talking to actual law enforcement people who investigate this
sort of stuff).  I don't use Truecrypt when I'm travelling specifically
because of this, I cross borders with a wiped-clean laptop and scp in anything
work-related that I need once I get there.  That seems to be a common strategy
among IT-savvy travellers who are worried about travelling with
NDA'd/commercially sensitive material.

@_date: 2017-12-20 01:49:28
@_author: Peter Gutmann 
@_subject: [Cryptography] Rubber-hose resistance? 
A password.  That's the one thing that's completely deniable (when it's used,
as in this case, to scp something over from some random server at some random
IP address).

@_date: 2017-12-20 14:26:13
@_author: Peter Gutmann 
@_subject: [Cryptography] Painted into a corner 
Cybernetica have just released their report on the Infineon (and by extension
CC and FIPS) fiasco as it applies to the Estonian national ID card:
This points out that:
  The alternative was to create a solution that would bypass the vulnerability
  by updating the existing cards. There is a requirement that keys must be
  generated on-card and never leave the card. This is required in order to be
  able to use the ID-card to give legally binding digital signatures.
So you've got a security system for which the regulations say you need to get
a FIPS and/or CC certification that's worthless in guaranteeing security, and
you can't fix it because of further regulations that say you have to use the
FIPS/CC-certified broken security.
What's really disturbing about this is that the solution will continue to be
FIPS/CC-certified Infineon cards, the same things that failed the last time:
  Devising the concept for the solution itself was done rather quickly, mainly
  due to the lack of alternatives

@_date: 2017-12-21 04:12:37
@_author: Peter Gutmann 
@_subject: [Cryptography] Rubber-hose resistance? 
[Multiple replies condensed into one]
My threat model is twofold, principally that I'll have to explain to
organisations I'm under contract and/or NDA to that their sensitive data is
now in the hands of someone who didn't sign an NDA, and secondly that I'll be
detained by customs & immigration and/or have the laptop I need in order to
work seized.
If someone wants to carry out a concerted nation-state-level attack (utilising
border controls, post-border network MITMs, evil maids, and in general the
whole Mossad Doing Mossad Things) then I'll need a lot more security than scp.
For example an ORWL... which also means I may as well be wearing a t-shirt
with "US Customs, please detain this guy" printed on it.
The point is to travel with only the most standard, boring stuff, not anything
special that attracts attention.  You don't even need to have putty or scp
installed, just grab it from the Putty web site when you get there.  You're
trying to not attract attention so you can get to where you're going and get
on with work, not to fly with a suitcase full of James Bond gadgets.
You don't really need that, just something strong enough to stop scripts from
getting in.  Add fail2ban, long failed-password retry timeouts, and most
importantly something at a random port (not 22 or 443 or whatever) and you
should be OK.
Salt and pepper as required.  One trick I've used in the past is portrotate,
which changes the listening port every few minutes.  NTP sources are readily
accessible by standard tools, and won't look like suspicious activity.

@_date: 2017-12-21 22:26:42
@_author: Peter Gutmann 
@_subject: [Cryptography] Rubber-hose resistance? 
How do the TSA guys react when they open your bag and see a pile of strange
wired-together electronics?

@_date: 2017-02-05 05:25:58
@_author: Peter Gutmann 
@_subject: [Cryptography] Why is a short HMAC key zero-padded instead 
That's already how it's used in the major protocols that use HMAC, SSH, SSL,
CMS, and so on.  So what's left will presumably be oddball DIY stuff, which
probably does all sorts of other odd things in any case so the HMAC keying
will be the least of your worries.

@_date: 2017-02-05 11:41:12
@_author: Peter Gutmann 
@_subject: [Cryptography] Why is a short HMAC key zero-padded instead of 
It came from the people who gave us IKE.
Except IKE.  I'm not sure if IKEv2 still does it, but in any case pretty much
everyone else just said "block size = key size".
Standards are designed by the original authors, and then there's a lot of
horsetrading to get them adopted.  Sometimes you have to do weird stuff to get
them through, vendor X wants feature Y in order to endorse it so it goes into
the standard.  Or, sometimes, vendor A invents new feature B, badly, so the
standard gets changed to include a fixed-up version of B before other people
copy the broken one.

@_date: 2017-02-05 13:16:59
@_author: Peter Gutmann 
@_subject: [Cryptography] Why is a short HMAC key zero-padded instead of 
It's just standard standards politics, no evil conspiracy or anything.

@_date: 2017-02-08 02:01:30
@_author: Peter Gutmann 
@_subject: [Cryptography] What is total world transaction volume? 
"A bit slow" is an understatement.  About a decade ago the Visa/MC payment
gateways were doing 5K transactions a second, which included ignoring some of
the crypto verification that was being used because it couldn't be processed
in time.
Then there's the case of ignoring the CVV2/CVC2, because most of the time when
it's wrong it's because the cardholder mistypes it or tries to guess it and
gets it wrong rather than fraud, or ignoring the ARQC because the software
stack gets it wrong, and what you lose in fraud is a fraction of what you lose
in rejecting otherwise valid transactions.
Some years ago during a discussion of another (proposed) e-payment system, a
banking guy joked that if they adopted it (even at the low volumes being used
back then), they'd be able to accept transactions for an hour and then have to
shut down and spend the remaining 23 hours processing them all so they could
have their next transaction acceptance hour the following day.
So you need a payment system that can run at the rate of (say) 10K/second and
that has a tunable level of fault-tolerance in order to avoid rejecting valid
but slightly incorrect transactions.
A small amount of fast symmetric crypto and credit cards is the closest we've
got to meeting that requirement.

@_date: 2017-02-08 08:13:01
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: So please tell me. Why is my 
Nope, it doesn't. We have about fifteen years' worth of both research and
real-world results showing that site images don't work. It's a great idea,
sure, it's just one that doesn't actually work in practice.

@_date: 2017-02-08 09:24:00
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: So please tell me. Why is my 
Sure, here's an abbreviated cut&paste from the refs in the book.  tl;dr: Site images are yet another example of the Simon Says problem applied
to security, you have to notice the absence of a stimulus to realise that the
security isn't present.  Most site images can be defeated simply by omitting
them, or if you're really keen, displaying a broken-image-link image or text
saying that the security on the site is being upgraded and images will return
"First live SiteKey exploit seen in operation", Jim Youll, October 2007, CR-
"Security Watch: Passwords and Credit Cards, Part 1", Jesper Johansson,
Microsoft TechNet, July 2008.
"The Emperors New Security Indicators, Stuart Schechter et al, Rachna
Dhamija, Andy Ozment and Ian Fischer, S&P07.
"Security Usability Studies: Risk, Roles and Ethics, Rachna Dhamija, CHI 2007.
"Conditioned-safe Ceremonies and a User Study of an Application to Web
Authentication, Chris Karlof et al, NDSS09.
"Modifying Evaluation Frameworks for User Studies with Deceit and Attack,
Maritza Johnson et al, 2008.
"[Prg] Malware Case Study, Secure Science Corporation, November 2006.
"Malware Targets E-Banking Security Technology", Brian Krebs, November 2007.
"Phishing kits take advantage of novice fraudsters", Paul Mutton, January
2008, Netcraft.
"Learning More About the Underground Economy: A Case-Study of Keyloggers and
Dropzones", Holz et al, University of Mannheim TR-2008-006.
"Security Theater on the Wells Fargo Website", Don Bixby, March 2013, Schneier
on Security.
"Studying the Effectiveness of Security Images in Internet Banking", Lee et
al, W2SP'14.

@_date: 2017-02-09 02:11:03
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: So please tell me. Why is my 
Now imagine someone running a real-world user study to see if it actually
works in practice rather than just in theory.  Given all the studies that have been done on this sort of thing in the past showing that it's not very
effective in practice, I would assume by simple extrapolation that the
answer remains "no", but I'm happy to be proven wrong.

@_date: 2017-02-09 03:07:27
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Fwd: Re: [FORGED] Re: So please tell 
Phishing doesn't require a CA to sell fake certs, that's the whole point of
phishing.  As you say, you don't have references to hand, and that's one of
the two major problems with your entire paper, the only references in there
are some generic ones on phishing and several to patents, one of them yours,
presumably for the contents of the paper.  You seem to be completely unaware
not just of the large amount of existing work that's been done in this area,
but even the fact that it exists.  This is not a good sign.
The second problem is that anyone can come up with (what they think is) a
good idea for a security mechanism.  We get this on the list from time to
time, typically someone pops up with some new unbreakable military-grade
pseudo-one-time pad that they've just invented, and then defends it to the
death when knowledgeable cryptographers point out that it's not (a) new,
(b) good, or (c) secure.
Anyone can come up with some sort of auth mechanism that they think is cool.
Here's one, based on cat videos.  When you connect to a site, your browser
hashes the URL and uses the LSBs of the hash to select a cat video to play
to you.  Since a phishing site and the real site will produce different
hashes, you see different cat videos and so you'll know it's a phishing Also, everyone loves cat videos.
There, a cool new foolproof auth technique, thought up on the spur of the
moment.  Patent application pending.
If you read all of the papers and reports that have looked at this sort of thing before you'll see they're all based on either real-world experience
or user studies.  Your paper contains no evidence of any evaluation of the
technique to determine whether it works.  All the evidence we have in this
area from prior work is that it doesn't.  If you're going to pitch your
technology to this crowd (or pretty much any crowd), you'll need to provide concrete evidence that it actually works in practice.
In the meantime, I'm putting my bets on cat videos.  They won't work any
better than anything else that's been tried, but at least you get to see
some cute kittens.

@_date: 2017-02-09 03:39:42
@_author: Peter Gutmann 
@_subject: [Cryptography] A little history, 
The same thing happened in Europe with thalers, some countries and states issued their version of the nominally equivalent currency in debased form.  For example Austria with control of the Joachimsthal mines had high silver-
content coinage while the Dutch debased theirs, melting down foreign currency and re-minting it as lower-grade coins that were in theory of equivalent value but in practice weren't.  In the case of discovered coin hoards, the people stashing them had a pretty good idea of what was worth
hoarding, you'll find lots of coins from the high-silver-content states and very few if any from the low silver-content states even though they
technically had the same value.

@_date: 2017-02-10 13:37:48
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: So please tell me. Why is my 
Uhhh... is this a trick question?  Look at any HCI security paper for examples
on how this is done.
Right, that's the real world that your system has to be able to handle.
No, it just requires something that's appropriate for evaluating whatever it
is you're testing.  Again, read any HCI paper for examples on how people have
evaluated security mechanisms without having to build a parallel universe to
test them in.
Of course you can, if you want, create a complete "ecosystem of software and
services" etc to test with, a.k.a. throw it over the wall in the real world.
That's how a lot of security mechanisms are evaluated, companies spend
millions of dollars and years of effort deploying something which is then
discovered not to work once it's put in the field.  This is why you want to
perform the evaluation before you invest all that effort.
(For "security", substitute medicines, automobiles, aircraft, consumer
electronics, whatever you want, it's the same there).

@_date: 2017-02-11 01:38:01
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: So please tell me. Why is my 
Right, and nor are most of the papers, so I'm not sure why you keep fixating
on this, other than that it's a convenient red herring.
In any case, like the amateur cryptographers with their pseudo one-time pads,
it's become pretty obvious that you're a True Believer, here to defend the One
Faith against the heathens and infidels.  No matter how many people tell you
your idea (a) isn't new and (b) won't work, you're determined to bring
enlightenment to the unbelievers.  You have now received, at least, several
thousand dollars (pounds, euros, whatever) in gratis consultation and advice
from (hopefully) experts in the field, telling you you need to try again.  You
asked why your solution was wrong, we told you, you ignored the advice.  It's
pretty clear that you're not going to change your mind on this, in which case
could I suggest perhaps switching your efforts to a sales pitch to patent
trolls?  There's bound to be one who'll buy it off you, and they won't care
whether it works or not.
(And remember, I have priority on using cat videos to do it, so no selling
that one to Intellectual Ventures).

@_date: 2017-02-11 08:14:57
@_author: Peter Gutmann 
@_subject: [Cryptography] So please tell me. Why is my solution wrong? 
Given the recent discussion I guess it's time to post a refresher link to
"Common Problems with Conference/Journal Papers",
Not looking at anyone in particular...

@_date: 2017-02-16 08:33:49
@_author: Peter Gutmann 
@_subject: [Cryptography] Security proofs prove non-failproof 
The IACR ePrint archive has just published a paper, "Attacks on Secure Logging
Schemes",  that breaks three secure logging
schemes.  What makes this one particularly interesting is that all of the
schemes that were broken came with security proofs.
Peter (who's a big fan of schemes that can be handled with "two beers' worth
       of analysis", uh, proofs).

@_date: 2017-02-17 00:57:44
@_author: Peter Gutmann 
@_subject: [Cryptography] Security proofs prove non-failproof 
I'm a fan of proofs of mathematical theorems and algorithms in the
mathematical sense (meaning things like "O( n ^ 2/3 ) worst-case"), proofs of
correctness less so.  The problem in that case is that you're not proving a
mathematical property but possibly proving (E&OE) that the algorithm
corresponds to some model that's defined by the defender/algorithm designer.
In other words the designer gets to define (or choose) whatever abstract model
they feel like, and then possibly proves that their design is OK within that
model.  The poor attacker, who doesn't even know that the model exists, also
doesn't know that they're supposed to be stopped by the provably-secure
security system and bypasses it after two beers' worth of analysis.
That was what I meant with the final comment, it was an allusion to making
something so simple there are obviously no holes or making it so complex there
are no obvious holes, requiring an even more complex proof that it's OK to
I guess we'll have to disagree on this... I've been hearing this since the
1970s (well, reading about it after the event in the case of the 1970s), any
minute now formal methods will magically start working and all our code will
be a lot better.  We've certainly come a long way since Ina Jo/Ina Mod and
Gypsy and the Boyer-Moore theorem prover (whose use was famously described as
"like trying to make a string go in a certain direction by pushing it"), but
in 40 years we've only managed to go from "near-impossible to use" to "very
difficult to use".  Extrapolating from 40 years of previous work, at this rate
it's going to be centuries before they're ready to go mainstream.  And given
that tools like PREfast and Coverity and Fortify haven't sat still in the
meantime, they've got pretty stiff competition to deal with.
It's also illustrative to look at the relative progress of static analysers,
they've gone from glorified greps 15 years ago (some of them were literally
glorified greps, "I found a call to gets(), this is dangerous") to extremely
powerful tools that ship as standard with compilers (clang, Visual Studio,
Misra compilers) and in many shops are part of the default build process.
PREfast was usable - not great, but usable - by about VS 2005, and the clang
analyser was about 2007?, so that's 5-7 years to general use, rather than 40
and counting for formal methods.

@_date: 2017-02-17 08:03:23
@_author: Peter Gutmann 
@_subject: [Cryptography] Verification of Identity 
OK, so you've started with a problem.
Now you've added about four or five more problems.
I think this is moving backwards rather than forwards...

@_date: 2017-02-20 00:30:25
@_author: Peter Gutmann 
@_subject: [Cryptography] HSMs or Intel SGX? Which is harder to hack? 
There's a legitimate use for those things? The only use I knew for them was
for phone fraud.
... or when the cellular provider cottons on and blocks them, at which point
you swap in your next batch of SIMs.

@_date: 2017-02-20 03:09:23
@_author: Peter Gutmann 
@_subject: [Cryptography] Security proofs prove non-failproof 
None of which are actually used in practice, at least not to any noticeable
extent.  I don't know of anything built around L4, mostly because by the time
you add enough bits to it to be useful you've got nothing like a formally
verified system any more.  CompCert is nice, but can't reach anywhere near the
code quality of any standard C compiler, and if there's one thing that the
continued use of gcc has taught us it's that people prefer optimised flaky code
to non-optimised correct code.  miTLS is a cool project, but I don't know of
anyone using it in production, they want WolfSSL compiled with gcc, not miTLS
compiled with... is there even a formally verified F* compiler?  In fact, is
there even an F* compiler of any kind?
miTLS specifically brings up another interesting point.  One of the skills
required in implementing a standard is knowing which bits you need to ignore
in order for it to work.  For example in TLS 1.2 everyone knows that you
ignore the requirement for the signature algorithms in the server certs to
match what's given in the TLS handshake algorithms extension otherwise things
break, pretend that the introduction of MD5 + RSA as a signature format, added
in 2008 (!!), never actually happened, and so on.
A specific miTLS example came up a while back where the miTLS developers
reported that they'd found issues when interleaving application data and
handshake records.  The initial response was that this is irrelevant because
you can't actually do that.  Turns out the spec does actually allow it, and
miTLS was (probably) the first implementation that had ever done it, which
showed up the problem.  Because miTLS very carefully implemented everything in
the spec while everyone else just did the bits that made sense, everything was
naturally immune to the issue.
So "correct" software isn't just "follows the spec", it's "follows a sensible
interpretation of the spec".  And that's something that previous attempts at
verifying both SSH and TLS had run into, the spec was such that it wasn't
possible to verify certain properties because it was unclear whether you were
supposed to do A, B, or C at a particular point, so people doing the
verification had to come up with their own best guess at what was intended.  A
lot of implementations of this stuff are relatively safe because they only
implement a basic subset of the full functionality of a particular spec.  It's
only when you try and implement every single bit of everything
coughOpenSSLcough that you run into problems.
Note that I'm not trying to bash any of L4, CompCert, miTLS, etc.  They're all
extremely useful... well OK, I don't know how useful L4 is but CompCert and
miTLS certainly are.  However, they're specialised, craftsman's tools, not
something you bake into a shipping product, in the same way that you don't use
a McLaren to deliver UPS packages.
It's not the error messages that are the problem, it's the learning cliff (not
a curve, a vertical cliff).  I've played with some tools built around Coq, and
even with my somewhat elevated level of masochism for doing code analysis and
lack of the-boss-wants-it-by-Tuesday constraints, it was just too painful to
Again, we'll have to disagree on this, I see no sign of formal methods going
mainstream not just in ten years but ever.  Better yet, if in ten years
they've gone mainstream, I'll buy you dinner at Tony's Steak House in Victoria
St, Auckland.  If not, you owe me dinner at the same venue.
The difference is that anyone can run Coverity, while almost no-one can do a
formal verification.  As the saying goes, a Smith and Wesson beats four aces
(which isn't expressing quite the right thing, but the general sentiment is
there, if anyone knows of something more apropos...).

@_date: 2017-02-20 14:26:38
@_author: Peter Gutmann 
@_subject: [Cryptography] Security proofs prove non-failproof 
I thought that was OKL4, as in "the OKL4 that Ralf-Philipp Weinmann hacked
five years ago, allowing him to seize control of the whole phone via the
baseband".  And from memory he actually exploited OKL4 via a privesc, rather
than going for the multi-megabytes of insecure gunk running on top of the
kernel, which are even more exploitable.

@_date: 2017-02-21 22:19:48
@_author: Peter Gutmann 
@_subject: [Cryptography] Security proofs prove non-failproof 
Can you provide some more details on what you're complaining about here?
Since the ephemeral key agreement output is fed into the master secret (you
can't go the other way round), this is a tautology.

@_date: 2017-02-24 04:36:41
@_author: Peter Gutmann 
@_subject: [Cryptography] Google announces practical SHA-1 collision attack 
I would also like to announce a collision for SHA-1.  Unlike Google's one, this one only takes about ten minutes on a fully functional quantum computer.
Following the same vulnerability disclosure policy, I will wait 90 days
before releasing code that allows anyone with their own fully functioning
quantum computer to create a pair of PDFs that hash to the same SHA-1 sum given two distinct images with some pre-conditions.

@_date: 2017-02-24 05:42:36
@_author: Peter Gutmann 
@_subject: [Cryptography] SHA1 collisions make Git vulnerable to attakcs 
After sitting through an endless flood of headless-chicken messages on
multiple media about SHA-1 being fatally broken, I thought I'd do a quick writeup about what this actually means.  In short:
  Reports of SHA-1's demise are considerably exaggerated.
What CWI/Google have done is confirmed what we've known for a long time, that SHA-1 is shaky.  Using a nation-state's worth of resources and a year of time ( they've shown that, with a very carefully-crafted document, you can create a collision.  Their presentation of the results is detailed and accurate, it's
the panicked misinterpretation of those results that are the problem.
Overall, this is a neat piece of work.  However, before everyone joins the headless-chicken rally, let's look at its effect on real-world protocols that use SHA-1.  Which ones are affected by this?
SSL: Nope.
SSH: Nope.
PGP: Nope (when used for email).
S/MIME: Nope (see above).
OCSP: Nope.
IPsec: Nope.
OpenVPN: Nope.
SCEP/CMP/CMC/EST: Nope.
: Nope.
So what is actually affected?
Situations where you're creating signatures that need to be valid for a long time, and where the enormous latency between legitimate signature creation and forgery isn't an issue (this is why email won't be affected, having to wait a year between email being sent and the forgery being received will
probably raise at least some suspicions of foul play).  What's left is long-term document signing and certificates, as pointed out
by the shattered.io FAQ.  With certificates the chances of it being exploitable in practice are fairly low, through a combination of CAs having moved away from SHA-1, the fact that certificates are only valid for a year
which means you have to race to forge before it expires, and the fact that
any CAs that weren't already randomising serial numbers before the earlier MD5 forged-cert attack will be doing it now.
Even for long-term document signing, these are frequently countersigned by
a TSA to deal with the fact that the original signing certificate will expire after a year, in which case they're safe as well.
Finally, with other stuff (software updates, ISOs, and others), (a) why were you still using SHA-1, and (b) you now have about 6-12 months to finally move to SHA-256, and this time we mean it.
For everything else, you really do need to plan the move to SHA-256.  Think
of this as a practical application of Wright's Principle, "Security won't get better until tools for practical exploration of the attack surface are made available".
Peter (who's at the tail end of a conference and only half awake, so I'll
       need to go through the paper in more detail tomorrow in case there's
       something I missed).

@_date: 2017-02-26 01:31:33
@_author: Peter Gutmann 
@_subject: [Cryptography] SHA-1 collision broke SVN 
Looks like the SHA-1 collision claimed its first casualty:
            It seems that the git-svn mirror stopped updating at r212950, and the bots
    all are red, the svn client prints an error that looks like:
    0svn: E200014: Checksum mismatch for [...] shattered-2.pdf'
(the trail of fail continues after that point in the thread).
However, this is really just bad programming rather than a crypto attack, that
SVN can completely bork itself when it hits a non-unique ID.  It looks like
SVN uses a NoSQL store called FSFS, rather than an SQL store for which the
first CREATE UNIQUE INDEX would have prevented the problem.
(Insert "MongoDB is Web Scale" link here, I guess FSFS is too).

@_date: 2017-02-25 23:06:35
@_author: Peter Gutmann 
@_subject: [Cryptography] Google announces practical SHA-1 collision attack 
They announced an attack that requires a nation-state's worth of resources and
a years' effort (when you're starting from scratch), then said they'd wait for
90 days before releasing full details.  It seemed along the lines of the "do
not try this with your own aircraft carrier/faster-than-light spacecraft/
particle accelerator/nuclear reactor" disclaimers.
Or perhaps: Peter (I can't believe I actually needed to explain that :-).

@_date: 2017-02-27 00:28:47
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: Schneier's Internet Security Agency 
That'll never fly, it would take any IoS device and reduce it to just a S
device.  It looks like you're thinking of "smart" TVs and the like, but most
IoS devices are "I" so you can monitor and/or control them via your phone, for
which disabling the I bit defeats the whole point of their existence.  Even
something like "must be explicitly enabled by the user" doesn't help, because
no-one's going to not enable it, the whole point of getting it was the I part.
And then you run into this (scroll down to "Update Mechanism"):
The lack if ability to update is actually a security feature in many devices.
I'm not so much worried about the government-surveillance angle, but more the
fact that commercial vendors are very good at co-opting regulatory bodies to
create barriers to entry that lock out anyone but them.

@_date: 2017-02-27 08:57:23
@_author: Peter Gutmann 
@_subject: [Cryptography] Google announces practical SHA-1 collision attack 
Lots of organisations, and even individuals, can scrape together that sort of
money, but "resources" is more than just finding the money, it's being able to
justify the expense and then take advantage of the product once you've spent
it.  How would you justify spending several hundred $K (depending on which
price level you manage to get) on a single forged cert?  And before someone
leaps in with "I'm sure the Russian mafia would love to get a forged Google
cert", and ignoring the fact that Google uses cert pinning to it wouldn't do
much good anyway, what would "the Russian mafia" (the universal bogeyman) do
with a Google cert that they aren't already doing without one?  So what's left is things like TAO, for whom it might actually be worthwhile
spending $100K or $500K or whatever on a forged cert (no-one nows what the
Flame cert cost), although from everything we know from Snowden there are lots
of ways of achieving their desired goal without spending that much on a single
I've just finished writing up an RFC security considerations section on SHA-1
that analyses its effect on a particular protocol, and after going through all
the possibilities the outcome is "yes, you could have a go at this, but apart
from proving you can do a SHA-1 forgery there's no benefit to it".  That's the
"resources" thing, the ability to (a) fund it, (b) justify spending that much
to your boss, and (c) exploit it once you've got it.

@_date: 2017-02-28 00:19:24
@_author: Peter Gutmann 
@_subject: [Cryptography] Schneier's Internet Security Agency - bad idea 
That makes you about 0.01% of the IoS vendor's audience.  Or perhaps 0.00%,
I've rounded up.
It's not IoS devices that expect it, it's people buying them.  They want
something they can plug in and that just works, with as little configuration
as possible, preferably zero.
This is a real problem for IoS device vendors, because your typical IoS device
has a UI that consists of an LED and possibly a button (fridges and whatnot
are atypical devices).  One commonly-used way to set these up is that on
first power-on they set up a temporary open AP that you connect to with your
phone and enter your standard AP's credentials.  They then restart, shut down
the temporary AP, and connect to your standard AP.  It's a simple, elegant
solution... if you're a geek.  Look at the support forums for some of these
devices for the amount of problems it causes for non-geek users.
This isn't helped by the fact that a great many IoS devices are singularly
incapable of maintaining an active WiFi + IP connection.  I've got some SCADA
gear that uses WiFi that just works (uptimes of years for the stuff I keep
running permanently), my Windows stuff just works, but anything IoS seems
totally unable to keep a network connection up for any amount of time.  For
some things like Raspberry Pis I wrote scripts to try and restart connections
(and there are lots of other versions on places like Stackexchange), but in
the end I just set up a WiFi bridge and ran wired ethernet to them.
That's fine, you're not the IoS vendors' intended audience.

@_date: 2017-02-28 23:45:32
@_author: Peter Gutmann 
@_subject: [Cryptography] formal verification +- resource exhaustion 
It's a bit underwhelming.  I much prefer MISRA-C (which the JPL stuff
incorporates by reference, but only the not-so-good 2004 version).  The 2012
version has finally reached a stage where it's pretty decent (although it's
been improving with every new release, the 1998 version was awful), and
they've finally made it affordable as well.
It's essential for hard realtime systems, you can't (a) wait for a memory
allocation to occur or (b) risk having one fail.  This is standard in many
If you can't deal with it easily you can always work around it by grabbing a
fixed-size block, say 64k, at startup and then using a high-water-mark
allocator within it.  I'm not sure if that's strictly compliant with the
intent of the no-dynamic-allocation rule, so you'll probably need to provide
some proof that you'll never run past the end of the block.

@_date: 2017-03-01 00:13:26
@_author: Peter Gutmann 
@_subject: [Cryptography] jammers, nor not 
"The Official Secrets Act is not to protect secrets but to protect officials"
 -- Sir Humphrey Appleby.

@_date: 2017-01-01 13:58:09
@_author: Peter Gutmann 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
Henry Baker  quotes:
That's often done out of necessity, because the security standards that are
expected to be applied were created in perfect isolation from reality, with
the creators throwing in every piece of crypto woo-woo they could think of and
expecting it to be used on a device like an MSP430.  A typical smart-meter CPU
from this family is the 420F148, an 8MHz 16-bit CPU with 2kB of RAM and 48kB
of flash.  It can add and subtract, but not multiply, the multiplier is an
external peripheral (you send data out a memory-mapped I/O port and wait for
the result to come back in on another port) and there's no divide.  On this
platform you're meant to manage X.509 certificates, CRLs, PKCS  requests,
enrolment, provisioning, update, and so on, the whole PKI shebang.  Here's an
example (from a vendor bid):
A lot of this was pushed by CAs and PKI vendors, who were positively drooling
over how much money they could make selling certs and PKI services, e.g:
but then the rest of the crypto woo-woo isn't much better than the PKI part.
So when vendors take shortcuts, it's not out of laziness (well, not always out
of laziness) but because they have no choice.  When faced with impossible
requirements, the best you can do is go through just enough of the motions to
make it look like you're doing what the spec says.
The smart meter mess has been a long time in the making, people have been
warning about it for years.  It's an engineered disaster, you could pretty
much see ten years ago that this was how it was going to end up.  Having said
that, there are some bright points like LoRaWAN, but I'm not aware of any
smart meter standards (currently) that specify use of LoRa stuff.

@_date: 2017-01-01 16:51:42
@_author: Peter Gutmann 
@_subject: [Cryptography] Smart electricity meters can be 
No.  They're built to be as cheap, compact, and low-power as possible.  All
they are is a minimal switchmode power supply powering an SoC with some power
sensing capabilities and a radio modem.  Adding heavy-duty power control
circuitry capable of switching, potentially, three-phase power, or at the
least several tens of amps (so thousands of kVA) of single-phase power, isn't
going to happen.

@_date: 2017-01-02 01:12:04
@_author: Peter Gutmann 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
That sounds like it's talking about the meters, not power to the house.
Presumably there's a requirement to do that if the customer opts out of
metered pricing or switches to a different electricity provider, and turning
the meter off remotely is cheaper than a truck roll to disconnect it.

@_date: 2017-01-02 02:22:12
@_author: Peter Gutmann 
@_subject: [Cryptography] Smart electricity meters can 
Radio?  Here it's done via ripple signalling, the tech goes back to (at least)
the 1940s or 1950s and uses huge (compared to a modern MCB) electromechanical
switches.  It was also used to turn street lights on and off.
Interesting, it must vary a lot from country to country.  All the ones I've
seen are purely passive monitoring devices.

@_date: 2017-01-02 05:22:28
@_author: Peter Gutmann 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
Ugh, so it is.  For people who don't want to click their way through all the
links, the data sheet for the iConA (Gen 4) Electric Meter says:
  Remote Disconnect     iConA type iSA4 - Forms 1S-RD, 2S-RD, 12S-RD, 25S-RD     2-Pole, single-throw     Magnetic latching     Rating: 200 A, 240 VAC, 60Hz, PF 0.7/0.8 lagging     Endurance: 5000 cycles, 200 A, 240 VAC, 60 Hz,      PF 0.7/0.8 lagging and 25000 cycles at no load
    Overload: 50 OPS, 300 A, 240 VAC, 60 Hz, PF 1.0     Current Withstand: Per IEC 1036, ANSI C12.1:7,000 A       Peak (5,000 Amps rms) 240 VAC, 60 Hz,           for 6 cycles at 0.7/0.8 PF with normal operation after       exposure; 12,000 Amps rms for 4 cycles with fail safe       conditions after exposure
Most of the models (from that line) are advertised with a remote disconnect
So you've got a mass of more or less insecure devices for which you could
create the electrical equivalent of a water hammer...

@_date: 2017-01-02 23:07:44
@_author: Peter Gutmann 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
So that's an interesting question, if I was uh terrist, would I target the
grid or the people using it?  I wouldn't go for the grid for several reasons.
Firstly, you'd need to be able to shut down vast numbers of power meters, of
different makes and models, run by different companies, at the same time.  In
political terms, attacking a national grid is equivalent to a declaration of
war on the country in question, which means you'd never be safe again.
Finally, you can get a lot of the same effect much cheaper by bringing down a
few HDVC bearers in strategic locations, which requires nothing more than some
explosives and a few volunteers, no high-tech needed.
OTOH demonstrating that you can affect individuals anywhere, any time, will be
far more effective in causing terr.  Instead of "nation attacked, everyone
pulls together" it's "phantom terrists strike again, you could be next".  Lord
Haw-Haw used this sort of thing to great effect during WWII, reporting trivia
about small towns in England (e.g. that the village clock in Upper Piddling
was running five minutes fast) to indicate that the enemy was everywhere among
you and there was nothing you could do about it.

@_date: 2017-01-02 23:20:19
@_author: Peter Gutmann 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
It's the same with water-moderated reactors, the cooling system of last
resort, the high-pressure injection system (HPI), has never been tested in a
real-world situation, and no-one actually wants to risk testing it.  In brief
(very brief), what it does in case of a loss of coolant accident (LOCA) and
when everything else has failed is inject cold water into the core.  So you've
got a core (and reactor components) at 1000+ degrees, inside a radiation-
embrittled reactor vessel (also at high temperature), and you're going to push
cold water (which is also a neutron moderator, so it increases reactivity)
into it.  It's only really been sort-of done twice, once at Three Mile Island
on a relatively new (non-embrittled) reactor vessel where it only ran very
briefly due to various other issues, and at Fukushima (a much older reactor),
where an operator shut it down fairly quickly, presumably because they'd
decided that the risk of running it was far higher than not running it.
(That's a very brief version, I can also do a three-hour one if required :-).

@_date: 2017-01-02 23:44:11
@_author: Peter Gutmann 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
Probably a few tens of cents.  Plus the cost of redesigning your hardware from
scratch, rewriting your code to fit the new hardware, testing it in the lab
and in the field, and getting the hardware and software certified to stringent
industry standards.  And then replacing all the infrastructure in the field.
Shouldn't take more than ten years of effort, fifteen tops.
(OK, that was somewhat sarcastic, but the "it's easy, just magically replace
all the hardware/software with totally new hardware/software that fixes it"
response gets old the five hundredth time you hear it).

@_date: 2017-01-03 04:37:13
@_author: Peter Gutmann 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
They're just eager to make money.  Both examples I gave were meant to
illustrate how totally unrealistic the "solutions" that vendors were
presenting to power companies were.  And while I would trust BT to provide
internet and phone service, I sure wouldn't use them to run a smart-meter PKI.
In fact a PKI is about the last thing I'd use for smart meters ("and now you
have two problems").  For ready-made off-the-shelf solutions, LoRa would be a
pretty good start in that area.

@_date: 2017-01-04 05:43:57
@_author: Peter Gutmann 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
That's way too optimistic.  Induction-disc meters essentially have an
indefinite life, there are 1960s and 1970s houses around here that have their
original junction boxes (with plugin circuit breakers replacing the original
ceramic fuses for houses that are old enough) containing meters that are
around the half-century mark, and a friend of mine lives in a house in an
older suburb than this one that's old enough that the garage was originally a
coach house, which had the electrics done some time after WWI, maybe the
1920s, with fabric-insulated wiring run through walls stuffed with unknown
organic combustibles (a.k.a. "insulation"), and an induction-disc meter
that'll be close to the century mark.
I don't know what the trend will be with smart meters (ask me again in 50
years' time), but I'm assuming the providers won't want to be replacing them
any more often than their predecessors.
Yep, that's the killer app for smart meters, that remote reading is possible
so you can ditch the meter readers.  Once you've got that capability in place,
there's no reason to upgrade/replace any more.

@_date: 2017-01-05 02:17:25
@_author: Peter Gutmann 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
This is a serious problem.  When the HTTP/2 spec was being done, a couple of
embedded systems guys commented that it was highly unsuited for use in a
constrained (embedded/SCADA) environment, and could it have a few minor tweaks
(not breaking changes, just small tweaks and options added) to make it usable
for SCADA.  The response from the HTTP/2 designers was "let them eat HTTP
1.1", effectively saying that HTTP/2 was designed for efficient content
delivery by web service providers and nothing else.  So now there's an
explicit fork of HTTP, HTTP4Google and HTTP 1.1 for everyone else.
It's the same with TLS4Google, it's design goal as with HTTP4Google is to more
efficiently serve content from web service providers to consumers.  The total
number of people on the TLS list who represent embedded/SCADA/whatever use of
TLS is... zero.  Zero people.  There are embedded folks who read the list, but
zero who contribute because no-one feels anything would be achieved by it, the
TLS list is all web browsers and servers (not necessarily web servers, also
MTAs, B2B, that sort of thing, but still big iron), people whose sole concern
is to shovel content from A to B at the highest possible rate, no matter how
badly the protocol has to be compromised (look at TLS 1.3 for an extreme
example of this), but no-one who just cares about a straightforward secure
pipe from A to B with a design spec with a 10-20 year lifetime that fits in
with existing deployed devices.
I've been trying to get an LTS profile of TLS 1.x through on behalf of a
number of users in the embedded space, but a typical response, to a comment
about why TLS4Google wasn't going to work in embedded, excerpted so you can
see what the problem is, was "[...] non-starter as web browsers [...] fix the
reasons why web browsers [...] the web browser vendors [...]" (etc).  Use
outside the web doesn't exist, and therefore doesn't have to be accommodated
in any way.
So in response to your question:
I'd say "there isn't anyone".  Cryptographers publish dinky protocols with all
sorts of fun tricks in them for toy devices in conference papers, but that's
more an exercise in fun protocol design than a real-world spec to work off.
The groups that should be covering this area ignore it, in some cases
explicitly (HTTP4Google), in others implicitly (TLS4Google).  What's left are
industry bodies who work in, and specialise in, high-availability, high-
reliability engineering who are now being asked to do crypto.  It's not
surprising that we get the stuff we've got so far.
That's why I was pleasantly surprised by LoRa, a bunch of guys I've never
heard of before, working in private, came up with a pretty decent design.  If
they ever end up in NZ, I'll buy them dinner.

@_date: 2017-01-05 03:40:17
@_author: Peter Gutmann 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
TLS is the universal substrate.  There is some use of SSH, but only where you
need to secure CLI access.  Even then, you need some special-case handling to
deal with the two main uses of SSH, secure telnet vs. VPN with everything
including an RPC mechanism for the Posix filesystem API (a.k.a. SFTP).  If all
you want is secure telnet then you can drop something like 90-95% of the
complexity of the protocol (after the initial handshake), as well as a lot of
the complexity in the handshake itself.  I did a draft profile of this some
years ago:
  The widespread adoption of SSH has seen the emergence of numerous SSH
  implementations, but also numerous interoperability problems among many of
  the non-mainstream versions.  This problem arises because the complexity and
  in places ambiguity of the specification makes it possible to create
  specification-compliant but non-interoperable implementations, and is
  exacerbated by the fact that in many cases where SSH is used, for example
  for the control interface of an embedded device or a Windows file transfer
  facility, the developers are required to implement a specification designed
  to provide a full-blown Unix VPN solution even though in their case they'll
  never use the majority of its facilities.
  This document describes a simplified profile of SSH that provides a standard
  minimal feature set for use in applications that just require a basic no-
  frills secure channel from A to B, building on a decade of SSH
  implementation experience to avoid known problem areas in the SSH protocol.
  As a side-effect this minimal profile reduces the large attack surface of
  SSH to a more manageable level by eliminating much of the complexity in the
  protocol.
but never went further with it apart from implementing it in my code and
making it the default.  Virtually no-one ever enables the non-default (i.e.
full SSH) behaviour, so it's minimalist and safe-by-default.
Which one to use depends on whether you control the client and server, if it's
both then you can probably go with whatever form of SSH you like, but
otherwise TLS is the universal substrate.

@_date: 2017-01-05 12:04:14
@_author: Peter Gutmann 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
I am not aware of anyone who works in embedded who still actively contributes
to the TLS list.  I am aware of several people who read it but don't
contribute any more (we occasionally exchange emails off-list), for the
reasons I gave in my post.
If you're aware of an active contributor from the embedded/SCADA space, please
let me know.

@_date: 2017-01-11 23:19:03
@_author: Peter Gutmann 
@_subject: [Cryptography] nytimes.com switches to https 
SSLv3 supported message padding.  It was just as ineffective then as it will
be in TLS 1.3, see e.g. "Peek-a-Boo, I Still See You: Why Efficient Traffic
Analysis Countermeasures Fail" by Dyer et al.  tl;dr version: To be truly
effective, the amount of overhead required in terms of dummy traffic and noise
is impractical, as much as 400%.  Since the goal of TLS 1.3 is to make content
delivery by Google et al as efficient as possible, they're not going to negate
all that again just to defeat traffic analysis.

@_date: 2017-01-22 02:20:04
@_author: Peter Gutmann 
@_subject: [Cryptography] Oracle discovers the 1990s in crypto 
In case anyone missed it, Oracle will soon deprecate MD5 and use of keys under
1024 bits, and allow keys larger than 1024 bits to be used:
In other news, I expect them to announce porting Oracle to that newfangled
Windows XP thing, and the upcoming release of a Windows 98 client for it.

@_date: 2017-01-22 13:20:42
@_author: Peter Gutmann 
@_subject: [Cryptography] Oracle discovers the 1990s in crypto 
A minor nit:
Twenty years, not ten.  TLS 1.0 is from 1999, but it had been around for
awhile before then (-00 was late 1996, the only change I can think of in the
wire format between that and the final one was from -02 to -03 in 1997) but
was stalled behind publication of the PKIX cert RFC.  An implementation written from the spec in 1997 would still work today.

@_date: 2017-01-23 02:35:58
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: Oracle discovers the 1990s in crypto 
That also cuts both ways, some distros are shipping versions of OpenSSH
configured to use a bunch of fashion-statement algorithms (chacha20-poly1305 at openssh.com and the like) but with the mandatory-to-
implement ones disabled... you can imagine what that does for
interoperability with standard implementations.

@_date: 2017-01-27 03:49:22
@_author: Peter Gutmann 
@_subject: [Cryptography] HSM's to be required for Code Signing 
This seems a lot like security by press release, if you look at the changes:
Since level 2 HSMs are expensive, not so easy to find, and a pain to use,
companies are probably going to take the other option of moving their keys
into the cloud.  So instead of having the key on an, at least on theory,
isolated machine on a private LAN it's now in the cloud.  Wonderful.
So the problem here was that if a malware researcher requested a revocation,
the CA typically did nothing.  Now they're still free to do nothing, as long
as they claim they're investigating.
The first of those two arguably makes things worse rather than better, and the
second is just business as usual.  The final one, use of TSAs, is necessitated
by the way certs work and mostly a no-op.  "Realizing the importance of the
case, my men are rounding up twice the usual number of suspects".

@_date: 2017-01-27 08:52:00
@_author: Peter Gutmann 
@_subject: [Cryptography] HSM's to be required for Code Signing 
That's not how HSM's work though. The interface to an HSM, at least for
signing purposes, is "perform a private-key operation on this short byte
string" (a.k.a. "sign this hash"). That's it. Using an HSM merely moves the
key from a potentially attacker-controlled PC to an external crypto box that
does anything the attacker-controlled PC wants. It's perfect for auditing
compliance because you can point to a physical artefact that contains the key,
but only offers a marginal increase in actual security. Recall that e.g. the
Adobe rogue signatures were created using an HSM, which offered at most a
speedbump, if that.
The solution to the problem isn't a FIPS-anything HSM, it's a FIPS-nothing
physical control over when something gets signed, and what gets signed.

@_date: 2017-01-28 06:31:20
@_author: Peter Gutmann 
@_subject: [Cryptography] HSM's to be required for Code Signing 
Where can you get a handful of FIPS 140-2 level 2 certified smart card/HSMs
with Microsoft-signed CSPs (or PKCS  drivers) for Authenticode use for
under $100?  Things must have improved radically in the last year or two if a
range of such devices are now readily available.  The raw hardware is
available if you're prepared to jump through a lot of hoops (e.g. the Gemalto
IDPrime MD meets the requirements, but then you need to deal with minimum
order quantities, find a reader to go with them, deal with finicky and often
nonfunctional drivers, etc).  You're not really sold an Authenticode signing
solution but a bunch of nuts and bolts and parts that you have to assemble and
get working yourself... when your job is to build video capture cards for TV
stations or pumping station controllers, not to fiddle with crypto meccano
The very, very few TPMs that I know of that are FIPS 140-2 certified are level
1, no higher (again, E&OE, someone may have got one certified to level 2 or 3
in the last year or two, although I can't see why anyone would bother).  In
addition TPMs are singularly unsuited for general-purpose crypto use, they're
fine for attestation and Bitlocker key storage, but that's about it.
(I know that there are TPMs that are advertised with all sorts of hypothetical
additional capabilities because they're just repurposed smart cards, but try
finding a CSP or PKCS  driver that allows them to be used as a general HSM
for Authenticode signing).
I've been working with PKCS  devices for around twenty years, and haven't
seen any sign of things getting better.  They've been more or less stagnant
for the last twenty years.
This, meaning Microsoft's requirements, also ignore another problem, the long
tail of small dev shops who don't have the resources to engage in any of these
shenanigans.  It's fine if you're Adobe (although it didn't help them in any
case), Oracle, Google, SAP, and so on, but there's a vast number of SME/SMB
devs who don't have the time or resources to deal with this.  Which means that
they're now required to store their keys in the cloud, since that's the only
other option that the requirements give them.
Stepping back a bit, you can see just how much this is security by press
release/rounding up twice the usual number of suspects.  We have fifteen years
of data on how people attack Authenticode, they either shop around CAs until
they find one who'll take their money, or they break into an Authenticode key-
holder's system and use their key.  There are no recorded cases, ever, of
someone physically breaking into the target premises, decapping their HSM, and
extracting the key using microprobing or equivalent, the thing that the level
1 vs. level 2 step is aimed at protecting against.  In terms of actual
attacks, there's no difference between an HSM at FIPS level 0, 1, 2, 3, or
6.022e23.  It's just rounding up twice the usual FIPS level of suspects and
issuing a press release to say you've done it.

@_date: 2017-01-28 06:35:47
@_author: Peter Gutmann 
@_subject: [Cryptography] HSM's to be required for Code Signing 
Excluding Radar O'Reilly attacks, that would actually do a pretty good job,
mostly because of the user button that requires a physical action to generate
the sig, removing the single biggest flaw of HSMs as crypto yes-boxes.
Unfortunately since it's neither FIPS 140-2 level 2 with a Windows CSP nor a
cloud, it's not secure enough to meet Microsoft's requirements.
(Uhh, did I mention "security by press release" before? :-).

@_date: 2017-01-29 10:04:47
@_author: Peter Gutmann 
@_subject: [Cryptography] HSM's to be required for Code 
That's been hypothesised before, a transaction-sequence enforcement mechanism
that only allows particular sequences of operations, assigned to user roles,
with auditing, and so on.  Unfortunately it gets complicated really quickly,
and by and large it's just an awkward way to build a higher-level API using a
sort of macro capability.  So in the long run it's better to provide an API
like "turn this text document into an S/MIME signed message" than to provide a
capability to string together a sequence of operations yourself that do the
same thing.

@_date: 2017-01-30 06:34:28
@_author: Peter Gutmann 
@_subject: [Cryptography] HSM's to be required for Code 
What's the threat here, and how would this defend against it?  If I was going
to attack the DNS I wouldn't go for such a high-profile target, and even if I
did, it'd be detected by having fake DNS entries turn up, at which point going
back to look at the logs to say "uh, yeah, it happened at this time" wouldn't
really add much.
It's a bit like certificate transparency, it served initially to embarrass CAs
into actually doing their job properly for the first time since they were
created (but then the EFF Observatory did that too, CT just outsources it so
anyone can have a poke around), but I'm not aware of it actually preventing
any attacks, e.g. dealing with certs issued to cybercriminals, it's just being
used to embarrass a few lightning-rod CAs when they slip up.
(Which, admittedly, is quite necessary, but it's not really doing anything to
stop the bad guys).
Ugh.  I gave up after "Fratres, agnoscamus peccata nostra, ut apti simus ad
sacra mysteria PKI celebranda".  Do they dress in priest's robes in the

@_date: 2017-01-30 09:37:09
@_author: Peter Gutmann 
@_subject: [Cryptography] Great IoS quote from LCA 2017 
Christopher Biggs, "The Internet of Scary Things":
"The Internet of Things is a new name for embedded systems that don't really
 work properly".
That's so very true, most of the IoS stuff I have quite literally is "embedded
systems that don't work properly".

@_date: 2017-01-31 11:20:54
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: HSM's to be required for 
Fortezza cards did this too, they had both a monotonic counter and a real-time
clock in the device.  The way it worked was that the untrusted host could read
out the time and counter value from the secure device to the untrusted host
and then bind it into the signature they told they secure device to create.
This is why I refer to some HSMs as crypto yes-boxes...

@_date: 2017-01-31 12:16:41
@_author: Peter Gutmann 
@_subject: [Cryptography] Great IoS quote from LCA 2017 
I'm not sure that you can say that an IoS device works fine just from seeing
it at a trade show, you need to actually use it for awhile to see how many
times you need to reflash it, reconfigure it, or reinstall it to keep it
running.  I have a pile of SCADA/embedded gear that I need to operate for
testing purposes, and then a bunch of IoS that's partially also for testing
and partially for fun.  The SCADA gear is all custom hardware designs,
typically running an RTOS, and created by people who understand SCADA and how
to keep things up and running (watchdogs, keepalives, timeout management,
consistency checking, etc).  I don't think I've ever needed to reflash,
reinstall, or even reboot any of them, some of them have been running for
years without being touched.
The IoS OTOH is a completely different story.  In the last couple of weeks,
all three of the Pi-based systems have had to be reflashed due to them
trashing their filesystem, one as it came out of the box from the vendor, one
reboot and it kernel-panicked due to a corrupt filesystem.  That's not
surprising, the combination of no power protection or management circuitry,
using a FS that's totally unsuited for flash use, using the flash as if it was
a hard disk, and the only way to reset/reboot the device being to pull the
power plug pretty much guarantees it's going to fail at some point.  It's The non-Pi stuff isn't much better, all of it needs more care and feeding then
a two-year-old.  A lot of this stuff seems to be being created by people who
are used to desktop PCs in which the OS does a lot of the failure handling for
you and the user can be relied on to click Reconnect to WiFi or whatever when
it drops out.  Look for reviews on Amazon of virtually anything that's IoS and
WiFi-based, you'll find endless complaints about it not connecting, losing the
connection, going catatonic, and other variants, with no ability for the
device to recover without external human intervention.  I've gone out of my
way to get devices that have less negative reviews than many of their
competitors, and I still have to do ridiculous amounts of babysitting to keep
them running.
Security-relevance of all this: If your device can't even manage to keep a
stable network connection, imagine what the security in it must be like...

@_date: 2017-07-01 07:27:43
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Possible SHA2 vulnerability 
No.  If you're going to try and anticipate highly improbably events then you
end up needing to defend against everything imaginable, and ten time as many
things you can't imagine.  In other words an unimaginable highly improbably
event will get you anyway because you didn't anticipate them.  There are more
than enough things that we do know about and that are highly probably that
we're not mitigating against, so it's better to address those first.
In addition, if you really do want to go down that path, you need to look at
the impact of being able to generate collisions for SHA-2.  What would be the
immediate impact of someone being able to do that?  I mean the actual real-
world impact on the general public, not the "imagine all the horrible things
that can happen".  About the only big thing that immediately springs to mind
is that various software update services will have to switch back to SHA-1 for
Even then, is it a major attack vector?  Sure, you can now subvert services
like Windows Update, but given the ease with which you can already implement a
Wannacry-like attack at close to no cost is it worth it even if you do have
the resources available?

@_date: 2017-07-09 11:56:32
@_author: Peter Gutmann 
@_subject: [Cryptography] OpenSSL CSPRNG work 
This is missing the one that actually occurs:
"Oh, you security geeks are always whining about something or other.  We've
been shipping these things for 20 years without any problems.  Besides, you've
just told me the problem is more or less unsolvable, so there's nothing we
could do even if we wanted to".
On the remote chance that the device gets compromised to a level where the
media pays attention and there's a need to do something, you issue an advisory
about putting them behind firewalls and promise updated firmware on the next
product rev, due in 2019 unless the schedule slips again.  Or you just ignore
the issue, that works most of the time as well.
Note that this isn't a "lets all collectively groan at the hypothetical
businessman", this is in all probability what will actually happen in this
And when has that ever happened?  For a very topical example, I was just
forwarded this:
So Broadcom or Google issue a fix, since it's Android most devices are never
updated and remain vulnerable to a remote code execution forever-day, and
nothing of any consequence happens to either Broadcom or Google.

@_date: 2017-07-10 04:40:03
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED]  Attackers will always win, 
However, in virtually all situations a >>> b.  In fact, in most cases you can safely ignore b, because such an attack is impossible/impractical/too expensive/far easier attacks exist/etc.
In addition, even where b is a concern, it's often far easier to do the crypto in whatever way is the most efficient and then deal with side-
channels through shielding, power supply decoupling, etc.
OTOH you really do need to worry about a.  Look at the number of 0days and other subversion mechanisms used by TLAs that have leaked in the last couple of years.  Making sure your code has no holes in it is a far more serious issue than side channels.
That's asking the wrong question.  Well, unless it's a gedanken-experiment style question to see what people will come up with.  The real question is, which attack vectors are being exploited the most, and how do we deal with Incidentally, we already know how to make secure systems that are pretty
side-channel-attack resistant.  There's a twenty-year-old HSM, IBM's 4758, that was resistant to pretty much all of the side-channel attacks that came along after it was developed, not because the developers were magically aware of them but because they used good engineering practice, power supply decoupling, filtering, etc.
The 4758 was attacked by the Cambridge folks, not via any side channels but
through a basic software flaw in the CCA firmware.  You don't need to bother
with b when a will always get you in.

@_date: 2017-07-13 05:42:33
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Attackers will always win, 
That's why we have this:
    The goal of the CrypTech project is to create an open-source hardware   cryptographic engine that can be built by anyone from public hardware   specifications and open-source firmware and operated without fees of any   kind.

@_date: 2017-07-13 05:45:54
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Attackers will always win, 
I don't know whether using an FPGA can strictly be described as "hardware" any
more.  Programmable crypto hardware does have the significant advantage that the crypto device is non-sensitive until the algorithm is loaded into it, and one
piece of hardware can do many jobs.

@_date: 2017-07-14 01:34:00
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Attackers will always win, 
The NSA control the supply chain and can institute whatever measures they feel
appropriate to provide the assurance they require.  That's the point, they have
the luxury to be able to decide to use whatever they feel is appropriate,
rather than the cheapest Arm core they can buy from a Chinese vendor augmented
by a toolchain they downloaded off the Internet and a patched BSP one of their
developers found on ftp.virusbucket.ru.

@_date: 2017-07-19 00:30:33
@_author: Peter Gutmann 
@_subject: [Cryptography] Raspberry Pi-like FPGA ?? 
Mostly tinfoil hattery (but see further down).  With an FPGA you define the
ARM (or whatever) CPU controlling things to be outside the hat and the FPGA to
be inside.  Comment from a talented crypto HW designer who's been doing this
for decades (details anonymised):
  Bottom line is that doing this (and crypto in general) in an FPGA is
  pointless nowadays. The RTL is not portable if you care about performance at
  all. There are umpteen ASICs to choose from that do this well and also are
  DPA/timing/etc. resistant.
  But we need to do it this way because we dont want to trust anyones RTL!
  Really?  OK, so why do you trust their synthesis tools or the RTL of the
  controlling ARM?
  How do I know Im even talking to your crypto FPGA and that someone didnt
  take over the controlling ARM CPU, etc. etc. etc.
Oh, and another reason is that it's fun and cool to do your own crypto in an
FPGA.  Never underestimate the fun/coolness aspect, it's kinda neat to be able
to say "we built our own crypto hardware starting from bare gates (well,

@_date: 2017-07-20 02:20:35
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Raspberry Pi-like FPGA ?? 
Another option if you want to play with crypto on an FPGA is Microsemi's *Fire
line, which provide the crypto both as highly optimised side-channel-attack-
resistant hard cores and soft cores.  So you could use the soft core to verify
the hard core, and then the hard core to do the actual work.  See e.g:
The FPGAs run in the $100-400 range, which isn't bad for what you're getting.
(There's more than this out there from other vendors, I just had a datasheet
for a PolarFire lying on the desk).

@_date: 2017-07-27 23:59:26
@_author: Peter Gutmann 
@_subject: [Cryptography] Anyone interested in a cheap security module for 
+1. There's been a huge amount of work done in that area, and much of it is
public, it's just quite hard to find because it's typically published as
research reports or at little-known symposia. Here's a good starting point:
While some of the techniques are pretty labour-intensive, ultrasonic imaging
of welds, others are simple and automated. The Cobra Seals are particularly
elegant, a mixture of relatively simple tech, fibre-optic bundles embedded in
a metallic-flake clear epoxy block, verified using the camera on a smart
phone.  Photos of them in action:

@_date: 2017-06-21 05:23:07
@_author: Peter Gutmann 
@_subject: [Cryptography] Brainstorming for encrypted text messaging 
Yep.  Pick just about any crypto, it should be good enough.  It's the key
management that'll let you down.
When I'm auditing crypto code for security vulns, I pretty much ignore the
crypto itself, I just use it as a beacon to where the mistakes are being made.

@_date: 2017-03-01 22:55:39
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: Google announces practical SHA-1 
As several people have pointed out, it's not in the report. Or perhaps more
accurately there is disagreement between the authors of the report and the
people reading it as to what's in there. From my multiple re-readings it
appears that the 6500 CPU-years is the one-off computation for a given
document and the 100 GPU years (100 GPU years, not 110 GPU hours) is for each
new collision with that document.
Another thing that the report is insufficiently clear about is that this isn't
about creating a collision with an existing document, it's about creating a
document from scratch that can be manipulated to have two different forms but
the same hash. So it's more a badly-designed-repository-stress-tester than a
signature-forgery attack.

@_date: 2017-03-01 23:01:14
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  cryptography Digest, Vol 47, 
It's not an oversight, it's in the spec, you're just using an obsolete form of
it.  The current spec is quite clear on this:
  Rule 18.8  Variable-length array types shall not be used
  Variable-length array types are specified when the size of an array declared
  in a block or a function prototype is not an integer c onstant expression.
  They are typically implemented as a variable size object stored on the
  stack. Their use can therefore make it impossible to determine statically
  the amount of memory that must be reserved for a stack.
  If the size of a variable-length array is negative or zero, the behaviour is
  undefined.
  If a variable-length array is used in a context in which it is required to
  be compatible with another array type, possibly itself variable-length, then
  the size of the array types shall be identical. Further, all sizes shall
  evaluate to positive integers. If these requirements are not met, the
  behaviour is undefined.
  If a variable-length array type is used in the operand of a sizeof operator,
  under some circumstances it is unspecified whether the array size expression
  is evaluated or not.
  [...]
  There is no use of variable-length arrays that is compliant with this rule.
  The examples show some of the undefined behaviour that can arise from their
  use.
  [...]

@_date: 2017-03-01 23:59:51
@_author: Peter Gutmann 
@_subject: [Cryptography] formal verification +- resource exhaustion 
If the spec was written before the feature existed or had any kind of
widespread use (I doubt many, if any, embedded-systems compilers had VLA
support in the early 00's), it's not an oversight.  They can't predict the
I'm using MISRA.  The JPL spec is... well, it may be OK for JPL use, but MISRA
is better in general, more comprehensive, includes a rationale and examples,
and so on.  MISRA'12 is actually something I can recommend, the earlier ones
less so.

@_date: 2017-03-02 00:28:33
@_author: Peter Gutmann 
@_subject: [Cryptography] TPM and SHA-1 
You're asking the wrong question.  It's not "will a SHA-1 break affect TPM
2.0" it's "will the current break affect TPM version anything?" (no), and
"will a more standard collision attack affect TPM version anything?", which is
a bit more complex.  The most common use of TPMs is just key storage
(Bitlocker etc), for which SHA-1 problems are irrelevant.  Then there's
attestation, which is... how lost in the noise floor is usage of that?  I'm
assuming someone must be using it for something, but is it used anywhere where
it's worth attacking?  And given the way SHA-1 is used for attestation, is
there a feasible attack?

@_date: 2017-03-02 10:02:56
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: Google announces practical SHA-1 
I'm not really aware of any situation in which someone is going to be signing
subverted data (apart from the web PKI's certificate-manufacturing operations
that is). For non-PKI signing you typically sign content you've created
yourself, whether it's code signing, a report sent from a remote-monitoring
site, payment instructions, etc, almost everything I know of that gets signed
is content that the signer has created. The one exception is countersigning
and timestamping, where the countersigner adds all manner of other stuff to
the data before (counter-)signing it, so it's signing a very different hash to
what the original signer did.
While I'm sure there's some weird corner case somewhere that's vulnerable, for
any normal use of signing it shouldn't be an issue.

@_date: 2017-03-02 10:16:14
@_author: Peter Gutmann 
@_subject: [Cryptography] formal verification +- resource exhaustion 
MISRA isn't for resource-exhaustion prevention, it's for safe programming in
general.  It's for creating highly reliable systems with the lowest possible
chances of unexpected behaviour.  Even without the additional security
guidelines (2012 Amendment 1), MISRA produces pretty solid code.
As far as I know they make it much harder to analyse, not easier.  That's why
things like DO-178C don't do C++ (just the basic C++ language, not things like
smart pointers) despite a decade or more of effort in trying to deal with it,
you can't analyse what's going on under the covers.
Almost certainly, since it bans a whole range of risky C behaviour, and also
forces you to think a lot more about how you structure your code.

@_date: 2017-03-06 00:17:04
@_author: Peter Gutmann 
@_subject: [Cryptography] Secret Handshake problem. 
I've got a variant of this which is an actual real-world problem in pressing
need of a solution, rather than a theoretical one created to justify
publishing a crypto idea.  And since it's a real-world problem, there are also
real-world constraints...
The problem involves sex workers identifying dangerous customers in a manner
in which it won't end up as another Ashley Madison.  Currently this is done
via printed blacklists that can be consulted in centralised locations (many of
the street workers don't have phones, this part is a bit out-of-scope for a
technical solution).  The blacklists are printed off from a database that,
depending on who's set it up, probably isn't secure against a concerted
attack, so having a catalogue of clients in electronically lootable form is
another problem to be solved.
Clients are identified by phone numbers and/or car plates, so the question is
"is it safe to get in this car" or "should I take a call from this number"?
The average lifetime of a sex worker's phone is about three weeks, so they
tend to buy the cheapest feature phone they can get rather than any kind of
smart phone.  In other words high-powered crypto protocols aren't going to cut
Finally, the checking mechanism should be done in a manner that makes
enumeration ("which of these car license places in the parliamentary car park
is on a blacklist of people who abuse sex workers?") difficult.  The UK used
to have a rather neat system where you could do live checking, but when a
client threatened legal action claiming that the fact that their name was on a
blacklist was defamatory, it was shut down, or at least reverted to offline
checks only (it seems unlikely that they would have actually pursued the case,
but the threat was enough to get it shut down).  So this aspect doesn't have
to be cryptographically secure, merely good enough that it can't be misused by
the media to say "politician X is definitely confirmed to be someone who
abuses sex workers".  This is also something that makes it slightly unusual
for a crypto protocol, none of it is black-and-white, both the problems and
the solutions are fuzzy and more tied to social factors rather than hard
In any case, that's the problem.  The purpose of this post isn't so much to
try and point out a solution but to publish details of an actual real-world
problem that crypto could be applied to.

@_date: 2017-03-12 11:41:58
@_author: Peter Gutmann 
@_subject: [Cryptography] Crypto best practices 
I assume you mean GCM there, the most popular AEAD mode.  So you're suggesting
switching from a mode that has some relatively low-impact, obscure issues
(various oracle attacks) to one that fails catastrophically if you get
it wrong.  That seems like a giant move backwards in terms of safety.
GCM is RC4 all over again, and look how well that turned out.

@_date: 2017-03-13 00:57:28
@_author: Peter Gutmann 
@_subject: [Cryptography] Secret Handshake problem. 
That won't work, the clients will never consent to that.  The reason why the
average lifetime of a sex worker's phone is about three weeks is a combination
of them being misplaced/left behind during work, and the clients grabbing them
and throwing them out the window of the car or otherwise destroying them in
order to cut down on the chances of them being tracked or being used as
evidence against them.  Asking them to carry a "I frequent prostitutes" token
is pretty much the opposite of what they're after.

@_date: 2017-03-14 22:41:53
@_author: Peter Gutmann 
@_subject: [Cryptography] USB firewall/condom HW/SW 
Yes, that the winning side had damn good propagandists.  Rightful heir to the
rulership/stewardship of Gondor dies under mysterious circumstances with no
witnesses, blamed on "orcs", current ruler dies under mysterious circumstances
by setting himself on fire, no reliable witnesses, throne is claimed by some
guy no-one's ever heard of who pops up out of nowhere waving a magic sword no-
one's ever seen before, all orchestrated by a dodgy hippie in a white cloak.
(Ref: "The Last Ringbearer", Kirill Eskov).
What were we discussing again?

@_date: 2017-03-15 11:19:16
@_author: Peter Gutmann 
@_subject: [Cryptography] Crypto best practices 
It's not "like a stream cipher", it's a KSG like RC4 is.  And the main problem
with RC4 isn't the biases, it's that it's so easy to get wrong that in most
uses outside of mainstream crypto protocols (SSL and SSH), the standard
approach on seeing RC4 used was to look for two lots of ciphertext so you
could XOR them together and cancel out the keystream.  I loved RC4 back when
everyone had it as their favourite go-to cipher, it was easy to use and
equally easy to bypass.
The situation was sufficiently dire that Microsoft was recommending against
its use more than a decade ago ("your applications use of RC4 should be
reviewed by a cryptographer. This last point is very importantthere are
numerous subtle errors that can arise when using stream ciphers such as RC4.
Best Practices: The RC4 stream cipher should be used with extreme caution, and
any use of the algorithm should be reviewed by a cryptographer").  That's for
legacy code, for new code the rules for which stream cipher to use was
"Noneuse a block cipher".
The fact that if you used the example CryptoAPI code for RC4, or probably any
code anywhere (Google rc4 encryption code example), you were going to get it
wrong, probably helped in this rule-making.  No need to perform actual
cryptanalysis, just XOR any two ciphertexts.
AES-CTR, and by extension AES-GCM, have exactly the same problem, if you use
them in their most straightforward modes where you memcpy() in a fixed or all-
zero IV, you've got RC4 again.

@_date: 2017-03-15 23:04:06
@_author: Peter Gutmann 
@_subject: [Cryptography] Crypto best practices 
It actually diminishes security because it makes it impossible to connect to
an OpenSSH server using a standards-compliant implementation.  My code now
checks for this and returns an error message about OpenSSH being non-
compliant with the core SSH RFCs and sorry, I can't connect to that.  Which is
kinda crazy, that the de facto reference server implementation isn't compliant
with the standard that it's used as the reference for, unless you specifically
reconfigure it to work properly.

@_date: 2017-03-15 23:06:28
@_author: Peter Gutmann 
@_subject: [Cryptography] Fw:  Crypto best practices 
Forwarded on behalf of the original poster, who can't post directly.
Sent: Thursday, 16 March 2017 07:17
On 2017-03-15 17:15, "cryptography on behalf of Dennis E. Hamilton"
Of course a fixed IV undermines any scheme, just with different
consequences. If you use, say, CBC with a fixed IV, what you get is that
equal plaintexts (or equal plaintext prefixes) get mapped to equal
ciphertexts. If you use RC4, CTR, or CGM with a fixed IV, you get THE SAME
KEY STREAM and you can undo the ENTIRE key stream on the ENTIRE entire
message, not just the equal prefix.
Thats what Peter meant with no cryptanalysis necessary.
Thats not to say that CTR, GCM and so on arent useful, but the question
is, if RC4 is banned (presumably not only because of its biases, but also
because of the IV reuse problem), then CTR and GCM should also be banned
because they suffer from the exact same problems.
(This message will probably bounce form the list because my employer
doesnt allow me to use my preferred email address in From: fields, but
feel free to forward this mail to the list if you think its useful.)

@_date: 2017-03-16 23:23:48
@_author: Peter Gutmann 
@_subject: [Cryptography] NSA says China's supercomputing advances put US 
The tl;dr version:
Fnord fnord China fnord supercomputers fnord more money fnord.  Did we mention
China fnord?  And MORE MONEY?
PS: More money!

@_date: 2017-03-17 00:37:53
@_author: Peter Gutmann 
@_subject: [Cryptography] Crypto best practices 
+1 to all of that.  It's the "RC4 all over again" thing, we have about two
decades of experience showing that if you give J.Random coder a stream cipher
to use for data encryption, they're probably going to get it wrong.  Even
experts get it wrong, e.g. tarsnap (although in that case I'm not sure if the
lesson is "stream ciphers are too dangerous to use" or "CTR mode is too
dangerous to use").  There are far safer alternatives to stream ciphers
around, in this case meaning "pretty much everything that isn't a stream

@_date: 2017-03-17 03:17:25
@_author: Peter Gutmann 
@_subject: [Cryptography] Crypto best practices 
There's a whole pile of them, my favourite being OCB, but they're pretty much
all patented.

@_date: 2017-03-19 03:33:37
@_author: Peter Gutmann 
@_subject: [Cryptography] Crypto best practices 
That's one of the big problems, no-one can agree on what the required
behaviour is.  For example in theory we should all be using the theoretically-
perfect, provably-secure encryption system of one-time pads, but they're so
hard to get right that no-one would ever suggest that.  Instead, we use close-
to-theoretically-perfect but incredibly brittle modes like CTR and GCM, which
have the same failure mode as OTPs but now it's OK because look at all the
elegance and mathematics and stuff!  Only very recently has there been any
interest in misuse-resistant crypto, but even then it's things like GCM-SIV
that inherits the brittleness of CTR mode (via GCM) but adds an inability to
use it in streaming mode because you need to make two passes over the data.
I'm willing to trade off a little bit of security in exchange for robustness,
because my code has to work in harsh environments and I can't afford to have
the first woodpecker that comes along destroy civilisation.  So my "required
behaviour" is "as secure as possible provided it doesn't compromise
robustness", which seems to be rather different from many people's "the
underlying hardware and software and developers work flawlessly, make it as
theoretically perfect as you can assuming completely error-free

@_date: 2017-03-20 01:37:23
@_author: Peter Gutmann 
@_subject: [Cryptography] Crypto best practices 
It depends on which side of the agency you're on.  If you're using crypto to
protect TAO-style stuff, as the Vault 7 stuff appeared to be, then your
concern isn't to have it super-secure but to have it detected.  In fact the
last thing you want to use is some super-secure classified technique, because
at some point your gear is going to be discovered and reverse-engineered (or,
in this case, leaked), and you don't want to risk using your best tech to
protect the exfiltration of Putin's laundry list.
Instead, you'd probably be following the rule about what sort of weapons to
use when staging an insurrection: whatever can use the same ammunition as the
government troops, and that sounds identical when fired to government weapons
(unless you're planning to draw them into an ambush with a fake firefight).
So make your exfiltration tunnels look identical to SSL as done by IE or
Chrome, or dress them up as video or VoIP streams, or whatever.
Going even further, lets say Vault 7 used DES in ECB mode.  That's not such a
good idea not because it's weak, but because no-one else uses it so it'd stick
out like a sore thumb.  However, ignoring that, what would happen?  What would
be the threat to a TAO-style operation from using DES in ECB mode?
(Before anyone says "a sufficiently-resourced attacker could decrypt it",
think about what else would be necessary before you can get to that point, and
what the whole point of a covert exfiltration operation is).

@_date: 2017-03-20 01:45:07
@_author: Peter Gutmann 
@_subject: [Cryptography] Crypto best practices 
It fails if you don't increment the counter, so it's one glitch or one typo
away from failure, as has already happened in a crypto app written by an
experienced cryptographer, a single-character typo broke it completely.
(We don't know how many more of these issues are out there.  The RNG bugs in
both PGP classic and GPG were present for around ten years before anyone
looked at the code and noticed them).
Perhaps people designing exotic high-speed link encryption hardware, but the
masses who use it don't even know it's parallelizable, let alone choose it for
that.  The near-universal justification for AES-GCM/CTR use I've seen is that
it's trendy.  Not "use it because it's parallelizable", or "use it because it
has mathemtical property XYZ", but "everyone knows you should use AES-GCM, why
are you still using [not AES-GCM]"?

@_date: 2017-03-21 06:22:34
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Crypto best practices 
It's not too hard to come up with a Rube-Goldberg mechanism that deals with
particular issues like misuse of IVs or streaming, but the problem with these
schemes is that they take the initial issue, that IVs are too confusing and
complex to deal with so people get it wrong, and make the problem ten times
worse. The reason why RC4 was so popular is because it's incredibly simple,
it takes a key and magics plaintext in-place into ciphertext and back again
with no message expansion or other complications. I'm not saying that you can
do the equivalent of RC4 in a sound manner, but if you've got something that
gets misused because of its complexity then the proposed replacement shouldn't
involve even more complexity.

@_date: 2017-05-02 23:46:55
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Escrowing keys 
I would say it's more:
Availability && ( Integrity >> Confidentiality )
with the '&&' in the shell-script sense, i.e. first you ensure availability,
then once that's done you move on to any other stuff. Sean Smith gives a
great example in his newest book of a vendor who advertises that their
hardcoded default password is more complex than other vendors'. This make
sense, you don't want availability impacted by lack of access, but if you've
got a hardcoded default password then making it complex protects you from
endless random scanning attacks, you have to actually perform a targeted
attack to get in.
(I'm sure several readers' heads will have exploded trying to think that one
through :-).

@_date: 2017-05-21 10:47:44
@_author: Peter Gutmann 
@_subject: [Cryptography] Password rules and salt 
Hashing functions for password processing have very different requirements
than for digital signatures.  There may not be any end-of-life for hash
functions used to obscure passwords, you could still be using MD4 without any
OK, there's the speed thing, but given that you can get ASICs to do SHA-2 at
insane rates MD4 might even be the safer bet against brute-force attacks.

@_date: 2017-11-01 04:18:43
@_author: Peter Gutmann 
@_subject: [Cryptography] MITM company acquires MITM tools 
This came up on another list, the Comodo CA and all of the roots it controls
(which include a pile of other CAs unrelated to Comodo that it's bought up
over the years) was recently acquired by Francisco Partners:
who also have a stake in SonicWall, "the leader in Deep Packet Inspection
(DPI) and we've got a lot going on in that space", Procera (same), and
formerly had a stake in Blue Coat, whose products have been used by repressive
regimes against their citizens.
It's amusing that a perfect mechanism for performing MITM attacks is now
controlled by a company who has other arms that actively perform MITM attacks.

@_date: 2017-11-03 01:12:03
@_author: Peter Gutmann 
@_subject: [Cryptography] How Google's Physical Keys Will Protect Your 
And that, generalised to be non-Google-specific, is why hardware-token 2FA has
failed to achieve any penetration despite twenty years of effort, and why it
will continue to fail to achieve any penetration for the rest of eternity.
It's also why companies shell out a fortune for SecurID gear, because they're
the closest 2FA you can get to the most usable authentication mechanism there
is, passwords (see e.g. "The quest to replace passwords: A framework for
comparative evaluation of web authentication schemes").

@_date: 2017-11-13 07:52:00
@_author: Peter Gutmann 
@_subject: [Cryptography] Is ASN.1 still the thing? 
Canonicalisation doesn't work in any format, even ASN.1.  Having said that, it
works vastly less in something like XML than it does in ASN.1.  In either case
though, "there is only one encoding rule and that is memcpy()" (me, years
ago).  In other words whoever produces the data decides how it's encoded, and
everything else memcpy()s the encoded blob around without trying to do any
canonicalisation or re-encoding.  Anything else and you're setting yourself up
for a lifetime of patching to handling every new mis-encoding that turns up.

@_date: 2017-11-14 04:42:37
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Is ASN.1 still the thing? 
[Several replies condensed into one]
What experience did you have that makes you say that?  My code runs on a ton
of embedded stuff and there's no problems with parsing it, as a stream or
That's because Google inhabit their own private universe, and can decree that
whatever they've dreamed up this morning be a market winner, whether it's
actually a good idea or not.
This is actually answering entirely the wrong question, it's not "does it work
for Google", it's "does it work for anyone who isn't Google"?  The answer, all
too frequently, is "not really".
Lest God come down and smite you, for we all know how seriously He/She/It
takes ASN.1 parsing.
A bigger question should be "what is the OP trying to achieve here"?  While
everyone's debating whose pet serialisation format is the most cromulent, it's
not clear to me that any attempt at canonical serialisation at all is a good
idea.  If the goal is to sign something then the only serialisation you need
is "start hashing after this point" and "stop hashing before this point".
Anything else dooms you to a lifetime of pain, no matter what format you
decide to use.
If you really want something simple, lightweight, and straightforward, what
about XDR?  That's pretty well defined, and dates from a time when primary
design goals were that stuff be efficient and easily implementable, rather
than as complex as you can make it, or as hip as possible, or with angle
brackets (some stuff encompasses all three of those).

@_date: 2017-11-15 01:18:39
@_author: Peter Gutmann 
@_subject: [Cryptography] Is ASN.1 still the thing? 
Exactly, and that's why BER exists.  So saying "ASN.1 isn't streamable" is,
apart from the technical incorrectness (it's a notation, not an encoding
format), also wrong because BER was specifically designed to be streamable.
Contrast that with things like TLS and SSH, which don't have any form of
indefinite-length encoding at all (I'm focusing on security-protocol formats
here, not any encoding in general, thus the mention of TLS, SSH, etc).
In fact BER does a much better job than other indefinite-length encoding
formats like the PGP one, whose crazy fixed-point length format requires that
you quantise the data into awkward lengths (ones that don't correspond to
cipher block boundaries, for example) dictated by the limits of what length
value you can encode.  With BER, you just drop a continuation marker whenever
you feel like ending a block.
ASN.1 has a lot of design-by-committee junk in it (the date format, for
example), but BER and DER are pretty clean.

@_date: 2017-11-15 01:56:53
@_author: Peter Gutmann 
@_subject: [Cryptography] Is ASN.1 still the thing? 
So it's a reinvention of DCE/RPC/MIDL/DCOM/whatever from twenty-plus years
Getting off-topic, I know, but the point I'm making is that you don't need to
invent yet another new encoding for this sort of thing, there are more than
enough already available.

@_date: 2017-11-17 10:14:10
@_author: Peter Gutmann 
@_subject: [Cryptography] Is ASN.1 still the thing? 
Pretty much every implementer [0] knows this already, because things break if
you do it the way PKIX says.
The PKIX spec has a long and proud history of ignoring reality.  See many,
many discussions on the PKIX list, particularly in the period 2000-2005 or so,
on this.
It's really not the best example to choose if you're going to push for a
standard that reflects a view of the real world :-).
[0] I'm hedging my bets here, I should say "everyone" but I'm sure someone
    will drag up an example from North Korea or somewhere where someone wasn't
    aware of which bits of the PKIX spec you needed to ignore to make it work.

@_date: 2017-11-17 10:17:39
@_author: Peter Gutmann 
@_subject: [Cryptography] Is ASN.1 still the thing? 
How about "do a string search through the encoded cert data until you find the
OID for the public key, then extract the key components from the bytes that
follow and use those".  That's what you get when the spec mandates the use of
a full-blown PKI and the target hardware is a Cortex M0 or an MSP430.

@_date: 2017-11-26 07:49:12
@_author: Peter Gutmann 
@_subject: [Cryptography] Is ASN.1 still the thing? 
That's OK, it's never stopped a lot of RFC authors either :-).
What's the funny impact?  The EXPLICIT tag means that instead of the [0]
overriding the INTEGER, you put a [0] before the INTEGER tag.
What is odd is that it's made an EXPLICIT tag in the first place, since the
only thing this tells you is that the author wanted to waste two bytes in each
certificate.  That and the fact that later tags are a random mixture of
IMPLICIT and EXPLICIT indicates that this may be one of those RFCs (or at
least ISO specs) where at least some of the authors didn't know ASN.1 (there
are several PKIX RFCs where this is literally the case).
And that's now a completely different issue, the fact that the determined
programmer can write FORTRAN in any language.  In the case of data formats,
you can create a mess using any type of notation you feel like.  Look at the
TLS extension RFC, for example, and try and guess which extensions were
defined by PKI people trying to turn whatever the TLS encoding is called into
For a longer discussion of this problem, see the "ASN.1 Design Guidelines" of
the X.509 style guide, including this gem of how to encode a URL:
  SEQUENCE { [0] { [0] { SEQUENCE { [6] " } } } }

@_date: 2017-11-26 09:08:33
@_author: Peter Gutmann 
@_subject: [Cryptography] Is ASN.1 still the thing? 
Given that it doesn't parse or verify *anything at all*, you're open to every
kind of attack there is.  I don't know why you'd bother with a packet-in-
packet attack, since anything will work.
(Note that in at least some of these cases, there's no attack possible since
the public key is hardcoded.  The spec says you have to have a PKI so you have
a PKI, even when it makes no sense.  It's only when you try and verify some of
the certs and find that not only don't they verify but they're barely valid as
certs do you realise it's all just theatre).

@_date: 2017-11-26 11:16:00
@_author: Peter Gutmann 
@_subject: [Cryptography] WIPEONFORK in Linux 4.14 
It's actually not that useful unless you rewrite your code's memory handling
just so you can use it.  madvise() works on a granularity of page boundaries,
despite the appearance of working on arbitrary memory regions.  This means
that unless the start address is page-aligned, it'll fail.  In addition it
functions at a much lower level than malloc(), so if you madvise() malloc'd
memory you'll end up messing with your heap in ways that will break memory
allocation.  For example MADV_WIPEONFORK will wipe the entire page or page
range containing the heap that the client gets, corrupting the heap on fork.
What this means is that you need to mmap() memory in order to madvise() on it,
and then implement your own allocator on top of that.  Or, every time you
allocate anything, make it a full page, again via mmap().  The chances of
something going wrong when you do your own memory management are probably a
lot higher than the chances of something ending up in a core dump when you

@_date: 2017-11-29 07:03:37
@_author: Peter Gutmann 
@_subject: [Cryptography] Intel Management Engine pwnd 
It doesn't hook your network stack, it implements a layer 2 rootkit.  Traffic
intended for the ME gets intercepted by the ME before anything at a higher
level sees it.  See Arrigo Triulzi's (much more creative) work on doing this
in the NIC, e.g:

@_date: 2017-10-17 01:04:20
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: Severe flaw in WPA2 protocol leaves 
Is it?  While it's a cool attack, and yet another reason why RC4-equivalent
ciphers like GCM should be banned (we finally got rid of RC4, and now we're
busy reintroducing it under another name), it's actually kinda hard to
identify what real impact this will have on most users.  The publication of
equivalent vulns in WPS hasn't led to an orgy of compromises, for the typical
user it's just business^H^H^Hviruses as usual.
Not to mention that fact that it's a forever-day on most devices...

@_date: 2017-10-18 05:39:15
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: Severe flaw in WPA2 protocol leaves 
RC4 is a stream cipher for which key/nonce reuse results in a catastrophic
  failure of the cryptosystem.
  GCM is a stream cipher for which key/nonce reuse results in a catastrophic
  failure of the cryptosystem.
For the benefit of similar non-experts on this list, could you please point
out which cryptographers disagree with that?  Since the view that they fail
the same way is one that is not widely shared, there must be lots of names you
can cite to support this.
(The reason for asking for names is so I can avoid any cryptosystem they've

@_date: 2017-10-19 10:24:47
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: Severe flaw in WPA2 protocol leaves 
Sure, but that's not what happened here.  GCM failed catastrophically because
it was a stream cipher, just like RC4.  It doesn't matter if you use the
strongest algorithm in the world, combined with GCM it's going to break in
this case.
What KRACK shows is that, through the careful selection of cipher modes, you
can turn a very robust algorithm (AES) into a very brittle one (GCM).  In fact
if you'd used the canonical worst-case never-use-this AES mode, ECB (+ HMAC to
account for the "G" part in GCM), KRACK would have been a mostly academic
information-leakage attack rather than a complete cryptosystem failure like it
is.  Even a tiny step up, to 1970s-vintage CBC, would have made it even less
of an issue, a very minor information leak but no more.
Another interesting aspect of this, which Matt Green has gone into in more
is that this was a failure of a proven-secure handshake using an encryption
mode with multiple proofs, and yet in combination they failed
catastrophically.  Drew Gross once observed that:
  I love crypto, it tells me what part of the system not to bother attacking
but I have a different take on that:
  I love crypto, it's a pointer to where all the mistakes are being made
It makes an auditor's job so much easier.  When I'm asked to look at a
security product or device, I look for the crypto, and then look at how it's
being used, because that's where I'll find the holes.  For a stream cipher,
I'd first look for places where there's key/nonce reuse, and then if I don't
find any (I often do, thus the "RC4 all over again", because I saw the same
stuff with RC4 in the 1990s, long before anyone knew of weaknesses in RC4
itself) I look for ways to force key/nonce reuse.  Job done, and it doesn't
matter what the actual algorithm was they were using.
Speaking of provable security, I would love to get a statement from both NIST
and the CC evaluators as to how a device with multiple FIPS and CC evaluations
stretching over many years could have a broken RSA key generator, the very
thing that the evaluation is meant to check.  You can't even get a FIPS level
1 without having the RSA keygen validated, so how did this happen?  Will
Infineon's products now be decertified like OpenSSL was?  Will there be an
investigation as to how a broken product passed its FIPS and CC
certifications, to make sure this doesn't happen for other products?  In fact
given that the certification doesn't seem to be able to catch this, how will
we have any confidence in the keygen of other evaluated products?

@_date: 2017-10-19 10:38:24
@_author: Peter Gutmann 
@_subject: [Cryptography] Severe flaw in all generality : key or nonce 
It depends on what you mean by "acceptable".  As I mentioned in my previous
message, CBC turns a catastrophic failure (GCM) into a minor information leak,
and is pretty much universally deployed in crypto libraries and hardware (CBC
is actually remarkably abuse-tolerant for something that wasn't designed for
this role).  So you've already got something right now that's probably good
enough in most cases.
If you want something that's fully reuse-tolerant then there are a pile of
very recent modes and mechanisms that typically make two passes over the data
and use the output from the first pass as a randomiser for the second pass
(although the first use of such modes dates back to at least the early 1990s).
Downside is that there's almost no support for them in anything, and you need
to make two passes, which both slows things down and makes streaming

@_date: 2017-10-19 23:24:08
@_author: Peter Gutmann 
@_subject: [Cryptography] Severe flaw in all generality : key or nonce 
And what is that reason?  The standard just says "An implementation of MAC
Security that claims full conformance to this standard shall implement the
mandatory Cipher Suites [...] GCM-AES-128", no rationale is given.  AFAIK the
reason why GCM was used was because it was trendy, any other encrypt+MAC
mechanism would have done just as well.

@_date: 2017-10-23 02:37:17
@_author: Peter Gutmann 
@_subject: [Cryptography] Severe flaw in all generality : key or nonce 
(I realise this is a trivial "me too" response, but that's pretty much the
right answer to this question).

@_date: 2017-10-25 01:28:58
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Response to weak RNGs in Taiwanese and 
What I'd like to see is a response from the organistions who certified them as
secure.  I've already posted this as part of a longer message last week, but
I'll re-post it to make it an open question to both NIST and the CC labs, and
in particular it's apropos right now due to the failure of yet another must-
be-used-for-FIPS-certification RNG (up until 2016):
(as Matt Green points out, it's been known for nearly two decades that that
generator has issues, which is why I used it as a postprocessor for the actual
RNG in my code.  So the NIST version gets certified and the actual RNG
provides the security, which is a pretty nonsensical situation to be in).
Anyway, the open questions to NIST and the CC labs:
  How could a device with multiple FIPS and CC evaluations stretching over
  many years have a broken RSA key generator, the very thing that the
  evaluation is meant to check.  You can't even get a FIPS level 1 without
  having the RSA keygen validated, so how did this happen?
  Will Infineon's products now be decertified like OpenSSL was?    Will there be an investigation as to how a broken product passed its FIPS
  and CC certifications multiple times to make sure this doesn't happen for
  other products?
  Given that the certification doesn't seem to be able to catch issues like
  this, how will we have any confidence in the keygen of other evaluated
  products?

@_date: 2017-10-26 23:53:35
@_author: Peter Gutmann 
@_subject: [Cryptography] Response to weak RNGs in Taiwanese and Estonian 
Ah, you need to distinguish between the X9.31 RSA keygen and the X9.31 RNG,
which is just the X9.17 RNG recycled.  Matt Green's work attacked the X9.31
RNG (I prefer to think of it as the X9.17 RNG, which is what it really is, and
in the context of wholesale banking key management it's perfectly adequate,
pointing out the dangers of cargo cult security design), while the ROCA
weakness presumably targeted the X9.31 RSA keygen.

@_date: 2017-10-30 08:08:37
@_author: Peter Gutmann 
@_subject: [Cryptography] How Google's Physical Keys Will Protect Your 
Isn't it cute how, when the Google children rediscover something that's been
around for decades, it makes the NY times?
Somewhat less snarkily, is there anything actually novel here, or is it just
really old news that's new again because the term "Google" is attached?  I
can't see anything to get excited about.  It's not even "Google's Keys", it's
someone else's stuff that Google has adopted.

@_date: 2017-10-30 23:29:46
@_author: Peter Gutmann 
@_subject: [Cryptography] Response to weak RNGs in Taiwanese and Estonian 
That's not the RNG, it's the prime/RSA keygen.  However, as you say, if the
primes have the form given in section 2.1 then they can't have been generated
using the X9.31 mechanism, which first defines Miller-Rabin and Lucas tests
(B.2 and B.3) and then defines how to use those to generate the RSA values
(B.4).  An attempt at providing it as ASCII is (for comparison with the
Infineon method):
  To generate a prime p (or q) of 512 + 128s bits,  for some s = 0, 1, 2, 3,
  perform the following steps.  First p will be generated, followed by the
  generation of q.
  Step 1: To generate p, set Xp = a random number in the range [  2 (2511 +
  128s),  (2512+128s) - 1].  This shall be done using a random number
  generator (RNG) or pseudo random number generator (PRNG) algorithm specified
  in an ANSI X9 standard (see Appendix A: Random Number Generation), along
  with an appropriate seed.  Likewise, to generate q, set Xq = a random number
  in the same range.  If the absolute value of the difference |Xp - Xq|
  exceeds 2412+128s, proceed to Step 2.  If the absolute value does not exceed
  2412+128s, keep regenerating new values of Xq until the difference does
  exceed 2412+128s.  This ensures that |p - q| is sufficiently large to guard
  against Fermat style factoring attacks, (see Appendix C: Security
  Considerations for details).
  Step 2: Randomly generate four 101-bit primes, p1, p2, q1, and q2 by
  randomly generating four positive 101-bit integers Xp1, Xp2, Xq1, and Xq2.
  Sequentially search successive odd integers starting at Xp1 until the first
  prime, p1, is found by using A.2 to do the primality testing. Repeat this
  process to find p2 starting the search at Xp2, q1 starting at Xq1, and q2
  starting at Xq2.  Thus, p1 is the first prime larger than Xp1, p2 is the
  first prime larger than Xp2, q1 is the first prime larger than Xq1, and q2
  is the first prime larger than Xq2.
  Step 3: Apply the Chinese Remainder Theorem (twice) to compute:
  R1 = (p2-1 mod p1) p2  - (p1-1 mod p2) p1 .  Let  Yp0 = Xp + (R1 - Xp mod p1
  p2).  Yp0 is now the first integer greater than Xp  such that p1 is a large
  prime factor of YP0-1 and p2 is a large prime factor of Yp0+1.
  R2 = (q2-1 mod q1) q2  - (q1-1 mod q2) q1 .  Let  Yq0 = Xq + (R2 - Xq mod q1
  q2).  Yq0 is now the first integer greater than Xq  such that q1 is a large
  prime factor of Yq0-1 and p2 is a large prime factor of Yq0+1.
  Step 4: Check the sequence of p and q candidates such that:
  If e is odd:
  ? check the sequence of p candidates Yp0, Yp0+p1 p2, Yp0+2p1 p2, Yp0 + 3p1
  p2  to see if the public key exponent GCD(e, p-1) = 1.   If so, apply the
  primality tests in Appendix B: Generation of Parameters for rDSA until the
  first prime is found.  This shall be p.
  ? check the sequence of q candidates Yq0, Yq0+q1 q2, Yq0+2q1 q2, Yq0 + 3q1
  q2  to see if the public key exponent GCD(e, q-1) = 1.   If so, apply the
  primality tests in Appendix B: Generation of Parameters for rDSA until the
  first prime is found.  This shall be q.
  If e is even:
  ? check the sequence of p candidates Yp0, Yp0+(8p1 p2), Yp+2(8p1 p2),
  Yp0+3(8p1 p2)  to see if the public key exponent GCD(e, p-1 2 ) = 1.   If
  so, apply the primality tests in Appendix B: Generation of Parameters for
  rDSA until the first prime is found.  This shall be p.
  ? check the sequence of q candidates Yq0, Yq0+(8q1 q2), Yq+2(8q1 q2),
  Yq0+3(8q1 q2)  to see if the public key exponent GCD(e, q-1 2 ) = 1.   If
  so, apply the primality tests in Appendix B: Generation of Parameters for
  rDSA until the first prime is found.  This shall be q.
As you point out, based on the form of their output this is nothing like what
the Infineon devices are doing.
Which leads (again) to the obvious question, how did these devices repeatedly
pass their security evaluations?

@_date: 2017-10-30 23:42:41
@_author: Peter Gutmann 
@_subject: [Cryptography] How Google's Physical Keys Will Protect Your 
I can't see how it will, given that Google is doing exactly the same thing
that every man and his wombat has already tried, and failed at.  In particular
use by organisations [0] like banks and Paypal, where there's real financial
value at stake, has failed to gain any significant adoption after a 1-2
decades (does the Paypal security key, the physical device not the "our
security key is now a phone app", still exist?  They seem to have removed most
mention of it, or redirect you to the app "security key").
[0] By "use" I mean general public adoption, not "we require our employees to
    use this, on pain of pain".

@_date: 2017-11-01 03:57:14
@_author: Peter Gutmann 
@_subject: [Cryptography] Severe flaw in WPA2 protocol leaves Wi-Fi 
============================== START ==============================
For those who aren't familiar with this, it's a cool thing that OpenVPN has
done for ages, they call it tls-auth.  Before you can even start connecting,
you need to authenticate yourself with an HMAC'd exchange, which protects
against attacks on the underlying OpenSSL or other parts of the protocol.

@_date: 2018-04-10 04:56:25
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED]  Will We Ever Learn? 
I would say UPnP has been flawed since the early 1990s, in that the entire
protocol is a security flaw.  Its sole security "feature" is that it relies on
UDP and you can't do that from Javascript in a browser, but otherwise anything
on your network that can talk UDP (for example a random app on your phone) has
complete control of a UPnP device.  Having a router suddenly open up ports to
sites in China when you plug in a webcam, as normal functioning of the UPnP
protocol, is something that really shouldn't be happening.

@_date: 2018-04-16 03:17:54
@_author: Peter Gutmann 
@_subject: [Cryptography] Claimed Breakthrough Slays Classic Computing 
Let's wait for the third lecture, the publication of his paper(s), and the six
to twelve months it usually takes for someone to either find the flaw in the
proof or show that it can't be mapped onto the factoring problem in a manner
more efficient than solving the factoring problem directly.

@_date: 2018-08-05 09:01:40
@_author: Peter Gutmann 
@_subject: [Cryptography] Perfect Integrity? 
Depends on the circumstances.  Lets say the MAC is being used as part of an
alarm circuit, where a keepalive is sent across the circuit every 50ms, with a
1-bit MAC attached.  The attacker would have to guess the bit, then 50ms later
guess the next bit, then 50ms later guess the next one, etc.  Get a single bit
wrong and you trigger the alarm.

@_date: 2018-08-15 00:15:58
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re: Throwing dice for "random" numbers 
Or you could just roll 16 of these and read the results out directly:
Also of interest, trying to connect to the given URL:
with Firefox (Chrome and others connect OK [0]) gives:
  An error occurred during a connection to wizardofodds.com. Peers
  Certificate has been revoked. Error code: SEC_ERROR_REVOKED_CERTIFICATE
Who needs site outages and server crashes when you've got the web PKI to
emulate the failure for you.  What's worse, I can't connect to that site at
all (with Firefox) because the browser has decided that since the web PKI has
deemed the site not worth of being connected to, I'm not allowed to see it in
any form.
[0] Yeah, OK, I'm one of the three Firefox users left.  I met one of the
    others at a conference once.  I remember him because of the purple hair.

@_date: 2018-08-29 21:36:04
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Is "perfect forward secrecy" the 
When you read a headline like this, you need to be able to translate it into
plain English to see what it's really saying.  When you see "quantum
computers" or "quantum cryptanalysis" you should mentally substitute what's
really meant, "magic" [0].  So the headline becomes:
 IBM warns of instant breaking of encryption by magic: 'Move your data today'
Without even reading the linked article, I already know what it continues
with: It'll be peddling a magic-resistant cryptosystem invented and patented
by IBM that we all need to move to.
[0] If you don't believe in magic then substitute instead space aliens or
    something involving the pyramids.

@_date: 2018-08-30 17:54:19
@_author: Peter Gutmann 
@_subject: [Cryptography] WireGuard 
I looked at it a while back and pretty much agree with the quoted paragraph
above, it's a very nice design.  A good independent analysis is:

@_date: 2018-08-31 14:43:19
@_author: Peter Gutmann 
@_subject: [Cryptography] WireGuard 
I wasn't going to respond to this because it's so obviously wrong, but since
someone else has now quoted it in a reply I'll comment on it...
What this should say is:
  Crypto algorithms have relatively short fashion lifespans.
I can take a set of algorithms that are between twenty-five and forty-five
years old, all dating back to the dawn of history in terms of modern
cryptography, and apply them to a perfectly secure crypto protocol [0].  Just
because it's fashionable to switch to the trendiest new algorithms every few
years doesn't mean the existing ones are any less secure, they're just not
trendy any more.
We know, from years of experience with this, that the more flexibility you
build into your protocol, the more problems you'll see with it.  Wireguard is
a manifestation of Grigg's Law, "There is only one mode of operation and that
is secure".  There aren't fifteen different modes, twenty-six algorithms,
sixteen mechanisms, eighteen protocol negotiation options, and thirty-five
handshake options and systems, all of which may or may not interact
destructively and 95% of which have never been tested or examined because
everyone only uses a stereotyped tiny subset [1], until someone comes up with
an attack that uses the 95% that no-one ever did anything with, but that were
nevertheless regarded as absolutely mission-critical when the protocol was
In contrast with a Grigg's-Law design you can be pretty sure that the one mode
that's there has been examined and beaten to death from every possible angle,
because everyone has to look at, and work with, that one mode.
[0] Well, at about the third or fourth version, once people had pointed out
    the slip-ups in the first few versions.
[1] This pretty much describes SSH, although I'm sure IPsec has the same
    problem.

@_date: 2018-02-16 11:00:59
@_author: Peter Gutmann 
@_subject: [Cryptography] Quantum computers will never overcome noise 
I don't think I'll be subject to compromise via quantum cryptanalysis (QCA) in
any foreseeable time period (ten years / twenty years / thirty years / my
lifetime / whatever [0]).  However, I, and everyone on this list, will be
compromised by any kind of non-QCA method you care to name (buffer overflow,
XSS, SQL injection, etc).  Again and again, endlessly [1].
And that's the problem with QCA, cryptographers have run out of things to do
with conventional algorithms (look at SHA-3 as an example, it's failed to
launch because the nearly 20-year-old SHA-2 is still good enough), while the
advent of QCA has provided a convenient excuse to spend the next ten to twenty
years publishing conference papers that would never have been considered
otherwise.  Is QCA a real threat or just a convenient bogeyman to justify lots
of new conference and journal papers that wouldn't otherwise be publishable?
In the meantime, while everyone's fixated on patching the theoretical
mousehole in the corner, they're conveniently avoiding addressing the fact
that entire walls of the barn are missing elsewhere.
[0] This is somewhat optimistically predicting that "my lifetime" >>
    $current_date + "thirty years".
[1] And before you say "I run xyzOS and I'm very careful with what I do, I'm
    safe", how about every web site and organisation you interact with, or
    don't interact with but that has data on you?  Think Equifax, for example.

@_date: 2018-02-28 21:45:11
@_author: Peter Gutmann 
@_subject: [Cryptography] CA reseller emails 23, 
In case anyone missed this, CA reseller Trustico recently emailed 23,000 of
its client's private keys that it generated and held for them to the CA that
issues certificates for it.
I think 20 January 2017 was the last time I saw so much fail packed together
in one place.

@_date: 2018-01-06 00:52:33
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Speculation considered harmful? 
It's actually a pretty poor fit for almost anything.  I think you're failing
to appreciate just how puny these cores (and their associated infrastructure)
really are.
If you do happen to have an embarrassingly parallel job, nVidia (or more
generally GPGPUs) have had the hardware for that for years now.

@_date: 2018-01-09 08:10:14
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Speculation considered harmful? 
I think you mean:
  VLIW isn't just wide microcode.  It's a combination of wide instructions
  with compilers that no-one can ever quite manage to write that take
  advantage of them.
There's a good reason why vendors went deep rather than wide to make things go
fast, it was, and still is, the easiest way to get performance.

@_date: 2018-01-17 01:31:24
@_author: Peter Gutmann 
@_subject: [Cryptography] Opinions requested on Sample Root Certificate 
What sort of comments were you looking for, apart from "it looks like a
certificate"?  In particular if you want comments on any details you'll need
to provide a better breakdown of what's in there, for example using dumpasn1.

@_date: 2018-01-17 02:18:32
@_author: Peter Gutmann 
@_subject: [Cryptography] Opinions requested on Sample Root Certificate 
I don't think it matters, any modern algorithm will be fine, the failures
won't happen there but everywhere else.  A specific answer will depend more on
what you want to use it for, do you need to interop with stuff that can't do
ECC, are there regulatory constraints, do your customers prefer A to B, etc.

@_date: 2018-01-25 10:46:12
@_author: Peter Gutmann 
@_subject: [Cryptography] RISC-V isn't the answer 
How are you going to convince memory manufacturers to add the extra bits to
their memory chips?  Or memory access controller designers to design custom
controllers to deal with the extra bits?  Or customers to pay the same rate
for 10% (or whatever) less usable RAM?

@_date: 2018-07-12 05:02:10
@_author: Peter Gutmann 
@_subject: [Cryptography] "Public Accountability vs. Secret Laws: Can They 
It's not a lack of common sense, it's what you need to do to get a paper published.  If you have a neat idea you can't just publish it because it's
a neat idea, you have to have some problem for it to solve.  So you invent
a problem and then publish your idea as the solution to it [0]
A great example of this happened a few years ago when some researchers came
up with a cool idea for something cryptographic (details fudged to hide
identities).  While it was kinda neat, there was no practical application
for it, so they invented a problem and published the paper as its solution.
The following year the same conference had three more papers submitted that
presented more efficient solutions for the nonexistent problem.  There may
well be an entire conference dedicated to it by now.
In any case, something like this isn't necessarily an artefact of the authors' level of common sense, it's an artefact of how academic publication
[0] You probably also need to add some sort of security proof, even if your
    paper is just a recipe for baking S-box shaped cookies.

@_date: 2018-07-23 08:17:10
@_author: Peter Gutmann 
@_subject: [Cryptography] Cryptokitties: When you've still got money to burn 
Dammit, why didn't I think of that?  Just as the bottom is falling out of
ICOs, someone comes up with a new way to extract money out of the gullible.
  1. Use actual real money to buy Ethereum.
2. Use Ethereum to buy a cryptokitty.
3. ??
4. ??
5. No, not profit, just more ??

@_date: 2018-06-19 06:13:02
@_author: Peter Gutmann 
@_subject: [Cryptography] How to make rowhammer less likely 
Another problem that encrypting or MAC'ing RAM does is that it turns an often
safely-ignored single-bit flip into a much harder to ignore 128-bit flip or
fatal MAC failure.  Most PCs run just fine with the occasional bit flip, but
they'll run a lot less fine when every one is amplified by two orders of

@_date: 2018-06-27 05:39:25
@_author: Peter Gutmann 
@_subject: [Cryptography] Fast-key-erasure RNG and fork()ing 
That's actually much less portable than pthread_atfork(), and also nowhere
near as clean.  With atfork you get immediate notification via a signal-like
mechanism that the process has forked, with INHERIT_ZERO you have to carefully
check your pool contents from each bit of code that accesses it to see whether
its suddenly gone to zero, as well as having to interact with, or at least be
aware of, the system's MMU in order to deal with the page level at which
things operate.
A less ugly way would be to have a page set aside that's filled with 0xFF's,
mark it as INHERIT_ZERO, and use that as a signal that you've forked, but even
that's still less portable and clunkier than atfork.

@_date: 2018-06-28 00:49:18
@_author: Peter Gutmann 
@_subject: [Cryptography] Wi-Fi WPA3 announced 
Another interesting feature is the fact that this may be the first encryption
system that goes to 10 1/2:
  "offers the equivalent of 192-bit cryptographic strength"
Since 128 bits is the benchmark 10 and going to 11 is 256 bits for when you
really need that factor of 340,282,366,920,938,463,463,374,607,431,768,211,456
extra safety margin, this one goes to 10 1/2.

@_date: 2018-03-01 23:59:06
@_author: Peter Gutmann 
@_subject: [Cryptography] CA reseller emails 23, 
It's now been pointed out that the server they were using to do this is
vulnerable to remote code exec as root.  For people who aren't doing so
already, this is definitely a soap opera worth following.

@_date: 2018-03-06 07:32:58
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] FOSS library recommendation for VB.NET 
Before we can answer, a question: If their policy prohibits the use of PBE (I
assume that means KEK-based mechanisms in general?) to encrypt passwords, what
are we supposed to use?  Does it need to be reversible?  If not, you're going
to run into a KEK at some point.

@_date: 2018-03-11 03:27:18
@_author: Peter Gutmann 
@_subject: [Cryptography] On those spoofed domain names... 
A general comment on this, these sorts of things are often only a problem if
you're relying on blacklists for security.  I know the PKI community pretty
much lives by blacklists as a security mechanism, but for everyone else the
problems can be avoided through the simple dictum _don't_ _use_ _blacklists_.
Using Unicode tricks to evade them is just one of a zillion instances where
they fail.  If your name check is a binary "compare( commonName, ASCII( " ) )" then one and only one name will
match.  If it's a vague "anything sort-of-OK is allowed except perhaps for
this shortlist of banned names" then you're always going to get spoofed.
My code ignores punycode.  One and only one name will ever match for " not a whole universe of lookalike names.  Even if the CA encodes
it in UTF-8 or UCS2/UCS16, " still won't match " I don't think this is the Unicode community's fault.  Their job is to figure
out how to encode glyphs, not to determine whether the visual Hamming distance
between glyph A and glyph B makes it unworthy of a codepoint.  It's entirely
the security software's fault if it enables this type of spoofing. I'm actually rather shocked there's still a browser that's vulnerable to this
after all this time.  Krebs did point out that none of the major browsers, IE,
Edge, Chrome, or Safari are vulnerable, I guess it's the also-rans around the
edges that are viable targets.  Interestingly, I've tried a few mobile
browsers including Chinese ones that you'd expect would go with punycode and
and none of them appear vulnerable.

@_date: 2018-03-13 03:17:20
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED]  Some Cryptocurrency Humor 
For anyone else who's discovered it's geoblocked, you can also see it on Dailymotion via the Grauniad:

@_date: 2018-05-08 00:53:25
@_author: Peter Gutmann 
@_subject: [Cryptography] Security weakness in iCloud keychain 
"Passwords are the worst kind of authentication mechanism, except for all the
 others".
Passwords aren't bad because they're inherently bad, they're bad because
security people have chosen to make them bad.  Everyone knows that they're no
good, so we won't put any effort into doing things properly.  Fifty-odd years
ago the state of the art in login security was that you connected to your
target system and handed over the password in cleartext.  Today, nearly half a
century later, the state of the art in login security is that you connect to
your target system and hand over the password in cleartext, but since it's
inside an SSH or TLS tunnel you get to pretend it's better.
All it would take as an initial step to make things vastly harder for the bad
guys is to exchange the hand-over-the-password step with any kind of password-
based challenge-response auth, circa late 1970s.  Then we could build from
See also "A Research Agenda Acknowledging the Persistence of Passwords":

@_date: 2018-05-15 01:08:26
@_author: Peter Gutmann 
@_subject: [Cryptography] Vulnerability found in badly broken email apps 
Unless your S/MIME implementation does authenticated encryption from 10+ years
ago, RFC 5083, in which case the gadget attack just bounces off.
Mind you, a mailer broken enough to auto-fetch images/auto-render HTML content
will also implement authenticated encryption as "Message tampering detected,
continue anyway?", default = Yes.

@_date: 2018-05-15 03:03:34
@_author: Peter Gutmann 
@_subject: [Cryptography] Rowhammer is now remote exploitable 
An interesting paper just turned up on arXiv:
Nethammer: Inducing Rowhammer Faults through Network Requests
[...] In this paper, we present Nethammer. Nethammer is the first truly remote
Rowhammer attack, without a single attacker-controlled line of code on the
targeted system. [...] Depending on the location, the bit flip compromises
either the security and integrity of the system and the data of its users, or
it can leave persistent damage on the system, i.e., persistent denial of
service. We investigated Nethammer on personal computers, servers, and mobile
phones. Nethammer is a security landslide, making the formerly local attack a
remote attack.

@_date: 2018-05-16 08:49:47
@_author: Peter Gutmann 
@_subject: [Cryptography] Vulnerability found in badly broken email apps 
If you're just after test vectors, here's one, password-encrypted with
password "test" (which saves having to do the "send me your public key" /
"which format do you want it in" / "I can't read that, what about ..." game):
begin 666 env_authenc.der
MAO<-`0D0`P\P0J`3!``"`0$P at AD
F2AI"L at 0
When it comes to mailers that implement it, I don't know of any.  That doesn't
mean none exist, merely that I don't know if any do it.  As with the PGP CFB-
attack paper from many years ago, maybe this will the the wakeup call that
gets mailers to finally support it.  In the same way that standard security is
typically by penetrate-and-patch, so mainstream crypto implementations often
go for the fix-it-after-it's-happened approach to these sorts of things.

@_date: 2018-05-17 05:35:59
@_author: Peter Gutmann 
@_subject: [Cryptography] Vulnerability found in badly broken email apps 
This is a real problem, and something I tried to address in RFC 6476:
The encrypt-then-MAC is already to some extent a problem because you can
ignore the MAC failure and decrypt anyway, but for a combined mode you
don't have any choice, you have to decrypt in order to perform the MAC
operation, and once the plaintext is sitting there it's very hard not to
act on it, with the MAC failure relegated to a token warning popup.

@_date: 2018-05-17 05:44:01
@_author: Peter Gutmann 
@_subject: [Cryptography] Attacks on PGP (and allegedly S/MIME) 
Just to confound things even further, the "encrypt everything" approach makes
this even worse. If you've only got sensitive, valuable email traffic
encrypted then you can afford to be careful with it, refuse to auto-render
HTML, follow links, and so on. However, if every piece of HTML-encrusted
gunk that turns up is also encrypted, you can no longer tell whether it's
something you want to isolate or not, and if you do isolate everything users
will switch to a different mailer that "works", in the sense that it
displays the HTML-encrusted gunk as intended.

@_date: 2018-11-21 05:03:00
@_author: Peter Gutmann 
@_subject: [Cryptography] Buffer Overflows & Spectre 
"Don't allow malicious, attacker-controlled code to run on the same CPU/CPU
cluster as your precious secret-containing code" would be a good start.
Maybe CPU vendors could break their existing products into two distinct lines,
one for people who think that sharing their CPU with code from
pavel at virusbucket.ru is a good idea (speculation disabled) and one for people
who don't (normal operation).

@_date: 2018-10-04 01:36:13
@_author: Peter Gutmann 
@_subject: [Cryptography] Elixxir 
Chaum was... not the easiest person to work with:
The government didn't have to intervene, or really do anything at all except

@_date: 2018-09-01 13:58:45
@_author: Peter Gutmann 
@_subject: [Cryptography] WireGuard 
Even then, you have to be very, very careful with that.  The TLS folks have
been struggling for years with anti-rollback mechanisms, it's really hard to
do them in a manner that isn't exploitable in some combination of
It'd be interesting to see a proper research paper on how to do anti-rollback
right, with full analysis and proofs to accompany it.  So far the mechanisms
have been mostly ad hoc, "this should probably do it unless someone
demonstrates otherwise".

@_date: 2018-09-01 20:53:46
@_author: Peter Gutmann 
@_subject: [Cryptography] WireGuard 
Grigg's Law, "There is only one mode of operation and that is secure"
A good overview of this, from someone who understands the issues with TLS very
well, is Adam Langley's essay on cryptographic agility:

@_date: 2018-09-03 14:48:58
@_author: Peter Gutmann 
@_subject: [Cryptography] WireGuard 
This was actually fixed with the (somewhat misnamed) Extended Master Secret,
EMS.  Admittedly you can still try and roll that back and it'll be detected,
but later on in the handshake process.

@_date: 2018-09-03 20:05:52
@_author: Peter Gutmann 
@_subject: [Cryptography] WireGuard 
It's an extension negotiated between client and server, the client doesn't
need to know whther the server supports it or not, and any server can deploy
it right now.

@_date: 2018-09-06 11:40:30
@_author: Peter Gutmann 
@_subject: [Cryptography] WireGuard 
That's a pretty accurate description of IPsec as seen by the typical sysadmin.
To establish an IPsec connection between two endpoints, you typically need
three instances of IPsec, one for endpoint A, one for endpoint B, and
OpenSWAN/Libreswan/whatever in debug mode to tell you why the other two can't
talk to each other.
IPsec's ideal runtime environment is a Powerpoint slide projector.  On
anything else, it's less than ideal.

@_date: 2018-09-21 18:29:39
@_author: Peter Gutmann 
@_subject: [Cryptography] Security proof for RSA PKCS #1 v1.5 
On the Security of the PKCS v1.5 Signature Scheme
Tibor Jager and Saqib A. Kakvi and Alexander May
  We present the first security proofs for RSA PKCS v1.5.  We prove full
  existential un-forgeability under adaptive chosen message attacks (UF - CMA)
  in the random oracle model.  This is the same security level that other
  practical signature schemes, such as RSA-PSS or RSA Full-Domain Hash
  provably achieve.

@_date: 2019-08-14 07:13:55
@_author: Peter Gutmann 
@_subject: [Cryptography] Well, that only took ten years 
Over the last few months, browsers have been quietly removing the UI bling
associated with EV certs in acknowledgement of a decade of data and research
publications showing that they have no effect on security, Chrome and Firefox
and Safari and Edge severely sidelining it.  Since EV was principally a CA
marketing technology, I've been trying to get comments from CAs on how they
feel about this, so far without response.
When EV was first introduced, various security people on this list predicted
it would have no effect.  Several also predicted, tongue-in-cheek, that after
EV failed there'd be EEV certificates, and then EEEV certificates, and then...
Or maybe like C -> C++ or D, they'll be EV+, EV++, and so on certificates.
In any case it'll be interesting to see what the next deckchair-rearrangement
in browser PKI will be.  Whatever it is, I'd like to take this opportunity to
predict in advance that it'll have no effect.

@_date: 2019-08-16 02:44:38
@_author: Peter Gutmann 
@_subject: [Cryptography] Well, that only took ten years 
In my earlier post, I mentioned that I'd been unable to get comments from any
CA on the change.  One has now commented:
  Yes, I work for a CA that issues EV certificates, but if there was no value
  in them, then our customers would certainly not be paying extra for them.
    - GlobalSign employee.
I've already added it to my quotes file :-).  In case you're wondering why I
find it amusing, consider this variant:
  Yes, I work for Monster Cable, but if there was no value in our cables then
  our customers would certainly not be paying extra for them.

@_date: 2019-08-18 05:19:19
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Let's not talk about DoH, or Well, 
It's also a classic example of "technology designed by geeks for geeks to
protect themselves against dangers that only geeks were worried about" (Phil
H-B).  Try explaining what it does to J.Random user, you'll initially get
blank looks and eventually bored looks if you persist (that's from real-world
Speaking not in my capacity as a co-moderator, I find discussion of issues
around security/crypto technology both interesting and useful.  In fact if we
had more discussion of this sort of thing we might end up with more
appropriate and useful crypto/security technology being deployed.

@_date: 2019-02-04 02:07:52
@_author: Peter Gutmann 
@_subject: [Cryptography] Crypto RNGs in 2019 
So I wanted to encrypt some files. Thought about using 7z+password.
  Stackexchange folks said "Didn't review it but it should be fine. You can
  browse the code yourself". So I did. After a few mins I noticed they use
  8byte "random" IV. Yes, half of IV is zeroes. But it gets worse.
  [...]
  I thought about reporting this at 7zip Sourceforge forums but then I vomited
  again when I saw a long thread of largely incoherent exchanges on how 7z
  should be using Twofish instead of AES-256 because obviously NSA backdoored
  AES back in 2001 didn't you hear
  The post closes with a good summary of how a lot of current crypto works:
  Seems typical of crypto/security code reviews in general - a bunch of folks
  fighting over which hipster encryption/hashing/signing algos to use, while
  overlooking the most obvious vulns and holes visible to anyone with half a
  brain and a few mins to spare.

@_date: 2019-02-08 00:52:39
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Schneier in Wired: There's No Good 
Yup, and it points out an important feature about trust in conventional
financial mechanisms: If something goes wrong with a conventional mechanism, I
trust that the bank or credit card company, or ultimately the courts and/or
government regulators, will sort it out.  That's worked for centuries.
The blockchain on the other hand is the car without seatbelts and brakes of
the financial system.  If something goes wrong with a blockchain-based
mechanism, I trust that I'm screwed.
Enthusiasm for blockchain seems to follow a pattern of being inversely
proportional to the enthusiasts knowledge of how large-scale financial systems
work (with the occasional exception of people who are aware of how they work
but have thrown in the blockchain because it'll get then funding).

@_date: 2019-02-12 06:46:57
@_author: Peter Gutmann 
@_subject: [Cryptography] [openpgp] AS2+OpenPGP protocol extension review 
Just a note on this, I don't know what the W3C's AS2 is but whatever it is
it's nothing like the real AS2, which is a secure EDI standard that's been
around for close to twenty years (there are actually several AS standards, but
the most widely-used one is AS2).  So if anyone goes looking for AS2 security
information, they're going to get a very different AS2...

@_date: 2019-02-15 03:19:08
@_author: Peter Gutmann 
@_subject: [Cryptography] [openpgp] AS2+OpenPGP protocol extension review 
Yeah, I think both AS2 and MLS, which someone else pointed out, have such a
long history of use in the security community that it'd be better to look for
non-conflicting terms.

@_date: 2019-02-17 00:57:07
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Practical Enclave Malware with Intel SGX 
Rule 37 of DRM technology: A technology created to be used against the user by
Hollywood will also be used against the user by everyone else.

@_date: 2019-02-19 16:26:23
@_author: Peter Gutmann 
@_subject: [Cryptography] Questions of taste on UDF presentation 
s/current attack vectors are shut immediately/
  C-specific attack vectors change to Java-specific attack vectors immediately/
Switching from $language_1 to $language_2 just means that all the attacks
specific to language_1 are swapped for ones from language_2.  A few years ago
(not sure what the current stats are but I assume it hasn't changed that much)
Java was the second-biggest source of 0day after Flash, which I'm not sure
it's actually possible to beat in terms of security vulns... well, WordPress
perhaps.  In particular, a lot of the Java vulns are in the JVM and similar,
which is an attack vector that doesn't even exist in C.

@_date: 2019-01-03 02:26:04
@_author: Peter Gutmann 
@_subject: [Cryptography] Security is the other person's problem 
Fascinating discussion on the OpenWall/GMP lists about the fact that GMP uses
assert() (or at least a homebrew ASSERT that ignores NDEBUG) for
parameter/argument checking rather than simply checking the parameters and
returning an error as you'd expect, thus writing sensitive data to coredumps
and possibly sending it to error reporting services if the OS does this.  The
general consensus seems to be that if this happens it's the user's fault for
passing in invalid parameters, having core dumps enabled, not catching
SIGABRT, not setting setrlimit, not using prctl with PR_SET_DUMPABLE or
ulimit, not disabling error reporting, and holding it wrong.  The thread
starts here:
There doesn't seem to be a thread index, but the following captures most of
Near the end of the discussion is this gem:
  A reasonable assumption is that this user has modified the sources to
  cause this bug.  The motive would be to support his auxesis about how
  insecure GMP is.
  Let's move on.  No bug to be found here.
I had no idea that Ulrich Drepper was also involved in GMP...

@_date: 2019-01-05 00:25:03
@_author: Peter Gutmann 
@_subject: [Cryptography] Blockchain without proof of work 
Or you could publish a paper on the IACR e-print archive,
 or arXiv, documenting what you've done to prevent others from claiming it.  Someone once
complained that the last page or two of a paper I'd written was "just a brain
dump of everything related to the work", which was exactly what it was
supposed to be, a description of a wide range of practical applications so
that no-one could take the theoretical part and render it into practice by way
of patents.

@_date: 2019-01-08 02:53:39
@_author: Peter Gutmann 
@_subject: [Cryptography] Came up with a weird use case, got questions 
Either there are two batteries and you swap the dead one out while the other
powers the anti-tamper, or there's a supercap and you get X days to replace
the battery.

@_date: 2019-01-11 00:53:38
@_author: Peter Gutmann 
@_subject: [Cryptography] SSH key text format: 
I think everyone who implements SSH has done this at one time or another.  In
brief, it's just the base64-encoded form of the SSH public key record.

@_date: 2019-01-11 08:54:07
@_author: Peter Gutmann 
@_subject: [Cryptography] Government shutdown: TLS certificates not 
Nevertheless, visitors are warned not to log in or perform any sensitive
  operations on these sites, as traffic and authentication credentials aren't
  encrypted and could be intercepted by threat actors.
Well, that bit at least is wrong.  The sites are no less secure now than they
were before the cert expired.  The appropriate handling for expired certs is
to just keep using them as normal for a week or so and give them a chance to
get replaced, maybe warn slightly (e.g. via a visual indicator) for the next
couple of months after that, then switch to a harder-to-ignore warning for the
next few months (something you have to click past), and after that go to the
current behaviour.

@_date: 2019-01-12 01:14:46
@_author: Peter Gutmann 
@_subject: [Cryptography] Government shutdown: TLS certificates not 
And that was my point, this isn't something the user should have to deal with,
the browsers should apply graceful degradation for them.  They bend over
backwards to accommodate every imaginable type of brokenness on the web, but
the one thing they don't deal with is some site admin paying their CA tithe a
few weeks late.

@_date: 2019-01-21 09:37:07
@_author: Peter Gutmann 
@_subject: [Cryptography] The Voynich Manuscript as a product of a mental 
The Voynich Manuscript has attracted interest almost entirely from
cryptographically-inclined people (alongside historians) who are by nature
inclined to try and interpret it as some form of cipher or code, because they
want to see a cipher or code there.  I recently tried a different approach, I
showed it to a psychologist and asked "which type of mental health disorder
could the creator of this work have?".
The first response was that it was created by a functional schizophrenic, a
disorder that developed over time where they were seen as deeply spiritual by
others (perhaps in a monastery) and their work was encouraged.  Less likely
would be a paranoid schizophrenic who sees themselves as a witch and writes in
a secret language so that no-one else can persecute them for it.  Another
possibility is someone with serious bipolar disorder who, during a period of
severe euphoria, thinks they're in direct touch with God and writes down
visions from God, with God giving them a new language to record in the
So that's another option for the book, and an explanation for why attempts to
decipher it have failed: There is no decipherment, since it came from the mind
of someone whose thought processes don't match ours.

@_date: 2019-01-21 23:42:01
@_author: Peter Gutmann 
@_subject: [Cryptography] The Voynich Manuscript as a product of a mental 
This is art, not science, you don't get a repeatable experiment.  Look at the
later works of Louis Wain, a classic example of this used in psych texts...
here's one story on it:
Here's a direct link showing the progression:
(note that these aren't a direct progression from start to finish, he often
had several works in progress at a time so some may be contemporaneous).  Wain
ended up in a (at the time) lunatic asylum, in earlier years if they were
lucky they would have been seen as highly spiritual and ended up in a secluded
religious life, which is where something like the Voynich could have been

@_date: 2019-01-22 23:54:04
@_author: Peter Gutmann 
@_subject: [Cryptography] The Voynich Manuscript as a product of a mental 
Given that the Cardan Grille postdates the Voynich by 100-150 years, I think
this may be another case of people trying to see something where there isn't

@_date: 2019-01-24 00:05:54
@_author: Peter Gutmann 
@_subject: [Cryptography] Historic Codebreaking. 
Given that it produces translated sentences like "abandon, suffering, inform,
agreeably, their, out, dew, unites, below, for, nourishing, tool" and
"turning, hurt, healing, from, turn, fortune, it goes, tame, from,
nourishment, from, mothers, it is, things-fluid, for, holy, more, acquire,
from", I'm going to go for the "every few months" answer.

@_date: 2019-01-24 03:25:13
@_author: Peter Gutmann 
@_subject: [Cryptography] Historic Codebreaking. 
More likely a medieval Diceware passphrase generator.

@_date: 2019-01-26 00:39:50
@_author: Peter Gutmann 
@_subject: [Cryptography] Stupid question on S-boxes 
Thus the restatement of Law  of the 10 Immutable Laws of Security, "If a bad
guy can persuade you to run his program on your computer, its not your
computer any more", which in its inverse form is the Immutable Law of Cloud
Computing Security:
"If a bad guy can persuade you to run your program on his computer, its not
 your program any more".

@_date: 2019-01-29 10:40:09
@_author: Peter Gutmann 
@_subject: [Cryptography] Introducing the world's worst hash function 
Modern hash function are designed to be as efficient as possible on pipelined,
superscalar processors, but what if you wanted one that was the exact
opposite?  I needed a function to create small random delays but couldn't just
Sleep() or usleep() or nanosleep() or whatever since that's too granular.
What I needed was a fine-grained delay function, which led to MurMurHash's
mutant cousin MerdeMerdeHash, the one they keep locked in the basement and
only let out once the guest have left.
MerdeMerdeHash uses the slowest and most unpredictable data-manipulation
instructions available, combined with endless data dependencies and pipeline
stalls.  Multiplies, while they often have a single-cycle throughput,
typically have a multiple-cycle latency, which we enforce with data
dependencies.  The magic instruction for poor performance though is the
divide, which is typically microcoded and with a huge, data-dependent,
latency. Comments and further pessimisations welcome.
int merdeMerdeHash( const int value1, const int value2 )

@_date: 2019-01-30 01:11:56
@_author: Peter Gutmann 
@_subject: [Cryptography] Introducing the world's worst hash function 
Linux, bogomips.
I left out the low-level gunk since the idea was to illustrate a truly awful
hash function design without loading it up with details.  If anyone wants the
rest of the code, contact me off-list.

@_date: 2019-02-01 00:03:07
@_author: Peter Gutmann 
@_subject: [Cryptography] the world's worst hash function 
============================== START ==============================
 no current compiler can optimise it away, and that's by
design since the final computed value is returned to the caller so there's no
short-circuit eval possible.  What is theoretically possible is that a
hypothetical future compiler that does cross-module interprocedural dataflow
analysis might be able to see that the computed value isn't used anywhere, but
I'm not aware of anything like that.  Worst-case is you just need to store the
result in a static var and feed it, or part of it, as input back into the next

@_date: 2019-07-07 02:44:54
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  DDoS'ing PGP keys 
Without looking at the code, I'm guessing it'll be something like an n^2
algorithm used to process keys.  Many years ago I encountered some (not PGP)
key-processing code and fed it a largeish key collection.  Based on the fact
that the entire system ground to a halt, I asked the developers whether they
were perhaps using an n^2 algorithm to do the processing.  The following day I
got the rather sheepish answer that it wasn't n^2, it was n^3.

@_date: 2019-06-03 22:32:46
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED]  Crptographic ticket tape (fake news) 
There has been endless work done on tamperproof (meaning within certain
cryptographic assumptions) logging, if you want to secure surveillance footage
you'd be far better using any of those schemes than trying something like

@_date: 2019-06-04 11:13:01
@_author: Peter Gutmann 
@_subject: [Cryptography] USPTO gives up on PKI 
Just noticed this today, looks like the USPTO is giving up on PKI auth:
to be replaced by TOTP auth:
In other words they're swapping RFC 5280 for RFC 6238.
To compensate for this progressive step, they still expire passwords every 6

@_date: 2019-05-09 05:57:48
@_author: Peter Gutmann 
@_subject: [Cryptography] NIST announced Round 1 candidates for 
I don't really care about whether something came from NIST or not but it's
got the Bernstein protocol suite in it and he's damn good at designing
practical algorithms, meaning ones that don't fall to pieces the first time
they come into contact with the real world.

@_date: 2019-05-10 05:48:47
@_author: Peter Gutmann 
@_subject: [Cryptography] peering through NAT 
That's one of many hole punching techniques, typically used by P2P software.
UPnP isn't used nearly as much as people seem to think, principally because
it's either not present in the first place or disabled by default, while hole
punching works in all cases except symmetric NAT, in which case you need an
external rendezvous servver (actually even then you can sometimes hole punch
it if the NAT uses predictable port allocation techniques).

@_date: 2019-05-19 23:40:52
@_author: Peter Gutmann 
@_subject: [Cryptography] Network Time Protocol security 
Read the linked requirements docs, I think they've covered things pretty well
(almost unheard of for IETF security standards, they've published a comprehensive analysis and threat model before designing a solution).  In particular the first ref discusses both time-critical and security-critical applications, where you need millisecond or better synchronisation and protection against possibly quite motivated attackers.

@_date: 2019-05-26 10:07:56
@_author: Peter Gutmann 
@_subject: [Cryptography] The race to Quantum machines. 
Uhh, you need to read the rest of the article, which you've actually quoted:
  "Starting its R&D on quantum computing as early as in 1996, IBM released a
  5-qubit quantum computer in 2016 and unveiled the world's first 20-qubit
  system, dubbed IBM Q System One, at CES 2019, Morimoto said, disclosing that
  the company will soon launch 58-qubit quantum computers."
From that we have a few data points, and there's more from non-IBM
sources, so we can extrapolate over time.  Technically we can't actually do
that because from everything I've read it's nonlinear, the first steps are
relatively easy and then it gets harder and harder [0], but let's say it's
linear just for argument's sake.  Anyway, to break 1kbit RSA you need about a
million qubits.  Soon we'll have a computer with 58 qubits.  Graphing things
and drawing a line to where even 1kbit RSA is at risk is left as an exercise
for the reader.
[0] Like getting a manned settlement set up on Mars.  So far we can get the
    whole thing a few hundred meters closer to Mars.  After that, it gets a     lot harder.

@_date: 2019-11-11 04:34:28
@_author: Peter Gutmann 
@_subject: [Cryptography] Why RSA-PSS is much less secure than PKCS #1 v1.5 
There is a view that RSA-PSS (henceforth referred to as PSS) is more secure
than PKCS  v1.5 (henceforth PKCS  presumably because PSS has the words
"provable" and "secure" in its description (the abbreviation PSS stands for
Probabilistic Signature Scheme, it's the scheme itself which has "provable" in
it).  In practice however PSS is much less secure and vastly more brittle than
PKCS  despite its "provable security".  Here's why.
To understand the problem, you need to look at PKCS  vs. PSS.  PKCS  has
the following external, unauthenticated parameters:
The PKCS  verification operation, following the RSA operation that both PSS
and PKCS  require, is as follows.  Given a hash of the data being signed,
perform the following:
  fixed_sig_data[ hash_offset ] = hash;
  result = memcmp fixed_sig_data, recovered_sig_data;
In other words, copy the hash onto the end of the fixed-format PKCS signature data and compare it with the recovered signature data.  The fixed-
format PKCS  signature data encodes the hash algorithm, parameters,
signature type, and anything else that's required to verify the signature.
PSS in contrast has the following external, unauthenticated parameters:
RSASSA-PSS-params ::= SEQUENCE {
    hashAlgorithm      [0] {
                       SEQUENCE {
         algorithm     OBJECT IDENTIFER,
         parameters    ANY DEFINED BY algorithm OPTIONAL
         },
    maskGenAlgorithm   [1] {
                       SEQUENCE {
         algorithm     OBJECT IDENTIFER DEFAULT mgf1SHA1,
         parameters    SEQUENCE {
             algorithm OBJECT IDENTIFIER,
             parameters ANY DEFINED BY algorithm OPTIONAL
             },
         }
    saltLength         [2] {
         length        INTEGER DEFAULT 20
         },
    trailerField       [3] {
         trailer       INTEGER DEFAULT trailerFieldBC
         }
    }
The PSS verification operation, again following the common RSA operation, is
as follows...
Actually I won't post it here.  No matter how much I try and condense it, I
can't get it down to less than four solid pages of pseudocode.  In addition, I
have no idea whether it's actually correct or not, meaning whether it captures
all of the boundless corner cases and conditions that it's required to handle.
Just to help with the explanation that follows, here's a diagram of what's
going on.  mHash is the hash of the message as for PKCS  (this diagram
requires a fixed-width font to see):
        +-----------------------------------+------+---+
   EM = |#           maskedDB               | mHash|xDB|
        +-----------------------------------+------+---+
                       |
                        v                       |
                       xor <------- MGF() ------+---+
                           |
                        v                           |
        +---------------------------+-------+       |
   DB = |        00 ... 00 01       | salt  |       |
        +---------------------------+-------+       |
           |
           |
                                        v           |
        +-------------------+-------+-------+       |
   M' = |      00 x 8       |  hash | salt  |---+   |
        +-------------------+-------+-------+   |   |
                                                v   |
                                             Hash() |
   |
                                                v   v
                                               Compare
Now let's look at what an attacker can do with the above.  The immediately
obvious, trivial attack is a hash-substitution attack.  Unlike PKCS  PSS
doesn't encode the hash algorithm used anywhere in the signature.  This
encoding, known as a hash function firewall ("On Hash Function Firewalls in
Signature Schemes", CT-RSA 2002) was ironically not added to PSS because it
would have caused problems with the security proof.  By changing hashAlgorithm
to whatever you like, for example an algorithm of the same size that you know
how to break, you can forge the signature.  Change the hash from SHA-2/256 to
Streebog, Blake2, Keccak, CRC-256, XOR256, whatever you like, PSS won't detect
the change, making it only as strong as the weakest hash algorithm of the same
bit width that the victim will accept.
There's a lot more fun you can have with this.  Remember that you've got a
huge pile of external parameters, all controllable by the attacker.  For
example look at the way MGF() is applied, it's a simple stream cipher, meaning
that you can flip any bit in DB by flipping the corresponding bit in EM.  Note
that there's just a single bit in DB that tells you where the salt starts, so
if you can flip a bit in EM it'll set the corresponding bit in DB to 1.  In
theory you then run into a problem with the salt, but since the salt length is
another attacker-controlled parameter you just extend it to cover the extra
data that's been added by the bit flip.
Then there's M', which is assembled by the victim under the attacker's control
since they can set the salt length and hash algorithm to anything they want,
which PSS again won't detect.
There's quite a large pile of problems present here:
* There's no internal structure to the data so you can't see what's what, and
  in particular where one field ends and another begins.  Specifically, it
  violates Abadi and Needham's Principle  "Every message should say what it
  means" ("Prudent Engineering Practice for Cryptographic Protocols", Security
  and Privacy 1994).
* There's a ton of external, attacker-controlled parameters that allow an
  attacker excessive control over every step of the verification process (see
  also the previous point).
* The algorithm is ridiculously over-parameterised (why would you want to use
  a different hash algorithm for the message hash and mask generation hash, or
  a salt of a different length than the hash, or ...), all of which helps the
  attacker.
* It uses a stream cipher (XOR-mask) that allows you to make easily
  predictable changes to the data.
* The victim has to assemble the data block M' using attacker-controlled
  parameters rather than recovering it from the signature.
* The high level of complexity and special-case checks and operations make it
  pretty much impossible to implement in a side-channel-free manner.
It's almost a textbook example of what not to do when designing a signature,
or more generally crypto, mechanism.
(In its defence, PSS was very much a product of the times, with building
blocks like the hash function and MGF and parameters values subject to change,
so that parameter-combinatorial-explosion ended up being a frequent design
pattern, with the expectation being that usage profiles would address some of
the issues that arose.  Unfortunately none were ever really created beyond de
facto usage patterns, there's a suggested one at the end of this writeup.  For
a great introduction to the early history of RSA signature mechanism design,
see Burt Kaliski's talk from ZKProof 2019,
At the moment there isn't an obvious attack that takes advantage of this
beyond the obvious hash-substitution, but it gives the attacker an awful lot
of control over the internals of the PSS verification operation.  Contrast
this with PKCS  where there's only one fixed-format string possible for
each hash algorithm, and no ambiguity or manipulation by the attacker is
In terms of the unauthenticated parameters, in theory there is sort-of
protection against these in both X.509 and S/MIME / CMS, but in practice there
isn't.  X.509 includes the signature algorithm parameters in the certificate
in the form of the badly-named 'signature' field, which doesn't contain the
signature at all but the algorithm parameters.  Support for this is hit-and-
miss, with some implementations skipping it (which is perfectly justifiable,
since there's never been a need for it if you're not using PSS), some
implementations checking basic parameters but not the mass of optional and
situation-specific values used in PSS, and some meticulously checking every
parameter.  CMS in theory has a means of protecting the parameters by signing
them into the SignedAttributes as an RFC 6211 Algorithm Identifier Protection
Attribute, but I know of only one implementation that supports that, and in
any case if you've got a hash function that you can create collisions with you
can rewrite the CMS signature to remove the SignedAttributes.  Note that this
problem is specific to PSS, it doesn't affect PKCS  which encodes the
algorithm information into the signature.
So for the vanishingly small number of users of PSS, I'd recommend switching
to something more secure like PKCS  and disabling the PSS code before
someone attacks you through it.  If you must use PSS then hardcode in one, and
only one, signature scheme, say { hashAlgorithm = SHA-256, maskGenAlgorithm =
mgf1 with SHA-256, saltLength = 32, trailerField = trailerFieldBC } and don't
allow any substitutions.
"Beware of bugs in the above signature scheme; I have only proved it secure,
 not implemented it"
 - Apologies to Donald Knuth.
Thanks to various members of the cryptography community who commented on early
drafts of this writeup.  My opinions, not theirs.

@_date: 2019-11-12 07:29:37
@_author: Peter Gutmann 
@_subject: [Cryptography] Why RSA-PSS is much less secure than PKCS #1 v1.5 
Sure, good point.  However given the general concern about side-channel
protection (I know of at least one crypto library that implement side-channel
protection in the public-key ops, just in case someone finds something to
exploit there), designing a scheme that makes it essentially impossible to
create a non-side-channeled implementation is kinda bad.
We do now:
  crc256   OBJECT IDENTIFIER ::= { 1 3 6 1 4 1 3029 3 1 }
  xor256   OBJECT IDENTIFIER ::= { 1 3 6 1 4 1 3029 3 2 }
I prefer xor256 because you can create completely standard messages with the
same hash value as the intended target, without having to stuff in a few bytes
of binary data as for the CRC.  If I get time over the weekend, and I can find
a CMS message signed with RSA-PSS, I'll create a forgery using xor256.

@_date: 2019-11-16 10:35:42
@_author: Peter Gutmann 
@_subject: [Cryptography] Encryption doesn't seem to have bothered 
Quoted from "Moscow contacted rebels charged with downing MH17: Investigators'"
  "There was almost daily telephone contact between the leadership of the DPR
  and their contacts in the Russian Federation," JIT [Joint Investigative
  Team] said in a statement on Thursday, using the acronym of the Donetsk
  People's Republic rebels.
  The calls mostly took place over secure phones provided by Russian security
  forces, it said.
Maybe the next time the FBI claim that the existence of encryption will lead
to the collapse of civilisation, they can ask the countries in the Joint
Investigative Team (Australia, Belgium, Malaysia, the Netherlands and Ukraine)
for help.  The use of Russian military encryption doesn't seem to have
bothered them much.

@_date: 2019-11-17 02:59:52
@_author: Peter Gutmann 
@_subject: [Cryptography] Encryption doesn't seem to have bothered 
I know that.  The point is that they saw there was encryption and dealt with
it, rather than using it to justify a power-grab.  The use of Russian military
crypto actually helped the investigators, since it presumably stuck out like a
sore thumb - who provided it, where the traffic was going to, and so on.

@_date: 2019-09-13 08:44:13
@_author: Peter Gutmann 
@_subject: [Cryptography] TRNGs as open source design semiconductors 
There are no discrete chips, but plenty of devices like USB keys built up from
COTS parts.  Just google "open source rng" for examples.
They wanted nearly half a million dollars funding to create something that you
can buy now for a few tens of dollars.
The problem isn't that there aren't any available, it's that when it comes to
trying to secure the Internet, a hardware source of random numbers, whether
it's open-source, closed-source, single-chip, discrete-parts, or anything
else, is pretty much irrelevant as a solution.

@_date: 2019-09-13 20:02:23
@_author: Peter Gutmann 
@_subject: [Cryptography] TRNGs as open source design semiconductors 
I assume it's Jeri Ellsworth, google the name and "chips" or similar, e.g.

@_date: 2019-09-14 16:53:36
@_author: Peter Gutmann 
@_subject: [Cryptography] TRNGs as open source design semiconductors 
Yup, that's the way to get good entropy, just combine enough sources and as
long as even a few of them are OK, the whole thing is OK.  If you look at all
of the published attacks on entropy sources they never get beyond ones that
are little more than time() ^ getpid() as the seed because once you start
using multiple nontrivial sources it gets too hard for an attacker to model
what's going on.  An that's a feature, not a bug: people complain that you
can't reason about a set of entropy sources that you can't reliably model, but
then the attacker can't model them either.

@_date: 2019-09-17 07:47:52
@_author: Peter Gutmann 
@_subject: [Cryptography] "Exclusive: Russia carried out a 'stunning' 
Given that the intelligence community fought tooth and nail against the
availability of strong crypto for many, many years, I'd say they got exactly
what they asked for, with the accompanying consequences.

@_date: 2020-08-26 06:31:54
@_author: Peter Gutmann 
@_subject: [Cryptography] any reviews of flowcrypt PGP for gmail? 
A contributing factor to this is that you've got an encrypted SMS app that
requires 161MB of code (three times the size of a complete Windows 95
install), with a neverending stream of updates that don't seem to update
anything.  If you wanted to push out a malicious update there's every
opportunity to do so, and plenty of space to hide it in.  What's in that
hundred-and-sixty-megabytes of gunk, and what do the neverending updates

@_date: 2020-08-30 07:09:27
@_author: Peter Gutmann 
@_subject: [Cryptography]  
=?iso-8859-1?q?=2C_AES128_CBC=2C_SHA_seriously_qualified_as_=ABbroken=BB?=
 =?iso-8859-1?q?=3F?=
It's not you, it's me... I mean, it's the SSL assessment tool.  There are a
pile of these around, and in general they're about 50% checking for known,
real security problems and 50% checking that you're making the fashion
statement that the creator of the tool believes everyone should be making this
year.  For example recently I ran into a large B2B site that most of the time
would immediately drop the connection on seeing a client hello, and at other
times would drop it after going through the crypto handshake.  Extensive
fingerprinting turned up the fact that the site's idea of "secure", apart from
running ~12-year-old server software, means a large list of RSA-only suites
(but they're secure because we use 2048-bit RSA), a much smaller list of ECDSA
suites, and nothing else.  So you've got the choice between the least secure
keyex mechanism provided by the protocol (pure RSA) and the endlessly
vulnerability-plagued ECC suites (the latest papers on private-key-leaking
vulns are only a few weeks old), but no DHE at all.  Someone's security best
practices doc apparently told them to do this.
"This site is not virtue-signalling in a manner that we approve of.  We will
therefore decry AES-128 and HMAC-SHA1, both unbroken, as insecure until they
perform the appropriate level of virtue signalling".
To quote Linus on the difference between kernel scheduler design and security:
(and yes, that includes this post).

@_date: 2020-12-21 02:55:21
@_author: Peter Gutmann 
@_subject: [Cryptography] Possible reason why password usage rules are 
There's almost nothing available on this in English, once you get past any
top-level pages in English it's all in Turkish, but isn't this just an
incredibly complex way of doing something via a government (or equivalent
centralised) portal?  What advantage is being provided by the use of X.509?

@_date: 2020-02-26 12:05:12
@_author: Peter Gutmann 
@_subject: [Cryptography] Well, that showed them! 
I was writing a small followup on Symantec as an example of TB2F CAs, and
wondered what had happened after they were distrusted by browsers.  Looks like
the timeline was:
Symantec gets distrusted.
Symantec, valued at just under a billion dollars *after the debacle* sells its
  business to DigiCert, and things continue as before ("DigiCert Closes
  Acquisition of Symantec's Website SSL Security Unit", eWeek, October 2017 /
  "Symantec Selling SSL Security Business to DigiCert for $950M", eWeek,
  August 2017).
DigiCert gets a massive investment from private equity firms ("DigiCert Gets
  New Investment Post Symantec SSL Acquisition", ChannelFutures, July 2019) /
  "Clearlake Capital Group and TA Associates to Make a Strategic Growth
  Investment in DigiCert", ThomaBravo, July 2019).
You simply cannot fail in the TB2F CA racket.  In particular Symantec didn't
actually fail, it just changed the name on the letterhead and came back even
stronger than before.
Let that be a lesson to all other CAs!

@_date: 2020-01-07 11:41:36
@_author: Peter Gutmann 
@_subject: [Cryptography] Recent improvements on SHA-1 attacks 
An interesting paper has just appeared on the IACR e-print archive:
  SHA-1 is a Shambles: First Chosen-Prefix Collision on SHA-1 and Application to
  the PGP Web of Trust
  tl;dr: Attacks sped up by a factor of ~16 over previous work, chosen-prefix
collision for ~$75k and 2 months effort.
It's a long (32 pages) but interesting read.  The only thing I have a bit of
an issue with is the conclusion:
  SHA-1 signatures now offers virtually no security in practice
It should really be "SHA-1 signatures where the attacker has two months time
and tens of thousands of dollars (there are some cheaper options than $75k) to
prepare a forgery offer no security in practice".
Even then, the demonstrated attack relies on the ability to stuff arbitrary
garbage data into the signed message (in this case into a JPEG image after the
End-of-Image marker), so add:
  "... and the ability to stuff arbitrary attacker-chosen data into the signed
  message..."
to that.
Not trying to downplay the findings in the paper, but more to provide some
perspective on where the major risks lie for people who need to think about
the use of SHA-1 in legacy products and systems.

@_date: 2020-07-03 14:55:47
@_author: Peter Gutmann 
@_subject: [Cryptography] IPsec DH parameters, other flaws 
Yes.  Like HTTP/2, the design goal for TLS 1.3 was to make pushing out web
content to clients as easy and efficient as possible, even if it meant
sacrificing security or functionality for things other than pushing out web
content efficiently in the process.  Look at the hacks around ~0RTT and
session tickets, for example.

@_date: 2020-07-06 01:27:30
@_author: Peter Gutmann 
@_subject: [Cryptography] Zoom publishes draft cryptographic design for 
The exponent/generator doesn't matter, it's n/p that should be different each
time, which it is for RSA/DSA/etc but not for DH (at least as used in IPsec
and the IPsec cargo-cult derivatives).

@_date: 2020-07-07 02:04:44
@_author: Peter Gutmann 
@_subject: [Cryptography] IPsec DH parameters, other flaws 
Interesting, so the RFC 5114 values are NSA-generated rather than NIST as the
RFC implies?  I'd always avoided them because, apart from not serving any
obvious purpose, they also use incredibly inefficient values for g, making
them a non-starter for any real use.

@_date: 2020-07-08 10:20:22
@_author: Peter Gutmann 
@_subject: [Cryptography] Statement from Attorney General William P. Barr 
Or they're using one of the neverending catalogue of vulns in the security
systems of the chipsets or the vendors' implementation to bypass any security
on them.  For example are you using any NXP (i.MX) ARM SoC manufactured before
mid-to-late 2017?  It's vulnerable to remote compromise via its secure boot
capability, see "ERR010973 ROM: Secure boot vulnerability when authenticating
a certificate".  And since it's a boot ROM problem there's no fix for it.
Yup, the "security" feature on the device is what makes it vulnerable to an
OTA compromise.

@_date: 2020-07-10 03:04:44
@_author: Peter Gutmann 
@_subject: [Cryptography] "Home router warning: They're riddled with known 
2.4.x and 2.6.x is the standard kernel of Linux embedded devices.  With
supporting programs of roughly the same vintage.
You also have to go even further down the rabbit hole than Linksys, a lot, if
not all of their gear is actually made by Gemtek, who also OEM for Buffalo,
Zyxel, Fortinet, Cisco, Alcatel-Lucent, Belkin, 3Com, Dell, Intel, and many
others, I've seen them described somewhere as "a prolific OEM".
Ah, here it is:
Oh yeah, and they hardcode private keys into their devices and have been doing
so for years, just pulled up one of their certs and it's dated 2007.

@_date: 2020-07-12 06:48:43
@_author: Peter Gutmann 
@_subject: [Cryptography] IPsec DH parameters, other flaws 
This was finally added in TLS 1.2 as the session hash/extended master secret.
No browser that I'm aware of implements it.
was still fought tooth and nail in TLS 1.2 before it was added.  In the end I
applied the nuclear option and called for a vote of no confidence in the WG
chair for blocking adoption, at which point it was reluctantly passed.
No browser that I'm aware of implements it.

@_date: 2020-07-21 05:24:39
@_author: Peter Gutmann 
@_subject: [Cryptography] IPsec DH parameters, other flaws 
[Replies consolidated into one message]
It was absolutely that: Never attribute to malice what is adequately
explained by incompetence.  Take two WG's I was on, let's call them S and P.
S was full of implementers, when some issue was discussed members would point out implementation, performance, and interop issues from real-world
experience and base the standard on that.  P in contrast was populated by professional meeting-goers who hadn't written ten lines of code in as many years, operating entirely free from any real-world constraints so they could dream up whatever they wanted in a perfect vacuum.  Occasionally some new guy would pop up and propose something like creating use cases or similar to evaluate P's work against, but they were quickly shouted down and left again.
There was actually a shadow list P' of implementers who had given up trying to contribute to P years ago but occasionally exchanged email amongst themselves when some particularly egregious discussion ran on P.  Beyond that, there were people who were bad even by P's standards, with one frequent contributor informally recognised as "the resident denial-of-
service attack" without anyone needing to explicitly mention their name.
So in short it was design by committee, not deliberate sabotage.  The MIB's didn't need to sabotage the process, all they had to do was sit back and I remember that, the sentiment was "IPsec will be bigger than NAT so we'll make sure that it breaks it and thereby kill NAT", sort of like the joke about the fly sitting on the horns of the ox team and thinking what a good job it's doing pulling the plough.

@_date: 2020-07-29 04:23:07
@_author: Peter Gutmann 
@_subject: [Cryptography] Cryptographically securing a two-phase commit 
Let's say you have a computationally somewhat expensive operation that's
performed as a two-phase commit (2PC).  The details aren't important, but in
crypto terms think of it as receiving a large blob of signed data in PGP or
S/MIME format where you can't tell until you reach the signature at the end
whether it's valid or not.  The prepare portion of the 2PC is receiving and
saving the blob, the commit/abort operation is checking the signature at the
end and either discarding it or acting on it.
In general this is fine since a failed signature check results in a rollback
and no damage is done.  However it's vulnerable to a DoS attack in which an
attacker feeds in a blob for which the signature check will eventually fail,
forcing the target to go through the 2PC prepare/abort overhead each time.
Can anyone suggest a means of avoiding this overhead that doesn't require
inventing a custom protocol or format for the purpose?  In other words that
works within the PGP or S/MIME format to try and avoid this issue?  One
obvious solution that doesn't work is to precede the blob with a signed token
saying "the blob that follows is legit", but since there's no way to
cryptographically tie it to the blob that follows it's still vulnerable to a
spoofing attack, capture and/or replay a signed token and follow it up with a
dummy blob to force the 2PC overhead.
Again, it needs to be achievable using a standard format like PGP or S/MIME,
inventing a new protocol or format to do it isn't an option.  Breaking the
single blob up into lots of little sub-blobs, each individually authenticated
and hash-chained together, is possible as a last resort but anything better
would be preferable.

@_date: 2020-07-30 05:58:34
@_author: Peter Gutmann 
@_subject: [Cryptography] Cryptographically securing a two-phase commit 
That was my feeling as well, but I wanted to get confirmation before I
declared it unsolvable.  In particular making an absolute statement like that
to security people invites vigorous pen-testing of the validity of the
The current thinking for dealing with it involves adding an SSH or TLS channel
and channel-binding the data being communicated.  So set up a TLS channel,
send some sort of bound proof of freshness inside the channel and then follow
it with the blob, with the proof-of-freshness -> blob link provided by the
channel.  That moves the issue to a MITM problem, which is much easier to deal
with.  The drop-the-nth-block DoS that you point out is still possible, but
most of the hole before that is closed.

@_date: 2020-06-03 06:29:58
@_author: Peter Gutmann 
@_subject: [Cryptography] Zoom publishes draft cryptographic design for 
I would avoid widely-used parameter sets for an entirely different reason,
namely the "don't be a target" defense strategy, one of the most effective
types of defence there is.  If you use the same parameters as any widely-used
protocol, IPsec's DH parameters [1] or the Bitcoin parameters, you make
yourself collateral damage to any attacker willing to commit the resources to
break IPsec or help themselves to BTC.  For BTC in particular the amount of
money involved both strongly motivates non-academic attackers to keep very
quiet about being able to break it while also strongly motivating otherwise
prohibitive attacks.
So I would avoid P256k1 like the plague in order to avoid being taken out as
collateral damage.  If you generate your own parameter set(s) using the same
mechanisms used to generate the well-known ones you get the same level of
security but without the collateral damage aspect.  In particular if your
protocol is relatively little-used or not protecting much of any value there's
little incentive for an attacker to even try attacking it.
Which makes P256k1 probably the most dangerous parameter set in the world to
[1] I've never understood why IPsec and the cargo-cult protocols that reused
    the parameters from it fixed on a single set of DH parameters.  IPsec is
    possibly one of the most unnecessarily flexible protocols in the world
    where absolutely everything is up for negotiation, but there's one single
    set of parameters that every single user has to share to create a single
    point of failure for attackers to exploit.

@_date: 2020-06-05 00:50:38
@_author: Peter Gutmann 
@_subject: [Cryptography] Zoom publishes draft cryptographic design for 
However, even that implementation complains about non-32-bit integers as
exponents. That's based on the fact that in 25 years of use there's only been
one implementation that required a bignum exponent and that was created by
people who were cleverer than everyone else and decided sizeof( e ) must equal
sizeof( n ).  In practice virtually everyone sets e = F4, which is fine.
Oh, except OpenSSH which uses e = 33 or 35, because SSH used an e relatively
prime to (p-1)(q-1), choosing odd (in both senses of the word) numbers > 31.
33 or 35 probably ended up being chosen frequently so it was hardcoded into
OpenSSH for cargo-cult reasons, until it was finally fixed in about version 6,
but there are still enough e = 33 / 35 keys around that you need to special-
case handling for them so either they get used forever once generated or
something is still generating them.

@_date: 2020-06-27 02:11:21
@_author: Peter Gutmann 
@_subject: [Cryptography] The Samsung global Blu-Ray meltdown 
Not sure if others have been following this, but since 18 June all Samsung
Blu-ray players globally have been stuck in a reboot loop with a period of
about five seconds, barring a very small number that have never been connected
to the Internet or had firmware updates.  The only way to fix it is to send it
back to Samsung for a replacement, presumably for a reflash, with no-one yet
able to figure out what's causing it.  Guesses are:
1. A certificate expired, since it hit globally at the same time.
2. Bad firmware update, would also match but you'd expect a more staggered
   rollout worldwide, and seems odd that they'd deploy a device-killing update
   without being aware of it.
3. Something triggered a DRM lockout, a variant of (1).
In any case it looks like an interesting illustration of the conflict between
security vs. availability if the reason is (1) or (3).

@_date: 2020-06-28 10:37:41
@_author: Peter Gutmann 
@_subject: [Cryptography] The Samsung global Blu-Ray meltdown 
In the (currently) 220-page thread in the Samsung support forums virtually
everyone that's mentioned it has said they've had it connected to the
Internet, for the simple reason that Blu-ray DRM requires connectivity/
updates in order to play newer discs.  One poster explicitly mentioned they'd
never connected or updated their firmware and things were fine, but that
doesn't really narrow the cause down.  In any case Samsung is now RMA'ing
players automatically if people call in with a problem.

@_date: 2020-03-04 12:16:51
@_author: Peter Gutmann 
@_subject: [Cryptography] Possible reason why password usage rules are such a 
There has been some speculation in the past over why we have so many cargo-
cult password security rules that make no sense in any modern context, the
prime example being the need to change passwords periodically.  I've found one
possible explanation, the Ware Report, which talks about authentication words
more than passwords, and in a manner in which they resemble military
countersigns rather than what we'd think of today as passwords:
  Authentication words or techniques must be obtained from an approved source,
  or, alternatively, must be generated and distributed under the cognizance of
  the System Security Officer by approved techniques. Specifically, a user
  cannot generate his own passwords [...] Authentication words must be changed
  as frequently as prescribed by the approved issuing source.
Looking at a WWII-era field manual, that looks very similar to the
requirements for countersigns given in that.  Perhaps this could be the source
of so much of the historical baggage of unknown origin that's attached to
passwords, they came from military countersigns that were repurposed for use
with computers.

@_date: 2020-03-05 00:32:52
@_author: Peter Gutmann 
@_subject: [Cryptography] Possible reason why password usage rules are 
Oh yes, it's far less secure than passwords, a study of SSH key storage a few
years ago showed, from memory, 80% were stored on disk in plaintext form, so
anyone who got one-off read access to the owner's files at any point could get
into every other account they had access to.? But hey, the magic of public-key
pixie dust makes it "secure".

@_date: 2020-03-05 00:37:48
@_author: Peter Gutmann 
@_subject: [Cryptography] Possible reason why password usage rules are 
I've run into exactly this a number of times.  For example many years ago I
was at a meeting of IT security managers from various government departments
where every single person agreed that forced password changes were a really
dumb idea resulting in inconvenience and reduced security.  Also, every single
person agreed that since the NIST guidelines said you had to do that, they
were going to keep doing it.

@_date: 2020-03-05 02:25:22
@_author: Peter Gutmann 
@_subject: [Cryptography] Possible reason why password usage rules are 
The "Manual For Noncommissioned Officers And Privates Of Cavalry Of The Army
Of The United States", section 11, "Countersigns and Paroles", could be one

@_date: 2020-03-06 00:43:20
@_author: Peter Gutmann 
@_subject: [Cryptography] Possible reason why password usage rules are 
The argument isn't technical, it's financial, 12 months is the CA billing
However, the same arguments against unnecessary password changes also hold for
unnecessary certificate changes.  Vast numbers of users just re-certify the
same key year in, year out [References available in request, couldn't be
bothered digging them up at the moment], so it provides no extra protection
for the key, but what it does provide is a large amount of extra exposure.
The riskiest time in the life cycle of a certificate is when it's (re-)issued
and installed.  When it's in use it's (presumably) safely locked up, but
during the rollover period it's at it's most vulnerable, exposed to attack.
Compounding the problem, every certificate publicly announces when it'll be in
its most-vulnerable phase, allowing attackers to target it during that time
interval.  What's more, CAs expect panicked certificate changes close to the
expiry period and allow more lax methods of authorising the changeover (in one
pen-test an urgent phonecall about "our servers are down because the cert has
expired, we need a new one quickly!" was all that was required to get a
certificate issued to a random third party), so the checking during the window
of maximum vulnerability is often far less than the often minimal enough
checking during normal operation.
Another thing with unnecessary changes is that probably the most suspicious
things to happen to a certificate is for it to suddenly change, particularly
in the presence of certificate pinning.  On the other hand if your certificate
pinning mechanism takes expiry into account then it completely defeats the
pinning because the attacker just has to look at the expiry date, perform
their attack then, and the pinning mechanism will happily accept it as the new
So forced certificate changes are just as useful as forced password changes,
meaning they have a net negative impact on security.
And then you've got Apple, who's adding fuel to the existing dumpster fire by
forcing all certs to change every 12 months.

@_date: 2020-03-08 09:21:32
@_author: Peter Gutmann 
@_subject: [Cryptography] Start the clock 
So the next question, or next two questions, are (1) is this protecting
anything anyone really cares about and (2) given that the IME already has more
vulnerabilities than a $a has $b, does it matter?

@_date: 2020-05-05 00:23:06
@_author: Peter Gutmann 
@_subject: [Cryptography] NSA security guidelines for videoconferencing 
It's called end-to-end encryption because possibly-compromised-device-
controlled-by-party-2 encryption doesn't roll off the tongue as easily.

@_date: 2020-05-05 03:38:47
@_author: Peter Gutmann 
@_subject: [Cryptography] NSA security guidelines for videoconferencing 
The thing with Zoom encryption is that in the current environment the threat
isn't Mossad doing Mossad things to you, it's preventing some random prankster
from inserting a dick pic into your conversation with your mother who's
worried about the fever your father is running.  It's the easiest - and given
that the alternatives are things like Microsoft's Skype, possibly only - way
for family members and the like to stay in touch via video during lockdown.
Zoom is addressing a usability issue, not a security issue.  If you need to
run sensitive conversations, don't use Zoom.

@_date: 2020-05-23 04:43:53
@_author: Peter Gutmann 
@_subject: [Cryptography] CMS or S/MIME test vectors 
For a real-world test data set, look for S/MIME encrypted or signed email via
Google or similar.  In particular for signatures, which are the most
problematic, just save off any from incoming signed email.  I have quite a
collection that I've simply accumulated over time as it's turned up.  Normally
I'd offer to share it but mixed in there are various non-public samples and
I'd have to pick through it all sorting out those ones from public-sourced
In terms of RFC 4134, that's not very useful since it exercises every weird
mechanism and oddball corner case in the spec, none of which you'll ever
encounter.  What you need to test most is all of the million ways of creating
theoretically valid but unexpected signatures on data, which is what you need
to scrape together from public sources.

@_date: 2020-05-23 05:08:44
@_author: Peter Gutmann 
@_subject: [Cryptography] Improving MITRE's description for "CWE-329: Not 
It also depends on the encryption mode.  With an industrial-strength mode like
CBC you can pretty much abuse the IV any way you want and all you'll get is
either the ability to identify repeated encrypted data or, pathological worst-
case and you zero the IV on each block, a degradation to ECB.
OTOH if you get it wrong with an incredibly brittle mode like GCM or CTR you
get a catastrophic failure of security.
So you need to take the mode used into account as well as what happens if you
mess up the IV.

@_date: 2020-05-25 06:38:58
@_author: Peter Gutmann 
@_subject: [Cryptography] CMS or S/MIME test vectors 
It really depends what the OP is trying to do with the test vectors: Is it
find bugs in the code, or make sure it's compatible with as many different
implementations as possible?  They'd lead to two very different sets of test
(Having said that, fuzz it anyway, that never hurts).

@_date: 2020-06-01 03:34:46
@_author: Peter Gutmann 
@_subject: [Cryptography] [FORGED] Re:  Cubbit 
============================== START ==============================
If you want the software-only version there's things like Storj, where your
data is stored on the spare hard drive space of random people who have signed
up to it, a.k.a. Airbnb for data.  So bits of your data will be on a gaming
machine in Kazakhstan, bits on a cash register in France, and bits on a Kodi
box in Peru.  What could possibly go wrong?

@_date: 2020-11-02 11:01:08
@_author: Peter Gutmann 
@_subject: [Cryptography] Windows security leads to 0-day in Windows 
Visual Studio is pretty good about warning about truncation/type conversion
errors if you set the warning level to 4 (and then disable the warnings for
Windows headers to allow them to compile, which leads me to believe that
Windows code isn't built with /W4), but doesn't do much to look for possible
over/underflow.  PREfast finds nothing in the code, two other good checkers
that perform a different type of analysis, PVS Studio and cppcheck, similarly
find, or at least report, nothing.  I'll raise a case with the devs (the PVS
Studio devs are particularly good here, whenever a vuln like this comes out
they tend to add a means of checking for it pretty quickly), but it may be
that warning for something like this will result either in an avalanche of
false positives or the need to apply not-necessarily-practical levels of
analysis to detect the problem.  If you look at the code flow analysis here
you need to either flow backwards from the use of the buffer to the allocation
to the allocate-size-calculation, or know that BCryptAlloc() is an allocation
function and flow back to the size calculation, to find it:
NTSTATUS CfgAdtpFormatPropertyBlock(PBYTE SourceBuffer, USHORT SourceLength, PUNICODE_STRING Destination) {

@_date: 2020-11-18 00:35:00
@_author: Peter Gutmann 
@_subject: [Cryptography] IPsec DH parameters, other flaws 
In particular, IPv6 solves a very real problem that affects a lot of the
world, the exhaustion of IPv4 address space (and even then it's usually easier
to keep kludging around it with NAT than to switch to IPv6).  QUIC just solves
the problem of efficient content delivery for Google, which no-one but Google
cares about.
If we can't get people to adopt IPv6, why would anyone care about QUIC?  And
more generally, why would anyone care about any next-big-thing when they've
already indicated that they prefer to keep extending what's worked in the past
indefinitely because it's less painful than moving to the next-big-thing?
Drifting back to security, a lot of the world values stability over the next
big thing.  I worked on a standard a while back which has a discussion on how
to keep a system that's been compromised by an attacker running, because the
only thing worse, far far worse, than having a running system co-managed by an
attacker is having a non-running system.  So you deal with it by adding
compensating controls and move on.

@_date: 2020-11-21 13:01:11
@_author: Peter Gutmann 
@_subject: [Cryptography] A bulletproof vest with moth holes 
Intel recently fixed a hardware-level vulnerability that allowed its Boot
Guard to be bypassed:
Unfortunately they've released no technical details on how they did this, but
what makes it interesting is that they appear to have modified the functioning
of the uber-secure can't-be-bypassed hardware-based security... with a
software patch.  Hmmm...

@_date: 2020-11-25 01:09:57
@_author: Peter Gutmann 
@_subject: [Cryptography] What happened to the E4M/TrueCrypt author 
In the late 1990s there was a Windows disk encryption program called
Encryption for the Masses or E4M, which later became TrueCrypt.  The author of
E4M, Paul LeRoux, later moved on to other endeavours:
  Along the way he had, among other endeavors, simultaneously fed the American
  opioid epidemic; built his own base operations in Somalia, protected by an
  armed militia; run gold and timber extraction operations in a half-dozen
  African countries; laundered millions of dollars through Hong Kong; plotted
  a coup in the Seychelles (later abandoned); bought off law enforcement in
  the Philippines, where he was based; trafficked methamphetamine out of North
  Korea; and overseen a team of engineers building missile guidance systems
  for Iran and drones for drug delivery.
  (From Ignoring the Bitcoin angle, it's hard to imagine it's the same person:

@_date: 2020-10-31 03:59:20
@_author: Peter Gutmann 
@_subject: [Cryptography] Windows security leads to 0-day in Windows security 
I'm always amused to see security components used to break security.  This
time it's Window's CNG, a.k.a. Cryptography API: Next Generation, which has an
0-day in it that affects every version of Windows back to Windows 7:
It's at the kernel level, and being exploited in the wild.  Very unsporting of
the attackers to ignore the "security line, do not cross" tape and attack
there anyway.

@_date: 2020-09-14 07:28:54
@_author: Peter Gutmann 
@_subject: [Cryptography] World's oldest security RFC published 
In January 2000, a group of Cisco folks published an RFC draft:
  SCEP: The Simple Certificate Enrollment Protocol
  January 2000
In September 2020, more than twenty years later:
  A new Request for Comments is now available in online RFC libraries.
  RFC 8894
  Title: Simple Certificate Enrolment Protocol
  Date: September 2020
Just to put this into perspective, at that time running it on a state-of-the-
art PC would have involved a 450MHz Pentium III with 128MB RAM running Windows
NT communicating via a dialup modem, or possibly early ADSL at 512kbps or so.
Today, even the minimal embedded devices and cellphones that are provisioned
with SCEP have many times that capacity.

@_date: 2020-09-22 10:28:55
@_author: Peter Gutmann 
@_subject: [Cryptography] World's oldest security RFC published 
There are two openly-available implementations that I know of, JSCEP (MIT
and my own cryptlib (Sleepycat license):
Both have been around forever and are mature code.

@_date: 2020-09-23 08:06:09
@_author: Peter Gutmann 
@_subject: [Cryptography] World's oldest security RFC published 
Those are SCEP implementations, but not necessarily of the RFC.  Don't forget
it's been around for twenty years, but only in the last few has it moved past
single DES and MD5.  In particular the OpenSCEP page has a date of 2001, the
OpenXPKI implementation doesn't look like it supports the RFC unless it's
pretty well hidden, and SSCEP looks equally old.

@_date: 2020-09-23 08:40:43
@_author: Peter Gutmann 
@_subject: [Cryptography] World's oldest security RFC published 
In any case though the minimal changes to get an old version up to speed
should be fairly minor, just replacing all the museum-grade algorithms with
current ones, implementing POST rather than GET, and checking that various
things that were ambiguous or even wrong in the original draft but that people
usually implemented correctly anyway really are done correctly.  That's not
the full RFC, but enough to meet the minimum mandatory-to-implement

@_date: 2020-09-30 09:20:58
@_author: Peter Gutmann 
@_subject: [Cryptography] Exotic Operations in Primitive Construction 
If you're referring to an implementation in C, it's not even that any more,
any compiler from the last 15-20 years has a rotate recogniser and will
translate 'x << y | x >> ( wordsize - y )' into a single rotate instruction.

@_date: 2020-10-01 00:05:21
@_author: Peter Gutmann 
@_subject: [Cryptography] Exotic Operations in Primitive Construction 
Also, don't forget throughput vs. latency.  Just because you can do something
in x cycles doesn't mean you can immediately use the result for further ops.
