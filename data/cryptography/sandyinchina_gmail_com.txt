
@_date: 2006-03-23 13:42:53
@_author: Sandy Harris 
@_subject: Entropy Definition (was Re: passphrases with more than 160 bits of entropy) 
I'll answer the easier questions. I'll leave the harder ones for someone
with a better grounding in information theory.
Roughly, they both measure unpredictability. Something that is hard
to predict is random, or has high entropy. There are mathematical
formulations that make this a lot more precise, but that's the basic
Absolutely not!
At best, you preserve the original entropy, just distributing it
differently. If you get the processing wrong, you can reduce or
even entirely destroy it, but no amount of any kind of processing
can increase it.
You can add more entropy, either from another source or more
from the same source. That is the only way to increase it.
Sandy Harris
Zhuhai, Guangdong, China

@_date: 2006-05-26 15:18:59
@_author: Sandy Harris 
@_subject: Status of opportunistic encryption 
Some years back I worked on the FreeS/WAN project (freeswan.org),
IPsec for Linux.
One of our goals was to implement "opportunistic encryption", to allow any two
appropriately set up machines to communicate securely, without pre-arrangement
between the two system administrators. Put authentication keys in DNS; they
look those up and can then use IKE to do authenticated Diffie-Hellman to create
the keys for secure links.
Recent news stories seem to me to make it obvious that anyone with privacy
concerns (i.e. more-or-less everyone) should be encrypting as much of their
communication as possible. Implementing opportunistic encryption is the
best way I know of to do that for the Internet.
I'm somewhat out of touch, though, so I do not know to what extent people
are using it now. That is my question here.
I do note that there are some relevant RFCs.
RFC 4322 Opportunistic Encryption using the Internet Key Exchange (IKE)
RFC 4025 A Method for Storing IPsec Keying Material in DNS
and that both of FreeS/WAN's successor projects (openswan.org and
strongswan.org) mention it in their docs. However, I don't know if it
actually being used.

@_date: 2007-02-07 05:42:49
@_author: Sandy Harris 
@_subject: Entropy of other languages 
The most general answer is in a very old paper of Mandelbrot's.
Sorry, I don't recall the exact reference or have it to hand.
He starts from information theory and an assumption that
there needs to be some constant upper bound on the
receiver's per-symbol processing time. From there, with
nothing else, he gets to a proof that the optimal frequency
distribution of symbols is always some member of a
parameterized set of curves.
Pick the right parameters and Mandelbrot's equation
simplifies to Zipf's Law, the well-known rule about
word, letter or sound frequencies in linguistics.
I'm not sure if you can also get Pareto's Law which
covers income & wealth distributions in economics.

@_date: 2007-02-26 09:36:40
@_author: Sandy Harris 
@_subject: Entropy of other languages 
No. There was some pretty heavy math in the paper. With it in my hand,
I understood enough to follow the argument. 20 years later with no paper
to hand, I haven't a clue.
Paper is likely somewhere under his home page.
Probably, but he did have a proof that the skewed distribution is
more efficient in some ways.

@_date: 2007-06-22 23:52:13
@_author: Sandy Harris 
@_subject: ad hoc IPsec or similiar 
The Linux FreeS/WAN project was working on "opportunistic encryption".
The general idea is that if you use keys in DNS to authenticate gateways
and IPsec for secure tunnels then any two machines can communicate
securely without their administrators needing to talk to each other or to
set up specific pre-arranged tunnels.
There is an RFC based on that work:
The FreeS/WAN project has ended. I do no know if the follow-on projects,
openswan.org and strongswan.org, support OE.

@_date: 2007-06-26 19:02:53
@_author: Sandy Harris 
@_subject: ad hoc IPsec or similiar 
It is certainly a problem, but you can get around it partially even if your IP
address is dynamically assigned:
You do need to use a dynamic DNS server to handle your keys, but there
are lots of those, and many do provide that service.
Also, this is limited to "initiate-only" IPsec; it does not handle incoming
connections. However, that may be enough for many client machines that live
in dynamic address space.

@_date: 2007-10-13 08:47:17
@_author: Sandy Harris 
@_subject: Password hashing 
Shouldn't it be USERID || SALT || PASSWORD to guarantee that if
two users choose the same password they get different hashes?
It looks to me like this wold make dictionary attacks harder too.

@_date: 2007-10-27 00:04:05
@_author: Sandy Harris 
@_subject: Password vs data entropy 
The entropy of the data is irrelevant. The question is its
value; that affects both the resources an attacker might
use to get it and the cost to you if it is lost.
If there is no attack on the crypto algorithm better than brute
force (a huge "if"!, but there are available algorithms for which
we can at least say no such attack has been published) and
you can estimate attacker's resources, then you can estimate
key size required for safety.
Te EFF's DES cracker (many custom-built chips in parallel)
broke 56-bit DES in a few days. Assume our brute force
enemy can search a 64-bit space (256 times larger) in a
second (a few million times faster). Then searching a
96-bit space takes him 2**32 seconds, well over a century.
For a 128-bit space, multiply that by another 2**32, so
something over 400 billion years.
You really don't care about minor variation here, e.g.if our
estimates are off by a million and he can do it in 400
million years instead.
So, if your crypto is sound,128 bits should theorectically
be enough for any data and any human time scale.
Practice and theory can differ, though, and you cannot
be utterly certain there's not some unpublished attack
that does awful things to the crypto. I'd use 256 bits
and a well-analyzed algorithm.

@_date: 2007-09-02 21:26:07
@_author: Sandy Harris 
@_subject: debunking snake oil 
On 8/30/07, travis+ml-cryptography at subspacefield.org
You can get a few spectacularly boneheaded ones from Sklyarov's
Defcon presentation, the one he was arrested for. Link here.

@_date: 2008-04-22 08:22:48
@_author: Sandy Harris 
@_subject: Cruising the stacks and finding stuff 
Another back-of-the-envelope estimate would be to look at the EFF
machine that could brute force s 56-bit DES key in a few days and
cost $200-odd thousand. That was 10 years ago and Moore's Law
applies, so it should be about 100 times faster or cheaper now.
Round numbers are nice. Overestimating the attacker a bit is
better than underestimating. So assume an attacker can brute
force a a 64-bit key (256 times harder than DES) in a second
(a few 100 thousand times faster).
Brute force against a 96-bit key should take 2^32 times as long.
Since pi seconds is a nano-century, that's somewhat over a
century. For a 128-bit key, over 2^32 centuries. If brute force
is the best attack, this is obviously secure.

@_date: 2008-04-24 07:22:45
@_author: Sandy Harris 
@_subject: Cruising the stacks and finding stuff 
Saving bits may not matter, or may not be possible. For example,
if you are ealing with a hybrid system -- say, using RSA to transmit
the symmetric cipher key or Diffie-Hellamn to construct it -- then for
any symmetric cipher key size less than the public key size, your
overheads are the same.

@_date: 2008-12-16 08:30:34
@_author: Sandy Harris 
@_subject: CPRNGs are still an issue. 
Any unused input device with noise can be used. Examples:
Soundcard: Camera: If anything in the system changes a lot, like processes starting
and stopping or files opening & closing, periodically hashing
the tables that describe that state is useful.
Is your threat model one-sided? e.g. for a home router, attacks
from the Internet side might be more of a worry than attacks
from the LAN. In that case, things like packet timing on the
LAN side are unknown to the feared attacker. Also, if you are
doing NAT, the port numbers on the LAN side since those are
not sent outside.
If the device does any crypto, mixing ciphertext into the pool
is nowhere near ideal since you would not be encrypting
unless some enemy might get the text and using things an
an enemy can get is exactly what you do not want here.
However, it is cheap and random-looking, and the volume
is proportional to the amount of crypto done, so it might
help in some cases.

@_date: 2008-02-01 14:51:36
@_author: Sandy Harris 
@_subject: Gutmann Soundwave Therapy 
What I don't understand is why you think tinc is necessary,
or even worth the trouble.
IPsec is readily available -- built into Windows, Mac OS
and various routers, and with implementations for Linux
and all the *BSDs -- has had quite a bit of expert
security analysis, and handles VPNs just fine.
Does tinc do something that IPsec cannot?

@_date: 2008-01-15 07:30:58
@_author: Sandy Harris 
@_subject: Death of antivirus software imminent 
That's a rather large and distinctly dangerous assumption. Here's the
IETF's official line on the question, the "abstract" section of RFC 2084:
   The Internet Engineering Task Force (IETF) has been asked to take a
   position on the inclusion into IETF standards-track documents of
   functionality designed to facilitate wiretapping.
   This memo explains what the IETF thinks the question means, why its
   answer is "no", and what that answer means.
The whole question was extensively discussed on an IETF mailing
list set up for the purpose before that RFC was written:
The aptly named RFC 1984 is also relevant.
Among the more obvious problems are the fact that complexity
is bad for security, that the US government has some history
of abusing wiretaps, and that other governments who would
have access to any such technology are even less trustworthy.

@_date: 2008-11-19 08:18:46
@_author: Sandy Harris 
@_subject: Hybrid cipher paper 
A paper of mine just went up on   It has some ideas
that I hope are new, I think are good, and I know are unorthodox. I'm well
aware of the usual fate of such innovations, especially from amateurs.
If anyone would like a break from looking at new hashes, perhaps they
could have a look.
Number    2008/473
Title      Exploring Cipherspace: Combining stream ciphers and block ciphers
This paper looks at the possibility of combining a block cipher and a stream
cipher to get a strong hybrid cipher. It includes two specific proposals for
combining AES-128 and RC4-128 to get a cipher that takes a 256-bit key
and is significantly faster than AES-256, and arguably more secure. One is
immune to algebraic attacks.

@_date: 2008-10-26 10:47:34
@_author: Sandy Harris 
@_subject: combining entropy 
So you need a 2:1 or heavier compression that won't lose
entropy. If you just need one 160 word out per N in, then
hashing them is the obvious way to do that.
Yes, but the proof holds for any reversible mapping. XOR
makes each output bit depend on exactly two inputs bits.
Sometimes you want a mapping that mixes them better.
If one input is entirely random, XOR is fine; random ^ x is
random for any x. It is also fine in the case above, where
only one generator works.
If  > 1 inputs have some entropy but none have enough,
which seems to me the commonest case, XOR is not
the best choice; it does not mix well enough.
Nyberg's perfect s-boxes are in some ways the ideal
mixer. 2n bits in, n out, all columns and all linear
combinations of columns are bent functions. Big
S-boxes are expensive though, and building even
small Nyberg S-boxes is going to take significant
effort. Designing something that uses a bunch of say
8 by 4 S-boxes to do good mixing on 160-bit chunks
is not trivial either.
You could use IDEA multiplication in mixing. Two 16-bit
words in, one out, and every output bit depends on all
input bits.
If every 16-bit input word has 50% entropy density
(not the same as every 160-bit word does, but perhaps
close enough) then the output should have 100%.
For N > 1, you need to combine those and worry about
overall mixing. If entropy density is known to be ~50%,
you can combine pairs with IDEA to get ~100%, then
use cheaper operations for any other mixing needed.
I'd use addition, which costs about the same as XOR
but gives slightly better mixing because of carries.
For N > 2 and density < 50%, you could use a cascade
of IDEA operations 8->4->2->1 or whatever. Or do
something like: combine two 160-bit chunks with 10
IDEA multiplications, circular shift the result 8 bits,
combine with next 160-bit input, ...
At some point, you may find yourself designing a hash.
If that happens, just give up and use a standard hash.

@_date: 2008-09-21 11:46:29
@_author: Sandy Harris 
@_subject: Lava lamp random number generator made useful? 
Not USB, but ...
There is an excellent Open Source design for a true random number
generator using a sound card at: Since many servers will have an unused sound card equivalent on
the motherboard, and a cheapo sound card is an easy addition to
others, this strikes me as a good solution.

@_date: 2008-09-24 06:15:37
@_author: Sandy Harris 
@_subject: Fake popup study 
popups with various malware warning signs. Many just clicked.

@_date: 2009-05-01 07:07:47
@_author: Sandy Harris 
@_subject: Destroying confidential information from database 
Yes, but that paper is over ten years old. In the meanwhile, disk
designs and perhaps encoding schemes have changed, journaling
file systems have become much more common and, for all I
know the attack technology may have changed too.
Is there a more recent analysis or is Guttman still the
best reference?

@_date: 2009-05-01 08:15:04
@_author: Sandy Harris 
@_subject: CSPRNG algorithms 
Not complete, but this encyclopedia article has some links:
It is a wiki so if you can improve it, please do.
No doubt Wikipedia has a list as well. All the usual
crypto texts have chapters on it, too.

@_date: 2009-06-08 12:22:19
@_author: Sandy Harris 
@_subject: Factoring attack against RSA based on Pollard's Rho 
I do not have it to hand, but at one point I had a solution for
   N = pq = (a-b)(a+b) = a^2 - b^2
where I could find unique values for a^2 and b^2 mod 9, mod 16,
or by combining those mod 144. Mod 25, mod 49 et cetera gave
constraints but not unique solutions.
After playing with this a while,  I concluded that it was not
actually useful,

@_date: 2009-05-03 19:59:31
@_author: Sandy Harris 
@_subject: [tahoe-dev] SHA-1 broken! 
Off-the-shelf FPGA-based device that breaks DES by brute force in
about a week, costs 9,000 euros: These are commercially available and programmable. Setting a
few of them up to break SHA-1 certainly would not be trivial,
but it looks feasible.

@_date: 2009-05-10 22:33:30
@_author: Sandy Harris 
@_subject: Fwd: 80-bit security? (Was: Re: SHA-1 collisions now at 2^{52}?) 
It was a best guess by a group of clever and well-informed people.
There's no way to tell if it was precisely right, but there's no way
to get a better estimate either, short of getting a similar group to
re-do the work today.
A back-of-the envelope approximation to today's requirements
can be had by saying Moore's Law gives twice the computer
speed every 18 months, so ciphers needs one more key bit
every 18months to keep up. They said minimum 75 bits to
keep an existing cipher in service, minimum 90 for any new
ones, as of 1996. Add 10 bits to each for a rough estimate
as of 2011.
I'd have thought that was obvious, and had been for a
decade or so. EFF broke DES in a few days for
$200,000 ten years ago. A 64-bit cipher is only
256 times harder, easily within reach on an
intelligence agency budget.
Copacobana break DES in a week for 9,000 euro.
256 of them would break a 64-bit cipher in a
week. This is within reach for a high-stakes
industrial espionage situation, say Boeing
and Airbus competing for big orders.

@_date: 2009-11-07 09:13:12
@_author: Sandy Harris 
@_subject: TLS man in the middle 
I'm in China and use SSL/TLS for quite a few things. Proxy connections,
Gmail set to "always use https" and so on. This is the main defense for
me and many others against the Great Firewall.
Should I be worrying about man-in-the-middle attacks from the Great
Firewall servers?

@_date: 2009-11-11 10:03:45
@_author: Sandy Harris 
@_subject: hedging our bets -- in case SHA-256 turns out to be insecure 
NIST are dealing with that via the AHS process. Shouldn't you just use
their results?
Yes, but there's also a risk that whatever you come up with will turn
out to be flawed.
This requires two hash(x) operations. A naive implementation needs
two passes through the data and avoiding that does not appear to
be trivial. This is not ideal since you seem very concerned about
What about this construction:
  C(x) = H1(H2(x) || H3(x))
H1 is something that gives the output size you require. Use SHA-256 or
choose an AHS candidate conservatively. This only hashes a few blocks
so you need not worry much about overheads here.
H2 is the 512-bit variant of a different AHS candidate, or Whirlpool, or
even Skein-1024. Here speed is a criterion, though of course not the
only one.
H3 might be some really cheap fast function invented for the situation.
As I recall, the GOST hash just used a sum of input blocks, and that's
enough to defeat the multi-block attacks. If it is simple enough, you
can code it into your implementation of H2 so you only need one
Since you are encrypting the files anyway, I wonder if you could
use one of the modes developed for IPsec where a single pass
with a block cipher gives both encrypted text and a hash-like
authentication output.  That gives you a "free" value to use as
H3 in my scheme or H2 in yours, and its security depends on
the block cipher, not on any hash.

@_date: 2009-11-17 12:39:09
@_author: Sandy Harris 
@_subject: hedging our bets -- in case SHA-256 turns out to be insecure 
I was suggesting using the authentication data in the construction:
 C(x) = H1(H2(x)||A(x))
where H1 is a hash with he required output size, H2 a hash with
a large block size and A the authentication data from your
This is likely a very bad idea if you already use that data in some
other way, e.g. for authenticating stored data. However, if C is
going to be your authentication mechanism, then this might be
a cheap way to get one input to it.

@_date: 2010-07-25 08:34:45
@_author: Sandy Harris 
@_subject: Intel to also add RNG 
Yes. A hardware RNG seems an obvious Good Thing. Not
a complete solution, but a very useful component.
IPsec gateways and web servers doing a lot of SSL are obvious
cases. Neither has much mouse or keyboard activity, they may
have solid state drives or smart RAID so disk timings are not
random. Packet timings might be somewhat random, but they
may also be knowable by an enemy.
In some cases, a non-kludge alternative is Turbid:
That uses a sound card or on-board equivalent. Some boards
will have this, or it is cheap & easy to stick in a slot.

@_date: 2010-06-03 22:39:20
@_author: Sandy Harris 
@_subject: What is required for trust? 
India recently forbade some Chinese companies from bidding on some
cell phone infrastructure projects, citing national security concerns:
Of course, the Chinese gov't and companies are by no means the only
ones one might worry about. AT&T and other US telcos have given
customer data to the NSA. What about fear of NSA trickery in Lucent
products? Or French intelligence in Alcatel? Or Israeli or Taiwan or
whoever? In all cases, you can argue about how plausible such threats
are, but it seems clear they are not utterly implausible.
Nor are the companies the only threat. Cisco and many other firms have
factories in China; if you are worried about Huawei colluding with
government here to spy on or sabotage other nations, then you likely
have to worry about that government slipping a team into Cisco staff
to subvert those products. I don't think this threat is realistic, but
I could be wrong.
The main devices to worry about are big infrastructure pieces --
telephone switches, big routers and the like. However, those are by no
means the only potential targets. Small home routers and various
embedded systems are others.
So, if one is building some sort of hardware that people may be
reluctant to buy because of security concerns, what does it take to
reassure them? Obviously, this is going to vary with both the
application and the people involved, but can we say anything useful in
Standard components help. If you use IPsec, or AES, or a commodity
processor, I can have some confidence in those parts, though I'll
still worry about other things. Use your own protocol or crypto
algorithm and I definitely won't trust it without publication and a
lot of analysis. Put big lumps of your own VLSI on the board and I'll
worry about what might be hidden in them.
Openness helps. Put an open source OS on the thing and give me the
application code in source for auditing. If you must use some VLSI or
FPGA parts, publish source for those.
Auditing helps. Intel got outsiders to audit their random number
generator. This is probably needed for some critical components, but
All of those help, but are they enough? If not, what else is needed?
Or is this an impossible task?

@_date: 2013-08-26 17:40:26
@_author: Sandy Harris 
@_subject: [Cryptography] Using Raspberry Pis 
Two things to look at. Onion Pi turns one into a WiFi hotspot & Tor input node:
Freedom Box is working on low-power home servers with goals
overlapping yours. They use a different machine as their
reference server, but it should work on Pi. There is some
discussion in their mailing list archive:

@_date: 2013-12-19 14:05:41
@_author: Sandy Harris 
@_subject: [Cryptography] Fwd: [IP] 'We cannot trust' Intel and Via's 
One more is:
Also, if you have an unused sound device or can add one,
a very good quality and quite high volume source is:

@_date: 2013-12-19 16:21:42
@_author: Sandy Harris 
@_subject: [Cryptography] Chinese Cryptography 
I worked for a while as an editor at Shanghai Jiatong U, in Lai's department,
improving the English in papers that various people there were sending off
to conferences and journals. The English ranged from appalling to excellent.
The range of topics I saw papers on was very broad, as was the range of
journals & conferences they were going to. The quality of the work, as far
as I could tell, varied fairly widely as I think it might at any university.
However, I can say quite definitely that some of the grad students doing
crypto stuff with Lai and others were very sharp indeed.

@_date: 2013-12-23 23:45:10
@_author: Sandy Harris 
@_subject: [Cryptography] What do we know? (Was 'We cannot trust' ...) 
That might be the case for many big businesses, especially US
companies, but it clearly would not be for someone like Airbus who
compete directly against US firms with ties to the Defense Department.
Or for foreign governments who are also potential customers. There are
various other cases for big business, and RSA want to sell to smaller
businesses too, so it is hard to estimate how badly this hurts the

@_date: 2013-11-01 14:25:39
@_author: Sandy Harris 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
Perhaps, but we also know that many (I think nearly all)
crypto protocols rely on random numbers so many that
are otherwise thought secure fail if the RNG does.
PGP generates a random key for each message. Use
a sufficiently bad RNG and PGP is easily breakable.
Use one with any weakness at all that the attacker
knows about and an attack on the block cipher is
cheaper than it should be.
The Diffie-Hellman key negotiation protocol used
in IPsec and other things requires that each player
generate a random number. It can be broken if
either RNG is weak.
There are other examples. The problem is not so
much that RNG attacks are known to be widespread
as that, if they do occur they can be very serious.

@_date: 2013-11-07 13:07:52
@_author: Sandy Harris 
@_subject: [Cryptography] randomness +- entropy 
As John says, the right solution in that case is almost certainly to
boot from USB instead so you can have some writable storage than can
hold a seed file between reboots. In other situations -- a Linux
smartphone or an embedded system with severe limitations -- none of
the known-good solutions may work. No on-board hardware RNG, no free
sound device for Turbid, no writable storage for a seed, ...
In those situations, it seems worth looking at RNGs based on various
sorts of timing jitter. At least two people on the list have written
something along those lines. My maxwell
(ftp://ftp.cs.sjtu.edu.cn:990/sandy/maxwell/) was specifically
designed for such limited systems. Stephan's jitter
( and Havege
( are more general. I am not
entirely convinced that these can be secure against an attacker with
enormous resources, but breaking them does not look anywhere close to

@_date: 2013-11-13 19:46:17
@_author: Sandy Harris 
@_subject: [Cryptography] Fwd: [Cfrg] Fwd: New Non-WG Mailing List: dsfjdssdfsd 
Reply-To: ietf at ietf.org
A new IETF non-working group email list has been created.
List address: dsfjdssdfsd at ietf.org
To subscribe: Purpose: The dsfjdssdfsd list provides a venue for discussion of
randomness in IETF protocols, for example related to updating RFC 4086.
For additional information, please contact the list administrators.
Cfrg mailing list
Cfrg at irtf.org

@_date: 2013-10-16 08:49:53
@_author: Sandy Harris 
@_subject: [Cryptography] /dev/random is not robust 
Yes. it is not at all clear that their analysis actually matters:
" Several security notions have been defined:
" ? Resilience: an adversary must not be able to predict future PRNG
outputs even if he can influence the entropy source used to initialize
or refresh the internal state of the PRNG;
" ? Forward security ( resp. backward security): an adversary must not
be able to predict past (resp. future) outputs even if he can
compromise the internal state of the PRNG.
" ... Barak and Halevi [BH05] model a PRNG with input ... and define a
new security property called robustness that implies resilience,
forward and backward security. This property actually assesses the
behavior of a PRNG after compromise of its internal state ...
None of this matters much if the enemy does not already have root on
your system. If an enemy does have root, he has far better targets
than the RNG available and the defenders have bigger worries. Without
root, he cannot see the internal state and, if you use the typical
setup where some saved entropy from last time is pumped in by the boot
scripts, he cannot read that file and using it seems to complicate the
state enough for security. It would take a completely different
analysis including a very clever new attack to show a problem there,
and this paper does not even attempt that. Moreover, having looked at
the driver internals, I do not think it is even possible.
Also, the definition of resilience mentions an adversary who "can
influence the entropy source" but the random(4) driver uses multiple
sources so the degree of influence is generally limited. There are
cases where this is a serious concern -- for example a router does not
have keyboard, mouse or hard disk, some skip the initialisation from a
file and an enemy can monitor or even control network inputs. However,
those are fixable; just using a file of saved entropy goes a long way
toward solving the problem.
There are a number of good ways to get additional entropy sources. My
attempt at writing one and a paper that comments on several others are
Another along similar lines but not mentioned in that paper is:

@_date: 2013-10-18 08:12:34
@_author: Sandy Harris 
@_subject: [Cryptography] OpenSSL not using /dev/random (was: Re: 
Snowden revealed that the NSA does sabotage things for easier
monitoring, OpenSSL would be a prime target, and a plausible
attack on RdRand has been published.
random(4) can use RdRand, but it sensibly treats it as only
one of many entropy sources, so even a sabotaged RdRand
is not fatal. I'd say it is quite clear OpenSSL should do that
as well. The simplest way to do that appears to be to use

@_date: 2013-10-19 10:17:04
@_author: Sandy Harris 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
Yes. I think urandom must block initially, though not at any other time.
There is a function add_device_randomness() with this
introductory comment.
 * Add device- or boot-specific data to the input and nonblocking
 * pools to help initialize them to unique values.
 *
 * None of this adds any entropy, it is meant to avoid the
 * problem of the nonblocking pool having similar initial state
 * across largely identical devices.
 */
I have not looked in detail; at first glance I suspect this
function could & should do more. But it is there.

@_date: 2013-10-19 12:55:29
@_author: Sandy Harris 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
I don't think anyone imagines that those provide an adequate
seed or more than a few bits of entropy at most. However,
they do have useful effects.
Mixing in the MAC addresses ensures that when a bunch
of routers all have the same ROM image or a bunch of
machines all get the same install from CD or USB, then
at least to some extent, they all behave differently. John's
fix for the CD/USB problem is far better, but it is not
certain to always be applied & won't work for masked
ROMs, so this is still worth doing.
Mixing in the clock makes a machine behave a bit
differently each time it is rebooted.  Again, there
are better fixes such as mixing in a saved file, but
again this is still worth doing.
These are reasonably cheap and done only once
at boot time. They can do no harm and are useful
in at least some cases, so worth doing.

@_date: 2013-10-21 18:50:54
@_author: Sandy Harris 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
There are at least half a dozen programs about that some
claim might replace random(4) or be used as an extra
source of entropy for it. I have written one, and the PDF
file on its page discusses several others, including
If it is avoidable, I would not want to trust any of
those (or anything else, really) as a sole source
of entropy, even though as far as I can tell Turbid
is close to ideal and the others seem plausible.
As I see it, the only way to be confident in the
face of risks like the NSA fiddling with RdRand
or Turbid being messed up by a hardware
failure or virtualisation is to use multiple
sources and have something pretty much
like the random device to cache, buffer and
mix those inputs.

@_date: 2013-10-24 17:42:55
@_author: Sandy Harris 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
There can be problems with inadequate initialisation of the Linux
random(4) driver, as demonstrated by the research that found many RSA
keys using duplicate primes and attributed their creation mainly to
badly configured routers. As several people have pointed out,
discussion of crypto without a threat model tends to be pointless.
This is an attempt to provide an explicit model for those threats.
This deals only with threats from weak initialisation; I do not
discuss general entropy gathering and estimation or the new threats
that may arise with virtualisation.
If Linux is configured in the usual way, there is a file of saved
random data that is pushed into /dev/random by boot scripts. On some
systems there is a high-volume entropy source -- RdRand
( Denker's Turbid
( Havege
( my maxwell
(ftp://ftp.cs.sjtu.edu.cn:990/sandy/maxwell/), Mueller's jitter-based
generator ( ...
In those cases, provided that the file or source is secure, the
initialisation question is simple; all we need to determine is whether
urandom should block until the system gets enough entropy to put it
into a secure state. I think it should, but that is not the question
What I want to model here is the harder case where none of the above
is in place or urandom starts producing output without waiting for
them. In an ideal world, this case would never arise but in the real
world it has and likely will again. The problem involves avoiding
duplicate outputs, so the birthday paradox works against us. If we
need a low risk of collision for 2^n cases, then we need about 2n bits
of input entropy.
There are two separate cases to consider: avoiding duplicate outputs
when a single device is rebooted many times and avoiding it when many
identical devices are deployed.
Multiple reboots of the same device are the easy case. One threat
involves a computer that is rebooted often (perhaps several times a
day) and whose outputs are monitored continuously over a fairly long
period (perhaps a year). Another involves an enemy that can force many
reboots of a network device without the administrators noticing and
blocking the attack. Neither threat looks at all likely to involve
more than a few thousand reboots, so a dozen or so bits of timing
information per reboot should be enough to block them.
Timing data appears to be the only thing we can use against this
threat; more-or-less everything else is constant across reboots. Linux
random already mixes in some timing information, but I am not certain
how much.
Many identical devices are a harder case. A manufacturer might produce
tens or hundreds of thousands of the same device and a Linux distro
might send out some huge number of installation images; allowing for
the birthday effect, we then need a fairly large amount of entropy.
For example, if there are 64K (2^16) devices, we need at least 32 bits
to make duplication unlikely. Something like 48 bits would probably be
enough for all cases that seem at all likely, though of course if we
can get more good bits, we should.
More-or-less anything that is unique per system will help here. System
serial number if there is one accessible, MAC addresses, IP addresses,
Just including a file of stored entropy does not solve this problem if
that file is the same on all devices or in all images. There is
available code to add different files to CD/USB images:
 at metzdowd.com/msg11552.html
However, it is not clear that this could be applied at reasonable cost
in manufacturing and in the case of downloadable images using this
would make every image unique which would complicate the use of hashes
to verify the downloads. For large numbers of images it might also
involve significant loads on the server.
Also, keeping a stored entropy file secure is much harder if it is
burned into a CD or flash image and can be attacked anywhere in the
distribution chain than for a file that changes on every boot and can
be attacked only with root privileges on the target system. Using
different files looks like a fine solution in some cases -- for
example if you are creating USB keys to set up a few dozen servers
within an organisation -- but it cannot help with all cases.
There are two ways one might get suitable material into the driver
state. One can build it into the kernel's device initialisation code
or do it externally with a script along the lines of:
    ifconfig > /dev/random
    netstat > /dev/random
    uname -a > /dev/random
    ....
Gutmann has written a full RNG along these lines as part of Cryptlib:
That works just fine on a reasonably complex multi-user multi-process
system, but it is not clear that it would be adequate on a very
limited system such as a router which might have no users and small
non-varying sets of devices and processes.
However, if all we need is a few bits to help initialise the random
device, then something like the script above may fit. We do not need
any mixing since the driver does that. Nor do we need the full range
of entropy sources that Gutmann's code uses, or its careful error
checking. No doubt my first-guess code above could be improved by
taking ideas from Gutmann, but we do not need his whole system.
In general, building such operations into the device initialisation
code in the kernel is preferable to leaving it to an external script.
The environment is better known; there is no chance of writing a
script that relies on some program the target system turns out not to
have. Also, there is less chance of user error; the system
administrator cannot just disable the script in search of efficiency
or to get rid of an annoying error message.
There may be an exception. The random driver is generally initialised
early in the boot sequence so that it is ready before other programs
need it; that is best done within the driver. On the other hand,
initialisation based on overall system state may be best done later,
after everything else has been set up. Recognising that situation may
be hard to do from within the kernel, but it is trivial in the init
scripts; making a script along the lines above the very last thing the
init scripts do is straightforward. It is not clear that this would be
useful, but it is easily done if it is.
It looks as though getting 64 bits or more is possible. One of the
FreeBSD guys wrote on the other crypto list:
" We also added entropy based on device attach times. Measurements show this
gives at least 4 bits of entropy per device (usually a lot more), and in
the worst case we saw, 32 devices were measured.
Ted is aware of this and says looking at something similar for Linux
is on his ToDo list. If something along those lines can be built and
the claim of four bits per device (or even two) is correct for Linux,
it appears that it would solve the problem.
If not, there may be other options. Any volatile kernel table --
processes, open files, devices ... -- might provide some entropy and
it would be possible to instrument some system processes (init?) or
system calls so they did as well.

@_date: 2013-10-28 12:35:29
@_author: Sandy Harris 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
Only an attacker who can log into the system. That blocks
most remote attackers, and something like a router (the
sort of system where this is most likely to be an issue)
should have tightly restricted logins.
Sure, and I definitely am not saying either that this sort of
thing should be given entropy credit or that you don't need
better sources as well. My only question is whether this is
useful as a worst case fallback measure.
Yes, and uname -a includes a timestamp.
This looks to be a real problem, or at least a restriction on
which things can be used there. Perhaps Ted can comment?
User space programs can inject data at any time, but it may
come too late if the driver produces output early and it may
be quite unnecessary once other entropy sources have made
some contributions. The stuff I suggest above is certainly
possible, but it may not actually be useful, let alone needed.

@_date: 2013-10-28 16:38:45
@_author: Sandy Harris 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
We all agree here, at least for /dev/urandom
My understanding, though is that /dev/random uses a different
model requiring a one-to-one relation between (estimated) input
entropy and output size. This makes it safe even if the crypto
used is flawed. That is clearly a desirable property. Whether it
is essential seems debatable, but this is not the place for that
In the worst case the attacker has subverted a network switch or router
so he or she knows all packet timings down to the resolution of that
device's clock. There might still be some tiny amount of entropy in
that case due to things on your machine like cache & TLB misses or
having to wait for a lock and lack of sync between your clock and
the attacker's, but basically, you are screwed.
We may also need to distinguish between an attacker who can get
root privilege on the running machine (with any sane configuration
non-root access will not pose a threat) and one who steals or seizes
the machine later.
Using an encrypted file system blocks seizure attacks, but that is
inconvenient in many situations and in some jurisdictions there
may be heavy pressure to give up the password.
As I see it, the current recommended Linux system of saving 4K bits of
new installs that have no history to get the file from.
This should be clear; PFS is a design requirement here. Given the
machine including the saved file, it should be impossible to infer
previous state. Since all output goes through SHA-1 which is
generally thought to have pre-image resistance, it seems clear
enough to me.
How about pushing /etc//shadow into /dev/random? Except for a
fully automated install that gives exactly the same accounts and
passwords on every device, that is bound to have some variation
that is unknown outside.

@_date: 2013-10-31 13:44:03
@_author: Sandy Harris 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
Yes, but the authentication is only that it must come from a process
that is running as root. There are a lot of candidates. Four people
I know of on the list (Peter Gutmann, John Denker, Stephan Mueller
and me) have written something that could be used, and I think
there are at least another half dozen available. Then there are
various ones built into CPUs or chipsets.
Quite likely not all of those are as solid as their authors hope, and
even the ones that sometimes are might fail in other situations.

@_date: 2013-09-10 14:01:44
@_author: Sandy Harris 
@_subject: [Cryptography] Thoughts on hardware randomness sources 
I have not looked at that. A well thought out & well documented
RNG based on a sound card is:
I wrote a very simple one (perhaps not yet trustworthy because
it has not had much analysis) based on timer calls. Its documentation
discusses a few others:

@_date: 2013-09-14 22:10:56
@_author: Sandy Harris 
@_subject: [Cryptography] real random numbers 
Let me a try a different way of stating (what I think is) Denker's point.
Discussing Denker's Turbid, found at:
The unique advantage of Turbid is that it provably delivers almost
perfectly random numbers. Most other generators ? including mine,
random(4), and the others discussed in this section ? estimate the
randomness of their inputs. Sensible ones attempt to measure the
entropy, and are very careful that their estimates are sufficiently
conservative. They then demonstrate that, provided that the estimate
is good, the output will be adequately random. This is a reasonable
approach, but hardly optimal.
Turbid does something quite different. It measures properties of the
sound device and uses arguments from physics to derive a lower bound
on the Johnson-Nyquist noise [3] which must exist in the circuit. From
that, and some mild assumptions about properties of the hash used, it
gets a provable lower bound on the output entropy. Parameters are
chosen to make that bound 159.something bits per 160-bit SHA context.
The documentation talks of ?smashing it up against the asymptote?.
{End quote)
The difference is real and it seems quite clear that an RNG with a
provable bound is preferable to one where analysis must rely on
assumptions about or estimates of input entropy. For a rather large
subset of servers -- basically any that have an unused sound card
equivalent or can easily add one -- Turbid should be the first thing
considered for use as an RNG. It is open source, so auditable, and
uses hardware that appears unlikely to be subject to fiddling by
three-letter agencies of any government.
The basic design of RDRAND looks like it could be proven secure in
much the same way, but with the Snowden revelations   plus this paper
it becomes harder to trust.
All that said, I still want to use something like Linux random(4) with
a large pool and multiple entropy sources.

@_date: 2013-09-17 11:41:33
@_author: Sandy Harris 
@_subject: [Cryptography] The paranoid approach to crypto-plumbing 
A paper of mine on combining a stream cipher with a block
cipher: AES-256 uses 14 rounds vs. 10 for AES-128, so it is about
40% slower. Given 256 bits of key and a stream cipher that
is 5x faster than AES, you can use AES-128 and have 128
bits to key the stream cipher. AES-128 plus whitening that
changes for every block (two 128-bit blocks of stream
cipher output) has roughly the same cost as AES-256.
There are several ways to reduce the cost and/or increase
the security from there; see the paper for details.
I am still working on this notion and will have a new and
much improved version of that paper sometime this year.
Anyone I know moderately well who wants to review it
can contact me off-list for the current draft.

@_date: 2014-04-02 22:08:23
@_author: Sandy Harris 
@_subject: [Cryptography] Preliminary review of the other Applied 
Both fine books & well worth reading. Arguably some of Schneier's
later books are even better.
However, if your main interest is how to build secure systems, I'd
put Anderson's "Security Engineering" at the top of the list:

@_date: 2014-04-10 13:34:05
@_author: Sandy Harris 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
Here's an alternate take saying don't do your own buggy allocation and
lose the error checking in system one.
A fine suggestion.
One reference has:
"As for databases and real-time programming, cryptography looks
deceptively simple. The basic ideas are indeed simple and almost any
programmer can fairly easily implement something that handles
straightforward cases. However, as in the other fields, there are also
some quite tricky aspects to the problems and anyone who tackles the
hard cases without both some study of relevant theory and considerable
practical experience is almost certain to get it wrong. This is
demonstrated far too often."

@_date: 2014-04-16 09:42:50
@_author: Sandy Harris 
@_subject: [Cryptography] I don't get it. 
There are plenty of such tools available. Modern compilers are often better
at this than older ones, especially if you enable the right options. Compiling
with two or more different compilers is a good check. So is running the code
through one of lint(1)'s descendants.
Some of this could be built into source code management, release
build scripts or make files. Require a clean compile with certain
options and a clean run of lint before a submission is accepted,
for example.
Some concerns can be raised with just a simple grep or a bit of perl
code to do your own checks. My first cut at this, for example, is a
< 10-line program that finds > 3800 files in the Linux kernel where
the word "switch" occurs more often than "default". Nowhere near
all of these will be actually be problematic, but they are worth a look.

@_date: 2014-04-16 10:27:39
@_author: Sandy Harris 
@_subject: [Cryptography] Simpler programs? 
Adding extra features increases the risk of various sorts of bugs
including security holes. The heartbleed bug was introduced when
adding a heartbeat feature, some versions of PGP acquired a nasty
vulnerability back in version 5.5 (late 90s, long since fixed) when an
"additional decryption keys" feature was added, and so on.
How much would using simpler programs reduce that attack surface?
For example, I once saw a  T-shirt at a Usenix conference that read
"Real cats don't have options" and checking the Plan 9 man page for
cat(1) I find that theirs has none. The FSF version on my Linux box,
on the other hand, has 11. Would replacing FSF programs with Plan 9
ones give a more secure Linux distro? Or using the FSF stuff but
removing or disabling all non-Posix options?
Or is this an argument for switching to Plan 9, to one of the BSDs or
to some Linux distro I am not aware of?
Is it an argument for a simpler ANSI-only compiler with no extensions?
Or simpler headers -- ctype.h is 350 lines on my system, stdio.h 947

@_date: 2014-04-19 12:31:06
@_author: Sandy Harris 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
One criterion, I think, is that forward secrecy is a MUST.
I'd also have MUST support AES, SHOULD support the
other AES finalists with open licenses (Twofish, MARS &
Is it perhaps time for another look at Photuris? That was a simpler
alternative to IPsec, might still have useful ideas to offer. There
are RFCs.
Definitely look at the JFK (Just Fast Keying) work, a
simpler alternative to IKE. There are academic papers
and were once Internet drafts; I do not know current
state of those.
Can we get rid of certificates instead? FreeS/WAN did not
use them; it just put hex representations of raw RSA public
keys in DNS reverse maps. The current work on DANE
may make this approach more feasible and widespread
deployment of DNSsec would make it more secure.
Can we punt that over to DNSsec and DANE?

@_date: 2014-04-21 17:57:23
@_author: Sandy Harris 
@_subject: [Cryptography] It's all K&R's fault 
Do we need swap on current systems? Both server and desktop
boxes now have gigabytes of RAM and I suspect that phones
and such do not have enough or fast enough storage to make
swap very useful. Why not just use an OS that does not swap?

@_date: 2014-04-22 21:19:34
@_author: Sandy Harris 
@_subject: [Cryptography] It's all K&R's fault 
A classic exposition of the basic Unix ideas:
A Unix FAQ document:

@_date: 2014-04-25 13:11:24
@_author: Sandy Harris 
@_subject: [Cryptography] Open Source developer employment agreements, 
You can also use technical writers. As in any field, the bad ones are
useless or even damaging. The good ones, though, definitely have quite
a bit to contribute. Not only do they take a lot of the documentation
load off developers, leaving them free to do what they are best at,
they are also the first to use the product, so informal testers and QA
people. I've caught more than one gratuitous user interface idiocy,
things that developers had not thought of, that way. I've also found
oodles of early-version bugs; with the worst I crashed a several
hundred user mainframe three times just by using an editor before
developers admitted maybe there was a problem.
Of course even good writers can be made useless by developers who
don't care enough to talk to them or, more often, by bad management.
In years as a tech writer, though, I've found that Dijkstra was mostly right:
"Besides a mathematical inclination, an exceptionally good mastery of
one's native tongue is the most vital asset of a competent
Yes, I've encountered illiterate programmers and engineers galore.
However in my experience, the really good ones are almost invariably
competent writers as well. Then the question is what they should write

@_date: 2014-08-14 15:38:16
@_author: Sandy Harris 
@_subject: [Cryptography] Dumb question -> 3AES? 
Sure, but there may be better alternatives.
Another post suggests AES-X, which is much cheaper; so is AES-256.
Either is secure against brute force and the Even-Mansour paper and
various follow-ups show that a whitened construction is secure against
a broader range of attacks.
The reason double DES is ineffective & 3DES only gives 112-bit
security is a meet-in-the-middle attack. That attack depends on
the two parts using different keys. Construct a big key schedule
that keys both parts from the same base key and doubling is
safe. AES-128 has ten rounds & 11 round keys, AES-256 14
& 15. Another AES candidate, Serpent, used 32 rounds so its
key schedule would give enough keys for a safe double AES.
My Enchilada paper suggests combining AES with the ChaCha
stream cipher, and argues that this is highly secure:

@_date: 2014-08-23 11:04:43
@_author: Sandy Harris 
@_subject: [Cryptography] On 40-bit encryption 
I'd say the main reason for the relaxation in the US was the Bernstein case
Dan Bernstein (grad student at Berkeley, later prof at U Illinois), supported
by the EFF, sued the US government arguing that, since code can be
read and discussed by humans, it qualifies legally as speech, so export
restrictions on source code are an unconstitutional restriction on free
speech. He won in the first court & again on appeal. There is an archive
of case documents with links to related cases:
At that point, the government changed their regulations -- moved the
controls from the State dep't to Commerce and allowed export of
"public domain" code, provided the Commerce dep't is notified. As
I see it, they were scrambling to salvage anything they could and
desperately trying to avoid having the Bernstein case make it to
the Supreme Court and perhaps overturn the regulations completely.
The EFF says the requirement for notification is still unconstitutional,
but I have not heard of any court case over that.

@_date: 2014-02-11 17:58:36
@_author: Sandy Harris 
@_subject: [Cryptography] Unified resource on Random Number Generation 
Doesn't RFC 4086 already cover that ground?
There has been some discussion on one of the lists of a revision of
that RFC. Anyone with a contribution to make might contact the authors
or find the appropriate IETF list and comment there.

@_date: 2014-07-22 11:04:47
@_author: Sandy Harris 
@_subject: [Cryptography] hard to trust all those root CAs 
What about restricting the Chinese CA to signing certs in .cn and imposing
similar restrictions on other CAs?

@_date: 2014-06-08 11:55:55
@_author: Sandy Harris 
@_subject: [Cryptography] Java: The 1990s called, 
As far back as the late 90s, FreeS/WAN refused to implement
things the IPsec RFCs required that we considered insecure:
single DES, null encryption, and 768-bit DH Group 1. This
caused almost no compatibility problems since more-or-less
all implementers provide 3DES and 1536-bit DH Group 5.
These are complete no-brainers, should have been fixed
in the RFCs.
FreeS/WAN did implement an option for IPsec without
forward secrecy, though PFS was the default. That was
required for some compatibility reasons, though it is
another thing the RFCs should have disallowed.

@_date: 2014-03-05 22:28:26
@_author: Sandy Harris 
@_subject: [Cryptography] Silly Diffie-Hellman question using XOR 
One introduction is here:
Not exactly. You can do DH without authentication, e.g. see
That is secure against passive eavesdroppers.
However, to resist active attacks (the enemy can alter packets
or send his own), you need authentication. It does you no
good at all to communicate securely so that only the
recipient can read things if you do not know who that
recipient is,

@_date: 2014-03-16 17:47:32
@_author: Sandy Harris 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
That's not enough. For a block cipher, tell me Callas designed it,
Knudsen says he cannot break the algorithm, and Bellovin has
approved the proposed usage, and you are pretty close.
You also need a gang for hashes -- Preneel and who else? Then
one for random number generators.
I think the problem is that you include DSA. As I see it, that should be
deprecated now and removed from future versions all standards as
hopelessly flawed. It is just too fragile.
I do. Better, take all of them with open licenses, which I think is all
except RC6. Anywhere that AES is a MUST, make them all
SHOULD. Cost is low since there are open source implementations.
So use Serpent, apparently the most conservative choice among
the AES candidates.

@_date: 2014-03-18 15:48:09
@_author: Sandy Harris 
@_subject: [Cryptography] Use process ID in mixing? 
A process ID is only a few bits long and in many cases is quite
predictable; it is entirely useless as an entropy source. However, I
wonder if it could play a role analogous to salt in a password
algorithm or the suggestion of stirring things like MAC addresses into
the pool at startup just so every machine does it slightly
On Linux, you can get the caller's pid from kernel code with   then look at current->pid. Probably there is something
similar for other systems and quite possibly there is other usable
data in the struct; I haven't looked.
Is it worth salting every call to (u)random? Mix the pid into the
output or the pool. This can do no harm, but does it do any
perceptible good?

@_date: 2014-03-19 07:50:38
@_author: Sandy Harris 
@_subject: [Cryptography] Use process ID in mixing? 
True, but is it worth throwing in PID as well?
No, but it applies whenever a user program reads or
writes either device so it could be used as salt then.
Yes, that must be done at boot time and I think the
current Linux code does it for interrupts as well, which
is a fine idea. MAC addresses can, and I think should,
also be used at boot time to make each system unique.
Mixing in PID info for every read/write call clearly would
not be of huge value. However the value doesn't
appear to be zero either, they depend on different
parts of system state than interrupts do, and the
overhead looks to be moderate. Is it worth doing?

@_date: 2014-03-19 19:31:13
@_author: Sandy Harris 
@_subject: [Cryptography] Italians invent SHA-7 in 2011 
The designer also seems to be a founder & CEO
over here: Claims to use "the new encryption algorithm SHA-7,
faster and more secure than any other technology
now known for encryption of data."
But then "SHA-7 allows to generate a unique
"message digest" of the content of a message",
So it is a hash, not an encryption algorithm.
They compare performance to SHA-2 and do
not even mention SHA-3. Perhaps they missed
the memo?
This does not inspire confidence.

@_date: 2014-03-24 19:53:08
@_author: Sandy Harris 
@_subject: [Cryptography] The role of the IETF in security of the 
the net?
There are already some policy documents, as well as the standards
track & BCP security documents:
RFC 1984 (best number choice on record?)
IAB and IESG Statement on Cryptographic Technology and the Internet
RFC 2804 IETF Policy on Wiretapping
There may be others that I don't know about.
But yes, some sort of Magna Carta would be a good idea and
the IETF would in some ways be a good place to develop one.
The trick would be to avoid most of the politics and keep
discussion to the technical issues.

@_date: 2014-03-26 11:49:41
@_author: Sandy Harris 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
One thing to worry about is whether that exposes your system to a
meet-in-the-middle attack. e.g DES has a 56-bit key so one might
expect that applying it twice would give 2^112 security and there is
probably some clever method for which that is true. The obvious method

@_date: 2014-03-26 14:06:45
@_author: Sandy Harris 
@_subject: [Cryptography] [RNG] Use process ID in mixing? 
My post that started this thread went to two lists, this one and an
rng list. I failed to set a Reply-to: header (I have yet to discover
how to do that in Gmail) so various people have replied on each list
without including the other. Here is a message I sent only to the rng
list, answering a comment there.
I suggest future discussion should go to the rng list.
---------- Forwarded message ----------
The essential requirement is enough real entropy for good
seeding, ideally a hardware RNG plus the stuff from device
interrupts. You also need good initialisation, ideally the
per-device provisioned seed Denker suggests plus the
file of stored output from the last run.
Stirring in timestamps is important, both at boot time to
make every instance different and later. I think the current
Linux code includes time in the code that collects entropy
from interrupts.
Those are the key concerns, but adding other things as
a defense-in-depth method also looks worthwhile. Stir
in more stuff which acts like salt -- just makes instances
different and complicates attacks a bit rather than being
expected to contribute entropy. These are all knowable
or guessable for at least some enemies, but perhaps
not all. They are cheap to add, cannot do harm and
may do some good.
In order of importance, the ones I can see are:
Including MAC addresses or other hardware identifiers
in boot time mixing makes every machine different.
When a user space process reads the device, the
kernel has access to a struct describing the process.
Mix in PID and anything else in the struct that looks
interesting. This depends on system state in ways
that may be quite complex on some systems, and
on different parts of the state than the timing data.
The driver contains a lot of variables. which could
be initialised at compile time. Using data from
the development system's /dev/urandom could
make those different for every compilation. In the
common cases where a single compiled image
gets installed on many devices or put into a
distro, this does not do much good, but at least
it makes every release use somewhat different
device code.

@_date: 2014-03-30 12:03:51
@_author: Sandy Harris 
@_subject: [Cryptography] A possibility for random device drivers 
I have been thinking about the design of the Linux random(4) device
and related devices and I have a suggestion that seems worth a look.
There is nothing like a finished design here, just a pointer to a line
that may be worth exploring.
The Linux  device generates output by running a hash, SHA-1 in the
current driver, over the pool. Yarrow just uses a single hash context,
in effect running the hash over the input. Fortuna uses several such
contexts. There may be some other variations but I think more-or-less
everyone uses a hash.
My question is whether it might be equally effective and somewhat more
efficient to use the authentication code from AES-GCM or a similar
system instead. These authenticators are designed to replace hashes,
specifically HMAC, for packet-level authentication in various
protocols, and efficiency is one of their main design goals.
AES-GCM is the commonest; there is a NIST standard for it and RFCs for
using it in IPsec, TLS and SSH. There are several other such
authenticators in use as well, and the CAESAR competition for new
authenticated cipher modes has proposals for more.
The GCM code uses a 128-bit quantiity H as a multiplier in its mixing,
and in normal use it gets that from AES encryption of an all-zero
block so, since the AES key is secret, H is effectively random. I do
not know if using a fixed H would make the method insecure or how a
pseudorandom H could best be provided in random(4).

@_date: 2014-05-19 09:58:44
@_author: Sandy Harris 
@_subject: [Cryptography] updating a counter 
There are quite a few applications for block ciphers in counter mode,
but for large block sizes it looks as though a simple counter is not
ideal. Can we discuss better ways?
With a straight counter only  a few bits change on most iterations and
the high bits almost never, even if the counter is initialised
randomly. If you start from zero, rest the counter when rekeying, and
rekey at some sensible interval like 2^32 iterations, 96 bits of a
128-bit counter or 224 bits of a 256-bit one will never change. This
may not break things, but it cannot be a good idea to use a series of
values with small Hamming differences and many known bits.
Simple improvements include random initialisation, not resetting when
rekeying, and adding a constant instead of just incrementing. Are
those enough? Optimal?
There are many transformations that might be applied to a counter to
get faster changes. Rotations, byte or word swapping and bit reversals
are easy to implement and change lots of bits. Pseudo-Hadamard
transforms can be done for any size block. The Aria cipher and the W
cipher in the Whirlpool hash each have a transform that mixes 128
bits; those must be reasonably efficient since they are used in every
round. GCM authentication also includes an efficient 128-bit mixer.
Should one of those be applied to a counter? Which? Would that
complicate the analysis of counter mode?
My solution would look like this, where p[4] is four 32-bit words for
a 128-bit counter.
Keep switch() labels < 256 to accommodate limitations on some
machines. Pick five numbers  (I'd use primes just because), 251 as
largest prime < 256 and four others in interval 0 <= x <= 251, with
values roughly 50, 100, 150, 200 so they are evenly spaced.
    switch( iter_count )    {
        /*
        mix three array elements
        each element is used
        once on left, once on right
        pattern is circular
        */
        case 47:
            p[1] += p[2] ;
            break ;
        case 101:
            p[2] += p[3] ;
            break ;
        case 197:
            p[3] += p[1] ;
            break ;
        /* inject p[0] into that loop */
        case 149:
            p[1] += p[0] ;
            break ;
        /* restart loop */
        case 251:
            iter_count = -1 ;
            break ;
        default:
            break ;
    }
    /*
    p[0] is just a counter
    nothing above affects it
    */
    p[0]++ ;
    iter_count++ ;
This seems to have most of the desired properties. It should be cheap
but it changes a fairly large number of bits fairly often, and it
keeps p[0] as a pure counter so existing analysis should mostly still
apply. What do others think?
There are many other ways to deal with this. Can anyone suggest a better one?

@_date: 2014-11-22 04:08:35
@_author: Sandy Harris 
@_subject: [Cryptography] [cryptography] random number generator 
Here is one in C, albeit with a rather limited range of applications.
The paper there discusses several others; one of those may be better
for your application.

@_date: 2014-10-02 22:37:01
@_author: Sandy Harris 
@_subject: [Cryptography] Creating a Parallelizeable Cryptographic Hash 
There has been a lot of work on parallelizable hashing. Web search for
"tree hashing" will turn up much of it. Several of the SHA-3
competition candidates, including at least the winner Keccak and
finalist Skein, had discussions in their submissions of how to do a
tree hash with their algorithm.

@_date: 2014-10-05 14:29:29
@_author: Sandy Harris 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Based partly on comments received both on and off list (Thanks!), I
now have a rather different version. Most changes were aimed at
cleaner code and clearer comments; I think I have achieved both. I
also added some things.
Current code is ~1800 lines, ~800 of which are comments and ~200 test
scaffolding so my file compiles to a standalone test program, not a
driver. I am not going to clutter the list with that, but I will
happily send it along to anyone who asks.
My next step will be to start submitting it as patches. (Thanks for
the instructions, Jason.) Organising that looks difficult, since what
I propose is a major rewrite, not easily expressed as a series of
incremental changes. However, I'll try to figure it out.

@_date: 2014-10-26 18:32:37
@_author: Sandy Harris 
@_subject: [Cryptography] A TRNG review per day: RDRAND and the right TRNG 
That is one good design, but far from the only one.
One alternative is a well-designed high-speed TRNG, such as Turbid.
    fast process with provable minimum entropy -> auditable compressor
Given some fairly mild assumptions about properties of the hash, this
can provably get within epsilon of perfectly random output. Also, it is
stateless, so it is completely immune to the state discovery attacks
which are a threat to CPRNGs.
This solves the problem, short of extremes like failure or saturation of
the hardware part (sound card in Turbid). Add auditable checks for
those conditions and there you go.
It looks to me like Intel or others with on-chip TRNGs could reach
the requirements of this model without excessive effort, at least
given an assumption that the hardware actually implements its
spec. Dealing with the possibility of subversion that makes the
chip different from the spec is a separate problem that looks
Another architecture that is correct is the type of design
used in various random(4) devices.
     several sources -> pool -> cryptographic hash
This requires stronger assumptions about the hash than the
Turbid-ish design does and it fails (at least short-term) if the
enemy learns pool contents.
All operations on the pool should be pool_word ^= input or
+= input, never pool_word = input, so that bad inputs cannot
reduce pool entropy. Given that plus reasonable assumptions
about the hash and that at least one source produces entropy
unknown to an attacker, it is easy to show that this must
recover from any state compromise attack eventually.

@_date: 2014-10-26 20:28:13
@_author: Sandy Harris 
@_subject: [Cryptography] Auditable logs? 
Various computer-mediated activities may end up in court for a range
of reasons and in many cases log files  will be used as evidence.
However for most log file formats, deleting a few lines or adding a
few bogus ones is trivial. Even forging an entire file or large chunk
thereof is not impossible.
Lawyers for one side or the other seem quite likely to attack the
credibility of log files and/or of the sys admin who provides them. In
at least some cases, proof "beyond reasonable doubt" is required and
that is going to be very difficult if the lawyers trying to create
some doubt are good.
What sort of crypto mechanisms might help here? I can see various
applications of digital signatures and timestamps that might help, but
noting close to a full solution.

@_date: 2014-09-12 19:18:58
@_author: Sandy Harris 
@_subject: [Cryptography] RFC possible changes for Linux random device 
I have some experimental code to replace parts of random.c It is not
finished but far enough along to seek comment. It does compile with
either gcc or clang, run and produce reasonable-looking results but is
not well-tested. splint(1) complains about parts of it, but do not
think it is indicating any real problems.
Next two posts will be the main code and a support program it uses.
I change nothing on the input side; the entropy collection and
estimation parts of existing code are untouched. The hashing and
output routines, though, are completely replaced, and much of the
initialisation code is modified.
It uses the 128-bit hash from AES-GCM instead of 160-bit SHA-1.
Changing the hash allows other changes. One design goal was improved
decoupling so that heavy use of /dev/urandom does not deplete the
entropy pool for /dev/random. Another was simpler mixing in of
additional data in various places.

@_date: 2014-09-12 19:21:39
@_author: Sandy Harris 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Experimental code to replace parts of random.c
    I change nothing on the input side; the
    whole entropy collection and estimation
    part of existing code are untouched. The
    hashing and output routines, though, are
    completely replaced, and much of the
    initialisation code is modified.
    Uses 128-bit hash from AES-GCM instead of
    160-bit SHA-1.
    This sort of hash-like primitive has largely
    replaced more complex hashes in applications
    such as IPsec authentication; in most cases
    the new methods are considerably faster and
    the code is simpler. It therefore seemed
    worth trying such a hash here. I chose the
    one from AES-GCM because it is widely used,
    well-analysed, and considered secure.
    References include RFCs 4106 and 5288, NIST
    standard SP-800-38D and many academic papers.
     discusses
    bugs in the Open SSL version of this hash.
    Intel have a reference on doing it faster on
    CPUs with a new instruction PCLMULQDQ.
    Whether it is secure for this applicatiion
    needs analysis. One concern is that IPsec
    generates a 128-bit hash but uses only 96
    bits in the authentication which makes some
    attacks much harder. This application uses
    the whole 128 bits. Also, the input for
    IPsec authentication is ciphertext, which
    is highly random with any decent cipher;
    input here is pool data which may be much
    less random.
    I add some complications beyond the basic
    hash; those need analysis as well.
    Changing the hash also allows other changes.
    One design goal was improved decoupling so
    that heavy use of /dev/urandom does not
    deplete the entropy pool for /dev/random.
    Another was simpler mixing in of additional
    data in various places. See later comments
    for details.
    Sandy Harris, sandyinchina at gmail.com
    September 2014
    memcpy(), memset(), ...
     Just for convenience
  typedef uint32_t u32 ;
typedef unsigned char byte ;
    define if arch_get_random_long() is available
 HAVE_HW_RAND
    Give every compiled version a different seed using
    bits from /dev/urandom on the development machine.
    This falls well short of the ideal solution, giving
    every installation a different seed. For that, see:
        On the other hand, neither sort of seed is necessary if
      either you have a trustworthy hardware RNG
      or /dev/random is initialised early with secure stored data
    In those cases, the device gets adequately initialised.
    This is just a defense-in-depth trick, can do no harm
    and may sometimes make attacks harder. Not ideal, not
    always necessary, but a cheap addition and probably the
    best we can do at compile time.
    random_init.h sets three       POOL_ROWS    128-bit rows per pool (4)
      ARRAY_ROWS    (POOL_ROWS*3)
      ARRAY_WORDS    (ARRAY_ROWS*4)
    creates
      u32 init_data[ARRAY_WORDS]
    and fills it with random data.
    See gen_random_init.c for details
    The choice of 12 128-bit quantities for this is
    partly arbitrary, partly reasoned.
     On the one hand, it would be possible to do more
     here, e.g. initialising all the pools randomly
     since using /dev/urandom on the development
     machine is presumably cheap.
     On the other, 128 or 256 random bits are almost
     certainly enough.
    The AES-GCM hash initialises its accumulator to
    128 zero bits and uses only one constant, H. I
    chose instead to use two constants, initialise
    with one and use the other in the role of H.
    Then I chose to use two hash iterations. The output
    of the first is used as feedback into the pool and
    as input for the second hash. The second, using
    different constants, then generates the actual
    output; this is intended to make the feedback and
    the output largely independent.
    This requires that a total of four 128-bit constants
    be used in each output operation. I chose to give
    each pool its own four instead of using one set for
    all pools, so there are twelve altogether. Finally,
    I chose to initialise all twelve with random data.
    Any of those choices might be changed, but they
    all seem reasonable to me.
 "random_init.h"
    copy some things from random.c
    just enough for testing my code
    modified a bit
 * Configuration information
 */
 INPUT_POOL_SHIFT    12
 INPUT_POOL_WORDS    (1 << (INPUT_POOL_SHIFT-5))
 INPUT_POOL_BYTES    (INPUT_POOL_WORDS<<2)
 OUTPUT_POOL_SHIFT    10
 OUTPUT_POOL_WORDS    (1 << (OUTPUT_POOL_SHIFT-5))
 OUTPUT_POOL_BYTES    (OUPUT_POOL_WORDS<<2)
static u32 input_pool_data[INPUT_POOL_WORDS];
static u32 blocking_pool_data[OUTPUT_POOL_WORDS];
static u32 nonblocking_pool_data[OUTPUT_POOL_WORDS];
    reduced version of struct
    OK for testing this code
    would need integration with existing struct
    before actual use
struct my_pool    {
    // used in hash & output mixing
    u32 *A, *B, *C, *D ;
    int which, count, entropy_count ;
    // used for mixing data back into pool
    u32 *p, *q, *end, delta ;
    // actual pool data
    u32 *data, size ;
    } ;
static struct my_pool input_pool, blocking_pool, nonblocking_pool;
static void simple_get( struct my_pool *, u32 * ) ;
static void mix_last( struct my_pool *, u32 * ) ;
*    various operations on 128-bit buffers
static inline void xor128( u32 *target, u32 *source)
    int i ;
    for( i = 0 ; i < 4 ; i++ )
        *target++ ^= *source++ ;
static inline void add128(u32 *target, u32 *source)
    int i ;
    for( i = 0 ; i < 4 ; i++ )
        *target++ += *source++ ;
    zero out a 128-bit buffer
    both buffer2pool() and buffer2array()
    call this to clear their inputs
    in the kernel context
    should use memzero_explicit()
static inline void zero128( u32 *target )
    // printf( "zero128() %08x\n", target );
    memset( (void *) target, 0, 16) ;
    4-way pseudo-Hadamard transform
    using 32-bit chunks
    every output word depends on every input word
    any size PHT is invertible
    it loses no entropy
    conceptually, a 2-way PHT on a, b is
        x = a + b
        y = a + 2b
        a = x
        b = y
    a better implementation is just
        a += b
        b += a
    a 4-way PHT is built from 4 2-way PHTs
    here it is unrolled into 8 += operations
static void pht128(u32 *t)
    // printf( "pht128()\n" ) ;
    t[0] += t[1] ;
    t[1] += t[0] ;
    t[2] += t[3] ;
    t[3] += t[2] ;
    t[0] += t[2] ;
    t[2] += t[0] ;
    t[1] += t[3] ;
    t[3] += t[1] ;
    mixer from the Aria block cipher, a Korean standard
    Cipher home page:
        See also RFC 5794
    Version here is based on GPL source at:
        mixes a 128-bit vector
    equivalent to a Boolean matrix multiplication
    every output byte depends on seven input bytes
    invertible; loses no entropy
     fairly efficient
     it is used in every round of the cipher
     there are 16 rounds & the cipher is fast
    some caution is needed in applying this since the
    function is its own inverse; using it twice on the
    same data gets you right back where you started
 */
static void aria_mix( byte *x )
    byte y[16] ;
    // printf( "aria_mix()\n" ) ;
    // each output byte is the XOR of 7 input bytes
    y[0] = x[3] ^ x[4] ^ x[6] ^ x[8] ^ x[9] ^ x[13] ^ x[14];
    y[1] = x[2] ^ x[5] ^ x[7] ^ x[8] ^ x[9] ^ x[12] ^ x[15];
    y[2] = x[1] ^ x[4] ^ x[6] ^ x[10] ^ x[11] ^ x[12] ^ x[15];
    y[3] = x[0] ^ x[5] ^ x[7] ^ x[10] ^ x[11] ^ x[13] ^ x[14];
    y[4] = x[0] ^ x[2] ^ x[5] ^ x[8] ^ x[11] ^ x[14] ^ x[15];
    y[5] = x[1] ^ x[3] ^ x[4] ^ x[9] ^ x[10] ^ x[14] ^ x[15];
    y[6] = x[0] ^ x[2] ^ x[7] ^ x[9] ^ x[10] ^ x[12] ^ x[13];
    y[7] = x[1] ^ x[3] ^ x[6] ^ x[8] ^ x[11] ^ x[12] ^ x[13];
    y[8] = x[0] ^ x[1] ^ x[4] ^ x[7] ^ x[10] ^ x[13] ^ x[15];
    y[9] = x[0] ^ x[1] ^ x[5] ^ x[6] ^ x[11] ^ x[12] ^ x[14];
    y[10] = x[2] ^ x[3] ^ x[5] ^ x[6] ^ x[8] ^ x[13] ^ x[15];
    y[11] = x[2] ^ x[3] ^ x[4] ^ x[7] ^ x[9] ^ x[12] ^ x[14];
    y[12] = x[1] ^ x[2] ^ x[6] ^ x[7] ^ x[9] ^ x[11] ^ x[12];
    y[13] = x[0] ^ x[3] ^ x[6] ^ x[7] ^ x[8] ^ x[10] ^ x[13];
    y[14] = x[0] ^ x[3] ^ x[4] ^ x[5] ^ x[9] ^ x[11] ^ x[14];
    y[15] = x[1] ^ x[2] ^ x[4] ^ x[5] ^ x[8] ^ x[10] ^ x[15];
    memcpy( x, y, 16 ) ;
    memset( y, 0, 16 ) ;
    code to manage the array of four 128-bit constants per pool
    not really constants; some of this code changes them
    treated as constants in the extract-from-pool code
    mixers to change things in the array
    two versions, XOR-then-add and add-then-XOR
    neither can reduce entropy, even with bad inputs
static void mix2array1( u32 *row, u32 *x)
    // stir old data, using XOR
    aria_mix( (byte *) row ) ;
    // mix in new, using 32-bit addition
    add128( row, x ) ;
static void mix2array2( u32 *row, u32 *x)
    // stir old data, using addition
    pht128( row ) ;
    // mix in new, using XOR
    xor128( row, x ) ;
    iterative updater
    call one or other of the above
    to update some part of the array data
    A and C are used in most of the hashing
    B and D are used for final output in mix_last()
    This updates them all in alphabetical order
    every call to this affects all future outputs
     from the pool
    any two successive calls change one of A or C,
     affecting all future feedback as well
static void buffer2array( struct my_pool *p, u32 *x)
    u32 *row ;
    // printf( "buffer2array() p %08x input %08x\n", p, x ) ;
    // different row on each call
    switch( p->which & 3 )    {
        case 0:
            row = p->A ;
            break ;
        case 1:
            row = p->B ;
            break ;
        case 2:
            row = p->C ;
            break ;
        case 3:
            row = p->D ;
            break ;
    }
    // alternate between update types
    if( p->which > 3 )
        mix2array1( row, x ) ;
    else
        mix2array2( row, x ) ;
    // sanitize
    zero128( x ) ;
    // update counter
    p->which++ ;
    if( p->which > 7 )
        p->which = 0 ;
    update one or more constants for a pool
    using output from that pool
    pool is also stirred
    all output routines call mix_last()
    that calls buffer2pool() which mixes in 128 bits
    and changes 8 32-bit words in the pool
static void mix2const( struct my_pool *p, int n )
    u32 temp[4] ;
    while( n-- > 0)    {
        simple_get( p, temp ) ;
        buffer2array( p, temp ) ;
    }
    Code to mix a 512-bit chunk of memory, 16 32-bit words
    based on part of Bernstein's ChaCha stream cipher code
    chacha_array() mixes the constants array for a pool
    stir_array() adds data then calls chacha_array()
    chacha_mix() could mix any 512-bit chunk of data
    current code uses it only to implement *_array()
    Bernstein comment & code
    chacha-ref.c version 20080118
    D. J. Bernstein
    Public domain.
 QUARTERROUND(a,b,c,d) \
  x[a] = PLUS(x[a],x[b]); x[d] = ROTATE(XOR(x[d],x[a]),16); \
  x[c] = PLUS(x[c],x[d]); x[b] = ROTATE(XOR(x[b],x[c]),12); \
  x[a] = PLUS(x[a],x[b]); x[d] = ROTATE(XOR(x[d],x[a]), 8); \
  x[c] = PLUS(x[c],x[d]); x[b] = ROTATE(XOR(x[b],x[c]), 7);
static void salsa20_wordtobyte(byte output[64],const u32 input[16])
  u32 x[16];
  int i;
  for (i = 0;i < 16;++i) x[i] = input[i];
  for (i = 12;i > 0;i -= 2) {
    QUARTERROUND( 0, 4, 8,12)
    QUARTERROUND( 1, 5, 9,13)
    QUARTERROUND( 2, 6,10,14)
    QUARTERROUND( 3, 7,11,15)
    QUARTERROUND( 0, 5,10,15)
    QUARTERROUND( 1, 6,11,12)
    QUARTERROUND( 2, 7, 8,13)
    QUARTERROUND( 3, 4, 9,14)
  }
  for (i = 0;i < 16;++i) x[i] = PLUS(x[i],input[i]);
  for (i = 0;i < 16;++i) U32TO8_LITTLE(output + 4 * i,x[i]);
 ROTL(v, n) ( ((v) << (n)) | ((v) >> (32 - (n))) )
static void quarterround( u32 *x, int a, int b, int c, int d )
  x[a] += x[b] ; x[d] ^= x[a] ; x[d] = ROTL( x[d], 16) ;
  x[c] += x[d] ; x[b] ^= x[c] ; x[b] = ROTL( x[b], 12) ;
  x[a] += x[b] ; x[d] ^= x[a] ; x[d] = ROTL( x[d],  8) ;
  x[c] += x[d] ; x[b] ^= x[c] ; x[b] = ROTL( x[b],  7) ;
    mix a 512-bit buffer in place
    treat it as a 4x4 array of 32-bit words
    ChaCha uses 8, 12 or 20 rounds
    this does 4 * n
static void chacha_mix( u32 *data, int n )
    u32 x[16] ;
    int i;
    // copy data[] into x[]
    memcpy( (byte *) x, (byte *) data, 64) ;
    // stir x[]
     for( i = 4*n ; i > 0; i -= 2) {
        // mix rows
        quarterround(  x, 0, 4, 8,12 ) ;
        quarterround(  x, 1, 5, 9,13 ) ;
        quarterround(  x, 2, 6,10,14 ) ;
        quarterround(  x, 3, 7,11,15 ) ;
        // mix columns
        quarterround(  x, 0, 5,10,15 ) ;
        quarterround(  x, 1, 6,11,12 ) ;
        quarterround(  x, 2, 7, 8,13 ) ;
        quarterround(  x, 3, 4, 9,14 ) ;
    }
    /*
    add x[] back into data[]
    Bernstein uses bytewise addition
    I use 32-bit, maybe faster & carries improve mixing
    */
    for( i = 0 ; i < 16 ; i++ )
        data[i] += x[i] ;
    // sanitize
    memset( (byte *) x, 0, 64) ;
    apply chacha_mix() to the constants array for a pool
 N_CHACHA  2
static void chacha_array( struct my_pool *p )
    // printf("chacha_array()\n") ;
    chacha_mix( p->A, N_CHACHA ) ;
    mix 128-bit chunks into two rows
    then mix whole array
static void stir_array( struct my_pool *p )
    // printf("stir_array(), p is %08x\n", p) ;
    mix2const( p, 2 ) ;
    chacha_array( p ) ;
    a 128-bit counter to mix in when hashing
    There is only one counter[] and one count() function to
    update it. mix_last() calls that so the count is affected
    by all output operations on any pool. It is initialised
    randomly and only affects outputs indirectly. The counter
    value should therefore be quite difficult for an enemy to
    discover.
    mix_last() also mixes in the counter so it affects all
    output from any pool and all feedback into any pool.
    That is, this counter provides a way for operations on
    any pool to automatically influence both of the other
    pools, albeit in an indirect and rather limited way.
    Operations on this counter do not affect the per-pool
    counts for any pool, neither the entropy count nor
    the p->count iteration counter.
static u32 counter[4] ;
 COUNTER_DELTA 0x67452301
static int iter_count = 0 ;
    code is based on my own work in the Enchilada cipher:
        Mix operations so Hamming weight changes more than for
    a simple counter. This may not be strictly necessary,
    but low Hamming weight differences do allow some attacks
    on block ciphers and the high bits of a large counter
    that is only incremented do not change for aeons. The
    extra code here is cheap insurance.
    A bit nonlinear since it uses +, XOR and rotation.
    For discussion, see mailing list thread starting at:
    static void count(void)
    // printf( "count(), iter_count %d\n", iter_count ) ;
    /*
    Limit the switch to < 256 cases
    should work with any CPU & compiler
    Five constants used, all primes
    roughly evenly spaced, around 50, 100, 150, 200, 250
    */
    switch( iter_count )    {
        /*
        mix three array elements
        each element is used twice
        once on left, once on right
        pattern is circular
        */
        case 47:
            counter[1] += counter[2] ;
            break ;
        case 101:
            counter[2] += counter[3] ;
            break ;
        case 197:
            counter[3] += counter[1] ;
            break ;
        /*
        inject counter[0] into that loop
        loop and counter[0] use +=
        so use ^ here
        */
        case 149:
            counter[1] ^= counter[0] ;
            break ;
        /*
        restart loop
        include a rotation for nonlinearity
        */
        case 251:
            counter[0] = ROTL( counter[0], 5) ;
            iter_count = -1 ;
            break ;
        /*
        for 246 out of every 251 iterations
        the switch does nothing
        */
        default:
            break ;
    }
    /*
    counter[0] is almost purely a counter
    += instead of ++ to change Hamming weight more
    nothing above affects it
    except the rotation every 251 iterations
    */
    counter[0] += COUNTER_DELTA ;
    iter_count++ ;
    manage pools, extract data from them
    basic mixing routine for pools
    mixes in 32 bits of data
    changes two pool elements per call
    mixes XOR and + for nonlinearity
    for when r is known to point to
    high-entropy data
        hardware RNG data
        hash output
        cipher output (not used here)
    input mixing should NOT use this
    existing driver code is far better for
    low-to-medium entropy inputs
    Existing code is OK for high-entropy
    stuff as well. I added this in hopes
    it would be faster. Also, adding a
    different mixer gives insurance if a
    weakness turns up in the existing one.
static inline void mixer( u32 *p, u32 *q, u32 *r)
    /*
    mix in external data
    this cannot reduce entropy, even with bad data
    */
    *p ^= *r ;
    /*
    pseudo-Hadamard transform
    spread the effect a bit
    invertible, cannot lose entropy
    */
    *q += *p ;
    *p += *q ;
    actual pool mixer for 32-bit chunks
    mixer() above plus pointer management
    if array size & inilialisation are sensible
    and array is N 32-bit words, then:
    after N/2 iterations,
    every array element has been changed
    after N iterations,
    every element has been changed twice
    once as *p, once as *q
    Eventually this stirs the entire pool,
    making every pool word depend both on
    every other pool word and on many
    external inputs. This is the only
    stirring the output pools get.
    If that is considered not fast enough,
    aria_mix(), pht128() or chacha_mix()
    could provide additional stirring.
    I do not think that is necessary.
static inline void mix2pool( struct my_pool *pool, u32 *r)
    // printf( "mix2pool() %08x %08x %08x %08x\n", pool->data ,
pool->p, pool->q, pool->end) ;
    mixer(pool->p, pool->q, r) ;
    // increment pointers
    pool->p += pool->delta ;
    pool->q += pool->delta ;
    // wrap around if needed
    if( pool->p >= pool->end )
        pool->p -= pool->size ;
    if( pool->q >= pool->end )
        pool->q -= pool->size ;
    mix a 128-bit buffer into a pool
    changes 8 32-bit pool words
    then clears its input buffer
    This is used in mix_last()
    to mix feedback data into the pool itself
    for other mixing, buffer2array() is preferred
    because the effects are more easily analysed
static void buffer2pool( struct my_pool *pool, u32 *r)
    int i ;
    u32 *s ;
    // printf( "buffer2pool(), data at %08x\n", r ) ;
    for( i = 0, s = r ; i < 4 ; i++, s++ )
        mix2pool( pool, s ) ;
    zero128( r ) ;
    hashing code
    AES-GCM authentication code
    from Dan Bernstein's example implementation
    distributed as part of CAESAR test code
        I changed his 64-bit length type to u32
    enough for this application
Bernstein's description:
    a = (a + x) * y in the finite field
    16 bytes in a
    xlen bytes in x; xlen <= 16; x is implicitly 0-padded
    16 bytes in y
static void addmul(byte *a, const byte *x, u32 xlen, const byte *y)
  int i;
  int j;
  byte abits[128];
  byte ybits[128];
  byte prodbits[256];
  for (i = 0;i < xlen;++i) a[i] ^= x[i];
  for (i = 0;i < 128;++i) abits[i] = (a[i / 8] >> (7 - (i % 8))) & 1;
  for (i = 0;i < 128;++i) ybits[i] = (y[i / 8] >> (7 - (i % 8))) & 1;
  /*
  splint(1) complains here about int assigned to byte
  for (i = 0;i < 256;++i) prodbits[i] = 0;
  so replace it with next line, maybe faster too?
  */
  memset( prodbits, 0, 256 ) ;
  for (i = 0;i < 128;++i)
    for (j = 0;j < 128;++j)
      prodbits[i + j] ^= abits[i] & ybits[j];
  for (i = 127;i >= 0;--i) {
    prodbits[i] ^= prodbits[i + 128];
    prodbits[i + 1] ^= prodbits[i + 128];
    prodbits[i + 2] ^= prodbits[i + 128];
    prodbits[i + 7] ^= prodbits[i + 128];
    prodbits[i + 128] ^= prodbits[i + 128];
  }
  /*
  for (i = 0;i < 16;++i) a[i] = 0;
  */
  memset( a, 0, 16 ) ;
  for (i = 0;i < 128;++i) a[i / 8] |= (prodbits[i] << (7 - (i % 8)));
    Mix n bytes into an accumulator using addmul()
    This is a keyed hash that takes nbytes of input,
    a 128-bit initial value and 128-bit key, and
    gives a 128-bit output.
    This routine does not either initialise the
    accumulator or finalise output. The expected
    calling sequence looks like this:
        intialise accumulator (from p->A)
        call this one or more times (with p->C)
          each call with different data
        finalise output (using p->B, C, D)
    The main use here is against the various pools
    replacing the hash previously used there. This
    should be faster, but it needs analysis.
    Note that it can be used with any data. In
    AES-GCM it is run over unencrypted headers so
    those can be authenticated along with encrypted
    text.
    Here it might be run over any kernel data
    structure that is expected to be unpredictable
    to an enemy, giving extra entropy.
    It can also be run over anything expected to
    be different on each machine (e.g. Ethernet
    MACs), on each boot (clock data) or on each
    read of /dev/urandom (process info for reader).
    Such data cannot be trusted for entropy (it
    may be unknown to some attackers, but not
    reliably to all), but it can still be useful
     in a role like that of salt in a hash; it
    makes brute force attacks much harder.
static void mix_in( byte *data, u32 nbytes, byte *mul, u32 *accum)
    u32 len, left ;
    byte *p ;
    for( p = data, left = nbytes ; left != 0 ; p += len, left -= len)    {
        len = (left >= 16) ? 16 : left ;
        addmul( (byte *) accum, p, len, mul ) ;
    }
    start of every output routine
    this may update the constants array for the pool
    based on p->count, a per-pool count different
    from global counter[] and count()
    this is not needed often since there is other mixing
    The Schneier et al Yarrow rng design uses 3DES
    with a 64-bit output on the output side. They
    reseed that from its own output every 10 blocks
    Here we have feedback into the pool on every
    iteration so we need not reseed as often.
    For the input and nonblocking pools, this is
    the only code that changes p->count.
    For the input pool, it is also the only code
    that uses p->count
    the nonblocking pool uses its p->count to decide
    when rekeying is needed. To decide where to get
    rekeying data, it looks at entropy count for the
    input pool and p->count for the nonblocking pool
    the blocking pool uses its p->count differently
    code in loop_nonblock_p()
    constants defined here are more-or-less arbitrary
    257 is 2^8+1 which divides 2^16-1
    so one constant is updated
    shortly before we reach MAX_COUNT at 2^16+1
 MAX_COUNT ((1<<16)+1)
 FREQUENCY 257
static void mix_first( struct my_pool *p, u32 *buffer )
    u32 temp[4] ;
    p->count++ ;
    // printf( "mix_first()\n" ) ;
    if( (p->count % FREQUENCY) == 0 )    {
        // update one array element
        memcpy( (byte *) temp, (byte *) p->A, 16 ) ;
        mix_last( p, temp) ;
        buffer2array( p, temp ) ;
    }
    if( p->count >= MAX_COUNT )    {
        // update the whole constants array
        stir_array( p ) ;
        p->count = 0 ;
    }
    // initialise the buffer
    memcpy( (byte *) buffer, (byte *) p->A, 16 ) ;
    // MAYBE ADD CODE HERE?
    Last step of any pool mixing sequence
    AES-GCM authentication is
      initialise accumulator all-zero
      mix in data with multiplier H
      xor in H before final output
    Our algorithm is
      call mix_first()
        maybe update the constants array
        initialise accumulator from p->A
      maybe do some other things
      call mix_last() for output
    What mix_last() does is
      mix in counter[] with multiplier p->C
      mix in pool data with multiplier p->C
      xor in p->C
      feed result back into pool
      re-initialise accumulator from p->B
      mix in previous result with multiplier p->D
      xor in p->D to get final output
    Both in the hashing and in the output mix,
    the same constant (C or D respectively) is
    used twice, in finite field multiplication
    then in an XOR. This is like the way AES-GCM
    authentication uses its constant H.
    It is also similar to a method of constructing
    a hash from a block cipher that XORs the cipher
    key into the output. Preneel, Govaerts and
    Vandewalle give a security proof for that.
    Their proof does not apply here. For one thing,
    they assume the block cipher is secure, but just
    multiplying by a key almost certainly does not
    give a secure cipher. However, their work does
    provide an argument that this construction is
    sensible.
static void mix_last( struct my_pool *p, u32 *buffer )
    u32 temp[4] ;
    // printf( "mix_last()\n" ) ;
    // MAYBE ADD CODE HERE?
    // mix in counter and update it
    addmul( (byte *) buffer, (byte *) counter, 16, (byte *) p->C) ;
    count() ;
    // then pool data
    mix_in( (byte *) p->data, p->size, (byte *) p->C, buffer ) ;
    xor128( buffer, p->C ) ;
    // save result for later use
    memcpy( temp, buffer, 16 ) ;
    // feed results back into pool
    buffer2pool( p, buffer ) ;
    /*
    create an output different from data used in feedback
    using another hash step with different constants B, D
    */
    memcpy( buffer, p->B, 16 ) ;
    // mix in saved data
    addmul( (byte *) buffer, (byte *) temp, 16, (byte *) p->D) ;
    xor128( buffer, p->D ) ;
    // sanitize
    zero128( temp ) ;
    The comments // MAYBE ADD CODE HERE? above
    indicate places where it would be logical
    to mix in timer data, hardware rng data, or
    data from something that uses CPU timing,
    such as Havege or Stephan Mueller's jitter.
    If we are running in a VM, data from the
    parent OS.
    Doing this in either mix_first() or mix_last()
    would ensure it was used for every output.
    The data could be put where convenient
    either in the pool or the output buffer
    either way it would affect both feedback & output
    "MAYBE" because there are other alternatives
    Just mix the data into the input pool and
    let the effects propagate from there?
    Existing code has add_timer_randomness() and
    related things to deal with the timer. If it
    ain't broke, ...
    Mix it in as part of the code to extract
    output from the input pool? This would affect
    all pools since that process also feeds data
    back into the input pool. It would be cheaper
    than doing it in mix_first() or mix_last().
    For now, I do none of the above, just note
    possibilities here.
    internal routines to get data from pools
    just fill a 128-bit buffer
    just get 128 bits from a pool
    do nothing extra
static void simple_get( struct my_pool *p, u32 *out )
    mix_first( p, out ) ;
    mix_last( p, out ) ;
    for either output pool we do more
    different for the two pools
static void get_block( u32 *out )
    struct my_pool *p ;
    u32 temp[4] ;
    p = &blocking_pool ;
    /*
    mix 128 new bits into our constants
    changes all future output
    */
    simple_get( &input_pool, temp ) ;
    buffer2array( p, temp ) ;
    /*
    reset counter since we have fresh data
    the nonblocking pool can use this
    to determine whether reseeding from here is safe
    */
    p->count = 0 ;
    /*
    we have some fresh data
    it seems wasteful to use it only for one output
    but risky to use it for >1 visible output
    so update the constants arrays for all pools
    stirring our data array in the process
    */
    simple_get( p, temp ) ;
    buffer2array( &input_pool, temp ) ;
    simple_get( p, temp ) ;
    buffer2array( &nonblocking_pool, temp ) ;
    /*
    this pool last
    so actual output uses different constants
    than feedback into other pools
    since this is the 2nd feedback into this
    pool's array, it also guarantees one of
    A or C is changed so all future feedback
    as well as output will be affected
    */
    simple_get( p, temp ) ;
    buffer2array( p, temp ) ;
    // generate output
    simple_get( p, out ) ;
    /*
    exit with p->count == 4 since there are
    4 simple_get() calls after the reset
    */
    constants to control external rekeying of
    the non-blocking pool; the values here
    are more-or-less arbitrary first guesses
    as usual for me, I have used some primes
    all pools do self-rekeying (from same pool)
    in mix_first()
    input pool rekeys from external data
    blocking pool rekeys from the input pool
    before every output
    the only place where external rekeying
    (using data from a different pool) needs
    management is here
 SAFE_OUT 331
 SAFE_BITS (INPUT_POOL_WORDS*24)
 SOME_BITS 1024
 K_LO    47
 K_HI    SAFE_OUT
static void get_nonblock( u32 *out )
    struct my_pool *p ;
    u32 temp[4] ;
    p = &nonblocking_pool ;
    /*
    IMPROVE CODE HERE
    this is first-cut general idea stuff
    design goal is to decouple the two
    output pools so that heavy use of
    /dev/urandom cannot starve /dev/random
    blocking pool reseeds whenever it is used
    so it can sometimes provide data to reseed here
    */
    if( (p->count % SAFE_OUT) == 0 )    {
        // try everything reasonable
        if( blocking_pool.count <= K_LO )
            simple_get( &blocking_pool, temp ) ;
        else if( input_pool.entropy_count > SAFE_BITS )
            simple_get( &input_pool, temp ) ;
        else if( blocking_pool.count <= K_HI )
            simple_get( &blocking_pool, temp ) ;
        else if( input_pool.entropy_count > SOME_BITS )
            simple_get( &input_pool, temp ) ;
        // if those all fail, do the best we can
        else    {
            // ADD REAL LOG MESSAGE HERE
            printf( "get_nonblock(), bad reseed\n") ;
            chacha_array( p ) ;
            simple_get( p, temp ) ;
            xor128( temp, counter );
            /*
            set p->count so we will rekey again sooner
            not immediately, give some time for other
            pools to get some entropy
            */
            p->count += ((3*SAFE_OUT)>>2) ;
        }
        buffer2array( p, temp ) ;
    }
    // generate output
    simple_get( p, out ) ;
    loop to fill a buffer with output data
    four somewhat different routines
        from input pool
        from blocking pool
        nonblocking to /dev/random
        nonblocking to get_random_bytes()
    perhaps a parent OS providing data to a child VM
    should use a 5th variant?
    all update their constants array first so that
    each call will generate an output stream almost
    independent of previous streams.
    For a rationale, see the Fortuna paper by Schneier et al.
    They are rekeying a counter-mode block cipher, but the
    same principle applies here.
    Some could also mix in extra data
    Doing that here, once per batch of output, is much
    cheaper than per block
    Comments indicate what I think is plausible
static void loop_input( u32 *out, u32 nbytes )
    u32 temp[4], n, m ;
    struct my_pool *p ;
    byte *x ;
    p = &input_pool ;
    mix_first( p, temp ) ;
    /*
    ADD CODE HERE?
    timer or hardware rng data could be mixed in
    see earlier comments on those
    consider running addmul() over volatile
     kernel data structures
    page tables? process list? ...?
    This could provide extra entropy.
    */
    mix_last( p, temp ) ;
    buffer2array( p, temp ) ;
    // produce output
    for( n = nbytes, x = (byte *) out ; n != 0 ; n -= m, x += m )    {
        m = (n >= 16) ? 16 : n ;
        simple_get( p, temp ) ;
        memcpy( x, (byte *) temp, m) ;
    }
    zero128( temp ) ;
static void loop_block( u32 *out, u32 nbytes )
    struct my_pool *p ;
    u32 temp[4], n, m ;
    byte *x ;
    p = &blocking_pool ;
    /*
    no need to add extra data here
    since get_block() does lots of that
    but mix the constants array
    */
    chacha_array( p ) ;
    // produce output
    for( n = nbytes, x = (byte *) out ; n != 0 ; n -= m, x += m )    {
        m = (n >= 16) ? 16 : n ;
        get_block( temp ) ;
        memcpy( x, (byte *) temp, m) ;
    }
    zero128( temp ) ;
static void loop_urandom( u32 *out, u32 nbytes )
    struct my_pool *p ;
    u32 temp[4], n, m ;
    byte *x ;
    p = &nonblocking_pool ;
    mix_first( p, temp ) ;
    /*
    ADD CODE HERE
    Mix in process info for reading process
    apply addmul() to task_info struct
    This depends on a different aspect of the
    system than anything else in the driver,
    namely the order in which user processes
    ask for data and the current state of those
    processes.
    Except on very simple embedded systems,
    this should be hard to guess. It should be
    impossible to monitor unless the attacker
    is logged into the system or has left a
    background process running on it. Even
    then, monitoring it would not be easy.
    The code should NOT update any entropy
    estimate since we have no clear idea how
    much entropy this gives and it may well
    be zero in some cases.
    */
    mix_last( p, temp ) ;
    buffer2array( p, temp ) ;
    /*
    if code is added above to mix extra stuff into this pool
    then we should propagate the changes to other pools
    if not, this does no harm
    */
    get_nonblock( temp ) ;
    buffer2array( &input_pool, temp ) ;
    // produce output
    for( n = nbytes, x = (byte *) out ; n != 0 ; n -= m, x += m )    {
        m = (n >= 16) ? 16 : n ;
        get_nonblock( temp ) ;
        memcpy( x, (byte *) temp, m) ;
    }
    zero128( temp ) ;
static void loop_random_bytes( u32 *out, u32 nbytes )
    struct my_pool *p ;
    u32 temp[4], n, m ;
    byte *x ;
    p = &nonblocking_pool ;
    get_nonblock( temp ) ;
    buffer2array( p, temp ) ;
    // produce output
    for( n = nbytes, x = (byte *) out ; n != 0 ; n -= m, x += m )    {
        m = (n >= 16) ? 16 : n ;
        get_nonblock( temp ) ;
        memcpy( x, (byte *) temp, m) ;
    }
    zero128( temp ) ;
    setup routines, called once at startup
    non-random initialisation
    set up pointers
    initialise variables
static void init_static()
    u32 *x ;
    struct my_pool *p ;
    input_pool.data = input_pool_data ;
    blocking_pool.data = blocking_pool_data ;
    nonblocking_pool.data = nonblocking_pool_data ;
    input_pool.size = INPUT_POOL_WORDS ;
    blocking_pool.size = nonblocking_pool.size = OUTPUT_POOL_WORDS ;
    input_pool.which = blocking_pool.which = nonblocking_pool.which = 0 ;
    input_pool.count = blocking_pool.count = nonblocking_pool.count = 0 ;
    input_pool.entropy_count = blocking_pool.entropy_count =
nonblocking_pool.entropy_count = 0 ;
    // A,B, C, D are pointers into array of random-per-compile data
    x = init_data ;
    p = &input_pool ;
    p->p = p->data ;
    p->q = p->data + p->size/2;
    p->end = p->data + p->size;
    p->delta = 3 ;
    p->A = x ; x += 4 ;
    p->B = x ; x += 4 ;
    p->C = x ; x += 4 ;
    p->D = x ; x += 4 ;
    // x is not reset between pools
    p = &blocking_pool ;
    p->p = p->data ;
    p->q = p->data + p->size/2;
    p->end = p->data + p->size;
    p->delta = 5 ;
    p->A = x ; x += 4 ;
    p->B = x ; x += 4 ;
    p->C = x ; x += 4 ;
    p->D = x ; x += 4 ;
    p = &nonblocking_pool ;
    p->p = p->data ;
    p->q = p->data + p->size/2;
    p->end = p->data + p->size;
    p->delta = 7 ;
    p->A = x ; x += 4 ;
    p->B = x ; x += 4 ;
    p->C = x ; x += 4 ;
    p->D = x ;
 HAVE_HW_RAND
 static int arch_get_random_long( u32 *x )
    *x = (u32) random() ;
    return 1 ;
static int hw_init()
    int i ;
    u32 *x, t ;
    printf( "hw_init()\n" ) ;
    // update the entire constants array
    for( i = 0, x = init_data ; i < ARRAY_WORDS ; i++, x++ )            {
        if( !arch_get_random_long(&t))
             return 0;
        else    *x ^= t ;
    }
    t = 0 ;
    // the counter
    for( i = 0, x = counter ; i < 4 ; i++, x++ )                    {
        if( !arch_get_random_long(x))
             return 0;
    }
    // and all three pools
    for( i = 0, x = input_pool.data ; i < INPUT_POOL_WORDS ; i++, x++ )        {
        if( !arch_get_random_long(x))
             return 0;
    }
    for( i = 0, x = blocking_pool.data ; i < OUTPUT_POOL_WORDS ; i++,
x++ )        {
        if( !arch_get_random_long(x))
             return 0;
    }
    for( i = 0, x = nonblocking_pool.data ; i < OUTPUT_POOL_WORDS ;
i++, x++ )    {
        if( !arch_get_random_long(x))
             return 0;
    }
    return 1 ;
    introduce random data
    This should not be done until there is
    enough (256 bits?) entropy in the input
    pool.
    This code does not deal with that problem!
    FIX BEFORE USING
static void init_random()
    u32 *x, temp[4] ;
    struct my_pool *p ;
    int i ;
    init_static() ;
    printf("static init done\n" ) ;
    i = 0 ;
 HAVE_HW_RAND
    srandom( init_data[0] ) ;
    i = hw_init() ;
    /*
    if either no hardware or it failed
    initialise everything from input pool data
    mix_last() and therefore simple_get()
    both stir feedback into input pool
    so this also stirs that pool quite well
    */
    if( i == 0 )    {
        printf( "main branch init_random()\n" ) ;
        p = &input_pool ;
        mix_first( p, temp ) ;
        /*
        ADD CODE HERE
        use addmul() to mix in static info
        things that can act as salt
        need not be unpredictable
        just different on different systems
        e.g. ethernet MAC, other hardware info
        */
        mix_last( p, temp ) ;
        /*
        use that first result to initialise the counter
        this will affect all future outputs
        */
        memcpy( counter, temp, 16 ) ;
        /*
        initialise the output pools with random data
        code requires OUTPUT_POOL_WORDS%4 == 0
        use simple_get() rather than get_input_p()
        saves a bit of overhead
        */
        if( (OUTPUT_POOL_WORDS%4) != 0)
            printf( "OUTPUT_POOL_WORDS mod 4 must be zero, but here it
is %d\n", (OUTPUT_POOL_WORDS%4) ) ;
        for( i = 0, x = blocking_pool.data ; i < OUTPUT_POOL_WORDS ; i
+= 4, x += 4 )    {
            simple_get( &input_pool, temp ) ;
            xor128( x, temp ) ;
        }
        for( i = 0, x = nonblocking_pool.data ; i < OUTPUT_POOL_WORDS
; i += 4, x += 4 ) {
            simple_get( &input_pool, temp ) ;
            xor128( x, temp ) ;
        }
        zero128( temp ) ;
    }
    printf("most of init done\n") ;
    /*
    update the constant arrays for all pools
    this also stirs the data arrays
    */
    stir_array( &input_pool ) ;
    stir_array( &blocking_pool ) ;
    stir_array( &nonblocking_pool ) ;
    minimal rather dumb test program
 BUFF_WORDS 64
 BUFF_BYTES (4*BUFF_WORDS)
static void printbuff(u32 * p, int nwords)
    int i ;
    for( i = 0 ; i < nwords ; i++ )    {
        printf( "%08x", p[i] ) ;
        if( (i%8) == 7 )    (void) putchar('\n') ;
        else            (void) putchar(' ') ;
    }
    putchar('\n') ;
int main( int argc, char **argv)
    u32 buffer[BUFF_WORDS] ;
    printf("Array at startup:\n" ) ;
    printbuff( init_data, ARRAY_WORDS ) ;
    init_random() ;
    printf( "back in main() after init()\n" ) ;
    printf("Array after init():\n" ) ;
    printbuff( init_data, ARRAY_WORDS ) ;
    printf( "input pool output\n" ) ;
    loop_input( buffer, BUFF_BYTES) ;
    printbuff( buffer, BUFF_WORDS) ;
    printf( "/dev/random pool output\n" ) ;
    loop_block( buffer, BUFF_BYTES) ;
    printbuff( buffer, BUFF_WORDS) ;
    printf( "/dev/urandom pool output\n" ) ;
    loop_urandom( buffer, BUFF_BYTES) ;
    printbuff( buffer, BUFF_WORDS) ;
    printf( "get_random_bytes() output\n" ) ;
    loop_random_bytes( buffer, BUFF_BYTES) ;
    printbuff( buffer, BUFF_WORDS) ;
    return 0 ;

@_date: 2014-09-12 19:24:03
@_author: Sandy Harris 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Program to select numbers for initialising things
                limits the range of Hamming weights
                every byte has at least one bit 1, one 0
        different every time it runs
        writes to stdout, expecting makefile to redirect
        results suitable for inclusion by random.c
          amount of data to output
    depends how much random.c will use
 NPOOLS        3    // input, blocking & nonblocking
 POOL_ROWS    4    // 128-bit rows per pool
 ARRAY_ROWS    (POOL_ROWS*NPOOLS)
 ARRAY_WORDS    (ARRAY_ROWS*4)
 ARRAY_BYTES    (ARRAY_WORDS*4)
unsigned data[ARRAY_WORDS] ;
int accept(unsigned) ;
void swap(int, int) ;
int fillarray( unsigned *, int) ;
main(int argc, char **argv)
        int urandom, i ;
    unsigned *p ;
        if(argc > 1)    {
        fprintf(stderr, "usage: getrand\n") ;
        exit(1) ;
        }
        if( (urandom = open("/dev/urandom", O_RDONLY)) == -1 )  {
        fprintf(stderr, "getrand: no /dev/urandom, cannot continue\n") ;
        exit(1) ;
    }
    /*
        normal case: development machine has /dev/urandom
    */
    read(urandom, data, ARRAY_BYTES) ;
    for( i = 0, p = data ; i < ARRAY_WORDS ; i++, p++ )    {
        // replace any entries that fail criteria
        while( !accept(*p) )
            read( urandom, (char *) p, 4) ;
        }
        /*
        output an array of random data
    */
    printf( " POOL_ROWS\t %d\n", POOL_ROWS ) ;
    printf( " ARRAY_ROWS\t%d\n", ARRAY_ROWS ) ;
    printf( " ARRAY_WORDS\t%d\n\n", ARRAY_WORDS ) ;
        printf("static unsigned int init_data[] = {\n" ) ;
    for( i = 0 ; i < ARRAY_WORDS ; i ++ )
        printf("0x%08x%s", data[i], (i == (ARRAY_WORDS-1) ? "\n" : ",\n") ) ;
    printf("} ;\n") ;
        exit(0) ;
        Choose from a range of Hamming weights around 16
 MIN  6
 MAX (32-MIN)
int accept(unsigned u)
        int h,i ;
        char *p ;
        // reject low or high Hamming weights
        h = hamming(u) ;
        if( ( h < MIN ) || ( h > MAX ) )
                return(0) ;
        // at least one 1 and at least one 0 per byte
        for( i = 0, p = (char *) &u ; i < 4 ; i++, p++ )        {
                switch(*p)      {
                        case '\0':
                        case '\255':
                                return(0) ;
                        default:
                                break ;
                }
        }
        return(1) ;
        Kernighan's method
        int hamming( unsigned x )
        int h ;
        for (h = 0 ; x ; h++)
          x &= (x-1) ; // clear the least significant bit set
        return(h) ;

@_date: 2014-09-13 15:08:29
@_author: Sandy Harris 
@_subject: [Cryptography] RFC possible changes for Linux random device 
At least as fast. The newer code such as GCM is faster than
SHA-1 in TLS or IPsec authentication so there is hope this
might be faster, but that is not a critical goal.
At least as strong. The goal is that the basic mixing be as
strong then extra things improve it. Adding the counter[],
mixing in process info in loop_urandom(), ...
That is critically important and not a problem I claim to solve.
I do provide a mechanism to insert different random constants
for every version compiled. This is utterly useless in many
cases and only a minor complication for attacks against
others, but if you compile your own kernel for a firewall it
comes close to being a full solution.
More important, the GCM hash can be run over any data
so it provides a clean interface for mixing in various things
at boot time or before an output sequence.
Running it over volatile kernel data structures (process
list, page tables, ...) might provide extra entropy at boot
time or later. I do not know the kernel code well enough
to tell how useful this might be.
Running it over data expected to be different for each
machine (hardware info such as the structs for network
interfaces) can work like salt in a password hash; this
is not expected to contribute entropy but it makes some
attacks (particularly using tables to speed up brute force)
a great deal harder.
In loop_urandom() I suggest running it over the task
struct for the process that is reading the device.
    This depends on a different aspect of the
    system than anything else in the driver,
    namely the order in which user processes
    ask for data and the current state of those
    processes.
    Except on very simple embedded systems,
    this should be hard to guess. It should be
    impossible to monitor unless the attacker
    is logged into the system or has left a
    background process running on it. Even
    then, monitoring it would not be easy.
I also add a global counter[]; comments on that:
    There is only one counter[] and one count() function to
    update it. mix_last() calls that so the count is affected
    by all output operations on any pool. It is initialised
    randomly and only affects outputs indirectly. The counter
    value should therefore be quite difficult for an enemy to
    discover.
    mix_last() also mixes in the counter so it affects all
    output from any pool and all feedback into any pool.
    That is, this counter provides a way for operations on
    any pool to automatically influence both of the other
    pools, albeit in an indirect and rather limited way.
On a simple embedded system there might be few enough
processes that both the task struct stuff and counter[]
would be knowable or guessable, hence nearly useless.
On a relatively complex & busy system, though, either
of them alone might be enough to make outputs random
(though they are never used alone, always with pool data
mixed in too). The counter[] directly affects all outputs
and feedback into all pools and loop_urandom() updates
the other pools.
Not a goal.
Not goals I considered.
There are papers on timing attacks on AES-GCM and on a variant
that resists them.
I have not yet done more than scan those papers. Clearly if this
were considered for real use, those would need to influence the

@_date: 2014-09-13 15:32:15
@_author: Sandy Harris 
@_subject: [Cryptography] RFC possible changes for Linux random device 
I do want to do that eventually. Thanks for the pointers to instructions.
However, the code is not complete; there are a bunch of comments
suggesting where things could be added, but those parts are not
yet written. Also, I'd like to have it reviewed by the cryptography
list (where most of the ideas in it have previously been discussed)
before proposing it for the kernel.
Also, it would be a large patch. My program is over 1500 lines,
though at least half of those are comments or scaffolding for
testing it. Add the missing bits and some changes to current
code to integrate it and it would be a 1000+ line patch. I am
not sure whether or how that should be split up.
I started off modifying random.c and deliberately chose a different
comment style so my comments could be distinguished from the
ones already in the file. I will fix this before I send patches.

@_date: 2014-09-15 12:24:14
@_author: Sandy Harris 
@_subject: [Cryptography] List of Proven Secure Ciphers / Hashes 
The Blum-Blum-Shub algorithm for generating random numbers,
given a good key and a secure system to run on, is provably
There's a paper by Even & Mansour proving security for an
XOR-permutation-XOR structure, and many follow-up papers
on variants of the scheme or attacks on it; none of the attacks
violate Even & Mansour's bound, though some reach it. I have
proposed a cipher using that structure:

@_date: 2014-09-15 13:12:33
@_author: Sandy Harris 
@_subject: [Cryptography] Simple non-invertible function? 
I have been working on changes to the Linux random driver; there is
another thread here with a proposal. A very useful (I think essential)
component there would be a non-invertible function that is fairly
simple in terms of both computational cost and data requirements.
The function I need must take some amount of data in and put the same
amount out. It could use some other data as well (block cipher rounds
keys, an array of constants in my current code, ...) but the
non-invertible property must not depend on that, only on the main
input data.
I currently use input data X and two constants B & D, all 128-bit.
   initialise output buffer from B
   mix in X using the finite field multiplication from
      AES GCM with multiplier D
   XOR D into the buffer
This is invertible if there has been a state compromise so the enemy
knows B & D. Is it enough to just change the XOR to use X instead of
Preneel et al., in a paper on block-cipher-based hashes, prove that
encrypt(X,key) XOR X and various related structures are
non-invertible. Clearly that would work here but I'd prefer to avoid
the block cipher overhead if possible.
The world's simplest block cipher is just IDEA multiplication of
plaintext and key, and that could be made non-invertible with an XOR.
The small block size (16 bits) does not appear to matter in my
application and a transform that mixes those blocks could easily be
added just in case it does matter.
Is GCM(X, key) a block cipher in the sense that would make the Preneel
at al. proof apply to showing  GCM(X, key) XOR X is non-invertible?
Can anyone suggest a better way?

@_date: 2014-09-15 17:19:07
@_author: Sandy Harris 
@_subject: [Cryptography] RFC possible changes for Linux random device 
I've suggested something similar in a comment in loop_urandom(),
done once for each /dev/urandom read.
    Mix in process info for reading process
    apply addmul() to task_info struct
    This depends on a different aspect of the
    system than anything else in the driver,
    namely the order in which user processes
    ask for data and the current state of those
    processes.
I am not sure if the task_info struct changes enough that this
would accomplish the goal or if you'd need a per-task key as
you suggest. If you do need the key, this would be the place
to add it.
I have the code for addmul(), the mixer from AES-GCM, but
not the actual code that would apply it here.

@_date: 2014-09-15 23:05:22
@_author: Sandy Harris 
@_subject: [Cryptography] RFC possible changes for Linux random device 
That is a very good idea.
In the long run, I doubt that either output pool is necessary.
For /dev/random anything that has equal amounts of data
in and out (so it guarantees the entropy) and is provably
non-invertible (so it cannot be used to attack the input
pool) is enough.

@_date: 2014-09-16 11:05:14
@_author: Sandy Harris 
@_subject: [Cryptography] Simple non-invertible function? 
Yes, but their key recommendation, page 33, is: "Backtracking
resistance can be provided by ensuring that the DRBG generate
algorithm is a one-way function."
Hence my question about simple functions with that property.
I agree. That is the obvious way to do it and the Preneel
et al. papers have all the analysis it needs.
It seems to me, though, that the question is worth some
exploration earlier.

@_date: 2014-09-16 11:49:13
@_author: Sandy Harris 
@_subject: [Cryptography] RFC possible changes for Linux random device 
There are at least two ways to reduce the dependence on AES security
while using AES.
The Preneel et al papers prove that encrypt(X, key) xor X is
non-invertible. Apply that to a counter-mode AES and you get a
guarantee the enemy cannot use the output data to attack the AES
instance. There are details and limits that apply, but in principle
this solves a large part of the problem.
The Even and Mansour paper looks at the xor-permutation-xor structure
and proves that for n-bit blocks and D known or chosen plaintexts, the
time T to break it is bounded by DT >= 2^n . We can choose D to make
that bound close to 2^128, and this is a provable minimum security
level not based on an assumption that there is no attack on AES better
than brute force. In fact their proof assumes the permutation is
broken -- the enemy has oracles that give him permutation output for
any input and vice versa. All we need from AES in this structure is
that it be a permutation, a requirement it obviously meets.
A Shamir et al paper shows that the Even-Mansour bounds holds with +
instead of xor and the pre-whitening and post-whitening data the same.
That means we can use:
     start with counter as plaintext
     whiten with +
     AES
     whiten with +
     xor in counter to guarantee non-invertibility
My submission to the CAESAR competition uses some of these ideas (not
the non-invertibility since that would be remarkably undesirable in a
cipher) and has a fairly detailed analysis:

@_date: 2014-09-18 12:13:55
@_author: Sandy Harris 
@_subject: [Cryptography] Simple non-invertible function? 
My current code uses idea_multiply(x, key) xor x which I think
is non-invertible. Then there's a GCM hash mixing step so I do
not think the 16-bit blocks are a problem.

@_date: 2014-09-19 12:21:20
@_author: Sandy Harris 
@_subject: [Cryptography] Simple non-invertible function? 
Yes. The Preneel at al. paper classifying ways to use a block cipher
to build the compression function for a hash looks at an exhaustive
list of 64 possible constructions and says 12 are secure. A block
cipher must be invertible (given the key) but a hash compression
function should not be, so all the ones they consider secure are
non-invertible in the sense I mean.
For example, given a secure block cipher, encrypt( x, key) xor x
works. So does the construction used in Salsa or ChaCha: copy input
into temp[], mix temp[] thoroughly, add the result back into the input
to get the output.
No, a hash needs lots of other properties: adequate bit length, good
mixing across that whole length, overall efficiency, resistance to
various attacks, ... You can get all those from a suitable choice of
hash algorithm or a block cipher used in a mode from Preneel at al.
but I'm not looking for them all, just the non-invertible property.
My context is that we have hashed a pool and want to use the resulting
data twice, once as feedback into the pool and once for output. The
two should be different so we need to apply some transform, use
untransformed data as the feedback and transformed data for output. We
have already paid the overheads of hashing, so we want a cheap
function here but it should be non-invertible.
My current code, different from what I posted earlier, is:
    /*
    create a temp[] value for use in generating output
    different from data used in feedback
    non-invertible encrypt(x, key) xor x transform
    (see Preneel, Govaerts & Vandewalle)
    "block cipher" is IDEA multiplication of input & key
    16-bit blocks here, so mixing is needed later
    */
    memcpy( temp, buffer, 16 ) ;
    idea128( temp, p->A ) ;
    xor128( temp, buffer ) ;
    /* feed untransformed result back into the pool */
    buffer2pool( p, buffer ) ;
    /*
    apply another hash step using different constants B, D
    and the transformed value in temp[]
    */
    memcpy( buffer, p->B, 16 ) ;
    addmul( (byte *) buffer, (byte *) temp, 16, (byte *) p->D) ;
    xor128( buffer, p->D ) ;
    zero128( temp ) ;
I suspect there is a better way to achieve my goal here, without the
cost of a full block cipher. However, I have not found one.

@_date: 2014-09-29 11:35:35
@_author: Sandy Harris 
@_subject: [Cryptography] Based on Even Mansour 
I think you need to read their paper more carefully. They assume the
enemy has oracles that give permutation output for any input & vice
versa, and define the time T to break the cipher as the number of
calls to those oracles required. This is roughly equivalent to
assuming a broken block cipher and asking how many cipher iterations
you need to find the whitening.
What they prove is a lower bound; for an n-bit permutation, 2n bits of
whitening and D known or chosen plaintexts, you have:
      DT >= 2^n
With  2^(n/2) chosen plaintexts, that only gives T > 2^(n/2) which is
not necessarily secure.

@_date: 2015-07-05 09:32:44
@_author: Sandy Harris 
@_subject: [Cryptography] Cuutilaimed generic attack on NP-complete problems 
Memcomputing NP-complete problems in polynomial time using polynomial
resources and collective states
If this is real, what are the implications for crypto? For factoring?
Discrete og?

@_date: 2015-11-09 15:40:26
@_author: Sandy Harris 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
Or do what AES-GCM does; AES encrypt an all-zero block to get a key to
use in authentication.

@_date: 2015-11-24 11:16:34
@_author: Sandy Harris 
@_subject: [Cryptography] basic cryptography ... was: key breaking 
Sounds like a stream cipher. If having key as long as the
message was practical, we could go to one time pads,
but it isn't so use a keyed PRNG to give enough key.
I do not think that, in general, that is enough. Most stream
ciphers just XOR to mix generator output with plaintext, so
any enemy with known plaintext can recover generator
output at trivial cost.
For details, see:
Enchilada also uses ChaCha to generate different whitening
for every block and I claim, based on the Even-Mansour
proof for XOR-permutation-XOR, that it therefore gives
provable /minimum/ security of 2^n where n is the block
cipher's block size.
That claim needs more analysis and it is worth noting that
Enchilada did not make it into the second round of the
CAESAR competition.

@_date: 2015-10-21 12:01:58
@_author: Sandy Harris 
@_subject: [Cryptography] Randomness for crypto, github repositories 
I've just created github repositories for two projects:
Test program for things I want to add to the Linux random(4) driver. I
am proposing a fairly radical rewrite. This gives an executable test
program for my new code, not a driver.
A demon to feed random(4) with entropy derived from the timer.
Intended mainly for use on limited systems which may lack other good

@_date: 2015-10-21 13:28:08
@_author: Sandy Harris 
@_subject: [Cryptography] Randomness for crypto, github repositories 
Yes, the basic idea is to use a series of GCM multiplies over the pool
data to replace the hashing of that data in the current driver. There
are complications; each hash uses two quasi-constants -- initialiser
and GCM multiplier -- and hashes a counter along with the pool data.
The counter changes on every iteration and is sometimes changed more
drastically, and the constants are sometimes updated
Good question. It seems to me th at if it is secure for its
authentication usage, where it replaces an HMAC, then it should be
safe in this application. But no, I don't have a proof & the question
is worth some analysis.

@_date: 2017-08-16 09:56:00
@_author: Sandy Harris 
@_subject: [Cryptography] Fwd:  Would haveged ever be cryptanalyzed? 
I am not sure what you are talking about here. I fail to see how
either sentence relates to Havege & or what attacks on AES have to do
with interrupts.
There are a lot of papers claiming they are unpredictable enough to
give good entropy with appropriate processing. For Havege, see:

@_date: 2017-08-28 14:38:51
@_author: Sandy Harris 
@_subject: [Cryptography] Turing letters 
!48 letters 1949-54 found at Manchester University:

@_date: 2017-07-10 19:00:00
@_author: Sandy Harris 
@_subject: [Cryptography] A software for combining text files to obtain 
I do not think it is viable, because it is not random; it depends
entirely on the input text. One article puts total size of the
web at 1.2 petabytes
Ignoring the facts that much of that is graphics, sound & video,
and another bunch is in languages other than English.
so your scheme might not work, and there's a whole lot
of duplication ...
That's 2^50 bytes so there are only 2^50 places to start
your hash or other algorithm. If you use local text only,
you might get 2^45 bytes (32 terabytes) or some such
on a big RAID array with current technology.
This is not nearly enough for any serious application,
& you still need a 2^45 or 2^50-bit seed from a true
RNG to choose a random starting point or it does
not work at all.
You are far better off using AES-CTR which has
far lower overheads than your data access
would require, and can take a larger key, 128
bits to initialise the counter then another 128,
192 or 256 for the actual key.
You can choose a stopping
point too, and maybe use some local non-web text as
well so it looks like you might get up to 2^64 or a bit
more possible different outputs.

@_date: 2017-07-11 20:03:35
@_author: Sandy Harris 
@_subject: [Cryptography] A software for combining text files to obtain 
The RFC that is more-or-less the standard reference
on RNGs has a section on this:
That is a subsection of "6.1.  Some Bad Ideas"

@_date: 2017-09-01 07:40:53
@_author: Sandy Harris 
@_subject: [Cryptography] TEXTCOMBINE-REV, 
pseudo-randomness in practice (replacing an earlier retracted software)

@_date: 2018-08-20 20:21:05
@_author: Sandy Harris 
@_subject: [Cryptography] A new method to (partly) factorize RSA 250 and 
Are you saying you have a general method to check approximately
N^(1/4) factors in parallel? Even if it applies only to factors of the
form 6n-1, that is interesting.

@_date: 2018-10-13 21:30:18
@_author: Sandy Harris 
@_subject: [Cryptography] China Spies In SuperMicro Mobos - Exemplar 
wrote
Given the power of current CPUs, the special instructions some have to
accelerate crypto, and the overheads of getting to & from a crypto
accelerator, those devices may be useless in many cases. Certainly
they are rarely necessary.
The exception is a hardware RNG which is nearly essential in many
applications. Of course it is subject to almost undetectable attack at
the design/manufacturing level, so its output should be mixed with
other entropy.

@_date: 2018-10-26 23:04:29
@_author: Sandy Harris 
@_subject: [Cryptography] Random permutation model for encryption as a 
Best book I know:
I'd be inclined to treat transposition & substitution briefly & go from
them to confusion & diffusion, then modern ciphers. I think both
the key-chooses-a-permutation model & the SP network design
go back to Shannon. Cover those briefly, then Feistel & DES,
(round function is an SPN), finally a bit about AES.
An encyclopedia article that is largely my writing is far from
complete, but I think a reasonable example of a logical
Nor I. My Enchilada submission to the CAESAR authenticated cipher
competition has a possible solution. Run a block cipher (AES) in a
variant of counter mode to get a stream cipher. Embed the block cipher
in an XOR-permutation-XOR structure (proven secure by Even & Mansour)
with another stream cipher (ChaCha) generating a new XOR key per block
cipher block.
Enchilada did not make it out of the first round of the competition.

@_date: 2018-09-01 10:05:10
@_author: Sandy Harris 
@_subject: [Cryptography] Is "perfect forward secrecy" the biggest fraud 
Yes. If the asymmetric crypto is broken then many things fail.
In some systems, that is catastrophic; for example for PGP
if I learn your private key I can read all your messages since
the symmetric algorithm keys are protected only by the
asymmetric encryption.
For a key agreement algorithm like DH, the asymmetric crypto
is generally only used to authenticate the players. Breaking it
does not give an attacker any of the symmetric session keys;
it only lets him conduct man-in-the-middle attacks toget them,
and it takes one MITM per session key.
Forward secrecy means that breaking the asymmetric crypto
does not let him read either old messages that he may have
archived or new messages for which he has not conducted
a successful MITM attack. That is a useful property. As I
see it, no sane person would specify a key agreement
protocol without it.
On the other hand if quantum computers can solve the
discrete log problem efficiently, then DH goes belly up.
That would let them break the protocol completely,
reading new messages without MITM and reading old
ones despite forward secrecy, provided they had also
archived the DH exchanges.
